INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "286"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-15 10:17:10.120569: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:17:10.120642: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:17:10.120668: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:17:10.120689: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:17:10.120819: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 10:17:11.031746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-15 10:17:11.031817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-15 10:17:11.031844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-15 10:17:11.031870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc_2/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc_2/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_2/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_3/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_3/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_3/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-15 10:17:16.617654: step 0, loss = 2.28, batch loss = 2.23 (2.0 examples/sec; 3.983 sec/batch; 367h:51m:17s remains)
2017-12-15 10:17:17.295375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4195313 -4.4195023 -4.4194784 -4.4194551 -4.4194465 -4.4194655 -4.419497 -4.4195318 -4.4195642 -4.4195862 -4.4195943 -4.4195857 -4.4195752 -4.4195681 -4.4195662][-4.4195695 -4.4195437 -4.419528 -4.4195137 -4.4195118 -4.419528 -4.41955 -4.4195704 -4.4195867 -4.4195962 -4.4195952 -4.4195862 -4.4195805 -4.4195786 -4.419579][-4.4195819 -4.4195642 -4.4195571 -4.4195514 -4.4195528 -4.4195623 -4.4195752 -4.4195843 -4.4195881 -4.4195857 -4.4195776 -4.4195681 -4.4195662 -4.4195642 -4.419558][-4.419559 -4.41955 -4.4195495 -4.4195433 -4.4195375 -4.4195371 -4.4195428 -4.41955 -4.4195523 -4.4195485 -4.4195404 -4.4195347 -4.4195361 -4.419529 -4.4195051][-4.4195018 -4.4195018 -4.4195032 -4.4194889 -4.4194655 -4.4194465 -4.4194455 -4.4194593 -4.4194756 -4.4194846 -4.419487 -4.4194913 -4.4194961 -4.4194832 -4.4194388][-4.4194303 -4.4194388 -4.4194431 -4.4194202 -4.4193764 -4.4193339 -4.4193292 -4.41936 -4.4193983 -4.4194279 -4.4194474 -4.419466 -4.4194808 -4.4194722 -4.41943][-4.419405 -4.4194269 -4.4194374 -4.4194164 -4.41937 -4.4193211 -4.4193153 -4.4193506 -4.4193959 -4.4194336 -4.4194613 -4.419488 -4.4195127 -4.419518 -4.4194937][-4.4194322 -4.419466 -4.4194832 -4.4194727 -4.4194403 -4.4194 -4.4193883 -4.4194069 -4.41944 -4.4194741 -4.4195023 -4.4195275 -4.4195533 -4.4195642 -4.4195533][-4.4194703 -4.4195018 -4.4195156 -4.4195075 -4.4194837 -4.41945 -4.4194326 -4.4194388 -4.4194646 -4.4194956 -4.4195204 -4.41954 -4.4195619 -4.4195733 -4.4195714][-4.4194903 -4.4195113 -4.4195151 -4.4195018 -4.4194775 -4.4194469 -4.4194264 -4.4194283 -4.4194555 -4.4194875 -4.4195118 -4.4195309 -4.41955 -4.4195614 -4.4195681][-4.4194865 -4.4194984 -4.4194975 -4.4194827 -4.4194584 -4.4194303 -4.4194088 -4.4194093 -4.4194407 -4.4194746 -4.4194994 -4.4195185 -4.4195371 -4.4195504 -4.4195623][-4.4194717 -4.4194784 -4.4194775 -4.4194665 -4.4194446 -4.4194188 -4.4193969 -4.4193935 -4.4194236 -4.4194574 -4.419486 -4.41951 -4.4195294 -4.4195442 -4.419558][-4.4194636 -4.4194684 -4.4194722 -4.4194703 -4.4194579 -4.4194403 -4.4194236 -4.4194164 -4.4194379 -4.419466 -4.4194937 -4.4195189 -4.4195385 -4.4195514 -4.41956][-4.4194651 -4.4194732 -4.419487 -4.4195013 -4.4195042 -4.419497 -4.4194889 -4.4194856 -4.4195027 -4.4195261 -4.4195471 -4.4195633 -4.4195743 -4.4195776 -4.4195728][-4.4194808 -4.4194946 -4.4195204 -4.4195461 -4.4195561 -4.4195523 -4.41955 -4.419549 -4.4195614 -4.4195795 -4.4195938 -4.4196029 -4.4196081 -4.4196043 -4.41959]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 10:17:25.028342: step 10, loss = 1.99, batch loss = 1.93 (12.3 examples/sec; 0.652 sec/batch; 60h:15m:32s remains)
INFO - root - 2017-12-15 10:17:31.437622: step 20, loss = 0.77, batch loss = 0.70 (12.9 examples/sec; 0.620 sec/batch; 57h:14m:41s remains)
INFO - root - 2017-12-15 10:17:37.833001: step 30, loss = 0.54, batch loss = 0.47 (12.3 examples/sec; 0.649 sec/batch; 59h:57m:47s remains)
INFO - root - 2017-12-15 10:17:44.285107: step 40, loss = 0.58, batch loss = 0.51 (12.6 examples/sec; 0.633 sec/batch; 58h:25m:52s remains)
INFO - root - 2017-12-15 10:17:50.669035: step 50, loss = 0.46, batch loss = 0.39 (12.9 examples/sec; 0.618 sec/batch; 57h:06m:20s remains)
INFO - root - 2017-12-15 10:17:57.106584: step 60, loss = 0.67, batch loss = 0.60 (12.2 examples/sec; 0.656 sec/batch; 60h:36m:27s remains)
INFO - root - 2017-12-15 10:18:03.561468: step 70, loss = 0.59, batch loss = 0.52 (12.7 examples/sec; 0.629 sec/batch; 58h:04m:57s remains)
INFO - root - 2017-12-15 10:18:10.062704: step 80, loss = 0.62, batch loss = 0.55 (12.2 examples/sec; 0.656 sec/batch; 60h:35m:09s remains)
INFO - root - 2017-12-15 10:18:16.521258: step 90, loss = 0.57, batch loss = 0.50 (12.0 examples/sec; 0.668 sec/batch; 61h:40m:02s remains)
INFO - root - 2017-12-15 10:18:23.014494: step 100, loss = 0.46, batch loss = 0.39 (12.1 examples/sec; 0.662 sec/batch; 61h:07m:49s remains)
2017-12-15 10:18:23.637916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.83136296 -0.47225118 0.1168716 0.78845668 1.2385671 1.3043149 0.96063161 0.39189982 -0.20509052 -0.67129326 -0.94365835 -1.1651604 -1.2866311 -1.3180637 -1.4218192][-0.54764128 -0.2220943 0.3101089 0.88565373 1.1883519 1.1005857 0.62196231 -0.068059206 -0.76317739 -1.3056877 -1.6349819 -1.8683767 -1.9458694 -1.8907862 -1.8815007][-0.23771572 0.090805292 0.57099557 1.0128744 1.1410005 0.90319753 0.32472444 -0.42479038 -1.1678395 -1.7545788 -2.1224368 -2.3843231 -2.462019 -2.3544526 -2.2330828][0.067599535 0.45026803 0.92318082 1.2881157 1.3099411 0.97818685 0.34660554 -0.40698051 -1.1402991 -1.7584486 -2.1825688 -2.4987693 -2.6158986 -2.4950533 -2.2989936][0.30619264 0.78106809 1.2996933 1.6608851 1.6611574 1.3335207 0.73732257 0.026692629 -0.678915 -1.3025203 -1.7659168 -2.1505044 -2.3347492 -2.2404823 -2.032454][0.35558963 0.9692409 1.5861132 2.0257075 2.1145966 1.8875644 1.3862503 0.74789405 0.080247641 -0.53915071 -1.0257564 -1.4562922 -1.6909182 -1.633328 -1.454174][0.16546512 0.89889216 1.6261294 2.1872818 2.42757 2.3766177 2.0397708 1.5159752 0.91787267 0.33703542 -0.13645411 -0.58710122 -0.85465932 -0.8467207 -0.74789882][-0.19204092 0.60516906 1.413686 2.0873067 2.480057 2.6111233 2.4449203 2.0539367 1.5602496 1.0678823 0.685169 0.28531289 0.019442797 -0.016658306 -0.017474413][-0.54044318 0.20720363 1.0070736 1.7305992 2.2255666 2.496733 2.4783933 2.2129524 1.8386819 1.463969 1.2085764 0.90834403 0.68972278 0.63039279 0.52567983][-0.71008086 -0.088820696 0.61385179 1.2783406 1.7692897 2.0965979 2.1752532 2.0159533 1.7490022 1.4952896 1.3724854 1.1727512 0.99573684 0.911854 0.73432565][-0.99872088 -0.55771208 -0.043394089 0.45796227 0.83498979 1.1021078 1.1925266 1.1006076 0.92381978 0.77038217 0.74408841 0.65054679 0.55301404 0.49564958 0.31181979][-1.3560789 -1.0870862 -0.77282691 -0.47933078 -0.28748059 -0.17551231 -0.16987014 -0.27502322 -0.417912 -0.51186252 -0.48354554 -0.49258351 -0.50250125 -0.47920203 -0.5667038][-1.9292517 -1.7919877 -1.6539345 -1.5578728 -1.5653479 -1.6515896 -1.7931502 -1.9797971 -2.1519344 -2.2500319 -2.2169127 -2.1479235 -2.0279033 -1.8530989 -1.7596152][-2.4952986 -2.4457271 -2.4297791 -2.4792459 -2.6542544 -2.9287715 -3.244477 -3.565696 -3.83732 -4.0052848 -4.0150928 -3.887929 -3.6185627 -3.23831 -2.8972034][-2.886425 -2.8622398 -2.901412 -3.0271256 -3.31568 -3.7286632 -4.1896877 -4.6411743 -5.0342283 -5.3182049 -5.4166141 -5.2844291 -4.9078865 -4.3604822 -3.8078573]]...]
INFO - root - 2017-12-15 10:18:30.114769: step 110, loss = 0.51, batch loss = 0.44 (12.5 examples/sec; 0.640 sec/batch; 59h:05m:22s remains)
INFO - root - 2017-12-15 10:18:36.664615: step 120, loss = 0.64, batch loss = 0.57 (12.0 examples/sec; 0.668 sec/batch; 61h:39m:42s remains)
2017-12-15 10:18:42.573915: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 60861 get requests, put_count=60852 evicted_count=1000 eviction_rate=0.0164333 and unsatisfied allocation rate=0.0182218
2017-12-15 10:18:42.573948: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
INFO - root - 2017-12-15 10:18:43.209738: step 130, loss = 0.49, batch loss = 0.42 (12.6 examples/sec; 0.633 sec/batch; 58h:28m:05s remains)
INFO - root - 2017-12-15 10:18:49.720159: step 140, loss = 0.44, batch loss = 0.37 (12.7 examples/sec; 0.630 sec/batch; 58h:09m:53s remains)
INFO - root - 2017-12-15 10:18:56.259254: step 150, loss = 0.49, batch loss = 0.42 (12.3 examples/sec; 0.648 sec/batch; 59h:51m:31s remains)
INFO - root - 2017-12-15 10:19:02.878534: step 160, loss = 0.45, batch loss = 0.38 (12.3 examples/sec; 0.649 sec/batch; 59h:57m:10s remains)
INFO - root - 2017-12-15 10:19:09.409880: step 170, loss = 0.50, batch loss = 0.43 (11.8 examples/sec; 0.676 sec/batch; 62h:22m:23s remains)
INFO - root - 2017-12-15 10:19:15.866962: step 180, loss = 0.63, batch loss = 0.56 (12.4 examples/sec; 0.644 sec/batch; 59h:29m:11s remains)
INFO - root - 2017-12-15 10:19:22.420473: step 190, loss = 0.45, batch loss = 0.38 (12.4 examples/sec; 0.647 sec/batch; 59h:42m:36s remains)
INFO - root - 2017-12-15 10:19:29.035917: step 200, loss = 0.40, batch loss = 0.33 (12.3 examples/sec; 0.653 sec/batch; 60h:15m:16s remains)
2017-12-15 10:19:29.615528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5080514 -3.3485901 -3.2064981 -3.2516661 -3.4176767 -3.5265493 -3.5600872 -3.481626 -3.3606114 -3.1945565 -3.0170341 -3.1915655 -3.6171808 -4.0620704 -4.51508][-3.439302 -3.3563666 -3.3321812 -3.4009824 -3.5384471 -3.6180308 -3.5791578 -3.4881129 -3.3644378 -3.1887438 -2.9938078 -3.130446 -3.5296879 -3.9516218 -4.396348][-2.8830252 -2.9321873 -2.9762859 -3.1302631 -3.3278329 -3.3227556 -3.180934 -3.00804 -2.8040621 -2.5893657 -2.3716493 -2.5373678 -2.9915164 -3.4638302 -3.969243][-2.2873125 -2.2984169 -2.3350811 -2.5423398 -2.6928945 -2.6751394 -2.5045595 -2.2466164 -1.9879992 -1.7550912 -1.5459394 -1.7486765 -2.270896 -2.8269198 -3.4006832][-1.6566324 -1.6782477 -1.7060447 -1.8486562 -1.9827433 -1.9563341 -1.7389474 -1.474288 -1.219959 -0.973181 -0.77363825 -1.0068555 -1.5843947 -2.224906 -2.8840942][-1.062417 -1.0774398 -1.0791409 -1.2008076 -1.3265119 -1.2310753 -1.0213754 -0.78557968 -0.55396771 -0.33388186 -0.15098 -0.3892684 -0.99788523 -1.7143142 -2.5447531][-0.73208904 -0.6250577 -0.554368 -0.58516169 -0.6310041 -0.56234217 -0.41667151 -0.25126052 -0.098320961 0.093325138 0.31947041 0.13896751 -0.43976712 -1.2042761 -2.1468887][-0.60578322 -0.31035781 -0.12300181 -0.1462183 -0.21517324 -0.1555829 -0.062843323 0.044236183 0.17504787 0.35781765 0.59099054 0.46730566 -0.05181694 -0.849308 -1.8800089][-0.56087255 -0.11850595 0.18773174 0.20146608 0.10551023 0.076973915 0.08434391 0.14996433 0.25942278 0.45256138 0.71835375 0.62622261 0.11525154 -0.66653419 -1.686744][-0.69101167 -0.15621805 0.24349213 0.31556177 0.24030161 0.17800665 0.15172958 0.18348265 0.27319765 0.47862959 0.75122452 0.70301628 0.28264904 -0.46773124 -1.4742184][-1.1443875 -0.62715816 -0.20717192 -0.029140949 -0.010777473 -0.014203548 -0.016497612 0.010328293 0.10454798 0.29309177 0.52932262 0.53375864 0.2330966 -0.3453455 -1.1201909][-1.6413982 -1.231461 -0.82812858 -0.566401 -0.43841267 -0.33175421 -0.26706386 -0.20266199 -0.13943815 -0.050579548 0.11094713 0.11613369 -0.044114113 -0.32808256 -0.8068893][-2.211092 -1.8741469 -1.5215156 -1.1835563 -0.89480066 -0.65839624 -0.50730276 -0.42684245 -0.42381883 -0.45461559 -0.39747596 -0.37362885 -0.34934664 -0.40391159 -0.59835577][-2.7678428 -2.4222765 -2.067205 -1.6665826 -1.2756855 -0.93639207 -0.72160792 -0.65557814 -0.71190238 -0.80455637 -0.80564737 -0.75385451 -0.59729934 -0.49376321 -0.511961][-3.17484 -2.8168941 -2.4310942 -2.0385041 -1.6166263 -1.2509053 -1.0320444 -0.99901795 -1.1141603 -1.2838004 -1.3410268 -1.2612524 -1.0634501 -0.92684364 -0.96125889]]...]
INFO - root - 2017-12-15 10:19:36.220811: step 210, loss = 0.46, batch loss = 0.40 (11.8 examples/sec; 0.675 sec/batch; 62h:20m:15s remains)
INFO - root - 2017-12-15 10:19:42.774385: step 220, loss = 0.43, batch loss = 0.36 (12.0 examples/sec; 0.665 sec/batch; 61h:21m:51s remains)
INFO - root - 2017-12-15 10:19:49.427318: step 230, loss = 0.43, batch loss = 0.36 (12.2 examples/sec; 0.653 sec/batch; 60h:16m:35s remains)
INFO - root - 2017-12-15 10:19:56.047838: step 240, loss = 0.44, batch loss = 0.37 (12.0 examples/sec; 0.668 sec/batch; 61h:38m:51s remains)
INFO - root - 2017-12-15 10:20:02.609973: step 250, loss = 0.34, batch loss = 0.27 (12.5 examples/sec; 0.641 sec/batch; 59h:09m:55s remains)
INFO - root - 2017-12-15 10:20:09.293443: step 260, loss = 0.45, batch loss = 0.38 (12.2 examples/sec; 0.654 sec/batch; 60h:19m:58s remains)
INFO - root - 2017-12-15 10:20:15.785862: step 270, loss = 0.43, batch loss = 0.36 (12.5 examples/sec; 0.642 sec/batch; 59h:13m:04s remains)
INFO - root - 2017-12-15 10:20:22.322486: step 280, loss = 0.47, batch loss = 0.40 (12.6 examples/sec; 0.635 sec/batch; 58h:34m:50s remains)
INFO - root - 2017-12-15 10:20:28.984555: step 290, loss = 0.41, batch loss = 0.35 (12.2 examples/sec; 0.657 sec/batch; 60h:39m:11s remains)
INFO - root - 2017-12-15 10:20:35.587937: step 300, loss = 0.46, batch loss = 0.39 (12.0 examples/sec; 0.669 sec/batch; 61h:43m:31s remains)
2017-12-15 10:20:36.133880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.124155 -1.7185497 -1.471616 -1.4697628 -1.6680393 -1.8832848 -2.0562983 -1.9779124 -1.6037056 -0.95945406 -0.29601645 -0.48548126 -1.3047681 -2.2683773 -3.3515911][-1.9097245 -1.5370336 -1.3068757 -1.3558588 -1.5911369 -1.8381062 -2.0774472 -1.9676201 -1.5176499 -0.84614038 -0.25521159 -0.54001594 -1.4332547 -2.5288525 -3.5492394][-1.4906893 -1.0288954 -0.77389145 -0.85207415 -1.18523 -1.5546262 -1.8084793 -1.7253006 -1.2245038 -0.63736439 -0.17973948 -0.49513865 -1.4434505 -2.498116 -3.5643113][-1.1382132 -0.66332841 -0.35075545 -0.39777756 -0.69850588 -1.0504704 -1.2871244 -1.0883276 -0.5283246 0.081228256 0.43598938 -0.18211293 -1.2742047 -2.4237857 -3.4988308][-0.748832 -0.33685136 -0.10540295 -0.17965579 -0.49827909 -0.8811419 -1.0894876 -0.94695234 -0.2623148 0.42030573 0.79351377 0.29144144 -0.92376304 -2.1805756 -3.35143][-0.029623985 0.3897357 0.57896757 0.38411045 -0.044528961 -0.52757049 -0.85526586 -0.67848182 -0.019689083 0.75977373 1.2117882 0.497962 -0.78295469 -2.1955485 -3.5121288][0.92185163 1.4099331 1.5753469 1.3721933 0.85011387 0.30125618 -0.051661015 0.081537724 0.67634487 1.329339 1.6237741 0.88570738 -0.58792305 -2.2447474 -3.6679623][1.7162347 2.1198745 2.3064542 2.0440111 1.4906654 0.91188622 0.63090754 0.76558733 1.2161479 1.7728491 2.0009956 1.1307912 -0.3493011 -1.9264476 -3.4213469][2.059854 2.3755169 2.5152669 2.4529004 2.0700598 1.5526147 1.2735491 1.3182092 1.5516267 1.9220939 2.0934029 1.4372892 0.079817295 -1.4180658 -2.8375645][1.9466047 2.2217364 2.3712316 2.375824 2.2177916 1.9085522 1.6903858 1.7048674 1.7226224 1.8632398 1.7791047 1.102458 -0.090693951 -1.3643525 -2.5543656][0.44915676 0.64532232 0.70728827 0.73987722 0.58214855 0.28304911 0.075848579 -0.0013608932 0.0044150352 0.18107033 0.11267424 -0.48124719 -1.4711895 -2.3931231 -3.2378366][-1.1701095 -1.1343105 -1.1406331 -1.1205299 -1.2555068 -1.4731238 -1.6474535 -1.6642919 -1.7410462 -1.7041218 -1.7020187 -2.0005882 -2.6122088 -3.2789803 -3.7996426][-2.4829946 -2.493155 -2.5292318 -2.5961971 -2.6918979 -2.8235295 -2.9287987 -2.9289217 -2.955363 -2.8256497 -2.8318529 -3.0621371 -3.4787285 -3.9522471 -4.3037586][-3.7300773 -3.7613089 -3.8243487 -3.8740776 -3.94442 -4.0638103 -4.145062 -4.1130943 -4.065259 -4.03011 -4.0777044 -4.227026 -4.5299087 -4.9246569 -5.1284862][-4.7632332 -4.8893037 -5.013042 -5.1564889 -5.3112087 -5.436295 -5.509902 -5.4499674 -5.3796124 -5.3085928 -5.3353519 -5.4785662 -5.7391033 -5.9970889 -6.1159859]]...]
INFO - root - 2017-12-15 10:20:42.678336: step 310, loss = 0.44, batch loss = 0.38 (12.3 examples/sec; 0.652 sec/batch; 60h:10m:58s remains)
INFO - root - 2017-12-15 10:20:49.259086: step 320, loss = 0.35, batch loss = 0.29 (12.5 examples/sec; 0.640 sec/batch; 59h:03m:24s remains)
INFO - root - 2017-12-15 10:20:55.950549: step 330, loss = 0.46, batch loss = 0.39 (12.1 examples/sec; 0.659 sec/batch; 60h:50m:36s remains)
INFO - root - 2017-12-15 10:21:02.444412: step 340, loss = 0.39, batch loss = 0.33 (12.4 examples/sec; 0.647 sec/batch; 59h:41m:11s remains)
INFO - root - 2017-12-15 10:21:09.013224: step 350, loss = 0.36, batch loss = 0.29 (11.6 examples/sec; 0.688 sec/batch; 63h:28m:49s remains)
INFO - root - 2017-12-15 10:21:15.571627: step 360, loss = 0.42, batch loss = 0.36 (11.9 examples/sec; 0.672 sec/batch; 62h:00m:20s remains)
INFO - root - 2017-12-15 10:21:22.142668: step 370, loss = 0.49, batch loss = 0.43 (12.4 examples/sec; 0.648 sec/batch; 59h:45m:44s remains)
INFO - root - 2017-12-15 10:21:28.782693: step 380, loss = 0.39, batch loss = 0.32 (12.2 examples/sec; 0.655 sec/batch; 60h:24m:42s remains)
INFO - root - 2017-12-15 10:21:35.275300: step 390, loss = 0.37, batch loss = 0.30 (12.3 examples/sec; 0.650 sec/batch; 59h:56m:10s remains)
INFO - root - 2017-12-15 10:21:41.987828: step 400, loss = 0.36, batch loss = 0.29 (12.2 examples/sec; 0.656 sec/batch; 60h:30m:20s remains)
2017-12-15 10:21:42.502000: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.098715782 -0.15975523 -0.42131495 -0.31654954 -0.36100268 -0.22864962 -0.10834074 0.04366684 0.0053906441 -0.18002939 -0.37044096 -1.5270827 -2.8089342 -3.7050819 -3.9865408][-0.7112844 -0.92003441 -1.1256394 -1.1928523 -1.2328739 -0.98750687 -0.62490129 -0.50779033 -0.64276958 -0.85885477 -1.0549791 -2.1252635 -3.4629574 -4.2951775 -4.5171061][-1.1660883 -1.4005866 -1.7501674 -1.9839616 -2.1288445 -1.7765524 -1.2898839 -1.0101628 -1.0345416 -1.2238326 -1.4131017 -2.5792265 -3.9708445 -4.9038267 -5.1332378][-1.0103443 -1.3817472 -1.8914173 -2.0921857 -2.0774689 -1.5419283 -0.92117262 -0.61569 -0.67135406 -1.0185547 -1.3249986 -2.545711 -3.9287176 -4.8793154 -5.3246145][-0.92799854 -1.2941902 -1.7653332 -1.737258 -1.4750326 -0.7645998 -0.112288 0.24757338 0.20818853 -0.22879887 -0.74348593 -2.073617 -3.6214075 -4.5414815 -4.9263682][-1.1953995 -1.4037795 -1.7058318 -1.3274763 -0.820091 0.10989332 0.85921907 1.0691738 1.1859193 0.83235836 0.12935209 -1.5026047 -3.3158774 -4.4186268 -4.9435849][-0.94371128 -1.0928242 -1.2524729 -0.65793228 -0.018989563 0.92545652 1.6155667 1.9741564 1.9813781 1.7497969 1.4236512 -0.40414619 -2.459136 -3.8075573 -4.5746889][-0.82801628 -0.8696568 -0.88989043 -0.19104528 0.5618906 1.4451008 2.2197189 2.740799 2.7780585 2.443419 2.0451145 0.38580561 -1.6419821 -3.1827383 -4.0661635][-0.76422024 -0.7513864 -0.729301 -0.11662912 0.50511503 1.1177335 1.811595 2.4361663 2.8209829 2.5032859 2.1715612 0.77796459 -1.2851481 -2.8255782 -3.8193412][-0.68729997 -0.65161657 -0.63742208 -0.10858917 0.26638365 0.83703041 1.4568553 1.9724464 2.4633927 2.3513823 2.0404668 0.71404314 -0.95715046 -2.5853024 -3.6994851][-0.85803676 -0.82892036 -0.92320275 -0.55512428 -0.36737561 -0.091130733 0.19238663 0.67065144 1.0291424 1.2276464 1.1781301 0.01313448 -1.4996686 -2.7743874 -3.6353583][-2.5127165 -2.6493959 -2.69614 -2.633225 -2.6269121 -2.4470246 -2.2365627 -1.9502623 -1.7547419 -1.5618899 -1.3755031 -2.1871183 -3.4796081 -4.2147541 -4.5458665][-4.2824864 -4.5854034 -4.6641517 -4.6063013 -4.7743516 -4.8338509 -4.6664329 -4.4343777 -4.1005359 -3.7774415 -3.6359105 -4.2287941 -5.1447859 -5.6300025 -5.6784439][-5.5033755 -5.8440471 -6.0108738 -5.9316335 -6.1522655 -6.1812172 -5.9632959 -5.7494369 -5.5065007 -5.2195587 -5.0762782 -5.2700839 -5.6589265 -5.94845 -5.9474182][-5.6368132 -5.8791142 -5.9686604 -5.8711019 -6.0495863 -6.0621548 -5.9412937 -5.6112928 -5.3114104 -5.2907248 -5.3695593 -5.4444876 -5.5100608 -5.5191426 -5.2548766]]...]
INFO - root - 2017-12-15 10:21:49.033699: step 410, loss = 0.38, batch loss = 0.32 (12.3 examples/sec; 0.651 sec/batch; 60h:00m:54s remains)
INFO - root - 2017-12-15 10:21:55.554510: step 420, loss = 0.34, batch loss = 0.27 (12.7 examples/sec; 0.630 sec/batch; 58h:06m:45s remains)
INFO - root - 2017-12-15 10:22:02.130588: step 430, loss = 0.47, batch loss = 0.40 (11.6 examples/sec; 0.687 sec/batch; 63h:22m:53s remains)
INFO - root - 2017-12-15 10:22:08.655208: step 440, loss = 0.42, batch loss = 0.35 (12.4 examples/sec; 0.646 sec/batch; 59h:32m:44s remains)
INFO - root - 2017-12-15 10:22:15.181231: step 450, loss = 0.41, batch loss = 0.34 (12.2 examples/sec; 0.655 sec/batch; 60h:24m:19s remains)
INFO - root - 2017-12-15 10:22:21.701789: step 460, loss = 0.36, batch loss = 0.29 (12.2 examples/sec; 0.655 sec/batch; 60h:23m:54s remains)
INFO - root - 2017-12-15 10:22:28.346002: step 470, loss = 0.31, batch loss = 0.24 (11.9 examples/sec; 0.674 sec/batch; 62h:07m:05s remains)
INFO - root - 2017-12-15 10:22:34.848210: step 480, loss = 0.40, batch loss = 0.34 (12.7 examples/sec; 0.631 sec/batch; 58h:13m:38s remains)
INFO - root - 2017-12-15 10:22:41.460097: step 490, loss = 0.39, batch loss = 0.32 (12.1 examples/sec; 0.661 sec/batch; 60h:59m:48s remains)
INFO - root - 2017-12-15 10:22:47.947187: step 500, loss = 0.37, batch loss = 0.30 (12.3 examples/sec; 0.652 sec/batch; 60h:05m:08s remains)
2017-12-15 10:22:48.480876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8987415 -2.0504494 -2.2074349 -2.4863391 -2.9651103 -3.2693686 -3.3025825 -3.2001219 -3.0440378 -3.0120044 -2.9185219 -3.1528418 -3.427557 -3.6332676 -3.9303076][-2.8403282 -3.0777216 -3.2682991 -3.477849 -3.7457566 -3.8696973 -3.7442863 -3.4630272 -3.1444125 -2.9155703 -2.6779361 -2.7910824 -2.9886794 -3.249753 -3.6067672][-3.7757936 -3.8697829 -3.9233377 -3.8362544 -3.79579 -3.599014 -3.2126968 -2.8618081 -2.5848327 -2.3598058 -2.1780729 -2.6112618 -2.9799614 -3.1192398 -3.4061584][-3.6337566 -3.694931 -3.5457308 -3.1563926 -2.7718523 -2.3547735 -2.0298321 -1.8261962 -1.7390416 -1.7332172 -1.6138189 -1.9005499 -2.1608119 -2.3520947 -2.7539253][-3.1865261 -2.8263912 -2.4309831 -1.8955386 -1.3003869 -0.79738569 -0.60663223 -0.66656995 -0.78811193 -0.88794374 -0.84223437 -1.0075283 -1.298383 -1.6643844 -2.1366422][-2.2234511 -1.9030135 -1.4655328 -0.69999719 0.039945602 0.40897036 0.49327612 0.35874367 0.11620188 -0.082043648 -0.12045288 -0.639416 -1.2618518 -1.6339133 -2.0700107][-0.7533989 -0.40827847 0.19057226 1.0427585 1.7598481 2.0870032 1.9180689 1.6425529 1.1905255 0.53098869 0.097789288 -0.55740786 -1.177444 -1.7040582 -2.3186336][0.25194502 0.58125544 1.1767159 1.940608 2.5445185 2.7722383 2.6186194 2.1694603 1.5751472 1.030221 0.59059763 -0.34097528 -1.3289046 -2.0496497 -2.8495884][-0.12869835 0.064043522 0.37285566 1.0910072 1.7045212 2.0479116 2.1359305 1.9405637 1.5699143 1.3104777 1.0324516 -0.014765263 -1.198925 -2.171133 -3.1325727][-0.98660827 -1.0549397 -0.82026529 -0.25948477 0.36613131 0.80107164 0.91607285 0.94090652 0.86150503 0.68055964 0.50035524 -0.435776 -1.5550821 -2.630465 -3.6258132][-2.4639897 -2.5814738 -2.4424081 -2.3103833 -2.0789752 -1.7104888 -1.5615778 -1.5400419 -1.5759029 -1.5526471 -1.5122142 -2.1972966 -3.0850787 -3.8779576 -4.7199783][-4.4461079 -4.6502433 -4.8198919 -4.7754869 -4.6642756 -4.4957509 -4.4263716 -4.3810253 -4.2888689 -4.0871143 -3.9324787 -4.2793369 -4.7255254 -5.0304046 -5.4406805][-6.5911627 -6.929141 -7.2243452 -7.1929994 -7.090775 -6.882699 -6.6477261 -6.353199 -6.1136913 -5.9143906 -5.8029771 -5.8152723 -5.7797813 -5.80622 -5.9430108][-6.471272 -6.7826614 -7.0315142 -7.1325293 -6.9756594 -6.5794725 -6.2021427 -5.977994 -5.7704439 -5.5658488 -5.5646863 -5.6850853 -5.6651592 -5.7840233 -5.9964352][-6.1446571 -6.5397797 -6.6905107 -6.7355971 -6.4777918 -6.0366855 -5.6316385 -5.4076638 -5.1931353 -5.1939754 -5.3942065 -5.4575181 -5.5842648 -5.8629546 -6.1443496]]...]
INFO - root - 2017-12-15 10:22:54.953880: step 510, loss = 0.36, batch loss = 0.30 (12.5 examples/sec; 0.641 sec/batch; 59h:04m:41s remains)
INFO - root - 2017-12-15 10:23:01.419991: step 520, loss = 0.37, batch loss = 0.30 (12.5 examples/sec; 0.641 sec/batch; 59h:04m:21s remains)
INFO - root - 2017-12-15 10:23:07.885883: step 530, loss = 0.36, batch loss = 0.29 (12.6 examples/sec; 0.635 sec/batch; 58h:34m:31s remains)
INFO - root - 2017-12-15 10:23:14.341350: step 540, loss = 0.34, batch loss = 0.27 (12.4 examples/sec; 0.643 sec/batch; 59h:16m:41s remains)
INFO - root - 2017-12-15 10:23:20.845530: step 550, loss = 0.32, batch loss = 0.26 (12.6 examples/sec; 0.636 sec/batch; 58h:36m:34s remains)
INFO - root - 2017-12-15 10:23:27.377901: step 560, loss = 0.40, batch loss = 0.34 (12.2 examples/sec; 0.654 sec/batch; 60h:16m:17s remains)
INFO - root - 2017-12-15 10:23:33.937370: step 570, loss = 0.44, batch loss = 0.38 (12.4 examples/sec; 0.643 sec/batch; 59h:19m:52s remains)
INFO - root - 2017-12-15 10:23:40.438402: step 580, loss = 0.34, batch loss = 0.28 (12.2 examples/sec; 0.655 sec/batch; 60h:23m:16s remains)
INFO - root - 2017-12-15 10:23:47.044511: step 590, loss = 0.39, batch loss = 0.33 (12.3 examples/sec; 0.651 sec/batch; 60h:00m:54s remains)
INFO - root - 2017-12-15 10:23:53.585140: step 600, loss = 0.37, batch loss = 0.31 (11.7 examples/sec; 0.685 sec/batch; 63h:11m:41s remains)
2017-12-15 10:23:54.199983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3614464 -2.2065125 -1.9816306 -1.8552337 -1.6615739 -1.5618002 -1.4708815 -1.2869449 -1.0926368 -0.84921169 -0.71729803 -1.0403054 -1.5382581 -2.1105115 -2.7023635][-2.5354619 -2.4174566 -2.2530007 -2.1426497 -2.0771346 -2.0051334 -1.8961296 -1.739697 -1.5488694 -1.3020451 -1.1284866 -1.3898971 -1.8168399 -2.3611219 -2.8494577][-2.4850969 -2.30243 -2.0841305 -1.9256306 -1.7739131 -1.7192121 -1.6566446 -1.5261159 -1.3647947 -1.202112 -1.1038356 -1.3618605 -1.7747242 -2.30332 -2.7424312][-2.214283 -1.9200115 -1.7058351 -1.4248273 -1.1570396 -0.96661925 -0.81398249 -0.67560816 -0.59593773 -0.56791258 -0.65792894 -1.1154251 -1.6907508 -2.2778566 -2.7632079][-1.6077158 -1.1891966 -0.86137033 -0.57215047 -0.30633831 -0.023055553 0.24964666 0.43881893 0.50420141 0.43144941 0.22116947 -0.43079233 -1.1962101 -1.9070735 -2.5508282][-1.0251482 -0.54585361 -0.1392231 0.2828517 0.60003853 0.86249352 1.1274447 1.3887119 1.4901586 1.3561311 1.0910506 0.3479085 -0.55635905 -1.4533837 -2.2737277][-0.3931284 0.1542573 0.58995676 1.141479 1.4960022 1.7665443 2.0216379 2.1868744 2.2412133 2.0516753 1.6936121 0.79845428 -0.19906092 -1.1290984 -2.0324521][0.063567638 0.73390675 1.2934995 1.8827786 2.3327498 2.6611214 2.8666158 2.930192 2.911181 2.6465907 2.2493129 1.2756743 0.14024544 -0.90507054 -1.8583748][0.15288687 0.9766345 1.7159 2.4327087 2.832171 3.1370454 3.2099838 3.2023373 3.1084995 2.8205433 2.5480351 1.5425463 0.36796618 -0.82015109 -1.9383304][-0.12727547 0.87345982 1.7508459 2.5978904 3.0681491 3.3479381 3.3593607 3.2937331 3.1563873 2.8401823 2.5197034 1.5060992 0.32234478 -0.92437315 -2.144115][-1.1607752 -0.18727541 0.75751495 1.6249905 2.0727773 2.358109 2.281857 2.1285167 1.9813271 1.6228805 1.2495708 0.30040359 -0.767308 -1.9499958 -3.0496793][-2.5411184 -1.5769329 -0.63108468 0.2616744 0.78144693 0.99196672 0.81483841 0.49626684 0.20077753 -0.25450277 -0.66795683 -1.4281693 -2.2564707 -3.0928006 -3.9188535][-3.7623117 -2.9387105 -2.1017215 -1.3335583 -0.977715 -0.88521528 -1.158653 -1.5153527 -1.8324137 -2.1418357 -2.3997152 -2.9392223 -3.5028281 -4.0784664 -4.6613178][-4.59826 -3.8530154 -3.0137839 -2.407429 -2.1846559 -2.15145 -2.4388039 -2.7464442 -2.9889379 -3.1723716 -3.2736697 -3.5766807 -3.8618619 -4.352355 -4.8128276][-5.2801914 -4.63738 -4.0740409 -3.591197 -3.4343128 -3.3321826 -3.4841871 -3.6938422 -3.8932812 -4.1220045 -4.302629 -4.4291635 -4.5109711 -4.6745186 -4.8322539]]...]
INFO - root - 2017-12-15 10:24:00.713977: step 610, loss = 0.30, batch loss = 0.24 (12.3 examples/sec; 0.648 sec/batch; 59h:47m:04s remains)
INFO - root - 2017-12-15 10:24:07.198628: step 620, loss = 0.40, batch loss = 0.34 (12.2 examples/sec; 0.657 sec/batch; 60h:35m:35s remains)
INFO - root - 2017-12-15 10:24:13.841643: step 630, loss = 0.40, batch loss = 0.34 (11.9 examples/sec; 0.673 sec/batch; 62h:00m:09s remains)
INFO - root - 2017-12-15 10:24:20.371386: step 640, loss = 0.39, batch loss = 0.32 (11.8 examples/sec; 0.681 sec/batch; 62h:43m:57s remains)
INFO - root - 2017-12-15 10:24:26.974913: step 650, loss = 0.39, batch loss = 0.33 (11.9 examples/sec; 0.671 sec/batch; 61h:53m:14s remains)
INFO - root - 2017-12-15 10:24:33.440159: step 660, loss = 0.42, batch loss = 0.35 (12.6 examples/sec; 0.637 sec/batch; 58h:43m:40s remains)
INFO - root - 2017-12-15 10:24:40.023633: step 670, loss = 0.43, batch loss = 0.37 (12.0 examples/sec; 0.668 sec/batch; 61h:35m:00s remains)
INFO - root - 2017-12-15 10:24:46.477847: step 680, loss = 0.37, batch loss = 0.30 (12.4 examples/sec; 0.645 sec/batch; 59h:27m:53s remains)
INFO - root - 2017-12-15 10:24:52.983706: step 690, loss = 0.34, batch loss = 0.27 (12.4 examples/sec; 0.644 sec/batch; 59h:23m:17s remains)
INFO - root - 2017-12-15 10:24:59.496768: step 700, loss = 0.30, batch loss = 0.24 (12.5 examples/sec; 0.639 sec/batch; 58h:52m:53s remains)
2017-12-15 10:25:00.019736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1069336 -0.73308945 -0.13153124 0.39782858 0.59504604 0.4037919 -0.063043118 -0.3918004 -0.4976759 -0.54300356 -0.44749594 -0.9751687 -1.5197227 -2.3999403 -3.2151425][-1.2019649 -0.79957771 -0.12628174 0.45784664 0.50256586 0.39390707 0.089643478 -0.14889956 -0.3508997 -0.46784878 -0.41644 -1.1151891 -1.8944664 -2.9986303 -3.8918309][-1.0836017 -0.862576 -0.40973711 0.19160271 0.58022404 0.46072006 0.16573238 0.044083118 0.052126884 0.010362148 -0.2167635 -1.1725388 -2.1515059 -3.2877998 -4.2022648][-0.81150842 -0.67644286 -0.34008551 0.22258806 0.43742895 0.36774588 0.077735424 -0.11752319 -0.1152854 -0.26435471 -0.48428535 -1.3020601 -2.3720212 -3.50936 -4.5570006][-0.46735907 -0.30894852 0.024524212 0.2691884 0.32239151 0.2242012 -0.018953323 -0.22545671 -0.41767931 -0.52988434 -0.67929983 -1.507972 -2.557919 -3.7608442 -4.6756921][-0.52285671 -0.30737925 -0.0718832 0.16030693 0.38648796 0.29142141 0.09993124 0.020013809 -0.082522392 -0.36051083 -0.79776025 -1.8153324 -2.8829024 -4.046381 -4.8316388][-0.35691643 -0.16179037 -0.074093819 0.042643547 0.10934448 0.19907236 0.12057734 0.077552795 0.0034942627 -0.18641758 -0.57451677 -1.7696445 -3.0512695 -4.3157916 -5.1736126][-0.50419569 -0.36959362 -0.22014475 -0.16595221 -0.17735529 -0.057824135 0.11150551 0.30746365 0.4110713 0.29823446 -0.13991022 -1.4449749 -2.9392304 -4.3012204 -5.1784062][-0.6922698 -0.54260921 -0.47414684 -0.35182571 -0.18086338 -0.0044445992 0.23707771 0.42274046 0.55654383 0.63694143 0.34910297 -0.79766607 -2.2883608 -3.7612398 -4.8077021][-0.64466572 -0.64115739 -0.60535407 -0.6168561 -0.50293016 -0.28226089 0.059347153 0.41241884 0.59929705 0.67909813 0.45913076 -0.50455189 -1.7130692 -3.142514 -4.3444963][-1.4456625 -1.4432588 -1.4178388 -1.4497998 -1.407908 -1.2138305 -0.90484 -0.66038489 -0.52812672 -0.46883249 -0.55600548 -1.3803806 -2.3808575 -3.3282075 -4.20487][-2.5657721 -2.4324396 -2.3810246 -2.4034352 -2.3374074 -2.203476 -1.9166145 -1.7013335 -1.5066276 -1.4460778 -1.7705157 -2.5724583 -3.5283411 -4.3154588 -4.8488293][-3.9407384 -3.8566732 -3.6335099 -3.4874306 -3.3933573 -3.313807 -3.1935232 -3.0844634 -2.9140177 -3.0119927 -3.2510517 -3.8683097 -4.5138083 -5.2106647 -5.6664805][-4.583632 -4.5008149 -4.3361926 -4.0978165 -3.9795187 -3.8369489 -3.7099614 -3.8181231 -3.9384327 -3.8749604 -3.8133149 -4.1183171 -4.6711764 -5.2364187 -5.7306175][-4.9350152 -4.8196626 -4.6909838 -4.502347 -4.325912 -4.097693 -3.9569819 -3.849633 -3.8743911 -4.0242019 -4.2379704 -4.552156 -4.9302149 -5.1816597 -5.4351215]]...]
INFO - root - 2017-12-15 10:25:06.505936: step 710, loss = 0.48, batch loss = 0.41 (12.4 examples/sec; 0.646 sec/batch; 59h:30m:39s remains)
INFO - root - 2017-12-15 10:25:13.098392: step 720, loss = 0.31, batch loss = 0.24 (12.4 examples/sec; 0.645 sec/batch; 59h:29m:03s remains)
INFO - root - 2017-12-15 10:25:19.673799: step 730, loss = 0.38, batch loss = 0.31 (12.3 examples/sec; 0.653 sec/batch; 60h:09m:32s remains)
INFO - root - 2017-12-15 10:25:26.194906: step 740, loss = 0.30, batch loss = 0.23 (12.3 examples/sec; 0.649 sec/batch; 59h:48m:06s remains)
INFO - root - 2017-12-15 10:25:32.694698: step 750, loss = 0.36, batch loss = 0.29 (12.5 examples/sec; 0.638 sec/batch; 58h:49m:42s remains)
INFO - root - 2017-12-15 10:25:39.233644: step 760, loss = 0.29, batch loss = 0.23 (12.3 examples/sec; 0.649 sec/batch; 59h:49m:54s remains)
INFO - root - 2017-12-15 10:25:45.735865: step 770, loss = 0.35, batch loss = 0.29 (12.0 examples/sec; 0.668 sec/batch; 61h:31m:23s remains)
INFO - root - 2017-12-15 10:25:52.323019: step 780, loss = 0.30, batch loss = 0.23 (12.2 examples/sec; 0.658 sec/batch; 60h:38m:07s remains)
INFO - root - 2017-12-15 10:25:58.841876: step 790, loss = 0.30, batch loss = 0.24 (12.7 examples/sec; 0.631 sec/batch; 58h:08m:17s remains)
INFO - root - 2017-12-15 10:26:05.434591: step 800, loss = 0.42, batch loss = 0.35 (12.2 examples/sec; 0.658 sec/batch; 60h:39m:16s remains)
2017-12-15 10:26:05.997943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2351782 -3.508657 -3.5591736 -3.2543461 -3.1920044 -3.2784622 -3.1763539 -2.827117 -2.5482657 -2.2901733 -2.1338058 -2.3755295 -2.8306589 -3.2454195 -3.2614164][-4.21036 -3.6516459 -3.6445184 -3.6336856 -3.4636912 -3.2626581 -2.991282 -2.7500482 -2.4372795 -2.1032786 -1.8627944 -2.0863848 -2.4341898 -2.8600206 -2.9926836][-4.1885953 -3.9709334 -3.9813082 -4.1282144 -4.0846958 -3.669215 -3.052206 -2.571516 -2.1247797 -2.0166407 -2.0706673 -2.2626297 -2.5855992 -2.9308949 -3.1043825][-4.6444488 -4.4433556 -3.9342687 -3.7484326 -3.7784204 -3.599741 -3.0965772 -2.4848135 -1.981406 -1.7984838 -1.7684674 -2.1457772 -2.7152948 -3.2108886 -3.4571433][-4.5436835 -4.3260193 -4.2075582 -3.6076539 -2.968447 -2.2812154 -1.8333843 -1.4939601 -1.257266 -1.2169535 -1.3003016 -1.7204764 -2.3245287 -3.1429491 -3.7394392][-4.2724962 -3.9105368 -3.6274276 -3.2386768 -2.5783203 -1.5131707 -0.85675812 -0.65270758 -0.69417953 -0.91093326 -1.0398433 -1.6125746 -2.420486 -3.3817 -4.0033474][-3.7560196 -3.989702 -3.8931186 -3.1205339 -2.2665527 -1.1957231 -0.2183466 0.10659695 -0.18246078 -0.6614821 -0.85905671 -1.5954967 -2.4676797 -3.3114843 -4.0421686][-4.0268183 -4.1632752 -4.0551147 -3.3139882 -2.2615743 -1.4089024 -0.8039403 -0.19724607 -0.086065292 -0.77403688 -1.203846 -1.8176723 -2.5284419 -3.3685341 -4.0771275][-4.0360184 -4.0126104 -3.8840299 -3.4638183 -2.6954188 -1.5743442 -0.78469348 -0.91845751 -1.3610816 -1.3919609 -1.3881507 -2.1625328 -2.9928322 -3.9904044 -4.6188188][-4.1239614 -4.1924548 -4.0337081 -3.7419808 -3.1357183 -2.4867537 -2.1249752 -2.1470022 -2.350322 -2.4577725 -2.4747086 -3.1053567 -3.8119502 -4.6724296 -5.4142838][-5.0713725 -5.5717983 -5.7256737 -5.4859314 -5.021925 -4.7015224 -4.3564792 -4.4112196 -4.6771951 -4.76249 -4.8692508 -5.1979589 -5.6050935 -6.2180443 -6.7800951][-5.7290792 -6.2850013 -6.5100918 -6.4143987 -6.1500053 -5.9262381 -5.7994785 -5.827714 -5.6813126 -5.65723 -5.8625755 -6.2521286 -6.6856771 -6.9640036 -7.2043238][-6.3968039 -6.9444919 -7.3003073 -7.5358477 -7.4964819 -7.228786 -6.945209 -6.9313273 -7.10628 -7.0376897 -6.890378 -7.043891 -7.4306211 -7.6730523 -7.8302774][-6.8995953 -7.4115257 -7.7105846 -8.0519781 -8.2076406 -8.1745729 -8.0672989 -7.8837848 -7.696207 -7.7009859 -7.627202 -7.4386587 -7.5558157 -7.7363729 -7.778511][-7.3086376 -7.8784552 -8.157548 -8.158762 -8.079608 -8.238287 -8.2395153 -7.8577728 -7.5066085 -7.312396 -7.40524 -7.4388885 -7.3584824 -7.4538317 -7.5177679]]...]
INFO - root - 2017-12-15 10:26:12.641634: step 810, loss = 0.39, batch loss = 0.33 (12.0 examples/sec; 0.669 sec/batch; 61h:37m:23s remains)
INFO - root - 2017-12-15 10:26:19.211873: step 820, loss = 0.39, batch loss = 0.33 (11.9 examples/sec; 0.671 sec/batch; 61h:49m:49s remains)
INFO - root - 2017-12-15 10:26:25.752745: step 830, loss = 0.39, batch loss = 0.32 (12.3 examples/sec; 0.651 sec/batch; 59h:58m:29s remains)
INFO - root - 2017-12-15 10:26:32.354726: step 840, loss = 0.36, batch loss = 0.30 (12.1 examples/sec; 0.659 sec/batch; 60h:45m:19s remains)
INFO - root - 2017-12-15 10:26:38.891173: step 850, loss = 0.53, batch loss = 0.47 (12.2 examples/sec; 0.654 sec/batch; 60h:13m:10s remains)
INFO - root - 2017-12-15 10:26:45.412464: step 860, loss = 0.33, batch loss = 0.26 (12.1 examples/sec; 0.664 sec/batch; 61h:08m:34s remains)
INFO - root - 2017-12-15 10:26:52.015093: step 870, loss = 0.43, batch loss = 0.37 (11.9 examples/sec; 0.673 sec/batch; 62h:00m:07s remains)
INFO - root - 2017-12-15 10:26:58.598720: step 880, loss = 0.30, batch loss = 0.24 (12.2 examples/sec; 0.657 sec/batch; 60h:33m:49s remains)
INFO - root - 2017-12-15 10:27:05.163972: step 890, loss = 0.37, batch loss = 0.31 (12.0 examples/sec; 0.667 sec/batch; 61h:24m:48s remains)
INFO - root - 2017-12-15 10:27:11.774453: step 900, loss = 0.42, batch loss = 0.36 (12.3 examples/sec; 0.649 sec/batch; 59h:48m:47s remains)
2017-12-15 10:27:12.328298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2085688 -2.4595311 -2.630434 -2.5484366 -2.3646319 -2.3135059 -2.3450124 -2.2925806 -2.2635539 -2.0822344 -1.891144 -2.1785631 -2.8925343 -3.2687664 -3.6554041][-2.4792197 -2.5036671 -2.5645344 -2.6769967 -2.7120967 -2.8722496 -3.0812709 -3.1435971 -3.0783553 -2.872227 -2.6928818 -2.8481345 -3.3823152 -3.8561642 -4.2568016][-2.2179427 -2.3196504 -2.4242175 -2.4417276 -2.474586 -2.633656 -2.8916843 -3.0831747 -3.0550303 -2.7027628 -2.4867568 -2.9292498 -3.5704718 -3.9341245 -4.2798023][-2.2538679 -2.0006204 -1.7565565 -1.6568356 -1.6092544 -1.6902554 -2.0021758 -2.1707964 -2.157372 -2.1498561 -2.2579358 -2.623296 -3.1738744 -3.7701256 -4.2921867][-1.4330146 -1.0347607 -0.89815784 -0.5684247 -0.31334162 -0.25628519 -0.53181648 -0.72097158 -0.9832201 -1.1093183 -1.3389952 -2.1029141 -2.9724607 -3.4608145 -3.7382061][-0.67532063 -0.17519474 0.20724344 0.65751696 0.88333416 1.1012197 1.0498986 0.87170792 0.59198523 0.095589161 -0.57125378 -1.283865 -1.9846721 -2.5309355 -3.0240707][-0.070818424 0.45167398 0.78002071 1.1980686 1.6466327 1.9039388 1.8956971 1.7434173 1.5078106 1.0014091 0.42503548 -0.49150896 -1.5496361 -2.1321778 -2.3378737][0.032549381 0.62224722 0.88383675 1.2413898 1.4179902 1.6929045 1.6697431 1.5230703 1.5350189 1.1742816 0.812973 0.11246395 -0.595129 -1.1525259 -1.7418306][-0.40600681 0.12672472 0.34817886 0.60573339 0.66091108 0.80695915 0.69734478 0.76540804 0.77106094 0.77387 0.87107611 0.31717873 -0.38922262 -1.0074468 -1.3410518][-0.32362318 -0.096796513 -0.083596706 -0.020991325 -0.14488554 -0.23089266 -0.53667879 -0.41320896 -0.11246872 0.03981638 0.073008537 -0.20352697 -0.50509167 -0.79002905 -1.1657622][-1.68577 -1.5246813 -1.5666115 -1.9605284 -2.3033175 -2.7395365 -3.0083978 -2.8358088 -2.5950832 -2.086885 -1.6074548 -1.7317064 -1.9873657 -2.0254455 -2.0687966][-2.8365874 -2.9104209 -3.1178908 -3.3103385 -3.7562618 -4.2032504 -4.3575993 -4.1800957 -3.7678204 -3.4757721 -3.1590314 -2.8783679 -2.7033544 -2.5411925 -2.392432][-4.6338997 -4.6087594 -4.8299708 -5.3050919 -5.7163482 -5.9959755 -6.2387424 -6.0871072 -5.6676092 -5.1970634 -4.7294269 -4.3984542 -3.9073374 -3.4423909 -3.0985384][-5.2903457 -5.366796 -5.6181707 -5.7874155 -6.08244 -6.2468348 -6.2441826 -6.0716558 -5.7544813 -5.4092851 -4.9445987 -4.4001379 -3.8411143 -3.3751268 -3.0861847][-5.4813452 -5.5015655 -5.5559254 -5.5946884 -5.7089872 -5.6535177 -5.585516 -5.5972934 -5.6555033 -5.4240503 -5.102222 -4.6757374 -4.1278486 -3.6669028 -3.3907037]]...]
INFO - root - 2017-12-15 10:27:18.878611: step 910, loss = 0.35, batch loss = 0.28 (12.7 examples/sec; 0.629 sec/batch; 57h:54m:11s remains)
INFO - root - 2017-12-15 10:27:25.490210: step 920, loss = 0.28, batch loss = 0.22 (12.3 examples/sec; 0.650 sec/batch; 59h:50m:18s remains)
INFO - root - 2017-12-15 10:27:32.061395: step 930, loss = 0.40, batch loss = 0.33 (12.3 examples/sec; 0.650 sec/batch; 59h:53m:08s remains)
INFO - root - 2017-12-15 10:27:38.622095: step 940, loss = 0.31, batch loss = 0.25 (12.0 examples/sec; 0.664 sec/batch; 61h:09m:55s remains)
INFO - root - 2017-12-15 10:27:45.169396: step 950, loss = 0.32, batch loss = 0.26 (12.3 examples/sec; 0.649 sec/batch; 59h:43m:33s remains)
INFO - root - 2017-12-15 10:27:51.730104: step 960, loss = 0.36, batch loss = 0.29 (12.6 examples/sec; 0.637 sec/batch; 58h:38m:08s remains)
INFO - root - 2017-12-15 10:27:58.276740: step 970, loss = 0.35, batch loss = 0.29 (12.2 examples/sec; 0.655 sec/batch; 60h:17m:02s remains)
INFO - root - 2017-12-15 10:28:04.795207: step 980, loss = 0.34, batch loss = 0.27 (12.6 examples/sec; 0.635 sec/batch; 58h:29m:12s remains)
INFO - root - 2017-12-15 10:28:11.428182: step 990, loss = 0.42, batch loss = 0.36 (11.5 examples/sec; 0.696 sec/batch; 64h:05m:26s remains)
INFO - root - 2017-12-15 10:28:17.997732: step 1000, loss = 0.38, batch loss = 0.32 (12.5 examples/sec; 0.641 sec/batch; 59h:02m:16s remains)
2017-12-15 10:28:18.489653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3197012 -4.6554 -4.9091086 -4.903111 -5.1572433 -5.2728696 -5.2914839 -5.2246976 -5.09269 -4.8533726 -4.6593485 -4.2477098 -3.9080379 -3.3298011 -2.9158616][-4.5431414 -4.7101679 -4.6102586 -4.8272686 -5.0565386 -5.1740723 -4.9759736 -4.6509838 -4.3027716 -3.9060581 -3.4374495 -3.1097631 -2.6625426 -2.2448595 -2.0724015][-4.2297363 -4.3071256 -4.1590734 -4.1103568 -4.1845059 -4.314724 -4.3089194 -4.1557131 -3.8283043 -3.5248508 -3.1732173 -3.0586791 -2.8783207 -2.4782603 -1.9728613][-4.054646 -3.8580618 -3.4021461 -2.9635303 -2.8062215 -2.6402457 -2.6263025 -2.4113398 -2.168819 -2.0319021 -1.8218498 -1.8533649 -1.8761363 -1.6446447 -1.4396908][-3.893343 -3.1938548 -2.2780137 -0.94647169 -0.091691017 0.15601444 0.090860844 -0.30582666 -0.64356709 -0.73111391 -0.69392586 -1.0660355 -1.5138638 -1.6995566 -1.747319][-3.894083 -2.7579913 -1.2934687 0.14230633 1.3851442 2.0348625 2.0556326 1.6359448 1.1661396 0.70889282 0.4185667 -0.087425232 -0.58469582 -0.95190191 -1.226804][-3.186722 -2.2565823 -0.85761118 0.83599377 2.180409 3.0866337 3.3958144 3.1037798 2.7269464 2.2112484 1.7507195 0.839097 -0.065486908 -0.76437044 -1.2187996][-3.028343 -2.1971109 -1.2630031 0.22741842 1.5846472 2.6977057 3.1304178 3.228765 3.0400591 2.5954232 1.9973512 0.95625639 -0.092581749 -0.97231531 -1.5927448][-2.2142274 -1.6105103 -0.74855042 0.48656273 1.4097924 2.3345075 2.871139 2.9515381 2.8616085 2.4212837 1.740766 0.48157072 -0.79378963 -1.840919 -2.6578283][-1.6572204 -1.1852105 -0.66418123 -0.047991276 0.64166546 1.1259975 1.3532157 1.4692807 1.2710934 0.73494577 0.34055042 -0.68161345 -1.9612536 -3.0128953 -3.7686772][-3.2324319 -3.0494413 -2.6021178 -2.136816 -1.8226469 -1.7081904 -1.8645053 -1.9057593 -2.1952085 -2.4205642 -2.5836558 -3.4080694 -4.1886888 -4.7436857 -5.2376027][-4.4667268 -4.2564583 -4.0573196 -4.0342464 -4.1749034 -4.1245956 -4.1804 -4.752564 -5.2515321 -5.5627761 -5.7639322 -6.01505 -6.2990937 -6.5256023 -6.6682377][-6.7327213 -6.771574 -6.8049083 -6.7795939 -6.756031 -6.7650886 -7.03117 -7.3616247 -7.5396595 -7.7875285 -8.188962 -8.45179 -8.6241264 -8.3657646 -8.0931683][-6.9391923 -7.0316639 -6.8991661 -6.9362612 -7.070837 -7.1577168 -7.15308 -7.4031529 -7.6063538 -7.6154089 -7.8475962 -7.9919109 -7.8979931 -7.7604737 -7.5125337][-6.3557873 -6.4100647 -6.6358285 -6.7350354 -6.7403593 -6.43197 -6.3421907 -6.2331672 -6.1784582 -6.332654 -6.7814593 -7.3200092 -7.497395 -7.3755941 -7.1015043]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 10:28:25.056441: step 1010, loss = 0.33, batch loss = 0.27 (12.2 examples/sec; 0.654 sec/batch; 60h:12m:39s remains)
INFO - root - 2017-12-15 10:28:31.668767: step 1020, loss = 0.37, batch loss = 0.31 (12.3 examples/sec; 0.648 sec/batch; 59h:40m:47s remains)
INFO - root - 2017-12-15 10:28:38.217613: step 1030, loss = 0.31, batch loss = 0.25 (12.7 examples/sec; 0.631 sec/batch; 58h:06m:37s remains)
INFO - root - 2017-12-15 10:28:44.713915: step 1040, loss = 0.31, batch loss = 0.25 (12.5 examples/sec; 0.642 sec/batch; 59h:07m:05s remains)
INFO - root - 2017-12-15 10:28:51.303693: step 1050, loss = 0.33, batch loss = 0.27 (12.5 examples/sec; 0.639 sec/batch; 58h:52m:20s remains)
INFO - root - 2017-12-15 10:28:57.895712: step 1060, loss = 0.30, batch loss = 0.24 (12.0 examples/sec; 0.665 sec/batch; 61h:10m:45s remains)
INFO - root - 2017-12-15 10:29:04.425743: step 1070, loss = 0.30, batch loss = 0.23 (12.2 examples/sec; 0.654 sec/batch; 60h:10m:11s remains)
INFO - root - 2017-12-15 10:29:11.007237: step 1080, loss = 0.31, batch loss = 0.24 (12.2 examples/sec; 0.653 sec/batch; 60h:08m:48s remains)
INFO - root - 2017-12-15 10:29:17.557335: step 1090, loss = 0.32, batch loss = 0.25 (12.2 examples/sec; 0.656 sec/batch; 60h:23m:53s remains)
INFO - root - 2017-12-15 10:29:24.206873: step 1100, loss = 0.36, batch loss = 0.30 (12.1 examples/sec; 0.661 sec/batch; 60h:49m:03s remains)
2017-12-15 10:29:24.713269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9750798 -4.3262134 -4.4756179 -4.8461761 -5.1903162 -5.2148242 -5.0854411 -4.4164567 -3.7022698 -2.9516 -2.2331848 -2.0738292 -2.3635743 -2.2854607 -2.3068361][-3.3101354 -3.9050326 -4.6188011 -5.2145953 -5.6481681 -5.7697396 -5.4078569 -4.7369647 -3.9025867 -3.1697135 -2.5631394 -2.2868216 -2.2999489 -2.039273 -1.8879724][-2.5528874 -3.1894093 -4.0735855 -4.7730393 -4.9609208 -4.8551774 -4.7676988 -4.5057321 -4.1541018 -3.6109064 -2.8778455 -2.6902657 -2.959775 -2.5487647 -2.3492143][-2.2289274 -2.9668441 -3.652982 -3.9636688 -4.0251069 -3.8644493 -3.5934644 -3.7686214 -3.7419531 -3.3102717 -3.2070088 -3.5177364 -3.6337228 -3.2550995 -2.9360056][-2.4771266 -3.2690759 -3.9909883 -3.6608205 -2.7506988 -2.1839767 -1.8907242 -2.1184254 -2.5721672 -2.8383167 -2.9981279 -3.589695 -4.0650377 -3.6491661 -3.5678225][-2.4157913 -2.733217 -2.9060993 -1.5283074 -0.24442005 0.53954029 1.1859007 0.82556534 -0.15235138 -1.1159258 -1.8418224 -2.8011613 -3.5759635 -3.5856404 -3.6526289][-2.48285 -1.8204689 -1.3400643 0.31718016 2.4245443 3.5661263 4.301064 4.3658051 3.4636226 1.4493537 -0.48139715 -1.928782 -3.1339269 -3.5922308 -3.849257][-3.5591111 -2.8694053 -1.9532433 -0.20215082 2.0645494 4.5972524 5.9057 5.296515 4.4253373 2.6357107 0.50913143 -1.7102754 -3.7049117 -4.5227585 -5.0349536][-5.0535822 -4.6283851 -3.6046457 -1.9778936 -0.056534767 2.4790211 4.4983783 4.7827477 4.3886695 2.9268222 1.2134175 -1.2387321 -3.6219518 -4.7412496 -5.3038716][-5.1518874 -5.09346 -4.7310266 -3.3244586 -1.896368 -0.080550194 1.5259438 2.4714665 2.5408607 1.5872612 0.52982569 -1.2767994 -3.4849668 -4.6863208 -5.8045983][-5.1770482 -5.2268548 -5.1313004 -4.6336422 -4.0061541 -3.1398985 -2.1955538 -1.4641201 -0.87437177 -1.2059259 -1.8176832 -3.2137878 -4.6433039 -5.4898171 -6.3749175][-6.4664831 -6.1980724 -5.6520524 -5.7196331 -5.7308478 -5.4607372 -5.369153 -5.011024 -4.5594807 -4.6641316 -4.6077242 -5.208446 -6.216176 -6.8851819 -7.5068111][-7.4406347 -7.3220806 -7.2942429 -7.2904954 -6.7992697 -6.8074217 -7.1248531 -7.2411137 -7.2389097 -7.142417 -6.9609747 -6.9450226 -6.8760304 -7.1106682 -7.4128594][-7.1588655 -7.5883832 -7.8524003 -7.559875 -7.402132 -7.4069209 -7.2589097 -7.41984 -7.4185934 -7.3877454 -7.3626852 -6.9499989 -6.5603085 -6.5265779 -6.5643048][-6.440321 -7.0873084 -7.6530623 -7.580936 -7.2846756 -6.9762716 -6.8179669 -6.7854142 -6.511826 -6.4214978 -6.5578313 -6.6082592 -6.7428536 -6.6397924 -6.4636068]]...]
INFO - root - 2017-12-15 10:29:31.256046: step 1110, loss = 0.30, batch loss = 0.24 (11.9 examples/sec; 0.675 sec/batch; 62h:06m:19s remains)
INFO - root - 2017-12-15 10:29:37.763735: step 1120, loss = 0.32, batch loss = 0.26 (12.7 examples/sec; 0.629 sec/batch; 57h:54m:11s remains)
INFO - root - 2017-12-15 10:29:44.287997: step 1130, loss = 0.29, batch loss = 0.23 (12.3 examples/sec; 0.650 sec/batch; 59h:48m:01s remains)
INFO - root - 2017-12-15 10:29:50.829022: step 1140, loss = 0.36, batch loss = 0.29 (12.3 examples/sec; 0.650 sec/batch; 59h:51m:36s remains)
INFO - root - 2017-12-15 10:29:57.387731: step 1150, loss = 0.36, batch loss = 0.30 (12.2 examples/sec; 0.656 sec/batch; 60h:21m:40s remains)
INFO - root - 2017-12-15 10:30:04.034448: step 1160, loss = 0.35, batch loss = 0.29 (12.1 examples/sec; 0.662 sec/batch; 60h:54m:56s remains)
INFO - root - 2017-12-15 10:30:10.566504: step 1170, loss = 0.33, batch loss = 0.27 (12.4 examples/sec; 0.648 sec/batch; 59h:35m:41s remains)
INFO - root - 2017-12-15 10:30:17.120502: step 1180, loss = 0.37, batch loss = 0.31 (12.5 examples/sec; 0.642 sec/batch; 59h:07m:17s remains)
INFO - root - 2017-12-15 10:30:23.645317: step 1190, loss = 0.31, batch loss = 0.25 (12.0 examples/sec; 0.669 sec/batch; 61h:36m:35s remains)
INFO - root - 2017-12-15 10:30:30.214217: step 1200, loss = 0.42, batch loss = 0.35 (12.4 examples/sec; 0.648 sec/batch; 59h:35m:52s remains)
2017-12-15 10:30:30.739090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.57781315 -0.23549604 0.26458359 0.069672585 -0.24172211 -0.55639744 -0.74353933 -0.59012842 -0.58206177 -0.33581781 0.21262169 0.068748951 -0.25738525 -0.84815931 -1.5126815][-0.67929649 -0.22505331 0.24444485 0.82541084 0.50402927 0.09677124 -0.25342369 -0.65270424 -0.84362745 -0.64076138 -0.19122887 0.0033054352 -0.529932 -1.1457963 -1.7536688][-0.33881998 0.22613668 0.49988222 0.89437103 0.75179863 0.51299858 0.185709 -0.058951378 -0.2020092 -0.28981352 -0.33499002 -0.27908754 -0.52558613 -1.3145616 -2.04751][-1.1197176 -0.72652197 -0.052144527 0.65390396 1.0895896 1.1735339 0.58471537 0.14628601 -0.1218338 -0.000688076 -0.17191935 -0.92134333 -1.3980277 -1.9645655 -2.2889676][-1.7286439 -1.3021522 -0.70782709 -0.16472054 0.031024933 0.35539246 0.51112986 0.43508387 -0.032283306 -0.32947779 -0.56449938 -1.2662799 -2.0375791 -2.5291765 -3.0275841][-2.0680351 -1.8166692 -1.1601939 -0.42201567 0.17175579 0.4994998 0.62814379 0.66463757 0.61246252 0.18546629 -0.39670563 -1.1980793 -2.1177967 -2.7881536 -3.2433882][-1.6960311 -1.4578583 -0.71380043 0.065346718 0.66056442 1.3521228 1.3654618 1.1736913 1.0596805 0.73005915 0.191473 -0.99203706 -1.9149942 -2.6601171 -3.0604866][-2.0130575 -1.1970456 -0.47554398 0.11823559 0.97754097 1.8019238 2.170218 2.1918216 1.6489177 1.1256595 0.8920927 -0.21890163 -1.4720201 -2.3416376 -2.6347651][-2.1680152 -1.31128 -0.10562229 0.61265326 1.2695856 1.5395665 1.641748 1.9833093 1.583076 0.91318035 0.46011782 -0.68634987 -1.7707298 -2.5456526 -3.0734987][-2.0485058 -1.5047894 -0.65241003 0.34776545 1.1226268 1.5671268 1.7860665 1.6264338 0.83124733 0.03827095 -0.64956856 -1.6968637 -2.26122 -2.8803129 -3.5012856][-2.6396067 -2.0719013 -1.2939436 -0.45613718 0.16791534 0.52519894 0.43700314 -0.064473629 -0.50729609 -0.83849144 -1.5008035 -2.7185802 -3.6254373 -4.4276633 -3.8822784][-3.7629843 -3.123327 -2.4203284 -1.8452744 -1.503217 -0.89103484 -0.50984 -0.78016806 -1.3995805 -2.3297281 -3.1257708 -3.7337441 -4.1535959 -4.4984975 -4.0698628][-4.3036737 -3.991642 -3.4925866 -2.7973993 -2.4636455 -2.4786205 -2.8952804 -2.9350028 -3.1151206 -3.5069189 -3.8338742 -4.3475986 -4.5286427 -4.7164545 -4.2341456][-4.5071349 -4.1894908 -4.2249045 -3.9181888 -3.8153629 -3.7281165 -3.5374331 -3.6312284 -4.0115504 -4.3976145 -4.773304 -5.239151 -4.8874016 -4.5593405 -3.9582469][-4.7032537 -4.3069611 -3.9119256 -3.6225674 -3.5144377 -3.6601291 -3.9970059 -4.3728294 -4.6056952 -4.6931481 -4.6999559 -5.1080327 -5.5327544 -5.1805773 -4.2164783]]...]
INFO - root - 2017-12-15 10:30:37.410791: step 1210, loss = 0.35, batch loss = 0.28 (11.7 examples/sec; 0.685 sec/batch; 63h:03m:12s remains)
INFO - root - 2017-12-15 10:30:44.023390: step 1220, loss = 0.30, batch loss = 0.24 (12.1 examples/sec; 0.659 sec/batch; 60h:37m:28s remains)
INFO - root - 2017-12-15 10:30:50.572413: step 1230, loss = 0.34, batch loss = 0.28 (12.2 examples/sec; 0.658 sec/batch; 60h:35m:12s remains)
INFO - root - 2017-12-15 10:30:57.118718: step 1240, loss = 0.26, batch loss = 0.20 (12.3 examples/sec; 0.648 sec/batch; 59h:39m:08s remains)
INFO - root - 2017-12-15 10:31:03.672636: step 1250, loss = 0.28, batch loss = 0.22 (12.0 examples/sec; 0.668 sec/batch; 61h:28m:26s remains)
INFO - root - 2017-12-15 10:31:10.289986: step 1260, loss = 0.39, batch loss = 0.33 (12.2 examples/sec; 0.657 sec/batch; 60h:29m:48s remains)
INFO - root - 2017-12-15 10:31:16.877650: step 1270, loss = 0.36, batch loss = 0.30 (11.9 examples/sec; 0.670 sec/batch; 61h:38m:41s remains)
INFO - root - 2017-12-15 10:31:23.466678: step 1280, loss = 0.28, batch loss = 0.22 (11.9 examples/sec; 0.670 sec/batch; 61h:40m:58s remains)
INFO - root - 2017-12-15 10:31:30.101172: step 1290, loss = 0.38, batch loss = 0.32 (11.8 examples/sec; 0.679 sec/batch; 62h:25m:55s remains)
INFO - root - 2017-12-15 10:31:36.608433: step 1300, loss = 0.39, batch loss = 0.33 (12.6 examples/sec; 0.635 sec/batch; 58h:26m:53s remains)
2017-12-15 10:31:37.100140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.83926439 -1.0430448 -1.2056353 -1.0815983 -0.91540813 -0.82620573 -0.74601126 -0.898715 -1.0390589 -1.2072017 -1.7512202 -2.5243146 -2.8547952 -3.3191555 -3.6998563][-1.3586829 -1.1496794 -0.93766952 -0.71102428 -0.13192892 0.078743458 0.079571247 -0.13624763 -0.6973424 -0.994473 -1.0945354 -1.7828858 -2.3094249 -3.1185203 -3.8159461][-1.1150558 -1.4085407 -1.2210717 -0.69884253 -0.20484734 0.38733006 0.5533886 0.35410595 0.042829514 -0.50232315 -0.99598169 -1.3567545 -1.6206119 -2.5107856 -3.3266802][-1.5224915 -1.3599167 -1.2259829 -0.87097168 -0.30077934 0.16667652 0.57686663 0.55212641 0.13211346 -0.069942 -0.17328835 -1.047682 -1.6223269 -2.4285805 -3.5361795][-1.5053582 -1.560288 -1.3809724 -1.0517061 -0.41394854 0.28894091 0.9409399 1.1460433 1.0549369 0.56330156 0.336277 -0.204463 -0.78828669 -1.9915597 -3.1515536][-2.0532382 -1.7356164 -1.4884117 -0.76769686 -0.067705631 0.53963614 1.2130404 1.3464699 1.3278823 1.2137241 1.0243249 0.21536732 -0.3127017 -1.3528578 -2.8110263][-1.6872118 -1.7530348 -1.700573 -1.2182059 -0.34862518 0.60832024 1.4357476 1.8243251 1.8800793 1.4701848 1.1357541 0.30332708 -0.52887774 -1.5364392 -2.6273031][-1.6209719 -1.7188549 -2.0008295 -1.5535123 -0.89431572 0.22142029 1.3525567 1.7783885 1.9948292 1.8065977 1.2652941 0.20851326 -0.62321329 -1.8778305 -3.3247766][-1.7325878 -2.02863 -2.3708963 -2.2970679 -1.8258648 -1.0771372 -0.020412922 0.99301195 1.5154691 1.2614331 0.84592628 -0.23161221 -1.2495255 -2.3535321 -3.6270356][-1.5692317 -2.2031643 -2.750901 -2.8299375 -2.7865107 -2.0822749 -1.1054657 -0.25564861 0.33902645 0.401258 0.22417402 -0.8662014 -1.7489445 -3.01272 -4.2948332][-2.7314229 -2.9734237 -3.3825512 -3.6813221 -3.7262344 -3.4695525 -2.826014 -2.1813898 -1.7257352 -1.5256591 -1.6044846 -2.4267113 -3.1936626 -4.2005978 -5.059422][-4.6778736 -4.737102 -4.7355795 -5.0204272 -5.1363297 -5.0947204 -4.7963395 -4.351284 -3.8935032 -3.6495185 -3.6428418 -4.1685324 -4.5947108 -5.1521544 -5.7236557][-6.0553718 -6.523222 -6.625896 -6.6570244 -6.536231 -6.2232652 -6.0963631 -5.903338 -5.5508146 -5.2993317 -5.2436218 -5.4459157 -5.7442269 -5.9389172 -6.130991][-6.4095387 -6.7958131 -7.19012 -7.0719919 -6.7606506 -6.5046525 -6.0384502 -5.7134175 -5.4235239 -5.2809243 -5.3431444 -5.5266972 -5.7085876 -5.8466139 -6.0809808][-5.7029943 -6.1567421 -6.5413656 -6.7440624 -6.8823709 -6.3907185 -5.7904458 -5.301271 -4.7767582 -4.5725017 -4.5995207 -4.85767 -5.3077779 -5.6403522 -5.6481194]]...]
INFO - root - 2017-12-15 10:31:43.720326: step 1310, loss = 0.30, batch loss = 0.24 (11.9 examples/sec; 0.675 sec/batch; 62h:04m:26s remains)
INFO - root - 2017-12-15 10:31:50.299995: step 1320, loss = 0.31, batch loss = 0.25 (12.2 examples/sec; 0.656 sec/batch; 60h:20m:53s remains)
INFO - root - 2017-12-15 10:31:56.883002: step 1330, loss = 0.46, batch loss = 0.40 (12.1 examples/sec; 0.662 sec/batch; 60h:51m:37s remains)
INFO - root - 2017-12-15 10:32:03.432179: step 1340, loss = 0.33, batch loss = 0.27 (12.4 examples/sec; 0.647 sec/batch; 59h:30m:20s remains)
INFO - root - 2017-12-15 10:32:09.899516: step 1350, loss = 0.23, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 58h:49m:25s remains)
INFO - root - 2017-12-15 10:32:16.485501: step 1360, loss = 0.25, batch loss = 0.19 (11.9 examples/sec; 0.672 sec/batch; 61h:46m:21s remains)
INFO - root - 2017-12-15 10:32:23.080550: step 1370, loss = 0.36, batch loss = 0.30 (11.7 examples/sec; 0.682 sec/batch; 62h:42m:48s remains)
INFO - root - 2017-12-15 10:32:29.628364: step 1380, loss = 0.40, batch loss = 0.34 (11.9 examples/sec; 0.674 sec/batch; 61h:58m:29s remains)
INFO - root - 2017-12-15 10:32:36.228384: step 1390, loss = 0.29, batch loss = 0.23 (11.8 examples/sec; 0.679 sec/batch; 62h:29m:35s remains)
INFO - root - 2017-12-15 10:32:42.869269: step 1400, loss = 0.34, batch loss = 0.28 (11.9 examples/sec; 0.674 sec/batch; 62h:00m:14s remains)
2017-12-15 10:32:43.422864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.49911 -4.6346679 -4.8765697 -4.7905498 -4.8381238 -5.1873636 -5.5350904 -5.6412082 -5.5284548 -5.1417756 -4.6647706 -5.0355725 -5.8718219 -6.229229 -6.5923309][-4.7585196 -4.9724889 -5.185637 -5.346168 -5.4913597 -5.7780561 -5.9722581 -5.9232078 -5.7778969 -5.4055805 -4.9559951 -5.527082 -6.0994282 -6.6741753 -6.9666014][-4.36078 -4.8640766 -5.0909009 -5.0716214 -5.0650826 -5.155766 -5.158752 -5.3719068 -5.2247047 -4.6774321 -4.3162503 -4.9440031 -5.8285379 -6.365592 -6.80306][-4.514327 -4.6083317 -4.5673771 -4.3264022 -4.0726686 -4.1762714 -3.8575673 -3.5486538 -3.3580434 -3.2149615 -3.2189202 -4.3571987 -5.2514973 -6.0900335 -6.6163039][-4.0311675 -4.0250912 -3.8382521 -2.9959397 -2.098788 -1.7356286 -1.2803109 -1.3433628 -1.0874054 -0.92385483 -1.2879381 -2.6462038 -4.4668269 -5.397737 -6.0083251][-3.959805 -3.7625928 -2.8765807 -1.3760173 -0.032218933 0.91870117 1.4339957 1.5017791 1.5089741 1.2357454 0.63641882 -1.1558588 -3.1598725 -4.7723289 -5.8824716][-3.5895667 -2.9361558 -2.3865252 -0.6281848 1.3481092 2.6472492 3.4080811 3.2028055 2.9479885 2.9500341 1.956162 -0.012500286 -2.4083583 -3.9215899 -5.1545715][-3.1645398 -2.2930694 -1.6187015 -0.082044125 1.304944 2.5964737 3.5996418 3.5464134 3.3599138 3.1530294 2.585958 0.91039944 -1.6610703 -3.6777174 -5.4163518][-3.2163541 -2.5112638 -1.7130401 -0.42802715 0.72177267 2.1458397 3.046401 2.9128022 2.782867 2.8364172 2.5253029 0.58979082 -1.9523058 -3.6784797 -5.3341556][-3.6961184 -2.6756105 -2.0840983 -1.0255387 0.091284752 0.86432028 1.7631612 1.5917435 1.2815351 1.198215 1.4631624 0.63163185 -1.4747267 -3.58808 -5.4274144][-3.7326479 -3.1876988 -2.6591012 -1.4778242 -0.6536541 0.081029415 0.64840317 0.53842974 0.12554026 -0.14348173 0.14088869 -0.39891338 -1.9710722 -4.0847454 -5.8043003][-4.1745811 -3.4507639 -2.7947116 -1.5725288 -0.6114192 -0.40361834 -0.26030779 -0.43411446 -0.79583311 -1.273334 -1.1124938 -1.3367853 -2.7183082 -4.4297543 -6.0227213][-4.5440331 -3.6113176 -2.9105146 -1.7785692 -1.0955131 -0.76553059 -1.1542673 -1.4148803 -1.563134 -1.7714639 -1.9622881 -2.34353 -3.6715837 -5.5366449 -6.7166367][-4.6260152 -3.8729005 -3.0430074 -1.9269137 -1.2884095 -1.4004831 -1.8835444 -2.575335 -2.8902619 -2.6757181 -2.7268484 -3.188642 -4.2930732 -5.5842581 -6.8149076][-4.5653191 -3.8906059 -3.140367 -2.5861056 -2.4916964 -2.5964713 -3.0085945 -3.4119434 -3.3786681 -3.232137 -2.9380028 -3.4105186 -4.377686 -5.4020433 -6.116642]]...]
INFO - root - 2017-12-15 10:32:49.993590: step 1410, loss = 0.33, batch loss = 0.27 (12.1 examples/sec; 0.664 sec/batch; 61h:03m:23s remains)
INFO - root - 2017-12-15 10:32:56.562030: step 1420, loss = 0.32, batch loss = 0.26 (12.3 examples/sec; 0.653 sec/batch; 60h:00m:47s remains)
INFO - root - 2017-12-15 10:33:03.123140: step 1430, loss = 0.34, batch loss = 0.28 (12.2 examples/sec; 0.655 sec/batch; 60h:16m:37s remains)
INFO - root - 2017-12-15 10:33:09.741906: step 1440, loss = 0.27, batch loss = 0.21 (12.4 examples/sec; 0.645 sec/batch; 59h:21m:30s remains)
INFO - root - 2017-12-15 10:33:16.355898: step 1450, loss = 0.24, batch loss = 0.18 (12.3 examples/sec; 0.648 sec/batch; 59h:34m:50s remains)
INFO - root - 2017-12-15 10:33:22.919629: step 1460, loss = 0.27, batch loss = 0.21 (12.3 examples/sec; 0.652 sec/batch; 59h:56m:06s remains)
INFO - root - 2017-12-15 10:33:29.553814: step 1470, loss = 0.32, batch loss = 0.26 (11.8 examples/sec; 0.678 sec/batch; 62h:23m:22s remains)
INFO - root - 2017-12-15 10:33:36.115720: step 1480, loss = 0.28, batch loss = 0.22 (11.9 examples/sec; 0.673 sec/batch; 61h:52m:14s remains)
INFO - root - 2017-12-15 10:33:42.746493: step 1490, loss = 0.30, batch loss = 0.24 (12.2 examples/sec; 0.657 sec/batch; 60h:22m:37s remains)
INFO - root - 2017-12-15 10:33:49.264864: step 1500, loss = 0.31, batch loss = 0.25 (12.1 examples/sec; 0.660 sec/batch; 60h:41m:05s remains)
2017-12-15 10:33:49.805702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.5354385 -9.0429859 -9.381321 -9.1952114 -8.6396332 -7.8722382 -7.4539652 -7.2899189 -6.8620558 -6.6059294 -6.5569129 -7.0055537 -7.2966375 -6.9901404 -6.6871853][-7.8245535 -7.8410969 -8.0584793 -8.6892433 -8.8797531 -8.4840221 -8.0605488 -7.6564779 -7.397891 -7.1834264 -7.0086174 -6.86208 -6.8327007 -6.7887421 -6.7084804][-6.2957315 -6.385499 -6.4359765 -6.3786507 -5.8957772 -5.8752489 -5.9640808 -5.9898648 -5.891778 -5.7928419 -5.6132 -6.0894589 -6.2077875 -5.9509487 -5.8313413][-5.065001 -4.2568569 -3.8616581 -3.8837621 -3.8949287 -3.9656496 -4.1708932 -4.3694205 -4.44327 -4.3166223 -4.0676036 -4.7967658 -5.2548385 -5.5661178 -6.0577016][-3.8918982 -2.569618 -1.706193 -1.2948596 -0.94821072 -0.9698534 -1.3332007 -1.6133928 -1.8407347 -1.8301213 -1.8414602 -2.98035 -4.0224352 -4.5570335 -5.0121284][-3.7926588 -2.1025116 -1.0176363 -0.32966232 0.56345987 1.2111936 0.80930614 0.36395264 0.14568424 -0.040614605 -0.39213228 -1.6000385 -2.6541548 -3.1968386 -3.7310486][-3.3148117 -1.2876565 -0.36082172 0.37433147 1.0120349 1.7877207 2.0054336 1.7324586 1.9084597 1.5038152 0.63572645 -0.7164712 -1.946974 -2.40494 -2.9232495][-2.5451138 -0.45282555 0.41596413 0.6686368 0.91301632 1.4968934 1.6296854 1.7926116 2.2163711 1.9500389 1.1462517 -0.28960657 -1.4025049 -1.9584999 -2.2644916][-1.7871788 -0.0023117065 0.54533863 0.30696344 0.28481007 0.86316347 1.1602669 1.4082193 1.6623926 1.4965243 0.94360209 -0.61386013 -1.9020298 -2.3557074 -2.6298554][-2.5604403 -1.2128856 -0.589838 -0.15197945 0.089334011 0.14131021 0.33322859 0.52624464 0.84324789 0.82189608 0.36834621 -0.977262 -1.9514048 -2.5303435 -2.9415932][-3.9040885 -2.7848935 -1.5875609 -1.1765289 -0.83774185 -0.56227541 -0.45657635 -0.35429 -0.40474606 -0.359797 -0.68762541 -1.8065898 -2.8347795 -3.171103 -3.243228][-4.9986944 -4.0720468 -3.2592807 -2.4069819 -1.8500512 -1.4426532 -1.1099751 -1.174804 -1.280432 -1.4576283 -1.7038109 -2.402247 -2.913677 -3.1669884 -3.0916891][-5.3603053 -4.5377955 -4.1585155 -3.47782 -2.7549798 -2.5752513 -2.5396035 -2.2816699 -2.2206278 -2.599411 -2.7587578 -2.7832885 -3.1912735 -3.1721888 -3.1862421][-5.4357195 -4.565258 -4.255898 -3.7519579 -3.4256654 -3.3145509 -3.6428027 -3.6751976 -3.5776691 -3.4549823 -3.2417197 -3.188586 -3.2549579 -3.3053341 -3.4676747][-5.050662 -4.2692819 -3.8672142 -3.6438661 -3.7232742 -4.1999955 -4.6690435 -4.5412297 -4.2938929 -4.13138 -3.8724055 -3.5014081 -3.3676443 -3.6921721 -4.0169897]]...]
INFO - root - 2017-12-15 10:33:56.354571: step 1510, loss = 0.38, batch loss = 0.32 (12.2 examples/sec; 0.655 sec/batch; 60h:13m:10s remains)
INFO - root - 2017-12-15 10:34:02.929581: step 1520, loss = 0.34, batch loss = 0.28 (12.3 examples/sec; 0.649 sec/batch; 59h:40m:39s remains)
INFO - root - 2017-12-15 10:34:09.484525: step 1530, loss = 0.34, batch loss = 0.28 (12.4 examples/sec; 0.646 sec/batch; 59h:20m:57s remains)
INFO - root - 2017-12-15 10:34:16.139851: step 1540, loss = 0.26, batch loss = 0.20 (12.2 examples/sec; 0.658 sec/batch; 60h:31m:22s remains)
INFO - root - 2017-12-15 10:34:22.791753: step 1550, loss = 0.27, batch loss = 0.21 (11.8 examples/sec; 0.680 sec/batch; 62h:32m:41s remains)
INFO - root - 2017-12-15 10:34:29.337170: step 1560, loss = 0.32, batch loss = 0.25 (12.2 examples/sec; 0.657 sec/batch; 60h:25m:23s remains)
INFO - root - 2017-12-15 10:34:35.904262: step 1570, loss = 0.22, batch loss = 0.16 (11.5 examples/sec; 0.695 sec/batch; 63h:52m:52s remains)
INFO - root - 2017-12-15 10:34:42.508422: step 1580, loss = 0.30, batch loss = 0.24 (11.8 examples/sec; 0.680 sec/batch; 62h:29m:38s remains)
INFO - root - 2017-12-15 10:34:49.146658: step 1590, loss = 0.33, batch loss = 0.27 (12.1 examples/sec; 0.660 sec/batch; 60h:40m:24s remains)
INFO - root - 2017-12-15 10:34:55.784641: step 1600, loss = 0.28, batch loss = 0.22 (12.2 examples/sec; 0.656 sec/batch; 60h:15m:46s remains)
2017-12-15 10:34:56.314731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1946111 -6.3744822 -7.2840781 -7.3325806 -7.0064445 -6.1899185 -5.0362139 -4.151464 -3.5691094 -3.3289387 -3.3979087 -4.3245335 -5.6094112 -6.8172917 -7.8155594][-5.417254 -6.3759561 -6.8576937 -7.3615179 -7.3011951 -6.9592047 -6.5763025 -5.9329996 -5.5591068 -5.3128357 -5.0616512 -5.4926624 -5.7790461 -7.1055908 -8.1099062][-5.1483212 -6.0768609 -6.91343 -7.0229425 -6.5315876 -6.1159191 -5.3200264 -4.9829545 -5.0583858 -5.1195126 -5.1682367 -5.8519378 -6.4531355 -7.4135394 -7.9848804][-5.5292773 -5.8228769 -5.7951078 -5.5575323 -4.8454337 -4.2287059 -3.6424565 -3.3661795 -3.3069372 -3.6562977 -4.0497165 -4.41192 -5.041163 -6.1583147 -7.0397272][-5.2911811 -5.1553431 -4.9669428 -4.078239 -2.8969862 -2.242269 -1.8548133 -1.6582129 -1.9136789 -2.3767967 -2.7260087 -3.3320727 -3.627979 -4.1052041 -4.66506][-3.9848304 -3.0220852 -2.3200767 -1.1043322 0.024146557 1.1024847 1.4399796 0.72578382 0.021307945 -0.90441227 -1.6032472 -2.1477509 -2.616329 -2.9622262 -3.09336][-3.1210885 -2.1887615 -0.78159952 1.1626921 2.5799141 4.3319306 4.9488549 4.0636086 2.8718281 1.0896287 -0.1126399 -1.4497778 -2.0359051 -2.7340221 -3.2839544][-3.8878794 -2.6420813 -1.182766 1.2522302 3.3615708 5.2476974 5.913177 5.5815463 4.993114 3.4588723 2.0624208 -0.02944231 -1.8244035 -3.118746 -3.7364364][-2.7203109 -2.40149 -1.6946986 0.46120882 2.0498543 3.8362937 4.4482646 4.1280866 3.5384212 3.0465283 3.0956855 1.1823258 -0.86972094 -3.3517222 -5.2187462][-1.6586263 -1.6497087 -0.67088127 0.41277122 1.0631804 2.1565409 2.2894015 1.6936741 1.0405536 0.56194496 0.50781059 -0.23708153 -1.0524096 -3.1170123 -5.1070414][-2.9473703 -2.3788104 -1.6920056 -1.2697878 -1.1829171 -1.3350165 -1.5003698 -1.8182819 -2.3540134 -2.8434687 -2.7755702 -2.8207653 -3.1860342 -4.4840646 -5.3951969][-4.614512 -4.73964 -4.5415049 -4.312748 -4.5400968 -4.4769545 -4.7636247 -5.2734461 -5.4995904 -5.80922 -5.7046 -5.5714226 -5.457314 -5.9709821 -6.34962][-7.6489444 -7.0427165 -6.3457708 -6.1914406 -6.0955434 -5.8670311 -6.0773153 -6.2567968 -6.2906704 -6.6606383 -6.5592694 -6.4422154 -6.361762 -6.6023097 -6.7196941][-8.6913013 -8.3761711 -8.0875244 -7.3965139 -6.3558407 -5.8055515 -5.5710506 -5.1658983 -5.265449 -5.3864055 -5.0134263 -4.9247127 -5.0519376 -5.4238896 -5.8656235][-9.5137043 -9.8610344 -9.4416733 -8.69132 -7.7743921 -7.045465 -6.1859965 -5.4072394 -5.4220815 -5.2660546 -5.1135983 -5.1631608 -4.7004156 -5.065515 -5.1021748]]...]
INFO - root - 2017-12-15 10:35:02.981490: step 1610, loss = 0.32, batch loss = 0.26 (12.7 examples/sec; 0.631 sec/batch; 58h:01m:09s remains)
INFO - root - 2017-12-15 10:35:09.680743: step 1620, loss = 0.27, batch loss = 0.21 (12.5 examples/sec; 0.642 sec/batch; 59h:02m:26s remains)
INFO - root - 2017-12-15 10:35:16.338928: step 1630, loss = 0.25, batch loss = 0.19 (11.6 examples/sec; 0.691 sec/batch; 63h:29m:01s remains)
INFO - root - 2017-12-15 10:35:22.967890: step 1640, loss = 0.26, batch loss = 0.20 (11.9 examples/sec; 0.673 sec/batch; 61h:49m:39s remains)
INFO - root - 2017-12-15 10:35:29.520676: step 1650, loss = 0.24, batch loss = 0.17 (12.5 examples/sec; 0.642 sec/batch; 58h:57m:43s remains)
INFO - root - 2017-12-15 10:35:36.104512: step 1660, loss = 0.36, batch loss = 0.30 (12.4 examples/sec; 0.646 sec/batch; 59h:24m:03s remains)
INFO - root - 2017-12-15 10:35:42.708272: step 1670, loss = 0.25, batch loss = 0.19 (12.1 examples/sec; 0.663 sec/batch; 60h:57m:26s remains)
INFO - root - 2017-12-15 10:35:49.275240: step 1680, loss = 0.28, batch loss = 0.22 (12.2 examples/sec; 0.654 sec/batch; 60h:07m:57s remains)
INFO - root - 2017-12-15 10:35:55.856771: step 1690, loss = 0.33, batch loss = 0.27 (12.3 examples/sec; 0.650 sec/batch; 59h:43m:24s remains)
INFO - root - 2017-12-15 10:36:02.427150: step 1700, loss = 0.25, batch loss = 0.19 (12.3 examples/sec; 0.653 sec/batch; 59h:57m:30s remains)
2017-12-15 10:36:02.987703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6590447 -2.7563405 -2.6762378 -2.532491 -2.4073305 -2.1227312 -1.8418162 -1.6301944 -1.4422927 -1.7653134 -2.0465345 -3.5217319 -4.6343842 -4.7964587 -4.5474][-2.7045352 -3.0377362 -3.0948153 -2.8839667 -2.6673248 -2.4774749 -2.5108087 -2.1982741 -1.9047081 -2.0581961 -2.0610332 -3.3138351 -4.0926762 -4.6238089 -4.7945228][-2.6700656 -2.811799 -2.8530965 -3.0788665 -2.7916596 -2.4717424 -2.3523722 -2.3134279 -2.4145277 -2.4758611 -2.4099081 -3.2936692 -3.7643895 -4.2809405 -4.4186168][-2.8070028 -2.9836605 -3.1045008 -2.9826567 -2.7237079 -2.3973372 -2.2622409 -2.3246977 -2.5265391 -2.704535 -2.8732214 -3.9327912 -4.3466282 -4.4201951 -4.2829928][-3.1735249 -3.0003469 -2.6173682 -2.39142 -2.1987116 -1.9030764 -1.8118238 -1.9037795 -2.0499365 -2.2764368 -2.438025 -3.4912305 -4.1616039 -4.46467 -4.3492064][-3.1489511 -2.6863606 -2.2753639 -1.8724852 -1.7147567 -1.4754665 -1.5592394 -1.5821986 -1.5317442 -1.5569222 -1.6480198 -2.8564801 -3.9086649 -4.566885 -4.5535226][-2.4385483 -2.3926518 -2.32549 -1.7978885 -1.2060685 -1.150239 -1.6670008 -1.7226837 -1.4630282 -1.2658439 -1.1252353 -2.4828451 -3.663388 -4.33229 -4.6385322][-2.253093 -1.8927946 -1.8811386 -1.6742501 -1.4856844 -1.274971 -1.2885947 -1.2858734 -1.2023635 -0.88242006 -0.52938461 -2.0207307 -3.5214972 -4.3572292 -4.5974078][-1.9327741 -1.6657345 -1.6336079 -1.3979192 -1.4305561 -1.2271662 -1.2171471 -1.2854371 -1.1606681 -0.67576694 -0.24347448 -1.765408 -3.292737 -4.4081421 -4.9348264][-1.8639822 -1.7757661 -1.7962723 -1.3425617 -1.2085605 -1.3539975 -1.6613922 -1.528826 -1.1880779 -0.85473347 -0.4965663 -1.8155727 -3.0156965 -4.14783 -4.8478761][-2.4635224 -2.5409646 -2.3142078 -2.2861061 -2.487246 -2.3919656 -2.5190773 -2.3876755 -2.3614132 -2.1175525 -1.8639402 -2.547704 -3.2500277 -4.0713491 -4.5488782][-4.1506524 -3.8394115 -3.6996329 -3.680656 -3.4312234 -3.4173789 -3.8590071 -4.0755534 -3.83408 -3.2476478 -2.952424 -3.5800636 -4.0861683 -4.5929685 -4.6583724][-6.9468837 -6.0629492 -5.3189864 -5.4099126 -4.9663053 -4.8673215 -5.075458 -5.1272635 -4.8584766 -4.3012094 -4.0060329 -4.0510416 -4.3256536 -4.7387033 -4.7269654][-7.6248627 -7.014739 -6.5162678 -6.0955572 -5.7963171 -5.4376907 -4.9178824 -4.990706 -5.1732507 -4.78352 -4.2169418 -3.9483037 -4.2404242 -4.4588947 -4.5144749][-6.3440952 -6.784102 -6.5568714 -5.600606 -4.9633708 -4.8292656 -4.4936147 -4.0538387 -3.6602976 -3.6432843 -3.6531005 -3.5684686 -3.7397389 -4.000885 -4.3553238]]...]
INFO - root - 2017-12-15 10:36:09.562778: step 1710, loss = 0.29, batch loss = 0.23 (12.3 examples/sec; 0.648 sec/batch; 59h:32m:08s remains)
INFO - root - 2017-12-15 10:36:16.094518: step 1720, loss = 0.28, batch loss = 0.22 (12.4 examples/sec; 0.643 sec/batch; 59h:05m:23s remains)
INFO - root - 2017-12-15 10:36:22.660271: step 1730, loss = 0.36, batch loss = 0.30 (12.0 examples/sec; 0.668 sec/batch; 61h:20m:01s remains)
INFO - root - 2017-12-15 10:36:29.212160: step 1740, loss = 0.35, batch loss = 0.29 (12.1 examples/sec; 0.663 sec/batch; 60h:56m:14s remains)
INFO - root - 2017-12-15 10:36:35.839194: step 1750, loss = 0.27, batch loss = 0.21 (12.4 examples/sec; 0.645 sec/batch; 59h:16m:22s remains)
INFO - root - 2017-12-15 10:36:42.422330: step 1760, loss = 0.31, batch loss = 0.25 (11.7 examples/sec; 0.684 sec/batch; 62h:53m:05s remains)
INFO - root - 2017-12-15 10:36:49.013229: step 1770, loss = 0.34, batch loss = 0.28 (12.0 examples/sec; 0.668 sec/batch; 61h:20m:22s remains)
INFO - root - 2017-12-15 10:36:55.586296: step 1780, loss = 0.35, batch loss = 0.29 (11.5 examples/sec; 0.693 sec/batch; 63h:39m:42s remains)
INFO - root - 2017-12-15 10:37:02.155316: step 1790, loss = 0.31, batch loss = 0.25 (12.4 examples/sec; 0.647 sec/batch; 59h:24m:30s remains)
INFO - root - 2017-12-15 10:37:08.697616: step 1800, loss = 0.29, batch loss = 0.23 (12.2 examples/sec; 0.655 sec/batch; 60h:08m:40s remains)
2017-12-15 10:37:09.292542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7875645 -3.0092168 -3.3366323 -3.2553439 -3.23764 -3.0793667 -2.8616662 -2.9190495 -2.8447604 -2.96261 -2.9185421 -3.912354 -5.126132 -5.5861893 -5.9584222][-2.8298581 -3.3951292 -3.8798337 -3.8560023 -3.763063 -3.5760622 -3.480567 -3.5351138 -3.4918854 -3.2317212 -2.8675597 -3.8527589 -4.9453697 -5.7916155 -6.319447][-2.877959 -3.2170739 -3.7105265 -4.0019588 -3.9591932 -3.5904679 -3.1728473 -2.9123526 -2.9580512 -2.7702227 -2.3787568 -3.3371358 -4.4023271 -5.4604197 -5.9709477][-4.1590438 -4.1709309 -4.2090063 -4.1218724 -3.7658436 -3.4240687 -3.0577621 -2.5339622 -2.169708 -2.0255091 -2.0240235 -3.2900884 -4.5848961 -5.8058052 -6.5669079][-4.4000378 -4.5040517 -4.3814178 -3.6256256 -2.6303809 -1.7817075 -0.92020941 -0.56982422 -0.5314188 -0.575644 -0.71503735 -2.3351705 -4.2031946 -5.7710319 -6.5903893][-4.1567535 -3.8778243 -3.2982516 -2.0461128 -0.95511246 0.43668032 1.508534 2.0052118 2.0472155 1.6542711 0.95474434 -1.0610867 -3.1322222 -4.8767071 -5.7896786][-3.6758754 -3.1963468 -2.407053 -0.8504405 0.59802294 2.0064106 2.8510957 3.4053493 3.5457873 2.7675934 1.694726 -0.52108574 -2.7905571 -4.6438751 -5.8697381][-2.97406 -2.2578351 -1.2155199 0.29412317 1.6230416 2.8716693 3.66849 4.0167613 4.0628972 3.4210582 2.3928189 -0.18980694 -2.7162097 -4.6804008 -5.7417264][-2.3332775 -1.4105141 -0.36078358 0.83476734 1.7061462 2.4689322 2.992126 3.1258693 2.8924823 2.424984 1.9034281 -0.31390953 -2.8337324 -4.7793226 -6.0148544][-2.0188835 -1.2040894 -0.4326663 0.70350266 1.2181034 1.5183334 1.5716529 1.62468 1.4590302 0.91529369 0.21717262 -1.670398 -3.4782071 -5.0686722 -6.3415718][-3.6236713 -2.9347389 -2.2850523 -1.620106 -1.5680566 -1.6378403 -1.9186246 -1.8260925 -1.8068745 -2.0645118 -2.4767227 -3.8725371 -5.2270231 -6.3217874 -6.8227921][-5.2553463 -5.0288291 -4.7596722 -4.1715736 -4.010191 -4.1879153 -4.6757565 -5.0291853 -5.3088951 -5.298739 -5.3240032 -6.2252264 -7.1919279 -7.5949531 -7.6197491][-7.1101041 -7.114213 -6.9386559 -6.41873 -6.1402159 -6.1871014 -6.3981185 -6.4788289 -6.721436 -6.8115897 -7.0126944 -7.4215369 -8.0379419 -8.2372322 -8.0401306][-8.96801 -8.8686256 -8.622282 -8.1307707 -7.7055883 -7.3693714 -7.0287552 -6.9760647 -6.9353304 -7.0200977 -7.3026152 -7.32771 -7.5446739 -7.6156778 -7.38414][-8.1851826 -8.509984 -8.4744406 -7.5468121 -6.6657395 -6.4157238 -6.1223269 -5.8399963 -5.3846078 -5.2937317 -5.666378 -6.0627332 -6.517467 -6.6220446 -6.4520493]]...]
INFO - root - 2017-12-15 10:37:15.822348: step 1810, loss = 0.32, batch loss = 0.26 (12.2 examples/sec; 0.657 sec/batch; 60h:22m:06s remains)
INFO - root - 2017-12-15 10:37:22.417525: step 1820, loss = 0.27, batch loss = 0.21 (11.9 examples/sec; 0.674 sec/batch; 61h:52m:47s remains)
INFO - root - 2017-12-15 10:37:29.005927: step 1830, loss = 0.34, batch loss = 0.28 (12.0 examples/sec; 0.668 sec/batch; 61h:22m:54s remains)
INFO - root - 2017-12-15 10:37:35.614179: step 1840, loss = 0.20, batch loss = 0.14 (11.8 examples/sec; 0.679 sec/batch; 62h:19m:40s remains)
INFO - root - 2017-12-15 10:37:42.229856: step 1850, loss = 0.38, batch loss = 0.32 (12.1 examples/sec; 0.664 sec/batch; 60h:58m:33s remains)
INFO - root - 2017-12-15 10:37:48.824900: step 1860, loss = 0.31, batch loss = 0.25 (12.2 examples/sec; 0.657 sec/batch; 60h:21m:33s remains)
INFO - root - 2017-12-15 10:37:55.398896: step 1870, loss = 0.32, batch loss = 0.26 (12.2 examples/sec; 0.656 sec/batch; 60h:17m:29s remains)
INFO - root - 2017-12-15 10:38:01.957638: step 1880, loss = 0.26, batch loss = 0.20 (12.2 examples/sec; 0.656 sec/batch; 60h:12m:04s remains)
INFO - root - 2017-12-15 10:38:08.584721: step 1890, loss = 0.26, batch loss = 0.20 (11.6 examples/sec; 0.690 sec/batch; 63h:23m:21s remains)
INFO - root - 2017-12-15 10:38:15.103658: step 1900, loss = 0.24, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 59h:47m:45s remains)
2017-12-15 10:38:15.621777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3563347 -6.08518 -6.2322645 -5.9555612 -5.4906435 -4.9586787 -4.536325 -4.1232634 -4.2597656 -4.2898164 -4.1781917 -4.6939158 -4.8492827 -5.2373991 -5.7217503][-5.9608035 -7.1028852 -7.8135257 -8.0349674 -7.4026537 -6.4953141 -5.4236546 -4.7492723 -4.5354567 -4.636497 -4.7996182 -5.5121722 -5.623951 -5.2383332 -5.7273593][-4.8553362 -6.1277447 -7.2066708 -7.4507866 -7.3515682 -6.599153 -5.8967023 -5.2713137 -5.3605647 -5.5378356 -5.7811494 -6.6378851 -6.9697738 -6.9821119 -6.980927][-3.6495893 -4.5383244 -5.3394566 -6.0047078 -5.9079714 -5.3522234 -4.7554059 -4.5825896 -4.7054024 -4.8875189 -5.3583927 -6.3308172 -6.910995 -7.2553005 -7.6166744][-4.758575 -4.8690128 -4.9602265 -4.5210323 -3.9481828 -3.5332346 -3.3803024 -3.1376178 -3.0971661 -3.6122494 -4.1429467 -5.4261351 -6.6077743 -7.3582621 -8.4118156][-5.3723278 -4.986619 -4.3797421 -3.38903 -2.2016664 -1.2723181 -0.61705494 -0.48764896 -0.58452272 -1.0205894 -1.706815 -3.5716095 -5.1933889 -6.4695439 -7.5247874][-5.11454 -4.8193355 -4.4572639 -3.0505772 -1.4718182 -0.1043129 1.0323167 1.2093329 1.01054 0.43427134 -0.58983994 -2.5553007 -4.444418 -5.9426427 -7.1563377][-3.3006713 -3.1271923 -3.2709734 -2.4634497 -1.7984951 -0.38959932 1.0081491 1.7474022 1.8911133 1.1262989 -0.10303831 -2.6083369 -4.6376705 -5.9263239 -7.1817141][-0.88221216 -0.42788887 -0.39653063 -0.23978949 -0.337667 0.1825037 0.89138174 1.3385711 1.7119637 1.4759684 0.19979095 -2.389226 -5.0233531 -6.6904688 -7.6246409][-0.12654018 0.40430641 0.35072136 0.41150618 0.23964119 0.21617937 0.169281 -0.0016670227 -0.099577427 -0.2614007 -1.2623618 -3.3513286 -5.3394432 -6.6237597 -7.3209305][-3.0021031 -2.7481341 -2.4837785 -2.8110728 -2.9394181 -3.2658715 -3.4731538 -3.4549117 -3.4316664 -3.203151 -3.1475825 -4.1758819 -5.2635503 -5.859252 -6.4834309][-7.8664141 -7.6090021 -7.5665994 -7.4875298 -7.3338747 -7.4602423 -7.371666 -7.1962385 -6.7272472 -6.1460462 -5.6101556 -5.1374621 -5.1580992 -5.4609456 -5.5354223][-11.244755 -11.563394 -11.596842 -11.142353 -10.580025 -10.097229 -9.818779 -9.3137531 -8.6996059 -7.9939413 -7.2699213 -6.9022632 -6.3488197 -5.9355741 -5.826149][-11.3805 -11.886485 -11.934639 -11.148595 -10.226646 -9.5330677 -9.0526114 -8.4695234 -7.9816532 -7.6806779 -7.3793974 -7.1350589 -7.2418556 -7.2281804 -7.1837993][-9.80439 -10.600976 -10.782922 -10.018729 -9.219841 -8.0084572 -6.9408154 -6.3145814 -6.0458345 -6.3824263 -6.5964985 -7.0913253 -7.6378088 -8.061327 -8.4700909]]...]
INFO - root - 2017-12-15 10:38:22.107653: step 1910, loss = 0.34, batch loss = 0.28 (12.3 examples/sec; 0.652 sec/batch; 59h:53m:42s remains)
INFO - root - 2017-12-15 10:38:28.689193: step 1920, loss = 0.21, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 59h:56m:32s remains)
INFO - root - 2017-12-15 10:38:35.274764: step 1930, loss = 0.37, batch loss = 0.31 (12.1 examples/sec; 0.659 sec/batch; 60h:32m:38s remains)
INFO - root - 2017-12-15 10:38:41.846138: step 1940, loss = 0.33, batch loss = 0.27 (12.5 examples/sec; 0.638 sec/batch; 58h:34m:01s remains)
INFO - root - 2017-12-15 10:38:48.404278: step 1950, loss = 0.32, batch loss = 0.26 (12.1 examples/sec; 0.661 sec/batch; 60h:41m:51s remains)
INFO - root - 2017-12-15 10:38:55.026214: step 1960, loss = 0.30, batch loss = 0.24 (12.2 examples/sec; 0.656 sec/batch; 60h:13m:22s remains)
INFO - root - 2017-12-15 10:39:01.577930: step 1970, loss = 0.22, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 60h:11m:36s remains)
INFO - root - 2017-12-15 10:39:08.190366: step 1980, loss = 0.26, batch loss = 0.20 (12.4 examples/sec; 0.645 sec/batch; 59h:12m:31s remains)
INFO - root - 2017-12-15 10:39:14.756682: step 1990, loss = 0.24, batch loss = 0.18 (12.2 examples/sec; 0.657 sec/batch; 60h:21m:27s remains)
INFO - root - 2017-12-15 10:39:21.358115: step 2000, loss = 0.24, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 59h:14m:41s remains)
2017-12-15 10:39:21.873093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.583354 -1.6370125 -1.6297815 -1.4176974 -2.0095675 -2.1349125 -1.8148191 -1.9263303 -1.6704049 -1.2875896 -1.1648116 -1.7815366 -2.5348289 -3.1975219 -3.7757311][-2.0983665 -1.9632039 -1.7769008 -1.9737577 -1.7711191 -1.8973773 -2.1290219 -1.8264363 -1.5614688 -1.7994015 -2.0591366 -2.4935629 -2.6408637 -3.1289716 -3.4788187][-2.9994495 -2.8449402 -2.7339325 -2.9006112 -2.8755276 -2.7867346 -2.4233685 -2.1680892 -2.3802943 -2.1529987 -2.3987553 -3.4331832 -3.7042074 -3.8712196 -4.3689203][-3.7255406 -3.493958 -3.2283044 -2.8623123 -2.4138589 -2.1787903 -2.7561617 -2.5193532 -2.148495 -2.7167561 -3.2401481 -4.2781997 -5.1349921 -5.7531614 -5.8144383][-4.1294236 -3.8995581 -3.2782116 -2.6419976 -2.4584208 -1.904896 -1.4957874 -1.4187102 -1.7182229 -2.2009099 -2.6337447 -4.1206431 -5.3771992 -6.453362 -7.2050223][-4.4941506 -3.9553056 -3.401243 -1.6638782 -0.44033289 0.40017939 1.1072159 0.82062149 0.38668728 -0.21237898 -1.1433215 -2.5692723 -3.7613895 -5.1963358 -6.2764673][-5.0564389 -4.0505867 -2.6935036 -1.7201693 -0.37019396 1.0813761 1.8043723 1.8046026 1.2762227 0.11907053 -0.51318455 -1.9391379 -3.4683409 -4.3138585 -5.2595749][-4.713645 -4.3309979 -2.9567354 -1.5329974 -0.28251648 1.1689029 2.1844063 1.9130974 1.4462051 0.037159443 -1.463011 -2.7927504 -3.8945568 -4.2871928 -4.317194][-4.6068945 -3.5215111 -2.5728412 -1.264327 -0.28559446 0.39021111 1.0162091 1.3731976 1.0524421 0.58510017 -0.12966299 -2.5775013 -4.3391752 -4.8765907 -5.37033][-5.3922772 -4.3967981 -3.0799828 -1.4347663 -0.7894311 -0.1348958 0.28761721 0.28790474 0.21967125 0.15638256 -0.34525013 -1.8216815 -3.3174107 -4.7865872 -5.6355023][-6.3715568 -5.616539 -4.7567244 -3.3443143 -2.5869539 -2.0148518 -1.7526686 -2.0472825 -2.2765594 -2.5350387 -2.4531374 -3.0220494 -3.6579437 -4.2404027 -5.4244528][-6.9124379 -6.680181 -5.8223152 -5.2478013 -4.9455409 -4.1607671 -3.7281294 -3.7910771 -3.6525142 -4.1964669 -4.7734876 -4.5493197 -4.2478929 -4.2282295 -3.907197][-6.9377313 -6.5883856 -6.3669243 -5.6962361 -5.6215839 -5.756155 -5.6134863 -5.3039513 -5.1436219 -5.2812071 -5.3276873 -6.0661349 -6.1421614 -5.8116078 -5.3395209][-5.9664345 -5.809505 -6.0201859 -5.4483562 -5.6321049 -6.0461097 -5.9259405 -6.2184811 -6.3764563 -6.2873955 -6.30049 -6.8971381 -7.2671375 -7.1366057 -6.5085378][-5.2167873 -5.0873594 -5.426352 -5.6237578 -5.6926856 -5.7502894 -5.5807276 -5.6686678 -5.849443 -6.5546012 -7.4168577 -7.5674305 -7.5850835 -7.6147766 -7.6218543]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 10:39:28.469125: step 2010, loss = 0.29, batch loss = 0.23 (12.4 examples/sec; 0.644 sec/batch; 59h:04m:49s remains)
INFO - root - 2017-12-15 10:39:35.053289: step 2020, loss = 0.26, batch loss = 0.20 (12.1 examples/sec; 0.662 sec/batch; 60h:47m:45s remains)
INFO - root - 2017-12-15 10:39:41.685958: step 2030, loss = 0.23, batch loss = 0.17 (11.6 examples/sec; 0.691 sec/batch; 63h:23m:47s remains)
INFO - root - 2017-12-15 10:39:48.241687: step 2040, loss = 0.31, batch loss = 0.25 (12.6 examples/sec; 0.637 sec/batch; 58h:26m:31s remains)
INFO - root - 2017-12-15 10:39:54.892092: step 2050, loss = 0.25, batch loss = 0.19 (12.1 examples/sec; 0.660 sec/batch; 60h:35m:54s remains)
INFO - root - 2017-12-15 10:40:01.549317: step 2060, loss = 0.22, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 60h:23m:36s remains)
INFO - root - 2017-12-15 10:40:08.187972: step 2070, loss = 0.35, batch loss = 0.29 (11.7 examples/sec; 0.681 sec/batch; 62h:33m:07s remains)
INFO - root - 2017-12-15 10:40:14.748024: step 2080, loss = 0.23, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 60h:34m:39s remains)
INFO - root - 2017-12-15 10:40:21.346550: step 2090, loss = 0.22, batch loss = 0.17 (11.9 examples/sec; 0.670 sec/batch; 61h:30m:30s remains)
INFO - root - 2017-12-15 10:40:28.056091: step 2100, loss = 0.33, batch loss = 0.27 (12.3 examples/sec; 0.649 sec/batch; 59h:32m:57s remains)
2017-12-15 10:40:28.547514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5064259 -4.9423747 -5.0102792 -4.7899251 -4.555644 -4.2855535 -4.3278651 -4.4933529 -4.2127318 -3.2824256 -2.4992313 -3.1129019 -3.6879258 -5.0079737 -5.656436][-4.9156027 -4.991879 -4.9865975 -4.8919067 -4.4529295 -4.2759767 -4.1302104 -4.0020924 -3.7763071 -2.9443314 -2.2381628 -2.9713383 -3.8048947 -5.5937395 -6.5811038][-4.5715737 -5.0132527 -5.0993528 -5.2211552 -5.1133046 -4.6506934 -3.8917663 -3.6984584 -3.3068268 -2.588388 -2.0413873 -3.0033467 -3.7852821 -5.4092307 -6.4773045][-4.5246959 -4.5443754 -4.532259 -4.6790371 -4.6735377 -3.9567623 -3.2001421 -2.2835822 -1.4541407 -1.2073889 -1.2012067 -2.9953215 -4.49786 -6.4852242 -7.6655164][-4.3783922 -4.0394282 -3.9849424 -3.8757899 -3.7070107 -3.0679591 -2.0755103 -1.2985809 -0.87321043 -0.55685806 -1.0477085 -3.2480855 -4.92872 -7.1818452 -8.1249123][-3.8172898 -3.4657433 -3.3508043 -2.8635378 -2.3726728 -1.5019908 -0.25913239 0.094698429 0.6052866 0.30743408 -0.8975563 -3.3429894 -5.3990493 -7.4717293 -8.4224243][-2.8388009 -2.4674463 -2.2087779 -1.3520489 -0.81934214 0.065136433 1.4463501 2.0338016 2.0845456 1.0841928 -0.29683113 -2.921314 -5.1623163 -7.2433815 -8.3398123][-1.8746235 -1.546633 -1.1585774 -0.82259321 -0.566381 0.47714806 1.7797003 2.4324813 2.89854 2.3752761 0.9222436 -2.1649036 -4.6078291 -7.0905538 -8.2574787][-1.7116764 -1.3105679 -0.99474573 -0.76446962 -0.54029942 0.32113552 1.4202609 1.926228 2.4351616 2.1236806 1.0936122 -1.5435231 -3.6841736 -6.0552416 -7.25058][-1.7738278 -1.7025054 -1.4867599 -1.1293941 -1.2655845 -0.81344795 0.016578674 0.71275234 1.3607574 1.1222916 0.68905115 -1.6077335 -3.3972464 -5.2879419 -6.3814464][-3.6044979 -3.5550251 -3.2645597 -3.1734407 -3.2946434 -3.0781574 -2.9031219 -2.5293312 -2.070816 -1.9048326 -1.6879599 -3.2081685 -4.3076692 -5.5802126 -6.1979961][-5.5919518 -5.4187937 -5.2278414 -5.1493568 -5.2390242 -5.1465559 -5.1457372 -4.7914014 -4.4059882 -4.2646227 -4.1908994 -4.8097119 -5.1783161 -6.0519838 -6.1377788][-6.9015317 -6.9665518 -6.7478633 -6.5326729 -6.9135995 -6.58899 -6.3203144 -6.0295887 -5.53116 -5.5305276 -5.6189289 -5.9352927 -6.223825 -6.7474666 -6.5928035][-6.6188378 -6.8323336 -6.8423367 -6.3596454 -6.1383095 -5.6964326 -5.5679965 -5.1035852 -4.9522805 -5.0003138 -5.1335306 -5.6984196 -5.8997011 -6.2983465 -6.2590771][-5.8289208 -6.0134926 -5.9295006 -5.5329704 -5.2500348 -5.0639806 -4.6988611 -4.1106853 -3.9439843 -4.2535787 -4.6200209 -4.9821329 -5.1997838 -5.7793317 -6.1606054]]...]
INFO - root - 2017-12-15 10:40:35.240272: step 2110, loss = 0.25, batch loss = 0.20 (12.0 examples/sec; 0.667 sec/batch; 61h:13m:47s remains)
INFO - root - 2017-12-15 10:40:41.794540: step 2120, loss = 0.23, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 60h:44m:03s remains)
INFO - root - 2017-12-15 10:40:48.399404: step 2130, loss = 0.28, batch loss = 0.22 (11.7 examples/sec; 0.681 sec/batch; 62h:31m:22s remains)
INFO - root - 2017-12-15 10:40:54.987479: step 2140, loss = 0.27, batch loss = 0.22 (12.2 examples/sec; 0.654 sec/batch; 60h:02m:15s remains)
INFO - root - 2017-12-15 10:41:01.557166: step 2150, loss = 0.40, batch loss = 0.34 (12.3 examples/sec; 0.652 sec/batch; 59h:48m:56s remains)
INFO - root - 2017-12-15 10:41:08.228848: step 2160, loss = 0.27, batch loss = 0.21 (12.5 examples/sec; 0.640 sec/batch; 58h:43m:18s remains)
INFO - root - 2017-12-15 10:41:14.897105: step 2170, loss = 0.25, batch loss = 0.19 (11.8 examples/sec; 0.677 sec/batch; 62h:05m:13s remains)
INFO - root - 2017-12-15 10:41:21.541308: step 2180, loss = 0.34, batch loss = 0.28 (12.1 examples/sec; 0.661 sec/batch; 60h:41m:12s remains)
INFO - root - 2017-12-15 10:41:28.146288: step 2190, loss = 0.21, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 60h:17m:02s remains)
INFO - root - 2017-12-15 10:41:34.744172: step 2200, loss = 0.28, batch loss = 0.22 (12.3 examples/sec; 0.653 sec/batch; 59h:54m:30s remains)
2017-12-15 10:41:35.346691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9521902 -2.4225962 -2.591985 -2.3552582 -2.5052431 -2.5153353 -2.8691471 -3.1940098 -3.1780603 -3.0406909 -2.5112267 -2.9347086 -3.2221806 -3.8143435 -4.1604753][-1.73205 -2.1143837 -2.168674 -2.5137413 -2.7038791 -2.766093 -3.1318812 -3.1600668 -3.0922565 -2.9460936 -2.5435886 -3.1876693 -3.3661087 -4.3201423 -4.7233882][-0.88843012 -2.1919577 -3.3792651 -3.8434153 -3.3871007 -3.4144487 -3.0419266 -2.7442076 -2.4617391 -2.2169142 -1.744312 -2.1849117 -2.8471394 -3.771266 -4.8004813][-0.93422318 -2.3349693 -3.487371 -4.0320849 -4.5687122 -4.2576303 -3.288079 -2.2503459 -1.5292041 -0.82874727 -0.51431704 -1.5744524 -2.5906596 -3.6778424 -4.7443051][-1.156858 -2.3728259 -3.3378747 -3.7135482 -4.0833435 -3.7031517 -2.728137 -1.5011585 -0.33928919 0.31544542 0.46891642 -0.34325457 -1.3402603 -2.8797319 -4.2203445][-1.9693332 -2.1591864 -2.3239205 -2.5651584 -2.2727823 -1.565743 -0.64588547 0.84516716 1.7942376 2.0156035 1.8069439 0.23274136 -1.6266978 -3.2455523 -4.5841732][-3.3304491 -3.2871768 -2.9862092 -2.1532958 -0.77868748 0.88555336 3.0986061 4.049139 4.2172718 3.7068644 2.1110039 -0.43635035 -2.3549106 -4.0529118 -5.3132834][-5.0221405 -5.2070365 -4.9577603 -3.4686909 -1.4450631 1.3306279 4.1311684 5.2510996 6.13377 5.0261936 2.7573938 -0.41515064 -3.4332223 -5.4823794 -6.1794424][-5.6165266 -5.7983437 -5.8857322 -4.9903622 -3.4626322 -0.501482 1.9203424 3.9193077 4.8549519 3.0045824 1.6085768 -1.5939419 -4.3944411 -6.0603743 -6.8480396][-5.2989206 -5.8085012 -6.1394577 -5.7062969 -4.7414284 -3.2788942 -1.2273273 0.70671082 0.6755662 -0.2573266 -1.7643907 -5.0463047 -6.6748595 -7.4346514 -7.6462379][-5.7321963 -5.6773367 -5.7786236 -6.4156709 -6.8019304 -5.9371567 -4.6124735 -4.2455053 -3.806752 -4.0792422 -5.4269943 -7.0926867 -7.9181323 -8.8533354 -8.3968821][-7.7427425 -7.5440874 -7.3826675 -7.5334353 -8.1252413 -8.03997 -8.0632658 -7.7956848 -7.7745285 -8.4584694 -8.8071976 -9.0817795 -8.9561052 -8.9789724 -8.1377792][-8.7548056 -8.6252346 -8.7810812 -8.6780167 -8.34005 -8.7333727 -8.7701149 -8.2800474 -8.3657665 -8.5055256 -8.2943478 -8.6200085 -8.8342028 -8.5699215 -8.0192032][-8.8635759 -8.9602308 -8.9536514 -8.1709356 -7.676765 -7.40184 -7.4232855 -7.6271925 -7.9594326 -7.5566783 -7.1062136 -7.0032606 -6.6379356 -6.6610923 -6.6711726][-8.0837421 -8.7756195 -9.033658 -8.77856 -8.16412 -6.9544582 -6.2375083 -6.3682604 -6.3006124 -6.44876 -6.9856329 -6.71828 -6.1256323 -5.8137541 -5.8230557]]...]
INFO - root - 2017-12-15 10:41:41.958867: step 2210, loss = 0.29, batch loss = 0.23 (12.1 examples/sec; 0.661 sec/batch; 60h:40m:39s remains)
INFO - root - 2017-12-15 10:41:48.500397: step 2220, loss = 0.21, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 58h:49m:25s remains)
INFO - root - 2017-12-15 10:41:55.123480: step 2230, loss = 0.24, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 59h:10m:42s remains)
INFO - root - 2017-12-15 10:42:01.720164: step 2240, loss = 0.23, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 60h:02m:50s remains)
INFO - root - 2017-12-15 10:42:08.269835: step 2250, loss = 0.24, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 60h:30m:23s remains)
INFO - root - 2017-12-15 10:42:14.791572: step 2260, loss = 0.33, batch loss = 0.28 (12.4 examples/sec; 0.646 sec/batch; 59h:17m:04s remains)
INFO - root - 2017-12-15 10:42:21.354377: step 2270, loss = 0.26, batch loss = 0.20 (12.3 examples/sec; 0.648 sec/batch; 59h:27m:27s remains)
INFO - root - 2017-12-15 10:42:27.952140: step 2280, loss = 0.31, batch loss = 0.26 (11.9 examples/sec; 0.675 sec/batch; 61h:54m:45s remains)
INFO - root - 2017-12-15 10:42:34.608456: step 2290, loss = 0.23, batch loss = 0.17 (12.4 examples/sec; 0.645 sec/batch; 59h:11m:34s remains)
INFO - root - 2017-12-15 10:42:41.142814: step 2300, loss = 0.22, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 59h:57m:33s remains)
2017-12-15 10:42:41.677089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6923909 -3.2030962 -3.8629599 -4.0537119 -3.8822131 -3.2958941 -2.537524 -2.5454707 -2.7398705 -2.3557236 -2.1263466 -2.6187382 -3.2862029 -3.8864803 -4.4606242][-3.0751922 -4.3360319 -4.68924 -5.2486691 -4.6936216 -3.9928155 -3.564297 -3.3818052 -2.6903813 -2.261112 -1.8744287 -2.6461473 -3.5790281 -4.3390374 -4.8092713][-5.1781611 -5.8841281 -6.3106742 -5.76396 -5.1387897 -4.6178918 -3.8843665 -3.0576932 -2.327188 -1.8455091 -1.6871283 -2.3502212 -3.4646637 -4.194694 -4.7417927][-6.6333184 -7.2841368 -7.4335256 -6.5427608 -5.9364891 -5.1947608 -4.0537019 -2.9651194 -1.9288158 -1.1836963 -0.6225934 -1.3700166 -2.71407 -4.1499534 -5.0074086][-8.4538012 -8.6826077 -8.2606945 -7.0996165 -6.0804214 -4.5950441 -3.0183582 -1.9163618 -1.349148 -0.55450964 -0.2583065 -1.110425 -2.4031141 -4.0012703 -5.2445359][-9.4194145 -9.3825321 -8.5480919 -6.9078121 -4.8053508 -2.5565698 -0.71036434 0.10842037 0.50697947 0.65834665 -0.0020151138 -1.0268164 -2.2072423 -3.7289634 -5.1348596][-9.3810806 -8.6167507 -7.6212678 -5.6645737 -3.2697942 -0.57715034 1.4068198 2.3069181 2.3855853 1.3834496 -0.091931343 -1.6104267 -3.2209337 -4.3145766 -5.3302383][-7.8754387 -6.661952 -5.3687248 -3.6530166 -1.0718632 1.5900354 3.5298572 4.338676 4.040771 2.2440119 0.40318632 -1.717948 -3.7935486 -4.9235792 -5.8720713][-7.2807112 -5.7842741 -3.5199895 -1.5223756 0.11730862 2.0055261 3.8760991 4.4246926 3.6534791 2.3800168 0.64570427 -2.105685 -4.0857468 -5.1986918 -6.4149508][-6.6454844 -5.0532961 -3.1419942 -1.5490353 -0.604373 0.65296841 1.8974404 2.2986817 1.9047909 0.53520727 -1.2155714 -2.9244916 -4.3731036 -5.6169753 -6.5626698][-7.2599812 -5.7263713 -4.2334576 -3.143369 -2.5596662 -1.6596353 -1.0096464 -0.96535063 -1.3949924 -2.0949433 -3.219842 -5.1879511 -6.4594603 -7.1520581 -7.6179218][-7.4885159 -6.7959423 -5.98954 -5.5835485 -4.9651637 -4.4370823 -4.1384544 -4.1393929 -4.7244124 -5.4355531 -6.1294608 -7.1299243 -8.6731339 -9.0480242 -8.2680817][-8.2801657 -7.2374306 -6.8273783 -6.5724549 -5.896203 -5.5512848 -5.3087621 -5.2271056 -5.8330641 -6.7565279 -7.4160061 -8.3035431 -9.4438782 -10.041739 -9.5979118][-8.4963846 -8.3905449 -8.4708414 -7.97167 -7.355432 -6.2793517 -5.5263233 -5.5883379 -5.6890144 -6.3723025 -7.3947372 -8.0925369 -8.5574074 -9.0224485 -8.7529583][-7.9466352 -7.8136387 -7.7406564 -7.68081 -7.3550558 -6.5809941 -6.0778728 -5.7758789 -5.8164711 -6.3603196 -7.1937733 -8.205946 -8.53417 -8.520051 -7.9239955]]...]
INFO - root - 2017-12-15 10:42:48.243091: step 2310, loss = 0.21, batch loss = 0.15 (12.8 examples/sec; 0.627 sec/batch; 57h:28m:33s remains)
INFO - root - 2017-12-15 10:42:54.825592: step 2320, loss = 0.27, batch loss = 0.21 (11.9 examples/sec; 0.671 sec/batch; 61h:35m:04s remains)
INFO - root - 2017-12-15 10:43:01.331691: step 2330, loss = 0.25, batch loss = 0.19 (12.5 examples/sec; 0.639 sec/batch; 58h:37m:01s remains)
INFO - root - 2017-12-15 10:43:07.926501: step 2340, loss = 0.23, batch loss = 0.18 (12.0 examples/sec; 0.664 sec/batch; 60h:53m:52s remains)
INFO - root - 2017-12-15 10:43:14.567314: step 2350, loss = 0.26, batch loss = 0.21 (12.2 examples/sec; 0.655 sec/batch; 60h:05m:21s remains)
INFO - root - 2017-12-15 10:43:21.182081: step 2360, loss = 0.23, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 59h:57m:45s remains)
INFO - root - 2017-12-15 10:43:27.793429: step 2370, loss = 0.23, batch loss = 0.17 (12.0 examples/sec; 0.669 sec/batch; 61h:19m:25s remains)
INFO - root - 2017-12-15 10:43:34.410568: step 2380, loss = 0.32, batch loss = 0.26 (12.1 examples/sec; 0.659 sec/batch; 60h:25m:57s remains)
INFO - root - 2017-12-15 10:43:40.984907: step 2390, loss = 0.22, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 61h:23m:57s remains)
INFO - root - 2017-12-15 10:43:47.581241: step 2400, loss = 0.22, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 58h:28m:56s remains)
2017-12-15 10:43:48.070711: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.0256705 1.0562282 1.23492 1.0889235 0.752532 0.19749594 0.18270302 0.17766094 0.16780472 0.48104954 1.2005816 -0.076486111 -1.5187323 -2.6303234 -3.4843612][1.6029601 1.4748616 1.400867 1.1252713 0.45460892 -0.24285793 -0.34408331 -0.30376244 0.019862175 0.67376852 1.4080744 -0.04496336 -1.8379974 -3.0140333 -4.0107675][0.38017607 0.62575769 0.69702721 0.33014822 -0.6652627 -1.0352588 -1.0579085 -0.44105911 0.55060863 0.91408777 1.1065965 -0.43879271 -2.1212456 -3.2816625 -4.3187189][-2.3571227 -2.0309937 -1.9717095 -1.6879175 -1.7814159 -1.2870641 -0.83820057 -0.1448164 1.0033884 1.9116964 2.2207246 -0.014119625 -2.3792293 -4.0492105 -5.1102309][-4.9683337 -4.7693653 -4.01827 -2.9147465 -2.3302588 -1.3054194 -0.47437286 0.62544346 1.5704036 1.9793315 2.0529842 -0.16819191 -2.45911 -4.8324385 -6.5136166][-5.8375125 -5.6256866 -4.6515784 -2.2276094 -0.59111309 0.96618605 2.1407785 2.6456051 2.9474511 2.6797328 1.8680639 -0.46884775 -2.91938 -4.9700871 -7.0757504][-5.775763 -4.8605032 -3.4991436 -0.96285772 0.80762148 2.409656 3.2384353 3.528851 3.7625623 3.0788941 1.8194461 -0.64989805 -3.2392471 -5.56694 -6.7991419][-5.1490026 -4.7439079 -3.8666377 -1.5365965 0.74533463 2.8426442 3.9365668 4.3654027 4.0415521 2.8828511 1.3405819 -0.96450138 -3.080987 -4.3848324 -5.2445836][-5.0865693 -4.9516044 -4.4399943 -2.7567344 -1.1511879 0.63166285 2.0810709 2.7815661 2.8461003 2.0014033 0.78026152 -1.6369758 -3.6738491 -4.399579 -4.8862295][-5.3744817 -5.9196792 -5.99364 -5.1153793 -4.2181382 -2.6240005 -1.1171956 -0.48656416 -0.13893604 -1.0241742 -1.7388968 -3.25477 -4.3989997 -4.7293158 -4.966599][-7.254797 -7.8849616 -7.986722 -6.9907508 -6.516552 -5.7885094 -5.0803652 -4.8880849 -4.2953596 -4.0965748 -3.8933144 -5.0111027 -5.4082842 -5.8670182 -5.7220049][-9.9724092 -9.584425 -8.5358877 -7.9954181 -7.4810228 -6.96675 -7.3194647 -7.5238771 -7.5030994 -6.8351879 -5.8392015 -5.4701004 -4.9420729 -4.5994692 -4.0415525][-12.515559 -11.075023 -9.7600231 -8.1966438 -7.2696567 -6.9602675 -7.4761834 -8.4189663 -8.7697353 -7.6959548 -6.9049845 -5.6321797 -4.9561162 -3.7894659 -3.1358316][-11.874626 -11.125164 -9.4830627 -8.0444212 -6.8422222 -5.9791179 -6.1389294 -7.0817327 -7.4538364 -7.2973957 -6.6907911 -5.1554008 -4.32963 -2.9817402 -2.2662671][-10.392736 -10.004536 -8.5867977 -7.13497 -5.6934657 -5.063168 -4.8413281 -5.7232304 -5.7778234 -6.007544 -6.1213069 -5.8157721 -5.1325397 -4.6187148 -4.2116318]]...]
INFO - root - 2017-12-15 10:43:54.635118: step 2410, loss = 0.24, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 60h:07m:58s remains)
INFO - root - 2017-12-15 10:44:01.194225: step 2420, loss = 0.29, batch loss = 0.23 (11.8 examples/sec; 0.676 sec/batch; 62h:00m:38s remains)
INFO - root - 2017-12-15 10:44:07.815057: step 2430, loss = 0.21, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 60h:41m:01s remains)
INFO - root - 2017-12-15 10:44:14.424061: step 2440, loss = 0.24, batch loss = 0.18 (11.9 examples/sec; 0.673 sec/batch; 61h:41m:25s remains)
INFO - root - 2017-12-15 10:44:20.972817: step 2450, loss = 0.24, batch loss = 0.18 (12.0 examples/sec; 0.669 sec/batch; 61h:22m:05s remains)
INFO - root - 2017-12-15 10:44:27.582686: step 2460, loss = 0.19, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 59h:51m:57s remains)
INFO - root - 2017-12-15 10:44:34.195489: step 2470, loss = 0.20, batch loss = 0.14 (11.8 examples/sec; 0.681 sec/batch; 62h:23m:24s remains)
INFO - root - 2017-12-15 10:44:40.782755: step 2480, loss = 0.21, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 59h:56m:50s remains)
INFO - root - 2017-12-15 10:44:47.376183: step 2490, loss = 0.21, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 60h:28m:28s remains)
INFO - root - 2017-12-15 10:44:53.923120: step 2500, loss = 0.30, batch loss = 0.24 (12.3 examples/sec; 0.652 sec/batch; 59h:44m:07s remains)
2017-12-15 10:44:54.426952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5461874 -2.0474758 -2.7187088 -3.3438864 -4.2962403 -4.359252 -4.3983569 -4.15318 -3.3355844 -3.774477 -3.9288392 -3.616302 -4.2150235 -4.8993363 -5.1379285][-2.6486328 -3.1933284 -3.5519376 -4.570056 -4.500123 -4.3887119 -5.0609713 -4.7367244 -4.6077986 -4.088768 -3.302705 -3.3609154 -4.4758635 -5.5938015 -6.0255427][-1.7616792 -2.397347 -4.7148538 -5.8930221 -5.6210809 -5.0199637 -4.5576344 -3.6749053 -3.6670237 -3.1549 -2.4885025 -2.5758402 -3.8575032 -5.2366509 -6.0120153][-3.359544 -2.8366859 -3.2062187 -4.3387408 -4.87558 -4.309309 -3.3796973 -3.1291249 -3.1552894 -2.2342591 -2.6221004 -3.1314716 -4.7086282 -6.545495 -7.4083695][-3.565248 -3.3785033 -3.2581396 -3.271281 -3.46517 -1.661263 -0.20414543 0.13074255 0.35220528 0.20736361 0.14175272 -0.2226038 -2.987982 -5.9678655 -7.8319044][-3.4120686 -2.9318149 -2.1891584 -1.6289818 -0.81924725 1.0781794 2.746139 3.0012145 2.8441892 2.2585344 1.78196 1.1667385 -1.2363763 -4.0073891 -6.6982312][-2.533761 -2.4154658 -2.0082345 -1.3566895 0.2004838 2.4058332 3.0325017 3.314796 3.970552 3.9934392 3.849339 2.4449849 -0.33619785 -3.339587 -5.6854048][-2.1611502 -1.7504125 -1.8009584 -1.1050282 0.27101898 2.1451297 3.9955068 3.9450793 2.8350558 3.6306252 5.1768508 4.0681748 0.732512 -2.965513 -5.5577784][-2.0863369 -1.8953509 -1.609731 -1.7193944 -0.57346153 0.82925987 2.1792259 3.045733 2.7298522 2.3318686 2.5464706 3.0861583 1.4591646 -2.3418567 -5.7843][-3.6396511 -3.5566845 -3.1726906 -2.7276974 -1.6499658 -0.1925602 0.6340971 0.97953558 0.78316069 1.2216921 1.5700779 2.2126665 0.011452675 -2.791352 -4.9389443][-5.4975376 -5.1702089 -5.234458 -5.3054223 -3.4647036 -1.0014472 -0.004860878 -0.095725536 0.11543989 0.662127 0.96005344 0.94717884 -1.4321284 -3.5765467 -5.49866][-9.1489677 -7.7345743 -6.4306793 -6.5242186 -5.12212 -2.4329314 -1.2485328 -0.84917259 -1.5930247 -1.7459364 -0.93817377 -0.68131351 -2.5778768 -4.0015874 -5.3825331][-9.1206121 -7.9076986 -7.3066778 -6.5733008 -5.1036973 -3.1632633 -1.8258712 -0.65616751 -0.82077122 -1.362031 -1.4009032 -1.1154256 -2.8084037 -4.3587179 -5.0274663][-7.841383 -7.5728979 -6.6784821 -5.06774 -4.1390066 -2.5291972 -1.7289793 -0.6601119 -0.091677666 0.14426851 -0.68301296 -1.7767837 -2.7816527 -4.2753935 -5.3179073][-6.48123 -5.9095678 -4.58705 -3.4020646 -3.1225564 -2.0646493 -1.1508188 -0.82713079 -0.36826229 -0.066574574 -0.93543339 -2.2486415 -3.6145372 -4.6298819 -5.7471881]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 10:45:01.816425: step 2510, loss = 0.30, batch loss = 0.24 (12.6 examples/sec; 0.635 sec/batch; 58h:11m:45s remains)
INFO - root - 2017-12-15 10:45:08.316403: step 2520, loss = 0.21, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 60h:25m:45s remains)
INFO - root - 2017-12-15 10:45:14.919241: step 2530, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 58h:46m:57s remains)
INFO - root - 2017-12-15 10:45:21.468435: step 2540, loss = 0.21, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 59h:58m:04s remains)
INFO - root - 2017-12-15 10:45:28.046282: step 2550, loss = 0.28, batch loss = 0.22 (12.2 examples/sec; 0.656 sec/batch; 60h:06m:41s remains)
INFO - root - 2017-12-15 10:45:34.580711: step 2560, loss = 0.27, batch loss = 0.22 (12.6 examples/sec; 0.636 sec/batch; 58h:17m:45s remains)
INFO - root - 2017-12-15 10:45:41.232846: step 2570, loss = 0.25, batch loss = 0.19 (12.2 examples/sec; 0.658 sec/batch; 60h:17m:23s remains)
INFO - root - 2017-12-15 10:45:47.810726: step 2580, loss = 0.28, batch loss = 0.22 (11.9 examples/sec; 0.672 sec/batch; 61h:33m:26s remains)
INFO - root - 2017-12-15 10:45:54.408284: step 2590, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 60h:33m:13s remains)
INFO - root - 2017-12-15 10:46:00.968880: step 2600, loss = 0.21, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 60h:51m:48s remains)
2017-12-15 10:46:01.481201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.193489 -10.755666 -10.336399 -9.7041559 -9.3568726 -7.8610954 -6.6272488 -6.032784 -5.3239665 -5.10618 -5.5054331 -6.4094877 -7.8089361 -8.0058708 -7.2256575][-8.006444 -8.2311764 -8.8085632 -9.090189 -8.9796886 -8.4121656 -7.9214792 -7.3503881 -6.1934042 -6.009666 -5.5612078 -6.0060139 -7.1532164 -7.5961051 -6.8299112][-5.7038307 -6.3026114 -6.6361356 -6.5604239 -7.26737 -7.4103017 -7.4619055 -7.3222752 -6.5789442 -5.8815312 -5.27711 -5.4322596 -6.499938 -6.9789624 -7.0305386][-4.0483336 -3.3772156 -3.6595864 -4.1493049 -4.6726441 -4.8819208 -5.3107314 -5.4787707 -5.28341 -4.8215027 -4.1274815 -4.14259 -5.0136685 -5.3778915 -5.4977579][-4.6122589 -3.6310434 -2.6687002 -2.2249744 -1.8667421 -1.3768239 -1.7268252 -2.7327397 -3.1502686 -3.118408 -3.0373886 -3.294426 -4.189064 -4.9039464 -4.9789047][-5.7893324 -3.9675748 -2.3890991 -1.1313448 0.16762304 1.185286 1.3227582 0.8847332 0.70269966 -0.14387894 -1.0935597 -1.8496988 -3.0988746 -3.6437871 -3.7586799][-5.5747862 -3.8940783 -2.1588295 -0.73198462 0.71490288 2.5246305 3.0483418 2.9819307 2.9773417 2.385489 1.417551 0.015607357 -1.8901534 -2.6119418 -2.66706][-3.4816036 -1.8829188 -0.63795424 0.2152586 0.89609528 2.589715 3.6435547 3.8086624 4.1392612 3.3502092 2.5493531 1.0369806 -1.0516047 -1.8956003 -2.126308][-2.8892226 -1.3194842 0.40908241 0.9301672 1.0195622 2.2438073 2.40521 2.6901484 3.1183834 2.6454382 2.2124209 0.79113579 -1.4033329 -2.4972432 -2.7407007][-3.4917181 -1.3652797 0.44165659 0.96812105 1.0936794 1.4429655 1.5614667 1.6667252 1.4558263 1.4969764 1.1692758 -0.27212143 -2.0347505 -2.9523141 -3.1207862][-4.99788 -3.0724483 -1.2592616 -0.52903652 -0.1241951 0.36088562 0.92087984 0.97940016 0.86489487 0.38821554 -0.09118557 -1.3315749 -2.8921049 -3.8557158 -3.8955104][-5.4192119 -4.0791092 -2.8901079 -2.0784619 -1.6854308 -1.5391569 -1.232193 -1.0036197 -1.0944118 -1.1946239 -1.5136964 -2.1429281 -3.267118 -3.4683752 -3.3142743][-5.9893279 -4.8451929 -3.967618 -3.3427022 -2.8061776 -2.6137958 -2.5055957 -2.4982789 -2.7657113 -3.0760124 -2.6959853 -2.7427642 -2.9274824 -2.7770302 -2.5193911][-5.9504585 -5.2902012 -4.842855 -3.9613185 -3.3591554 -3.3145545 -3.5225883 -3.5419946 -3.7574856 -3.8304226 -3.5299995 -3.1049092 -2.6339316 -2.333981 -2.1476376][-5.7071657 -4.3585825 -4.0249043 -3.676682 -3.2143085 -2.8938901 -3.3029022 -4.010344 -4.2476387 -4.2146783 -3.9517713 -3.8863437 -3.6361017 -3.3744941 -2.9420156]]...]
INFO - root - 2017-12-15 10:46:07.974249: step 2610, loss = 0.18, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 59h:46m:34s remains)
INFO - root - 2017-12-15 10:46:14.599715: step 2620, loss = 0.23, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 58h:46m:43s remains)
INFO - root - 2017-12-15 10:46:21.227174: step 2630, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 60h:48m:52s remains)
INFO - root - 2017-12-15 10:46:27.810317: step 2640, loss = 0.18, batch loss = 0.12 (11.7 examples/sec; 0.686 sec/batch; 62h:52m:16s remains)
INFO - root - 2017-12-15 10:46:34.328444: step 2650, loss = 0.22, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 59h:37m:14s remains)
INFO - root - 2017-12-15 10:46:40.990614: step 2660, loss = 0.25, batch loss = 0.19 (12.0 examples/sec; 0.667 sec/batch; 61h:06m:49s remains)
INFO - root - 2017-12-15 10:46:47.555256: step 2670, loss = 0.27, batch loss = 0.22 (12.4 examples/sec; 0.645 sec/batch; 59h:04m:03s remains)
INFO - root - 2017-12-15 10:46:54.019335: step 2680, loss = 0.23, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 59h:44m:06s remains)
INFO - root - 2017-12-15 10:47:00.522759: step 2690, loss = 0.24, batch loss = 0.19 (11.9 examples/sec; 0.670 sec/batch; 61h:20m:11s remains)
INFO - root - 2017-12-15 10:47:07.116726: step 2700, loss = 0.28, batch loss = 0.22 (12.0 examples/sec; 0.664 sec/batch; 60h:52m:22s remains)
2017-12-15 10:47:07.671296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4599543 -7.1216559 -7.6776743 -8.2155333 -8.4339991 -8.5518913 -8.5615587 -7.734241 -6.3328013 -5.3068895 -4.0614157 -5.444118 -5.9589858 -6.9260249 -7.2113338][-4.4456692 -6.6718407 -8.2349644 -8.9288015 -9.5593739 -10.182135 -10.425408 -9.6551952 -8.553791 -6.9794855 -5.8391266 -7.15006 -7.3132892 -7.9505444 -7.8678994][-2.2812903 -4.7314982 -6.6822147 -7.4362664 -8.7191772 -9.7755508 -10.107527 -9.3841143 -8.8327475 -7.7835956 -6.7727189 -7.5074377 -7.734345 -8.3732052 -8.5605278][-2.7195277 -3.6370783 -4.1846437 -4.843049 -5.6756244 -6.2071543 -6.4509006 -6.9032845 -7.1829367 -6.24211 -5.5907335 -6.6846433 -7.1907415 -7.8287644 -7.8998041][-4.962595 -5.032115 -4.9234104 -3.796567 -3.0281377 -2.3662641 -1.7063363 -2.8793139 -4.2421951 -4.4609766 -4.7883015 -5.8918071 -6.6221504 -7.7717495 -7.7241106][-6.8945475 -6.3628211 -5.1636248 -2.4794171 -0.19128561 1.6947584 3.4866824 2.6942906 1.7593002 -0.6141696 -3.1685381 -5.31904 -6.5166869 -7.1865969 -6.9982853][-7.09202 -5.8638911 -4.0584855 -1.2891827 0.75002337 3.7299972 6.2229104 6.8501439 7.2552009 3.3739266 -0.21476173 -3.8246939 -6.7945919 -7.6313162 -7.4558907][-5.8145328 -4.289803 -2.5380144 -0.29295111 1.0738626 3.8971 6.3854823 7.5000329 8.0157986 5.6736531 3.3600392 -2.2575755 -6.09669 -7.3966064 -8.4879875][-3.5093551 -2.4221928 -1.58868 -0.4405632 0.67850208 2.8569503 4.2403512 5.5191989 6.1583352 5.1986194 3.749146 -1.3026886 -5.0028028 -7.6486473 -8.7516165][-4.0217047 -2.6037018 -1.5692089 -1.1693125 -0.92604876 0.55676794 1.9475956 3.2252336 3.8828406 3.4095521 1.9024258 -2.0001843 -4.5308757 -7.4812784 -9.0018177][-6.1087766 -5.5117674 -4.707376 -4.3035622 -3.7757988 -3.5371642 -2.4590087 -1.1714382 -0.81515789 -1.1629372 -1.7521801 -4.5359554 -7.0516186 -9.0837212 -9.4510975][-9.8126822 -8.9311581 -8.2621889 -7.8006516 -7.8371162 -7.6582513 -7.1584835 -6.7609735 -6.4459338 -6.2625327 -6.4299564 -7.7728996 -9.0673695 -10.360955 -10.676294][-12.652047 -12.006507 -10.605776 -10.020692 -9.905714 -9.4569759 -9.3090115 -9.4984007 -9.60674 -9.3206453 -9.1318483 -8.7916689 -8.4581976 -9.0615816 -9.335763][-10.043522 -10.642112 -10.459254 -10.158249 -9.528388 -9.1042871 -9.3067913 -9.5063629 -9.6652985 -9.2378635 -8.80674 -7.9423742 -7.4585943 -7.052947 -6.4505606][-8.1102753 -8.5280371 -8.6170979 -8.71874 -8.6102123 -9.0749187 -9.3981771 -9.383997 -9.321825 -9.0858259 -8.807312 -8.1677885 -7.868166 -6.6801453 -6.0308638]]...]
INFO - root - 2017-12-15 10:47:14.259067: step 2710, loss = 0.22, batch loss = 0.17 (11.9 examples/sec; 0.671 sec/batch; 61h:25m:35s remains)
INFO - root - 2017-12-15 10:47:20.820749: step 2720, loss = 0.21, batch loss = 0.15 (11.8 examples/sec; 0.679 sec/batch; 62h:12m:22s remains)
INFO - root - 2017-12-15 10:47:27.388243: step 2730, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 60h:26m:15s remains)
INFO - root - 2017-12-15 10:47:33.963257: step 2740, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 60h:39m:02s remains)
INFO - root - 2017-12-15 10:47:40.509646: step 2750, loss = 0.22, batch loss = 0.16 (12.0 examples/sec; 0.667 sec/batch; 61h:03m:07s remains)
INFO - root - 2017-12-15 10:47:47.087189: step 2760, loss = 0.24, batch loss = 0.18 (12.3 examples/sec; 0.653 sec/batch; 59h:46m:28s remains)
INFO - root - 2017-12-15 10:47:53.695222: step 2770, loss = 0.22, batch loss = 0.16 (11.8 examples/sec; 0.677 sec/batch; 62h:00m:16s remains)
INFO - root - 2017-12-15 10:48:00.364179: step 2780, loss = 0.28, batch loss = 0.22 (12.2 examples/sec; 0.656 sec/batch; 60h:06m:48s remains)
INFO - root - 2017-12-15 10:48:06.939685: step 2790, loss = 0.19, batch loss = 0.13 (11.7 examples/sec; 0.683 sec/batch; 62h:32m:45s remains)
INFO - root - 2017-12-15 10:48:13.612380: step 2800, loss = 0.20, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 60h:39m:04s remains)
2017-12-15 10:48:14.135174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6226966 -3.5888021 -3.1272042 -2.4281538 -2.5455494 -2.3063951 -2.4129469 -2.1742642 -2.2712924 -1.8719301 -1.4842799 -3.3064978 -4.4172983 -5.4518914 -5.9649806][-3.1098456 -2.6694307 -2.2449799 -1.9994688 -1.7485242 -1.7124057 -2.2426865 -2.3848791 -2.3246841 -1.7080865 -1.4513862 -2.7471676 -4.2240314 -5.7550383 -6.9502287][-1.1784387 -1.9239912 -2.1443787 -1.6837056 -1.5517018 -1.5910752 -1.3755903 -1.2775016 -1.2821412 -1.3657322 -1.4074273 -2.8484278 -4.2037191 -5.431191 -6.2223673][-1.7251303 -1.5702598 -1.1822619 -0.92568541 -1.0731874 -0.89020443 -0.6733408 -0.48606205 -0.50706625 -0.73849487 -0.8868103 -2.969099 -4.6713376 -5.7474179 -6.2148952][-1.3083625 -0.93306303 -0.74135065 0.051017761 0.24239683 0.60014534 0.65108204 0.38819504 0.38029194 -0.041883945 -0.82207108 -2.9028749 -4.996767 -6.2658792 -6.6769123][-2.2664711 -1.9219081 -0.76155567 0.48696136 1.0518436 1.6451969 2.0777493 1.7302794 1.4250793 0.77546978 -0.1430254 -2.544944 -5.1453948 -6.6889119 -7.3144593][-3.326196 -2.4381704 -1.0396657 0.39844131 1.7707119 2.7688136 3.109591 3.0265965 2.6955724 1.6966815 0.6900177 -2.0289128 -4.9599876 -6.8788419 -7.8671656][-2.0942128 -2.0250483 -1.6983664 -0.00762558 1.6012187 2.9466243 3.0897279 2.6990705 2.5915008 1.8154249 1.1332641 -1.3045626 -4.1242275 -6.5080137 -8.1938572][-1.9357314 -1.4218609 -1.6702371 -0.39997149 0.51694059 1.4119515 2.0481029 2.0478487 1.6747756 1.4128156 1.1636662 -1.2512589 -3.5548425 -5.7290058 -7.2610564][-1.830893 -1.8640883 -2.2441337 -1.4058022 -0.75828218 -0.28828192 0.53971672 0.71390581 0.29209661 0.13035345 0.25508404 -1.8141036 -3.7830505 -5.6133461 -6.6686206][-5.1882272 -4.6937485 -3.9626594 -3.481267 -3.7962518 -3.5311465 -3.1268976 -2.5693867 -2.2707777 -2.2311578 -2.4580421 -4.525207 -5.9494619 -7.1946144 -7.3298826][-7.5622177 -6.3298988 -5.4194269 -4.7867889 -5.122714 -5.1257038 -5.1745057 -4.7177687 -4.2220888 -4.2077985 -4.5532212 -6.2196026 -7.3557639 -8.1480818 -7.65539][-8.6669645 -7.9906607 -7.2804871 -6.203959 -5.6695628 -5.8281045 -6.2702479 -6.1302476 -6.0704794 -5.6514316 -5.4981751 -6.9776611 -8.5062008 -8.6155853 -8.2060995][-7.636672 -7.98422 -7.5631428 -6.476841 -5.8501062 -5.6948671 -5.7199607 -5.8132815 -5.986402 -5.9923739 -5.8777747 -6.4002347 -7.0352349 -7.1145926 -7.5005054][-5.9731269 -6.2609239 -6.459002 -5.868557 -5.1110578 -4.7880545 -4.3805218 -4.5531521 -4.8325424 -5.1490788 -5.6926336 -6.4890985 -7.1298013 -7.1289721 -7.370801]]...]
INFO - root - 2017-12-15 10:48:20.709116: step 2810, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 60h:21m:18s remains)
INFO - root - 2017-12-15 10:48:27.410227: step 2820, loss = 0.22, batch loss = 0.16 (12.6 examples/sec; 0.637 sec/batch; 58h:19m:59s remains)
INFO - root - 2017-12-15 10:48:33.961450: step 2830, loss = 0.19, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 58h:25m:58s remains)
INFO - root - 2017-12-15 10:48:40.492695: step 2840, loss = 0.27, batch loss = 0.21 (12.2 examples/sec; 0.654 sec/batch; 59h:50m:56s remains)
INFO - root - 2017-12-15 10:48:47.190649: step 2850, loss = 0.21, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 60h:03m:38s remains)
INFO - root - 2017-12-15 10:48:53.856836: step 2860, loss = 0.23, batch loss = 0.17 (12.1 examples/sec; 0.664 sec/batch; 60h:46m:40s remains)
INFO - root - 2017-12-15 10:49:00.462333: step 2870, loss = 0.25, batch loss = 0.20 (12.5 examples/sec; 0.641 sec/batch; 58h:42m:48s remains)
INFO - root - 2017-12-15 10:49:07.139273: step 2880, loss = 0.25, batch loss = 0.19 (11.9 examples/sec; 0.672 sec/batch; 61h:30m:10s remains)
INFO - root - 2017-12-15 10:49:13.763972: step 2890, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 60h:11m:10s remains)
INFO - root - 2017-12-15 10:49:20.386049: step 2900, loss = 0.24, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 60h:09m:22s remains)
2017-12-15 10:49:20.901350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5653934 -3.7308929 -3.871376 -3.9423752 -3.36724 -3.1848719 -3.9037278 -3.4988623 -2.9983714 -3.058579 -4.0371094 -6.3805203 -7.5232038 -6.3493342 -5.3741388][-3.31333 -3.1975482 -3.7079177 -3.7969193 -3.1399941 -2.1146839 -1.3611746 -1.8222895 -3.2940552 -3.6012163 -3.1943016 -4.5335441 -5.97308 -6.4737358 -5.8087459][-3.8973789 -3.8960054 -4.1852617 -3.4334497 -2.2302563 -1.3985872 -0.97564793 -1.2192993 -1.7912815 -2.1354604 -2.5483637 -4.67962 -5.3042431 -5.0685916 -5.4858041][-4.06831 -3.9744387 -4.1231737 -3.960114 -3.7882211 -1.5753105 -0.076781273 0.033908844 -0.76100874 -1.386198 -1.73159 -3.7688885 -4.7110171 -5.1219358 -5.3543434][-3.0643935 -2.9417055 -3.1490922 -3.1295538 -2.4697151 -0.51312017 1.0263481 1.0167923 -0.15973139 -0.6410203 -0.50439739 -1.932924 -2.6274724 -4.4852986 -5.6471152][-4.0058713 -2.85971 -2.442591 -1.7049754 -0.4517746 1.0592442 2.3600917 2.3612013 0.92180395 -0.35730553 -0.74556732 -1.8030643 -2.6796212 -3.516078 -4.1649137][-4.2671609 -3.4715893 -2.4734607 -0.82997084 1.2059007 3.1414771 4.4319386 4.3503728 2.9608388 1.0239763 -0.16318178 -1.8501489 -2.8007104 -3.5921121 -4.5449853][-3.7749949 -3.1941657 -2.7281566 -0.19522619 2.1644797 3.9257045 4.839767 4.7806497 3.7009225 2.1169958 0.98928642 -1.3188276 -2.7218373 -3.7685249 -4.5506468][-4.4957519 -3.2014053 -1.2545376 -0.21721125 0.93232107 2.6184573 3.8548198 3.4534268 2.1973567 0.98527575 0.20677757 -2.0862288 -3.6444421 -4.6762772 -4.5846987][-3.7325444 -3.62323 -2.7130029 -1.0687189 0.54477787 1.5084667 1.9885764 1.8340077 1.0022836 0.17249727 -0.44562054 -2.6861613 -3.974648 -4.6464195 -4.60018][-5.2396474 -4.3959684 -3.8510294 -3.356113 -2.600944 -1.8546221 -1.4137216 -0.93613386 -1.6384692 -2.5214696 -3.0201223 -4.3622341 -5.5497804 -5.4504614 -4.2897568][-6.8415222 -5.9977956 -4.8777184 -4.1590071 -4.1341534 -3.6430807 -3.2367127 -3.364816 -3.5978646 -4.1997428 -4.7521515 -5.3334885 -6.3443775 -6.0418825 -5.0586262][-6.8551145 -5.5021014 -5.2210722 -4.3195753 -4.1506929 -3.8747685 -4.0326819 -4.3455343 -4.1347823 -4.1446838 -4.2844086 -4.5920997 -4.9862514 -5.1301303 -4.3610477][-7.0335159 -5.2066336 -4.28804 -4.1241522 -4.2565861 -3.9409595 -4.0934553 -4.4727893 -4.1074171 -4.1866856 -4.248209 -3.9921367 -3.9256091 -3.8922939 -3.8270988][-5.6024585 -5.0381765 -4.9834037 -4.3659086 -3.7242785 -3.3124642 -3.7426062 -3.8576708 -3.9851532 -4.2788057 -3.5238605 -3.5389192 -3.6805468 -3.7410913 -4.3352952]]...]
INFO - root - 2017-12-15 10:49:27.501107: step 2910, loss = 0.32, batch loss = 0.26 (12.1 examples/sec; 0.660 sec/batch; 60h:24m:35s remains)
INFO - root - 2017-12-15 10:49:34.117525: step 2920, loss = 0.25, batch loss = 0.20 (11.8 examples/sec; 0.675 sec/batch; 61h:49m:07s remains)
INFO - root - 2017-12-15 10:49:40.781755: step 2930, loss = 0.17, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 61h:26m:51s remains)
INFO - root - 2017-12-15 10:49:47.404873: step 2940, loss = 0.25, batch loss = 0.20 (11.9 examples/sec; 0.670 sec/batch; 61h:19m:32s remains)
INFO - root - 2017-12-15 10:49:53.926647: step 2950, loss = 0.20, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 60h:12m:20s remains)
INFO - root - 2017-12-15 10:50:00.523373: step 2960, loss = 0.20, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 60h:01m:12s remains)
INFO - root - 2017-12-15 10:50:07.257049: step 2970, loss = 0.23, batch loss = 0.17 (11.4 examples/sec; 0.700 sec/batch; 64h:02m:04s remains)
INFO - root - 2017-12-15 10:50:13.978059: step 2980, loss = 0.24, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 59h:11m:28s remains)
INFO - root - 2017-12-15 10:50:20.553075: step 2990, loss = 0.25, batch loss = 0.20 (12.1 examples/sec; 0.662 sec/batch; 60h:35m:33s remains)
INFO - root - 2017-12-15 10:50:27.145754: step 3000, loss = 0.22, batch loss = 0.16 (11.7 examples/sec; 0.685 sec/batch; 62h:40m:20s remains)
2017-12-15 10:50:27.652367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3794994 -4.2990642 -3.9698439 -3.2701614 -3.2399673 -2.5278516 -1.9521751 -1.9759843 -1.6352568 -1.6646264 -1.7958508 -3.4149451 -5.3943887 -5.7395005 -5.6003065][-5.9751568 -6.372488 -6.0941916 -5.3541265 -4.4471264 -3.3295064 -2.3363581 -1.6743188 -1.3243442 -1.0411725 -1.1815801 -3.3000195 -5.3858223 -5.7638049 -6.2551765][-5.4523854 -6.3429389 -7.0647488 -6.36783 -5.4588294 -4.6308532 -3.5462346 -2.4645405 -2.085639 -2.4639325 -3.0722444 -4.5671668 -6.0484338 -7.1099224 -8.0093241][-5.857358 -5.3092647 -5.1894565 -5.3314052 -4.89713 -4.3252921 -3.8453407 -3.5948846 -3.2428896 -2.8318388 -3.4477735 -5.6851144 -8.1496038 -8.5886812 -8.4607458][-6.240118 -6.4214544 -6.7185578 -5.4431272 -3.9452922 -2.6327581 -2.3428674 -2.8547063 -3.2910218 -4.0998192 -4.8440285 -6.2938333 -8.7665138 -10.050684 -10.742662][-5.7362523 -5.0041361 -4.6359134 -3.4358315 -2.352015 -1.4334784 -0.40780497 -0.11784744 -0.9433279 -2.1439569 -3.154727 -5.3959608 -7.8808689 -9.2285471 -9.8627186][-6.3352327 -5.9931436 -5.3962369 -2.8463607 -0.70147324 0.31568766 1.2674589 1.9103255 1.8318648 0.19085455 -1.9878349 -4.9028311 -7.9302688 -9.0926952 -9.9237013][-5.4431109 -5.4572639 -5.0154185 -2.4831533 -0.40176821 0.83217812 2.1725922 2.4133363 1.4537225 0.80436945 -0.47977114 -4.1074424 -8.1542339 -10.073358 -10.59772][-4.2475066 -3.013041 -2.1768432 -1.0684147 -0.14657736 1.5140181 3.1192894 2.6224108 1.5987358 0.900301 -0.53473806 -3.482018 -7.6547928 -10.360104 -10.994183][-2.6901302 -2.5470555 -2.3889153 -1.250761 -0.6579299 0.77932167 1.5324054 1.0154943 0.29458523 -0.65268946 -1.6993346 -4.435873 -8.0387449 -10.140721 -11.283373][-3.9726081 -3.4086418 -3.2468915 -3.685266 -3.8730245 -3.4351525 -2.5674903 -1.9168501 -2.1842027 -2.4496918 -3.2092447 -5.3301706 -7.4646482 -9.193819 -10.36327][-7.2537479 -6.68173 -6.1826797 -5.7487769 -5.7602258 -6.040719 -5.9454207 -5.645875 -4.9614611 -4.6339517 -5.0217085 -5.9386454 -7.1145716 -7.9214969 -8.4236841][-10.874067 -10.165672 -9.4308882 -8.8174677 -8.6307087 -8.0073223 -7.968564 -7.060339 -6.3762245 -6.0603633 -5.6927314 -5.8753071 -7.0404253 -8.4968615 -9.5231934][-11.627454 -11.138889 -10.018999 -9.0875483 -8.5234814 -8.4503994 -8.2053757 -7.5711579 -6.650136 -6.3658452 -6.4742236 -5.9679575 -5.7886419 -7.0351887 -8.3290892][-11.414198 -10.967731 -10.898727 -9.86712 -8.0962639 -7.1120858 -6.7589531 -6.1933646 -6.0441837 -6.0133133 -6.0318179 -6.5935974 -7.564436 -7.9948816 -8.4114771]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 10:50:34.387766: step 3010, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 59h:43m:32s remains)
INFO - root - 2017-12-15 10:50:40.948424: step 3020, loss = 0.18, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 58h:30m:20s remains)
INFO - root - 2017-12-15 10:50:47.550945: step 3030, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 60h:43m:03s remains)
INFO - root - 2017-12-15 10:50:54.170726: step 3040, loss = 0.21, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 60h:43m:26s remains)
INFO - root - 2017-12-15 10:51:00.724304: step 3050, loss = 0.22, batch loss = 0.17 (12.4 examples/sec; 0.647 sec/batch; 59h:14m:51s remains)
INFO - root - 2017-12-15 10:51:07.219165: step 3060, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 60h:22m:59s remains)
INFO - root - 2017-12-15 10:51:13.879236: step 3070, loss = 0.22, batch loss = 0.16 (12.0 examples/sec; 0.668 sec/batch; 61h:09m:40s remains)
INFO - root - 2017-12-15 10:51:20.462520: step 3080, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 60h:14m:48s remains)
INFO - root - 2017-12-15 10:51:27.045552: step 3090, loss = 0.24, batch loss = 0.19 (12.3 examples/sec; 0.650 sec/batch; 59h:29m:47s remains)
INFO - root - 2017-12-15 10:51:33.677737: step 3100, loss = 0.19, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 58h:34m:47s remains)
2017-12-15 10:51:34.187620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9472129 -2.2250793 -2.7694516 -2.9633713 -3.8743334 -4.3376427 -4.331408 -3.4087126 -2.6682215 -0.84852552 0.69669247 -1.1299853 -1.7328091 -1.4318433 -1.244791][-3.0903151 -3.013335 -2.4914052 -2.7667603 -3.1024029 -3.17201 -3.0386884 -2.1783254 -1.4025297 -0.90515423 -0.22269821 -0.39716434 -0.84331512 -2.8485055 -3.2135117][-3.0332499 -3.4441593 -3.8499122 -3.1847594 -2.6886613 -2.5959971 -2.3798323 -1.783335 -0.77401876 0.87175417 1.6773314 -0.95774221 -2.6265786 -2.9500997 -3.2387657][-3.8038969 -4.0835762 -3.8083076 -3.3123605 -2.8649592 -1.8573506 -1.6725004 -1.1109953 -0.5944829 -0.42779589 -0.0081925392 -1.0912461 -2.354219 -4.3103924 -5.2198977][-5.3495736 -4.8366461 -4.8325915 -3.9310765 -2.4321265 -1.6289084 -0.70851755 -0.23137712 -0.17835474 -0.12577581 -0.11448717 -2.5977716 -3.8210981 -3.8693109 -4.3250761][-6.5438657 -6.8636131 -6.0288897 -3.3001873 -1.2037597 0.34918261 1.2071371 0.77101278 0.25777626 -0.0439291 -0.18519926 -2.2506397 -3.3995979 -4.4592438 -4.8349953][-6.1297135 -5.4435716 -4.6892037 -2.4169176 -0.65035057 1.7496018 3.373899 2.8930488 2.4902005 0.81038952 -0.19675207 -2.6515102 -4.1659832 -4.7419443 -5.2618279][-6.067647 -5.0496092 -3.1854589 -1.0776448 0.17031336 2.8157697 4.6072707 3.8986602 2.7311759 1.304409 0.27385664 -3.345264 -5.3017406 -5.435801 -5.6843352][-6.1812 -5.5686569 -3.8784652 -1.5487032 -0.38397264 1.4245343 2.461134 2.9094558 2.4585738 0.36320496 -1.4077549 -3.9143538 -5.4299541 -6.44344 -6.5867777][-6.5160856 -5.5483274 -4.5685906 -3.0845537 -2.1229026 0.0076861382 1.114747 1.5898623 1.5670762 0.094571114 -1.6249511 -5.107511 -6.9275427 -7.1684752 -7.4053926][-6.540669 -6.4176612 -5.3390689 -4.3908896 -3.061337 -2.4879909 -2.5175202 -1.3779206 -1.6211255 -2.5783067 -3.2204127 -5.786593 -7.2789087 -8.1968708 -7.7440124][-7.2407494 -7.8224659 -7.6350451 -6.6084633 -6.513504 -5.6828876 -4.9736767 -5.4821868 -5.3762465 -5.390624 -6.0146465 -7.0824833 -7.3328156 -8.4895535 -8.95866][-8.4734039 -8.3969631 -8.5171194 -7.9858484 -7.5299511 -7.278142 -7.4464808 -6.6009851 -6.1356292 -6.5883632 -6.6925883 -7.5760188 -7.2426009 -7.0486379 -6.572876][-7.9210668 -8.2515574 -8.4736977 -8.5654383 -7.9679785 -6.7949953 -6.6843677 -6.4292173 -5.90139 -5.6182423 -6.0211015 -6.9600554 -7.2157125 -7.2044773 -6.3676348][-7.2702742 -7.3279595 -7.2762289 -6.9032426 -7.1648941 -6.2997165 -5.1950221 -5.2872391 -6.2911353 -6.7688046 -6.9144859 -7.0158758 -7.4202313 -7.87152 -7.0992794]]...]
INFO - root - 2017-12-15 10:51:40.764602: step 3110, loss = 0.23, batch loss = 0.18 (11.9 examples/sec; 0.670 sec/batch; 61h:16m:28s remains)
INFO - root - 2017-12-15 10:51:47.368190: step 3120, loss = 0.20, batch loss = 0.15 (11.8 examples/sec; 0.675 sec/batch; 61h:47m:12s remains)
INFO - root - 2017-12-15 10:51:53.970048: step 3130, loss = 0.22, batch loss = 0.17 (12.3 examples/sec; 0.651 sec/batch; 59h:32m:28s remains)
INFO - root - 2017-12-15 10:52:00.533287: step 3140, loss = 0.27, batch loss = 0.22 (12.3 examples/sec; 0.649 sec/batch; 59h:24m:05s remains)
INFO - root - 2017-12-15 10:52:07.209299: step 3150, loss = 0.24, batch loss = 0.19 (11.5 examples/sec; 0.695 sec/batch; 63h:36m:41s remains)
INFO - root - 2017-12-15 10:52:13.826725: step 3160, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 60h:04m:45s remains)
INFO - root - 2017-12-15 10:52:20.437787: step 3170, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 61h:25m:59s remains)
INFO - root - 2017-12-15 10:52:27.080820: step 3180, loss = 0.20, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 60h:54m:49s remains)
INFO - root - 2017-12-15 10:52:33.804418: step 3190, loss = 0.23, batch loss = 0.17 (11.6 examples/sec; 0.690 sec/batch; 63h:05m:45s remains)
INFO - root - 2017-12-15 10:52:40.409543: step 3200, loss = 0.20, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 58h:45m:13s remains)
2017-12-15 10:52:40.928061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6949058 -5.2858992 -6.1499314 -5.9639225 -5.6541214 -6.5299845 -7.2501903 -7.0814371 -7.2205725 -7.6685171 -8.3239441 -9.589695 -9.7795582 -7.7739849 -5.3981662][-4.0327477 -5.1199379 -4.8972058 -4.6916084 -5.5535731 -5.6958179 -6.2012205 -6.9795008 -7.1790428 -7.2474775 -7.6986375 -9.014369 -9.1849995 -8.7131538 -7.9571462][-5.6928663 -6.1954012 -6.7166405 -6.130517 -5.6345153 -5.6333752 -4.98148 -5.0061445 -5.593298 -6.1481075 -6.3717947 -7.5723248 -8.4894657 -7.8499446 -6.7726417][-7.4980445 -7.4373484 -6.7248726 -5.6009741 -4.7585068 -4.34879 -4.5474644 -4.5042295 -4.5590811 -4.853548 -5.4756908 -7.3619976 -8.4110537 -8.0677233 -7.0001245][-7.9713259 -7.8640757 -6.9860096 -4.4283962 -2.3430374 -0.53854036 0.45998812 -0.54857731 -2.614049 -3.8141026 -4.7351747 -6.6077232 -7.9776192 -7.6639776 -7.1277819][-7.851222 -7.7758512 -6.8071671 -3.1994328 -0.0084767342 2.1754818 3.871038 3.2990546 1.9840097 -0.76606083 -3.2475924 -5.2745032 -6.6194429 -6.9149189 -7.1037769][-6.8285875 -5.7874084 -4.9655571 -2.3833454 0.029694557 3.1964774 5.0074277 4.8677163 4.63345 2.5515323 0.27439165 -3.1168122 -5.6893415 -5.3885078 -5.409615][-9.0733347 -6.7385726 -3.9764533 -1.7601743 0.33544922 3.5423589 5.3225808 4.4292493 3.1926436 1.6650887 0.41299677 -2.5975404 -5.0910473 -4.9008627 -4.5637431][-8.09178 -7.4035678 -5.3976364 -1.9197345 1.0163183 2.2016106 3.0535674 3.373178 2.5184865 0.27367687 -1.5257361 -4.1591253 -5.9364944 -6.2740989 -6.2823348][-6.5922012 -5.2563291 -5.1964865 -2.6490157 -0.058168888 1.4134688 2.3116856 1.9370918 0.81574392 -1.183928 -2.9308317 -5.8805289 -7.5842943 -7.0298667 -6.7646241][-8.1600981 -7.2174044 -5.5514541 -4.2593188 -4.099875 -2.3522496 -1.093605 -2.1978405 -3.6424909 -4.8702855 -6.13484 -8.2513876 -9.081254 -9.0966873 -8.8184128][-8.53598 -8.0110626 -7.6419916 -7.273839 -7.0706949 -6.6640668 -5.9808016 -5.7131691 -6.0654826 -7.313046 -8.4775782 -9.275795 -9.6250381 -9.9727249 -9.464592][-9.0794449 -8.4198093 -7.5441437 -6.9740648 -6.8299446 -6.5413322 -6.5889583 -6.7289305 -7.2227192 -8.2598848 -8.65818 -9.5313883 -9.699295 -8.7530785 -7.6125565][-8.474637 -8.1914377 -7.9580064 -6.7331767 -6.3137031 -5.5557485 -5.8330507 -5.9778972 -6.6213036 -7.3540349 -8.4641685 -9.6187229 -9.827713 -9.2205267 -8.1020279][-7.9207869 -8.17262 -7.777832 -7.1362967 -6.7397704 -4.9605212 -3.8156567 -4.043263 -5.1307368 -6.1651506 -7.2334433 -8.7698908 -9.38312 -8.99085 -8.7084246]]...]
INFO - root - 2017-12-15 10:52:47.531005: step 3210, loss = 0.25, batch loss = 0.19 (12.1 examples/sec; 0.659 sec/batch; 60h:14m:36s remains)
INFO - root - 2017-12-15 10:52:54.128395: step 3220, loss = 0.21, batch loss = 0.16 (11.8 examples/sec; 0.681 sec/batch; 62h:15m:44s remains)
INFO - root - 2017-12-15 10:53:00.762513: step 3230, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 61h:52m:59s remains)
INFO - root - 2017-12-15 10:53:07.417919: step 3240, loss = 0.30, batch loss = 0.25 (12.1 examples/sec; 0.661 sec/batch; 60h:27m:08s remains)
INFO - root - 2017-12-15 10:53:14.053583: step 3250, loss = 0.23, batch loss = 0.17 (11.8 examples/sec; 0.679 sec/batch; 62h:05m:50s remains)
INFO - root - 2017-12-15 10:53:20.693742: step 3260, loss = 0.21, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 59h:02m:55s remains)
INFO - root - 2017-12-15 10:53:27.322103: step 3270, loss = 0.24, batch loss = 0.18 (11.9 examples/sec; 0.673 sec/batch; 61h:35m:33s remains)
INFO - root - 2017-12-15 10:53:33.957950: step 3280, loss = 0.24, batch loss = 0.19 (11.6 examples/sec; 0.687 sec/batch; 62h:51m:25s remains)
INFO - root - 2017-12-15 10:53:40.573431: step 3290, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 60h:11m:59s remains)
INFO - root - 2017-12-15 10:53:47.191975: step 3300, loss = 0.37, batch loss = 0.32 (12.3 examples/sec; 0.653 sec/batch; 59h:41m:28s remains)
2017-12-15 10:53:47.678020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.203721 -4.189395 -4.2433252 -4.1402531 -4.4030643 -4.4038553 -4.6130023 -4.2602305 -3.8918447 -3.7923493 -3.7149873 -4.6628027 -5.0367723 -5.5215735 -4.9187689][-3.3490217 -2.319201 -2.1792748 -2.4231789 -3.0185003 -3.6312 -3.8242583 -3.9789343 -3.856143 -3.7918739 -3.0565691 -3.5202174 -3.974288 -4.2996039 -3.6833012][-1.5294497 -1.3662434 -1.595731 -1.7132378 -2.0400441 -2.4483087 -2.8674479 -3.1659174 -2.9990153 -3.3165562 -3.3054202 -3.744123 -3.5243349 -3.7340086 -2.9133346][-2.2802644 -2.0947971 -2.4760387 -2.5955958 -2.8920085 -2.7461288 -2.8515303 -3.0611596 -3.404547 -3.3788161 -3.2416456 -3.6377358 -3.8374772 -3.8803458 -3.4524453][-3.2390168 -3.2740347 -3.1545589 -2.4741037 -1.9671226 -1.1317801 -1.0484705 -1.4636574 -1.9809668 -2.1461754 -2.8297887 -3.5670328 -3.8224645 -4.6501651 -4.4835367][-3.8334565 -3.65859 -3.1825132 -2.1318731 -1.3859296 0.028017521 1.2841282 1.0859461 0.34815741 -0.57337523 -1.3890724 -2.641907 -3.8083007 -4.5492678 -4.173378][-4.6599402 -4.081707 -2.5008235 -0.83698225 0.53071594 2.0005713 3.2242641 2.9913607 3.1900253 1.6109819 -0.63555479 -2.8244483 -3.6554561 -4.6244397 -4.5033374][-4.1695185 -3.2830803 -1.8583517 -0.098553658 1.5095763 3.385416 4.6114264 4.4370022 3.9458971 2.3872714 1.0434728 -1.6380026 -3.47846 -4.0646172 -3.6407151][-3.3331828 -2.2388477 -0.6942296 1.1109128 2.0504704 2.9942656 3.6271124 3.2379923 2.1974192 0.4435544 -1.6337183 -3.7707779 -5.0335054 -5.7621565 -5.1248426][-4.6099482 -2.797977 -1.0968633 -0.037356377 0.59033108 1.5491729 2.0257721 1.471715 0.29231882 -1.6640885 -3.1365633 -5.6602664 -6.8886123 -7.0736132 -6.2229276][-6.2940187 -5.015214 -3.0005982 -2.2776361 -2.0580196 -1.6415493 -1.4986582 -1.6640685 -2.2107952 -3.5818887 -5.0316138 -6.2322903 -6.4698324 -6.5283895 -5.7756367][-7.9724154 -5.8095145 -4.4699459 -3.4173512 -2.6534808 -2.5141325 -2.3261371 -2.7978461 -3.7819395 -4.6029444 -5.0138249 -6.2992029 -6.1572294 -5.6661868 -4.5470986][-8.3004169 -6.378891 -4.36253 -3.3715236 -3.0917387 -3.3722634 -4.0572844 -4.3256173 -4.5478249 -5.1129012 -5.3950262 -5.7047315 -5.4999604 -5.2838545 -4.4543056][-6.4110875 -4.0932403 -2.8776975 -2.3233418 -1.8157291 -2.5059254 -3.1743181 -4.076685 -4.6784925 -4.6126809 -4.7833948 -5.0717554 -4.9660068 -4.9371614 -4.6248145][-5.4194412 -3.8822093 -1.8029661 -1.3067832 -1.5766566 -2.2796724 -3.0405655 -3.606549 -4.196053 -4.7883649 -5.076221 -4.6672421 -5.03178 -5.1308522 -5.1219645]]...]
INFO - root - 2017-12-15 10:53:54.199737: step 3310, loss = 0.38, batch loss = 0.33 (12.3 examples/sec; 0.651 sec/batch; 59h:31m:01s remains)
INFO - root - 2017-12-15 10:54:00.769445: step 3320, loss = 0.22, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 60h:34m:10s remains)
INFO - root - 2017-12-15 10:54:07.352692: step 3330, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 60h:22m:07s remains)
INFO - root - 2017-12-15 10:54:13.914079: step 3340, loss = 0.19, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 61h:39m:27s remains)
INFO - root - 2017-12-15 10:54:20.489155: step 3350, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 60h:21m:29s remains)
INFO - root - 2017-12-15 10:54:27.045016: step 3360, loss = 0.20, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 59h:29m:36s remains)
INFO - root - 2017-12-15 10:54:33.638152: step 3370, loss = 0.29, batch loss = 0.23 (11.9 examples/sec; 0.672 sec/batch; 61h:27m:18s remains)
INFO - root - 2017-12-15 10:54:40.108380: step 3380, loss = 0.17, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 59h:46m:23s remains)
INFO - root - 2017-12-15 10:54:46.605265: step 3390, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 60h:45m:42s remains)
INFO - root - 2017-12-15 10:54:53.211942: step 3400, loss = 0.36, batch loss = 0.31 (11.9 examples/sec; 0.670 sec/batch; 61h:16m:22s remains)
2017-12-15 10:54:53.700519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4401693 -5.8714566 -6.0897355 -5.7899709 -6.0120449 -6.45951 -7.0607576 -6.7666221 -6.1362548 -5.3064809 -4.3216267 -5.3011293 -5.952395 -5.9601536 -5.3909879][-5.5876389 -6.1336818 -5.7327938 -5.613698 -6.3584385 -6.6420059 -6.8431339 -6.8023887 -6.3272481 -5.34432 -4.68959 -5.4243069 -6.307157 -6.8119888 -7.3968134][-6.7150846 -7.2445149 -6.746973 -6.0276394 -5.9657607 -6.1222095 -6.1290503 -5.863894 -5.4954815 -5.0322928 -4.3869925 -5.8275189 -7.1098576 -7.0762067 -7.2340221][-7.6830964 -7.5566425 -6.889596 -6.4999142 -5.9865446 -5.1446095 -4.7287054 -4.5668559 -4.1381555 -3.800436 -3.4474962 -5.1846313 -6.7807546 -7.5892782 -8.03758][-7.5887036 -7.5964456 -7.4139609 -6.3283224 -5.1761565 -3.2585273 -2.2560494 -1.6406214 -1.593415 -1.9150937 -1.582938 -3.7009902 -5.8134317 -6.9849081 -7.1290541][-7.6081824 -6.6659126 -5.6806092 -3.8316936 -1.6714258 0.060245037 1.229671 2.0193605 1.5679035 0.9291029 -0.24631834 -2.5603762 -4.2866626 -5.4060984 -5.4210677][-7.752574 -6.6570873 -4.6273012 -1.5607221 0.82493877 2.8683467 4.2706885 4.4045987 4.3851047 3.554215 1.6020575 -1.2294621 -3.854691 -4.9053154 -5.4549184][-7.0574551 -5.3821554 -3.4818141 -0.771657 1.1905279 3.9689317 5.3759027 4.8506007 4.7178769 3.1711974 1.7163534 -1.3414011 -4.8744326 -5.7452617 -6.3133497][-6.6215596 -4.5497975 -2.4838974 -0.95699406 0.30122328 2.5693421 3.4321704 3.1882043 2.8558583 1.4069672 0.50903273 -2.5782146 -5.5118942 -6.6022038 -7.581831][-5.5093455 -4.2716289 -2.2139633 -0.77534103 -0.3053112 -0.066473484 0.16445923 0.4037981 -0.16587496 -0.57684851 -0.70120049 -3.9313538 -6.4898925 -7.5335755 -8.7439766][-6.3405161 -5.979723 -5.0240278 -3.3418922 -3.1200552 -3.8624077 -3.7925575 -3.8446794 -3.9976425 -3.8461347 -4.0136771 -6.3339458 -7.7374625 -9.2256775 -10.060567][-8.3744326 -8.1284752 -7.3030663 -6.4132581 -6.321115 -6.6658115 -6.3589888 -6.6133752 -6.3702574 -6.5471745 -6.6729493 -7.8612332 -8.0772276 -9.1439762 -9.4951477][-10.535686 -9.8764763 -9.3084812 -8.2846384 -7.8445015 -7.2978249 -7.286303 -6.8960109 -6.8154154 -7.4078317 -7.4782386 -7.738822 -7.482379 -8.1350031 -7.2468352][-8.9885054 -8.7998486 -8.4779854 -7.4084959 -6.7700868 -6.2100282 -6.4503345 -5.8556337 -5.969698 -6.1985 -6.1998777 -6.0486054 -6.001111 -6.2023768 -5.77652][-8.0200958 -7.3866067 -6.9277854 -6.5349259 -5.5860963 -4.4361596 -4.1444607 -4.1931686 -5.0169082 -5.0397615 -5.2650418 -5.9792356 -6.357811 -6.0065551 -5.1042423]]...]
INFO - root - 2017-12-15 10:55:00.280663: step 3410, loss = 0.20, batch loss = 0.15 (11.7 examples/sec; 0.685 sec/batch; 62h:36m:24s remains)
INFO - root - 2017-12-15 10:55:06.829009: step 3420, loss = 0.27, batch loss = 0.22 (12.2 examples/sec; 0.655 sec/batch; 59h:51m:09s remains)
INFO - root - 2017-12-15 10:55:13.543917: step 3430, loss = 0.25, batch loss = 0.20 (12.1 examples/sec; 0.662 sec/batch; 60h:30m:18s remains)
INFO - root - 2017-12-15 10:55:20.122922: step 3440, loss = 0.18, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 59h:18m:26s remains)
INFO - root - 2017-12-15 10:55:26.649051: step 3450, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 59h:47m:38s remains)
INFO - root - 2017-12-15 10:55:33.237934: step 3460, loss = 0.34, batch loss = 0.29 (12.0 examples/sec; 0.665 sec/batch; 60h:46m:01s remains)
INFO - root - 2017-12-15 10:55:39.841158: step 3470, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 59h:28m:55s remains)
INFO - root - 2017-12-15 10:55:46.483787: step 3480, loss = 0.28, batch loss = 0.23 (12.0 examples/sec; 0.668 sec/batch; 61h:01m:59s remains)
INFO - root - 2017-12-15 10:55:53.128623: step 3490, loss = 0.22, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 60h:09m:18s remains)
INFO - root - 2017-12-15 10:55:59.685673: step 3500, loss = 0.21, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 60h:18m:35s remains)
2017-12-15 10:56:00.212936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6590147 -5.9653029 -5.23596 -4.9859648 -4.6955109 -4.528347 -4.388227 -3.9985349 -3.8068111 -3.5450943 -3.194618 -4.6502409 -4.8084655 -5.1376028 -4.573997][-6.3813391 -4.9440517 -3.2535317 -2.7819192 -2.6315696 -2.5088289 -2.3939171 -2.2826436 -2.3020294 -2.2135289 -1.7429845 -3.7310238 -4.0065622 -4.62984 -4.2537413][-4.5762625 -3.9542754 -3.627913 -2.3730392 -1.853363 -1.7669108 -1.9388928 -1.7915161 -2.0389278 -1.7066314 -0.93687868 -2.6193604 -2.963177 -3.5429583 -3.6674192][-5.98582 -4.5988383 -2.9396513 -1.9961212 -1.877387 -1.3218045 -1.6473317 -1.7091484 -1.6389365 -1.2333055 -0.81359768 -3.1300983 -3.8019991 -4.2844734 -4.1996546][-5.7526646 -5.0884428 -3.4428508 -1.2933822 -0.37903786 0.05803299 -0.049702168 -0.91193533 -1.4825568 -1.2127423 -1.2336521 -3.5392265 -4.4326105 -5.1135035 -4.8962][-6.2064686 -4.2595739 -2.1707838 -0.019909382 1.4795551 2.2794685 2.7932258 1.7716928 0.80103779 0.16069365 -0.96864605 -3.4988084 -4.7678003 -5.6654158 -5.1428003][-7.9642019 -5.5430336 -2.6330206 1.0089736 2.6582656 3.174026 4.1723523 3.4351907 2.7031398 1.2710376 -0.5358572 -3.5689597 -5.343616 -6.4489789 -6.2187777][-9.2079887 -7.0002666 -4.0142574 0.3239975 3.6179538 4.9453444 5.7170615 4.5033307 3.4923921 1.8310871 0.230721 -3.6022248 -5.8460793 -7.1300335 -7.3009071][-9.6560574 -7.4505892 -5.0402694 -1.9467986 0.782444 3.0763421 4.0414462 2.6146665 1.3600307 -0.12508249 -1.4644814 -5.2391162 -7.2846923 -8.3746681 -8.4427853][-10.56777 -9.0196285 -6.935708 -4.2754812 -2.2727888 -0.9578619 -0.20125723 -1.1374831 -1.7092321 -2.9646327 -3.9513307 -7.1336212 -8.5392857 -9.9415092 -9.878787][-13.411441 -11.485703 -10.371987 -8.5338631 -7.7141857 -7.0852766 -6.5920048 -7.2402644 -7.9739733 -8.5822916 -8.6560812 -10.823542 -11.190265 -11.067318 -9.717103][-14.780914 -13.296932 -12.302232 -11.384878 -10.874304 -10.681583 -10.66821 -10.659121 -11.250162 -11.693964 -11.642864 -12.212817 -11.833637 -11.702211 -9.9356546][-13.505822 -12.242876 -11.342146 -11.000347 -11.256627 -10.930054 -10.876735 -11.123262 -11.566177 -11.497688 -11.779606 -11.844524 -11.16102 -10.375166 -8.0817347][-11.757261 -11.56085 -11.794985 -11.156155 -10.456963 -9.73733 -9.5727758 -9.7995472 -9.9091911 -9.8899393 -10.512337 -10.009121 -9.4123116 -8.50505 -6.724194][-10.808103 -11.234453 -11.67293 -11.049573 -10.889111 -9.2841053 -8.020607 -7.5311513 -7.6151404 -7.9695845 -8.4226723 -8.34622 -8.59177 -7.919539 -6.9332142]]...]
INFO - root - 2017-12-15 10:56:06.764196: step 3510, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 59h:27m:17s remains)
INFO - root - 2017-12-15 10:56:13.422288: step 3520, loss = 0.20, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 59h:44m:15s remains)
INFO - root - 2017-12-15 10:56:20.008798: step 3530, loss = 0.18, batch loss = 0.13 (11.5 examples/sec; 0.694 sec/batch; 63h:26m:26s remains)
INFO - root - 2017-12-15 10:56:26.568466: step 3540, loss = 0.20, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 57h:51m:14s remains)
INFO - root - 2017-12-15 10:56:33.127754: step 3550, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 60h:12m:53s remains)
INFO - root - 2017-12-15 10:56:39.640957: step 3560, loss = 0.22, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 58h:33m:55s remains)
INFO - root - 2017-12-15 10:56:46.207704: step 3570, loss = 0.28, batch loss = 0.22 (12.4 examples/sec; 0.646 sec/batch; 59h:03m:32s remains)
INFO - root - 2017-12-15 10:56:52.719366: step 3580, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 58h:19m:05s remains)
INFO - root - 2017-12-15 10:56:59.259142: step 3590, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 58h:21m:17s remains)
INFO - root - 2017-12-15 10:57:05.933661: step 3600, loss = 0.20, batch loss = 0.15 (11.7 examples/sec; 0.687 sec/batch; 62h:43m:56s remains)
2017-12-15 10:57:06.466696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4986088 -2.8112004 -3.2617471 -4.4429536 -4.6845007 -4.0131812 -4.1610794 -4.0339007 -3.5017655 -3.2595158 -2.5615809 -4.7855296 -5.956389 -6.2543435 -6.1369376][-0.71776104 -0.91574907 -1.5263138 -2.3748832 -2.8939891 -3.051465 -3.1763809 -3.0990076 -2.8214982 -2.7516272 -1.5286269 -3.4050255 -4.6041274 -5.8879008 -4.9373269][-0.80875349 -0.80274534 -0.8971858 -1.8655636 -2.6004333 -2.7459116 -2.9257288 -2.5945835 -1.8669384 -1.6776247 -1.8581002 -3.3209023 -3.9338436 -5.4540796 -5.2201557][-1.5737903 -1.4225607 -1.453258 -1.7438695 -2.0816729 -2.35324 -2.1921961 -1.9672368 -1.3954329 -0.72842836 -0.86898661 -3.3592515 -4.3277469 -4.5293961 -5.2384386][-2.2673917 -2.0063472 -2.2348967 -2.3793745 -2.438328 -1.2110095 -0.4131403 -0.79646969 -0.33683062 -0.24524736 -0.7271347 -2.7473772 -3.9589534 -5.0521083 -5.04994][-1.9293756 -1.6009018 -1.5899553 -0.9489274 -0.13186073 0.48267126 0.74553442 0.9207902 1.2159357 0.45782137 0.028707504 -1.1896167 -2.6295624 -3.4556167 -3.4145339][-2.6820531 -1.5740745 -0.52961588 0.70864677 0.97835255 1.3151131 2.258245 2.2289863 2.3338265 2.1159234 1.4514041 -0.45376444 -2.0487309 -4.0873761 -4.2902784][-3.2240696 -1.7974906 -0.76780272 0.12279844 1.0313039 2.3150802 3.1279531 2.8125949 2.3987613 1.6588783 1.3669934 0.90459108 0.29511547 -2.2259924 -3.3214328][-3.6437616 -2.5105097 -1.1794486 -0.536829 -0.19379425 0.54714727 1.0685439 1.9980454 2.6300397 2.9641337 2.7396131 -0.213449 -2.4969494 -3.0519366 -3.1390774][-3.3937993 -2.9393337 -2.484848 -1.7149174 -1.352777 -0.56163073 0.15252256 0.1165657 0.15962124 2.0823278 3.1056733 0.91418982 -0.61856556 -3.8244071 -5.7867994][-5.5468688 -5.5075593 -4.4593339 -3.3913929 -3.528666 -3.0120366 -2.7455168 -2.5799499 -2.2366321 -1.6074703 -1.283391 -2.434181 -3.2819173 -4.4911647 -5.202301][-7.500968 -6.4199076 -6.1732011 -6.1588478 -6.3444738 -5.8058071 -4.8812523 -3.92805 -3.9426587 -3.6140385 -2.9035251 -4.3918591 -6.1179595 -6.2844553 -5.7761054][-10.540977 -10.264318 -9.0010452 -7.5690155 -6.8429108 -6.8615627 -7.4002481 -6.7515373 -5.8768291 -5.4964719 -5.4526215 -5.8215413 -5.0866489 -5.84066 -7.4963789][-9.3955212 -9.278429 -9.0980215 -7.7642055 -7.0701723 -6.8550215 -6.13457 -5.7055655 -5.4418583 -5.5936146 -5.9367156 -5.0121841 -5.0319686 -7.1673594 -7.0916114][-7.9040279 -6.960885 -7.4625287 -6.9969583 -6.0496249 -4.9183321 -4.7446995 -5.5290651 -5.5456171 -5.5583081 -5.8718753 -6.4612627 -6.7172508 -7.7860403 -8.2398853]]...]
INFO - root - 2017-12-15 10:57:13.028337: step 3610, loss = 0.21, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 60h:52m:43s remains)
INFO - root - 2017-12-15 10:57:19.587203: step 3620, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 60h:23m:49s remains)
INFO - root - 2017-12-15 10:57:26.192997: step 3630, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 60h:07m:24s remains)
INFO - root - 2017-12-15 10:57:32.740672: step 3640, loss = 0.21, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 59h:04m:26s remains)
INFO - root - 2017-12-15 10:57:39.391102: step 3650, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 60h:33m:42s remains)
INFO - root - 2017-12-15 10:57:45.982666: step 3660, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.693 sec/batch; 63h:15m:24s remains)
INFO - root - 2017-12-15 10:57:52.605473: step 3670, loss = 0.25, batch loss = 0.20 (12.2 examples/sec; 0.658 sec/batch; 60h:06m:06s remains)
INFO - root - 2017-12-15 10:57:59.161551: step 3680, loss = 0.17, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 59h:24m:44s remains)
INFO - root - 2017-12-15 10:58:05.743431: step 3690, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 59h:21m:53s remains)
INFO - root - 2017-12-15 10:58:12.394243: step 3700, loss = 0.21, batch loss = 0.15 (11.8 examples/sec; 0.680 sec/batch; 62h:07m:27s remains)
2017-12-15 10:58:12.904738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3916378 -6.5457077 -6.1324449 -4.4249744 -4.4920211 -4.5409794 -4.0192266 -4.0548429 -4.4566727 -4.7745066 -4.5349503 -5.3094254 -5.5165911 -4.5981812 -3.889679][-6.7740884 -6.8706088 -5.8291039 -4.5851145 -4.9276433 -5.0561318 -4.7432418 -4.8148241 -4.6391611 -3.992157 -2.6863034 -3.6615627 -4.5713634 -5.0989285 -5.0187612][-3.4193039 -3.8439319 -4.642343 -3.7819002 -4.0515041 -4.336894 -4.3651924 -4.7476559 -5.0660758 -4.994 -4.1805048 -4.9520884 -5.6367154 -4.8685217 -5.2141094][-1.9199252 -2.0238276 -2.740258 -2.8734305 -3.8455274 -3.382452 -2.6148331 -3.5138113 -4.5095496 -4.3390608 -3.9473844 -4.6477938 -5.17237 -5.786459 -6.586803][-1.6091459 -3.0302424 -2.9168472 -2.3791084 -2.8969488 -1.8124807 -0.61139345 -1.9621716 -3.0450284 -3.4750021 -3.5306089 -4.6330748 -5.4153776 -5.77226 -6.0917826][-2.8783567 -3.4122138 -2.9909825 -2.8459618 -1.8966389 -0.2361412 1.6401572 1.9532275 1.1502037 -0.48959112 -2.00189 -3.4181321 -4.135078 -5.117691 -5.9150033][-6.8864322 -6.3551049 -4.7209034 -2.5278113 0.41757584 2.5434279 4.2748303 4.8895917 4.1577554 1.8352137 -0.638711 -3.2241139 -5.1660876 -5.2783031 -4.8000493][-6.3235784 -6.8431768 -5.8181105 -2.6584976 0.90694666 3.7147164 5.3953128 5.2036424 4.3381505 1.5791984 -0.93873596 -4.5028062 -6.8742433 -6.8282285 -6.6064968][-4.6934652 -5.60193 -4.6235037 -1.9401627 0.71980143 3.4298997 4.8604732 4.2916894 2.7081971 0.47997284 -1.2865539 -5.0201125 -7.550724 -8.6020374 -9.1208153][-3.5757313 -4.7844377 -4.4626484 -2.7529714 -0.54553652 1.4972019 3.1732869 3.023416 1.2956958 -1.1919284 -4.0131598 -7.2945571 -9.4183531 -10.58437 -10.452833][-5.2221541 -6.2869582 -6.6412444 -5.0373826 -3.4534035 -1.3055301 -0.014469147 0.20197773 -0.73750639 -3.2998736 -6.1668439 -9.3669386 -11.549866 -12.411224 -12.054449][-7.1142712 -7.1815896 -6.5420804 -5.5089922 -4.6833363 -2.9566202 -2.2707973 -1.8158536 -2.4835041 -4.0986056 -6.1907434 -8.5735931 -10.075888 -10.613272 -10.311119][-8.3004007 -9.05467 -7.8362164 -6.7168374 -5.6106572 -3.7275667 -3.1270339 -3.5232847 -4.9669819 -5.8630676 -6.6120768 -8.1266508 -8.8524523 -8.5782137 -8.6902456][-7.560194 -8.6385136 -7.9157 -6.2404046 -5.7889128 -5.2584662 -4.3368006 -3.2568321 -3.0698359 -3.731142 -4.9316254 -5.6695752 -6.412127 -6.3817492 -6.3079877][-6.9596128 -9.0557365 -9.9989471 -7.8924627 -7.3081155 -6.3564558 -5.981041 -5.6833482 -5.4446764 -5.5529022 -5.6101274 -5.8981996 -6.3333464 -6.4015913 -6.1908855]]...]
INFO - root - 2017-12-15 10:58:19.496450: step 3710, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 59h:25m:36s remains)
INFO - root - 2017-12-15 10:58:26.115520: step 3720, loss = 0.17, batch loss = 0.12 (11.5 examples/sec; 0.694 sec/batch; 63h:21m:40s remains)
INFO - root - 2017-12-15 10:58:32.672549: step 3730, loss = 0.21, batch loss = 0.16 (12.7 examples/sec; 0.631 sec/batch; 57h:38m:35s remains)
INFO - root - 2017-12-15 10:58:39.256399: step 3740, loss = 0.21, batch loss = 0.16 (11.3 examples/sec; 0.709 sec/batch; 64h:44m:30s remains)
INFO - root - 2017-12-15 10:58:45.870529: step 3750, loss = 0.19, batch loss = 0.14 (11.7 examples/sec; 0.683 sec/batch; 62h:24m:58s remains)
INFO - root - 2017-12-15 10:58:52.451505: step 3760, loss = 0.20, batch loss = 0.15 (11.7 examples/sec; 0.684 sec/batch; 62h:29m:09s remains)
INFO - root - 2017-12-15 10:58:58.948490: step 3770, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 59h:01m:22s remains)
INFO - root - 2017-12-15 10:59:05.631554: step 3780, loss = 0.22, batch loss = 0.17 (11.7 examples/sec; 0.681 sec/batch; 62h:12m:17s remains)
INFO - root - 2017-12-15 10:59:12.270970: step 3790, loss = 0.33, batch loss = 0.27 (12.2 examples/sec; 0.658 sec/batch; 60h:06m:37s remains)
INFO - root - 2017-12-15 10:59:18.906532: step 3800, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 58h:22m:34s remains)
2017-12-15 10:59:19.372281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.7725554 -9.541399 -9.9473543 -10.605339 -11.075247 -11.037333 -10.6555 -9.30933 -8.64521 -7.9221554 -6.6456881 -7.4300251 -8.7027035 -8.5000887 -8.4436][-8.7992468 -9.6350689 -11.262706 -11.902315 -12.523701 -12.646151 -12.290585 -12.063852 -11.523387 -10.182125 -9.0494442 -9.782135 -10.722435 -10.053947 -9.698473][-8.51838 -9.2808628 -11.301523 -11.248937 -12.002695 -12.689107 -12.773819 -11.886175 -11.514532 -11.85273 -11.199816 -11.084696 -12.000649 -11.575151 -10.643667][-8.5252266 -8.62984 -9.12494 -9.302618 -10.33018 -10.132875 -9.8437061 -10.504467 -11.246445 -10.430367 -9.4186163 -10.574801 -11.710702 -11.540556 -11.208591][-8.9142189 -9.3353767 -9.8419247 -9.23974 -8.67015 -7.2922516 -6.1666451 -7.2019205 -8.80656 -8.7003469 -8.2447357 -9.1830158 -10.205357 -10.814748 -11.261282][-11.07942 -10.939003 -10.482731 -9.0501318 -6.6899137 -4.368598 -2.2782159 -1.9991255 -2.9351764 -4.8191662 -7.0381155 -7.6060505 -9.4125376 -10.219036 -10.132437][-10.835163 -11.000847 -10.768686 -7.0064073 -3.7092614 -1.5170135 1.2218771 2.7457581 2.2526021 -1.0588942 -4.0406713 -6.5260639 -9.8608551 -10.153156 -10.377949][-10.025986 -9.4831343 -9.275877 -5.4249387 -1.382617 2.363184 4.6075392 4.474968 3.6484871 1.7078147 -1.4312496 -5.5126705 -9.68905 -10.32889 -10.7347][-8.766674 -7.8533568 -7.1858387 -4.1175375 -0.59669113 2.8585844 4.6169062 4.4126043 3.2305031 1.2386465 -1.3072124 -5.3914046 -9.7653275 -11.210163 -11.118759][-6.824501 -6.8401618 -6.8536477 -4.09495 -0.83801746 1.5320563 3.7401066 3.301672 1.2584863 -0.54996443 -2.4959428 -6.2392306 -10.320091 -11.936247 -12.825958][-7.2391968 -7.9670792 -7.8902187 -6.8791809 -5.6909676 -3.4135504 -0.75961161 -0.7064209 -1.5382352 -3.1600957 -5.30988 -8.7045593 -11.904427 -13.404612 -13.89167][-11.772153 -11.006856 -10.416068 -10.343848 -10.413366 -9.1353025 -7.5246015 -7.1558437 -6.8389378 -7.7656364 -9.1926689 -11.220078 -13.169557 -14.628915 -14.431403][-13.944514 -13.541445 -12.863197 -12.523096 -11.954039 -10.974035 -10.774353 -10.035374 -10.479803 -11.002497 -11.146067 -12.079879 -13.238983 -13.715601 -12.866449][-13.374657 -13.523171 -13.152737 -12.53948 -11.355583 -11.35269 -11.646351 -10.744756 -11.029814 -11.542998 -12.102556 -12.035221 -12.753863 -12.184128 -10.954442][-11.193901 -11.498278 -11.399303 -10.869164 -9.9206352 -9.3666515 -9.2008533 -10.058382 -10.775716 -10.593565 -10.989741 -11.519218 -11.998166 -11.391512 -11.132452]]...]
INFO - root - 2017-12-15 10:59:25.925206: step 3810, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 58h:47m:56s remains)
INFO - root - 2017-12-15 10:59:32.530345: step 3820, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 59h:34m:51s remains)
INFO - root - 2017-12-15 10:59:39.132028: step 3830, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 60h:06m:40s remains)
INFO - root - 2017-12-15 10:59:45.652273: step 3840, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 59h:32m:35s remains)
INFO - root - 2017-12-15 10:59:52.301250: step 3850, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 61h:10m:10s remains)
INFO - root - 2017-12-15 10:59:58.812666: step 3860, loss = 0.21, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 58h:23m:42s remains)
INFO - root - 2017-12-15 11:00:05.452987: step 3870, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 62h:04m:03s remains)
INFO - root - 2017-12-15 11:00:12.039486: step 3880, loss = 0.27, batch loss = 0.21 (11.9 examples/sec; 0.671 sec/batch; 61h:16m:17s remains)
INFO - root - 2017-12-15 11:00:18.634873: step 3890, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 60h:26m:38s remains)
INFO - root - 2017-12-15 11:00:25.244465: step 3900, loss = 0.23, batch loss = 0.18 (12.0 examples/sec; 0.667 sec/batch; 60h:53m:54s remains)
2017-12-15 11:00:25.691386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4624176 -1.5167694 -1.3386965 -1.0425224 -2.2541974 -2.9640605 -3.1573679 -3.7747684 -4.110929 -4.3357811 -3.9954009 -5.1470475 -6.0603642 -7.1670704 -5.6599503][-3.3534799 -3.3196321 -2.4134641 -2.7801795 -3.3553376 -4.9252 -5.7323666 -5.6420817 -5.6730437 -5.2199068 -4.9897103 -6.9726286 -7.1571159 -8.3658333 -7.66404][-3.2998881 -3.983216 -4.0124788 -4.1904078 -4.5090289 -4.8380919 -4.5976143 -4.9871564 -4.6876 -4.6644254 -4.8879628 -6.9389954 -8.5280542 -8.8579493 -7.6471128][-3.0123258 -2.3076372 -2.0097864 -2.0175915 -2.8034961 -2.8568127 -2.6965692 -2.9352181 -2.6057265 -1.6988821 -2.1063969 -5.0511026 -7.3682642 -9.0294952 -7.052835][-4.1844063 -3.5394607 -2.4683926 -1.6164465 -1.3389544 -0.57898951 -0.046640873 0.33933449 0.0950551 0.17082024 -0.044186592 -2.8581228 -5.3077989 -7.457583 -6.8147292][-5.6887903 -4.7661357 -3.1578283 -1.210506 0.032838821 1.6718521 2.3230152 1.698545 1.1723447 0.43160629 -1.0935373 -3.8860719 -5.4491978 -7.2480669 -5.9326286][-5.7579265 -5.6312122 -3.9039578 -1.007113 0.96467066 2.4870291 2.9459963 2.9080667 2.1308241 1.2955155 0.16903019 -3.3403118 -5.2477632 -7.0321136 -6.4186735][-5.3559885 -4.32886 -3.2225385 -0.32331848 1.1922936 2.7003441 3.206377 2.5504618 1.5931287 0.60963345 1.0122709 -1.9738591 -3.8410501 -5.732832 -6.0171037][-4.0379524 -3.323992 -2.0524285 -0.32790947 0.25517082 1.6722646 1.9592686 1.724052 1.0737319 0.90756512 1.3677502 -0.70004654 -2.380388 -5.13953 -4.9780235][-4.1022496 -4.0426207 -3.3089025 -1.3603415 -0.83505869 0.12815809 0.71008396 0.26358175 -0.36546993 -0.41189671 0.4197607 -0.78901863 -2.6848524 -4.2632108 -4.0108271][-4.6409168 -5.7107968 -6.0251884 -5.4544868 -5.3506751 -4.6957855 -4.0876842 -3.2170713 -2.2740443 -1.7718401 -1.2143936 -3.1584349 -2.7761571 -4.4474039 -4.2408285][-8.6534252 -9.016942 -8.70064 -8.4321318 -7.8878551 -6.9528408 -6.5908585 -6.4042711 -5.6375771 -5.8326178 -5.3218579 -5.9879541 -5.4023867 -5.4662709 -3.5278168][-10.759716 -10.618874 -10.830035 -10.316613 -10.13653 -8.8935318 -7.8932481 -7.642436 -6.7294836 -6.0411391 -6.1658483 -7.2999277 -6.597846 -7.1414843 -5.0133834][-8.4897232 -8.3648567 -8.6294518 -7.4125423 -6.895329 -6.2299204 -5.9094396 -5.5392179 -5.8131685 -5.5160179 -5.2031331 -4.6941462 -4.8857241 -5.7034006 -5.0249109][-6.2057319 -5.6310492 -5.2064581 -4.1166177 -4.0921803 -3.2268775 -3.0345912 -2.7653809 -2.846925 -2.9749291 -2.962286 -3.7897763 -3.6451492 -4.1889882 -4.3877511]]...]
INFO - root - 2017-12-15 11:00:32.344107: step 3910, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 59h:19m:48s remains)
INFO - root - 2017-12-15 11:00:38.956659: step 3920, loss = 0.23, batch loss = 0.18 (11.8 examples/sec; 0.678 sec/batch; 61h:50m:38s remains)
INFO - root - 2017-12-15 11:00:45.493400: step 3930, loss = 0.20, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 60h:02m:24s remains)
INFO - root - 2017-12-15 11:00:51.988291: step 3940, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.674 sec/batch; 61h:28m:18s remains)
INFO - root - 2017-12-15 11:00:58.580131: step 3950, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.675 sec/batch; 61h:33m:54s remains)
INFO - root - 2017-12-15 11:01:05.126850: step 3960, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 59h:33m:25s remains)
INFO - root - 2017-12-15 11:01:11.695223: step 3970, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 58h:47m:59s remains)
INFO - root - 2017-12-15 11:01:18.209866: step 3980, loss = 0.27, batch loss = 0.22 (12.0 examples/sec; 0.666 sec/batch; 60h:44m:29s remains)
INFO - root - 2017-12-15 11:01:24.866476: step 3990, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.681 sec/batch; 62h:05m:58s remains)
INFO - root - 2017-12-15 11:01:31.412721: step 4000, loss = 0.23, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 59h:28m:45s remains)
2017-12-15 11:01:31.881347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.82061 -3.6138513 -5.5502443 -5.9389739 -6.4991841 -6.8104968 -7.0895743 -8.3397036 -9.1667433 -8.7485523 -8.3135729 -8.111722 -6.07294 -6.5527821 -5.6168652][-3.417783 -4.6466 -5.8339815 -7.2238011 -8.1277676 -8.9786758 -9.4911013 -9.54303 -10.223387 -10.315405 -9.9257784 -9.4453735 -7.0096717 -8.0688019 -6.8181453][-3.3784151 -3.9779024 -6.3559122 -7.1732597 -8.54381 -9.1044407 -9.0456839 -9.4050722 -9.3432961 -9.5416107 -9.7384586 -10.034168 -8.375783 -9.0587149 -8.5564842][-5.6674833 -5.2396636 -5.4756517 -6.0204511 -8.6147623 -8.9734545 -8.4213133 -8.7010555 -9.6023188 -8.8381 -8.3063583 -9.3658562 -7.8773766 -9.1889391 -8.55708][-5.611846 -7.5658364 -9.6330767 -7.1032925 -7.5818839 -6.289628 -3.8174577 -6.209693 -8.6706419 -8.9286518 -9.4376545 -9.2918453 -8.2018166 -9.8867264 -9.47772][-7.3390307 -6.9097834 -8.1293058 -6.8669262 -4.5047889 -0.50747061 2.8932133 0.817471 -0.68503237 -4.276865 -8.5212469 -8.523119 -7.5330286 -9.8960075 -9.5754833][-9.86697 -7.4436388 -5.1588831 -2.2710776 -1.6386731 3.4066372 9.101553 9.5193682 10.160571 2.1953287 -5.5772028 -8.2870283 -8.9221087 -11.032406 -10.975361][-12.630186 -10.065067 -7.7188921 -2.7200882 0.048426151 5.4587374 10.48023 11.137163 12.585148 6.8584518 -0.59738731 -6.3853955 -9.2618074 -10.887508 -10.214087][-11.245921 -10.081014 -9.4220076 -5.4575925 -2.5670466 3.193984 7.3507624 8.8511095 10.35823 5.3596764 -0.4153347 -6.5560365 -10.737534 -12.906038 -11.057785][-9.3823891 -8.89721 -9.6771011 -6.9792166 -5.2703962 -1.848834 2.7302055 4.4943323 4.6698561 0.21333838 -4.9688148 -8.6240883 -11.168038 -14.185691 -14.50727][-12.071533 -10.938265 -10.629663 -10.352703 -9.8384848 -7.8960238 -5.648139 -4.0196886 -3.3375535 -5.2600236 -8.1004982 -11.228313 -13.444647 -15.525512 -14.975673][-14.745772 -13.748981 -11.859318 -11.753489 -11.570752 -11.360137 -11.802235 -11.067238 -10.003345 -11.248558 -12.888161 -13.792003 -13.690325 -14.331827 -14.09417][-15.268661 -13.871753 -13.36967 -12.406484 -11.419619 -11.51283 -12.007009 -12.848488 -13.368788 -13.175665 -12.916045 -13.171779 -13.796229 -13.597354 -11.920647][-11.228655 -12.466293 -12.936644 -10.903156 -9.1237087 -9.3318176 -9.9980221 -10.508207 -10.357786 -10.919986 -11.336334 -11.212904 -11.366959 -10.943932 -10.258425][-8.5276051 -7.3997235 -7.6638212 -6.9900746 -7.2313027 -7.8801546 -7.7461205 -8.1018744 -7.8545513 -8.0912523 -8.5735111 -9.4576759 -9.8482485 -9.5055122 -9.6215134]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 11:01:38.448979: step 4010, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 60h:23m:43s remains)
INFO - root - 2017-12-15 11:01:44.989521: step 4020, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.636 sec/batch; 58h:04m:07s remains)
INFO - root - 2017-12-15 11:01:51.526293: step 4030, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 60h:56m:22s remains)
INFO - root - 2017-12-15 11:01:58.153335: step 4040, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 59h:53m:25s remains)
INFO - root - 2017-12-15 11:02:04.704337: step 4050, loss = 0.18, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 59h:20m:21s remains)
INFO - root - 2017-12-15 11:02:11.341822: step 4060, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 60h:06m:27s remains)
INFO - root - 2017-12-15 11:02:17.987378: step 4070, loss = 0.26, batch loss = 0.21 (12.4 examples/sec; 0.646 sec/batch; 58h:57m:18s remains)
INFO - root - 2017-12-15 11:02:24.635775: step 4080, loss = 0.29, batch loss = 0.24 (12.1 examples/sec; 0.663 sec/batch; 60h:30m:38s remains)
INFO - root - 2017-12-15 11:02:31.189062: step 4090, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 58h:50m:23s remains)
INFO - root - 2017-12-15 11:02:37.782285: step 4100, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 59h:47m:49s remains)
2017-12-15 11:02:38.411754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2875228 -1.9130249 -2.738658 -3.0889969 -3.5413191 -4.0936193 -4.4690461 -4.6810322 -4.9323764 -4.5285783 -4.05073 -5.3530297 -6.1311355 -6.9732466 -6.3872242][-3.3173361 -3.9173737 -4.5061274 -4.8202677 -5.0802455 -5.97995 -6.2665472 -6.789609 -6.7124772 -5.8215442 -5.265409 -6.5827322 -7.49056 -8.2792654 -7.8475542][-4.3185143 -4.7442756 -5.1420097 -5.612103 -6.0974064 -6.5122504 -6.3160896 -6.2335958 -5.3358212 -5.1234756 -4.55126 -6.0847235 -7.7017775 -8.2937794 -8.316637][-5.8862519 -4.9436073 -4.6424341 -4.8320494 -5.4937348 -5.3966341 -5.0356326 -5.0668449 -4.4667559 -3.9888887 -2.9154744 -4.0524364 -5.5132713 -6.8370414 -6.9261761][-5.9649949 -5.7466555 -5.5395131 -4.5321026 -3.8396461 -3.0840435 -2.2276776 -1.9776473 -1.3430324 -1.5697365 -1.0350895 -2.9696388 -4.5731936 -6.149189 -6.8226509][-3.5150435 -3.4611285 -2.5177832 -1.3969479 -0.84901857 0.12576199 0.43141174 0.48411131 0.88029623 0.628922 0.84888315 -1.1381178 -2.3290963 -4.1519289 -4.4226017][-3.5676692 -3.0663836 -2.2214375 -0.626029 0.49511957 2.1780705 3.0901656 2.8131404 2.4157062 1.4459791 0.76551104 -1.1282277 -2.4400845 -3.4028203 -3.7847281][-4.0978031 -3.3677232 -2.0219011 -0.50495863 0.40418959 1.6745462 1.9734564 1.9546475 1.1358609 0.44761562 0.27118063 -1.6946185 -2.9139593 -3.2425849 -2.5765467][-6.6478891 -5.7645063 -4.3883109 -2.7556949 -1.5084815 0.0451231 1.0841308 1.5894818 1.2005424 0.31027174 -0.52570486 -2.5311468 -3.4814785 -3.9781723 -3.3232906][-8.0462322 -7.5988264 -6.0434566 -4.0494957 -3.3617027 -2.1854367 -1.6053426 -1.0798035 -0.29845 0.0058579445 -0.74325466 -3.3672192 -4.4596205 -5.0009623 -4.3630238][-11.39801 -11.583918 -8.6594448 -5.9578452 -4.8070364 -3.7057862 -3.4665868 -3.0721405 -2.449791 -2.6075759 -3.3921425 -4.3392935 -4.3306093 -4.6042638 -3.6050429][-10.883768 -11.139616 -10.189357 -7.6149874 -5.7787213 -4.2370071 -3.7082543 -3.7932618 -3.7399254 -3.8575993 -3.8941536 -3.5592453 -3.1772592 -3.1222568 -2.061471][-10.811706 -9.6902323 -7.5441685 -6.1373482 -4.6488113 -3.504107 -4.3823996 -4.7321372 -4.7650676 -4.1722693 -3.5760868 -3.0968041 -2.704097 -1.9105313 -0.66749525][-9.1195765 -8.6268263 -7.7265034 -5.8299122 -4.8109841 -3.7351367 -3.05114 -3.9981327 -4.5290489 -3.9594283 -3.4557979 -2.7689342 -2.6941342 -2.038599 -1.5991249][-8.4764538 -7.0621533 -5.1505084 -4.2149372 -3.6302226 -3.7099535 -4.260766 -4.3978539 -4.2503681 -4.2247348 -4.0402064 -4.151732 -4.1133356 -3.5310576 -3.6757665]]...]
INFO - root - 2017-12-15 11:02:44.985785: step 4110, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 61h:07m:51s remains)
INFO - root - 2017-12-15 11:02:51.550625: step 4120, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 61h:02m:13s remains)
INFO - root - 2017-12-15 11:02:58.160842: step 4130, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 61h:25m:09s remains)
INFO - root - 2017-12-15 11:03:04.755313: step 4140, loss = 0.21, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 59h:12m:35s remains)
INFO - root - 2017-12-15 11:03:11.350144: step 4150, loss = 0.23, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 59h:11m:31s remains)
INFO - root - 2017-12-15 11:03:17.931926: step 4160, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.683 sec/batch; 62h:19m:59s remains)
INFO - root - 2017-12-15 11:03:24.467067: step 4170, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 60h:16m:47s remains)
INFO - root - 2017-12-15 11:03:31.052078: step 4180, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 59h:13m:24s remains)
INFO - root - 2017-12-15 11:03:37.556344: step 4190, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 59h:50m:13s remains)
INFO - root - 2017-12-15 11:03:44.120270: step 4200, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.673 sec/batch; 61h:19m:50s remains)
2017-12-15 11:03:44.608856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.09256649 -1.109344 -1.7160859 -1.1745644 -2.104208 -3.6558828 -4.3279986 -3.1190658 -1.8025234 -0.4110446 0.95247984 1.3171563 -0.66631222 -2.4102652 -4.7338018][0.30618 -0.31891966 -0.43905544 0.064732552 -1.3188777 -2.8407152 -4.5390339 -4.9965911 -3.9748869 -1.849499 -0.32451153 -0.082923412 -2.5252876 -4.2193365 -5.5088582][-2.2164738 -1.9309912 -1.0550575 -0.78437471 -1.5610642 -2.8469214 -3.9440017 -3.5608451 -2.3858011 -1.1980691 -0.5180068 -1.2454548 -4.332221 -5.8227 -7.8245649][-4.2247248 -3.5768721 -2.4603314 -1.646697 -1.978807 -2.571449 -2.9445448 -2.9142168 -2.3175085 -1.0661669 -0.29163742 -1.3772531 -5.0346365 -7.4521642 -8.9976206][-4.7173538 -4.5201397 -3.75029 -1.756124 -0.87599516 -0.86365509 -0.85995674 -1.4705939 -1.3796315 -0.76152134 -0.81243229 -1.8343501 -5.4596381 -7.4302764 -8.9851265][-4.4477539 -4.4554582 -2.8114095 -1.0626493 -0.20226765 1.3589916 1.7745228 1.1751413 1.0594864 0.50161362 -0.49455357 -1.0609345 -3.9789462 -6.9958873 -9.4802341][-3.7187538 -4.2091165 -3.0914273 -0.48285151 1.8345666 3.2681794 3.4940991 2.994329 2.6129127 1.295033 0.24593258 -0.8601265 -3.6445751 -5.2517123 -6.570539][-2.6264558 -2.8530662 -1.613265 0.40684414 2.2506742 4.1462588 4.2672982 3.6679969 3.1673036 2.2176948 1.2394037 0.90030146 -1.6953099 -3.9553938 -6.0363607][-1.2843618 -1.5224166 -0.963222 1.0697784 2.7128453 3.20676 3.204433 2.7849021 2.2189689 1.2281122 0.74244547 0.40761042 -2.1848195 -3.2717366 -4.2384505][-1.2941437 -1.6668973 -1.292213 -0.00844717 1.0965815 1.0568242 0.62995338 0.21202374 -0.43187237 -1.2271581 -1.8874922 -1.9639976 -3.9807415 -4.5335655 -4.7154088][-5.0708585 -5.2664213 -5.3671837 -4.9945278 -4.3803177 -3.8588946 -3.5136707 -4.0954142 -5.0347128 -5.038085 -4.7686262 -5.3949809 -6.2776961 -6.0449247 -5.4991679][-9.1601505 -9.1155691 -8.4835138 -8.1373863 -7.4860511 -7.4047756 -7.930212 -8.2484512 -8.0840092 -7.5787234 -7.1865072 -7.0062137 -7.1879015 -6.7583294 -5.6410618][-10.062378 -9.6929674 -9.2985916 -9.1933212 -9.2840147 -8.8743563 -8.5837069 -8.8492737 -9.0937958 -7.9096069 -6.6140451 -6.8996534 -7.2827144 -7.2784023 -6.5524387][-9.3251591 -8.2867785 -6.9408474 -6.8679819 -6.8286147 -7.3182297 -8.17061 -7.9151983 -7.6507797 -7.3458748 -6.9347973 -5.9264545 -5.5397353 -6.2772841 -6.2695723][-6.954062 -6.1475129 -4.6836753 -3.7974267 -3.7461038 -3.8793716 -4.6091824 -5.735599 -6.2695251 -5.5129423 -5.4916039 -5.9238605 -6.9543743 -7.4368505 -6.8299212]]...]
INFO - root - 2017-12-15 11:03:51.198290: step 4210, loss = 0.19, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 57h:25m:55s remains)
INFO - root - 2017-12-15 11:03:57.751504: step 4220, loss = 0.21, batch loss = 0.16 (11.7 examples/sec; 0.681 sec/batch; 62h:05m:35s remains)
INFO - root - 2017-12-15 11:04:04.280718: step 4230, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 60h:00m:55s remains)
INFO - root - 2017-12-15 11:04:10.801579: step 4240, loss = 0.22, batch loss = 0.17 (12.7 examples/sec; 0.630 sec/batch; 57h:27m:28s remains)
INFO - root - 2017-12-15 11:04:17.284768: step 4250, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 59h:46m:48s remains)
INFO - root - 2017-12-15 11:04:23.795975: step 4260, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 59h:13m:23s remains)
INFO - root - 2017-12-15 11:04:30.341318: step 4270, loss = 0.24, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 58h:27m:54s remains)
INFO - root - 2017-12-15 11:04:36.918590: step 4280, loss = 0.23, batch loss = 0.18 (11.7 examples/sec; 0.684 sec/batch; 62h:23m:53s remains)
INFO - root - 2017-12-15 11:04:43.551270: step 4290, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 59h:16m:41s remains)
INFO - root - 2017-12-15 11:04:50.067185: step 4300, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 58h:09m:01s remains)
2017-12-15 11:04:50.576639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0644498 -1.279851 -1.4209423 -1.157712 -1.7512705 -2.1117392 -2.2579548 -2.0538743 -1.5529261 -1.16223 -0.78569651 -2.712079 -3.6934977 -4.178586 -3.94279][-1.5853858 -1.052382 -1.2151337 -1.8147099 -2.2131023 -2.3601372 -2.4165356 -1.7930961 -0.90509462 -0.14988232 0.34198236 -1.5459919 -2.6086864 -3.5610931 -3.7174268][-1.4471722 -1.5885406 -2.1849103 -1.8627744 -2.0431314 -2.0213592 -1.8033967 -1.6427541 -1.1326542 -0.40899611 0.21496439 -1.576406 -2.3679397 -3.369477 -3.6753135][-3.060143 -2.5996456 -2.4315624 -1.9973896 -1.7988093 -1.76266 -1.6544545 -1.5237713 -1.3664856 -0.86153269 -0.23882151 -2.311331 -3.1063581 -3.7108481 -3.4669685][-4.1985726 -4.071105 -3.960156 -2.7531815 -1.8487153 -0.996675 -0.38672447 -0.68727779 -1.0263109 -1.0440888 -1.087008 -2.8406408 -3.3521731 -4.2212591 -4.1235132][-4.9580069 -4.5465345 -4.2149472 -2.485039 -1.0978012 0.25126743 1.2994108 0.9180007 0.38594484 -0.09411335 -0.64375877 -2.6309161 -3.54121 -4.5460644 -4.6482224][-5.4411311 -4.5124979 -3.873265 -2.4854853 -1.2218719 0.77637482 2.4081941 2.4637256 2.1307058 0.80404234 -0.32231092 -2.7954078 -4.2909803 -5.4827671 -5.6950712][-5.8401628 -4.6573753 -3.7831788 -2.3517253 -1.0448937 0.88086462 2.4373355 2.4799657 2.1342559 1.1472631 0.04660511 -3.1933579 -5.2199922 -6.787612 -6.934248][-5.7253976 -4.750473 -3.7954326 -2.4568982 -1.1417613 0.47319269 1.699687 1.7787781 1.6238027 0.75892925 -0.42682076 -3.6517737 -5.7170706 -7.485569 -7.8649416][-5.2533755 -4.392653 -4.172019 -2.9385653 -1.9627728 -0.56899261 0.42386675 0.73031139 0.68320179 -0.44230032 -1.7361951 -4.9133973 -6.966764 -8.6228971 -8.9421635][-7.4686122 -6.87275 -6.4450269 -5.499094 -5.0719318 -4.2601833 -3.5890322 -3.3805919 -3.3372071 -3.7761784 -4.5335083 -6.9223824 -8.37254 -9.3166656 -9.2297735][-10.239966 -9.2774525 -8.3800325 -7.5061245 -7.1711855 -7.1961145 -7.38622 -7.0820704 -6.7161193 -7.066637 -7.71297 -8.8968315 -9.2711048 -9.6666651 -9.2533607][-10.439324 -9.545742 -8.8477716 -8.6271582 -8.4193172 -7.962636 -8.0901661 -8.5754967 -8.839304 -8.4735937 -8.7072315 -9.0741415 -9.1876554 -9.1572695 -8.3763885][-9.09436 -8.5033 -8.2346382 -7.8262758 -7.214541 -7.6702633 -8.2758427 -8.37171 -8.6069946 -8.99182 -9.157403 -8.4946375 -8.1472425 -8.3282309 -7.8364458][-8.5031166 -7.7884083 -7.2109294 -6.6619635 -6.1883888 -6.6744218 -6.8412886 -7.2742243 -7.5682936 -7.4937258 -7.5523582 -7.896543 -8.0423021 -8.0979443 -7.8806138]]...]
INFO - root - 2017-12-15 11:04:57.109860: step 4310, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 60h:01m:15s remains)
INFO - root - 2017-12-15 11:05:03.708025: step 4320, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 60h:59m:32s remains)
INFO - root - 2017-12-15 11:05:10.401752: step 4330, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 60h:04m:09s remains)
INFO - root - 2017-12-15 11:05:16.905065: step 4340, loss = 0.21, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 58h:18m:28s remains)
INFO - root - 2017-12-15 11:05:23.569877: step 4350, loss = 0.18, batch loss = 0.13 (11.3 examples/sec; 0.708 sec/batch; 64h:31m:18s remains)
INFO - root - 2017-12-15 11:05:30.186996: step 4360, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.664 sec/batch; 60h:29m:22s remains)
INFO - root - 2017-12-15 11:05:36.720171: step 4370, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 59h:51m:45s remains)
INFO - root - 2017-12-15 11:05:43.287097: step 4380, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 60h:02m:10s remains)
INFO - root - 2017-12-15 11:05:49.867046: step 4390, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 58h:49m:01s remains)
INFO - root - 2017-12-15 11:05:56.456774: step 4400, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 58h:08m:25s remains)
2017-12-15 11:05:56.947482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6111097 -6.055438 -5.09275 -4.3061838 -4.7770448 -5.8836608 -6.6960192 -7.2838407 -6.6677294 -5.3326778 -4.9336958 -7.1548433 -9.6351566 -9.7413712 -7.9625525][-7.4205761 -6.0495968 -4.6901913 -3.8134127 -4.3411627 -5.439105 -6.4723706 -7.0836477 -6.6046796 -6.2633677 -6.3497043 -8.31299 -10.087099 -9.8463488 -8.08734][-6.9963512 -5.8039994 -5.40445 -3.8267603 -4.7388549 -5.83519 -6.6744609 -6.8199172 -5.9122429 -5.0778461 -4.9960041 -7.5081358 -10.447349 -11.398386 -10.160655][-8.03572 -6.9801331 -6.1614203 -4.7740088 -5.0257435 -5.1295967 -5.09741 -4.3943648 -3.4380021 -3.1428761 -4.1291838 -7.3945236 -10.343914 -10.581494 -9.6246376][-6.7758861 -6.5987535 -5.6185889 -4.6784153 -4.3540535 -3.4384854 -2.8880436 -2.2418656 -1.0784545 -0.16137552 -1.3000364 -5.0598011 -9.2339306 -10.501303 -9.467659][-6.3513813 -6.26636 -5.6212454 -4.4720788 -3.3204758 -1.3432517 0.33953428 1.2704096 1.8641758 2.0920153 0.99331284 -2.9419873 -8.0267324 -9.4381018 -8.6998816][-7.4125404 -6.8711939 -5.9640636 -4.2356238 -2.4192188 0.34253168 3.3855782 4.8566637 5.3076849 4.2140522 2.1292572 -2.1868234 -7.083005 -8.632966 -8.9337893][-8.6764364 -8.4968405 -7.3946409 -4.2977366 -1.3933444 1.4776225 3.5250225 5.1168146 6.1485157 5.92476 3.914464 -1.3155336 -6.3711205 -8.0399265 -8.3053274][-9.2762127 -9.008954 -7.9766912 -5.02374 -2.0396028 1.5036664 4.2998219 5.1622944 5.0432615 4.3153591 2.9873886 -2.0412757 -7.1320806 -8.84272 -8.3333492][-9.3581963 -9.4814329 -7.9209952 -4.5114975 -2.2254097 0.52698421 3.2851696 3.498107 3.1429396 3.1283398 2.4354267 -1.8528042 -6.3600907 -8.6197739 -9.2818031][-12.214722 -11.48735 -9.8530569 -7.0120358 -5.4333034 -2.9217184 -1.2797856 -1.0745211 -1.0301003 -1.8637233 -3.2083883 -5.7002039 -8.8764629 -10.123998 -9.4533424][-12.471179 -11.300205 -10.271425 -8.2600021 -6.44367 -5.1086874 -5.2129207 -4.9673295 -5.1818004 -5.1224632 -5.5769339 -6.7260723 -8.739994 -9.691576 -9.2430992][-11.885019 -10.146874 -8.2282553 -6.9228683 -6.4140968 -5.9327703 -5.9245687 -5.8428621 -6.3707619 -6.2284064 -6.3925357 -6.8881855 -8.166564 -8.4234238 -7.8653984][-10.940992 -10.068521 -9.077219 -6.467875 -5.722198 -5.3872232 -5.4113307 -6.478838 -6.6462288 -6.9514089 -6.9892569 -7.3156304 -7.6996393 -7.6796007 -6.94481][-8.1597557 -8.5322933 -8.6775036 -7.227191 -6.4315867 -5.5970621 -6.1781793 -6.10806 -6.4130068 -6.85361 -7.1423435 -8.1652393 -8.1053925 -8.1469049 -7.5028667]]...]
INFO - root - 2017-12-15 11:06:03.542340: step 4410, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 61h:30m:59s remains)
INFO - root - 2017-12-15 11:06:10.105517: step 4420, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 59h:35m:06s remains)
INFO - root - 2017-12-15 11:06:16.792301: step 4430, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 61h:41m:21s remains)
INFO - root - 2017-12-15 11:06:23.354434: step 4440, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 60h:13m:16s remains)
INFO - root - 2017-12-15 11:06:29.998962: step 4450, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 60h:21m:21s remains)
INFO - root - 2017-12-15 11:06:36.688042: step 4460, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 60h:33m:01s remains)
INFO - root - 2017-12-15 11:06:43.295112: step 4470, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 59h:35m:45s remains)
INFO - root - 2017-12-15 11:06:49.857094: step 4480, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.648 sec/batch; 59h:00m:57s remains)
INFO - root - 2017-12-15 11:06:56.516113: step 4490, loss = 0.25, batch loss = 0.20 (11.7 examples/sec; 0.684 sec/batch; 62h:18m:01s remains)
INFO - root - 2017-12-15 11:07:03.010760: step 4500, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 58h:50m:49s remains)
2017-12-15 11:07:03.522328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6492319 -6.3594809 -6.4052811 -5.6618261 -4.5580969 -4.5862732 -4.6211438 -4.4784307 -4.2672677 -4.0425463 -3.6140802 -4.9590721 -6.9209437 -7.9339161 -7.3058772][-5.41363 -5.0649366 -4.909843 -4.5106316 -4.5131607 -4.7846737 -4.5533333 -4.7025414 -4.6190071 -4.13423 -3.7200587 -5.1538644 -6.9538393 -8.3918629 -7.5518932][-4.4394221 -4.1741667 -4.3453832 -4.0835543 -4.3718567 -4.6469374 -4.8458076 -4.407567 -3.9508283 -4.086122 -4.2141018 -5.5476985 -7.6264544 -9.1112843 -8.2056856][-4.159781 -3.7214913 -4.3275175 -4.5158176 -4.8076363 -4.0527983 -3.6337857 -3.6004081 -3.2831151 -3.2549136 -3.0342591 -4.910006 -7.6937923 -9.1354752 -8.3417263][-3.0253568 -3.3581357 -4.790009 -5.1166086 -5.1603909 -3.0591011 -1.3058801 -1.4248819 -1.7710564 -1.6391041 -1.7482138 -3.6634028 -6.2729797 -8.3387671 -7.7581606][-2.7543983 -2.6523442 -3.5747337 -3.5939565 -2.8453109 -0.70729637 0.86502552 1.9694047 1.8099713 0.22972536 -1.2222328 -3.065721 -5.3949332 -7.4585934 -7.0010233][-2.1154985 -1.7908537 -2.147274 -1.5317302 -0.27736187 2.197824 4.3654757 5.533783 4.7904005 2.3814564 0.508101 -2.2776012 -5.4585962 -7.1619577 -6.6345959][-1.4503746 -1.2171669 -0.73874712 0.10425282 0.89983368 3.380578 5.1139011 5.5670719 4.7555704 2.8256102 1.018959 -2.1336217 -5.1801734 -6.7944889 -6.3095708][-1.6427445 -1.1052337 -0.55309772 0.45568752 1.7145901 2.8481817 2.7249098 2.4935083 2.3323102 1.5716877 0.17218828 -2.2981248 -4.3020349 -6.0127592 -5.0911636][-2.3076284 -1.9687102 -1.7400305 -0.051582813 1.8372235 1.8965087 1.325614 1.3338447 0.77521944 0.55221176 0.19646931 -2.1594014 -4.3334484 -5.7498221 -5.2737026][-6.418839 -5.5955429 -4.3214836 -2.2127314 -1.161479 -0.037026405 0.7037034 -0.050506592 -0.77940035 -1.0413432 -1.3966541 -2.9120483 -4.1349697 -5.30956 -5.1503644][-9.0692959 -9.0199957 -7.6377549 -5.232688 -4.3536453 -2.7120101 -1.827466 -2.3264613 -2.4694047 -2.2643523 -2.6764388 -3.5293617 -4.615972 -6.238246 -6.4627514][-9.4160738 -8.7673244 -7.103157 -5.5812349 -5.3331561 -5.0032315 -5.2995019 -5.5418463 -5.2485695 -4.9710846 -4.265851 -4.8683529 -5.6098447 -5.8034282 -5.1225123][-7.037303 -6.9366412 -6.1345711 -4.6718059 -4.0813837 -5.0658092 -6.5304985 -6.8940783 -7.60273 -7.4858971 -6.5691133 -6.322998 -6.585885 -5.65741 -5.0882559][-4.6619396 -4.4931164 -4.1040564 -3.741672 -4.0387974 -4.3394337 -4.84192 -6.618608 -7.9616346 -8.0350857 -8.2485332 -8.2086582 -7.8470449 -7.0183725 -6.7908235]]...]
INFO - root - 2017-12-15 11:07:10.062609: step 4510, loss = 0.26, batch loss = 0.21 (12.1 examples/sec; 0.662 sec/batch; 60h:20m:19s remains)
INFO - root - 2017-12-15 11:07:16.588859: step 4520, loss = 0.22, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 60h:41m:26s remains)
INFO - root - 2017-12-15 11:07:23.198758: step 4530, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.669 sec/batch; 60h:55m:49s remains)
INFO - root - 2017-12-15 11:07:29.844713: step 4540, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 61h:42m:06s remains)
INFO - root - 2017-12-15 11:07:36.433290: step 4550, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.682 sec/batch; 62h:08m:16s remains)
INFO - root - 2017-12-15 11:07:43.024190: step 4560, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 58h:43m:22s remains)
INFO - root - 2017-12-15 11:07:49.638554: step 4570, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 60h:20m:02s remains)
INFO - root - 2017-12-15 11:07:56.236557: step 4580, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 59h:23m:43s remains)
INFO - root - 2017-12-15 11:08:02.792129: step 4590, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 60h:04m:50s remains)
INFO - root - 2017-12-15 11:08:09.374728: step 4600, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 59h:50m:09s remains)
2017-12-15 11:08:09.877993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9967752 -7.6296177 -6.9725075 -6.4828811 -6.0787106 -5.7091026 -5.4610052 -4.9235148 -4.6097255 -4.9250603 -5.159565 -6.3848786 -6.743444 -6.3321967 -5.6220965][-8.42418 -8.9305611 -9.064229 -7.8432932 -6.9693384 -6.7417068 -6.7702837 -6.7071395 -6.8914084 -6.614027 -6.2039537 -7.6922321 -8.0978241 -7.0656433 -6.452517][-8.1738558 -8.4143009 -8.6530113 -7.6195073 -7.07305 -7.2193623 -7.1170449 -7.3839631 -7.7544231 -7.8752809 -7.491787 -8.47134 -7.9333878 -7.3097491 -6.6461587][-7.1792274 -7.4421124 -7.0766797 -6.4748435 -6.4279451 -6.1900496 -5.7382336 -6.7349181 -7.5376472 -7.6023288 -7.1637616 -8.8087826 -8.6259785 -7.3989563 -6.2718582][-7.6112485 -7.587677 -7.1184311 -5.6637692 -4.8721132 -3.784235 -3.1255417 -4.5967088 -5.6818328 -6.0412245 -6.3863497 -7.6028695 -7.0548697 -6.8518252 -6.3734651][-8.4288483 -7.8278389 -7.0944905 -4.9429226 -3.299777 -1.1482458 0.36657 -0.88037872 -2.3075886 -3.2597094 -3.809545 -5.5827608 -5.2664094 -5.0311537 -5.0364695][-8.8185358 -8.0117912 -6.5038795 -3.5583975 -2.2522707 -0.47737455 0.84602308 0.79394007 0.05606699 -1.4210329 -2.2296278 -3.6739373 -3.1532762 -2.7275081 -2.93971][-7.6759439 -6.6978607 -5.7222018 -2.4328108 -0.11468697 1.5896153 2.3771925 1.5817165 1.3398361 0.49257183 -0.72885323 -2.5387652 -2.7369392 -2.074985 -1.780879][-6.238018 -5.3426733 -4.7439361 -2.687953 -0.84294653 0.69272757 2.1782527 2.519145 2.1400747 1.0244074 0.20768404 -1.5256443 -2.2681608 -1.4408336 -1.3485694][-5.2582827 -4.8063807 -5.0531912 -3.4880955 -2.8000596 -1.2174096 0.15941048 0.33701944 0.2696166 0.068697929 -0.58554792 -2.6230068 -3.0677421 -2.847997 -3.2526898][-6.2482333 -5.3474121 -5.2231054 -4.5371304 -4.2234306 -3.5506995 -2.8506095 -1.8444602 -1.5689363 -1.6400619 -2.5173891 -5.6954279 -6.8043194 -6.2599273 -5.9798875][-9.0766172 -8.548893 -8.321764 -8.3382788 -8.1949 -7.8054824 -7.1354465 -6.6774111 -6.6243691 -6.8922167 -7.8805461 -9.4746952 -9.9278765 -9.563797 -8.88468][-9.9121513 -8.9551172 -8.0195923 -8.1500826 -7.8814487 -6.7495017 -6.1634612 -7.0420866 -8.1472082 -8.72814 -9.89526 -10.963498 -11.140505 -9.2285891 -7.5888858][-9.5512524 -8.0982933 -6.748105 -6.492878 -6.7165642 -6.1261086 -5.5418668 -5.8398619 -5.8201585 -6.9286938 -8.3834858 -8.0023022 -7.6920986 -7.0633698 -5.8894095][-7.2691126 -6.0533743 -4.4089394 -3.852541 -3.6462898 -3.5222242 -3.7248104 -4.0097289 -4.333559 -4.2372 -4.5501041 -5.24037 -4.7183256 -4.5346727 -4.0200543]]...]
INFO - root - 2017-12-15 11:08:16.511465: step 4610, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 58h:23m:26s remains)
INFO - root - 2017-12-15 11:08:23.107262: step 4620, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 60h:22m:00s remains)
INFO - root - 2017-12-15 11:08:29.627741: step 4630, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 60h:38m:04s remains)
INFO - root - 2017-12-15 11:08:36.257368: step 4640, loss = 0.15, batch loss = 0.11 (11.4 examples/sec; 0.704 sec/batch; 64h:05m:46s remains)
INFO - root - 2017-12-15 11:08:42.830662: step 4650, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.662 sec/batch; 60h:17m:11s remains)
INFO - root - 2017-12-15 11:08:49.374255: step 4660, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 59h:11m:03s remains)
INFO - root - 2017-12-15 11:08:55.986275: step 4670, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 60h:39m:02s remains)
INFO - root - 2017-12-15 11:09:02.532878: step 4680, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 60h:06m:42s remains)
INFO - root - 2017-12-15 11:09:09.159228: step 4690, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 61h:47m:09s remains)
INFO - root - 2017-12-15 11:09:15.739803: step 4700, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.675 sec/batch; 61h:29m:15s remains)
2017-12-15 11:09:16.248285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.61587 -4.0539742 -4.1993046 -4.7466564 -5.126945 -5.7057009 -6.0418215 -6.0381327 -5.5805411 -5.2109485 -4.5955296 -5.3507271 -6.313508 -6.8066049 -6.4805408][-1.7972438 -2.9568415 -3.5543423 -3.572099 -3.3966308 -4.0643845 -4.5030117 -4.7047639 -4.7055197 -4.67742 -4.1468363 -4.8028369 -6.3636074 -7.8543072 -6.9044204][-1.1553621 -1.4401259 -2.0654571 -2.5176451 -2.6422026 -2.9233689 -2.9048321 -2.9696321 -3.6014571 -2.8658652 -2.8652062 -4.0730233 -5.3483753 -7.1089725 -7.9407878][-1.3866091 -1.3507266 -1.4078612 -1.2194877 -0.85505533 -1.1490922 -1.3171482 -1.1523442 -1.2274981 -1.2194963 -1.4555693 -2.8375189 -4.5801458 -6.28246 -6.63366][-2.7241192 -2.6340148 -2.3269444 -1.5243282 -0.88822794 -0.63963413 -0.081040382 -0.10035133 -0.5406065 -0.49895287 -0.71680832 -1.941278 -3.2526498 -5.2412696 -5.6114087][-3.7487173 -3.1392667 -2.6435452 -1.3588853 0.12539577 1.0227084 1.6984224 1.6404963 1.3373132 0.78957939 0.52452993 -0.47941256 -1.4426808 -3.129519 -3.8517146][-3.392904 -2.8865438 -1.8088799 -0.094765186 0.61821175 1.7882667 2.6770172 2.6412182 2.2307134 1.9827056 0.9318924 0.21450424 -0.65846062 -2.7219899 -2.9341824][-3.19663 -2.1831226 -0.83768129 0.57234955 1.3504977 2.0954566 2.3547273 2.3335357 2.1428418 1.777977 1.2887201 0.071895123 -2.1538374 -3.4466054 -3.9169674][-3.4017282 -2.1312506 -0.55914354 0.91584396 1.409059 1.522285 1.3903279 1.5923424 2.3112249 1.9234972 1.9057622 0.071107864 -3.2323258 -6.0743594 -7.1966715][-4.2181578 -2.6230738 -0.90361071 0.27939606 0.3783989 0.12112999 -0.40793467 0.065389156 0.4475708 1.269279 1.3621902 -0.3100729 -2.5826426 -6.0717545 -8.43735][-5.8378968 -5.586452 -4.4469433 -3.021311 -3.2426996 -3.8029022 -4.2766609 -3.6402609 -3.0705891 -1.9396012 -1.3549995 -2.3657174 -4.5922651 -7.0418768 -8.7545757][-7.7929239 -7.6951318 -7.1330137 -7.035284 -7.5615592 -7.7473764 -7.4065709 -6.8928237 -6.0260944 -4.8115935 -3.4634173 -4.2114739 -5.2576923 -6.6900043 -7.5790544][-9.0748863 -9.015336 -8.4747868 -8.1517763 -8.5885534 -8.9135189 -9.2458458 -8.3365145 -7.4265804 -6.983077 -6.5550694 -6.561904 -6.8380194 -7.8924618 -7.2023597][-9.7667618 -10.227325 -9.0685568 -8.1822643 -8.5901623 -8.7989988 -8.4580574 -8.3460751 -8.0222015 -6.8434811 -6.4646511 -6.8666968 -7.0030112 -7.7156968 -7.7759981][-8.4074783 -9.1525583 -8.7448807 -7.8999958 -6.7290268 -6.4277005 -7.0254951 -7.0065746 -7.7471118 -7.9732761 -7.7509527 -7.3412123 -7.3784971 -8.1888609 -8.4393425]]...]
INFO - root - 2017-12-15 11:09:22.787925: step 4710, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 60h:03m:10s remains)
INFO - root - 2017-12-15 11:09:29.418051: step 4720, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 60h:18m:08s remains)
INFO - root - 2017-12-15 11:09:36.049539: step 4730, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 59h:56m:49s remains)
INFO - root - 2017-12-15 11:09:42.587816: step 4740, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 59h:22m:31s remains)
INFO - root - 2017-12-15 11:09:49.150066: step 4750, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 60h:06m:15s remains)
INFO - root - 2017-12-15 11:09:55.694289: step 4760, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 60h:04m:18s remains)
INFO - root - 2017-12-15 11:10:02.220560: step 4770, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 58h:44m:05s remains)
INFO - root - 2017-12-15 11:10:08.775382: step 4780, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 60h:35m:19s remains)
INFO - root - 2017-12-15 11:10:15.415778: step 4790, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 60h:15m:48s remains)
INFO - root - 2017-12-15 11:10:21.973058: step 4800, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 59h:23m:27s remains)
2017-12-15 11:10:22.503495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.677619 -7.0670052 -6.7518225 -6.2446051 -6.1613131 -6.1158805 -5.9230824 -5.7006965 -5.6015191 -4.9519391 -4.0690794 -4.3723469 -4.8507643 -5.509481 -6.359458][-7.8146811 -6.5965943 -6.0084486 -4.9847755 -5.5965719 -5.5254955 -4.8697252 -5.0837827 -5.5712895 -5.0083842 -3.7771144 -3.5511141 -4.5045156 -5.8335648 -7.3150244][-6.1372108 -5.6130848 -4.7483177 -4.0966439 -4.5192008 -4.6072512 -4.1206293 -3.75382 -3.6832829 -3.6168959 -3.2190158 -3.7382922 -4.3888822 -6.5220575 -8.3688278][-6.2777238 -5.5947518 -4.4499178 -3.5178595 -3.4944179 -3.1973505 -2.7696211 -2.2899394 -2.0351651 -1.8567188 -1.524827 -2.045222 -3.1683517 -5.2173166 -6.9556079][-8.4140186 -7.1423159 -5.5462084 -3.3498662 -1.8029003 -0.13465691 0.71440697 -0.203197 -0.39588261 0.14869976 0.13217449 -1.0223613 -2.2968912 -4.0577369 -5.406034][-8.53388 -7.3191171 -5.0394139 -1.8372502 0.684155 3.0768323 4.98529 3.8714719 2.8225684 2.022912 1.5593567 0.34632254 -1.2963948 -3.0440431 -4.0598683][-7.9186373 -6.24981 -3.790699 -1.1320138 0.33815813 4.22164 7.575069 7.3890615 5.5675921 2.5168471 1.0518441 -0.80964375 -3.0143402 -4.40064 -4.8724213][-8.308115 -5.9850469 -2.6323066 0.70194721 1.5964851 3.8460088 6.2390113 7.0286689 7.2551894 4.7411704 2.0226388 -1.2937722 -3.9412122 -5.6746984 -7.0873656][-6.1092138 -4.7064109 -2.071228 0.88762188 1.9787045 3.0039248 3.2464919 4.0118213 4.6855206 3.912745 2.5933309 -1.1720929 -4.568985 -7.4895592 -9.3296652][-3.9812419 -3.1205232 -1.4219122 0.74583817 1.9252687 3.6455874 4.0145583 2.8995128 1.2453647 1.0978575 1.2773604 -1.4029684 -3.8605576 -7.1699624 -9.9991789][-7.7363811 -6.3512616 -5.0540957 -3.5836544 -2.9440906 -2.3815413 -1.1661644 -1.4566765 -2.9253423 -4.1041431 -4.5734072 -5.52812 -6.1025872 -7.6491137 -8.9140739][-11.332058 -10.392056 -9.2048664 -7.4835954 -6.9596753 -7.0996623 -6.5303426 -6.2829094 -6.4106531 -7.0969763 -7.7440796 -7.1565318 -7.0930467 -8.0771694 -8.8694782][-12.729733 -12.221454 -10.683002 -8.8234453 -8.0631828 -7.6567359 -7.8387146 -8.10758 -8.3686361 -8.1298227 -7.4063144 -6.18987 -5.7804112 -5.6412086 -6.1358285][-10.943367 -10.272829 -9.633976 -8.4120846 -7.9798903 -6.5473766 -5.9951963 -5.8380356 -5.8218107 -6.1733332 -5.9497828 -4.6810288 -4.40644 -4.1520047 -4.225842][-7.8473015 -7.6120143 -6.9027948 -5.5511036 -5.8486905 -5.1585312 -5.0645981 -5.22623 -5.210618 -5.0245013 -4.8688803 -4.0593615 -4.5104384 -4.1201196 -4.64771]]...]
INFO - root - 2017-12-15 11:10:29.075365: step 4810, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 60h:09m:04s remains)
INFO - root - 2017-12-15 11:10:35.674918: step 4820, loss = 0.27, batch loss = 0.22 (11.7 examples/sec; 0.683 sec/batch; 62h:11m:15s remains)
INFO - root - 2017-12-15 11:10:42.211084: step 4830, loss = 0.20, batch loss = 0.15 (12.7 examples/sec; 0.631 sec/batch; 57h:25m:23s remains)
INFO - root - 2017-12-15 11:10:48.806719: step 4840, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 59h:29m:12s remains)
INFO - root - 2017-12-15 11:10:55.410794: step 4850, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 60h:52m:30s remains)
INFO - root - 2017-12-15 11:11:02.063401: step 4860, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 60h:57m:23s remains)
INFO - root - 2017-12-15 11:11:08.739274: step 4870, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 61h:47m:23s remains)
INFO - root - 2017-12-15 11:11:15.296485: step 4880, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 60h:57m:33s remains)
INFO - root - 2017-12-15 11:11:21.893760: step 4890, loss = 0.22, batch loss = 0.17 (11.8 examples/sec; 0.675 sec/batch; 61h:26m:30s remains)
INFO - root - 2017-12-15 11:11:28.476789: step 4900, loss = 0.25, batch loss = 0.21 (11.6 examples/sec; 0.690 sec/batch; 62h:49m:19s remains)
2017-12-15 11:11:29.014680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.052938 -6.8232532 -7.8631668 -8.2384892 -9.4924879 -10.238699 -11.02914 -11.139427 -11.670321 -11.959297 -11.28947 -10.724268 -11.221519 -9.3489733 -8.8505535][-8.4140291 -9.1072273 -9.58833 -9.8070259 -10.171206 -10.828817 -12.218885 -13.55636 -14.601157 -13.666371 -12.399679 -12.018059 -12.622391 -11.824708 -10.571166][-6.8341131 -9.01146 -10.562892 -10.844277 -11.421457 -11.544175 -12.51906 -14.073687 -14.763191 -14.140923 -13.129611 -11.619349 -12.217796 -11.133408 -10.438664][-8.3068151 -9.6823912 -10.934998 -11.583255 -11.887796 -10.762662 -10.937493 -12.491083 -13.015522 -12.19936 -10.788424 -10.557721 -12.210712 -10.052391 -8.5930271][-9.0897179 -10.279306 -11.217852 -11.264247 -10.674095 -7.2958055 -6.1891608 -8.3641586 -9.5221987 -9.646615 -9.2115088 -9.41827 -10.501356 -9.9514694 -9.4365063][-9.8230457 -10.152046 -10.164774 -10.099849 -7.5498905 -2.7547736 0.52419853 1.2395453 -0.39616776 -3.3384204 -5.4837914 -5.6439695 -7.54862 -8.3816624 -8.6914492][-11.018282 -10.509477 -9.5542841 -8.2842588 -5.3479214 -1.6701787 3.2015572 7.5340919 7.9662542 2.8812828 -2.5848556 -4.2886696 -6.9374137 -6.9402771 -7.3645244][-11.720062 -10.831013 -9.9235973 -7.1836414 -2.6604414 0.16737652 4.6616955 7.9625111 8.1118183 5.6988173 0.90232038 -3.2572372 -7.5895972 -8.2671156 -8.8305407][-10.544302 -10.533537 -10.154846 -7.6432743 -4.7649593 -0.889204 3.98843 6.4667988 7.4392033 5.5111742 0.98698616 -2.460397 -7.5003843 -9.4606657 -9.8608551][-9.500576 -9.8037062 -9.3402023 -7.8384132 -5.640491 -2.8222041 0.14670753 2.5021448 3.1840296 1.7092681 -1.7402105 -5.0902171 -8.8654337 -10.898289 -12.576153][-11.275084 -10.879093 -10.951736 -10.647182 -9.5224972 -8.1269112 -5.9587297 -4.1349382 -3.3149295 -3.4643703 -6.014472 -9.5788784 -13.570251 -13.520343 -13.08674][-14.44711 -13.971133 -13.220448 -12.673287 -12.444521 -11.343864 -10.357544 -10.927203 -10.424908 -8.9034739 -10.268456 -12.10626 -13.329178 -11.996077 -12.248296][-13.462838 -13.674586 -12.196791 -12.652319 -12.720602 -10.594439 -9.7507229 -10.274647 -10.056224 -10.031115 -10.08402 -11.28279 -12.011477 -10.577232 -8.8771763][-11.719334 -11.430508 -9.9422226 -9.7333145 -9.9249687 -10.28785 -10.649725 -9.5885658 -8.89514 -9.295166 -9.3647013 -9.7040977 -8.8240938 -8.2173166 -7.0001841][-8.1730413 -7.5829792 -6.7442474 -5.1962438 -5.0354719 -6.1123409 -6.4269538 -6.4404659 -6.7013025 -6.2923117 -6.5172749 -7.6483607 -8.6766491 -8.14385 -6.8200631]]...]
INFO - root - 2017-12-15 11:11:35.552877: step 4910, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 59h:13m:44s remains)
INFO - root - 2017-12-15 11:11:42.179585: step 4920, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 58h:52m:34s remains)
INFO - root - 2017-12-15 11:11:48.785222: step 4930, loss = 0.20, batch loss = 0.16 (11.8 examples/sec; 0.676 sec/batch; 61h:29m:47s remains)
INFO - root - 2017-12-15 11:11:55.373242: step 4940, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 59h:45m:47s remains)
INFO - root - 2017-12-15 11:12:01.918420: step 4950, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 58h:31m:40s remains)
INFO - root - 2017-12-15 11:12:08.553560: step 4960, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 58h:50m:11s remains)
INFO - root - 2017-12-15 11:12:15.071925: step 4970, loss = 0.24, batch loss = 0.19 (11.9 examples/sec; 0.672 sec/batch; 61h:09m:01s remains)
INFO - root - 2017-12-15 11:12:21.793089: step 4980, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 59h:02m:00s remains)
INFO - root - 2017-12-15 11:12:28.425107: step 4990, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.681 sec/batch; 61h:57m:57s remains)
INFO - root - 2017-12-15 11:12:35.013900: step 5000, loss = 0.21, batch loss = 0.16 (12.7 examples/sec; 0.630 sec/batch; 57h:20m:38s remains)
2017-12-15 11:12:35.490359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.14334 -5.1215744 -5.6074519 -5.1513743 -5.18182 -4.4886284 -3.6794844 -2.3738892 -1.4647107 -1.5470304 -1.5445571 -3.6704865 -5.754952 -6.3769765 -5.6975765][-5.2180037 -4.6551971 -4.3783474 -3.7858467 -3.4674559 -2.9863281 -2.4044182 -1.0723586 -0.22232723 -0.10544062 -0.18011951 -2.621398 -4.8225746 -5.9344964 -5.6343741][-3.3208153 -3.4409349 -3.9451537 -2.6598876 -2.1067111 -1.3977394 -1.0131726 -0.50454617 -0.64034367 -0.663465 -0.68483829 -2.5887341 -4.1883106 -5.3468866 -5.0239034][-3.1035135 -2.6866827 -3.0007339 -2.1248944 -2.441932 -1.6705792 -1.1921048 -1.0767355 -0.84457064 -0.62526083 -0.77182388 -2.8786826 -4.64815 -5.434289 -4.1145186][-3.3764837 -4.0676045 -4.0067053 -2.7056057 -1.7914314 0.039725304 0.4390254 -0.52905607 -1.0033293 -0.88415861 -1.0472307 -2.687583 -4.3353853 -5.2293615 -4.444613][-3.8768463 -4.0873194 -3.5685174 -1.3653693 0.391356 2.4234004 3.3818536 2.5097251 1.540092 0.52572155 -0.47358179 -1.7997065 -3.6232793 -4.9336143 -4.2767873][-5.9786859 -5.2071133 -3.7427998 -0.51926661 1.2405205 3.777986 5.5838094 5.2187285 4.717443 2.3644824 0.35963249 -1.8367305 -4.4086661 -5.547133 -5.1123896][-7.1193366 -6.36644 -4.5753288 -0.64211464 2.480145 5.0578341 6.28178 5.7897582 5.3467412 3.8402114 2.0571828 -1.2994866 -4.6175938 -6.2221336 -6.168129][-6.6871834 -6.2854128 -4.5274692 -1.3876233 1.3472776 3.8246913 5.0434375 4.7835879 3.967854 2.3932939 1.1923738 -2.2743082 -4.8967776 -6.4295712 -6.4472671][-7.0528603 -6.3871841 -5.1309018 -2.071115 -0.19292116 1.5877566 2.81667 2.82056 1.8920546 0.83942556 -0.32584333 -3.0926318 -5.322113 -6.6220956 -6.2226214][-9.6584377 -8.831912 -7.5257111 -4.9024677 -3.3207035 -2.0289595 -1.5765138 -1.4494233 -1.9657156 -2.3389676 -2.7146084 -5.0626183 -6.34437 -7.04257 -5.9853668][-11.844788 -11.281116 -10.138317 -7.971652 -6.768434 -5.3353181 -4.7668462 -4.7667761 -5.4013462 -5.3631945 -5.2241635 -6.4271259 -6.735323 -6.7483592 -5.696754][-11.107181 -10.339816 -9.6405849 -8.5342712 -7.941535 -6.4528446 -6.0509305 -6.2259932 -6.2916832 -6.133481 -5.97097 -6.7918787 -7.1383152 -6.8193879 -5.516932][-9.3474779 -8.7719135 -8.0385 -7.2164741 -6.6733551 -6.0765133 -5.7144365 -5.1775661 -5.1165957 -5.8290472 -6.1545534 -6.4545803 -6.7370071 -6.673645 -5.5458384][-7.71295 -7.8619194 -7.2665343 -6.2064667 -5.5020494 -4.7722616 -4.5041461 -4.3397775 -4.355968 -4.4482431 -4.7683506 -6.1397486 -7.1879063 -7.2455354 -6.5536194]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 11:12:43.207922: step 5010, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 58h:59m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 11:12:49.830909: step 5020, loss = 0.18, batch loss = 0.13 (11.3 examples/sec; 0.706 sec/batch; 64h:11m:31s remains)
INFO - root - 2017-12-15 11:12:56.396874: step 5030, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 60h:03m:39s remains)
INFO - root - 2017-12-15 11:13:02.982254: step 5040, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 59h:58m:48s remains)
INFO - root - 2017-12-15 11:13:09.562008: step 5050, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 59h:26m:26s remains)
INFO - root - 2017-12-15 11:13:16.083249: step 5060, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 59h:32m:50s remains)
INFO - root - 2017-12-15 11:13:22.722515: step 5070, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 58h:38m:55s remains)
INFO - root - 2017-12-15 11:13:29.368113: step 5080, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 60h:06m:01s remains)
INFO - root - 2017-12-15 11:13:35.946367: step 5090, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 58h:06m:29s remains)
INFO - root - 2017-12-15 11:13:42.566158: step 5100, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 60h:18m:24s remains)
2017-12-15 11:13:43.079169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8758864 -5.5139852 -5.8664188 -4.3137856 -3.3685107 -3.3177814 -3.4126127 -3.4315495 -3.6213689 -3.8304045 -3.5397837 -5.0833759 -7.6932974 -7.3363571 -6.3890762][-5.4691849 -4.9401464 -3.4204876 -2.8246658 -2.7340086 -2.1306639 -1.1387663 -0.687943 -1.1959186 -1.8655126 -2.34262 -4.0009518 -6.0089216 -5.5107493 -5.068459][-4.4698992 -5.1452022 -4.2348456 -2.4186738 -1.927192 -2.4839656 -2.6194673 -2.200726 -1.8828149 -1.9237742 -2.3161988 -4.4803128 -6.6607542 -6.0958414 -5.4194603][-2.7556756 -3.7217686 -3.315722 -3.3369234 -2.7137091 -2.3649614 -2.1850369 -1.8570204 -1.677031 -2.0687807 -2.3710458 -4.4566364 -6.7755218 -6.6657133 -6.0885077][-2.6364906 -3.8803163 -3.5028305 -2.8472648 -2.0560184 -1.5659981 -1.7718425 -2.7427104 -2.3140678 -1.7578816 -2.0363007 -3.5217173 -5.7639604 -5.4896431 -5.2909122][-2.4484129 -2.3238325 -2.2474911 -1.3141832 -0.0451622 1.453198 1.8562136 1.3950033 0.55746365 -0.38664913 -0.72889471 -2.2066989 -4.1694894 -3.5340917 -2.300571][-3.0394518 -2.9932053 -2.0420721 -0.1315484 1.7926655 3.9729886 4.4712491 3.794414 2.4563766 1.4861908 1.2946196 -1.2135963 -3.7628756 -3.1296601 -1.5876269][-1.2290173 -0.98421812 -0.024692059 1.7239499 3.1114602 4.8057904 5.433043 5.092381 3.3260617 2.3769755 1.9445124 -0.20855427 -2.693368 -2.27188 -1.4011455][-1.5522161 -0.34501362 0.8654685 1.4668112 2.1152472 3.2218022 2.8434253 2.4957261 1.1561346 0.67334318 0.56298637 -1.0586848 -3.0239809 -2.7294984 -2.4495218][-2.2861612 -1.6577592 -0.59784555 -0.40821552 -0.52867603 -0.46013546 -1.3961778 -1.8636281 -2.3935428 -2.2582042 -1.5896516 -3.0400057 -4.5968165 -3.6109824 -2.3502343][-6.4476027 -6.394608 -5.717279 -4.7851081 -4.8232608 -4.8844409 -4.9105563 -5.6126165 -6.5348287 -6.2840657 -5.0031662 -6.3813653 -7.6141262 -6.5282173 -5.2016912][-10.305998 -10.207874 -9.6798429 -8.37298 -6.9930935 -6.2352133 -5.7457643 -6.1530247 -6.6032672 -6.5398107 -6.4851689 -7.1979194 -7.8267345 -7.1722775 -6.2440944][-10.890787 -9.8054123 -8.790041 -7.4349403 -6.4629726 -6.06743 -6.5224466 -6.6508203 -6.388371 -6.3997755 -6.9649792 -7.7837667 -8.41709 -7.1893077 -5.6269689][-7.18762 -6.8673997 -6.2032895 -4.977685 -4.2778745 -4.3999224 -5.3207836 -6.0859442 -6.12643 -5.7523332 -5.8123593 -6.710587 -7.8282137 -7.2889538 -6.081778][-4.523324 -3.99823 -3.9528222 -3.7848134 -2.9474275 -3.5931606 -3.8133321 -4.5839343 -4.9763913 -5.149828 -5.5321097 -6.1530447 -6.5695577 -7.0861545 -6.8419743]]...]
INFO - root - 2017-12-15 11:13:49.616133: step 5110, loss = 0.19, batch loss = 0.15 (12.6 examples/sec; 0.634 sec/batch; 57h:40m:24s remains)
INFO - root - 2017-12-15 11:13:56.231669: step 5120, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 60h:45m:41s remains)
INFO - root - 2017-12-15 11:14:02.862516: step 5130, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 59h:24m:49s remains)
INFO - root - 2017-12-15 11:14:09.376148: step 5140, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.675 sec/batch; 61h:21m:40s remains)
INFO - root - 2017-12-15 11:14:15.909634: step 5150, loss = 0.26, batch loss = 0.22 (12.2 examples/sec; 0.653 sec/batch; 59h:24m:58s remains)
INFO - root - 2017-12-15 11:14:22.430640: step 5160, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 59h:59m:39s remains)
INFO - root - 2017-12-15 11:14:29.027856: step 5170, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 59h:29m:48s remains)
INFO - root - 2017-12-15 11:14:35.542347: step 5180, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 59h:46m:08s remains)
INFO - root - 2017-12-15 11:14:42.212994: step 5190, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 61h:20m:38s remains)
INFO - root - 2017-12-15 11:14:48.784922: step 5200, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 60h:49m:58s remains)
2017-12-15 11:14:49.280634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6632695 -4.6720285 -3.0691283 -1.5652757 -1.1650376 -2.1615541 -2.439348 -2.7377925 -3.1389186 -5.2419267 -5.3825121 -6.2164264 -8.2706757 -9.435955 -8.6711273][-3.4288847 -3.4304295 -3.7325046 -2.5399914 -1.7590666 -1.1456261 -0.85394239 -1.6999724 -2.4312871 -3.2198904 -4.8559241 -6.9681044 -8.9601688 -9.3065987 -8.6267271][-3.6271951 -3.9476089 -3.8613946 -3.0119255 -2.5582302 -0.82054424 0.45664454 -0.079906464 -2.3233914 -3.8068655 -4.2310672 -6.5794239 -9.3022909 -9.3838882 -8.93322][-4.7688384 -5.5564451 -5.0002627 -4.3615112 -3.4514246 -1.7950447 0.17801428 0.98406315 0.42502832 -2.6216047 -5.8319569 -8.5841866 -10.488434 -11.460945 -9.39756][-4.8986177 -6.3920918 -6.8525095 -5.9667387 -3.0432997 0.66548157 2.3874569 0.58784485 -1.5304141 -2.16647 -3.6741793 -7.7648 -11.408438 -13.179563 -11.950808][-6.3526354 -5.9945192 -5.9779353 -5.5607157 -2.5049222 2.3338947 6.7535658 7.3039465 2.4416065 -1.7033918 -5.072927 -6.7044973 -10.031956 -12.781733 -12.796335][-8.559516 -7.9229364 -6.4905024 -4.39989 -2.2284973 2.0660772 7.2595558 9.6435051 8.0738993 1.788269 -4.9673719 -6.3280048 -8.4919758 -10.702959 -11.42789][-8.5695086 -9.92485 -8.8360748 -4.5876169 -1.3889623 3.6178522 8.4481554 8.085763 7.4146843 2.5812159 -2.4304867 -7.3324351 -10.438763 -10.727555 -9.6028709][-7.5475535 -8.6543179 -7.9868584 -5.4356389 -3.5243392 1.9667039 6.6756067 7.778491 6.8302 0.5084343 -3.56897 -8.5631714 -12.978248 -13.5557 -11.98302][-6.3263803 -7.2777133 -6.8034635 -4.6459532 -3.3428285 -0.25296926 2.0289259 3.1239061 2.9569702 -1.5143204 -6.7142954 -11.454128 -15.417203 -15.752254 -13.988404][-8.8464718 -10.113111 -9.1088915 -7.2542148 -6.1113939 -3.7989275 -3.6733868 -4.2850747 -5.3703909 -7.6804838 -9.9277515 -13.661819 -17.10561 -17.857477 -15.68724][-12.777533 -12.776407 -11.898918 -10.40658 -7.9646044 -6.1379404 -5.9835234 -7.5170646 -9.0221844 -11.045181 -11.89551 -14.54766 -15.410957 -14.987297 -13.798997][-15.904759 -14.880743 -12.966395 -12.200732 -11.170714 -9.6111517 -8.6422453 -9.0324268 -9.542078 -10.342627 -10.665459 -11.660614 -11.958354 -11.985936 -10.15669][-12.478934 -12.238314 -10.85884 -11.277422 -11.548046 -10.226351 -9.2375164 -8.3982468 -8.5799751 -9.0139122 -8.6336718 -7.280674 -7.1266809 -7.4133787 -8.0176506][-8.7601824 -8.5843048 -5.8999557 -5.4481421 -6.3139458 -8.1503716 -8.56503 -7.46957 -6.3967628 -5.9444938 -5.704771 -5.8211036 -6.2090526 -6.4687977 -6.9122109]]...]
INFO - root - 2017-12-15 11:14:55.857018: step 5210, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 61h:11m:52s remains)
INFO - root - 2017-12-15 11:15:02.452724: step 5220, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 62h:41m:14s remains)
INFO - root - 2017-12-15 11:15:09.056684: step 5230, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 59h:35m:19s remains)
INFO - root - 2017-12-15 11:15:15.668330: step 5240, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 59h:10m:15s remains)
INFO - root - 2017-12-15 11:15:22.208271: step 5250, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 59h:48m:01s remains)
INFO - root - 2017-12-15 11:15:28.825913: step 5260, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 58h:32m:33s remains)
INFO - root - 2017-12-15 11:15:35.406292: step 5270, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 58h:16m:16s remains)
INFO - root - 2017-12-15 11:15:42.079487: step 5280, loss = 0.24, batch loss = 0.19 (12.1 examples/sec; 0.662 sec/batch; 60h:11m:58s remains)
INFO - root - 2017-12-15 11:15:48.637096: step 5290, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 58h:13m:00s remains)
INFO - root - 2017-12-15 11:15:55.312839: step 5300, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 59h:18m:09s remains)
2017-12-15 11:15:55.795232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8098037 -3.69417 -5.0315094 -5.4459486 -7.581605 -9.578064 -9.1968155 -8.1181335 -6.4859729 -4.274107 -1.4363446 -2.1449015 -3.7652564 -4.344686 -5.7511935][0.29511166 -1.7240398 -2.8993709 -4.6281166 -7.4989057 -9.5504551 -11.565104 -11.473256 -9.4698038 -6.7404108 -3.4152634 -3.5176041 -4.5696387 -5.7754116 -7.4002757][2.3097744 0.67369556 -1.1285558 -2.0144877 -4.3403721 -7.5182548 -9.476737 -9.77104 -9.0606747 -7.3619471 -5.3179173 -6.3892493 -7.1493754 -7.8647804 -9.0079556][0.2763114 -0.33003616 -0.78006268 -1.0753884 -2.1695518 -3.8659232 -5.4095006 -6.2706203 -6.9572034 -6.3562422 -5.3616071 -7.4748487 -9.050806 -9.8146744 -10.38726][-0.96422911 -2.5747418 -3.1065392 -0.23029995 0.41830873 0.75442743 1.1588211 -2.1372168 -5.0194359 -4.8891215 -4.8755426 -7.5832753 -9.5738449 -10.979715 -11.759325][-4.9659486 -5.3737206 -4.1663094 -1.0971417 0.55097866 3.6405778 5.8071914 4.3553061 3.1307511 -0.28920412 -2.9926677 -5.7864294 -8.64541 -10.527584 -11.795778][-8.6839 -8.9062 -6.2124414 -2.8353148 -1.3861904 3.5525241 6.5103974 6.3191161 6.75915 3.4413133 0.25894403 -3.0860293 -6.2630868 -8.3321562 -10.293528][-9.2109337 -8.16065 -5.8123155 -2.715344 -0.25550556 3.5813861 5.7010169 6.3802419 6.6342826 4.5082355 2.7047763 -1.2793989 -5.5935488 -7.5163445 -8.514205][-5.9925551 -5.8499265 -5.0959344 -2.6267233 -0.013528347 2.6152472 4.2463641 5.7503419 6.566896 5.3432422 2.9237866 -2.1037755 -5.822937 -7.5141525 -8.87672][-4.0260763 -3.3583894 -2.5600235 -1.1687522 0.28307962 1.8689413 3.4756341 3.99929 2.8078113 1.7214532 1.1273985 -2.6172655 -6.0821285 -7.8096313 -9.0621815][-8.0047865 -6.1670351 -4.3817387 -3.1090481 -1.8651638 -1.358129 -0.9204216 -1.2864366 -2.1259933 -2.8781273 -4.245111 -7.0712018 -8.93496 -8.9541941 -9.1877289][-14.141325 -11.500904 -7.6605859 -6.3360362 -5.0009651 -5.2490373 -5.6120992 -5.4890523 -6.37292 -6.854908 -7.6753945 -9.1577368 -10.269591 -10.086037 -10.049676][-14.331629 -12.285745 -10.274764 -7.9455938 -6.4098382 -6.7285233 -6.6708007 -7.5885305 -9.1807184 -8.9901676 -8.7646532 -9.3243484 -9.96401 -8.9091434 -7.6287642][-14.19911 -11.958555 -10.162951 -8.6024628 -6.9880486 -7.1420622 -7.9150314 -7.8298044 -7.8517809 -8.0908146 -8.3474436 -8.2835541 -8.6696148 -8.2376423 -7.2563419][-9.6855326 -10.307262 -9.046977 -7.4426556 -6.2819476 -4.8301878 -4.3273268 -5.6845541 -7.3569717 -7.2244596 -6.4606891 -6.8661742 -7.909606 -7.4826107 -7.1028929]]...]
INFO - root - 2017-12-15 11:16:02.425502: step 5310, loss = 0.22, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 59h:50m:01s remains)
INFO - root - 2017-12-15 11:16:08.997352: step 5320, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 59h:32m:18s remains)
INFO - root - 2017-12-15 11:16:15.650664: step 5330, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 59h:51m:08s remains)
INFO - root - 2017-12-15 11:16:22.278968: step 5340, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 60h:55m:19s remains)
INFO - root - 2017-12-15 11:16:28.882236: step 5350, loss = 0.22, batch loss = 0.18 (12.0 examples/sec; 0.666 sec/batch; 60h:33m:07s remains)
INFO - root - 2017-12-15 11:16:35.545283: step 5360, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 59h:22m:20s remains)
INFO - root - 2017-12-15 11:16:42.151242: step 5370, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 61h:31m:21s remains)
INFO - root - 2017-12-15 11:16:48.674392: step 5380, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 59h:16m:55s remains)
INFO - root - 2017-12-15 11:16:55.329982: step 5390, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 59h:32m:21s remains)
INFO - root - 2017-12-15 11:17:01.929447: step 5400, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 59h:51m:35s remains)
2017-12-15 11:17:02.404593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0473087 -2.0790687 -2.4310334 -2.7274728 -3.6186182 -4.5259905 -5.2138352 -5.7614136 -5.4928904 -4.8911104 -4.5778551 -4.58268 -6.7461157 -7.7686734 -7.6572943][-3.1994567 -3.2347772 -3.6387136 -4.469408 -5.3525381 -5.9012456 -6.3046474 -5.4475551 -4.4165134 -3.9270656 -3.8695378 -4.55929 -7.3798227 -8.9925451 -8.8088579][-3.0093882 -3.4079394 -4.4046431 -5.2505689 -5.8933115 -5.6663752 -5.2141571 -5.1276255 -4.6653533 -4.53802 -4.4183574 -4.1143017 -6.64988 -8.0626354 -8.1941757][-3.9287548 -4.1800895 -4.6942759 -5.0884247 -5.4878135 -4.9094677 -4.0348368 -4.0004187 -3.9617085 -4.208962 -4.6805086 -5.5070376 -8.6701736 -8.6989241 -7.896102][-5.0769463 -5.7063618 -5.7118659 -5.1792345 -4.7154455 -2.3663728 -0.72918653 -1.1196756 -2.3174589 -3.4924078 -5.4246135 -6.1100564 -8.699152 -9.573554 -8.99635][-5.4996524 -6.0773025 -5.78099 -4.6497421 -3.6093988 -0.38519192 2.3856058 2.743681 1.3464665 -1.8545203 -4.5860481 -5.57995 -8.3611212 -8.2897091 -7.8811455][-5.3010936 -7.0911779 -6.8571668 -5.5153928 -4.0203161 -0.16487885 3.5091391 5.3380756 4.9836464 1.4514737 -2.0516791 -3.3195615 -6.4586687 -7.4156303 -7.1469412][-5.0944242 -5.8593407 -5.0667562 -4.0888972 -3.0332007 0.29381943 3.7483845 5.4316692 5.2451897 3.4697142 0.85836172 -1.7386611 -5.545146 -6.2292814 -6.2848368][-3.9354 -4.8275385 -4.1458578 -3.1090086 -2.5622289 0.18584299 3.4702444 5.7693615 6.1939306 4.2314329 2.1947541 0.21928644 -3.5933006 -5.9183455 -6.3646936][-2.9348843 -3.6632223 -2.4473062 -0.79571056 -0.66691685 -0.17977 1.5603447 3.4183235 3.6304083 2.3585358 0.3898139 -1.7635207 -4.8641982 -5.5501528 -5.2474756][-6.1692729 -6.2882671 -5.6414294 -4.463098 -3.8424807 -2.6449144 -1.4250784 -0.31392717 -0.46658754 -1.8654885 -3.5585995 -5.6996503 -8.2921438 -7.7675209 -6.1427464][-8.5184813 -8.7316427 -8.654254 -7.0606718 -6.3580713 -5.854218 -5.0902262 -4.958231 -5.5964055 -6.1669583 -7.595293 -9.6383257 -10.631973 -9.857789 -7.9580507][-10.481297 -10.391746 -9.0070267 -8.9391775 -8.8909864 -7.275527 -6.4193859 -6.1176963 -6.3287778 -7.3676248 -8.7117119 -10.877853 -11.17873 -10.272899 -8.3302841][-10.029969 -10.873409 -10.578187 -9.0320683 -7.8732767 -7.5451555 -6.9769211 -7.1833525 -7.329608 -8.0301094 -8.3521566 -8.5290346 -8.4947929 -8.1958513 -7.2100158][-8.0317154 -8.4887762 -8.3514414 -8.5038195 -8.5172539 -7.7000189 -6.8071918 -5.8231907 -5.45681 -6.165339 -6.5224118 -7.31343 -7.440681 -6.1536808 -5.9285207]]...]
INFO - root - 2017-12-15 11:17:08.957758: step 5410, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.630 sec/batch; 57h:15m:34s remains)
INFO - root - 2017-12-15 11:17:15.455742: step 5420, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 59h:36m:22s remains)
INFO - root - 2017-12-15 11:17:22.018986: step 5430, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 58h:48m:40s remains)
INFO - root - 2017-12-15 11:17:28.735380: step 5440, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 59h:16m:35s remains)
INFO - root - 2017-12-15 11:17:35.286970: step 5450, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 57h:55m:17s remains)
INFO - root - 2017-12-15 11:17:41.838255: step 5460, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 59h:11m:34s remains)
INFO - root - 2017-12-15 11:17:48.377901: step 5470, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.678 sec/batch; 61h:33m:51s remains)
INFO - root - 2017-12-15 11:17:55.011820: step 5480, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 59h:21m:41s remains)
INFO - root - 2017-12-15 11:18:01.621661: step 5490, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 61h:03m:43s remains)
INFO - root - 2017-12-15 11:18:08.200215: step 5500, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 61h:16m:19s remains)
2017-12-15 11:18:08.688189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9333439 -4.2750826 -3.9189835 -4.3241396 -5.1090603 -5.9593811 -6.1044121 -6.2911386 -6.6101189 -6.3664308 -6.1014719 -7.1939845 -8.6382141 -9.2159367 -8.817358][-3.9007092 -4.2625828 -4.9295154 -5.7935634 -6.5456724 -7.3438034 -7.214067 -6.96588 -7.0067892 -6.7791672 -6.607666 -7.6412115 -8.9346228 -9.853714 -9.8352852][-3.10608 -4.3180666 -5.7278228 -6.2394552 -7.2769895 -7.484695 -6.9329114 -6.3209558 -5.7591515 -5.8318968 -6.0666418 -7.0946646 -8.4354506 -9.518198 -9.8686714][-5.7254124 -6.0497303 -6.6008148 -7.1268206 -6.9665589 -6.46739 -4.9523268 -4.0560312 -4.1059947 -4.1620941 -4.5226026 -6.3751864 -8.6103516 -9.29982 -8.9415178][-4.9706907 -6.5121384 -8.0759993 -7.74076 -6.140337 -4.3950834 -2.4503856 -1.9545622 -2.813889 -3.2965562 -3.5085654 -5.5616856 -8.2229252 -9.340559 -9.4004745][-5.6987309 -6.1802812 -6.2278628 -5.7364788 -3.2780113 -1.0678568 1.2141609 1.7445984 0.60527039 -1.6011095 -4.5055075 -5.5746636 -6.905283 -8.149725 -7.8770342][-5.5477786 -6.1184745 -6.0404754 -3.943398 -1.490365 1.7206564 4.9213052 5.0451112 3.391747 0.47569609 -2.5660858 -4.9209986 -7.3221979 -7.52938 -6.7524767][-6.4539208 -6.3823748 -5.2177711 -2.0217338 0.29478359 3.7411819 6.9492793 5.8448291 3.7913146 1.2336335 -1.5653262 -4.6391921 -6.903461 -6.7307224 -5.8708339][-6.6014919 -5.9122729 -4.6116023 -2.3379648 -0.049551964 3.0477128 5.346045 4.2616191 3.1475024 0.84270287 -2.0891032 -5.5306277 -8.94429 -8.740756 -7.2766018][-6.8513508 -7.1011562 -5.956079 -4.2814569 -2.655972 -0.16836786 3.0306973 2.2994447 0.73633385 -1.7799392 -4.3163509 -7.3505154 -10.138207 -10.309288 -9.9688549][-10.153316 -9.716836 -8.8532314 -7.6214428 -6.3421669 -4.6045179 -3.393141 -3.2766762 -3.4566631 -4.9481244 -6.9534607 -9.8526831 -11.448663 -11.143083 -10.109921][-13.200224 -13.139278 -11.578369 -9.73948 -8.0030384 -6.0885472 -5.2188249 -5.71594 -6.2100348 -7.3681679 -8.7109671 -10.378742 -10.075426 -9.9890308 -8.9232349][-13.599874 -12.88323 -11.834108 -9.4729395 -9.1001692 -7.1495385 -6.6984463 -7.2222295 -7.8157177 -7.8422832 -7.8839874 -8.8426619 -9.4116888 -8.95796 -7.7017307][-11.389647 -10.542571 -8.7360382 -7.7400594 -6.8398757 -5.9481263 -6.0014477 -5.6868334 -5.3825674 -6.2945042 -8.0301685 -7.7663755 -8.4269867 -7.77108 -6.7718897][-8.9334173 -7.9047761 -6.3860779 -5.2587872 -4.1403012 -4.5963039 -4.6952786 -4.6646814 -5.4774938 -5.5888309 -5.9675994 -6.6748757 -7.7433529 -8.1804695 -8.8742723]]...]
INFO - root - 2017-12-15 11:18:15.258108: step 5510, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 59h:56m:24s remains)
INFO - root - 2017-12-15 11:18:21.886790: step 5520, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 59h:53m:54s remains)
INFO - root - 2017-12-15 11:18:28.508442: step 5530, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 60h:35m:26s remains)
INFO - root - 2017-12-15 11:18:35.137449: step 5540, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 58h:06m:01s remains)
INFO - root - 2017-12-15 11:18:41.742645: step 5550, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 58h:49m:23s remains)
INFO - root - 2017-12-15 11:18:48.380282: step 5560, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.686 sec/batch; 62h:20m:17s remains)
INFO - root - 2017-12-15 11:18:55.046634: step 5570, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 59h:30m:17s remains)
INFO - root - 2017-12-15 11:19:01.718795: step 5580, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 59h:23m:53s remains)
INFO - root - 2017-12-15 11:19:08.387614: step 5590, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 61h:35m:55s remains)
INFO - root - 2017-12-15 11:19:15.014477: step 5600, loss = 0.17, batch loss = 0.12 (11.6 examples/sec; 0.687 sec/batch; 62h:21m:30s remains)
2017-12-15 11:19:15.513454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7253108 -4.7388191 -4.0662684 -3.8587258 -4.7696667 -4.6155319 -5.0036392 -4.5828996 -3.6806426 -2.1021419 -0.90384722 -3.5374253 -5.8969741 -7.1427412 -7.8689213][-4.9474225 -5.2470121 -4.8452263 -4.940886 -5.97754 -6.1689434 -6.1786342 -5.8292894 -5.549161 -4.888669 -3.69194 -6.1140647 -8.53026 -9.9516668 -9.918438][-3.6601727 -4.9914904 -5.9154568 -5.2453718 -5.1557317 -5.5864325 -6.0605068 -5.9939833 -5.3894882 -4.952167 -4.5431838 -7.605123 -9.7642689 -10.698189 -10.962273][-4.8937573 -5.3634338 -5.5600314 -4.8976936 -5.7021675 -4.7210851 -4.5294333 -5.3861165 -5.3525953 -4.9217896 -4.1724358 -7.1537809 -9.4814672 -10.46178 -9.985117][-5.7034254 -7.5112133 -7.6009436 -4.7564216 -3.57341 -2.1278734 -1.3620615 -2.4478734 -3.3761859 -3.2814844 -3.0561259 -6.5610762 -8.8786306 -9.77615 -10.430694][-6.832839 -7.3926988 -6.1218972 -3.2009261 -1.1742558 1.528976 3.3538876 2.6255827 1.8756986 0.1308341 -1.7565434 -5.0819407 -7.5806646 -9.456316 -9.5605545][-7.5149355 -6.7640371 -4.8698883 -1.6933627 0.83217669 4.7539167 7.8861532 7.1945076 6.4458828 3.2842646 -0.24413347 -3.9897718 -6.3360023 -7.9831948 -8.3654995][-7.3212852 -6.2261782 -4.26897 -0.9215703 1.9473 5.6509132 7.806098 7.9222603 7.6975069 4.4342232 1.6136527 -3.3899496 -7.0649366 -7.2916231 -6.699554][-6.3709149 -4.96512 -3.9378529 -1.2925611 1.444932 4.2825165 5.6917744 5.8843546 5.8902149 3.887538 1.5799384 -3.5635474 -6.7921352 -7.974988 -7.9497848][-6.7597179 -5.2693172 -4.2636366 -1.7983093 -0.55379009 1.833231 3.9065952 4.1284676 3.17723 1.9004154 0.54596281 -4.6751633 -7.2451696 -8.2324371 -9.1020679][-10.924022 -10.208808 -8.611393 -5.7243018 -4.86418 -3.6008339 -2.23589 -1.5933418 -2.1096365 -2.7989526 -3.9828002 -7.5020618 -9.7646637 -10.352301 -8.9821329][-14.819634 -13.809136 -11.542969 -8.986495 -7.7031231 -6.8017659 -6.9076462 -6.6624417 -6.3809595 -7.0349846 -7.7598681 -9.4058218 -10.656966 -10.279057 -10.07313][-12.619774 -11.555556 -9.7497568 -8.0730848 -7.677568 -6.5917678 -6.0413065 -6.5169525 -7.2217293 -7.49154 -7.3179011 -7.7799106 -8.2462749 -7.8005548 -7.8451777][-11.333313 -10.960599 -9.4338226 -7.1646972 -5.7677603 -5.9177651 -6.5788445 -6.1626248 -5.8231707 -6.1960239 -6.4471927 -7.302722 -7.225955 -6.7039442 -6.9353571][-8.2541571 -8.2971735 -7.3273954 -5.6436768 -4.5220051 -3.7638049 -3.6131356 -4.0346823 -4.5007005 -4.8232813 -4.9459543 -5.9366078 -6.4419303 -6.8935847 -7.0207939]]...]
INFO - root - 2017-12-15 11:19:22.186189: step 5610, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.686 sec/batch; 62h:19m:05s remains)
INFO - root - 2017-12-15 11:19:28.855885: step 5620, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 59h:39m:23s remains)
INFO - root - 2017-12-15 11:19:35.426357: step 5630, loss = 0.18, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 57h:12m:40s remains)
INFO - root - 2017-12-15 11:19:42.119648: step 5640, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 59h:10m:37s remains)
INFO - root - 2017-12-15 11:19:48.695628: step 5650, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 61h:03m:53s remains)
INFO - root - 2017-12-15 11:19:55.230905: step 5660, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 57h:45m:03s remains)
INFO - root - 2017-12-15 11:20:01.757598: step 5670, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 58h:36m:52s remains)
INFO - root - 2017-12-15 11:20:08.313227: step 5680, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 60h:06m:24s remains)
INFO - root - 2017-12-15 11:20:14.912284: step 5690, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 60h:12m:36s remains)
INFO - root - 2017-12-15 11:20:21.476271: step 5700, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 58h:42m:39s remains)
2017-12-15 11:20:21.979582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3561759 -4.540967 -4.6553578 -4.6168494 -4.68449 -5.1301026 -5.1117954 -5.3547697 -5.0442162 -4.8527384 -5.0692229 -6.3702707 -8.9801073 -10.362204 -9.6494026][-4.9704075 -5.3351784 -5.2688656 -5.24944 -5.0755539 -5.4060068 -5.7147355 -5.7644796 -5.736474 -5.64397 -5.4990735 -6.7474346 -8.7823591 -10.257046 -10.39435][-3.6989222 -4.7738619 -5.6516576 -5.17221 -5.4923782 -5.5522232 -5.2348061 -5.418222 -5.4370322 -5.2123566 -4.9621978 -6.4176445 -8.9440966 -10.377092 -11.074478][-4.9814939 -6.0345464 -6.76243 -6.5649672 -6.0791254 -4.4603748 -3.7875414 -4.4782071 -4.8562465 -4.9364133 -4.9746466 -6.4245687 -8.517148 -9.6508207 -9.7943974][-6.535583 -8.3880625 -8.7475882 -7.7491512 -6.8253069 -3.6053011 -1.7830887 -2.2792737 -3.0252724 -3.5041964 -3.966311 -5.3758869 -8.0425291 -8.82848 -8.9023123][-8.475318 -9.5567532 -9.5617819 -7.63855 -5.1772628 -1.040864 1.8392887 1.542841 0.39825535 -1.0240388 -1.1612859 -2.3714728 -4.8203826 -6.4826574 -7.0286722][-7.7994757 -8.4407949 -7.1979251 -4.549912 -2.6063225 0.37454033 3.2042594 4.1048455 4.5094724 2.0906711 0.66040516 -0.60263729 -3.0407944 -5.1292181 -6.0133991][-6.9314032 -6.8042173 -5.9180479 -2.5980887 0.30005217 2.9225154 4.6033607 4.6963553 4.6249747 3.8338666 2.4238496 0.096831322 -2.9991326 -4.5628004 -6.2609272][-4.0511513 -4.1335444 -5.2376356 -3.2148416 -1.4656997 1.4830122 3.8700423 4.1636357 4.4936748 3.1351647 2.1086183 -0.67576551 -4.0732765 -5.7961173 -6.4585915][-3.332619 -3.6098921 -3.9891248 -3.1402936 -2.9865654 -0.89943218 1.4147911 1.7995276 1.5533285 1.2766266 0.72601604 -1.9438879 -5.5733781 -7.4679251 -9.3421993][-8.1865215 -7.7929535 -8.0812559 -6.9525118 -7.2648697 -6.1249566 -4.9073763 -3.7234666 -3.3665614 -4.4481316 -5.4226861 -7.1891809 -9.6968756 -10.769377 -11.152929][-11.697891 -11.415694 -10.78895 -9.7591133 -8.60021 -7.761642 -8.24058 -8.3124514 -7.8313 -7.74769 -8.4880047 -10.514757 -11.320511 -11.904751 -12.001869][-13.91213 -12.771881 -11.260512 -10.109827 -9.8541956 -8.5046434 -8.1039581 -8.6514282 -9.3476915 -10.013314 -9.9622459 -10.863487 -11.182274 -10.35976 -9.54236][-12.540203 -12.572505 -11.543188 -9.3989725 -7.6190519 -7.02671 -7.4545274 -8.0091286 -7.9207182 -7.3458161 -7.1678448 -8.0751505 -8.9091005 -9.1622581 -8.3422747][-7.825511 -8.6319008 -7.6217618 -6.1808276 -6.1055589 -6.0855503 -5.5110154 -4.5264168 -4.9274979 -5.8698306 -6.3428268 -6.9357166 -7.2985234 -6.8216753 -7.2218351]]...]
INFO - root - 2017-12-15 11:20:28.547626: step 5710, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 60h:19m:58s remains)
INFO - root - 2017-12-15 11:20:35.163619: step 5720, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 58h:50m:36s remains)
INFO - root - 2017-12-15 11:20:41.800631: step 5730, loss = 0.19, batch loss = 0.14 (11.8 examples/sec; 0.677 sec/batch; 61h:24m:23s remains)
INFO - root - 2017-12-15 11:20:48.346653: step 5740, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 58h:13m:50s remains)
INFO - root - 2017-12-15 11:20:54.839331: step 5750, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 59h:39m:54s remains)
INFO - root - 2017-12-15 11:21:01.428852: step 5760, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 58h:58m:32s remains)
INFO - root - 2017-12-15 11:21:08.002144: step 5770, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 60h:58m:32s remains)
INFO - root - 2017-12-15 11:21:14.593195: step 5780, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 59h:30m:07s remains)
INFO - root - 2017-12-15 11:21:21.134617: step 5790, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 60h:41m:18s remains)
INFO - root - 2017-12-15 11:21:27.753873: step 5800, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 58h:59m:21s remains)
2017-12-15 11:21:28.284543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.92666 -8.5465593 -9.2776842 -9.39108 -10.049363 -9.9228907 -9.2612534 -8.6096191 -7.9242344 -9.0287962 -9.8148365 -11.170094 -13.233641 -11.604486 -9.3450089][-6.2276893 -6.5937061 -7.6890078 -8.6441288 -9.5459824 -10.058655 -9.62186 -9.2374821 -8.9602 -8.7200317 -7.8183527 -9.04457 -11.716076 -11.547659 -9.298811][-5.5051551 -5.9620166 -6.9362793 -8.2016792 -9.6756573 -10.089981 -9.816021 -8.883812 -8.6964321 -9.1161432 -9.0145483 -9.794899 -10.501301 -9.9995193 -9.3346767][-5.3372536 -5.6799688 -6.4436851 -6.4213386 -6.6647911 -7.2177362 -7.068141 -6.7654247 -6.8188295 -7.0742912 -7.7617321 -8.95831 -10.723813 -9.9616356 -8.1984825][-4.8735185 -5.5381823 -5.8376446 -4.7688169 -4.0014267 -2.488982 -2.1644602 -2.8209183 -3.2499013 -4.0709372 -5.2479143 -6.8972239 -9.539856 -9.3771381 -8.3476534][-6.79933 -5.7762022 -4.6055069 -2.2394824 0.022240639 2.5498071 4.4856725 4.709867 2.8193679 -0.42183638 -2.4794564 -4.8588524 -7.2519951 -7.4921904 -8.0786371][-6.7180853 -5.6274552 -4.5200653 -1.9282625 1.1784749 4.3069062 7.9289107 8.8863487 6.9641261 3.5414691 0.016829967 -3.5049019 -5.8841057 -6.2573328 -6.1732903][-6.3799267 -5.34892 -4.5551796 -1.4277263 1.3464065 5.093019 7.4973578 7.824367 7.25944 4.8033986 1.1567369 -2.767962 -5.9632297 -5.3998847 -5.0983534][-6.819325 -5.4951015 -4.6231718 -2.3318353 -0.38782215 2.5961661 4.9961505 6.0071759 5.130199 1.8645258 -0.642046 -4.2804928 -9.645 -9.8872747 -8.28545][-6.333076 -5.9638958 -5.23855 -3.6912742 -3.0675473 -0.638741 1.883173 3.0187802 1.4545712 -0.71702862 -3.4932199 -8.8018055 -12.033137 -13.585781 -14.596558][-8.962904 -8.5265236 -8.0435953 -6.4432282 -5.6266823 -4.7140646 -3.6312079 -3.1729002 -3.6776142 -4.9205561 -8.0746231 -12.816935 -16.85351 -18.506029 -17.276781][-10.390914 -10.246091 -9.6740322 -8.5622826 -6.9436831 -5.7243638 -5.9052014 -6.583869 -7.26793 -8.341608 -10.927666 -12.678524 -14.55159 -17.152802 -15.810116][-12.154566 -10.899614 -9.6134462 -8.2104883 -7.9228373 -7.5460815 -7.1041565 -6.928968 -7.2457819 -8.3917084 -9.23416 -10.704756 -12.392513 -12.48592 -10.90417][-10.378705 -9.4418888 -7.7439919 -6.6779981 -6.3289938 -6.140759 -5.8056631 -5.3419805 -4.935133 -5.309659 -6.3317537 -7.3675442 -6.9824762 -6.2913332 -5.7302971][-7.918313 -7.063529 -5.1569362 -3.20074 -2.3240161 -3.0644875 -4.4308524 -4.6532965 -4.8198342 -4.8408918 -5.1999035 -5.6178622 -5.7302361 -5.8549013 -6.0560803]]...]
INFO - root - 2017-12-15 11:21:34.886862: step 5810, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 59h:50m:57s remains)
INFO - root - 2017-12-15 11:21:41.356600: step 5820, loss = 0.19, batch loss = 0.15 (12.7 examples/sec; 0.627 sec/batch; 56h:56m:20s remains)
INFO - root - 2017-12-15 11:21:47.891586: step 5830, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 58h:53m:22s remains)
INFO - root - 2017-12-15 11:21:54.467983: step 5840, loss = 0.18, batch loss = 0.13 (12.9 examples/sec; 0.622 sec/batch; 56h:26m:48s remains)
INFO - root - 2017-12-15 11:22:00.989197: step 5850, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 59h:22m:26s remains)
INFO - root - 2017-12-15 11:22:07.575366: step 5860, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 60h:45m:10s remains)
INFO - root - 2017-12-15 11:22:14.135159: step 5870, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 59h:14m:18s remains)
INFO - root - 2017-12-15 11:22:20.745629: step 5880, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 60h:09m:11s remains)
INFO - root - 2017-12-15 11:22:27.317465: step 5890, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 59h:11m:20s remains)
INFO - root - 2017-12-15 11:22:33.941593: step 5900, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.681 sec/batch; 61h:49m:05s remains)
2017-12-15 11:22:34.448441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.4516182 -8.7068968 -8.2502565 -7.8410282 -8.5516968 -9.2800045 -9.4843025 -8.6728611 -7.6974487 -7.0058017 -5.7896662 -3.940167 -6.6378856 -10.179976 -12.138445][-7.6033106 -7.7223282 -6.6931572 -6.4352345 -7.7809052 -8.7889442 -8.9702282 -8.0887012 -7.7067466 -7.3210311 -6.0817423 -5.4629607 -8.328186 -10.374392 -12.181815][-5.1156383 -6.336854 -6.6647968 -5.4873419 -6.5840788 -7.7676163 -8.0955725 -6.9379187 -6.8763828 -7.4030628 -6.9421821 -5.2614546 -7.3563962 -10.138819 -11.86818][-6.0385561 -6.392416 -5.5847235 -5.3823495 -6.4036336 -5.7342253 -5.1965075 -5.98525 -6.9368496 -6.3959713 -5.3150854 -4.7139468 -6.1707554 -8.9625111 -10.693242][-5.9768891 -8.0826607 -7.3361197 -6.1185188 -5.8998957 -2.9926085 -0.19256163 -2.057884 -4.761591 -5.149343 -5.1437035 -3.6300659 -4.3908772 -6.8878722 -9.1471815][-7.4120693 -7.7784452 -6.9474707 -5.0938797 -3.0549691 1.0288343 4.2766957 3.2637067 1.6273627 -0.84417963 -3.1157603 -1.2601233 -2.5073929 -5.5149217 -7.760869][-8.702508 -8.21794 -5.9799452 -3.2381716 -1.1107955 3.1826997 7.2596374 7.7084403 6.5597644 1.7745156 -1.9852247 -0.76097155 -2.8208916 -5.23228 -7.9236755][-8.4332714 -7.7801647 -5.7171593 -1.9752004 0.6926918 4.6008296 7.7717595 8.2019958 6.7509732 2.5048661 -0.84242868 -1.5000324 -4.6226778 -7.4551849 -10.396402][-6.2260394 -5.9614325 -4.7003846 -2.2061498 0.16571665 2.4059815 4.6891723 5.2926364 3.6466174 0.87440443 -1.1576405 -3.0612395 -6.9194651 -9.6403475 -11.741976][-5.0269427 -4.7159643 -3.7558894 -1.9074376 -0.28726435 0.12731886 1.9921708 2.5771041 0.56006432 -0.51961184 -2.0778027 -3.910212 -7.3425736 -10.877481 -13.77651][-7.8085461 -7.8154173 -7.45415 -5.669138 -3.7928815 -3.760906 -2.4316447 -2.3400509 -3.2563777 -3.9236035 -6.259429 -7.0662189 -9.0277519 -11.836748 -13.020212][-12.928545 -12.238548 -10.218319 -9.1016932 -8.960041 -8.2847595 -7.2528415 -7.7057667 -8.0788965 -7.92776 -9.1078205 -8.7714539 -9.9025154 -11.601372 -12.152296][-13.651139 -13.74482 -12.313833 -10.374924 -9.2044067 -8.83063 -9.1226711 -9.5586386 -9.7165833 -9.1255856 -8.7482147 -7.1840334 -7.6583142 -8.6163254 -8.5511017][-11.715553 -11.661379 -10.229193 -7.9798608 -6.5269585 -6.9516878 -7.9624538 -6.8617115 -6.325633 -6.8036718 -7.2926097 -5.3711848 -5.6064525 -6.1998153 -6.6160197][-8.2135067 -8.2631111 -6.8394685 -5.5264225 -4.0517941 -3.2972798 -3.3660045 -4.1845989 -5.6032233 -5.3181276 -4.5838132 -4.44909 -5.882834 -5.9034758 -6.4031491]]...]
INFO - root - 2017-12-15 11:22:40.984900: step 5910, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.681 sec/batch; 61h:46m:31s remains)
INFO - root - 2017-12-15 11:22:47.519535: step 5920, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 58h:47m:51s remains)
INFO - root - 2017-12-15 11:22:54.153040: step 5930, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.696 sec/batch; 63h:08m:31s remains)
INFO - root - 2017-12-15 11:23:00.765280: step 5940, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 59h:35m:15s remains)
INFO - root - 2017-12-15 11:23:07.391163: step 5950, loss = 0.17, batch loss = 0.12 (11.4 examples/sec; 0.701 sec/batch; 63h:35m:06s remains)
INFO - root - 2017-12-15 11:23:13.929830: step 5960, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 57h:35m:15s remains)
INFO - root - 2017-12-15 11:23:20.527552: step 5970, loss = 0.19, batch loss = 0.14 (11.7 examples/sec; 0.681 sec/batch; 61h:48m:20s remains)
INFO - root - 2017-12-15 11:23:27.129776: step 5980, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 59h:23m:23s remains)
INFO - root - 2017-12-15 11:23:33.699188: step 5990, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 58h:06m:25s remains)
INFO - root - 2017-12-15 11:23:40.263892: step 6000, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.659 sec/batch; 59h:44m:31s remains)
2017-12-15 11:23:40.820040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.2819824 -10.141281 -11.245283 -10.688065 -10.755894 -10.776825 -10.285656 -9.3332186 -8.7518711 -7.6727633 -6.3571486 -7.4162989 -9.5657434 -8.4905844 -8.0081377][-8.0128164 -8.1778088 -7.884963 -8.2693138 -9.4513636 -9.4169559 -9.5921707 -9.498085 -9.3996716 -8.7031965 -8.1041021 -9.46505 -10.563449 -9.0200672 -7.8408256][-4.9262476 -6.0335789 -7.5695839 -7.1530218 -7.5416079 -7.8048129 -8.47357 -8.4083977 -7.858614 -8.2462177 -8.24176 -9.380105 -11.362196 -9.706727 -8.7258787][-5.4179382 -4.9986625 -5.729495 -6.431879 -7.017662 -6.30184 -6.2819905 -7.2414956 -7.6078835 -6.5135479 -5.7423234 -8.2006435 -10.729994 -9.76816 -8.99396][-5.7476845 -7.2639437 -8.568306 -7.0251455 -4.8367858 -3.3772483 -2.5302842 -3.939136 -7.4775715 -7.3002186 -6.041822 -7.3891678 -9.4743853 -9.3203182 -9.2785959][-7.21696 -7.6491213 -7.9424148 -5.7060752 -2.4527392 0.63679743 3.2359447 1.4389901 -2.7502737 -5.1205006 -7.2397861 -7.5768147 -8.3988 -8.0529575 -8.2600117][-9.9105453 -8.6149063 -6.84576 -3.6640065 -1.1075525 2.9761286 7.8359318 6.0016947 2.175993 -2.2584426 -7.1105204 -7.801208 -9.0668945 -8.0245333 -8.0130444][-10.236322 -8.6112947 -7.1826591 -2.7931259 -0.040223122 3.8323164 7.9998531 7.0201721 4.9577923 0.56504154 -4.329381 -7.3023934 -10.33419 -9.3423862 -8.407198][-9.4506931 -8.6597118 -7.5577121 -4.2454486 -1.3203502 2.0441809 4.5338593 4.275856 3.838975 -0.098902225 -3.7898531 -6.9776373 -11.381843 -10.744751 -9.8000231][-8.1243114 -8.1141481 -7.6357679 -5.2599564 -3.3469808 -1.5645638 0.55008221 0.9204278 0.14376163 -2.1161897 -4.5310526 -8.1048679 -11.374684 -11.133455 -11.820885][-10.161304 -10.363199 -9.3213758 -7.8834867 -7.225318 -6.1684856 -4.51844 -4.2859139 -4.6678419 -5.1074252 -6.3403358 -9.350812 -11.777816 -11.496634 -11.619131][-14.649212 -14.06962 -13.423268 -12.990614 -12.250108 -11.544483 -10.960714 -10.777775 -9.9378862 -9.8088951 -10.127384 -10.877831 -11.571216 -11.732302 -11.693143][-15.537012 -14.894643 -14.455367 -13.852622 -12.841949 -12.307695 -12.015675 -11.723866 -11.776649 -10.292301 -8.8604841 -10.19484 -10.712969 -9.2942133 -8.7251663][-14.233829 -13.254845 -12.813711 -12.220642 -11.084414 -11.422141 -10.594459 -10.24509 -10.659019 -10.318338 -10.050639 -8.88685 -8.0592289 -7.5790772 -6.4828787][-10.018475 -8.9728069 -8.4195089 -7.1565757 -6.5213332 -7.0810061 -6.4570789 -7.3892126 -7.6343532 -7.6246195 -8.3309517 -8.415226 -8.0287333 -6.7007389 -5.8198495]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 11:23:47.425803: step 6010, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 60h:49m:03s remains)
INFO - root - 2017-12-15 11:23:53.986987: step 6020, loss = 0.20, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 58h:35m:44s remains)
INFO - root - 2017-12-15 11:24:00.662824: step 6030, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.682 sec/batch; 61h:49m:43s remains)
INFO - root - 2017-12-15 11:24:07.253375: step 6040, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 58h:31m:42s remains)
INFO - root - 2017-12-15 11:24:13.874470: step 6050, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 60h:23m:54s remains)
INFO - root - 2017-12-15 11:24:20.390618: step 6060, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 59h:47m:51s remains)
INFO - root - 2017-12-15 11:24:26.910160: step 6070, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 58h:21m:30s remains)
INFO - root - 2017-12-15 11:24:33.499783: step 6080, loss = 0.29, batch loss = 0.24 (12.2 examples/sec; 0.653 sec/batch; 59h:13m:46s remains)
INFO - root - 2017-12-15 11:24:40.068538: step 6090, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 58h:45m:42s remains)
INFO - root - 2017-12-15 11:24:46.613222: step 6100, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 59h:24m:21s remains)
2017-12-15 11:24:47.113474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1955752 -1.28794 -1.2555857 -1.3365812 -2.0284026 -2.0035663 -1.7190022 -2.0838726 -2.3352256 -1.6934233 -1.2211928 -2.9683316 -5.0302987 -6.7450433 -7.6082826][-1.2063527 -1.6410871 -1.9135172 -2.2695365 -2.9608405 -3.5781302 -4.0435219 -3.9851718 -3.9150577 -3.0137515 -1.9759443 -3.5555282 -5.5381746 -7.3512526 -7.8489342][-0.070109367 -0.66313314 -1.4060426 -1.990262 -2.6996133 -3.2759972 -3.9783125 -4.21967 -4.0455155 -3.8287208 -3.6680551 -4.8291664 -6.3928041 -7.2155089 -6.7858348][-1.074224 -1.0737123 -0.83077574 -1.2390366 -1.6869795 -2.0915601 -2.2488081 -2.8693852 -3.5892706 -3.4391639 -3.3666902 -4.781662 -5.855042 -6.3736439 -6.315618][-2.6177976 -2.7035153 -2.3515599 -1.9050808 -1.3300543 -0.41453838 0.15024424 -1.0258374 -2.4100668 -2.8847702 -3.1090889 -4.1736217 -5.4191885 -6.5680141 -6.8209472][-5.06901 -4.6896482 -4.0694184 -2.5191035 -0.45398283 1.2536783 2.7802548 1.7397547 0.032328606 -1.4584875 -2.1267362 -2.8599432 -3.9371943 -4.7139759 -4.9101734][-6.9793177 -6.7960305 -5.6663647 -3.452462 -0.77127934 2.0092359 4.7690377 4.7123885 3.5719504 1.5374079 0.14588213 -0.90302992 -2.3570733 -3.1652358 -3.6856492][-7.262435 -7.3778963 -6.5793452 -4.4499764 -2.0219038 1.2741385 4.472507 4.3071113 3.8947873 2.8397059 1.7589188 0.28462934 -1.4078059 -2.4502947 -3.6211181][-6.1249228 -6.3805327 -6.1463437 -4.7411594 -2.4198015 0.66584635 2.7553144 2.8141866 2.9339576 2.1686988 2.0754881 0.46331835 -1.8845181 -3.5481615 -5.1556363][-6.0908155 -6.7100639 -6.3074651 -4.9216 -3.407944 -1.0196877 0.79387903 1.4285369 1.4753437 0.91609764 1.2430296 -0.17411947 -2.2023525 -3.8450913 -6.0447454][-9.7594776 -9.7404013 -9.3717651 -8.1301594 -6.6033959 -4.8636441 -3.1276343 -2.3986712 -2.6061523 -2.4734204 -2.0333779 -3.3526759 -4.0819025 -5.3910561 -7.2886009][-13.304749 -13.073809 -11.933143 -10.779108 -9.62558 -7.8486748 -6.5362473 -6.0375586 -6.2556615 -6.5873151 -6.2620869 -6.6522255 -6.6482263 -6.9019027 -7.9442444][-12.918057 -12.927801 -12.604419 -12.216448 -11.999155 -10.722244 -10.00839 -9.2649307 -8.5356312 -8.3994951 -7.9983473 -8.1967268 -7.7898159 -7.2383347 -6.7173615][-10.043263 -9.82744 -9.9494 -9.7507677 -9.75407 -9.6979713 -9.6177292 -8.8018684 -8.6511869 -9.0750465 -8.9687872 -8.5308638 -7.8255816 -7.4434924 -7.0201111][-6.8038 -7.3102264 -7.1132417 -6.3233376 -6.3010092 -6.4375396 -6.510613 -6.7413583 -7.5832787 -7.4560037 -7.5281358 -8.7404785 -9.1001167 -8.9358616 -8.6637154]]...]
INFO - root - 2017-12-15 11:24:53.694491: step 6110, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 60h:28m:15s remains)
INFO - root - 2017-12-15 11:25:00.299160: step 6120, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.675 sec/batch; 61h:11m:03s remains)
INFO - root - 2017-12-15 11:25:06.853079: step 6130, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 60h:22m:08s remains)
INFO - root - 2017-12-15 11:25:13.407661: step 6140, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 59h:29m:06s remains)
INFO - root - 2017-12-15 11:25:20.044416: step 6150, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 61h:50m:22s remains)
INFO - root - 2017-12-15 11:25:26.695797: step 6160, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 59h:48m:48s remains)
INFO - root - 2017-12-15 11:25:33.276850: step 6170, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 58h:25m:46s remains)
INFO - root - 2017-12-15 11:25:39.891391: step 6180, loss = 0.14, batch loss = 0.09 (11.4 examples/sec; 0.702 sec/batch; 63h:36m:21s remains)
INFO - root - 2017-12-15 11:25:46.411022: step 6190, loss = 0.23, batch loss = 0.18 (12.1 examples/sec; 0.659 sec/batch; 59h:41m:26s remains)
INFO - root - 2017-12-15 11:25:52.975882: step 6200, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 58h:54m:05s remains)
2017-12-15 11:25:53.551331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1195855 -5.7582784 -6.7346959 -6.3322339 -6.6880374 -7.1174378 -8.1135349 -9.2804546 -9.360815 -9.5623713 -9.5212708 -8.8642292 -9.9707565 -9.3097486 -7.518569][-5.795361 -6.6074986 -6.1609221 -5.4090843 -6.3444309 -7.2779994 -8.3640709 -9.2484379 -10.453505 -11.132117 -10.836892 -8.6083851 -8.6748667 -8.7459784 -6.7170019][-4.9259672 -5.9900527 -6.6780081 -6.0647964 -7.3324027 -7.4922085 -7.9890862 -8.6850033 -9.5759659 -10.192177 -10.200649 -8.1112595 -7.6271048 -8.0497532 -6.3543992][-6.08573 -6.0241327 -6.4780731 -7.2078018 -8.1824436 -6.9985876 -7.146987 -8.9755659 -9.626915 -8.4889393 -7.8162346 -6.5353394 -7.7465 -7.7354417 -5.5288711][-8.7423992 -11.146135 -11.958202 -10.118858 -9.4678516 -6.5088444 -3.7861829 -5.5455513 -8.5806761 -8.08673 -6.8083091 -4.7905078 -7.3162808 -8.8463955 -7.0323281][-10.17618 -11.742269 -12.1236 -10.398968 -7.2405095 -3.1691895 0.1481595 -0.55012751 -1.5028901 -3.5782156 -6.5722241 -4.462183 -6.265831 -8.3390989 -7.5387354][-11.134462 -12.280628 -10.207602 -7.2322659 -5.4382582 0.66559696 7.9410253 7.7023129 5.7507653 0.57727003 -5.3491707 -5.0800519 -7.8702188 -8.8984394 -8.1587849][-11.363234 -12.324806 -11.045246 -6.5535021 -2.7261519 2.6665463 9.3893032 10.024141 9.8829536 3.5166039 -2.8427689 -4.5728674 -9.7842646 -10.729929 -8.2164459][-11.369049 -10.68817 -9.5561295 -6.3121295 -2.3170614 2.9895768 7.3415704 8.4916077 8.9716625 2.4684329 -4.3298531 -7.1869626 -11.78088 -11.843033 -9.1975727][-10.109028 -9.9986458 -10.286782 -7.8704023 -6.0654249 -1.5718222 3.4433541 4.3165574 3.0300326 -1.6808677 -6.15902 -7.2965231 -11.399893 -14.14786 -12.963708][-11.217147 -12.49114 -11.910582 -9.43232 -8.90471 -7.0204954 -4.9358625 -3.8479664 -3.5758736 -6.3136272 -8.9220524 -9.7506771 -12.636886 -13.2906 -11.967001][-16.509565 -16.616003 -15.281857 -14.081604 -13.672522 -12.341003 -12.603004 -11.562925 -10.13229 -11.457569 -13.345282 -12.1932 -12.636255 -13.33563 -11.85438][-14.815569 -15.979963 -15.754358 -14.135891 -12.687894 -11.740078 -11.624802 -11.374253 -11.041842 -11.524802 -11.290024 -10.278061 -11.154907 -11.236696 -9.460969][-12.509914 -13.149641 -13.51066 -11.93375 -11.47316 -10.956171 -10.810315 -10.363253 -10.053479 -9.6593361 -9.1823883 -8.7832546 -9.5428429 -8.6583357 -7.0006366][-9.5536356 -9.4535313 -7.94703 -7.7556033 -8.2035971 -7.5727482 -7.21457 -7.0686779 -7.6682868 -7.6102743 -7.5051827 -6.9758663 -7.4687934 -7.8462205 -8.6055822]]...]
INFO - root - 2017-12-15 11:26:00.141173: step 6210, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.635 sec/batch; 57h:31m:46s remains)
INFO - root - 2017-12-15 11:26:06.760466: step 6220, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 60h:16m:18s remains)
INFO - root - 2017-12-15 11:26:13.279365: step 6230, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 58h:30m:12s remains)
INFO - root - 2017-12-15 11:26:19.822589: step 6240, loss = 0.18, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 57h:24m:49s remains)
INFO - root - 2017-12-15 11:26:26.363754: step 6250, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 59h:13m:42s remains)
INFO - root - 2017-12-15 11:26:33.029947: step 6260, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 59h:53m:16s remains)
INFO - root - 2017-12-15 11:26:39.657729: step 6270, loss = 0.24, batch loss = 0.19 (11.9 examples/sec; 0.671 sec/batch; 60h:48m:08s remains)
INFO - root - 2017-12-15 11:26:46.303572: step 6280, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.671 sec/batch; 60h:47m:23s remains)
INFO - root - 2017-12-15 11:26:52.907132: step 6290, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 60h:15m:24s remains)
INFO - root - 2017-12-15 11:26:59.554461: step 6300, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 59h:27m:33s remains)
2017-12-15 11:27:00.073887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4396133 -5.34464 -4.6017685 -2.0372508 -0.18384695 -1.2910032 -2.9263003 -3.935904 -4.7955527 -5.8421469 -6.0209413 -6.3036714 -8.067194 -9.195612 -7.9136248][-4.2081351 -3.7776096 -4.200542 -3.7322631 -2.777513 -2.1502788 -0.97724152 -1.8517942 -3.2007394 -5.4510231 -8.6289892 -9.7632961 -9.3912306 -9.941925 -8.6570063][-2.7631717 -3.507019 -4.6591344 -3.313956 -3.3967149 -2.6001246 -1.6969883 -2.3182106 -2.8574395 -4.00927 -5.6417646 -7.5494351 -8.98736 -8.9115858 -7.8702736][-3.9847107 -4.90923 -5.9452157 -5.9390011 -6.5275803 -4.486928 -1.6781647 0.1358161 0.056526184 -3.2228742 -7.4320154 -9.9494038 -10.307798 -9.8018169 -7.6874056][-5.33963 -5.7582059 -6.5404892 -6.2279172 -5.5267067 -2.4380593 -1.27776 -1.6608624 -1.7909403 -2.5585606 -4.064723 -6.9678907 -10.076809 -11.055571 -9.3472118][-7.4476557 -6.3204327 -5.8903484 -5.0626378 -3.1661074 1.4485416 5.7247949 5.3346848 2.3901687 -1.6245012 -4.9403462 -6.7181768 -8.8647308 -10.674397 -10.131557][-8.1227436 -7.8228498 -7.4638224 -5.6880569 -2.4808536 2.6491199 6.4717278 7.6057544 6.3172193 0.94616747 -3.7806532 -4.2923088 -7.067317 -9.3963547 -9.8918114][-8.81357 -9.5436306 -9.2839432 -5.7157331 -2.4469655 2.2934937 7.1952262 7.8468823 7.1574864 2.5051208 -3.222497 -6.064 -7.9067492 -8.3821917 -8.5906239][-7.4055357 -9.9931965 -9.6096973 -6.869854 -4.5247221 1.1743054 4.8904462 5.8590751 5.555222 0.085449696 -3.0041611 -6.5535941 -10.898672 -11.949172 -10.507973][-6.9171443 -7.7196379 -7.3622751 -6.0857511 -5.0756793 -1.4519205 1.6014237 3.4940238 2.7244058 -2.1629729 -6.5980339 -10.811495 -13.626314 -14.43013 -13.322908][-9.4147491 -10.64295 -9.8510628 -7.600821 -6.3700147 -4.8525662 -4.5171714 -3.5965011 -4.8770747 -7.6385665 -9.7697163 -12.274683 -15.201355 -16.385515 -14.480021][-12.955711 -12.995968 -12.843025 -10.770746 -7.8612375 -5.8474479 -5.9193964 -6.2767658 -8.0428028 -10.45031 -12.673024 -13.676219 -13.832109 -13.261621 -12.615086][-14.086014 -12.936758 -11.388335 -10.527797 -9.9933605 -8.7820454 -7.5533047 -7.8860769 -8.9197884 -9.1565666 -9.6360083 -10.621253 -11.577035 -10.964855 -9.1687212][-11.022602 -9.3414907 -8.8249464 -9.0697126 -9.2861176 -8.6724033 -7.9437256 -8.0814476 -8.4283791 -9.3012123 -9.3259315 -7.191864 -6.9692507 -6.5644903 -6.4588594][-6.6964955 -5.7824726 -4.5559316 -4.2702184 -5.4137959 -7.3594146 -8.0729294 -6.8599358 -6.2139707 -6.467442 -6.2450318 -6.7905021 -7.3201261 -6.4037704 -6.1217294]]...]
INFO - root - 2017-12-15 11:27:06.653246: step 6310, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 61h:05m:53s remains)
INFO - root - 2017-12-15 11:27:13.280151: step 6320, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 58h:51m:10s remains)
INFO - root - 2017-12-15 11:27:19.812111: step 6330, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 58h:31m:39s remains)
INFO - root - 2017-12-15 11:27:26.441178: step 6340, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 60h:38m:17s remains)
INFO - root - 2017-12-15 11:27:32.991385: step 6350, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 58h:46m:13s remains)
INFO - root - 2017-12-15 11:27:39.571826: step 6360, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 57h:56m:39s remains)
INFO - root - 2017-12-15 11:27:46.156824: step 6370, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 58h:19m:24s remains)
INFO - root - 2017-12-15 11:27:52.773104: step 6380, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 59h:25m:13s remains)
INFO - root - 2017-12-15 11:27:59.419875: step 6390, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.683 sec/batch; 61h:49m:48s remains)
INFO - root - 2017-12-15 11:28:05.978804: step 6400, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 61h:15m:21s remains)
2017-12-15 11:28:06.474779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8403931 -3.1686895 -1.9637868 -1.5440607 -1.7218866 -1.6623802 -1.6000843 -1.1177902 -1.0562248 -1.8844585 -3.3816469 -7.6166558 -10.89293 -10.987569 -9.3354588][-4.5942616 -4.8609753 -5.2005692 -4.1923451 -3.7882133 -3.783145 -3.4592729 -3.0928192 -3.3401942 -4.2928486 -5.4050088 -9.5825291 -12.30836 -12.120228 -9.7708521][-4.9214187 -5.0363126 -5.1725183 -4.2350678 -4.4618545 -5.0830622 -4.9480534 -3.9503469 -3.4320121 -4.289577 -6.211062 -10.186232 -12.596357 -12.18614 -9.48591][-6.5825195 -6.2838421 -5.4426503 -4.2834311 -4.6546559 -4.170105 -3.8045411 -4.4172707 -4.9576807 -5.00916 -5.2182956 -9.4148178 -12.44886 -11.444242 -8.7090454][-7.3501949 -7.8422647 -7.4827738 -5.1718459 -3.6566699 -1.4400167 -0.96976805 -2.7314906 -5.0261574 -5.4022622 -5.6192651 -8.6421566 -10.967327 -11.823523 -9.8394127][-8.8613367 -8.2004509 -6.5325885 -3.6285446 -1.1067648 1.5859766 3.2278843 2.4904356 0.081846237 -3.3221698 -6.0702052 -8.540514 -10.831717 -11.294146 -10.206688][-10.176659 -8.9487715 -7.0779018 -3.3234847 0.82994223 4.0204563 6.2427559 5.3917913 3.0965023 -0.9668498 -4.3694749 -7.7723255 -10.832358 -11.23506 -10.082945][-10.02568 -8.1602659 -5.9592414 -2.1250243 1.4936266 5.2918587 8.3390474 7.5264921 5.1454334 1.0785866 -2.6471448 -7.5322208 -10.62339 -11.055037 -10.343282][-7.9471 -6.9779577 -5.9836173 -2.3235312 1.226449 4.2857113 6.4172659 6.7314262 5.592555 1.2160406 -2.1489739 -7.2636342 -10.810072 -11.094749 -9.4083767][-7.0071626 -5.8021803 -4.4896622 -2.5503061 -0.91116095 0.8171649 2.4116745 3.1842594 2.3169489 0.1268239 -2.2344003 -7.1930923 -10.577378 -11.546795 -10.300615][-8.2257385 -7.761405 -6.6041017 -4.7026339 -4.1635103 -3.3615053 -2.2880878 -1.8049226 -2.7562983 -3.4759769 -4.9244061 -9.6734457 -11.582542 -10.585734 -9.5852776][-11.826569 -11.079699 -8.8482714 -6.7626753 -6.3667421 -6.3906336 -6.7973032 -7.0212226 -7.1494265 -7.4340057 -8.1043282 -10.013121 -10.571617 -10.229252 -9.7037048][-12.133316 -10.950167 -8.9770985 -8.0828257 -7.666533 -7.097456 -8.0064154 -8.9874344 -9.6521416 -9.5217991 -9.3036852 -10.120228 -10.262152 -8.6664991 -7.0119028][-10.119196 -8.8380814 -7.6955881 -6.7080936 -5.9311957 -6.6301117 -7.799005 -8.3215427 -8.559166 -9.1631088 -9.531888 -8.6691332 -8.6180277 -7.8292322 -7.4449873][-6.5954003 -5.8957229 -4.963161 -4.3398886 -4.7040586 -5.266345 -6.0441337 -7.1518412 -7.835146 -7.3015909 -6.9973927 -8.370101 -8.68237 -7.9230356 -8.3702631]]...]
INFO - root - 2017-12-15 11:28:12.960608: step 6410, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 59h:14m:49s remains)
INFO - root - 2017-12-15 11:28:19.509266: step 6420, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 58h:01m:08s remains)
INFO - root - 2017-12-15 11:28:26.100384: step 6430, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 60h:21m:51s remains)
INFO - root - 2017-12-15 11:28:32.745763: step 6440, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 60h:24m:34s remains)
INFO - root - 2017-12-15 11:28:39.345304: step 6450, loss = 0.11, batch loss = 0.07 (11.9 examples/sec; 0.672 sec/batch; 60h:53m:45s remains)
INFO - root - 2017-12-15 11:28:45.894630: step 6460, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 60h:48m:25s remains)
INFO - root - 2017-12-15 11:28:52.477783: step 6470, loss = 0.19, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 58h:22m:40s remains)
INFO - root - 2017-12-15 11:28:58.977064: step 6480, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 58h:29m:44s remains)
INFO - root - 2017-12-15 11:29:05.595159: step 6490, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 58h:29m:26s remains)
INFO - root - 2017-12-15 11:29:12.163900: step 6500, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 58h:28m:41s remains)
2017-12-15 11:29:12.668047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6541066 -4.8149118 -4.73763 -4.1932659 -4.4135885 -4.6349611 -5.4369125 -4.9681497 -3.6354024 -2.4688053 -1.5577402 -1.6398921 -2.9454796 -2.4444401 -1.6951897][-3.8586848 -3.294368 -2.2318516 -2.2473638 -3.38831 -3.0424292 -2.6022236 -2.6264005 -2.3503191 -1.335887 -0.12049675 -0.43712139 -2.2282345 -2.4600086 -2.9429584][-1.8718936 -2.3148086 -3.4016573 -2.0966759 -1.4948339 -1.7633438 -1.7813766 -1.0382571 -0.19657612 0.38748837 0.81729412 -0.59197092 -2.7917359 -3.9828272 -5.1617146][-3.7087345 -3.5750997 -2.9181809 -2.1736398 -2.4444532 -1.5603056 -1.1009593 -0.83081722 -0.62671804 -0.17875767 0.42451382 -0.68254375 -3.2442551 -4.9416833 -5.6819758][-4.4675646 -4.3074412 -3.9857514 -2.1493454 -1.1038814 0.11361551 0.46798992 -0.0998826 -1.1074657 -0.94278049 -0.57415247 -1.4225254 -3.1541724 -4.6530819 -5.5835686][-5.6453524 -4.6331525 -3.2292717 -0.50952768 1.1009264 2.7182474 3.3486547 2.7439833 2.0147042 0.84506083 -0.4812851 -0.91546583 -2.4231339 -3.4470243 -3.7478585][-5.8774023 -4.0858579 -1.2236643 0.67268372 1.9158821 4.1477785 5.2329092 4.6939526 3.8941889 2.4440193 0.94050407 -0.25261831 -2.5285175 -3.8351455 -4.3627481][-5.8254848 -4.4828939 -2.550632 0.98954725 2.4603634 3.6478338 4.4464951 4.1201649 3.4439735 2.2555385 0.8626895 -0.5356164 -2.8590479 -3.9221861 -4.5586629][-5.6263514 -4.2256866 -2.6802773 -0.036091805 1.2021437 2.5750113 3.3317099 3.3504429 2.9119577 1.1290703 -0.60764742 -2.4137208 -5.4188514 -6.4982476 -6.2998476][-5.6819429 -4.72657 -4.1349831 -1.9069521 -0.66222668 0.81150579 1.8951731 2.11416 1.7576656 -0.1387949 -2.2609558 -4.12997 -6.8438678 -8.1400194 -8.6173267][-8.2605047 -7.3949203 -6.1921163 -4.6753578 -4.7138953 -4.4050879 -3.5749373 -2.6044886 -2.0155318 -2.9829752 -4.4543514 -6.6492662 -8.5526247 -8.5409889 -7.6631155][-11.046312 -10.003601 -8.3856792 -7.5039759 -7.2410417 -6.7474246 -6.763062 -6.1778083 -5.6981678 -6.4362946 -7.4492016 -8.0549545 -8.38213 -7.6275911 -6.8882251][-9.4689293 -8.2796955 -6.7953148 -6.3540626 -7.0355797 -6.96737 -6.8679447 -6.8418484 -7.3665972 -7.1601515 -7.1484423 -7.4180689 -7.9827309 -6.4589086 -5.2639513][-7.6401353 -6.591126 -5.3259788 -4.155592 -3.674896 -4.3452396 -5.1825137 -4.8489132 -4.7682991 -6.0447445 -7.4174061 -6.5981426 -6.1950088 -5.57924 -4.4392061][-5.9466162 -4.6288223 -3.0740993 -1.8372369 -1.4638319 -1.093843 -1.1523185 -2.1498311 -3.0107908 -2.9600298 -2.9553585 -3.9039443 -5.1293721 -5.9271073 -5.955349]]...]
INFO - root - 2017-12-15 11:29:19.246948: step 6510, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 58h:21m:33s remains)
INFO - root - 2017-12-15 11:29:25.868765: step 6520, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 59h:14m:11s remains)
INFO - root - 2017-12-15 11:29:32.481981: step 6530, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.671 sec/batch; 60h:43m:52s remains)
INFO - root - 2017-12-15 11:29:39.119686: step 6540, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.686 sec/batch; 62h:04m:11s remains)
INFO - root - 2017-12-15 11:29:45.666730: step 6550, loss = 0.20, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 57h:39m:55s remains)
INFO - root - 2017-12-15 11:29:52.253591: step 6560, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 60h:08m:17s remains)
INFO - root - 2017-12-15 11:29:58.867997: step 6570, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 60h:53m:16s remains)
INFO - root - 2017-12-15 11:30:05.481239: step 6580, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 59h:18m:02s remains)
INFO - root - 2017-12-15 11:30:12.057619: step 6590, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 58h:23m:15s remains)
INFO - root - 2017-12-15 11:30:18.561175: step 6600, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 59h:25m:42s remains)
2017-12-15 11:30:19.027760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0017314 -7.0290771 -8.3895407 -9.93405 -11.241524 -11.885168 -11.337469 -9.241437 -8.92773 -8.81416 -8.5670166 -10.265074 -14.44651 -13.716904 -12.799997][-7.50394 -6.8816714 -7.2019029 -8.3264427 -10.192519 -12.16954 -13.297838 -13.322155 -13.188574 -11.858203 -11.244057 -11.976086 -14.273493 -13.503275 -13.49785][-4.7775288 -5.6166868 -6.542098 -6.5798326 -8.5038214 -11.024858 -12.541828 -13.361206 -13.530933 -12.805657 -12.580403 -12.636532 -14.778616 -13.035233 -12.119421][-3.3504405 -4.4335413 -4.956027 -5.3111563 -6.4166479 -6.8092256 -7.23575 -9.3935289 -10.945405 -10.910685 -11.45169 -11.901972 -14.647837 -13.469006 -12.520952][-5.2783346 -5.3703251 -5.5612249 -5.6408486 -4.7747936 -3.3242693 -2.4584577 -4.6040096 -6.0178652 -7.3678346 -8.7355413 -9.5377445 -13.275643 -12.333761 -11.251734][-9.09704 -8.1396332 -6.840364 -6.2048116 -3.9426789 -0.57253981 3.6090455 3.4713969 1.8811297 -1.8823593 -5.9460645 -6.2827291 -10.115894 -10.318363 -10.138254][-12.679015 -10.946947 -8.0595493 -5.7620311 -2.8890679 1.4078016 6.8052182 9.0291119 8.3224068 2.6616063 -3.0615671 -4.8827019 -9.8828869 -8.9722862 -8.6832027][-10.933629 -10.906496 -9.5435181 -6.4676118 -3.3581765 1.281899 6.091331 9.0118256 9.1184082 4.7517591 -1.0683661 -5.2508292 -12.021126 -10.441097 -9.0021515][-7.1987305 -7.638176 -6.6474938 -4.869422 -3.5682843 -0.060228825 3.0416608 6.0842729 7.0340819 3.4342141 -1.6974878 -8.0278654 -14.507811 -13.817884 -13.170617][-5.3995204 -4.840363 -3.9185469 -3.2164278 -2.9967859 -2.9895411 -1.0287991 1.1408415 2.2697425 0.55952692 -3.9895158 -10.413303 -17.020086 -18.015875 -17.487026][-8.3438492 -7.8374953 -6.6531653 -6.9881358 -7.2075629 -7.7841096 -6.8416839 -6.5010939 -6.4762788 -6.9260554 -9.3166695 -13.126316 -17.671682 -19.159122 -19.233698][-12.985476 -11.544285 -10.354133 -9.681839 -9.9693975 -10.187042 -10.491196 -11.363064 -11.965839 -11.781763 -12.756292 -13.512636 -15.623667 -15.952236 -15.543484][-14.606253 -13.142099 -12.433292 -12.434708 -12.842051 -11.674425 -11.833633 -12.547112 -13.18515 -13.193169 -13.682848 -13.895653 -14.175749 -12.998049 -10.836763][-12.369178 -11.806693 -10.949292 -10.016138 -10.236651 -10.689365 -10.666191 -10.330795 -11.307904 -11.684338 -11.528433 -11.417896 -10.061226 -9.1442223 -7.4866819][-7.973321 -7.3075833 -6.271389 -6.0396357 -5.804996 -5.8937597 -6.385788 -6.9281087 -7.584166 -8.2229691 -9.2479382 -10.064003 -10.008661 -8.6978693 -7.1327224]]...]
INFO - root - 2017-12-15 11:30:25.633935: step 6610, loss = 0.20, batch loss = 0.16 (11.8 examples/sec; 0.679 sec/batch; 61h:29m:06s remains)
INFO - root - 2017-12-15 11:30:32.263115: step 6620, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 58h:23m:26s remains)
INFO - root - 2017-12-15 11:30:38.978875: step 6630, loss = 0.25, batch loss = 0.20 (11.8 examples/sec; 0.676 sec/batch; 61h:09m:04s remains)
INFO - root - 2017-12-15 11:30:45.628244: step 6640, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.671 sec/batch; 60h:46m:53s remains)
INFO - root - 2017-12-15 11:30:52.246091: step 6650, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 59h:04m:01s remains)
INFO - root - 2017-12-15 11:30:58.859928: step 6660, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 59h:18m:34s remains)
INFO - root - 2017-12-15 11:31:05.551122: step 6670, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 60h:56m:05s remains)
INFO - root - 2017-12-15 11:31:12.143286: step 6680, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 59h:37m:08s remains)
INFO - root - 2017-12-15 11:31:18.645651: step 6690, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 60h:18m:28s remains)
INFO - root - 2017-12-15 11:31:25.198575: step 6700, loss = 0.21, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 58h:39m:57s remains)
2017-12-15 11:31:25.777168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4877744 -6.1136808 -4.244638 -3.592418 -4.2710152 -4.7416778 -4.8465796 -4.2397938 -3.5457878 -3.5952837 -3.011703 -4.543704 -6.1759772 -5.947464 -5.7022195][-8.3385849 -6.056632 -4.2436004 -3.746789 -4.3923163 -5.5743461 -6.2699528 -5.8334837 -5.4189591 -5.2354369 -4.7167964 -6.1258373 -7.6036525 -6.6107235 -5.9372778][-6.5195379 -5.1162047 -4.3708563 -3.4307423 -4.1227818 -5.8121319 -6.764524 -6.30796 -5.4602704 -5.6548848 -6.0801096 -7.5451717 -8.617157 -7.4079871 -6.4980025][-6.384963 -5.0301003 -3.8549564 -3.2006297 -4.3107243 -4.8135304 -4.5547872 -4.3315706 -4.7951651 -5.5038004 -6.3505659 -8.752593 -10.427844 -9.350606 -7.7851667][-6.4396782 -5.592597 -4.48506 -3.3758295 -3.2509148 -2.8803508 -2.4629662 -2.3617415 -1.9623656 -2.8800769 -5.327992 -8.5877247 -10.733375 -9.9513168 -9.0791035][-8.6917572 -7.4052911 -5.5227122 -3.7155745 -2.437372 -0.18148184 1.0513597 1.4395308 1.7994556 0.41375923 -1.9265685 -5.9862967 -9.9242706 -9.85562 -9.2421484][-9.8070316 -8.4360809 -6.4710574 -3.4894423 -0.927341 2.0452037 4.1441326 4.890295 4.8646431 2.742135 0.27721739 -4.2264 -8.675066 -9.2339783 -8.8721046][-8.3132591 -6.8772216 -4.8969274 -1.2019982 1.4033966 4.4641161 6.9248314 7.7034483 7.2729912 4.7585554 2.1030393 -3.236644 -7.3707042 -7.3332243 -6.765758][-7.0014071 -6.2310638 -4.7585812 -2.0823958 0.44534779 3.5834541 6.0760679 6.6604862 5.624032 3.9964266 2.3813047 -2.8400922 -7.2705622 -6.9243674 -5.0122824][-6.3806305 -5.7814393 -4.7074919 -2.2460241 -0.37019539 1.7827544 3.8499479 4.2706289 3.2494087 1.82938 0.1194849 -3.3929675 -6.2982125 -5.7222042 -4.8602571][-9.14068 -7.6429362 -5.1041965 -2.6273103 -1.2534251 -0.0076746941 0.574512 0.55642128 0.19497013 -0.60529852 -1.3345041 -4.0872221 -5.6060152 -5.0059481 -3.7366776][-11.97615 -9.7092667 -7.2775526 -5.0582881 -3.86271 -3.188303 -3.0105848 -3.23318 -4.4751344 -4.8879929 -4.2630024 -4.9477892 -5.0996594 -4.7056322 -3.5131152][-10.734175 -8.3294125 -6.3044786 -4.796061 -3.974628 -3.6661606 -3.7270813 -4.2253418 -5.2973013 -5.24602 -4.5728407 -4.38443 -3.9351907 -3.1790962 -2.3425672][-8.5097866 -7.0928497 -5.860692 -4.2626276 -3.2649515 -3.8305514 -4.3209133 -4.1215568 -4.5803165 -5.0677319 -4.7286248 -3.8453515 -3.4444389 -2.2268085 -1.6434627][-5.1783137 -4.625052 -3.1058028 -2.2351718 -1.9604514 -1.9901702 -1.9988446 -3.2700727 -4.8877711 -5.0751352 -4.634531 -4.375123 -4.3566751 -3.634783 -3.8765306]]...]
INFO - root - 2017-12-15 11:31:32.252537: step 6710, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 59h:23m:51s remains)
INFO - root - 2017-12-15 11:31:38.869209: step 6720, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 59h:03m:10s remains)
INFO - root - 2017-12-15 11:31:45.444693: step 6730, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 59h:29m:46s remains)
INFO - root - 2017-12-15 11:31:52.080662: step 6740, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 61h:11m:51s remains)
INFO - root - 2017-12-15 11:31:58.643818: step 6750, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 57h:49m:00s remains)
INFO - root - 2017-12-15 11:32:05.259836: step 6760, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 59h:23m:16s remains)
INFO - root - 2017-12-15 11:32:11.983243: step 6770, loss = 0.22, batch loss = 0.17 (11.4 examples/sec; 0.704 sec/batch; 63h:43m:33s remains)
INFO - root - 2017-12-15 11:32:18.589699: step 6780, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 59h:45m:37s remains)
INFO - root - 2017-12-15 11:32:25.251608: step 6790, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 61h:21m:42s remains)
INFO - root - 2017-12-15 11:32:31.808461: step 6800, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 60h:21m:48s remains)
2017-12-15 11:32:32.286460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4098825 -6.3875666 -7.0303946 -7.6397958 -8.0468893 -9.224081 -10.412017 -10.267094 -10.025436 -9.4974661 -8.8776569 -10.415277 -11.797197 -12.362823 -12.715246][-8.0556736 -8.7959862 -8.5084591 -8.97477 -9.1841755 -10.0006 -10.942348 -12.250199 -12.932825 -11.755154 -10.802987 -12.331367 -13.189421 -13.138557 -12.584467][-5.0911708 -7.6814575 -9.7121735 -9.3605928 -9.0303116 -10.272404 -11.653776 -11.843898 -11.261686 -12.00951 -12.059074 -12.340997 -12.669601 -13.537228 -13.269707][-8.1327572 -9.7336063 -10.23219 -9.2450342 -8.973278 -8.5970812 -8.388504 -10.806717 -12.208058 -10.386961 -8.841898 -12.238119 -14.036693 -13.793886 -12.564309][-9.5073977 -12.007826 -13.326826 -11.850831 -8.9950228 -3.7968454 -1.6283331 -5.9242229 -9.55771 -10.186589 -10.429003 -10.810827 -11.180902 -13.423492 -14.232338][-11.678842 -13.254848 -13.394427 -10.943266 -7.7972584 -2.01235 3.2472382 2.6277227 0.10596466 -5.4071465 -10.185659 -10.483355 -11.03693 -12.599876 -13.026888][-14.728598 -15.098206 -12.897501 -8.0862846 -3.424257 2.0134277 6.2892218 5.9048958 5.235549 -0.68854237 -6.8211584 -9.67667 -11.873464 -11.858074 -11.857251][-13.023178 -13.904779 -12.956491 -7.3143582 -0.54181814 6.8182192 11.16836 8.1545286 5.4345045 0.47653008 -4.9793978 -8.8145924 -11.132607 -12.613991 -14.023767][-9.87075 -10.255515 -10.930891 -7.5130172 -2.6019275 3.9561014 9.0152016 8.7434206 6.096633 -1.0133834 -6.4438472 -10.160719 -13.625212 -14.277579 -13.892555][-7.5984011 -7.9782162 -8.54362 -6.5091009 -3.5817313 0.31563187 3.7637148 3.9168081 2.2595353 -2.243422 -7.0570273 -11.397245 -14.020855 -15.154073 -16.09322][-9.54249 -11.158415 -12.324471 -10.827631 -8.5936308 -5.5905328 -2.8294616 -3.0378323 -4.2379284 -6.3400459 -9.29258 -14.067166 -16.349459 -15.789766 -15.074268][-15.283866 -15.14878 -15.373186 -14.604585 -13.830806 -11.6915 -10.218657 -10.590527 -10.86183 -11.479847 -12.597137 -14.473512 -15.321771 -15.553852 -14.622098][-15.58261 -13.994966 -13.679711 -15.279181 -15.529175 -13.245527 -12.272034 -12.553789 -13.185572 -13.223873 -13.057012 -13.708863 -14.035228 -13.257391 -12.049686][-13.465075 -13.392834 -11.893953 -10.519821 -10.589406 -11.841406 -12.617022 -11.836426 -11.291344 -11.880842 -12.434916 -12.032557 -11.43431 -10.648298 -9.741703][-9.8527956 -9.3703308 -7.9255581 -6.6424532 -6.1741714 -6.0984755 -6.5473309 -8.400631 -9.6437006 -9.094243 -8.7476187 -10.133471 -11.556444 -10.775764 -10.372579]]...]
INFO - root - 2017-12-15 11:32:38.921288: step 6810, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 59h:29m:25s remains)
INFO - root - 2017-12-15 11:32:45.494279: step 6820, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 57h:51m:25s remains)
INFO - root - 2017-12-15 11:32:52.090805: step 6830, loss = 0.16, batch loss = 0.12 (11.5 examples/sec; 0.698 sec/batch; 63h:08m:51s remains)
INFO - root - 2017-12-15 11:32:58.660810: step 6840, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.673 sec/batch; 60h:52m:34s remains)
INFO - root - 2017-12-15 11:33:05.211610: step 6850, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.689 sec/batch; 62h:20m:42s remains)
INFO - root - 2017-12-15 11:33:11.819333: step 6860, loss = 0.24, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 58h:43m:17s remains)
INFO - root - 2017-12-15 11:33:18.456151: step 6870, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.658 sec/batch; 59h:33m:45s remains)
INFO - root - 2017-12-15 11:33:25.011266: step 6880, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 58h:06m:43s remains)
INFO - root - 2017-12-15 11:33:31.564765: step 6890, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.684 sec/batch; 61h:51m:56s remains)
INFO - root - 2017-12-15 11:33:38.211054: step 6900, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.684 sec/batch; 61h:52m:16s remains)
2017-12-15 11:33:38.744489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4985185 -4.6408129 -4.7769814 -4.5762749 -4.8186183 -4.6317406 -3.6114578 -3.1587903 -2.7329025 -2.9855828 -3.5835624 -3.2558727 -4.0579863 -4.6616726 -5.4127932][-2.4608183 -3.5219848 -3.9283988 -3.131784 -2.6454661 -2.3347294 -1.7147198 -2.0550199 -1.9327712 -2.5760732 -3.5275531 -2.5935194 -3.706306 -4.5474024 -5.3882031][-2.5293555 -3.7375107 -3.2759564 -2.6694298 -2.6101253 -1.0130906 -0.64169168 -1.0380578 -1.3216543 -2.4559927 -2.5466495 -2.2041185 -3.6354926 -4.031354 -5.2989459][-3.0411546 -4.5825958 -5.16245 -4.6019464 -3.2749612 -1.6564846 -0.94266319 -0.98209953 -1.2391644 -1.442379 -1.3470793 -1.8185058 -4.1809797 -5.7511249 -7.4683704][-3.8545802 -5.89667 -6.6517534 -6.6738157 -5.14711 -1.8908961 -0.6873765 -0.71264172 -1.0194688 -0.60855865 -0.04288578 -1.0701957 -4.2205043 -7.158803 -9.8557835][-5.450129 -5.6998506 -5.3599019 -4.5279675 -2.4123974 0.3538084 2.2875977 1.5394363 0.23297262 0.57259369 0.30867863 -0.29134989 -3.53269 -7.0893497 -9.82552][-5.6742177 -5.2783751 -4.2207747 -2.9528894 -0.87669373 2.5601187 5.3258057 5.9302158 4.7341413 1.9663653 -0.64152718 -0.91658258 -3.8936639 -7.2328434 -9.8823948][-5.5198712 -5.308979 -4.7425041 -2.50335 -0.30778265 2.1614432 4.9791574 6.4801416 6.1589584 3.3825502 0.38834715 -1.2406683 -4.8941889 -7.2659912 -9.2799273][-4.399704 -4.1590075 -4.0585456 -2.2766638 -0.71490479 1.9156747 3.9570227 4.3114748 3.1269884 0.70134687 -1.0891323 -2.8101089 -6.4423475 -8.3350534 -9.5149851][-3.3792186 -3.9737592 -3.4180133 -2.8507266 -1.6243978 -0.6700139 1.1332116 2.0428562 0.50273418 -1.3633556 -2.8874853 -4.388689 -7.708611 -9.6226816 -11.119897][-6.8788471 -6.6557975 -6.7866907 -6.0007825 -4.7371688 -5.612762 -4.6507812 -3.7772813 -4.2553053 -4.2862554 -5.6978421 -6.9639411 -9.5365877 -11.100195 -12.196543][-10.358423 -10.700274 -10.222979 -9.9194889 -10.094245 -9.960146 -9.6431351 -9.86026 -9.507988 -8.5682068 -8.8797 -8.7607288 -9.9842253 -11.321419 -12.52323][-12.855898 -12.96381 -12.276084 -11.930363 -12.417963 -12.330275 -12.204453 -12.266142 -11.633732 -10.256025 -9.6565418 -9.6458864 -10.518035 -9.9932613 -9.648757][-12.001255 -12.101919 -12.218665 -11.436128 -11.141794 -10.916752 -11.792591 -11.891876 -11.467068 -10.864304 -9.9613438 -8.5721426 -8.2979984 -7.8640175 -7.0913515][-7.900795 -8.2336388 -8.4074554 -8.1534023 -7.8368535 -7.3144855 -7.9048147 -8.0464592 -8.5463285 -8.8489332 -8.7520771 -8.35258 -8.43018 -7.6200542 -7.3610382]]...]
INFO - root - 2017-12-15 11:33:45.316715: step 6910, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.684 sec/batch; 61h:51m:17s remains)
INFO - root - 2017-12-15 11:33:51.929446: step 6920, loss = 0.22, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 58h:00m:48s remains)
INFO - root - 2017-12-15 11:33:58.496627: step 6930, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.648 sec/batch; 58h:33m:37s remains)
INFO - root - 2017-12-15 11:34:05.064076: step 6940, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 57h:41m:28s remains)
INFO - root - 2017-12-15 11:34:11.797311: step 6950, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 61h:11m:05s remains)
INFO - root - 2017-12-15 11:34:18.387227: step 6960, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 59h:29m:20s remains)
INFO - root - 2017-12-15 11:34:25.116120: step 6970, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 60h:54m:32s remains)
INFO - root - 2017-12-15 11:34:31.706131: step 6980, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 61h:06m:42s remains)
INFO - root - 2017-12-15 11:34:38.313190: step 6990, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 59h:38m:21s remains)
INFO - root - 2017-12-15 11:34:44.869372: step 7000, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 59h:21m:40s remains)
2017-12-15 11:34:45.375481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.0212164 -9.1707668 -9.6406727 -9.8641853 -11.363269 -12.020018 -12.490195 -11.440408 -9.4793472 -7.6286969 -5.8786287 -5.3648539 -6.3108425 -6.2145138 -6.8398027][-8.9309406 -9.3770256 -9.311533 -9.1452951 -10.305472 -10.374697 -10.755648 -10.29904 -8.8320293 -7.2921448 -6.1252637 -5.6693592 -6.665462 -7.7799063 -7.7876158][-8.5622187 -9.2883224 -9.711916 -9.0519066 -8.5951538 -8.6573448 -9.3371 -8.3521624 -7.2654462 -6.1568274 -4.5877228 -4.495594 -6.5475307 -7.5990019 -7.3562365][-8.84781 -9.4765291 -9.201355 -6.869617 -5.7676105 -5.6405368 -5.8965926 -6.0350094 -5.0395551 -3.6926579 -2.9904768 -3.4805808 -6.0677295 -7.7846093 -8.5276918][-10.248259 -10.593853 -9.9288464 -5.7033515 -3.9297628 -2.7041512 -1.8964787 -2.8159993 -3.186178 -2.7738631 -3.0632885 -3.5192745 -5.4303994 -7.438796 -8.0755167][-11.460584 -11.705326 -9.1532612 -4.5891171 -1.434083 0.80778694 0.74252319 -1.0486345 -2.234525 -2.6124709 -2.9964216 -3.8628204 -5.474349 -6.80015 -6.5355873][-11.69446 -10.822305 -7.537405 -2.6949303 1.3657861 3.7614703 3.0592303 0.77220631 -0.909585 -2.0738008 -2.4012151 -3.1274486 -4.9417958 -5.9137249 -5.61495][-9.9491463 -8.0354013 -4.7697973 -0.3639884 3.292747 5.0902781 3.9723897 1.3763542 -0.15693283 -1.5336323 -2.021256 -2.4969099 -3.1698079 -4.3164816 -3.852982][-8.9306507 -6.8605695 -3.8768435 0.1277771 3.2915139 3.9750371 2.1727753 -0.34713984 -2.4345658 -2.565906 -2.3379431 -3.316762 -3.8216906 -3.7772086 -2.7568932][-7.476738 -6.7237306 -5.6282139 -2.7066593 0.60283184 1.0413175 -0.62855387 -3.2317605 -5.4485912 -5.5777745 -4.4859719 -4.3477211 -4.8821535 -4.3944535 -2.6765769][-8.85371 -9.0466518 -8.9902611 -6.9447384 -6.1839161 -5.3260789 -4.9350562 -6.7481947 -7.8568892 -8.23576 -8.32433 -7.7373734 -8.0166435 -7.1082611 -5.3481088][-10.720507 -11.248167 -11.640226 -10.9121 -9.7856941 -9.4152241 -9.90098 -9.9977417 -10.31957 -10.705513 -10.612419 -9.822525 -9.4094954 -7.1953406 -5.21296][-11.343641 -10.032622 -9.1466036 -9.1983471 -8.8839235 -7.3440418 -6.7082605 -7.403708 -7.8417726 -7.5374746 -7.0360355 -7.0358248 -6.3222976 -5.1447105 -3.3664668][-10.55221 -9.4915066 -8.0782022 -6.2322164 -5.4239612 -5.0422773 -5.2511725 -4.3420544 -3.8447351 -4.556757 -4.3688464 -3.6942942 -3.0383475 -2.1923215 -1.4686155][-8.1848583 -7.4471722 -6.4535861 -4.910327 -2.8239784 -1.5668964 -1.6923897 -2.1201677 -3.1321862 -3.541384 -3.5180168 -3.8474002 -4.0498409 -3.9511206 -3.4112227]]...]
INFO - root - 2017-12-15 11:34:51.992566: step 7010, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 61h:09m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 11:34:58.597734: step 7020, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 57h:44m:45s remains)
INFO - root - 2017-12-15 11:35:05.264117: step 7030, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 61h:53m:50s remains)
INFO - root - 2017-12-15 11:35:11.987415: step 7040, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 61h:58m:18s remains)
INFO - root - 2017-12-15 11:35:18.548978: step 7050, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 58h:34m:42s remains)
INFO - root - 2017-12-15 11:35:25.127302: step 7060, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 60h:29m:54s remains)
INFO - root - 2017-12-15 11:35:31.709585: step 7070, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 60h:03m:34s remains)
INFO - root - 2017-12-15 11:35:38.233380: step 7080, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 58h:15m:59s remains)
INFO - root - 2017-12-15 11:35:44.808177: step 7090, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 59h:47m:52s remains)
INFO - root - 2017-12-15 11:35:51.357547: step 7100, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 60h:07m:46s remains)
2017-12-15 11:35:51.886747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0256085 -8.4410534 -9.77486 -10.93808 -12.030003 -12.209791 -11.966149 -10.637695 -9.3825474 -8.9940062 -8.1750517 -8.9618282 -11.240221 -12.289738 -12.571711][-7.4700708 -8.3480139 -8.702631 -10.161905 -11.116457 -11.264112 -11.121696 -10.785297 -9.9102592 -8.6862183 -8.2960367 -10.259458 -12.442207 -13.353601 -13.949215][-7.1712208 -8.7068415 -9.9958229 -10.412233 -11.336748 -11.635744 -11.299665 -10.089701 -8.9680662 -8.5369291 -7.6198106 -8.4846344 -10.984163 -12.316157 -13.796436][-8.5085821 -9.7863655 -10.36302 -10.563839 -10.631262 -9.2334385 -8.3404436 -8.618516 -8.5635529 -7.4228716 -6.0492744 -7.4906888 -9.8204365 -11.140427 -12.595545][-9.7137184 -11.539303 -12.113461 -10.790071 -8.5232563 -4.8241091 -2.530072 -4.3268394 -7.082778 -6.7635784 -6.1899996 -6.7856312 -8.95483 -10.525915 -12.361647][-11.077797 -12.310825 -12.971238 -10.285852 -6.5940213 -0.545341 4.9757981 3.442925 -0.3537178 -3.7897434 -6.4812794 -6.4960623 -8.3013039 -10.029219 -11.904313][-11.184454 -11.78997 -11.312106 -7.7277169 -4.4293 1.2674723 7.1333385 7.9070926 6.5026069 0.64621162 -4.6225061 -5.7908554 -9.1000547 -10.727934 -11.871876][-11.171958 -11.364941 -10.044334 -6.138164 -2.7052588 3.2276511 7.786675 8.073103 7.2337203 2.5542598 -1.5343237 -4.593771 -9.445035 -10.595245 -11.511029][-7.9438806 -8.4150734 -8.08471 -5.04341 -2.3800664 1.9660997 5.8891931 6.9065328 5.6449285 1.2666483 -2.0251863 -6.2211781 -10.633947 -11.456406 -12.612389][-5.9165244 -5.4623742 -5.9529991 -5.05245 -3.6866493 -0.78452015 2.4371133 3.49678 2.131609 -1.3991027 -4.70795 -8.0408173 -11.835625 -12.976583 -14.072321][-8.7063093 -8.949192 -8.8094521 -8.03706 -7.5163889 -7.0911283 -5.2432761 -4.9161825 -4.6877661 -5.7269149 -7.7723112 -11.592751 -13.769016 -13.883257 -13.843992][-12.776705 -13.288845 -12.140517 -11.094439 -10.432448 -10.685051 -10.436535 -10.482225 -10.119402 -10.420012 -11.163124 -12.178963 -13.19311 -14.417603 -14.304726][-16.420652 -15.717426 -15.914158 -15.637562 -14.726679 -13.645093 -13.138746 -13.966177 -14.178703 -13.809603 -13.032246 -13.344685 -13.932467 -13.814169 -12.798043][-15.569139 -15.316446 -15.530005 -14.272747 -12.99912 -13.238022 -12.148853 -11.522881 -11.602234 -12.11393 -12.193121 -11.764691 -11.449045 -12.10239 -11.691487][-12.928767 -12.437249 -11.942978 -11.1481 -9.743145 -9.2606211 -8.7911968 -9.4451895 -9.7019615 -9.2755852 -9.6873884 -10.254416 -11.068144 -10.83036 -10.349576]]...]
INFO - root - 2017-12-15 11:35:58.500630: step 7110, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.688 sec/batch; 62h:09m:52s remains)
INFO - root - 2017-12-15 11:36:05.157466: step 7120, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 59h:42m:31s remains)
INFO - root - 2017-12-15 11:36:11.800695: step 7130, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 61h:36m:17s remains)
INFO - root - 2017-12-15 11:36:18.380216: step 7140, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 59h:12m:36s remains)
INFO - root - 2017-12-15 11:36:24.990510: step 7150, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 59h:57m:00s remains)
INFO - root - 2017-12-15 11:36:31.542511: step 7160, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 58h:12m:54s remains)
INFO - root - 2017-12-15 11:36:38.035944: step 7170, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 57h:43m:25s remains)
INFO - root - 2017-12-15 11:36:44.693462: step 7180, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 60h:54m:22s remains)
INFO - root - 2017-12-15 11:36:51.271527: step 7190, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 61h:08m:18s remains)
INFO - root - 2017-12-15 11:36:57.782594: step 7200, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 60h:29m:06s remains)
2017-12-15 11:36:58.284778: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5874884 -2.9454834 -2.8483763 -2.7015193 -2.8994451 -2.6396394 -2.2520962 -2.384279 -2.3158927 -1.9370162 -1.6189475 -5.1735611 -6.6283989 -8.0389252 -8.083992][-2.3897266 -3.2118685 -3.3585708 -4.0072937 -4.2316809 -4.2427735 -4.8664479 -4.7668576 -4.2468281 -3.7168832 -3.1209755 -5.8860984 -7.1057215 -8.6635189 -7.8924894][-1.134057 -2.030174 -2.85916 -3.4267397 -3.9519143 -4.6229181 -4.9446568 -5.130867 -5.4043112 -5.1354141 -4.244525 -6.7049417 -7.8403111 -8.1504 -7.1353397][-1.9382505 -2.3121943 -2.36006 -2.8170006 -2.9718184 -2.9640841 -2.7945147 -4.05972 -5.2662377 -5.3223844 -5.15796 -7.3151312 -7.6510153 -8.3652658 -7.6303139][-3.1709492 -3.5284953 -3.9010327 -3.0626075 -1.8715808 -1.1439495 -0.71670389 -2.1114879 -3.2630203 -3.7741365 -4.507782 -6.60886 -7.1702614 -8.2774162 -7.67659][-5.7097945 -5.4850411 -5.0741177 -3.1213415 -0.78895807 1.0890284 2.0836816 0.2923522 -1.0023165 -1.8237677 -2.2060239 -4.3645282 -5.2457561 -5.8349624 -5.9534206][-7.8003983 -7.2645473 -6.0553169 -3.465306 -0.47493649 2.7452159 4.7841115 3.3787398 2.1678643 0.94691515 0.22578669 -2.1612296 -3.0526888 -4.5696011 -5.0531316][-8.1666021 -7.9176035 -6.7035608 -3.9603624 -0.83810234 2.6638846 4.5396695 3.6237407 2.6212602 1.8476934 1.7448831 -0.28573275 -1.9117062 -4.0320368 -4.4570732][-7.5680327 -7.7760668 -7.1068387 -4.0694461 -0.91360664 2.1318893 3.2071271 2.5063062 1.7094851 1.2884064 2.0731764 0.25131035 -1.5918183 -4.48847 -6.1716475][-7.5092049 -7.9259996 -7.3502049 -4.7526207 -2.1925533 0.63121939 1.6474113 1.4881024 0.82979679 0.74728823 1.697216 -0.009452343 -1.5569749 -5.1238608 -7.7284822][-11.279657 -11.28718 -10.550148 -8.54521 -6.00089 -4.0144386 -3.0108488 -3.2472928 -3.6754906 -2.8776586 -1.8112891 -2.9513962 -3.8696394 -6.3491273 -8.1772375][-14.187086 -14.30501 -13.11507 -10.922556 -8.8874741 -7.0607185 -6.4787331 -6.9864836 -7.33972 -6.6524315 -5.6264443 -5.7825322 -5.6282239 -7.4843659 -8.6747608][-12.764255 -13.273956 -11.985294 -11.766265 -11.236847 -10.012709 -9.7657881 -9.87842 -9.6544724 -8.52852 -7.7867775 -7.6096253 -7.2887983 -8.0560541 -7.910018][-10.507608 -10.106438 -10.044341 -9.8018885 -9.4062843 -9.5392523 -9.8148575 -9.8211632 -9.6578188 -9.06243 -8.9157944 -8.8987389 -8.5088539 -8.3218794 -8.0962305][-7.4941044 -7.5190506 -6.3355756 -5.4492588 -5.7040896 -6.2775407 -6.5424023 -7.6872435 -8.3130741 -7.9521265 -8.2448454 -8.6791286 -9.5717421 -9.7846565 -9.893919]]...]
INFO - root - 2017-12-15 11:37:04.841744: step 7210, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 58h:23m:19s remains)
INFO - root - 2017-12-15 11:37:11.360896: step 7220, loss = 0.21, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 58h:59m:49s remains)
INFO - root - 2017-12-15 11:37:17.938698: step 7230, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 59h:00m:21s remains)
INFO - root - 2017-12-15 11:37:24.588246: step 7240, loss = 0.22, batch loss = 0.17 (11.8 examples/sec; 0.677 sec/batch; 61h:10m:16s remains)
INFO - root - 2017-12-15 11:37:31.192250: step 7250, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 60h:18m:11s remains)
INFO - root - 2017-12-15 11:37:37.780870: step 7260, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 59h:17m:53s remains)
INFO - root - 2017-12-15 11:37:44.345447: step 7270, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 60h:04m:17s remains)
INFO - root - 2017-12-15 11:37:50.990074: step 7280, loss = 0.18, batch loss = 0.13 (11.6 examples/sec; 0.688 sec/batch; 62h:09m:37s remains)
INFO - root - 2017-12-15 11:37:57.595762: step 7290, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 58h:53m:39s remains)
INFO - root - 2017-12-15 11:38:04.177347: step 7300, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 58h:07m:10s remains)
2017-12-15 11:38:04.690076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7291059 -5.19191 -4.8055716 -4.6976676 -5.9761162 -7.0374374 -7.4831152 -6.9913931 -6.870585 -6.7873435 -6.5797272 -8.2783461 -8.8721809 -8.8068733 -9.040988][-5.090167 -4.3750386 -2.9054317 -2.5984552 -3.6855757 -4.6876888 -5.2575092 -5.9302197 -6.7540774 -6.2925754 -6.3790359 -8.3819008 -8.7632942 -9.4003105 -11.26885][-1.1265621 -1.9991801 -2.7414026 -2.4172413 -2.4025936 -3.4987535 -4.1817646 -4.4149642 -5.1730528 -5.7550073 -6.1514435 -8.1035089 -9.1403446 -9.8943033 -11.286352][-2.4276276 -2.5987575 -2.40946 -2.3177361 -2.7131021 -3.4350054 -3.4763703 -4.3267 -5.4896865 -5.3028879 -5.2769408 -7.5152211 -8.8898735 -10.202587 -11.618303][-1.9022267 -3.1845567 -3.5936484 -2.4245942 -2.0055649 -1.4674935 -0.70761681 -1.9852185 -4.0454721 -4.7565641 -4.8627133 -6.7283225 -7.9483976 -9.1245623 -10.482377][-5.2666755 -5.3623619 -3.4818099 -1.1919618 0.520936 2.6593943 3.9362154 2.5161419 1.1605473 -0.45427847 -2.3024218 -4.22972 -5.5044465 -7.1707749 -8.754365][-7.0766869 -6.7694535 -3.7549264 -0.92256212 1.2160435 4.63671 7.4631624 6.9558849 6.0104885 3.3694754 0.40194035 -2.4922783 -4.5881252 -6.2841868 -7.8002138][-7.4582081 -7.388268 -5.2653661 -1.2711015 1.7618189 4.9280491 7.43388 7.4495544 7.0414619 4.682888 1.9057283 -1.7663758 -4.6257968 -6.5851707 -7.8932686][-6.3683653 -6.8145394 -5.3697248 -1.8909323 0.95018196 3.1656446 4.7417126 5.0072155 4.8147945 2.8740034 0.8046627 -2.7247474 -5.7721739 -7.845623 -9.0137644][-5.4866977 -6.0960908 -5.3401637 -3.463275 -1.5652275 0.49166679 2.5876455 2.3232889 1.3543715 -0.13981867 -1.8740027 -4.9281116 -7.1022315 -8.9123116 -10.301714][-9.5005131 -9.77801 -9.4754524 -8.1658134 -7.4709864 -6.2235694 -3.9333472 -3.9321766 -4.031333 -4.4303756 -5.6998863 -8.83314 -9.6938744 -10.478455 -10.791004][-12.662817 -12.792459 -12.32163 -10.907902 -10.872375 -9.9445992 -8.7242393 -8.5701714 -7.89985 -8.4261112 -9.4724083 -11.319658 -10.797363 -10.202708 -9.5563269][-12.340345 -11.181894 -9.8339081 -9.4128418 -9.7143726 -9.5076714 -9.4170218 -9.5296955 -10.104561 -10.166871 -9.76795 -10.200136 -9.5674152 -8.7712917 -8.4683838][-10.591217 -9.4566975 -7.9248538 -6.5962543 -5.9074125 -7.1161656 -8.5653 -8.0943346 -8.2123661 -8.3118048 -8.7923164 -9.4272394 -8.8542681 -8.0867453 -7.7737265][-7.9606 -7.5946884 -7.2570391 -6.0124388 -4.5716858 -4.6337605 -4.8741708 -5.808074 -7.0144415 -6.6323872 -6.774189 -7.6284394 -8.0252714 -8.23861 -9.2654114]]...]
INFO - root - 2017-12-15 11:38:11.231072: step 7310, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 58h:59m:16s remains)
INFO - root - 2017-12-15 11:38:17.820664: step 7320, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 60h:10m:10s remains)
INFO - root - 2017-12-15 11:38:24.282543: step 7330, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 59h:44m:57s remains)
INFO - root - 2017-12-15 11:38:30.843369: step 7340, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 59h:23m:12s remains)
INFO - root - 2017-12-15 11:38:37.431650: step 7350, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 57h:41m:37s remains)
INFO - root - 2017-12-15 11:38:44.027821: step 7360, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 59h:49m:26s remains)
INFO - root - 2017-12-15 11:38:50.532511: step 7370, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 58h:55m:39s remains)
INFO - root - 2017-12-15 11:38:57.085074: step 7380, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 58h:59m:43s remains)
INFO - root - 2017-12-15 11:39:03.661711: step 7390, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 58h:33m:10s remains)
INFO - root - 2017-12-15 11:39:10.221194: step 7400, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 60h:23m:50s remains)
2017-12-15 11:39:10.708254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1814284 -7.9549179 -7.027071 -7.8047867 -9.360033 -9.80955 -9.6074 -8.5156908 -7.5077944 -7.6187706 -7.267293 -7.94526 -9.2699022 -9.7125587 -9.5588684][-7.9852009 -9.5482731 -9.2053471 -9.0186415 -9.874855 -11.497185 -11.824253 -10.853534 -9.9028435 -9.4618378 -8.0040913 -8.1180143 -9.6835022 -9.9172087 -9.7453041][-8.0072126 -8.7180882 -9.6622019 -9.5020294 -10.238786 -11.000004 -11.309187 -10.226267 -9.6276979 -10.687559 -10.185991 -10.053205 -11.683874 -12.028372 -11.193565][-10.424185 -10.337743 -9.3266621 -9.25595 -9.9544945 -9.4751749 -9.02918 -8.7323465 -8.393343 -9.0145359 -9.8654194 -11.118713 -12.51747 -13.274033 -12.845416][-9.57214 -11.152298 -10.62781 -8.7400723 -6.865799 -4.4557166 -3.2909985 -5.7367187 -7.9237413 -8.0284586 -8.5272427 -9.8892584 -12.74189 -13.953211 -13.712372][-10.23275 -10.465173 -8.982398 -7.1940961 -4.1043787 0.75743532 4.7013173 3.3693709 -0.51887894 -4.8915019 -7.6742558 -7.3521228 -10.62005 -12.900846 -12.370618][-11.074984 -11.101377 -8.86145 -4.5516596 -0.7741251 3.2340961 8.75103 9.8815022 6.8386579 -0.16756773 -5.9104009 -6.3895292 -8.7336569 -10.039668 -9.5762653][-10.533901 -10.922329 -8.4263859 -3.9897561 1.5385032 6.1754079 10.862917 10.870224 8.6790524 2.8555837 -3.6127343 -6.5580792 -9.6689405 -9.3508472 -7.7364016][-9.5061274 -9.9370308 -8.3584118 -6.5047989 -2.8707173 2.718514 7.8254695 6.9535365 5.8236384 1.0695281 -4.0468884 -6.3468785 -10.554149 -10.350918 -8.2947083][-8.8330879 -9.4473505 -9.902895 -8.8172951 -6.2752805 -2.6668463 1.3166103 2.2844296 1.2020917 -2.8243539 -6.1007452 -7.8958254 -10.971592 -11.275601 -10.540283][-11.626989 -13.291521 -13.557356 -12.42725 -10.652076 -7.7752714 -5.4746418 -3.8071785 -3.8250566 -6.5335217 -9.1821938 -10.672785 -12.6061 -12.131126 -10.883927][-16.134995 -17.125317 -16.573143 -15.49534 -13.400925 -11.039949 -10.084442 -8.6347084 -8.2451077 -9.6757679 -11.184515 -11.993213 -12.176561 -12.018721 -11.007078][-17.344183 -17.75992 -16.039421 -14.462467 -12.648726 -10.925266 -9.6478786 -9.1285524 -9.781889 -9.8333721 -9.9732819 -10.653915 -11.787108 -11.348022 -10.796111][-13.992744 -13.807322 -13.412605 -12.46549 -10.126024 -9.8503323 -9.8000488 -8.5216408 -8.6082659 -9.2730532 -10.064627 -9.1746349 -10.129114 -10.480747 -10.476896][-9.5109158 -10.025455 -8.9736729 -7.2434282 -5.9804835 -6.2292204 -6.641335 -6.9817524 -7.678843 -7.6250396 -8.7721272 -9.7105732 -11.319832 -11.965328 -12.59481]]...]
INFO - root - 2017-12-15 11:39:17.244018: step 7410, loss = 0.19, batch loss = 0.15 (11.8 examples/sec; 0.679 sec/batch; 61h:18m:39s remains)
INFO - root - 2017-12-15 11:39:23.852134: step 7420, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 60h:35m:06s remains)
INFO - root - 2017-12-15 11:39:30.426964: step 7430, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 60h:22m:33s remains)
INFO - root - 2017-12-15 11:39:36.988385: step 7440, loss = 0.21, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 58h:10m:25s remains)
INFO - root - 2017-12-15 11:39:43.653143: step 7450, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 59h:44m:16s remains)
INFO - root - 2017-12-15 11:39:50.273927: step 7460, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 59h:24m:27s remains)
INFO - root - 2017-12-15 11:39:56.972441: step 7470, loss = 0.23, batch loss = 0.18 (11.8 examples/sec; 0.677 sec/batch; 61h:04m:51s remains)
INFO - root - 2017-12-15 11:40:03.481638: step 7480, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 57h:53m:33s remains)
INFO - root - 2017-12-15 11:40:10.052492: step 7490, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 58h:29m:48s remains)
INFO - root - 2017-12-15 11:40:16.624168: step 7500, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 58h:45m:42s remains)
2017-12-15 11:40:17.114201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3502688 -4.243813 -3.7389627 -4.0724168 -4.4683928 -5.0275569 -5.6384788 -5.7514091 -5.2943978 -4.1256433 -3.6229405 -6.0121889 -7.0026865 -9.1199045 -9.0033569][-5.0461793 -4.5429649 -4.2361364 -4.9951105 -5.3571525 -5.6917973 -5.9939237 -6.499095 -6.6124816 -5.7187977 -5.1498208 -7.2796588 -8.0307264 -10.136275 -10.430177][-3.6067982 -3.8281536 -4.8009486 -4.9172039 -5.1824436 -5.9924836 -6.6397619 -6.5500135 -6.1716309 -5.5667577 -5.0948071 -7.2814522 -8.4584885 -10.426388 -11.052526][-4.0730414 -4.1827235 -4.3730845 -4.6391497 -5.2574859 -5.090847 -4.6757684 -4.9894686 -5.0232453 -4.5611696 -4.1967297 -6.9346471 -8.2248154 -10.285583 -10.861452][-4.5637941 -5.3505096 -5.4539156 -4.4611111 -3.8004394 -2.485286 -1.7212727 -2.2543361 -2.9072115 -2.9038234 -2.7977741 -5.762475 -6.7129669 -9.544488 -10.664577][-6.2403593 -5.9258389 -4.3316255 -2.7642121 -1.4951439 0.22711754 1.8887577 2.536562 2.4665251 0.99442244 -0.75211954 -4.1954222 -5.3112664 -8.2510185 -8.8466711][-7.5722823 -6.4122643 -4.3372579 -1.1603484 1.1349239 3.7775431 5.6913185 5.3928614 5.0771017 2.9187593 0.55916643 -2.909667 -4.603991 -7.2553434 -7.5529504][-7.0152097 -5.3806381 -3.0569174 -0.22090101 2.2809834 5.0254564 6.9627428 6.6365719 6.2917643 4.1183786 1.3547606 -3.3776913 -5.2068596 -7.5174479 -7.8421478][-5.42258 -5.2009611 -4.0582795 -1.7428246 0.70889378 3.6451817 5.6113749 4.9135857 4.0729294 1.7629557 -0.31711292 -4.933774 -6.6868386 -8.5589113 -8.5129709][-5.7649126 -5.11052 -4.3199067 -2.463717 -1.96065 -0.057418346 1.7601199 1.6647315 0.94689465 -1.4455318 -3.3337202 -7.3753638 -8.1595688 -9.8651485 -9.9485235][-9.1269875 -8.2626972 -7.7845669 -6.1513028 -5.4263945 -3.8883457 -3.8418651 -4.648767 -4.3533993 -4.8370857 -6.0898414 -9.9584274 -9.4946423 -10.070127 -8.8962555][-12.414858 -11.436174 -10.13088 -9.088459 -8.6671886 -8.3745947 -8.4586544 -8.4953537 -8.107399 -7.7309914 -7.8997 -10.129889 -8.86971 -9.5864019 -8.3924007][-12.749179 -11.636341 -10.297831 -10.144834 -10.547042 -9.618907 -8.8949642 -8.7053337 -9.0646038 -8.7327223 -8.6971626 -9.182704 -8.63171 -8.2479916 -6.8171358][-9.1513271 -9.47611 -8.56145 -7.7248549 -6.9707308 -7.4464006 -7.6622348 -7.0147743 -6.868381 -6.5633006 -8.0458126 -7.7517977 -7.0622139 -7.2468786 -6.9157534][-7.239583 -6.6277494 -5.721971 -4.7947812 -5.3658409 -5.3717461 -5.3764496 -5.23548 -5.0733824 -5.189805 -5.7462897 -6.6907864 -6.9403877 -7.8695712 -8.1163483]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 11:40:24.445032: step 7510, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 58h:41m:19s remains)
INFO - root - 2017-12-15 11:40:31.039763: step 7520, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 58h:57m:57s remains)
INFO - root - 2017-12-15 11:40:37.573681: step 7530, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 60h:09m:59s remains)
INFO - root - 2017-12-15 11:40:44.118816: step 7540, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 58h:34m:44s remains)
INFO - root - 2017-12-15 11:40:50.707068: step 7550, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 59h:27m:49s remains)
INFO - root - 2017-12-15 11:40:57.283847: step 7560, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 59h:53m:00s remains)
INFO - root - 2017-12-15 11:41:03.955651: step 7570, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 61h:41m:20s remains)
INFO - root - 2017-12-15 11:41:10.533154: step 7580, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 58h:42m:59s remains)
INFO - root - 2017-12-15 11:41:17.161537: step 7590, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 58h:00m:27s remains)
INFO - root - 2017-12-15 11:41:23.734512: step 7600, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 57h:56m:39s remains)
2017-12-15 11:41:24.249327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0134826 -6.7593527 -6.400404 -6.2607918 -7.0899134 -8.14101 -8.7186537 -8.8934631 -8.32603 -7.6848197 -7.353126 -9.4808731 -12.56698 -13.842449 -13.293296][-7.4855804 -8.8489113 -7.843534 -7.3320417 -7.358727 -8.6242428 -10.293041 -11.535997 -11.39904 -9.7873573 -9.0934067 -11.466858 -13.69503 -15.19689 -14.829241][-6.9115052 -8.88019 -9.1004238 -8.5244923 -7.8860593 -8.7942924 -10.458084 -11.721937 -11.888933 -11.314367 -10.778242 -12.697733 -14.576357 -15.640511 -15.44668][-7.5559511 -9.075841 -8.9520741 -8.1176558 -7.6145992 -7.7578545 -8.3298492 -9.8368387 -10.682569 -9.1804466 -8.3182468 -11.334414 -13.712971 -15.527128 -15.351156][-8.4039526 -10.703521 -10.891204 -8.4391088 -5.9422784 -3.9426293 -3.2775106 -5.4276719 -7.1821995 -6.7233562 -6.7888012 -8.7743711 -10.681995 -13.159869 -13.697674][-11.276528 -12.012318 -10.636343 -7.3523269 -3.5076163 0.12537098 3.0687451 2.2314072 0.46352005 -1.4969368 -4.1941929 -6.4176335 -8.4155149 -10.453434 -10.908554][-11.80686 -12.224072 -10.12512 -5.8363662 -1.6697044 3.4546628 7.7646985 7.3017631 6.8057704 2.9057155 -1.7663078 -4.7822852 -8.2137623 -9.4364071 -9.610405][-10.694221 -11.27547 -9.0737314 -5.2492728 -0.87772274 5.2172904 8.9172363 8.6933842 9.451395 5.734509 0.83377457 -4.499351 -9.7844658 -11.309533 -10.963461][-8.0483084 -8.92528 -8.6526976 -6.531271 -2.4022737 2.7371678 5.8488307 7.7416911 8.1815624 4.0386977 0.65153694 -5.3276329 -11.222297 -13.461817 -12.94898][-6.9041634 -7.3448 -7.5101128 -6.6207714 -4.1470294 -0.97799397 2.3406367 4.1757016 3.7756581 1.7096958 -0.63286161 -6.7714944 -12.112345 -15.103809 -16.156713][-8.499608 -10.15909 -10.611931 -9.6064949 -8.28323 -5.9318328 -3.5095141 -2.3571353 -2.24108 -3.3437939 -5.8593283 -10.771941 -14.529716 -16.021544 -16.607889][-12.08099 -12.499764 -11.783285 -11.835569 -11.632648 -9.7050457 -9.5213318 -9.7268772 -9.0863791 -9.2585478 -10.288185 -12.529079 -13.809532 -15.487141 -15.580946][-13.817312 -13.071821 -11.817371 -11.602757 -11.578927 -11.781267 -11.856206 -11.398396 -11.708097 -11.612042 -11.498428 -12.30308 -12.483707 -11.857431 -10.57025][-12.183968 -11.540462 -10.283856 -9.823946 -9.6856642 -10.33192 -10.553434 -10.544409 -10.72361 -10.370224 -10.237021 -9.2271757 -8.8777628 -8.6849356 -8.5537586][-9.2789507 -9.0761986 -7.5011568 -6.3526969 -5.397387 -5.4706621 -6.04654 -7.0975428 -7.7379193 -7.5771761 -7.7582073 -8.2365322 -8.1949749 -7.2955618 -6.7908139]]...]
INFO - root - 2017-12-15 11:41:30.799501: step 7610, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 60h:03m:08s remains)
INFO - root - 2017-12-15 11:41:37.439941: step 7620, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 60h:03m:28s remains)
INFO - root - 2017-12-15 11:41:44.058338: step 7630, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 59h:58m:03s remains)
INFO - root - 2017-12-15 11:41:50.702040: step 7640, loss = 0.20, batch loss = 0.16 (11.5 examples/sec; 0.693 sec/batch; 62h:32m:36s remains)
INFO - root - 2017-12-15 11:41:57.339851: step 7650, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 59h:10m:06s remains)
INFO - root - 2017-12-15 11:42:03.919888: step 7660, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 58h:21m:45s remains)
INFO - root - 2017-12-15 11:42:10.547172: step 7670, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 58h:43m:24s remains)
INFO - root - 2017-12-15 11:42:17.067993: step 7680, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 58h:01m:54s remains)
INFO - root - 2017-12-15 11:42:23.726461: step 7690, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 59h:34m:34s remains)
INFO - root - 2017-12-15 11:42:30.433670: step 7700, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 57h:49m:17s remains)
2017-12-15 11:42:30.908695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1269803 -6.3574724 -6.211257 -5.8090472 -6.0584965 -7.2536969 -7.1282568 -6.094276 -6.1198721 -6.2694454 -6.57228 -6.808815 -6.9617314 -8.16852 -9.0334806][-5.4761143 -6.0631104 -6.6912832 -6.4934397 -6.6737723 -6.4070468 -5.3285804 -4.8259621 -4.8091526 -5.0679674 -5.2262955 -6.1340165 -7.5693207 -7.8796873 -8.5028305][-5.6253819 -6.0569825 -6.62066 -5.8956738 -6.3310275 -7.4304595 -6.7927675 -5.1231174 -3.8708477 -3.09587 -3.1232367 -4.6908045 -6.7426782 -8.8142319 -9.3104887][-5.4323082 -6.3602762 -6.6700482 -6.4535632 -7.7731466 -7.7008362 -6.948617 -5.5793295 -3.3199828 -1.7188058 -2.4755168 -4.3570366 -5.4766631 -6.5074296 -6.6940246][-5.4102125 -6.4716754 -7.4653592 -7.8331203 -7.0460544 -6.0586367 -4.9096017 -3.2247386 -1.9656062 -0.90201187 -1.0188384 -3.0333121 -4.8247528 -5.9062252 -6.1601858][-6.9114132 -7.2836552 -7.2256069 -6.3261147 -4.5526571 -1.4751997 1.6585073 1.5872855 0.52665567 -0.17528772 -2.0455832 -2.8092039 -4.1249657 -5.8177905 -6.00903][-6.5479445 -6.71997 -6.4159937 -4.5795441 -1.7332244 2.2587233 4.9746904 5.5506649 4.6856546 1.4986529 -1.6024566 -2.913707 -4.4580145 -6.1289 -6.830287][-7.8946171 -6.7292213 -5.5174518 -3.183238 -1.5524578 1.6995697 5.6865158 6.3782063 6.606391 3.2840614 -0.51924753 -3.2052031 -5.3550463 -5.8289227 -6.3449025][-7.2916431 -5.8221536 -4.1353626 -2.4933796 -2.2229733 0.52875566 2.8341627 3.9711466 5.0373859 2.5883427 -0.13209105 -3.2017775 -6.5692487 -8.1933212 -7.9736991][-6.0308342 -6.4826736 -5.7055 -4.0341148 -1.9123592 -0.51653194 0.4929738 1.8548083 1.9715385 0.12014675 -2.9688542 -6.149251 -9.1384382 -10.689118 -10.889128][-7.0020819 -7.2468495 -7.4969592 -5.9518456 -5.4658923 -3.7408416 -2.2101214 -1.6157994 -1.5411091 -2.5550785 -4.6342473 -8.6529684 -11.817513 -13.410427 -13.362106][-10.85174 -10.491924 -9.3779182 -8.8082275 -9.0343313 -8.4505081 -7.46729 -6.783401 -6.4412289 -7.0960522 -8.0372066 -8.6978016 -10.113091 -11.695691 -12.045908][-12.769877 -12.391991 -11.288733 -9.7166939 -10.270412 -10.603086 -10.781672 -10.410728 -9.0578871 -8.8778648 -9.193572 -9.9159908 -9.9959393 -10.159279 -9.7146969][-11.546702 -11.148414 -10.935049 -9.8089142 -9.0790329 -9.5465555 -9.8778839 -10.084165 -10.348488 -9.5309839 -8.3009453 -7.7059259 -7.7221479 -8.0512028 -7.6129942][-7.9898815 -8.0323391 -7.5553823 -8.14773 -7.9656544 -7.7456913 -7.6485152 -7.9377856 -8.4578543 -8.7990875 -8.8038015 -8.1788893 -7.8836136 -7.6042652 -7.9429293]]...]
INFO - root - 2017-12-15 11:42:37.508838: step 7710, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 60h:03m:00s remains)
INFO - root - 2017-12-15 11:42:44.051888: step 7720, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 58h:47m:40s remains)
INFO - root - 2017-12-15 11:42:50.681742: step 7730, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 60h:48m:08s remains)
INFO - root - 2017-12-15 11:42:57.315053: step 7740, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 60h:12m:34s remains)
INFO - root - 2017-12-15 11:43:03.916286: step 7750, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 58h:04m:21s remains)
INFO - root - 2017-12-15 11:43:10.568174: step 7760, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.681 sec/batch; 61h:23m:45s remains)
INFO - root - 2017-12-15 11:43:17.242412: step 7770, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 58h:29m:34s remains)
INFO - root - 2017-12-15 11:43:23.883344: step 7780, loss = 0.17, batch loss = 0.12 (11.6 examples/sec; 0.689 sec/batch; 62h:08m:11s remains)
INFO - root - 2017-12-15 11:43:30.445930: step 7790, loss = 0.21, batch loss = 0.16 (12.6 examples/sec; 0.635 sec/batch; 57h:18m:18s remains)
INFO - root - 2017-12-15 11:43:37.007590: step 7800, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 57h:56m:04s remains)
2017-12-15 11:43:37.503571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.29832 -5.382041 -5.6983051 -6.4092913 -7.8122048 -8.1858339 -7.8939505 -8.0698328 -8.63917 -8.4779377 -8.2177334 -7.0491748 -5.9035711 -6.1769953 -5.09438][-6.7325211 -6.3102422 -6.2912626 -5.7316961 -6.2433882 -7.3962917 -8.2774324 -8.38681 -8.87032 -9.3638754 -9.3597441 -8.2438736 -6.0031352 -6.1939445 -5.7611141][-4.3062253 -5.4456811 -6.841464 -6.6548948 -6.5279484 -6.5202432 -6.3464856 -6.9910789 -7.9277983 -8.3194466 -8.6016922 -7.3486128 -5.8804789 -6.5663276 -7.1686134][-3.8008595 -4.7514176 -5.3742285 -5.3629422 -5.4782505 -5.6722083 -5.690444 -6.5785542 -8.0948391 -8.2113667 -7.8946657 -7.1311879 -5.7909174 -6.8879004 -7.019743][-5.9121747 -5.3933463 -5.6370797 -4.7215447 -3.118978 -1.3389115 0.18570566 -1.5894761 -3.8602381 -5.6091037 -6.6301808 -4.6747937 -3.2677159 -4.6061382 -5.1587553][-7.9332938 -6.4866161 -5.8015513 -3.6298721 -1.3092384 1.0137348 4.2919106 4.1229424 1.9603634 -1.5742612 -5.6040292 -4.93195 -2.9229999 -3.0458238 -3.0081859][-7.3743215 -6.9129353 -5.7920604 -2.4478607 -0.23765564 2.622622 6.1489062 5.9542565 5.8770928 0.91881704 -5.1812663 -4.7877784 -4.81134 -4.9343009 -2.834192][-7.5983191 -7.0168848 -7.0423646 -4.2351556 -0.44666052 3.7616172 6.4009557 5.8718505 6.0075126 2.4950852 -2.34914 -4.5890789 -6.2471309 -6.9571953 -6.7024875][-5.8526688 -5.5917277 -6.6897664 -5.9007812 -4.1470585 -0.32110691 3.5068173 3.9677176 4.0136104 1.1300468 -2.3368964 -4.5768142 -6.87131 -8.26887 -7.9600549][-5.631743 -5.0928884 -5.0692792 -5.0312181 -5.9483433 -3.9890761 -1.1117473 -0.059325695 1.1074252 -1.9985991 -5.6309805 -6.216898 -6.5858803 -7.8478746 -7.7561374][-8.998785 -9.1468554 -9.2811069 -9.3307762 -8.6043873 -7.4462109 -7.0934167 -6.4902577 -5.1448693 -6.0888739 -7.509212 -9.3741179 -11.342042 -10.05421 -8.3828182][-12.350991 -12.613176 -13.351505 -12.267835 -11.350265 -11.607652 -12.327177 -11.552963 -9.9982052 -10.050196 -10.344477 -10.746605 -11.621887 -11.864017 -11.15083][-10.94029 -11.100705 -12.260109 -12.051825 -11.360191 -10.110994 -9.5969124 -10.335037 -10.952957 -10.680454 -10.061478 -9.8797569 -8.9278955 -8.1086979 -8.1050215][-10.747532 -10.13317 -9.5599308 -9.2878428 -9.1676874 -8.5225544 -8.2440319 -8.0719843 -8.2541533 -8.7929487 -9.2522554 -8.4341011 -6.7973051 -6.0809927 -4.6345711][-7.1242018 -6.2752976 -5.3632526 -3.8904452 -3.328016 -3.9419482 -4.7643995 -4.76035 -4.3128762 -4.568541 -5.0596409 -5.5607791 -5.3227487 -4.9103479 -4.4293423]]...]
INFO - root - 2017-12-15 11:43:44.027080: step 7810, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.635 sec/batch; 57h:17m:46s remains)
INFO - root - 2017-12-15 11:43:50.681667: step 7820, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 61h:06m:17s remains)
INFO - root - 2017-12-15 11:43:57.266294: step 7830, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 58h:40m:03s remains)
INFO - root - 2017-12-15 11:44:03.954300: step 7840, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.676 sec/batch; 60h:56m:23s remains)
INFO - root - 2017-12-15 11:44:10.506161: step 7850, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 57h:08m:56s remains)
INFO - root - 2017-12-15 11:44:17.088918: step 7860, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 58h:46m:38s remains)
INFO - root - 2017-12-15 11:44:23.735865: step 7870, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 58h:47m:51s remains)
INFO - root - 2017-12-15 11:44:30.336831: step 7880, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 61h:36m:54s remains)
INFO - root - 2017-12-15 11:44:36.934045: step 7890, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 58h:57m:39s remains)
INFO - root - 2017-12-15 11:44:43.582317: step 7900, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 59h:52m:05s remains)
2017-12-15 11:44:44.073161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7245121 -5.0055346 -5.4807954 -6.4164128 -7.6836405 -8.3853922 -9.0485086 -9.4051962 -9.6312971 -9.7547588 -9.7880478 -10.000402 -10.741457 -9.371562 -5.6304803][-7.4652634 -6.1896477 -6.8886828 -7.6838183 -8.8172932 -10.173582 -10.957225 -10.954088 -11.04015 -10.811584 -10.208786 -10.527918 -11.432878 -10.6015 -6.9515891][-5.8973694 -6.6311283 -7.9978552 -8.1190681 -9.29003 -10.140692 -10.8825 -10.962412 -10.598955 -9.9437389 -9.8480053 -9.7808437 -10.436258 -10.631148 -7.9621496][-6.5755329 -6.6769447 -7.907 -8.35199 -9.48156 -9.3816452 -9.1848507 -9.9343948 -9.8394775 -9.275362 -9.0378819 -8.9177837 -9.8124647 -9.6818752 -7.3939719][-7.0414209 -8.2853546 -9.1631966 -9.0941744 -8.6287441 -7.2503109 -6.3584676 -6.7127104 -7.3469596 -7.3092957 -7.8370585 -8.1298094 -9.6353035 -9.4101334 -6.248559][-7.7107534 -7.4338083 -7.40191 -6.6933384 -5.6049523 -2.0082517 0.99183941 0.44051933 -0.48241091 -2.8715994 -5.0182414 -5.6664639 -7.5413351 -8.0449638 -6.5075855][-8.798583 -8.2578373 -8.1010532 -6.6364312 -3.4138622 0.3893919 4.1056657 6.2033877 6.1086431 1.9954805 -2.3820884 -3.9518013 -6.3897476 -7.6666551 -6.7606091][-7.9255304 -8.1101246 -7.9899311 -5.101018 -1.4496131 2.8092437 6.2776189 8.8000584 9.5165482 4.161036 -1.8540921 -4.63048 -6.9675465 -7.8136168 -6.9882512][-5.3321457 -5.7623496 -6.6136379 -4.9978728 -2.1558886 1.4797697 4.9082589 7.0618377 6.9025178 4.1872211 -0.017782211 -4.5717568 -8.888464 -9.9348049 -8.1671114][-4.4932008 -4.0388885 -4.6112623 -3.6273346 -2.1955597 0.36779594 2.6567149 3.3698316 2.9046311 -0.12303066 -3.4892826 -6.66123 -10.494955 -12.371895 -11.338217][-5.9888291 -4.8425703 -5.0144105 -4.9034052 -4.3989568 -3.2532959 -2.52446 -3.1848624 -4.9380445 -6.366262 -8.23367 -10.296936 -13.377326 -14.266876 -11.907603][-10.149459 -9.7207079 -9.2648344 -7.8494644 -7.0439582 -7.4059258 -7.8154411 -8.3597755 -9.470191 -11.47982 -13.070412 -13.46517 -13.92165 -12.970081 -10.455063][-11.424096 -10.267273 -9.189086 -8.3726215 -8.9129372 -8.4712257 -8.317709 -9.968853 -10.794456 -11.612711 -12.052847 -12.836899 -12.692635 -10.674917 -7.5407653][-10.667199 -10.256857 -9.0459719 -8.3931026 -8.5100689 -8.1517534 -8.9098969 -9.37583 -9.0855293 -9.6592808 -10.068264 -9.3688717 -8.7597513 -8.15967 -6.4990897][-8.5800762 -8.0509205 -7.0753946 -5.946538 -5.7517924 -5.4786692 -5.9804955 -5.73716 -6.1877518 -7.1061277 -7.2775483 -7.5845914 -7.4585481 -6.546783 -5.1698294]]...]
INFO - root - 2017-12-15 11:44:50.658349: step 7910, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 58h:46m:38s remains)
INFO - root - 2017-12-15 11:44:57.330374: step 7920, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 60h:07m:06s remains)
INFO - root - 2017-12-15 11:45:04.010983: step 7930, loss = 0.25, batch loss = 0.20 (11.7 examples/sec; 0.685 sec/batch; 61h:44m:50s remains)
INFO - root - 2017-12-15 11:45:10.664520: step 7940, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 60h:41m:42s remains)
INFO - root - 2017-12-15 11:45:17.306593: step 7950, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 59h:06m:51s remains)
INFO - root - 2017-12-15 11:45:23.943750: step 7960, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 59h:25m:49s remains)
INFO - root - 2017-12-15 11:45:30.550946: step 7970, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 59h:15m:36s remains)
INFO - root - 2017-12-15 11:45:37.176341: step 7980, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 59h:33m:37s remains)
INFO - root - 2017-12-15 11:45:43.874410: step 7990, loss = 0.19, batch loss = 0.15 (11.6 examples/sec; 0.693 sec/batch; 62h:26m:01s remains)
INFO - root - 2017-12-15 11:45:50.412235: step 8000, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 59h:35m:30s remains)
2017-12-15 11:45:50.953759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0390034 -6.62495 -6.0106359 -6.2973104 -7.5582285 -9.1220045 -10.247608 -9.0002861 -6.6571927 -5.4657288 -5.2122259 -7.9915624 -10.784263 -10.692417 -8.6619778][-7.63082 -6.672121 -5.3346 -6.2819705 -7.7900934 -9.765213 -10.886722 -10.564121 -9.16027 -7.0765991 -5.9334126 -9.0819311 -11.757143 -12.144622 -10.425182][-4.999342 -4.8857656 -4.9870872 -5.4021463 -6.3303151 -8.3093472 -9.0511751 -8.8347931 -8.6754684 -7.7480297 -6.7050018 -9.0249166 -10.848192 -11.293213 -10.231659][-4.8590136 -4.5346212 -4.2202358 -4.2700009 -4.5706511 -5.4294028 -5.7822409 -6.8241119 -6.9874892 -6.066895 -5.5568266 -8.5152626 -10.810823 -11.514629 -10.195297][-6.1998563 -6.7157569 -6.5053248 -4.7458296 -3.3455334 -2.4119508 -1.9243572 -4.2177153 -6.2270412 -6.2629852 -5.8693995 -8.0730143 -9.4603415 -9.6770706 -8.3960056][-6.9721417 -6.6881509 -5.7186451 -2.8472831 -0.61876822 1.5972896 3.4643655 1.2445335 -0.52081156 -2.9183693 -5.3304229 -7.0309758 -8.5261164 -8.3637486 -6.8973842][-8.6300411 -7.8718567 -5.3596706 -1.6656919 1.0389462 4.0906529 6.9219775 6.1761861 4.8123088 0.86574316 -3.034229 -6.5203066 -8.4724407 -7.4666734 -6.3016553][-8.5605736 -8.2502842 -6.546854 -2.6453836 0.98226118 5.0046058 8.2080688 7.6205344 6.4775767 3.5091844 -0.26701069 -5.4805765 -8.7230072 -8.708971 -7.603476][-7.7729454 -7.1576095 -5.9393549 -3.5326395 -0.49790049 2.9733367 4.9824576 5.3011785 5.39595 2.1772165 -1.4752665 -6.3810921 -10.158501 -10.757952 -9.7330914][-8.09846 -7.1433134 -6.0698266 -3.3526876 -0.82714319 -0.38297224 0.43734312 1.8063588 2.2561588 -0.33145618 -3.0726721 -7.8734465 -11.445599 -12.320482 -11.97413][-10.232138 -10.184194 -9.4288931 -7.5938721 -6.2308016 -5.0368791 -4.19575 -4.1191273 -4.2204952 -5.3727431 -6.4967732 -11.443314 -14.07505 -13.432695 -12.081347][-14.865747 -13.854641 -12.68503 -11.693229 -11.505292 -10.748028 -10.479528 -10.560821 -10.290228 -10.496807 -10.771148 -13.177731 -13.7351 -13.390837 -11.895155][-16.508574 -15.029425 -13.888924 -13.545234 -12.552618 -11.901878 -11.715595 -11.78351 -12.270794 -12.113678 -11.814032 -13.01659 -12.909142 -11.309204 -8.485961][-13.227152 -12.438196 -11.755442 -11.018641 -10.63389 -10.846079 -11.244456 -10.383348 -9.9821243 -10.56542 -10.915361 -10.868294 -10.156897 -9.3832893 -7.2987623][-9.2481937 -8.1428947 -7.2898159 -6.2794628 -6.3516283 -6.6258097 -6.750257 -7.3987007 -8.0901833 -7.7991967 -7.4190669 -8.8986387 -9.4291506 -8.79388 -7.3848886]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 11:45:57.552177: step 8010, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.686 sec/batch; 61h:51m:20s remains)
INFO - root - 2017-12-15 11:46:04.143844: step 8020, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 60h:42m:23s remains)
INFO - root - 2017-12-15 11:46:10.788915: step 8030, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 57h:58m:01s remains)
INFO - root - 2017-12-15 11:46:17.442425: step 8040, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 59h:40m:09s remains)
INFO - root - 2017-12-15 11:46:24.051279: step 8050, loss = 0.24, batch loss = 0.20 (11.8 examples/sec; 0.681 sec/batch; 61h:21m:14s remains)
INFO - root - 2017-12-15 11:46:30.656312: step 8060, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.654 sec/batch; 58h:58m:51s remains)
INFO - root - 2017-12-15 11:46:37.253220: step 8070, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 60h:17m:37s remains)
INFO - root - 2017-12-15 11:46:43.895337: step 8080, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 60h:02m:58s remains)
INFO - root - 2017-12-15 11:46:50.463448: step 8090, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 59h:10m:55s remains)
INFO - root - 2017-12-15 11:46:57.009232: step 8100, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 58h:24m:14s remains)
2017-12-15 11:46:57.518821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.58933 -3.5039892 -3.7297349 -3.7316689 -4.6577168 -5.0717645 -5.1738605 -4.7278209 -3.740376 -3.1534107 -2.941653 -4.5318842 -7.0885234 -8.2879581 -8.3366251][-2.9353223 -3.0753956 -2.8121843 -3.6728826 -4.5518804 -5.0482635 -4.7700696 -3.9074221 -3.077148 -2.4557259 -1.9606981 -4.064394 -6.7549 -8.135828 -7.9042749][-2.650012 -3.5471284 -4.6022253 -4.6105576 -5.2084079 -5.3428459 -4.969573 -4.325099 -3.429893 -3.0850565 -2.9501271 -4.7174644 -6.7949181 -7.4304948 -7.53309][-2.8028047 -3.3036859 -4.2425022 -4.5035152 -4.88179 -4.7328467 -4.4615345 -4.35225 -3.7922716 -2.8265171 -2.2023199 -3.3955379 -5.2044697 -6.8246703 -7.5343223][-3.4100375 -4.4775009 -5.9937463 -4.7958159 -3.9530749 -3.1317239 -2.5309896 -2.7959418 -3.0463214 -2.7365761 -2.3051908 -3.0141127 -4.5752344 -5.5377808 -5.8211317][-3.9425228 -3.8634572 -4.0189767 -3.620753 -1.762224 0.7156601 2.2945952 1.8646965 0.95131493 -0.21842003 -0.48642302 -1.525012 -3.6124537 -4.8398876 -5.7369061][-4.517941 -4.1432805 -3.6928399 -2.5755177 -1.5742149 1.4699178 4.2244644 4.5943203 4.0487442 1.7208672 0.19814014 -1.531961 -3.7743807 -4.7106962 -5.1535754][-4.6505232 -4.0440865 -3.3683498 -2.2537026 -0.99724531 1.9614606 4.5933294 5.2424154 5.0224905 2.9728374 0.50545359 -2.1016941 -4.7647982 -5.7040362 -6.2800436][-4.2101135 -4.0349717 -3.1281557 -1.7571061 -0.5748167 1.218204 2.8694782 3.8968496 3.6096516 2.0373602 0.34514856 -3.6324687 -7.1311336 -7.6172895 -7.5435319][-3.4810963 -3.1036873 -3.0528042 -2.0204844 -1.4877357 0.039645195 1.2732625 1.5198522 1.1175771 -0.75610495 -2.3375864 -5.6918106 -8.7632675 -9.4315834 -9.1631451][-6.4389663 -6.419991 -6.1383386 -5.3142142 -4.9046354 -3.8413196 -3.5825324 -4.156981 -4.8883219 -5.467608 -6.384397 -9.0764771 -10.460011 -10.899174 -10.178723][-9.4248762 -9.7462034 -9.7174759 -8.6553259 -8.4696178 -7.6858773 -7.8002219 -8.6125593 -9.0039072 -9.53788 -9.9308376 -10.744253 -11.293217 -11.66044 -10.772413][-12.563923 -12.21501 -11.485969 -10.182025 -10.223248 -9.4332066 -9.8359289 -10.46703 -11.586798 -11.566829 -11.03734 -10.953158 -10.780905 -10.75164 -10.027891][-9.4850693 -9.2039108 -9.0663557 -8.4707136 -8.4247065 -8.0857468 -8.8190012 -8.9860792 -9.0617638 -9.2432013 -8.9015236 -8.3211784 -7.7567949 -8.0162086 -8.1203909][-6.0614228 -6.5396461 -5.8153467 -5.2846541 -4.2348623 -4.7053728 -4.5001736 -4.7524633 -5.3112497 -5.5575747 -6.1502404 -6.2100968 -5.8897719 -6.5359707 -7.5721245]]...]
INFO - root - 2017-12-15 11:47:04.158936: step 8110, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 61h:06m:23s remains)
INFO - root - 2017-12-15 11:47:10.736278: step 8120, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 58h:25m:02s remains)
INFO - root - 2017-12-15 11:47:17.332684: step 8130, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 58h:44m:46s remains)
INFO - root - 2017-12-15 11:47:23.958424: step 8140, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 58h:51m:07s remains)
INFO - root - 2017-12-15 11:47:30.571466: step 8150, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 57h:54m:47s remains)
INFO - root - 2017-12-15 11:47:37.158865: step 8160, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.672 sec/batch; 60h:31m:24s remains)
INFO - root - 2017-12-15 11:47:43.809420: step 8170, loss = 0.27, batch loss = 0.23 (12.2 examples/sec; 0.657 sec/batch; 59h:11m:40s remains)
INFO - root - 2017-12-15 11:47:50.414533: step 8180, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.696 sec/batch; 62h:39m:46s remains)
INFO - root - 2017-12-15 11:47:57.113277: step 8190, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 61h:17m:37s remains)
INFO - root - 2017-12-15 11:48:03.668077: step 8200, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 59h:32m:14s remains)
2017-12-15 11:48:04.189913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1434484 -5.8375998 -6.7895923 -6.2490788 -6.5841908 -7.0002537 -7.5257859 -7.6152363 -7.9874821 -8.7119093 -9.1244574 -9.2949886 -9.8210268 -7.4534903 -3.7104158][-7.2549167 -6.274838 -5.3802972 -4.7904091 -5.1995769 -5.0308361 -5.0626979 -5.3543234 -5.79517 -6.641149 -7.2622356 -7.5277653 -9.0319128 -7.9624596 -3.9536986][-5.2390919 -5.7376375 -6.0637751 -4.3779626 -4.8531322 -5.13104 -4.5659609 -4.23831 -3.4847519 -4.4766984 -5.3300986 -6.1259518 -6.3054876 -5.7040105 -2.1495755][-6.2502427 -5.556499 -5.7061543 -5.8885131 -6.2894483 -5.3182282 -4.5936756 -4.8178229 -3.9946637 -3.8339868 -4.2281566 -5.17539 -6.3744111 -5.2991986 -1.403492][-7.3054056 -7.1411891 -7.8339758 -7.3465118 -7.2452421 -4.9694743 -3.2909863 -3.722224 -3.6787226 -2.582206 -2.8594229 -4.3061337 -6.4828796 -5.1649537 -1.2117796][-6.6222849 -6.26183 -5.8196549 -4.7728863 -3.7678919 -1.1313195 1.8594708 2.2745805 1.1371908 0.17747211 -0.97125196 -0.93538046 -3.6120968 -4.7289968 -2.3164685][-8.1076336 -7.0293865 -5.3671737 -2.5865455 -1.0678487 1.5161381 5.2062855 5.8159523 5.4841948 1.9504972 -1.9058545 -2.83403 -5.1599131 -5.058929 -2.6264546][-7.3587265 -6.9540987 -6.9422312 -4.4705877 -1.5663533 2.5810995 6.4271269 6.3161383 6.2311544 3.5140648 0.22755814 -2.4272807 -6.0213456 -5.575191 -3.3768642][-6.6926436 -4.1434321 -4.2163506 -2.2266436 -0.66943359 2.431983 4.8551254 5.3907194 4.8047113 1.0622325 -1.8810802 -4.1341634 -6.8673468 -7.2796183 -5.262002][-6.1248384 -5.1845732 -5.3168783 -3.7708511 -2.3329442 0.77453613 2.6994476 3.42527 2.1258106 -1.7864151 -4.0668917 -5.2819891 -7.3858624 -7.49916 -5.5648737][-5.6950746 -5.1102004 -5.1351309 -4.2218981 -4.6293583 -4.5395179 -4.0070963 -2.6575363 -3.7286539 -4.9319439 -5.5263014 -7.2755437 -8.33486 -8.62681 -6.819603][-11.397493 -10.312857 -9.4360008 -9.6906557 -9.7595234 -9.45207 -9.3711233 -9.2108669 -9.1252975 -9.2213182 -9.7617455 -9.4723692 -9.7759 -9.7981987 -7.1133995][-11.767763 -11.439094 -11.63373 -10.417927 -10.295301 -10.651432 -11.032766 -11.61759 -11.356323 -10.365326 -8.6960812 -8.5520077 -9.5538521 -9.7252483 -6.79284][-8.9115314 -8.2998257 -8.5191555 -8.6651678 -7.9980831 -7.8758993 -8.7878475 -8.4498053 -8.5645828 -8.30929 -7.3679748 -6.4439483 -6.7950726 -6.7378445 -4.7656527][-8.7292261 -6.9792252 -6.6509314 -5.3371239 -5.4134769 -5.6190343 -5.2172179 -5.31012 -6.3351965 -6.0849471 -6.2992415 -6.2221818 -6.7628551 -7.2698984 -6.0761285]]...]
INFO - root - 2017-12-15 11:48:10.769245: step 8210, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 58h:58m:50s remains)
INFO - root - 2017-12-15 11:48:17.426783: step 8220, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 57h:49m:38s remains)
INFO - root - 2017-12-15 11:48:23.983013: step 8230, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 61h:02m:10s remains)
INFO - root - 2017-12-15 11:48:30.631663: step 8240, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 59h:03m:28s remains)
INFO - root - 2017-12-15 11:48:37.338105: step 8250, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 60h:10m:38s remains)
INFO - root - 2017-12-15 11:48:43.898447: step 8260, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 58h:27m:05s remains)
INFO - root - 2017-12-15 11:48:50.651334: step 8270, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 57h:31m:29s remains)
INFO - root - 2017-12-15 11:48:57.265744: step 8280, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 58h:28m:35s remains)
INFO - root - 2017-12-15 11:49:03.844798: step 8290, loss = 0.24, batch loss = 0.19 (12.3 examples/sec; 0.649 sec/batch; 58h:24m:46s remains)
INFO - root - 2017-12-15 11:49:10.529363: step 8300, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 57h:22m:02s remains)
2017-12-15 11:49:11.092858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8500633 -3.4336848 -3.7497935 -3.112983 -3.5366883 -4.414834 -4.5987582 -4.7478089 -5.16908 -4.8793397 -4.6366549 -6.45174 -8.4499416 -9.0667391 -8.0260782][-1.8500257 -2.1169219 -2.3971727 -3.046551 -3.3471696 -3.0792947 -3.3246024 -3.3851013 -3.8433051 -3.9668293 -3.6980844 -6.2205677 -8.2425022 -9.6644363 -9.8378258][-0.17425013 -0.63173294 -1.4122005 -1.6032529 -2.395889 -2.7622888 -2.9353514 -2.93226 -3.0962451 -3.4289227 -3.064307 -5.2566285 -7.1283307 -8.4451818 -9.8654652][-1.1048117 -1.4558115 -1.5335593 -1.6273365 -2.3497977 -2.4478381 -2.8102224 -3.2340531 -3.5047767 -3.0569935 -2.321569 -4.3381205 -6.4449186 -8.3693 -9.9474878][-0.99593306 -2.4690275 -3.7152395 -2.5782082 -2.0189533 -1.6479306 -1.6236434 -2.5456014 -3.2968509 -2.8522918 -2.2912133 -3.9990563 -6.0017424 -8.7790041 -10.187187][-3.0398474 -4.17131 -4.5662489 -3.3091946 -1.3904319 0.67168522 2.0113759 0.9302783 -0.24975204 -0.83664274 -1.4340258 -3.3965878 -5.8142285 -8.4975929 -10.148938][-5.3216763 -5.7789702 -5.0162611 -2.136476 -0.041258812 2.6062107 5.6259317 5.6978626 4.0687141 2.1946397 0.27989817 -2.8286321 -5.9970503 -8.5893831 -10.053889][-6.6847029 -6.9208908 -5.5529356 -1.8269193 1.5688376 4.900692 7.1100521 6.6310072 6.1229544 3.8099184 1.7163758 -1.9490242 -5.64427 -8.51291 -9.227211][-5.732398 -6.0081344 -4.9878964 -1.7004046 1.1761169 3.6206322 5.6967573 6.0756569 5.1425753 3.2952232 1.3199863 -3.01252 -6.999701 -9.3467808 -10.098253][-5.3460941 -5.267858 -4.5024204 -1.4769201 0.6870389 1.9146867 3.4728203 3.4920521 2.1153355 0.55818748 -0.86634493 -4.9445486 -8.59074 -10.365199 -11.198498][-9.8405762 -8.3173008 -6.2289109 -3.6436231 -2.9279711 -2.2214408 -0.83900642 -0.66348457 -1.008234 -2.379648 -4.5705047 -9.7133694 -11.972181 -12.540288 -12.210756][-13.406833 -11.189798 -8.5507383 -5.7508917 -4.6230192 -4.399353 -4.8404775 -5.5345106 -5.0577621 -6.1240664 -8.00082 -10.739643 -12.289998 -12.973568 -12.069838][-13.990173 -12.085516 -10.234269 -8.5881367 -7.7171068 -6.7453794 -7.1425967 -7.7470188 -7.9063845 -8.7488174 -9.9503279 -12.169945 -12.939362 -11.817766 -10.022703][-12.350594 -11.557698 -10.872383 -8.471838 -7.4304543 -6.393292 -6.7616482 -6.6001096 -6.83953 -7.8860517 -8.9361572 -9.9045181 -10.262287 -9.8528013 -9.2242441][-10.433994 -10.881519 -9.8754673 -8.62185 -7.6987739 -6.67851 -6.5836105 -6.9943705 -6.4394345 -5.6624732 -6.5456905 -7.79035 -8.9895639 -8.5606728 -8.9114218]]...]
INFO - root - 2017-12-15 11:49:17.656343: step 8310, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 58h:30m:34s remains)
INFO - root - 2017-12-15 11:49:24.260604: step 8320, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 60h:22m:41s remains)
INFO - root - 2017-12-15 11:49:30.860961: step 8330, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 59h:15m:02s remains)
INFO - root - 2017-12-15 11:49:37.435198: step 8340, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 57h:26m:27s remains)
INFO - root - 2017-12-15 11:49:44.022780: step 8350, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.664 sec/batch; 59h:49m:09s remains)
INFO - root - 2017-12-15 11:49:50.564991: step 8360, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 58h:38m:08s remains)
INFO - root - 2017-12-15 11:49:57.132618: step 8370, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 59h:06m:36s remains)
INFO - root - 2017-12-15 11:50:03.746666: step 8380, loss = 0.20, batch loss = 0.15 (11.4 examples/sec; 0.699 sec/batch; 62h:56m:14s remains)
INFO - root - 2017-12-15 11:50:10.403781: step 8390, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 60h:57m:17s remains)
INFO - root - 2017-12-15 11:50:17.071890: step 8400, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 58h:14m:13s remains)
2017-12-15 11:50:17.556163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4970872 -3.5703425 -3.5674958 -3.3605514 -3.9278202 -4.5013895 -5.01793 -4.6890378 -5.0239849 -5.4769731 -5.6015229 -6.8759995 -8.31439 -8.5602913 -8.2656021][-2.8832231 -2.0988748 -2.1091733 -2.0721912 -2.6999068 -3.2492702 -3.3813174 -4.0540552 -5.1863031 -5.0590644 -5.239676 -6.8785968 -8.2578421 -8.8505154 -9.0606737][-0.010771751 -0.66335678 -1.8970876 -1.2797656 -2.2412894 -2.6205561 -2.8219886 -3.0319567 -3.4546311 -4.0643215 -4.64303 -6.4077973 -8.1887941 -9.0066395 -9.0433331][-2.5764959 -2.3374851 -2.0865495 -1.4769106 -2.4440565 -2.1383405 -2.2155824 -2.8185368 -3.4162202 -3.2815287 -3.6124976 -5.7460718 -7.5580444 -8.89704 -9.234622][-2.4324257 -2.6186531 -3.304595 -2.2437067 -1.73067 -0.34041691 0.19376564 -1.0602198 -2.295773 -2.3004549 -2.9214997 -4.456696 -6.5527554 -8.2194672 -8.6750851][-4.9102755 -4.1044912 -3.4210453 -0.85017967 1.0686173 2.664701 3.3988719 2.5013709 1.659265 0.39764738 -0.94155216 -2.4769349 -5.0132456 -6.6737375 -7.5007238][-6.2532897 -5.133441 -3.1018398 0.46631241 2.1359668 4.4884143 6.1313605 5.779139 4.7494445 2.7664914 1.0301757 -1.0404296 -3.9159775 -5.8196087 -6.4559188][-6.9588747 -5.6419306 -3.7053585 0.058829784 2.51154 5.44377 7.1109319 6.1968207 5.0103736 3.6385589 1.7898064 -1.3082538 -4.3672404 -5.9541645 -6.1796422][-5.9149942 -5.6256351 -4.308814 -0.97460508 1.0046043 3.1373153 4.5938287 4.1938691 3.6768069 2.2697043 -0.0068063736 -3.1143777 -6.2038841 -7.9354458 -7.9982595][-4.236547 -4.5370893 -4.2660084 -2.5251839 -1.0247288 0.67979 2.0314627 2.0212569 1.6618037 0.19355822 -1.692997 -4.3245306 -7.3883371 -9.0269451 -9.3898478][-7.6655011 -8.145215 -8.0302505 -6.4551134 -6.2972856 -5.2836475 -3.8907342 -3.2937014 -3.2612109 -4.0643635 -4.8533974 -7.4312634 -9.3019733 -10.274975 -9.7100716][-10.932978 -11.10874 -10.989788 -9.3203087 -9.0462418 -8.3860168 -7.9840221 -7.6619053 -7.2744656 -7.5774488 -8.0649614 -9.0128393 -9.6438255 -9.6903639 -8.8942652][-11.050508 -9.5123339 -7.8353992 -7.8026876 -9.1753407 -8.8774309 -9.0352249 -9.2696609 -9.3938847 -9.0857782 -9.0573788 -9.2115 -9.3943624 -8.4020634 -7.4294434][-8.06368 -7.4324555 -6.2047472 -5.0108128 -4.9666538 -5.7340941 -7.36759 -7.5619092 -7.630527 -7.7774868 -8.0354919 -7.7393627 -7.8585644 -7.4592595 -6.7830286][-6.3839493 -6.2406335 -5.3528533 -4.6047482 -3.7515421 -4.0240579 -4.3531275 -5.1557093 -5.8781343 -5.815309 -5.9756551 -6.8694611 -7.5632758 -8.2540112 -8.1357679]]...]
INFO - root - 2017-12-15 11:50:24.174410: step 8410, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 58h:28m:29s remains)
INFO - root - 2017-12-15 11:50:30.816054: step 8420, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 57h:03m:07s remains)
INFO - root - 2017-12-15 11:50:37.429177: step 8430, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 59h:30m:24s remains)
INFO - root - 2017-12-15 11:50:44.032701: step 8440, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 58h:15m:24s remains)
INFO - root - 2017-12-15 11:50:50.597381: step 8450, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 59h:45m:23s remains)
INFO - root - 2017-12-15 11:50:57.129287: step 8460, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 57h:45m:39s remains)
INFO - root - 2017-12-15 11:51:03.749341: step 8470, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 61h:12m:10s remains)
INFO - root - 2017-12-15 11:51:10.437195: step 8480, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.681 sec/batch; 61h:16m:14s remains)
INFO - root - 2017-12-15 11:51:17.129777: step 8490, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 58h:14m:32s remains)
INFO - root - 2017-12-15 11:51:23.739053: step 8500, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 59h:06m:49s remains)
2017-12-15 11:51:24.257847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5855918 -8.0302744 -7.6933393 -7.3039651 -7.9141922 -8.7862568 -8.75982 -7.6854811 -6.9587388 -6.3585458 -5.4287777 -6.4863105 -9.6003456 -10.075911 -9.1333656][-6.4037757 -6.2778091 -6.3597527 -6.3859863 -7.5844769 -8.8147335 -9.79978 -10.106316 -9.6296272 -8.77502 -7.9451485 -8.6967087 -11.027428 -11.336111 -10.483853][-2.8393898 -3.9392593 -4.8247333 -4.863049 -6.5160718 -7.6399307 -8.6223888 -9.3643379 -9.6088772 -8.9689665 -8.7756681 -9.5102806 -11.569628 -11.99938 -11.361235][-3.1542561 -3.6338158 -2.7583878 -2.4500761 -3.906616 -4.2985477 -4.7312622 -5.8104095 -6.6909518 -7.0431633 -6.7139544 -7.4077215 -10.838022 -11.373317 -10.68326][-4.4721451 -4.7930346 -4.1646523 -2.8417277 -2.03979 -2.0403883 -2.4861429 -3.2192988 -3.2350891 -3.4302604 -3.9835944 -5.5844154 -8.6830359 -10.158808 -10.024639][-7.3781424 -6.3762579 -5.5427566 -4.4660563 -3.1064065 -0.60680819 0.91226435 0.75136518 0.26008797 -0.70322514 -1.4628563 -3.5797396 -7.2541838 -8.9117746 -9.7745285][-8.9935617 -7.6255693 -5.6567154 -3.2864602 -1.988663 0.21751213 2.2377377 3.4870639 4.1076722 2.837254 1.043942 -1.6575799 -5.9344869 -7.3507915 -7.3281407][-8.2564993 -7.839828 -6.6907911 -3.473217 -0.44379234 2.2468629 4.1875854 5.1982188 5.2015939 4.2391829 2.8587251 -0.22286701 -4.5287437 -6.1739492 -6.3584366][-7.8509712 -7.197463 -5.1278324 -2.0008779 -0.079415321 1.497592 3.090961 3.8521638 4.5917439 4.6089997 3.4654469 0.499609 -3.5120814 -4.6239719 -4.4523525][-7.9172425 -6.8684454 -5.0046072 -3.0125864 -1.7423732 -0.33922815 0.83982134 1.7968736 2.6788564 2.4627137 1.5488119 -0.28772068 -3.620749 -3.620785 -3.2295089][-9.6695385 -9.3570089 -8.4284573 -6.272408 -4.9152284 -4.7526488 -4.3405447 -3.8035266 -2.9868932 -2.125402 -1.9084632 -4.1084142 -6.42651 -5.7851357 -3.6613436][-13.432128 -11.793348 -9.2691851 -7.0681233 -6.5454369 -5.8754382 -4.9220452 -5.4062176 -6.1725283 -6.0729713 -6.5685415 -7.5377326 -8.4885588 -6.7024946 -4.7775545][-14.498493 -13.067077 -10.673614 -8.4950027 -7.947238 -7.4554019 -6.9407139 -6.4742537 -6.3827958 -6.1927814 -6.3163266 -7.8752055 -8.6479139 -7.6942935 -5.7019296][-14.269716 -13.223325 -10.042873 -8.7604561 -7.0851989 -6.7512369 -6.7470994 -5.6211891 -4.1824512 -3.953517 -5.0687528 -5.7100897 -6.3074212 -5.9936771 -4.2850285][-9.9959583 -9.7181 -9.343792 -7.5796232 -5.9580059 -6.0795627 -5.3985214 -4.4504108 -3.5911417 -2.3080173 -1.7412279 -1.8648696 -2.9364011 -4.5024862 -5.2910752]]...]
INFO - root - 2017-12-15 11:51:30.791299: step 8510, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 60h:35m:01s remains)
INFO - root - 2017-12-15 11:51:37.358536: step 8520, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 58h:27m:53s remains)
INFO - root - 2017-12-15 11:51:43.919833: step 8530, loss = 0.25, batch loss = 0.21 (11.9 examples/sec; 0.674 sec/batch; 60h:40m:43s remains)
INFO - root - 2017-12-15 11:51:50.413144: step 8540, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 58h:31m:26s remains)
INFO - root - 2017-12-15 11:51:56.940871: step 8550, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 58h:10m:11s remains)
INFO - root - 2017-12-15 11:52:03.448697: step 8560, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 58h:25m:57s remains)
INFO - root - 2017-12-15 11:52:09.986382: step 8570, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.653 sec/batch; 58h:42m:51s remains)
INFO - root - 2017-12-15 11:52:16.549989: step 8580, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 58h:35m:19s remains)
INFO - root - 2017-12-15 11:52:23.144029: step 8590, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.690 sec/batch; 62h:02m:49s remains)
INFO - root - 2017-12-15 11:52:29.784091: step 8600, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 58h:47m:54s remains)
2017-12-15 11:52:30.302616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6128526 -3.9714923 -2.7083125 -1.7319643 -2.0177727 -2.9216666 -3.7959447 -4.2907343 -5.4452071 -6.0174956 -5.707355 -8.1043453 -10.087532 -10.688602 -8.9084988][-6.7145958 -6.6069493 -6.4264488 -4.9641876 -4.3721 -4.8214436 -5.7967896 -7.4806814 -8.8398361 -8.8637333 -8.634634 -10.82865 -11.822954 -11.799894 -10.263399][-4.6822758 -6.1357827 -6.4686456 -4.815721 -4.6977558 -4.7364268 -5.40768 -6.944005 -8.0792236 -8.5543976 -8.4046125 -10.128441 -11.269964 -11.394003 -10.419382][-5.1309752 -5.7457819 -5.5749021 -5.0449643 -5.1979113 -4.8239064 -4.9254332 -6.2819662 -7.5753469 -7.2845011 -6.5204096 -8.2663 -9.6354923 -10.332916 -9.69161][-6.6095428 -6.8204479 -6.2593889 -4.8115773 -4.5354519 -3.8955121 -4.2697077 -5.9501076 -6.7380953 -7.0674295 -6.8033442 -7.7242007 -8.5106506 -9.0863581 -8.3191586][-10.340571 -8.7902794 -6.3246136 -3.9823108 -2.9786196 -1.6878424 -1.3052535 -2.1362391 -3.2242789 -4.7906804 -5.3398046 -6.5338559 -7.4642286 -8.3642282 -7.6764431][-11.577309 -9.5628786 -7.2521582 -3.7436903 -1.8569062 0.3334136 1.7342129 1.810791 1.5431113 -1.1509323 -3.3376827 -6.1830854 -8.441843 -8.6372318 -7.7783947][-8.6495247 -7.2348123 -5.7709827 -2.5825436 -1.1646872 1.1544275 2.9604621 3.6100717 3.9202571 1.5374732 -0.70567989 -4.6835747 -8.0215874 -9.0681267 -9.0081081][-6.649375 -5.1483231 -3.7367766 -1.5249906 -0.89693308 0.9411726 2.0288949 2.6521554 3.2692075 1.8014688 0.088513374 -4.657795 -8.4693 -9.8354006 -9.3789234][-3.9161844 -3.295399 -2.7053154 -1.1358576 -0.64325714 0.17128325 0.81942368 1.4372997 1.9485178 1.1656437 -0.56017542 -4.5700355 -7.9603505 -9.9027319 -9.7138481][-7.6485567 -6.0832505 -4.5849867 -3.8369746 -3.5791564 -2.6407368 -1.9469409 -1.5606356 -1.3170733 -1.6878762 -2.8227167 -6.8260269 -9.3890562 -10.79864 -10.404202][-11.647827 -10.119029 -7.8006639 -6.3497095 -6.0383458 -5.7916956 -5.1662068 -5.0914645 -4.7909317 -4.9828224 -6.296813 -9.1005869 -10.456478 -10.906583 -10.303785][-12.943724 -11.954075 -10.342476 -8.4490919 -8.0280142 -7.2265759 -6.4000807 -6.3951406 -6.8122406 -7.1900754 -7.862752 -9.0264721 -9.666256 -9.2071362 -7.7404232][-11.081018 -10.759177 -9.8909712 -8.9735947 -8.0298462 -7.7312279 -7.6562815 -6.8006263 -6.3707 -6.9337778 -7.817975 -8.0998383 -7.488667 -6.2747416 -6.0372515][-8.9567146 -8.7922554 -8.084507 -7.0774822 -6.1923752 -5.9565887 -5.9711866 -5.6352592 -5.3402843 -5.314754 -5.7632518 -7.038867 -7.7825379 -7.025023 -6.1785855]]...]
INFO - root - 2017-12-15 11:52:36.784030: step 8610, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 58h:28m:39s remains)
INFO - root - 2017-12-15 11:52:43.367617: step 8620, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 57h:01m:55s remains)
INFO - root - 2017-12-15 11:52:49.956384: step 8630, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.683 sec/batch; 61h:25m:31s remains)
INFO - root - 2017-12-15 11:52:56.626895: step 8640, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 59h:44m:46s remains)
INFO - root - 2017-12-15 11:53:03.210949: step 8650, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 59h:21m:56s remains)
INFO - root - 2017-12-15 11:53:09.754608: step 8660, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 57h:49m:35s remains)
INFO - root - 2017-12-15 11:53:16.265329: step 8670, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 57h:27m:34s remains)
INFO - root - 2017-12-15 11:53:22.915449: step 8680, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 61h:21m:45s remains)
INFO - root - 2017-12-15 11:53:29.574625: step 8690, loss = 0.16, batch loss = 0.11 (11.5 examples/sec; 0.697 sec/batch; 62h:40m:23s remains)
INFO - root - 2017-12-15 11:53:36.106358: step 8700, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 57h:24m:12s remains)
2017-12-15 11:53:36.602122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2634313 -2.3689146 -1.6337242 -1.0694695 -1.6732116 -1.6733599 -1.9352496 -2.9550674 -4.112586 -5.297184 -5.8399577 -7.4201 -8.7290916 -9.2311192 -7.6807761][-1.4982233 -0.715888 0.032514095 0.36415243 -0.17304373 -0.46630478 -0.82141495 -1.4727573 -2.3962193 -3.2773194 -3.9856129 -5.6590452 -7.7462511 -8.5468645 -7.6172457][-1.1923523 -0.87445068 -0.71725559 -0.089488983 -0.27157354 -0.064301491 -0.30156708 -0.70096493 -1.2559843 -2.1588824 -2.40087 -4.2357254 -5.9643474 -7.24096 -6.4858751][-3.0370498 -1.8419976 -1.3369598 -0.51365137 -0.17043161 0.19599438 0.43303967 0.5947752 -0.35688734 -0.7723279 -0.82905483 -2.6732252 -4.541503 -6.5673375 -6.7102542][-3.7533526 -3.9433727 -3.9868293 -1.5298586 -0.47998905 0.38306665 0.86681986 0.89452744 0.49709463 0.48073053 0.29103327 -1.7228715 -3.9129376 -6.104322 -6.3741894][-5.2007694 -4.841815 -3.5968981 -0.94111967 1.0579691 2.7105207 3.610878 2.8583803 1.9008355 1.6674747 1.5556388 -0.25328493 -2.125088 -4.3533564 -5.2883797][-5.5155644 -4.716867 -2.8954115 -0.08987999 1.9620209 3.5105724 4.4955988 3.9789867 3.0354567 2.400218 2.3095951 0.43877363 -1.6681771 -3.1779213 -3.2954948][-5.1330581 -3.5610964 -1.8842466 1.2303762 2.8747673 4.5215836 5.2238703 4.4878645 3.6088715 2.6277037 1.9286594 0.57506561 -0.8040657 -2.3970523 -2.3671885][-4.4421463 -2.8151278 -0.92434978 1.549716 2.4384317 2.8420238 3.4784842 3.4526825 2.8270874 2.1197696 0.92520046 -0.84772539 -1.9533162 -3.3741064 -2.6849287][-4.4574528 -3.7145858 -2.706233 0.15895605 0.64488173 0.84101009 1.6469398 0.76575041 0.13317013 -0.88123035 -1.6839161 -2.9709642 -3.7432938 -4.0390472 -3.3789198][-8.22617 -7.2758069 -6.4051046 -4.5469723 -4.2243981 -3.7939513 -3.1089418 -3.1250875 -2.6432164 -3.3546793 -4.1557283 -6.6437049 -7.0312572 -6.3486719 -4.1890297][-11.97958 -11.012798 -9.4482594 -7.2582765 -6.9495478 -6.5072694 -6.4502068 -6.6066127 -5.8802009 -6.16446 -7.0227213 -7.4255543 -6.8738022 -6.5377607 -4.2786036][-12.054853 -10.307516 -9.0481873 -8.1800823 -7.5436716 -6.1541052 -5.8162842 -6.1358585 -6.8967514 -7.4989142 -7.9068837 -7.7895322 -7.230041 -6.2810369 -4.127532][-7.8610764 -6.6884665 -4.7155809 -4.3714762 -4.0245385 -3.9046369 -3.4477136 -3.5048177 -4.2874646 -5.5688114 -7.7128057 -7.3184733 -6.8483639 -5.7064342 -4.4499006][-5.0016994 -3.7256126 -2.4167418 -0.48551035 -0.16245127 -1.0886612 -1.4410987 -1.7785585 -1.9984169 -2.9379175 -4.4223037 -6.1358318 -6.9340754 -6.1405816 -5.7776923]]...]
INFO - root - 2017-12-15 11:53:43.103952: step 8710, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 57h:29m:38s remains)
INFO - root - 2017-12-15 11:53:49.673558: step 8720, loss = 0.20, batch loss = 0.16 (12.0 examples/sec; 0.665 sec/batch; 59h:49m:12s remains)
INFO - root - 2017-12-15 11:53:56.318861: step 8730, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 60h:02m:42s remains)
INFO - root - 2017-12-15 11:54:02.898827: step 8740, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 57h:49m:40s remains)
INFO - root - 2017-12-15 11:54:09.564883: step 8750, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 58h:41m:49s remains)
INFO - root - 2017-12-15 11:54:16.111204: step 8760, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 58h:26m:43s remains)
INFO - root - 2017-12-15 11:54:22.663568: step 8770, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.632 sec/batch; 56h:47m:16s remains)
INFO - root - 2017-12-15 11:54:29.258277: step 8780, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.647 sec/batch; 58h:12m:38s remains)
INFO - root - 2017-12-15 11:54:35.840319: step 8790, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 58h:59m:34s remains)
INFO - root - 2017-12-15 11:54:42.490744: step 8800, loss = 0.16, batch loss = 0.12 (11.5 examples/sec; 0.696 sec/batch; 62h:33m:34s remains)
2017-12-15 11:54:43.005965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7186775 -6.7694178 -8.2458973 -9.2773151 -9.8669186 -9.9346218 -10.081749 -10.333611 -10.88796 -11.00818 -10.33519 -9.7483406 -10.880016 -9.41737 -6.0315857][-7.3686442 -7.6760707 -8.5943222 -10.04676 -11.095808 -11.15699 -10.631832 -10.261381 -11.045633 -11.33087 -10.411496 -9.1158743 -10.052528 -10.167774 -8.33689][-5.43556 -7.7681084 -10.645132 -10.510136 -10.636642 -10.872993 -10.286425 -9.9491844 -10.495317 -10.567478 -10.518482 -9.1880369 -9.7526569 -9.9042149 -8.2024622][-7.3659377 -8.4902792 -9.7488432 -10.020098 -10.197964 -8.023591 -6.2579131 -7.1079721 -8.9737616 -9.6445351 -9.6981163 -8.6248989 -10.297749 -10.299382 -8.6743259][-7.4940743 -9.4747791 -11.718386 -11.460624 -9.6981544 -5.1427488 -1.3730297 -2.2915075 -5.6998267 -7.8332257 -9.8497963 -9.2501259 -10.073026 -9.7908573 -8.26641][-9.7050858 -11.031998 -10.865517 -10.121668 -7.8132114 -1.9155092 3.4549165 3.8477349 1.6109352 -2.3990977 -7.1531253 -7.0183482 -7.8413148 -8.5137835 -7.9094486][-8.8071079 -9.8367424 -8.9146938 -6.6326828 -4.1000257 1.108789 7.182385 8.8875961 7.5459609 2.2169828 -3.5727861 -5.0480852 -7.496491 -7.6388769 -6.2541456][-8.3182411 -10.065917 -9.6699867 -5.2900763 -0.61410475 3.1460509 6.6749177 8.3822975 7.739408 2.879168 -2.2257254 -3.34451 -6.8089819 -8.0271006 -6.7960062][-6.9328051 -7.1913166 -7.9717121 -5.8249555 -1.8090425 3.1692166 6.2793946 5.6782317 5.5937848 2.2263608 -1.8062723 -3.3718183 -7.6285038 -9.5815458 -8.7227755][-5.3822117 -5.0432076 -6.0592704 -5.7397184 -4.2576838 -0.27456808 3.3583646 3.0346837 2.4129071 -0.19459963 -2.8772738 -3.9556005 -7.422626 -9.8584881 -11.019564][-9.2506275 -9.2869959 -9.975564 -9.1469269 -8.5147781 -6.2317257 -3.2427192 -3.2417829 -4.3794088 -6.2277083 -6.9708161 -7.393486 -10.770087 -12.350702 -12.201726][-12.729635 -12.429718 -12.669307 -11.852274 -11.189998 -9.9011059 -9.1949644 -8.39603 -8.5446033 -9.9725533 -11.140969 -11.82118 -12.954792 -13.336338 -13.389648][-11.786469 -11.76873 -12.058805 -11.804599 -11.640827 -10.024738 -9.219141 -9.9630375 -10.703885 -10.351349 -10.121399 -11.153523 -12.333479 -12.560255 -11.88415][-9.8695126 -10.00387 -10.104581 -9.415617 -7.9280572 -7.2880435 -7.7467194 -8.2361317 -9.5192814 -9.6876879 -9.0054855 -8.6087751 -9.4235525 -9.4262724 -9.0683632][-6.71132 -7.4766903 -7.33342 -7.1647224 -5.9531713 -4.7977066 -4.3791132 -4.7600832 -6.4385114 -7.9665966 -8.0581989 -7.32835 -7.8796644 -8.70444 -8.3678732]]...]
INFO - root - 2017-12-15 11:54:49.648792: step 8810, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 58h:56m:02s remains)
INFO - root - 2017-12-15 11:54:56.274246: step 8820, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 61h:37m:50s remains)
INFO - root - 2017-12-15 11:55:02.825165: step 8830, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 57h:50m:52s remains)
INFO - root - 2017-12-15 11:55:09.402537: step 8840, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 59h:00m:33s remains)
INFO - root - 2017-12-15 11:55:15.959779: step 8850, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.644 sec/batch; 57h:51m:49s remains)
INFO - root - 2017-12-15 11:55:22.574524: step 8860, loss = 0.23, batch loss = 0.19 (12.1 examples/sec; 0.661 sec/batch; 59h:26m:22s remains)
INFO - root - 2017-12-15 11:55:29.042725: step 8870, loss = 0.17, batch loss = 0.12 (12.7 examples/sec; 0.629 sec/batch; 56h:32m:48s remains)
INFO - root - 2017-12-15 11:55:35.519234: step 8880, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 59h:22m:08s remains)
INFO - root - 2017-12-15 11:55:42.113559: step 8890, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 57h:56m:43s remains)
INFO - root - 2017-12-15 11:55:48.688328: step 8900, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 58h:05m:54s remains)
2017-12-15 11:55:49.183230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2428169 -5.5337095 -5.5456066 -5.4809909 -5.5094457 -4.9122462 -4.5381889 -4.9121017 -5.3606644 -5.5261769 -5.5759883 -6.8420911 -7.4268475 -7.5118189 -7.3646059][-3.1380284 -3.5754704 -4.1100211 -4.6414595 -4.4402313 -4.2032919 -4.8325009 -4.8769403 -5.2742515 -6.9242015 -7.3412514 -8.4548073 -9.1205769 -9.7961407 -9.50182][-3.661274 -3.6755245 -3.8032937 -4.9051728 -5.4685197 -5.5366788 -5.3946323 -5.1783123 -5.0854053 -6.4592533 -8.4926844 -10.117655 -10.123528 -9.9118319 -9.8121128][-5.6008821 -6.0880513 -5.9052992 -6.3012824 -7.0199261 -7.2579594 -6.4657559 -5.7421536 -5.95235 -6.5579023 -7.3184261 -10.524245 -11.754333 -11.334063 -10.498892][-6.7379336 -7.8434267 -8.0922422 -6.9273005 -6.7837868 -6.2657337 -4.5438919 -4.314548 -4.7758365 -4.91677 -5.5920286 -8.3015375 -9.3188391 -10.382787 -10.959799][-6.4872003 -7.1187263 -6.5182276 -4.95891 -2.9355249 -0.32819939 1.7319269 0.26492643 -0.78422308 -1.9243314 -4.0192394 -6.1667881 -6.9334831 -8.7006912 -9.51726][-7.796567 -7.4393392 -5.5216689 -3.5668483 -0.88314009 3.3289409 6.8278861 6.7414317 5.74777 1.3846521 -2.8576469 -4.3677368 -6.0511909 -7.7141848 -8.4427376][-7.8285456 -7.5711088 -6.065371 -3.76477 -0.0040402412 4.4613147 8.3291492 8.9650745 7.6931272 3.2678924 -1.0521407 -5.0202804 -7.2100215 -7.5166383 -7.9334469][-6.9902353 -6.21326 -5.1268449 -3.9006445 -2.1895044 1.4836574 5.6446776 6.9230647 5.3152385 0.96548319 -2.7444663 -7.1938887 -9.8724594 -10.102705 -9.8583565][-6.0791359 -5.7007751 -5.4577804 -3.376024 -1.6753435 -0.54962778 1.0899811 2.0459185 1.0140119 -2.5507131 -6.32377 -11.050448 -12.909664 -13.54194 -13.650112][-8.3236446 -7.9012594 -7.6985841 -5.8077321 -5.0238609 -4.5777969 -4.1276188 -4.9021854 -5.8635473 -7.2335987 -9.7126884 -13.866026 -16.463894 -16.667763 -15.184574][-13.300055 -12.610413 -11.520144 -10.057319 -9.5702744 -8.9914541 -8.89236 -10.306467 -11.376633 -12.591092 -14.102488 -15.028988 -15.683104 -14.842947 -13.808632][-14.482435 -13.107487 -11.063052 -10.320432 -10.782141 -10.183052 -9.9277763 -10.55855 -10.967294 -11.730065 -12.280878 -13.578833 -13.579948 -11.725779 -9.4962931][-12.971757 -12.142637 -11.052658 -10.167779 -9.480463 -9.6179905 -10.689091 -10.930807 -10.387823 -10.535075 -10.435858 -9.674387 -8.4890375 -8.0647945 -7.5325675][-7.454268 -6.4793034 -5.9758148 -5.7887363 -5.78722 -6.5043387 -7.0027914 -7.3912582 -7.8139172 -7.7184086 -7.5704913 -8.9785471 -9.2048759 -7.3679066 -6.3840327]]...]
INFO - root - 2017-12-15 11:55:55.701190: step 8910, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 61h:27m:30s remains)
INFO - root - 2017-12-15 11:56:02.272367: step 8920, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 57h:43m:34s remains)
INFO - root - 2017-12-15 11:56:08.828182: step 8930, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 59h:03m:00s remains)
INFO - root - 2017-12-15 11:56:15.390785: step 8940, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.684 sec/batch; 61h:27m:41s remains)
INFO - root - 2017-12-15 11:56:22.037923: step 8950, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 57h:33m:24s remains)
INFO - root - 2017-12-15 11:56:28.638576: step 8960, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 59h:26m:20s remains)
INFO - root - 2017-12-15 11:56:35.250473: step 8970, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.674 sec/batch; 60h:33m:37s remains)
INFO - root - 2017-12-15 11:56:41.845104: step 8980, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 59h:38m:57s remains)
INFO - root - 2017-12-15 11:56:48.466115: step 8990, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 61h:22m:02s remains)
INFO - root - 2017-12-15 11:56:55.029232: step 9000, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 59h:24m:14s remains)
2017-12-15 11:56:55.510650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6559629 -5.0860872 -5.5637693 -6.1188021 -7.4340353 -7.8412733 -8.3922071 -8.8341522 -9.1848135 -9.667098 -9.7149773 -9.9415369 -10.685202 -9.7066956 -6.0239038][-6.8012857 -5.501493 -5.928092 -6.3774343 -7.2465563 -8.66877 -9.6096811 -9.6863365 -9.6754713 -9.586544 -9.08378 -9.3072348 -10.577082 -10.749931 -7.2577949][-5.2020617 -5.8065524 -7.0544605 -6.8007679 -7.4918442 -8.13347 -9.1362209 -9.5270748 -9.3214207 -8.8086853 -8.6390591 -8.4392014 -8.8762379 -10.008713 -7.9332657][-5.5018716 -5.2906494 -6.0209293 -6.2424464 -7.3522568 -7.4339395 -7.6763172 -8.4448318 -8.3595486 -7.7654858 -7.8092189 -7.5555191 -8.2890129 -8.37956 -6.396769][-5.7151065 -6.4757767 -7.4838734 -6.6643825 -6.3654566 -5.4611893 -5.007967 -5.6924214 -6.2763233 -6.2414403 -6.6417069 -6.8960209 -8.4037971 -8.93073 -5.9452162][-5.7407184 -5.3543477 -5.3826137 -4.5830436 -3.7686758 -0.60582495 1.8354034 1.4413142 0.59589434 -1.6372361 -3.9449091 -4.5404305 -6.1608844 -7.2006216 -6.0626464][-6.8755331 -6.40089 -6.1859169 -4.4106369 -2.0381124 1.4998374 5.1859379 7.2877054 6.9666719 2.6789627 -1.6972871 -2.7006478 -4.5792689 -6.2982478 -5.6504879][-6.5777235 -6.3859019 -6.0286603 -3.5617311 -1.0152712 2.6637363 5.7201996 8.4514008 9.7960339 4.5995817 -1.5546389 -3.97479 -5.4209805 -6.2807431 -5.4223037][-3.7774916 -3.9582887 -5.0076246 -3.4765599 -1.8344793 1.5974078 4.6738319 6.5587206 7.1487722 4.4244719 0.31323433 -3.573734 -7.5957146 -8.6765137 -6.7391958][-2.7692659 -1.5837078 -3.040971 -2.5671291 -2.0347352 0.024588585 1.192349 2.395484 2.9858494 0.32846069 -2.3750415 -4.9681015 -8.458621 -10.954699 -9.9977655][-4.5961437 -3.7503943 -3.92387 -3.9818 -4.074048 -3.2616975 -3.2975459 -3.4783304 -4.666183 -6.287869 -7.7201419 -9.2112 -11.52223 -12.658107 -10.963188][-8.9690857 -8.1090832 -8.3722267 -7.3554592 -6.9511633 -7.4095392 -8.2103329 -8.3114824 -8.9042292 -10.335043 -11.774807 -12.508093 -13.100558 -12.793705 -9.8211117][-10.177423 -9.4018908 -8.3901548 -7.7192688 -8.3187981 -7.809864 -7.6019311 -8.803709 -9.879858 -10.644922 -11.068108 -11.843636 -11.713135 -10.442087 -7.27913][-10.194273 -9.474575 -8.385458 -7.5241838 -7.8335342 -7.2933664 -7.4876652 -8.0294943 -7.8911448 -8.7629108 -9.0020561 -8.11774 -7.9260731 -7.8438883 -5.9811606][-8.8336849 -7.7964139 -6.8889537 -5.4494448 -4.6123247 -4.282403 -4.5638418 -4.0481391 -4.8681326 -5.6874204 -5.8575692 -6.3346839 -6.9219036 -6.30345 -5.038372]]...]
INFO - root - 2017-12-15 11:57:02.094651: step 9010, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.653 sec/batch; 58h:41m:17s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 11:57:08.615163: step 9020, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.672 sec/batch; 60h:23m:44s remains)
INFO - root - 2017-12-15 11:57:15.223190: step 9030, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 58h:33m:39s remains)
INFO - root - 2017-12-15 11:57:21.733312: step 9040, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 58h:10m:43s remains)
INFO - root - 2017-12-15 11:57:28.275404: step 9050, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 58h:20m:38s remains)
INFO - root - 2017-12-15 11:57:34.922776: step 9060, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 59h:36m:10s remains)
INFO - root - 2017-12-15 11:57:41.521102: step 9070, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.674 sec/batch; 60h:34m:33s remains)
INFO - root - 2017-12-15 11:57:48.070626: step 9080, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 58h:15m:29s remains)
INFO - root - 2017-12-15 11:57:54.606499: step 9090, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 57h:10m:57s remains)
INFO - root - 2017-12-15 11:58:01.115813: step 9100, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 56h:59m:11s remains)
2017-12-15 11:58:01.602041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9881477 -7.1403303 -8.0670462 -8.6166115 -9.7215366 -10.600225 -11.420011 -11.899555 -11.903953 -11.719291 -11.329943 -11.738576 -10.354218 -11.726358 -9.6353016][-10.405409 -8.7633419 -9.0836563 -9.0275993 -9.6573811 -10.736338 -11.641635 -12.380049 -12.572087 -12.27635 -11.795568 -11.519325 -9.80686 -11.873886 -9.5774164][-9.0466566 -9.7230854 -10.860834 -10.495548 -11.193943 -11.105066 -11.471399 -11.803296 -12.311691 -12.617235 -12.399437 -12.48265 -10.671828 -12.676752 -9.7585335][-9.2084284 -9.1272993 -10.971868 -11.214206 -11.223421 -9.4684181 -8.9392424 -10.0876 -10.361156 -10.339851 -11.031343 -11.516688 -9.9771757 -11.85526 -9.2766619][-10.150026 -10.162285 -11.658201 -11.928264 -11.42454 -7.2055035 -4.0661087 -6.4642348 -9.30923 -8.9213448 -9.3426075 -10.689898 -10.17122 -12.35528 -9.0858955][-11.089349 -10.451655 -11.357705 -10.678174 -10.274511 -4.2467785 1.7311573 1.3350754 -1.078372 -5.078104 -8.7872171 -8.2713327 -7.0863838 -11.189623 -10.260274][-14.451194 -12.415907 -11.034325 -8.7441406 -6.9614539 -2.0184813 3.6010809 6.5751743 7.1622419 0.35191345 -6.8484025 -7.8910503 -7.5914183 -9.5842228 -7.7300768][-13.833601 -13.733604 -13.669678 -7.2313485 -2.1248431 1.4929276 5.5207405 7.6142673 9.0472155 4.1213732 -3.1364894 -7.2557964 -8.5918016 -10.134899 -7.8619919][-10.923267 -9.8612289 -11.723047 -8.6218319 -4.5271754 2.8175945 7.06882 6.0836687 6.3923349 2.5658655 -3.7647209 -8.4465981 -10.189999 -12.647738 -10.59695][-8.3400364 -7.0336075 -8.7419214 -8.17378 -7.8502941 -2.1547184 2.3102951 2.9820652 2.8447781 -2.1656089 -6.7333837 -9.6795692 -11.210667 -14.603832 -12.493015][-11.762007 -9.7579517 -9.88117 -9.2580576 -9.7545586 -8.5062361 -6.8440371 -4.8990917 -4.9162812 -7.9979315 -11.142909 -13.600603 -13.574376 -15.535718 -14.02264][-13.914254 -13.311811 -13.830405 -11.986144 -11.616434 -11.871866 -12.637981 -12.237265 -11.935616 -12.606557 -13.340669 -15.224944 -14.720572 -14.787007 -11.678976][-13.857582 -14.150094 -14.256716 -13.463573 -13.476876 -11.935569 -11.797727 -13.085493 -13.202162 -12.546682 -12.636772 -13.748974 -12.919369 -12.978914 -9.6953926][-11.2145 -11.444774 -11.360331 -11.13331 -11.130594 -10.438691 -9.8843136 -9.3781986 -9.4632425 -9.8571177 -9.776165 -9.5667858 -9.1569672 -9.1710005 -7.2305455][-8.1791439 -7.8855891 -7.0710673 -5.9950442 -5.1719584 -6.2411313 -6.2651272 -5.2826529 -5.1341615 -5.3521152 -5.9024611 -6.8268509 -7.30379 -7.4298229 -7.0304022]]...]
INFO - root - 2017-12-15 11:58:08.223537: step 9110, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 58h:19m:49s remains)
INFO - root - 2017-12-15 11:58:14.889208: step 9120, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 61h:26m:22s remains)
INFO - root - 2017-12-15 11:58:21.468593: step 9130, loss = 0.19, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 56h:46m:07s remains)
INFO - root - 2017-12-15 11:58:28.065479: step 9140, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.629 sec/batch; 56h:29m:21s remains)
INFO - root - 2017-12-15 11:58:34.698948: step 9150, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 59h:07m:00s remains)
INFO - root - 2017-12-15 11:58:41.270148: step 9160, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 58h:44m:47s remains)
INFO - root - 2017-12-15 11:58:47.843003: step 9170, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 57h:46m:15s remains)
INFO - root - 2017-12-15 11:58:54.555837: step 9180, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.686 sec/batch; 61h:37m:22s remains)
INFO - root - 2017-12-15 11:59:01.113121: step 9190, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 60h:54m:17s remains)
INFO - root - 2017-12-15 11:59:07.698025: step 9200, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.633 sec/batch; 56h:49m:21s remains)
2017-12-15 11:59:08.334046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7554929 -2.996743 -2.3367157 -1.7096088 -2.3032045 -2.6747851 -2.675601 -3.2705703 -3.3405182 -2.9790497 -3.0982766 -4.393877 -5.149 -7.3977318 -8.97946][-2.2739487 -1.6648974 -1.2209129 -1.354104 -1.5202699 -1.8745687 -2.7387972 -3.141829 -3.4174805 -3.5004036 -3.3735044 -4.4445915 -4.4518719 -5.4269013 -6.7158895][-1.3026524 -1.1928673 -0.98873186 -0.48808336 -0.87818527 -1.1182456 -1.139843 -1.846323 -2.3979006 -2.3158474 -2.4649582 -3.6754823 -4.3356223 -5.4683733 -5.8211331][-1.043448 -1.3333712 -1.3277221 -0.82160234 -0.723135 -0.6971221 -0.44220638 -0.40681839 -0.49977064 -0.054517746 0.25349903 -0.88703918 -2.3461235 -4.4910207 -6.0977583][-0.58615637 -1.621624 -1.9035242 -1.71084 -2.125824 -1.0752583 -0.270298 0.85677767 1.8597088 1.6551452 1.2045836 0.048540592 -1.5078464 -3.3216832 -4.4936233][-3.1248841 -3.1680975 -2.8359818 -1.27877 -1.0306463 -0.49959898 0.1213088 0.823956 1.4069743 2.4274526 3.1399302 1.4286008 -0.2390461 -2.0005291 -3.2788086][-4.6763268 -4.1397972 -3.1983178 -2.3369622 -1.2812333 0.45220089 1.1822906 1.1521649 1.5400395 1.9872088 2.4129539 1.0873327 -0.39738226 -2.3941474 -3.255743][-5.4989076 -5.1292076 -3.359268 -1.2803926 -0.32360697 0.81564808 1.5985866 1.509068 1.8750653 1.4809647 1.0949187 0.40497684 -0.20333624 -1.9108379 -2.5831127][-5.1728463 -4.8522239 -3.6420643 -1.0217695 0.56977463 1.1016726 1.4298496 1.4551983 1.7272072 1.7514119 1.7455258 -0.32375336 -2.0648704 -3.4628875 -3.3660409][-3.9770131 -3.597162 -2.5440829 -0.5799799 -0.17106867 0.66809511 0.67412424 -0.1185298 -0.1302228 -0.15526867 -0.087099075 -1.2159429 -2.4563725 -4.392262 -5.1483727][-5.5788379 -4.5962014 -3.5669477 -1.8827982 -1.6330981 -1.923053 -1.8938138 -2.3200195 -2.0117705 -1.8097379 -1.7843828 -3.4286802 -4.6957316 -5.3541102 -5.3181458][-8.4912682 -7.4547362 -5.4155674 -3.8915706 -4.0348711 -4.0293288 -4.2682753 -4.3349743 -3.8590937 -3.670469 -3.5771317 -4.8621454 -5.964004 -5.8826094 -5.0840669][-10.707817 -9.5116711 -8.4274521 -6.4668622 -5.7252226 -5.8776722 -6.4286351 -6.7576952 -6.5638919 -5.8657112 -5.65433 -6.6000504 -7.4052591 -6.6964951 -4.6730309][-6.7157626 -7.2494411 -6.9972348 -5.2648449 -4.1420579 -3.5250831 -3.8473444 -4.5056057 -4.8945332 -4.8934488 -4.9766297 -4.6714277 -4.7273426 -5.17115 -5.2611127][-2.7577012 -3.3229222 -2.9766598 -2.4769843 -2.0346224 -1.4789028 -1.1747041 -1.4421577 -1.6315379 -2.2340264 -2.6992853 -3.560081 -4.7877445 -5.1593251 -5.1129541]]...]
INFO - root - 2017-12-15 11:59:14.872465: step 9210, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 59h:35m:03s remains)
INFO - root - 2017-12-15 11:59:21.539169: step 9220, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 60h:26m:45s remains)
INFO - root - 2017-12-15 11:59:28.093034: step 9230, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 58h:04m:44s remains)
INFO - root - 2017-12-15 11:59:34.631842: step 9240, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 58h:26m:53s remains)
INFO - root - 2017-12-15 11:59:41.317426: step 9250, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 59h:44m:34s remains)
INFO - root - 2017-12-15 11:59:47.959733: step 9260, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 58h:50m:35s remains)
INFO - root - 2017-12-15 11:59:54.492514: step 9270, loss = 0.18, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 56h:33m:55s remains)
INFO - root - 2017-12-15 12:00:01.101629: step 9280, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 59h:48m:08s remains)
INFO - root - 2017-12-15 12:00:07.655864: step 9290, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 59h:27m:40s remains)
INFO - root - 2017-12-15 12:00:14.240045: step 9300, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 58h:44m:27s remains)
2017-12-15 12:00:14.728550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.9201326 -9.9434357 -9.6308012 -10.344442 -11.521658 -11.589615 -12.221665 -11.918015 -10.614983 -10.056827 -9.3909616 -7.64115 -8.8313179 -7.111145 -6.4691482][-7.5380788 -9.5895405 -9.121541 -8.0435581 -8.2627449 -8.7158689 -9.7203922 -10.623125 -11.081688 -10.895113 -9.6754932 -8.1712036 -9.6295938 -8.8756256 -8.5017414][-6.7076283 -8.1526146 -9.3193512 -8.6734018 -8.415266 -8.3034163 -8.7090015 -9.391573 -10.049451 -10.47204 -9.6156645 -7.9237289 -9.1745481 -8.9300032 -8.943718][-6.9436917 -6.525279 -6.6293945 -7.091238 -7.0982342 -6.8092766 -6.7006388 -7.7346516 -9.0771618 -9.0528011 -8.1493521 -7.647233 -9.0156612 -8.6010113 -8.4223614][-8.05399 -7.005518 -6.5457888 -6.0768533 -4.0962481 -1.3828125 -0.0099601746 -3.7185469 -7.6760097 -7.2625518 -6.4772854 -5.8814411 -7.8529339 -8.532259 -8.8549118][-10.388819 -9.500268 -6.9965844 -4.6174335 -1.1612124 3.3102241 7.2925439 3.603107 -1.7266393 -4.521759 -7.202569 -4.7772026 -5.5138135 -7.0813837 -8.1521053][-11.511928 -10.732959 -8.2834568 -2.8309462 -0.15147209 2.9937148 9.236969 8.6438942 5.4846225 -0.51400423 -7.1176343 -5.5217247 -6.4011159 -6.8384323 -7.5329132][-11.294923 -10.01663 -7.802598 -2.8542435 0.81608009 3.9036756 8.1392841 7.8740296 7.7476106 2.5563712 -3.7509234 -5.0968156 -8.8229465 -8.7373934 -7.9571342][-8.4707966 -7.8175249 -7.0251346 -4.5747628 -1.6658807 3.4292989 5.4668584 4.1666255 4.6380372 1.4409547 -2.552325 -5.0759244 -10.532833 -11.176294 -11.413437][-6.6027784 -6.1933022 -6.574182 -6.038712 -4.1124816 -0.7901907 0.97289896 1.3053074 1.0284872 -1.877871 -3.7301502 -5.080328 -9.608901 -10.697655 -13.076727][-9.6847744 -9.2735357 -9.323288 -9.3274136 -8.4078979 -7.5339317 -5.8626447 -4.0639491 -4.4844408 -6.0257382 -7.1293855 -8.1547108 -10.372952 -11.015543 -12.349118][-14.982567 -13.811577 -12.468235 -12.94357 -12.85684 -12.628683 -12.371521 -10.452341 -10.185135 -9.8485184 -9.6806259 -9.3352385 -10.281005 -11.6675 -11.499484][-13.931955 -13.668646 -12.668407 -12.986347 -12.621649 -11.874456 -12.011512 -11.650053 -11.969865 -10.633329 -9.080142 -8.3230124 -9.4968491 -9.7480736 -9.1641788][-12.541964 -11.629293 -11.670007 -11.845284 -10.47303 -11.039711 -10.775208 -10.016819 -10.399393 -10.716076 -10.614172 -7.9075308 -7.9527636 -6.8600378 -7.1260171][-8.7758389 -7.7607536 -6.7312193 -6.0764718 -5.1062412 -4.9789624 -4.9572721 -5.7932405 -6.8025961 -6.5059423 -7.1147032 -6.4160533 -7.418479 -7.013073 -7.2376943]]...]
INFO - root - 2017-12-15 12:00:21.351425: step 9310, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 58h:40m:28s remains)
INFO - root - 2017-12-15 12:00:27.972271: step 9320, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 60h:40m:01s remains)
INFO - root - 2017-12-15 12:00:34.537814: step 9330, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 60h:49m:03s remains)
INFO - root - 2017-12-15 12:00:41.133706: step 9340, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 59h:04m:01s remains)
INFO - root - 2017-12-15 12:00:47.699477: step 9350, loss = 0.29, batch loss = 0.25 (11.9 examples/sec; 0.671 sec/batch; 60h:16m:10s remains)
INFO - root - 2017-12-15 12:00:54.313283: step 9360, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 61h:04m:46s remains)
INFO - root - 2017-12-15 12:01:00.918794: step 9370, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 61h:02m:08s remains)
INFO - root - 2017-12-15 12:01:07.419870: step 9380, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 60h:13m:50s remains)
INFO - root - 2017-12-15 12:01:14.133922: step 9390, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 60h:55m:26s remains)
INFO - root - 2017-12-15 12:01:20.746500: step 9400, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 58h:58m:15s remains)
2017-12-15 12:01:21.262760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2046828 -6.823669 -5.5358367 -4.6105833 -4.6641226 -4.6489153 -4.5329738 -4.4432244 -4.4936638 -4.5233011 -4.2755389 -6.0272341 -7.7302141 -8.1815224 -7.3635116][-6.2599325 -5.9986377 -4.889863 -3.9170043 -3.4907098 -3.7628326 -4.0499387 -4.19837 -4.336154 -4.6056547 -4.0141382 -5.6293006 -7.1854119 -8.0680313 -7.54171][-4.2731667 -4.860714 -4.9643164 -3.8469305 -3.5376334 -3.3570707 -3.5915663 -4.0631123 -4.5961928 -4.7940512 -4.1145506 -5.6783748 -7.169641 -7.9077992 -8.1475544][-5.7197275 -5.6854219 -5.5986876 -5.2859292 -5.0598068 -3.883966 -3.6244013 -4.3294921 -4.8123088 -4.3781195 -3.5821056 -5.5274415 -7.4011221 -8.5936108 -8.6842937][-5.5029383 -6.2965245 -6.6247063 -5.9151549 -4.9886446 -2.6264863 -1.0403557 -1.9646564 -3.136632 -3.5837741 -3.2727487 -5.1188359 -7.0334935 -8.966608 -9.268301][-7.2223644 -6.925981 -6.4170542 -4.5821509 -2.756846 0.056282997 2.6067648 2.2611055 0.54855061 -1.7090182 -2.9684744 -4.7901545 -6.83152 -8.3881741 -8.9587][-8.7681866 -8.0452328 -6.819983 -3.5971529 -0.5821104 2.4089842 5.2640676 5.4210844 3.8527317 0.24417162 -2.4634576 -5.0911779 -7.3209405 -8.4479351 -8.6817913][-8.3664036 -7.7194481 -6.5361419 -3.2016294 0.26307631 4.084012 6.7767715 5.5919018 4.304575 1.4649711 -1.2144465 -4.5922346 -7.3536072 -8.6679564 -8.8820877][-6.9535947 -6.4156361 -5.5936909 -3.1195807 -1.0146222 2.1264987 4.4130106 3.5204153 2.1717939 -0.078298569 -1.337378 -4.5999713 -7.9160438 -9.246151 -8.8497877][-5.2477007 -5.1756043 -4.5196948 -2.8941031 -2.2133245 -0.61147165 1.2653813 0.91752338 -0.754087 -2.5602219 -3.45786 -5.7721815 -7.8550982 -9.447278 -9.7628231][-8.1898441 -7.7979383 -7.1859632 -5.4571748 -4.729351 -4.5804563 -4.4762225 -5.0191045 -5.2535338 -6.0883551 -6.7300606 -8.8836021 -9.9130554 -10.609694 -9.6309233][-12.774889 -11.766727 -10.466555 -9.2612782 -8.3462009 -7.74897 -8.29039 -9.02655 -9.1990547 -9.5459557 -9.4612494 -10.386834 -10.560625 -10.522923 -9.2756548][-12.809124 -11.540717 -10.401189 -10.279207 -10.019182 -8.6025486 -8.18734 -8.6927891 -9.440136 -9.3119926 -8.9082651 -9.118784 -9.1852741 -8.56465 -6.9817963][-10.355675 -9.3174028 -8.5141382 -8.2200289 -7.8846378 -8.0504608 -7.9025755 -7.0123463 -6.7047334 -7.0207767 -7.0634251 -6.3748856 -6.3310366 -6.0696044 -5.6326776][-7.4886613 -7.1138277 -5.9973474 -5.3868561 -5.02156 -5.1602068 -5.0770416 -5.1986685 -5.1325512 -4.6623807 -4.5435991 -5.2933092 -5.81988 -6.3638582 -6.5580654]]...]
INFO - root - 2017-12-15 12:01:27.830458: step 9410, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 58h:35m:19s remains)
INFO - root - 2017-12-15 12:01:34.385928: step 9420, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 61h:56m:00s remains)
INFO - root - 2017-12-15 12:01:40.877324: step 9430, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 59h:18m:01s remains)
INFO - root - 2017-12-15 12:01:47.493263: step 9440, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.648 sec/batch; 58h:06m:30s remains)
INFO - root - 2017-12-15 12:01:54.005796: step 9450, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 57h:09m:59s remains)
INFO - root - 2017-12-15 12:02:00.598951: step 9460, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 58h:12m:26s remains)
INFO - root - 2017-12-15 12:02:07.207725: step 9470, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.671 sec/batch; 60h:14m:59s remains)
INFO - root - 2017-12-15 12:02:13.806370: step 9480, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 58h:45m:07s remains)
INFO - root - 2017-12-15 12:02:20.473284: step 9490, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 60h:04m:58s remains)
INFO - root - 2017-12-15 12:02:27.138500: step 9500, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.681 sec/batch; 61h:05m:30s remains)
2017-12-15 12:02:27.639137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7754188 -2.3188112 -1.6741791 -0.72650003 -1.3292627 -2.763571 -3.9848022 -4.9438925 -4.923079 -4.8299637 -4.9051471 -5.7223358 -7.7106652 -7.3734989 -7.6677942][-2.0501807 0.088474274 1.8786664 2.5318022 1.6977978 0.1957674 -1.4852934 -2.8509061 -4.4816289 -5.3845935 -6.1443563 -8.4704037 -10.442768 -10.217979 -10.425863][-2.1765647 -0.55899668 0.32187986 1.9809976 1.9639874 1.1722031 0.10516834 -1.6349812 -3.277951 -4.9842544 -6.7131996 -9.4199247 -11.488666 -11.842842 -10.473946][-3.5406663 -2.6744695 -0.91293526 1.1386304 1.0623932 0.55785704 -0.066885471 -0.8664875 -2.1786957 -3.9471056 -5.6499352 -8.6058445 -10.872181 -11.297937 -10.640025][-4.2997227 -4.667151 -3.9276128 -1.3614635 0.097110271 0.82797623 0.86188078 -0.70525122 -2.4677012 -3.5727651 -4.544795 -6.5927696 -9.2134151 -9.7134085 -9.3292961][-5.6240807 -5.2546005 -4.1119709 -1.9565532 0.060481071 0.9557066 1.235796 0.81853247 -0.079247952 -1.1571436 -2.2985563 -3.5639551 -5.69928 -6.5152936 -7.2828407][-5.3358736 -4.5614147 -3.1152434 -0.86822271 0.66722918 2.056447 3.0816336 2.6051636 1.7826319 0.59797812 -0.34725761 -1.8349617 -3.9168334 -4.7133908 -4.9508691][-5.3819361 -4.070116 -2.244765 0.10676336 1.0076628 2.5120525 3.7561283 3.4433126 2.8864326 2.2547312 1.6025262 -0.87504435 -3.3249178 -3.7945261 -4.1270237][-6.3967762 -4.4570928 -2.4502153 -0.11592817 0.89013481 1.891995 2.1816626 1.7407265 1.8821621 1.7379851 1.3192558 -0.61289787 -2.8300135 -2.9429989 -3.6302032][-6.4910617 -4.7140059 -3.3762012 -1.5486627 -0.89399958 0.27048779 0.97202873 1.0066719 1.5582852 1.7069335 1.7300644 -0.26899815 -3.312541 -4.1503115 -5.9497433][-10.571836 -9.112566 -7.1606851 -5.1000714 -3.7185705 -3.25672 -2.7577441 -2.1126332 -1.8591962 -1.28718 -0.92507792 -3.7707071 -5.7373033 -5.7626534 -6.1351995][-13.894293 -12.663477 -10.471533 -7.8816109 -6.8502741 -5.8226566 -5.4642935 -5.1872454 -4.7590833 -4.8003087 -4.6204343 -6.3114591 -7.1495409 -7.6248407 -7.9521046][-14.038154 -13.339978 -12.15424 -10.017219 -8.842618 -7.8304448 -7.730051 -7.3308764 -7.8232622 -8.0403652 -7.7578993 -8.1331453 -7.9770317 -7.3686829 -6.5618749][-11.726783 -11.462419 -10.256716 -8.6758671 -7.9770546 -7.4341993 -7.3401546 -6.7849121 -7.3747129 -7.6410365 -7.7990608 -6.6128478 -6.3444958 -6.44504 -6.6159244][-7.1900949 -7.5178251 -7.5724277 -6.887692 -5.4877496 -4.7729716 -4.2619867 -4.3386612 -5.0410051 -5.0136557 -5.4823322 -6.4753942 -7.6183176 -7.3937259 -7.4045711]]...]
INFO - root - 2017-12-15 12:02:34.212846: step 9510, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 57h:57m:02s remains)
INFO - root - 2017-12-15 12:02:40.793498: step 9520, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.633 sec/batch; 56h:46m:05s remains)
INFO - root - 2017-12-15 12:02:47.380781: step 9530, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 59h:17m:18s remains)
INFO - root - 2017-12-15 12:02:54.017364: step 9540, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 60h:07m:53s remains)
INFO - root - 2017-12-15 12:03:00.622127: step 9550, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 57h:44m:17s remains)
INFO - root - 2017-12-15 12:03:07.143956: step 9560, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 58h:09m:22s remains)
INFO - root - 2017-12-15 12:03:13.753016: step 9570, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.677 sec/batch; 60h:43m:46s remains)
INFO - root - 2017-12-15 12:03:20.314008: step 9580, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 59h:56m:36s remains)
INFO - root - 2017-12-15 12:03:26.953853: step 9590, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 58h:28m:46s remains)
INFO - root - 2017-12-15 12:03:33.533702: step 9600, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 57h:40m:09s remains)
2017-12-15 12:03:34.038625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9473104 -4.1329684 -5.0906615 -6.6662359 -8.7365713 -9.8081627 -10.465856 -9.8304749 -8.3151941 -7.6034913 -7.3726668 -7.863122 -10.405487 -10.913601 -9.8438616][-5.287919 -5.6191235 -4.9505272 -4.3881626 -5.34416 -7.932929 -9.6554966 -9.39061 -8.8822184 -7.1534014 -5.56983 -6.4695621 -8.0327177 -8.5786667 -8.3930826][-4.0210743 -4.7434616 -5.4173331 -5.3742871 -5.9450507 -6.2398157 -6.8307619 -6.3252387 -5.3301382 -5.3294196 -4.9568987 -6.788105 -8.9327307 -8.8472862 -8.1302252][-5.3444414 -5.7433405 -5.7959743 -5.1800294 -5.6542921 -4.8865504 -4.279707 -4.2838659 -4.0332279 -3.8507547 -4.1997685 -6.89084 -8.247076 -8.5015163 -7.568171][-3.7810814 -5.1468248 -6.4231491 -5.4471874 -4.4930773 -2.5840812 -0.52699804 -0.38080311 -0.91262436 -1.0161381 -1.5302544 -4.8550396 -8.2823315 -8.23235 -6.9390469][-4.2794065 -4.2533522 -4.3743143 -3.4774883 -1.7128603 0.025568008 2.5445561 2.7827926 1.3490324 -0.025442123 -1.2275257 -3.7474711 -6.3593802 -6.8239632 -6.0921593][-4.3940344 -4.0398135 -3.2169154 -1.9381108 0.078323364 3.048265 5.7454057 6.5064588 5.2319779 3.5978265 1.7905574 -1.7353413 -5.2193961 -5.4139037 -4.8702221][-4.8622866 -5.1305141 -4.06409 -1.4583306 0.88180447 3.9253373 7.652041 7.6063256 5.5852008 3.5326972 1.3764091 -1.5417199 -4.156456 -5.184217 -4.510426][-4.3318396 -3.7655313 -4.0078163 -2.7533786 -1.9151661 0.076824665 3.6715035 5.2119021 5.0546212 2.2912278 0.10835838 -3.1975849 -6.2636514 -5.7032118 -4.4755764][-7.7752867 -6.9079504 -6.7846518 -5.8002167 -4.684185 -2.9306955 -0.45713806 0.77060747 0.24738836 -1.6602879 -2.8754029 -5.646666 -7.91203 -7.2260132 -5.0098743][-9.34228 -8.8005352 -8.3707457 -8.2642231 -8.5186968 -7.0703573 -6.0835509 -4.9625478 -4.2496448 -4.3707428 -4.8811307 -7.5999851 -9.6112938 -7.5405416 -4.3307686][-12.940564 -11.536548 -11.510498 -10.30257 -9.85149 -8.4246445 -6.7482767 -6.2362175 -6.3217921 -6.385519 -6.9415646 -7.5876575 -7.6659651 -6.8982925 -5.2469864][-13.886658 -12.048599 -10.55984 -10.510376 -10.714417 -9.2278357 -7.7123728 -6.6909919 -6.2467608 -6.4328141 -6.4300218 -7.3597808 -7.6946974 -5.9916959 -3.2025774][-13.585077 -11.920017 -9.7736053 -9.3050776 -8.5055771 -8.2551928 -7.9878435 -6.2635322 -4.3574452 -4.1211081 -4.5119638 -4.8326325 -5.1272192 -4.7781072 -4.4255304][-9.157196 -9.52761 -8.2996731 -7.4384732 -7.3249483 -5.5625939 -3.3425472 -3.7456498 -3.4263363 -2.4203887 -2.3202024 -3.3613336 -4.4493136 -5.2330232 -5.5071783]]...]
INFO - root - 2017-12-15 12:03:40.649999: step 9610, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 59h:29m:30s remains)
INFO - root - 2017-12-15 12:03:47.261051: step 9620, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 59h:34m:20s remains)
INFO - root - 2017-12-15 12:03:53.920953: step 9630, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 59h:25m:46s remains)
INFO - root - 2017-12-15 12:04:00.514199: step 9640, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 58h:11m:26s remains)
INFO - root - 2017-12-15 12:04:07.140107: step 9650, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 59h:38m:56s remains)
INFO - root - 2017-12-15 12:04:13.747346: step 9660, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 60h:44m:58s remains)
INFO - root - 2017-12-15 12:04:20.381863: step 9670, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 58h:29m:21s remains)
INFO - root - 2017-12-15 12:04:27.098230: step 9680, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 60h:45m:59s remains)
INFO - root - 2017-12-15 12:04:33.761210: step 9690, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 60h:20m:16s remains)
INFO - root - 2017-12-15 12:04:40.425727: step 9700, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 59h:16m:17s remains)
2017-12-15 12:04:40.918496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.5923886 -8.6016064 -7.8914175 -5.7617221 -4.7388887 -4.1513686 -4.1784196 -4.5789185 -4.311964 -3.6917212 -3.6622076 -4.771584 -6.2169948 -5.788712 -5.7275052][-6.1688581 -5.7907724 -4.3484149 -2.8738227 -3.0503595 -3.0480874 -3.3526371 -4.429234 -5.3423967 -5.3250856 -4.9972725 -6.3310947 -8.1164265 -8.2546129 -7.711144][-4.4584236 -4.204298 -3.0775311 -1.7501004 -1.4747624 -2.0854795 -3.2169838 -4.2399454 -5.2820263 -6.0618081 -6.5136986 -7.4564209 -9.2994022 -9.5719643 -9.2258615][-4.0914226 -4.0272975 -3.3537936 -1.6819725 -1.3351321 -1.4230251 -2.3032186 -3.236371 -4.908915 -5.558682 -6.0408177 -7.6760197 -9.9131012 -10.059803 -9.9063082][-4.3803425 -4.1480246 -3.0043519 -1.1512847 -0.92127609 -0.39350796 -0.48679447 -1.1944027 -2.9198792 -3.9675074 -5.1158195 -7.0731239 -10.03048 -10.649098 -10.395398][-4.9796262 -3.4981964 -1.3920083 -0.058209419 0.71906614 1.3419471 1.1700835 0.42013645 -0.91215992 -2.2723563 -4.0502148 -5.89593 -8.6724119 -9.273941 -9.8799953][-3.9964848 -3.2338755 -1.3821898 0.818223 1.659987 1.6337976 1.8576188 1.3248544 0.28344774 -0.68473291 -1.6892657 -4.4879284 -7.7702055 -7.6851854 -7.5841379][-4.7307105 -2.9294508 -1.6913519 -0.21368408 1.1614513 1.8936296 2.0222878 1.2227731 0.54150629 -0.60124111 -0.97429705 -2.3030083 -4.0050154 -4.6541352 -5.1893873][-4.6875539 -2.9308038 -1.5849533 -0.25371122 0.71825504 1.2426105 1.3790202 0.78373814 -0.18194056 -0.00050210953 0.20244837 -1.0760956 -2.6297128 -2.4968572 -2.2784774][-6.0117526 -4.6827121 -3.6409945 -2.161978 -1.3912854 -0.85089016 0.33191776 0.97017479 0.45111084 0.15241957 0.82824564 0.71086359 -0.38910675 -0.22784424 -0.4548974][-11.464218 -10.241862 -7.7080951 -7.7088218 -7.4563837 -5.7412682 -4.1375871 -2.2439687 -1.5511746 -1.2691889 -0.85037661 -0.28526306 -0.041165352 -0.061180115 0.12785292][-13.423247 -13.441574 -12.614123 -11.040989 -10.555777 -9.174902 -8.0255976 -6.2243814 -4.6984682 -4.0681176 -2.5586286 -1.8209546 -1.6551471 -0.84201288 0.17792559][-12.053917 -10.935736 -10.289709 -9.6989746 -9.3384533 -7.8799152 -6.9582419 -5.7439504 -5.618856 -5.8727846 -4.6309657 -2.6866796 -1.2938361 -0.49406481 -0.9394145][-10.203189 -9.0119686 -6.7387819 -6.191896 -6.4699864 -6.0317731 -7.151474 -6.0658007 -4.9946508 -4.9505997 -4.9635983 -3.4408903 -2.6470919 -1.7605 -2.1006842][-6.2393565 -6.4546509 -5.8301554 -5.2711124 -6.06558 -6.014276 -5.7339449 -4.972806 -5.228097 -4.9754944 -3.7709503 -3.2604766 -3.9948545 -5.1384439 -5.4715333]]...]
INFO - root - 2017-12-15 12:04:47.451738: step 9710, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 59h:15m:09s remains)
INFO - root - 2017-12-15 12:04:53.984658: step 9720, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 57h:56m:10s remains)
INFO - root - 2017-12-15 12:05:00.626540: step 9730, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 59h:01m:39s remains)
INFO - root - 2017-12-15 12:05:07.274540: step 9740, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 58h:49m:27s remains)
INFO - root - 2017-12-15 12:05:13.873947: step 9750, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 58h:48m:47s remains)
INFO - root - 2017-12-15 12:05:20.448671: step 9760, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 57h:53m:49s remains)
INFO - root - 2017-12-15 12:05:27.116008: step 9770, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 58h:37m:01s remains)
INFO - root - 2017-12-15 12:05:33.750735: step 9780, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 60h:18m:28s remains)
INFO - root - 2017-12-15 12:05:40.360589: step 9790, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 59h:04m:24s remains)
INFO - root - 2017-12-15 12:05:46.944887: step 9800, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 59h:24m:00s remains)
2017-12-15 12:05:47.468377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2546723 -4.2187181 -5.8353014 -6.3320165 -6.413167 -6.5283065 -6.2985787 -5.9568439 -5.4911308 -5.6387239 -5.9140139 -8.5535221 -12.208212 -11.574047 -10.433877][-5.659039 -5.6938519 -6.2121744 -6.4633512 -7.1584358 -6.6383476 -5.5478125 -4.5997496 -4.3335028 -4.649591 -4.9861488 -7.9296818 -10.910034 -11.7342 -11.11462][-5.4760394 -5.8473673 -6.4382925 -6.7288995 -7.4203525 -6.6866341 -5.6822696 -4.9273505 -4.3291111 -4.3024292 -4.7649021 -7.610146 -10.863897 -10.932008 -10.796667][-4.0066853 -5.0160732 -5.9074531 -6.1117406 -7.0167913 -5.7694144 -4.2906365 -4.2178931 -3.9125619 -3.9882259 -5.096498 -8.2887249 -11.355855 -11.033864 -9.6877985][-3.1925309 -3.7279043 -3.8812881 -3.8117614 -4.156436 -2.9030569 -2.962671 -2.5586803 -2.5467887 -3.6963959 -4.8210015 -8.0180044 -11.249771 -11.028596 -9.4979572][-3.7379689 -3.1189954 -2.4816782 -1.738203 -1.4417071 0.0030431747 0.53870058 0.71676779 -0.25731659 -2.0730484 -4.3337822 -7.7827187 -11.122092 -10.19651 -9.0013494][-4.44802 -4.5987396 -3.6231813 -2.0256076 -0.376307 1.2841105 2.1085243 2.5293097 2.6699252 0.8282032 -1.8868487 -6.1806297 -9.7497959 -9.5758171 -9.677906][-3.7122288 -3.5523973 -2.9222064 -0.7844286 0.85130024 2.1356058 2.2386041 2.9286122 3.4016185 1.9235315 -0.072067738 -4.2553587 -8.4484692 -8.8933039 -8.6899891][-2.553709 -2.1310875 -1.9150305 -0.43114185 0.78208828 2.0325241 1.6842074 1.7998295 1.4686794 1.225965 0.28050661 -4.1101227 -7.9447412 -8.8099213 -8.8043461][-2.6532927 -1.9306738 -1.676569 -0.20109415 1.4768791 2.3332906 1.8467216 2.0314326 1.6388087 0.23393488 -0.37770653 -3.7987556 -7.7797723 -8.3586025 -8.1028309][-3.7437649 -2.9155593 -1.7318077 -1.4337578 -0.51586151 0.85723829 1.7711535 1.6662488 0.7032814 -0.62376833 -2.5708382 -5.3791566 -8.5454969 -9.2802467 -9.0492115][-6.1006079 -5.0654025 -4.5837817 -3.3820498 -2.7342241 -2.5043654 -2.2875593 -2.2550983 -2.755873 -3.5589352 -4.101748 -6.6954064 -8.4934855 -8.8631067 -8.5103254][-8.451395 -6.8184977 -5.6315241 -4.9277039 -5.2108355 -4.9952359 -4.8374057 -4.7034836 -5.2160788 -6.7149 -7.4302359 -8.4687738 -10.050617 -9.0906715 -8.36032][-7.5375628 -7.0853357 -6.2032194 -4.9372559 -5.268291 -5.5359583 -5.7029662 -6.2219939 -7.4498892 -8.1102524 -8.433713 -9.1564007 -9.5230923 -9.3572721 -9.2144222][-5.1153774 -5.0280714 -4.8916726 -5.3311868 -5.8993173 -5.3708916 -4.9158287 -5.3234959 -6.5136175 -7.5299253 -8.9209862 -10.321472 -10.939703 -10.496319 -10.45723]]...]
INFO - root - 2017-12-15 12:05:54.058068: step 9810, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 60h:13m:03s remains)
INFO - root - 2017-12-15 12:06:00.671332: step 9820, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 60h:48m:38s remains)
INFO - root - 2017-12-15 12:06:07.308181: step 9830, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 58h:21m:26s remains)
INFO - root - 2017-12-15 12:06:13.961136: step 9840, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.681 sec/batch; 61h:01m:53s remains)
INFO - root - 2017-12-15 12:06:20.570701: step 9850, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 58h:21m:30s remains)
INFO - root - 2017-12-15 12:06:27.210658: step 9860, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 58h:18m:39s remains)
INFO - root - 2017-12-15 12:06:33.876206: step 9870, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.653 sec/batch; 58h:32m:11s remains)
INFO - root - 2017-12-15 12:06:40.516910: step 9880, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 59h:47m:10s remains)
INFO - root - 2017-12-15 12:06:47.061353: step 9890, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 59h:36m:32s remains)
INFO - root - 2017-12-15 12:06:53.784779: step 9900, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 59h:24m:19s remains)
2017-12-15 12:06:54.323203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2681503 -8.3763256 -9.2228022 -9.7023706 -10.735916 -10.195395 -9.0633488 -8.4224625 -8.5945711 -9.5742779 -11.061038 -13.729954 -15.959265 -13.530603 -11.391611][-6.7060251 -7.4805536 -7.9467864 -8.347353 -9.3454514 -9.7808876 -9.4148912 -8.8115578 -8.9432373 -8.9045124 -8.2099123 -9.9093132 -13.192586 -12.654409 -11.609095][-6.041616 -6.7626786 -7.3915906 -8.0625114 -8.72189 -8.5185213 -8.1743155 -7.7744226 -7.8177633 -8.3695822 -8.7891245 -9.598299 -11.37334 -10.166922 -9.7442341][-5.4119253 -6.4764943 -6.623724 -6.0320096 -5.836194 -5.3582191 -4.9781971 -5.5132041 -6.0377259 -6.3398361 -7.3985367 -9.0345116 -10.552616 -9.1177692 -7.9884644][-5.1640506 -5.6474361 -6.3311563 -5.1124568 -3.6827242 -2.3061504 -1.4558306 -2.672389 -3.1920238 -3.8113093 -5.0626621 -6.5057735 -8.7177572 -8.7667217 -8.8153963][-6.7844844 -5.8890214 -4.4637628 -2.403187 -0.38968754 2.0752153 4.0687332 2.7003345 0.95521879 -1.0568247 -2.7200785 -4.0774107 -6.7220407 -6.6528955 -7.9963923][-6.3589406 -5.0523434 -3.5269639 -1.0873556 1.2926345 3.949234 7.1996813 6.9684992 5.4989624 2.16709 -1.2748513 -2.8926146 -4.9342852 -5.2760487 -6.7036266][-6.3921409 -4.4478927 -2.8179016 -0.23646307 2.8876209 5.6302557 8.1122446 6.5638123 5.46342 3.43511 0.82178974 -2.1567264 -5.877387 -5.2380972 -5.5082579][-6.1427317 -4.7207041 -3.4305356 -0.75095034 0.94777966 3.8009195 6.4320316 5.8433456 4.8379602 1.972537 -1.021637 -4.0126739 -8.672246 -8.5363293 -8.375761][-6.6719432 -5.4985275 -5.0841789 -2.9707823 -1.8711457 0.36613131 2.9014664 3.1172533 1.8370199 -0.093919754 -2.7016356 -7.28938 -12.057367 -12.384171 -13.265955][-9.4821424 -9.2215014 -7.9777756 -5.7019811 -4.5648689 -3.4657955 -2.4528458 -2.1527197 -2.8843408 -4.2568164 -6.3863072 -10.61443 -15.407475 -17.07674 -16.582081][-11.177279 -10.970913 -10.245594 -7.9477377 -6.8743343 -5.2046013 -5.3577771 -6.5964732 -7.5185695 -8.0110283 -9.9033394 -12.47419 -14.909984 -16.432072 -15.93102][-12.295864 -10.740204 -9.7051907 -7.8088722 -7.7184134 -6.448267 -6.2323365 -6.1249356 -7.1219053 -8.57316 -9.8894138 -12.1 -14.14019 -13.525015 -11.53475][-11.077808 -9.9750671 -8.5405321 -7.5847263 -7.4158907 -6.8301954 -6.5158825 -5.8647537 -5.5726724 -6.2797203 -7.5015993 -8.7485628 -8.9758406 -7.587492 -6.7609634][-7.3017731 -6.601717 -5.4080606 -3.4571154 -2.4246461 -3.6047761 -4.3847694 -4.5286379 -4.7343121 -5.1192865 -5.1911387 -5.8374882 -6.3594937 -5.6036458 -5.1059904]]...]
INFO - root - 2017-12-15 12:07:00.954725: step 9910, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.681 sec/batch; 60h:59m:24s remains)
INFO - root - 2017-12-15 12:07:07.583033: step 9920, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 59h:42m:24s remains)
INFO - root - 2017-12-15 12:07:14.162373: step 9930, loss = 0.20, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 56h:54m:19s remains)
INFO - root - 2017-12-15 12:07:20.702883: step 9940, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 59h:16m:36s remains)
INFO - root - 2017-12-15 12:07:27.314307: step 9950, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 58h:56m:33s remains)
INFO - root - 2017-12-15 12:07:33.897260: step 9960, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 59h:13m:02s remains)
INFO - root - 2017-12-15 12:07:40.571366: step 9970, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 60h:15m:00s remains)
INFO - root - 2017-12-15 12:07:47.212730: step 9980, loss = 0.23, batch loss = 0.19 (12.1 examples/sec; 0.663 sec/batch; 59h:25m:47s remains)
INFO - root - 2017-12-15 12:07:53.875910: step 9990, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 60h:12m:10s remains)
INFO - root - 2017-12-15 12:08:00.532324: step 10000, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 57h:37m:49s remains)
2017-12-15 12:08:01.040961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2340784 -6.3085814 -7.6613517 -8.2226982 -9.0553083 -7.7723689 -6.66159 -6.0075827 -5.4798021 -5.736474 -5.4692836 -6.1377344 -6.1195765 -5.9804416 -7.4919839][-4.4159684 -4.9467273 -5.4818368 -6.597261 -7.1480322 -7.6509447 -7.9816294 -7.0800366 -6.5318532 -6.5974503 -7.1301084 -7.6929817 -7.4578133 -8.1071844 -9.3603354][-3.7102721 -5.2468514 -6.180367 -6.224278 -7.0888 -7.8787374 -8.2616682 -7.73254 -7.3127332 -7.7488008 -9.0449762 -9.9113312 -9.8185768 -8.9129524 -9.7411385][-4.4418478 -4.9142528 -4.929914 -5.3842106 -6.3952274 -7.3702183 -7.5693893 -7.3612967 -7.6721096 -7.7754726 -7.9543052 -10.25149 -11.146805 -10.086988 -10.220503][-5.5024166 -7.1253366 -7.8367448 -6.6862 -6.1489573 -5.2483168 -4.6941476 -6.3729544 -7.7383351 -6.4652104 -7.19141 -9.1766825 -10.07641 -10.620617 -11.290879][-5.54763 -8.0189648 -8.4467659 -6.5006905 -4.3042784 -1.2022972 0.99122334 -0.37340069 -2.9059973 -4.8158531 -6.4680943 -5.9301209 -6.766726 -9.0678387 -11.038752][-6.9845524 -7.5060396 -7.4036975 -5.1797667 -2.8615925 1.0775928 6.0400476 6.6026421 5.021781 -0.85708046 -5.62761 -5.766314 -5.5745983 -6.1655521 -8.7594309][-7.3480706 -7.3167996 -7.578002 -4.9631929 -2.085654 3.323679 8.472291 8.6301 7.7166233 3.8065224 -2.3520231 -5.7134867 -6.4497056 -5.9139843 -7.6099415][-6.3082614 -5.7553797 -4.8826714 -4.3500443 -3.83541 0.74911165 4.8427219 5.7963791 6.4523163 1.5269976 -2.8591802 -5.5196214 -8.6070433 -9.1105824 -9.4201326][-6.0366011 -6.0699182 -5.705369 -3.8163955 -1.8580933 -0.97347069 0.44494104 1.7440505 1.5382066 -2.2753928 -6.8261118 -9.8222322 -12.024679 -12.001043 -13.614202][-8.250824 -8.5439816 -8.4873648 -6.1928592 -4.8680496 -4.8139563 -4.0993366 -4.5503106 -5.4365115 -7.0105481 -9.8981209 -13.428061 -15.446305 -14.994608 -15.771302][-11.521305 -10.866148 -10.510746 -9.8534889 -8.7076721 -7.2478948 -7.2558341 -9.2091093 -10.06966 -11.552429 -13.810931 -15.295087 -15.085269 -14.470275 -14.164649][-14.156473 -12.37488 -10.968795 -10.776976 -10.702684 -9.4482031 -9.1488094 -10.008041 -10.81625 -11.970108 -12.710041 -13.570241 -13.798299 -12.878811 -10.900969][-12.054874 -11.66069 -11.597464 -10.649951 -10.132848 -9.8220634 -10.026074 -9.4119358 -9.9009838 -10.323759 -10.791927 -9.6485748 -8.8497524 -8.5277491 -8.4343624][-6.9393077 -6.7415361 -6.0385957 -5.4171143 -5.4320664 -6.0604138 -6.7597733 -7.0469208 -7.0325761 -6.7905474 -6.7104006 -7.4931993 -8.0974979 -6.4476252 -6.4437127]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 12:08:08.620280: step 10010, loss = 0.21, batch loss = 0.17 (12.6 examples/sec; 0.635 sec/batch; 56h:53m:45s remains)
INFO - root - 2017-12-15 12:08:15.219198: step 10020, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 58h:49m:44s remains)
INFO - root - 2017-12-15 12:08:21.786640: step 10030, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 57h:53m:24s remains)
INFO - root - 2017-12-15 12:08:28.380107: step 10040, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 57h:45m:13s remains)
INFO - root - 2017-12-15 12:08:34.933655: step 10050, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 58h:11m:51s remains)
INFO - root - 2017-12-15 12:08:41.542014: step 10060, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 59h:44m:08s remains)
INFO - root - 2017-12-15 12:08:48.106199: step 10070, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 59h:19m:53s remains)
INFO - root - 2017-12-15 12:08:54.679740: step 10080, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 58h:36m:43s remains)
INFO - root - 2017-12-15 12:09:01.297086: step 10090, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 60h:12m:08s remains)
INFO - root - 2017-12-15 12:09:07.904908: step 10100, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 57h:42m:58s remains)
2017-12-15 12:09:08.498356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7190754 -3.0624189 -2.8926494 -1.8938358 -2.0044467 -3.1410327 -3.882169 -5.1396918 -5.6684051 -5.4351058 -5.1887217 -3.8091564 -4.6026406 -6.2193475 -7.6115503][-5.3100548 -4.91322 -4.084 -4.115139 -4.1941237 -5.2557383 -5.9596009 -6.1952586 -6.3084168 -6.0698266 -5.7504263 -5.5715561 -7.4390745 -6.8026085 -5.5368648][-5.7952576 -5.597971 -5.26788 -5.1392541 -5.6533604 -5.7389526 -5.6375346 -5.7422986 -5.7909045 -5.8578634 -5.9683809 -4.8749094 -5.6755929 -5.8997703 -5.4430165][-3.3727264 -4.223412 -4.8295507 -5.7163606 -6.76792 -6.9619961 -6.9034967 -7.206419 -5.7950196 -5.1103525 -4.62824 -4.36411 -5.8627248 -5.3773346 -5.790946][-4.7550054 -4.8549662 -5.4832106 -5.7671161 -5.1278939 -4.40683 -3.6855266 -3.9847054 -4.3817267 -3.1753879 -2.8056128 -2.7425206 -4.6718216 -5.3816643 -6.3268361][-6.0740185 -4.6588659 -3.2192171 -3.3357208 -3.3085928 -1.0499434 0.90487814 0.73549414 1.0150247 -0.035373688 -0.3253808 -0.66955757 -3.4695923 -4.7602487 -6.5650816][-6.1658597 -6.4294295 -5.0260878 -3.9399891 -2.0593202 0.24150419 3.7284646 6.2574935 7.3409057 4.4932084 1.522296 -0.23625565 -3.626935 -5.3953371 -6.5557704][-6.127511 -7.331728 -5.6257081 -3.2649937 -1.1196961 2.1611514 4.4773469 7.4541945 7.4430327 4.9447713 1.8594747 -1.601707 -5.9704809 -6.2813148 -7.2675018][-7.8078036 -8.6680717 -6.5721612 -4.5466623 -2.4379852 1.2641683 4.0030513 5.6758924 5.990232 4.532414 1.840766 -2.2390292 -6.6282697 -7.1833229 -7.8207092][-7.4067259 -7.6357794 -5.8287077 -4.3995981 -2.3972681 0.40000963 2.376986 3.3415141 2.8405614 0.75759792 -1.9389732 -4.2431111 -6.6752405 -8.5259495 -10.25041][-9.3698912 -8.5576038 -6.6505594 -5.2821717 -4.022718 -3.0416293 -2.4917407 -1.3780808 -1.6972275 -3.449796 -5.6663156 -6.784708 -8.6714115 -7.4330587 -7.209209][-9.7066336 -9.5955954 -8.2268629 -7.2944927 -5.6814523 -5.4505386 -5.2400422 -5.3503294 -6.0612812 -7.3137608 -8.745079 -8.9789343 -9.3033438 -8.8925533 -7.94498][-9.9853754 -9.3848019 -8.554884 -7.8816357 -7.2338343 -6.6370554 -7.2073278 -7.6818147 -8.2316475 -8.7408915 -9.2249546 -9.762516 -10.643893 -8.7001 -7.6643476][-6.4819517 -6.950335 -6.4466739 -6.8799047 -6.9028034 -7.0096827 -7.5134225 -8.010004 -7.6394467 -8.3808308 -8.9634323 -7.654326 -7.3850784 -7.7105179 -8.3139658][-6.3751025 -7.8933325 -7.9577427 -7.2876463 -7.5337405 -6.6641974 -6.4441414 -6.2814903 -5.9393864 -5.8129253 -6.6896844 -6.811069 -7.050364 -7.2197013 -7.7676258]]...]
INFO - root - 2017-12-15 12:09:15.206089: step 10110, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.682 sec/batch; 61h:03m:58s remains)
INFO - root - 2017-12-15 12:09:21.741851: step 10120, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.640 sec/batch; 57h:18m:24s remains)
INFO - root - 2017-12-15 12:09:28.347061: step 10130, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 56h:45m:41s remains)
INFO - root - 2017-12-15 12:09:34.943480: step 10140, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 60h:12m:02s remains)
INFO - root - 2017-12-15 12:09:41.448042: step 10150, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 57h:46m:40s remains)
INFO - root - 2017-12-15 12:09:48.106101: step 10160, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.687 sec/batch; 61h:31m:53s remains)
INFO - root - 2017-12-15 12:09:54.667265: step 10170, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 59h:35m:35s remains)
INFO - root - 2017-12-15 12:10:01.212631: step 10180, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 58h:54m:22s remains)
INFO - root - 2017-12-15 12:10:07.812289: step 10190, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.682 sec/batch; 61h:02m:32s remains)
INFO - root - 2017-12-15 12:10:14.441253: step 10200, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 60h:45m:25s remains)
2017-12-15 12:10:14.943149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5838637 -6.9609013 -6.189445 -4.8795385 -3.9486394 -3.6270564 -3.4875965 -3.4813247 -3.6500397 -3.7948561 -3.7662449 -5.4360251 -8.0545149 -8.9505711 -9.3168135][-4.9868231 -5.4307704 -5.9558716 -5.371129 -5.1595454 -4.5255146 -3.8754592 -3.43797 -3.1257565 -3.1220136 -3.1712878 -4.5621691 -7.9902992 -9.0168371 -8.9599457][-2.9663675 -3.8936486 -4.6542897 -4.1720257 -4.509182 -4.5645928 -4.1556611 -3.8086951 -3.620733 -3.6977761 -3.3028753 -4.9680724 -7.6368895 -8.1965637 -8.43022][-1.6734409 -1.9319994 -2.272887 -2.7312927 -3.3410466 -3.4651754 -3.8255234 -3.5716252 -3.4804132 -3.3129137 -3.2848766 -5.2916369 -8.2600822 -8.9791765 -9.15999][-1.6873589 -2.3377876 -3.0762069 -2.2703853 -1.9061666 -1.6563807 -1.928514 -2.0125339 -2.2054954 -2.3278811 -2.4664505 -4.7848792 -7.99002 -9.0678511 -9.24556][-4.3713632 -4.2418137 -3.1709836 -1.7481916 -0.11157227 1.1747398 1.8450327 1.3912702 0.31028938 -0.57173491 -1.0683618 -3.7784469 -7.6936789 -8.7953072 -9.5185089][-4.9458742 -4.75749 -3.8269205 -1.7042756 0.51607609 2.4655652 3.6291986 3.3372054 1.9342446 0.53462076 -0.47641563 -3.9373579 -7.9222965 -8.9765224 -9.4916573][-4.17092 -4.4798045 -3.4601529 -1.5953288 -0.45138645 1.7208624 3.1581502 2.8797288 2.0906177 0.4774437 -1.128161 -4.1787605 -7.6317358 -9.3583584 -9.5927887][-4.4241295 -4.1528897 -3.2790864 -1.4334888 -0.06188345 0.55085182 0.52318811 0.82119846 0.86903048 0.72511864 -0.065953732 -3.6530533 -7.6183405 -8.4635553 -8.30871][-4.5819726 -4.246551 -3.4983559 -1.5559378 -0.2722373 1.0154605 0.91463518 0.26542711 -0.96293688 -1.6383681 -1.9700296 -4.43332 -7.3678603 -8.4071131 -8.7270184][-7.1869864 -5.9775224 -4.0404778 -2.3766499 -2.0389934 -2.1762946 -3.1523306 -4.1961737 -5.6363754 -5.8357706 -5.9603434 -7.6186938 -9.3319149 -9.7985058 -9.0071888][-9.8003941 -8.5434828 -7.1867504 -5.1102452 -5.1325321 -5.9329696 -6.8631554 -8.1031 -8.8798122 -9.5666227 -9.70196 -10.031395 -10.306572 -10.047344 -9.5889158][-11.82902 -9.3541155 -7.0083938 -6.6305647 -6.553689 -6.8387289 -8.1779032 -9.2270451 -9.4675846 -9.8291416 -9.7701244 -10.086256 -10.824377 -10.219032 -9.2798872][-10.581182 -9.5924959 -8.5533848 -7.1016459 -6.4877062 -6.8994884 -6.9772062 -6.9199219 -7.0545139 -7.7462921 -8.6867332 -8.9910059 -9.4381762 -9.2906332 -9.389082][-9.8622818 -9.6381979 -8.5274982 -7.0704036 -6.4461465 -5.9864535 -5.4935021 -5.4436245 -6.1253362 -6.5138369 -6.7864819 -8.014246 -8.8836575 -9.4239979 -9.9942322]]...]
INFO - root - 2017-12-15 12:10:21.536210: step 10210, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 57h:57m:19s remains)
INFO - root - 2017-12-15 12:10:28.055108: step 10220, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 58h:42m:06s remains)
INFO - root - 2017-12-15 12:10:34.596659: step 10230, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 58h:44m:16s remains)
INFO - root - 2017-12-15 12:10:41.150663: step 10240, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 57h:55m:26s remains)
INFO - root - 2017-12-15 12:10:47.708504: step 10250, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 58h:04m:05s remains)
INFO - root - 2017-12-15 12:10:54.392778: step 10260, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 61h:07m:04s remains)
INFO - root - 2017-12-15 12:11:00.960632: step 10270, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 58h:15m:36s remains)
INFO - root - 2017-12-15 12:11:07.548935: step 10280, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.689 sec/batch; 61h:38m:05s remains)
INFO - root - 2017-12-15 12:11:14.213103: step 10290, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 59h:24m:27s remains)
INFO - root - 2017-12-15 12:11:20.816188: step 10300, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 58h:05m:05s remains)
2017-12-15 12:11:21.286417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.308794 -4.1221232 -4.5274858 -3.3254313 -2.3978083 -2.4398861 -0.959774 0.40362406 1.1654296 -0.3129673 -2.1515608 -6.2296033 -8.462471 -12.850012 -14.0914][-3.3780823 -3.7533996 -4.3324642 -4.1445904 -3.7583752 -2.8977258 -1.0281968 -0.47559404 0.60266972 0.023663044 -1.7004304 -6.4746265 -8.7341938 -14.008104 -15.608366][-3.2235305 -3.7262361 -4.3540797 -3.8977182 -3.9857357 -3.4368258 -1.5950484 0.14835787 -0.14688301 -1.3270841 -2.2944641 -6.8191404 -9.3785868 -13.395815 -15.222731][-2.4188728 -3.0706565 -2.8573148 -1.8469439 -1.4917102 -0.9517169 -0.75090361 0.037298679 0.35631847 -0.16913986 -2.1761725 -6.443367 -8.6994352 -13.644127 -14.869904][-1.6767774 -1.745404 -1.6244702 -1.7089362 -1.4501848 -0.12298536 0.28559446 0.63688469 0.60451841 0.41508198 -1.6994162 -6.790123 -9.4302893 -13.64992 -15.464916][-2.221442 -3.019551 -2.0464494 -1.1878791 -0.30426264 0.75627327 1.6453152 1.8915296 1.2975035 0.028569698 -1.9577963 -6.5621209 -9.9804087 -13.959887 -14.460793][-2.5651667 -3.8911691 -3.6980581 -1.5465074 0.13704014 2.0453153 3.6891437 3.7141423 3.9366174 1.4505882 -1.3873038 -5.5733032 -8.1863794 -12.537307 -13.792473][-4.5076771 -4.4958329 -4.0683422 -2.1181526 -0.042600632 2.7228532 4.6251931 4.8213916 4.822577 3.1991372 0.61544991 -4.4214168 -6.9117522 -11.823729 -13.31152][-5.3251963 -6.6390696 -5.6136417 -2.908679 -1.4177485 1.0791225 3.8820662 4.8748679 4.5361876 2.77041 0.37964153 -4.5163374 -7.31309 -11.603486 -12.726122][-5.0279365 -5.8692155 -5.954423 -3.9882779 -2.0226305 -0.34015656 1.1062236 2.5032535 2.6134534 0.77539539 -1.1282997 -4.8392067 -7.229322 -11.302024 -12.482094][-6.2588706 -6.9038005 -6.5929322 -5.7214332 -5.2023406 -3.8345518 -2.0448439 -1.544466 -1.429141 -2.2552555 -3.5254478 -7.4782166 -9.4416809 -11.779398 -11.638717][-7.2078319 -8.1932993 -8.024147 -7.1472478 -6.4649835 -6.496387 -5.80326 -5.5954566 -5.3082085 -5.464838 -6.821898 -9.0764647 -9.9393673 -11.478222 -12.189963][-9.0393133 -8.8178043 -7.5209842 -7.5922 -8.4260006 -8.2904263 -7.804492 -7.7058973 -7.0739779 -7.2826214 -7.7411184 -8.9379587 -10.747663 -10.783175 -10.535347][-7.946 -8.2318325 -7.632647 -7.0126009 -6.9842863 -8.2566042 -8.975276 -8.0532293 -7.1554747 -7.2679377 -7.3070874 -7.1206 -7.3377151 -9.0325232 -9.9522533][-8.8221769 -8.3587494 -8.01743 -7.6255445 -7.7966433 -7.8618574 -7.8945355 -8.364378 -8.28389 -7.1337929 -6.7823176 -7.834259 -9.2419415 -10.369009 -10.870046]]...]
INFO - root - 2017-12-15 12:11:27.943721: step 10310, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 59h:29m:12s remains)
INFO - root - 2017-12-15 12:11:34.587169: step 10320, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 59h:06m:55s remains)
INFO - root - 2017-12-15 12:11:41.185767: step 10330, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 57h:39m:42s remains)
INFO - root - 2017-12-15 12:11:47.776794: step 10340, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 57h:51m:12s remains)
INFO - root - 2017-12-15 12:11:54.420722: step 10350, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 58h:59m:36s remains)
INFO - root - 2017-12-15 12:12:01.124170: step 10360, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 59h:39m:40s remains)
INFO - root - 2017-12-15 12:12:07.624551: step 10370, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 58h:29m:41s remains)
INFO - root - 2017-12-15 12:12:14.202344: step 10380, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 59h:00m:04s remains)
INFO - root - 2017-12-15 12:12:20.770989: step 10390, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 58h:50m:35s remains)
INFO - root - 2017-12-15 12:12:27.314710: step 10400, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 56h:46m:49s remains)
2017-12-15 12:12:27.803474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7704325 -6.8282366 -6.5131512 -6.5531111 -7.5371265 -8.1932669 -8.6769981 -9.2500019 -9.5350513 -9.5147324 -8.44569 -9.297039 -10.004019 -10.283499 -9.5871906][-5.8996468 -5.0893655 -4.2666397 -3.987432 -5.0120239 -5.8320284 -6.9955144 -7.8066812 -8.6701126 -8.6751709 -7.8795576 -9.74152 -10.879881 -11.4918 -11.362452][-3.1093469 -4.2336235 -5.1243711 -3.2475741 -3.201061 -4.3558779 -5.2124548 -5.1173649 -5.4124184 -5.972549 -6.33744 -8.63754 -10.163313 -11.372988 -11.814558][-5.3879786 -5.75376 -5.5553069 -4.3448215 -4.4942274 -3.9835992 -3.8955269 -4.723691 -5.8429141 -5.6654038 -5.4745564 -7.6717815 -9.2998095 -11.033388 -12.254189][-5.6225295 -7.4776559 -7.6117086 -4.6211209 -3.4915569 -1.6124654 -0.85063791 -3.2719579 -5.7854943 -5.3803287 -4.9911642 -7.2731433 -9.1521931 -11.179913 -12.781288][-9.7041206 -10.336227 -8.520257 -4.4595423 -2.5233569 1.2292314 4.2865925 1.884726 -0.50759363 -2.6528871 -4.39549 -5.4210987 -6.495676 -8.80913 -9.7405882][-12.96876 -12.407722 -9.4658127 -5.0866737 -1.3721242 4.3331089 8.25404 7.0493565 6.0191755 1.371901 -3.0436757 -5.0642629 -6.6856031 -8.144803 -8.7101173][-13.139283 -12.338832 -10.42761 -5.5104284 -0.40081453 5.5958414 9.6100254 9.073101 8.465704 4.1785984 0.075895309 -3.5265257 -6.7220607 -8.48218 -8.6911182][-10.548946 -10.446587 -9.5168447 -4.8507662 -1.0568819 2.1916981 5.2657957 6.0890217 5.6903505 2.8135071 0.52992249 -4.4112539 -8.4147882 -9.0919561 -9.027668][-7.2932596 -8.27681 -9.228631 -7.5134792 -5.0120454 -1.9922333 1.2134252 2.3105593 2.0666614 0.10537958 -2.2175295 -5.9008932 -8.8094292 -9.8053608 -9.9690666][-13.907632 -14.33941 -14.050556 -12.241237 -10.587385 -8.87722 -6.2422585 -5.2593474 -4.610117 -4.20297 -5.25504 -7.9332743 -9.6316071 -9.8098249 -8.8284245][-17.585316 -17.968998 -17.926748 -17.597431 -16.172146 -14.463306 -13.037632 -11.428623 -9.4673519 -8.2911968 -9.2746449 -10.588264 -11.729056 -10.829906 -10.153631][-16.599522 -16.634451 -16.1614 -15.198248 -15.3163 -14.816765 -14.028319 -12.869798 -11.489977 -10.144129 -9.2376289 -9.7755013 -10.831903 -10.909828 -11.385706][-14.95356 -14.197629 -12.819452 -12.854862 -10.993513 -10.639042 -11.26771 -10.758011 -9.9934826 -9.84866 -8.03693 -7.31553 -8.6995373 -9.6540833 -10.777543][-11.053755 -11.995558 -10.943396 -7.9383063 -6.0516243 -5.8675852 -5.894793 -6.74203 -7.3130336 -7.0736136 -6.4514766 -7.1580753 -7.89676 -9.7271252 -11.845337]]...]
INFO - root - 2017-12-15 12:12:34.356345: step 10410, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 59h:35m:33s remains)
INFO - root - 2017-12-15 12:12:40.951435: step 10420, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 59h:28m:35s remains)
INFO - root - 2017-12-15 12:12:47.506010: step 10430, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 59h:04m:46s remains)
INFO - root - 2017-12-15 12:12:54.028808: step 10440, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 57h:38m:48s remains)
INFO - root - 2017-12-15 12:13:00.660166: step 10450, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 59h:55m:44s remains)
INFO - root - 2017-12-15 12:13:07.296850: step 10460, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.682 sec/batch; 61h:00m:28s remains)
INFO - root - 2017-12-15 12:13:13.879227: step 10470, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 58h:35m:44s remains)
INFO - root - 2017-12-15 12:13:20.495310: step 10480, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 59h:40m:09s remains)
INFO - root - 2017-12-15 12:13:27.026370: step 10490, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 58h:42m:29s remains)
INFO - root - 2017-12-15 12:13:33.603450: step 10500, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 59h:58m:06s remains)
2017-12-15 12:13:34.121378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9119048 -7.3417778 -6.570065 -5.7270827 -5.5263791 -5.9878664 -6.6188588 -6.9709911 -7.2550917 -6.4276271 -5.0072308 -6.1056666 -6.8651223 -8.3104877 -7.8832731][-5.772151 -6.3132825 -5.4445305 -5.3855848 -6.015245 -6.0159 -6.6414566 -7.1595902 -7.2347126 -6.8450174 -6.0808997 -7.5094724 -7.5391884 -7.8180518 -8.119688][-3.3147354 -4.8125529 -5.9434991 -5.5259428 -5.6098824 -5.8075089 -5.9091682 -6.0701985 -6.4006672 -6.3774185 -6.0685396 -8.4525461 -9.1910515 -8.9269724 -8.5349226][-3.6682651 -5.1308975 -5.5902562 -5.0635538 -5.1371984 -3.8591561 -3.4467897 -4.1107664 -4.5468283 -4.836833 -4.9760618 -6.995625 -8.1873484 -9.4956532 -9.143014][-3.9649558 -6.5611143 -7.2504306 -5.0876155 -3.8382459 -1.6858778 -0.39270067 -1.8482571 -3.6051519 -3.6825373 -3.4263508 -6.160852 -7.9638677 -8.8620949 -8.7085781][-7.0217457 -7.9189396 -7.0074816 -3.8350043 -0.936501 2.2421942 3.6362777 2.5154529 0.92455149 -0.79138422 -2.6986089 -4.4633989 -5.370141 -6.6904197 -7.0257025][-8.787693 -9.364274 -7.324945 -2.6639938 0.92918015 4.9407725 6.9474044 6.0810704 4.9411206 0.90265369 -2.6176684 -5.134726 -6.0939479 -6.6520157 -5.8273072][-9.8140755 -8.9927464 -7.0145636 -3.1542702 0.69281006 5.5901532 8.4948578 7.3868651 6.0308652 1.9210806 -1.8822825 -5.286458 -6.5440264 -6.9122548 -5.725884][-6.6879444 -6.7250395 -5.96904 -3.0181189 -0.68104839 2.4291654 5.3587556 5.7239766 4.6032653 1.5897574 -1.2189136 -5.8307152 -8.0493212 -7.7805886 -7.0105367][-5.6012855 -5.4494381 -5.4163857 -3.6841328 -2.4035041 -1.3357449 0.44441938 2.6963496 2.8877606 0.49845934 -1.8258901 -6.4031663 -9.1908531 -9.9620533 -10.031755][-7.7448773 -7.9889016 -7.8094625 -6.2737088 -5.1612473 -4.4721613 -4.4887304 -4.4168539 -3.6436388 -4.2776136 -5.8209085 -9.0609035 -10.947819 -12.624711 -13.090717][-11.863811 -12.035326 -11.24115 -9.7130394 -9.409668 -8.3543692 -8.3483248 -8.8209476 -8.7862644 -8.7092133 -8.57196 -10.510038 -11.734194 -11.981123 -12.340796][-12.624422 -12.907381 -12.319605 -11.338448 -10.366166 -9.4216824 -9.3005743 -8.7688293 -8.9280052 -9.1080885 -8.5608921 -8.7639513 -8.9935856 -7.7837963 -7.2187734][-11.203164 -11.182827 -10.997476 -9.500555 -8.2217617 -8.035758 -8.2802973 -7.5075183 -6.9476404 -6.9745393 -6.6308851 -6.63337 -5.9443851 -5.8149338 -5.0604925][-9.2617521 -9.7107334 -9.6705894 -8.312191 -6.4953918 -4.6298466 -4.0802708 -4.7809424 -4.7707138 -4.5647545 -4.9140034 -5.7952361 -6.435545 -6.4099736 -5.6416326]]...]
INFO - root - 2017-12-15 12:13:40.719579: step 10510, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 60h:19m:31s remains)
INFO - root - 2017-12-15 12:13:47.336752: step 10520, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 58h:17m:55s remains)
INFO - root - 2017-12-15 12:13:53.896988: step 10530, loss = 0.20, batch loss = 0.15 (11.8 examples/sec; 0.678 sec/batch; 60h:37m:42s remains)
INFO - root - 2017-12-15 12:14:00.507265: step 10540, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 58h:50m:06s remains)
INFO - root - 2017-12-15 12:14:07.120388: step 10550, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.650 sec/batch; 58h:06m:34s remains)
INFO - root - 2017-12-15 12:14:13.828019: step 10560, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.690 sec/batch; 61h:41m:05s remains)
INFO - root - 2017-12-15 12:14:20.452224: step 10570, loss = 0.13, batch loss = 0.08 (11.5 examples/sec; 0.698 sec/batch; 62h:24m:55s remains)
INFO - root - 2017-12-15 12:14:27.045857: step 10580, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 60h:47m:20s remains)
INFO - root - 2017-12-15 12:14:33.541511: step 10590, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 57h:59m:35s remains)
INFO - root - 2017-12-15 12:14:40.113792: step 10600, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 59h:29m:11s remains)
2017-12-15 12:14:40.597802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.229805 -11.668655 -10.949311 -10.237852 -10.859922 -10.92663 -10.259616 -9.7736015 -9.3286362 -7.5190997 -5.724659 -6.4511518 -5.5104289 -6.8182926 -5.4181738][-8.612133 -8.7529135 -7.5197768 -7.2215886 -7.766284 -9.1776762 -9.4451361 -8.2340946 -7.4918833 -6.7193522 -5.3706679 -5.7038994 -4.9024773 -6.2114649 -4.8358502][-6.6061969 -6.92217 -6.294714 -5.1506619 -5.5600266 -6.3303118 -6.3024354 -5.7738576 -4.9439545 -4.0544462 -3.5498831 -5.3884559 -5.6675053 -7.2212005 -7.1952677][-6.7635193 -6.3908134 -5.1687021 -4.438796 -4.6927843 -5.3440671 -5.66096 -5.4191194 -4.65631 -4.5995364 -4.3052068 -6.0415287 -6.631568 -9.3629131 -9.65282][-7.4802141 -8.0110741 -6.32146 -4.0174818 -3.5283298 -3.6650336 -3.3512683 -3.9058256 -4.1928353 -4.2977715 -4.3856511 -7.2884607 -8.25091 -10.964757 -10.649134][-8.299614 -7.0040216 -5.6002393 -3.034904 -1.2330575 0.023069382 0.9204812 -0.16064739 -0.78069305 -2.3954868 -3.9333181 -7.413094 -8.8070726 -11.43265 -10.541115][-9.5196257 -7.6182904 -4.8836761 -2.4943292 -0.98784065 1.7364702 4.4554248 4.7254987 4.0713763 0.19036913 -3.615078 -7.2034588 -8.3675919 -11.163386 -10.314226][-9.3574638 -7.1291604 -4.3366275 -1.5679684 -0.43071795 3.0456147 5.6594882 5.9654317 5.9391551 2.0825748 -2.2759664 -7.5217857 -9.4475174 -11.383423 -10.332194][-7.9178967 -5.5537653 -3.3446307 -1.0969582 -0.81689787 2.1410928 4.5822263 4.7856631 4.36881 1.2857485 -2.0915861 -8.1600552 -11.149357 -13.832554 -12.117586][-6.4980917 -5.5309424 -3.733592 -1.5852914 -1.8324788 0.19402838 1.2248096 2.1941171 2.027781 -0.97310877 -3.8161094 -9.3995991 -12.867268 -16.268873 -15.122345][-10.279392 -9.9755974 -8.3459692 -6.2332215 -5.5734997 -4.6827354 -4.0736394 -3.1667588 -3.5227993 -5.7201071 -7.9974151 -13.329447 -15.217787 -16.908728 -15.327497][-13.829361 -14.22316 -12.993565 -11.507415 -10.097599 -8.6857824 -8.7669 -8.9621353 -8.8428326 -10.061291 -11.526003 -14.014492 -13.931683 -15.450415 -13.884943][-16.50149 -14.994438 -13.152933 -12.885117 -12.629669 -11.70364 -10.770861 -10.591401 -10.970263 -11.120319 -11.038911 -12.147523 -11.427017 -11.208464 -8.4390917][-13.41628 -13.306858 -11.606936 -10.390612 -9.6604414 -9.5465851 -9.8817654 -9.6971664 -9.4978027 -9.3994942 -9.3045235 -8.2506571 -6.5119438 -7.3939805 -5.5875158][-10.602757 -9.73826 -9.4022417 -7.231781 -6.1893649 -7.0633707 -6.9727068 -7.1059542 -6.8308382 -6.5002489 -6.5505767 -6.5427194 -6.6762691 -6.1707964 -5.6122694]]...]
INFO - root - 2017-12-15 12:14:47.118792: step 10610, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 58h:11m:31s remains)
INFO - root - 2017-12-15 12:14:53.665699: step 10620, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 58h:17m:55s remains)
INFO - root - 2017-12-15 12:15:00.208854: step 10630, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 56h:49m:58s remains)
INFO - root - 2017-12-15 12:15:06.810444: step 10640, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 59h:53m:19s remains)
INFO - root - 2017-12-15 12:15:13.386300: step 10650, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 59h:54m:07s remains)
INFO - root - 2017-12-15 12:15:19.967224: step 10660, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 59h:41m:12s remains)
INFO - root - 2017-12-15 12:15:26.593331: step 10670, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 57h:39m:31s remains)
INFO - root - 2017-12-15 12:15:33.146218: step 10680, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 58h:34m:39s remains)
INFO - root - 2017-12-15 12:15:39.665822: step 10690, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.635 sec/batch; 56h:46m:23s remains)
INFO - root - 2017-12-15 12:15:46.314320: step 10700, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 60h:16m:26s remains)
2017-12-15 12:15:46.842753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6834834 -2.5215571 -1.0008788 0.1602149 -0.16278172 -0.54322767 -2.3323681 -3.1210396 -3.140012 -3.0342922 -2.8514972 -5.375596 -7.3435049 -10.24366 -11.071568][-2.429034 -2.0199609 -1.286675 -0.5117135 -0.50371027 -1.3688035 -1.9222255 -2.5466223 -4.041039 -4.6281576 -4.70903 -6.6034656 -8.3574562 -10.01409 -10.34664][-1.3973141 -2.2331691 -2.7944283 -2.770874 -3.1863561 -3.153496 -2.7699449 -2.4904673 -3.136632 -3.8743587 -3.6389463 -5.643384 -7.3675466 -9.0125847 -9.6913891][-3.0186613 -3.2179425 -4.1634922 -4.6564794 -5.3834844 -4.8917875 -4.3939757 -3.7816031 -3.2425294 -3.7875447 -4.019217 -6.3769226 -8.0614319 -9.3699627 -9.9068623][-2.1170626 -4.0233264 -5.0016575 -5.1282268 -6.5683308 -5.5134211 -3.6798313 -2.400353 -1.8405313 -1.6065516 -1.3261065 -5.5164704 -7.7580891 -9.0101957 -10.266537][-3.0600147 -4.109848 -4.5913639 -4.8310738 -4.4255462 -2.946831 -1.6488361 0.10472298 1.293499 0.56152153 -0.79921436 -4.3639288 -6.1731377 -8.1528645 -9.2469959][-3.643337 -4.3070879 -3.8660059 -3.6656959 -3.1633914 -1.1448302 0.53547955 2.3180141 3.5388327 2.4345903 0.85254574 -4.2701168 -6.3773003 -7.3103971 -8.1321087][-3.6594517 -4.1875987 -3.7965918 -3.160605 -2.6211939 -0.78302908 0.14147806 1.0806351 2.4380922 1.7503242 0.39353085 -3.988167 -5.4682031 -6.5460835 -6.6226983][-3.9417858 -4.3832407 -4.1084962 -2.829982 -2.3895323 -1.6457238 -1.3383827 0.57968807 1.9380417 1.5437431 0.540678 -4.3343024 -6.2362413 -5.9814162 -4.8340859][-4.115551 -4.39978 -4.9841766 -4.3590646 -3.2064381 -0.97804689 -0.40512991 0.60221529 2.0085268 1.9564948 1.2430167 -3.5305624 -6.4642134 -7.7050705 -7.6440954][-7.1329355 -7.3102608 -6.4561586 -5.0530114 -4.2137623 -3.1529822 -2.046335 -0.66767454 0.068912983 -0.79997158 -2.3642745 -6.5911179 -9.8500776 -10.175422 -8.9786072][-10.772066 -9.3658352 -8.2085266 -6.3314891 -5.3206978 -3.6906772 -2.1360991 -1.5120111 -2.2427559 -3.5277081 -4.5358005 -7.18863 -9.1565733 -9.6450825 -9.0516224][-12.240843 -11.454016 -10.056211 -8.4014435 -7.5381246 -6.4301219 -5.0816784 -4.3006048 -4.4666638 -6.2755871 -7.8167953 -9.3101225 -9.4496527 -8.3173189 -5.4707489][-10.872478 -9.836113 -8.20468 -7.36432 -7.7171497 -7.7869258 -7.6315966 -7.7838717 -8.2459831 -8.7468319 -8.527998 -8.6845684 -8.0383 -7.400022 -5.8316283][-6.5652971 -6.8343029 -6.8923168 -6.3658271 -6.5982528 -7.0619354 -6.51336 -5.5978613 -5.8181462 -6.2820864 -6.845861 -7.4588518 -7.7073445 -7.3399191 -6.9761782]]...]
INFO - root - 2017-12-15 12:15:53.380338: step 10710, loss = 0.25, batch loss = 0.21 (12.0 examples/sec; 0.669 sec/batch; 59h:47m:54s remains)
INFO - root - 2017-12-15 12:15:59.884461: step 10720, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 59h:06m:08s remains)
INFO - root - 2017-12-15 12:16:06.467195: step 10730, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 59h:18m:23s remains)
INFO - root - 2017-12-15 12:16:13.157836: step 10740, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.683 sec/batch; 61h:04m:50s remains)
INFO - root - 2017-12-15 12:16:19.699692: step 10750, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 59h:13m:47s remains)
INFO - root - 2017-12-15 12:16:26.293824: step 10760, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 59h:38m:43s remains)
INFO - root - 2017-12-15 12:16:32.842261: step 10770, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 58h:55m:43s remains)
INFO - root - 2017-12-15 12:16:39.374841: step 10780, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 58h:03m:16s remains)
INFO - root - 2017-12-15 12:16:46.010184: step 10790, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 60h:44m:30s remains)
INFO - root - 2017-12-15 12:16:52.625042: step 10800, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 58h:03m:15s remains)
2017-12-15 12:16:53.203307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7633433 -4.1618004 -3.7504988 -2.9155929 -2.8530109 -3.5653372 -4.8995275 -5.6046662 -5.6451688 -5.0206394 -4.502759 -5.2313333 -6.5135279 -7.1712294 -7.2080255][-4.0940642 -3.5576527 -2.8597381 -1.7412438 -1.6909528 -2.3997874 -4.4994812 -6.1480751 -6.7704649 -6.70571 -5.9669485 -6.1877065 -6.7936735 -7.1380825 -6.6919045][-1.9492667 -2.4151089 -2.3617039 -1.5984788 -1.7474196 -2.5010095 -3.5405364 -4.8301868 -5.2742033 -5.2496333 -5.0504766 -6.0440307 -7.3786507 -7.6723633 -6.7787371][-2.6346567 -2.8014824 -2.6738951 -1.9572971 -1.31002 -1.6492682 -2.5971901 -3.4518015 -4.0161667 -4.0073209 -3.8269222 -5.1190429 -6.6452246 -6.9788465 -6.8684216][-3.5762382 -4.2861848 -4.49256 -3.4577003 -2.4276042 -1.0819821 -1.0502725 -1.9780648 -2.4674861 -2.2985716 -2.6918747 -4.3726912 -6.1861687 -6.8729396 -6.6409683][-3.7484603 -4.2611179 -4.3491898 -3.2428133 -2.028677 -0.40244293 0.47506666 0.45403814 0.22024107 -0.67129755 -1.8626339 -3.2391262 -4.718565 -4.9596939 -4.9937587][-5.3082161 -4.6897812 -3.9227619 -2.6158042 -0.87637615 0.90654469 1.3375421 1.4918528 1.8746076 0.25677633 -1.1052308 -2.5848768 -3.68404 -4.0797811 -5.2999353][-5.9834719 -5.9325542 -4.4241171 -1.7486372 0.057801723 1.9093099 2.5364976 2.1288404 1.8374329 0.47040272 -1.3433771 -2.6244755 -3.8375022 -4.7383637 -5.0043464][-6.4465704 -5.7667022 -4.481421 -2.474647 -0.84847689 0.91238546 1.5240831 1.338841 1.3219929 0.15690327 -0.39247847 -1.0457139 -3.103075 -4.1160307 -3.9168801][-7.0433817 -6.1500716 -4.5668449 -3.0440385 -1.4144788 -0.8716135 -1.2840319 -0.6073451 0.30743551 0.64724588 0.83675337 -0.56722212 -2.8347082 -3.3285499 -3.7817469][-9.9515285 -8.830307 -6.6825447 -3.847712 -2.4543953 -3.4031982 -3.6297274 -2.6940913 -1.8821437 -0.73074341 -0.60216 -2.1502542 -3.9938474 -4.5914869 -5.1575384][-11.533945 -10.633011 -8.7874126 -6.4686813 -4.5774431 -3.9686232 -4.1772532 -4.201405 -3.595952 -2.9838479 -2.8933713 -3.9643426 -4.9954128 -4.9280472 -5.1135082][-10.035924 -9.225522 -7.5120263 -5.8315778 -5.1153016 -4.509119 -4.3818674 -4.7780333 -4.7138991 -4.35027 -4.0350008 -4.8686452 -5.427494 -4.2137337 -4.151113][-7.7679186 -6.6622028 -5.6905928 -5.3489575 -4.9434223 -5.1070943 -5.8953815 -4.8562841 -4.2518129 -4.6192141 -4.8339019 -5.2649326 -5.3787985 -5.22046 -5.3889027][-4.2611823 -4.9733291 -4.8207788 -3.795758 -3.4113908 -4.6903181 -4.5466948 -4.3442326 -5.1744423 -4.9570651 -5.2659411 -6.0168438 -6.6028647 -7.42567 -7.8695416]]...]
INFO - root - 2017-12-15 12:16:59.788698: step 10810, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 60h:35m:12s remains)
INFO - root - 2017-12-15 12:17:06.421873: step 10820, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 60h:00m:14s remains)
INFO - root - 2017-12-15 12:17:13.032351: step 10830, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 59h:08m:17s remains)
INFO - root - 2017-12-15 12:17:19.635749: step 10840, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 59h:44m:54s remains)
INFO - root - 2017-12-15 12:17:26.306102: step 10850, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 58h:25m:27s remains)
INFO - root - 2017-12-15 12:17:32.893359: step 10860, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 56h:49m:12s remains)
INFO - root - 2017-12-15 12:17:39.561617: step 10870, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 59h:32m:38s remains)
INFO - root - 2017-12-15 12:17:46.124275: step 10880, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 57h:49m:20s remains)
INFO - root - 2017-12-15 12:17:52.727315: step 10890, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 58h:56m:52s remains)
INFO - root - 2017-12-15 12:17:59.301005: step 10900, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 59h:15m:31s remains)
2017-12-15 12:17:59.779644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8873739 -5.3539128 -5.1671667 -5.3919258 -6.5244212 -7.6224542 -8.14039 -8.0164042 -7.8384371 -7.4620829 -7.1156735 -9.2490273 -11.492462 -13.554157 -14.408103][-4.8629565 -5.7879944 -5.2872071 -5.465498 -6.4215808 -7.3480053 -8.53782 -9.510705 -9.4186916 -8.2796259 -7.7790546 -10.656408 -12.956684 -14.481325 -15.416318][-4.447154 -5.356895 -5.8560762 -5.9652572 -6.1670589 -7.4937415 -8.9240046 -9.4708338 -9.150219 -9.1674137 -8.7761984 -11.114749 -13.05837 -14.182357 -15.164392][-7.1827211 -7.386085 -6.1788979 -5.3150954 -4.8621569 -5.3463774 -6.2159233 -7.5583897 -8.3362865 -7.6025438 -7.0588617 -10.467949 -13.009643 -14.725614 -16.196983][-7.6676788 -8.9737768 -8.59013 -6.4887323 -4.0272179 -2.4004431 -1.6786971 -2.9802108 -5.045022 -5.330935 -5.4448047 -8.1357384 -10.433851 -12.976994 -15.348642][-9.8931856 -10.106375 -8.8112717 -5.3390751 -1.8638582 0.80432224 3.1400404 3.1800308 1.7646656 -0.62489128 -3.1746111 -5.7856851 -8.7039928 -11.151545 -13.051233][-10.984877 -10.990557 -9.0357161 -4.6130404 -0.11151552 3.7400589 7.299304 7.8896961 7.3090038 3.47445 -1.2914534 -5.3598075 -9.2801533 -10.988512 -12.352711][-11.372618 -10.74082 -8.7901745 -4.756588 -0.65029669 4.8572216 8.98358 9.1241522 8.9502211 5.4549551 0.76830626 -5.4991474 -11.47797 -13.005642 -13.922668][-8.2279129 -8.6446571 -7.2033367 -4.8503118 -2.4798787 1.7654476 4.9752197 6.4790926 7.3847513 4.3292847 0.4273386 -7.2488046 -14.135613 -15.254642 -15.962951][-7.0374651 -7.2133775 -6.3544741 -4.3006086 -2.669605 -1.2063231 0.73156691 2.7939386 3.4060068 1.9883199 -0.41683912 -7.86218 -15.265512 -18.086777 -19.103691][-9.3620987 -10.028044 -10.262148 -8.3445377 -6.8067837 -5.8963351 -4.6178589 -3.9379168 -2.9222579 -3.033555 -4.332737 -10.551167 -15.435025 -17.94788 -19.25758][-13.656499 -13.581338 -12.773084 -12.43259 -11.986714 -10.6133 -9.7957878 -9.7697735 -8.9132938 -8.3741856 -8.4313755 -10.965561 -12.728712 -16.061916 -17.798582][-15.937934 -14.773521 -13.505766 -13.641805 -13.344627 -13.111317 -13.124726 -12.591559 -12.06633 -11.238955 -10.486112 -11.112201 -11.534642 -11.608881 -11.843962][-14.603338 -13.435393 -11.992244 -10.61338 -10.086695 -10.944347 -11.595861 -10.981354 -10.925781 -10.950581 -10.698627 -9.9793367 -9.2592735 -9.8002682 -9.8223724][-10.251208 -9.3057308 -7.7459679 -6.6231041 -5.7777619 -5.646822 -5.6789551 -7.1271029 -8.0052834 -8.1536388 -8.1076937 -9.449255 -10.570566 -10.457832 -10.185949]]...]
INFO - root - 2017-12-15 12:18:06.383917: step 10910, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 57h:20m:55s remains)
INFO - root - 2017-12-15 12:18:12.957098: step 10920, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 58h:50m:05s remains)
INFO - root - 2017-12-15 12:18:19.487737: step 10930, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 57h:49m:13s remains)
INFO - root - 2017-12-15 12:18:26.087560: step 10940, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 57h:14m:49s remains)
INFO - root - 2017-12-15 12:18:32.660545: step 10950, loss = 0.29, batch loss = 0.24 (12.2 examples/sec; 0.656 sec/batch; 58h:36m:57s remains)
INFO - root - 2017-12-15 12:18:39.213927: step 10960, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 59h:07m:22s remains)
INFO - root - 2017-12-15 12:18:45.918013: step 10970, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 60h:35m:57s remains)
INFO - root - 2017-12-15 12:18:52.458583: step 10980, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 60h:28m:59s remains)
INFO - root - 2017-12-15 12:18:59.091133: step 10990, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 59h:31m:42s remains)
INFO - root - 2017-12-15 12:19:05.731921: step 11000, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 59h:35m:00s remains)
2017-12-15 12:19:06.295817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0716634 -4.7377124 -5.2424316 -4.8528223 -6.3854208 -7.2737389 -8.262413 -8.9930067 -9.7153587 -9.9810429 -9.2812672 -11.345148 -12.397176 -13.482766 -12.177718][-7.3910322 -6.5228004 -6.7325706 -7.1749654 -8.0820656 -8.9303455 -9.6198444 -10.074886 -10.223669 -10.945753 -10.29163 -12.097134 -12.870737 -13.408415 -10.938084][-6.502697 -7.9828978 -8.9422131 -9.5105362 -10.893795 -11.802197 -12.217921 -11.339066 -10.466612 -10.457235 -10.250479 -12.173448 -12.588387 -12.728202 -10.136087][-7.9183855 -10.015505 -11.305305 -11.047668 -12.258894 -10.863278 -9.4806528 -10.516264 -10.873421 -9.7682905 -9.7716618 -11.987 -12.568393 -13.00453 -10.326176][-8.6980429 -11.014529 -12.258563 -11.192177 -10.412952 -7.8638449 -6.4495859 -6.1906748 -7.0035152 -7.3440204 -7.18425 -10.352504 -11.76055 -12.432469 -10.587873][-10.399397 -10.763742 -11.235678 -8.8622246 -6.2971892 -3.3707936 -0.99913073 0.62584209 -0.61685896 -4.1118355 -5.6603727 -8.1346359 -11.29411 -13.279539 -11.476614][-10.043961 -10.019773 -9.5886078 -6.6928878 -1.9631455 1.4521871 3.8449869 4.4503326 3.6069937 0.90886164 -1.8479354 -6.63595 -9.189045 -11.703682 -10.739843][-8.3492022 -7.5592394 -6.8180733 -4.6871219 -1.7450333 4.0030823 7.9770107 8.1374884 7.0050259 4.248682 1.198421 -4.4509058 -7.9122829 -10.709381 -10.657435][-8.3757391 -7.5118766 -5.9157848 -3.6474226 -1.3928599 2.5769358 6.4482174 7.51202 6.2620382 3.6817799 2.4794912 -2.6539011 -6.772984 -9.6511469 -8.9049606][-7.2119441 -7.594635 -7.6113415 -4.8062348 -2.6827519 -0.085819721 2.6243324 4.4264059 3.9011917 0.87017584 -1.7212579 -5.696414 -8.1465359 -10.389925 -9.278738][-9.539608 -8.9754744 -8.85691 -6.67773 -4.5313497 -2.0951877 -1.5846176 -1.5357738 -2.7067618 -4.4054432 -6.5247688 -11.362415 -13.662853 -13.118605 -9.6162615][-12.665375 -10.951104 -9.4410982 -6.9657564 -3.998961 -2.537709 -3.6996775 -5.6258841 -7.9686379 -9.688345 -10.04777 -14.108368 -16.045208 -14.200863 -10.338579][-13.236145 -10.81378 -7.2114482 -5.3255649 -4.6211987 -2.894804 -3.7869954 -6.0770531 -8.7431278 -11.054047 -10.919514 -10.648584 -11.212555 -10.881311 -8.3164444][-12.143353 -8.915555 -5.0419736 -3.7565851 -3.2802775 -2.8163347 -4.1553993 -6.0822554 -7.2150183 -8.8048935 -8.9199753 -7.08331 -6.371274 -5.9859338 -4.6169157][-9.9391813 -7.9346161 -5.6140847 -4.0907154 -3.9646306 -3.7384353 -4.4798884 -5.6459665 -6.0984745 -6.0228281 -5.565762 -6.3815651 -6.4352946 -6.5188627 -5.8294392]]...]
INFO - root - 2017-12-15 12:19:12.883663: step 11010, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 59h:01m:56s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 12:19:19.447128: step 11020, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 59h:24m:05s remains)
INFO - root - 2017-12-15 12:19:26.133467: step 11030, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 60h:22m:30s remains)
INFO - root - 2017-12-15 12:19:32.730584: step 11040, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 57h:10m:49s remains)
INFO - root - 2017-12-15 12:19:39.354315: step 11050, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 58h:27m:44s remains)
INFO - root - 2017-12-15 12:19:45.857011: step 11060, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 58h:14m:20s remains)
INFO - root - 2017-12-15 12:19:52.428098: step 11070, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 58h:28m:43s remains)
INFO - root - 2017-12-15 12:19:58.998014: step 11080, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 57h:12m:48s remains)
INFO - root - 2017-12-15 12:20:05.558756: step 11090, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.633 sec/batch; 56h:28m:53s remains)
INFO - root - 2017-12-15 12:20:12.105646: step 11100, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 58h:48m:46s remains)
2017-12-15 12:20:12.647051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7682743 -2.7630537 -2.7325132 -2.0601659 -2.8502667 -3.1556478 -3.360805 -3.2089026 -3.2330739 -2.778209 -3.1000335 -4.1909876 -4.4057965 -6.2681108 -6.7947426][-2.3455024 -1.9186361 -1.9862809 -1.2663755 -1.8595042 -0.93819904 -0.56298018 -0.59432316 -1.0463262 -1.5625935 -1.7181931 -1.7646239 -2.172725 -5.1676946 -6.0894103][-1.572413 -1.2804623 -1.4190435 -0.69491005 -1.819936 -1.1363621 -0.20989656 -0.052581787 -0.061709881 0.12268448 -0.49068451 -1.2728639 -1.2900739 -4.6224561 -5.73088][-1.9812689 -2.1937747 -2.1585858 -1.2446675 -1.9560525 -1.4452066 -1.2542458 0.017878056 0.81204033 0.63470221 0.10984373 -1.1411452 -1.5992193 -4.1087785 -4.436986][-2.1562886 -2.6419446 -2.3338509 -1.5738959 -2.2228625 -0.74859524 0.21392536 0.82238436 0.82120848 0.65198374 0.2665863 -0.59133863 -1.4049172 -4.8445282 -5.0506315][-2.8026609 -3.0098011 -2.7067924 -1.171936 -0.65549469 0.34937334 0.7964654 1.202333 1.4983268 1.3415298 0.96379232 0.41190863 -0.38249111 -3.6214237 -4.0247335][-3.4413056 -3.6242776 -2.5345805 -1.2620807 -0.63718796 0.81314182 2.0638218 3.1961751 4.1274934 3.2923884 2.1352711 0.58933353 -0.29712772 -2.5155005 -3.1482089][-4.7364841 -4.4537849 -3.3468139 -2.8580022 -2.2875273 0.25313663 2.4250083 3.4988122 4.1742234 3.2367635 2.5214276 0.89596224 -0.72723293 -3.8942895 -4.1356645][-5.3117418 -5.1051579 -4.2423878 -2.3743107 -2.1716545 -1.4599071 0.31113911 1.5427532 2.7275815 3.1080661 2.2030435 -0.34658146 -2.0081193 -4.4005132 -3.9435806][-7.0022507 -5.9964142 -5.2668085 -4.1807928 -3.4382639 -1.7459407 -0.043998241 1.0040407 2.1366611 0.79742908 -0.048277855 -1.2724252 -2.6137209 -6.133574 -6.8009253][-8.493082 -8.2703619 -6.9831476 -5.4627118 -5.5283742 -4.4894023 -3.2034893 -2.4652264 -2.488625 -2.436929 -2.1850896 -4.3638897 -5.387476 -6.9983268 -6.2414989][-9.7340488 -8.6726217 -8.1578369 -6.9297643 -6.7465954 -6.6333027 -6.4340229 -4.97933 -4.1139746 -5.0759826 -5.2793717 -5.6843209 -6.0783887 -6.7182331 -6.2820454][-10.844223 -9.1167669 -8.0202713 -6.9380069 -6.9099126 -6.700294 -6.3477397 -6.4119449 -5.9286938 -5.8745708 -6.1571193 -6.3304172 -5.4356103 -6.2981644 -6.2449284][-9.9320812 -9.8209372 -8.9437866 -7.1958141 -6.22507 -6.0524592 -6.3734465 -5.5730996 -5.1043744 -5.5132804 -5.2592974 -4.9778214 -4.30888 -4.3276663 -3.5899668][-5.8836923 -5.2869463 -5.06416 -4.9039392 -4.5346284 -3.9162626 -3.6913209 -3.4214382 -3.4556551 -3.2300296 -3.0784826 -3.5438142 -3.7820749 -4.949605 -5.8227735]]...]
INFO - root - 2017-12-15 12:20:19.300078: step 11110, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 57h:03m:22s remains)
INFO - root - 2017-12-15 12:20:25.916396: step 11120, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 59h:26m:29s remains)
INFO - root - 2017-12-15 12:20:32.478520: step 11130, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 57h:42m:34s remains)
INFO - root - 2017-12-15 12:20:39.108130: step 11140, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 59h:50m:32s remains)
INFO - root - 2017-12-15 12:20:45.652418: step 11150, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 57h:20m:34s remains)
INFO - root - 2017-12-15 12:20:52.330304: step 11160, loss = 0.20, batch loss = 0.16 (11.8 examples/sec; 0.681 sec/batch; 60h:45m:46s remains)
INFO - root - 2017-12-15 12:20:58.882217: step 11170, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 58h:42m:18s remains)
INFO - root - 2017-12-15 12:21:05.419484: step 11180, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 58h:28m:20s remains)
INFO - root - 2017-12-15 12:21:12.007882: step 11190, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 60h:26m:30s remains)
INFO - root - 2017-12-15 12:21:18.597710: step 11200, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 60h:06m:23s remains)
2017-12-15 12:21:19.064634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6839666 -1.3954973 -1.507791 -2.5935657 -3.816361 -4.3077974 -5.064743 -4.7847548 -3.7035599 -4.2131958 -5.4094677 -5.3635082 -6.7470922 -8.5205679 -5.8410826][-5.2223477 -5.0231347 -5.1236711 -4.8420587 -4.5689673 -6.0016479 -6.2084765 -5.1138473 -4.3743696 -3.9653759 -3.6725602 -6.139648 -8.1577587 -8.3694115 -7.9767747][-4.2521391 -4.3756838 -5.9200211 -6.5889373 -7.362432 -6.8897653 -6.3501368 -6.2048254 -4.9429774 -4.3339987 -4.9647141 -5.761055 -6.8440666 -9.102355 -7.3250532][-4.0119934 -4.1138706 -4.8398986 -5.3761754 -6.5561795 -7.3777232 -6.996685 -5.9505396 -5.5216894 -5.0375843 -3.9004669 -5.2666698 -6.672318 -7.1061025 -5.361764][-7.4359779 -6.9728551 -6.6730895 -6.0633416 -5.8729143 -4.2305117 -3.4480007 -4.4059811 -4.5623851 -4.3752136 -4.3406124 -4.6339493 -5.5049629 -6.6580496 -4.1707525][-6.9091091 -6.7910714 -6.425416 -4.3520174 -3.0336938 -0.7156229 1.3168468 0.59686995 -0.43010569 -2.7258136 -3.4300086 -3.9314313 -4.1301136 -4.1095414 -2.7344935][-8.0410423 -7.6676407 -6.1829 -3.3758483 -1.3947334 1.8504157 4.09726 4.2676053 4.2045083 1.1261902 -1.3538165 -2.8920252 -3.6573715 -4.17929 -2.2256529][-8.3857689 -7.5022688 -5.800106 -2.8162467 -0.039660454 2.3448138 3.2816558 3.9873462 4.1112084 2.2251105 0.54296827 -2.406558 -5.0419827 -5.3431096 -3.5928819][-5.8345423 -5.4340239 -4.4713211 -2.6006351 -0.92471361 1.7162151 3.1236653 3.7442422 4.2197881 2.5538306 0.80666494 -2.0267034 -5.1883049 -7.5257874 -6.3129392][-5.5543032 -3.8634026 -3.2305281 -1.3587737 -0.39226484 0.58974314 1.4578781 2.6305852 2.0995793 -0.046404362 -0.89358091 -3.5228379 -6.1263351 -8.3758469 -8.354495][-4.9873147 -4.4647913 -4.2981911 -3.3825817 -2.8244295 -2.2568138 -2.3098435 -1.887089 -1.4027872 -2.2790391 -3.4485457 -6.2984686 -8.909359 -10.452082 -8.819747][-8.38822 -7.3769703 -7.583137 -7.1131067 -7.4988656 -7.6319294 -7.4653845 -8.104497 -8.7210217 -7.7792063 -7.3286171 -8.9385967 -9.9911852 -11.418457 -10.792813][-11.150399 -10.367408 -9.75526 -10.539136 -11.441818 -11.068653 -11.176954 -11.432966 -11.311342 -11.430109 -11.087105 -11.095575 -11.150793 -10.826208 -9.6274967][-9.9825249 -9.3432827 -8.9558887 -8.947876 -9.7040062 -10.448669 -11.063547 -10.63489 -10.742158 -10.706705 -10.06915 -9.8300381 -9.2621708 -9.4035263 -8.6843061][-7.5969515 -7.2752514 -6.6664124 -6.2172441 -6.6192384 -6.5469165 -6.7269974 -7.4719448 -7.43182 -7.5370474 -8.3991394 -9.4371624 -9.5589466 -8.5973444 -7.668221]]...]
INFO - root - 2017-12-15 12:21:25.726424: step 11210, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.663 sec/batch; 59h:10m:08s remains)
INFO - root - 2017-12-15 12:21:32.301248: step 11220, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 59h:07m:37s remains)
INFO - root - 2017-12-15 12:21:39.008987: step 11230, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 58h:53m:19s remains)
INFO - root - 2017-12-15 12:21:45.556255: step 11240, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 57h:46m:50s remains)
INFO - root - 2017-12-15 12:21:52.104046: step 11250, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 58h:24m:38s remains)
INFO - root - 2017-12-15 12:21:58.732493: step 11260, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 60h:46m:08s remains)
INFO - root - 2017-12-15 12:22:05.337243: step 11270, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 58h:10m:35s remains)
INFO - root - 2017-12-15 12:22:11.903035: step 11280, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 56h:49m:18s remains)
INFO - root - 2017-12-15 12:22:18.471160: step 11290, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 59h:10m:41s remains)
INFO - root - 2017-12-15 12:22:25.037226: step 11300, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 59h:16m:09s remains)
2017-12-15 12:22:25.529816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3212976 -6.1545887 -5.05466 -3.7090206 -3.6121941 -3.4482658 -3.2320011 -2.89549 -2.9974308 -3.4613955 -3.5946364 -7.0387678 -9.5245 -10.135165 -9.9251842][-5.0120835 -4.669157 -4.8056812 -3.9452863 -4.0921259 -3.9495831 -3.5106573 -2.9179204 -2.4660573 -2.6250391 -2.863035 -6.0081348 -8.6071491 -9.8143845 -9.5939236][-3.0466263 -3.3546731 -3.4552867 -3.1541729 -4.0418539 -3.9895544 -3.5120642 -3.0654962 -2.9416709 -2.7808244 -2.6581056 -5.7568145 -7.8870707 -8.5828047 -8.7492752][-2.9604745 -3.2386339 -3.3619661 -2.697659 -2.945787 -3.1468523 -3.3559217 -3.0534172 -2.7603774 -2.5253234 -2.3798773 -6.024271 -8.562171 -9.2923365 -9.5195847][-2.784394 -3.5624218 -4.1396942 -2.97489 -2.7640119 -2.0125289 -1.8356729 -1.7867808 -1.8897374 -1.9018433 -2.1720686 -6.0631514 -8.7234249 -9.4463081 -9.610815][-5.4149671 -5.2031469 -3.86269 -1.8268266 -0.99356556 0.53210258 1.1571651 1.1696358 0.82705164 -0.062091351 -0.86892796 -5.3053236 -8.6702156 -9.784708 -10.086775][-6.1489549 -5.71866 -4.2010989 -1.8090773 0.013401985 1.6048255 1.9087892 1.5047808 0.98936653 0.62064791 -0.50744438 -5.336751 -8.5788231 -9.9622641 -10.510059][-5.3813357 -5.4974341 -3.7825017 -1.4805117 -0.56756163 1.5444984 1.9633651 1.747076 1.6015015 0.8808732 -0.28050423 -5.1570139 -8.3261585 -9.7940159 -10.014698][-4.876133 -3.8046544 -2.483453 -0.0055136681 0.746799 1.2352834 0.70342922 0.47627831 0.87881851 0.63105965 0.34059286 -4.1015778 -7.4700232 -9.0998468 -9.0234375][-3.9500551 -3.1764319 -2.6634052 -0.77429247 0.38818073 0.96605778 0.49298811 0.20603704 -0.22188711 -0.80297565 -1.4477801 -4.9244738 -7.3177176 -8.4774094 -8.885026][-6.8806534 -5.2850819 -3.3228986 -2.0539019 -1.9939468 -2.582602 -3.4598565 -3.6819994 -4.2758064 -4.5435276 -4.8681364 -7.0968471 -8.4847679 -8.8465958 -8.005558][-9.5303535 -7.6405087 -5.7426128 -4.6054506 -4.909812 -5.4346695 -6.607141 -7.3934917 -7.4156933 -8.0093269 -8.5343628 -9.6990223 -9.8862457 -9.4574528 -8.5943222][-10.800964 -8.7555962 -6.57966 -5.5091672 -5.6306696 -6.6447706 -7.1160154 -7.1847644 -7.8675938 -8.9897251 -9.3667355 -10.182756 -11.206617 -10.67362 -8.8102436][-9.3970184 -8.5157118 -7.5237751 -6.6686683 -5.8141041 -6.008112 -6.4798837 -6.55329 -6.7679763 -7.5932417 -8.6374111 -9.493166 -9.4074631 -9.2619038 -9.2410069][-9.3483458 -9.224062 -8.0428143 -6.1581407 -5.7929373 -5.6943808 -5.1819587 -4.3890376 -5.07403 -5.4914322 -6.1289253 -7.2595444 -8.53273 -9.0222359 -9.4586868]]...]
INFO - root - 2017-12-15 12:22:32.107687: step 11310, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 56h:59m:57s remains)
INFO - root - 2017-12-15 12:22:38.644665: step 11320, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 57h:53m:30s remains)
INFO - root - 2017-12-15 12:22:45.235211: step 11330, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.650 sec/batch; 58h:00m:27s remains)
INFO - root - 2017-12-15 12:22:51.824395: step 11340, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 58h:45m:45s remains)
INFO - root - 2017-12-15 12:22:58.393065: step 11350, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 57h:18m:22s remains)
INFO - root - 2017-12-15 12:23:04.941591: step 11360, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.648 sec/batch; 57h:45m:55s remains)
INFO - root - 2017-12-15 12:23:11.515408: step 11370, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 57h:53m:26s remains)
INFO - root - 2017-12-15 12:23:18.067398: step 11380, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.681 sec/batch; 60h:46m:48s remains)
INFO - root - 2017-12-15 12:23:24.619821: step 11390, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.637 sec/batch; 56h:49m:02s remains)
INFO - root - 2017-12-15 12:23:31.196776: step 11400, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 58h:36m:44s remains)
2017-12-15 12:23:31.806975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0607138 -4.4177275 -4.2614255 -3.8501856 -4.0199986 -4.2152276 -4.3224831 -3.7645121 -3.1845634 -2.7459888 -2.4631524 -5.1756773 -6.9783897 -9.00769 -8.9639807][-4.7622218 -4.8503733 -4.6981049 -4.1351566 -4.3328586 -4.6898794 -4.93907 -4.9567876 -4.5894747 -3.8294439 -3.2581789 -5.6926584 -7.1575193 -9.5635242 -10.048052][-4.1618147 -4.5172997 -4.938149 -4.430685 -5.0583158 -5.5015922 -5.9584246 -5.8239741 -5.5852633 -5.0971084 -4.7821512 -7.2595305 -8.803544 -10.55752 -10.764791][-4.5352383 -4.4805679 -4.1676664 -3.9484806 -4.943356 -5.051085 -5.157793 -5.3518386 -5.1979904 -4.7987657 -4.568429 -7.505105 -9.53895 -11.484626 -11.383669][-5.109499 -5.795341 -5.7911315 -4.7155671 -4.9529071 -4.3845267 -4.2364445 -4.2690153 -4.0039916 -3.4326041 -3.2360711 -6.3873763 -8.5031309 -11.245012 -11.935966][-6.2131982 -6.0798116 -5.4692206 -3.749789 -3.0442743 -1.3819866 -0.69735336 -1.0305681 -1.1707101 -1.3591466 -1.6981249 -4.3700671 -6.3687096 -9.3879452 -10.600843][-7.0473747 -6.6073136 -5.7230716 -3.6939898 -1.6295819 0.11798334 1.4543962 2.1394181 2.095633 0.66627932 -0.81432295 -3.750751 -5.6785431 -8.4121494 -9.7549915][-7.1029415 -5.6973038 -5.0916266 -2.6795096 -0.82568169 1.9455376 3.5118771 3.0741148 3.1573248 2.1717567 0.14093876 -3.984823 -6.4408937 -8.61755 -9.12292][-4.2810631 -4.3094435 -3.064899 -2.0952713 -1.3981886 1.5371099 2.926033 3.227808 3.522665 1.9311547 -0.0053153038 -4.2330971 -7.1942329 -9.46263 -9.4187489][-1.9010382 -1.8801665 -2.0912285 -0.8259635 -0.69870377 0.14242315 0.57263422 1.3286085 1.182457 -0.27212429 -1.4607744 -5.6157432 -8.59372 -10.901173 -11.180838][-2.9622169 -2.3999987 -2.6072047 -2.6489701 -2.5153134 -2.3389864 -2.6560764 -2.4486406 -3.0209112 -3.9003737 -4.8798037 -8.680356 -11.163139 -12.486166 -12.28302][-7.5658159 -6.2380695 -5.9462519 -5.8219056 -5.3280277 -5.4143467 -6.3929825 -7.1435905 -7.6247129 -7.97866 -8.5767174 -10.416853 -11.265594 -12.357571 -11.884651][-10.140618 -9.3263912 -8.4272184 -8.3890867 -8.964447 -8.2199211 -7.8664594 -8.7416019 -9.849514 -10.164722 -10.276745 -10.681311 -10.918697 -11.198572 -9.9975681][-10.713133 -10.231594 -9.6905222 -9.2951336 -8.8716536 -8.6431656 -9.114542 -9.189187 -9.0837946 -9.4186459 -9.5380278 -8.4985676 -8.1133766 -9.2379379 -8.7448864][-8.2214508 -8.6911335 -8.3835258 -7.3847327 -6.6838932 -7.1074133 -7.5955815 -7.6776371 -7.8548365 -7.8914614 -7.47295 -8.2453842 -9.043993 -8.5371389 -8.4727077]]...]
INFO - root - 2017-12-15 12:23:38.467099: step 11410, loss = 0.17, batch loss = 0.12 (11.6 examples/sec; 0.689 sec/batch; 61h:27m:38s remains)
INFO - root - 2017-12-15 12:23:45.120509: step 11420, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 57h:30m:49s remains)
INFO - root - 2017-12-15 12:23:51.743245: step 11430, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 58h:46m:06s remains)
INFO - root - 2017-12-15 12:23:58.334012: step 11440, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 58h:59m:56s remains)
INFO - root - 2017-12-15 12:24:04.904900: step 11450, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 58h:29m:51s remains)
INFO - root - 2017-12-15 12:24:11.592562: step 11460, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 59h:54m:39s remains)
INFO - root - 2017-12-15 12:24:18.142635: step 11470, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 59h:16m:19s remains)
INFO - root - 2017-12-15 12:24:24.704746: step 11480, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.642 sec/batch; 57h:14m:23s remains)
INFO - root - 2017-12-15 12:24:31.275755: step 11490, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.658 sec/batch; 58h:37m:49s remains)
INFO - root - 2017-12-15 12:24:37.964833: step 11500, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 58h:13m:50s remains)
2017-12-15 12:24:38.536894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4571867 -3.9233131 -3.5981534 -3.0264766 -3.2181582 -3.1677825 -2.9021857 -2.8657355 -2.6181233 -2.5818214 -2.7369108 -4.5458865 -6.0500112 -7.2237325 -7.6982174][-2.2076139 -2.5810254 -3.6767747 -4.2765579 -4.1166143 -3.8910232 -3.4613743 -2.5289891 -2.364713 -2.688627 -3.0120118 -4.7270107 -5.5639267 -6.7911839 -7.8372746][-1.6986456 -2.6861408 -3.7718377 -3.3000708 -2.8933094 -2.6127687 -2.6879902 -3.0930684 -2.9343145 -2.921627 -3.5585506 -5.4166889 -6.474535 -7.30047 -7.883338][-2.0252597 -2.1438234 -2.2916012 -2.2780719 -2.3496902 -2.0264902 -1.3170161 -1.3257017 -2.3902066 -3.1288903 -3.5407574 -5.4591022 -6.8198352 -6.7945271 -6.8410311][-2.4933434 -3.3023915 -3.2646341 -2.1393437 -1.0311975 -0.030520439 0.68561888 0.15295553 -1.6122518 -2.3660808 -2.1103487 -4.3001981 -5.7430434 -6.9091229 -7.7085595][-3.7783556 -3.1757686 -2.3418427 -0.89524317 0.16938019 1.2784715 2.432992 2.4815817 1.3396597 -0.54397678 -1.6279621 -4.0804205 -4.9674234 -5.5128608 -6.2107921][-4.7988429 -4.5621109 -3.3117838 -1.1699548 0.77152014 3.2990694 4.7508593 4.4210253 3.1500368 1.4598718 0.33578777 -2.8282089 -4.68804 -5.7853751 -5.9493775][-4.7568398 -3.7304533 -2.648252 -0.45172548 0.6512537 2.8079758 4.4186459 4.2986307 3.503808 1.7864151 -0.31848478 -4.4801745 -6.4722624 -6.864584 -6.8042622][-3.8602116 -3.5302804 -2.6234293 -0.42258072 1.437449 3.0473337 3.278511 3.3286991 2.8333387 0.98484325 -0.8162446 -4.9003835 -7.2149262 -8.3752422 -8.8060646][-4.0568118 -3.1634455 -2.4201279 -0.56964922 0.84729719 1.7515574 1.8830142 1.6677265 1.0972495 -0.60627413 -2.3057046 -6.4030123 -8.9935589 -10.361697 -10.065526][-6.6210785 -5.9767604 -5.0519347 -3.1137867 -2.01533 -1.8012671 -3.0779989 -4.0280581 -5.0973644 -6.0923991 -6.803112 -10.123188 -11.069691 -11.586919 -10.720482][-9.987505 -9.1010008 -8.0043869 -5.8316808 -5.2446384 -5.8636432 -6.628953 -7.4779477 -8.755434 -9.9024477 -10.625948 -11.761 -12.061313 -11.796411 -10.919442][-11.083111 -10.58876 -8.7328424 -7.7548218 -7.9100819 -8.1431675 -8.9127426 -9.8379917 -11.071986 -11.72341 -12.14417 -12.859612 -12.411028 -11.369404 -10.055546][-11.96434 -10.936073 -10.104309 -8.0443153 -6.8338242 -7.871284 -8.59486 -9.1171389 -9.8595066 -9.5914125 -9.6501331 -9.8543425 -9.435957 -9.496171 -9.0260592][-9.8012571 -10.210339 -9.7149525 -8.5101833 -6.7472034 -7.2939353 -7.7906909 -7.5419388 -7.2160244 -7.3304739 -7.1355081 -8.3011551 -9.1784067 -9.0143976 -8.3517113]]...]
INFO - root - 2017-12-15 12:24:45.085754: step 11510, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.631 sec/batch; 56h:18m:01s remains)
INFO - root - 2017-12-15 12:24:51.599229: step 11520, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 57h:22m:39s remains)
INFO - root - 2017-12-15 12:24:58.176556: step 11530, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 59h:02m:24s remains)
INFO - root - 2017-12-15 12:25:04.742361: step 11540, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 58h:07m:49s remains)
INFO - root - 2017-12-15 12:25:11.253448: step 11550, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 61h:02m:02s remains)
INFO - root - 2017-12-15 12:25:17.848461: step 11560, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 59h:53m:48s remains)
INFO - root - 2017-12-15 12:25:24.398522: step 11570, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 59h:53m:06s remains)
INFO - root - 2017-12-15 12:25:31.130089: step 11580, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 59h:47m:59s remains)
INFO - root - 2017-12-15 12:25:37.667041: step 11590, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 57h:30m:12s remains)
INFO - root - 2017-12-15 12:25:44.252825: step 11600, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 58h:12m:56s remains)
2017-12-15 12:25:44.713351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4664989 -6.0974088 -6.0917864 -6.227201 -5.519309 -5.0692105 -4.4869061 -3.2509174 -2.9008739 -2.9490292 -2.9260163 -6.305407 -7.8900881 -10.054838 -9.2507877][-3.4177368 -4.58927 -4.7726479 -5.3858809 -5.1351404 -4.7101374 -4.194953 -4.0546823 -4.250618 -4.26054 -4.1051083 -6.8904352 -8.2847233 -10.909774 -11.482947][-2.6913378 -4.1427226 -5.2467327 -5.0591021 -4.8132572 -4.8133726 -4.6586852 -4.7506452 -4.6621809 -4.6902361 -5.0983286 -8.9515753 -11.40703 -13.70695 -13.536811][-2.6347451 -3.8274941 -4.6442556 -4.3711457 -4.9815097 -5.2645879 -5.1226363 -5.7898855 -5.89143 -5.3652921 -5.510828 -9.3866158 -12.571751 -15.472302 -16.132977][-3.4164755 -5.0580559 -6.2005095 -5.0080223 -3.7329431 -3.2236018 -3.9192519 -5.5610003 -6.8595915 -6.1387091 -5.8815489 -9.7053089 -13.133217 -16.802994 -18.060293][-4.3671823 -4.8793468 -4.3586683 -2.3607626 -0.91015625 0.055447578 0.8146801 -1.1650977 -2.9296656 -3.9197795 -5.3776164 -8.3927155 -11.021353 -15.367564 -16.829494][-4.8835139 -5.0627046 -3.8016181 -0.2379961 1.6978979 2.5773802 3.8440771 3.2883987 2.8808422 -0.43307877 -3.5005765 -7.1420593 -10.724613 -14.039166 -14.81144][-5.0272884 -4.5303211 -3.4063344 -0.5063858 1.4787264 3.561203 5.4140368 4.9410572 4.6317787 2.1121306 -0.22964525 -5.7337866 -9.8227262 -12.518497 -13.393074][-3.046792 -3.118834 -2.8906176 -1.4197478 -0.2279563 2.358819 3.839756 3.359705 4.1895628 3.269681 1.8048959 -3.6989031 -7.6985011 -11.209211 -11.227098][-1.1384611 -2.5070624 -2.9388418 -1.8167775 -0.9264102 -0.13139343 0.6042738 1.5491524 2.7175646 2.4980974 2.470993 -2.1316473 -5.9233131 -9.342824 -10.388731][-4.9266453 -5.6298203 -6.0715313 -5.1139684 -4.7126164 -5.141788 -4.3118677 -3.2474287 -2.2569418 -1.4752879 -0.54681826 -4.6157966 -6.4145021 -8.4022141 -8.4525795][-10.1457 -10.235296 -9.2734518 -8.1187134 -8.267765 -8.7837906 -8.6587334 -8.5172968 -7.95012 -7.6967945 -7.0739703 -7.9865265 -7.8919249 -9.5540295 -8.4939671][-13.340364 -11.545815 -10.112455 -9.48967 -9.0644617 -8.7887945 -9.2661037 -10.524578 -11.316795 -10.020781 -9.0166178 -9.6304884 -8.4490862 -8.0342207 -6.8726239][-10.842647 -9.7166061 -8.181736 -6.02092 -5.0903239 -5.7476339 -7.0357089 -8.0499735 -9.2923489 -9.8316259 -10.217713 -9.7025757 -8.3409767 -8.4662247 -8.0407486][-8.3248825 -7.5735979 -6.2453265 -4.9024386 -3.7660861 -4.3676834 -5.6036086 -6.9594607 -8.4209652 -9.2141457 -9.1396141 -10.008371 -10.550941 -9.73313 -9.0377693]]...]
INFO - root - 2017-12-15 12:25:51.337397: step 11610, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 59h:48m:21s remains)
INFO - root - 2017-12-15 12:25:57.947230: step 11620, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 57h:50m:07s remains)
INFO - root - 2017-12-15 12:26:04.543403: step 11630, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 58h:16m:54s remains)
INFO - root - 2017-12-15 12:26:11.202625: step 11640, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 58h:46m:37s remains)
INFO - root - 2017-12-15 12:26:17.772754: step 11650, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 58h:03m:35s remains)
INFO - root - 2017-12-15 12:26:24.346466: step 11660, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 58h:58m:17s remains)
INFO - root - 2017-12-15 12:26:30.901054: step 11670, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 57h:23m:30s remains)
INFO - root - 2017-12-15 12:26:37.457276: step 11680, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 56h:34m:10s remains)
INFO - root - 2017-12-15 12:26:44.173950: step 11690, loss = 0.15, batch loss = 0.11 (11.3 examples/sec; 0.706 sec/batch; 62h:54m:14s remains)
INFO - root - 2017-12-15 12:26:50.757384: step 11700, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 57h:23m:31s remains)
2017-12-15 12:26:51.252684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.9104376 -11.635487 -11.771264 -10.727002 -11.123671 -10.806333 -9.98439 -8.8226776 -7.738102 -6.1239038 -4.4212756 -2.9516122 -3.4425898 -3.7473164 -2.7132325][-8.0783386 -8.9033222 -9.1350937 -7.7058473 -8.1477346 -8.3465309 -7.6199288 -5.8539476 -4.4909391 -3.2380614 -2.0936389 -0.64847469 -1.6227865 -2.6727848 -1.7423186][-5.9756551 -6.8168545 -7.160954 -6.4638648 -6.3082523 -6.5550146 -5.6068578 -4.8009815 -3.4704285 -2.1113167 -1.2794142 -0.72306061 -3.1719091 -4.5466108 -4.3393664][-6.8635583 -6.5866222 -5.856864 -5.4523454 -5.52566 -5.1765456 -4.9528093 -4.7396193 -3.6892607 -2.8228235 -2.0894458 -2.0075567 -4.5669379 -7.0219922 -7.76805][-7.47603 -8.3341293 -7.7073927 -5.6084719 -3.8704927 -2.5195248 -2.0407722 -3.2348201 -3.8520586 -3.2386959 -2.5631721 -2.9168429 -6.3413124 -9.503088 -9.6909008][-9.0826969 -9.0100021 -7.0386791 -4.5737333 -2.126416 0.27328968 1.1277361 -0.72653389 -2.0374215 -2.9297681 -3.9813552 -4.4595051 -7.6710119 -10.935465 -10.344692][-10.880825 -9.7196331 -6.8865142 -2.8560591 0.028685093 2.0083022 2.8898654 2.6526189 1.8444395 -1.3608985 -3.442889 -3.6838493 -7.3843408 -9.8142891 -9.4908285][-9.9347954 -8.1372013 -5.5560236 -1.519598 1.5398631 3.3631902 3.8189349 2.788981 2.0607114 -0.61535645 -3.4095664 -4.5742211 -8.1409416 -9.9256124 -8.7311649][-7.5252404 -5.4401984 -3.1190758 -0.60735321 0.68251085 3.3397431 4.03826 2.5427489 0.97031164 -1.6586614 -4.4038343 -6.7904115 -10.957665 -11.761616 -10.001915][-5.7398477 -4.33089 -2.564249 -0.75062084 -0.48786116 0.464931 0.42264366 0.00070095062 -1.3107862 -4.5234747 -7.1563587 -8.7571468 -12.677103 -14.470438 -12.885677][-9.3272781 -8.2393475 -7.1051035 -5.2996345 -4.6518583 -4.6180973 -5.1553874 -5.7569547 -7.3069777 -9.7640572 -11.047146 -12.610824 -15.930927 -16.479116 -14.621939][-15.016779 -14.032618 -12.591883 -10.267482 -9.4984941 -9.7624378 -10.255024 -10.552124 -11.64546 -13.072836 -13.67774 -15.048374 -16.919353 -16.896397 -14.836065][-17.528032 -16.112272 -13.836992 -11.667536 -11.45428 -10.558665 -10.867235 -11.481607 -11.999968 -12.068529 -12.308954 -13.676672 -14.827269 -14.152678 -11.535807][-15.434946 -14.065687 -12.523865 -10.460078 -10.057451 -9.7209167 -10.188378 -9.4684639 -9.1269341 -9.5878792 -9.8428669 -10.109184 -10.661754 -10.792843 -8.5174026][-11.191124 -10.555611 -8.7676411 -6.4765234 -5.2664618 -5.7795143 -6.1244254 -5.5703206 -5.5520144 -5.6082392 -6.0597491 -7.3412819 -8.6140175 -8.866909 -7.4679241]]...]
INFO - root - 2017-12-15 12:26:57.808282: step 11710, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 57h:59m:19s remains)
INFO - root - 2017-12-15 12:27:04.430591: step 11720, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 57h:28m:16s remains)
INFO - root - 2017-12-15 12:27:11.042953: step 11730, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 58h:42m:40s remains)
INFO - root - 2017-12-15 12:27:17.675025: step 11740, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 60h:14m:43s remains)
INFO - root - 2017-12-15 12:27:24.268359: step 11750, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 58h:16m:59s remains)
INFO - root - 2017-12-15 12:27:30.812162: step 11760, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 56h:38m:14s remains)
INFO - root - 2017-12-15 12:27:37.442401: step 11770, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 59h:05m:07s remains)
INFO - root - 2017-12-15 12:27:44.060309: step 11780, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.675 sec/batch; 60h:09m:46s remains)
INFO - root - 2017-12-15 12:27:50.680867: step 11790, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 58h:00m:27s remains)
INFO - root - 2017-12-15 12:27:57.234777: step 11800, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 58h:10m:43s remains)
2017-12-15 12:27:57.734841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2263308 -6.4492774 -5.1376195 -4.7572885 -5.4159474 -6.2867036 -6.880383 -7.9344754 -8.7619057 -8.0091543 -6.4823036 -5.8328915 -6.0063305 -5.4268017 -4.6531119][-7.3555527 -5.9706793 -4.8704009 -3.6685073 -3.8945971 -4.5271616 -5.0711646 -5.5627418 -6.3633046 -6.1509013 -4.4953732 -3.4736271 -3.6712768 -3.82282 -4.0173268][-6.1735692 -5.2482615 -5.6317692 -4.53447 -3.283715 -2.8668067 -2.9852986 -3.0692809 -3.6046476 -3.6268833 -2.2992055 -1.521174 -1.646472 -1.8799536 -2.1338212][-8.1257839 -7.5454235 -7.1215005 -6.2435923 -5.5195422 -3.7135048 -2.4628868 -2.7685289 -3.4874151 -3.1424146 -2.6642358 -2.7089205 -2.9025495 -2.8459864 -2.886611][-9.4395781 -9.8905029 -10.30253 -9.7068214 -8.4452238 -4.6063724 -1.5776062 -1.7701876 -3.8914313 -3.3302579 -3.1936176 -3.7347732 -4.0419993 -4.4097252 -5.069943][-8.9547911 -9.6786261 -9.7678719 -7.947732 -5.1956563 -1.0714483 2.5521698 2.7776451 1.1951079 -1.6169415 -4.7070136 -3.8261063 -4.1044397 -4.9112883 -5.0483923][-10.255129 -9.5834579 -8.4843712 -6.1201158 -2.9304888 1.8992367 6.6846895 6.934082 5.5070448 0.49856567 -5.0449457 -5.5332704 -6.275425 -6.2919283 -5.4213362][-10.381189 -10.681061 -9.6885519 -5.928772 -1.132381 3.1550198 7.7832422 8.0429611 6.592577 1.9103966 -3.3676357 -5.471806 -7.4355354 -6.5187049 -5.3512778][-9.19001 -8.41422 -9.1613388 -6.0422421 -2.5507531 1.2419829 5.5982561 5.608942 4.06569 -0.26646185 -4.79045 -6.8867474 -8.9836683 -8.1963215 -6.5492868][-8.9405289 -8.8421412 -9.1410065 -7.6846619 -5.6347566 -2.3899195 1.9790635 3.1619339 2.0535827 -1.8058455 -5.6717086 -7.6346045 -9.3854942 -9.7828293 -9.300374][-11.783694 -11.475645 -11.525214 -9.8478193 -8.7715521 -7.27102 -4.6743283 -3.34276 -3.1118326 -4.8297997 -7.1138496 -8.9764938 -9.4396429 -10.046202 -8.6372423][-16.069736 -14.373703 -11.644622 -10.623115 -10.268416 -9.25149 -9.0822344 -8.464736 -7.8534036 -8.5569391 -9.973547 -10.892676 -10.898901 -10.893774 -9.2310677][-13.177535 -12.177303 -10.576161 -8.7766676 -8.1909933 -7.965116 -7.8885169 -8.1398315 -8.740406 -9.1782312 -9.8277206 -10.442152 -10.268061 -9.4384394 -8.184453][-11.261595 -10.030461 -9.0394735 -8.0707417 -7.4915133 -8.1365013 -8.4537392 -7.7343616 -7.6329765 -7.9693356 -8.7924891 -8.4949207 -8.3613014 -7.34809 -6.0354743][-7.7786255 -6.9646935 -5.3254685 -4.8995161 -5.0972118 -5.8485479 -5.7415104 -6.3122106 -6.3378811 -6.0406108 -5.8585734 -5.8153253 -5.8183584 -6.737515 -7.4136119]]...]
INFO - root - 2017-12-15 12:28:04.349702: step 11810, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 57h:58m:48s remains)
INFO - root - 2017-12-15 12:28:10.965422: step 11820, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 57h:39m:08s remains)
INFO - root - 2017-12-15 12:28:17.449665: step 11830, loss = 0.22, batch loss = 0.18 (12.1 examples/sec; 0.662 sec/batch; 58h:59m:25s remains)
INFO - root - 2017-12-15 12:28:23.984671: step 11840, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 57h:50m:34s remains)
INFO - root - 2017-12-15 12:28:30.656885: step 11850, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 58h:59m:37s remains)
INFO - root - 2017-12-15 12:28:37.276091: step 11860, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 58h:12m:48s remains)
INFO - root - 2017-12-15 12:28:43.879604: step 11870, loss = 0.15, batch loss = 0.10 (11.4 examples/sec; 0.704 sec/batch; 62h:41m:06s remains)
INFO - root - 2017-12-15 12:28:50.435973: step 11880, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 57h:49m:27s remains)
INFO - root - 2017-12-15 12:28:56.997898: step 11890, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 59h:07m:05s remains)
INFO - root - 2017-12-15 12:29:03.581064: step 11900, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 58h:07m:12s remains)
2017-12-15 12:29:04.097476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1484461 -4.9799428 -4.1343927 -3.5737238 -3.4268112 -3.385931 -3.1696942 -2.8861198 -2.6689544 -2.1227632 -1.8094177 -4.6221876 -6.2530165 -6.9459162 -6.4580173][-6.1540484 -5.2505789 -4.5513067 -4.052794 -4.184371 -4.3438768 -4.0140595 -3.9515848 -4.0568695 -3.7952509 -3.6363859 -6.2960081 -7.7232561 -7.888483 -7.2964492][-4.2858639 -5.4099755 -5.7093329 -4.9682035 -5.3132467 -5.1119614 -4.7426944 -4.7841191 -5.1604433 -5.2829018 -5.5413332 -8.2959127 -9.8934746 -9.9315567 -8.7133293][-3.9071646 -4.7991562 -5.2599807 -5.2672067 -6.186398 -5.6193309 -4.9904494 -5.2118096 -5.2317786 -5.3817997 -5.6509628 -8.6703615 -10.395884 -10.58791 -9.8762913][-4.064045 -5.0199223 -5.3100896 -4.7845397 -4.770752 -3.3751345 -2.3830428 -3.4669042 -4.370338 -4.4447865 -4.9474998 -8.2765007 -9.7129745 -10.51759 -9.6792889][-6.1780529 -5.7856646 -5.056541 -3.5519209 -2.1567068 0.519938 2.0174761 1.0855169 -0.26100397 -1.6874743 -2.8726325 -5.9537082 -7.6817245 -9.0814915 -8.6353674][-6.7152052 -5.76554 -5.0551324 -2.0805855 0.26005316 2.9983902 4.8744802 4.9085007 3.8731432 0.65675831 -1.4031577 -4.6355228 -6.4518423 -7.5618296 -7.0748549][-5.9347229 -4.7554364 -3.3292079 -0.37598658 1.6756129 4.1838293 6.0253105 5.7561274 4.7693472 2.1246996 0.15557432 -3.2152202 -5.0552449 -5.7791057 -4.7417932][-3.1135256 -2.7903397 -1.7830925 0.17378521 1.3224311 3.4685078 5.3257647 4.5459175 3.5584707 2.228467 0.75734949 -2.429996 -4.050137 -4.6042972 -3.3192909][-2.8643894 -2.9424436 -1.7748542 0.11236858 0.77338362 2.0877771 2.9908438 2.5596657 2.2257586 1.1313457 0.028533459 -2.9057922 -4.2856455 -4.4719148 -3.4284496][-8.8195648 -7.8042703 -5.8488908 -3.2382498 -1.8344223 -0.84881592 -0.47842932 -0.64784193 -0.30780935 -1.5469151 -2.8762388 -5.2817106 -6.5601664 -6.2441468 -4.46464][-14.352951 -12.608234 -9.9848251 -6.4071927 -4.3071203 -3.6665642 -3.4468684 -3.2435496 -3.0969872 -4.0568357 -5.4132352 -6.64095 -7.432601 -7.7498884 -6.6248169][-15.040519 -13.891076 -11.496254 -8.6498518 -6.5105143 -5.2144175 -4.828732 -5.4145293 -6.3521495 -6.547245 -7.5476303 -8.1688089 -8.2720585 -8.32493 -7.230608][-11.106606 -11.124037 -9.7745085 -8.5365086 -7.4761629 -6.3438258 -5.8909321 -5.8560443 -6.4923806 -7.3907218 -8.387866 -8.2629242 -8.3866749 -8.0087986 -7.2494678][-8.4584846 -8.3990164 -7.3395457 -6.484735 -5.9842544 -5.3245206 -4.7679505 -4.6604881 -5.0950413 -5.6813045 -6.5405359 -7.6363153 -8.6521111 -8.2823982 -7.3092046]]...]
INFO - root - 2017-12-15 12:29:10.697154: step 11910, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 59h:15m:34s remains)
INFO - root - 2017-12-15 12:29:17.207105: step 11920, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.632 sec/batch; 56h:17m:06s remains)
INFO - root - 2017-12-15 12:29:23.775233: step 11930, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 58h:49m:54s remains)
INFO - root - 2017-12-15 12:29:30.421950: step 11940, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.690 sec/batch; 61h:25m:33s remains)
INFO - root - 2017-12-15 12:29:37.056368: step 11950, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 57h:03m:39s remains)
INFO - root - 2017-12-15 12:29:43.660290: step 11960, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 56h:42m:34s remains)
INFO - root - 2017-12-15 12:29:50.162234: step 11970, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 59h:05m:08s remains)
INFO - root - 2017-12-15 12:29:56.810973: step 11980, loss = 0.13, batch loss = 0.09 (11.4 examples/sec; 0.700 sec/batch; 62h:21m:29s remains)
INFO - root - 2017-12-15 12:30:03.461332: step 11990, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.686 sec/batch; 61h:06m:54s remains)
INFO - root - 2017-12-15 12:30:10.018753: step 12000, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 57h:50m:15s remains)
2017-12-15 12:30:10.489892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.2125969 -10.414168 -9.8860188 -8.870018 -8.8679552 -8.9239416 -8.5915508 -7.200109 -5.8323522 -5.0419626 -4.9531054 -5.3019443 -5.5843892 -5.6274958 -4.9938068][-9.16484 -9.7277937 -9.5850954 -8.2927513 -8.6507616 -8.5468893 -7.6316452 -6.7690477 -6.0988083 -4.6758084 -3.5915685 -4.1880312 -4.9009342 -5.1685572 -4.4781671][-7.6616516 -7.6488209 -7.243247 -6.4924045 -6.8982525 -6.5313468 -6.3751483 -5.5151224 -4.574018 -4.4928417 -4.3032384 -5.1732359 -6.1268635 -6.4046988 -6.1140237][-7.7159643 -7.12837 -5.2597361 -3.9888024 -4.3908238 -4.4805579 -4.4732852 -4.246685 -3.7901278 -3.4597967 -3.3260109 -5.17485 -6.8518505 -7.6450019 -7.8177462][-8.4029541 -7.946187 -5.6183653 -2.4018898 -1.3945251 -0.99825144 -0.89156008 -1.4514279 -2.4498658 -2.5385807 -2.3907833 -4.8568182 -6.9149208 -7.90662 -8.6679745][-8.8206768 -7.7693033 -4.9722352 -1.6953182 0.043854237 1.0658922 1.9177837 2.21412 1.5042434 0.015901566 -1.4974651 -3.5930879 -5.3209467 -7.7156143 -8.3838425][-9.5245991 -7.9665012 -4.5879803 -0.1031518 2.5063553 3.5660696 4.4819279 4.2246418 4.2637577 2.754117 0.35488319 -3.1745577 -5.4687443 -6.9159608 -7.4304619][-9.1649475 -7.3397655 -3.5627208 1.0614009 3.478961 5.0094461 5.03232 3.3447695 3.7195311 3.0952749 1.2444735 -2.0886507 -4.9095321 -6.8402734 -6.6581693][-7.1376591 -5.6370554 -3.0447662 0.58753872 2.1163697 4.092082 4.5053682 3.3493881 3.214324 2.1026058 0.49623203 -2.6470845 -6.0589881 -7.68584 -7.9755154][-5.0050316 -3.9304957 -2.0290694 0.52513123 1.9773855 2.6203918 2.074563 0.92140055 0.7573595 0.39642334 -0.77147818 -4.0782652 -7.4095459 -9.149251 -9.7946224][-7.9826512 -6.1811643 -4.6867781 -2.6455054 -2.4408627 -1.9951696 -1.8009806 -2.4337318 -3.0036197 -3.4831994 -4.49148 -7.8472986 -9.58555 -10.166459 -10.023901][-10.158911 -8.934207 -7.5868707 -6.3404918 -6.6147985 -5.6959853 -5.6217823 -6.2557211 -7.1671314 -7.5640268 -8.0314751 -9.4351492 -10.439715 -11.766318 -11.163225][-12.51088 -10.962873 -9.5036135 -8.6296053 -8.4706059 -7.9415417 -7.8156834 -7.8605661 -7.97591 -8.3347836 -8.8715792 -9.3450489 -9.5318594 -8.6314983 -7.477201][-11.148308 -11.038902 -9.6652021 -7.8172684 -6.7933078 -6.4336271 -6.2894773 -5.2027807 -5.6686859 -6.297616 -6.4695907 -6.1247716 -5.4334197 -5.5767493 -5.0967331][-9.0376606 -9.5407438 -9.4891281 -8.5521164 -6.6441579 -4.9495049 -3.9955006 -3.9160671 -4.2668 -3.4900432 -3.5537491 -4.58503 -5.6026363 -5.4403028 -4.86053]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 12:30:17.228814: step 12010, loss = 0.15, batch loss = 0.11 (11.0 examples/sec; 0.724 sec/batch; 64h:28m:56s remains)
INFO - root - 2017-12-15 12:30:23.906927: step 12020, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 58h:00m:55s remains)
INFO - root - 2017-12-15 12:30:30.578189: step 12030, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 57h:10m:58s remains)
INFO - root - 2017-12-15 12:30:37.114486: step 12040, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 58h:59m:16s remains)
INFO - root - 2017-12-15 12:30:43.764780: step 12050, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 57h:55m:30s remains)
INFO - root - 2017-12-15 12:30:50.396526: step 12060, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.697 sec/batch; 62h:03m:44s remains)
INFO - root - 2017-12-15 12:30:56.975539: step 12070, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 58h:52m:22s remains)
INFO - root - 2017-12-15 12:31:03.600394: step 12080, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 58h:08m:44s remains)
INFO - root - 2017-12-15 12:31:10.233853: step 12090, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 58h:04m:58s remains)
INFO - root - 2017-12-15 12:31:16.870355: step 12100, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 58h:07m:37s remains)
2017-12-15 12:31:17.391512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.986537 -8.3649654 -9.4375887 -9.9411516 -10.948915 -10.385942 -9.9066639 -10.127144 -9.9546127 -10.21246 -10.105818 -9.5161819 -10.762356 -10.098215 -9.7563457][-7.3236761 -10.044567 -10.811058 -11.654173 -12.283209 -12.08559 -11.937102 -11.340234 -11.274335 -10.262926 -9.5583677 -11.284361 -13.423057 -12.40357 -11.353118][-6.1309218 -8.7202139 -10.94059 -11.618455 -12.084355 -12.542797 -12.118961 -11.896202 -11.667413 -10.597445 -10.345751 -10.170307 -11.729111 -12.978724 -13.634298][-7.286128 -8.9023743 -10.714068 -11.011347 -10.197203 -8.9164057 -9.2232342 -10.670102 -11.245392 -10.529989 -9.8890457 -10.199566 -12.529315 -13.109187 -14.212224][-8.0449848 -11.16954 -13.394999 -11.664953 -9.0941792 -4.3355255 -1.4652796 -5.3127637 -9.9808273 -9.4574814 -8.5372744 -9.2766685 -12.437919 -13.298305 -13.588733][-8.7940464 -11.390957 -11.874989 -10.09743 -8.2686224 -1.6646261 5.7883015 4.2021112 -0.48769617 -4.9545975 -6.8457689 -6.6729221 -8.7312651 -9.6765909 -11.665756][-9.4201345 -12.481862 -12.04627 -8.0611343 -5.0535212 1.5654745 6.6260433 8.07799 7.1325569 0.41960526 -6.4145317 -6.2270784 -7.5088167 -9.4952822 -11.435013][-7.8765564 -10.88083 -11.933722 -8.59514 -2.3766284 3.6990395 7.3534336 8.4555864 8.3085 2.1536999 -3.1092918 -5.6604619 -9.349514 -9.1539125 -9.6923714][-5.4892187 -7.0917678 -8.632678 -8.7188692 -5.5241237 -1.0805497 4.5678043 6.8827972 6.2929249 1.9652781 -1.7201264 -5.6526814 -9.7094479 -10.047856 -11.45488][-4.0097122 -5.5837135 -7.366302 -7.9988308 -6.7412481 -4.0096755 0.40671253 2.854166 2.7287388 -0.15680122 -2.8360023 -6.279922 -10.773132 -11.94031 -13.402325][-7.5438209 -9.47419 -11.289211 -12.104918 -11.244277 -9.7051 -7.8644171 -6.2545333 -5.5054054 -5.9368529 -7.3021936 -8.9952812 -9.39176 -10.949471 -11.940943][-11.522224 -12.087407 -13.185135 -13.108801 -13.281985 -11.993979 -11.563013 -11.457226 -11.548571 -11.23172 -10.840296 -10.999496 -11.722361 -11.380169 -11.487411][-9.18689 -10.307516 -11.266306 -11.349004 -11.898293 -11.233929 -10.407944 -10.677819 -12.43591 -11.638639 -10.03953 -10.281907 -10.319723 -9.7231188 -9.3595161][-7.0907545 -6.9898286 -7.6821537 -8.5508156 -10.479847 -10.304636 -9.3970985 -9.2584152 -10.134863 -11.019635 -11.687099 -9.7408419 -8.4992161 -7.9067745 -8.01718][-4.8392277 -5.6733794 -4.6172237 -5.1730509 -6.0990615 -6.0750055 -6.8464823 -6.4875565 -6.4991059 -7.4296365 -8.7685261 -9.9179707 -9.5695591 -9.0321589 -7.8819304]]...]
INFO - root - 2017-12-15 12:31:23.993461: step 12110, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 60h:52m:33s remains)
INFO - root - 2017-12-15 12:31:30.615032: step 12120, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 57h:20m:03s remains)
INFO - root - 2017-12-15 12:31:37.187318: step 12130, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 59h:25m:00s remains)
INFO - root - 2017-12-15 12:31:43.832753: step 12140, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 58h:47m:05s remains)
INFO - root - 2017-12-15 12:31:50.478160: step 12150, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 60h:26m:04s remains)
INFO - root - 2017-12-15 12:31:57.100269: step 12160, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.682 sec/batch; 60h:42m:29s remains)
INFO - root - 2017-12-15 12:32:03.730232: step 12170, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 59h:04m:50s remains)
INFO - root - 2017-12-15 12:32:10.367434: step 12180, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 59h:13m:32s remains)
INFO - root - 2017-12-15 12:32:17.013717: step 12190, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 57h:34m:07s remains)
INFO - root - 2017-12-15 12:32:23.602843: step 12200, loss = 0.11, batch loss = 0.07 (12.0 examples/sec; 0.666 sec/batch; 59h:17m:31s remains)
2017-12-15 12:32:24.119842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2017088 -4.926856 -4.509181 -4.4209194 -5.1559825 -4.9863057 -4.76967 -3.4130168 -1.9813693 -1.3326325 -0.68753624 -3.1782444 -4.4627161 -5.8481946 -5.9275761][-4.1879888 -3.5110605 -3.7140415 -3.3035555 -3.9005249 -4.2488203 -4.0298963 -2.7310977 -1.1502285 0.13928127 1.0264544 -1.405313 -3.2380939 -5.4342318 -6.082356][-3.5071294 -2.5885918 -3.3109102 -2.9470222 -3.9384375 -4.0668297 -3.8838127 -2.4353373 -0.30002546 0.64653063 1.3016477 -1.0828118 -2.5475397 -3.9610677 -5.0194244][-3.302233 -1.919436 -2.2922373 -2.9225483 -4.4663658 -4.7552643 -4.3150363 -2.5750835 -0.55504894 0.68172646 1.4249229 -1.0302649 -2.6977527 -4.7440557 -5.6846061][-2.5479174 -2.4218595 -3.1281369 -3.3724411 -4.3760324 -3.99744 -3.0173969 -2.2723002 -1.3919559 -0.044702053 0.76685381 -2.1429057 -3.9564052 -5.9581885 -6.2117643][-3.6849124 -3.465169 -3.3684492 -2.4288983 -2.1968446 -0.56473732 1.6720238 1.4458175 0.23355198 -0.69279909 -1.1410203 -2.9453621 -4.2719917 -6.37567 -7.3757515][-5.4972286 -4.658299 -3.5784841 -1.51088 -0.24020767 1.731174 4.3406515 4.8228474 4.0555019 0.62817955 -2.3967688 -4.3802958 -5.4994307 -7.19013 -7.9608145][-6.9241343 -6.0343819 -4.8262281 -2.1395018 -0.049773216 2.7280931 5.3711128 5.5342264 5.0573997 2.1072302 -1.4473348 -5.0035272 -6.7682009 -7.689703 -8.2793617][-5.8610353 -5.1462784 -4.6644812 -2.8355572 -0.7859354 2.3113337 4.0713835 4.260294 4.4386806 2.2324524 -0.7404542 -5.5094657 -8.2147512 -9.2698011 -9.1258173][-5.4940982 -4.4445353 -4.4811296 -2.7995188 -0.37499 1.5992031 2.8391418 2.8889742 2.2325907 0.064217091 -2.6633327 -7.5175304 -10.227719 -11.366466 -11.825365][-10.464563 -8.6110229 -7.4199944 -4.9980865 -2.8402445 -1.7674863 -1.3917089 -1.8278921 -2.4280782 -3.5744443 -5.7327614 -10.17447 -13.154629 -13.898309 -13.626925][-13.097933 -11.457521 -10.165418 -8.1578579 -6.6563678 -5.4512534 -5.2941914 -5.9906769 -7.2988424 -7.8010063 -8.6029387 -11.039795 -12.212982 -13.17655 -13.081152][-13.126647 -12.132225 -10.747796 -9.23461 -8.7823973 -7.846734 -7.7997656 -8.7757339 -10.119055 -10.581079 -10.809361 -12.069858 -12.460636 -11.455046 -10.005953][-11.18087 -10.71652 -10.636497 -9.4602966 -8.6919851 -8.3429394 -8.7159557 -9.1664066 -9.8007317 -10.552055 -10.93693 -10.884778 -11.178725 -10.413764 -9.4064226][-7.58304 -8.1731167 -7.8587456 -7.6871333 -7.5554242 -7.4018364 -7.6748862 -7.0855269 -7.4134207 -8.1150875 -8.5949984 -10.034432 -11.579096 -10.524821 -9.9485817]]...]
INFO - root - 2017-12-15 12:32:30.782261: step 12210, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 59h:51m:50s remains)
INFO - root - 2017-12-15 12:32:37.341096: step 12220, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 58h:40m:16s remains)
INFO - root - 2017-12-15 12:32:43.965995: step 12230, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 57h:22m:23s remains)
INFO - root - 2017-12-15 12:32:50.628127: step 12240, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 58h:54m:23s remains)
INFO - root - 2017-12-15 12:32:57.208320: step 12250, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 59h:22m:55s remains)
INFO - root - 2017-12-15 12:33:03.841064: step 12260, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 59h:39m:17s remains)
INFO - root - 2017-12-15 12:33:10.410807: step 12270, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 57h:18m:14s remains)
INFO - root - 2017-12-15 12:33:17.027826: step 12280, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 59h:00m:54s remains)
INFO - root - 2017-12-15 12:33:23.689032: step 12290, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 58h:12m:50s remains)
INFO - root - 2017-12-15 12:33:30.390457: step 12300, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.676 sec/batch; 60h:08m:48s remains)
2017-12-15 12:33:30.926750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7044435 -6.8425531 -7.0821724 -6.3415465 -6.3429623 -6.853704 -8.2329912 -8.2034836 -7.56166 -6.2276816 -5.214406 -5.8185196 -6.9328279 -7.6751366 -6.2698212][-5.7136116 -5.9379163 -4.3772688 -4.3014994 -4.50718 -5.4204636 -5.6000195 -5.9915218 -5.4741011 -4.7702341 -4.4591036 -6.2948003 -6.569581 -7.477694 -7.1901073][-4.4175577 -5.4168634 -4.8933706 -3.3899782 -3.7278583 -4.7079005 -5.1704178 -5.5922656 -4.62098 -3.9533329 -3.7284927 -5.8385444 -6.4268909 -6.6562281 -6.8010149][-6.3418288 -6.2601018 -5.5542688 -4.2044721 -3.5596616 -4.1072288 -5.2240186 -5.428956 -4.5603905 -3.8577468 -3.3889503 -5.6587734 -7.1918688 -8.4059925 -8.7526913][-8.5952415 -9.0066147 -7.8167381 -5.035799 -3.1739793 -2.3049092 -2.3871875 -3.1915357 -3.9730966 -2.6555893 -2.1107874 -4.9445181 -6.6808095 -7.8306131 -8.1641064][-10.48915 -9.7831726 -7.3325744 -3.3917243 -0.12271547 1.6075969 2.6348448 0.75962353 -0.38381195 -0.19460869 -0.98803473 -2.6037345 -3.7863228 -6.5664945 -7.0881987][-9.065856 -8.9032211 -6.2517147 -1.1394663 1.3715768 3.7852812 5.995574 4.5654182 3.9637632 1.767745 -1.0956426 -3.3466983 -5.077322 -6.9546785 -7.6029124][-9.4074249 -7.7942038 -4.9915314 -0.40196562 2.014523 4.3257866 6.2954016 4.4471874 3.5191622 2.1942835 0.375319 -3.2869439 -6.247211 -7.8009262 -8.0143089][-7.6109424 -6.8789725 -5.309679 -2.6907558 -0.38202858 2.9728498 4.4820862 2.9738932 3.4046364 1.5559473 -0.46008205 -4.1849203 -7.2296214 -9.0685415 -9.499568][-6.4692845 -6.7628326 -5.8781285 -3.3817134 -1.8791614 -0.053830624 1.543376 1.858119 2.3051748 0.040150166 -1.9511342 -5.5916963 -7.6219263 -9.4748249 -11.11758][-10.178537 -9.7429419 -8.1513987 -7.2339907 -6.3725796 -5.6870122 -4.99303 -4.3923931 -3.4396424 -3.6840143 -4.1555424 -8.7588673 -11.000741 -11.996197 -10.655201][-15.021358 -14.36262 -12.912739 -10.729837 -9.6443272 -9.1044559 -9.0597324 -10.0083 -9.7113981 -9.1445026 -8.8474064 -11.195961 -11.638046 -13.104039 -13.142011][-15.307212 -14.423667 -14.043581 -13.001728 -12.004562 -10.496357 -9.6618528 -10.313126 -10.926222 -10.909117 -10.481155 -11.185302 -10.738385 -11.074987 -9.7122469][-12.997969 -12.219976 -11.261583 -10.090389 -8.9522676 -9.0750761 -9.390914 -8.87344 -8.9065733 -9.4024191 -9.4964132 -9.0221481 -8.6109371 -9.6167154 -8.7508974][-10.025152 -9.0003119 -7.5066671 -6.0562325 -5.67992 -5.9891233 -6.0195909 -7.075408 -7.665349 -6.7775769 -6.5450583 -7.7505274 -8.3320541 -7.843545 -8.539876]]...]
INFO - root - 2017-12-15 12:33:37.538362: step 12310, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 57h:12m:39s remains)
INFO - root - 2017-12-15 12:33:44.155406: step 12320, loss = 0.11, batch loss = 0.07 (11.6 examples/sec; 0.689 sec/batch; 61h:15m:13s remains)
INFO - root - 2017-12-15 12:33:50.835198: step 12330, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 57h:41m:13s remains)
INFO - root - 2017-12-15 12:33:57.478015: step 12340, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 58h:51m:48s remains)
INFO - root - 2017-12-15 12:34:04.051582: step 12350, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 59h:16m:45s remains)
INFO - root - 2017-12-15 12:34:10.741689: step 12360, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 59h:38m:20s remains)
INFO - root - 2017-12-15 12:34:17.353868: step 12370, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 60h:14m:52s remains)
INFO - root - 2017-12-15 12:34:23.847580: step 12380, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 58h:07m:02s remains)
INFO - root - 2017-12-15 12:34:30.401784: step 12390, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 57h:32m:23s remains)
INFO - root - 2017-12-15 12:34:37.029458: step 12400, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 60h:18m:59s remains)
2017-12-15 12:34:37.497793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0422688 -5.0972753 -4.1119533 -3.2393742 -3.2254074 -3.1510916 -3.0652249 -3.1521316 -3.6405306 -3.3929403 -2.8194807 -3.8562918 -4.6134219 -5.6299171 -6.1526389][-5.121026 -5.0270858 -4.51588 -5.0718446 -5.0261526 -4.4562273 -3.9966223 -3.790894 -4.1722193 -4.7508864 -4.1622429 -4.7882991 -4.8096695 -4.7289786 -5.0075493][-3.9139659 -4.654027 -5.4827051 -5.35723 -5.0420184 -4.5972261 -3.8958397 -3.7884912 -4.1459613 -4.5658979 -4.5104847 -4.6509285 -4.5414815 -4.8872705 -4.460474][-3.8820167 -3.9716117 -4.0943346 -3.8507237 -3.87572 -3.3002245 -2.1873345 -2.2424505 -2.7723448 -2.7825894 -3.0716136 -4.7904119 -5.2259665 -5.7348642 -5.7198882][-4.4730616 -4.6196408 -3.9276216 -2.7697561 -2.5466321 -1.2965875 -0.20735073 -0.01993227 -0.3896265 -1.1925144 -1.7760651 -3.4821804 -4.2192183 -5.340857 -5.6335759][-5.6283388 -4.6874423 -2.9299533 -1.7564304 -1.0552926 0.3240428 1.0208268 1.1532564 0.90420008 0.18366814 -0.17175531 -2.0908794 -3.1996686 -3.8510017 -4.3759141][-6.9693823 -5.98901 -3.889128 -1.7041159 0.0024833679 1.7297468 1.99544 1.6760287 1.5127368 1.0550866 0.81507874 -0.77236986 -1.5351181 -2.570173 -3.079524][-6.1617017 -5.6715755 -3.9216819 -1.2551479 0.49229813 2.0823011 2.4741592 2.2057338 2.0565219 2.0605502 1.9636245 0.18681669 -1.277277 -2.7067223 -3.4193938][-5.0496144 -4.2749147 -2.71905 -0.33753777 1.1538939 2.0074105 1.7975688 2.0343747 2.4302731 2.5374479 2.9102287 0.53075552 -1.4548597 -2.9811261 -3.936862][-4.0107679 -3.7012215 -2.6766016 -1.2004929 -0.67867136 0.59660912 0.781271 1.0083823 1.466517 2.1505375 2.848134 0.25352192 -1.888854 -3.7938197 -4.7656364][-7.415297 -6.38583 -4.9230633 -3.738234 -2.7432346 -1.9111464 -2.112349 -1.4225936 -0.5561142 0.18913221 0.41516781 -2.2783287 -4.5108247 -5.7765903 -6.33619][-11.235699 -9.9476871 -7.8801613 -6.0965343 -5.1092458 -4.2145538 -3.7498631 -3.6711168 -3.7333517 -3.1085167 -2.8306069 -4.1483049 -5.5497408 -6.0091691 -5.7936373][-11.942015 -10.403814 -8.7165241 -6.758172 -5.9558525 -5.3966689 -4.47593 -4.1571674 -4.0354271 -3.9937544 -4.4231362 -5.2185879 -6.0783634 -5.78918 -5.1916685][-9.33454 -8.3389473 -6.7319584 -5.6001267 -5.1963682 -5.0013752 -4.7173653 -3.9338696 -3.3851087 -3.386235 -3.8485169 -3.8622704 -3.9945002 -4.0513411 -3.8881981][-6.2127457 -6.3270712 -5.3447638 -4.1924858 -3.1052756 -2.8591383 -2.7640188 -3.1961477 -3.4694104 -2.5855339 -2.1646261 -3.2164216 -4.25397 -4.7861214 -5.2932343]]...]
INFO - root - 2017-12-15 12:34:44.076371: step 12410, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 58h:32m:17s remains)
INFO - root - 2017-12-15 12:34:50.701495: step 12420, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 59h:30m:50s remains)
INFO - root - 2017-12-15 12:34:57.274069: step 12430, loss = 0.26, batch loss = 0.21 (12.2 examples/sec; 0.654 sec/batch; 58h:08m:04s remains)
INFO - root - 2017-12-15 12:35:03.959225: step 12440, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 59h:30m:35s remains)
INFO - root - 2017-12-15 12:35:10.616492: step 12450, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 60h:03m:26s remains)
INFO - root - 2017-12-15 12:35:17.216779: step 12460, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 59h:47m:07s remains)
INFO - root - 2017-12-15 12:35:23.752437: step 12470, loss = 0.21, batch loss = 0.16 (11.8 examples/sec; 0.675 sec/batch; 60h:02m:34s remains)
INFO - root - 2017-12-15 12:35:30.417456: step 12480, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 58h:10m:46s remains)
INFO - root - 2017-12-15 12:35:37.019405: step 12490, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 57h:49m:09s remains)
INFO - root - 2017-12-15 12:35:43.697957: step 12500, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 58h:13m:02s remains)
2017-12-15 12:35:44.217655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9784026 -4.4086018 -4.4185061 -3.9359961 -4.3219333 -4.6240678 -4.9058533 -4.6005368 -4.2408466 -4.0540037 -3.4322877 -6.0233092 -7.7954607 -9.4025888 -9.0913124][-5.4725733 -5.4428825 -5.3661122 -4.6865644 -4.83737 -5.2694297 -5.6549182 -5.8965154 -5.7955065 -5.0872879 -4.1538811 -6.4061322 -7.5916262 -9.9754982 -10.427573][-4.7933383 -5.5286889 -6.1432438 -5.3834419 -5.8537092 -6.1286621 -6.5334158 -6.4841466 -6.3262353 -6.1552715 -5.5277534 -7.5151205 -9.0275 -10.872369 -11.201806][-5.2798877 -6.2850389 -6.1438956 -5.1751752 -5.7035913 -5.3383789 -5.4245658 -5.7610331 -5.6602216 -5.3231812 -4.6279297 -7.3166828 -9.3838339 -11.671074 -11.795122][-6.4201689 -7.4078145 -7.7012262 -5.622086 -5.3586721 -4.268074 -3.9728487 -4.4195251 -4.4939508 -3.931222 -3.3305709 -6.4167509 -8.6259174 -11.283783 -12.311512][-7.5381455 -7.868741 -7.134016 -4.6635566 -2.4719656 -0.61188173 0.13677454 -0.80206013 -1.5808272 -1.7596359 -1.7484589 -4.4074097 -6.6612549 -9.563941 -10.757936][-8.8193026 -8.1371784 -6.4459047 -3.3959987 -0.67946148 1.1684518 2.4429035 2.7598963 2.2731051 0.30507755 -1.0482922 -3.8649738 -5.9544506 -8.6022739 -9.7740154][-8.3487148 -7.8387461 -5.534781 -1.8473301 0.56016874 3.1613455 4.582962 3.6347265 3.2072239 1.8629389 -0.014733315 -4.0442872 -6.6553235 -8.9508944 -9.5868244][-5.4700732 -5.3459816 -4.3064585 -1.6400251 -0.45872593 2.6285334 3.7565517 3.2878771 3.36063 1.7382431 -0.045767784 -4.3456321 -7.3740482 -9.76446 -9.8201923][-4.1084127 -2.9887624 -2.8035066 -1.372097 -0.6636796 0.27002478 0.53490734 1.4787698 1.3096414 -0.20799017 -1.2603798 -5.4327626 -8.4883995 -11.018171 -11.554745][-5.1676369 -4.3600516 -3.7660284 -3.0271022 -3.1646914 -2.9900203 -3.3699696 -3.1197958 -3.5124938 -3.8527374 -4.5818644 -8.5769749 -11.173781 -12.655277 -12.285826][-8.8233042 -7.1329031 -6.0852156 -5.9474063 -5.9197617 -6.2897749 -7.1599669 -7.9052982 -8.2177868 -8.10138 -8.4717445 -10.142265 -11.262527 -12.528953 -11.888433][-11.081921 -9.9165115 -8.8269711 -8.4202547 -9.1309509 -8.7326241 -8.6884546 -9.400507 -10.378685 -10.573796 -10.417937 -10.764128 -11.214846 -11.353228 -9.8141193][-11.419569 -10.925684 -10.066374 -9.2784824 -8.9085827 -8.7025976 -9.09751 -9.3596869 -9.5274105 -9.8816881 -10.001299 -8.8672981 -8.2107067 -9.2627945 -8.960165][-9.212924 -9.21031 -8.9360876 -7.5662851 -7.0236273 -6.7586756 -7.1440792 -7.4086881 -7.8443255 -7.9287496 -7.8053074 -8.4697313 -9.3161011 -8.5443945 -8.3573141]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 12:35:51.855780: step 12510, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 58h:37m:59s remains)
INFO - root - 2017-12-15 12:35:58.437766: step 12520, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.675 sec/batch; 59h:57m:53s remains)
INFO - root - 2017-12-15 12:36:05.048138: step 12530, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 59h:33m:08s remains)
INFO - root - 2017-12-15 12:36:11.617557: step 12540, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 56h:53m:25s remains)
INFO - root - 2017-12-15 12:36:18.073170: step 12550, loss = 0.12, batch loss = 0.07 (12.7 examples/sec; 0.632 sec/batch; 56h:09m:09s remains)
INFO - root - 2017-12-15 12:36:24.624157: step 12560, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 59h:21m:41s remains)
INFO - root - 2017-12-15 12:36:31.092433: step 12570, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 59h:02m:35s remains)
INFO - root - 2017-12-15 12:36:37.807017: step 12580, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 58h:56m:48s remains)
INFO - root - 2017-12-15 12:36:44.488722: step 12590, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 59h:23m:16s remains)
INFO - root - 2017-12-15 12:36:51.121640: step 12600, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 58h:37m:34s remains)
2017-12-15 12:36:51.656637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1205263 -5.9388485 -4.8165665 -3.6146967 -3.8347323 -3.440562 -2.3134305 -0.94395924 -0.32878733 0.51377535 -0.44784927 -4.1153913 -8.3764124 -9.5423956 -8.103158][-5.9933319 -6.9806581 -8.0418758 -7.4009204 -5.5379729 -4.051908 -2.6766963 -0.93269968 -0.69602203 -1.1412406 -1.4844913 -4.7166023 -8.8758507 -9.5391445 -9.32668][-4.5480728 -5.645864 -7.4940076 -7.2736225 -7.01846 -5.4247427 -2.3981962 -1.2372413 -1.1567016 -2.0312254 -3.3960035 -6.7565842 -10.165319 -11.0805 -9.6534214][-3.1841474 -4.5474572 -5.767621 -5.5940514 -6.2118173 -5.3093691 -3.9387951 -2.4542551 -1.5979013 -2.4873798 -3.986465 -8.6651964 -12.556239 -11.656292 -9.6981621][-4.5396872 -4.3469734 -4.9162521 -4.3349361 -4.2837195 -2.6348982 -1.1157603 -1.9279919 -2.6172497 -3.106909 -4.2956352 -8.1402006 -12.178896 -12.08267 -10.13141][-7.5490313 -5.8401542 -5.3441968 -3.8123174 -3.2402115 -0.67771387 1.3121104 0.87823772 -0.26162672 -2.6529431 -4.6659007 -8.0228767 -11.536722 -12.495957 -10.371822][-7.7511811 -6.8571491 -6.5064864 -4.5926929 -3.3942831 -0.22616053 2.8679004 3.9851089 3.2445531 -0.44911718 -4.2143669 -7.0386343 -9.7595119 -10.246876 -9.9392223][-6.5056772 -5.8455758 -5.9613285 -3.3071361 -1.5516787 1.4728894 4.2614837 5.3054786 5.0317607 2.3022923 -1.1036263 -6.448082 -10.902538 -10.39483 -8.9612551][-5.5198779 -4.2017832 -4.3270931 -2.1767962 -1.3916106 1.5958076 4.0449243 5.4353576 5.247664 1.7758365 -1.1615839 -6.123961 -11.127993 -12.180389 -10.512908][-4.9038649 -4.1686893 -3.9579992 -2.3940136 -1.9130301 -0.22530317 0.90304089 2.7584815 3.1021628 1.668035 -0.622252 -6.3095522 -11.179482 -12.52054 -11.295938][-5.7394423 -6.8009272 -6.9072213 -6.0629354 -4.9351735 -3.768904 -2.5034349 -1.3578296 -1.0512915 -1.2709641 -2.3875842 -7.4673052 -11.485656 -13.077261 -12.071884][-10.794341 -9.2855606 -8.4546814 -8.0005627 -7.9006481 -7.4639273 -7.3232884 -7.7115788 -7.6114559 -6.8762741 -7.2843671 -9.4609194 -10.901775 -11.391214 -10.440064][-13.377673 -11.660513 -9.149931 -8.6930761 -8.8521013 -9.20298 -9.5352058 -8.7785158 -8.4840422 -8.0571795 -8.3021507 -9.9041348 -12.106569 -11.220755 -9.1860123][-12.644999 -10.827559 -8.8593884 -8.3378048 -7.9644561 -7.0771551 -7.3617325 -7.8607817 -7.0139728 -7.7567935 -8.10933 -6.8040214 -7.5115175 -9.0609646 -9.4934416][-8.4859848 -6.8329835 -6.11391 -5.3803225 -5.6382575 -6.3001809 -6.5027366 -5.6389241 -5.4207053 -6.0982504 -6.5558662 -8.7634182 -9.7915812 -8.80921 -9.5508461]]...]
INFO - root - 2017-12-15 12:36:58.187760: step 12610, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 58h:32m:06s remains)
INFO - root - 2017-12-15 12:37:04.769716: step 12620, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 59h:22m:18s remains)
INFO - root - 2017-12-15 12:37:11.411424: step 12630, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 58h:37m:57s remains)
INFO - root - 2017-12-15 12:37:18.015648: step 12640, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 56h:59m:53s remains)
INFO - root - 2017-12-15 12:37:24.668741: step 12650, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 58h:32m:11s remains)
INFO - root - 2017-12-15 12:37:31.283397: step 12660, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 60h:01m:48s remains)
INFO - root - 2017-12-15 12:37:37.891830: step 12670, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 58h:02m:31s remains)
INFO - root - 2017-12-15 12:37:44.554156: step 12680, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 60h:18m:14s remains)
INFO - root - 2017-12-15 12:37:51.123952: step 12690, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 59h:56m:00s remains)
INFO - root - 2017-12-15 12:37:57.868527: step 12700, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.676 sec/batch; 60h:03m:50s remains)
2017-12-15 12:37:58.378335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4664106 -1.9811509 -1.7768211 -0.64418983 -1.3766499 -1.4729047 -1.9321167 -2.4821064 -1.9309132 -0.7046237 0.69975328 -1.8676858 -3.1697156 -6.1342783 -8.1496515][-0.87671185 -1.0725727 -0.45174408 0.070216179 -0.4379077 -1.1505842 -2.1028888 -2.8061302 -2.6470742 -2.17699 -1.2996125 -2.3653054 -3.9573712 -6.3385539 -7.0673795][-0.3876524 -1.6007633 -2.3020823 -2.344161 -3.3245265 -3.4971559 -3.4239302 -3.4198005 -2.4192941 -2.0455847 -2.1295319 -4.9819136 -6.9889336 -8.6396179 -10.048601][-2.3009682 -2.56491 -2.826452 -3.5732141 -4.0924296 -3.6552689 -3.9936404 -3.0685675 -2.2034059 -2.2227938 -2.1632771 -5.2306838 -7.9276295 -9.9750843 -11.129232][-2.8133771 -4.0438228 -4.6011763 -4.3943605 -3.8503606 -2.9029193 -2.7046471 -2.2314904 -1.6203408 -1.4098263 -2.2226388 -5.7550893 -8.7519846 -11.40135 -12.849869][-2.4077854 -2.8058782 -3.0016737 -3.0563319 -1.8610556 0.27618122 0.9250679 1.3465133 0.72288036 -0.7577095 -2.092948 -5.1474957 -8.1580257 -10.614374 -11.979903][-3.019068 -3.1496506 -3.0895491 -1.8541007 -1.2099953 0.93890095 2.8966336 3.4953656 3.67662 1.5392275 -1.4812083 -5.1549306 -7.7482333 -9.9459438 -11.23411][-3.6945488 -2.7319391 -1.2812386 -0.30584049 0.32443428 2.4986548 3.6546607 3.3298116 3.5847239 1.6672935 -0.82961226 -5.28236 -8.8210115 -10.615423 -10.585922][-3.1389117 -2.95291 -2.4319119 -1.2480893 -0.30620193 1.7082338 2.4260106 2.3641019 2.5444713 0.62555027 -0.82138586 -4.5591288 -8.332819 -9.97042 -10.001499][-4.3817964 -3.8245556 -3.0588 -1.818784 -1.0011063 -0.37678576 0.084310055 0.28075552 -0.094750404 -0.86123133 -0.83206272 -4.5287228 -7.2091308 -8.43931 -9.2010479][-8.33917 -7.8041573 -6.10894 -4.6510963 -4.0491419 -3.7902408 -3.7271204 -4.15364 -4.5028405 -3.8951681 -3.4636135 -5.979095 -6.4730558 -7.242939 -7.163918][-12.019279 -10.903612 -8.8939447 -7.72548 -7.5296154 -7.2930527 -7.2485743 -7.551568 -7.400703 -6.801425 -5.9903574 -6.787672 -6.5897746 -6.0826011 -5.4633818][-10.936033 -10.531719 -9.6897383 -9.5261631 -9.5919151 -9.1576023 -8.6546135 -8.7677193 -8.9885492 -8.5725956 -7.7749958 -8.0386038 -6.8799143 -5.4095716 -4.4014645][-8.4896584 -8.6202154 -8.5646734 -8.2513361 -8.6250286 -8.5580692 -7.9109764 -7.64059 -7.8731995 -8.1277227 -8.4036226 -8.7949629 -7.999052 -7.3727794 -6.9037323][-7.4268751 -7.2117567 -7.1473022 -6.4494357 -6.4495354 -5.6758356 -5.7880592 -6.2787542 -6.1478734 -6.7648706 -8.2392435 -9.6593008 -10.353174 -10.709068 -10.019762]]...]
INFO - root - 2017-12-15 12:38:05.045173: step 12710, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.687 sec/batch; 61h:01m:08s remains)
INFO - root - 2017-12-15 12:38:11.668847: step 12720, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 59h:19m:52s remains)
INFO - root - 2017-12-15 12:38:18.268648: step 12730, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.684 sec/batch; 60h:44m:23s remains)
INFO - root - 2017-12-15 12:38:24.917849: step 12740, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 58h:07m:05s remains)
INFO - root - 2017-12-15 12:38:31.552961: step 12750, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 58h:49m:45s remains)
INFO - root - 2017-12-15 12:38:38.144288: step 12760, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 58h:04m:12s remains)
INFO - root - 2017-12-15 12:38:44.757844: step 12770, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 58h:08m:32s remains)
INFO - root - 2017-12-15 12:38:51.330069: step 12780, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 59h:38m:38s remains)
INFO - root - 2017-12-15 12:38:57.879299: step 12790, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 56h:16m:03s remains)
INFO - root - 2017-12-15 12:39:04.566460: step 12800, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 58h:45m:57s remains)
2017-12-15 12:39:05.134321: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22369671 0.11914062 0.5663147 1.2902513 1.1090198 -0.26028252 -2.9752507 -4.94115 -5.209343 -5.6570964 -5.9573231 -8.7665529 -11.465097 -12.365341 -11.006235][-0.0096344948 2.14142 4.6654925 4.9466567 3.4588161 1.0035672 -1.7442849 -3.5940707 -5.9951849 -7.4003897 -8.2732267 -12.10941 -14.728653 -15.384529 -14.759882][0.71778631 1.0794787 1.7268972 3.6705861 4.1591573 2.3727183 0.50178051 -1.8437572 -4.4338956 -6.5193863 -8.60346 -11.475679 -14.044458 -16.178963 -16.152037][-0.7322216 0.075211525 2.0303636 3.8129811 2.9112344 1.5788345 0.36612225 -1.1634989 -3.1410096 -4.6977983 -6.0714045 -9.5237389 -12.806059 -13.908026 -13.525883][-0.065461636 -0.48184443 0.20064211 1.7776871 1.9506145 2.5829406 2.5521159 0.49571609 -1.0193596 -2.2669258 -3.8410387 -7.6784406 -11.212864 -13.275089 -12.904497][-2.1288791 -1.7052083 -1.3897066 1.4674582 3.2598042 4.6962714 5.0152497 4.1969123 3.2531924 1.5364771 -0.59632969 -3.5981307 -6.8194036 -9.052228 -9.0941658][-2.349308 -2.4214306 -0.98480844 1.6263652 2.9860168 5.0195251 6.817472 7.0440288 6.9294295 4.4235787 1.686965 -1.7717459 -5.9568396 -7.8973637 -8.466856][-5.8646922 -4.42234 -2.2017946 1.2136965 3.3186879 5.2581124 6.6903605 6.4479365 6.8389478 5.7044721 3.4616418 -1.0191145 -6.1224995 -8.4287262 -9.2319727][-6.6142454 -4.8318229 -2.6400211 0.01042366 1.9708896 4.4670615 5.6792731 4.7375941 4.9055982 4.2079177 2.90688 -1.18784 -5.9817748 -8.6239643 -8.830368][-7.3872981 -5.98089 -3.7919292 -1.109427 1.099669 2.6471229 3.2846618 3.318542 3.4735913 2.2329702 0.55496025 -3.1576281 -6.9794517 -9.0942106 -9.2083626][-11.900555 -10.76786 -8.8534994 -5.4816689 -3.8430696 -2.6592584 -1.3373876 -0.89306784 -0.84727335 -1.3539586 -1.9010952 -5.5945 -8.1219654 -9.5927629 -8.4695854][-14.761196 -14.546135 -12.792843 -9.96581 -8.9418221 -7.6512394 -7.5629568 -7.2214947 -6.5811157 -6.2929335 -5.9564371 -7.4074545 -8.1789455 -9.3196735 -8.458272][-15.344465 -15.596561 -14.585352 -12.086182 -10.902874 -9.4277134 -9.7425919 -10.628084 -10.997393 -10.314848 -9.086791 -8.909626 -9.1057215 -9.3197727 -7.54][-11.374624 -11.48168 -11.762369 -10.066998 -9.41112 -8.6467638 -9.0708427 -8.7379084 -8.3188524 -8.7773075 -8.6911459 -7.45461 -6.8280811 -7.2795596 -7.1070395][-8.8845387 -8.2038927 -7.2125411 -6.1864982 -7.21572 -5.9582057 -5.1832628 -5.341692 -5.9943719 -6.1545038 -6.1738443 -6.8005633 -7.5625305 -7.1127715 -7.528656]]...]
INFO - root - 2017-12-15 12:39:11.761128: step 12810, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 59h:13m:51s remains)
INFO - root - 2017-12-15 12:39:18.362223: step 12820, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 56h:56m:14s remains)
INFO - root - 2017-12-15 12:39:25.029933: step 12830, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 59h:03m:13s remains)
INFO - root - 2017-12-15 12:39:31.653475: step 12840, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 59h:12m:09s remains)
INFO - root - 2017-12-15 12:39:38.260283: step 12850, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 60h:21m:24s remains)
INFO - root - 2017-12-15 12:39:44.897083: step 12860, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 60h:50m:14s remains)
INFO - root - 2017-12-15 12:39:51.509281: step 12870, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.681 sec/batch; 60h:25m:24s remains)
INFO - root - 2017-12-15 12:39:58.162735: step 12880, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.685 sec/batch; 60h:48m:14s remains)
INFO - root - 2017-12-15 12:40:04.671441: step 12890, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 56h:40m:48s remains)
INFO - root - 2017-12-15 12:40:11.240842: step 12900, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 57h:43m:47s remains)
2017-12-15 12:40:11.752123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4609761 -3.0312972 -2.8671763 -2.3803058 -3.6641343 -4.4451876 -4.6222706 -4.7693305 -4.6636953 -3.8112819 -2.461509 -1.5651226 -3.3660934 -3.1679468 -1.8213737][-2.8911533 -2.6772027 -3.057303 -2.9992628 -2.9202871 -3.4299362 -3.5389349 -2.6850576 -2.1279595 -1.372231 -0.27569294 0.43644381 -1.156281 -2.0604844 -2.3940456][-1.9137838 -2.5046256 -3.0280035 -3.0123837 -3.5471463 -3.4341488 -2.9289594 -2.5830176 -2.4871681 -2.0804594 -0.57121611 -0.26563692 -1.3094616 -1.5438571 -1.9379015][-3.5319474 -3.4245391 -4.6427269 -4.3648682 -4.3267875 -3.4224708 -2.447295 -2.5149615 -2.5821836 -2.3857059 -0.97942448 -0.97274876 -2.3208735 -2.7594559 -2.8416369][-4.5228615 -5.3081489 -6.58613 -6.1177034 -4.7565446 -1.7418196 0.10669565 -0.75428295 -1.3408952 -1.1200495 -0.99642611 -0.85128832 -1.4850788 -2.453423 -3.4022026][-4.2828379 -4.600359 -5.3725848 -4.4567871 -2.7277989 -0.7342453 2.452424 3.0717478 1.109488 0.13550615 -0.41485882 -0.61186981 -3.0631065 -3.8337502 -3.654938][-5.3047523 -5.0107832 -4.9520054 -2.7250528 0.44590712 3.4403944 6.585165 5.7925434 3.2464447 0.58352375 -0.84425926 -0.96378756 -3.1481178 -4.5856485 -5.179472][-5.4667535 -5.5193295 -5.3955431 -2.9428015 0.21700621 3.5457401 6.5857906 5.8470268 3.6264362 1.2062483 -0.91754961 -2.0646112 -3.6428356 -4.2296357 -4.3233542][-2.8945627 -2.6320283 -2.9083459 -2.0670755 -1.0076547 1.7580504 4.4280558 4.0054836 1.866642 -0.59143686 -3.3418899 -4.290391 -5.3836074 -5.6698651 -5.5592771][-4.2420769 -3.8246245 -3.7124424 -1.4417529 -0.78014517 1.1628509 3.404809 3.8120413 3.236269 0.06867981 -3.3597887 -5.2660217 -7.6769037 -7.6059108 -6.8505716][-7.2485781 -7.2138968 -6.77789 -5.4215136 -5.75216 -4.5038056 -2.5815783 -0.94099855 -0.40332937 -2.60715 -4.4649835 -6.5989556 -8.8788557 -8.7531242 -8.0170355][-9.5527029 -8.5445023 -8.8692513 -7.633831 -6.8457303 -5.7633615 -4.5476155 -3.7393413 -3.7804611 -4.4774694 -5.7914257 -7.6725826 -9.09702 -9.05846 -8.0412292][-10.308565 -9.25608 -8.2473087 -7.559299 -7.5443549 -6.1866503 -5.2186465 -5.7673244 -6.2617726 -6.8419476 -7.9845772 -8.7790747 -8.81602 -8.619504 -7.4968653][-7.5338621 -7.2013884 -6.8760562 -7.055594 -7.0239553 -6.3703752 -6.1693063 -4.9835086 -4.8157477 -5.8490953 -6.7414017 -6.0703192 -6.2757044 -6.4291234 -6.327981][-6.9571695 -7.426229 -5.6850529 -4.4625812 -4.161819 -5.0224667 -5.9190607 -5.0191469 -4.787621 -5.0579758 -5.5964875 -5.3448558 -6.0173216 -5.7682638 -5.8156919]]...]
INFO - root - 2017-12-15 12:40:18.248698: step 12910, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 58h:57m:18s remains)
INFO - root - 2017-12-15 12:40:24.867249: step 12920, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 60h:22m:20s remains)
INFO - root - 2017-12-15 12:40:31.399771: step 12930, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 59h:51m:19s remains)
INFO - root - 2017-12-15 12:40:37.902041: step 12940, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 59h:34m:14s remains)
INFO - root - 2017-12-15 12:40:44.468993: step 12950, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 57h:57m:03s remains)
INFO - root - 2017-12-15 12:40:51.128718: step 12960, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 57h:41m:17s remains)
INFO - root - 2017-12-15 12:40:57.774107: step 12970, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.653 sec/batch; 57h:58m:03s remains)
INFO - root - 2017-12-15 12:41:04.518491: step 12980, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 60h:29m:17s remains)
INFO - root - 2017-12-15 12:41:11.088952: step 12990, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 58h:55m:48s remains)
INFO - root - 2017-12-15 12:41:17.582874: step 13000, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 59h:06m:33s remains)
2017-12-15 12:41:18.078940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.81346 -6.3366494 -5.1731248 -4.4510808 -4.3278332 -4.6199203 -4.9070125 -4.7647491 -4.55837 -4.1623259 -3.7731514 -6.5697651 -8.6271467 -10.172897 -10.687559][-6.1276994 -6.0926132 -4.56966 -4.4047832 -5.0755763 -5.5241919 -6.5389366 -6.61569 -6.5709553 -6.4752755 -6.7524557 -8.8198614 -10.737413 -11.730815 -11.124138][-4.08804 -5.0114903 -5.1862593 -4.3623872 -4.99832 -6.173727 -6.5690784 -7.3363767 -7.6400013 -7.2475915 -7.5315533 -10.102755 -11.947037 -12.694116 -12.043089][-2.7826519 -4.2366772 -4.0577335 -3.60811 -4.4244561 -4.6599059 -5.0023108 -5.9753981 -6.1162515 -6.296237 -6.7905817 -8.9950514 -11.561114 -13.479731 -13.406678][-3.3174582 -4.2046161 -3.6152508 -2.5378537 -2.6558256 -2.2008171 -1.9015102 -2.596868 -3.214298 -3.5243261 -3.9265668 -7.2480049 -10.186738 -11.909676 -12.809307][-6.2116027 -6.0552368 -4.6180596 -2.5817981 -1.2634788 0.16573811 1.1490693 1.2478137 1.1203227 0.10618639 -1.0450301 -3.740109 -6.3476982 -8.8414364 -10.473221][-7.8740768 -6.6146307 -4.5481958 -1.693541 0.32087517 1.5225601 2.7175064 3.612319 3.9672689 2.5893793 0.99624109 -2.0265622 -4.4488316 -6.2899675 -7.9623628][-6.316412 -5.7412047 -3.8515546 -1.3382454 0.39604473 2.2425351 2.9239368 3.2450256 3.9715261 3.5853949 2.4550438 -1.0177989 -3.7241268 -5.606173 -6.5730567][-5.1903477 -4.2499757 -2.4800315 0.0028457642 1.1311808 2.0244961 2.6171246 2.9070835 2.9745293 2.9507122 2.7804708 -1.5794315 -3.8426127 -5.2840219 -6.73553][-3.2729177 -3.5314119 -1.9578497 -0.19530869 0.13823891 0.32246017 0.91736221 2.1435843 2.3468313 2.5917902 3.0515575 -0.89211321 -3.8327212 -5.8729625 -6.7906032][-6.7355256 -5.8793597 -4.2535276 -2.250509 -1.2363629 -0.5045495 0.29133511 1.3154335 2.0471749 1.5579991 0.28458786 -2.4413397 -3.6031659 -6.0774255 -6.2679682][-10.97885 -9.6691866 -7.2965302 -5.1566992 -4.0920248 -3.1638453 -2.1936226 -0.84503269 0.17954397 0.52597761 0.46343565 -2.5298223 -4.01314 -4.9949241 -5.2826805][-13.40033 -11.85944 -9.3467207 -7.4981914 -6.0321493 -5.3840494 -4.508852 -3.5965841 -3.022752 -3.4567437 -4.1059742 -5.1402955 -5.259922 -5.1538486 -3.5889463][-10.706142 -10.660294 -8.4936638 -7.1769381 -6.8386431 -7.2643137 -6.4738617 -5.5760036 -5.5210323 -5.161284 -5.2384615 -5.7063537 -5.4441628 -5.9206862 -5.4174304][-8.0105457 -7.6680174 -8.0225544 -7.4317923 -6.6674776 -7.0936527 -7.19858 -6.8440046 -6.4361496 -7.2547684 -8.273571 -9.1376686 -9.0928717 -8.1515121 -6.35496]]...]
INFO - root - 2017-12-15 12:41:24.584048: step 13010, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.637 sec/batch; 56h:30m:32s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 12:41:31.166998: step 13020, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 56h:42m:57s remains)
INFO - root - 2017-12-15 12:41:37.725419: step 13030, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 59h:08m:08s remains)
INFO - root - 2017-12-15 12:41:44.236934: step 13040, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 58h:14m:23s remains)
INFO - root - 2017-12-15 12:41:50.848817: step 13050, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 58h:26m:15s remains)
INFO - root - 2017-12-15 12:41:57.383276: step 13060, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.635 sec/batch; 56h:19m:54s remains)
INFO - root - 2017-12-15 12:42:03.940791: step 13070, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.688 sec/batch; 61h:01m:27s remains)
INFO - root - 2017-12-15 12:42:10.520233: step 13080, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 58h:08m:51s remains)
INFO - root - 2017-12-15 12:42:17.085189: step 13090, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 58h:56m:11s remains)
INFO - root - 2017-12-15 12:42:23.622217: step 13100, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.654 sec/batch; 57h:58m:57s remains)
2017-12-15 12:42:24.099098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017571449 -1.8024933 -4.1742058 -6.7233415 -8.7432232 -9.07375 -7.9245358 -6.3346028 -5.0825853 -3.1782901 -2.1613176 -4.572382 -5.5189238 -6.1197376 -5.8404274][-1.2458901 -2.8983831 -4.9406686 -8.5140619 -11.246008 -11.522376 -10.469111 -9.5114021 -8.7048235 -6.6545391 -5.0703955 -6.2810478 -7.1252804 -6.6205726 -6.3478994][-0.62976122 -2.0249543 -4.3996377 -7.0617919 -8.4297581 -9.2259674 -8.9049616 -8.5905361 -8.84849 -8.1172009 -7.377387 -8.6485195 -9.1990395 -9.06372 -9.5485191][-3.0329413 -2.8605785 -3.7217135 -5.2960033 -5.9231882 -5.8281026 -5.21287 -6.077353 -7.4619832 -7.8810043 -7.8673711 -10.477193 -12.137818 -12.048174 -11.961074][-5.3412647 -5.2048922 -5.6136689 -5.4657273 -4.0464697 -1.9943082 -0.46228266 -2.4997218 -5.0641289 -6.3623519 -7.1152654 -9.9344034 -11.595514 -12.83235 -13.292646][-7.6712518 -7.8411503 -7.4275608 -5.4186678 -2.6575363 1.1693501 4.2937465 2.8342271 0.54113626 -2.3802993 -4.8915772 -7.9735126 -9.8494949 -11.056381 -12.018696][-9.7893257 -10.658973 -10.059189 -6.5656705 -2.7850964 2.158412 7.3243527 7.6324191 5.861815 1.2256265 -2.8803663 -6.1932206 -8.5118494 -9.627286 -10.65001][-9.9154091 -10.94955 -10.479609 -6.1921029 -2.2825212 4.1611314 9.5220165 8.9159889 8.3690186 4.1586785 -0.819149 -5.8111572 -9.3722868 -10.072536 -10.660084][-8.5768538 -8.8619127 -8.40661 -5.625133 -3.0257461 2.8253093 6.4342723 6.7970052 6.9821963 3.2716956 -0.0859499 -5.4831972 -10.08461 -11.550066 -12.390703][-6.4017363 -7.074019 -7.2169881 -4.8005495 -2.87311 0.32773066 2.9820142 4.1432605 3.5507493 0.81154108 -1.6803794 -6.4639916 -10.142242 -11.909825 -13.889704][-9.3687344 -9.9936638 -10.222221 -7.761229 -6.7226944 -5.2571239 -3.6434264 -2.6873753 -2.4479222 -3.6757176 -5.9862194 -10.476772 -12.127361 -13.054714 -14.058407][-14.050306 -13.800917 -12.965027 -11.519987 -11.279046 -9.8950453 -9.5557337 -9.564249 -9.070693 -9.40948 -10.40233 -12.173903 -12.129927 -12.723005 -12.656156][-13.838291 -12.93223 -12.138048 -11.943093 -12.479155 -11.689491 -11.581081 -11.33917 -11.077842 -11.128351 -11.008145 -11.390833 -11.82099 -10.812191 -8.7208242][-11.096995 -10.277559 -10.294533 -9.7107086 -9.7699757 -10.571681 -11.051327 -9.9089327 -9.4731932 -9.6833591 -9.9894571 -9.2106552 -8.9377041 -7.5297761 -6.446187][-8.4374714 -7.6947231 -7.5561738 -6.7651534 -6.4545588 -6.2930274 -6.2095881 -6.0783968 -5.8362942 -5.7038851 -6.2935452 -6.9541759 -8.0272417 -7.6491537 -7.5521193]]...]
INFO - root - 2017-12-15 12:42:30.653231: step 13110, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 56h:58m:27s remains)
INFO - root - 2017-12-15 12:42:37.160113: step 13120, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 57h:16m:51s remains)
INFO - root - 2017-12-15 12:42:43.735793: step 13130, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 59h:36m:06s remains)
INFO - root - 2017-12-15 12:42:50.268014: step 13140, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 57h:22m:25s remains)
INFO - root - 2017-12-15 12:42:56.947406: step 13150, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 57h:10m:19s remains)
INFO - root - 2017-12-15 12:43:03.496270: step 13160, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 58h:35m:19s remains)
INFO - root - 2017-12-15 12:43:10.114565: step 13170, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 58h:34m:30s remains)
INFO - root - 2017-12-15 12:43:16.732199: step 13180, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 57h:59m:05s remains)
INFO - root - 2017-12-15 12:43:23.279295: step 13190, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 59h:20m:42s remains)
INFO - root - 2017-12-15 12:43:29.821468: step 13200, loss = 0.17, batch loss = 0.12 (12.8 examples/sec; 0.625 sec/batch; 55h:28m:17s remains)
2017-12-15 12:43:30.297043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4291143 -6.7481804 -7.3219576 -6.709991 -7.2564945 -8.0568466 -8.6669226 -8.5745239 -7.8731833 -6.9277935 -5.9365869 -7.485908 -8.808363 -8.79785 -8.4177008][-3.1048639 -4.0275908 -5.1308117 -4.9470854 -5.7627087 -6.614099 -7.3813372 -8.1133919 -8.1591759 -7.3040156 -6.1460114 -8.2417517 -9.6131706 -10.558411 -9.4994707][-2.025672 -1.8994424 -2.7046914 -2.6933918 -3.8525505 -4.3576636 -5.1344242 -5.5160561 -5.7842126 -5.5754161 -5.4400678 -7.5943828 -9.6386213 -9.7184381 -9.72377][-2.6152163 -3.5865412 -3.8301692 -3.5072658 -3.6130557 -3.3035967 -3.1724777 -3.1792502 -3.2333078 -3.4098873 -4.167057 -6.6242504 -9.0733719 -9.2124071 -8.8127337][-3.4367397 -4.5153093 -5.1872716 -4.0316095 -3.470217 -2.4054019 -1.8434424 -1.2864714 -1.0789208 -1.5485039 -1.9906518 -4.7924719 -7.6297708 -8.16029 -7.4548416][-4.3843479 -3.9917769 -4.3933344 -3.7272882 -1.5410285 0.063521385 1.13766 0.90752745 1.4136224 0.56320429 0.0613966 -2.1800892 -4.4243588 -5.5495853 -6.1048918][-4.8221378 -4.3143811 -3.4914846 -1.8624403 -0.014453888 1.9047441 3.4467683 3.788054 3.4975677 2.5453649 1.0044804 -1.6873002 -4.5565195 -5.9936023 -6.1709695][-4.5326929 -3.2199378 -2.7450643 -1.0844355 0.095812321 2.0365448 3.2973695 3.3316159 3.200377 1.9719257 0.72591591 -2.4710057 -5.821238 -6.7939177 -6.0405569][-5.237875 -3.2846332 -1.9351661 -0.82301188 -0.26748228 0.85367489 1.6227121 1.7560081 2.7173429 1.6853714 1.1562352 -3.4944952 -7.8792448 -9.9444752 -10.434319][-6.3812542 -5.1054692 -3.8632779 -2.2503512 -2.227277 -1.6334496 -1.245801 -0.58935404 -0.36400175 0.75974989 0.74845314 -3.1505437 -7.011713 -10.727451 -12.5737][-8.4917183 -8.0850849 -6.8507662 -4.7675819 -5.0830321 -4.819808 -4.1671906 -3.8198795 -3.3654108 -2.1749854 -2.3376851 -5.6911206 -9.1269836 -11.67731 -13.198127][-9.5959187 -10.162151 -10.091768 -9.0628719 -9.2072763 -8.8490925 -8.7048626 -8.0136881 -7.3127556 -6.8605552 -6.19038 -8.7872734 -10.551422 -12.428267 -13.727287][-12.068111 -11.570562 -11.361164 -12.117318 -12.335876 -11.754477 -11.557108 -10.235874 -9.7541046 -9.0650043 -9.0036869 -10.538789 -11.955781 -12.676094 -11.380606][-12.585798 -12.499534 -11.840395 -12.136793 -12.708689 -13.129111 -11.961043 -10.871598 -10.401331 -9.7954712 -9.4665918 -10.506264 -10.520346 -11.349249 -10.070681][-9.2912073 -9.9919357 -10.347462 -9.3515739 -8.4276142 -8.9507732 -9.1657219 -8.8743992 -9.3056393 -9.2884874 -10.225842 -10.65318 -10.020031 -10.553614 -10.872087]]...]
INFO - root - 2017-12-15 12:43:36.857781: step 13210, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 57h:58m:52s remains)
INFO - root - 2017-12-15 12:43:43.497660: step 13220, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 58h:50m:32s remains)
INFO - root - 2017-12-15 12:43:50.139771: step 13230, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 59h:40m:41s remains)
INFO - root - 2017-12-15 12:43:56.700629: step 13240, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 56h:54m:11s remains)
INFO - root - 2017-12-15 12:44:03.259249: step 13250, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 59h:09m:48s remains)
INFO - root - 2017-12-15 12:44:09.769771: step 13260, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.628 sec/batch; 55h:43m:20s remains)
INFO - root - 2017-12-15 12:44:16.340448: step 13270, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 58h:30m:40s remains)
INFO - root - 2017-12-15 12:44:22.958060: step 13280, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 59h:42m:14s remains)
INFO - root - 2017-12-15 12:44:29.547940: step 13290, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 58h:23m:33s remains)
INFO - root - 2017-12-15 12:44:36.083548: step 13300, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.666 sec/batch; 59h:01m:21s remains)
2017-12-15 12:44:36.559743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-12.055614 -13.221243 -13.502195 -13.958889 -14.4163 -14.050797 -12.8558 -10.818747 -9.1856689 -8.76754 -9.4334745 -10.781569 -12.873275 -12.891889 -10.891232][-11.457596 -10.867461 -10.579239 -11.443489 -12.133009 -13.112568 -12.925118 -12.060135 -11.368152 -10.268738 -10.418709 -11.518333 -12.679348 -13.130257 -11.961086][-5.5235462 -6.5786123 -7.9623566 -9.8248367 -10.763335 -10.934357 -10.375742 -9.7037125 -9.37959 -8.5101891 -7.5905962 -8.5563841 -10.45351 -11.940355 -11.958564][-3.4999523 -2.8858888 -3.2731707 -4.625659 -6.3286676 -6.591958 -6.1831837 -6.1354475 -6.7749476 -6.9015951 -7.2124562 -7.4418678 -8.2380819 -9.5416718 -9.6963482][-4.1628957 -4.39987 -4.7978878 -3.5329237 -3.2282653 -2.0119841 -0.83112764 -2.3357096 -3.9636815 -3.8621798 -4.7069106 -6.195879 -7.9792786 -9.1597729 -8.4940681][-6.31379 -6.3137536 -5.9083676 -4.1843224 -3.0811214 0.54542255 3.3050504 2.4728208 1.1393714 -0.65572405 -2.2862854 -3.5519028 -5.6873174 -6.921278 -6.8315592][-8.4500179 -7.7585726 -6.6767197 -3.7298098 -1.3207459 1.9094896 4.5659976 5.475739 6.305388 4.0965323 1.064106 -0.96069384 -4.0282655 -5.7867665 -5.3487535][-7.5085707 -6.74049 -5.4445968 -3.055347 0.030190945 3.1066422 5.3032718 5.6928649 5.9532781 4.1639915 1.7286148 -0.52587891 -2.9601984 -3.7152112 -2.9853177][-4.7725749 -3.5609403 -2.9972951 -1.3588457 0.34989691 2.0690093 3.8372788 4.8464961 4.8661332 3.3301187 1.3080277 -1.4304042 -3.7626367 -4.6024723 -3.3101382][-4.1435714 -2.9341035 -1.1604085 -0.76454115 -0.35756779 0.4905901 1.6437597 2.1372719 1.5155377 0.40262651 -0.62236118 -2.3687632 -4.0635657 -4.177763 -3.2621427][-6.3411131 -4.4168816 -3.5202391 -3.4049742 -3.1601429 -2.2984538 -0.58952379 0.50299454 -0.60133076 -2.3668523 -3.3752277 -4.8095484 -6.7448473 -6.3953242 -4.5087695][-9.4455 -7.3764892 -5.9437733 -5.957458 -6.1883097 -5.35183 -4.0332642 -3.2570796 -3.7054071 -4.42232 -4.3533688 -4.56302 -5.2601633 -5.5991983 -5.2707968][-9.7977495 -8.82533 -8.0402069 -7.5140777 -6.9128761 -5.3973403 -4.1584296 -3.6980634 -4.29286 -5.3124228 -5.4100089 -5.6135287 -5.6525931 -5.2398534 -4.1593776][-8.4310408 -7.8877649 -7.7133231 -7.1915865 -6.7773848 -5.3420496 -4.3786573 -3.7997355 -4.0116014 -4.167367 -4.5586519 -4.3505306 -3.6103461 -3.6751842 -3.9501915][-6.1617379 -5.1716552 -4.832242 -4.8084569 -3.8658128 -3.2720759 -2.6263332 -2.8105822 -3.9800997 -4.8088861 -5.2787681 -5.9425063 -6.7579627 -6.7030158 -5.9518447]]...]
INFO - root - 2017-12-15 12:44:43.083242: step 13310, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 58h:42m:43s remains)
INFO - root - 2017-12-15 12:44:49.701963: step 13320, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 60h:22m:02s remains)
INFO - root - 2017-12-15 12:44:56.389683: step 13330, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 59h:30m:02s remains)
INFO - root - 2017-12-15 12:45:02.989347: step 13340, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 57h:10m:11s remains)
INFO - root - 2017-12-15 12:45:09.563254: step 13350, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 57h:11m:32s remains)
INFO - root - 2017-12-15 12:45:16.110014: step 13360, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 58h:36m:01s remains)
INFO - root - 2017-12-15 12:45:22.696688: step 13370, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 57h:43m:33s remains)
INFO - root - 2017-12-15 12:45:29.182047: step 13380, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.643 sec/batch; 56h:58m:38s remains)
INFO - root - 2017-12-15 12:45:35.821237: step 13390, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 59h:21m:11s remains)
INFO - root - 2017-12-15 12:45:42.459994: step 13400, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.689 sec/batch; 61h:05m:08s remains)
2017-12-15 12:45:42.932320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5701485 -7.1776352 -8.7577753 -9.6215382 -10.690603 -10.904428 -10.799938 -9.1895208 -8.5392237 -9.1683493 -9.1789017 -11.057617 -13.07086 -13.82997 -12.066468][-6.4371243 -7.2612314 -7.503746 -8.4556637 -9.5751162 -9.8684006 -9.9053669 -10.21859 -10.525153 -9.5544214 -8.3620338 -11.431248 -13.689482 -14.112382 -13.808272][-4.9792719 -7.5061426 -9.1186819 -8.6906881 -8.8837051 -9.3222666 -9.779686 -8.7839441 -8.4083643 -8.9596357 -9.0470791 -11.807089 -13.228121 -13.733061 -14.099797][-7.566184 -8.3887892 -8.0604963 -7.0489011 -6.8190365 -6.4768372 -6.6890225 -7.9375095 -8.3803568 -7.7660112 -6.9682512 -10.748726 -13.826848 -14.487348 -13.992564][-9.3234682 -10.498021 -11.636978 -9.3240213 -5.8439846 -2.8622749 -1.5007181 -3.8165047 -7.4334788 -7.7789845 -7.6600623 -10.708822 -12.536024 -14.123107 -14.344615][-10.021896 -10.919741 -11.624317 -7.976634 -3.495162 0.59102535 4.0942879 2.7597237 -1.1299758 -4.6350017 -8.0393467 -10.382079 -11.390091 -12.380068 -12.118125][-10.012926 -12.021873 -10.523352 -5.3433585 -0.88338518 2.2033343 6.1811161 7.4736586 5.234067 -0.47724009 -6.1760726 -9.7385235 -12.120007 -12.695869 -11.873585][-10.209327 -10.705853 -9.0635147 -3.4752426 0.89420176 5.4683561 9.3055868 6.5467434 4.5813637 2.3659258 -2.467118 -8.14448 -11.862581 -12.571492 -12.754789][-6.2739758 -8.3087568 -8.03605 -4.811089 -1.3118138 4.8636608 7.8337841 6.3206158 6.2769756 1.7318678 -3.2604585 -8.4511509 -12.407388 -13.424047 -13.172564][-5.2478633 -5.6937103 -6.2762246 -4.2003803 -1.6094146 0.90782261 2.3309851 3.7799902 3.9008312 0.21354103 -2.834408 -8.2945452 -13.521414 -14.370872 -13.60725][-9.1294 -10.317807 -11.063806 -8.8075275 -7.1412158 -5.8931203 -3.7673481 -2.863894 -2.6946809 -3.4745178 -4.8563938 -11.526447 -14.914377 -15.077652 -14.263838][-13.822644 -12.826289 -13.29435 -12.939165 -12.758505 -9.9871445 -7.8947291 -9.0115261 -8.6858444 -8.5502377 -9.2672367 -12.124908 -13.93583 -16.096939 -14.749761][-13.886949 -13.738485 -14.405771 -15.103791 -14.803163 -12.837851 -12.021046 -11.818989 -11.559074 -10.668945 -10.462683 -12.767649 -13.937664 -14.22952 -12.01877][-12.854959 -11.961489 -11.534203 -10.949238 -10.969299 -12.624562 -12.521129 -10.781263 -10.669828 -11.286432 -11.329037 -11.94383 -11.982075 -12.578972 -10.412825][-9.907239 -9.5713463 -8.9786377 -7.6844435 -6.8372235 -7.3052378 -7.9472351 -9.1176672 -10.390595 -9.61204 -9.4989357 -11.791527 -12.887259 -12.114859 -11.102541]]...]
INFO - root - 2017-12-15 12:45:49.521749: step 13410, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 60h:11m:45s remains)
INFO - root - 2017-12-15 12:45:56.039882: step 13420, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 59h:16m:19s remains)
INFO - root - 2017-12-15 12:46:02.554973: step 13430, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 56h:44m:27s remains)
INFO - root - 2017-12-15 12:46:09.160880: step 13440, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 59h:13m:17s remains)
INFO - root - 2017-12-15 12:46:15.687012: step 13450, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 58h:33m:52s remains)
INFO - root - 2017-12-15 12:46:22.191380: step 13460, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 58h:48m:22s remains)
INFO - root - 2017-12-15 12:46:28.778964: step 13470, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 59h:12m:28s remains)
INFO - root - 2017-12-15 12:46:35.359326: step 13480, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 57h:48m:56s remains)
INFO - root - 2017-12-15 12:46:41.942346: step 13490, loss = 0.20, batch loss = 0.16 (12.0 examples/sec; 0.667 sec/batch; 59h:08m:44s remains)
INFO - root - 2017-12-15 12:46:48.450363: step 13500, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 58h:44m:36s remains)
2017-12-15 12:46:48.952471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9550159 -4.572196 -5.005013 -4.7488184 -4.5609627 -3.6837258 -2.8188629 -1.4801579 -0.59781122 -0.47259283 -0.56629896 -2.9437273 -4.2694077 -5.1121306 -6.1859937][-4.9760056 -4.6519608 -3.98402 -4.262599 -4.2479992 -3.5452704 -2.4710417 -1.0403032 -0.20120096 0.76096344 0.57149839 -2.7291627 -4.0287981 -5.6421251 -6.9887342][-2.0387166 -2.8080583 -3.39195 -2.77249 -2.8643274 -2.8339617 -2.4618044 -1.0847859 -0.096113682 -0.23320866 -0.40146828 -2.673178 -4.0462523 -5.4711385 -6.7843165][-1.7907143 -2.6672487 -3.0129588 -2.1018386 -1.9813845 -1.663281 -1.5138025 -1.2517943 -1.1088338 -0.62023497 -0.35618973 -3.2654343 -4.8544307 -5.7782683 -6.7541142][-2.5783772 -4.1293631 -4.2103662 -3.1447139 -3.0786366 -1.3123336 -0.57320929 -1.2654896 -1.7513332 -1.0750184 -0.61167 -3.1712489 -4.467041 -5.5186791 -6.5892811][-3.9979057 -4.4445767 -4.6496115 -2.7650983 -0.88923216 1.2793746 2.2359233 1.3497496 0.65841293 -0.21549034 -1.2126441 -2.8837063 -3.5267706 -4.650465 -5.9539909][-6.07048 -5.3724656 -3.6644123 -1.1694627 1.2651706 3.0636826 3.8831015 4.0032673 3.725769 1.5301452 -0.47987652 -2.9674261 -4.1984406 -5.3463807 -6.3146806][-6.8682623 -5.5582776 -4.0184555 -0.94034767 1.954731 4.8059521 6.6002426 5.3461113 4.2902594 2.8049355 1.1584311 -2.43834 -4.7226405 -6.40792 -7.898963][-5.3716145 -4.3939853 -3.9038754 -2.00718 -0.11943626 2.5119743 4.8853664 4.5252037 3.6806383 2.1443481 0.788692 -2.7319353 -5.0621128 -6.6425939 -8.2384987][-5.3565474 -4.5579138 -3.8021708 -1.5801859 -0.77662945 0.31951618 1.9810739 2.1697412 2.1016159 0.36449242 -1.406312 -4.6041989 -6.3242188 -7.6845608 -8.8739243][-8.008049 -7.1812372 -6.6513515 -4.5863142 -3.6718237 -3.345535 -2.4127824 -1.883688 -1.8980305 -2.81545 -3.9191613 -7.72875 -9.5817118 -9.4406853 -9.6238136][-10.861481 -10.075647 -9.2385788 -8.2654428 -7.5368776 -6.8052464 -6.7057672 -6.8918271 -6.5152588 -6.7632828 -7.5191813 -9.7109652 -10.637974 -10.884022 -10.906193][-13.318264 -12.733311 -11.389404 -10.247928 -10.265058 -9.6663351 -9.538413 -10.142771 -10.474773 -10.140778 -9.8081779 -11.046564 -11.1467 -10.654605 -10.077068][-11.973741 -11.231821 -10.726036 -9.7278681 -9.5800362 -9.7150764 -9.97661 -9.4781971 -9.8984556 -10.553312 -10.350848 -9.8506145 -9.1640949 -9.10391 -9.186533][-8.0370312 -8.1687651 -7.7567611 -6.6748371 -6.8220415 -6.9415169 -7.1192255 -7.0667162 -7.697237 -8.0051136 -7.9196529 -8.5899544 -8.7848291 -8.5886879 -9.0008926]]...]
INFO - root - 2017-12-15 12:46:55.420345: step 13510, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.635 sec/batch; 56h:18m:20s remains)
INFO - root - 2017-12-15 12:47:01.951379: step 13520, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 56h:46m:56s remains)
INFO - root - 2017-12-15 12:47:08.458207: step 13530, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 59h:33m:42s remains)
INFO - root - 2017-12-15 12:47:14.992342: step 13540, loss = 0.19, batch loss = 0.14 (11.7 examples/sec; 0.686 sec/batch; 60h:49m:01s remains)
INFO - root - 2017-12-15 12:47:21.613370: step 13550, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 56h:46m:04s remains)
INFO - root - 2017-12-15 12:47:28.278267: step 13560, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 58h:52m:29s remains)
INFO - root - 2017-12-15 12:47:34.925231: step 13570, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 58h:40m:02s remains)
INFO - root - 2017-12-15 12:47:41.569609: step 13580, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.694 sec/batch; 61h:27m:36s remains)
INFO - root - 2017-12-15 12:47:48.222648: step 13590, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 56h:33m:10s remains)
INFO - root - 2017-12-15 12:47:54.823483: step 13600, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.668 sec/batch; 59h:11m:20s remains)
2017-12-15 12:47:55.340927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4191666 -3.5335112 -2.7191858 -2.9692822 -4.4953604 -5.3062863 -6.1651349 -5.2429547 -3.4272542 -3.8929539 -4.4510221 -5.96762 -7.8862147 -8.1816435 -7.7144461][-3.777565 -3.4557166 -2.6174822 -2.5577006 -3.3004804 -3.6847348 -3.0541654 -2.0342345 -1.6853323 -2.0244837 -2.5896962 -3.9360709 -5.4585814 -6.9359508 -7.2662153][-2.6845279 -3.1328588 -3.818943 -3.6522446 -3.9376843 -3.5257666 -2.8029594 -1.5390754 -0.78753805 -1.3248143 -1.8371079 -3.3197889 -4.6300859 -5.1935506 -5.5629544][-4.6691093 -4.6594319 -4.9350562 -4.9480433 -5.5925374 -4.2114754 -2.809464 -1.9292026 -1.1833014 -0.7197628 -0.69389772 -2.6596687 -5.0738 -6.0302157 -5.3102546][-3.3353961 -4.909122 -4.8670135 -4.6745071 -5.9376655 -4.5419588 -2.2843802 -1.1778898 -0.83415461 0.46985626 2.050662 -0.17428255 -2.8708839 -5.2017436 -6.5331903][-3.7610288 -3.9106884 -3.3786504 -2.3976865 -1.3742938 -1.0443988 0.07376194 2.0247846 1.4981213 0.91620779 0.74335241 -0.86688757 -2.9802332 -4.6958661 -5.2427945][-3.3038359 -2.5452592 -2.0456636 -0.41746712 1.464551 3.3312054 4.1727762 4.0704575 3.9023952 2.68606 1.0781293 -0.7550869 -2.5430653 -4.8289747 -6.0781059][-2.6543524 -2.3585839 -2.6722124 0.28833055 3.8042479 5.5300789 7.3888664 6.1463928 3.8600321 2.6660786 2.0337782 0.11394405 -1.8313231 -3.9415631 -5.0230489][-3.7592258 -2.4939291 -1.6745491 -0.37897587 1.0462751 4.490057 6.0735245 5.2475672 3.0472507 1.0057278 0.44916391 -0.7545023 -2.5932105 -4.077662 -4.4731441][-3.9162514 -4.1174574 -2.5896013 -0.61715269 -0.35792685 1.4598632 3.5073271 3.4752455 1.6532588 -0.063274384 -0.84693909 -1.8839326 -2.7986248 -4.0454378 -4.9169245][-6.8181539 -6.2111664 -5.5156922 -4.27942 -3.1895571 -2.3694327 -2.1272135 -1.4812684 -2.0088372 -3.0069373 -2.8995566 -3.7332773 -5.1861591 -5.80538 -5.7630291][-9.9317923 -8.6539135 -6.7876697 -5.701489 -6.3749919 -6.0689282 -5.9570231 -6.103436 -6.1490641 -6.1654139 -5.9311786 -6.4309449 -7.3174362 -7.8318768 -6.382093][-9.8165855 -8.7159042 -7.8578539 -6.3467884 -6.8454647 -8.0512657 -8.1165657 -7.8464041 -7.7509365 -6.8661118 -6.1371126 -6.6150331 -7.1995654 -6.2333808 -5.1253834][-8.883749 -7.4116521 -6.2983508 -5.8775992 -6.1359096 -6.9818449 -7.1091695 -6.7536764 -6.116396 -6.6473517 -6.488553 -4.658946 -4.4577742 -4.3711495 -2.927645][-5.3545146 -5.6907434 -5.730803 -4.9478693 -4.406919 -4.7669835 -4.38896 -4.85168 -5.1623816 -4.3569345 -4.0740523 -4.7605147 -4.9097519 -5.0053453 -5.3126364]]...]
INFO - root - 2017-12-15 12:48:01.952257: step 13610, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 58h:44m:54s remains)
INFO - root - 2017-12-15 12:48:08.517933: step 13620, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.684 sec/batch; 60h:37m:26s remains)
INFO - root - 2017-12-15 12:48:15.186760: step 13630, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.689 sec/batch; 61h:01m:49s remains)
INFO - root - 2017-12-15 12:48:21.721999: step 13640, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 57h:12m:10s remains)
INFO - root - 2017-12-15 12:48:28.194012: step 13650, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 56h:19m:46s remains)
INFO - root - 2017-12-15 12:48:34.804131: step 13660, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 59h:34m:29s remains)
INFO - root - 2017-12-15 12:48:41.358979: step 13670, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.637 sec/batch; 56h:23m:42s remains)
INFO - root - 2017-12-15 12:48:47.953665: step 13680, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 57h:21m:26s remains)
INFO - root - 2017-12-15 12:48:54.566209: step 13690, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 59h:42m:25s remains)
INFO - root - 2017-12-15 12:49:01.145348: step 13700, loss = 0.19, batch loss = 0.15 (11.7 examples/sec; 0.684 sec/batch; 60h:36m:34s remains)
2017-12-15 12:49:01.619814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3648582 -6.8237667 -4.5502577 -1.1571927 0.41275549 -1.3615584 -3.7400303 -4.6882157 -6.0956173 -6.5332918 -6.0784826 -8.1678782 -11.248774 -12.057549 -9.7901239][-5.8032079 -6.367713 -7.2711692 -4.9636083 -3.6270108 -2.284672 -0.857604 -2.3314927 -4.0648265 -5.6930404 -7.7299476 -9.9622307 -11.136528 -13.210818 -11.201235][-4.4023566 -6.4011149 -7.4964681 -5.4420042 -5.33454 -3.7680695 -2.6588283 -2.9730537 -3.76541 -4.322433 -4.3332453 -7.6144428 -10.208619 -11.552532 -10.599445][-4.5109997 -6.5998249 -7.979074 -7.9040308 -7.9792147 -4.9716463 -1.8195176 -0.55824041 -1.7149935 -5.1050906 -8.3791466 -11.385717 -11.649549 -12.716015 -10.647831][-6.2487531 -7.7105894 -8.51152 -7.7960415 -5.7636747 -2.2750537 -0.74386454 -1.1877203 -2.0570045 -3.0505245 -5.6795382 -10.640624 -12.978326 -14.869162 -13.010866][-7.4295387 -6.448534 -6.4901543 -6.3223648 -3.6293454 2.4440351 7.9976 7.2465081 2.8043771 -2.4301488 -6.144011 -9.4144659 -12.045296 -14.577848 -13.530873][-8.7076817 -8.4032612 -8.2800446 -5.395575 -2.0767908 4.1870751 9.218092 10.669622 9.3204594 1.4338999 -4.9887218 -7.00911 -9.2547112 -12.649218 -13.042578][-8.524436 -10.213829 -10.484455 -5.5356407 -0.751688 5.982409 11.350952 10.984892 9.66559 2.9899573 -3.6470897 -8.4280157 -10.322533 -10.792626 -10.098426][-7.7755494 -10.077339 -9.1696949 -6.07433 -3.302474 3.3470159 7.9983211 8.9389172 7.2231021 -0.057322979 -4.4712033 -9.4955235 -12.688913 -13.687872 -11.765529][-6.9600887 -8.1073914 -8.0125494 -5.97476 -3.8974695 0.12310791 2.6700902 3.9369779 2.4412704 -3.5621457 -9.4327374 -14.563419 -16.378071 -16.205004 -14.122938][-10.054708 -11.509428 -10.13657 -7.9900012 -6.69555 -4.5955153 -4.3170118 -3.8130112 -5.5105042 -8.7110958 -11.756847 -15.153461 -18.090092 -18.392118 -15.642008][-14.078814 -14.596783 -14.537121 -11.554081 -7.9883327 -6.7969432 -7.3772821 -8.4135895 -10.159407 -12.402223 -13.964003 -15.955723 -16.048395 -15.224771 -13.889366][-15.470116 -14.95046 -13.799942 -12.647972 -11.748854 -10.786774 -10.006493 -10.622557 -11.271049 -11.462376 -11.632411 -12.678507 -13.142682 -11.618745 -9.4642086][-11.233433 -10.17116 -10.886971 -11.438713 -11.296227 -10.377041 -9.9982138 -10.136312 -10.296932 -10.530622 -10.328093 -8.4815779 -8.1028557 -7.5456491 -7.5021334][-6.4924049 -6.0094633 -4.7808037 -4.4444532 -6.3693981 -8.1366673 -8.3011742 -6.4702325 -6.3615952 -7.1128931 -6.8512712 -7.1401567 -8.3630562 -7.3639765 -6.9992218]]...]
INFO - root - 2017-12-15 12:49:08.223480: step 13710, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.689 sec/batch; 61h:03m:08s remains)
INFO - root - 2017-12-15 12:49:14.820015: step 13720, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 59h:09m:00s remains)
INFO - root - 2017-12-15 12:49:21.442936: step 13730, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 59h:00m:13s remains)
INFO - root - 2017-12-15 12:49:27.983604: step 13740, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 58h:01m:56s remains)
INFO - root - 2017-12-15 12:49:34.474387: step 13750, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 56h:30m:03s remains)
INFO - root - 2017-12-15 12:49:41.079742: step 13760, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 57h:48m:58s remains)
INFO - root - 2017-12-15 12:49:47.588981: step 13770, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 58h:38m:55s remains)
INFO - root - 2017-12-15 12:49:54.118196: step 13780, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 57h:34m:46s remains)
INFO - root - 2017-12-15 12:50:00.623730: step 13790, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 57h:45m:56s remains)
INFO - root - 2017-12-15 12:50:07.116378: step 13800, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 57h:06m:09s remains)
2017-12-15 12:50:07.596725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2741261 -4.7137785 -4.3626423 -4.3772888 -4.7902679 -5.8479843 -6.7915192 -6.8019042 -7.518291 -7.6347904 -7.2920184 -8.2554693 -8.172121 -7.3580937 -6.7243595][-1.7071843 -2.133939 -1.7189457 -3.0898049 -4.8414392 -5.9520388 -6.7359567 -7.3823123 -7.5468864 -6.4756036 -5.5438743 -6.3915973 -6.9538584 -6.6669168 -5.59111][-0.46714258 -2.2500165 -3.187242 -3.1318564 -3.9840713 -5.0396862 -4.9390221 -5.0639977 -5.0753484 -4.4639349 -3.9172881 -4.9843636 -5.2649965 -5.4959254 -4.8507586][-3.5428221 -3.8818495 -3.8123055 -4.28962 -4.624918 -3.9278677 -3.4586809 -3.9334624 -3.8535409 -2.8116343 -2.4781818 -4.1060548 -5.3044748 -5.791338 -5.0662279][-5.5203261 -6.3680139 -6.5117598 -5.4331365 -4.7707129 -2.2535982 -0.041406155 -0.013637543 -1.3362103 -2.0979657 -2.2827837 -4.4187555 -6.0690594 -7.1775017 -6.9186506][-8.6815052 -7.7243395 -6.4390626 -4.3397751 -1.6486144 1.5697451 4.1391687 4.4111691 3.7928467 0.67506742 -2.3936572 -4.4958897 -6.4486876 -7.5871172 -6.6346331][-10.044901 -8.6544828 -6.3336883 -2.944248 -0.38557434 2.9984932 6.3627472 6.7666759 6.298665 2.4252729 -1.9933307 -5.1310406 -7.6084857 -7.5398784 -6.13094][-9.8342514 -8.6728287 -7.2482767 -3.3048117 -0.18160439 3.8430529 7.6148977 6.9665756 5.5241804 1.7291279 -1.7126026 -5.16719 -7.6015625 -7.0409622 -5.1955338][-7.2911263 -7.77499 -7.1232309 -3.6352377 -1.0291939 2.2666759 4.4191732 4.030983 3.3253431 -0.021718979 -3.0596776 -6.4711871 -8.8950882 -8.2173424 -5.9298568][-7.90096 -7.8489695 -7.6303234 -5.3334279 -3.9575143 -1.3295579 0.66335726 1.0547109 0.64336586 -1.709949 -3.3793213 -6.3738976 -8.3360748 -8.3180294 -6.3420753][-13.709512 -13.639679 -12.499509 -10.370355 -9.2605839 -7.7277946 -6.5824337 -5.4771895 -4.8011312 -5.4012823 -6.5702028 -8.7094564 -9.4917994 -8.6337461 -6.718431][-16.568317 -17.896204 -16.197037 -13.442547 -11.922158 -10.239343 -9.4091635 -9.1321383 -8.4802942 -8.0304241 -8.282629 -8.8946857 -9.173974 -8.2594213 -6.8153992][-13.641193 -13.715479 -12.774143 -11.467346 -10.552645 -9.3269558 -8.5482483 -7.9744487 -7.8157892 -7.1860695 -6.8544521 -7.221993 -7.3935142 -6.3436894 -5.5157466][-10.62645 -10.785823 -9.8874865 -7.8595796 -6.5937152 -7.6080823 -8.0584669 -7.4551935 -6.7619371 -7.0128689 -7.5264978 -7.1403446 -7.0009718 -5.76833 -4.5013013][-7.4444833 -6.76196 -6.3428884 -5.9450164 -5.7578721 -5.4748034 -5.7656827 -7.4097195 -7.8292212 -6.4787917 -5.2644682 -5.3275695 -5.9577804 -6.3040352 -6.0986857]]...]
INFO - root - 2017-12-15 12:50:14.147768: step 13810, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 59h:39m:36s remains)
INFO - root - 2017-12-15 12:50:20.644362: step 13820, loss = 0.11, batch loss = 0.06 (12.2 examples/sec; 0.654 sec/batch; 57h:51m:48s remains)
INFO - root - 2017-12-15 12:50:27.167732: step 13830, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 57h:33m:00s remains)
INFO - root - 2017-12-15 12:50:33.776586: step 13840, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 59h:35m:09s remains)
INFO - root - 2017-12-15 12:50:40.238068: step 13850, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 56h:38m:40s remains)
INFO - root - 2017-12-15 12:50:47.035689: step 13860, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 58h:58m:17s remains)
INFO - root - 2017-12-15 12:50:53.636178: step 13870, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 59h:52m:03s remains)
INFO - root - 2017-12-15 12:51:00.218896: step 13880, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 57h:05m:05s remains)
INFO - root - 2017-12-15 12:51:06.707710: step 13890, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 56h:16m:22s remains)
INFO - root - 2017-12-15 12:51:13.344544: step 13900, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 59h:10m:06s remains)
2017-12-15 12:51:13.829388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.4046822 -8.5452328 -7.6965618 -6.6648755 -7.0457354 -7.8904333 -8.1369734 -7.692904 -7.5996218 -7.0838747 -6.22872 -7.0565329 -8.042079 -6.1645532 -4.3737035][-7.7200017 -8.4605541 -8.0509148 -7.0030112 -7.2403326 -7.9433212 -8.0883226 -8.4520569 -8.8003807 -8.1863832 -7.5489154 -8.246398 -8.5984869 -7.30767 -6.5344696][-5.384161 -6.9083204 -7.9575253 -6.5612063 -6.2407069 -6.5263939 -6.7405953 -7.0305915 -7.390502 -7.5264273 -7.363338 -8.2210283 -8.9224424 -7.0057926 -5.64029][-5.8517275 -6.2482748 -6.00311 -5.5996337 -5.7358785 -4.9121518 -4.6418085 -5.4857736 -6.5462484 -6.4815326 -6.3104362 -7.6363306 -8.0781393 -6.1571951 -5.2110834][-6.7833891 -7.1974878 -7.4840517 -5.7562408 -4.0416465 -2.7478826 -2.5069845 -3.4904366 -5.1227503 -5.4836416 -5.9512558 -7.1230283 -7.8907709 -6.3702683 -5.24441][-10.591484 -8.8014278 -7.0591235 -4.3689537 -2.2547972 -0.76363707 0.23662949 -0.23432779 -1.3333125 -3.0313144 -5.079998 -6.5102677 -7.3415995 -6.0796609 -4.8404551][-10.347308 -7.9406691 -6.651649 -3.3459055 -1.4796796 0.47669458 2.2411056 2.3637047 1.8104649 -0.26422358 -2.7053564 -4.9140573 -6.2827048 -4.5530577 -3.505203][-8.6667118 -6.6700678 -5.520761 -2.0261915 -0.14801168 1.7505512 3.0773239 2.1310463 1.7554526 1.1060033 0.29442787 -2.9660184 -5.6575627 -4.6281862 -3.5167329][-7.253777 -5.4436665 -3.5569742 -0.67297363 -0.074145317 1.3180199 2.6655836 2.2098255 1.6831584 0.36365747 -0.82236624 -2.9029808 -4.9417305 -4.8790011 -4.68699][-5.8153958 -3.677557 -2.4605246 -0.54271841 -0.27369022 0.84785891 1.69632 1.7149343 1.2243276 0.20265818 -0.42248917 -2.9153178 -5.2789712 -4.5829782 -4.8600307][-7.3405566 -5.4388623 -3.1907582 -1.0835085 -0.87181282 -0.86157227 -0.55594683 -0.89502668 -1.158577 -1.2695603 -1.8502128 -4.4172087 -5.5876656 -5.9121728 -6.5595846][-10.851345 -9.365633 -6.8050289 -5.364398 -5.5209484 -4.7843442 -4.2730055 -4.66353 -4.7382445 -5.0495777 -5.818718 -7.354641 -7.7435784 -8.21423 -8.2623062][-10.930628 -9.4179268 -7.8584113 -6.3042903 -5.7500091 -5.8326263 -6.3443217 -6.2932329 -6.3084869 -6.6626706 -6.7052312 -6.7739272 -6.7216773 -5.94289 -6.0211039][-9.5645885 -8.7898159 -6.4451513 -4.7375937 -4.09653 -4.4713197 -5.1218638 -5.0262394 -5.3634863 -4.8510365 -4.8510513 -4.0223136 -3.2928462 -2.1515446 -2.2774365][-7.1477146 -6.6677485 -5.4746985 -3.6777477 -3.0091085 -2.9856553 -2.4985602 -2.8891792 -3.4638431 -3.13633 -3.1094742 -3.3892682 -4.1801844 -3.3984468 -2.759692]]...]
INFO - root - 2017-12-15 12:51:20.407973: step 13910, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.675 sec/batch; 59h:45m:48s remains)
INFO - root - 2017-12-15 12:51:27.024039: step 13920, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 58h:35m:01s remains)
INFO - root - 2017-12-15 12:51:33.550846: step 13930, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 56h:13m:24s remains)
INFO - root - 2017-12-15 12:51:40.150090: step 13940, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 57h:35m:26s remains)
INFO - root - 2017-12-15 12:51:46.780104: step 13950, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 58h:17m:48s remains)
INFO - root - 2017-12-15 12:51:53.335520: step 13960, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 58h:21m:40s remains)
INFO - root - 2017-12-15 12:51:59.907583: step 13970, loss = 0.25, batch loss = 0.20 (12.2 examples/sec; 0.655 sec/batch; 57h:56m:57s remains)
INFO - root - 2017-12-15 12:52:06.560022: step 13980, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 59h:02m:14s remains)
INFO - root - 2017-12-15 12:52:13.281043: step 13990, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.682 sec/batch; 60h:19m:55s remains)
INFO - root - 2017-12-15 12:52:19.894662: step 14000, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 58h:37m:39s remains)
2017-12-15 12:52:20.411483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.199084 -3.8365788 -3.8620584 -4.4937115 -4.3895445 -3.5768445 -2.8328342 -3.1330752 -4.3492131 -6.169858 -6.2736931 -6.40386 -8.2443266 -9.2011709 -8.9356728][-3.087121 -4.1301994 -4.6625967 -4.65846 -4.8668294 -4.4762859 -3.5754738 -3.8213217 -4.4501834 -4.8268733 -4.9809175 -5.4887886 -6.3343716 -7.2932873 -7.9052868][-1.9699216 -2.6974692 -3.812068 -4.3054647 -5.3187675 -4.6076622 -3.0903313 -2.9938953 -4.3374314 -4.0874977 -4.0338993 -4.1205907 -4.9463148 -6.9151497 -7.0394721][-2.6593308 -3.5995157 -4.267417 -4.4409389 -5.1430225 -3.8300197 -2.718442 -2.3560531 -2.2957833 -3.1136801 -2.8533742 -2.8490911 -5.3250051 -7.0433559 -6.84835][-3.6096849 -4.0220537 -5.3673196 -5.6261778 -4.7209091 -2.6254811 -1.2500396 -0.6772418 -1.0924454 -1.3838935 -1.5019293 -1.9812791 -3.6352561 -6.1591377 -6.9840388][-4.9561219 -5.5726666 -5.9507642 -5.5109324 -4.0058627 -0.97775555 1.0289979 1.254065 0.94048786 -0.76119566 -1.348206 -1.2319994 -2.8149345 -5.4276075 -5.2563844][-5.5936823 -5.6357522 -6.1395235 -4.6650867 -2.6142404 -0.25087738 2.765099 3.7475848 3.4505038 1.5553231 0.064176083 -0.69400835 -2.4772971 -4.379683 -4.6923237][-6.2180829 -5.7735014 -5.5504041 -4.0534687 -1.692636 0.76546335 2.9877887 3.5466676 3.5807509 2.7604747 1.1101332 -1.0425172 -3.8695621 -5.3102856 -5.8910785][-6.273602 -6.4171042 -6.4702134 -4.6171975 -2.6644254 0.55076885 2.6650987 3.4749293 3.6148734 1.7121019 0.0076823235 -2.0483408 -5.041997 -7.3422308 -7.5726352][-7.717586 -8.7475033 -8.2070036 -6.5157866 -4.80249 -1.1623554 0.81143332 2.2503967 2.1584477 0.49857903 -0.594028 -3.2758176 -6.504776 -8.12023 -8.3331356][-11.654358 -12.444435 -11.571565 -8.870985 -7.4716454 -5.2190814 -3.6239455 -2.6823072 -3.1630237 -3.5129938 -4.5152006 -6.7651081 -9.2459755 -10.203087 -9.3859148][-13.003935 -13.30772 -12.781692 -10.427315 -8.8242579 -6.5095725 -5.5768394 -5.145442 -5.3567152 -7.1543307 -8.2803011 -9.77387 -10.877308 -11.741006 -10.671776][-11.417351 -10.820377 -10.095183 -9.8863258 -9.1910629 -7.5086446 -6.6982269 -6.2597909 -6.3079128 -7.6172295 -8.6420288 -9.76535 -10.222695 -8.6164293 -7.9743195][-9.55558 -7.7433395 -6.703196 -6.5911422 -5.8505449 -5.9921045 -6.4462442 -6.2327242 -5.9621172 -6.669836 -7.6637006 -7.6473303 -7.3356719 -7.5722194 -6.4562564][-6.3518009 -5.90971 -5.41094 -4.12895 -3.7432108 -4.4879465 -4.7117629 -4.8479667 -5.4296174 -5.8688908 -6.1773939 -6.7413378 -6.9359074 -7.2209892 -6.7070594]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 12:52:27.089840: step 14010, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 59h:28m:41s remains)
INFO - root - 2017-12-15 12:52:33.658177: step 14020, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 57h:54m:09s remains)
INFO - root - 2017-12-15 12:52:40.209469: step 14030, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 59h:34m:11s remains)
INFO - root - 2017-12-15 12:52:46.786123: step 14040, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 60h:21m:21s remains)
INFO - root - 2017-12-15 12:52:53.275435: step 14050, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 57h:50m:36s remains)
INFO - root - 2017-12-15 12:52:59.831099: step 14060, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 58h:21m:04s remains)
INFO - root - 2017-12-15 12:53:06.419635: step 14070, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 58h:30m:57s remains)
INFO - root - 2017-12-15 12:53:12.965118: step 14080, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 57h:23m:42s remains)
INFO - root - 2017-12-15 12:53:19.455703: step 14090, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 58h:01m:27s remains)
INFO - root - 2017-12-15 12:53:25.991195: step 14100, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 60h:06m:01s remains)
2017-12-15 12:53:26.462012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9933906 -5.8852696 -4.6216955 -4.1519895 -4.2292471 -3.8348839 -3.6853449 -3.6025467 -3.3213105 -3.3171356 -3.118391 -5.8047748 -7.6867762 -7.6477275 -7.96855][-5.9667878 -5.7081971 -6.0427275 -5.7830644 -5.0424104 -4.5092354 -4.1310639 -3.7734866 -3.5676043 -3.723712 -4.1148491 -6.7697082 -8.9043636 -9.278553 -9.4355221][-4.4911213 -5.0881338 -5.2892151 -4.8035183 -5.1257715 -5.1764107 -4.6554356 -4.0599203 -3.8420432 -4.2438622 -4.807796 -7.2732983 -9.74041 -10.066056 -10.078644][-2.9319236 -3.1417508 -3.563817 -3.8661361 -4.877162 -5.2946138 -4.911561 -4.3864131 -4.1671252 -3.944797 -4.4591751 -6.9327197 -9.2023335 -9.572958 -9.5196314][-3.3423271 -3.6662321 -3.6589286 -2.9203634 -3.3433704 -3.2440152 -2.9323552 -3.7505445 -5.034668 -4.3908634 -3.9937611 -6.2482705 -7.8436251 -7.8258991 -8.2201328][-4.7844172 -4.9143605 -3.6198957 -2.2646258 -1.7211096 -0.45354939 0.65192175 0.36234474 -0.67045784 -1.8248525 -2.9957685 -4.834651 -6.2831073 -6.6373391 -7.4263077][-6.3026943 -6.16211 -4.8368187 -1.7800622 0.68480825 2.3379674 3.3217025 3.68397 3.4727111 1.4761624 -0.63288116 -3.7387 -5.8355556 -5.9764404 -6.3138809][-5.2398787 -4.8515468 -3.0318801 -0.33381367 1.7946005 3.9704642 5.0589681 5.2879272 4.8033509 3.3509412 1.3460956 -1.8877509 -3.9731634 -4.4013238 -4.37113][-1.7223797 -1.6172934 -0.44253397 0.47644234 2.331943 4.6250014 5.2726922 4.65962 3.4947028 2.39856 1.3622584 -0.88744831 -2.6351938 -3.2912407 -3.4746783][0.29343939 0.31503296 0.35200644 0.63261414 1.6085954 2.7158589 2.9939075 2.3331714 1.2426052 0.62527132 -0.56200838 -2.7485514 -4.3408704 -3.8499982 -3.5436099][-4.6685333 -4.126894 -3.1244943 -2.4239554 -1.7655954 -0.88340759 -0.51616049 -1.9092646 -2.692981 -3.1496849 -3.832828 -6.1012349 -7.0039759 -5.5077696 -4.0128331][-9.8072414 -8.6953354 -6.9670081 -5.3152356 -4.7162189 -4.4281168 -5.106513 -5.7671571 -6.6270461 -7.2794609 -7.4398093 -7.931015 -6.8769159 -4.9835067 -3.2170668][-10.157393 -9.3095646 -7.8805075 -7.1293249 -5.9820356 -4.8559437 -5.5758734 -7.4139752 -9.2353649 -9.0926809 -8.3903122 -7.885747 -5.89054 -3.4937778 -1.879431][-7.8634253 -7.73319 -6.508503 -5.4314461 -4.4431062 -4.5160332 -5.0133519 -5.9354773 -7.5726705 -8.4747219 -9.0179653 -7.6505527 -5.6137114 -3.5346432 -2.5796912][-5.6436749 -5.5498681 -4.4706936 -4.0664663 -3.5482759 -2.8176739 -2.8281019 -4.0173836 -5.84696 -6.86791 -7.5846119 -7.7730322 -7.54987 -6.02648 -5.1147265]]...]
INFO - root - 2017-12-15 12:53:32.915881: step 14110, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 56h:30m:23s remains)
INFO - root - 2017-12-15 12:53:39.491289: step 14120, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 58h:31m:27s remains)
INFO - root - 2017-12-15 12:53:46.013164: step 14130, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 58h:58m:23s remains)
INFO - root - 2017-12-15 12:53:52.596893: step 14140, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 57h:25m:21s remains)
INFO - root - 2017-12-15 12:53:59.243099: step 14150, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 59h:10m:26s remains)
INFO - root - 2017-12-15 12:54:05.829808: step 14160, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 57h:21m:43s remains)
INFO - root - 2017-12-15 12:54:12.394464: step 14170, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 57h:31m:30s remains)
INFO - root - 2017-12-15 12:54:18.977781: step 14180, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 59h:10m:50s remains)
INFO - root - 2017-12-15 12:54:25.579579: step 14190, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 58h:42m:04s remains)
INFO - root - 2017-12-15 12:54:32.207785: step 14200, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 57h:31m:24s remains)
2017-12-15 12:54:32.753730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8489404 -3.8375444 -5.0533929 -5.9496083 -6.7072048 -6.7388525 -6.19756 -5.873775 -5.447926 -3.9042268 -2.8732276 -3.8384938 -5.1004233 -7.274426 -8.1615505][-3.5470331 -3.9837985 -5.144505 -5.7889671 -6.114603 -6.0726218 -5.6193743 -4.9047866 -4.148056 -3.8045394 -3.3190963 -4.6143579 -6.7372317 -7.9440389 -7.95016][-3.0134606 -4.167655 -5.5764489 -6.017509 -6.171844 -6.0353093 -5.5784812 -5.0834107 -3.7951941 -3.3700743 -3.347235 -5.4504476 -7.22576 -8.8569374 -9.464694][-3.9542303 -3.943541 -5.3439536 -5.8585906 -6.1718826 -5.74878 -4.3096247 -3.8907814 -3.8644369 -3.0867217 -2.993423 -5.7616754 -7.9278631 -9.4138613 -8.9093065][-5.1045623 -5.8335638 -6.5936036 -6.8106694 -6.0357251 -2.9872468 -1.3840914 -2.7223473 -3.6965468 -3.6967881 -3.8882108 -6.1103678 -8.2002993 -10.13016 -10.425514][-6.9715614 -6.7201905 -6.2932405 -4.5424881 -0.90110016 1.7958875 4.3284545 3.5201421 1.7986832 -1.1371117 -4.2997789 -5.5091434 -7.2499657 -9.5303869 -10.30097][-8.6799965 -8.6639032 -7.507093 -3.6618857 -0.2951436 4.0786662 9.0518417 8.8464375 7.0477395 1.85078 -3.054888 -5.4086432 -7.9039221 -9.2776079 -9.3977337][-9.3412971 -9.6504488 -8.9643354 -5.4873266 -1.9126499 4.0800757 9.9735279 10.376581 9.0408974 4.1169486 -0.72865915 -5.1813507 -8.3910894 -9.1050625 -8.6065788][-6.47522 -8.11254 -8.54954 -6.2442942 -3.8245156 1.4631801 5.8465791 6.6952872 7.1843438 3.3968701 -1.3144565 -6.5332003 -10.034674 -10.540791 -9.4573135][-6.25419 -8.0170126 -9.0342283 -7.3422914 -5.6944036 -3.0588288 -0.091875553 2.0942898 3.1195769 0.11133766 -3.6094444 -8.1750317 -11.417759 -11.853807 -10.926908][-10.36271 -11.549814 -11.619915 -10.026966 -9.2277689 -7.1952424 -5.4035258 -4.7592978 -4.5883355 -5.6361122 -6.7029295 -11.193828 -13.592083 -13.529868 -12.184795][-15.650087 -16.136984 -15.819994 -13.03634 -11.687861 -10.26624 -10.042305 -10.476977 -10.691959 -10.968395 -11.066063 -12.922834 -13.255152 -13.193014 -12.321119][-14.97303 -15.33256 -14.708286 -13.082411 -11.816904 -10.226757 -10.020483 -10.924336 -11.798717 -11.790781 -11.619219 -12.38664 -12.048711 -11.148941 -9.6892509][-12.096178 -12.175504 -12.330193 -10.863104 -9.7106647 -9.4966421 -9.9047222 -10.135827 -11.003757 -12.111765 -12.156202 -10.856556 -9.3627234 -8.7906666 -8.0924177][-8.3422422 -8.2062 -8.3919506 -6.8263326 -5.7654276 -5.366446 -6.016355 -7.7862616 -9.5955973 -9.7377911 -9.4625187 -9.7623329 -9.3894129 -8.928875 -8.9892206]]...]
INFO - root - 2017-12-15 12:54:39.380496: step 14210, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.687 sec/batch; 60h:44m:49s remains)
INFO - root - 2017-12-15 12:54:46.008221: step 14220, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 59h:30m:46s remains)
INFO - root - 2017-12-15 12:54:52.596034: step 14230, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 57h:39m:41s remains)
INFO - root - 2017-12-15 12:54:59.177184: step 14240, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 58h:36m:38s remains)
INFO - root - 2017-12-15 12:55:05.713196: step 14250, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.669 sec/batch; 59h:10m:57s remains)
INFO - root - 2017-12-15 12:55:12.418828: step 14260, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 58h:41m:51s remains)
INFO - root - 2017-12-15 12:55:18.978003: step 14270, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 58h:06m:51s remains)
INFO - root - 2017-12-15 12:55:25.566957: step 14280, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 56h:26m:21s remains)
INFO - root - 2017-12-15 12:55:32.109659: step 14290, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 57h:45m:24s remains)
INFO - root - 2017-12-15 12:55:38.705003: step 14300, loss = 0.17, batch loss = 0.12 (11.3 examples/sec; 0.706 sec/batch; 62h:23m:56s remains)
2017-12-15 12:55:39.261612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4447718 -5.839324 -5.5289512 -5.0544124 -5.124402 -5.8869004 -7.049325 -7.7161164 -7.5920448 -7.2202406 -6.22454 -8.3087511 -9.1850624 -9.8708115 -10.182302][-1.9976192 -2.6421087 -3.261771 -2.7971117 -3.5640523 -4.50152 -5.4414635 -6.3216333 -6.7259326 -6.0498896 -5.4091592 -7.9158559 -9.8581619 -10.864641 -10.031649][-0.902442 -1.6569166 -1.8479018 -2.0281992 -3.1490757 -3.4463682 -3.769805 -4.3709412 -4.6628857 -4.6060567 -4.5835624 -7.7594757 -9.7537861 -10.758444 -10.9967][-2.5342984 -2.7102535 -3.1906796 -3.4145651 -3.2763209 -3.2294419 -3.3676817 -3.5809133 -3.4232395 -3.502943 -3.5973125 -6.9603 -8.9214773 -9.7321568 -10.038773][-3.0318258 -4.5801029 -4.9817967 -4.0795546 -3.7426753 -2.4352269 -1.9555984 -2.2946072 -1.7564306 -1.7236021 -1.6899552 -4.5871067 -7.5991564 -8.6305389 -8.34699][-3.5385096 -4.7112207 -4.4453568 -2.5399148 -1.5019069 -0.03143692 1.1941643 0.75493717 0.92456627 0.78692055 0.70799732 -2.1200414 -4.7823157 -6.2130675 -6.7503767][-3.9833069 -4.2568254 -3.7488084 -1.0739846 0.49303246 1.7656002 3.4183407 3.4287386 3.1915083 2.0088239 1.1687183 -2.3108079 -5.192451 -6.2522941 -7.3278446][-4.2158728 -3.3748839 -2.5722675 -1.0672297 0.72851372 2.2740669 3.558712 3.5180674 3.2252464 1.7888689 0.51311016 -3.3842955 -5.60986 -7.0804687 -7.4535952][-4.8955464 -3.8218236 -2.4043076 -1.4896708 -1.27003 0.63004732 1.8920646 2.5163794 3.3799839 2.3205528 1.3092642 -3.8828497 -8.1598835 -10.662132 -10.831396][-5.3639708 -4.6386328 -3.3142047 -1.5249295 -1.9315488 -1.1338253 -0.72947979 -0.51376867 0.59802818 2.0742579 1.9965029 -3.3732207 -7.2181334 -11.305373 -13.851977][-8.7649231 -8.1375141 -6.638381 -4.7723432 -4.7731533 -4.5978217 -4.1874189 -3.8533382 -3.2800205 -2.5339391 -2.0618114 -6.067347 -9.764286 -12.599995 -14.386666][-11.064358 -11.184793 -10.382836 -8.5263958 -8.8714905 -8.7055082 -8.4416389 -8.3029909 -7.9242487 -7.1191044 -7.2540526 -9.6350327 -11.542488 -13.54446 -13.950412][-12.402916 -12.560781 -11.840034 -11.944211 -12.235431 -11.155821 -11.115852 -10.135268 -10.073389 -9.6474676 -9.9472046 -11.520297 -12.676548 -13.19219 -11.870861][-12.801443 -13.355695 -12.379053 -11.987882 -12.320472 -12.860996 -12.275765 -10.870298 -10.253866 -9.1086721 -9.923933 -10.9372 -11.367374 -12.855581 -11.753185][-10.250315 -11.063049 -10.330221 -9.5518007 -9.056366 -9.683918 -10.075892 -9.3477077 -9.3025455 -8.6914129 -8.6877108 -9.8902855 -10.791903 -11.922461 -12.642345]]...]
INFO - root - 2017-12-15 12:55:45.848511: step 14310, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 56h:49m:27s remains)
INFO - root - 2017-12-15 12:55:52.411526: step 14320, loss = 0.28, batch loss = 0.24 (12.4 examples/sec; 0.646 sec/batch; 57h:05m:29s remains)
INFO - root - 2017-12-15 12:55:58.983946: step 14330, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 58h:55m:50s remains)
INFO - root - 2017-12-15 12:56:05.638413: step 14340, loss = 0.12, batch loss = 0.08 (11.4 examples/sec; 0.699 sec/batch; 61h:45m:09s remains)
INFO - root - 2017-12-15 12:56:12.227391: step 14350, loss = 0.21, batch loss = 0.16 (11.4 examples/sec; 0.702 sec/batch; 62h:00m:34s remains)
INFO - root - 2017-12-15 12:56:18.847799: step 14360, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 59h:07m:37s remains)
INFO - root - 2017-12-15 12:56:25.433822: step 14370, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 57h:31m:39s remains)
INFO - root - 2017-12-15 12:56:31.966830: step 14380, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 58h:16m:20s remains)
INFO - root - 2017-12-15 12:56:38.554174: step 14390, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 58h:41m:41s remains)
INFO - root - 2017-12-15 12:56:44.995990: step 14400, loss = 0.19, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 56h:12m:19s remains)
2017-12-15 12:56:45.473434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7672129 -5.6610508 -5.7122278 -5.1609826 -5.5135055 -6.2853141 -6.2697272 -5.5936642 -5.7746186 -5.8520908 -5.6170335 -7.4635038 -8.3290329 -8.6918135 -7.7749066][-5.3139973 -4.7786932 -3.9596789 -4.1516757 -4.9813876 -5.5288281 -5.6326728 -5.542645 -5.3127303 -4.8506122 -4.9023685 -6.8051968 -6.7871652 -7.5280018 -7.09888][-3.3337228 -4.0547705 -5.0179605 -4.8017654 -5.2642221 -5.7283998 -5.6504 -5.1548939 -4.8664675 -5.0789738 -4.9683771 -6.9849663 -8.369669 -8.8490219 -7.3767762][-2.4461985 -2.5905068 -2.6906285 -3.686342 -5.3817811 -4.5460386 -3.5496688 -4.0670929 -3.6240144 -3.9252062 -4.3280172 -6.3725185 -7.7350225 -9.6634007 -10.482402][-3.2467566 -3.4450662 -4.1174774 -3.8678517 -4.4765482 -3.5779445 -1.617476 -2.0848145 -2.3212376 -3.004992 -4.1007118 -7.1905403 -9.6435995 -10.833214 -11.202168][-3.7879829 -3.1661775 -3.3567266 -2.6978683 -1.6346979 0.874362 3.2940154 3.132297 2.7344527 0.27864695 -1.5678411 -4.3604012 -6.9938612 -9.4656181 -9.6798124][-5.415741 -4.8699107 -4.5175662 -1.5153441 0.5756197 3.0970893 5.8287034 6.7409825 6.9129052 2.9813809 -0.0025382042 -3.7249355 -6.727469 -8.82121 -9.5214968][-5.3695016 -5.7455983 -5.6341734 -2.5424235 0.56428623 5.37806 8.890276 7.843472 6.8290744 3.9065595 1.2434678 -2.6791291 -5.1837611 -7.6934252 -8.9629][-4.3876247 -4.2463226 -4.2027574 -2.2939312 -1.0874782 2.8088088 6.4789662 5.9435391 4.7590156 2.0779347 -0.17694426 -4.7259622 -8.1701336 -10.044289 -10.70685][-2.2790656 -2.7349362 -3.4309807 -2.1395481 -1.0972509 0.67224216 1.897748 1.5031333 0.33322859 -1.7205911 -3.9285774 -7.1577797 -8.78549 -11.566386 -12.561914][-5.7884088 -5.857275 -5.8339081 -4.8698869 -5.3196759 -4.4393721 -4.1650209 -4.6277895 -5.381598 -7.3370333 -8.7504377 -12.045887 -13.696388 -14.622396 -14.563574][-10.109035 -9.906395 -9.9269066 -9.044507 -8.623455 -8.4655619 -8.566081 -9.8482037 -10.600374 -11.048986 -11.443632 -12.981516 -13.436268 -14.391182 -14.113642][-14.436081 -14.017014 -13.105541 -12.791374 -13.054641 -12.479677 -12.313499 -12.408202 -12.347435 -12.429792 -12.066858 -12.747078 -13.306931 -13.030039 -11.609695][-12.631476 -12.412838 -12.662506 -12.360538 -11.83215 -11.900091 -11.430056 -10.633641 -10.705318 -10.419344 -9.6425486 -9.4391479 -9.0177307 -9.6843739 -8.8566256][-10.955206 -11.038586 -10.637398 -10.53882 -9.5508451 -9.3883581 -9.5030632 -9.2325525 -8.0848026 -8.0161819 -7.9529943 -8.7431946 -8.8952446 -8.4465647 -7.7590332]]...]
INFO - root - 2017-12-15 12:56:52.173240: step 14410, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 58h:09m:28s remains)
INFO - root - 2017-12-15 12:56:58.743842: step 14420, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.629 sec/batch; 55h:36m:20s remains)
INFO - root - 2017-12-15 12:57:05.301171: step 14430, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 57h:33m:51s remains)
INFO - root - 2017-12-15 12:57:11.926993: step 14440, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 59h:46m:00s remains)
INFO - root - 2017-12-15 12:57:18.573668: step 14450, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 59h:07m:10s remains)
INFO - root - 2017-12-15 12:57:25.095025: step 14460, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 56h:35m:44s remains)
INFO - root - 2017-12-15 12:57:31.663446: step 14470, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.671 sec/batch; 59h:16m:04s remains)
INFO - root - 2017-12-15 12:57:38.262900: step 14480, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 57h:23m:53s remains)
INFO - root - 2017-12-15 12:57:44.858414: step 14490, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.675 sec/batch; 59h:36m:27s remains)
INFO - root - 2017-12-15 12:57:51.485898: step 14500, loss = 0.19, batch loss = 0.14 (11.8 examples/sec; 0.681 sec/batch; 60h:08m:00s remains)
2017-12-15 12:57:52.006369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.5118046 -8.2070284 -8.2922592 -7.2747712 -6.9261951 -6.7479124 -6.6352158 -6.2872696 -5.87279 -6.351069 -6.2976179 -8.2867374 -9.5371923 -8.4358044 -5.7491832][-6.465981 -6.3754053 -6.0484347 -6.5429811 -6.82192 -6.5595737 -5.8582082 -6.3048582 -6.8662705 -7.0185313 -6.9486837 -9.0946541 -9.6735859 -9.8423252 -8.4078426][-4.0546227 -4.6346126 -5.497088 -5.422574 -5.7454739 -5.8461647 -5.8002372 -5.6229167 -5.6914105 -6.1074238 -6.5032659 -8.8471222 -9.8347073 -9.6470451 -8.0658245][-3.6262298 -4.6817441 -4.7077303 -5.1461735 -5.404532 -4.7677298 -4.5721855 -4.6638842 -4.7936459 -5.1989675 -5.1841183 -7.6392603 -9.07694 -9.8475285 -8.2402344][-5.9243727 -6.5739431 -6.9234791 -5.6252646 -4.3339777 -2.362263 -0.77016115 -1.7155209 -2.9733937 -3.5488148 -4.0456891 -6.4720941 -8.1380692 -9.4102888 -8.5959263][-6.7493291 -7.2551551 -6.5037413 -5.3663406 -2.6975977 0.7429266 3.691267 3.8840494 2.1046576 -0.25516319 -2.8946378 -5.2124543 -6.3769674 -7.6431665 -7.316102][-8.796814 -6.6892433 -4.9117112 -2.999824 -1.0019474 3.4348335 7.5951519 7.9222536 6.64491 2.8583412 -0.70139074 -4.1515775 -6.529623 -7.5556259 -6.8581743][-8.4115372 -7.1112504 -4.2678547 -2.0365729 0.43350363 4.3343229 7.7547436 9.0639133 8.81666 5.2720366 1.1126952 -4.093689 -7.415977 -8.3384981 -7.323163][-6.6890907 -5.5602393 -4.4898987 -1.3876109 1.5241661 4.901228 6.9837046 8.0851278 7.8524771 4.1694517 1.3839679 -3.5502264 -7.3645267 -8.8477068 -9.158268][-6.9442611 -4.8363304 -3.3986919 -0.38817406 1.1932325 3.6755991 6.1160679 5.9164495 4.2627649 1.5239549 -0.80852556 -5.5657477 -8.7283115 -10.930349 -11.324814][-8.8390017 -8.2565727 -6.4996009 -3.7870977 -2.2623305 -1.0934668 0.26060438 -0.15583801 -1.2101421 -3.0226645 -4.6109648 -9.2942486 -11.529467 -12.709267 -11.524194][-13.48377 -12.002182 -10.518235 -8.066308 -6.8970232 -6.0290041 -5.5014672 -6.6977377 -7.3255515 -8.7876167 -9.7918453 -11.563948 -11.995597 -13.116243 -12.579781][-14.301882 -12.785911 -11.448338 -9.8053074 -8.5812035 -7.7132683 -7.9718118 -8.8285942 -9.64395 -10.324299 -10.036913 -11.073566 -11.54488 -11.301954 -9.9858484][-12.02969 -11.408131 -10.881134 -8.6939373 -7.294569 -7.0400286 -6.8906322 -6.9628382 -7.3372092 -7.9354534 -7.9702291 -7.9545622 -7.4378014 -8.3165493 -7.5979605][-9.6733055 -9.0578423 -8.4416647 -7.6848116 -6.84826 -5.8194261 -4.3955579 -4.8685884 -5.6200962 -5.8143749 -5.9278784 -7.0824919 -6.910326 -5.8957596 -6.0155339]]...]
INFO - root - 2017-12-15 12:57:58.590764: step 14510, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 59h:18m:47s remains)
INFO - root - 2017-12-15 12:58:05.130401: step 14520, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 57h:39m:32s remains)
INFO - root - 2017-12-15 12:58:11.625561: step 14530, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 58h:29m:32s remains)
INFO - root - 2017-12-15 12:58:18.196811: step 14540, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.648 sec/batch; 57h:11m:34s remains)
INFO - root - 2017-12-15 12:58:24.808692: step 14550, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 58h:58m:29s remains)
INFO - root - 2017-12-15 12:58:31.437600: step 14560, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 59h:17m:24s remains)
INFO - root - 2017-12-15 12:58:37.981165: step 14570, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 57h:05m:11s remains)
INFO - root - 2017-12-15 12:58:44.492339: step 14580, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 57h:09m:59s remains)
INFO - root - 2017-12-15 12:58:51.015086: step 14590, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 60h:18m:49s remains)
INFO - root - 2017-12-15 12:58:57.622242: step 14600, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 60h:00m:16s remains)
2017-12-15 12:58:58.177512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0179224 -5.2953453 -5.1515303 -4.1755514 -4.0374317 -3.5487869 -3.3238866 -2.8800144 -2.441124 -2.2303691 -2.056181 -5.204597 -8.0944176 -8.4971466 -8.20474][-4.6825328 -4.6734452 -4.0779357 -3.3289516 -2.9193847 -2.8370955 -2.4399862 -1.6485343 -0.7599535 -0.11209202 -0.30327463 -4.1667151 -7.1368484 -8.6854582 -9.2359314][-3.1412296 -4.5956488 -4.9719076 -3.3359272 -3.1191225 -2.6864228 -2.090853 -1.0627861 -0.38200521 -0.49448347 -0.5176425 -3.7605426 -6.4993954 -8.5869761 -9.6973467][-4.1683307 -4.9317064 -4.9415665 -3.4420397 -3.4867163 -2.5150185 -1.7187274 -1.3958187 -0.68911219 -0.054160595 -0.0914464 -3.6779966 -6.7440724 -8.09981 -8.6001215][-5.1927776 -7.5803123 -7.8288965 -5.7321258 -4.3939056 -1.8500657 -0.84212828 -1.5152564 -1.7678843 -0.99604368 -0.8137989 -3.5962694 -6.4796948 -8.3747435 -9.0424957][-5.5197215 -7.1231661 -6.6621389 -3.7510786 -1.7009411 1.2242675 2.8506293 1.7838197 0.74640131 -0.39915609 -1.5128717 -3.3971899 -5.8382621 -7.7187648 -8.1568851][-7.6087084 -7.1497126 -5.1632433 -2.2153769 0.4585042 3.4465137 5.3653569 4.6250529 3.5909467 0.96782684 -1.2695231 -3.7651317 -6.5933557 -7.7417707 -8.5262518][-7.71222 -7.6623435 -5.8678389 -1.9560144 1.723031 5.0360818 6.8665557 5.3958735 4.2835069 2.4892249 0.51296473 -3.3535895 -6.8476663 -8.2431049 -9.2678165][-6.5896993 -6.1493239 -5.8744993 -2.8772268 -0.14899635 2.8061519 4.5804563 3.8190894 2.8657346 0.95580673 -0.22867155 -3.8683753 -6.736454 -7.9102192 -8.8115263][-7.162199 -6.2950807 -5.4464178 -3.0193145 -1.9189827 0.38828754 2.2533231 2.2599106 1.1379328 -0.71222448 -2.1567616 -5.1583815 -7.3160815 -8.49049 -9.47796][-9.8768539 -9.6407042 -8.8859062 -6.0889745 -5.5718684 -4.222271 -3.0318315 -2.0904706 -2.2845871 -3.0080595 -3.997113 -7.4547224 -9.08304 -9.25391 -9.3285637][-12.334146 -12.313551 -11.725885 -10.072361 -9.1768827 -7.3825288 -6.9377108 -6.8524752 -6.9085445 -6.6908836 -6.906354 -8.5505276 -9.4399614 -9.9293346 -9.7633591][-13.162388 -12.853765 -11.974527 -11.522049 -10.758717 -8.6236305 -8.7346659 -9.4386673 -9.6158276 -9.28498 -8.9113121 -9.6204643 -10.015091 -9.7182064 -9.0339155][-11.867606 -11.523792 -10.512938 -9.3861 -8.5654259 -7.9756355 -8.1894188 -8.2887135 -8.8969269 -9.709197 -9.8439541 -9.2518015 -8.9618874 -9.0084438 -8.8347387][-8.174902 -7.8892322 -7.4097366 -6.3430467 -5.9192133 -5.3026676 -5.4883871 -6.0924964 -6.86312 -7.0673614 -7.7829466 -8.4418869 -9.1912079 -9.4379587 -9.5021706]]...]
INFO - root - 2017-12-15 12:59:04.739618: step 14610, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 58h:17m:29s remains)
INFO - root - 2017-12-15 12:59:11.371754: step 14620, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 60h:22m:28s remains)
INFO - root - 2017-12-15 12:59:17.930439: step 14630, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 58h:56m:34s remains)
INFO - root - 2017-12-15 12:59:24.562369: step 14640, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 59h:17m:30s remains)
INFO - root - 2017-12-15 12:59:31.156881: step 14650, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.661 sec/batch; 58h:24m:03s remains)
INFO - root - 2017-12-15 12:59:37.787450: step 14660, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 58h:26m:26s remains)
INFO - root - 2017-12-15 12:59:44.375887: step 14670, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 57h:26m:18s remains)
INFO - root - 2017-12-15 12:59:50.966452: step 14680, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 57h:22m:35s remains)
INFO - root - 2017-12-15 12:59:57.604519: step 14690, loss = 0.16, batch loss = 0.11 (12.8 examples/sec; 0.626 sec/batch; 55h:16m:26s remains)
INFO - root - 2017-12-15 13:00:04.237634: step 14700, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 57h:15m:41s remains)
2017-12-15 13:00:04.754977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8089213 -5.4050045 -4.5606508 -4.6646166 -5.2835741 -6.97255 -8.0067024 -8.4095926 -8.7923565 -8.5871153 -8.248848 -8.111268 -9.96258 -7.8216858 -4.7124586][-5.941752 -5.048358 -4.73125 -3.9341359 -4.7662892 -5.1808529 -3.8098998 -4.68842 -5.24938 -5.6875815 -5.759716 -5.6329818 -8.3173161 -5.9296117 -3.7635398][-4.8321505 -5.3622041 -4.9075961 -3.2482221 -4.0163727 -3.9399266 -4.1272478 -3.3822834 -3.80008 -4.9033265 -4.0741744 -4.2285771 -5.6614051 -4.5557914 -3.3149836][-6.6770449 -6.7266049 -6.4465547 -5.5751987 -5.252975 -4.375967 -3.9911447 -3.6199708 -4.2232323 -3.612359 -4.1025467 -3.7626281 -5.6572275 -4.9565034 -2.9315217][-7.3961654 -7.6407657 -8.0066576 -6.9876633 -5.6038733 -3.8362589 -3.0219457 -3.2458138 -2.9825852 -2.1587641 -1.9476702 -2.5919132 -5.8591614 -4.9047627 -3.7357538][-8.566968 -7.5561552 -6.5332527 -4.528502 -1.9821439 -0.72053671 0.48931789 0.40574551 -1.2004132 -2.1006942 -2.5851591 -2.2852123 -4.24202 -4.4788504 -3.5820215][-8.1120892 -7.3478541 -5.8147264 -2.1015406 0.87271404 2.8650875 4.3205986 4.2616949 3.2097158 0.44295645 -1.7577195 -1.9525244 -4.5190296 -4.4672565 -3.3711002][-6.1917138 -5.4232249 -4.5772443 -1.3557553 2.743434 5.559123 6.8725691 6.1875248 4.5007877 2.1858249 -0.14312315 -1.7306991 -4.3327541 -3.58987 -2.9409494][-6.632205 -4.8680344 -2.6781332 -0.23395634 1.9049058 4.5160303 5.9943352 4.9712639 2.776968 1.061738 -1.1092544 -2.2842975 -4.7150178 -4.1990228 -2.6520443][-6.5460534 -5.8440928 -4.4096489 -1.8577998 -0.85396147 1.0530567 2.970047 2.2396178 0.30382586 -1.7428081 -2.7892671 -3.1040552 -5.759429 -5.0865035 -3.6335526][-6.8686581 -6.34941 -5.7734795 -4.6264648 -3.3241086 -2.7439346 -2.757313 -3.223227 -4.2409759 -4.9411421 -5.3980274 -5.46122 -6.6375036 -6.5332236 -5.613][-10.990969 -9.3793163 -7.6732812 -6.4537039 -6.0549827 -5.7678785 -5.9903164 -6.9140272 -7.7895918 -7.6673112 -8.18017 -6.633954 -6.424737 -5.9742246 -4.7742867][-11.079401 -9.4466228 -7.1208811 -5.5537314 -5.6105256 -5.7895093 -6.4311113 -6.9692836 -7.7991314 -8.0124989 -7.9782295 -6.4838433 -6.7021751 -5.724937 -4.3869882][-9.4180288 -8.4814529 -6.2573347 -4.4584961 -3.71191 -4.0995164 -4.1823053 -5.1565828 -6.2054515 -6.5787449 -6.5806675 -4.976521 -5.0543466 -4.0322428 -3.1382003][-6.5847645 -5.7646861 -4.5682287 -4.020997 -3.2060149 -2.8150864 -2.9645522 -3.4059279 -3.4479897 -4.5248656 -4.6666842 -4.0607872 -4.3759661 -4.1667004 -4.1002989]]...]
INFO - root - 2017-12-15 13:00:11.364160: step 14710, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 58h:55m:43s remains)
INFO - root - 2017-12-15 13:00:18.010251: step 14720, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 56h:23m:29s remains)
INFO - root - 2017-12-15 13:00:24.550950: step 14730, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 57h:14m:44s remains)
INFO - root - 2017-12-15 13:00:31.076819: step 14740, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 58h:20m:29s remains)
INFO - root - 2017-12-15 13:00:37.714296: step 14750, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 58h:20m:52s remains)
INFO - root - 2017-12-15 13:00:44.308544: step 14760, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 57h:06m:42s remains)
INFO - root - 2017-12-15 13:00:50.864205: step 14770, loss = 0.11, batch loss = 0.07 (12.3 examples/sec; 0.649 sec/batch; 57h:19m:20s remains)
INFO - root - 2017-12-15 13:00:57.475327: step 14780, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 59h:01m:09s remains)
INFO - root - 2017-12-15 13:01:04.083136: step 14790, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 58h:05m:01s remains)
INFO - root - 2017-12-15 13:01:10.701719: step 14800, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 57h:45m:00s remains)
2017-12-15 13:01:11.171140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3573325 -2.1547668 -1.818639 -1.6717644 -2.4905953 -2.0446906 -1.8529961 -2.4385149 -3.2251105 -4.5399885 -5.5807986 -7.6535716 -10.180586 -11.207214 -11.15583][-1.7231855 -2.2747781 -2.0022686 -1.9695427 -2.3036768 -2.6470685 -2.7424746 -2.5645568 -3.5012219 -4.6385794 -5.04424 -7.206439 -9.6364717 -11.156275 -11.846283][-0.17830801 -1.086555 -2.0213277 -3.1392875 -4.0696173 -4.4612193 -4.16354 -4.0513906 -4.217155 -4.5191145 -4.7735519 -6.3812757 -9.2579489 -10.205904 -11.637381][-1.9441624 -3.0496957 -4.1821871 -4.8380613 -5.390862 -6.0922561 -5.7581625 -5.451921 -5.5505614 -5.2324772 -5.1568589 -7.3273582 -9.52498 -11.237266 -12.909922][-2.1371343 -4.2634 -5.605299 -6.265554 -6.5009437 -6.0273809 -4.874877 -4.1972437 -3.7231367 -3.6666911 -4.020432 -6.3712487 -9.1890783 -10.477194 -12.09413][-4.7912965 -4.5187593 -4.6417661 -4.82829 -4.4072676 -3.6920679 -1.9915049 -0.70244455 -0.52944469 -1.1838861 -1.3866315 -3.6929605 -7.0492525 -8.1040306 -9.2815208][-3.9233513 -3.8049028 -3.6037257 -3.1845236 -2.4637635 -0.89963484 1.0214539 2.3768806 3.0397863 1.6385326 -0.1501565 -3.4911687 -6.557642 -7.0264163 -8.2071648][-2.8347449 -2.9120524 -2.63663 -1.6548553 -1.4407253 0.147933 1.9305086 3.1848707 4.0596457 2.9656134 1.2558436 -3.2089086 -6.7742987 -7.7891016 -9.04954][-3.6717083 -3.4043868 -2.6071007 -1.2707877 -1.8308494 -1.6939983 -0.9855895 0.70485258 1.9778638 0.82325888 -0.44992447 -4.753727 -9.1249838 -10.393089 -10.926955][-5.15557 -5.5225167 -4.4990625 -3.2372632 -3.3384211 -2.7886004 -2.3573394 -1.5692406 -0.87039137 -0.75389719 -1.7991221 -6.2492042 -10.086027 -11.793133 -13.798211][-8.060339 -8.49695 -7.4165735 -6.5138707 -6.6656561 -5.9557247 -4.3107014 -3.4203911 -2.9571812 -2.8338318 -3.4770386 -8.1351528 -11.171198 -11.909114 -12.033239][-11.974185 -12.052294 -11.695608 -9.8161592 -8.6022558 -6.560544 -5.5493689 -4.83904 -4.4815054 -5.8829837 -7.1147995 -9.716651 -10.162203 -10.587447 -10.332066][-11.362757 -10.342963 -9.6041031 -8.9797411 -8.701458 -6.5353374 -4.7539506 -4.3807583 -4.8492174 -5.7053747 -6.7972908 -8.5955629 -8.8048019 -7.982439 -7.3985491][-8.2505236 -7.0608559 -7.0007639 -6.1036634 -5.9736395 -6.297936 -5.5214958 -4.3195915 -4.6960459 -5.0983834 -6.0088687 -6.7722454 -6.9149432 -6.9436507 -7.0224638][-6.5293469 -6.0567079 -5.4334278 -3.1733952 -2.1538792 -2.8923895 -3.4328146 -3.7511637 -4.2601118 -4.964354 -5.4335403 -7.2914848 -7.941082 -7.8026857 -7.9678545]]...]
INFO - root - 2017-12-15 13:01:17.706889: step 14810, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 58h:16m:35s remains)
INFO - root - 2017-12-15 13:01:24.270585: step 14820, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 56h:18m:33s remains)
INFO - root - 2017-12-15 13:01:30.847993: step 14830, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 57h:12m:36s remains)
INFO - root - 2017-12-15 13:01:37.412552: step 14840, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 58h:46m:59s remains)
INFO - root - 2017-12-15 13:01:43.986350: step 14850, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.632 sec/batch; 55h:44m:54s remains)
INFO - root - 2017-12-15 13:01:50.491608: step 14860, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 57h:27m:36s remains)
INFO - root - 2017-12-15 13:01:57.047140: step 14870, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 58h:21m:18s remains)
INFO - root - 2017-12-15 13:02:03.622819: step 14880, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.664 sec/batch; 58h:36m:29s remains)
INFO - root - 2017-12-15 13:02:10.120929: step 14890, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 58h:20m:47s remains)
INFO - root - 2017-12-15 13:02:16.682548: step 14900, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 58h:29m:59s remains)
2017-12-15 13:02:17.230938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.382175 -4.7167897 -4.7834358 -3.3882451 -3.9532242 -4.4940567 -5.1142087 -5.4098969 -5.1627574 -4.2399669 -2.7490523 -4.2515783 -6.5796571 -9.4194508 -8.4618511][-3.3494811 -2.6627014 -1.7811394 -0.34185362 -1.4183931 -3.6822321 -4.659564 -5.1149468 -5.2375956 -4.7277956 -4.39033 -6.077167 -8.7944508 -10.776279 -8.9904118][-4.1379447 -3.2742119 -2.2893915 -0.28745365 -1.1201272 -2.477674 -3.1176007 -3.8662109 -4.3810792 -4.6050954 -4.5694208 -6.2032437 -8.5243149 -10.631598 -9.8230658][-3.1790202 -2.4627495 -1.2463713 -0.1577754 -1.2078681 -1.4829559 -2.1605752 -2.7546859 -3.1105113 -3.3661716 -3.9136686 -6.0176039 -7.7179775 -9.4534779 -8.3520107][-2.3539479 -2.3792861 -1.6510267 -0.28046894 -0.2854023 -0.11330128 -0.62997389 -1.5524716 -2.1125195 -2.2908487 -2.7479467 -4.8705211 -6.9581294 -9.1537 -8.0973253][-2.7064674 -1.547904 -0.41504097 1.085835 2.2012362 2.9308386 2.4713497 1.8810763 0.79868746 -0.63463688 -1.5499659 -2.8352911 -3.9315689 -6.6253953 -5.8341246][-3.5162013 -2.0071974 -0.85744095 1.1296601 2.2037005 2.4198828 3.8312616 4.3801765 4.1242924 2.1701241 -0.60272837 -2.4815836 -4.2661762 -5.9947448 -4.9754448][-3.5503995 -1.3374476 -0.019565105 2.3495064 3.0599461 3.2871046 3.7464132 3.74899 4.0946035 2.84507 1.2391467 -1.9364581 -5.5662808 -7.6119037 -6.4175487][-1.6509023 -0.5673027 0.028469563 1.450109 1.321084 2.7178278 3.710063 3.2675953 3.3208933 2.9704723 2.610673 -0.49420738 -4.6780157 -7.4756336 -7.269352][-2.9778297 -1.4645863 -0.79550886 0.13397646 0.15881062 0.76094913 1.2670369 1.9063697 2.7740402 2.6732597 2.4856796 -0.088077068 -3.0351124 -6.5844669 -6.9904461][-4.7451663 -4.3494706 -2.9366443 -1.5215688 -1.3962488 -1.9576085 -2.5447669 -1.1173224 -0.687871 0.30183554 0.96648884 -0.77972364 -3.2636182 -6.3034787 -6.4724545][-9.4741068 -8.1501293 -6.7266817 -5.4164267 -4.4445143 -4.9540563 -5.8320546 -5.3311028 -3.9972024 -2.56321 -1.4901381 -3.1242373 -4.8538365 -6.6080222 -6.66691][-10.555202 -9.4781237 -8.3397532 -7.7653365 -7.841568 -7.2254162 -7.7830143 -8.2677 -7.7025666 -5.5501814 -3.7636905 -3.8213055 -4.1802673 -5.3197274 -6.0765214][-7.8774295 -7.171628 -6.842371 -6.44446 -5.6041822 -5.4314981 -7.1439657 -7.6910224 -8.0099459 -8.1539183 -6.9500775 -5.6709309 -4.8251338 -6.1508226 -6.248702][-4.1016068 -3.6884279 -3.1928594 -2.1743407 -1.9454985 -3.2249727 -4.4259262 -5.3712983 -6.3844123 -6.6987538 -6.8025737 -6.6491442 -6.791204 -7.8830442 -8.6713]]...]
INFO - root - 2017-12-15 13:02:23.749076: step 14910, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 58h:16m:33s remains)
INFO - root - 2017-12-15 13:02:30.339427: step 14920, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 58h:00m:29s remains)
INFO - root - 2017-12-15 13:02:36.864489: step 14930, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 58h:58m:26s remains)
INFO - root - 2017-12-15 13:02:43.301847: step 14940, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 56h:25m:50s remains)
INFO - root - 2017-12-15 13:02:49.845117: step 14950, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 56h:15m:51s remains)
INFO - root - 2017-12-15 13:02:56.426757: step 14960, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 59h:23m:15s remains)
INFO - root - 2017-12-15 13:03:02.935633: step 14970, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 56h:46m:08s remains)
INFO - root - 2017-12-15 13:03:09.451975: step 14980, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 58h:06m:50s remains)
INFO - root - 2017-12-15 13:03:16.064084: step 14990, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 58h:38m:53s remains)
INFO - root - 2017-12-15 13:03:22.568308: step 15000, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 58h:01m:05s remains)
2017-12-15 13:03:23.102911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1866949 -1.1333508 0.51007891 1.6138921 1.2039118 0.004424572 -1.1120009 -2.021328 -2.5447423 -2.4857028 -2.370816 -4.0473208 -5.2562404 -5.88622 -4.4540854][-1.9671829 -0.65618896 1.0326476 1.4638472 1.46142 0.46691322 -0.12897348 -0.79605293 -1.458828 -1.4849634 -0.97716379 -2.3532691 -3.2797463 -3.9137695 -3.5047493][-0.8400178 -0.28472376 0.30388975 1.1726174 0.77962494 -0.23322773 -0.76268244 -1.1174855 -0.65470695 -0.35093927 -0.23258591 -2.2633317 -4.0406752 -4.7649331 -4.31846][-1.6069479 -1.6636691 -1.2808595 -1.1902766 -1.9234509 -2.4048848 -2.1694698 -1.7493243 -0.92873907 -0.13635302 0.11749792 -2.2788928 -4.4464021 -6.1407619 -6.1940212][-2.3359182 -2.8713593 -2.9196651 -2.472893 -2.6318312 -2.5197773 -2.1885417 -1.5635519 -0.5802784 0.33860683 0.46945333 -2.1559339 -4.3883739 -6.5936947 -7.1574469][-3.560185 -3.4717481 -3.0742953 -2.4196737 -2.0909655 -1.3500862 -0.2374258 0.43303204 1.1274238 1.3552022 0.64933205 -2.025053 -4.70621 -6.9058046 -7.0290422][-5.2099247 -4.2361917 -2.6661603 -1.2302399 -0.35466719 0.7144866 2.0507517 2.7742629 3.4816585 2.8702302 1.5128036 -1.5674534 -4.656364 -6.6590419 -6.8777704][-4.3403273 -3.5808287 -1.7561677 -0.040799141 0.5649457 1.4057345 2.8367219 3.189383 4.0809455 3.8457952 2.2366614 -1.9560072 -5.4611244 -7.6069069 -7.6107368][-3.4964 -2.782635 -1.4871874 0.49672461 0.72222281 1.4452691 2.2682638 2.2839932 2.6273961 1.9758987 1.1761093 -2.5025671 -6.3578424 -8.6957817 -8.8080378][-3.6113074 -2.7825198 -2.3128037 -0.43712807 0.013393402 0.61944628 1.1458564 1.3611174 1.8168216 0.90608168 -0.11221695 -3.9037304 -7.2589545 -9.7412033 -10.176045][-7.1057215 -6.2625065 -4.5939541 -3.2309172 -2.740833 -2.45119 -2.53026 -2.9613531 -2.9584255 -3.2597136 -3.69735 -7.1636486 -9.3165522 -10.750703 -10.360676][-10.462357 -9.19357 -7.0213819 -5.0857549 -4.7316179 -4.2070847 -4.7719607 -5.9306393 -6.2359924 -6.6879797 -7.0279555 -9.01745 -9.1422386 -10.507566 -9.9909286][-12.7185 -10.886309 -8.56031 -7.0494142 -6.3963251 -6.1585355 -6.7006531 -7.2536097 -8.14772 -8.9047241 -9.2646217 -10.091789 -9.5058737 -9.7329235 -7.4819536][-10.82802 -9.8307581 -9.0482826 -6.7892842 -5.7173524 -5.9425254 -6.31865 -6.635046 -7.7746325 -8.0902538 -8.3515549 -7.8964577 -6.8200989 -6.5627294 -5.7403131][-7.9528432 -7.443593 -6.6236157 -5.4608345 -5.1417732 -4.9146271 -4.8471408 -5.09197 -5.278101 -5.4242859 -6.3530173 -7.0528774 -6.5175548 -5.824964 -5.3220925]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 13:03:30.640970: step 15010, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.667 sec/batch; 58h:47m:14s remains)
INFO - root - 2017-12-15 13:03:37.196524: step 15020, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 58h:19m:37s remains)
INFO - root - 2017-12-15 13:03:43.790897: step 15030, loss = 0.25, batch loss = 0.20 (12.4 examples/sec; 0.648 sec/batch; 57h:07m:13s remains)
INFO - root - 2017-12-15 13:03:50.386954: step 15040, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 58h:19m:38s remains)
INFO - root - 2017-12-15 13:03:57.019334: step 15050, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 59h:44m:30s remains)
INFO - root - 2017-12-15 13:04:03.580254: step 15060, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 57h:29m:32s remains)
INFO - root - 2017-12-15 13:04:10.146225: step 15070, loss = 0.18, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 56h:09m:21s remains)
INFO - root - 2017-12-15 13:04:16.829065: step 15080, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 60h:16m:20s remains)
INFO - root - 2017-12-15 13:04:23.494139: step 15090, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 59h:27m:41s remains)
INFO - root - 2017-12-15 13:04:30.064337: step 15100, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 57h:28m:33s remains)
2017-12-15 13:04:30.524306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.756197 -9.1615944 -8.4659052 -7.4142213 -7.10627 -7.0965786 -7.0307188 -7.265892 -7.4516239 -7.1481056 -6.4964123 -8.1642971 -9.6881132 -9.0007172 -8.2642527][-6.6020617 -6.2922573 -5.8683467 -5.9767742 -6.9554257 -6.9657927 -6.4144287 -6.3047562 -6.1395369 -6.3851142 -5.9730458 -8.0701675 -10.102694 -9.08931 -8.5533466][-4.2636013 -5.5618281 -6.3768663 -5.2385669 -5.4773374 -5.4361496 -5.5293427 -5.5401564 -6.057703 -6.7469559 -6.5370641 -9.4575624 -11.9593 -11.55571 -11.963785][-5.0613866 -6.0325694 -6.5339541 -5.4415188 -5.3537765 -4.7943287 -4.5144777 -4.4116859 -5.6165457 -6.6670485 -6.9052105 -9.5565367 -11.683571 -11.929602 -12.228476][-5.57748 -7.0484667 -7.2147923 -5.35393 -4.5074196 -2.9331007 -1.6076932 -2.1295626 -3.8223722 -4.9790335 -6.1119723 -10.240306 -12.880143 -13.36324 -13.214542][-6.4962769 -7.5175462 -6.6958518 -4.125864 -2.6279113 0.044018745 2.5483384 2.3931231 0.12857103 -2.7879691 -5.5252662 -8.3098412 -10.17589 -10.84524 -10.334139][-7.8177142 -7.4604564 -5.4252687 -2.0964355 -0.69337416 2.0962358 5.6692071 6.0871167 4.7354779 1.3086805 -3.3741066 -7.1607342 -9.51372 -9.6870527 -8.9305058][-8.3991127 -7.9913731 -5.4330411 -1.5554695 1.1552596 3.4778705 5.688127 5.6921468 5.6839871 2.7994924 -0.9624095 -5.7020454 -9.2513952 -9.024229 -8.502182][-7.89489 -7.10804 -5.1071191 -2.2819903 -0.26758003 1.6473331 3.313313 2.8588891 2.6917329 1.5138602 -0.7053504 -5.9451084 -9.8398666 -10.22225 -10.322184][-6.3821692 -5.799716 -4.7290735 -2.6889467 -1.4131417 -0.44249725 0.42469645 0.039703369 -0.60502481 -1.7413783 -3.4514079 -7.9526949 -11.358479 -11.664169 -11.966406][-10.892193 -10.121786 -9.1619358 -6.9714656 -6.040657 -5.8860726 -5.4315619 -5.8628135 -6.78448 -7.3284979 -8.57642 -12.128611 -13.207264 -12.894558 -12.783899][-12.976128 -12.775548 -11.877491 -10.204266 -9.9472752 -9.9018917 -9.730381 -9.9171276 -10.001051 -10.41136 -11.276294 -13.533396 -13.404636 -12.809239 -12.482035][-15.175268 -13.839306 -11.65584 -10.735397 -11.020771 -10.978054 -11.439692 -11.778454 -12.063364 -12.419706 -12.820249 -13.238604 -12.749378 -11.245211 -9.9081955][-11.544351 -11.653591 -10.725466 -9.0300055 -8.6108618 -8.1628752 -8.7499685 -9.3623466 -10.12463 -10.681733 -10.786057 -9.9835987 -9.9373484 -8.9499769 -7.6105485][-9.2950306 -9.2756758 -10.016893 -7.9694319 -5.7117491 -4.3052444 -4.8830719 -5.5556946 -6.0467272 -6.4149189 -7.1062121 -7.6596384 -7.817482 -7.0648546 -6.141326]]...]
INFO - root - 2017-12-15 13:04:37.195611: step 15110, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.675 sec/batch; 59h:32m:30s remains)
INFO - root - 2017-12-15 13:04:43.721149: step 15120, loss = 0.24, batch loss = 0.20 (12.2 examples/sec; 0.655 sec/batch; 57h:42m:37s remains)
INFO - root - 2017-12-15 13:04:50.307949: step 15130, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 59h:38m:47s remains)
INFO - root - 2017-12-15 13:04:56.912543: step 15140, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 57h:33m:04s remains)
INFO - root - 2017-12-15 13:05:03.466832: step 15150, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 57h:28m:37s remains)
INFO - root - 2017-12-15 13:05:10.107694: step 15160, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 58h:21m:48s remains)
INFO - root - 2017-12-15 13:05:16.683718: step 15170, loss = 0.20, batch loss = 0.16 (11.5 examples/sec; 0.698 sec/batch; 61h:32m:26s remains)
INFO - root - 2017-12-15 13:05:23.259568: step 15180, loss = 0.18, batch loss = 0.13 (11.6 examples/sec; 0.690 sec/batch; 60h:51m:01s remains)
INFO - root - 2017-12-15 13:05:29.870884: step 15190, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 59h:46m:10s remains)
INFO - root - 2017-12-15 13:05:36.421351: step 15200, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 57h:46m:23s remains)
2017-12-15 13:05:37.035034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7679214 -6.3946767 -6.7657804 -5.9106946 -5.5798211 -5.542222 -5.2789445 -4.7879448 -4.7561693 -4.7720137 -4.7855649 -5.9909563 -7.1914964 -7.2590604 -6.6854458][-4.6833644 -5.1954665 -5.6566186 -5.1973743 -5.5512037 -5.9966011 -5.7749581 -5.5030451 -5.8107409 -5.90206 -5.6632371 -6.3406458 -7.0543852 -7.0777254 -6.6328363][-3.3102303 -3.8681684 -4.2909336 -3.8089566 -4.5905795 -5.4818635 -6.0974035 -5.9018621 -5.16071 -5.299108 -5.3880424 -6.8149271 -8.0658693 -8.1901951 -7.906518][-3.1952374 -4.285131 -4.8210406 -4.2985044 -4.62784 -4.1089926 -3.6771364 -3.6065731 -3.9047236 -4.1092553 -4.2613463 -6.7028809 -8.51395 -9.1554451 -9.079915][-3.807559 -4.6047759 -5.5162816 -4.3710451 -2.9538245 -1.850337 -2.01879 -2.2919509 -2.09251 -1.8018284 -3.0223923 -5.2814355 -6.7691731 -8.138464 -8.7604847][-5.3933964 -5.0591955 -4.1788845 -1.8296335 -0.69818687 0.087642193 0.60082912 0.22809172 0.30309486 0.36680079 -0.057763577 -2.1412239 -5.0207057 -6.8319335 -7.5489507][-6.6209555 -5.4848142 -3.9412766 -0.91787767 -0.35471296 1.0801606 2.0604978 1.7651715 2.1351 1.9662046 1.6267653 -0.28856182 -2.8906226 -4.9506383 -5.8915977][-5.0358357 -4.1447992 -2.6314411 -0.19458532 0.17590952 2.1126475 3.6275105 3.2538457 3.1578326 2.2473369 0.94173 -1.0146394 -3.2211967 -4.4085426 -4.8451457][-4.3568068 -3.3907776 -1.3009062 0.32965374 0.24678516 1.3929076 2.6439757 3.4478269 3.9485726 2.6956277 1.0034404 -2.3465738 -4.989068 -5.863234 -5.6748652][-4.22961 -3.10716 -1.1673026 0.44153833 0.79072189 1.1909475 1.6205249 1.9231472 1.789782 1.1158772 -0.24751806 -3.3695261 -5.8123574 -6.8230572 -7.2135983][-6.6560278 -5.154305 -3.5739083 -1.9008293 -1.4632897 -2.38488 -2.7615607 -2.0328412 -1.8288455 -2.8901074 -4.1563015 -6.4229422 -7.7729278 -8.620326 -8.0114174][-9.3749952 -8.9594984 -7.4403543 -5.308919 -5.1253057 -5.55229 -6.1287355 -6.2957397 -6.3076429 -6.0660772 -6.024879 -7.0770941 -7.5872364 -8.3332071 -8.0388994][-11.187319 -10.733705 -8.9435863 -7.0607605 -6.1593218 -5.7596269 -6.3256273 -6.9998593 -7.3244953 -7.1854095 -7.1263294 -6.62464 -5.9140925 -5.9520283 -5.7210054][-11.023311 -10.099579 -8.7161922 -6.6823616 -5.7073593 -5.0969467 -5.7352858 -6.194685 -6.2890549 -6.7524147 -6.5890627 -5.0349379 -4.3296576 -4.0498757 -3.5192833][-7.3423815 -6.9341364 -5.8802462 -5.2797275 -4.1415854 -2.9558997 -3.7071714 -4.6058254 -5.9097371 -6.1689992 -5.128715 -5.1696568 -5.8442421 -5.9385729 -6.1039391]]...]
INFO - root - 2017-12-15 13:05:43.635485: step 15210, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 57h:21m:32s remains)
INFO - root - 2017-12-15 13:05:50.160009: step 15220, loss = 0.13, batch loss = 0.08 (12.7 examples/sec; 0.628 sec/batch; 55h:18m:22s remains)
INFO - root - 2017-12-15 13:05:56.804106: step 15230, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 57h:39m:28s remains)
INFO - root - 2017-12-15 13:06:03.399520: step 15240, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 59h:00m:14s remains)
INFO - root - 2017-12-15 13:06:10.041451: step 15250, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 59h:50m:59s remains)
INFO - root - 2017-12-15 13:06:16.572828: step 15260, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 57h:38m:06s remains)
INFO - root - 2017-12-15 13:06:23.213161: step 15270, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 60h:21m:40s remains)
INFO - root - 2017-12-15 13:06:29.794646: step 15280, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 56h:25m:42s remains)
INFO - root - 2017-12-15 13:06:36.393578: step 15290, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 59h:14m:25s remains)
INFO - root - 2017-12-15 13:06:42.988578: step 15300, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 57h:38m:30s remains)
2017-12-15 13:06:43.510474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.81654 -4.2786179 -3.5049319 -3.38833 -4.9649572 -6.710093 -7.3950682 -7.205019 -6.5402837 -5.090498 -3.7080081 -2.8921096 -4.0291696 -4.5397105 -3.9917893][-3.9944079 -2.131844 -1.642302 -1.8919914 -3.5284958 -5.2111478 -5.2822967 -4.4797392 -3.6368864 -2.938715 -1.4020214 -1.1271076 -2.6502542 -3.9084105 -3.7442856][-0.78686953 -0.96815395 -2.7721169 -2.206511 -2.5508058 -3.3856444 -3.8657217 -3.6861715 -2.5210724 -0.72270727 0.28128719 0.2636652 -1.3040857 -3.5239329 -4.7180562][-2.8195672 -1.7035174 -1.9854629 -2.7027717 -4.2277713 -3.6203928 -3.4308932 -3.489485 -2.5764394 -1.5124269 -0.574759 -1.4879265 -3.6805279 -5.463191 -5.0444179][-4.1480556 -4.0021429 -3.9267955 -3.1610711 -2.5288651 -0.045567036 1.8722715 0.23283005 -0.96705818 -0.43018055 -1.1078372 -2.3845158 -4.42509 -6.04383 -5.7743149][-6.2362828 -5.1479049 -4.9214754 -1.9670556 0.57324171 3.2977448 5.7851105 4.3252869 3.4084501 1.5773721 -1.019979 -2.4005229 -4.7716627 -6.2464318 -5.1567459][-9.7492828 -7.2203693 -4.4799051 -2.173496 0.84493065 5.806241 9.8149719 8.5616808 7.0893688 3.0222492 -0.884542 -2.6756492 -5.5223627 -6.574141 -6.035646][-10.277412 -9.3519764 -7.652112 -2.6514165 0.3427372 5.2255793 9.6724682 8.0442514 6.3106003 2.4535847 -1.1796999 -3.560235 -6.7091241 -8.1851988 -7.5316606][-10.822166 -8.4234247 -7.0756965 -4.2897711 -1.8386168 2.6437712 6.0328531 5.7966909 5.0127163 1.6072354 -2.5101056 -5.6513362 -8.57062 -9.5495272 -8.4016113][-10.510187 -9.7580786 -8.28091 -5.4088788 -3.6185732 -0.19403315 2.9356813 2.9774332 2.4530582 -1.1652961 -4.6399393 -6.4424262 -8.7057238 -10.476166 -9.9585533][-15.723068 -14.458632 -12.8981 -10.276007 -8.5414457 -6.0114493 -3.9400547 -3.4955251 -3.5025764 -5.1546779 -7.0939555 -9.2549582 -10.166962 -9.97113 -8.2034569][-19.519901 -18.0396 -15.487689 -13.745855 -12.784421 -10.380938 -8.713336 -8.8079367 -8.9600964 -9.9178705 -10.360207 -10.346233 -10.704672 -10.311196 -8.70769][-15.20733 -13.882916 -12.446758 -12.08007 -11.716122 -10.127343 -8.9510012 -8.891614 -9.0395079 -9.1655645 -8.9779434 -9.3362713 -8.5836725 -7.7075453 -5.8613992][-11.421249 -10.42477 -8.9277811 -8.0505114 -7.5237441 -7.5295758 -8.1731586 -7.9288993 -7.8725281 -7.8949332 -7.6291962 -7.4161654 -6.9499135 -5.9832692 -5.0562925][-9.2872162 -7.6315084 -5.8088231 -4.2040205 -3.568454 -3.7770226 -3.8514581 -4.4969521 -5.4475789 -5.69291 -5.5712471 -6.3665709 -6.9764476 -7.0506492 -6.7717972]]...]
INFO - root - 2017-12-15 13:06:50.074414: step 15310, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 57h:47m:13s remains)
INFO - root - 2017-12-15 13:06:56.753521: step 15320, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.675 sec/batch; 59h:29m:41s remains)
INFO - root - 2017-12-15 13:07:03.308573: step 15330, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.691 sec/batch; 60h:53m:47s remains)
INFO - root - 2017-12-15 13:07:09.878986: step 15340, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 58h:16m:54s remains)
INFO - root - 2017-12-15 13:07:16.448154: step 15350, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 58h:01m:08s remains)
INFO - root - 2017-12-15 13:07:22.995008: step 15360, loss = 0.21, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 56h:36m:26s remains)
INFO - root - 2017-12-15 13:07:29.523828: step 15370, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 59h:14m:06s remains)
INFO - root - 2017-12-15 13:07:36.222046: step 15380, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 58h:37m:10s remains)
INFO - root - 2017-12-15 13:07:42.842967: step 15390, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 56h:56m:29s remains)
INFO - root - 2017-12-15 13:07:49.421747: step 15400, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 57h:54m:46s remains)
2017-12-15 13:07:49.929913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9854379 -7.5791316 -8.0428553 -8.408988 -8.426836 -8.7251759 -8.484601 -8.6074791 -8.0684958 -7.2348566 -6.1121578 -5.9851046 -7.9007235 -7.5958328 -7.4860353][-3.950254 -5.2387252 -5.5910864 -5.7808824 -6.6822414 -7.2714248 -7.9219508 -8.4801693 -8.3099575 -7.9556446 -6.7321544 -6.6907339 -8.4903269 -9.6092606 -10.04454][-2.5790589 -3.0385613 -3.1169515 -3.9805627 -5.3068018 -5.899035 -6.3179178 -7.2763195 -7.3472939 -7.4987035 -7.4824247 -7.5705976 -9.8111944 -10.575758 -11.350944][-2.9482706 -3.5044212 -3.0243087 -3.6933579 -4.6868286 -5.3827944 -5.91556 -6.3535509 -6.2816849 -6.6926823 -6.6584387 -6.9824896 -9.637495 -10.601202 -10.895663][-3.3229568 -4.4770832 -4.288074 -3.3349633 -3.2382634 -2.9513919 -2.5026278 -4.2028341 -5.6319094 -5.582737 -4.7584887 -5.0956244 -7.2180934 -9.0290966 -10.655106][-3.5823197 -4.008574 -4.3667035 -3.0676963 -1.2767196 0.10870695 1.1867213 -0.021875381 -1.9725206 -3.187242 -3.1674867 -3.1976264 -5.6028762 -6.6101122 -7.284234][-4.4069228 -4.3473253 -3.1301308 -1.0948195 0.56775379 2.6363258 4.3825188 3.680191 2.7029524 -0.26268911 -3.263546 -4.1252971 -5.80632 -6.3760266 -7.1949368][-5.399313 -4.014266 -2.7033536 -0.076934338 1.7552319 3.3667765 5.1154213 4.806941 4.322258 2.3881426 -0.32172966 -3.1294127 -7.9462032 -9.0665436 -9.0590477][-6.4551096 -5.0239215 -2.6012051 -0.84135675 0.24023485 2.0279732 3.5751019 3.9150639 4.9383411 3.7797146 2.0447555 -1.0470276 -8.22228 -12.116078 -13.701904][-6.5331159 -5.8370528 -4.1425 -2.001756 -0.79673719 0.24123001 1.8975 2.2560863 2.1378117 2.9608297 3.0347471 -1.1943684 -7.1990042 -12.189228 -16.37001][-8.8340206 -8.4942522 -7.7981124 -5.7622948 -4.5072675 -3.7318125 -2.7632337 -2.3531322 -2.0003982 -1.4918599 -2.2233369 -4.3662138 -9.4597807 -13.90289 -16.297897][-11.85116 -11.691658 -10.758283 -9.02161 -8.7565937 -8.3560314 -7.6035075 -7.0459146 -6.4923487 -6.100409 -6.4396544 -8.2470875 -12.48163 -14.675555 -15.304774][-13.55796 -12.906292 -12.115099 -11.032028 -10.82729 -10.57401 -10.522547 -9.880928 -8.8246937 -8.0045395 -7.2853942 -9.5277615 -12.837273 -14.950356 -15.675345][-14.545245 -14.559914 -13.392323 -11.690413 -11.154446 -11.328758 -11.844513 -11.35289 -10.718834 -9.5109558 -8.4805584 -9.3013554 -10.596645 -12.876503 -14.4044][-10.912228 -11.755585 -11.468916 -10.551231 -9.3322992 -8.3342457 -8.6894312 -8.7837229 -9.3766041 -9.6138668 -9.3930874 -9.7222309 -9.7905731 -10.862362 -12.272891]]...]
INFO - root - 2017-12-15 13:07:56.466274: step 15410, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 56h:44m:04s remains)
INFO - root - 2017-12-15 13:08:03.049178: step 15420, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 56h:42m:58s remains)
INFO - root - 2017-12-15 13:08:09.622888: step 15430, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 58h:18m:59s remains)
INFO - root - 2017-12-15 13:08:16.236631: step 15440, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 56h:56m:54s remains)
INFO - root - 2017-12-15 13:08:22.863477: step 15450, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 59h:12m:42s remains)
INFO - root - 2017-12-15 13:08:29.403982: step 15460, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 57h:38m:19s remains)
INFO - root - 2017-12-15 13:08:35.960884: step 15470, loss = 0.21, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 57h:36m:40s remains)
INFO - root - 2017-12-15 13:08:42.509304: step 15480, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 58h:40m:18s remains)
INFO - root - 2017-12-15 13:08:49.052458: step 15490, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 56h:05m:01s remains)
INFO - root - 2017-12-15 13:08:55.630300: step 15500, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 57h:13m:16s remains)
2017-12-15 13:08:56.138394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8186226 -4.7893429 -3.8494818 -4.3141928 -4.6755085 -4.69403 -5.6876564 -6.1428576 -5.9556937 -6.0430288 -5.9204054 -7.3393607 -7.1293068 -8.59381 -7.27098][-6.2256479 -5.424768 -5.4610114 -6.2673197 -6.5476031 -7.0506887 -7.463541 -7.7469239 -8.280941 -8.4639225 -8.15313 -9.8008232 -9.756362 -10.450788 -7.644187][-4.7563848 -5.1006184 -6.0899258 -6.9540682 -6.9304838 -7.0744514 -7.2354288 -7.1906662 -7.3257732 -7.7615566 -8.5380955 -10.204666 -9.9286823 -11.660796 -9.7714052][-5.8033791 -5.6911521 -7.7096434 -8.5328026 -8.8749275 -8.0247536 -6.6625834 -7.1030455 -7.8378038 -7.9686255 -8.0556822 -10.405388 -11.57068 -12.873425 -9.9152889][-6.4321117 -6.5833297 -8.0664139 -7.5015297 -6.4059319 -4.4216938 -3.0291467 -4.5205159 -6.1780491 -7.3393388 -8.201622 -9.6779671 -9.7133408 -12.28255 -11.401161][-8.1534519 -7.8240724 -8.586092 -7.2964268 -5.6229973 -1.324276 2.2130828 0.38280249 -2.082926 -5.0033092 -7.4401422 -9.3311462 -10.635461 -12.664966 -10.913175][-9.5138588 -9.1880817 -9.0283508 -6.7085934 -3.3242564 1.8987112 6.1213231 6.7898774 5.5910974 -0.53515339 -6.1310368 -8.6670341 -9.6587181 -12.248524 -11.015881][-9.5530977 -8.9277935 -8.45124 -6.1452618 -2.9037936 1.3128357 5.00238 6.6206927 7.1365256 2.9265943 -2.0049031 -7.0568991 -10.259209 -12.431737 -10.992918][-8.9079742 -8.3486557 -7.7318316 -5.0176926 -2.1278291 1.6812129 4.0837235 4.9343686 5.1105247 2.1491094 -1.1489229 -6.1528788 -9.2645378 -11.393786 -9.9924345][-8.463089 -8.4156418 -8.4451218 -5.2995081 -2.310456 0.6481061 1.8085885 2.9157166 3.2531896 0.21232224 -2.2146542 -5.7268195 -7.7399521 -10.878235 -10.551233][-10.745292 -10.429186 -10.038425 -7.6044855 -5.7860327 -3.5691669 -2.7682991 -2.511982 -3.3932996 -4.4445434 -5.5736394 -9.093832 -10.076324 -10.164656 -7.8149481][-15.124336 -13.713627 -12.454053 -9.6649008 -7.7872262 -7.4050713 -7.7816825 -7.7398615 -8.6795263 -9.7653732 -10.275057 -10.791651 -10.033573 -9.7044706 -7.3969765][-12.534452 -10.805914 -9.2068367 -8.2606926 -8.2200031 -7.2941275 -6.9536815 -8.3455753 -9.6020288 -9.9655056 -9.8563194 -11.034712 -11.253561 -9.5886221 -5.9948077][-8.9530983 -7.6775775 -7.1365991 -6.6774254 -6.45977 -7.1052361 -7.9392767 -7.99732 -8.04147 -8.819787 -9.2995958 -8.3377819 -7.4095316 -7.05066 -6.2665792][-5.6605349 -3.9366186 -1.982837 -2.3198388 -3.5787895 -4.2269592 -4.5094094 -5.5199771 -6.8052149 -6.682344 -6.1797204 -7.1982327 -8.2675591 -8.0064783 -6.8279128]]...]
INFO - root - 2017-12-15 13:09:02.725447: step 15510, loss = 0.32, batch loss = 0.27 (11.8 examples/sec; 0.677 sec/batch; 59h:36m:10s remains)
INFO - root - 2017-12-15 13:09:09.365895: step 15520, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.675 sec/batch; 59h:23m:40s remains)
INFO - root - 2017-12-15 13:09:16.005938: step 15530, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 57h:54m:00s remains)
INFO - root - 2017-12-15 13:09:22.644132: step 15540, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 58h:45m:41s remains)
INFO - root - 2017-12-15 13:09:29.220277: step 15550, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 57h:02m:12s remains)
INFO - root - 2017-12-15 13:09:35.807357: step 15560, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 56h:51m:49s remains)
INFO - root - 2017-12-15 13:09:42.442093: step 15570, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 57h:13m:58s remains)
INFO - root - 2017-12-15 13:09:48.984847: step 15580, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 58h:45m:40s remains)
INFO - root - 2017-12-15 13:09:55.564225: step 15590, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 57h:15m:26s remains)
INFO - root - 2017-12-15 13:10:02.175522: step 15600, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.676 sec/batch; 59h:29m:54s remains)
2017-12-15 13:10:02.685482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.7442245 -9.7060232 -10.648826 -11.177688 -11.639212 -10.994356 -9.0362349 -7.0208821 -7.0156074 -7.6295705 -7.309185 -8.8366747 -10.751247 -10.246348 -10.171707][-9.34795 -8.0752611 -7.2435541 -8.0581512 -9.3340054 -9.549984 -8.0013552 -6.48109 -6.3914909 -6.5857706 -6.8551793 -9.4946175 -11.641756 -11.383398 -11.338289][-5.392765 -6.6783085 -8.35553 -8.2896614 -9.5957794 -9.7634907 -9.1260948 -7.2063031 -6.6731682 -6.9857631 -7.092289 -8.4585838 -10.835684 -11.19551 -11.78441][-6.0078344 -6.0207214 -6.9536448 -8.3645039 -9.8189144 -8.8053608 -7.8348 -7.7944241 -7.83146 -7.1351295 -6.8256254 -8.5129623 -10.606971 -11.747675 -13.423336][-7.9576168 -9.277216 -10.279335 -9.8170013 -8.8339958 -5.7042546 -3.2181892 -5.2741385 -8.7706261 -7.9310207 -6.9864473 -8.3442469 -10.903456 -11.829287 -13.472029][-11.214695 -10.825169 -10.834536 -8.411047 -5.54269 -1.5906825 3.7447581 3.1378236 -1.6070795 -5.5536771 -8.2189856 -7.2694874 -8.4557371 -10.855593 -13.345961][-12.761708 -11.55051 -9.2977428 -4.6093454 -1.7694435 1.9818993 8.5178738 8.4022446 5.0427051 -0.78634214 -5.7428594 -7.4007483 -10.647211 -12.090927 -13.405035][-13.245562 -11.926966 -10.364279 -4.756671 -0.50940037 5.4688106 10.577682 9.1622162 6.8143067 1.5531368 -3.8670459 -6.6774669 -9.8992481 -11.646546 -13.345669][-10.065388 -10.444438 -9.6015072 -5.4713492 -1.8815711 3.2473149 7.6146417 7.5669942 4.6775317 -0.345901 -4.35184 -8.4782686 -12.270633 -12.605766 -13.936186][-7.6580043 -8.4484959 -8.2841215 -6.110455 -3.9062674 -1.0077257 3.8581624 4.6374879 1.3787479 -2.7683196 -6.5801716 -9.5459843 -12.643013 -13.330086 -15.346344][-12.598443 -12.117575 -11.829399 -9.7842541 -8.5442963 -7.7177305 -5.0069056 -4.7239065 -4.82222 -5.7200413 -8.378932 -11.765886 -13.064843 -13.467018 -14.617918][-16.508802 -15.972691 -15.781826 -14.407539 -14.075033 -13.338877 -12.374808 -12.30477 -11.069356 -10.682747 -11.593038 -12.689899 -13.127228 -13.693666 -14.111626][-18.183437 -18.683487 -18.519388 -16.989296 -16.755222 -16.000406 -15.602779 -16.073513 -16.522825 -14.9487 -13.521261 -12.601721 -13.120165 -13.399626 -12.903679][-15.487389 -14.53602 -14.377733 -13.741901 -12.535418 -14.397312 -15.181042 -14.725275 -13.250194 -12.915473 -12.723318 -11.472315 -11.806913 -11.525248 -11.687514][-11.054405 -10.086814 -9.2135487 -8.43294 -7.43605 -8.9515486 -9.1154118 -10.276534 -10.661573 -9.8861523 -9.4701462 -9.6774616 -10.188265 -10.668694 -12.192997]]...]
INFO - root - 2017-12-15 13:10:09.247367: step 15610, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.675 sec/batch; 59h:23m:58s remains)
INFO - root - 2017-12-15 13:10:15.855798: step 15620, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 59h:34m:29s remains)
INFO - root - 2017-12-15 13:10:22.419619: step 15630, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 58h:04m:30s remains)
INFO - root - 2017-12-15 13:10:28.994436: step 15640, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 57h:27m:39s remains)
INFO - root - 2017-12-15 13:10:35.660562: step 15650, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 59h:38m:54s remains)
INFO - root - 2017-12-15 13:10:42.303546: step 15660, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 56h:52m:23s remains)
INFO - root - 2017-12-15 13:10:48.888534: step 15670, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 58h:29m:05s remains)
INFO - root - 2017-12-15 13:10:55.426935: step 15680, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 57h:20m:07s remains)
INFO - root - 2017-12-15 13:11:02.066826: step 15690, loss = 0.22, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 57h:22m:03s remains)
INFO - root - 2017-12-15 13:11:08.747483: step 15700, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.677 sec/batch; 59h:35m:07s remains)
2017-12-15 13:11:09.252705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5840387 -4.4202881 -3.2961247 -1.6301885 -1.4638567 -1.3728018 -1.8650975 -2.6505039 -3.6391459 -4.4091678 -5.1325135 -8.8201141 -12.011187 -12.775606 -13.225236][-3.5083327 -2.2120695 -1.2995191 -0.94560909 -1.3211441 -1.6146669 -2.4546316 -3.4165711 -4.0852518 -4.6001325 -5.1987371 -8.4974422 -11.902615 -12.159266 -12.214808][-1.8332992 -1.4779892 -0.73777819 0.11210394 -0.23335934 -0.91573238 -1.4894648 -2.8726416 -3.4808712 -4.08302 -4.0351858 -7.0896144 -9.7834644 -10.710495 -11.422726][-1.8148899 -1.1162181 0.015510082 0.62123728 0.43610954 0.10986233 -0.75196743 -1.5201507 -2.2228184 -2.1539543 -1.4531827 -4.7560053 -7.6981597 -8.579814 -8.8588381][-1.9554999 -1.565155 -1.4938526 0.19802046 1.231431 1.7112403 1.3838587 0.044147968 -1.108108 -1.2779365 -1.0728779 -3.39858 -5.9610267 -6.8418555 -8.2639055][-4.1469684 -3.0910296 -1.6758566 0.60045052 1.9272275 2.4135075 2.2490306 1.1576042 0.023154736 0.028018951 -0.044764519 -2.5925171 -5.0759525 -5.8307643 -6.8604956][-4.2816715 -3.2128296 -2.0553126 -0.011034012 1.4911075 2.1542554 1.7791338 1.5288615 1.1470356 1.2253881 1.3607693 -0.88869619 -3.3632636 -4.5082884 -5.1339564][-3.7921896 -2.0026522 -0.63584948 1.1939321 2.390275 2.4713764 2.0992851 1.9926419 2.3247986 2.7706327 2.7821388 -0.15808487 -2.5140989 -3.37952 -4.0752954][-3.7682977 -2.1323373 -0.60782194 0.97188711 0.78165388 0.45291805 0.91017818 0.98460293 1.057796 1.5811572 2.6269531 0.02388382 -2.8289962 -3.2982662 -3.5301256][-3.4171114 -2.2809384 -1.1921554 -0.1376276 -0.60496855 -1.3837967 -1.5889297 -1.1298223 -0.22204876 0.18601179 0.33474731 -2.4326081 -4.24739 -3.7269778 -3.6669102][-6.6494579 -5.5856147 -4.1340313 -3.3171809 -4.09657 -4.7006702 -5.3181686 -4.9645929 -4.2945166 -3.2437737 -2.7214782 -5.76037 -6.7310491 -5.2865763 -3.346781][-9.7898455 -8.9810438 -8.3247929 -8.0858345 -8.2466164 -8.0264158 -7.312933 -7.0869889 -6.9852529 -6.5618906 -6.3230467 -8.2906017 -7.8068175 -5.5480423 -3.2541456][-11.536942 -9.9342813 -9.2942934 -8.9414768 -8.4385614 -7.9876256 -7.3294039 -6.4650869 -6.6344953 -7.3924046 -7.6251197 -7.945014 -7.8158817 -5.4297953 -3.0416296][-9.1323414 -8.924221 -8.8135071 -8.0292711 -7.3202915 -6.8450375 -6.5224757 -5.2126231 -4.6320996 -5.3035197 -6.1105161 -6.7961769 -6.6015525 -5.3683858 -4.4045248][-5.6576877 -6.7624583 -6.884541 -6.1901426 -6.0110054 -5.0528212 -3.5332811 -3.7035389 -4.058157 -3.4319847 -3.415061 -4.8940196 -5.5882759 -5.7148361 -6.0956635]]...]
INFO - root - 2017-12-15 13:11:15.841673: step 15710, loss = 0.22, batch loss = 0.17 (11.9 examples/sec; 0.673 sec/batch; 59h:13m:00s remains)
INFO - root - 2017-12-15 13:11:22.478418: step 15720, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 57h:40m:21s remains)
INFO - root - 2017-12-15 13:11:29.099559: step 15730, loss = 0.19, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 56h:29m:34s remains)
INFO - root - 2017-12-15 13:11:35.647472: step 15740, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 55h:39m:27s remains)
INFO - root - 2017-12-15 13:11:42.295383: step 15750, loss = 0.14, batch loss = 0.10 (11.4 examples/sec; 0.701 sec/batch; 61h:42m:15s remains)
INFO - root - 2017-12-15 13:11:48.939558: step 15760, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 56h:55m:27s remains)
INFO - root - 2017-12-15 13:11:55.630998: step 15770, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 58h:50m:07s remains)
INFO - root - 2017-12-15 13:12:02.249920: step 15780, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 60h:12m:09s remains)
INFO - root - 2017-12-15 13:12:08.829020: step 15790, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 57h:08m:02s remains)
INFO - root - 2017-12-15 13:12:15.430647: step 15800, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 59h:23m:15s remains)
2017-12-15 13:12:15.894898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.5178604 -7.0464 -6.8571262 -5.4317551 -4.4564209 -4.5859933 -4.62994 -4.77353 -5.4605479 -5.7395253 -5.2761426 -7.161531 -8.1411629 -7.4921064 -6.2653117][-7.3971105 -6.88774 -4.8559117 -4.2322373 -4.1761036 -3.8671358 -3.1952012 -3.0655315 -3.0503001 -3.4242594 -4.1314158 -5.9188538 -6.5480294 -7.340652 -6.2209568][-6.1852889 -6.7975817 -6.3460269 -4.4019966 -3.1722512 -2.6484144 -3.2059448 -2.9866688 -2.9219744 -3.2316756 -3.6135483 -5.8969026 -7.205883 -7.1115794 -6.2030468][-4.2092834 -4.9952636 -5.4274426 -4.8337588 -4.1547928 -3.4736822 -3.0787468 -3.0152144 -2.3812888 -2.8821206 -3.2310181 -5.3132129 -6.6586533 -6.7571712 -5.7105379][-4.7305183 -5.8489246 -5.0740047 -3.8953376 -2.7593637 -1.7313523 -1.7067571 -2.6284754 -2.3941302 -2.4382951 -3.1992822 -5.10113 -5.5289588 -5.1831474 -4.6279068][-6.7824149 -5.8175459 -4.8332973 -3.6155589 -1.4824 -0.19495249 -0.12598991 -0.36243677 -0.59140253 -1.4828105 -2.0090842 -3.6802783 -4.5227437 -4.1012526 -2.3222327][-6.0386214 -5.9212279 -4.8986077 -2.6748977 -0.11457205 1.9790845 2.8958206 2.4363785 0.88474274 0.061038017 0.24945545 -2.2341478 -3.6681011 -3.2969832 -2.0525212][-5.0406208 -4.131712 -2.0989072 -0.39667654 0.49606705 2.1716566 3.1429863 2.7504625 1.6629758 0.991734 0.57087326 -1.3068643 -2.4769447 -2.0238013 -0.32790518][-4.0011783 -2.4492605 -0.54809952 0.316226 0.86652327 1.622714 1.6238289 0.48734426 -0.18870687 0.087266922 0.06742382 -1.678081 -2.7973585 -2.8076336 -1.0275321][-3.4733202 -2.6459875 -1.0334754 -0.15186977 0.20359707 -0.013622761 -0.55637789 -1.7040782 -2.9024432 -2.201684 -1.2013202 -3.29767 -4.395237 -3.8095922 -2.0037396][-5.3936357 -4.7336359 -3.0633321 -2.3833051 -2.677052 -2.7898879 -2.7292478 -3.4468431 -4.1437249 -4.4156456 -4.0922694 -5.6268373 -6.2883949 -5.1359916 -3.9569757][-9.453618 -9.3580723 -7.6126118 -5.8076382 -5.3695445 -5.9220376 -5.3413429 -5.0960078 -5.33334 -5.5766196 -6.0875616 -7.2998 -7.6275654 -7.1313109 -5.4060669][-10.402382 -9.9736671 -8.1079483 -6.6493125 -6.3368115 -6.2762866 -6.8718538 -7.2587724 -6.7120581 -6.5285068 -7.1287084 -8.37755 -8.2970171 -6.805305 -5.3524237][-6.6500845 -6.8553267 -6.29831 -5.2280793 -4.6865044 -4.5899487 -4.9204612 -5.5824413 -6.34673 -6.5591598 -6.6071181 -6.9219732 -6.5904169 -6.0379524 -5.3525882][-3.9799972 -3.1271873 -3.5370522 -3.0529437 -2.6942096 -3.2071631 -3.3747597 -3.7773204 -4.2185249 -4.8486652 -5.2337017 -5.0454788 -5.1475616 -5.6612763 -5.7749863]]...]
INFO - root - 2017-12-15 13:12:22.510748: step 15810, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 58h:41m:44s remains)
INFO - root - 2017-12-15 13:12:29.053697: step 15820, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 57h:04m:41s remains)
INFO - root - 2017-12-15 13:12:35.709984: step 15830, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 59h:04m:56s remains)
INFO - root - 2017-12-15 13:12:42.281058: step 15840, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 57h:14m:47s remains)
INFO - root - 2017-12-15 13:12:48.917513: step 15850, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.681 sec/batch; 59h:53m:17s remains)
INFO - root - 2017-12-15 13:12:55.523674: step 15860, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 57h:19m:46s remains)
INFO - root - 2017-12-15 13:13:02.122396: step 15870, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 57h:28m:59s remains)
INFO - root - 2017-12-15 13:13:08.762392: step 15880, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 57h:00m:18s remains)
INFO - root - 2017-12-15 13:13:15.359418: step 15890, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.677 sec/batch; 59h:33m:55s remains)
INFO - root - 2017-12-15 13:13:21.982544: step 15900, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 56h:06m:59s remains)
2017-12-15 13:13:22.531283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3285189 -6.1722422 -4.0342374 -2.0238173 -0.85791111 -1.5875525 -2.8822963 -4.0871258 -4.6075859 -6.045301 -6.268918 -8.6795959 -11.351612 -10.758612 -9.4204636][-5.078105 -5.179224 -5.4012084 -3.73107 -2.5294867 -1.9806793 -1.2933745 -2.7967255 -4.1942964 -5.3472505 -6.6292853 -8.9412174 -10.108152 -11.074526 -10.645988][-4.6705027 -5.8184457 -6.0531507 -4.4476175 -4.0407643 -2.7274373 -1.8673017 -2.2650037 -3.4981418 -4.1150689 -4.4467416 -7.6653671 -10.374075 -9.9370031 -10.13126][-3.6156635 -6.0896807 -7.5698042 -7.3992643 -6.6702933 -4.5327015 -1.7186766 -0.79756784 -2.3172035 -4.7316475 -7.0783386 -9.9919987 -11.194869 -10.825027 -9.7963591][-5.1075492 -6.4420981 -7.5630903 -7.7052855 -5.6216669 -2.1555982 -0.29368544 -0.54491377 -1.7675879 -3.0389168 -4.9594479 -9.3187647 -12.247894 -12.950118 -12.494127][-6.0345893 -5.2444735 -5.1839619 -5.680059 -2.9573696 1.5121379 5.8518968 5.645164 1.758554 -2.5469062 -6.1664271 -8.68203 -10.811769 -12.58935 -12.972172][-7.5443134 -7.4654074 -7.0985379 -4.4475312 -1.233665 3.347858 7.94869 9.52733 7.3479581 1.2512093 -5.3577623 -7.2130046 -9.234107 -10.574787 -11.791695][-7.1809478 -8.8133011 -8.5676 -4.6485696 -0.30932093 5.0611434 9.948328 9.45282 7.136766 2.8852973 -2.5417142 -7.74111 -9.9567051 -9.1090565 -9.2111626][-7.2583828 -8.6522388 -7.4100885 -5.5131249 -3.2035275 3.0604491 7.6063867 7.92744 5.8593488 -0.023103714 -3.9484849 -8.738142 -11.913395 -12.144245 -10.877035][-6.0113945 -7.3238487 -7.4069262 -5.5343604 -3.40493 -0.26104164 1.5899649 3.3851905 2.6972585 -2.843677 -8.1296415 -12.501759 -14.670448 -13.809628 -12.871243][-9.5299606 -10.879997 -9.8779268 -7.7997694 -6.5524688 -4.6190376 -4.6959615 -4.01136 -5.1103859 -7.6852279 -10.539686 -14.319897 -17.716908 -16.175291 -14.106363][-13.016107 -12.965793 -12.875477 -10.895578 -8.5293369 -7.14441 -7.3704691 -8.226737 -9.77575 -11.522589 -12.645936 -15.323627 -15.534224 -14.308605 -12.767926][-14.74444 -13.787695 -12.45399 -12.685917 -12.287189 -10.866791 -9.8116608 -9.9638491 -10.195992 -10.830904 -10.97817 -12.005564 -12.782263 -11.390957 -9.1045542][-10.958529 -10.170532 -10.349386 -10.758125 -10.362917 -9.9889917 -9.5103922 -9.1464062 -8.8822613 -9.3978615 -9.2804871 -7.5951977 -7.3817163 -7.9304781 -7.9349222][-5.6593094 -5.9933119 -4.3724418 -3.9203939 -5.5092921 -7.1973104 -7.505754 -6.3546548 -5.993506 -6.396719 -6.1327305 -7.2343411 -7.6374264 -7.0050855 -7.1861715]]...]
INFO - root - 2017-12-15 13:13:29.070733: step 15910, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 57h:55m:26s remains)
INFO - root - 2017-12-15 13:13:35.567134: step 15920, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 58h:43m:59s remains)
INFO - root - 2017-12-15 13:13:42.190936: step 15930, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.675 sec/batch; 59h:21m:16s remains)
INFO - root - 2017-12-15 13:13:48.717813: step 15940, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 58h:02m:48s remains)
INFO - root - 2017-12-15 13:13:55.316820: step 15950, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 56h:05m:44s remains)
INFO - root - 2017-12-15 13:14:01.872823: step 15960, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 56h:39m:30s remains)
INFO - root - 2017-12-15 13:14:08.448259: step 15970, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 57h:57m:27s remains)
INFO - root - 2017-12-15 13:14:15.010947: step 15980, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 58h:22m:32s remains)
INFO - root - 2017-12-15 13:14:21.542704: step 15990, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.648 sec/batch; 56h:56m:17s remains)
INFO - root - 2017-12-15 13:14:28.143218: step 16000, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 57h:53m:53s remains)
2017-12-15 13:14:28.710086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.5505047 -7.8909168 -7.0922432 -4.8270245 -4.1761112 -1.7767842 -0.40882921 -0.51707458 -0.28742027 -0.78253651 -1.7450328 -4.8943186 -6.3232265 -7.0705366 -5.882555][-6.1067834 -4.647131 -3.3671398 -1.9805214 -1.5227547 -1.3919449 -0.7843647 -0.665689 -0.38108015 -0.85861731 -1.6813059 -5.7841249 -8.89507 -9.0986261 -7.3038869][-3.4033344 -2.1540906 -0.93299484 0.21996689 -0.3530302 -0.15429068 0.088013649 -1.2566853 -1.0150638 -2.9447608 -4.682272 -8.0645924 -9.5534382 -10.509129 -8.3888083][-3.1874971 -1.9269814 -0.077517033 1.8420205 0.9833436 0.44229031 0.46658897 0.05245018 -0.73629856 -2.5945661 -4.0413742 -9.1962509 -11.274501 -11.278933 -9.377553][-3.4937096 -3.0416267 -1.7278655 0.3483448 1.3804693 2.1215339 1.975049 1.3560686 0.25480652 -1.4726286 -2.7761862 -7.3728008 -9.625351 -11.631191 -10.199217][-3.7275217 -2.8942327 -1.0615396 0.85922289 1.9459934 3.1898823 3.2438269 2.1062799 0.17039156 -1.359405 -2.5600674 -7.7125258 -9.2031832 -10.84847 -9.8010168][-5.140285 -2.3029447 0.2524457 2.5192347 3.7839174 4.3645372 3.80132 3.4522648 2.0624728 -0.10198307 -1.2326951 -6.1946025 -8.5881538 -10.701363 -9.2104292][-6.0409827 -2.9729986 -0.23496675 2.7037621 4.2765474 5.1223249 5.1235075 4.1201372 1.9498315 1.0985518 -0.2156167 -4.9217954 -5.9809532 -7.2936215 -6.0515156][-5.3684759 -3.0102315 -0.59941244 2.2076859 3.5862608 3.6529665 3.7363796 3.3498201 1.9878983 0.761137 -0.78020763 -4.8086944 -5.4716854 -5.6135731 -3.5742168][-6.0958571 -4.3157058 -2.4277189 0.68984318 1.584836 2.3671665 2.3366051 1.8078566 1.2792797 0.044633865 -0.94893217 -4.5008569 -5.4680648 -5.2544436 -2.785444][-10.787586 -8.837553 -6.4418902 -3.8770385 -3.0096054 -2.272733 -2.2128575 -2.4920869 -2.1090331 -2.1270237 -2.4844882 -5.0714717 -5.10245 -3.8889804 -0.83882189][-15.266091 -13.245173 -10.209832 -7.5046353 -7.62527 -6.8329458 -6.4051089 -6.210381 -5.592907 -5.8588738 -6.1545434 -7.40349 -7.5216775 -5.649507 -2.0057516][-15.99184 -14.528719 -11.80637 -8.6927814 -7.31571 -6.8862929 -7.0059276 -7.6854095 -7.7211995 -6.7761269 -6.5446916 -6.874753 -6.7945676 -5.9058847 -3.5024185][-13.866598 -13.33954 -11.164778 -8.9203119 -7.4544291 -6.6656137 -6.6049161 -6.4514122 -5.8586421 -5.7054553 -5.6685777 -5.2521648 -4.396903 -4.5808487 -3.9568496][-9.6086512 -9.0366888 -8.7192144 -7.9349222 -7.352088 -6.1342154 -5.8645244 -5.4232035 -5.6310277 -4.8618355 -4.9529533 -4.7010603 -4.1209621 -4.7850256 -3.62381]]...]
INFO - root - 2017-12-15 13:14:35.282912: step 16010, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 59h:11m:28s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 13:14:41.874315: step 16020, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 57h:36m:28s remains)
INFO - root - 2017-12-15 13:14:48.447432: step 16030, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.692 sec/batch; 60h:52m:21s remains)
INFO - root - 2017-12-15 13:14:55.056088: step 16040, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 56h:35m:14s remains)
INFO - root - 2017-12-15 13:15:01.696563: step 16050, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.692 sec/batch; 60h:52m:21s remains)
INFO - root - 2017-12-15 13:15:08.195420: step 16060, loss = 0.11, batch loss = 0.06 (12.7 examples/sec; 0.631 sec/batch; 55h:26m:45s remains)
INFO - root - 2017-12-15 13:15:14.856048: step 16070, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 59h:54m:18s remains)
INFO - root - 2017-12-15 13:15:21.429769: step 16080, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 58h:29m:45s remains)
INFO - root - 2017-12-15 13:15:28.044185: step 16090, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 59h:36m:48s remains)
INFO - root - 2017-12-15 13:15:34.571796: step 16100, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 55h:59m:58s remains)
2017-12-15 13:15:35.071690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.75902 -7.9299083 -7.7994256 -8.1903343 -8.9389439 -9.2990608 -9.2710781 -9.1305466 -8.97493 -8.5943871 -7.9613986 -8.4817457 -11.240306 -11.505886 -10.335543][-7.4957142 -8.36096 -8.4126911 -8.2496176 -9.10247 -9.9782047 -9.9044294 -9.012104 -8.14444 -7.820611 -7.9573317 -9.0668926 -11.325965 -12.45893 -12.444124][-6.6025867 -7.15881 -7.343298 -8.3398275 -9.2111893 -9.5029984 -8.67599 -7.7811871 -7.3904142 -8.4318905 -8.8557205 -9.2555389 -11.478931 -11.933676 -11.78772][-6.551899 -6.6932049 -5.8709936 -5.5888166 -5.6458697 -5.8015866 -5.5742955 -5.8504119 -6.2797947 -6.4894347 -7.1971469 -8.7851086 -10.780651 -10.428984 -10.102779][-6.7893381 -6.302844 -5.516273 -4.3956332 -2.9372923 -1.485374 -0.47407627 -1.8475323 -3.945806 -5.6987491 -7.107533 -7.9545269 -10.355464 -10.582487 -10.061274][-7.0072308 -6.9893608 -5.3919492 -3.1988266 -2.1764257 1.596971 4.6537609 3.097106 0.23300648 -2.7706065 -4.5168 -5.7072048 -8.2781172 -9.1688519 -9.7540894][-7.1424441 -7.9704866 -6.0905666 -3.3275664 -1.6248894 1.7475362 5.3440208 6.8369865 6.1924343 1.5752816 -2.6860924 -4.32569 -6.936213 -7.799715 -7.6329536][-7.46599 -8.1885347 -6.2443218 -2.6129711 0.022130013 3.5006304 6.8273869 7.1017027 5.9117556 2.4524651 -1.2116165 -3.9989848 -6.5306907 -6.9070497 -6.522593][-5.0782008 -6.0964236 -5.2144 -2.8378491 -0.33953094 2.2880259 4.5333843 5.5286913 5.2743587 3.2209682 0.52206182 -2.7467997 -6.5029616 -7.3848209 -6.8957882][-3.5709622 -4.59431 -3.9299853 -2.0617275 -0.5415926 0.81738806 2.1096387 2.0648665 1.4510784 0.15340662 -2.2908125 -4.9046822 -7.3157187 -8.9010372 -9.277667][-7.8936968 -8.1147079 -7.159306 -5.7538781 -5.1965208 -4.5546579 -4.1302671 -4.1914153 -4.3023467 -4.3587608 -5.5569978 -7.8673153 -10.575687 -10.587973 -9.5281448][-13.159849 -13.083923 -11.168831 -9.5639029 -8.7552395 -9.1639442 -10.658422 -11.326168 -11.54025 -11.015046 -10.716404 -10.97617 -11.733014 -11.560911 -11.195213][-14.349317 -14.096192 -12.418536 -12.668772 -12.178097 -11.245667 -11.449638 -12.636374 -12.974875 -12.532309 -11.496443 -10.941464 -11.175825 -10.005925 -8.5290833][-12.012205 -13.313085 -13.595789 -12.370384 -11.479013 -11.559076 -11.875895 -11.623541 -10.639838 -10.23827 -9.49816 -8.7746553 -7.8753672 -7.9135818 -8.2552977][-8.9655514 -9.9905109 -10.302979 -9.929203 -9.1257324 -7.5241885 -7.1034174 -6.970129 -6.3978834 -6.0845819 -6.1592555 -7.1253738 -7.545011 -6.8208261 -6.4951172]]...]
INFO - root - 2017-12-15 13:15:41.686510: step 16110, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 56h:48m:57s remains)
INFO - root - 2017-12-15 13:15:48.282245: step 16120, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 58h:50m:21s remains)
INFO - root - 2017-12-15 13:15:54.874594: step 16130, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.664 sec/batch; 58h:19m:43s remains)
INFO - root - 2017-12-15 13:16:01.435200: step 16140, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 57h:32m:40s remains)
INFO - root - 2017-12-15 13:16:08.071610: step 16150, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 58h:11m:34s remains)
INFO - root - 2017-12-15 13:16:14.703835: step 16160, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 58h:03m:57s remains)
INFO - root - 2017-12-15 13:16:21.320189: step 16170, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 56h:14m:11s remains)
INFO - root - 2017-12-15 13:16:28.075056: step 16180, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.683 sec/batch; 60h:00m:21s remains)
INFO - root - 2017-12-15 13:16:34.721923: step 16190, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 58h:58m:26s remains)
INFO - root - 2017-12-15 13:16:41.338796: step 16200, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 57h:55m:57s remains)
2017-12-15 13:16:41.846349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.6596794 -9.2222567 -8.6148872 -7.0164537 -7.1090794 -6.9965463 -6.7446518 -6.5164189 -5.6859365 -6.1352181 -7.2361336 -7.6997795 -7.528553 -8.1187611 -7.7908688][-9.4665728 -9.0392456 -8.1712732 -7.1003847 -6.7253971 -6.4290185 -5.9068375 -5.400269 -4.9876213 -5.0764351 -5.1446924 -6.4671078 -6.9591107 -6.9546251 -6.2846975][-7.7452035 -8.099453 -8.645771 -7.90926 -7.9386606 -7.2857881 -6.5171213 -5.8806567 -4.7669206 -4.6302848 -4.9136152 -5.1866574 -5.7341175 -7.2047796 -7.1833658][-6.5642443 -6.3030057 -6.52446 -6.651391 -6.8076715 -6.0739679 -4.6881609 -4.0351138 -4.195497 -4.0503926 -4.096859 -4.752265 -4.8726482 -6.9090791 -6.8108482][-6.2254057 -6.3989296 -6.1523404 -4.6618638 -4.6668983 -3.6200004 -3.0724044 -2.343204 -2.5452425 -3.1217489 -3.8020308 -4.2855196 -4.2588892 -5.9808693 -6.6957631][-6.9346046 -6.542088 -5.40121 -3.0955305 -1.7864845 -0.27115679 1.2653632 1.3486652 0.17745686 -1.286252 -2.7697988 -3.2658639 -3.2444441 -4.9077225 -5.8846893][-5.44467 -4.6957507 -4.568254 -2.3045475 0.082416534 1.7046423 3.4208627 4.3029389 4.0885434 1.1469526 -2.3319232 -3.6799078 -3.6496053 -4.7226515 -5.2162042][-5.2754455 -4.7821841 -4.3925424 -3.047369 -1.4709778 1.9128351 4.9902911 4.86204 4.4408689 2.2459002 -0.40751791 -2.7591047 -4.9529486 -6.9154677 -6.844533][-6.6297874 -4.6437168 -3.3116832 -2.564116 -2.8952782 -0.35981989 2.2705169 3.2206187 3.452765 1.7682962 -0.10107374 -2.8721836 -6.2338853 -9.2761173 -10.253531][-5.33232 -6.2026572 -5.4584265 -3.1428354 -2.5110517 -1.6952996 0.61078691 1.7772017 0.9556489 -0.48480368 -1.7985132 -3.5117137 -5.629971 -10.147129 -12.71302][-7.1609983 -7.5384822 -8.2022829 -5.9541011 -5.0003519 -4.9289088 -4.0621896 -3.1123793 -2.5899091 -3.1775029 -4.6070275 -6.4300494 -8.9334164 -11.792759 -12.550631][-9.0669584 -8.2703714 -9.1593657 -8.7825 -8.8822222 -9.0355988 -8.0652494 -7.6712408 -6.6746902 -6.5417075 -7.7306714 -9.3243713 -10.267421 -12.017218 -12.566154][-11.577858 -9.9282131 -8.7244616 -8.8375978 -9.6229086 -8.5820227 -8.2632055 -8.3774195 -8.6262817 -8.7923708 -8.8954248 -9.20181 -10.579329 -11.218584 -10.740162][-9.9843063 -9.4079647 -7.8459568 -6.772213 -7.4536624 -7.6444077 -7.629096 -7.6234064 -8.6140728 -8.1518583 -7.7486248 -7.0821438 -6.7576847 -8.0761261 -9.2826309][-5.8337121 -7.0145683 -5.2748194 -3.637651 -3.5985286 -4.59799 -4.7816167 -5.2178121 -6.1728535 -6.1945968 -7.0132885 -6.7423019 -7.03712 -7.1185551 -7.3814898]]...]
INFO - root - 2017-12-15 13:16:48.426061: step 16210, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 59h:35m:15s remains)
INFO - root - 2017-12-15 13:16:55.011609: step 16220, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 57h:31m:57s remains)
INFO - root - 2017-12-15 13:17:01.647398: step 16230, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 58h:50m:55s remains)
INFO - root - 2017-12-15 13:17:08.241805: step 16240, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.654 sec/batch; 57h:25m:55s remains)
INFO - root - 2017-12-15 13:17:14.831083: step 16250, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 58h:48m:16s remains)
INFO - root - 2017-12-15 13:17:21.402759: step 16260, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.671 sec/batch; 58h:58m:03s remains)
INFO - root - 2017-12-15 13:17:28.106476: step 16270, loss = 0.19, batch loss = 0.14 (11.8 examples/sec; 0.676 sec/batch; 59h:21m:55s remains)
INFO - root - 2017-12-15 13:17:34.796633: step 16280, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.681 sec/batch; 59h:47m:22s remains)
INFO - root - 2017-12-15 13:17:41.577668: step 16290, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 59h:25m:31s remains)
INFO - root - 2017-12-15 13:17:48.204302: step 16300, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 57h:44m:39s remains)
2017-12-15 13:17:48.742310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2663083 -5.1964626 -4.3204212 -3.8918378 -4.6483078 -5.5799184 -6.52172 -6.81826 -7.0580258 -7.0646691 -6.9876928 -8.8798437 -9.45577 -10.634048 -10.361476][-3.9112768 -4.5988231 -4.2781944 -3.3593335 -4.1101489 -5.6444006 -6.94163 -7.98077 -8.53865 -8.5303574 -8.4105644 -10.335732 -10.859617 -11.893609 -11.165192][-3.363323 -3.6718414 -3.8685541 -4.0876279 -4.7732582 -5.3234725 -6.1373711 -7.0646935 -7.8762813 -8.1387091 -8.1672611 -9.44454 -10.463827 -11.612073 -11.205881][-4.7989335 -5.3337712 -4.9089928 -4.4992633 -4.6129518 -4.8219337 -4.7866569 -5.6644597 -6.5582848 -7.0207963 -7.3005538 -9.1894894 -10.148186 -11.725573 -11.713848][-5.3044353 -5.6229157 -5.5847235 -4.2386703 -3.2660909 -1.7223964 -0.81302929 -2.5380356 -3.9920292 -4.7217426 -5.2495189 -7.4056211 -9.2331238 -10.935387 -11.511101][-6.4011497 -6.0446911 -4.7983532 -3.2074106 -2.1720974 0.400537 2.3083348 2.4324899 1.7499719 -1.065166 -3.3861241 -5.7553225 -7.6225209 -9.4939041 -10.271286][-7.1261659 -6.8488712 -5.47454 -2.3084228 -0.5653944 2.0788293 5.4714561 5.6427283 4.9567933 2.061058 -1.3441591 -5.3077793 -7.8111553 -9.0862074 -9.1505671][-6.7796493 -6.2493672 -4.6535053 -1.6551843 0.21654415 3.2934127 6.7199683 6.6390662 6.1979232 3.5917544 -0.28581047 -4.669373 -7.0338731 -9.2237358 -9.8279371][-4.5896592 -3.9241691 -2.919734 -0.82641077 0.55392694 2.970222 5.45206 5.269793 5.0465932 2.1030726 -1.3072371 -5.9543753 -8.83643 -10.047485 -9.6369972][-2.7874093 -2.5426729 -2.1249228 -0.28128767 0.34098196 -0.01861763 1.5292511 2.5927482 2.8327079 0.081030369 -2.7717183 -6.6747136 -8.4217787 -9.5447979 -9.3968782][-7.9018946 -6.9360113 -6.3117962 -5.3623347 -4.777998 -4.2749691 -4.0331831 -4.2759466 -3.5157437 -4.2607336 -5.9783788 -8.6363945 -9.4805565 -9.9112206 -9.0782976][-10.655006 -9.4716988 -8.5146093 -7.97549 -7.5043817 -7.0360861 -6.71865 -7.4531403 -7.4757752 -7.3733435 -7.6991158 -8.2136927 -7.4566216 -8.3956814 -8.8269758][-10.330183 -9.0576305 -8.1339045 -8.2008076 -7.9944005 -7.6072259 -8.0254459 -8.2772808 -7.9742546 -7.6429243 -7.4059463 -7.5433021 -7.02981 -6.62567 -6.701086][-8.9106874 -7.4166627 -7.0771518 -6.6293931 -5.9497728 -6.9277 -7.2909317 -6.9448261 -6.6446152 -6.772222 -7.1374063 -7.1119776 -6.3871512 -6.2866216 -6.71726][-7.0935454 -6.3563814 -6.0107207 -5.7550216 -5.0985065 -5.3975081 -5.7446136 -5.9064789 -5.78503 -5.1282954 -4.8407078 -5.7300172 -6.0704207 -6.6800427 -7.4477105]]...]
INFO - root - 2017-12-15 13:17:55.357412: step 16310, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 58h:50m:27s remains)
INFO - root - 2017-12-15 13:18:02.041702: step 16320, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.692 sec/batch; 60h:45m:14s remains)
INFO - root - 2017-12-15 13:18:08.737182: step 16330, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 58h:55m:02s remains)
INFO - root - 2017-12-15 13:18:15.450995: step 16340, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 58h:15m:51s remains)
INFO - root - 2017-12-15 13:18:22.150279: step 16350, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 57h:43m:27s remains)
INFO - root - 2017-12-15 13:18:28.722310: step 16360, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 57h:28m:16s remains)
INFO - root - 2017-12-15 13:18:35.361845: step 16370, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 59h:56m:03s remains)
INFO - root - 2017-12-15 13:18:41.989521: step 16380, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 57h:59m:22s remains)
INFO - root - 2017-12-15 13:18:48.565058: step 16390, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 55h:49m:45s remains)
INFO - root - 2017-12-15 13:18:55.227219: step 16400, loss = 0.19, batch loss = 0.15 (11.8 examples/sec; 0.681 sec/batch; 59h:46m:01s remains)
2017-12-15 13:18:55.735700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9974041 -7.0910196 -5.870327 -4.908494 -4.3449936 -4.1604052 -4.3274155 -4.4541221 -4.13284 -3.0148995 -2.9201124 -4.9395728 -6.5481138 -6.4228125 -6.6937428][-5.8876648 -4.7245836 -2.7275186 -2.1225541 -1.8327441 -1.6892695 -1.3914695 -1.3560247 -1.9980938 -2.8701386 -3.0442817 -4.1997213 -6.2888832 -6.1202826 -5.8907623][-2.5474696 -2.774235 -1.9081988 -0.55376434 -0.36466408 0.27478504 0.70960951 0.31429482 0.054731369 -0.0692296 -0.46655893 -2.2877848 -3.9515457 -4.0784993 -5.0399518][-1.9524722 -1.5418935 -0.48625135 0.05044651 -0.19637775 0.47141838 0.50539064 0.30932331 -0.19111013 -0.69002867 -1.0605011 -3.05565 -4.4605112 -4.1015596 -4.4381437][-1.6891465 -2.3498633 -1.7379403 -0.24817467 0.56506062 1.2466679 1.6723695 0.36212015 -0.77495146 -0.30864334 -0.39428425 -3.1345265 -5.0713973 -5.4075513 -6.5464106][-3.812377 -3.5961642 -2.6246376 -0.94405031 0.28701067 1.2445512 1.728395 1.2599688 1.1436205 0.61058569 -0.01030302 -2.2951603 -4.5695648 -5.2898445 -6.3905983][-5.6947627 -4.7248964 -2.6939838 0.067352295 0.74056911 1.7035894 2.7527571 2.8347006 2.6714106 1.1472101 0.12733126 -2.2747173 -5.1000051 -5.8050451 -7.1965566][-6.1059966 -5.2635269 -3.2826667 -0.6653657 0.080555916 1.6493349 2.5170202 2.1655021 1.9238701 1.3102021 1.0054121 -3.0334756 -7.0849504 -7.8945594 -8.960638][-5.1029649 -5.1732697 -4.0047169 -1.5539351 0.10233641 1.0182228 1.1350665 1.4107618 2.1212425 1.2998548 0.32808161 -3.5109174 -7.0606017 -8.4071541 -9.8801622][-5.1112504 -4.70703 -3.3972774 -1.7030625 -0.61792183 0.24543953 1.0303411 1.2039132 0.21472073 0.23052263 0.65510464 -3.3362148 -7.4741516 -9.2630634 -11.284657][-6.401989 -5.5933661 -4.4964428 -2.8954659 -2.1906991 -1.9641469 -1.5816941 -1.9935317 -2.7085776 -2.563117 -2.3384912 -6.3612614 -8.9793129 -9.9617691 -10.95413][-11.268276 -9.407279 -7.420403 -7.1175842 -7.5187912 -7.7961707 -7.098505 -7.1754227 -7.1371355 -6.6980705 -7.5042653 -9.6974926 -10.878618 -10.727961 -11.588362][-11.617887 -10.401459 -9.641037 -8.9142761 -9.3839769 -9.9916525 -9.9478416 -9.7862854 -9.6088839 -9.8494072 -9.8213177 -10.236214 -11.45533 -11.057349 -11.099527][-12.441974 -12.335766 -11.278492 -9.0229969 -8.7498226 -10.559401 -10.787281 -9.8900928 -9.5834274 -9.2778625 -9.2727633 -10.286729 -10.763224 -9.3574371 -9.0710421][-10.359452 -10.131644 -10.088419 -10.859831 -10.292381 -8.464426 -6.7165422 -7.6844134 -9.2888412 -8.9004221 -8.799964 -9.1491852 -9.7332153 -10.413967 -10.57131]]...]
INFO - root - 2017-12-15 13:19:02.342705: step 16410, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 57h:15m:35s remains)
INFO - root - 2017-12-15 13:19:08.990328: step 16420, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 57h:16m:08s remains)
INFO - root - 2017-12-15 13:19:15.648284: step 16430, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 58h:44m:47s remains)
INFO - root - 2017-12-15 13:19:22.209831: step 16440, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.652 sec/batch; 57h:15m:09s remains)
INFO - root - 2017-12-15 13:19:28.808346: step 16450, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 57h:03m:10s remains)
INFO - root - 2017-12-15 13:19:35.375541: step 16460, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 58h:19m:56s remains)
INFO - root - 2017-12-15 13:19:42.028049: step 16470, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 58h:21m:36s remains)
INFO - root - 2017-12-15 13:19:48.539450: step 16480, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.680 sec/batch; 59h:42m:11s remains)
INFO - root - 2017-12-15 13:19:55.161783: step 16490, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 57h:45m:05s remains)
INFO - root - 2017-12-15 13:20:01.684871: step 16500, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 57h:17m:07s remains)
2017-12-15 13:20:02.222910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8137331 -7.2400236 -9.9072933 -11.138456 -13.261524 -12.483883 -10.750968 -9.0905571 -8.3632479 -8.977725 -7.6854444 -7.9902711 -11.442791 -14.074612 -13.997933][-3.7817445 -5.3745008 -7.8335867 -9.3922548 -10.651475 -10.734989 -10.308159 -8.3764067 -7.8157325 -7.866106 -7.040906 -6.7385015 -10.105762 -13.066885 -13.763529][-2.4845243 -3.8864613 -7.0884666 -8.94063 -10.034275 -9.6815777 -9.4764385 -8.5411148 -7.3361678 -6.2638779 -6.4027615 -6.858274 -10.170669 -12.544556 -12.891918][-3.7434635 -4.284842 -6.1439037 -7.1241841 -8.400959 -6.2646422 -5.5145864 -5.5610662 -5.7179751 -5.20255 -5.3796644 -6.9080009 -10.301073 -12.239986 -12.946396][-4.2569385 -4.4058456 -6.2174797 -7.1584778 -7.0641451 -3.5025027 -0.63604355 -0.47606373 -1.9000998 -3.5612664 -3.989325 -4.0901566 -8.244833 -12.40114 -14.544444][-6.4740562 -6.2500458 -6.3975353 -5.5221672 -4.1836171 -1.0483193 2.7278557 3.8951569 2.2037454 -0.18627071 -1.8761506 -2.183568 -5.9785213 -9.5870724 -12.509466][-5.3055539 -3.9052165 -4.4182229 -3.696661 -1.4490256 1.5082183 5.9732094 7.6448307 6.1440687 4.0992832 1.5405345 0.043164253 -4.0869417 -7.0690246 -8.9248428][-5.8592277 -4.8137145 -3.57655 -3.5202515 -1.9308531 2.7556353 7.4609175 9.2165585 8.1178551 5.2925158 3.3082266 1.4871359 -3.55756 -6.3908477 -7.7092581][-5.04479 -4.2790027 -3.5185726 -2.6270165 -2.3058579 1.0145965 5.7675514 7.9914951 7.5949669 3.2558479 0.87206841 -0.38905382 -5.7176065 -9.2146082 -10.963097][-6.0010719 -4.8719039 -4.1897497 -2.2731233 -2.1639495 0.790349 3.0550585 4.4714003 3.5380726 0.84075594 -2.578613 -4.5547519 -8.7296486 -12.032974 -13.23497][-10.457685 -8.6328449 -6.7539244 -4.4297767 -4.2471476 -3.9737778 -2.9114473 -2.2193942 -2.9988732 -4.7461014 -6.9632435 -9.6758728 -14.455425 -16.162556 -15.19615][-11.096508 -9.5891418 -7.5497971 -5.7845764 -6.129621 -6.1620626 -6.3493614 -6.3029761 -7.6567378 -9.55242 -11.249058 -11.673982 -14.522827 -16.95583 -14.815971][-11.807943 -9.9292784 -7.5948925 -6.8694344 -7.39248 -7.4794741 -8.1764364 -8.9387026 -9.8429794 -10.824739 -11.883597 -12.108607 -12.725109 -14.423721 -13.258402][-11.372082 -10.827904 -9.6125555 -8.369276 -7.0564461 -7.2078657 -7.5539927 -7.5637684 -6.7691259 -8.6802406 -9.6020451 -9.1299353 -9.44514 -10.562128 -9.6906528][-8.0540218 -6.7023716 -6.26536 -5.6761308 -4.9689474 -5.4368415 -5.9183564 -6.6002946 -7.4217091 -7.233634 -6.8864965 -7.5148005 -9.5770817 -9.9636974 -10.017902]]...]
INFO - root - 2017-12-15 13:20:08.819361: step 16510, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 59h:10m:45s remains)
INFO - root - 2017-12-15 13:20:15.408322: step 16520, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 57h:53m:35s remains)
INFO - root - 2017-12-15 13:20:22.060394: step 16530, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 60h:09m:01s remains)
INFO - root - 2017-12-15 13:20:28.590676: step 16540, loss = 0.15, batch loss = 0.10 (12.8 examples/sec; 0.624 sec/batch; 54h:45m:02s remains)
INFO - root - 2017-12-15 13:20:35.249635: step 16550, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 57h:01m:54s remains)
INFO - root - 2017-12-15 13:20:41.852884: step 16560, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 56h:29m:42s remains)
INFO - root - 2017-12-15 13:20:48.471121: step 16570, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 58h:19m:44s remains)
INFO - root - 2017-12-15 13:20:55.086316: step 16580, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 57h:56m:02s remains)
INFO - root - 2017-12-15 13:21:01.696472: step 16590, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 59h:25m:44s remains)
INFO - root - 2017-12-15 13:21:08.294124: step 16600, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 56h:16m:15s remains)
2017-12-15 13:21:08.768286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7402954 -4.0787725 -4.5305705 -4.8240094 -5.921382 -6.576282 -6.1480894 -6.428133 -7.1415038 -7.8622642 -7.5050182 -7.6236038 -8.7100649 -10.463043 -10.879877][-2.8897803 -2.2376366 -3.17193 -4.5226645 -5.3947492 -5.485323 -4.8317561 -4.6557426 -5.3897424 -6.2284455 -6.7909436 -7.1413016 -7.7771039 -11.034361 -11.469358][-1.6181283 -2.6932919 -4.4896755 -5.40731 -6.752162 -6.7132506 -6.373446 -5.4874778 -5.2750993 -5.3045611 -5.2548218 -5.8254514 -7.32468 -10.249784 -11.179823][-4.2943325 -4.8735752 -6.6137094 -8.07789 -9.4066715 -8.332901 -6.6896782 -6.0393128 -5.17599 -4.1087751 -4.2653017 -4.7427273 -6.4264431 -9.33311 -10.294006][-4.3918209 -7.4173212 -9.9714775 -10.100113 -10.275146 -7.1073017 -3.8870778 -3.7721133 -4.5909495 -3.8639147 -3.7174177 -4.4406586 -6.9969044 -10.432623 -11.79417][-4.2376962 -6.646987 -9.3705235 -10.119843 -9.5115128 -4.2459116 1.9309187 2.9638276 1.0553703 -1.3960133 -2.509891 -2.3710272 -4.8099875 -9.0374537 -11.539323][-3.2248149 -4.8827977 -7.0441523 -7.8111062 -6.0397229 -0.76207161 6.069056 8.523407 8.0614319 2.1296277 -2.8472016 -3.8573279 -5.1402006 -7.8637447 -9.47264][-3.8390522 -4.48418 -5.5189338 -4.8929076 -4.2280989 0.42030191 6.8767104 10.104662 10.542143 6.1698842 1.228488 -2.7203715 -6.6672821 -8.6821556 -9.1988573][-5.2292995 -5.831377 -5.547565 -2.9488907 -2.3304574 0.23508263 2.9651065 5.2841115 6.2164445 3.5790467 -0.54712343 -3.2404525 -6.90705 -10.289574 -11.010638][-9.81608 -10.672764 -10.448619 -7.7758145 -4.5116549 -0.72500134 1.7524447 1.6405687 1.5974417 -0.98710966 -3.3924243 -5.0683756 -8.751 -12.977674 -13.860941][-12.611007 -13.055797 -11.947551 -10.390638 -9.8085213 -8.1039543 -5.0126271 -2.9289665 -2.1244063 -3.9839129 -6.8337555 -8.1905613 -10.905252 -12.814447 -13.171818][-18.335703 -15.812437 -13.058245 -10.031261 -9.16601 -8.98509 -8.77993 -7.0983992 -4.954958 -5.3179088 -6.9296527 -8.029705 -9.36605 -11.360506 -11.714952][-14.550394 -14.691063 -12.311182 -9.4885025 -8.1915321 -6.8900514 -7.4603324 -7.2595067 -5.8219137 -5.4388032 -6.5549707 -7.7659659 -7.4281368 -7.7631464 -7.32051][-8.0946083 -8.2771463 -7.6509352 -7.981555 -7.7427034 -6.9934487 -7.0089064 -6.4440737 -5.8050456 -5.4358969 -5.7380204 -5.2406564 -4.3622003 -5.8255529 -6.0485659][-4.9743834 -3.4003162 -3.1588709 -3.4532061 -4.4264426 -5.3989544 -5.1660781 -5.0188589 -5.436758 -5.6413064 -5.3335996 -5.8190517 -6.8314633 -6.1684823 -7.2693853]]...]
INFO - root - 2017-12-15 13:21:15.266206: step 16610, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 56h:11m:17s remains)
INFO - root - 2017-12-15 13:21:21.766604: step 16620, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 58h:05m:27s remains)
INFO - root - 2017-12-15 13:21:28.412067: step 16630, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 57h:56m:41s remains)
INFO - root - 2017-12-15 13:21:34.952191: step 16640, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 56h:47m:24s remains)
INFO - root - 2017-12-15 13:21:41.538799: step 16650, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 59h:24m:37s remains)
INFO - root - 2017-12-15 13:21:48.131824: step 16660, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 57h:42m:50s remains)
INFO - root - 2017-12-15 13:21:54.725833: step 16670, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 58h:20m:30s remains)
INFO - root - 2017-12-15 13:22:01.325285: step 16680, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 56h:53m:11s remains)
INFO - root - 2017-12-15 13:22:07.904654: step 16690, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 58h:04m:03s remains)
INFO - root - 2017-12-15 13:22:14.513485: step 16700, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 58h:31m:22s remains)
2017-12-15 13:22:15.028346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9564314 -9.9577246 -10.577875 -10.737735 -10.702334 -9.5606117 -8.0906353 -7.1047211 -7.2705936 -7.9944305 -8.3541 -9.3105812 -10.280654 -9.1971264 -9.655386][-7.178956 -8.3642845 -8.4514456 -8.652956 -8.7782021 -7.9136868 -7.0867014 -6.0869026 -6.0842409 -6.6077819 -7.0408497 -9.2159939 -11.067064 -10.335281 -10.027147][-6.1295023 -7.6289043 -8.7027864 -7.7121034 -8.16335 -8.2709093 -8.30041 -7.050817 -6.4325476 -6.7009873 -6.7721992 -8.7375956 -10.605322 -10.383325 -10.691813][-7.6431284 -8.4116812 -9.2449713 -8.6814156 -8.0242882 -6.0546927 -5.999547 -6.9173889 -8.1604576 -7.2269363 -6.5181246 -8.157937 -10.007696 -10.715378 -11.814724][-9.4854021 -12.377174 -12.237843 -9.697732 -6.8677206 -2.8038156 -1.3478127 -5.7309508 -9.89526 -8.5938444 -7.0468206 -8.0682735 -9.4839926 -10.945229 -12.609642][-10.185631 -12.62409 -12.728193 -8.6964741 -4.2368207 0.98178339 4.7683291 1.0009155 -3.9817343 -7.0377331 -9.5441685 -7.5309634 -7.8914037 -9.1840019 -11.565247][-12.742004 -12.952019 -11.3895 -5.5428329 -0.98512077 3.6303353 8.1439095 6.6675248 3.5971227 -2.9332032 -7.7997613 -7.2896595 -9.2190247 -10.327005 -11.385851][-12.625689 -12.249462 -10.709225 -6.3132129 -1.1605358 6.113111 10.590324 8.2042637 5.3803964 0.10442209 -4.8339653 -6.7213964 -9.5024195 -10.218184 -11.693909][-9.6837883 -9.1528721 -8.7294149 -6.5692234 -2.7783864 2.856771 7.4037137 5.9381709 3.0182853 -1.0597415 -4.9355593 -8.086071 -10.331633 -10.526651 -12.071831][-6.9277668 -6.9982438 -6.1915207 -4.9834371 -3.895123 -1.581852 1.4721699 2.4848733 0.52871084 -4.6435847 -8.0038719 -9.9637632 -12.30231 -12.017517 -13.445906][-10.654261 -10.452007 -10.049637 -8.4169846 -6.6755137 -6.8549142 -6.2839079 -5.3594441 -5.78996 -7.3310781 -9.1592464 -12.230278 -13.948502 -12.761431 -12.789924][-13.114994 -13.801109 -13.350554 -12.221189 -11.11141 -11.16226 -10.991304 -10.583965 -9.815011 -10.336413 -11.250286 -12.563591 -12.906942 -12.675972 -12.475243][-15.821798 -15.652378 -15.999537 -15.798437 -14.751507 -13.415528 -12.911356 -13.562557 -14.029034 -12.835794 -12.109963 -12.332616 -12.185528 -11.495934 -10.174534][-15.231651 -14.710608 -13.521578 -11.809093 -10.956944 -12.125871 -12.949694 -12.221407 -11.36237 -10.831223 -11.000031 -10.51089 -10.528032 -9.948698 -9.0347939][-9.9794445 -9.8083963 -9.0175056 -7.692698 -6.8397193 -6.8367586 -6.5150862 -7.8990221 -8.6325512 -8.5968742 -8.2172966 -8.8777924 -9.6307421 -8.9285955 -8.5972767]]...]
INFO - root - 2017-12-15 13:22:21.602419: step 16710, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 57h:07m:28s remains)
INFO - root - 2017-12-15 13:22:28.144048: step 16720, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 58h:09m:19s remains)
INFO - root - 2017-12-15 13:22:34.669914: step 16730, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 56h:44m:00s remains)
INFO - root - 2017-12-15 13:22:41.249646: step 16740, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 57h:19m:26s remains)
INFO - root - 2017-12-15 13:22:47.790639: step 16750, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 58h:44m:42s remains)
INFO - root - 2017-12-15 13:22:54.309021: step 16760, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 57h:18m:06s remains)
INFO - root - 2017-12-15 13:23:00.868054: step 16770, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 59h:04m:38s remains)
INFO - root - 2017-12-15 13:23:07.538915: step 16780, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 59h:28m:32s remains)
INFO - root - 2017-12-15 13:23:14.104461: step 16790, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 56h:38m:52s remains)
INFO - root - 2017-12-15 13:23:20.737674: step 16800, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 57h:50m:25s remains)
2017-12-15 13:23:21.234183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0873847 -5.8307133 -4.5747552 -2.987442 -2.9895034 -3.1265388 -3.4288678 -3.7021797 -4.5934744 -5.2269344 -5.7274365 -9.17304 -9.8697834 -9.5083275 -8.188612][-5.0703211 -3.5990784 -2.9532764 -1.9962885 -2.5195928 -3.4811342 -4.0496082 -4.1770844 -4.90633 -5.5624967 -6.0973153 -9.6282654 -10.573425 -10.718608 -10.3368][-4.2109742 -3.8528962 -2.8589251 -1.056252 -1.3937802 -2.358438 -3.3312097 -4.21 -4.5347757 -5.0443754 -5.7610736 -8.8531761 -9.3008642 -9.4838657 -8.9279079][-4.0080914 -2.7831535 -1.8208289 -0.9415884 -1.1030231 -1.9395163 -2.9545553 -3.15238 -3.3174624 -3.2866597 -3.3515959 -6.928154 -7.8663912 -7.6243482 -6.576436][-5.039969 -4.2468152 -3.3339162 -1.5068579 -0.68477726 -0.50035 -1.3601599 -1.9918911 -2.805757 -2.5002453 -1.8837619 -4.5451865 -4.7773104 -5.6764035 -6.0708947][-6.6348228 -5.4389849 -3.6020789 -1.6447487 -0.71683168 -0.07781601 -0.25065851 -0.74555016 -1.0706434 -0.91428947 -0.86408663 -3.8397875 -4.3207994 -4.70326 -4.3044529][-6.9836421 -5.6902733 -4.1273632 -1.6234932 -0.45221186 0.37974977 0.45805931 0.86935329 1.0883679 1.2094932 1.3236198 -1.6840177 -2.1563504 -2.7358177 -3.4436705][-5.6340342 -4.3248444 -2.3686569 -0.47482967 0.24226856 0.50817156 0.99889374 1.9196734 2.4425211 2.2817907 2.4808092 -0.49679947 -1.6412187 -2.4852824 -2.9917943][-5.8030825 -4.6319966 -3.1251097 -0.60002804 0.31685305 0.85278654 1.2005739 0.58193779 0.88966084 1.0929961 1.3964539 -1.2425141 -2.0277469 -2.4878049 -2.7852154][-5.5174017 -4.1690869 -2.493784 -0.50227928 -0.70895767 -0.64919996 -0.09387064 -0.34262943 -0.19010973 -0.51756477 -0.8454318 -3.8825455 -3.5308735 -3.0260293 -3.6702206][-7.6571417 -6.3215361 -4.2662077 -2.7329879 -2.881382 -3.1760802 -3.1928444 -3.4652228 -3.3174684 -3.1986187 -2.219398 -5.1525621 -5.8295331 -5.2364845 -3.5550573][-9.3685846 -8.6012 -7.4674168 -6.6971917 -6.5100846 -6.0812564 -5.8773451 -5.6438141 -5.4006076 -4.979126 -4.5752707 -5.913991 -5.1144271 -4.4799061 -3.8439267][-10.674606 -9.6824541 -9.0720959 -7.9414959 -7.770391 -6.9406924 -6.3380852 -6.1161637 -6.1494279 -5.864543 -5.5169683 -5.4150782 -4.8399534 -3.7219896 -2.5769622][-8.2848186 -8.1948519 -7.511857 -6.67537 -6.312768 -6.2031355 -5.3711987 -4.8004861 -4.8206482 -4.7810111 -4.8525362 -4.4875464 -2.8668444 -1.8585186 -1.7853701][-5.6272039 -5.8594632 -6.6657209 -5.3582826 -4.1541986 -3.2651091 -2.61214 -2.6536784 -2.865905 -2.7905281 -2.8222356 -3.4999957 -3.8913784 -3.5810485 -3.3049436]]...]
INFO - root - 2017-12-15 13:23:27.785020: step 16810, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 57h:29m:18s remains)
INFO - root - 2017-12-15 13:23:34.326061: step 16820, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 56h:40m:58s remains)
INFO - root - 2017-12-15 13:23:40.788881: step 16830, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 56h:40m:35s remains)
INFO - root - 2017-12-15 13:23:47.347131: step 16840, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 57h:03m:25s remains)
INFO - root - 2017-12-15 13:23:53.913107: step 16850, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 58h:04m:58s remains)
INFO - root - 2017-12-15 13:24:00.563701: step 16860, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 58h:03m:51s remains)
INFO - root - 2017-12-15 13:24:07.106791: step 16870, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 57h:51m:53s remains)
INFO - root - 2017-12-15 13:24:13.732178: step 16880, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 57h:13m:37s remains)
INFO - root - 2017-12-15 13:24:20.327638: step 16890, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 57h:12m:10s remains)
INFO - root - 2017-12-15 13:24:26.942793: step 16900, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 57h:47m:00s remains)
2017-12-15 13:24:27.503417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.975157 -5.416749 -7.1786132 -7.6448135 -8.1407757 -8.2460213 -8.6254568 -8.6167984 -8.6206837 -9.5430536 -9.0238829 -9.0071745 -11.737822 -11.455856 -11.497547][-2.8701766 -4.7042246 -6.8012209 -7.8556638 -9.1051016 -8.2952518 -7.2698503 -7.5208955 -8.1581078 -8.4320345 -7.8520393 -8.7490749 -11.146352 -11.334417 -10.857201][-1.6457176 -5.1585236 -8.4233675 -9.0889292 -9.1490612 -8.4038315 -7.8672981 -7.3362565 -8.5991764 -9.6011419 -7.9801841 -7.5526657 -10.791523 -10.630296 -10.492462][-4.6386938 -6.71279 -10.617539 -11.937765 -11.734418 -8.5119572 -6.0063677 -6.6014886 -7.76338 -8.74836 -9.0496531 -9.3581667 -11.483643 -11.890581 -11.818962][-8.2448425 -12.162495 -15.142159 -15.687862 -12.686348 -5.6772027 -1.934233 -4.931108 -8.6480417 -8.9021921 -8.7243633 -9.8619928 -13.187021 -14.205879 -15.047421][-10.000099 -14.923979 -17.056103 -15.36808 -10.917564 -3.2825377 3.1097598 2.544086 -1.0517983 -5.5887532 -9.4888449 -8.6610031 -12.376043 -14.427269 -14.820858][-10.78675 -12.644448 -13.474819 -10.299326 -4.0873156 2.7940145 8.8320732 9.0773163 6.6293426 -1.1688766 -6.8627162 -7.601975 -12.289227 -13.693471 -14.396515][-10.7737 -12.89091 -12.598883 -9.0381565 -1.5429745 6.9752793 12.587725 11.228649 8.8720455 2.7808948 -2.625386 -6.3108969 -12.093773 -13.062836 -13.450585][-7.9571857 -9.6630287 -11.18829 -8.5719061 -2.108253 5.46343 10.80477 9.7650089 5.542604 -0.38764381 -4.2554169 -7.623755 -12.852836 -13.43274 -13.420555][-6.5936818 -8.3354187 -9.7465591 -9.273901 -6.4016633 -0.6729846 4.560164 5.8228812 2.1693029 -3.8522873 -7.429935 -9.31615 -14.119045 -14.995523 -14.912535][-8.958848 -10.753607 -12.610635 -11.85231 -10.219713 -9.6288471 -6.3770618 -5.3282428 -6.4484596 -8.0584488 -10.518673 -13.210012 -15.912233 -15.569239 -15.033068][-14.561783 -14.886646 -15.476961 -15.476486 -14.335697 -14.053169 -14.05909 -13.848087 -12.720163 -13.569408 -15.237209 -15.364109 -16.156418 -15.081127 -14.699001][-15.810633 -17.073648 -17.774027 -16.570049 -16.01 -16.14217 -15.658133 -16.037336 -16.082607 -14.547915 -13.674295 -14.92358 -15.998709 -14.158539 -12.149468][-15.858727 -16.306349 -16.462616 -14.910349 -14.007757 -14.239904 -15.140768 -14.179733 -13.441816 -13.276641 -13.27666 -12.466585 -12.803595 -11.775664 -10.412383][-11.081181 -11.793411 -10.542622 -10.191767 -9.8964787 -9.3261871 -9.4118681 -9.1373138 -9.2856369 -8.9954348 -9.42116 -10.687668 -11.329138 -11.552595 -12.311697]]...]
INFO - root - 2017-12-15 13:24:34.054087: step 16910, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 57h:24m:19s remains)
INFO - root - 2017-12-15 13:24:40.582518: step 16920, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 57h:52m:56s remains)
INFO - root - 2017-12-15 13:24:47.075045: step 16930, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.630 sec/batch; 55h:12m:54s remains)
INFO - root - 2017-12-15 13:24:53.601488: step 16940, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.630 sec/batch; 55h:10m:50s remains)
INFO - root - 2017-12-15 13:25:00.136571: step 16950, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 55h:53m:21s remains)
INFO - root - 2017-12-15 13:25:06.742420: step 16960, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 58h:42m:29s remains)
INFO - root - 2017-12-15 13:25:13.360577: step 16970, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 56h:35m:37s remains)
INFO - root - 2017-12-15 13:25:19.903995: step 16980, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 56h:34m:37s remains)
INFO - root - 2017-12-15 13:25:26.466535: step 16990, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 57h:53m:41s remains)
INFO - root - 2017-12-15 13:25:32.984356: step 17000, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 58h:02m:13s remains)
2017-12-15 13:25:33.474781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2793441 -8.396533 -8.33067 -8.2368126 -8.91535 -9.9797926 -10.094699 -9.1020327 -8.0082417 -6.5764666 -5.4248562 -6.6575933 -7.9375906 -9.700633 -9.8676081][-7.7861176 -9.009428 -8.6308136 -8.167532 -8.4615955 -10.403929 -11.303031 -11.059605 -10.490542 -9.899353 -9.2491159 -10.738251 -11.242985 -11.078518 -9.5490541][-6.9766827 -8.3570089 -8.4183083 -8.0197573 -8.5022736 -9.2560225 -9.9577141 -10.07856 -9.638114 -9.5241432 -9.4685278 -11.500832 -13.279537 -13.511923 -12.083608][-10.640334 -11.325445 -10.652694 -9.5374126 -8.9730883 -8.5472937 -8.2525873 -8.4992046 -8.39688 -8.739624 -9.8825188 -12.516106 -14.138531 -14.211094 -12.886169][-11.497966 -13.840416 -14.008228 -9.91383 -5.7577929 -3.8371611 -3.6490624 -5.4894218 -6.906395 -7.784564 -8.6762085 -11.137486 -13.686407 -14.597679 -13.898512][-13.216976 -14.291155 -13.499378 -9.5683432 -4.3882327 1.7995877 5.659306 2.6912808 -1.284801 -4.56365 -7.0246391 -10.041838 -12.532519 -13.346195 -13.070193][-12.677353 -13.102215 -11.094088 -6.4164906 -2.5607927 4.3721118 10.196667 7.985518 4.6184993 -1.5360465 -6.48437 -8.3973227 -10.198344 -11.835662 -11.274683][-14.309708 -13.100101 -10.532824 -6.0373125 -0.84485531 5.4572072 10.101374 9.2845173 7.1412911 0.11933661 -6.0288992 -9.1719265 -11.007748 -10.442242 -8.7695789][-12.761232 -12.96945 -11.304672 -6.9906869 -2.8119442 2.2423835 5.2959857 4.9228749 3.3933015 -2.0481491 -6.0509906 -9.4366884 -12.750096 -11.54159 -8.7849922][-10.205715 -10.796278 -11.257753 -8.7049007 -5.8534632 -2.2597995 1.5265956 1.772913 -0.37364435 -4.4229932 -7.3357816 -11.304535 -13.613813 -13.253099 -12.140441][-12.173485 -13.711899 -14.222021 -12.496641 -10.331274 -7.1784983 -4.345717 -4.097991 -5.1164289 -7.8615837 -10.218207 -13.102436 -13.752525 -13.346079 -11.891731][-16.431168 -17.026964 -16.730621 -14.673822 -12.882399 -11.124401 -9.8026476 -8.7048073 -8.3659344 -10.426558 -12.413629 -13.117468 -13.208931 -13.378134 -12.1059][-18.125607 -17.996969 -17.546783 -16.432808 -14.682676 -13.046269 -11.181301 -10.567937 -10.96417 -11.077033 -11.819068 -12.94483 -13.009985 -12.289151 -11.118843][-13.491638 -13.544283 -13.757904 -13.260667 -12.096857 -11.711695 -11.773823 -11.499121 -10.690641 -10.610051 -11.313287 -10.906811 -11.29247 -11.240059 -10.754905][-8.2181587 -8.9664192 -8.33161 -6.9434366 -6.4287496 -6.0466685 -5.6238194 -7.0963783 -8.2425432 -9.2475071 -9.8415966 -10.73706 -11.796059 -12.311696 -12.587749]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 13:25:40.089300: step 17010, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 57h:15m:17s remains)
INFO - root - 2017-12-15 13:25:46.715606: step 17020, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 58h:23m:57s remains)
INFO - root - 2017-12-15 13:25:53.287563: step 17030, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 57h:01m:29s remains)
INFO - root - 2017-12-15 13:25:59.913208: step 17040, loss = 0.21, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 57h:12m:14s remains)
INFO - root - 2017-12-15 13:26:06.518815: step 17050, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.675 sec/batch; 59h:09m:35s remains)
INFO - root - 2017-12-15 13:26:13.052116: step 17060, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 57h:05m:42s remains)
INFO - root - 2017-12-15 13:26:19.695210: step 17070, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 60h:00m:01s remains)
INFO - root - 2017-12-15 13:26:26.374606: step 17080, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 56h:33m:47s remains)
INFO - root - 2017-12-15 13:26:32.969522: step 17090, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.687 sec/batch; 60h:12m:59s remains)
INFO - root - 2017-12-15 13:26:39.569974: step 17100, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.628 sec/batch; 55h:03m:04s remains)
2017-12-15 13:26:40.055249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.376677 -7.4673777 -7.3131318 -6.4655733 -6.906014 -7.1815853 -7.2731495 -6.9761319 -6.8131475 -6.1094446 -5.195549 -6.7777796 -7.8088846 -8.0096035 -6.0573616][-7.0050926 -6.8397188 -6.8362923 -6.0339046 -5.8733 -6.0685425 -6.0520682 -6.0341339 -6.2435718 -5.5552664 -5.651515 -7.4376106 -8.6449413 -9.2526321 -7.2155218][-4.59169 -5.7507586 -6.7370381 -5.1329017 -5.3071651 -5.3134813 -5.0260596 -5.0057888 -5.0747852 -5.0659041 -5.0052176 -6.5614333 -7.3530655 -7.8222518 -6.7339544][-6.9915075 -6.7514286 -5.1953449 -3.0398166 -3.773397 -4.0187545 -4.7060418 -5.6129918 -6.42164 -5.5812373 -4.1333594 -5.8156614 -7.3742242 -7.8600883 -6.4005513][-8.21373 -8.5811615 -7.8640432 -5.2309933 -3.502552 -1.275867 -1.1968074 -3.8641334 -6.3834486 -6.1910357 -4.6911206 -5.9938626 -7.128458 -7.6871843 -6.4548612][-10.864989 -9.8581161 -7.8443618 -3.4820154 -1.090744 1.6678948 2.9962945 1.7846422 -0.0608387 -2.6284811 -4.643064 -5.1143417 -4.9626093 -6.234158 -5.6546378][-13.033468 -10.876974 -7.6021934 -2.2715681 -0.066625595 2.2531071 4.1696467 4.2333274 4.3028655 0.66513157 -2.4010499 -4.8074527 -6.676754 -6.8995676 -5.2278538][-14.059519 -10.995506 -7.1579657 -0.86160994 2.7293715 4.792769 5.2577577 3.875361 4.0106244 1.8510952 0.31967545 -3.3955262 -6.9925218 -7.9753017 -6.7802978][-11.582338 -8.381608 -6.065259 -0.2577796 2.1350884 4.7864623 5.372396 2.8521733 2.1474972 0.75107956 -0.37749863 -4.1252623 -6.9549847 -7.7228365 -7.2168312][-10.034712 -8.163125 -5.5880976 -2.0652077 -0.30276728 2.4502664 4.1209884 2.3233647 0.71055984 -1.3174663 -2.1153941 -5.2929487 -7.636445 -8.8098726 -7.766017][-11.858513 -9.3945484 -7.3597345 -4.6680956 -3.990449 -3.054426 -2.1508062 -2.2719586 -3.1621642 -4.3061333 -4.7655611 -8.6214437 -10.599012 -10.447408 -8.9984293][-15.438768 -14.288437 -11.871099 -9.3796043 -8.8886166 -8.6052742 -8.3360643 -8.6129913 -8.6657009 -8.82742 -8.2183285 -10.131553 -11.12011 -11.796808 -10.491226][-16.468872 -15.514112 -13.336967 -11.330707 -10.275933 -9.5936174 -10.00898 -10.514073 -10.212654 -9.4529648 -8.7044258 -9.4560938 -9.4228106 -9.62352 -8.3625374][-13.316565 -11.957975 -11.148251 -9.9839878 -7.9087982 -7.41745 -7.8141575 -7.6701136 -7.7215376 -8.3506317 -8.3046875 -8.0123253 -7.6080604 -7.676084 -6.5090971][-11.176919 -9.0474119 -7.7207937 -5.4656386 -3.6540749 -3.4656472 -3.5864413 -4.3795834 -5.3164115 -5.155952 -4.97735 -6.3636994 -7.4702506 -7.5667572 -7.2216663]]...]
INFO - root - 2017-12-15 13:26:46.485775: step 17110, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 55h:51m:25s remains)
INFO - root - 2017-12-15 13:26:53.122895: step 17120, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 57h:26m:14s remains)
INFO - root - 2017-12-15 13:26:59.675566: step 17130, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 58h:21m:01s remains)
INFO - root - 2017-12-15 13:27:06.222773: step 17140, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.631 sec/batch; 55h:14m:08s remains)
INFO - root - 2017-12-15 13:27:12.827136: step 17150, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 58h:21m:48s remains)
INFO - root - 2017-12-15 13:27:19.478319: step 17160, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 57h:02m:30s remains)
INFO - root - 2017-12-15 13:27:26.047479: step 17170, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 57h:19m:50s remains)
INFO - root - 2017-12-15 13:27:32.559770: step 17180, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 55h:50m:25s remains)
INFO - root - 2017-12-15 13:27:39.063480: step 17190, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 55h:41m:41s remains)
INFO - root - 2017-12-15 13:27:45.645674: step 17200, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 57h:10m:53s remains)
2017-12-15 13:27:46.175607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1329908 -3.5052948 -3.1051061 -3.0827534 -4.5373483 -6.1495905 -8.0463438 -8.6169558 -8.2640648 -7.6854815 -6.3489923 -5.6978641 -5.7750411 -5.2054911 -4.2479658][-2.9791899 -3.1608176 -2.2440031 -1.9131906 -3.6347871 -5.0713615 -6.1966615 -6.7151337 -6.9373956 -5.9449687 -3.9292283 -4.0418043 -4.7809196 -4.6764693 -4.7582216][-1.0812116 -2.1370451 -3.1534836 -2.3807108 -2.5441868 -4.1993408 -5.455883 -5.4405107 -4.8907146 -4.0272632 -2.8887966 -3.678206 -5.1583261 -6.1038103 -6.8011479][-3.8848605 -4.2818146 -4.064229 -3.5856774 -4.2921538 -3.9495707 -3.5480618 -3.4756067 -3.6842523 -3.2623045 -2.5201116 -4.2436447 -6.2943082 -7.0839605 -7.1924877][-5.1832695 -6.613903 -7.0509462 -5.2124658 -4.1615129 -2.3830266 -0.67588186 -1.2302833 -2.7564378 -2.4727972 -2.4704335 -4.4164858 -6.1833811 -7.326839 -7.3460546][-6.5213952 -6.9844494 -5.8650718 -2.7880189 -0.18955088 1.9615674 3.6483727 3.6398478 3.0655785 0.91427755 -1.7250478 -3.1823006 -4.6861429 -5.1895719 -4.6826596][-7.4556456 -6.3457389 -4.5069647 -0.93266535 2.3980808 5.7801743 8.2506485 7.1195717 6.123147 3.5619516 0.25303221 -2.3736238 -5.0195503 -5.2103477 -4.548274][-7.199441 -5.7329364 -4.6561751 -1.4372802 1.3055158 5.3795557 9.11394 7.9247932 6.0204153 3.2315912 0.49028158 -2.6918287 -5.881536 -6.2871795 -6.1987119][-5.4075546 -5.2910972 -4.8808427 -2.6329002 -0.91350508 2.2534671 4.96208 5.5625381 4.4159336 1.002409 -1.4062805 -4.1518 -7.3713679 -7.8425322 -7.039917][-5.95966 -6.437501 -6.7305994 -5.4923353 -3.7068315 -1.6679606 0.696537 1.5260549 0.78110409 -1.5881977 -4.0858116 -6.9178381 -9.3289137 -9.1977158 -8.69962][-11.847745 -11.906725 -11.388956 -10.562226 -9.9048 -7.8222466 -5.6271167 -5.1147203 -5.4017882 -6.0634308 -7.5938663 -10.529521 -11.82152 -11.050473 -9.4138174][-16.141201 -16.712074 -15.854619 -14.707245 -14.369711 -12.943027 -11.816091 -11.108568 -10.863756 -11.342937 -12.010865 -12.853695 -13.129696 -12.35243 -10.622473][-14.895721 -14.535269 -13.872172 -14.008739 -14.120913 -12.97112 -12.455202 -12.023136 -11.994238 -11.659386 -11.288399 -11.23188 -11.21091 -9.9578648 -7.8097076][-10.530696 -10.391279 -10.206539 -9.2300234 -8.3994579 -8.7416525 -9.2808456 -8.2170334 -7.8233786 -7.8522177 -8.7826815 -8.3393726 -7.5773735 -6.8964128 -6.1802568][-8.0468607 -7.49983 -6.5647349 -4.9940915 -4.2192979 -4.169014 -4.0382972 -4.5200295 -4.9602132 -4.2104073 -4.4233446 -4.8659315 -6.0692048 -6.7610822 -5.6840315]]...]
INFO - root - 2017-12-15 13:27:52.785218: step 17210, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 56h:53m:31s remains)
INFO - root - 2017-12-15 13:27:59.443465: step 17220, loss = 0.18, batch loss = 0.14 (11.6 examples/sec; 0.691 sec/batch; 60h:30m:15s remains)
INFO - root - 2017-12-15 13:28:05.997818: step 17230, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 56h:48m:35s remains)
INFO - root - 2017-12-15 13:28:12.533829: step 17240, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 58h:05m:49s remains)
INFO - root - 2017-12-15 13:28:19.142269: step 17250, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 58h:32m:30s remains)
INFO - root - 2017-12-15 13:28:25.764033: step 17260, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 58h:38m:49s remains)
INFO - root - 2017-12-15 13:28:32.368577: step 17270, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 57h:51m:37s remains)
INFO - root - 2017-12-15 13:28:38.882508: step 17280, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 55h:59m:40s remains)
INFO - root - 2017-12-15 13:28:45.440982: step 17290, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.638 sec/batch; 55h:51m:03s remains)
INFO - root - 2017-12-15 13:28:52.043711: step 17300, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 57h:49m:19s remains)
2017-12-15 13:28:52.570772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4792829 -6.7465343 -7.0429654 -7.0073733 -7.869648 -8.6633081 -9.21426 -9.15766 -8.7927418 -8.71106 -7.64262 -7.6677651 -10.616707 -11.032803 -9.1174421][-7.1063843 -6.8047266 -6.6084452 -6.8091259 -7.9603891 -9.0642281 -9.8473034 -10.224699 -10.662971 -10.322189 -9.5689459 -9.9385395 -11.542427 -12.027832 -11.537483][-5.6708827 -6.6818056 -7.94152 -7.2058172 -7.5708833 -8.7288322 -9.61994 -10.213929 -10.973473 -11.3444 -11.350204 -11.804049 -13.34771 -13.780352 -12.038877][-5.4761868 -6.4747758 -6.5488491 -6.2142897 -7.6169357 -7.2992477 -6.2262273 -7.10087 -8.5160542 -9.51832 -9.9011707 -10.602808 -12.306747 -13.093639 -12.145828][-6.6718392 -7.6398458 -7.6525207 -5.5854626 -4.8701649 -3.4980888 -2.2339857 -3.0465312 -4.4644904 -6.4074011 -8.1300077 -8.9359684 -10.588501 -11.652224 -11.173096][-9.08251 -9.5084076 -8.6094255 -5.2636538 -2.2449834 1.274056 3.6505823 2.0753322 0.46710682 -2.1392753 -4.6168227 -6.0349793 -8.3623543 -9.4048729 -9.2496014][-10.3055 -11.170645 -10.611047 -7.0678988 -1.6049981 4.0078688 7.33222 7.5351725 6.5111666 1.2156038 -2.872818 -3.710387 -6.6755834 -8.2596006 -8.5472212][-9.5348673 -10.151566 -9.6652985 -6.3497324 -1.7149057 3.989749 8.55418 9.0503235 8.8653231 4.2468777 0.10058022 -3.1044898 -8.0080471 -8.7790213 -8.7213783][-7.5736389 -7.7444148 -7.4622583 -5.1821122 -2.2328508 2.1771293 6.1362114 7.3726254 6.6968803 3.7455883 1.5597482 -3.8253534 -10.350784 -11.507162 -11.224306][-5.201189 -6.5168471 -6.5877237 -6.0268788 -4.3035488 -1.1140642 1.5297284 2.9979792 2.3613205 0.34638834 -1.5249715 -5.3095322 -9.9655819 -12.515427 -13.3408][-6.4547143 -7.1118908 -7.580718 -6.4338632 -5.828867 -5.40737 -3.8809156 -2.3159893 -3.1205027 -4.0477738 -5.6234207 -8.9168072 -12.05908 -13.773973 -13.800039][-10.550459 -11.466272 -10.572303 -10.078033 -10.242216 -10.543962 -10.475143 -9.6417942 -9.1512661 -8.9427757 -10.458574 -10.312159 -11.587418 -14.052461 -14.040147][-12.678593 -12.995947 -12.930098 -12.682741 -13.069516 -13.209312 -12.292984 -12.090838 -12.343876 -11.82582 -11.793676 -12.0422 -13.329985 -13.170647 -11.914183][-11.88243 -11.590738 -11.282578 -11.375317 -10.99039 -10.81312 -11.481342 -11.414659 -11.171704 -11.08617 -10.842253 -9.4414177 -9.465168 -10.143803 -9.5034142][-10.04088 -8.4706974 -7.431653 -6.9418545 -6.9325185 -6.1514492 -6.2212687 -6.9442816 -7.5447383 -7.7295055 -7.9630728 -8.5191345 -9.6100006 -8.8447924 -8.0395441]]...]
INFO - root - 2017-12-15 13:28:59.034950: step 17310, loss = 0.25, batch loss = 0.21 (12.7 examples/sec; 0.632 sec/batch; 55h:21m:57s remains)
INFO - root - 2017-12-15 13:29:05.655732: step 17320, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 57h:08m:29s remains)
INFO - root - 2017-12-15 13:29:12.295304: step 17330, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 57h:19m:52s remains)
INFO - root - 2017-12-15 13:29:18.853269: step 17340, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 57h:01m:49s remains)
INFO - root - 2017-12-15 13:29:25.411936: step 17350, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 58h:08m:37s remains)
INFO - root - 2017-12-15 13:29:31.990909: step 17360, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.634 sec/batch; 55h:32m:18s remains)
INFO - root - 2017-12-15 13:29:38.513600: step 17370, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 58h:45m:23s remains)
INFO - root - 2017-12-15 13:29:45.116132: step 17380, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 56h:36m:36s remains)
INFO - root - 2017-12-15 13:29:51.630054: step 17390, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 58h:25m:00s remains)
INFO - root - 2017-12-15 13:29:58.228041: step 17400, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 57h:30m:19s remains)
2017-12-15 13:29:58.715175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0494847 -7.2049828 -7.7875981 -7.2826042 -8.0239716 -8.3765869 -8.1696 -7.6935139 -8.0213833 -9.121007 -8.6709929 -8.68286 -9.6760511 -8.59544 -6.8396206][-6.6612225 -6.7606554 -6.390358 -6.6646962 -8.1064548 -7.7470851 -7.3317127 -7.0270581 -7.2568755 -7.9701347 -8.1491213 -8.7201366 -10.210893 -10.482157 -8.9577532][-5.2649565 -5.3353391 -6.0601864 -5.80797 -6.5068417 -7.0505257 -7.6363921 -7.1099257 -6.74808 -7.3931541 -7.5122747 -8.23914 -10.009056 -10.191741 -9.2739658][-5.2320132 -6.1593184 -6.6726775 -6.1139965 -7.0553718 -6.8534365 -6.59428 -6.6164179 -6.6570611 -6.317935 -5.3874478 -6.7015343 -8.8150558 -9.6869144 -8.9843941][-7.4865065 -8.4254684 -8.92121 -7.8990564 -6.8857117 -4.8267126 -4.13438 -5.105464 -6.4247122 -6.5793276 -6.0589948 -6.539598 -7.924633 -9.3571348 -9.4478464][-8.73239 -8.6598291 -8.0413837 -6.868125 -5.0067377 -1.01194 2.9047723 2.2991042 -0.38687754 -2.2559557 -3.7231274 -4.9117641 -6.8955064 -8.7021523 -8.8951759][-10.405527 -9.2560854 -7.7467337 -5.0795884 -2.2830758 1.8366728 5.804462 7.3701568 6.1821876 1.9204144 -1.9637413 -3.6209238 -5.6817403 -7.221067 -7.7863755][-9.975069 -8.87273 -7.3973994 -4.2265348 -0.7749896 2.9894996 6.8857512 7.9233193 7.3572803 4.2214847 0.14486027 -3.4302278 -6.8038139 -8.6079512 -8.8005276][-7.8552403 -6.5616579 -5.5016942 -2.4297338 0.36680269 2.8165689 4.6764321 5.31029 4.4637485 1.7929478 -0.32553864 -3.5672185 -7.8315792 -9.63377 -9.596818][-6.8458548 -5.5470862 -4.6600294 -1.8591022 0.20423126 2.1036572 3.8572268 2.8169112 0.16646194 -1.4430876 -2.5808773 -4.7412171 -8.28282 -10.458347 -11.147451][-6.8146029 -7.0720925 -7.1014681 -4.9049788 -3.1801746 -2.3641243 -0.87907982 -1.0052848 -3.2134812 -4.9049845 -5.9109206 -7.8595624 -10.078331 -11.46501 -11.783619][-10.573964 -9.8447056 -9.0806513 -8.1043768 -8.3096962 -7.5585842 -6.9367838 -7.5244627 -7.5106087 -7.3781786 -8.4018421 -8.90173 -10.168827 -11.231094 -10.927683][-13.804537 -12.916131 -11.60647 -10.209049 -9.9401093 -9.5966206 -9.53625 -9.1453514 -8.7701092 -8.6573381 -8.7975073 -9.5092068 -10.29624 -10.362677 -9.1905155][-10.287365 -10.039762 -8.6500149 -7.8680687 -8.0665627 -8.2407684 -8.5712662 -8.0958729 -7.5114241 -7.259522 -7.0118756 -6.9899473 -6.6839018 -7.4282446 -7.9870768][-7.4889889 -6.7161169 -6.0882125 -4.9081345 -4.1682234 -4.1001425 -3.966743 -4.1850615 -4.8803091 -5.2986445 -6.33151 -7.0542088 -7.7858496 -7.4010735 -7.5191679]]...]
INFO - root - 2017-12-15 13:30:05.264012: step 17410, loss = 0.20, batch loss = 0.16 (11.7 examples/sec; 0.682 sec/batch; 59h:40m:37s remains)
INFO - root - 2017-12-15 13:30:11.917980: step 17420, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 59h:11m:27s remains)
INFO - root - 2017-12-15 13:30:18.559878: step 17430, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 58h:18m:22s remains)
INFO - root - 2017-12-15 13:30:25.195010: step 17440, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 56h:49m:09s remains)
INFO - root - 2017-12-15 13:30:31.818591: step 17450, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 57h:34m:46s remains)
INFO - root - 2017-12-15 13:30:38.393411: step 17460, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 56h:56m:13s remains)
INFO - root - 2017-12-15 13:30:44.972681: step 17470, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 59h:02m:41s remains)
INFO - root - 2017-12-15 13:30:51.531979: step 17480, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 55h:54m:19s remains)
INFO - root - 2017-12-15 13:30:58.110476: step 17490, loss = 0.24, batch loss = 0.20 (12.3 examples/sec; 0.653 sec/batch; 57h:07m:30s remains)
INFO - root - 2017-12-15 13:31:04.646440: step 17500, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 56h:56m:44s remains)
2017-12-15 13:31:05.193342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7789428 -1.7289159 -2.1432123 -2.504513 -3.4456489 -3.9197721 -4.6038713 -4.7851815 -5.0339584 -5.3658724 -6.312767 -6.1262493 -7.3906565 -7.4166269 -6.0514545][-3.2938623 -1.8327534 -2.4430265 -3.6213775 -4.3733678 -4.6601052 -4.5468168 -4.6448631 -4.3571696 -4.8644481 -4.8944664 -5.0739946 -6.9461279 -8.3671761 -7.9553509][-1.6691289 -2.0671549 -3.6171396 -4.0268273 -4.22878 -4.2813435 -4.6088619 -4.4252844 -4.7165341 -4.2325506 -3.8073969 -3.0032208 -4.1887684 -7.1979418 -7.4873219][-3.2353113 -4.0848808 -5.4059997 -6.1590462 -6.6781244 -5.5801625 -4.2107444 -3.5275233 -3.4799795 -3.8419666 -4.6280556 -3.3160772 -3.8699508 -4.7422309 -4.4114814][-4.1154008 -6.554266 -8.7501345 -9.1719351 -8.6074724 -5.815341 -2.8964641 -2.4868007 -3.4131494 -4.0837259 -4.3724933 -3.4419234 -4.437531 -5.1853533 -4.297534][-4.4155531 -5.8419242 -7.2942452 -8.4312754 -7.5848541 -2.6643887 1.7504363 2.2190609 1.4989309 -0.87390041 -2.4004605 -1.6981559 -2.8727295 -3.7612548 -2.9149609][-3.8418653 -3.0641563 -4.8057861 -6.1695113 -5.111299 -0.48279047 4.031251 6.7334852 6.2555046 2.180675 -1.5095873 -1.6443243 -3.2134228 -4.3332396 -3.6223378][-2.8869324 -2.7726867 -3.4501882 -4.5947285 -3.6676481 0.0065255165 5.0052443 8.3830318 8.2518463 3.6675253 -0.42131615 -1.9547889 -5.1258855 -6.1110635 -5.4737267][-1.9625239 -1.279355 -2.2375476 -3.1078169 -3.2697759 -1.5437832 1.7715044 5.1856842 5.4548683 3.4138575 0.99840355 -1.5896268 -5.1165342 -7.4015927 -7.487298][-3.37066 -3.8105383 -4.31574 -4.202991 -4.7028303 -2.6296706 0.31598139 1.2279568 1.0471406 -0.31860304 -2.5471935 -3.782198 -7.0836139 -9.6403694 -9.7965059][-6.319037 -6.4760113 -7.2910357 -7.3615685 -7.9139185 -7.0958319 -5.8649335 -4.3624549 -3.6365073 -4.7921352 -6.2186956 -6.9371634 -10.05397 -11.281965 -11.046041][-10.770617 -9.7230158 -9.5735741 -9.7593412 -9.4030123 -9.95637 -10.11747 -9.2780418 -8.9607868 -9.4094276 -10.502214 -10.362462 -11.249952 -11.317279 -10.205556][-11.903694 -11.239859 -10.896055 -9.5069342 -9.7739468 -9.0909939 -8.5080776 -8.9388666 -9.3475552 -9.5793619 -9.988924 -10.857676 -11.586291 -9.9985228 -7.8035007][-9.2304163 -9.1165667 -8.4663649 -7.7416849 -8.53559 -8.2969923 -7.8402009 -7.5109363 -7.1357317 -7.6194592 -8.0734549 -7.9793062 -8.4429951 -8.4921083 -7.1413584][-5.8877068 -6.3118668 -5.9789333 -4.7890859 -3.8308969 -4.532064 -4.8757725 -3.9070358 -4.2488317 -4.67187 -5.1811781 -6.2935352 -7.8689785 -8.0842867 -6.8830853]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 13:31:12.788208: step 17510, loss = 0.21, batch loss = 0.16 (11.8 examples/sec; 0.677 sec/batch; 59h:15m:30s remains)
INFO - root - 2017-12-15 13:31:19.423713: step 17520, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 56h:41m:04s remains)
INFO - root - 2017-12-15 13:31:26.064470: step 17530, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 57h:17m:39s remains)
INFO - root - 2017-12-15 13:31:32.578000: step 17540, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 58h:05m:15s remains)
INFO - root - 2017-12-15 13:31:39.115264: step 17550, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 57h:42m:27s remains)
INFO - root - 2017-12-15 13:31:45.626977: step 17560, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 57h:34m:30s remains)
INFO - root - 2017-12-15 13:31:52.272288: step 17570, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 57h:09m:04s remains)
INFO - root - 2017-12-15 13:31:58.888454: step 17580, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 56h:49m:44s remains)
INFO - root - 2017-12-15 13:32:05.465491: step 17590, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 57h:34m:19s remains)
INFO - root - 2017-12-15 13:32:11.968550: step 17600, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 56h:44m:42s remains)
2017-12-15 13:32:12.472734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8427544 -9.6929846 -10.464902 -10.579717 -11.013447 -10.540199 -10.13974 -9.6922359 -9.821559 -10.567505 -11.682265 -14.489162 -16.234629 -14.992413 -10.64222][-6.6833682 -7.1682086 -8.4737511 -8.9797916 -9.9108315 -10.493089 -10.316665 -9.9202881 -9.8663864 -9.533884 -9.149 -11.763371 -14.13545 -14.856728 -11.569272][-6.0273066 -7.2554932 -8.39493 -9.0382967 -9.9610558 -10.241055 -10.115705 -9.5928755 -9.4490938 -9.5566168 -9.5635586 -10.749289 -12.261826 -12.870266 -10.182673][-6.2869129 -7.0896416 -8.3787889 -7.7427511 -7.4788475 -7.1478596 -6.7464614 -6.6968226 -6.4179697 -6.6739988 -7.8290219 -9.6635885 -11.065478 -10.577469 -7.8954077][-6.7646418 -8.1261959 -8.6772652 -6.6693568 -5.1713228 -2.9471436 -1.9488537 -2.7533381 -2.8034277 -3.488039 -4.1427932 -6.9218273 -9.2713079 -9.9150009 -7.75505][-8.2305441 -7.4401855 -6.7275648 -3.7447243 -1.5041828 1.987865 4.62919 3.7767005 1.7888918 -0.77021551 -2.0647914 -3.7445974 -6.0738444 -8.382206 -7.445786][-8.6730947 -7.2815514 -6.218143 -2.5091221 0.48668194 4.4770103 8.0439034 7.6816754 5.957952 2.2158446 -1.410171 -4.0170736 -5.5744414 -5.8362346 -5.159472][-8.3804455 -7.4434533 -6.804327 -2.32731 1.5885525 5.5436063 8.2577381 8.3758125 7.1803193 3.484098 0.1016326 -3.2753429 -6.0749192 -6.0946341 -4.7074251][-7.9625196 -6.7516956 -5.591475 -2.2401049 0.62849665 3.8010879 5.5731082 5.6701026 4.3972707 1.3184485 -1.51268 -5.8325396 -9.9317122 -10.487345 -8.4753532][-7.1725907 -6.3889108 -5.3471479 -2.9949524 -2.2089627 0.91925049 2.5421014 2.4650159 1.1413445 -1.3725471 -4.2940321 -9.84324 -13.495768 -14.956249 -13.568514][-10.756129 -9.7629051 -8.7292118 -6.167582 -5.9110022 -4.911593 -4.0299444 -3.3062656 -4.1236811 -5.9962983 -8.9159088 -14.23703 -18.184284 -19.112373 -16.758881][-11.756111 -11.715809 -10.992765 -7.9570103 -6.5270748 -6.1078434 -6.3572783 -7.4270067 -8.7634153 -10.320906 -11.906643 -15.125164 -17.478041 -18.057528 -15.499739][-12.165581 -10.493774 -9.1877289 -7.6134963 -6.9140272 -6.4277296 -6.4500508 -7.4370317 -8.8940792 -10.659451 -11.938354 -14.228336 -15.169975 -14.1684 -10.595836][-11.110918 -9.1237688 -7.6964188 -7.3317156 -6.9159279 -6.3806505 -6.23747 -6.3991418 -7.3585038 -8.7332106 -9.374589 -9.68841 -9.2487583 -8.4476166 -6.0993185][-9.1282845 -8.09355 -5.8936777 -4.3827844 -3.3811204 -4.4873934 -6.0241823 -7.1281176 -8.1076508 -8.56473 -9.0004358 -9.0572367 -8.3907318 -7.8160591 -6.41413]]...]
INFO - root - 2017-12-15 13:32:18.993827: step 17610, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 58h:34m:40s remains)
INFO - root - 2017-12-15 13:32:25.559832: step 17620, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 56h:41m:34s remains)
INFO - root - 2017-12-15 13:32:32.244536: step 17630, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.689 sec/batch; 60h:17m:11s remains)
INFO - root - 2017-12-15 13:32:38.755979: step 17640, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 56h:57m:45s remains)
INFO - root - 2017-12-15 13:32:45.353926: step 17650, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.672 sec/batch; 58h:45m:14s remains)
INFO - root - 2017-12-15 13:32:51.902067: step 17660, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 57h:25m:03s remains)
INFO - root - 2017-12-15 13:32:58.516045: step 17670, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 57h:47m:21s remains)
INFO - root - 2017-12-15 13:33:05.239104: step 17680, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 58h:15m:28s remains)
INFO - root - 2017-12-15 13:33:11.862662: step 17690, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 59h:37m:52s remains)
INFO - root - 2017-12-15 13:33:18.503401: step 17700, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 58h:53m:28s remains)
2017-12-15 13:33:19.066302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2898519 -3.8488708 -4.1248422 -3.9976497 -4.7082019 -5.2186971 -5.2336922 -4.566967 -3.4488902 -2.3033597 -1.6570687 -3.735302 -5.950253 -7.1404142 -7.46106][-3.3202562 -3.6221662 -3.8895671 -4.0999851 -4.5280704 -5.4062595 -5.1167874 -3.8837585 -3.2356641 -2.53027 -1.5402365 -3.0983055 -5.3211012 -6.7313404 -6.7845783][-2.9415238 -3.3079209 -3.9257216 -4.0419397 -4.855689 -5.2645078 -4.9514475 -4.3178406 -3.5136728 -3.0763152 -2.6990981 -4.2249079 -6.1344042 -7.0247421 -7.3562093][-2.6143379 -3.2884541 -3.535553 -3.7556977 -4.5786295 -4.7338953 -4.3701615 -4.2238936 -4.0847712 -3.2820547 -2.3305454 -3.6196413 -4.9945688 -5.9168587 -6.3850346][-4.0372133 -4.8553314 -5.4235673 -4.327549 -3.6653671 -2.4014497 -1.5092454 -1.8077512 -2.3063662 -2.460041 -2.3405478 -3.0525663 -3.6886959 -4.2989578 -4.566196][-5.0509682 -5.1166396 -4.3389082 -3.2110035 -1.2681985 1.5815215 2.85495 1.7023129 0.43051624 -0.4707222 -0.93996143 -1.8222783 -2.9870474 -3.4281108 -3.4633117][-4.6454225 -4.7968073 -4.0995512 -2.4999831 -0.55385303 2.6958752 5.3332491 5.241643 3.89396 1.2160788 -0.75191212 -2.4265268 -4.0079961 -3.8200488 -3.5601752][-4.5302396 -3.5470257 -2.3945863 -1.5177727 -0.44492912 2.6972408 5.2541437 5.7336121 5.3608131 2.3754034 -0.58995295 -2.6853709 -4.4584656 -4.4128017 -3.9986873][-3.6031771 -2.9591615 -2.2372091 -0.70838022 0.13901138 1.681571 3.3560162 4.7847891 4.7511415 2.0576186 -0.46327066 -4.1684179 -7.2977614 -7.0255585 -5.6075959][-4.2218919 -2.9605904 -1.9140506 -0.4129715 0.13550091 1.1088352 2.1354656 2.8210087 2.2759643 -0.15609169 -2.6383729 -6.333992 -9.0076113 -9.4884014 -8.8591576][-6.9049411 -5.7234244 -4.1974559 -2.8797488 -2.8593507 -2.1634858 -1.9731991 -2.0188866 -2.5437939 -3.9468164 -5.7716694 -9.9094687 -12.292526 -11.816397 -10.258118][-9.83041 -9.184226 -7.8519092 -6.3138905 -5.507194 -4.4067922 -4.923233 -5.6678081 -6.641953 -7.6583762 -9.15723 -11.158367 -11.426254 -12.114376 -11.448891][-11.758162 -10.451786 -9.1393738 -8.0283489 -7.1031222 -6.5477929 -6.8760371 -7.5172572 -9.0975552 -9.8286991 -10.729933 -11.723946 -11.413613 -10.484173 -9.4757519][-9.2414532 -9.1284971 -8.3309669 -7.0294008 -5.9111652 -6.2202597 -6.7885075 -7.6166916 -8.4576063 -9.0569105 -9.4501581 -8.6482067 -7.8701248 -7.7212596 -7.9351015][-6.2670059 -5.8908014 -5.2721238 -5.2325778 -5.2815046 -5.4366198 -4.89959 -5.1566811 -5.9479809 -5.8482718 -5.7881756 -6.260294 -6.2579865 -6.7425776 -8.3342381]]...]
INFO - root - 2017-12-15 13:33:25.670768: step 17710, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 57h:11m:29s remains)
INFO - root - 2017-12-15 13:33:32.234942: step 17720, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 58h:00m:31s remains)
INFO - root - 2017-12-15 13:33:38.850095: step 17730, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 57h:53m:21s remains)
INFO - root - 2017-12-15 13:33:45.362004: step 17740, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 57h:04m:06s remains)
INFO - root - 2017-12-15 13:33:52.059159: step 17750, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 58h:19m:57s remains)
INFO - root - 2017-12-15 13:33:58.680694: step 17760, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 57h:23m:11s remains)
INFO - root - 2017-12-15 13:34:05.253760: step 17770, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 58h:28m:17s remains)
INFO - root - 2017-12-15 13:34:11.818424: step 17780, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 56h:33m:46s remains)
INFO - root - 2017-12-15 13:34:18.373975: step 17790, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 58h:03m:43s remains)
INFO - root - 2017-12-15 13:34:24.965205: step 17800, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 58h:40m:10s remains)
2017-12-15 13:34:25.468155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8127794 -6.2380285 -5.7406583 -5.3070893 -5.7909555 -6.3302546 -6.9463105 -6.774518 -6.6384611 -6.7467403 -7.0449486 -6.9672394 -7.363616 -5.9558291 -3.6143019][-7.3156071 -6.0843954 -5.3129616 -4.5634828 -4.5929713 -4.7258997 -5.1668282 -5.2245679 -4.3426981 -3.9562163 -3.6235027 -4.0617723 -5.3358779 -3.9496083 -1.3172431][-5.4849367 -5.5333204 -5.4097228 -3.9470189 -4.2291579 -4.211298 -3.6969869 -2.6912255 -1.9268854 -1.7137308 -2.094753 -2.478652 -3.552444 -3.2076275 -1.120852][-5.636435 -5.8415585 -6.3484092 -6.2770939 -6.5007572 -5.4342079 -4.4184771 -4.2490973 -3.4039385 -2.2977185 -2.4915318 -3.3737116 -3.7944322 -3.6175172 -1.6230793][-7.1230006 -7.5525737 -7.9796515 -7.2069044 -6.9402 -3.974371 -1.8314543 -2.158457 -1.9305911 -1.2381544 -2.3429611 -2.8614025 -4.3940907 -4.2571249 -2.6045825][-7.3555059 -6.6387157 -5.7452731 -4.1565728 -2.1427743 1.0486836 4.2206483 4.3177924 2.9226851 0.66889906 -1.2778239 -1.6400428 -3.9987803 -5.0848346 -3.3049662][-8.9697533 -7.2192478 -6.1003408 -3.0431786 -0.31540537 3.4490504 7.6881604 7.11167 5.3049655 1.7374725 -2.2022581 -3.2547379 -6.132956 -5.881114 -4.7414222][-8.7396021 -8.4431 -7.8791866 -4.1464081 -0.011278629 3.350513 6.8459954 6.6513495 5.3572164 1.2008014 -2.0103717 -3.5446515 -7.2049618 -7.7803655 -6.0738096][-8.2265854 -6.7554994 -6.1939149 -3.5674295 -1.0903668 1.748261 4.543263 3.9219279 1.850029 -1.5514841 -4.2515755 -6.0518775 -8.5516329 -7.7458148 -5.8833895][-7.822649 -6.3902545 -5.5801487 -4.184773 -2.9729092 -0.37326622 1.7624907 0.12366056 -1.8022342 -4.1176429 -5.3188443 -5.9791565 -8.0381575 -8.1621561 -6.8546805][-9.2977228 -8.1771212 -7.7089047 -6.2600832 -5.8928022 -4.8810987 -4.1028652 -4.661973 -5.4405293 -7.0124946 -7.6452026 -8.21907 -8.6973457 -8.8299007 -7.6421971][-14.317535 -13.520113 -13.07127 -11.855906 -11.854599 -11.035529 -10.895343 -11.044394 -11.173752 -11.789961 -11.571844 -11.007708 -10.5945 -9.6533813 -8.5849934][-14.553106 -14.091848 -13.671103 -12.841969 -12.227129 -11.595932 -11.948938 -12.465473 -12.316639 -11.695531 -10.401772 -9.7170868 -8.9044 -8.3376322 -7.0116076][-10.65006 -9.6102982 -9.6780071 -9.349514 -9.5878315 -9.6648312 -9.6457872 -9.34593 -9.0919418 -8.6222267 -7.9357634 -6.9369178 -6.6994705 -5.8310695 -5.0404339][-7.822464 -6.3590717 -5.2051506 -4.9851856 -5.028656 -4.3988123 -4.8519263 -5.9310832 -6.0235829 -6.2948341 -5.8438106 -5.2116365 -5.8964024 -7.1028047 -6.5579405]]...]
INFO - root - 2017-12-15 13:34:32.011649: step 17810, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 56h:13m:59s remains)
INFO - root - 2017-12-15 13:34:38.631994: step 17820, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 57h:52m:49s remains)
INFO - root - 2017-12-15 13:34:45.191699: step 17830, loss = 0.28, batch loss = 0.23 (11.8 examples/sec; 0.679 sec/batch; 59h:22m:22s remains)
INFO - root - 2017-12-15 13:34:51.741330: step 17840, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 57h:28m:57s remains)
INFO - root - 2017-12-15 13:34:58.302169: step 17850, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 58h:17m:45s remains)
INFO - root - 2017-12-15 13:35:04.838224: step 17860, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 56h:06m:48s remains)
INFO - root - 2017-12-15 13:35:11.537225: step 17870, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 58h:27m:41s remains)
INFO - root - 2017-12-15 13:35:18.114461: step 17880, loss = 0.13, batch loss = 0.08 (12.7 examples/sec; 0.631 sec/batch; 55h:07m:02s remains)
INFO - root - 2017-12-15 13:35:24.754187: step 17890, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 56h:30m:51s remains)
INFO - root - 2017-12-15 13:35:31.369359: step 17900, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 56h:15m:07s remains)
2017-12-15 13:35:31.875458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7035439 -3.6377439 -3.1087098 -2.963443 -3.6122189 -3.4923005 -3.4025412 -3.5273082 -3.5401955 -2.8617887 -2.2535672 -3.8329129 -6.7203164 -8.0089645 -8.018014][-3.3443711 -3.8012848 -4.0241766 -3.7828455 -4.3757749 -4.9880829 -5.7451987 -5.9324756 -6.1450062 -5.2608509 -4.0281572 -4.9402971 -7.2339449 -8.1148586 -7.7512856][-2.1742468 -2.8488429 -3.3190105 -3.3961923 -4.0731525 -4.2929673 -5.1013193 -5.624332 -6.0114155 -5.5994396 -4.8550582 -5.9596972 -7.5978589 -7.6680145 -6.80534][-2.692759 -2.6332746 -3.0085511 -3.3044665 -3.5018933 -3.2245255 -3.0597932 -4.155951 -5.021225 -5.1852736 -5.3232179 -6.0891843 -7.6470184 -7.2151842 -6.7928357][-3.262372 -3.5818484 -3.682981 -2.3122818 -1.5454283 -0.90294027 -0.6733799 -1.8517694 -3.2509825 -4.09463 -4.6852245 -5.2848821 -6.8949966 -7.4078975 -7.150207][-6.0904756 -5.32113 -4.4228778 -2.0809934 -0.10641766 1.6771564 2.6645741 1.4166908 -0.033414364 -1.1014929 -1.9682763 -3.3416121 -4.9150934 -4.4372697 -4.4098082][-8.286171 -7.521019 -6.0877333 -3.4272618 -0.4795866 3.4028296 5.88174 5.3130193 3.8608241 1.5378003 0.1248188 -0.54797888 -2.2551618 -3.0125344 -3.7728305][-8.4964657 -8.3808279 -7.2847581 -4.4162483 -1.0149918 2.9842572 5.0989785 5.0608869 4.2986541 1.9407225 0.94884348 -0.33077383 -2.3715374 -2.7729406 -4.1716442][-6.9903164 -7.4478312 -7.2280612 -4.5703917 -1.7506917 1.2065005 2.608664 2.611279 2.0690799 1.3664908 1.7837968 0.55254936 -1.8271954 -3.4541256 -5.1573439][-6.300313 -6.4712915 -6.4311028 -4.2580643 -2.4224303 0.35139608 1.5316529 1.586144 0.69717216 0.6547184 1.3741961 0.34416914 -1.4279566 -3.5710058 -7.2448964][-9.7127962 -9.39974 -8.7022171 -7.2595158 -5.6900506 -3.2398314 -2.3114347 -2.4958863 -3.1148498 -2.7773814 -1.8937364 -2.514384 -2.8670273 -4.262588 -6.5795259][-13.156954 -12.84843 -11.510604 -10.114131 -8.9111738 -7.086432 -6.3245521 -6.516973 -7.1496854 -6.925087 -5.975246 -5.2432532 -5.3389692 -5.3803525 -6.3544617][-11.913025 -11.317581 -10.295471 -10.299958 -10.363026 -9.785944 -9.6410437 -8.9491444 -8.50625 -8.1652241 -7.0398417 -6.9634204 -6.6569395 -5.7033815 -5.4454522][-8.9100084 -8.76894 -8.3348217 -8.7572269 -8.759656 -8.4705191 -9.386014 -9.2670336 -9.1273737 -8.5592394 -7.9079485 -7.9602604 -7.1201148 -6.7607231 -6.6245184][-6.1651273 -5.9608197 -6.0330753 -5.2565012 -4.35841 -4.1235075 -4.311388 -5.3620257 -6.8179879 -7.09367 -6.789288 -7.9646912 -8.7515869 -9.3702831 -9.67329]]...]
INFO - root - 2017-12-15 13:35:38.290086: step 17910, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 55h:55m:20s remains)
INFO - root - 2017-12-15 13:35:44.863345: step 17920, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.679 sec/batch; 59h:18m:22s remains)
INFO - root - 2017-12-15 13:35:51.487148: step 17930, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 57h:48m:34s remains)
INFO - root - 2017-12-15 13:35:58.035138: step 17940, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.633 sec/batch; 55h:16m:33s remains)
INFO - root - 2017-12-15 13:36:04.724682: step 17950, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.688 sec/batch; 60h:05m:00s remains)
INFO - root - 2017-12-15 13:36:11.343799: step 17960, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 55h:33m:20s remains)
INFO - root - 2017-12-15 13:36:17.940028: step 17970, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 56h:18m:56s remains)
INFO - root - 2017-12-15 13:36:24.492948: step 17980, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 55h:40m:15s remains)
INFO - root - 2017-12-15 13:36:31.081072: step 17990, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 57h:22m:51s remains)
INFO - root - 2017-12-15 13:36:37.636704: step 18000, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 57h:28m:33s remains)
2017-12-15 13:36:38.181836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2546182 -6.3326616 -8.443985 -8.4397182 -8.327383 -8.395874 -8.7727852 -9.0472059 -9.1606007 -9.7517529 -9.3261147 -8.3686371 -8.1786547 -9.2219849 -9.2152281][-3.8150702 -5.1410189 -6.826354 -7.5994177 -8.6072521 -8.2565613 -7.2382836 -7.5781198 -8.4082909 -9.0695324 -8.3917933 -7.9332008 -7.9088764 -8.7285633 -7.8413305][-2.4637251 -5.5086265 -8.4347248 -8.5249691 -8.971055 -7.7886343 -7.3774433 -7.4683809 -8.7350445 -9.98012 -8.7135448 -7.4364853 -7.896174 -8.2133446 -7.7858472][-5.470367 -6.6024361 -10.111305 -11.221775 -11.554293 -8.7531967 -6.4793496 -6.9350834 -7.7208366 -9.105854 -9.7094431 -9.1265755 -8.8728075 -9.9122782 -9.446682][-8.7593555 -11.267509 -14.078817 -14.443644 -11.984509 -5.2523165 -1.478858 -4.8209577 -8.9002743 -9.5791492 -9.4494772 -9.5663166 -10.196539 -11.692845 -12.123657][-9.4908562 -13.121115 -15.299263 -13.559744 -10.141394 -2.8185139 4.0188966 2.8308883 -1.2745061 -6.0052724 -9.7126007 -8.0124168 -9.07248 -11.679285 -12.009168][-9.1992741 -10.552961 -11.219007 -8.4501781 -4.7175889 1.959301 9.2627544 9.62421 7.2406468 -1.2618108 -7.3425035 -6.8124776 -8.37295 -10.532289 -11.428534][-9.6197195 -10.817182 -10.807734 -7.3276711 -2.7587831 5.5944066 12.089762 10.637863 9.8041191 3.2635951 -3.0363865 -5.8974276 -8.7587013 -10.343039 -10.826713][-8.43321 -9.7688084 -11.264957 -9.0495453 -4.3581429 3.7344937 8.9996758 8.5821629 6.2601562 -0.23036957 -4.6865654 -7.3665504 -9.7639694 -10.933813 -11.089168][-7.0960588 -8.2494183 -10.526634 -10.65024 -9.0484324 -2.8622737 2.5982118 4.3439546 2.54347 -3.9674711 -8.204052 -9.280632 -11.242164 -12.841118 -12.573047][-8.8458624 -9.9742661 -12.29731 -12.300072 -12.310223 -10.661383 -7.3034143 -5.3863707 -5.6942644 -8.2463589 -10.867613 -12.452966 -13.074429 -13.620121 -13.045366][-14.438908 -13.658989 -14.056456 -14.690292 -14.577864 -14.640699 -14.844309 -13.811026 -12.039155 -13.271637 -15.042555 -14.83127 -13.428745 -12.920172 -12.162208][-15.919342 -15.464815 -16.025892 -15.378559 -14.885605 -14.874437 -15.045183 -15.067125 -15.272087 -13.8013 -12.781829 -13.267093 -13.648836 -12.59876 -10.005184][-14.260189 -14.402683 -14.842173 -12.581005 -10.745124 -10.852448 -11.72447 -11.541142 -11.80769 -11.102832 -11.126656 -11.06169 -11.216812 -10.09633 -8.742815][-9.1013918 -9.4356241 -9.5529633 -7.7706413 -6.9687395 -6.7482891 -6.127461 -6.559967 -7.1676297 -7.2318144 -8.0403624 -9.1359243 -9.65033 -10.124191 -10.882771]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 13:36:44.794882: step 18010, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 57h:06m:13s remains)
INFO - root - 2017-12-15 13:36:51.418951: step 18020, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 58h:02m:41s remains)
INFO - root - 2017-12-15 13:36:58.034643: step 18030, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.665 sec/batch; 58h:05m:12s remains)
INFO - root - 2017-12-15 13:37:04.560136: step 18040, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 57h:17m:57s remains)
INFO - root - 2017-12-15 13:37:11.184740: step 18050, loss = 0.24, batch loss = 0.19 (11.8 examples/sec; 0.680 sec/batch; 59h:26m:17s remains)
INFO - root - 2017-12-15 13:37:17.847647: step 18060, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.658 sec/batch; 57h:29m:05s remains)
INFO - root - 2017-12-15 13:37:24.357804: step 18070, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 57h:04m:56s remains)
INFO - root - 2017-12-15 13:37:30.914929: step 18080, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 57h:34m:40s remains)
INFO - root - 2017-12-15 13:37:37.497228: step 18090, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 55h:58m:46s remains)
INFO - root - 2017-12-15 13:37:44.016510: step 18100, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 56h:28m:24s remains)
2017-12-15 13:37:44.489287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8873644 -3.9869881 -4.0230579 -4.3804188 -6.4530277 -7.300416 -7.59567 -6.8979564 -5.7696571 -4.6163359 -3.7939367 -4.3882384 -6.9044733 -7.7959814 -7.1811733][-5.1099358 -5.5464458 -6.7014661 -7.0009804 -8.2144489 -9.3974409 -9.7618666 -9.3652925 -8.6346159 -7.2347922 -5.947197 -6.4322963 -8.3429756 -8.6542768 -7.834199][-3.9962611 -4.6854515 -5.5051956 -5.5530219 -7.1802111 -7.9114256 -8.2122765 -8.0265083 -7.7685065 -7.55956 -7.0796213 -6.996109 -8.2726383 -7.7633152 -7.1623545][-3.2743759 -2.7480381 -2.7855802 -3.076617 -3.8703442 -4.6664858 -4.7550306 -5.2748518 -5.5548735 -5.5218925 -5.6484408 -7.0106559 -9.2063179 -8.7414284 -7.5630445][-4.9761796 -4.0923772 -3.6212418 -2.274832 -2.2739115 -1.2618012 -0.47562504 -1.9548941 -2.359807 -3.0881538 -3.5874624 -5.086607 -7.6169004 -8.1895685 -7.2065845][-6.250875 -6.05093 -4.4804277 -2.3590825 -1.1651759 1.4005079 2.9585347 2.7803826 2.7919078 0.43154621 -1.4210548 -3.2886734 -6.1638784 -7.0005145 -6.2513447][-8.9539261 -7.9699235 -5.8496704 -2.199156 -0.18386412 2.7747359 5.0503125 6.420507 6.3304038 3.3239408 0.54595327 -2.3484333 -6.3516335 -6.7273884 -5.4530716][-8.8536491 -8.4663181 -6.5143094 -2.7814951 0.54412031 4.0956511 5.9668641 6.6569333 7.2797942 5.18304 1.8983078 -1.8895409 -6.7441 -7.116498 -6.357491][-6.7599454 -6.6668773 -5.3394222 -2.2879808 -0.061902046 3.6167998 5.1233582 6.230516 7.0872827 5.0368962 2.9269652 -0.90578985 -5.9089046 -7.9374571 -7.9602289][-4.131906 -3.0957885 -2.282131 -0.46217012 1.0245886 2.5349603 3.3201852 4.61371 4.9530625 3.8616948 2.4481082 -0.96452427 -5.6072092 -7.4622531 -8.4698257][-6.227715 -4.477375 -3.3620806 -1.8075795 -1.1192145 -0.94337177 -0.091008186 0.29505062 -0.22711229 -0.87592268 -1.7029443 -4.6200743 -7.5504589 -8.9663219 -9.0249577][-8.94327 -7.3332229 -5.8485417 -4.2762032 -3.8390722 -3.5359459 -3.4048853 -3.8810573 -4.258585 -4.8262782 -6.1579947 -8.3061047 -9.6544542 -10.423928 -9.7485256][-10.440924 -9.48118 -7.6403227 -6.1853194 -5.4973288 -4.800292 -5.2693186 -6.4844151 -7.2791777 -7.8079453 -8.2135906 -9.0376453 -9.92425 -9.2750816 -8.2465267][-9.0628738 -8.1141157 -7.2619019 -6.2551646 -5.0360513 -4.7985148 -5.9341521 -6.7057714 -7.0313234 -8.0651369 -8.6986456 -8.18817 -8.3448849 -7.4275379 -6.9328866][-6.79292 -6.9484625 -6.6459322 -5.6636081 -4.8926945 -4.9980774 -5.1328392 -5.4151478 -5.9903917 -6.3954749 -6.9471602 -7.2999482 -8.6504011 -7.1257796 -6.4778752]]...]
INFO - root - 2017-12-15 13:37:51.042702: step 18110, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.677 sec/batch; 59h:09m:47s remains)
INFO - root - 2017-12-15 13:37:57.646083: step 18120, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 57h:37m:32s remains)
INFO - root - 2017-12-15 13:38:04.268488: step 18130, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.696 sec/batch; 60h:48m:53s remains)
INFO - root - 2017-12-15 13:38:10.865674: step 18140, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 57h:47m:50s remains)
INFO - root - 2017-12-15 13:38:17.477886: step 18150, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 56h:28m:08s remains)
INFO - root - 2017-12-15 13:38:24.028822: step 18160, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 57h:38m:00s remains)
INFO - root - 2017-12-15 13:38:30.596928: step 18170, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 55h:48m:58s remains)
INFO - root - 2017-12-15 13:38:37.234042: step 18180, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 59h:17m:04s remains)
INFO - root - 2017-12-15 13:38:43.882606: step 18190, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 56h:56m:18s remains)
INFO - root - 2017-12-15 13:38:50.504717: step 18200, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 57h:45m:00s remains)
2017-12-15 13:38:51.029324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0828767 -6.1075077 -6.2353253 -5.428472 -5.7472911 -5.5972648 -5.2203755 -4.8815355 -4.0914211 -3.5738161 -2.3931561 -4.2280364 -5.6703696 -5.1063004 -4.0595264][-5.5035477 -5.4941807 -4.5724382 -4.0986762 -4.6447597 -4.48589 -3.9227262 -3.9389679 -4.1687608 -3.9478111 -2.3015759 -3.5153513 -4.57101 -4.4134793 -3.4650505][-1.5055094 -2.1014442 -2.3386741 -2.0216959 -2.9158089 -3.6109064 -3.3990583 -3.3276267 -3.0422595 -2.4815593 -1.9138103 -3.7460601 -6.2238526 -5.6010132 -3.7306449][-1.7684002 -1.4538455 -1.2101922 -1.203002 -1.8706746 -2.7314434 -3.5183318 -3.1127009 -1.9383664 -1.5624781 -0.97738647 -3.1310964 -5.7732663 -6.0819597 -5.5677767][-3.1256084 -2.7078669 -2.3615465 -1.8966691 -1.5111461 -1.2245603 -1.4960308 -1.3951921 -1.2355504 -1.067235 -0.74593687 -3.5413973 -6.5841765 -7.305747 -6.7743649][-3.4022834 -3.0630434 -1.7282043 -1.5474334 -0.79612923 0.61060095 1.1236444 1.6173472 0.69896317 0.14028215 -0.2617116 -3.3548968 -6.0428972 -6.2751918 -6.2591295][-4.8755059 -3.5107431 -2.1949522 -0.2053318 0.83433962 2.0333757 2.9276462 3.5184302 2.9424176 1.6146178 -0.010331631 -2.9928606 -5.9179273 -6.9845262 -7.1130219][-6.595274 -5.0078225 -3.2284369 -1.2910624 0.04940176 1.5060387 3.2208743 3.5422416 3.3822846 2.2649064 1.0312366 -3.0910451 -6.999959 -7.2743731 -6.5022087][-6.5372 -5.5399504 -3.0667706 -1.3127313 -0.67393541 0.45991468 1.9347563 3.300406 4.0345798 3.186254 2.0024104 -2.2113721 -6.2037959 -7.9557762 -6.7610803][-6.3504009 -5.206275 -4.4969959 -2.6194782 -1.0833158 0.21841764 1.1295843 1.639987 1.8053074 1.8931823 1.7786837 -1.713943 -5.8260365 -7.949482 -7.4927168][-8.3572006 -7.9325314 -6.7495117 -4.827177 -2.8578243 -1.6298256 -0.84663439 -0.5732975 -0.68821716 -1.1159658 -1.7665973 -4.5609469 -7.2448745 -8.1335611 -7.7444048][-12.392376 -11.20202 -8.2149067 -6.3629885 -5.3753042 -3.8286324 -3.0005283 -3.0734708 -3.1811695 -3.5830112 -4.5190296 -6.039454 -7.0524015 -7.9535222 -8.46707][-12.522009 -10.650332 -8.0043259 -5.4874115 -4.4618254 -3.3054526 -2.7620518 -2.9855664 -3.6579506 -4.3925695 -4.8198457 -6.851624 -8.18949 -7.0079322 -5.3618894][-11.588478 -9.416049 -7.5709987 -5.4858956 -4.25075 -3.2006545 -2.4896135 -2.0118473 -2.5109148 -3.9001427 -5.6427479 -6.9724426 -7.5514746 -7.262434 -6.3240428][-8.2001829 -6.9247804 -5.539269 -4.4981966 -3.9941721 -2.8469813 -1.3721757 -0.97533178 -1.6544194 -2.2756093 -3.3201311 -6.4177041 -8.3647308 -7.8985996 -7.663888]]...]
INFO - root - 2017-12-15 13:38:57.646333: step 18210, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 59h:00m:53s remains)
INFO - root - 2017-12-15 13:39:04.212261: step 18220, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 56h:42m:49s remains)
INFO - root - 2017-12-15 13:39:10.769925: step 18230, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 57h:12m:32s remains)
INFO - root - 2017-12-15 13:39:17.352191: step 18240, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 56h:36m:24s remains)
INFO - root - 2017-12-15 13:39:23.890860: step 18250, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 57h:13m:57s remains)
INFO - root - 2017-12-15 13:39:30.386038: step 18260, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 57h:36m:45s remains)
INFO - root - 2017-12-15 13:39:37.022577: step 18270, loss = 0.12, batch loss = 0.07 (11.6 examples/sec; 0.692 sec/batch; 60h:26m:15s remains)
INFO - root - 2017-12-15 13:39:43.587440: step 18280, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 56h:22m:05s remains)
INFO - root - 2017-12-15 13:39:50.087177: step 18290, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 56h:44m:52s remains)
INFO - root - 2017-12-15 13:39:56.669512: step 18300, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.642 sec/batch; 56h:00m:00s remains)
2017-12-15 13:39:57.151736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.141335 -6.2019162 -6.04929 -5.9627161 -7.1311016 -6.7298083 -6.7148533 -6.4056234 -5.9131637 -5.6665025 -5.2711377 -8.6872768 -11.938702 -14.193071 -12.868473][-7.8342829 -8.67187 -9.1116238 -8.2386742 -7.5523348 -7.7255106 -7.145175 -6.3381968 -5.8521881 -5.1584287 -5.1275859 -8.2571983 -11.530538 -14.788731 -14.48451][-6.5756946 -7.6074071 -9.2900476 -8.5204134 -7.9703269 -7.109725 -6.3538189 -6.5419436 -6.4391813 -5.6181512 -5.3630738 -8.6419287 -10.660932 -13.336862 -13.714796][-4.1674714 -5.3703127 -5.9807563 -6.2197261 -6.7115121 -6.0391097 -4.525713 -3.8520648 -4.4100094 -5.2371321 -5.4615951 -8.9188852 -11.396063 -14.19453 -13.884178][-4.0939074 -4.037879 -4.5104322 -3.5307498 -2.7408907 -1.9251084 -1.8015294 -2.497458 -3.3386562 -4.0600204 -4.9638538 -8.8971748 -11.02506 -13.430058 -13.549046][-3.5083838 -3.3193977 -2.591692 -2.110496 -1.8155055 0.023172855 1.649756 1.9759579 0.74229527 -1.7428539 -3.8023939 -8.200079 -10.96225 -13.448511 -12.656246][-5.8117428 -5.5842581 -4.3862619 -1.635818 0.29040861 1.9715328 3.1916108 4.2889886 4.3532681 1.799159 -1.6508641 -7.1835175 -10.178179 -12.743669 -12.572733][-4.552464 -4.283731 -3.44392 -0.66027355 1.4791574 3.0279298 3.7796798 4.0975318 4.4693084 3.7981143 1.3735051 -5.0361547 -9.0243263 -11.526551 -12.177726][-3.4012961 -3.4099078 -2.0034761 0.091074944 1.4123154 3.247807 3.898479 4.5900426 3.9146762 3.0615401 1.85921 -3.5971985 -7.1698 -11.411889 -12.157017][-2.780695 -2.3503129 -1.6900854 -0.4349122 1.2435088 2.6590858 2.7755666 3.4709496 3.2964959 2.4033504 1.4529161 -3.1982336 -7.0626516 -10.007893 -10.474155][-4.6149545 -4.1550121 -3.659107 -2.7289488 -2.1491418 -0.73810577 0.62210083 1.4232426 0.64407349 -0.11950493 -1.7321639 -5.8365707 -8.7735968 -10.956154 -9.4235525][-7.63709 -7.0215373 -7.0142136 -5.8052735 -3.9905682 -3.7431507 -3.7401252 -2.6501589 -2.8990705 -3.7657907 -4.4807658 -7.7594032 -10.28042 -11.602451 -10.705559][-8.7743788 -7.9582634 -7.0850282 -7.08167 -6.7757998 -5.6245441 -5.0538216 -5.0802536 -6.2661881 -6.67718 -8.2460985 -10.867292 -11.698833 -12.146025 -10.898983][-7.5767813 -8.0654268 -7.7732205 -7.54377 -6.8896589 -6.1943917 -6.2493896 -5.9086328 -5.8932538 -7.3562965 -8.4383974 -9.9103451 -10.28386 -10.63077 -10.531245][-6.8162727 -7.1933789 -7.0411348 -6.3846927 -6.8466229 -7.0910864 -6.5690918 -5.886147 -6.2504249 -6.9144092 -7.8624878 -9.7481956 -11.311365 -11.251278 -10.546677]]...]
INFO - root - 2017-12-15 13:40:03.668484: step 18310, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 57h:13m:01s remains)
INFO - root - 2017-12-15 13:40:10.262852: step 18320, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 59h:07m:40s remains)
INFO - root - 2017-12-15 13:40:16.866447: step 18330, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 58h:08m:03s remains)
INFO - root - 2017-12-15 13:40:23.463117: step 18340, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 57h:31m:44s remains)
INFO - root - 2017-12-15 13:40:30.087538: step 18350, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 57h:49m:47s remains)
INFO - root - 2017-12-15 13:40:36.696463: step 18360, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 57h:05m:51s remains)
INFO - root - 2017-12-15 13:40:43.350256: step 18370, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 57h:16m:21s remains)
INFO - root - 2017-12-15 13:40:49.898199: step 18380, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 56h:03m:35s remains)
INFO - root - 2017-12-15 13:40:56.460454: step 18390, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 58h:27m:25s remains)
INFO - root - 2017-12-15 13:41:03.096203: step 18400, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 59h:27m:48s remains)
2017-12-15 13:41:03.612504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2605381 -7.3307166 -7.0739393 -7.9058924 -8.4163589 -8.7149134 -8.3925209 -7.2544394 -7.733942 -8.5067368 -7.8176785 -7.6643276 -10.598972 -10.516632 -10.403811][-6.7538867 -7.6792006 -7.6066117 -7.8684397 -7.8512783 -7.5577278 -6.1106787 -5.3557181 -5.9133215 -5.9035487 -6.1647744 -7.0399613 -10.823177 -10.33901 -10.867879][-6.3544912 -6.4135494 -7.3597097 -7.2720366 -7.378726 -7.9206238 -6.58073 -4.7703643 -4.08035 -3.8499427 -4.1915197 -5.6574993 -9.6636276 -10.628943 -11.094605][-5.9792876 -8.103899 -8.2867212 -7.06957 -8.0296726 -7.6696978 -6.4162984 -4.4244394 -3.0912983 -2.5515099 -3.8911459 -5.7615247 -8.7759075 -8.9087286 -8.8679218][-6.4466753 -7.15411 -8.108696 -8.5468369 -7.2530904 -5.2165508 -3.5173447 -2.0420628 -2.017101 -2.0230656 -2.9014897 -5.2048526 -8.2982016 -8.2598162 -8.7348671][-8.2781429 -7.3658786 -7.2289066 -6.1752973 -3.9190736 -0.34678936 4.2932148 3.151762 0.72361755 -1.0367079 -4.0353203 -4.8686814 -7.4960546 -7.6657772 -7.638083][-8.5579195 -8.0926218 -6.7147064 -4.4135547 -1.1261005 3.5252972 7.1913776 8.4009066 6.4582882 0.46445322 -4.0668831 -5.0182724 -8.2655172 -8.7101421 -8.5554581][-9.376791 -8.1167755 -7.1330996 -4.3483896 0.028979778 3.7492237 8.4348249 9.8904095 8.6570091 3.8802242 -2.3076172 -6.213429 -10.060191 -9.2281475 -9.1179123][-8.9687033 -7.7825813 -6.3395028 -5.1148906 -3.3098078 0.97966146 5.3159828 6.5294065 5.8591318 2.5533962 -1.8989396 -5.9562044 -11.813879 -12.517249 -11.266518][-8.4550676 -7.8927226 -7.2667837 -6.9260907 -5.6901641 -3.0990906 0.86049557 3.2178011 1.9816608 -1.0130692 -5.9760466 -9.176487 -14.106313 -14.978538 -14.39772][-9.87641 -9.6476555 -10.799094 -9.394558 -8.9056664 -7.5568686 -5.1428924 -4.094069 -3.39109 -4.6463861 -8.3306065 -12.320971 -16.674992 -17.31901 -15.976216][-13.023207 -11.967972 -11.555473 -11.539042 -11.36326 -10.68592 -9.9927464 -9.2731237 -8.8700571 -9.7501583 -10.725321 -11.757011 -14.532645 -16.492941 -14.962727][-15.111414 -13.834639 -13.222317 -12.60146 -12.714111 -12.794855 -12.63212 -11.932953 -10.936842 -11.344972 -11.788926 -12.333001 -13.326767 -13.578541 -12.221784][-13.162966 -13.382137 -12.139818 -11.397635 -10.963177 -11.798525 -12.08989 -11.768989 -11.419382 -10.566277 -9.5816746 -8.93008 -9.2882481 -10.454503 -9.3690968][-10.099129 -10.217077 -9.0020666 -9.2888269 -8.4520512 -8.7102938 -8.9188042 -9.4496422 -10.187962 -10.188086 -9.7742138 -8.9349174 -8.68887 -8.3627863 -8.6381979]]...]
INFO - root - 2017-12-15 13:41:10.179204: step 18410, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 57h:42m:27s remains)
INFO - root - 2017-12-15 13:41:16.831473: step 18420, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 58h:26m:07s remains)
INFO - root - 2017-12-15 13:41:23.409086: step 18430, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.689 sec/batch; 60h:04m:49s remains)
INFO - root - 2017-12-15 13:41:29.985112: step 18440, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 59h:18m:32s remains)
INFO - root - 2017-12-15 13:41:36.564940: step 18450, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.669 sec/batch; 58h:23m:32s remains)
INFO - root - 2017-12-15 13:41:43.180645: step 18460, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 57h:06m:36s remains)
INFO - root - 2017-12-15 13:41:49.762409: step 18470, loss = 0.21, batch loss = 0.17 (12.8 examples/sec; 0.626 sec/batch; 54h:34m:17s remains)
INFO - root - 2017-12-15 13:41:56.368509: step 18480, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 58h:31m:07s remains)
INFO - root - 2017-12-15 13:42:02.927471: step 18490, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 56h:16m:35s remains)
INFO - root - 2017-12-15 13:42:09.548708: step 18500, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 59h:45m:06s remains)
2017-12-15 13:42:10.067055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2685585 -5.5101123 -5.1436071 -5.6656404 -6.8542619 -7.1207275 -7.2984352 -7.8229189 -9.1777039 -9.2363291 -8.2400112 -7.0549941 -8.0725193 -8.2566786 -7.3517165][-5.9259324 -6.6886539 -6.3256435 -5.7354493 -6.0661044 -7.55149 -8.2532539 -8.310833 -9.1882229 -9.9996319 -9.6681414 -7.9938 -7.6937718 -8.5927172 -8.4756489][-4.413938 -5.924047 -6.913836 -6.4213309 -6.6486206 -6.7284741 -6.719243 -7.9260321 -8.7578621 -9.28973 -9.6404381 -8.2566538 -8.7188148 -9.5140953 -10.237436][-4.7756763 -6.1505351 -6.2998161 -5.8789654 -5.868763 -5.8771977 -5.7151823 -6.7717538 -8.2462807 -8.7148781 -8.5601339 -7.9990511 -9.4498034 -10.905125 -11.433222][-6.6339583 -7.0137014 -7.4889569 -6.4770141 -4.7702975 -1.8152037 -0.17033052 -2.4524271 -4.08442 -5.7500935 -7.1211281 -5.2609587 -6.2651577 -8.4607553 -9.5217342][-8.3274422 -7.5591826 -6.5176883 -5.1931839 -2.6032534 0.71509933 4.1031361 4.7014494 3.4537888 -0.83440733 -5.0652552 -4.6090484 -5.4547572 -6.1268649 -6.586154][-7.2971935 -7.5937538 -6.3477831 -3.310977 -0.56597996 2.6327085 5.9611292 6.699996 6.7910671 1.3882947 -4.2518086 -4.0934539 -7.4537792 -8.4821882 -6.9181218][-9.21409 -8.2690067 -7.3229623 -4.0537081 -0.55708027 3.0199852 6.7692909 7.4443064 6.6739025 2.7811413 -0.80150127 -3.425302 -8.0527468 -9.5495787 -8.8870478][-7.627049 -7.6564674 -7.7437105 -6.2786756 -4.4699287 -0.29229498 3.8870802 5.300086 5.3683071 1.9349079 -1.3219128 -4.1635017 -9.5207577 -11.036324 -10.649244][-6.5221324 -6.6484852 -6.612915 -6.2599249 -6.8738737 -4.4950666 -1.6307034 0.3640027 1.8763089 -0.23496962 -4.0639844 -6.4304023 -8.9070063 -11.456223 -12.018688][-9.7844305 -10.750066 -10.484831 -10.199898 -9.9349651 -8.9016933 -8.7644081 -7.5133829 -6.2023683 -6.3216047 -7.8365664 -10.075384 -12.864271 -12.654512 -11.686174][-14.472829 -14.697221 -14.590335 -13.569787 -12.959758 -13.351103 -14.076963 -13.556063 -12.532867 -11.914065 -11.93301 -11.890224 -12.232815 -13.161921 -12.735312][-13.180276 -13.373087 -13.310986 -13.889137 -14.408672 -13.032391 -12.345133 -13.137072 -13.077001 -12.791797 -11.97536 -11.287403 -9.9049339 -9.2089634 -8.8230114][-11.954226 -11.448732 -10.755816 -10.619055 -10.332386 -10.392849 -10.860422 -10.624448 -10.932018 -11.271203 -10.866156 -9.507638 -8.1776085 -7.2722216 -6.0800691][-8.4678545 -7.8873439 -5.836009 -4.7642608 -3.9701152 -3.6135647 -4.8814116 -5.8123875 -6.0932288 -6.4466152 -6.5710964 -6.9552884 -6.8991585 -6.8918366 -6.9206991]]...]
INFO - root - 2017-12-15 13:42:16.543507: step 18510, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 56h:20m:19s remains)
INFO - root - 2017-12-15 13:42:23.115681: step 18520, loss = 0.25, batch loss = 0.21 (12.0 examples/sec; 0.665 sec/batch; 58h:02m:22s remains)
INFO - root - 2017-12-15 13:42:29.729654: step 18530, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 59h:31m:16s remains)
INFO - root - 2017-12-15 13:42:36.336832: step 18540, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 56h:26m:04s remains)
INFO - root - 2017-12-15 13:42:43.029548: step 18550, loss = 0.12, batch loss = 0.07 (11.6 examples/sec; 0.689 sec/batch; 60h:03m:36s remains)
INFO - root - 2017-12-15 13:42:49.629641: step 18560, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 59h:18m:01s remains)
INFO - root - 2017-12-15 13:42:56.126423: step 18570, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 56h:18m:05s remains)
INFO - root - 2017-12-15 13:43:02.734374: step 18580, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 58h:59m:46s remains)
INFO - root - 2017-12-15 13:43:09.345361: step 18590, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 57h:28m:37s remains)
INFO - root - 2017-12-15 13:43:15.960715: step 18600, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 57h:53m:38s remains)
2017-12-15 13:43:16.487512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.846225 -4.4726033 -5.9250665 -6.2457361 -6.4517279 -7.02873 -6.9501562 -6.799943 -6.3272238 -6.0107236 -5.3181629 -6.666194 -9.1811028 -9.1669731 -9.1749706][-2.9909143 -4.4675441 -5.4019079 -6.4861965 -6.8359532 -6.7701988 -7.2463431 -7.6128006 -6.83135 -6.7477713 -6.6149244 -7.8465037 -9.3290787 -9.9366913 -10.7087][-2.5984325 -4.5765448 -6.1216736 -6.1889629 -6.6463075 -7.1454678 -6.6715994 -6.8448706 -6.8715906 -6.6819215 -6.7409673 -8.2021408 -9.9556761 -10.665762 -11.248991][-4.8560414 -7.0133929 -8.1371 -8.496419 -7.4751596 -6.0299177 -5.509656 -5.5880742 -5.81192 -5.4660368 -4.8157115 -6.7768731 -9.1537485 -9.625515 -10.344501][-6.8345957 -10.070837 -12.004402 -11.471523 -9.2444468 -6.0700092 -3.3954048 -3.6866515 -5.1158705 -4.6875596 -3.7817998 -4.3407364 -5.9649134 -7.488904 -9.2208138][-8.2171612 -10.963406 -12.077999 -10.782921 -7.4749961 -3.0242844 0.33761072 0.78473282 -0.66062069 -2.591145 -3.5389678 -4.3872709 -5.9717431 -6.7486038 -8.3437872][-5.9818311 -8.2938147 -9.0410919 -6.5602913 -3.0637553 0.61337137 3.6025848 4.3440485 3.1447039 0.39705276 -2.2979357 -4.3934846 -6.4451318 -7.5564246 -8.4848537][-4.8903022 -6.5785151 -7.1789284 -5.0801492 -0.72924614 4.26198 7.596427 7.255682 5.3388081 2.6335106 0.044336319 -3.3763611 -6.9957366 -8.36727 -9.1524448][-2.4777105 -3.9893932 -4.8379741 -4.0503449 -0.98809481 2.6081944 5.9051876 5.9539232 4.2259254 1.7957439 -0.13337803 -3.9744053 -8.2325935 -9.8401966 -11.679228][-1.5505137 -3.4928358 -5.3018165 -4.8415885 -2.7235436 0.047805309 3.0102062 2.9098215 1.5598602 -0.62669039 -2.3642313 -5.6111865 -9.6540089 -11.379785 -12.971095][-4.2616634 -6.2160382 -7.8598032 -7.4777431 -6.3994751 -4.6542535 -3.2090328 -2.8352017 -3.5506833 -5.2161813 -6.0487547 -9.1750813 -11.756371 -12.802349 -13.896497][-9.6979647 -10.153673 -10.571526 -10.056056 -9.3516712 -8.17935 -7.8501811 -8.6955547 -9.2548485 -9.6584053 -10.327141 -11.948692 -13.117608 -13.019711 -13.195683][-12.806224 -12.889011 -12.351863 -12.06978 -12.581899 -11.721033 -11.203724 -11.408149 -11.250179 -11.581693 -11.942839 -12.350998 -12.600216 -11.918026 -11.550777][-11.749004 -11.804607 -11.893963 -10.418997 -9.2928295 -9.3535051 -10.283114 -10.399589 -10.594578 -10.936235 -11.133604 -11.349627 -11.041052 -10.8771 -10.464399][-7.8181181 -8.7024193 -8.51817 -6.7410631 -5.7313633 -5.8730631 -6.6076307 -7.508656 -8.0437479 -8.3354378 -8.7534189 -9.7006369 -10.498781 -10.789505 -10.377655]]...]
INFO - root - 2017-12-15 13:43:23.084851: step 18610, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 56h:30m:09s remains)
INFO - root - 2017-12-15 13:43:29.719694: step 18620, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 58h:33m:16s remains)
INFO - root - 2017-12-15 13:43:36.297718: step 18630, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 57h:03m:59s remains)
INFO - root - 2017-12-15 13:43:42.858812: step 18640, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 56h:17m:07s remains)
INFO - root - 2017-12-15 13:43:49.357373: step 18650, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 55h:49m:24s remains)
INFO - root - 2017-12-15 13:43:55.993834: step 18660, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 57h:01m:43s remains)
INFO - root - 2017-12-15 13:44:02.583541: step 18670, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 56h:33m:04s remains)
INFO - root - 2017-12-15 13:44:09.187051: step 18680, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 57h:42m:48s remains)
INFO - root - 2017-12-15 13:44:15.794374: step 18690, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 58h:10m:17s remains)
INFO - root - 2017-12-15 13:44:22.423747: step 18700, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.628 sec/batch; 54h:45m:48s remains)
2017-12-15 13:44:22.900370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.19108 -8.7781534 -9.0385351 -9.1839132 -9.0325871 -9.0333118 -9.36591 -9.4360161 -9.7844391 -10.130537 -9.3416843 -8.9661121 -7.9466357 -5.7459092 -2.854732][-6.8836417 -8.3058281 -9.1443272 -8.9745579 -8.6249552 -8.6317225 -8.5765572 -9.1165962 -9.9884377 -10.469284 -9.910285 -10.720465 -10.791735 -9.2217579 -5.5023742][-7.3423562 -7.692791 -7.2352495 -7.2986312 -7.4750514 -7.0365467 -6.6608481 -6.6870174 -7.0339103 -7.9753885 -9.1249132 -11.652153 -12.561617 -10.803304 -7.9880881][-7.6273603 -8.2366123 -7.8587294 -7.44446 -6.9392366 -5.9964285 -5.5282807 -5.7404985 -6.0469737 -6.8177881 -7.2004023 -9.852932 -10.629736 -11.063974 -9.8897915][-9.3864765 -10.473736 -10.351049 -8.80562 -7.00371 -4.5078354 -3.3352959 -5.14211 -6.8388886 -7.0070009 -6.828177 -9.3737574 -11.214569 -11.263607 -9.9633684][-9.2680807 -10.041682 -9.5523529 -6.46757 -3.5883181 -1.0444946 1.5210204 0.46048021 -1.8710268 -3.9981866 -6.2746906 -8.4663925 -9.823987 -9.1744385 -7.0808258][-9.8618622 -9.1387272 -7.2426496 -3.0590031 0.043053627 3.6035953 6.6783519 6.4586349 4.653791 0.020372868 -4.5369444 -8.0083628 -9.328474 -9.0676231 -7.328939][-9.4331808 -8.3451681 -6.0730529 -2.5062354 -0.21736622 3.9016013 6.5613456 5.7529292 4.369452 0.68384027 -2.2684164 -7.4384985 -10.790379 -10.259491 -8.6591129][-5.879374 -4.5683546 -4.0051532 -2.4973643 -1.233532 0.84371662 2.452909 2.860847 2.7536349 -0.24030638 -2.5074108 -6.723815 -9.8533993 -10.374428 -10.32205][-3.3171923 -2.0611019 -1.4188323 -0.79385042 -0.2083168 0.00027418137 0.11475945 0.52074194 0.57588816 -0.66944361 -1.5641813 -6.349741 -9.1595373 -9.9364643 -11.155227][-5.8284874 -4.6889338 -4.1975465 -2.807653 -2.3807015 -3.2673476 -4.338304 -4.8289156 -5.3435421 -6.1982112 -5.5970197 -8.4865513 -10.400721 -11.895958 -12.054431][-10.437633 -8.9384193 -8.066637 -8.4934044 -9.4173765 -9.4001389 -9.3760357 -10.979059 -11.911627 -11.574621 -10.943194 -12.152479 -12.600877 -12.907053 -12.270983][-13.682704 -12.299425 -10.92226 -10.687225 -11.783475 -12.640121 -13.09048 -13.750475 -13.976675 -12.670864 -11.408116 -11.810743 -11.853907 -11.628475 -9.6405458][-13.484743 -12.214044 -11.340452 -10.477119 -10.171345 -11.427034 -12.371723 -12.221304 -11.766636 -11.624249 -11.158106 -9.8588219 -8.4429817 -8.0331278 -6.86691][-9.5756283 -9.2345591 -8.4860487 -6.4834414 -6.0441236 -5.8338647 -5.737783 -7.5032554 -8.7468739 -8.1205215 -7.8872442 -8.8377781 -8.272191 -7.2770972 -5.1225386]]...]
INFO - root - 2017-12-15 13:44:29.503836: step 18710, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 57h:42m:02s remains)
INFO - root - 2017-12-15 13:44:36.078870: step 18720, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 58h:09m:29s remains)
INFO - root - 2017-12-15 13:44:42.700043: step 18730, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 56h:58m:49s remains)
INFO - root - 2017-12-15 13:44:49.382099: step 18740, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 57h:02m:54s remains)
INFO - root - 2017-12-15 13:44:55.908581: step 18750, loss = 0.19, batch loss = 0.15 (12.4 examples/sec; 0.643 sec/batch; 56h:00m:12s remains)
INFO - root - 2017-12-15 13:45:02.552552: step 18760, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.682 sec/batch; 59h:24m:09s remains)
INFO - root - 2017-12-15 13:45:09.186902: step 18770, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 57h:03m:59s remains)
INFO - root - 2017-12-15 13:45:15.740023: step 18780, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 55h:38m:33s remains)
INFO - root - 2017-12-15 13:45:22.320826: step 18790, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 57h:29m:33s remains)
INFO - root - 2017-12-15 13:45:28.880588: step 18800, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 58h:00m:13s remains)
2017-12-15 13:45:29.381642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3343868 -5.9229012 -7.4936557 -7.7208281 -8.4803066 -9.3740444 -9.5083818 -9.0889387 -8.71406 -8.7022228 -8.4920864 -9.89377 -10.25956 -9.805584 -8.7021427][-6.4061193 -7.4700928 -9.4731836 -10.538131 -10.684601 -10.693768 -9.5303459 -9.2729425 -9.75248 -9.9121838 -9.1877451 -9.9627781 -9.6554775 -10.337936 -9.2605515][-5.7422209 -7.882308 -10.830446 -11.751722 -11.544423 -11.30558 -9.972683 -8.8307905 -9.0167789 -9.1602764 -9.0013351 -10.130003 -9.9053917 -9.9181633 -8.3963995][-6.6722126 -8.515873 -10.240802 -11.434885 -11.85597 -9.5094976 -6.7477722 -6.3849874 -7.2289658 -7.3711481 -6.7595448 -8.3793259 -8.7749834 -9.9207344 -10.249365][-7.6582956 -10.446324 -12.432072 -11.626186 -10.891312 -7.5464268 -3.3260102 -3.438683 -4.8644047 -4.8377514 -4.5360546 -5.8523526 -5.9220061 -8.485281 -9.6482449][-8.5812588 -10.740271 -12.032408 -10.021198 -7.0134072 -3.0303838 1.0508924 2.6964788 2.2238884 -1.0877666 -3.385694 -4.4792151 -4.8084393 -6.775497 -7.6207032][-8.16063 -8.6932678 -9.6588984 -5.7423682 -1.4305615 2.3221493 5.2139721 6.6708364 6.493556 2.4712467 -1.2728662 -3.5246675 -3.5509014 -5.1538539 -6.8732567][-6.6483064 -6.5131536 -6.42692 -3.777931 0.49173307 5.4103031 8.9552984 8.3353567 6.2784595 3.0412798 0.42143726 -3.0740166 -4.3762169 -6.3597479 -7.7501616][-4.8920608 -4.7851996 -5.4693856 -3.4531653 -1.3639283 2.0760102 5.2162809 5.5432816 4.4372282 1.567203 -0.73594666 -4.8951712 -6.5864468 -8.5168314 -9.5596361][-4.7000294 -5.391284 -6.5554767 -4.9354172 -3.9468136 -2.1914384 0.17668915 1.2472596 0.542429 -0.64717722 -1.6177707 -5.5556378 -7.5706277 -10.330524 -12.203781][-7.056735 -7.72338 -9.21038 -8.4177933 -7.6249 -6.5834541 -5.4631891 -4.267508 -3.9516397 -4.0132647 -4.8829479 -8.4980106 -10.079728 -12.301579 -12.83626][-10.748032 -10.984406 -11.320966 -10.983736 -11.465364 -11.080116 -10.23579 -10.329178 -10.104051 -8.4804277 -8.5497026 -10.920321 -11.538229 -12.159893 -11.635122][-11.862539 -11.834621 -11.902727 -12.00799 -13.27969 -13.553261 -13.408195 -12.95179 -11.983529 -11.799278 -11.824039 -11.667525 -11.327116 -11.106548 -10.564803][-10.115282 -9.3555489 -9.4335394 -10.044962 -10.600798 -11.769972 -12.214753 -11.212952 -10.718197 -10.079717 -10.100513 -9.559639 -8.9166021 -8.5308514 -8.6465425][-6.3747716 -7.114161 -7.460825 -7.0136647 -5.8842611 -6.1946497 -6.22322 -6.46519 -6.7932639 -6.7262464 -7.0009136 -8.1202631 -8.9247856 -8.8189144 -8.4857006]]...]
INFO - root - 2017-12-15 13:45:35.955583: step 18810, loss = 0.22, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 56h:12m:28s remains)
INFO - root - 2017-12-15 13:45:42.580255: step 18820, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.656 sec/batch; 57h:09m:08s remains)
INFO - root - 2017-12-15 13:45:49.209360: step 18830, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 56h:05m:54s remains)
INFO - root - 2017-12-15 13:45:55.755519: step 18840, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 57h:56m:33s remains)
INFO - root - 2017-12-15 13:46:02.225753: step 18850, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 56h:44m:54s remains)
INFO - root - 2017-12-15 13:46:08.842858: step 18860, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 57h:54m:34s remains)
INFO - root - 2017-12-15 13:46:15.498757: step 18870, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 57h:46m:35s remains)
INFO - root - 2017-12-15 13:46:22.008137: step 18880, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 55h:46m:10s remains)
INFO - root - 2017-12-15 13:46:28.564482: step 18890, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 55h:54m:20s remains)
INFO - root - 2017-12-15 13:46:35.185767: step 18900, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 58h:43m:19s remains)
2017-12-15 13:46:35.693551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2654152 -7.3924613 -8.1431541 -8.6763315 -10.241501 -10.888451 -11.732811 -11.727565 -11.451536 -11.271966 -10.347385 -10.92831 -11.73514 -11.961132 -11.469547][-8.4348993 -10.376274 -11.465014 -11.515938 -12.189557 -12.725519 -13.520311 -14.584399 -15.193021 -14.650989 -13.882575 -13.943176 -13.851055 -14.215376 -12.784278][-6.2093229 -9.1705055 -11.828309 -11.757685 -12.199394 -12.778743 -13.010114 -13.403116 -13.570713 -14.156251 -14.299433 -14.054993 -14.418196 -14.507987 -13.962628][-8.6210022 -10.658058 -12.595734 -11.717405 -12.025993 -11.359969 -11.076115 -12.763309 -13.115213 -12.113043 -11.849871 -13.688007 -15.205802 -14.86747 -13.777562][-10.301685 -12.942255 -14.236205 -11.871563 -8.89686 -4.9836817 -4.2767811 -7.957509 -10.615031 -10.806114 -11.368801 -12.116495 -13.513458 -15.92169 -15.412385][-12.830988 -12.617519 -12.275303 -11.746548 -8.2322464 -1.6284132 4.581748 3.129385 -0.73373747 -5.9763842 -10.16193 -9.9383965 -12.187901 -14.58288 -15.925969][-15.817008 -14.640893 -11.672426 -8.1471739 -4.3699417 -0.2964468 6.6521626 8.6906252 7.9023695 0.012496948 -7.7585955 -8.7455044 -11.354811 -12.522198 -13.70887][-15.596705 -14.780025 -13.631899 -7.1172352 -1.2614756 4.1920137 8.9195309 7.6444426 7.8774776 2.4541359 -4.8448744 -9.1613865 -12.983942 -13.669003 -13.441442][-12.784515 -12.924899 -14.028021 -10.03751 -4.8152719 2.6502037 7.8540211 7.07298 5.9734573 -0.49780941 -6.8975868 -11.27281 -15.764126 -16.700045 -15.629848][-9.4825268 -9.54538 -10.79556 -10.060999 -8.79758 -3.4996989 2.4655094 3.6655779 2.6548834 -2.9509335 -9.1886005 -13.319534 -17.394215 -18.593246 -18.940281][-13.312516 -13.773919 -13.36343 -12.274298 -12.802466 -11.416643 -8.1930647 -6.1371446 -5.8479862 -8.0228834 -11.694256 -15.654194 -17.546638 -17.820782 -17.740662][-18.64624 -17.543823 -16.665857 -16.913691 -17.094738 -15.984712 -15.850336 -14.846854 -13.028482 -14.339143 -15.883186 -17.254135 -16.51685 -16.222334 -14.447512][-14.992778 -14.486057 -14.895248 -15.712835 -15.681931 -14.719809 -14.017641 -13.844901 -14.267059 -13.889137 -13.433594 -14.893991 -14.694403 -13.430676 -11.682469][-11.740572 -11.582186 -10.874572 -11.02618 -11.632147 -12.451704 -12.399935 -11.257013 -11.255376 -11.715288 -11.967273 -11.565048 -11.36116 -10.519093 -10.57375][-8.129055 -7.3393254 -6.2290344 -5.9187155 -6.0328283 -6.4022207 -6.9290061 -8.022192 -8.1082516 -7.8549166 -7.95673 -9.1273975 -9.8329563 -10.279572 -11.248293]]...]
INFO - root - 2017-12-15 13:46:42.325961: step 18910, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 56h:37m:32s remains)
INFO - root - 2017-12-15 13:46:48.917173: step 18920, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 59h:09m:57s remains)
INFO - root - 2017-12-15 13:46:55.552677: step 18930, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 59h:13m:06s remains)
INFO - root - 2017-12-15 13:47:02.087670: step 18940, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 56h:32m:28s remains)
INFO - root - 2017-12-15 13:47:08.800709: step 18950, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 58h:23m:48s remains)
INFO - root - 2017-12-15 13:47:15.422822: step 18960, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 56h:45m:49s remains)
INFO - root - 2017-12-15 13:47:22.046606: step 18970, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 58h:20m:31s remains)
INFO - root - 2017-12-15 13:47:28.646579: step 18980, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 57h:42m:52s remains)
INFO - root - 2017-12-15 13:47:35.357664: step 18990, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 57h:16m:14s remains)
INFO - root - 2017-12-15 13:47:41.994417: step 19000, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.698 sec/batch; 60h:48m:56s remains)
2017-12-15 13:47:42.503974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.557848 -5.4215527 -5.2805042 -4.8002353 -5.0591784 -5.0895095 -5.0627403 -4.9463968 -4.7552419 -4.6110158 -4.1376128 -5.2549472 -6.724977 -7.6058235 -6.1942382][-6.1095 -5.0289416 -4.2791882 -4.3963537 -5.1567807 -5.199861 -4.9601564 -4.9707103 -4.975925 -5.0948715 -4.6899748 -5.5829053 -6.6787314 -7.690074 -6.78868][-4.4088392 -4.9073405 -5.5030193 -4.4881759 -4.5795784 -4.7300925 -4.7972274 -4.8424435 -4.9738207 -5.2766428 -4.9291048 -5.8351345 -6.6841755 -7.7233887 -7.1335106][-5.4060721 -5.1407671 -5.6150155 -5.6290779 -6.1559224 -5.4266787 -4.9127617 -5.2980638 -5.6221046 -5.2537351 -4.6486254 -5.5240083 -6.7144852 -8.1901073 -7.8147135][-4.7977066 -5.4052243 -6.1674628 -6.0918965 -5.5163407 -3.6575584 -2.762934 -3.5542314 -4.5009103 -4.8248572 -4.3826623 -5.3267937 -6.6175046 -8.150383 -8.0582438][-5.9879184 -5.7493014 -6.2009377 -5.2832303 -4.0330954 -1.238831 1.4181542 0.91873264 -1.0759773 -2.8701544 -3.7264183 -4.7846494 -6.1350632 -7.87061 -7.9032269][-8.1586838 -7.5292969 -6.555706 -4.1354785 -2.5358882 0.65103149 3.9427547 4.3705959 3.2560091 -0.30829954 -3.4353511 -5.1377869 -6.5907469 -7.8492551 -7.5700169][-9.0474129 -7.9365435 -7.0531974 -4.0453925 -1.256362 2.834692 5.9564571 5.8760567 4.9308739 1.4961505 -1.3510709 -4.0495043 -6.5533252 -8.01245 -7.9072418][-6.8753557 -6.4360323 -6.3322792 -3.7582121 -1.5502763 1.6913548 4.1977949 4.0874786 3.5018606 1.0195017 -1.0504909 -4.0286684 -6.7559614 -8.395216 -7.7681856][-5.5102863 -4.7147622 -4.7759314 -3.3470249 -2.6860013 -0.66133261 1.3711057 1.2203836 0.25237513 -1.6033359 -2.7235839 -4.5438008 -6.7041984 -8.6047506 -8.8401613][-8.7755184 -7.6848664 -7.1812606 -5.6608891 -5.1881819 -4.5942955 -4.2513313 -4.8004704 -5.0667572 -5.5142546 -5.8985968 -7.8315592 -9.0272846 -10.134975 -8.7344618][-13.146786 -11.563202 -10.164104 -9.07703 -8.804718 -8.3736639 -8.887145 -9.6413183 -9.6577549 -9.8196554 -9.6956005 -10.067944 -9.7844143 -10.107918 -8.3704176][-13.197906 -11.836332 -10.988382 -10.167315 -9.5740137 -8.7382545 -9.0789223 -9.7853031 -10.105221 -9.6773243 -9.1113853 -9.87302 -10.397875 -9.9374161 -7.3637767][-10.887064 -10.303533 -10.040527 -9.6395512 -9.5551109 -9.2062454 -9.0963783 -8.5821056 -8.1164541 -8.1298685 -8.0551348 -7.7399421 -7.4783649 -7.2793174 -6.178093][-8.6606054 -7.7327938 -6.7447214 -6.8384438 -6.9989605 -6.945816 -7.1800723 -6.9468565 -6.4028277 -5.9333491 -5.7092028 -6.5652714 -7.3973446 -7.6932731 -7.1908793]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 13:47:49.052763: step 19010, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 55h:41m:12s remains)
INFO - root - 2017-12-15 13:47:55.629906: step 19020, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 57h:14m:57s remains)
INFO - root - 2017-12-15 13:48:02.243010: step 19030, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 58h:24m:27s remains)
INFO - root - 2017-12-15 13:48:08.886704: step 19040, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 57h:10m:46s remains)
INFO - root - 2017-12-15 13:48:15.555129: step 19050, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 57h:46m:48s remains)
INFO - root - 2017-12-15 13:48:22.149691: step 19060, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 57h:10m:16s remains)
INFO - root - 2017-12-15 13:48:28.734500: step 19070, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 57h:12m:48s remains)
INFO - root - 2017-12-15 13:48:35.343523: step 19080, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 57h:24m:39s remains)
INFO - root - 2017-12-15 13:48:42.013620: step 19090, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 58h:06m:25s remains)
INFO - root - 2017-12-15 13:48:48.685788: step 19100, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 57h:20m:59s remains)
2017-12-15 13:48:49.186136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3890576 -6.834878 -7.2957549 -6.8843665 -7.7998209 -7.7471309 -7.2773571 -6.9523296 -7.3844681 -8.5461063 -9.4874783 -10.468113 -11.146629 -11.183825 -8.8996754][-7.5629187 -7.2362666 -7.5837421 -7.2455826 -7.2235036 -7.1439304 -7.1721668 -7.0036478 -7.1173983 -6.8661819 -7.4555721 -9.5814514 -10.345533 -10.129303 -8.6213417][-2.575078 -4.8937206 -7.4859939 -7.4791007 -7.0980253 -6.1579537 -5.3575163 -5.8877416 -6.952754 -7.5781889 -8.0667725 -9.1306725 -9.5888958 -10.381334 -9.41477][-2.806695 -3.1722407 -4.5411491 -4.7738261 -5.1161041 -4.0916591 -3.4075091 -4.1097012 -6.1919384 -7.5115223 -8.234272 -9.6517267 -10.626628 -11.833994 -11.411669][-2.1743059 -2.1964288 -3.5281844 -3.5705628 -2.4592102 -0.42919493 0.72924376 -2.150012 -5.3792696 -6.0872154 -7.0034256 -9.4263735 -11.617672 -13.25651 -13.085438][-3.0956655 -2.4093614 -1.4665627 -0.28667259 0.60783195 2.2167554 3.4619441 2.7793326 0.0058169365 -3.444993 -5.5774007 -6.9554992 -8.84658 -11.877958 -13.024913][-5.2955651 -3.9844947 -2.9890847 -0.70819044 1.523118 3.0530648 4.9650769 5.7701659 5.1764855 0.15803051 -4.5251637 -6.5497489 -8.5243206 -10.72827 -11.473547][-5.1012673 -3.2870245 -2.4963441 -0.66683912 1.6044755 3.7904973 5.5663152 5.3680577 5.6784558 3.7986531 1.1482744 -3.4210155 -6.7414055 -8.6169844 -10.636962][-3.9043195 -2.7069795 -2.4571521 -1.6584134 -1.5857139 1.9705925 5.0664582 3.9963508 2.8517709 0.934196 -1.0811019 -4.121767 -7.1185241 -10.485907 -12.381865][-1.690495 -2.8692765 -4.3655987 -4.025619 -3.71115 -1.8132327 0.079772472 1.3197927 1.5747313 -1.0491261 -3.3982425 -5.951355 -7.9032278 -12.151544 -15.324869][-4.5121036 -6.6090083 -8.3712368 -7.2753825 -6.875206 -6.1538835 -5.82875 -4.7771645 -3.9472034 -5.1442137 -7.067431 -10.709379 -12.631535 -13.335259 -13.768696][-10.620188 -12.464555 -13.259062 -11.937799 -11.062037 -10.174743 -9.9935474 -10.265955 -9.6770229 -9.6439867 -10.151869 -11.760394 -13.074058 -14.119263 -13.645638][-13.398529 -12.339224 -11.289543 -11.780533 -12.122536 -10.79673 -10.225531 -10.129601 -10.060841 -10.764418 -11.515847 -11.217844 -11.030006 -11.19899 -11.0672][-11.122798 -9.634573 -9.2972107 -7.9466209 -7.7240705 -7.0441236 -7.5995317 -7.8582478 -7.4059544 -7.5029535 -8.5215826 -8.9054413 -7.6353359 -6.8000269 -6.61703][-7.233932 -6.6331096 -5.5585494 -3.1807053 -2.0951765 -2.8380306 -2.8342831 -3.384517 -4.3462105 -4.9397397 -5.3504987 -5.9072876 -5.5624609 -4.6147184 -4.8859434]]...]
INFO - root - 2017-12-15 13:48:55.755648: step 19110, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 56h:06m:45s remains)
INFO - root - 2017-12-15 13:49:02.363618: step 19120, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 55h:47m:39s remains)
INFO - root - 2017-12-15 13:49:09.006125: step 19130, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 56h:56m:25s remains)
INFO - root - 2017-12-15 13:49:15.534366: step 19140, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 56h:44m:51s remains)
INFO - root - 2017-12-15 13:49:22.159166: step 19150, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 56h:07m:44s remains)
INFO - root - 2017-12-15 13:49:28.859304: step 19160, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 59h:07m:04s remains)
INFO - root - 2017-12-15 13:49:35.490635: step 19170, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.682 sec/batch; 59h:22m:07s remains)
INFO - root - 2017-12-15 13:49:42.026543: step 19180, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 55h:47m:50s remains)
INFO - root - 2017-12-15 13:49:48.650218: step 19190, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 57h:15m:36s remains)
INFO - root - 2017-12-15 13:49:55.273258: step 19200, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 58h:53m:39s remains)
2017-12-15 13:49:55.775091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6119642 -5.7642431 -5.2161317 -5.1869807 -6.1425743 -7.1524663 -7.6578517 -6.7699828 -6.2839971 -6.5470824 -5.82015 -7.5796838 -11.055643 -11.247865 -10.268927][-4.8513551 -5.7712116 -6.1008348 -6.126265 -7.0516634 -8.1042986 -9.4012241 -9.3747349 -8.9290609 -8.2674665 -7.6347051 -9.5991983 -12.960115 -13.105572 -11.539725][-2.3669305 -2.8290861 -3.7313466 -3.9260607 -5.5203013 -6.7649074 -7.1364861 -7.5031376 -8.2769928 -7.8996615 -7.0152769 -9.0375814 -12.444897 -13.155547 -11.88855][-3.5983644 -3.9868517 -3.7427049 -2.3533726 -2.497591 -2.6100769 -3.105041 -4.1901622 -4.5695648 -4.858326 -5.0051708 -6.9040775 -9.90534 -10.978577 -11.054329][-5.5107613 -5.8845778 -5.5444617 -2.9036553 -1.4906597 -0.93340158 -0.99553919 -0.97809792 -0.87557125 -1.6788149 -2.004667 -4.8043218 -9.1417828 -9.9693317 -9.5568991][-8.1616468 -7.43186 -5.7402692 -3.5585949 -2.7593186 -0.11375237 2.2127671 2.5194569 2.6126719 1.7574701 1.218092 -2.5803821 -7.4146428 -9.3310566 -9.9272556][-9.3143139 -8.7399569 -6.4405532 -3.496074 -1.7937915 0.60977793 2.5041485 4.2146635 5.9937477 4.8853045 2.6548605 -1.7044215 -5.920928 -7.0932627 -7.851727][-6.8085623 -6.9489908 -6.5233717 -3.3260307 -0.64009762 1.9348745 3.8880272 4.3068628 4.9593229 4.5810165 2.9825058 -1.4869556 -5.5479913 -6.4716082 -5.9204111][-7.1179795 -5.9757109 -4.4784374 -2.0598965 -0.5230875 0.49013996 1.6394625 3.2432652 3.638278 2.9290428 1.6412101 -1.9717433 -5.7269 -5.6090317 -4.2725587][-7.0609045 -6.1872368 -3.9224405 -2.2081494 -1.5492806 -1.0262165 -0.10705233 0.087376595 0.59915924 0.67721653 0.29209709 -2.9858692 -6.4355326 -5.7462168 -4.0616097][-8.4821653 -7.8391638 -6.5523348 -5.1860418 -4.4234037 -4.3442397 -4.3022566 -4.3619919 -3.882823 -3.4124739 -3.9286332 -7.137785 -9.0956783 -7.3639 -4.8013983][-11.647148 -10.1975 -7.2932711 -5.5585432 -4.503304 -4.4024448 -5.0738606 -6.6489916 -7.3924737 -7.5902615 -8.0754662 -9.5489483 -10.974541 -9.8571815 -7.7244711][-11.756571 -9.2629824 -6.7016883 -5.467063 -5.0031509 -5.5149856 -6.0770946 -6.4371476 -7.1127977 -7.1474118 -6.9655228 -8.6399555 -10.056693 -8.373888 -5.5685854][-12.52404 -10.901789 -8.4667587 -6.9826112 -5.7761374 -6.0599351 -5.9328437 -4.7423778 -3.8070569 -3.955246 -5.1774015 -5.0592217 -5.1752062 -4.7429667 -3.7880762][-8.37491 -8.492672 -7.0984254 -5.9151249 -5.9315505 -5.4584446 -4.7125206 -4.7859173 -3.843843 -1.7812631 -0.87918043 -2.9634933 -4.303607 -4.2113171 -4.446804]]...]
INFO - root - 2017-12-15 13:50:02.343554: step 19210, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 57h:27m:52s remains)
INFO - root - 2017-12-15 13:50:08.909292: step 19220, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.690 sec/batch; 60h:03m:08s remains)
INFO - root - 2017-12-15 13:50:15.496087: step 19230, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 57h:15m:34s remains)
INFO - root - 2017-12-15 13:50:22.210442: step 19240, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.681 sec/batch; 59h:13m:28s remains)
INFO - root - 2017-12-15 13:50:28.667605: step 19250, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 55h:35m:50s remains)
INFO - root - 2017-12-15 13:50:35.244990: step 19260, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.686 sec/batch; 59h:39m:13s remains)
INFO - root - 2017-12-15 13:50:41.855443: step 19270, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 55h:46m:26s remains)
INFO - root - 2017-12-15 13:50:48.535692: step 19280, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 58h:47m:51s remains)
INFO - root - 2017-12-15 13:50:55.190595: step 19290, loss = 0.13, batch loss = 0.08 (11.3 examples/sec; 0.708 sec/batch; 61h:37m:31s remains)
INFO - root - 2017-12-15 13:51:01.717162: step 19300, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 56h:45m:32s remains)
2017-12-15 13:51:02.250017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1372972 -4.4547877 -3.5663946 -2.4598324 -1.9460745 -0.95154238 -0.59783125 -1.0508881 -1.3120198 -1.5248842 -1.7062898 -3.5476885 -4.7456708 -4.6244822 -4.4407377][-4.3662333 -2.7230105 -2.0572479 -1.4023504 -1.7165108 -1.2622075 -1.0220881 -1.4697042 -1.8565209 -2.1768045 -1.8514745 -3.735069 -4.47733 -4.9005232 -4.7202916][-3.7508526 -3.7294347 -3.2721686 -2.8734105 -3.2075613 -3.0030806 -2.9014394 -3.0898929 -3.2282341 -2.6841824 -2.3131716 -5.1677694 -6.5057907 -6.8907042 -7.598021][-6.2248573 -5.7468095 -5.2684441 -4.3039565 -3.7645092 -3.8335133 -5.0105896 -4.7266808 -3.9686093 -3.6586246 -3.0183904 -5.435904 -7.6399689 -9.6255894 -10.45431][-7.6851125 -8.171298 -7.1195459 -4.6415935 -3.4610035 -3.4306998 -4.1460991 -4.7241626 -4.9789877 -4.4280863 -3.9014626 -7.7996836 -10.341097 -12.156723 -12.730173][-8.5985384 -7.1107168 -4.7231336 -3.0694427 -1.84427 -0.71353817 -1.9896224 -2.9330604 -3.0323074 -3.6814833 -4.6520391 -7.7695742 -9.6359768 -11.425458 -10.69165][-5.3644032 -3.1787424 -1.5378938 -0.32594395 0.45290232 0.73663187 0.14376163 -0.53140545 -1.2558351 -2.7562053 -3.9629989 -7.53531 -9.2724791 -10.250533 -9.82031][-5.602941 -3.8575668 -1.4687638 0.1587491 1.2619734 0.98776484 0.42474461 0.4422946 0.69191504 -0.38887405 -1.7479832 -5.8598609 -7.6818662 -8.4496422 -8.8585081][-4.8306975 -3.8753426 -2.384017 -0.91502428 -0.51026487 -0.23087215 -0.1889143 -0.06997776 0.2907033 0.47655344 0.3208437 -3.9559381 -6.3553429 -7.8334646 -8.0984268][-6.8089905 -6.3662357 -5.3890395 -3.6364059 -2.7395368 -2.4292898 -1.8140199 -0.48936176 0.17398691 0.72326851 1.1498685 -2.8695941 -5.8342314 -7.3523836 -8.8563156][-10.910618 -10.080848 -8.94802 -6.6078305 -4.7109432 -3.5833187 -2.6422498 -1.8448384 -1.1825209 -0.88821983 -1.1675591 -4.7471538 -6.8469815 -7.7154703 -7.9285564][-15.25827 -13.972654 -11.791775 -8.9109745 -6.8781834 -5.7459464 -5.5247211 -5.4357233 -5.3187714 -4.8737774 -5.0856919 -7.6570826 -8.7168369 -8.2896948 -8.4876328][-13.733196 -12.948307 -10.28759 -8.0036278 -7.3169775 -6.3176379 -5.8671665 -6.3137155 -6.8840671 -7.7958641 -8.1697063 -9.4250164 -9.4354115 -7.7324085 -5.9010181][-11.206741 -10.528352 -8.9369116 -6.6633863 -5.7637768 -6.0283613 -6.3662386 -6.3812304 -6.8141632 -8.1037073 -8.5263023 -8.7286472 -8.1417723 -7.1351418 -5.61621][-7.8511086 -7.9071836 -6.9285135 -5.2248569 -4.6597543 -4.7649198 -5.0000553 -4.8830605 -6.3054485 -6.9588966 -6.8422775 -8.3846483 -9.1695356 -8.1260691 -7.5513673]]...]
INFO - root - 2017-12-15 13:51:08.811223: step 19310, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 58h:53m:58s remains)
INFO - root - 2017-12-15 13:51:15.456569: step 19320, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.686 sec/batch; 59h:40m:52s remains)
INFO - root - 2017-12-15 13:51:22.045441: step 19330, loss = 0.19, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 54h:50m:49s remains)
INFO - root - 2017-12-15 13:51:28.628788: step 19340, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 57h:48m:59s remains)
INFO - root - 2017-12-15 13:51:35.233373: step 19350, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 57h:07m:35s remains)
INFO - root - 2017-12-15 13:51:41.755511: step 19360, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 56h:16m:52s remains)
INFO - root - 2017-12-15 13:51:48.317314: step 19370, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 57h:02m:52s remains)
INFO - root - 2017-12-15 13:51:54.911498: step 19380, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 58h:57m:50s remains)
INFO - root - 2017-12-15 13:52:01.515365: step 19390, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 56h:28m:45s remains)
INFO - root - 2017-12-15 13:52:08.129387: step 19400, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 56h:04m:08s remains)
2017-12-15 13:52:08.633609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3158221 -4.9083252 -4.0308142 -3.531266 -3.8189039 -3.7951727 -3.6045051 -2.7451608 -2.2772524 -1.7361767 -1.9202464 -5.5212746 -9.9856949 -12.450069 -11.952461][-5.2663436 -4.61147 -4.4969053 -3.5828874 -3.3261082 -3.465435 -3.15173 -2.8513191 -3.0979774 -2.9745083 -3.1124365 -6.5611835 -10.063369 -11.967363 -11.965168][-2.3397052 -3.7733102 -4.956018 -4.3481503 -4.2890358 -4.5841193 -4.2812147 -3.590625 -3.5864823 -3.74712 -4.012383 -7.4818053 -10.614 -12.56922 -11.920695][-2.4093404 -2.8122435 -4.6843362 -4.3664093 -4.5113907 -3.7859044 -3.1069124 -3.7504492 -4.5585136 -4.1991959 -4.4950213 -8.4311619 -11.517579 -12.269897 -11.03545][-3.3798635 -3.7348359 -4.5938811 -4.4220986 -3.952827 -2.1559436 -1.5456247 -2.2781754 -3.84174 -4.3421803 -5.3099952 -8.7760916 -11.450437 -12.434177 -11.398848][-5.8329811 -5.3074989 -4.7872176 -4.1135364 -2.0197678 0.72043753 2.0749178 2.0077462 0.30017233 -2.7459464 -6.0848789 -8.97309 -11.137888 -12.521231 -12.00484][-7.6424894 -7.68076 -6.649581 -3.5545046 0.0817399 3.0530686 5.4570069 5.685307 3.8607507 -0.43394804 -4.7330103 -8.5143547 -11.355338 -12.193754 -12.477651][-6.7321033 -7.401062 -6.7883496 -2.9250071 0.87504721 3.4465098 6.4027085 6.3721986 5.6631045 2.4779143 -1.2249331 -6.7296429 -10.803164 -12.040054 -12.992019][-5.1363039 -4.881917 -4.1397209 -1.7569101 0.8472147 3.828012 5.4047103 5.0641809 4.3733678 1.6981988 -0.65638638 -4.8731394 -9.2735453 -11.337301 -11.87377][-4.0955796 -4.33407 -3.1845169 -0.68435812 1.1903729 2.6150384 2.978354 3.0081592 2.574019 1.2239013 -0.49951744 -4.7431569 -8.474432 -9.8703852 -11.120501][-8.3829966 -7.4020958 -4.9794116 -2.4878798 -0.51711988 0.47877073 0.13969088 -0.47912645 -1.2948766 -2.415163 -4.3470831 -7.6160078 -8.872673 -9.7369423 -9.4512386][-15.013178 -12.55142 -8.5552759 -4.3638725 -1.4349294 -0.98756981 -2.0641329 -3.4650929 -4.8842068 -6.1415439 -7.4255939 -9.3659115 -10.29654 -9.2738562 -8.3523607][-15.942295 -13.227858 -9.6496868 -6.8286562 -4.8686113 -3.5264723 -4.326694 -6.2041254 -8.1424141 -8.855463 -9.8517456 -10.991131 -10.712912 -9.5790291 -7.9728365][-12.554653 -11.083517 -8.7499943 -6.7108884 -5.8789377 -5.3299804 -5.7064152 -6.5013132 -8.0759716 -9.9062471 -10.92743 -10.698198 -10.330317 -9.2674885 -8.9696856][-8.5271282 -8.627058 -7.9178224 -7.5526485 -6.08351 -5.3632941 -5.1313128 -5.5314035 -7.2281294 -8.65667 -9.9422617 -11.231565 -11.896185 -11.027188 -11.284986]]...]
INFO - root - 2017-12-15 13:52:15.153925: step 19410, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 55h:36m:00s remains)
INFO - root - 2017-12-15 13:52:21.730341: step 19420, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 56h:16m:19s remains)
INFO - root - 2017-12-15 13:52:28.258346: step 19430, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 57h:13m:33s remains)
INFO - root - 2017-12-15 13:52:34.840669: step 19440, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 58h:06m:00s remains)
INFO - root - 2017-12-15 13:52:41.459580: step 19450, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 57h:41m:46s remains)
INFO - root - 2017-12-15 13:52:48.041606: step 19460, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 57h:27m:18s remains)
INFO - root - 2017-12-15 13:52:54.634788: step 19470, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 57h:04m:25s remains)
INFO - root - 2017-12-15 13:53:01.287373: step 19480, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 56h:20m:27s remains)
INFO - root - 2017-12-15 13:53:07.880631: step 19490, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 56h:27m:46s remains)
INFO - root - 2017-12-15 13:53:14.400648: step 19500, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 56h:19m:23s remains)
2017-12-15 13:53:14.913760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8222895 -8.2664452 -9.0485821 -9.69841 -9.6912537 -9.6797352 -9.6809549 -10.483595 -11.477161 -11.755066 -10.803878 -9.6161957 -9.7811918 -7.4252458 -5.6850157][-8.4493666 -9.0828953 -9.0890179 -9.7163124 -10.262531 -10.478382 -9.79598 -9.0096073 -9.8085785 -10.124125 -9.8623428 -9.7327909 -9.727787 -7.7099104 -7.914958][-7.1495361 -8.3713331 -10.344612 -9.6270733 -9.5037146 -9.9601059 -9.3872681 -8.8960924 -8.9755669 -9.0409565 -9.453414 -9.1016331 -8.8414478 -8.4913092 -8.8592157][-7.9173446 -8.92645 -9.3239384 -8.8352118 -8.6728687 -6.9991474 -5.5569253 -7.1563659 -9.0558023 -8.6256676 -8.2248659 -7.757257 -8.5388546 -7.592186 -7.9887009][-7.0754089 -9.0665188 -11.351344 -10.438062 -8.4305744 -4.7700496 -0.74553156 -2.200563 -6.0662289 -7.07528 -8.839345 -8.3430023 -7.7638125 -7.09919 -7.6943026][-8.6867657 -9.5182428 -9.52932 -8.5762253 -6.7939463 -1.7791104 3.9065175 4.1591754 2.1476436 -1.7006326 -7.2131329 -6.4505711 -5.6886029 -5.7499709 -6.5968504][-8.9796867 -9.9322309 -7.9982548 -5.7827091 -4.0370092 1.3800054 7.2503791 8.3779221 7.3387456 2.4325023 -3.7149875 -5.112782 -5.6992569 -4.8130879 -4.60645][-8.0271807 -9.750494 -8.8566923 -5.4269757 -1.1500201 3.401135 7.0273261 7.9812741 7.6305232 2.7960362 -2.005708 -4.1095576 -5.9409027 -5.444984 -6.0202422][-6.2577233 -7.2477646 -7.9869046 -6.1959004 -2.3312511 1.3540473 4.1447244 5.0501838 5.8568358 2.5214095 -1.2447739 -3.8432693 -7.3502159 -7.3973732 -8.2025013][-5.7764535 -5.4683294 -6.6855512 -6.7545071 -4.6885529 -1.5990796 0.87543297 2.2094831 1.93959 -0.061038494 -2.172092 -3.9561436 -6.0812211 -8.0087624 -10.326813][-9.2573805 -9.8429642 -11.32111 -10.022266 -8.3991308 -6.6159658 -3.7099876 -3.8358467 -5.6332912 -5.9554911 -7.323195 -8.4968605 -8.7697229 -9.7821579 -11.688595][-13.901125 -13.11347 -13.26127 -12.499439 -12.064632 -10.724791 -10.576232 -10.218019 -10.143725 -9.512023 -10.806223 -12.145617 -11.910858 -11.821447 -12.112532][-12.342157 -12.232559 -12.087944 -11.485374 -11.729071 -10.41157 -10.215943 -11.304186 -11.510113 -10.17364 -9.7540379 -10.50173 -10.016357 -10.554343 -10.90577][-9.8443947 -10.420053 -10.052693 -9.58967 -7.5495882 -7.453783 -7.7682257 -8.5411243 -10.174116 -10.116924 -9.6321478 -8.6799278 -8.8026762 -7.1916809 -7.4190516][-6.5774817 -7.2342396 -6.8308716 -6.5814352 -5.2735066 -4.3790364 -3.9627104 -4.7575603 -6.634233 -7.1546645 -7.8837051 -6.6738973 -7.2040739 -6.6980128 -7.046124]]...]
INFO - root - 2017-12-15 13:53:21.480515: step 19510, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 56h:06m:20s remains)
INFO - root - 2017-12-15 13:53:28.170948: step 19520, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 57h:19m:31s remains)
INFO - root - 2017-12-15 13:53:34.754780: step 19530, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 57h:22m:29s remains)
INFO - root - 2017-12-15 13:53:41.400240: step 19540, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 58h:21m:19s remains)
INFO - root - 2017-12-15 13:53:47.962479: step 19550, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 55h:38m:06s remains)
INFO - root - 2017-12-15 13:53:54.569377: step 19560, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 56h:05m:45s remains)
INFO - root - 2017-12-15 13:54:01.210597: step 19570, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.692 sec/batch; 60h:06m:58s remains)
INFO - root - 2017-12-15 13:54:07.810744: step 19580, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.691 sec/batch; 60h:03m:31s remains)
INFO - root - 2017-12-15 13:54:14.402748: step 19590, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 58h:09m:13s remains)
INFO - root - 2017-12-15 13:54:20.988591: step 19600, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 58h:50m:46s remains)
2017-12-15 13:54:21.478817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9701457 -4.9996586 -5.242558 -5.5972228 -6.9230266 -7.2655382 -7.5866995 -8.0167274 -8.8078632 -8.9333839 -8.3733368 -7.2123661 -7.2016897 -7.1396723 -6.0417285][-5.8417034 -6.1733203 -6.107944 -5.7568297 -6.0561004 -7.1398096 -7.8067484 -8.0237865 -8.3475351 -9.1678972 -9.2555828 -8.1077614 -7.2983713 -7.369976 -6.8396931][-4.0609655 -5.6415391 -7.0959511 -6.8111353 -6.7756882 -6.74187 -6.63779 -7.7801414 -8.5122766 -8.7806911 -9.272624 -8.1718683 -7.8751721 -8.58836 -8.6322088][-3.8950524 -5.5839672 -6.3610263 -6.1592655 -5.836195 -5.5433326 -5.463469 -6.0958972 -7.7543406 -8.2545719 -8.1984043 -7.7553234 -7.8582506 -8.8280439 -9.4284945][-6.2759147 -6.5397625 -7.2119179 -6.3452063 -4.3809586 -1.5414772 0.41291475 -1.9263732 -4.0692525 -5.3932819 -6.6948428 -5.4865623 -5.2129507 -6.8922787 -7.4910059][-7.7263107 -7.1780086 -6.3248506 -5.455142 -2.825686 0.71630907 4.2215004 4.6930141 2.7605085 -1.2328734 -5.6623816 -5.4724073 -4.9546037 -5.3379326 -5.3608952][-6.857831 -7.2617779 -6.0878334 -3.1644485 -0.4590373 2.4204273 5.6622057 6.28986 6.34632 1.1140404 -4.9445119 -4.9358912 -6.3462968 -6.8382812 -4.9178977][-8.5732946 -7.7553654 -7.4881473 -4.4264455 -0.28762865 3.3783159 6.434329 6.7638617 6.1362314 2.1638145 -2.3056195 -4.7871485 -7.4569063 -8.4824686 -7.629693][-7.27396 -6.8702183 -7.3380423 -6.6680202 -4.5613232 -0.25264359 3.8090658 4.9452019 4.4762783 0.92915583 -2.5997522 -4.7578139 -8.6047678 -10.477634 -9.7011623][-6.328516 -6.0047975 -6.2549653 -6.3692079 -7.0306926 -4.859499 -1.444809 0.28941727 1.4340973 -1.1726947 -4.7804813 -6.8052049 -8.4652939 -10.104275 -10.668928][-10.32888 -10.648958 -10.508011 -10.370152 -10.165949 -9.3138494 -8.9020615 -7.6131811 -5.9588647 -6.3427896 -8.0798388 -10.17155 -12.614172 -12.413481 -10.700813][-13.284267 -13.850479 -13.939932 -12.837742 -11.925961 -12.141941 -13.197435 -12.815098 -11.572983 -10.956264 -10.782986 -11.329119 -11.976266 -12.898338 -12.41003][-12.046447 -12.195541 -12.765724 -12.72764 -12.330267 -11.04528 -10.411612 -11.715044 -12.223243 -11.3765 -11.23753 -11.137182 -9.8712788 -8.7349186 -8.6972914][-10.847338 -10.436151 -9.5718279 -8.9307384 -9.0865812 -9.0887527 -9.240118 -9.0324306 -9.3887291 -10.273239 -10.703251 -9.2523928 -7.8641481 -7.0248241 -5.3824377][-7.8790016 -7.2964993 -5.5883279 -4.3519316 -3.370111 -2.7068143 -4.2254667 -5.4055052 -5.20218 -5.1996388 -5.9909558 -6.6490593 -6.4320879 -6.1703062 -6.0902572]]...]
INFO - root - 2017-12-15 13:54:28.111810: step 19610, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 58h:13m:27s remains)
INFO - root - 2017-12-15 13:54:34.715762: step 19620, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.634 sec/batch; 55h:04m:10s remains)
INFO - root - 2017-12-15 13:54:41.408018: step 19630, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.672 sec/batch; 58h:25m:44s remains)
INFO - root - 2017-12-15 13:54:48.001221: step 19640, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 58h:08m:16s remains)
INFO - root - 2017-12-15 13:54:54.619684: step 19650, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 56h:30m:34s remains)
INFO - root - 2017-12-15 13:55:01.223826: step 19660, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 57h:27m:40s remains)
INFO - root - 2017-12-15 13:55:07.860155: step 19670, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 56h:47m:32s remains)
INFO - root - 2017-12-15 13:55:14.468846: step 19680, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 56h:53m:32s remains)
INFO - root - 2017-12-15 13:55:21.083504: step 19690, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 57h:52m:14s remains)
INFO - root - 2017-12-15 13:55:27.618556: step 19700, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 55h:04m:37s remains)
2017-12-15 13:55:28.134438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1416602 -4.0166559 -4.6395097 -5.2450666 -6.7225151 -7.2957492 -8.0917215 -8.2531881 -8.6069183 -9.5289 -8.9376488 -9.0932779 -10.967178 -10.010876 -7.3165874][-6.4695048 -5.7907667 -5.7689486 -6.512228 -7.6470909 -9.0381775 -10.019776 -10.185999 -10.107811 -10.001926 -10.065973 -10.684526 -11.734873 -11.630984 -9.5422831][-4.8128395 -5.8423395 -7.1996865 -6.9724631 -7.6234732 -8.5200691 -9.4979162 -9.6844311 -9.5326881 -9.4051847 -9.5159855 -9.5017424 -11.092463 -11.437593 -10.360429][-5.6744614 -6.138092 -6.9734335 -7.2966404 -8.2450943 -8.2109575 -8.4169874 -9.3204556 -9.2327862 -8.696763 -8.8081741 -8.9176769 -9.8463326 -10.039302 -9.966486][-6.148005 -7.9225073 -8.9450188 -8.1096382 -7.3176537 -6.1744957 -5.6363192 -6.5390191 -7.50297 -7.3242841 -7.6139989 -7.9151926 -9.8154745 -9.8189907 -8.4789238][-7.0944705 -7.718049 -6.8010893 -6.6375122 -4.9682617 -1.5434966 1.0094128 0.41794062 -0.76681709 -3.0530493 -5.2609177 -5.805583 -8.0843019 -9.1352615 -8.9418983][-7.9764776 -8.1459227 -6.997684 -5.7198582 -3.1530263 0.20919418 4.5844984 6.978179 6.2160559 1.7482095 -2.4186065 -3.6713042 -6.9829607 -8.9967613 -9.7160034][-7.7869253 -8.4137573 -7.4037833 -4.3368368 -1.1606841 2.3641644 5.8285561 8.1988754 9.4683743 4.4685335 -1.6112771 -4.6750946 -7.5617037 -8.5301275 -9.3710747][-5.2782092 -6.390265 -7.1410642 -4.8666968 -2.4744642 0.89554405 4.3246241 6.56849 7.093749 3.8573656 -0.63221693 -4.8782887 -9.6078739 -10.975086 -10.354317][-2.8172355 -3.0352612 -4.3237729 -3.8443909 -2.3379352 -0.24417734 1.7579937 3.1528063 3.122448 -0.19142532 -3.5692847 -6.9891138 -11.380894 -13.316687 -13.824338][-4.9776521 -4.9090443 -4.7743816 -4.8830042 -4.8874598 -4.0651865 -3.2513387 -3.5806317 -4.5623169 -6.105453 -8.6881981 -11.245944 -14.000902 -14.586391 -14.195628][-9.8073435 -9.709837 -9.64014 -8.7476387 -8.1550665 -8.3527479 -8.98431 -9.40089 -9.7456741 -11.133628 -13.335816 -14.382209 -15.255209 -14.591912 -13.000701][-11.261301 -10.44577 -9.809885 -8.8907366 -9.74284 -9.8263779 -9.7562838 -11.117333 -11.919506 -12.268911 -12.867399 -13.691473 -13.984055 -12.519257 -10.049706][-10.376726 -10.005174 -9.1286068 -9.13168 -9.2153883 -8.9702415 -9.5806 -9.6463289 -9.5520763 -10.343728 -10.367266 -10.112885 -9.7990551 -10.092838 -9.133769][-8.1330824 -7.7305379 -6.898829 -5.8415542 -5.8584156 -6.1301641 -6.5138011 -6.4248443 -6.5103793 -7.0149956 -7.3779979 -8.1061974 -8.2496815 -7.6283541 -7.5780988]]...]
INFO - root - 2017-12-15 13:55:34.730476: step 19710, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 56h:35m:52s remains)
INFO - root - 2017-12-15 13:55:41.360075: step 19720, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 58h:03m:24s remains)
INFO - root - 2017-12-15 13:55:47.925397: step 19730, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 55h:29m:43s remains)
INFO - root - 2017-12-15 13:55:54.480665: step 19740, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 57h:39m:56s remains)
INFO - root - 2017-12-15 13:56:01.055248: step 19750, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 57h:34m:37s remains)
INFO - root - 2017-12-15 13:56:07.599280: step 19760, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 57h:12m:50s remains)
INFO - root - 2017-12-15 13:56:14.143736: step 19770, loss = 0.19, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 55h:40m:00s remains)
INFO - root - 2017-12-15 13:56:20.755784: step 19780, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 57h:08m:26s remains)
INFO - root - 2017-12-15 13:56:27.287992: step 19790, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.644 sec/batch; 55h:58m:12s remains)
INFO - root - 2017-12-15 13:56:33.898807: step 19800, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 56h:57m:28s remains)
2017-12-15 13:56:34.421377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5821924 -7.0802927 -8.3598652 -9.0432367 -10.361516 -11.648355 -12.421624 -12.752714 -13.172275 -12.867427 -11.821733 -12.405087 -11.858904 -12.919645 -10.453176][-10.237226 -9.2058086 -9.6256676 -9.6284618 -10.248819 -11.604525 -12.706493 -14.056717 -14.467327 -14.137661 -13.796827 -13.099407 -11.540513 -13.538031 -9.9530945][-8.3832779 -9.8972244 -11.463469 -10.689201 -10.996771 -11.315574 -11.895444 -12.695175 -13.816656 -14.581594 -14.069456 -14.114916 -12.918634 -14.290846 -11.182958][-9.4012728 -9.812314 -11.903003 -11.942772 -11.75837 -9.7894506 -9.2547169 -11.312938 -12.118072 -12.095108 -12.641569 -13.410723 -12.256746 -13.642381 -10.329859][-11.091938 -11.971479 -13.474636 -13.147552 -11.697784 -7.361589 -4.9593697 -7.6628685 -10.939924 -11.151894 -11.315102 -12.442658 -12.276335 -14.439285 -10.469712][-13.3957 -12.557845 -13.524043 -12.328207 -9.972826 -3.53632 2.0551882 1.1414008 -2.5791905 -7.3869419 -10.720976 -9.9888573 -9.5391912 -12.928514 -11.671883][-16.017717 -13.766495 -11.977528 -9.2134628 -6.6965752 -0.90362549 5.8533931 8.3774071 6.8918242 -1.0799975 -8.6005 -9.8930588 -9.8576488 -11.57424 -9.59065][-15.968084 -15.608274 -14.64991 -7.3077717 -1.5209928 3.2690358 7.6693606 9.2095356 9.4857693 2.9805055 -5.5500751 -9.9366961 -10.770458 -12.088555 -9.542078][-12.823536 -12.052651 -13.335461 -9.5946789 -4.5592356 3.484899 8.3200684 6.93868 5.9212432 0.94326305 -6.0482225 -11.464764 -13.651554 -15.520937 -12.276384][-9.5651779 -8.3981237 -10.874552 -9.6729012 -8.9151545 -2.6249897 3.441978 3.5609307 2.1109757 -4.1392803 -9.7225246 -12.721134 -14.377359 -17.275656 -15.105925][-14.236389 -11.862265 -12.114572 -11.461754 -12.287556 -10.175678 -7.527082 -5.93746 -5.2937369 -9.2731457 -13.813278 -16.599648 -15.360832 -16.77713 -14.862436][-17.568392 -16.395426 -16.582378 -14.272852 -13.855391 -13.562472 -14.135916 -13.731852 -12.377737 -14.328173 -15.548695 -17.491182 -16.083021 -15.837366 -12.117348][-17.27973 -16.845509 -17.240244 -16.237598 -15.935457 -13.803608 -13.884737 -15.047422 -14.960896 -14.247192 -14.103365 -15.83148 -14.365734 -13.580738 -10.079759][-12.40958 -12.225336 -12.242992 -12.138535 -12.640004 -11.520574 -11.272647 -10.590534 -11.021952 -11.304453 -11.110024 -10.654486 -10.700741 -10.524137 -8.1573639][-8.78544 -7.7411747 -6.1381354 -5.6615806 -5.7200742 -6.4267387 -7.1585264 -6.2608156 -5.9360847 -6.2098088 -6.8149414 -8.0086775 -7.7390642 -8.4884644 -7.9781885]]...]
INFO - root - 2017-12-15 13:56:41.121224: step 19810, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 57h:13m:41s remains)
INFO - root - 2017-12-15 13:56:47.654001: step 19820, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 57h:58m:42s remains)
INFO - root - 2017-12-15 13:56:54.200552: step 19830, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 57h:49m:54s remains)
INFO - root - 2017-12-15 13:57:00.819278: step 19840, loss = 0.21, batch loss = 0.17 (11.6 examples/sec; 0.687 sec/batch; 59h:41m:04s remains)
INFO - root - 2017-12-15 13:57:07.408917: step 19850, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 57h:32m:58s remains)
INFO - root - 2017-12-15 13:57:14.016911: step 19860, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 55h:38m:47s remains)
INFO - root - 2017-12-15 13:57:20.572064: step 19870, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 56h:57m:08s remains)
INFO - root - 2017-12-15 13:57:27.210160: step 19880, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 57h:18m:20s remains)
INFO - root - 2017-12-15 13:57:33.839049: step 19890, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 58h:26m:07s remains)
INFO - root - 2017-12-15 13:57:40.347745: step 19900, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 55h:18m:03s remains)
2017-12-15 13:57:40.883039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.89604 -5.3202872 -5.0038996 -3.6383493 -4.1721234 -3.5295961 -3.3738334 -3.1558309 -3.2687607 -3.2118065 -2.9368355 -6.0218406 -7.2598009 -9.4286509 -12.343864][-5.5345774 -5.3427925 -5.2111697 -3.6938896 -3.4953725 -3.8755651 -4.6593285 -4.7754641 -4.6060953 -3.8874946 -3.0715849 -5.160728 -5.8726573 -7.4301748 -10.154757][-4.7071643 -5.1186342 -4.995863 -3.8793309 -4.3495145 -4.0069351 -4.36536 -4.9756441 -5.0730104 -4.4897909 -3.9840107 -5.0449729 -5.2437482 -6.0125232 -7.7519932][-2.7165902 -3.4764843 -3.4558256 -3.1311581 -3.7058024 -3.9206705 -3.3790176 -2.9036138 -1.9567499 -1.3832388 -1.7565441 -4.49555 -5.3579359 -5.727139 -7.2980156][-2.2195771 -2.7677748 -2.5040429 -1.0833244 -1.4949903 -0.99799442 -1.4829826 -1.427475 -1.2548108 0.26275349 0.30391169 -2.5934379 -4.111167 -5.3540244 -7.2558055][-3.7421081 -3.2845132 -1.5931582 0.1104331 0.53123331 1.5484524 1.9305873 1.7426305 1.4545417 1.8995695 1.5847516 -1.800739 -3.2614813 -4.8065882 -6.9949718][-3.4402711 -3.8418536 -3.4323092 -0.92844296 0.49383259 2.806592 4.6678028 3.9560022 3.2990789 2.532238 0.64585209 -1.9569492 -3.3735797 -5.807795 -8.0275507][-4.9024377 -5.0547366 -4.1276813 -0.91540718 0.90560722 3.3770685 5.0773983 5.2659168 5.7989521 3.4331675 0.43049955 -3.4296355 -4.3225789 -4.9076343 -7.220396][-4.1177764 -4.6398888 -4.14425 -2.3929703 -0.60802889 2.3666105 3.4657679 2.6692047 2.5524759 1.933423 0.3891716 -2.7428722 -4.3099313 -5.1490736 -5.8918996][-2.7778993 -2.7831016 -2.7599297 -1.6490717 -1.625617 0.30790281 1.2747664 0.52465773 0.22317553 -0.50979233 -1.3721542 -3.4047732 -4.1462383 -5.1568928 -7.0538092][-4.81682 -4.5096064 -3.9017859 -3.5624814 -3.5105875 -2.5119681 -2.198854 -2.6157813 -3.3697813 -3.6321371 -4.4410648 -6.3756552 -6.2521973 -6.2453923 -7.03911][-9.2108765 -9.5300674 -8.3756943 -6.9650192 -6.7606721 -6.8406205 -7.14866 -7.0981641 -7.1781707 -7.4216056 -6.97859 -7.2266989 -6.9181728 -6.675396 -7.1392965][-11.722807 -11.650862 -10.670658 -9.5998659 -8.83683 -8.0231085 -8.3149252 -8.5255775 -9.2038517 -8.8841124 -8.0562963 -7.9460936 -7.3125167 -6.45162 -6.4937449][-9.3517017 -10.196092 -8.9025936 -7.9974031 -7.0816278 -6.2461414 -5.8898869 -6.2487082 -6.4800892 -6.7956028 -7.088028 -6.4611773 -5.7575045 -5.8827977 -6.4936337][-3.8037648 -4.573524 -4.3582163 -3.0222297 -2.5567131 -2.02308 -2.0582254 -2.1283743 -2.741842 -3.8006494 -4.1510019 -5.2561269 -6.2927122 -6.8535118 -7.6518297]]...]
INFO - root - 2017-12-15 13:57:47.407048: step 19910, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 57h:08m:42s remains)
INFO - root - 2017-12-15 13:57:53.969880: step 19920, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 57h:24m:27s remains)
INFO - root - 2017-12-15 13:58:00.570045: step 19930, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 57h:07m:26s remains)
INFO - root - 2017-12-15 13:58:07.143481: step 19940, loss = 0.19, batch loss = 0.15 (11.8 examples/sec; 0.679 sec/batch; 58h:59m:30s remains)
INFO - root - 2017-12-15 13:58:13.869736: step 19950, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 58h:26m:40s remains)
INFO - root - 2017-12-15 13:58:20.453292: step 19960, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 56h:23m:21s remains)
INFO - root - 2017-12-15 13:58:27.081781: step 19970, loss = 0.20, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 58h:11m:13s remains)
INFO - root - 2017-12-15 13:58:33.649592: step 19980, loss = 0.20, batch loss = 0.16 (12.0 examples/sec; 0.665 sec/batch; 57h:42m:32s remains)
INFO - root - 2017-12-15 13:58:40.196996: step 19990, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 57h:02m:37s remains)
INFO - root - 2017-12-15 13:58:46.812778: step 20000, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 57h:10m:01s remains)
2017-12-15 13:58:47.296943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1438475 -5.5166559 -5.3173728 -4.5695267 -4.3149314 -4.5470634 -4.3636279 -4.1031365 -4.065856 -4.9030523 -5.5375619 -7.2936454 -10.356056 -11.197577 -11.807491][-3.4492087 -3.8743834 -3.3461637 -3.7154799 -4.2246771 -3.7016251 -3.374789 -2.9153044 -2.6253095 -3.4316013 -3.320184 -4.8920946 -8.6143169 -10.414522 -11.568741][-0.2054019 -1.3074622 -2.3254685 -2.1516552 -2.752115 -2.8588607 -2.1571574 -1.8557694 -2.3195546 -2.2443616 -1.8504348 -3.8678541 -7.27159 -7.9588957 -9.3337412][-1.9008021 -1.9049189 -1.9065039 -1.9030948 -2.7342577 -2.5331726 -2.1501033 -1.3728619 -1.114521 -2.0421994 -2.4886947 -4.7850471 -7.3618641 -8.0074768 -8.9632244][-2.5719388 -3.3378694 -3.3762088 -2.3959 -2.3133192 -1.0287671 0.34647083 -0.4643755 -2.1458654 -2.3934159 -2.8754144 -5.3926773 -7.8491545 -8.4801693 -9.0101128][-5.0560379 -4.7984076 -3.6892297 -1.2069836 0.6061244 1.7623706 3.3470893 2.7280631 0.64691162 -1.129849 -2.9849272 -4.9062481 -6.9546933 -7.2223563 -8.3306732][-6.8199854 -5.6398726 -3.6670156 -0.639853 0.820107 2.8588252 5.4502597 5.3411946 4.3628459 1.0034494 -2.8448124 -4.9802418 -7.28666 -7.7832046 -8.4663315][-9.1291571 -7.8844981 -4.9486508 -1.6814952 -0.067069054 3.1913428 6.0875382 5.860393 5.6149945 2.8163772 -0.82405615 -4.9622669 -9.6056767 -9.6177111 -10.077281][-8.5393906 -7.5467787 -4.8790307 -2.3733649 -1.3370714 0.77106476 3.154798 5.1859365 6.2248626 3.4673 0.33113718 -4.4200368 -10.125159 -11.855931 -12.643294][-7.4927297 -6.7362976 -5.0354285 -2.4656668 -1.6985464 -0.53442574 0.89928055 2.3221164 3.3229613 2.6206946 0.55566692 -4.9624062 -10.385845 -12.501936 -15.247761][-12.427309 -10.610105 -8.1126995 -5.4839168 -4.6098914 -4.9093304 -4.8211679 -4.1595359 -3.4886062 -3.1493676 -2.9801941 -6.4555669 -10.555595 -12.966695 -14.220018][-15.010506 -14.047651 -10.699158 -7.6826525 -7.5288243 -7.8387966 -8.8890238 -9.4302664 -9.1147137 -8.3183479 -7.4969978 -8.4580545 -10.391621 -11.950768 -13.540247][-14.735867 -13.178255 -10.302883 -8.47263 -8.6336155 -8.5094166 -8.8427038 -9.5767536 -10.138475 -9.9758377 -9.5664415 -9.2964478 -9.4403925 -9.7897434 -10.413412][-13.598349 -12.984318 -11.12216 -8.9091434 -8.1776905 -9.0365782 -9.8056965 -9.5972643 -9.6221981 -9.7908382 -9.9799881 -8.9033051 -8.8524532 -8.7582455 -8.6131592][-10.372252 -9.4267931 -7.6628838 -6.6313057 -6.3511438 -5.7449036 -5.4647503 -7.0030947 -8.2042675 -8.4738817 -8.2308731 -8.5636463 -9.6998882 -9.9260836 -10.791523]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 13:58:54.661370: step 20010, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 56h:15m:53s remains)
INFO - root - 2017-12-15 13:59:01.252185: step 20020, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.666 sec/batch; 57h:49m:43s remains)
INFO - root - 2017-12-15 13:59:07.877786: step 20030, loss = 0.22, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 56h:54m:35s remains)
INFO - root - 2017-12-15 13:59:14.558300: step 20040, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 56h:53m:19s remains)
INFO - root - 2017-12-15 13:59:21.300569: step 20050, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 57h:44m:42s remains)
INFO - root - 2017-12-15 13:59:27.881810: step 20060, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 57h:03m:30s remains)
INFO - root - 2017-12-15 13:59:34.467743: step 20070, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 55h:30m:44s remains)
INFO - root - 2017-12-15 13:59:41.087879: step 20080, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 57h:01m:28s remains)
INFO - root - 2017-12-15 13:59:47.737166: step 20090, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 57h:55m:38s remains)
INFO - root - 2017-12-15 13:59:54.291127: step 20100, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 56h:56m:33s remains)
2017-12-15 13:59:54.872112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6575384 -4.9193974 -5.1102476 -5.314867 -6.2163444 -6.1268029 -5.8831587 -5.5495715 -5.1588283 -4.9306836 -4.2293587 -6.3862419 -7.4314408 -7.064414 -6.4171233][-6.1183968 -6.1055264 -6.2336087 -5.94712 -6.5143867 -6.8012743 -6.8058605 -6.3672528 -5.9735012 -5.7921958 -5.9065657 -8.1512165 -8.9472218 -9.2970934 -8.1499481][-4.5623808 -5.9191937 -6.7549896 -6.1973262 -6.5019341 -6.5239286 -6.4384537 -6.2287498 -5.9093966 -6.199791 -6.6625366 -9.3331413 -10.547735 -10.730137 -10.305096][-5.3129725 -6.2067471 -6.1639857 -5.3977056 -5.7233629 -4.6705074 -3.7795634 -4.7153988 -5.0330939 -5.0608482 -5.484921 -9.0386915 -11.362666 -11.185186 -10.573272][-5.6744862 -7.9817371 -8.0436049 -5.4582272 -4.0763655 -2.422303 -1.2516842 -2.1096895 -2.5331218 -2.8723505 -3.2974842 -6.9472027 -8.8966894 -10.026613 -10.449506][-7.2690988 -8.4161186 -7.505022 -4.1332474 -1.8745251 1.1912131 3.4697909 2.5133195 1.6602974 0.0081248283 -0.84677124 -4.4687533 -7.1838479 -8.3123112 -7.9594307][-6.6010213 -7.4371848 -7.9115405 -3.660954 0.281981 3.6314425 5.3232512 5.3380084 5.2391267 2.5325427 0.17858362 -3.0726693 -5.3554978 -7.5728455 -7.8857527][-6.2847042 -6.457551 -5.3355188 -2.4189105 1.0152912 5.0600743 6.3619409 5.8668551 5.2884154 3.5156341 2.4118361 -1.926055 -5.75886 -7.2952681 -7.1238322][-4.5966063 -4.9305067 -4.4670115 -2.5621703 -1.2188601 2.2981477 4.7283683 4.7867064 4.5870605 3.299726 2.0856509 -1.9343581 -4.8120823 -6.7782316 -7.5229883][-3.23573 -4.166748 -4.4691753 -3.1359446 -1.8961353 0.3592844 2.3467975 2.7393413 2.4591889 1.9187779 0.84474039 -3.307627 -5.5014963 -7.340097 -8.6545248][-4.5148354 -5.7841749 -5.727046 -4.1617522 -2.4837034 -1.8452387 -1.4959326 -1.3749299 -1.770448 -1.7814193 -2.1611736 -6.4915409 -9.2428284 -10.055353 -9.5594034][-8.9491625 -8.2775726 -7.7479696 -6.659225 -4.7531362 -4.078126 -4.6456823 -4.9857831 -5.8533297 -6.0458174 -6.260366 -8.3349028 -9.7229481 -10.948149 -11.009307][-10.743048 -9.6016932 -7.5485864 -6.8727641 -6.4971933 -5.6967778 -6.0321708 -6.6834455 -7.6647434 -8.2335777 -7.9276633 -8.6290712 -9.8240318 -9.4695911 -8.397171][-10.997966 -10.7262 -8.42567 -6.6204438 -5.8700562 -5.633389 -5.7771788 -6.1111732 -6.5946465 -7.0706267 -7.6475592 -6.5494041 -5.6611581 -5.6939969 -6.0350184][-7.9420843 -8.1721964 -7.7906771 -6.0577917 -3.3703506 -2.3792996 -3.3013592 -4.1394162 -4.4294395 -4.2649541 -4.7094703 -5.6678758 -6.3675709 -5.3478293 -4.9965496]]...]
INFO - root - 2017-12-15 14:00:01.483884: step 20110, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 57h:03m:12s remains)
INFO - root - 2017-12-15 14:00:08.102951: step 20120, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 56h:42m:04s remains)
INFO - root - 2017-12-15 14:00:14.638818: step 20130, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 55h:29m:14s remains)
INFO - root - 2017-12-15 14:00:21.273692: step 20140, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 58h:14m:56s remains)
INFO - root - 2017-12-15 14:00:27.880941: step 20150, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 57h:06m:08s remains)
INFO - root - 2017-12-15 14:00:34.420157: step 20160, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 56h:35m:55s remains)
INFO - root - 2017-12-15 14:00:41.060762: step 20170, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 58h:05m:23s remains)
INFO - root - 2017-12-15 14:00:47.648614: step 20180, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 57h:26m:19s remains)
INFO - root - 2017-12-15 14:00:54.207924: step 20190, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 58h:50m:07s remains)
INFO - root - 2017-12-15 14:01:00.797553: step 20200, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 56h:59m:11s remains)
2017-12-15 14:01:01.272994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1110067 -7.9373932 -7.6042137 -7.3613048 -7.8788919 -7.9475307 -7.4933023 -6.9062839 -6.2882438 -5.3153839 -4.4315948 -7.5540242 -10.386703 -11.598886 -10.772144][-7.7110343 -8.2834091 -7.4852862 -6.8336859 -7.0515289 -7.5114479 -7.6510515 -7.6421032 -7.035512 -6.2727895 -5.5538821 -8.7281628 -11.543098 -12.625547 -12.213095][-6.5413752 -7.6795731 -7.5158072 -6.4219265 -6.8963008 -7.2308016 -7.5970006 -7.3825502 -7.4379449 -6.7829819 -5.5925078 -9.0255795 -12.505548 -13.441265 -12.805047][-6.3981347 -7.6458626 -7.4481325 -6.7526464 -6.0855212 -5.8297963 -6.2373409 -6.416297 -6.205132 -5.9660416 -5.8320532 -8.613554 -10.949139 -12.731593 -13.108759][-7.184381 -8.6563282 -8.9407663 -6.768908 -4.8564806 -4.2861576 -4.4826846 -4.3431549 -3.941313 -3.6291053 -3.8138278 -7.0129228 -9.9375925 -11.695142 -11.836789][-8.22648 -9.7685738 -9.7609024 -6.0604458 -2.2371817 0.16052771 1.2107806 -0.20017052 -1.6690416 -1.3002558 -0.95254374 -3.8363709 -7.6592813 -10.169649 -10.576504][-8.7871523 -9.3022718 -8.10486 -4.5862885 -2.1879084 2.0255346 5.9166322 4.7059054 2.7811866 0.44722414 -0.69427776 -3.1374056 -6.1122823 -8.47748 -9.2162819][-8.5753 -8.9584923 -7.0879297 -2.7717376 0.73161793 5.0992379 6.9999738 5.6363606 4.9045043 2.2660599 0.15596533 -4.1960783 -8.2884588 -9.6153088 -8.9468031][-7.2051296 -6.6844854 -5.05729 -2.4535584 -0.56318378 3.299356 5.1042905 4.1601424 3.568027 0.90738106 -1.1104031 -5.7709789 -9.401495 -10.55319 -10.779807][-5.1895604 -5.7676239 -4.6362276 -2.0734138 -0.33727312 1.6151705 1.6189175 1.0440269 0.27594376 -1.7877448 -3.0455403 -7.9673519 -11.575759 -13.241924 -12.679838][-8.5252085 -8.5471888 -7.7970104 -6.5386453 -5.7327003 -4.3476257 -3.4852021 -2.9586306 -3.5573974 -5.021533 -6.14326 -11.356866 -13.73621 -14.893894 -14.134863][-11.924921 -12.193645 -12.394229 -10.847385 -9.6591082 -8.85609 -8.5813732 -8.2090464 -7.896307 -8.8665495 -9.5408354 -11.897164 -12.552094 -14.965796 -13.998123][-14.283121 -13.869778 -13.236197 -12.100863 -12.098524 -11.229273 -11.403904 -11.182602 -10.729811 -10.751402 -11.173008 -12.201918 -11.52742 -11.230279 -9.1243219][-13.353079 -12.122667 -11.159231 -10.391029 -10.325253 -9.7985764 -9.78462 -9.73486 -10.333489 -9.6189461 -9.04574 -8.803194 -9.1175232 -9.8437738 -8.2951765][-9.5981293 -8.8997049 -7.6349978 -6.9971046 -6.9798193 -6.4272676 -6.2874923 -6.99522 -7.4084816 -7.6156559 -8.4573555 -8.64217 -8.05786 -7.7319245 -7.6972694]]...]
INFO - root - 2017-12-15 14:01:07.814419: step 20210, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 56h:29m:38s remains)
INFO - root - 2017-12-15 14:01:14.372569: step 20220, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 56h:05m:18s remains)
INFO - root - 2017-12-15 14:01:20.987449: step 20230, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 57h:20m:59s remains)
INFO - root - 2017-12-15 14:01:27.602275: step 20240, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 57h:11m:44s remains)
INFO - root - 2017-12-15 14:01:34.218112: step 20250, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 56h:00m:27s remains)
INFO - root - 2017-12-15 14:01:40.811889: step 20260, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 56h:17m:25s remains)
INFO - root - 2017-12-15 14:01:47.350662: step 20270, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 56h:38m:04s remains)
INFO - root - 2017-12-15 14:01:53.983712: step 20280, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 57h:36m:43s remains)
INFO - root - 2017-12-15 14:02:00.629186: step 20290, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 58h:25m:31s remains)
INFO - root - 2017-12-15 14:02:07.214761: step 20300, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 56h:00m:01s remains)
2017-12-15 14:02:07.709099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8694904 -4.5603924 -3.6758401 -3.2914722 -3.8348174 -3.3838003 -3.6359911 -3.9986868 -3.506475 -2.9644554 -3.2100935 -2.7063842 -5.0091944 -4.483048 -5.5440464][-3.5800424 -4.4801731 -4.3559542 -3.5725558 -3.1097367 -3.3349075 -3.3054523 -3.1687245 -2.4148376 -1.5518513 -1.2516909 -0.96583033 -3.0853744 -4.0194964 -5.466876][-2.699743 -3.9308152 -4.2734551 -4.7522945 -4.8610778 -4.0312748 -3.8107913 -2.7601416 -1.0589247 -0.71837187 -0.62995768 -0.65614986 -3.6479189 -3.78128 -5.498198][-3.6037982 -4.5294528 -4.6977777 -4.7404528 -4.6787405 -3.5323663 -2.4578104 -2.4115427 -1.8152854 -0.92436695 -0.837296 -1.3015203 -4.2117634 -4.9835844 -5.6971011][-4.8671169 -5.8014116 -5.5231719 -4.3992729 -2.3902092 -1.1724014 -1.1376171 -0.92933273 -1.2982244 -1.2157683 -1.2442265 -1.1219816 -3.9544344 -5.664454 -7.3932738][-5.7628436 -5.7596235 -4.430871 -2.0864148 -0.2496562 1.5330701 2.4790311 1.5260077 0.11554003 -0.2336483 -0.69801235 -0.77759075 -3.8519838 -5.0841703 -6.9238324][-3.6314998 -3.664433 -2.9589939 -0.763772 1.5234632 3.4468102 3.9461732 3.3408046 2.6518149 1.8255258 0.97249746 0.897768 -2.4468026 -3.9745193 -5.7995229][-4.0730476 -2.865407 -1.4094715 0.63458824 2.261919 3.3614736 3.5690742 3.2213378 3.7686353 3.6245713 2.6243453 2.0395198 -2.0235717 -3.5441806 -4.9040813][-3.5060253 -2.6610503 -0.82885313 0.34378624 0.61278009 0.88105869 1.5515466 2.1523433 1.8635988 1.9370236 2.3042178 1.3073521 -2.2224312 -4.320992 -5.7853446][-2.9640248 -2.3565755 -1.4241376 -0.65711212 -0.39401245 -0.25568151 -0.42386961 -0.14795876 0.57648373 0.85079145 0.048037529 -1.1514831 -4.7961574 -6.9623194 -8.9368868][-6.4786 -6.1404991 -5.4384332 -4.259861 -2.9761112 -2.5778587 -2.5313206 -2.2214465 -2.3340511 -2.8148406 -4.4282875 -5.8333969 -9.055625 -10.303306 -10.511056][-10.856267 -9.5791063 -7.7612104 -6.6276116 -5.1839104 -3.8765554 -4.5749578 -5.8603215 -6.5472436 -6.35408 -7.004385 -7.7887821 -10.038252 -10.717806 -11.044905][-11.219301 -10.912527 -9.270422 -6.8869085 -6.2369905 -5.0347581 -4.42198 -4.4687228 -5.1316509 -5.7619443 -6.6298614 -7.5429029 -8.6038036 -8.17829 -7.1790581][-9.1445751 -9.1908741 -7.89643 -6.2266469 -4.8311615 -3.5338159 -3.5336897 -3.3529384 -2.9565353 -3.1114621 -3.8904564 -4.3180966 -4.8855019 -4.4122343 -4.011466][-4.1913471 -4.0971279 -4.187717 -2.9807384 -2.5656397 -1.6439958 -1.5645437 -1.2954998 -1.4475689 -1.5629458 -2.4856882 -3.4267802 -3.7339346 -2.8662283 -2.7264955]]...]
INFO - root - 2017-12-15 14:02:14.337860: step 20310, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 56h:20m:49s remains)
INFO - root - 2017-12-15 14:02:21.004697: step 20320, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 56h:41m:10s remains)
INFO - root - 2017-12-15 14:02:27.617261: step 20330, loss = 0.12, batch loss = 0.08 (12.7 examples/sec; 0.632 sec/batch; 54h:48m:02s remains)
INFO - root - 2017-12-15 14:02:34.243523: step 20340, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 56h:43m:22s remains)
INFO - root - 2017-12-15 14:02:40.834990: step 20350, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 56h:00m:02s remains)
INFO - root - 2017-12-15 14:02:47.431812: step 20360, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 56h:43m:16s remains)
INFO - root - 2017-12-15 14:02:54.069599: step 20370, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 57h:45m:28s remains)
INFO - root - 2017-12-15 14:03:00.677557: step 20380, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 56h:56m:43s remains)
INFO - root - 2017-12-15 14:03:07.309737: step 20390, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 58h:05m:49s remains)
INFO - root - 2017-12-15 14:03:13.942515: step 20400, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 56h:17m:12s remains)
2017-12-15 14:03:14.441252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3140392 -4.7619019 -3.8901005 -1.7806334 -1.4687958 -1.0945926 -0.84655523 -0.59735632 0.26130533 0.68800116 0.86259508 -0.55649853 -1.278687 -2.0629368 -3.7250228][-2.2521574 -2.093467 -1.2504039 -0.4724369 -1.2252479 -1.1353402 0.3447876 0.40886211 0.2231431 0.52356815 1.6014066 -0.021068096 -1.3346267 -1.5348268 -3.0253923][-0.59351063 -1.6406093 -1.6766663 -0.29995584 -0.68834352 -0.95738363 -1.0293455 -0.88743734 0.3315134 0.47268009 0.69436073 -1.2669067 -3.220643 -4.3357048 -6.9256678][-4.1430283 -4.79305 -3.9140983 -2.8090916 -1.8377771 -2.2028635 -2.8635619 -2.5291181 -2.1562605 -1.9461327 -1.3091931 -4.0057988 -6.446734 -7.3653841 -9.3297329][-5.7000046 -5.9628019 -5.201962 -3.5035179 -1.6307707 -0.9920311 -0.89558268 -1.7829599 -2.9495082 -2.6680765 -2.664016 -5.8740315 -7.9441047 -8.8990126 -11.088397][-5.8440595 -4.9628863 -3.211447 -1.9358387 -0.45002365 0.707788 0.58329821 0.65877104 0.83682871 -0.57675743 -2.8962369 -5.8157191 -7.9430881 -9.6445169 -12.140083][-5.4414024 -3.5003607 -1.5214725 -0.89338541 -0.41397572 0.12600613 0.65600204 1.2175932 1.2731595 0.49826908 -1.137238 -5.4852915 -9.3271656 -11.324445 -12.840452][-6.2785854 -5.16224 -2.3764212 -0.70879364 -0.69889212 -1.01508 -1.0943036 -0.49886465 0.41866159 -0.34157038 -2.1768513 -6.1697474 -9.4494686 -11.822342 -13.603218][-7.0234327 -5.8916054 -3.0424337 -1.4572477 -0.70366096 -0.53898525 -1.410068 -1.6102271 -1.172554 -1.0387836 -1.7862611 -6.1410646 -9.99467 -11.381987 -11.84881][-7.4890842 -7.560648 -5.5131731 -2.9601383 -1.2731066 -0.90364218 -1.1054587 0.017359257 0.35799217 -0.093416691 -1.1877017 -4.8829546 -7.9445877 -10.310692 -12.02758][-12.008073 -10.991182 -8.634059 -5.9240661 -4.1855273 -3.6187649 -2.7685344 -2.4545829 -3.228513 -2.7802911 -3.0667899 -6.7353029 -9.0462942 -9.832222 -10.205055][-14.006466 -13.592495 -11.442931 -8.544548 -6.9739561 -6.3603797 -6.6926422 -6.80779 -6.83904 -6.5478592 -7.0515213 -8.6561279 -9.53857 -10.005226 -10.182744][-13.374027 -12.074017 -9.9307051 -8.3059721 -8.121212 -7.0538583 -7.3281779 -7.8744407 -8.3090172 -8.9909086 -8.9335651 -9.0537014 -8.6156578 -8.4673958 -8.0338135][-11.612576 -10.399489 -9.2392063 -6.4423718 -5.368607 -6.7731953 -8.4664936 -8.3894005 -8.3185587 -9.1760807 -9.53188 -9.3692608 -9.2274542 -8.2825375 -7.2667708][-8.069376 -8.5548067 -7.7557592 -6.8980513 -6.6557927 -5.9496531 -5.881712 -7.4112415 -8.4146233 -8.1659012 -8.5095968 -9.6879025 -10.425253 -9.97588 -10.281794]]...]
INFO - root - 2017-12-15 14:03:21.128334: step 20410, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 59h:10m:12s remains)
INFO - root - 2017-12-15 14:03:27.772184: step 20420, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 57h:09m:22s remains)
INFO - root - 2017-12-15 14:03:34.485732: step 20430, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 55h:27m:13s remains)
INFO - root - 2017-12-15 14:03:41.096600: step 20440, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 55h:16m:05s remains)
INFO - root - 2017-12-15 14:03:47.706259: step 20450, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.632 sec/batch; 54h:49m:29s remains)
INFO - root - 2017-12-15 14:03:54.327018: step 20460, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 57h:49m:44s remains)
INFO - root - 2017-12-15 14:04:00.885954: step 20470, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 58h:25m:53s remains)
INFO - root - 2017-12-15 14:04:07.493431: step 20480, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.682 sec/batch; 59h:06m:38s remains)
INFO - root - 2017-12-15 14:04:14.087644: step 20490, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 56h:49m:38s remains)
INFO - root - 2017-12-15 14:04:20.690416: step 20500, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 57h:41m:46s remains)
2017-12-15 14:04:21.188427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.601336 -10.238469 -8.8431 -7.7626853 -7.7944226 -7.3554811 -7.7478223 -7.8068476 -7.087863 -6.2444277 -5.6367884 -8.2073364 -9.358429 -11.186979 -10.384238][-10.477703 -10.844254 -11.317337 -10.176809 -9.5569305 -9.4388475 -8.986681 -8.7405338 -8.7652922 -8.2494125 -7.726203 -9.5673885 -10.44663 -11.508157 -11.272514][-8.1424084 -9.4405975 -10.286354 -9.1362247 -8.9747467 -8.8948259 -8.6703424 -8.9192019 -9.22787 -8.726059 -8.0626488 -10.523191 -11.11512 -11.891373 -11.121532][-5.4117122 -5.89372 -6.0460558 -6.3261271 -6.5668964 -6.3152356 -6.54024 -6.2014246 -6.2295156 -7.6569595 -7.9452057 -10.007318 -11.892891 -13.390499 -11.642092][-5.5813656 -5.0343666 -5.69694 -4.2502294 -3.4324977 -2.2668664 -2.2189538 -2.9985769 -3.477782 -3.6575043 -4.4399576 -8.2558823 -9.71486 -12.265936 -12.532846][-5.4329677 -5.49145 -4.8667312 -3.6359642 -3.0541277 -0.80641651 0.51155519 1.0765805 0.96486855 -0.74064016 -2.2165873 -5.5830646 -8.0179653 -10.472508 -10.504404][-6.8683286 -6.7263479 -6.3874655 -4.1300497 -1.8737633 0.66573572 1.7434897 2.7949662 3.1782966 2.8316283 1.4953332 -3.3195951 -6.3803215 -8.74307 -8.8223505][-6.2793336 -6.0261917 -5.2581134 -2.5334914 -0.54964304 2.2575903 3.5906968 3.6929154 3.7152925 3.5692821 2.7947783 -1.5850358 -4.9082813 -7.3094854 -7.1485872][-5.210875 -4.8947082 -4.1260304 -2.1591969 -0.15705252 2.2811289 3.458694 4.7255263 5.3232942 3.7868972 2.5614591 -0.52210951 -3.1291707 -5.9675617 -5.87886][-5.8591728 -4.862401 -3.7770557 -1.6843214 0.31600904 1.4491763 1.5512309 2.6675248 2.9477944 2.9552627 2.6237912 -1.1742635 -3.535532 -4.4677777 -3.7840681][-7.7369967 -7.0787177 -5.9102683 -3.9354753 -3.2223234 -2.00058 -1.3566804 -1.559731 -1.7161398 -1.2835159 -1.2666249 -3.6642885 -5.225996 -6.5405321 -5.08515][-11.202694 -9.6439924 -8.646471 -6.2110424 -5.4418216 -5.5334244 -6.0859914 -5.4183192 -5.5045881 -6.1290088 -5.7000055 -7.106 -8.0637436 -8.3111134 -7.0378828][-11.816523 -10.122248 -8.5373793 -7.760891 -7.1442251 -6.4537535 -6.173584 -6.2728114 -7.16884 -8.0840311 -8.1356468 -8.7881107 -8.9907227 -8.4580574 -6.9874778][-11.354862 -10.537821 -9.2124958 -7.340838 -6.0132837 -5.46737 -5.8261461 -5.84758 -5.9682708 -7.3669577 -8.4051819 -7.8511767 -7.5666676 -8.4699249 -7.6705093][-9.7390995 -10.076984 -8.5981522 -7.2108183 -6.4422474 -5.1526985 -4.9580607 -4.8747578 -5.5757632 -5.8484077 -6.1103678 -7.1582065 -7.9521246 -8.0110941 -7.9131551]]...]
INFO - root - 2017-12-15 14:04:27.726666: step 20510, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 56h:46m:57s remains)
INFO - root - 2017-12-15 14:04:34.317956: step 20520, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 57h:51m:20s remains)
INFO - root - 2017-12-15 14:04:40.974761: step 20530, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 56h:24m:29s remains)
INFO - root - 2017-12-15 14:04:47.558839: step 20540, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 55h:30m:31s remains)
INFO - root - 2017-12-15 14:04:54.159356: step 20550, loss = 0.21, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 56h:42m:41s remains)
INFO - root - 2017-12-15 14:05:00.738854: step 20560, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 55h:47m:50s remains)
INFO - root - 2017-12-15 14:05:07.391656: step 20570, loss = 0.12, batch loss = 0.07 (11.5 examples/sec; 0.694 sec/batch; 60h:10m:06s remains)
INFO - root - 2017-12-15 14:05:13.971741: step 20580, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 57h:02m:04s remains)
INFO - root - 2017-12-15 14:05:20.619969: step 20590, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 56h:18m:02s remains)
INFO - root - 2017-12-15 14:05:27.172783: step 20600, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 56h:37m:17s remains)
2017-12-15 14:05:27.703872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2756996 -6.6320696 -6.2084441 -5.6588936 -6.1709614 -6.6688404 -6.6591563 -6.8536806 -7.03499 -7.3877478 -8.11136 -10.88299 -13.261967 -13.194309 -10.668842][-7.1144938 -6.9781256 -6.2876792 -6.4879541 -7.8608828 -8.6438456 -8.6612329 -9.0727253 -9.8177624 -9.4945717 -9.0977812 -11.04738 -12.666173 -13.211714 -11.804916][-6.0237284 -6.9697986 -7.351007 -7.218648 -8.5771847 -9.459734 -9.8738422 -10.317585 -10.902683 -11.17375 -11.299641 -12.05022 -13.163609 -13.281853 -10.227168][-5.9631534 -6.4059172 -6.1966681 -6.9625254 -8.0619049 -7.536108 -7.5171194 -8.97562 -10.01704 -9.6588726 -9.3158855 -11.692544 -13.304256 -12.951906 -11.263106][-6.7942014 -7.1730938 -6.7736359 -6.5679851 -6.1860061 -3.9233656 -2.9699881 -4.2524347 -4.6021237 -5.2394662 -6.8508835 -7.8811731 -9.5577478 -11.439555 -10.708231][-9.4966125 -8.5755606 -7.2709441 -4.92481 -3.4871795 -0.31700993 2.1182203 2.2300315 1.7769465 -0.92324877 -2.7501006 -4.8626404 -8.1268063 -10.091646 -9.6241875][-9.4244967 -8.7692242 -7.9881415 -4.9467993 -1.9740257 1.649817 5.2802043 6.6750894 7.4673228 3.3354602 -0.99884558 -4.1039534 -8.07276 -10.188127 -9.8671751][-8.8934021 -8.0869961 -6.95326 -4.1390953 -1.1723094 4.3973594 8.0979843 7.9695935 7.1773572 4.3123927 0.21258926 -4.2856855 -9.0792027 -10.513065 -10.180807][-8.16813 -7.33025 -5.2705722 -3.1205697 -1.3280087 2.5928311 5.6918263 7.2744336 6.6773314 2.7906442 -1.1971931 -6.505188 -12.342153 -14.315643 -13.759617][-6.4195933 -7.2109246 -6.5686984 -4.3888373 -2.7085135 -1.0220399 1.5153589 3.3581691 3.4196887 0.49350309 -3.670634 -10.33926 -15.00124 -17.790657 -17.553619][-9.4796009 -8.49722 -8.1919146 -7.6715274 -6.9937496 -5.5798888 -3.4935451 -2.4092321 -3.0118561 -4.5075092 -7.3930912 -12.679779 -18.122305 -19.801495 -17.921309][-12.491507 -12.157312 -11.183248 -9.8725576 -9.8987551 -9.4284258 -7.9437547 -6.9733095 -7.1419234 -8.47478 -11.095425 -14.095455 -16.139196 -18.341597 -16.589392][-14.564337 -12.655397 -12.744436 -11.544766 -10.795365 -10.303919 -9.7121048 -8.8093977 -8.0915651 -9.3898163 -11.415397 -12.761337 -14.643929 -14.220024 -11.560616][-12.892162 -12.922716 -11.353077 -9.5025635 -8.8354921 -9.1406622 -8.9507294 -8.079113 -7.378469 -7.2295804 -8.3669262 -8.7473249 -9.1780872 -9.8027735 -8.1305428][-8.3721619 -7.9377394 -6.9447465 -5.8484344 -4.8862047 -4.5263906 -4.1681433 -5.3874617 -6.014555 -5.6521997 -5.7813544 -6.98511 -9.1180382 -7.117034 -5.6547594]]...]
INFO - root - 2017-12-15 14:05:34.252229: step 20610, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 56h:23m:25s remains)
INFO - root - 2017-12-15 14:05:40.874821: step 20620, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 56h:32m:29s remains)
INFO - root - 2017-12-15 14:05:47.483572: step 20630, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.691 sec/batch; 59h:51m:37s remains)
INFO - root - 2017-12-15 14:05:54.089797: step 20640, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 57h:53m:24s remains)
INFO - root - 2017-12-15 14:06:00.686199: step 20650, loss = 0.21, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 57h:12m:30s remains)
INFO - root - 2017-12-15 14:06:07.260870: step 20660, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.688 sec/batch; 59h:33m:45s remains)
INFO - root - 2017-12-15 14:06:13.950320: step 20670, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 55h:52m:30s remains)
INFO - root - 2017-12-15 14:06:20.563507: step 20680, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 58h:32m:59s remains)
INFO - root - 2017-12-15 14:06:27.127943: step 20690, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 56h:40m:13s remains)
INFO - root - 2017-12-15 14:06:33.692099: step 20700, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 57h:00m:34s remains)
2017-12-15 14:06:34.195576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6991405 -4.6298275 -4.9391651 -5.0887656 -5.988235 -6.0920568 -6.6243672 -6.5942612 -6.5682049 -6.9486141 -6.0723777 -4.6024327 -5.1337457 -6.55929 -6.9004045][-5.005177 -5.4990764 -6.18378 -7.5848522 -8.1562452 -9.1948032 -9.3385563 -9.17886 -9.3116646 -8.6557941 -7.8177614 -6.1636534 -7.13255 -8.5935049 -7.8174276][-4.2865486 -5.5533977 -7.6674323 -8.2002573 -9.608078 -10.627531 -11.622281 -11.03805 -10.068352 -10.149261 -9.677249 -7.5440354 -8.2371149 -9.3194942 -9.103529][-5.0618987 -5.2438779 -6.8668318 -8.220027 -10.026741 -10.571428 -9.3773746 -9.5917358 -9.7874851 -9.4704552 -9.4127808 -8.5601368 -9.8189831 -11.256342 -10.320223][-5.1890788 -6.2246089 -8.5307083 -9.3929119 -9.6693678 -6.4997792 -4.7993212 -6.3355684 -7.1404867 -7.1011481 -7.9455223 -7.78757 -9.5205956 -11.91143 -11.687954][-7.0812664 -7.4932313 -8.1991415 -9.4560623 -8.8750429 -4.1777134 1.0276546 1.5259352 -0.47496462 -3.9657612 -7.2315164 -5.842104 -8.1112127 -11.081621 -12.102163][-8.837182 -8.0606384 -8.42689 -9.2030888 -6.369555 -0.928236 4.569736 7.927732 6.8571191 -0.19222736 -6.0842233 -6.0184021 -7.782136 -9.544591 -10.104315][-9.3135414 -8.2532692 -8.0918541 -7.5780172 -4.509304 0.89182663 6.9584336 10.485109 10.228828 4.8500733 -1.909806 -4.832715 -8.3229713 -9.6268444 -9.1904182][-7.8517561 -7.7527819 -7.1086507 -7.101541 -5.6746516 -0.26308346 5.8514352 9.4633236 8.8791828 4.47082 -0.31765461 -3.7582021 -7.9193506 -10.394447 -10.203855][-7.0252471 -7.2456336 -7.3129225 -7.4232531 -6.1599097 -2.814445 1.2458534 5.261085 5.664309 2.3312573 -2.4108372 -4.4780264 -7.8204994 -11.471071 -12.896374][-9.8093243 -8.7855682 -9.0304317 -8.786191 -7.8199029 -7.1026917 -5.2870874 -2.3147333 -1.0111804 -1.8891706 -5.3640747 -8.0074883 -11.183619 -12.017948 -11.972464][-12.042318 -10.75629 -9.4797411 -8.8227673 -8.462657 -8.875247 -8.5485983 -6.9817162 -6.3035064 -6.4610085 -8.2641525 -9.3294506 -11.222363 -13.053762 -12.025679][-12.214773 -10.820961 -8.7456894 -8.6167631 -9.1141577 -8.7915783 -8.5708523 -8.1127329 -7.5091834 -7.3877578 -7.8181267 -8.6536531 -10.712321 -11.775861 -9.6365433][-9.2035313 -8.8810139 -8.4565868 -8.2578249 -7.7827754 -8.6866531 -9.2771578 -7.8712726 -6.5006313 -7.0479174 -7.4915557 -6.4522853 -7.0385771 -7.5534673 -8.1125269][-5.1768036 -5.3989959 -5.4280772 -5.1161575 -5.2755055 -6.48605 -6.5352478 -6.1713972 -5.9024463 -4.9433904 -4.8097925 -5.076685 -6.8370252 -6.7654819 -7.0964079]]...]
INFO - root - 2017-12-15 14:06:40.792575: step 20710, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 58h:45m:01s remains)
INFO - root - 2017-12-15 14:06:47.406028: step 20720, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.678 sec/batch; 58h:41m:46s remains)
INFO - root - 2017-12-15 14:06:54.036899: step 20730, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.681 sec/batch; 59h:01m:01s remains)
INFO - root - 2017-12-15 14:07:00.640200: step 20740, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 55h:34m:53s remains)
INFO - root - 2017-12-15 14:07:07.335528: step 20750, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 57h:56m:22s remains)
INFO - root - 2017-12-15 14:07:13.946678: step 20760, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 56h:47m:59s remains)
INFO - root - 2017-12-15 14:07:20.439539: step 20770, loss = 0.13, batch loss = 0.08 (12.7 examples/sec; 0.631 sec/batch; 54h:36m:46s remains)
INFO - root - 2017-12-15 14:07:27.053312: step 20780, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 56h:54m:32s remains)
INFO - root - 2017-12-15 14:07:33.646916: step 20790, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 56h:41m:27s remains)
INFO - root - 2017-12-15 14:07:40.227561: step 20800, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 55h:55m:00s remains)
2017-12-15 14:07:40.724481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.6054006 -8.1559467 -8.7830982 -8.4322243 -9.1656914 -9.2356968 -8.623353 -7.6812468 -6.8403606 -6.893712 -7.077075 -9.9940414 -12.37944 -12.287872 -12.054259][-8.7924118 -9.380724 -8.7440443 -9.0523663 -9.8142262 -9.4519176 -9.0138025 -8.6248493 -8.3901234 -8.5358658 -9.0620575 -12.39424 -14.492903 -14.026541 -14.102137][-8.1424465 -8.9698391 -9.0407753 -9.7669134 -10.211744 -10.249115 -9.68905 -8.4424238 -8.0770855 -8.9542484 -10.058514 -13.428405 -15.56049 -14.749205 -13.575724][-7.3842316 -8.5554428 -8.1661215 -8.09421 -8.0483761 -7.797739 -7.1932578 -6.6511688 -6.5439725 -6.8332658 -8.12146 -12.152332 -14.668177 -14.334538 -14.116098][-7.486084 -8.4679127 -8.3150587 -6.636076 -4.9957266 -3.514601 -2.4494522 -3.3807538 -5.0136714 -5.8238969 -6.2564216 -9.3496637 -11.909905 -12.335424 -11.597391][-7.51754 -8.0597353 -7.7106853 -5.1323252 -2.5182595 1.1201606 3.9165616 2.8393149 0.54064035 -2.0490522 -4.49059 -7.3652458 -9.1975584 -9.0765972 -8.4610958][-9.0280666 -9.0756845 -7.0186191 -3.1039476 -0.56633663 3.4871325 7.1211963 7.0182166 6.2104316 2.1997519 -2.3894169 -6.037262 -8.1032085 -7.8451424 -7.3062973][-7.9820981 -7.7989912 -6.0278778 -3.0800934 -0.51694822 3.9732318 8.0000114 8.0776711 7.9039207 5.2001615 1.423511 -3.6604285 -7.3969307 -7.3105078 -6.7190104][-5.00654 -5.8018475 -5.5697551 -2.5638132 0.26393652 2.8555808 4.717442 4.4501872 5.1096568 3.3207278 0.902967 -3.7505217 -8.1958752 -8.6609955 -8.5061111][-3.0735149 -3.3092942 -3.4741068 -2.0030437 0.037102222 0.93694162 1.2778492 0.60672903 -0.18693209 -1.5073075 -2.0183554 -5.7145438 -9.1307325 -9.844841 -10.867339][-5.6574469 -6.5188084 -6.2929492 -5.1285062 -4.0339527 -3.0838656 -2.2236848 -3.4048548 -5.2897234 -6.4365134 -6.7293167 -9.7830038 -11.556068 -12.631062 -13.911942][-7.8791962 -7.9042463 -7.441925 -8.10863 -8.0861712 -7.2827511 -6.9691582 -7.82333 -9.1464767 -9.7682705 -9.7381134 -10.998747 -11.548128 -12.386725 -13.585047][-12.528296 -10.779545 -9.5881519 -9.3187857 -8.2874432 -9.587923 -10.55007 -10.025356 -9.8842659 -10.111165 -10.339895 -11.214571 -11.417867 -10.606775 -10.438701][-11.544711 -10.584547 -9.5695591 -8.4243135 -7.2054958 -8.2478247 -8.6684484 -9.8020554 -10.212654 -8.7861185 -8.0221682 -8.201952 -8.333643 -7.56921 -7.4971766][-9.6182966 -8.79786 -8.0911312 -7.0037088 -5.6634169 -5.6522985 -5.3692741 -5.92855 -5.8385048 -5.0901747 -5.526669 -5.5266604 -5.8859534 -6.1323643 -6.5277548]]...]
INFO - root - 2017-12-15 14:07:47.310791: step 20810, loss = 0.24, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 56h:35m:10s remains)
INFO - root - 2017-12-15 14:07:53.885644: step 20820, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 56h:11m:20s remains)
INFO - root - 2017-12-15 14:08:00.442611: step 20830, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 58h:14m:31s remains)
INFO - root - 2017-12-15 14:08:06.981228: step 20840, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 55h:55m:12s remains)
INFO - root - 2017-12-15 14:08:13.559000: step 20850, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 55h:45m:30s remains)
INFO - root - 2017-12-15 14:08:20.142405: step 20860, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 55h:20m:21s remains)
INFO - root - 2017-12-15 14:08:26.816154: step 20870, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.695 sec/batch; 60h:10m:34s remains)
INFO - root - 2017-12-15 14:08:33.418896: step 20880, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 55h:11m:26s remains)
INFO - root - 2017-12-15 14:08:40.027112: step 20890, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.696 sec/batch; 60h:14m:43s remains)
INFO - root - 2017-12-15 14:08:46.664219: step 20900, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 56h:50m:56s remains)
2017-12-15 14:08:47.181064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1054111 -6.2122669 -5.7857423 -5.0238 -5.3566871 -5.7021017 -5.6148663 -4.7821407 -4.717526 -4.6628814 -3.8532081 -6.7236166 -9.8419209 -8.9817114 -9.1406317][-4.5590916 -4.0917568 -3.4800725 -2.8166149 -3.598078 -4.5917964 -4.7990174 -4.7144828 -4.32329 -3.5608051 -3.8050337 -6.4378872 -8.0769253 -9.0018187 -9.943614][-3.805419 -3.621634 -2.475399 -2.1632106 -2.8126647 -2.7201276 -3.1375077 -3.3160594 -2.75729 -3.0688746 -3.1317956 -5.7966862 -9.0841179 -9.1492462 -9.0929661][-4.335433 -3.891125 -2.7762365 -1.89172 -2.0806696 -2.2245371 -1.9497333 -2.1368942 -2.984853 -2.8614144 -2.5374496 -5.4829664 -8.4231014 -8.469059 -8.0952806][-2.4098492 -3.3359351 -2.6575584 -1.4364605 -1.0382338 -0.54180622 -0.53830147 -0.39508772 0.0080881119 -0.39418888 -1.1610537 -4.3272204 -6.6470103 -6.8813391 -7.5427694][-3.9340801 -2.9204574 -1.6184387 -0.57010937 0.14279842 1.022151 1.0325499 0.89795256 0.76770926 0.66088104 -0.089643 -3.0455081 -5.3651485 -5.2301807 -5.6086264][-6.1477761 -5.2935452 -3.029954 -1.113337 0.68569231 1.9981012 2.4860129 2.3245482 2.0674205 1.6695228 1.1545815 -1.8456616 -4.4143887 -4.3741803 -5.0713496][-6.5420942 -5.856226 -3.8872528 -0.76361227 1.2359996 2.025125 2.3941312 2.3087602 2.0851722 1.5883994 0.71102 -1.8582492 -4.3020525 -4.6151452 -5.244853][-5.8673677 -5.0827026 -2.786134 0.073625565 1.1993303 2.2773538 2.0709109 1.664227 1.9228616 1.6436229 0.75561762 -2.5396821 -5.5051823 -5.74301 -5.996335][-5.6655221 -4.4487705 -2.1859412 -0.0044283867 1.0311661 1.7206087 1.7027626 0.7931962 0.033630371 0.64859629 0.7344017 -2.7075109 -5.3886223 -5.8324966 -6.6070623][-8.5705318 -7.4450116 -4.7454853 -1.613277 -0.63526487 -0.2607336 -0.68720627 -1.8838103 -2.4634237 -2.2605119 -2.5182967 -5.4974422 -7.320086 -7.4455357 -7.787539][-9.3395281 -8.3555975 -5.4014273 -2.8418975 -2.039434 -2.4126427 -3.4871795 -4.8971992 -5.2548037 -5.2742929 -5.4093804 -6.7664528 -7.7672706 -7.2837172 -7.1049509][-10.077326 -8.5905132 -5.5220242 -4.1064878 -3.7440658 -3.7602644 -5.0125494 -5.8206115 -6.1018209 -6.2991519 -6.2318525 -6.8943849 -7.3650756 -6.0557671 -5.0849061][-9.6858721 -7.9249954 -6.6383271 -5.5542278 -4.8003554 -5.0573716 -5.9567537 -5.8839703 -5.8068867 -5.6173959 -5.3008475 -5.2906828 -5.2900825 -4.5789323 -4.6386747][-6.8812242 -6.3395238 -6.2383041 -5.2817688 -5.2309923 -4.9893332 -4.3743033 -4.4934993 -4.5267453 -3.6279397 -3.8482218 -4.6770325 -4.87743 -5.3735232 -6.1737185]]...]
INFO - root - 2017-12-15 14:08:53.728369: step 20910, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 58h:24m:10s remains)
INFO - root - 2017-12-15 14:09:00.344156: step 20920, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 57h:13m:17s remains)
INFO - root - 2017-12-15 14:09:06.969625: step 20930, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.690 sec/batch; 59h:44m:13s remains)
INFO - root - 2017-12-15 14:09:13.545357: step 20940, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 55h:53m:59s remains)
INFO - root - 2017-12-15 14:09:20.138675: step 20950, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.684 sec/batch; 59h:12m:06s remains)
INFO - root - 2017-12-15 14:09:26.718401: step 20960, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.683 sec/batch; 59h:06m:03s remains)
INFO - root - 2017-12-15 14:09:33.269166: step 20970, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.681 sec/batch; 58h:56m:33s remains)
INFO - root - 2017-12-15 14:09:39.828728: step 20980, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 56h:41m:26s remains)
INFO - root - 2017-12-15 14:09:46.523873: step 20990, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 57h:20m:01s remains)
INFO - root - 2017-12-15 14:09:53.047199: step 21000, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.634 sec/batch; 54h:51m:13s remains)
2017-12-15 14:09:53.570753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.963913 -4.0042033 -4.0482435 -2.6701314 -2.1316445 -2.044199 -0.72623348 0.980052 1.9236312 0.19078922 -2.0371659 -6.4673538 -9.4935122 -13.073654 -13.795509][-2.8055041 -3.2941837 -3.5921781 -3.6877511 -3.6762953 -2.6783671 -0.66053772 0.13061571 1.057035 0.71728992 -1.036902 -5.8416924 -9.9662676 -14.25876 -16.171152][-2.9535284 -3.8199992 -3.7132878 -3.0880692 -3.6835828 -3.2677021 -1.5483828 0.41605043 0.17758036 -0.70741034 -1.8060887 -6.9546418 -10.381355 -13.615993 -15.594299][-2.0496125 -2.8258843 -2.5267541 -1.5698967 -1.2652917 -0.77048588 -0.85974407 0.054051876 0.85524654 -0.16200447 -2.0617385 -6.6228228 -9.9248352 -14.262421 -15.394705][-1.3741922 -1.165309 -1.0070281 -1.3111072 -1.0187387 0.26690769 0.687222 0.89407969 0.7432332 0.60802174 -1.9895475 -7.1807666 -10.074389 -13.954248 -15.74939][-2.0175018 -3.0731726 -2.0196552 -1.1580276 0.0016169548 0.7810626 2.0648723 2.6571536 1.8540192 0.3690176 -1.5878973 -6.6562657 -11.104502 -13.941248 -15.021311][-2.9744165 -4.0109477 -3.8152184 -2.1457436 -0.025521278 2.0884738 3.3900208 3.9558034 4.1452708 1.9698753 -0.90387011 -5.5957861 -8.76586 -12.767302 -14.477491][-4.3543234 -4.5219345 -4.0107241 -2.7871046 -0.71550369 2.2095809 4.1622396 4.6416764 4.6664062 3.4444976 0.8581605 -4.4402151 -7.7124906 -11.945358 -13.857086][-4.3823791 -6.1876993 -5.2142868 -2.7891004 -1.4804926 0.51321793 3.2094769 4.6089377 3.9512391 2.6821713 0.36838102 -4.4002819 -8.4079542 -11.851501 -13.215225][-3.7383027 -5.0772924 -5.7596726 -4.3843932 -2.5708969 -1.1043963 0.67658138 2.1018543 2.3988242 0.64385366 -1.0164661 -4.7637687 -8.1113892 -11.54014 -13.106123][-5.89917 -6.8108668 -7.0602674 -6.2690511 -5.8776083 -4.5947218 -2.9853215 -2.5857563 -2.0166042 -2.6010041 -4.0565748 -8.7208824 -9.872016 -12.191008 -11.894405][-7.6258888 -8.15045 -8.0148382 -7.0304108 -7.006413 -7.1086535 -6.4083791 -6.0069194 -5.4763222 -5.9334464 -7.4562283 -9.8186626 -10.348963 -12.251642 -12.880129][-9.0904465 -9.2272568 -8.0604448 -7.8194132 -7.9639077 -7.9603119 -8.0745449 -7.8108187 -7.1548281 -7.4404464 -8.1014137 -9.9551907 -11.226854 -11.319983 -11.178953][-8.5715275 -8.336792 -7.6617646 -6.7149844 -6.4114532 -7.5657153 -8.6308661 -8.5005894 -8.0065012 -8.0551825 -7.7957821 -7.9092803 -7.8159552 -9.6632147 -10.214206][-9.169261 -8.5802689 -8.095212 -7.6583853 -7.6618023 -7.6306238 -8.3324652 -9.032711 -8.7710371 -7.9452629 -7.9132032 -9.1560364 -9.1092014 -10.061924 -10.66736]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 14:10:00.203767: step 21010, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 58h:31m:16s remains)
INFO - root - 2017-12-15 14:10:06.737361: step 21020, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 55h:29m:52s remains)
INFO - root - 2017-12-15 14:10:13.328619: step 21030, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 57h:09m:56s remains)
INFO - root - 2017-12-15 14:10:19.931975: step 21040, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 56h:01m:11s remains)
INFO - root - 2017-12-15 14:10:26.514892: step 21050, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 56h:21m:41s remains)
INFO - root - 2017-12-15 14:10:33.052661: step 21060, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 56h:55m:40s remains)
INFO - root - 2017-12-15 14:10:39.671418: step 21070, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.676 sec/batch; 58h:26m:35s remains)
INFO - root - 2017-12-15 14:10:46.324386: step 21080, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 57h:55m:24s remains)
INFO - root - 2017-12-15 14:10:52.918225: step 21090, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 57h:02m:53s remains)
INFO - root - 2017-12-15 14:10:59.558543: step 21100, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 57h:57m:03s remains)
2017-12-15 14:11:00.074521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.757498 -4.2585783 -4.925837 -5.07354 -6.6721854 -8.2795715 -8.9744549 -8.7027988 -8.40369 -7.611856 -6.338963 -8.0293312 -9.4713793 -9.7318249 -8.8294621][-4.0172358 -3.7884681 -3.374428 -3.9712927 -5.8785567 -7.1845527 -8.0037279 -8.607502 -8.7944412 -8.6123714 -8.1251869 -9.308919 -11.584553 -11.483203 -8.9941616][-1.1252155 -2.90995 -3.9899745 -3.5771692 -4.7231226 -6.1094441 -7.389576 -8.0201206 -7.8120193 -8.1599131 -8.0962925 -9.28991 -11.287544 -11.625416 -10.399494][-4.0623374 -4.7859297 -4.8978148 -5.1447043 -5.2765923 -4.679275 -4.4871178 -5.6223307 -6.9371085 -7.2958431 -7.189539 -9.1190519 -10.806137 -10.711527 -9.5165367][-5.7468739 -8.0925636 -9.025053 -7.15331 -5.6280351 -2.480257 -1.3708596 -3.2076769 -4.9739919 -6.3883429 -7.5907068 -8.8647957 -9.327424 -9.5894184 -8.4743681][-9.0591488 -10.118971 -10.116289 -7.2710457 -3.9293106 0.21337843 2.6060252 2.3437524 0.28768778 -3.6509032 -6.1000652 -7.4288225 -8.8306513 -8.3388233 -6.4606676][-10.816998 -9.9681129 -9.2799 -6.7779026 -3.8916988 0.98387909 6.3366284 7.2030797 5.0991197 0.2767396 -3.9222255 -6.5473223 -8.2891283 -7.9279327 -6.7361813][-13.75342 -12.360229 -10.20857 -7.2471662 -4.4549475 0.66935873 5.5815206 6.2649817 6.3628793 2.5027881 -1.8755412 -5.5510192 -8.7342129 -8.1830692 -6.135365][-13.663834 -13.373212 -11.412422 -6.7442956 -3.3438487 -0.70986509 1.5629683 3.8190093 5.2219043 1.3712916 -1.7802677 -5.2455897 -8.9773178 -8.7918491 -6.9584508][-12.640615 -12.563366 -11.838672 -7.92832 -4.5528631 -2.0895228 0.22599173 1.4014468 0.975029 -0.70621538 -1.9888372 -5.4167466 -8.8825617 -9.7611809 -9.2318249][-14.654089 -14.557259 -14.041813 -11.121059 -8.5982761 -6.0101128 -3.1078446 -2.9330072 -3.6934972 -3.4734402 -4.1707458 -8.2553158 -10.754524 -10.373523 -9.7174587][-19.11153 -18.358454 -16.962196 -14.230799 -12.740122 -10.257376 -7.4890428 -7.4387665 -7.3894043 -7.2765741 -8.0478926 -9.5885963 -10.195396 -10.462542 -10.571037][-19.004971 -17.791988 -16.580111 -14.448893 -12.279186 -10.424677 -9.21017 -8.5175018 -7.9579134 -7.7521257 -7.6435876 -8.5568333 -9.21706 -8.7819691 -8.4655228][-15.773315 -14.129192 -12.886797 -10.602302 -8.6084938 -8.691803 -8.6999769 -7.9707532 -7.8822551 -8.3079939 -8.308032 -8.5909138 -9.0561628 -7.9828558 -7.2741795][-11.180085 -9.7572937 -7.9953213 -5.7089119 -5.0265427 -4.6809807 -4.2393532 -5.3578534 -6.3163171 -6.0817294 -6.5049357 -8.0074844 -9.6220675 -9.7698193 -9.2386971]]...]
INFO - root - 2017-12-15 14:11:06.621517: step 21110, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 57h:32m:17s remains)
INFO - root - 2017-12-15 14:11:13.275889: step 21120, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 56h:18m:49s remains)
INFO - root - 2017-12-15 14:11:19.892556: step 21130, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 56h:17m:04s remains)
INFO - root - 2017-12-15 14:11:26.523918: step 21140, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 58h:08m:17s remains)
INFO - root - 2017-12-15 14:11:33.132831: step 21150, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 56h:18m:24s remains)
INFO - root - 2017-12-15 14:11:39.860643: step 21160, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 57h:04m:10s remains)
INFO - root - 2017-12-15 14:11:46.446154: step 21170, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 57h:51m:59s remains)
INFO - root - 2017-12-15 14:11:52.965961: step 21180, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.637 sec/batch; 55h:04m:51s remains)
INFO - root - 2017-12-15 14:11:59.607546: step 21190, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 57h:24m:03s remains)
INFO - root - 2017-12-15 14:12:06.194690: step 21200, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 57h:07m:44s remains)
2017-12-15 14:12:06.673314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.360033 -5.7761745 -5.9517121 -5.9172816 -6.6902261 -7.0646076 -6.5043092 -5.7937126 -5.3289409 -5.0860295 -4.6614671 -6.5891523 -8.7884521 -10.111551 -9.6737537][-6.3741112 -6.8081894 -6.9015608 -7.1773248 -7.84528 -7.9185104 -7.4492297 -6.8534932 -6.5391808 -6.3788662 -6.2562346 -8.1919527 -9.84931 -10.97785 -10.837851][-4.4827209 -6.183095 -7.8282533 -8.1161642 -8.6852007 -8.62097 -8.0081892 -6.882236 -6.5158854 -7.0662827 -7.4554439 -9.6761141 -11.732187 -12.368345 -12.359073][-5.3683858 -6.3451886 -7.1123676 -8.2577219 -8.9089556 -7.6049347 -5.82273 -5.4917822 -5.5522127 -5.9672937 -6.9490266 -9.766573 -12.004719 -13.14023 -12.900416][-6.2119522 -6.8390603 -8.02015 -8.6526279 -7.8644085 -5.4276881 -3.0239515 -3.2256098 -4.0167284 -5.0324125 -5.9316578 -8.7944565 -11.465004 -13.085524 -12.934338][-8.0239191 -8.1517382 -7.9400682 -6.7710552 -5.1271524 -2.5147111 0.51107454 1.8767896 0.82012367 -2.5682673 -4.5892668 -7.0874381 -10.101234 -11.815685 -12.071987][-8.9815578 -9.2242584 -8.9984465 -5.8992538 -2.8167813 0.078715324 3.5282264 5.3472133 5.0018449 1.1548905 -2.8934283 -6.745945 -9.8012609 -11.282426 -11.323662][-9.4672585 -9.2473087 -8.747242 -5.4779525 -1.9976113 3.0067382 6.2676616 5.8196521 5.6169744 3.324966 -0.65942907 -5.9449077 -9.9859095 -11.839285 -11.783108][-8.0408087 -7.941926 -7.5596614 -5.3644819 -2.8644559 1.9778833 5.0440135 5.229074 4.0534329 1.3556223 -0.68936586 -5.4268413 -10.180023 -12.249756 -12.077824][-5.7782669 -6.0366144 -5.9949603 -4.3262615 -3.8054879 -1.7988274 0.50032568 2.2731967 1.4800186 -1.1531591 -2.832969 -6.9773831 -10.40749 -12.096688 -12.517006][-9.091383 -9.0939827 -8.26222 -6.9186611 -6.7027435 -5.9361534 -5.6556931 -5.2225575 -5.0631142 -5.7447486 -6.9853926 -10.477552 -12.115773 -13.3422 -12.198062][-13.12292 -12.758915 -11.794456 -10.635211 -10.12132 -9.1506529 -9.4141846 -10.220692 -10.642075 -10.786024 -11.006182 -12.119892 -12.123357 -12.77057 -11.671769][-14.038126 -13.277863 -12.398746 -12.201504 -12.199157 -11.085039 -10.930451 -11.128784 -11.851078 -11.727621 -11.595301 -11.726851 -11.353729 -11.392243 -9.8517036][-11.319414 -10.915947 -10.54609 -10.479378 -10.349678 -10.033848 -10.035691 -9.5313206 -9.4020958 -9.8823814 -10.279252 -9.3184185 -8.9001484 -9.0439911 -8.2347374][-8.79082 -8.7454844 -8.2874317 -7.1194429 -6.4021649 -6.53375 -6.7581697 -7.2662163 -7.7125025 -7.5178032 -7.8804588 -8.6884928 -9.118433 -9.222271 -9.3230419]]...]
INFO - root - 2017-12-15 14:12:13.296636: step 21210, loss = 0.10, batch loss = 0.06 (11.9 examples/sec; 0.672 sec/batch; 58h:06m:54s remains)
INFO - root - 2017-12-15 14:12:19.917572: step 21220, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.686 sec/batch; 59h:16m:38s remains)
INFO - root - 2017-12-15 14:12:26.534900: step 21230, loss = 0.29, batch loss = 0.25 (12.2 examples/sec; 0.656 sec/batch; 56h:43m:07s remains)
INFO - root - 2017-12-15 14:12:33.054714: step 21240, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 56h:28m:23s remains)
INFO - root - 2017-12-15 14:12:39.629485: step 21250, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 56h:47m:10s remains)
INFO - root - 2017-12-15 14:12:46.172860: step 21260, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 57h:06m:25s remains)
INFO - root - 2017-12-15 14:12:52.802123: step 21270, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 58h:33m:55s remains)
INFO - root - 2017-12-15 14:12:59.377128: step 21280, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.684 sec/batch; 59h:08m:48s remains)
INFO - root - 2017-12-15 14:13:05.947067: step 21290, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 55h:21m:31s remains)
INFO - root - 2017-12-15 14:13:12.505262: step 21300, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 55h:10m:34s remains)
2017-12-15 14:13:13.056030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7317226 -1.8845713 -1.3656812 -1.595695 -2.7383337 -3.5962045 -4.4793921 -4.2496243 -3.5822952 -4.0047436 -4.264349 -7.5710225 -10.634451 -11.05464 -10.500054][-0.37155151 0.069850922 0.54679155 -0.40707827 -2.218967 -3.649976 -4.8789654 -5.604857 -6.5877523 -6.9839931 -7.3268003 -10.43322 -12.658819 -13.767328 -13.594372][-0.34264231 -0.13424206 0.84835768 1.4180865 0.68721247 -0.88795662 -2.7676337 -4.7118392 -6.3192363 -7.393363 -8.2115784 -10.97765 -13.066022 -13.648682 -12.773109][-1.6235633 -0.42433643 0.4048152 0.99323177 0.14344263 -0.84912252 -1.4227009 -2.5085969 -3.6197629 -4.8706551 -6.1497297 -9.1481867 -11.777208 -12.483927 -12.12219][-2.8259361 -2.937 -2.334208 -0.55808449 -0.66526651 -0.52305031 -0.67912769 -1.7403529 -2.6294055 -3.5213525 -4.4858146 -7.2323751 -9.9012566 -11.191224 -10.885515][-4.5148335 -4.0118017 -3.1023681 -1.0473423 0.2059927 1.4251919 1.971941 1.5670328 0.73082352 -0.79399824 -2.4435809 -5.1117187 -8.1034365 -9.3911629 -9.52276][-5.9847565 -5.36839 -3.7339635 -1.284482 0.83075523 2.6567826 3.8821759 4.0539823 3.6561236 1.5794048 -0.89481592 -4.1565032 -7.9426479 -9.4630251 -9.6359377][-7.3841143 -6.8023067 -4.7508278 -1.512404 0.30185175 2.4828839 4.0994763 4.3273044 4.6963 3.4882026 1.6283607 -2.761512 -7.5770669 -9.1184139 -9.5424767][-8.0447054 -7.1381655 -5.6221714 -3.010112 -0.97204351 0.92470837 2.331677 2.5289111 2.8296685 2.1003008 1.0361133 -3.4665515 -7.7668004 -8.9704056 -9.2066822][-9.0733786 -8.5111952 -6.7411795 -4.386054 -2.6715574 -0.77183485 0.38437748 0.87383938 0.83726692 0.14381027 -0.8937068 -4.2672968 -7.3426723 -7.9498072 -7.5939884][-12.471507 -11.134014 -9.1200418 -6.6870995 -5.6108856 -4.8453183 -3.663286 -3.2827451 -3.3602898 -3.7601075 -4.5466552 -7.5938516 -9.4403858 -9.6370926 -8.2472725][-15.300877 -14.057589 -12.227549 -10.340528 -9.0998955 -8.0417433 -7.2283387 -6.8587856 -6.5668807 -6.868382 -7.4515514 -9.0479956 -10.397744 -10.360232 -9.148243][-16.125937 -14.782402 -13.283031 -12.08522 -11.423167 -10.784638 -10.214172 -10.092684 -9.9347134 -9.68088 -9.6609688 -10.305202 -10.55081 -9.6169758 -8.6713953][-12.645219 -11.86191 -11.365699 -9.6484947 -8.7241869 -8.51861 -8.7216749 -8.8958645 -9.119319 -9.2778521 -9.5972166 -9.3237925 -9.393631 -8.6276112 -7.9871254][-9.056879 -8.4105 -7.0668411 -5.4767847 -4.9267673 -4.4076614 -4.8424039 -5.399014 -6.0488834 -6.6725025 -7.9061441 -9.1819315 -10.341167 -10.472645 -10.120724]]...]
INFO - root - 2017-12-15 14:13:19.664362: step 21310, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 58h:43m:54s remains)
INFO - root - 2017-12-15 14:13:26.226774: step 21320, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 57h:39m:31s remains)
INFO - root - 2017-12-15 14:13:32.925291: step 21330, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 57h:20m:46s remains)
INFO - root - 2017-12-15 14:13:39.424734: step 21340, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 55h:59m:01s remains)
INFO - root - 2017-12-15 14:13:45.953476: step 21350, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 56h:24m:43s remains)
INFO - root - 2017-12-15 14:13:52.598635: step 21360, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.632 sec/batch; 54h:39m:33s remains)
INFO - root - 2017-12-15 14:13:59.221755: step 21370, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 57h:49m:00s remains)
INFO - root - 2017-12-15 14:14:05.762433: step 21380, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 56h:01m:07s remains)
INFO - root - 2017-12-15 14:14:12.358009: step 21390, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.677 sec/batch; 58h:27m:46s remains)
INFO - root - 2017-12-15 14:14:19.000789: step 21400, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.672 sec/batch; 58h:04m:00s remains)
2017-12-15 14:14:19.557841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1814752 -3.7977912 -2.4637246 -1.488646 -1.1806688 -1.5190673 -3.101265 -3.10781 -2.5503104 -2.5533435 -1.0228653 -1.7119927 -3.2673807 -4.8411016 -5.2301354][-4.3589926 -4.1395245 -4.2563787 -3.1266587 -1.9675274 -2.4942224 -3.6400135 -3.5460646 -3.5776825 -3.0309842 -2.6360705 -2.6184509 -3.3803036 -4.811193 -4.2854586][-2.7850363 -3.4167535 -4.3914423 -4.3699093 -3.521965 -3.7697854 -4.1226721 -4.2255335 -4.6112547 -4.4550838 -3.8297505 -4.524229 -6.4034052 -7.0332823 -6.823009][-5.2462511 -5.7349262 -5.30938 -5.2814097 -5.8489938 -5.7731628 -5.49385 -5.7559414 -5.3348894 -4.5975361 -4.9876232 -5.8939128 -8.6440773 -10.001305 -9.7408209][-5.8870645 -6.7179193 -6.44333 -5.5951724 -5.6709166 -5.3135309 -4.6896915 -4.3022718 -4.0357742 -3.7929831 -3.1659174 -5.5938282 -9.8426685 -11.901423 -11.86727][-5.9450688 -6.0993681 -4.791523 -2.4950302 -2.0740714 -1.2989001 -1.3630896 -0.857285 -0.16826153 -1.6241231 -3.146183 -5.9141784 -10.749981 -12.784233 -13.256628][-6.7128263 -4.9160671 -3.7604096 -1.5739036 0.57806778 1.6710854 2.1962495 2.3434644 2.2607942 1.4331989 -0.11920977 -3.68437 -8.0403881 -10.397158 -12.281266][-4.2606 -4.2788205 -2.2550442 -1.0754805 0.97698736 2.7924523 3.2241168 3.7189212 4.1180568 3.1986575 2.3742385 -0.63463593 -5.2672215 -8.1100454 -9.0392351][-4.6643529 -4.8827806 -4.98114 -2.9628303 -0.054042816 2.4737926 3.3561873 3.8457923 4.8095365 5.1949897 4.8543363 1.2061372 -2.990577 -5.9142575 -6.6161814][-3.7949214 -5.0804448 -5.0262361 -4.3357315 -2.9200358 -0.062002659 1.7233891 2.8523088 3.8303409 4.1646781 4.7370639 2.4156003 -1.8959117 -4.3373632 -5.4241481][-6.4895673 -6.0788136 -6.0574074 -4.4487915 -3.3840704 -2.5389009 -2.1162424 -0.648757 -0.078327656 0.026200294 0.4019351 -1.4603672 -3.3229921 -4.6405692 -3.973423][-10.258724 -8.6385822 -6.4447012 -5.1704154 -5.5485363 -4.561965 -4.1463203 -3.9847078 -3.6901882 -3.4212875 -3.4039242 -5.7111979 -7.1110358 -7.0859642 -5.7059083][-10.087021 -7.9941339 -5.7002554 -4.2927761 -3.1769798 -2.9141152 -4.4190664 -6.0442533 -6.8938665 -6.5833244 -6.8046961 -7.4173083 -7.8639588 -8.2456875 -7.2347479][-9.6519279 -7.6899271 -5.9576969 -4.8418112 -4.0247269 -3.7419157 -5.6698222 -7.3745522 -8.0249119 -8.76555 -9.5605278 -8.5323639 -8.4185915 -8.2564878 -7.4823146][-6.58245 -5.6549244 -5.204071 -5.1654353 -5.244792 -5.525702 -6.0013142 -6.2649617 -6.5393009 -7.2517271 -7.1840792 -7.7679167 -9.0674458 -9.61772 -9.2834644]]...]
INFO - root - 2017-12-15 14:14:26.105236: step 21410, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 57h:07m:40s remains)
INFO - root - 2017-12-15 14:14:32.704558: step 21420, loss = 0.11, batch loss = 0.06 (12.0 examples/sec; 0.666 sec/batch; 57h:34m:31s remains)
INFO - root - 2017-12-15 14:14:39.255243: step 21430, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 56h:23m:09s remains)
INFO - root - 2017-12-15 14:14:45.924599: step 21440, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.686 sec/batch; 59h:16m:46s remains)
INFO - root - 2017-12-15 14:14:52.466711: step 21450, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 55h:52m:10s remains)
INFO - root - 2017-12-15 14:14:59.090190: step 21460, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 57h:05m:48s remains)
INFO - root - 2017-12-15 14:15:05.654606: step 21470, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 56h:57m:10s remains)
INFO - root - 2017-12-15 14:15:12.239276: step 21480, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 56h:55m:42s remains)
INFO - root - 2017-12-15 14:15:18.834605: step 21490, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 57h:04m:12s remains)
INFO - root - 2017-12-15 14:15:25.535974: step 21500, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 58h:11m:46s remains)
2017-12-15 14:15:26.055243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4820623 -8.2091818 -7.840332 -7.5767508 -7.5273585 -7.410533 -7.881258 -7.7414064 -7.1415305 -6.1650047 -5.4767261 -6.5186043 -7.4617686 -6.8022513 -5.3173046][-5.5022197 -6.7420154 -6.6730618 -5.7446485 -5.4802451 -6.1872931 -6.8329926 -7.3782854 -7.8853436 -7.3595829 -6.4273863 -7.2541032 -7.9098625 -7.5443068 -6.4273558][-4.5171041 -5.4652982 -5.6513333 -4.6951404 -4.060668 -4.5974588 -5.4792356 -6.0026712 -6.5863204 -6.8693 -6.8040543 -7.889329 -7.6008554 -7.4654889 -6.9205875][-6.5909472 -6.5777373 -6.0532932 -4.3761616 -3.6988893 -2.8763793 -3.3243227 -4.5875406 -5.5089726 -5.4317665 -4.7316666 -6.2815886 -7.6209288 -7.3836112 -6.3483486][-7.1485534 -8.4688473 -7.8525996 -5.1169839 -3.0168405 -1.6479936 -1.0222077 -1.6879616 -2.4925027 -2.9475574 -3.5877359 -5.2186828 -6.1569357 -6.0654225 -4.9402089][-8.826436 -9.2167225 -6.5106773 -3.624604 -1.4806786 0.94495964 2.7141519 2.300324 1.1016431 -0.0099730492 -1.2810526 -3.0411375 -4.1089621 -5.40153 -5.7441173][-9.4088421 -8.8768311 -5.76291 -1.4955869 1.3536272 4.137732 5.0210767 5.053576 4.1612639 1.8684001 0.23699951 -1.3518372 -3.191596 -3.6865709 -3.3897328][-8.6898613 -7.2949004 -4.7592835 -0.84350157 2.0964947 4.4804845 5.7364192 5.5202413 4.9444642 2.951282 0.66875219 -1.7682209 -3.0424354 -3.1410668 -2.3186133][-6.7474971 -5.596879 -3.7359378 -1.2923994 0.052389622 2.31854 4.1572175 4.5415072 3.3303742 1.1566415 -0.37447739 -2.4986892 -3.5780814 -3.9501638 -3.1928186][-6.68124 -5.4224963 -4.1037683 -1.9392102 -0.57442808 0.88514376 1.8481655 2.0295172 1.5544996 0.41136551 -1.392436 -3.5330822 -3.6905127 -3.4301217 -3.2695887][-11.329138 -9.8614674 -7.6088934 -5.1407371 -4.2499952 -3.157264 -2.1521907 -1.9497359 -2.2727778 -3.2446194 -4.4607048 -5.8339157 -5.6597424 -5.0443449 -3.7148974][-13.33552 -11.878033 -10.40911 -8.8259811 -8.1830235 -6.6119733 -6.3063774 -5.7061124 -5.57541 -5.8714433 -6.9108667 -7.7150521 -7.5913224 -6.43605 -4.6798716][-12.710663 -11.759102 -9.8248949 -8.253768 -7.5481372 -6.5101132 -6.8265238 -6.2643085 -6.2449656 -6.1785083 -6.5468922 -6.1733623 -6.434679 -5.4316678 -4.0188475][-12.322311 -11.036686 -8.783205 -7.1215677 -5.8263073 -5.4296894 -6.1356606 -5.8573046 -5.6939631 -5.9102721 -6.2678547 -5.8118038 -5.7297769 -4.6596217 -4.3447742][-8.9650364 -8.7551575 -7.1346774 -6.7567177 -5.6473608 -4.911797 -4.7684135 -3.9575367 -4.2119675 -4.6929531 -4.574892 -4.8246574 -5.7247753 -5.8008704 -5.9324946]]...]
INFO - root - 2017-12-15 14:15:32.716884: step 21510, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 58h:43m:41s remains)
INFO - root - 2017-12-15 14:15:39.272048: step 21520, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 55h:20m:08s remains)
INFO - root - 2017-12-15 14:15:45.950041: step 21530, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 57h:10m:54s remains)
INFO - root - 2017-12-15 14:15:52.538479: step 21540, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 57h:56m:56s remains)
INFO - root - 2017-12-15 14:15:59.168275: step 21550, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 55h:43m:39s remains)
INFO - root - 2017-12-15 14:16:05.845596: step 21560, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 57h:19m:51s remains)
INFO - root - 2017-12-15 14:16:12.473063: step 21570, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 55h:42m:24s remains)
INFO - root - 2017-12-15 14:16:19.121265: step 21580, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 55h:42m:16s remains)
INFO - root - 2017-12-15 14:16:25.754487: step 21590, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 57h:10m:03s remains)
INFO - root - 2017-12-15 14:16:32.478189: step 21600, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.690 sec/batch; 59h:36m:05s remains)
2017-12-15 14:16:32.961761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.4672031 -8.84258 -9.6968927 -9.3322639 -9.5300388 -9.6174116 -9.8394232 -9.6360321 -9.4921265 -9.0569763 -8.3442459 -7.9102116 -8.3777266 -7.2067986 -5.2045803][-7.9423895 -8.5470295 -9.0633821 -8.8469381 -9.20353 -9.2124863 -9.1519394 -8.9246817 -8.81897 -8.5408411 -8.03355 -7.8892536 -8.3484354 -7.5130911 -6.3243904][-7.1940222 -7.823061 -7.7768164 -6.9580359 -7.5539346 -7.4658904 -7.8870325 -7.8601422 -7.5653462 -7.8631687 -8.0275736 -8.7835989 -9.6369934 -8.8575325 -7.9948168][-7.6001678 -7.5057068 -6.9982591 -5.8302693 -6.0924387 -5.4608116 -4.8899055 -4.8487105 -5.2524471 -5.5566854 -5.3485141 -7.1296368 -9.2186012 -9.4341917 -9.8955736][-7.9219666 -8.3246632 -7.6760683 -5.6626182 -4.6237707 -2.6396296 -1.8174329 -2.0179267 -1.8360231 -2.3155744 -2.5719752 -4.2014627 -5.7234263 -7.4266233 -8.7753687][-8.0160732 -8.304842 -6.6126466 -4.5101671 -3.0295134 0.13101101 1.4375863 1.5151844 1.3307571 0.7378912 0.24977922 -1.620286 -3.7265549 -5.1613083 -6.5022697][-8.6411171 -8.41683 -6.8655071 -3.883821 -1.1763892 1.7531829 3.3254075 4.3236394 4.2399726 2.0868564 0.658288 -1.2102938 -3.8302715 -5.6983933 -6.6756907][-8.3991776 -7.4080796 -5.7282333 -2.4741027 0.18528938 2.7695327 4.1546912 4.6675429 4.8574347 3.9533496 2.3164182 -1.3071589 -4.7501006 -6.5246716 -7.3670731][-7.7112756 -6.6078558 -4.2394075 -1.216826 0.52844477 2.0618596 2.1405406 2.4726262 3.3430886 2.8768573 1.3680525 -1.8060305 -5.0868435 -7.6592236 -8.0884686][-6.3694997 -4.9160089 -3.2210193 -0.534194 0.68265438 0.71848774 0.63392305 0.9437952 0.5739789 1.0328412 1.3940158 -1.6601009 -5.5491991 -7.7130203 -9.464551][-8.19342 -6.5699449 -5.1071739 -2.8555973 -2.5741215 -3.03899 -2.7882898 -2.650579 -2.9915793 -3.1161947 -3.6040046 -5.3784347 -6.9325585 -7.8774977 -8.9561758][-10.676707 -9.629055 -8.4160032 -6.3163514 -6.2246995 -6.1987934 -6.4968209 -6.7090621 -6.90041 -6.6208024 -7.1867952 -8.1679945 -8.3452158 -8.199707 -7.97795][-11.816193 -10.680657 -9.1006908 -7.6691647 -8.1916628 -7.9723988 -7.5238986 -8.0296669 -7.5510988 -7.9145494 -7.7743406 -7.1583161 -6.7745457 -5.7338295 -4.2584834][-10.951895 -10.233686 -9.3835907 -8.0194674 -6.6787868 -6.080976 -6.9275122 -6.8428783 -6.5109887 -6.0184836 -6.2608366 -4.8795242 -3.8726296 -2.7632871 -2.4550157][-8.0062094 -7.86421 -7.1693525 -5.8027053 -5.60001 -4.2051158 -3.6895387 -4.9634514 -6.2939453 -6.3016605 -5.4139524 -5.3176203 -5.1698461 -4.3760982 -4.119174]]...]
INFO - root - 2017-12-15 14:16:39.593088: step 21610, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 56h:19m:57s remains)
INFO - root - 2017-12-15 14:16:46.186444: step 21620, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 57h:24m:31s remains)
INFO - root - 2017-12-15 14:16:52.800225: step 21630, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 57h:31m:14s remains)
INFO - root - 2017-12-15 14:16:59.486924: step 21640, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.688 sec/batch; 59h:23m:29s remains)
INFO - root - 2017-12-15 14:17:06.122441: step 21650, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 57h:35m:39s remains)
INFO - root - 2017-12-15 14:17:12.714227: step 21660, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 56h:58m:40s remains)
INFO - root - 2017-12-15 14:17:19.305849: step 21670, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 55h:06m:16s remains)
INFO - root - 2017-12-15 14:17:25.916560: step 21680, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 55h:10m:37s remains)
INFO - root - 2017-12-15 14:17:32.527672: step 21690, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 59h:10m:38s remains)
INFO - root - 2017-12-15 14:17:39.178001: step 21700, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 57h:36m:29s remains)
2017-12-15 14:17:39.730593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5632448 -5.3957615 -5.5529714 -4.212697 -4.8686275 -5.3739991 -6.0843434 -7.1887984 -8.4154606 -10.024499 -9.8853683 -7.8490391 -9.45466 -9.3939161 -8.3776131][-5.4662795 -5.8667636 -6.4782891 -5.7641068 -5.4283948 -6.0817223 -6.5408573 -7.1113954 -7.8051844 -8.2624865 -8.8225212 -7.040318 -8.8803034 -8.8913689 -8.4442215][-4.0956359 -5.747643 -7.1644897 -7.0707831 -7.5690966 -7.266675 -7.2886477 -7.6586075 -8.4075546 -9.07028 -8.75903 -6.1740508 -7.9451227 -7.4570475 -6.3769226][-4.5009546 -6.5271478 -7.2310224 -7.8900766 -8.8273582 -7.4055729 -6.5860658 -7.798418 -8.0711946 -8.44749 -8.5615788 -5.8020597 -7.7574062 -7.7569809 -7.2881832][-5.0875268 -7.2694578 -8.3819866 -7.0864153 -7.2160439 -5.2780294 -3.6166859 -4.4257 -6.1401505 -6.8318233 -7.1385307 -5.2892179 -6.7291989 -7.2610741 -7.166276][-7.6301765 -7.7490616 -8.4140015 -7.13917 -5.0403557 -2.5938601 -0.61456203 0.092294216 -1.2761593 -4.3334303 -6.1337824 -4.0759382 -6.4402194 -7.2824349 -7.4941983][-7.2928219 -7.4181881 -7.4159489 -5.7073722 -1.9666731 1.0901256 3.6066051 4.2839866 3.503593 0.52806234 -2.8875554 -2.89576 -5.094378 -6.3947563 -7.458652][-7.9471588 -7.5310516 -6.4665637 -4.4466033 -1.6924858 3.0244679 6.0433335 6.2537923 4.97002 2.8342147 0.088057995 -1.1306324 -4.6095409 -5.7269607 -7.2830305][-7.0703459 -7.3127069 -6.8357992 -4.6276402 -2.8623362 0.13761377 4.2991519 4.3287063 1.9777389 -0.098673344 -0.55935335 -0.25205326 -3.9991064 -6.172451 -6.4586935][-8.48 -7.7738042 -7.4877415 -5.6115894 -3.5459363 -0.86081314 0.93033981 1.3355832 0.14985847 -2.1339655 -3.71883 -1.7007904 -4.6041555 -6.1365681 -6.8205032][-11.694597 -10.766159 -10.090445 -8.5873833 -6.29152 -3.9916339 -2.9272406 -4.0238013 -4.0758529 -5.3202825 -5.7201304 -4.934638 -7.3875914 -7.143374 -6.70018][-14.195627 -12.287121 -11.036511 -8.7794313 -6.0532489 -3.7732973 -3.2053604 -4.5795126 -6.0874367 -6.0740223 -5.6336904 -4.6320295 -7.3733883 -7.3640752 -5.3025026][-13.368103 -11.762072 -8.8907948 -6.9910989 -4.8869357 -2.1398232 -1.786953 -3.9699512 -5.4925394 -6.3327141 -5.3246312 -3.8750539 -4.9201417 -6.1443624 -6.0262804][-10.80758 -9.80294 -6.3942032 -3.5036416 -2.0902297 -1.763521 -2.6877987 -3.3276498 -4.1115913 -4.7197642 -4.3733315 -3.80294 -3.6085429 -2.9224536 -3.3961725][-9.62373 -9.3273277 -6.0478568 -2.2253854 -1.7555721 -2.7985725 -3.8660913 -4.8929081 -4.9528379 -4.4410696 -3.7286649 -3.9055991 -4.3557076 -3.265522 -3.6081562]]...]
INFO - root - 2017-12-15 14:17:46.347700: step 21710, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 56h:35m:36s remains)
INFO - root - 2017-12-15 14:17:53.026564: step 21720, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 56h:14m:42s remains)
INFO - root - 2017-12-15 14:17:59.623178: step 21730, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 56h:27m:05s remains)
INFO - root - 2017-12-15 14:18:06.245706: step 21740, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 57h:18m:13s remains)
INFO - root - 2017-12-15 14:18:12.900493: step 21750, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 55h:43m:41s remains)
INFO - root - 2017-12-15 14:18:19.417985: step 21760, loss = 0.12, batch loss = 0.08 (12.7 examples/sec; 0.628 sec/batch; 54h:13m:09s remains)
INFO - root - 2017-12-15 14:18:25.995076: step 21770, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 57h:28m:27s remains)
INFO - root - 2017-12-15 14:18:32.637430: step 21780, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 58h:26m:09s remains)
INFO - root - 2017-12-15 14:18:39.344676: step 21790, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 56h:26m:37s remains)
INFO - root - 2017-12-15 14:18:45.922529: step 21800, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 55h:34m:16s remains)
2017-12-15 14:18:46.437512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.769752 -3.963635 -4.2749405 -3.9710183 -4.4799232 -4.5694122 -4.418829 -4.1654053 -4.1110811 -3.6566772 -3.2919364 -4.0848031 -3.8975542 -6.2978988 -5.4853349][-4.411685 -3.6622655 -2.6407022 -2.5574605 -2.9328606 -2.6048203 -2.8677776 -3.0355878 -3.3482285 -2.9374516 -2.0333409 -2.4975677 -2.4300232 -5.6858544 -5.7453194][-3.3514459 -3.4852326 -3.4936552 -2.3612497 -2.7632971 -2.3763306 -1.7841229 -1.1812987 -0.10988665 -0.31231785 -0.81095314 -1.6257691 -1.3404727 -5.3592234 -5.7739978][-3.68961 -4.1267052 -4.0800486 -2.7765844 -1.6744366 -0.88230276 -0.52542067 0.081970692 0.76984024 0.94020653 0.67665815 -0.16089344 0.25255871 -2.9405282 -3.7397337][-3.1385436 -4.3822794 -4.6536131 -2.7469726 -1.1056027 0.6828723 1.0995998 1.7028069 2.3455596 1.0253325 -0.031220436 -1.1422081 -1.7266369 -4.506855 -4.105979][-4.0489244 -4.888555 -4.2191133 -2.2389193 0.05802393 2.7271733 3.2769856 2.6609607 2.7275643 1.1587839 -0.20700741 -1.1384683 -1.4466658 -4.8848109 -5.1291003][-3.715626 -3.9254937 -2.5205641 -1.138371 0.72528172 4.0949035 5.4201913 4.6576028 4.082623 2.6840682 1.7708488 -0.57961512 -1.7815592 -4.7297268 -4.539361][-4.084394 -3.8882585 -2.1790261 0.51070976 3.0321755 6.1190562 6.9753222 5.6371779 4.9577689 2.942615 1.1796598 0.012268543 -0.10187197 -4.1395493 -5.1249042][-4.986804 -5.199544 -4.150671 -0.51139021 1.6529622 3.9507165 5.3975387 4.7825675 3.6370444 1.5018744 0.61216021 -0.94828367 -1.665339 -4.4255457 -5.0500903][-5.1922145 -5.9097815 -5.5156121 -2.5646732 -0.60740614 1.2201624 2.5866847 2.3098769 1.7656136 -0.27552557 -1.4881229 -2.7855124 -3.1767049 -6.6696143 -6.9235716][-7.4987745 -8.1641006 -7.2444487 -5.450666 -4.5021605 -2.9880726 -1.7840278 -1.3285503 -1.1452827 -2.3666613 -3.4222348 -4.6439037 -5.1173096 -7.207581 -6.9916754][-10.599949 -9.5844135 -8.8585052 -7.165967 -5.6981282 -5.0267096 -5.0930753 -5.0772495 -5.1084275 -4.8275514 -4.3561778 -4.8010435 -5.0340033 -6.862205 -7.5799608][-9.9812775 -9.8755836 -8.467905 -7.1932659 -7.0384722 -4.88054 -4.446763 -4.8197908 -4.9658909 -4.8999987 -5.0477533 -5.2192874 -4.831522 -5.9324269 -6.5665135][-8.4400578 -8.2965651 -8.5524359 -7.5761633 -5.871695 -4.714798 -3.9112449 -2.920907 -3.3382905 -3.8262453 -3.7818944 -3.0205605 -2.6291711 -4.4227991 -5.0594645][-6.5919271 -6.1938958 -4.8794141 -5.4357481 -5.566359 -3.848135 -3.12203 -2.565912 -2.3389046 -2.7422645 -3.3526492 -3.7587914 -3.8735561 -4.8662176 -5.5058823]]...]
INFO - root - 2017-12-15 14:18:53.022678: step 21810, loss = 0.24, batch loss = 0.20 (11.9 examples/sec; 0.670 sec/batch; 57h:50m:21s remains)
INFO - root - 2017-12-15 14:18:59.497620: step 21820, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 54h:58m:21s remains)
INFO - root - 2017-12-15 14:19:06.086092: step 21830, loss = 0.13, batch loss = 0.08 (12.7 examples/sec; 0.629 sec/batch; 54h:15m:12s remains)
INFO - root - 2017-12-15 14:19:12.701599: step 21840, loss = 0.12, batch loss = 0.08 (12.7 examples/sec; 0.631 sec/batch; 54h:29m:18s remains)
INFO - root - 2017-12-15 14:19:19.263379: step 21850, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 56h:13m:37s remains)
INFO - root - 2017-12-15 14:19:25.859473: step 21860, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 55h:51m:15s remains)
INFO - root - 2017-12-15 14:19:32.523522: step 21870, loss = 0.13, batch loss = 0.09 (11.3 examples/sec; 0.710 sec/batch; 61h:16m:43s remains)
INFO - root - 2017-12-15 14:19:39.016046: step 21880, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 56h:37m:35s remains)
INFO - root - 2017-12-15 14:19:45.566850: step 21890, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.653 sec/batch; 56h:22m:35s remains)
INFO - root - 2017-12-15 14:19:52.253449: step 21900, loss = 0.21, batch loss = 0.17 (12.3 examples/sec; 0.649 sec/batch; 55h:59m:22s remains)
2017-12-15 14:19:52.769325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6807203 -3.4945896 -2.7472489 -3.3905473 -4.7022133 -4.853754 -4.9744358 -4.7965617 -3.9988613 -3.3610766 -4.1972475 -5.0131783 -6.0291638 -6.4189224 -4.7066545][-6.0439811 -6.0860724 -5.988637 -5.1513233 -4.8972931 -5.8984451 -6.0622387 -5.0105376 -4.6992269 -4.151638 -3.0755274 -5.1773753 -6.462358 -5.9451742 -4.8836317][-5.1784143 -5.2012873 -5.9368944 -6.428761 -6.5724096 -5.9346342 -5.2489948 -5.5144281 -4.8818746 -3.7778678 -4.1124759 -4.8175106 -5.56315 -6.1265574 -4.7565322][-4.7113881 -3.9734807 -4.0864248 -4.82326 -5.5745325 -5.7687974 -5.7346358 -5.4830904 -4.8900743 -4.2186174 -3.1413016 -4.5675259 -5.0922246 -4.1229458 -2.7249148][-7.0209608 -6.2134795 -4.9052763 -3.7992339 -3.6137304 -2.8515658 -2.2489889 -3.1218166 -2.832195 -2.8103516 -3.5897596 -3.9252462 -4.3425627 -4.8036051 -2.8391342][-6.7584629 -5.5365562 -4.1630206 -2.356796 -0.91110992 0.80028439 2.1356554 1.802206 0.69624043 -1.7707353 -2.4432518 -3.278239 -3.3957396 -2.4850516 -1.1035409][-7.6787186 -6.8752389 -4.9289961 -2.2002783 -0.40730858 2.3511958 4.6969628 5.3462825 5.3150792 2.2588477 -0.40430164 -2.0059118 -3.1298072 -2.4960921 -0.64409971][-8.4083843 -7.0670366 -4.8927164 -1.9457014 0.63986444 2.7472167 3.6121736 4.4667554 4.3419738 2.6538959 1.593399 -1.8645587 -4.36126 -3.7291098 -2.1834686][-6.139246 -5.571229 -4.0378718 -2.3306479 -0.5544467 2.1999092 3.0566678 3.3716788 3.8091521 2.1679482 1.2267971 -1.5830169 -5.0764713 -6.108315 -4.7509184][-5.8504615 -5.1521306 -3.9625964 -1.2456851 0.16047573 1.0409617 1.558063 1.6202426 0.45416975 -0.74609232 -1.1920409 -3.8199711 -5.5019169 -6.818923 -6.9737678][-6.4847994 -6.1255965 -5.0865588 -4.1319141 -2.8044355 -2.0498133 -2.066361 -1.8912063 -1.701571 -2.5362 -3.5438261 -6.0058441 -8.1683006 -8.687788 -7.7139421][-8.2042561 -7.7510343 -7.2025824 -6.6557441 -7.0181751 -7.0073986 -6.9187212 -7.1416216 -7.4660573 -6.790453 -6.3897724 -7.7741804 -8.3722105 -9.8885612 -9.3807125][-10.831553 -9.9467545 -9.0936975 -9.820118 -10.392525 -10.145504 -10.355852 -10.352048 -10.021019 -10.011723 -9.85844 -9.7285633 -9.6172209 -9.7687054 -8.5968809][-10.109532 -9.7216835 -9.1389332 -8.8855391 -9.42205 -9.9087648 -10.213254 -9.82037 -9.8424969 -9.4343042 -8.8270292 -8.5977459 -8.0840645 -8.1613007 -7.9439688][-8.6842737 -8.377615 -7.3637938 -6.8467994 -6.5639505 -6.6946917 -6.9220071 -7.490602 -7.3916807 -7.1895742 -7.5749831 -7.745677 -8.0260353 -7.2917976 -6.2685766]]...]
INFO - root - 2017-12-15 14:19:59.361825: step 21910, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 56h:55m:10s remains)
INFO - root - 2017-12-15 14:20:05.910286: step 21920, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 55h:31m:04s remains)
INFO - root - 2017-12-15 14:20:12.588560: step 21930, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 56h:44m:08s remains)
INFO - root - 2017-12-15 14:20:19.255202: step 21940, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.676 sec/batch; 58h:19m:26s remains)
INFO - root - 2017-12-15 14:20:25.805701: step 21950, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 55h:49m:11s remains)
INFO - root - 2017-12-15 14:20:32.395674: step 21960, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.688 sec/batch; 59h:21m:53s remains)
INFO - root - 2017-12-15 14:20:38.930005: step 21970, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.675 sec/batch; 58h:15m:56s remains)
INFO - root - 2017-12-15 14:20:45.487390: step 21980, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 56h:02m:49s remains)
INFO - root - 2017-12-15 14:20:52.025550: step 21990, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 55h:59m:50s remains)
INFO - root - 2017-12-15 14:20:58.581973: step 22000, loss = 0.26, batch loss = 0.21 (12.5 examples/sec; 0.641 sec/batch; 55h:19m:07s remains)
2017-12-15 14:20:59.106276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7879987 -6.1261725 -5.5902524 -4.803966 -4.7152357 -4.7934136 -5.0463328 -4.5803509 -4.5524273 -4.9343796 -4.6939936 -7.4823294 -10.408086 -9.434741 -9.2971992][-4.3932638 -3.8155732 -3.3066134 -2.3842604 -3.2781796 -4.0470867 -4.1828442 -4.0808039 -3.765048 -3.3445995 -3.8196259 -6.0807056 -8.1032867 -9.3260355 -10.047832][-3.2578573 -3.2280588 -2.8019617 -2.0200787 -2.3687105 -2.2749286 -2.5128734 -2.3141184 -1.9382415 -2.2702179 -2.3839948 -5.1392736 -8.6841688 -8.6405659 -8.4991875][-3.0561497 -3.0983 -2.7257473 -1.8716922 -2.3710339 -2.2925541 -1.6776781 -1.8863008 -2.2002177 -2.3296673 -2.3404329 -4.9550529 -7.5683212 -7.7864542 -7.5721207][-2.8429847 -3.8938293 -3.4793942 -2.1071365 -1.6894727 -1.0577283 -0.87856007 -0.16157961 -0.025075436 -0.44217587 -0.76923895 -3.4598427 -6.4676847 -6.7239265 -6.9798641][-4.6641049 -3.6920154 -2.8788669 -1.8051507 -0.56100273 0.39825058 0.926754 1.0567889 1.0228677 0.38063669 -0.38674831 -2.7957313 -5.1497116 -5.3138618 -5.2123175][-7.1139507 -5.896946 -4.1510839 -2.1203928 -0.31504774 0.79213572 1.7111411 2.4060655 2.4640746 1.6361685 1.2001939 -1.5149226 -4.3646617 -4.3306623 -4.5856][-6.9450917 -6.2784986 -4.8234644 -2.06805 0.19611883 0.99789333 1.9548049 2.3923659 2.0639195 1.8011189 1.0666695 -1.5993881 -4.4035335 -4.6536541 -4.9769583][-6.1150031 -5.4962287 -4.1266804 -0.89785862 0.59152603 1.2382474 1.6462202 1.8044748 1.8101811 1.8129411 1.3956723 -1.8158073 -4.9747066 -5.0820284 -5.7743955][-7.29859 -6.4099488 -4.3491206 -1.4696155 0.15216637 0.78123522 1.3995771 1.1083727 0.24561024 0.38371563 0.30013227 -2.3551705 -4.8355656 -5.3666353 -6.00181][-10.079943 -9.1735077 -6.7147503 -2.794152 -1.4630556 -1.0422974 -0.95639467 -1.6563196 -2.1546619 -2.1385822 -2.3657596 -4.9657917 -7.0664349 -7.1463866 -6.896976][-10.542732 -9.7178936 -7.0433106 -4.3013225 -2.7758725 -2.505157 -3.1927445 -4.4038773 -4.773037 -4.8361192 -5.0703192 -6.0057244 -7.1816554 -6.6616797 -5.9519238][-10.145004 -8.8212872 -5.4252748 -3.8923135 -3.626986 -3.5119014 -3.8768845 -4.1068077 -4.3464317 -5.0610533 -4.9821243 -6.0162725 -6.6015859 -5.6463509 -4.7743406][-9.55068 -8.1950493 -6.8831477 -5.4257183 -4.2224445 -3.9568806 -4.4498005 -4.6888618 -4.8352432 -4.449657 -4.1357908 -4.5638509 -4.4161563 -4.0825138 -4.3876667][-6.1567392 -6.2322865 -6.0187163 -5.2219739 -4.4969511 -3.4700696 -2.8835232 -2.9513333 -3.0913446 -2.6633968 -3.0684226 -4.0050573 -3.9930773 -4.7598066 -5.4621086]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 14:21:05.592165: step 22010, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 55h:03m:51s remains)
INFO - root - 2017-12-15 14:21:12.223817: step 22020, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 55h:47m:55s remains)
INFO - root - 2017-12-15 14:21:18.797283: step 22030, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 55h:29m:56s remains)
INFO - root - 2017-12-15 14:21:25.239578: step 22040, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.634 sec/batch; 54h:38m:31s remains)
INFO - root - 2017-12-15 14:21:31.689838: step 22050, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 57h:05m:38s remains)
INFO - root - 2017-12-15 14:21:38.202245: step 22060, loss = 0.21, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 57h:24m:06s remains)
INFO - root - 2017-12-15 14:21:44.858163: step 22070, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 56h:07m:19s remains)
INFO - root - 2017-12-15 14:21:51.473451: step 22080, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.691 sec/batch; 59h:36m:40s remains)
INFO - root - 2017-12-15 14:21:58.002516: step 22090, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.629 sec/batch; 54h:15m:25s remains)
INFO - root - 2017-12-15 14:22:04.581928: step 22100, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 57h:45m:05s remains)
2017-12-15 14:22:05.142874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9891112 -4.3130674 -4.3577824 -2.994159 -2.3176086 -2.4435842 -1.1606164 0.61353111 1.528811 -0.40787458 -2.7172933 -6.5887866 -9.6145344 -13.882978 -14.45752][-3.2557845 -3.6011782 -4.0612326 -4.0839281 -3.9215093 -2.890928 -0.83253765 -0.048470974 0.97137308 0.31169987 -1.700202 -6.5526371 -9.9658 -14.628433 -16.850765][-3.2172844 -3.884841 -3.9993024 -3.533258 -4.1514153 -3.6864057 -1.7003751 0.33835268 -0.026775837 -0.92136192 -1.8704174 -6.901185 -10.466179 -14.183472 -15.935253][-2.2844892 -2.705163 -2.7219875 -1.7779932 -1.8591852 -1.1984491 -1.147615 -0.046123505 0.89693928 0.047307014 -2.077826 -6.4294238 -9.7958832 -14.649958 -15.495955][-1.6436343 -1.4689617 -1.4778194 -1.8541181 -1.5898709 -0.26229811 0.10033321 0.48778534 0.37920856 0.57776785 -1.7615459 -7.1085114 -10.219223 -14.42029 -16.289898][-1.7816644 -3.0748522 -2.0014832 -1.0206466 -0.035164356 0.39035892 1.6391096 2.2301116 1.4020538 0.20050526 -1.6134534 -6.4505486 -11.042053 -14.516539 -15.361486][-2.7745171 -3.8408682 -3.8601735 -1.7476008 0.27390766 1.9781384 3.3889451 3.6910272 4.1787057 1.7688785 -1.005106 -5.49748 -8.8021641 -13.226961 -14.756001][-4.4191542 -4.4592228 -4.0501084 -2.2710462 -0.2265358 2.4790502 4.4661593 4.8625131 4.9182954 3.6408019 0.82742119 -4.373498 -7.6337872 -12.589106 -14.147139][-4.7586823 -6.5139937 -5.487772 -2.8051672 -1.4692197 0.83337545 3.5159745 4.7702479 4.4202743 2.8843274 0.41097784 -4.2705 -8.1160908 -12.097853 -13.507845][-4.167325 -5.5041356 -6.0502753 -4.5544229 -2.703907 -1.0747733 0.5750227 2.279295 2.6295114 0.83671713 -0.98123026 -4.7347274 -8.024559 -11.829399 -13.196171][-5.9645243 -6.8640819 -7.356462 -6.6870446 -6.126339 -4.8608332 -3.3065598 -2.6545725 -2.0185237 -2.8856289 -4.2229662 -8.600071 -9.6981506 -12.263897 -11.926335][-7.4428639 -8.0061531 -7.9968605 -7.4031305 -7.4155946 -7.46378 -6.8113661 -6.6606193 -6.28153 -6.2918124 -7.6536007 -9.7604275 -10.779917 -12.095684 -12.624956][-8.7590351 -9.0607548 -7.930974 -7.9536772 -8.3660278 -8.4373865 -8.3484449 -8.3385811 -7.6467423 -7.9543481 -8.323864 -9.5373039 -11.255529 -11.253817 -11.418885][-8.4172935 -8.3719578 -7.7939367 -7.0617876 -7.1384325 -8.24393 -8.8967953 -8.5838909 -7.9333057 -8.1397572 -7.8637104 -7.6280446 -7.8377337 -9.8988419 -10.414124][-9.3531055 -8.7057219 -8.4189329 -7.9660625 -8.0450945 -8.0710278 -8.8315859 -9.3157215 -8.7906713 -7.9067745 -7.9710245 -8.774313 -9.8169041 -10.992741 -11.319189]]...]
INFO - root - 2017-12-15 14:22:11.649484: step 22110, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 55h:14m:15s remains)
INFO - root - 2017-12-15 14:22:18.270670: step 22120, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 57h:58m:08s remains)
INFO - root - 2017-12-15 14:22:24.890018: step 22130, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 57h:59m:01s remains)
INFO - root - 2017-12-15 14:22:31.453096: step 22140, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 56h:44m:31s remains)
INFO - root - 2017-12-15 14:22:37.982976: step 22150, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 56h:11m:05s remains)
INFO - root - 2017-12-15 14:22:44.617864: step 22160, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 56h:07m:16s remains)
INFO - root - 2017-12-15 14:22:51.183688: step 22170, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 55h:15m:43s remains)
INFO - root - 2017-12-15 14:22:57.691579: step 22180, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 58h:39m:31s remains)
INFO - root - 2017-12-15 14:23:04.215070: step 22190, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 55h:29m:05s remains)
INFO - root - 2017-12-15 14:23:10.752581: step 22200, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 55h:29m:04s remains)
2017-12-15 14:23:11.258443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3846207 -2.4046347 -2.4290614 -2.7645433 -4.0892353 -5.0148029 -6.3191533 -6.7163367 -6.1758008 -5.1089053 -3.6794648 -5.0776019 -7.0801506 -7.3197808 -6.8115916][-4.26942 -3.3180203 -2.8465242 -2.3593767 -3.1648846 -4.4547353 -5.1655636 -5.3723822 -4.3046732 -2.503041 -1.2605696 -2.8298473 -4.6439853 -4.7952905 -5.2172852][-2.9427631 -3.9237485 -4.4507332 -2.9436145 -2.7141802 -3.6595559 -4.2906151 -4.8619375 -3.5343409 -1.8745828 -0.85040903 -1.9223068 -2.8818121 -4.353487 -5.305078][-4.3098788 -3.8990302 -3.4965112 -2.7945905 -2.7624309 -2.3838847 -1.946799 -1.8562052 -1.0036941 -0.13284922 0.049287319 -2.2149861 -3.77667 -4.3827829 -4.9633307][-4.0942578 -5.1887584 -5.7074947 -4.0107346 -3.8248305 -2.9115314 -1.9453328 -0.49664497 0.3850131 0.91478825 0.12919474 -3.4697649 -4.7815089 -5.2590795 -6.3963547][-5.1807771 -5.5789738 -4.6410561 -2.4578509 -1.8044634 -1.0868073 0.40458155 1.0625997 1.0522141 1.1575027 0.13752222 -2.6222522 -4.5634375 -5.9257689 -5.4708581][-4.3987823 -4.7108064 -4.92054 -3.356113 -1.2556906 0.70275784 2.0209026 2.7400956 2.8050504 1.4736195 0.18596411 -3.3062708 -5.3908381 -5.8812752 -6.6789441][-5.2628813 -4.7495642 -4.3326154 -2.0093975 0.07094574 1.9375815 3.5978646 3.7980371 3.2002883 1.8623385 -0.11848688 -3.6668193 -5.8078413 -6.6063986 -7.114995][-3.7161086 -4.3833265 -3.70856 -2.2781186 -0.73818254 1.0444651 1.9142914 2.3172317 2.0181446 0.85877562 -0.21460438 -3.7714887 -6.5751343 -7.1438866 -7.4852619][-5.0733075 -5.1410704 -3.9014077 -2.1475003 -0.68709421 -0.10755491 0.2754612 0.54325914 0.1067338 -0.66849184 -1.275228 -4.5113869 -6.6690507 -7.7720585 -8.6925611][-6.3803239 -7.02433 -6.5115275 -4.3402934 -3.53737 -2.4805026 -2.2098567 -2.5199878 -2.8892136 -2.968013 -3.4126933 -6.0664768 -8.3582039 -9.5036039 -9.8215084][-9.4318714 -8.8350058 -7.5703793 -5.3933525 -4.3398352 -3.8682842 -4.2544866 -4.8138976 -5.4846921 -6.062994 -6.8786826 -8.3938808 -9.4950848 -9.9268913 -9.616497][-9.4512253 -7.1997766 -5.3423033 -4.5766015 -5.2141209 -5.3169365 -5.7844315 -6.2238326 -6.4023943 -6.7848606 -7.16179 -7.0540118 -8.0467653 -8.150692 -6.9981389][-7.2245092 -5.8192639 -4.1706967 -3.0455728 -3.7064505 -4.4257479 -5.417778 -5.58032 -5.4122877 -5.4338431 -5.4016976 -4.996933 -4.5746803 -3.5120549 -4.0645485][-5.2925639 -5.1287832 -3.7072279 -2.9510117 -3.4876747 -4.0410657 -4.4099913 -4.9622602 -4.7931619 -4.4338713 -4.2247524 -4.3966436 -4.2612047 -5.3593559 -5.4297671]]...]
INFO - root - 2017-12-15 14:23:17.778915: step 22210, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 57h:30m:05s remains)
INFO - root - 2017-12-15 14:23:24.354533: step 22220, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 55h:20m:01s remains)
INFO - root - 2017-12-15 14:23:30.842438: step 22230, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 56h:08m:21s remains)
INFO - root - 2017-12-15 14:23:37.390412: step 22240, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.648 sec/batch; 55h:48m:20s remains)
INFO - root - 2017-12-15 14:23:43.998098: step 22250, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 56h:41m:17s remains)
INFO - root - 2017-12-15 14:23:50.515715: step 22260, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 57h:03m:06s remains)
INFO - root - 2017-12-15 14:23:57.095226: step 22270, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.683 sec/batch; 58h:49m:58s remains)
INFO - root - 2017-12-15 14:24:03.803451: step 22280, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 56h:17m:20s remains)
INFO - root - 2017-12-15 14:24:10.388481: step 22290, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 54h:48m:58s remains)
INFO - root - 2017-12-15 14:24:17.036260: step 22300, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 55h:37m:04s remains)
2017-12-15 14:24:17.531095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.133883 -9.2986622 -8.1366644 -6.5921707 -6.6828332 -6.8321695 -6.2837439 -4.3763456 -2.5745964 -1.304749 -0.34110498 -3.4831493 -6.1581306 -7.5919108 -7.5345631][-8.6790371 -7.3808479 -7.8085532 -6.1473117 -6.5787258 -6.9510345 -7.5228686 -5.6585684 -3.8232522 -1.9096789 -0.93561316 -3.7695489 -5.9668536 -7.1850619 -7.3083534][-5.6071973 -6.23519 -6.5208597 -5.6337938 -6.5619283 -7.2997541 -7.2900686 -5.78405 -4.0236249 -3.2764208 -3.4596279 -6.545413 -8.8120317 -9.0496235 -8.3687992][-6.1743064 -5.9484773 -6.1453228 -6.5497718 -7.4522185 -6.9104538 -6.5468488 -6.0767803 -5.217864 -3.9505258 -3.635884 -6.3306355 -9.1075172 -10.443494 -10.326286][-6.5059061 -7.0639467 -7.9418077 -7.5035543 -6.5618052 -4.7669287 -4.331893 -5.1935558 -4.0180092 -3.1251173 -2.9404271 -5.6698833 -8.5817566 -10.495947 -10.088339][-7.8658571 -7.4556532 -6.2863703 -4.64816 -4.05064 -2.0867889 -0.12412882 -0.060634613 -0.58032417 -1.7152691 -2.3366482 -4.5125971 -7.3293433 -9.8424263 -10.020679][-9.0612736 -7.5463467 -5.983705 -3.368134 -1.020781 1.0111799 3.0125785 4.0942845 3.76681 0.9609642 -0.815609 -4.2815666 -7.4146576 -8.9177017 -8.7530193][-6.8498421 -5.8091459 -4.8335667 -1.2873282 0.74721527 2.7294602 4.6359468 6.1073728 5.5205112 3.5585561 0.28905678 -4.9032598 -8.7264261 -10.238493 -8.10123][-4.479166 -3.3160408 -2.1352162 -0.059669971 0.91152239 2.3365483 3.3922629 3.6198163 2.6206746 2.1477895 1.0480309 -4.3625374 -9.13685 -10.309219 -7.8622746][-1.855511 -1.7101946 -1.6914954 0.28082371 1.1814613 1.1609807 1.1512885 1.2939563 0.76310778 0.64782953 0.21004581 -3.4022307 -6.8597465 -9.12448 -7.4908381][-5.717442 -4.5385962 -3.1184793 -2.4425056 -2.4768333 -3.0612774 -2.4674649 -2.2924607 -2.7597082 -2.4633615 -1.653409 -5.1990471 -7.262795 -7.6743631 -6.4119539][-11.168773 -10.554914 -9.3312626 -7.7598529 -7.4241557 -7.3427687 -7.2399988 -7.1756663 -6.5600691 -6.59526 -5.8017192 -7.4133687 -7.5805478 -6.4823618 -5.0582142][-12.559269 -12.837872 -11.053974 -9.8489485 -9.5600424 -8.5213709 -8.5483284 -9.3619022 -8.9609146 -7.7081861 -7.616704 -8.8663673 -8.3547583 -5.1743622 -3.3486645][-11.397847 -10.375866 -9.761096 -8.4820042 -8.1841249 -7.0659795 -7.4360485 -8.4099083 -9.0406609 -8.5865145 -8.238369 -8.8405247 -8.5891228 -6.4753838 -4.4188261][-8.7482157 -8.9095755 -6.9951286 -6.1058669 -4.7964664 -3.8643165 -4.4493055 -5.5900702 -7.1661987 -7.7095852 -8.7098055 -10.39024 -11.367704 -9.1736927 -7.653161]]...]
INFO - root - 2017-12-15 14:24:24.063890: step 22310, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 56h:54m:38s remains)
INFO - root - 2017-12-15 14:24:30.611871: step 22320, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 54h:38m:19s remains)
INFO - root - 2017-12-15 14:24:37.177826: step 22330, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 56h:30m:49s remains)
INFO - root - 2017-12-15 14:24:43.846341: step 22340, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 57h:15m:23s remains)
INFO - root - 2017-12-15 14:24:50.413489: step 22350, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 55h:44m:35s remains)
INFO - root - 2017-12-15 14:24:57.036579: step 22360, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.630 sec/batch; 54h:14m:30s remains)
INFO - root - 2017-12-15 14:25:03.631180: step 22370, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 58h:15m:16s remains)
INFO - root - 2017-12-15 14:25:10.234860: step 22380, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 56h:48m:33s remains)
INFO - root - 2017-12-15 14:25:16.815931: step 22390, loss = 0.18, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 54h:37m:46s remains)
INFO - root - 2017-12-15 14:25:23.322469: step 22400, loss = 0.16, batch loss = 0.11 (12.7 examples/sec; 0.632 sec/batch; 54h:28m:02s remains)
2017-12-15 14:25:23.856274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.085288 -8.1557693 -7.8311043 -7.3999176 -8.0765524 -8.106822 -8.02108 -7.2326407 -6.2804441 -5.9606452 -5.5801086 -6.6485696 -7.6896553 -8.6156826 -8.9596157][-7.4456882 -8.5174675 -8.2937155 -6.9699392 -7.5510297 -7.5483966 -7.7483759 -7.86544 -7.38472 -6.249836 -5.3458533 -7.6796889 -8.5348358 -10.308717 -11.228501][-6.4069462 -7.8986492 -8.1138382 -7.4108167 -7.1058321 -7.1297026 -7.3528295 -6.9088435 -6.7614603 -6.9008489 -6.883646 -8.7920246 -10.195539 -11.126606 -10.950959][-8.0492086 -8.9214783 -8.9931135 -7.5790658 -6.615335 -5.3176713 -4.788836 -5.1820941 -5.8644624 -6.1723518 -6.9171104 -9.6898327 -10.834612 -12.248992 -11.38087][-9.1304321 -11.689861 -12.62033 -9.0091867 -5.9519639 -2.2349515 -0.26088428 -1.481451 -3.4822421 -5.297111 -6.5368385 -8.2481213 -8.5628414 -9.4676266 -10.327662][-11.336636 -12.599337 -11.048435 -7.229661 -3.7317276 1.1243606 4.3921351 4.0780892 2.7310548 -0.80628586 -3.7972732 -5.75664 -6.434361 -7.472435 -7.4624734][-12.227102 -12.819489 -10.440714 -5.9821167 -2.0302858 3.4549241 7.7215991 7.8552165 6.9985957 2.6030269 -1.2473235 -4.5408382 -7.17046 -8.2541866 -8.2911415][-13.076799 -12.498125 -9.6393337 -5.0889673 -0.99978781 4.9259887 8.6134682 8.69389 8.9030228 4.6497693 0.48664665 -4.0768929 -8.4415274 -10.265018 -10.997063][-10.388792 -10.457541 -8.9705124 -4.7018433 -0.78514528 3.4118161 5.1291108 5.9337192 6.3500619 2.3133521 -0.47474003 -4.5280848 -8.9929876 -12.136918 -13.626201][-9.8242588 -8.52016 -7.651607 -4.22209 -1.1734524 0.57767248 2.6281 3.350513 2.0577583 -0.11566591 -1.1902981 -5.5579405 -9.5982666 -13.348074 -16.040089][-12.168743 -12.146826 -11.472723 -7.9702587 -5.8896441 -4.0413666 -2.2862616 -2.8629017 -3.7753911 -4.630693 -6.1610694 -9.8646116 -11.80821 -14.131546 -15.248011][-16.888075 -15.40106 -13.664438 -12.516471 -11.904617 -9.3076286 -8.1518908 -9.6316833 -9.361558 -8.8299437 -9.8266048 -12.061886 -12.900505 -13.962084 -14.153858][-16.94725 -15.528193 -13.659277 -12.603303 -12.128668 -11.516649 -11.11273 -10.684709 -10.335045 -10.34856 -10.358541 -10.962433 -10.550563 -10.151173 -9.7267523][-14.760284 -13.32926 -11.634181 -9.54163 -8.5041332 -8.2064495 -8.3811646 -8.25209 -8.5406876 -8.4476156 -8.87523 -9.0992041 -8.0768242 -7.6812816 -7.3907428][-11.00363 -10.094339 -7.7109323 -5.477438 -4.8683043 -3.5832713 -2.9475074 -4.3812842 -5.2201366 -5.3190255 -5.6677155 -7.3211136 -7.6495352 -7.2814021 -8.072834]]...]
INFO - root - 2017-12-15 14:25:30.354813: step 22410, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 56h:19m:32s remains)
INFO - root - 2017-12-15 14:25:36.889982: step 22420, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.634 sec/batch; 54h:34m:26s remains)
INFO - root - 2017-12-15 14:25:43.413430: step 22430, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 57h:00m:14s remains)
INFO - root - 2017-12-15 14:25:50.003182: step 22440, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 55h:56m:13s remains)
INFO - root - 2017-12-15 14:25:56.511837: step 22450, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 57h:09m:59s remains)
INFO - root - 2017-12-15 14:26:03.131219: step 22460, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 56h:47m:23s remains)
INFO - root - 2017-12-15 14:26:09.823917: step 22470, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.684 sec/batch; 58h:56m:13s remains)
INFO - root - 2017-12-15 14:26:16.331543: step 22480, loss = 0.21, batch loss = 0.17 (12.6 examples/sec; 0.637 sec/batch; 54h:49m:42s remains)
INFO - root - 2017-12-15 14:26:22.869380: step 22490, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 56h:33m:25s remains)
INFO - root - 2017-12-15 14:26:29.494810: step 22500, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 56h:10m:48s remains)
2017-12-15 14:26:29.952801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.365355 -10.043705 -8.6080608 -8.4127159 -8.6737671 -9.8172722 -11.118466 -10.860325 -9.79344 -8.3635855 -6.2640305 -4.7909102 -3.961483 -3.1802449 -2.7715716][-9.7003336 -8.2635918 -5.5477052 -5.72897 -6.5952692 -8.5129013 -10.144545 -9.8450756 -9.2832766 -7.2506127 -5.035984 -3.4042356 -2.2210186 -1.8870986 -2.5197742][-7.0812449 -5.9157386 -5.9634156 -5.6961613 -5.381391 -7.0865397 -8.9073238 -8.7998466 -7.5104146 -5.5957775 -3.9631953 -1.7472136 -1.0444984 -1.2263575 -2.2467947][-7.3848505 -6.2608404 -5.4385214 -4.5116515 -5.4505496 -6.3326225 -6.8202782 -7.4272194 -6.68246 -5.0140176 -3.7529998 -2.5802994 -2.8134711 -2.7860024 -3.0028849][-9.869545 -9.8798542 -8.8267956 -6.2142358 -5.020123 -3.1963611 -2.2005954 -4.4353771 -6.01034 -5.0301371 -4.6247873 -4.3698368 -4.5367527 -4.5401492 -5.1916637][-11.891508 -11.573612 -10.325247 -7.5309591 -4.1929922 -0.13391829 2.6951165 1.137877 -0.3882556 -2.5952892 -5.72729 -4.4523787 -3.8383245 -4.6815844 -5.0762367][-12.903763 -13.021818 -10.269278 -6.4648585 -3.034816 1.2429371 4.8326421 4.3625522 3.57768 -0.82146168 -5.6718764 -5.4713683 -6.0019407 -5.8487411 -5.57566][-14.484024 -12.255393 -9.0953045 -4.8997183 -0.40281677 2.8918519 6.0814691 5.658649 4.2849889 0.26346207 -3.9350331 -5.402091 -7.6003437 -7.0845423 -6.3000355][-10.780746 -10.253043 -8.9863682 -5.8037877 -1.3265429 2.5724292 5.3834949 4.6139588 2.6319079 -1.4430408 -4.861968 -6.2620034 -8.7960358 -8.6711712 -7.7981005][-10.007257 -9.1634989 -7.5077019 -4.4391 -2.2417138 -0.069045544 3.2539887 2.531507 -0.18113947 -4.0683312 -7.2494078 -8.0930948 -9.2943716 -9.6020813 -10.03966][-13.481094 -11.605335 -9.6394176 -7.2193279 -5.6369352 -4.0967274 -2.2274241 -2.5550985 -4.319551 -7.1477356 -9.2666006 -10.765316 -10.928205 -10.677355 -9.41801][-16.413973 -14.000841 -10.867798 -8.2534952 -6.7913942 -6.7455807 -6.6325145 -6.4723859 -7.2585855 -8.775526 -10.498157 -11.745989 -11.733274 -11.144677 -9.5055733][-14.897896 -12.010426 -8.9157467 -7.611712 -6.9053617 -6.7939205 -6.3103523 -6.8945003 -9.11434 -9.8657589 -10.323302 -11.212862 -11.436934 -10.344469 -8.7960844][-12.608198 -10.488182 -7.6074481 -6.0925779 -5.557693 -5.9285154 -6.500782 -7.1654081 -8.0910625 -8.5949173 -9.6299524 -9.4332533 -9.3972645 -8.0655556 -6.4751682][-9.5031185 -7.4208155 -3.9489846 -3.1776862 -3.1683795 -4.0569506 -5.4556689 -5.7891688 -6.1903949 -6.6917624 -7.0549784 -6.7086549 -7.01563 -7.042345 -7.430584]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 14:26:37.538910: step 22510, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 57h:20m:33s remains)
INFO - root - 2017-12-15 14:26:44.132690: step 22520, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 58h:05m:51s remains)
INFO - root - 2017-12-15 14:26:50.650005: step 22530, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 57h:15m:26s remains)
INFO - root - 2017-12-15 14:26:57.138505: step 22540, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.633 sec/batch; 54h:30m:08s remains)
INFO - root - 2017-12-15 14:27:03.708408: step 22550, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.682 sec/batch; 58h:42m:36s remains)
INFO - root - 2017-12-15 14:27:10.203159: step 22560, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 55h:31m:50s remains)
INFO - root - 2017-12-15 14:27:16.790611: step 22570, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 55h:26m:46s remains)
INFO - root - 2017-12-15 14:27:23.341307: step 22580, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 57h:06m:47s remains)
INFO - root - 2017-12-15 14:27:29.963902: step 22590, loss = 0.14, batch loss = 0.10 (11.4 examples/sec; 0.699 sec/batch; 60h:09m:07s remains)
INFO - root - 2017-12-15 14:27:36.553741: step 22600, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 55h:09m:48s remains)
2017-12-15 14:27:37.030255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3479004 -7.26475 -5.9060626 -4.7340565 -4.8200111 -4.6680765 -4.3523059 -3.9692872 -3.314435 -2.5331635 -1.6070766 -3.6802719 -5.4905858 -7.1046553 -6.9458904][-5.4130836 -5.9165235 -5.5027652 -4.9439192 -5.6502395 -6.0695043 -6.186924 -5.9771791 -5.501565 -4.6924415 -4.0768681 -5.51517 -6.8663893 -8.4537191 -7.67514][-4.912149 -4.9301019 -4.1468344 -4.4285021 -5.3078852 -5.7899165 -6.0025826 -5.4274821 -4.7906041 -4.5521708 -4.6554108 -6.4508395 -8.0553293 -9.251483 -8.4638805][-4.4384542 -3.7149208 -4.4689093 -5.4749279 -5.9761124 -5.7858934 -5.0960836 -4.6894774 -4.638566 -4.2412958 -4.5089655 -7.0352058 -8.917758 -10.024245 -9.3045216][-3.3139963 -4.0514913 -4.6287212 -4.44161 -4.3494406 -3.1707802 -2.0703194 -2.4785666 -3.0439661 -3.7572241 -4.8782635 -7.5245667 -9.657937 -10.948608 -10.310268][-4.5794363 -5.1433544 -4.8421941 -3.3541536 -2.058305 -0.00540781 2.30133 3.0087066 2.1118531 -0.96928167 -3.5290246 -6.4173746 -8.92426 -11.086855 -10.73679][-6.0331378 -5.7106471 -4.3855109 -2.148422 -0.9400425 1.5404243 4.5441022 5.0229411 4.390161 1.6620526 -1.4008613 -5.7155066 -8.8545637 -10.207064 -9.8535042][-5.4485946 -4.4832907 -2.7451901 -0.71828175 0.300354 3.327312 5.658216 5.4951406 5.5723062 2.6582904 -0.86333179 -4.6765375 -7.48582 -9.5821524 -9.576376][-4.4049482 -3.4680529 -1.8430524 0.28061247 1.5657825 3.0716777 3.7109761 4.499392 4.9284873 2.3214097 0.68027782 -2.5530529 -5.8782191 -7.6051512 -6.3361177][-3.9003329 -2.8702772 -1.5988736 0.44783878 2.3173509 2.1981091 1.8664761 2.750154 2.7087255 1.9239321 0.99823475 -1.8008935 -3.7946777 -4.8464184 -4.4505386][-8.4274406 -6.888989 -4.320857 -2.176862 -0.73523712 -0.88244104 -0.53721666 -1.2843595 -2.0074387 -1.8620667 -2.1085944 -3.8906312 -4.4370632 -5.4318 -4.1713929][-10.547606 -9.9587984 -8.3290176 -5.9836941 -4.4693041 -4.4740086 -4.3079391 -4.5529685 -4.7957811 -4.5661087 -4.140903 -4.95077 -4.805994 -4.7185068 -3.8825421][-11.130158 -9.9081392 -7.9627542 -7.0039363 -6.707552 -6.0383811 -5.9048505 -5.3608775 -5.3852129 -5.5030584 -5.2004161 -5.3000174 -3.9596837 -3.3329511 -2.8592405][-10.565756 -9.0295792 -8.9313822 -7.1376448 -5.757164 -5.7410579 -6.1530147 -5.9222622 -6.2081118 -6.4366422 -6.4836192 -5.9416351 -5.4527636 -4.5936494 -4.2655582][-7.4369316 -7.50074 -6.0989532 -3.9800215 -3.4306126 -3.6448858 -3.3508842 -4.1029615 -5.304534 -5.0000153 -5.24017 -6.3135066 -7.9491148 -7.358674 -7.3721142]]...]
INFO - root - 2017-12-15 14:27:43.594514: step 22610, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 56h:27m:42s remains)
INFO - root - 2017-12-15 14:27:50.220374: step 22620, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 55h:43m:58s remains)
INFO - root - 2017-12-15 14:27:56.808980: step 22630, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 56h:33m:15s remains)
INFO - root - 2017-12-15 14:28:03.372954: step 22640, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 56h:48m:44s remains)
INFO - root - 2017-12-15 14:28:09.950472: step 22650, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 55h:30m:39s remains)
INFO - root - 2017-12-15 14:28:16.507794: step 22660, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 56h:56m:17s remains)
INFO - root - 2017-12-15 14:28:23.082186: step 22670, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 57h:38m:54s remains)
INFO - root - 2017-12-15 14:28:29.626744: step 22680, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 55h:42m:27s remains)
INFO - root - 2017-12-15 14:28:36.187521: step 22690, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.681 sec/batch; 58h:35m:11s remains)
INFO - root - 2017-12-15 14:28:42.810771: step 22700, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.685 sec/batch; 58h:54m:39s remains)
2017-12-15 14:28:43.312729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2912021 -3.2878203 -2.1060925 -1.1414332 -1.4777398 -1.6777849 -2.1963487 -2.3961725 -3.3218815 -4.4761066 -5.7244873 -9.0809755 -11.063538 -11.677057 -10.731857][-3.837646 -3.4756124 -2.9651175 -1.8875751 -1.7156706 -1.2995601 -1.1683025 -1.7622874 -3.0622358 -3.913178 -4.6272235 -7.2921638 -8.9836788 -9.9833765 -10.03559][-3.0523646 -3.8757372 -3.5814402 -2.1745672 -1.4582019 -1.1839995 -0.65705156 -0.75180149 -1.2858734 -2.2051203 -3.0801871 -5.6479826 -7.1289129 -8.3466234 -8.4545612][-4.37804 -3.8751509 -2.7079611 -1.6483874 -1.6748819 -1.2307248 -0.65753603 -0.85623789 -1.2792101 -1.8988738 -1.8877556 -3.9385164 -5.0342665 -6.3555093 -6.41767][-3.9186697 -3.3991766 -2.8076537 -1.5591311 -0.79903793 0.5873208 0.91464186 0.60673714 0.22461319 -0.27987576 -0.52946281 -2.715915 -3.9914141 -5.818181 -6.1952367][-4.3554573 -3.5329092 -2.0832009 -0.327806 0.86140585 1.5515318 1.4307575 1.1439719 0.7136097 0.3333168 0.087364674 -1.6799231 -3.0258493 -5.0833316 -4.8944535][-5.5584822 -4.0440226 -1.8053081 0.012243271 1.1623545 1.9708753 2.1108556 1.9542708 1.6327105 1.1000142 0.71882248 -1.3243523 -2.7506263 -4.1068473 -4.1157932][-4.7331944 -3.5528431 -1.9857955 0.16593361 1.6170497 2.6938338 2.7741952 2.4047027 2.2623119 1.9467006 1.3583951 -0.84051323 -2.3939667 -4.0757093 -3.7443056][-5.1496468 -3.9427776 -1.9785476 0.35092497 1.3523517 2.2315359 2.3373094 2.0941448 2.4422374 2.3930774 1.9435205 -0.68494034 -2.9145615 -4.1790609 -4.0522504][-3.6424911 -3.0740657 -1.6888299 -0.064436913 0.74830818 1.7552857 1.9236603 1.7671256 1.5405045 1.1876106 1.0596871 -1.5001073 -3.670296 -5.6161079 -6.2351804][-6.2001915 -5.3760514 -3.7456002 -2.5023911 -2.5898001 -2.6819143 -2.5276051 -2.2892261 -2.2833116 -2.7301335 -2.9602742 -6.0958104 -7.6788616 -8.5455332 -7.2913537][-8.79711 -7.6859436 -6.7626858 -5.8591962 -5.5172482 -5.4803576 -5.8477736 -6.3922796 -6.4365544 -6.5132928 -6.7835121 -8.31969 -8.8199635 -10.130815 -9.8062038][-9.7990036 -8.9928627 -8.2366848 -7.8834124 -7.9885836 -7.5301127 -7.4659066 -7.6227264 -8.0087414 -8.4068747 -8.4897718 -8.9227486 -9.0690136 -9.3658619 -8.04237][-10.538892 -10.331919 -9.7211161 -8.2626314 -7.3089876 -7.0191221 -6.8040571 -6.5611324 -5.990417 -5.5455761 -5.7375093 -5.6518688 -5.948441 -5.7925434 -5.5776353][-9.6220608 -9.6356564 -8.97694 -7.31412 -6.0618482 -5.1620932 -4.3478403 -4.24524 -3.8449521 -3.7416816 -3.4758182 -4.0252957 -4.5531168 -4.5327554 -4.8557549]]...]
INFO - root - 2017-12-15 14:28:49.853990: step 22710, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 56h:59m:34s remains)
INFO - root - 2017-12-15 14:28:56.403361: step 22720, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 56h:39m:56s remains)
INFO - root - 2017-12-15 14:29:02.920602: step 22730, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 55h:16m:54s remains)
INFO - root - 2017-12-15 14:29:09.476033: step 22740, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 56h:22m:47s remains)
INFO - root - 2017-12-15 14:29:16.076538: step 22750, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 57h:48m:01s remains)
INFO - root - 2017-12-15 14:29:22.723829: step 22760, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 57h:19m:58s remains)
INFO - root - 2017-12-15 14:29:29.321555: step 22770, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 55h:10m:46s remains)
INFO - root - 2017-12-15 14:29:35.868288: step 22780, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.630 sec/batch; 54h:10m:59s remains)
INFO - root - 2017-12-15 14:29:42.481017: step 22790, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 56h:18m:24s remains)
INFO - root - 2017-12-15 14:29:49.064173: step 22800, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 57h:23m:33s remains)
2017-12-15 14:29:49.544271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1758914 -7.5961905 -8.4897032 -8.7286987 -10.018785 -10.648523 -10.43363 -10.829781 -10.600412 -9.8498306 -8.5522938 -10.39896 -10.325434 -11.566502 -11.299889][-7.5808821 -9.3219023 -10.102677 -10.831445 -12.07563 -12.701121 -12.870327 -12.733678 -11.210775 -10.079033 -9.0596657 -10.442428 -10.68877 -11.970186 -11.098344][-7.2832894 -9.97701 -12.018751 -13.020842 -13.670422 -13.708868 -13.288033 -12.754187 -11.244308 -10.169365 -9.1275158 -11.011467 -11.75211 -13.334896 -12.482147][-7.6474485 -9.3492851 -10.752724 -11.689911 -13.06187 -11.83559 -10.384418 -9.5947609 -7.8771019 -7.9909658 -7.9080057 -10.359694 -11.468826 -13.684546 -14.258348][-8.2179222 -10.155837 -12.048967 -10.834278 -10.114141 -6.9978948 -4.6277342 -5.0244155 -5.1677227 -4.6362009 -4.7310238 -7.4335418 -9.3917847 -13.168596 -13.935644][-9.4548111 -11.277647 -12.603484 -9.5084133 -6.170898 -1.1034675 3.3446403 3.2402577 1.5866656 -1.3188024 -3.1887596 -5.825336 -8.2914333 -12.262888 -13.443664][-7.9183249 -9.6343107 -11.064445 -7.6930437 -4.2589221 1.3330493 7.0731988 8.8314514 8.4327564 4.6337113 0.78806782 -4.7581615 -7.8931456 -11.480101 -13.059923][-5.6116352 -7.9260983 -8.6089077 -6.5422058 -3.2665365 4.6796288 10.897419 10.756599 9.7530174 4.8203111 0.32384014 -5.487071 -8.6444073 -12.064685 -12.394503][-3.5974131 -5.9418736 -6.984447 -6.1259885 -5.0981259 0.41008711 7.1956954 7.580358 6.6793685 1.6746068 -2.7512195 -8.8716488 -12.337883 -15.155699 -14.927227][-2.9139891 -4.6726327 -7.1027455 -6.0433187 -5.6432219 -3.3346584 -1.0704665 1.0813489 1.6868296 -3.2928407 -7.2068048 -13.340879 -16.000652 -18.856485 -19.136728][-6.0248389 -7.1726022 -9.1108561 -9.2367735 -9.3012133 -8.086606 -7.055768 -5.7415762 -5.2906518 -7.9832935 -10.186987 -15.986931 -17.444355 -20.059393 -19.626207][-9.1801805 -10.011132 -10.980861 -10.599398 -10.989418 -10.380142 -11.073652 -11.20755 -11.155846 -12.283298 -12.651461 -14.98345 -14.808189 -17.376791 -15.766115][-11.806416 -11.092856 -11.496191 -11.357672 -12.242091 -11.269745 -12.107996 -12.454611 -12.995798 -14.015162 -14.055426 -14.209585 -13.141369 -14.702488 -12.148178][-9.7672634 -9.08929 -9.5152264 -9.2566519 -8.6071014 -8.780015 -9.6609926 -10.162121 -10.492611 -10.505131 -10.786737 -10.552385 -9.8186264 -10.466976 -9.8977518][-7.85551 -8.6001034 -7.371911 -5.8348346 -5.5974073 -5.5720153 -5.4390211 -5.9536748 -6.8353305 -7.5815907 -8.076004 -8.31106 -8.28764 -8.4927959 -8.1973476]]...]
INFO - root - 2017-12-15 14:29:56.265987: step 22810, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 56h:46m:50s remains)
INFO - root - 2017-12-15 14:30:02.853694: step 22820, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 57h:44m:25s remains)
INFO - root - 2017-12-15 14:30:09.434623: step 22830, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 58h:17m:31s remains)
INFO - root - 2017-12-15 14:30:16.078550: step 22840, loss = 0.20, batch loss = 0.16 (11.5 examples/sec; 0.694 sec/batch; 59h:40m:46s remains)
INFO - root - 2017-12-15 14:30:22.659051: step 22850, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 55h:52m:02s remains)
INFO - root - 2017-12-15 14:30:29.192681: step 22860, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 54h:41m:06s remains)
INFO - root - 2017-12-15 14:30:35.782038: step 22870, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 56h:35m:05s remains)
INFO - root - 2017-12-15 14:30:42.346180: step 22880, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 56h:11m:30s remains)
INFO - root - 2017-12-15 14:30:48.981571: step 22890, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 55h:09m:03s remains)
INFO - root - 2017-12-15 14:30:55.562059: step 22900, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.634 sec/batch; 54h:33m:56s remains)
2017-12-15 14:30:56.050518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-11.534304 -12.529705 -12.572149 -12.962763 -13.631339 -13.183969 -12.388451 -10.375238 -8.4759722 -7.8476562 -8.9362173 -10.401989 -11.995014 -12.175371 -9.822053][-10.86797 -10.154779 -10.103148 -11.190153 -12.20825 -12.660926 -12.086103 -11.131111 -10.69229 -9.6442919 -9.6482182 -10.719961 -12.198531 -12.641846 -11.287103][-5.1395912 -6.2298117 -7.7032828 -9.3042336 -10.367828 -10.695219 -9.9470711 -8.9242306 -8.5863533 -7.7222557 -7.1550808 -7.8772039 -9.93286 -11.869703 -11.879766][-3.423595 -2.7526836 -3.4773567 -5.2444358 -6.5283923 -6.2636461 -5.519804 -5.7858753 -6.3697181 -6.2394495 -6.4530697 -7.0846596 -8.0277166 -9.566782 -9.76553][-3.9388509 -4.1553054 -4.7732258 -3.8121405 -3.2838695 -2.0557127 -0.56572533 -1.6865597 -3.277514 -3.5446267 -4.3512383 -5.8964911 -7.4799261 -9.33364 -8.6569719][-6.5347118 -6.7118073 -6.0617948 -4.2819266 -2.7249207 0.44320011 3.2754493 2.7973456 1.6223855 -0.50961018 -2.4722631 -3.9330254 -5.9139428 -7.4038815 -6.8969078][-8.5364609 -8.4099674 -7.219234 -4.27763 -1.6718688 1.8419161 4.598556 5.3675818 6.30407 4.1903472 1.1234303 -1.5223637 -4.4928436 -6.85085 -5.9002938][-7.6494646 -7.1669927 -5.8332472 -3.2252934 -0.33819675 2.7938838 5.1530347 5.5113759 5.7179894 3.6955371 1.3509779 -1.0128126 -3.4725137 -4.9061122 -3.8651738][-4.9166136 -3.7983243 -3.2897484 -1.4868603 -0.010018826 1.8744397 3.3834043 4.0891566 4.1530929 2.5484719 0.85916138 -1.8952792 -4.2217875 -5.5066667 -3.826889][-3.8052516 -2.67519 -0.97557354 -0.411541 -0.12144899 0.6828723 1.5633245 1.5317936 0.75923061 -0.62383366 -1.6326957 -3.14238 -4.4265289 -5.2690544 -4.0079165][-6.699204 -4.3699269 -3.4565279 -3.5941598 -3.5732684 -2.5920458 -1.0497332 -0.36489344 -1.4647808 -3.3756969 -4.0710168 -5.4360356 -7.1147304 -7.2871113 -5.0780325][-9.4254732 -7.6585169 -6.4433088 -6.4073119 -6.786653 -5.8935537 -4.6310797 -4.0205622 -4.7214012 -5.3539643 -5.0485697 -5.1897755 -5.5950947 -6.0926867 -5.4762039][-9.9141064 -9.085207 -8.2505035 -7.9806719 -7.5356936 -6.0463586 -4.6884341 -4.1929693 -4.5700417 -5.6047506 -5.6991358 -5.8885217 -5.9438491 -5.6414313 -4.4420238][-8.5217638 -7.8870363 -8.0649271 -7.02464 -6.7140069 -5.4647593 -4.7172093 -4.2015839 -4.5337076 -4.748878 -5.0067048 -4.7406979 -3.9171093 -3.9780087 -4.3937593][-5.9001255 -5.180943 -4.7030039 -4.9832253 -3.8343539 -2.9287045 -2.6325598 -3.0473289 -4.2217073 -5.0068073 -5.521997 -6.5734668 -7.2958126 -7.6783276 -6.79221]]...]
INFO - root - 2017-12-15 14:31:02.651969: step 22910, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 58h:36m:28s remains)
INFO - root - 2017-12-15 14:31:09.266296: step 22920, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 56h:47m:38s remains)
INFO - root - 2017-12-15 14:31:15.857931: step 22930, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 56h:27m:32s remains)
INFO - root - 2017-12-15 14:31:22.484923: step 22940, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.631 sec/batch; 54h:18m:01s remains)
INFO - root - 2017-12-15 14:31:29.109551: step 22950, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 58h:43m:08s remains)
INFO - root - 2017-12-15 14:31:35.655680: step 22960, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 56h:02m:20s remains)
INFO - root - 2017-12-15 14:31:42.264005: step 22970, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 55h:23m:13s remains)
INFO - root - 2017-12-15 14:31:48.824508: step 22980, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.685 sec/batch; 58h:54m:09s remains)
INFO - root - 2017-12-15 14:31:55.473832: step 22990, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 56h:16m:02s remains)
INFO - root - 2017-12-15 14:32:02.057246: step 23000, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 57h:28m:09s remains)
2017-12-15 14:32:02.587309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3855309 -7.3269415 -7.2447696 -6.4882946 -6.2748051 -5.8115168 -5.4089355 -5.1748919 -4.8739347 -4.9251585 -4.8929644 -6.6595421 -8.56567 -7.706625 -5.5113015][-4.9170265 -4.9058337 -5.2851858 -5.4490275 -5.2539062 -4.9829206 -4.3932886 -4.6143723 -4.6421208 -4.7938151 -5.0302258 -6.9021444 -8.5241241 -8.4750118 -7.7128286][-2.7197688 -3.4212546 -4.6705518 -4.7489419 -4.8197212 -4.3657646 -4.2464652 -4.1503267 -4.0479212 -4.2406654 -4.6121206 -6.7000971 -8.5926857 -9.2734222 -8.9363041][-3.5908847 -4.8833542 -4.892416 -5.260767 -5.1373253 -4.2778115 -3.9762783 -3.8722403 -3.8772202 -4.132617 -3.8655519 -6.30929 -8.5942783 -9.71233 -9.1819715][-5.3850522 -6.9666719 -7.7655454 -6.1959014 -4.3052597 -1.9695783 -0.45055389 -1.3651958 -2.7421379 -3.0125878 -3.25606 -5.2682037 -7.6312466 -9.6347456 -9.8040724][-6.4700184 -7.7892332 -7.0526037 -4.6468468 -2.1867137 1.0583043 4.0603652 3.4903817 1.1901712 -0.393332 -2.2404838 -4.3359442 -6.0513163 -7.9218464 -8.0868969][-7.9820986 -6.5729809 -5.0526004 -2.8486521 -0.7259059 3.741755 7.436379 6.63349 5.4921727 2.1173882 -1.4963045 -4.1749244 -7.1920829 -8.2704649 -7.7227831][-7.6830478 -6.8792505 -4.7826 -2.5140035 -0.22889709 4.0237832 7.1038079 7.5386596 7.0951953 3.4227452 -0.36754227 -4.7523265 -8.351016 -9.6023951 -8.6803255][-6.5481334 -5.6858764 -4.75764 -2.2164996 0.0046806335 3.5433908 5.5583234 6.37233 6.4429135 2.7046418 0.11982155 -4.5413723 -8.7851944 -10.172016 -10.678432][-7.824676 -5.4750743 -3.617866 -1.1390133 0.06817627 2.0996566 4.0863318 4.6572785 3.32021 0.7987709 -1.3639841 -5.9855084 -9.7060566 -12.01215 -12.478525][-9.4615221 -9.133317 -7.5984454 -4.9887209 -3.0649133 -2.7715437 -1.995846 -1.4932966 -2.3787889 -3.3265855 -4.5628662 -8.890974 -11.484531 -12.991709 -12.377987][-14.154203 -12.591625 -10.916191 -8.3933811 -7.5076666 -7.0087948 -6.6772738 -7.8090019 -8.0028315 -8.1756392 -8.6502476 -9.902338 -11.486626 -13.184669 -12.87334][-14.621799 -13.816414 -11.32887 -9.9450417 -8.5023365 -7.7975569 -8.3016567 -8.611659 -8.9136086 -9.373558 -9.22356 -9.6592417 -10.687181 -10.400437 -8.9836159][-13.435755 -12.143175 -10.877089 -8.6388493 -7.532578 -7.5749974 -7.5324144 -7.3690529 -7.1899571 -7.5682878 -6.7671165 -6.800252 -7.184351 -8.4263363 -8.5362587][-9.3266726 -8.8742085 -8.2709427 -7.5304203 -6.7647076 -5.8584418 -4.6180844 -4.8083267 -5.3392553 -5.5042953 -6.14841 -7.1253805 -7.7450562 -6.8618155 -6.1277919]]...]
INFO - root - 2017-12-15 14:32:09.183739: step 23010, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 56h:03m:10s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 14:32:15.761104: step 23020, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 57h:28m:05s remains)
INFO - root - 2017-12-15 14:32:22.338154: step 23030, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 56h:17m:59s remains)
INFO - root - 2017-12-15 14:32:28.981358: step 23040, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 56h:13m:06s remains)
INFO - root - 2017-12-15 14:32:35.526902: step 23050, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 56h:19m:17s remains)
INFO - root - 2017-12-15 14:32:42.112441: step 23060, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 55h:49m:49s remains)
INFO - root - 2017-12-15 14:32:48.785505: step 23070, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 56h:07m:09s remains)
INFO - root - 2017-12-15 14:32:55.368568: step 23080, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 55h:54m:07s remains)
INFO - root - 2017-12-15 14:33:01.980542: step 23090, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 58h:28m:24s remains)
INFO - root - 2017-12-15 14:33:08.627984: step 23100, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 55h:06m:46s remains)
2017-12-15 14:33:09.169034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4779007 -2.433018 -2.5220325 -2.3573492 -3.0525062 -2.9383807 -3.2201996 -2.9005349 -3.1522729 -4.7903605 -5.0972681 -8.0231419 -9.2145052 -11.454718 -11.125682][-3.187012 -2.6850905 -2.5128434 -2.5637825 -3.4572663 -4.1691275 -4.203784 -3.5358377 -3.8816395 -3.8156474 -4.0131621 -7.3590326 -8.9647608 -11.603626 -12.592779][-1.5666728 -2.585875 -3.4229066 -3.5015941 -4.0572071 -4.0039968 -3.9621916 -3.3762205 -3.0584676 -3.5431626 -4.7248726 -7.1317067 -8.4617481 -10.084828 -10.220787][-1.4417005 -2.0676949 -2.3084729 -2.9044714 -4.1480618 -4.3413973 -4.1144009 -3.3159716 -3.4072576 -3.8890004 -4.8502045 -7.8429456 -9.1687527 -11.088293 -10.590044][-1.7885129 -2.9650369 -3.1172905 -2.5494719 -2.8580711 -1.8941648 -1.5646071 -2.9056687 -3.6644931 -3.7553668 -4.3690138 -7.3610859 -7.9992466 -9.827116 -10.625664][-3.2600224 -3.4746914 -2.512044 -1.2288046 -1.1526618 0.34688568 1.4429936 1.4157505 0.033506393 -1.6927242 -3.4783406 -6.4330115 -8.490983 -11.18602 -11.17512][-4.3883505 -4.3853626 -3.25294 -0.50512552 1.298563 2.6166091 3.736732 4.7635989 3.8147807 1.2198586 -1.0962548 -4.7549229 -7.2783175 -10.363083 -11.171057][-3.8171754 -3.8972435 -3.706363 -1.3424816 0.9450717 3.6802773 5.3004355 5.7561173 5.4650149 3.2764392 1.8683791 -2.098491 -5.5196452 -9.1620522 -11.090723][-2.9230554 -3.2607751 -3.4654999 -2.239 -0.66730452 2.2178812 3.6137862 4.6435285 4.8078408 3.3971238 1.6810398 -2.1860008 -4.9878283 -8.7763166 -10.226678][-2.8940291 -4.31531 -4.8242249 -3.6781507 -3.0146337 -1.3015122 0.015676022 1.7688313 2.335176 1.2220073 0.26016712 -3.9566185 -6.0561109 -8.9286957 -10.382798][-7.6734562 -7.6122279 -7.6555352 -6.8822618 -6.3300161 -5.5849028 -5.139883 -3.5988371 -2.401094 -2.19276 -2.4278269 -6.3406157 -8.2988358 -9.5196781 -9.7518616][-11.125481 -10.924227 -10.367226 -10.082435 -10.008007 -8.9411621 -8.827776 -8.1589 -6.73501 -5.6824064 -5.6391392 -7.8169117 -9.1313334 -10.563679 -9.7184134][-14.011049 -13.729118 -11.956496 -10.768377 -10.831491 -11.094419 -11.121323 -10.824995 -9.7094278 -7.6800876 -7.0187812 -8.517601 -9.7267227 -9.6543083 -8.2815571][-11.441301 -12.781695 -12.208155 -10.937304 -10.391315 -10.3995 -10.079844 -8.9598179 -8.3128452 -8.1748848 -7.671526 -7.0911665 -7.5588636 -8.0047436 -7.2052097][-9.2216949 -10.072356 -10.263668 -9.7705288 -9.1115055 -9.1688557 -8.9787216 -8.1804905 -8.3899879 -8.593029 -7.6156011 -8.6035461 -9.5500736 -9.3974037 -9.5140247]]...]
INFO - root - 2017-12-15 14:33:15.750619: step 23110, loss = 0.20, batch loss = 0.16 (11.6 examples/sec; 0.687 sec/batch; 59h:01m:08s remains)
INFO - root - 2017-12-15 14:33:22.405189: step 23120, loss = 0.24, batch loss = 0.19 (11.9 examples/sec; 0.674 sec/batch; 57h:57m:29s remains)
INFO - root - 2017-12-15 14:33:28.967865: step 23130, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 55h:06m:27s remains)
INFO - root - 2017-12-15 14:33:35.567072: step 23140, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 55h:48m:16s remains)
INFO - root - 2017-12-15 14:33:42.124524: step 23150, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 55h:08m:52s remains)
INFO - root - 2017-12-15 14:33:48.671363: step 23160, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 56h:42m:27s remains)
INFO - root - 2017-12-15 14:33:55.159405: step 23170, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 57h:34m:14s remains)
INFO - root - 2017-12-15 14:34:01.761429: step 23180, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 57h:55m:05s remains)
INFO - root - 2017-12-15 14:34:08.366248: step 23190, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 58h:07m:56s remains)
INFO - root - 2017-12-15 14:34:15.045370: step 23200, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 56h:06m:45s remains)
2017-12-15 14:34:15.548699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.421433 -3.5756741 -4.3104553 -2.9521937 -2.212476 -2.4243538 -1.4329576 0.34693003 1.646421 0.14582586 -1.8614895 -6.9881687 -9.6944885 -13.943602 -14.97876][-3.6174562 -3.8416772 -4.4236727 -4.329586 -4.1085472 -3.0903573 -0.87491274 0.19150639 1.2394381 0.68342686 -1.318532 -6.2801504 -10.053376 -15.490494 -17.581356][-2.1431561 -3.1173646 -4.1881661 -3.0931208 -3.5680311 -3.7604189 -1.7510681 0.60330582 0.39970636 -0.67967081 -1.790905 -7.3998294 -10.992836 -15.240112 -16.86319][-2.4664886 -2.3784018 -3.3902066 -1.9964948 -1.1971002 -0.69411612 -0.84005356 0.15090561 1.137877 -0.19301462 -2.3985796 -7.17336 -10.21138 -15.615456 -16.251896][-0.854424 -0.907403 -1.0458627 -1.1014676 -1.2805243 0.22670841 0.77345657 0.67649174 0.64358664 0.78758049 -1.8142028 -8.1011333 -10.548473 -15.10742 -16.663891][-1.2960086 -2.7558882 -1.9516234 -0.9954114 -0.52177143 0.99018908 2.5497413 3.2995772 2.6119485 1.2814465 -1.0326657 -6.8736415 -11.467587 -15.149799 -15.948505][-2.5017114 -3.6352844 -4.0382538 -2.4672861 -0.76489735 2.1392975 3.255487 3.8803315 4.6394973 2.3095565 -0.6274209 -5.7559929 -8.7572622 -13.660942 -15.252136][-4.6518826 -4.8541493 -4.546154 -3.1414626 -1.1779394 2.1066642 4.334022 4.5031352 4.9229589 3.913177 1.080977 -4.6362271 -7.9706469 -12.779467 -14.351572][-3.9457932 -5.9475942 -5.5602942 -3.3939426 -1.7355292 0.051151276 2.9817014 4.5089993 4.0227809 2.7569833 0.63580322 -4.8384709 -8.5733738 -12.912535 -14.20042][-3.4516766 -4.4446964 -5.6084123 -4.3295403 -3.081502 -1.768703 0.16912937 1.7621236 1.9999385 -0.25644875 -1.6263747 -5.7832108 -8.7610226 -12.420982 -13.540066][-6.63628 -7.3822122 -8.2439051 -6.9997516 -6.6341839 -4.9243727 -3.5348444 -3.3669484 -2.8397391 -3.7845597 -5.20862 -10.466789 -10.917548 -13.925947 -13.424204][-8.8951845 -9.4158411 -8.8485146 -7.46379 -7.4503822 -7.8032188 -7.1780949 -7.0607705 -7.1128416 -7.3718281 -8.7941256 -11.284162 -12.125185 -13.78149 -14.226343][-10.639569 -11.07773 -10.031321 -9.1696253 -8.7323856 -8.8021526 -9.481391 -9.336237 -8.9725227 -9.6846237 -10.195219 -12.190523 -12.99201 -12.997009 -12.707405][-9.61084 -9.0409508 -9.0426722 -7.8996754 -7.4345794 -8.1904774 -9.3070116 -9.55502 -9.4126444 -10.210802 -9.4741535 -9.9122372 -9.0203114 -10.188088 -10.388899][-10.21926 -10.352091 -10.022297 -9.6232243 -9.30762 -9.1602221 -10.065411 -10.98402 -10.270851 -9.3023348 -9.62369 -10.682186 -11.420033 -12.134633 -12.526476]]...]
INFO - root - 2017-12-15 14:34:22.099705: step 23210, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 55h:11m:03s remains)
INFO - root - 2017-12-15 14:34:28.730002: step 23220, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 56h:13m:14s remains)
INFO - root - 2017-12-15 14:34:35.325651: step 23230, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 56h:47m:47s remains)
INFO - root - 2017-12-15 14:34:41.936547: step 23240, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 56h:32m:15s remains)
INFO - root - 2017-12-15 14:34:48.553054: step 23250, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 54h:59m:40s remains)
INFO - root - 2017-12-15 14:34:55.107771: step 23260, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 56h:29m:42s remains)
INFO - root - 2017-12-15 14:35:01.676763: step 23270, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 54h:50m:41s remains)
INFO - root - 2017-12-15 14:35:08.309616: step 23280, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 54h:58m:00s remains)
INFO - root - 2017-12-15 14:35:14.876212: step 23290, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 55h:43m:57s remains)
INFO - root - 2017-12-15 14:35:21.469661: step 23300, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.681 sec/batch; 58h:30m:28s remains)
2017-12-15 14:35:22.045260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5917959 -4.7397251 -5.2327757 -6.7473936 -7.9395542 -8.6195974 -8.4982834 -9.0922155 -9.2908974 -9.8824 -9.146327 -9.338582 -10.229214 -9.3356743 -7.0207615][-2.9688668 -3.1395993 -4.3109345 -6.2804623 -8.8338518 -10.65084 -10.131576 -9.5958452 -9.0481434 -8.0682631 -7.5392637 -8.3717031 -8.6095276 -8.5650492 -7.483644][-2.1743729 -2.6589732 -4.5104365 -6.3980823 -7.2474728 -7.8328304 -8.4411192 -7.9927931 -8.3886156 -9.2673731 -8.6066885 -8.1461935 -7.8698153 -7.9924011 -6.8783569][-4.9685864 -5.1548033 -5.1753221 -5.78031 -7.5876107 -7.3398447 -6.3641748 -6.3193679 -6.0892048 -6.4874873 -6.8787265 -7.4850721 -8.1378069 -8.3916149 -6.7194066][-6.3111529 -6.7216425 -6.4920716 -6.2736483 -5.2893515 -3.2517054 -1.7022796 -2.2710705 -3.8531084 -4.8941774 -4.9819961 -5.8831606 -7.0302372 -7.4429803 -6.5117865][-6.1750255 -6.8331823 -6.4788427 -4.3131609 -2.3067863 0.57316065 2.7021775 2.298408 0.3704772 -2.4866986 -3.0135479 -3.1139538 -4.7401695 -5.9433088 -5.46647][-6.6748476 -7.3017573 -7.0558405 -3.36294 -0.83847713 2.4541001 6.0329509 6.3863339 4.6095176 1.1029963 -1.163239 -3.1827753 -4.4408884 -4.7959061 -3.522486][-7.6380506 -6.113637 -4.8439145 -2.3902009 0.50191736 3.7941442 6.1460853 6.2377181 5.0460715 2.6965613 0.29172707 -2.2858078 -4.350184 -5.4696956 -3.5081103][-6.8119369 -6.0037079 -5.6631756 -3.0007927 -0.39224386 1.9989376 4.4002004 5.0567861 3.4044309 1.1629372 -0.68102646 -3.586576 -5.7190466 -6.1933718 -3.9956222][-6.3907967 -5.8876891 -4.4495621 -3.0742683 -2.355118 -0.5988102 2.616363 3.2389998 1.8061132 -0.74714708 -2.4171546 -4.5984111 -6.3483095 -6.3428035 -4.3043518][-10.662599 -9.1339912 -8.32892 -6.098031 -5.0551777 -3.9940434 -2.9595692 -1.882642 -1.5513296 -2.827785 -4.610054 -6.8554506 -7.61543 -7.0840445 -3.1596782][-12.715652 -11.005085 -9.6102314 -7.3890233 -5.703217 -3.7986155 -3.858557 -4.3103137 -4.9693508 -5.6384521 -7.123559 -6.8597775 -6.6413813 -6.0501122 -3.7013404][-11.599752 -9.9681053 -8.072834 -6.0802217 -5.2028713 -3.6493845 -2.597827 -3.3701966 -4.3403606 -5.0870075 -5.5117955 -6.2177396 -6.4019442 -5.2824163 -3.5910504][-9.8787155 -8.7669926 -7.1876411 -5.4990625 -4.1171103 -3.4810486 -2.8367805 -2.2868338 -1.6845775 -2.7847562 -3.3810492 -3.8852837 -4.2419586 -4.0362287 -2.7574975][-8.5094872 -8.39769 -7.6173649 -6.5265942 -4.7818556 -3.7465711 -4.2208934 -3.7073379 -2.6939862 -2.5869818 -2.6876 -3.9818749 -4.9396367 -5.9184694 -5.6183867]]...]
INFO - root - 2017-12-15 14:35:28.610553: step 23310, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 55h:50m:07s remains)
INFO - root - 2017-12-15 14:35:35.113681: step 23320, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 56h:59m:31s remains)
INFO - root - 2017-12-15 14:35:41.670493: step 23330, loss = 0.19, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 54h:48m:16s remains)
INFO - root - 2017-12-15 14:35:48.207927: step 23340, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 55h:00m:43s remains)
INFO - root - 2017-12-15 14:35:54.734547: step 23350, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 56h:42m:33s remains)
INFO - root - 2017-12-15 14:36:01.344162: step 23360, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 58h:20m:54s remains)
INFO - root - 2017-12-15 14:36:07.964407: step 23370, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 54h:52m:34s remains)
INFO - root - 2017-12-15 14:36:14.605806: step 23380, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 56h:32m:11s remains)
INFO - root - 2017-12-15 14:36:21.233806: step 23390, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 56h:27m:02s remains)
INFO - root - 2017-12-15 14:36:27.887755: step 23400, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 56h:53m:52s remains)
2017-12-15 14:36:28.456588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0318341 -4.9123039 -4.8117805 -3.8055677 -3.4304163 -3.4469995 -3.1335678 -2.8777609 -2.9034216 -2.6400409 -3.475955 -6.5894127 -8.92814 -10.596237 -10.078056][-1.8239861 -2.6676106 -3.1391716 -2.6336184 -2.2656147 -2.6503241 -3.1442516 -2.6857169 -2.4732952 -2.9734085 -3.4597416 -6.3849659 -8.67654 -9.42485 -9.668396][0.10399485 -1.617763 -2.8047218 -2.1559114 -2.6070409 -3.1827815 -3.1685097 -2.7576053 -2.2566144 -2.1779583 -3.9018221 -7.6350656 -9.8615627 -11.107013 -11.651957][-3.5423632 -3.3703661 -3.4852841 -2.9415784 -3.2497807 -3.5920496 -3.4598606 -2.819315 -2.0796864 -1.7235539 -1.8634739 -5.5631375 -8.99274 -10.841519 -11.567372][-1.5725298 -3.6235743 -4.5107727 -3.6550393 -2.8977566 -1.8138835 -1.7381017 -2.7360075 -3.0500696 -2.4401498 -2.3550949 -5.6297808 -8.488615 -10.148027 -11.381562][-3.2652631 -3.6034119 -3.1312943 -1.2513785 0.013337135 0.99957323 2.1263394 1.6044807 0.3943243 0.10844088 -0.80729151 -3.5372841 -5.8420782 -7.1831384 -8.7817793][-4.6285253 -3.8781967 -1.6527958 0.34809208 1.1371918 3.1013646 4.83303 4.8480697 4.0142207 1.3026319 -1.7869725 -3.4267254 -5.3235469 -6.5039072 -7.7309594][-5.86251 -4.4753456 -2.7164071 -0.89942026 0.83480787 2.8013005 4.9865766 5.0675836 5.6060309 3.5491996 0.15036154 -4.1698093 -7.0926247 -7.6405697 -8.0599442][-3.4344316 -3.7908897 -3.0105608 -1.9344075 -1.8099015 -0.14612913 1.3992982 1.887886 3.2113585 1.7528114 -0.64847326 -5.715209 -9.5026932 -9.7781887 -9.67744][-3.8402567 -4.0463572 -3.6843445 -3.175755 -3.3116999 -3.3148286 -2.4611096 -1.1915674 0.10608912 -0.48083067 -2.62399 -7.1567521 -10.389056 -11.64768 -12.047916][-8.9071865 -7.9253578 -7.8578029 -7.2372503 -7.1321311 -7.0525041 -7.0942745 -6.862711 -5.6954956 -6.0550113 -7.0089197 -10.49386 -12.402628 -12.738514 -12.700657][-12.767845 -11.975349 -11.390887 -10.797249 -11.260451 -10.306394 -10.418503 -10.634967 -9.8964119 -9.8445339 -10.179254 -11.289787 -11.446403 -12.356707 -12.713208][-13.879337 -13.642356 -13.138007 -13.237 -12.749301 -11.935008 -11.627914 -11.264247 -11.05556 -10.664865 -10.500223 -9.6933765 -8.8251066 -9.0970745 -8.6589832][-12.444237 -11.912603 -11.933804 -12.775314 -12.114602 -12.223117 -10.610182 -8.5603714 -8.5090914 -8.3584518 -8.2908545 -7.7776132 -7.4430118 -7.169806 -6.6842155][-10.387991 -10.323839 -10.988815 -10.654583 -9.93861 -9.1820068 -8.9573345 -8.92787 -8.3333311 -6.6517091 -7.2735438 -8.1996174 -8.46207 -7.9003229 -7.5469542]]...]
INFO - root - 2017-12-15 14:36:35.020805: step 23410, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.669 sec/batch; 57h:27m:31s remains)
INFO - root - 2017-12-15 14:36:41.638152: step 23420, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 57h:07m:11s remains)
INFO - root - 2017-12-15 14:36:48.309414: step 23430, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 56h:38m:58s remains)
INFO - root - 2017-12-15 14:36:54.997179: step 23440, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 58h:43m:00s remains)
INFO - root - 2017-12-15 14:37:01.580916: step 23450, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 56h:42m:37s remains)
INFO - root - 2017-12-15 14:37:08.254459: step 23460, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 56h:27m:50s remains)
INFO - root - 2017-12-15 14:37:14.938045: step 23470, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 56h:36m:18s remains)
INFO - root - 2017-12-15 14:37:21.598817: step 23480, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.682 sec/batch; 58h:31m:23s remains)
INFO - root - 2017-12-15 14:37:28.183834: step 23490, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 56h:48m:51s remains)
INFO - root - 2017-12-15 14:37:34.808676: step 23500, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 55h:13m:54s remains)
2017-12-15 14:37:35.356362: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.4401255 1.1413827 1.7565069 2.3222919 1.6127162 -0.66611385 -3.5467625 -5.0001812 -5.9679332 -6.2214003 -6.6411734 -10.211545 -12.915422 -11.939817 -12.315547][1.244967 1.7683854 4.0114284 4.8611226 3.7598815 0.98230982 -2.1979916 -4.7036457 -7.2941885 -9.1356239 -10.069674 -14.908186 -17.220282 -16.1204 -16.755995][1.6617546 2.1131768 2.7056518 4.1296992 3.666966 1.9267921 -0.50314283 -3.5762811 -6.206707 -8.4080505 -10.286324 -14.685593 -17.353462 -17.81019 -18.607304][-1.1875396 -0.66168308 1.2082887 2.6475148 2.0541725 0.83694077 -0.87485886 -2.9589272 -5.0868835 -6.5119758 -7.5319376 -12.046774 -15.409929 -15.614038 -17.278797][-1.3962913 -1.869767 -0.90080452 0.87791586 1.5250735 1.5950489 1.0532632 -0.14662409 -1.8628829 -3.9512799 -5.9495454 -10.40588 -13.670082 -14.611504 -16.172504][-3.1640835 -3.0371482 -2.3405755 1.1236959 3.0094094 3.6499619 3.6618915 2.3710408 1.437315 0.33148861 -2.2581282 -7.147481 -10.182858 -10.372988 -11.732481][-2.9258263 -3.0128324 -1.5894475 0.72496033 3.05619 5.602037 6.6693006 5.4592738 4.6586127 3.0976005 0.48850203 -4.9956036 -9.4039917 -9.5583839 -11.289217][-5.9932718 -5.3539686 -3.4011443 0.04458046 2.7319541 5.3847032 7.4844384 6.3145986 4.8259273 3.4198308 1.3109369 -4.7409568 -9.20557 -9.6597614 -11.549231][-7.42418 -6.3575864 -4.189743 -1.4556499 0.38903761 3.1227851 5.2365355 4.8844094 3.9826436 2.4713025 0.22477627 -5.3483582 -9.3773966 -9.7952337 -11.236451][-8.1157856 -7.0722508 -4.75996 -2.0453675 -0.04907608 1.3610635 2.0416098 1.9015298 2.1099916 1.3059955 -0.71583557 -6.3891292 -10.346349 -11.099732 -12.778255][-12.290762 -12.168871 -10.128932 -6.65711 -4.8471527 -2.9227424 -1.9315345 -2.5593567 -2.679738 -2.9812806 -3.3570282 -7.0615239 -9.8497658 -11.250135 -11.735859][-14.81109 -15.490652 -14.260841 -12.1089 -10.458553 -8.0443783 -7.64043 -7.5096836 -7.6057324 -7.8478489 -7.8180504 -9.0415621 -9.6553955 -10.664367 -11.951489][-15.100437 -14.471958 -14.003603 -12.678108 -12.049055 -10.914454 -10.24346 -10.229237 -10.036065 -9.606555 -9.4856834 -9.561964 -9.3386812 -8.99143 -9.2746372][-13.706282 -13.185656 -12.283136 -11.10577 -11.142487 -10.707611 -11.314959 -11.472542 -10.475641 -9.722496 -8.9067116 -8.9054127 -8.2569332 -7.7892952 -7.4160872][-9.7903557 -10.164259 -9.4866323 -7.5073457 -7.1679487 -6.2429867 -6.4254274 -6.9814959 -7.80621 -7.703896 -7.2994385 -7.4610004 -8.2191944 -8.6916161 -8.559351]]...]
INFO - root - 2017-12-15 14:37:41.930805: step 23510, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 56h:08m:26s remains)
INFO - root - 2017-12-15 14:37:48.524993: step 23520, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 57h:47m:23s remains)
INFO - root - 2017-12-15 14:37:55.164418: step 23530, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 56h:35m:58s remains)
INFO - root - 2017-12-15 14:38:01.711338: step 23540, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 54h:46m:10s remains)
INFO - root - 2017-12-15 14:38:08.293513: step 23550, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 58h:34m:30s remains)
INFO - root - 2017-12-15 14:38:14.883522: step 23560, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 55h:43m:23s remains)
INFO - root - 2017-12-15 14:38:21.435571: step 23570, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 56h:05m:03s remains)
INFO - root - 2017-12-15 14:38:28.080070: step 23580, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 56h:38m:01s remains)
INFO - root - 2017-12-15 14:38:34.683651: step 23590, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 57h:11m:49s remains)
INFO - root - 2017-12-15 14:38:41.370430: step 23600, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 58h:20m:49s remains)
2017-12-15 14:38:41.884560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8655384 -1.4441929 -1.6018496 -2.6164246 -4.1887012 -4.5360007 -6.0497961 -8.2935352 -8.6202879 -8.5111742 -8.5662479 -9.506731 -11.980757 -13.554211 -12.530054][-2.5558629 -1.9652917 -2.2211738 -2.8246536 -3.598841 -3.7103367 -3.9519882 -4.1234965 -4.5032625 -5.23046 -5.9510136 -7.8632078 -9.8842506 -11.434681 -11.09008][-2.3044693 -1.7196152 -2.2800241 -3.4688177 -5.1954517 -5.9621549 -5.9068723 -5.3794546 -3.8841963 -3.0946662 -4.3554206 -6.2322149 -7.9503031 -9.6711416 -10.115925][-5.8031383 -4.8818817 -4.2183409 -4.857965 -7.0842028 -5.7286716 -4.4218392 -3.8569622 -1.9653065 -1.4772568 -1.9896164 -3.5055013 -6.4256315 -8.6374846 -8.3041868][-6.4600563 -7.0642595 -7.123971 -8.2578211 -8.9523125 -7.4094338 -4.7205076 -1.1469517 0.80500031 0.48185921 -0.19933653 -3.3321941 -5.562542 -6.921031 -6.6018734][-6.8932076 -7.510757 -6.7148333 -5.8721476 -6.4912176 -5.27484 -2.4300408 0.52513027 1.9118257 1.3844709 -1.1369119 -2.4409628 -3.5425658 -5.4706879 -5.1180472][-7.9653659 -8.1061525 -8.2091942 -6.7898855 -4.6220813 -1.0938931 1.8826976 4.2946391 4.9449487 3.2531037 0.44923496 -1.9215796 -4.2588377 -5.411253 -3.5341861][-8.8939219 -8.846221 -9.0855865 -8.554184 -6.9123363 -2.5832024 3.1128201 6.4899669 7.7894597 5.7839284 2.5229459 0.19002581 -3.463989 -5.5969796 -5.2606487][-10.346973 -9.7258835 -8.9848843 -6.7103958 -5.7629461 -3.6888649 0.47215939 3.9972825 5.9241576 4.5552335 2.076901 -0.52585983 -4.569468 -7.2034483 -7.3731155][-10.108835 -10.868206 -10.132154 -7.7016497 -7.3425779 -4.3683834 -1.489481 0.71277475 2.9156346 2.6863704 1.2003832 -1.7505479 -5.4321361 -9.0944862 -10.852774][-11.966924 -10.786158 -9.8363371 -9.1030979 -8.6148853 -7.86885 -6.4061265 -3.5043004 -1.7764719 -2.2208292 -2.9314003 -4.931282 -8.9336691 -11.247677 -12.053438][-14.13302 -11.659652 -9.527256 -7.9700131 -8.3848629 -8.7077646 -8.1659069 -7.2249508 -7.140759 -6.6290388 -6.2980037 -7.2020674 -9.5821991 -11.882565 -12.589796][-12.362301 -10.895174 -8.3556137 -6.9356594 -7.3916731 -6.1253381 -5.7608471 -7.0350862 -7.241209 -7.1685157 -7.1756878 -7.9471707 -8.8961067 -9.29868 -9.1854191][-10.743001 -9.4404411 -7.5379581 -6.9174261 -6.4683142 -6.1733322 -6.2572927 -4.7897749 -4.7058811 -4.9977541 -4.69401 -4.4130845 -5.5747948 -7.3882008 -6.9596338][-8.7700148 -8.8502493 -7.4370294 -6.2678323 -5.2911954 -5.2389412 -4.9078546 -4.2802591 -4.2617226 -3.5013523 -3.6037323 -4.8891535 -6.057384 -6.7282524 -7.2526035]]...]
INFO - root - 2017-12-15 14:38:48.477422: step 23610, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 55h:53m:32s remains)
INFO - root - 2017-12-15 14:38:55.032805: step 23620, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 55h:48m:20s remains)
INFO - root - 2017-12-15 14:39:01.676197: step 23630, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 56h:50m:27s remains)
INFO - root - 2017-12-15 14:39:08.249797: step 23640, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 54h:40m:09s remains)
INFO - root - 2017-12-15 14:39:14.941599: step 23650, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 57h:10m:04s remains)
INFO - root - 2017-12-15 14:39:21.445549: step 23660, loss = 0.21, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 56h:06m:54s remains)
INFO - root - 2017-12-15 14:39:28.094417: step 23670, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 56h:49m:28s remains)
INFO - root - 2017-12-15 14:39:34.754075: step 23680, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.689 sec/batch; 59h:06m:53s remains)
INFO - root - 2017-12-15 14:39:41.345200: step 23690, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 55h:20m:56s remains)
INFO - root - 2017-12-15 14:39:47.932572: step 23700, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 56h:07m:08s remains)
2017-12-15 14:39:48.490034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9393835 -6.3115487 -6.2626028 -6.595808 -7.5477557 -7.1986327 -6.707459 -5.6145124 -4.550436 -3.0548153 -1.4830956 -4.3744941 -5.8399329 -7.7304935 -9.3407536][-6.0022006 -6.386519 -6.7489491 -6.797689 -7.3010483 -7.212676 -6.7583995 -6.2025237 -5.9253674 -4.9987893 -4.1843152 -6.6885896 -7.9820366 -9.3863363 -9.7480488][-6.1876788 -6.8290029 -7.315321 -7.4717684 -7.6178117 -7.2121949 -7.0014267 -6.4263606 -6.2133603 -6.136518 -6.432981 -8.5338335 -9.6278486 -10.393442 -10.328531][-6.1728039 -6.2246828 -6.0224285 -5.9228444 -5.4956541 -5.0497327 -4.8356338 -5.2846618 -5.9952812 -6.2093067 -6.4732332 -9.2166643 -11.144957 -11.365228 -11.046592][-4.4681053 -5.158287 -5.9091005 -3.9461799 -3.3762293 -2.3081717 -0.86583853 -1.7113752 -3.2691052 -4.6504126 -5.61944 -9.0591221 -11.47158 -11.927925 -11.328672][-3.6815503 -3.7686536 -3.8658061 -2.2092106 -0.7430563 1.3697152 3.3042636 2.2874084 0.80302191 -1.6715331 -3.6332874 -6.8778291 -8.77223 -9.8159389 -9.8070831][-3.3939514 -3.0527506 -2.4599621 -0.70352316 0.3992281 2.8179908 5.3119273 5.6245303 5.1121364 2.0132513 -0.72665977 -4.9001012 -7.2723804 -8.3292065 -8.1017494][-4.2547841 -3.4170165 -1.7796459 0.077763081 0.53511477 3.162766 5.5766931 5.8612123 5.8992314 4.1803279 1.9293227 -2.259161 -4.4173865 -5.562253 -5.4874439][-4.048892 -2.8672009 -1.3214364 0.29316711 1.4329004 3.3704638 4.8509774 5.628799 5.77916 4.1444335 2.6056294 -1.6870623 -3.9021873 -4.5278153 -3.7016392][-5.080832 -3.5360043 -1.8340526 -0.014618874 1.7236094 2.4408631 2.8591952 3.2922902 2.9419217 1.3040605 0.073078632 -3.5039544 -5.2912331 -5.3586507 -4.1298904][-9.2803221 -7.2793627 -4.3801441 -2.6236105 -1.8329895 -1.6066012 -1.4511437 -1.4776368 -1.9080901 -2.4863288 -3.6682093 -6.9766235 -8.0927486 -7.2124205 -5.8577118][-13.576237 -11.055009 -7.789341 -4.8225155 -2.8854904 -2.9787543 -3.951791 -4.952271 -5.9641347 -6.8297997 -7.9034553 -9.5635176 -9.9303169 -9.3794937 -8.3499126][-14.173695 -11.014302 -7.62222 -4.5456443 -3.0293958 -3.2025757 -4.1190395 -5.6492524 -7.3586426 -8.6770554 -9.7506895 -10.576183 -10.831755 -9.8334141 -8.664238][-10.411453 -8.5582886 -6.5045495 -4.7575541 -3.1870453 -2.3702483 -3.0342042 -4.5775194 -6.3481522 -8.1793175 -9.8196554 -10.190388 -9.9697971 -9.4573956 -9.6610775][-7.1327758 -6.1877933 -5.0003338 -3.2345238 -1.5187693 -1.9599738 -2.672333 -3.73767 -4.9012394 -6.3811636 -7.7063084 -9.2405853 -10.492605 -10.669505 -10.768676]]...]
INFO - root - 2017-12-15 14:39:55.076966: step 23710, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 56h:09m:38s remains)
INFO - root - 2017-12-15 14:40:01.743465: step 23720, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 57h:02m:47s remains)
INFO - root - 2017-12-15 14:40:08.421445: step 23730, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 58h:33m:26s remains)
INFO - root - 2017-12-15 14:40:14.979512: step 23740, loss = 0.12, batch loss = 0.07 (11.6 examples/sec; 0.692 sec/batch; 59h:21m:38s remains)
INFO - root - 2017-12-15 14:40:21.596547: step 23750, loss = 0.20, batch loss = 0.16 (11.8 examples/sec; 0.679 sec/batch; 58h:16m:21s remains)
INFO - root - 2017-12-15 14:40:28.142619: step 23760, loss = 0.20, batch loss = 0.16 (12.0 examples/sec; 0.665 sec/batch; 57h:02m:11s remains)
INFO - root - 2017-12-15 14:40:34.746987: step 23770, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 55h:17m:43s remains)
INFO - root - 2017-12-15 14:40:41.320086: step 23780, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 56h:08m:38s remains)
INFO - root - 2017-12-15 14:40:47.896551: step 23790, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 55h:55m:25s remains)
INFO - root - 2017-12-15 14:40:54.468917: step 23800, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 56h:43m:02s remains)
2017-12-15 14:40:54.977335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7800169 -2.7480109 -2.820416 -3.8259921 -4.3868146 -3.6659584 -1.7203205 0.040119171 1.9118676 2.4475083 1.7335052 -0.53253508 -3.4990554 -5.5985255 -6.8042636][-2.5023861 -3.2139008 -3.7393179 -5.1496005 -4.6528687 -2.9416459 0.54533529 2.9279494 3.8172765 4.1877141 3.6287627 1.0681863 -2.1470642 -4.2953176 -5.3744588][-2.044693 -2.2947462 -2.7683606 -4.5645394 -5.9267406 -5.329381 -2.4824889 0.60445166 3.8607678 4.5455403 3.5666442 1.240315 -2.6522567 -4.9192123 -5.6630974][-3.94107 -2.6506052 -1.8058743 -2.5407438 -3.1826513 -2.5251279 -1.4192324 0.41234016 1.7013183 2.1460371 1.9306412 -0.020399094 -2.8578937 -4.8777804 -6.2285557][-5.4481697 -3.625767 -2.5111716 -2.92939 -2.5448222 -1.8450284 -1.8586204 -0.95246983 -0.17595863 -0.095593452 0.0042586327 -1.0969796 -3.4672534 -5.7496419 -7.138104][-6.1640568 -5.01336 -3.2314723 -3.1448236 -1.7256551 -0.26334333 0.32900524 0.48668575 0.37420034 -0.52407503 -1.1139274 -2.2974045 -3.6639616 -5.1271968 -6.7831244][-6.8167787 -6.0844941 -4.5903921 -3.5853872 -1.2022648 0.81736088 1.9565792 2.2864013 1.2187328 0.043133259 -0.76943588 -2.8956337 -4.5153279 -5.0245385 -5.5500741][-6.4585776 -6.2075157 -5.1089268 -3.6017225 -0.13640928 2.889483 3.9467864 3.0946383 1.1452088 -0.37569952 -1.1135221 -3.0449731 -4.7451057 -4.9859829 -3.9379945][-6.4349093 -6.344717 -5.1424351 -3.1905417 -0.29438257 2.4528642 3.7838588 3.4999194 1.9706831 -0.097498417 -1.4264627 -2.9454007 -4.4291067 -5.183887 -3.1524203][-7.9986191 -7.8882608 -6.9616828 -4.9066563 -2.4704905 0.29883146 1.3272371 1.1229339 0.05509758 -1.9390202 -3.0029807 -4.2535434 -5.8293347 -6.2664738 -5.4784617][-12.408394 -12.943691 -11.051254 -8.5709057 -5.8942571 -3.8651223 -4.6177974 -5.2402945 -6.1227045 -6.8774619 -6.4928346 -7.1959972 -7.8650436 -7.3323545 -5.8694444][-14.437639 -14.277529 -11.769472 -9.01577 -7.2459145 -7.0834508 -8.3015308 -9.74849 -11.207363 -10.542633 -9.099143 -8.8426514 -7.7512112 -6.5867348 -4.7778482][-13.918589 -13.230484 -10.236094 -7.3454361 -5.8039675 -6.0283284 -8.0323563 -9.5485115 -10.740024 -9.3132191 -7.7856469 -7.4863434 -7.0689754 -5.4406648 -2.4054091][-11.740408 -11.254718 -9.5580511 -6.1536546 -3.4757001 -4.1139193 -6.3113122 -6.8470149 -6.3523088 -5.1699848 -4.5898848 -3.105747 -2.904635 -2.1954515 -1.2869258][-8.7638321 -9.5609665 -8.7814465 -6.6137114 -4.0976849 -3.2617385 -2.9338412 -2.9326282 -3.1831145 -1.6805239 -0.31991577 -0.2339735 -1.1885653 -1.1956773 -1.3913021]]...]
INFO - root - 2017-12-15 14:41:01.545905: step 23810, loss = 0.23, batch loss = 0.18 (12.0 examples/sec; 0.669 sec/batch; 57h:22m:23s remains)
INFO - root - 2017-12-15 14:41:08.194906: step 23820, loss = 0.19, batch loss = 0.14 (11.4 examples/sec; 0.702 sec/batch; 60h:14m:01s remains)
INFO - root - 2017-12-15 14:41:14.814934: step 23830, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.671 sec/batch; 57h:29m:49s remains)
INFO - root - 2017-12-15 14:41:21.439403: step 23840, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 57h:44m:14s remains)
INFO - root - 2017-12-15 14:41:28.024573: step 23850, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 56h:37m:23s remains)
INFO - root - 2017-12-15 14:41:34.666699: step 23860, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 56h:51m:53s remains)
INFO - root - 2017-12-15 14:41:41.261258: step 23870, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 56h:52m:38s remains)
INFO - root - 2017-12-15 14:41:47.840554: step 23880, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 55h:43m:27s remains)
INFO - root - 2017-12-15 14:41:54.466476: step 23890, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 57h:42m:31s remains)
INFO - root - 2017-12-15 14:42:01.087699: step 23900, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 57h:15m:14s remains)
2017-12-15 14:42:01.641590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5450649 -7.7077208 -8.4807453 -8.8172951 -9.7861986 -10.407732 -10.388418 -10.593719 -10.242859 -9.4894981 -8.1864948 -10.135293 -10.385864 -12.34803 -11.560745][-7.3900323 -9.181469 -10.18996 -10.905313 -11.749424 -12.614698 -12.677071 -12.188654 -10.658504 -9.3969288 -8.4766226 -10.014687 -10.664812 -12.903206 -11.724789][-6.909421 -9.73307 -12.040709 -13.283581 -13.835825 -13.915354 -13.667171 -12.705557 -10.923571 -9.7889709 -8.8422031 -10.783233 -11.976011 -14.273792 -13.093878][-7.5277557 -9.4532661 -11.318071 -12.518116 -13.958321 -12.097303 -10.299126 -9.5721426 -7.865901 -7.879385 -8.2448788 -10.890177 -12.104548 -14.856733 -14.914993][-8.1845131 -10.646523 -12.754523 -11.550217 -10.983484 -7.4235954 -4.5329461 -5.0277996 -5.1408076 -4.7976174 -4.9583678 -7.7647276 -10.103985 -14.724174 -14.856138][-9.3157959 -11.546221 -13.302609 -9.6724873 -6.3621244 -1.4036913 3.0283065 3.3750596 1.6534672 -1.5918298 -3.6288393 -6.2666178 -8.60467 -13.562123 -14.130716][-7.8249254 -9.71531 -11.498493 -7.9111013 -3.9552913 1.8072968 7.254662 9.2725925 8.4429455 4.2813611 0.2485714 -5.4328308 -8.2362986 -12.384791 -13.268444][-5.5839663 -7.75478 -8.694376 -6.6880989 -2.8067577 5.5241036 11.734282 10.972422 9.2673607 4.3559709 -0.23817682 -6.5056572 -9.2374344 -13.066754 -12.94739][-3.3528886 -5.7318788 -7.076231 -6.1452465 -4.8756342 1.3354516 7.6528649 7.5087123 6.1137166 0.61955118 -3.5866179 -9.6400213 -12.688906 -16.453144 -15.803568][-2.6979291 -4.5286674 -7.1722465 -6.1004148 -5.5871906 -3.1448514 -0.78183174 1.2894955 1.2241831 -3.8934507 -7.8916235 -13.976887 -16.101406 -19.804705 -19.375385][-6.002264 -7.4893451 -9.524579 -9.5092077 -9.4884262 -8.4902468 -7.4064054 -5.8890872 -5.691555 -8.4841471 -10.415515 -16.492418 -17.729757 -20.269657 -19.468941][-9.8415012 -10.39279 -11.375656 -10.874983 -11.036035 -10.384731 -11.052204 -11.178705 -11.226018 -12.103559 -12.58156 -14.921106 -14.779982 -17.385685 -15.566668][-12.182362 -11.607613 -11.881655 -11.749321 -12.720142 -11.380365 -12.097624 -12.491819 -12.731081 -13.87388 -13.677449 -13.67179 -13.213425 -14.600845 -11.779606][-10.497835 -10.063738 -10.18636 -9.7249336 -9.1772137 -9.21567 -9.8776941 -10.071276 -10.388433 -10.499415 -10.767958 -10.618726 -9.4903851 -10.650736 -10.196925][-8.3190823 -8.7072144 -7.5006342 -6.1502495 -6.0564084 -5.999351 -5.80942 -6.6697621 -7.4853091 -7.9026384 -8.2821407 -8.795743 -8.86599 -8.6776285 -8.58906]]...]
INFO - root - 2017-12-15 14:42:08.216523: step 23910, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 57h:27m:39s remains)
INFO - root - 2017-12-15 14:42:14.780952: step 23920, loss = 0.11, batch loss = 0.07 (12.4 examples/sec; 0.646 sec/batch; 55h:21m:28s remains)
INFO - root - 2017-12-15 14:42:21.404508: step 23930, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 56h:35m:57s remains)
INFO - root - 2017-12-15 14:42:27.905141: step 23940, loss = 0.17, batch loss = 0.12 (12.7 examples/sec; 0.628 sec/batch; 53h:50m:42s remains)
INFO - root - 2017-12-15 14:42:34.490627: step 23950, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 56h:30m:54s remains)
INFO - root - 2017-12-15 14:42:41.071447: step 23960, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 56h:39m:18s remains)
INFO - root - 2017-12-15 14:42:47.706123: step 23970, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 56h:24m:11s remains)
INFO - root - 2017-12-15 14:42:54.278619: step 23980, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 56h:21m:03s remains)
INFO - root - 2017-12-15 14:43:00.868605: step 23990, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 57h:12m:19s remains)
INFO - root - 2017-12-15 14:43:07.426818: step 24000, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 56h:08m:31s remains)
2017-12-15 14:43:07.929035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9420152 -5.3171597 -5.0032034 -4.5991616 -4.8251934 -4.850451 -4.2572479 -2.9909091 -2.0893381 -1.4999571 -1.1367478 -3.2135913 -5.4161806 -7.2580338 -8.2081194][-3.5124803 -2.8071692 -3.6666784 -4.1007786 -5.237061 -5.6248064 -4.9807978 -3.4266677 -1.732044 -0.22694874 -0.014214516 -2.1685286 -4.6666427 -6.8918605 -8.2597122][-1.9887908 -2.4154265 -4.1895127 -4.3210726 -5.6012759 -4.9486732 -3.7852113 -2.2990463 -1.1090999 -0.70680666 0.22810125 -1.3740654 -3.8271461 -5.5452943 -6.8989453][-2.2221756 -1.7811399 -2.5089273 -3.0522985 -4.4181247 -4.471868 -3.9238248 -2.530885 -0.61906433 0.36824369 0.72334242 -1.2084451 -3.5845866 -5.5523367 -6.1303449][-2.9945323 -2.9114661 -3.2795339 -3.7933145 -4.6413193 -3.2475545 -2.2392304 -2.3125584 -1.9940476 -0.61143923 0.30208731 -1.9914725 -4.5978537 -6.2424197 -6.598134][-4.2549596 -4.2136779 -3.5148365 -2.1895089 -1.3562937 0.622005 2.1635432 0.80687571 -0.43357325 -0.94406509 -0.71720982 -1.8723257 -4.248868 -5.9295349 -5.8752995][-5.6852951 -5.3314314 -4.3423805 -1.4589891 0.24541903 3.2789912 5.7519393 5.397037 3.6513839 -0.14356327 -2.3337147 -3.5898015 -5.5387812 -7.0271392 -7.6385126][-8.2258453 -7.1747923 -5.227807 -1.5949831 0.75434637 4.2842059 6.745564 6.3683534 4.6871543 1.1353922 -1.9878533 -4.5919185 -7.0525351 -7.4731803 -7.9973946][-7.248971 -6.3319035 -5.1440516 -2.46326 -0.40063667 3.2858567 6.2672677 6.5005403 5.1917968 1.7345834 -1.3068237 -5.2337046 -8.42879 -9.2677841 -9.4423637][-6.07997 -5.5713506 -5.4344034 -2.6557493 0.60446596 2.5200915 3.986114 4.3074231 3.4151616 -0.056818008 -3.6242113 -7.5394764 -10.524082 -11.191778 -11.085291][-10.994246 -9.1010876 -8.1529751 -4.745297 -2.4120924 -1.4487791 -0.72660923 -0.7441783 -1.7029166 -3.5716026 -6.4596882 -11.049828 -14.256977 -14.049854 -13.040962][-14.373066 -13.021822 -11.41548 -8.4190235 -6.6634269 -5.44909 -4.9944124 -5.7845664 -6.7030735 -7.6915317 -9.4577789 -11.664353 -13.783304 -14.382387 -13.81991][-13.932529 -12.485231 -11.070372 -9.5621548 -9.18248 -8.4254351 -8.4080534 -9.1021662 -9.5734167 -9.8520317 -10.310057 -11.469906 -12.739597 -12.290792 -11.794214][-11.721689 -10.56179 -10.417688 -9.4919357 -8.7137613 -8.1459045 -8.1822815 -8.8389416 -9.495429 -10.104934 -10.908072 -11.217235 -11.80302 -10.920553 -10.095554][-8.2429981 -8.6004848 -8.1810007 -7.4210072 -6.1952987 -5.6634383 -5.5393786 -5.9507732 -7.2495832 -8.7150993 -9.7092371 -10.974905 -11.815251 -10.44147 -9.66168]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 14:43:14.479728: step 24010, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 58h:40m:55s remains)
INFO - root - 2017-12-15 14:43:21.079159: step 24020, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 56h:12m:39s remains)
INFO - root - 2017-12-15 14:43:27.618822: step 24030, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 56h:14m:33s remains)
INFO - root - 2017-12-15 14:43:34.316143: step 24040, loss = 0.12, batch loss = 0.08 (11.5 examples/sec; 0.697 sec/batch; 59h:44m:51s remains)
INFO - root - 2017-12-15 14:43:40.920768: step 24050, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 57h:57m:08s remains)
INFO - root - 2017-12-15 14:43:47.498513: step 24060, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 55h:36m:02s remains)
INFO - root - 2017-12-15 14:43:54.066579: step 24070, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 55h:06m:32s remains)
INFO - root - 2017-12-15 14:44:00.678987: step 24080, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 56h:50m:26s remains)
INFO - root - 2017-12-15 14:44:07.213857: step 24090, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 55h:11m:19s remains)
INFO - root - 2017-12-15 14:44:13.779959: step 24100, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 56h:18m:14s remains)
2017-12-15 14:44:14.312455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3769774 -6.9340672 -5.8342953 -5.7881355 -6.6721268 -7.6006951 -8.4872513 -7.8844619 -7.3537445 -7.6034975 -8.0400124 -8.741272 -12.126472 -11.960087 -10.39468][-7.4800034 -7.566525 -6.8153677 -6.46809 -6.8390641 -8.0072861 -9.228797 -9.573967 -9.9475327 -9.649786 -9.8593922 -11.728416 -14.398869 -13.702293 -12.922718][-4.8959565 -5.4810219 -6.1187491 -6.3413162 -6.8227477 -8.095973 -8.8807487 -8.899168 -9.332201 -10.169636 -11.021137 -11.474412 -13.494307 -12.938348 -11.741721][-4.691958 -5.4782181 -5.3192086 -5.3214331 -6.2804604 -6.6122851 -6.5293617 -7.3817649 -8.1616631 -8.1067886 -8.4592228 -9.9072418 -12.367487 -11.129181 -10.127588][-5.5106773 -6.1576061 -5.6573005 -4.8910279 -4.0854197 -2.9328451 -2.6373806 -3.8755727 -5.3773322 -6.0227704 -6.7635555 -7.500824 -9.715127 -9.0400839 -8.0753012][-6.955328 -7.3015451 -5.1166177 -3.5102024 -2.1129026 0.038538933 2.0434132 1.4863997 0.16660738 -1.5081687 -3.1026545 -4.2159052 -6.9364648 -6.2422504 -5.654521][-7.4960632 -6.920845 -5.0618558 -3.0139771 -0.587234 2.1253567 4.2051616 5.187449 5.1201205 2.234868 -0.38838339 -1.4984016 -4.0209618 -4.5283661 -4.92983][-6.9589381 -6.3855658 -4.410326 -1.523931 1.6844997 3.927424 5.4806094 5.8095489 5.3134503 3.9061675 2.1992874 -0.70359182 -5.1914864 -5.6816483 -6.3603497][-5.26698 -5.3999286 -4.2239208 -2.0799453 0.77428246 3.0734782 3.984632 4.3290429 4.0746675 3.4184089 2.5914626 0.30229235 -4.7963762 -6.9433885 -8.81762][-2.0779338 -3.0253 -2.7521625 -0.92250538 0.54574919 1.0573931 1.340014 0.81471205 0.076482773 0.91625977 1.2767038 -1.0161939 -5.0007739 -6.642395 -9.0578537][-4.9563375 -5.0985093 -5.4360824 -5.3908658 -4.381073 -3.7581115 -3.3681793 -3.7642913 -4.36337 -4.8868227 -5.2544141 -6.4811182 -8.7248316 -9.6097412 -9.92082][-10.688572 -11.166557 -10.564897 -9.8632421 -9.2100649 -8.9592609 -9.0520754 -10.110327 -10.424053 -9.6156645 -9.5935307 -10.813808 -12.267082 -11.257555 -10.099671][-12.390268 -11.715864 -11.352362 -10.306816 -9.8787441 -9.434473 -9.9988222 -11.073393 -11.358866 -11.301103 -10.423182 -10.2692 -11.539318 -10.243921 -9.0677691][-10.844393 -11.168455 -10.585195 -8.9687967 -8.41947 -8.7203617 -9.14077 -9.5591984 -10.097631 -9.4783173 -7.9901638 -7.6959805 -6.8851466 -7.2147651 -9.0076313][-9.3051367 -9.3910437 -8.9334946 -7.6260977 -6.1012053 -5.3526697 -6.0208263 -6.3122926 -6.6911893 -6.5239997 -6.3564515 -6.7063928 -6.71535 -6.2695336 -5.9037104]]...]
INFO - root - 2017-12-15 14:44:20.874410: step 24110, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 55h:52m:55s remains)
INFO - root - 2017-12-15 14:44:27.531079: step 24120, loss = 0.20, batch loss = 0.16 (11.8 examples/sec; 0.677 sec/batch; 57h:59m:38s remains)
INFO - root - 2017-12-15 14:44:34.199399: step 24130, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 56h:23m:49s remains)
INFO - root - 2017-12-15 14:44:40.809190: step 24140, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 56h:09m:58s remains)
INFO - root - 2017-12-15 14:44:47.327048: step 24150, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.632 sec/batch; 54h:06m:49s remains)
INFO - root - 2017-12-15 14:44:53.964420: step 24160, loss = 0.21, batch loss = 0.16 (11.8 examples/sec; 0.680 sec/batch; 58h:15m:24s remains)
INFO - root - 2017-12-15 14:45:00.514550: step 24170, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 55h:11m:25s remains)
INFO - root - 2017-12-15 14:45:07.211662: step 24180, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 57h:20m:33s remains)
INFO - root - 2017-12-15 14:45:13.781583: step 24190, loss = 0.24, batch loss = 0.19 (12.1 examples/sec; 0.659 sec/batch; 56h:28m:27s remains)
INFO - root - 2017-12-15 14:45:20.329286: step 24200, loss = 0.11, batch loss = 0.07 (12.0 examples/sec; 0.666 sec/batch; 57h:04m:16s remains)
2017-12-15 14:45:20.877066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9710302 -2.5453885 -2.378556 -2.3407004 -2.7728488 -3.0103281 -3.2595546 -2.9447079 -3.7231579 -4.3050594 -4.999629 -8.3511181 -9.7216663 -12.543903 -11.9722][-4.8641138 -2.7817502 -1.7395465 -1.5385804 -1.9279575 -2.309695 -3.4552877 -2.9612656 -3.1789398 -4.2763834 -4.4542117 -8.09081 -9.7682266 -13.191635 -12.854996][-3.2592235 -3.1606815 -2.7016299 -2.1972764 -2.717072 -3.1059275 -2.5801229 -2.8174865 -3.2024884 -4.0030627 -5.2147408 -7.7867 -8.0413761 -10.658286 -10.475209][-2.87566 -2.7093058 -1.9574461 -1.9674313 -2.5237679 -2.6206214 -2.5515428 -2.8198223 -3.7919817 -4.38704 -4.7717118 -7.8586044 -9.0247593 -11.810135 -10.959447][-3.314137 -3.7858844 -2.8884273 -1.3951125 -1.4763923 -0.88569164 -0.18905354 -1.9939253 -3.6699271 -4.2504444 -4.561079 -7.5655708 -8.15691 -11.05663 -11.047382][-4.4872179 -3.7481484 -2.205179 -0.47128582 0.17223358 1.3377814 2.0773106 1.2897191 -0.8877821 -3.0758724 -4.387639 -7.1538239 -8.4596033 -11.416828 -11.989438][-5.3402 -4.0597148 -2.8407123 0.29685593 2.4788976 3.2882924 3.3696198 3.8631234 3.0783353 0.020083427 -2.3806374 -5.8056006 -7.9143791 -11.631105 -11.695707][-5.5290504 -4.9596453 -4.0406361 -1.1362033 1.7533526 4.8947806 5.9570489 5.0384917 3.8570714 2.1623688 0.71776056 -3.7024202 -6.2877092 -10.31789 -11.571991][-4.0537605 -4.1516213 -3.3772278 -1.5835333 -0.65718603 1.625104 3.7263923 3.9760394 3.5350051 2.6884503 1.6402836 -2.243372 -4.9330263 -9.28271 -10.251627][-3.0734761 -4.44498 -5.3053026 -3.2275155 -2.1542151 -1.3200088 -0.31003523 1.1269951 1.7134356 1.1352048 0.76601124 -2.6796839 -4.472436 -8.8227816 -10.096712][-7.3009834 -7.1672063 -7.8662939 -6.7142496 -6.155077 -5.2594657 -5.3263907 -4.7798119 -3.073117 -2.3992569 -2.3027825 -5.6250863 -7.7857409 -9.498271 -8.9507332][-11.731714 -11.386892 -10.578007 -9.9896793 -10.212221 -9.2320766 -8.8964529 -8.6990347 -8.0341139 -6.694654 -5.6142974 -7.7640042 -9.2586241 -10.960392 -10.056944][-14.95146 -13.994802 -12.360142 -11.036312 -11.724176 -11.880676 -11.525097 -11.151803 -10.68562 -9.5045862 -8.150773 -9.6081867 -10.198772 -10.669296 -9.3322926][-11.848841 -12.431487 -11.733612 -11.65378 -10.796604 -10.660753 -10.986077 -9.6103964 -8.6421423 -9.2942324 -9.0619125 -7.9533267 -7.5170369 -8.95358 -8.3598976][-9.4301405 -9.3186321 -9.3584938 -9.5683975 -8.8413477 -8.6122131 -8.5736027 -8.1941814 -8.2060738 -8.1026525 -7.8528557 -9.0650177 -9.0221472 -8.8339367 -8.9526653]]...]
INFO - root - 2017-12-15 14:45:27.532378: step 24210, loss = 0.21, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 56h:28m:57s remains)
INFO - root - 2017-12-15 14:45:34.090583: step 24220, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 57h:13m:34s remains)
INFO - root - 2017-12-15 14:45:40.684959: step 24230, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 57h:20m:32s remains)
INFO - root - 2017-12-15 14:45:47.340012: step 24240, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.668 sec/batch; 57h:13m:24s remains)
INFO - root - 2017-12-15 14:45:53.881697: step 24250, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 55h:49m:22s remains)
INFO - root - 2017-12-15 14:46:00.510698: step 24260, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 57h:09m:10s remains)
INFO - root - 2017-12-15 14:46:07.098852: step 24270, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 56h:14m:43s remains)
INFO - root - 2017-12-15 14:46:13.693798: step 24280, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 55h:41m:26s remains)
INFO - root - 2017-12-15 14:46:20.254591: step 24290, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 56h:09m:02s remains)
INFO - root - 2017-12-15 14:46:26.875343: step 24300, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.648 sec/batch; 55h:26m:15s remains)
2017-12-15 14:46:27.401079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9738526 -5.110909 -3.735683 -3.0213461 -3.5788188 -4.0015264 -4.2045074 -4.2582178 -4.0031853 -4.2355084 -4.327755 -6.8534989 -8.9012289 -8.9007664 -8.2114563][-6.2984376 -6.8499365 -6.1981888 -5.5938964 -5.4659562 -5.89209 -6.3044767 -6.5536656 -6.5242658 -6.4033413 -6.1409078 -8.4665918 -10.320109 -10.613523 -10.132542][-5.7460942 -5.9955974 -5.8241525 -5.6436915 -5.8307061 -6.2648039 -6.6073561 -6.4286427 -6.3477936 -6.4375253 -6.2005854 -8.1703033 -9.8653393 -9.9565487 -10.20295][-6.167861 -5.9989228 -4.5544767 -3.7820244 -4.103755 -4.5615249 -5.0413465 -5.2699389 -5.2587776 -5.0174265 -4.721982 -7.0026722 -8.8405037 -8.77022 -9.2311287][-7.4103203 -7.232132 -5.9121428 -4.5967894 -3.4305129 -2.8996584 -2.9549365 -3.242471 -3.7364726 -3.7099235 -3.4042537 -5.3985672 -7.3982511 -7.8498869 -8.760397][-7.2969503 -6.4255219 -4.5537148 -2.6904478 -1.2452822 -0.11758661 0.83006811 1.2245364 0.95050097 0.30103827 -0.77572727 -3.0720696 -5.0835009 -5.6439333 -6.5372791][-8.6910744 -8.3598318 -5.5738034 -1.7987795 1.1639051 2.8512206 4.0252671 4.4270835 4.4946675 2.9885993 1.1257071 -1.6524897 -4.3869305 -5.5314827 -6.0840096][-7.2215109 -7.6635942 -6.2834411 -3.438113 -0.4252224 2.3330092 4.0574527 4.0798392 4.702498 3.9651418 2.2574477 -2.00919 -5.71686 -7.2033167 -7.6333041][-5.0519276 -5.4584322 -5.1863065 -3.5085902 -1.499289 1.0703754 2.4665885 2.7740645 3.2714276 2.1764517 1.5568371 -1.7658446 -5.5898566 -7.2903295 -8.5587873][-3.284445 -2.4519036 -2.1393192 -1.7085376 -0.85513163 1.2207441 2.0569453 2.0590277 1.7949753 1.1550102 0.33387947 -3.3517392 -6.4917464 -7.6290512 -9.2983723][-4.7981758 -4.6643844 -4.0354452 -4.5507617 -4.4652977 -3.6409295 -3.3771331 -2.9533947 -3.0702908 -3.3053811 -3.5812325 -7.0217409 -8.96404 -9.28983 -9.3732176][-9.4013405 -9.0307093 -8.0280819 -7.9456973 -8.30282 -8.5168238 -8.68314 -9.0322037 -9.4657345 -8.9755383 -8.5367842 -10.005519 -9.809906 -9.8131285 -9.55036][-11.401165 -10.750977 -10.204964 -11.012264 -11.656275 -11.432617 -10.797134 -10.263687 -10.243269 -10.097963 -9.677145 -9.3589649 -8.7535849 -7.02988 -6.0744276][-9.4667253 -8.9623966 -8.3519688 -8.02007 -8.7040482 -9.5576124 -9.69262 -8.9700718 -8.2670631 -7.9261079 -7.5042424 -6.2008691 -5.1979547 -4.617485 -4.3251305][-8.7956924 -8.6478 -7.3405614 -6.2847018 -4.813364 -4.8236046 -5.3032613 -5.587163 -5.46613 -5.4538341 -5.6217628 -6.2852311 -6.5488667 -5.4632168 -5.2937169]]...]
INFO - root - 2017-12-15 14:46:33.926116: step 24310, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 55h:15m:06s remains)
INFO - root - 2017-12-15 14:46:40.628675: step 24320, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 55h:22m:13s remains)
INFO - root - 2017-12-15 14:46:47.151115: step 24330, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 54h:57m:50s remains)
INFO - root - 2017-12-15 14:46:53.777806: step 24340, loss = 0.12, batch loss = 0.07 (11.4 examples/sec; 0.700 sec/batch; 59h:57m:12s remains)
INFO - root - 2017-12-15 14:47:00.373652: step 24350, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 56h:25m:58s remains)
INFO - root - 2017-12-15 14:47:06.975692: step 24360, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 56h:11m:03s remains)
INFO - root - 2017-12-15 14:47:13.492732: step 24370, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 56h:01m:13s remains)
INFO - root - 2017-12-15 14:47:20.092370: step 24380, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 55h:01m:53s remains)
INFO - root - 2017-12-15 14:47:26.655581: step 24390, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.648 sec/batch; 55h:26m:07s remains)
INFO - root - 2017-12-15 14:47:33.170491: step 24400, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 58h:22m:38s remains)
2017-12-15 14:47:33.719752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6314158 -5.8717527 -5.58392 -4.3771334 -4.5169315 -3.9435875 -2.5263186 -1.3870902 -0.90939283 0.016306877 -1.3189569 -5.3820615 -7.9638062 -9.3606834 -8.3696671][-5.7430491 -7.1716814 -8.080431 -7.1140757 -5.6024232 -4.2795877 -3.0793245 -1.1245027 -0.98744154 -1.6218238 -2.2253377 -6.1718092 -8.8476067 -8.9693995 -8.8737831][-5.4585538 -6.4242668 -7.4239073 -7.3821039 -7.1339445 -5.3866534 -2.8837123 -1.736181 -1.9542294 -2.7459266 -4.0691471 -8.2475348 -10.366295 -10.476461 -9.3802547][-4.8283949 -4.8719964 -5.2812457 -5.6182294 -5.9450517 -4.9631324 -3.5388432 -2.5042565 -2.297657 -3.0166106 -4.3074431 -9.5720291 -12.069628 -10.795418 -9.1628656][-5.1717114 -4.9063187 -5.0328207 -4.5992937 -4.2771344 -2.3923352 -1.0929151 -1.9436314 -2.6629374 -3.6126823 -4.8010015 -8.3602905 -11.02914 -11.1126 -9.4099][-8.4909172 -6.4510903 -5.7623167 -4.08924 -2.8405931 -1.0679994 0.51809788 0.38844204 -0.50667715 -3.1487458 -4.8128061 -8.5196781 -10.430752 -10.443493 -9.1550255][-8.16391 -6.9477692 -6.253654 -4.0408716 -2.7407904 0.11035109 3.017345 3.5359483 2.3499808 -0.71782684 -3.8492901 -7.7304468 -9.2418518 -8.8523417 -8.7856951][-6.6068563 -5.9601536 -5.682445 -3.0244627 -1.5778222 1.2728128 4.1905551 4.3626189 3.9669719 1.7056985 -1.6681266 -7.134819 -9.84906 -9.1060944 -7.9717159][-5.3717737 -4.0770507 -3.9489295 -1.471488 -0.6368103 1.6974368 3.9372239 4.3841891 4.3127313 1.1068025 -1.5626979 -6.4744616 -10.440372 -11.002541 -9.4517765][-4.5978079 -3.7686036 -3.6952443 -1.9575965 -1.0631485 1.0078206 1.7114496 1.7897787 2.0418029 0.58655691 -1.6621747 -7.0084639 -10.546137 -11.646784 -10.85338][-6.5858479 -6.0660796 -6.2653236 -5.2491403 -4.5053754 -3.3459058 -2.1931922 -1.9840047 -2.0794098 -2.9243367 -3.8871498 -9.0991507 -11.345581 -11.751121 -10.974941][-10.259747 -9.2371054 -8.2611237 -7.7173271 -7.5527067 -7.2248363 -7.4611664 -7.7650194 -7.821804 -7.6772342 -7.5282793 -8.9576788 -9.5160828 -10.565676 -9.4786263][-13.859806 -12.194695 -9.7872124 -8.9319315 -9.1229343 -9.3150787 -9.4039831 -8.26282 -8.183075 -8.29017 -8.55622 -9.7381582 -10.250753 -10.108047 -8.1806574][-12.223488 -10.988424 -8.7447109 -7.1029077 -6.863925 -6.9602628 -7.5434837 -7.6183519 -7.0171223 -7.8078165 -8.5511875 -7.3372741 -6.9566813 -9.0149775 -9.0726051][-8.0265942 -6.9487157 -6.2730136 -5.3038616 -5.3649597 -5.9420133 -5.5914583 -5.3199968 -5.2084737 -5.4684176 -5.9070182 -8.7657547 -8.9628839 -8.032198 -8.6309614]]...]
INFO - root - 2017-12-15 14:47:40.267305: step 24410, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 54h:24m:56s remains)
INFO - root - 2017-12-15 14:47:46.829135: step 24420, loss = 0.23, batch loss = 0.19 (12.3 examples/sec; 0.651 sec/batch; 55h:42m:53s remains)
INFO - root - 2017-12-15 14:47:53.505468: step 24430, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 57h:44m:56s remains)
INFO - root - 2017-12-15 14:48:00.093641: step 24440, loss = 0.18, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 54h:25m:23s remains)
INFO - root - 2017-12-15 14:48:06.685568: step 24450, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 56h:43m:10s remains)
INFO - root - 2017-12-15 14:48:13.331713: step 24460, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 57h:35m:02s remains)
INFO - root - 2017-12-15 14:48:19.957526: step 24470, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 57h:09m:07s remains)
INFO - root - 2017-12-15 14:48:26.507873: step 24480, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 55h:09m:27s remains)
INFO - root - 2017-12-15 14:48:33.104720: step 24490, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 56h:03m:32s remains)
INFO - root - 2017-12-15 14:48:39.727278: step 24500, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 56h:17m:32s remains)
2017-12-15 14:48:40.284475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4357648 -3.0038607 -3.308373 -3.5818925 -4.4631243 -5.1337261 -5.6428089 -5.5530148 -5.15673 -3.6939566 -2.2921169 -3.5167229 -4.3525038 -5.7718134 -5.7836113][-2.3251414 -3.5915687 -3.8866844 -4.973341 -6.8201332 -8.255723 -8.91505 -8.651247 -8.49748 -6.8731747 -5.0617347 -5.2805171 -6.5363159 -7.4222493 -6.1806426][-2.2067552 -2.6674469 -3.7870679 -4.8523293 -5.744081 -6.6030869 -7.3919969 -7.5292344 -7.4790196 -7.1399035 -7.0147228 -7.2047977 -7.878377 -8.959075 -8.392396][-3.9513133 -3.9749064 -3.9478741 -4.8752804 -5.690701 -5.4019547 -4.6547527 -5.2007208 -5.6603293 -4.9945121 -4.8365855 -6.8926291 -9.4038887 -9.8834648 -8.9735947][-5.3840489 -6.6495676 -7.7822914 -6.3269486 -5.2877831 -3.258287 -1.287097 -2.3450403 -3.9585881 -4.714467 -5.5014563 -7.115346 -9.3307838 -10.763625 -10.24184][-7.6371765 -7.3349924 -6.3870897 -5.5030127 -3.0895848 0.063289165 2.8515363 3.1223712 2.6612611 0.0085334778 -3.4373634 -5.323451 -7.3439765 -9.3192892 -9.932745][-7.3636928 -8.2142124 -7.6569018 -5.0954428 -2.6393919 2.4764485 6.9411139 7.2823997 7.1036887 3.4969449 -0.352396 -2.8725541 -5.9870105 -7.7857862 -7.4809337][-8.8676615 -8.9217472 -7.7627726 -5.2548561 -2.4351368 2.4286628 7.0679297 8.0217228 7.74991 4.4665351 0.73889303 -2.8964388 -6.033649 -7.2951822 -7.146616][-8.1792345 -8.49256 -7.701201 -5.0057526 -2.5448484 1.3163514 4.5541787 6.9351087 7.4905381 3.5908122 0.22721815 -3.4656069 -6.9056454 -7.0813923 -6.7209516][-7.8869514 -8.8180523 -8.1988821 -5.4356155 -3.9768503 -0.63771725 2.6744428 3.8888812 4.4199271 2.5741467 -0.44553041 -3.8170528 -6.2489367 -7.748776 -8.1665678][-11.529699 -11.949853 -10.550607 -8.4088392 -7.2943754 -4.66856 -2.541575 -1.495544 -0.625906 -1.752367 -3.9355183 -6.9777641 -8.1809425 -8.6804171 -7.8951983][-14.060928 -14.674768 -13.192228 -10.644929 -9.7214222 -7.6653142 -6.5131111 -6.0064635 -5.4796195 -5.3474956 -6.2526097 -7.7988882 -8.2384491 -9.0449142 -8.2196035][-12.723932 -12.83481 -12.522296 -10.664919 -9.5769739 -8.3843193 -6.9454236 -6.5357294 -5.986361 -5.9833426 -6.6272979 -7.3862948 -7.3841982 -7.0844183 -5.7836204][-10.530745 -10.552847 -9.962225 -8.348835 -7.7638106 -6.5373888 -5.9539704 -5.6036558 -5.3593516 -5.6269565 -6.1585269 -5.9422927 -5.8721447 -5.4213262 -4.4275455][-6.4922266 -6.8387895 -6.5731883 -5.4538078 -4.3270874 -3.1365573 -2.7057972 -3.5998092 -3.875061 -4.0773096 -4.3499055 -5.4319072 -6.5656209 -7.1465006 -6.9110689]]...]
INFO - root - 2017-12-15 14:48:46.784383: step 24510, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 54h:14m:53s remains)
INFO - root - 2017-12-15 14:48:53.343120: step 24520, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 56h:27m:48s remains)
INFO - root - 2017-12-15 14:48:59.857598: step 24530, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.646 sec/batch; 55h:15m:58s remains)
INFO - root - 2017-12-15 14:49:06.456222: step 24540, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 56h:28m:53s remains)
INFO - root - 2017-12-15 14:49:13.129195: step 24550, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.695 sec/batch; 59h:28m:34s remains)
INFO - root - 2017-12-15 14:49:19.746801: step 24560, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 58h:22m:38s remains)
INFO - root - 2017-12-15 14:49:26.367956: step 24570, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 57h:26m:58s remains)
INFO - root - 2017-12-15 14:49:33.005431: step 24580, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 56h:53m:44s remains)
INFO - root - 2017-12-15 14:49:39.653498: step 24590, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 57h:11m:54s remains)
INFO - root - 2017-12-15 14:49:46.176559: step 24600, loss = 0.19, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 55h:22m:19s remains)
2017-12-15 14:49:46.645672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.565589 -8.49524 -8.1367178 -8.0604248 -8.7088156 -9.2700624 -9.5492239 -9.0937767 -9.0845013 -8.56225 -7.8824472 -8.5761032 -8.8464756 -6.3068848 -3.3884625][-10.16595 -9.68043 -8.6427279 -7.7080727 -8.4104843 -8.962883 -9.0624294 -9.3927288 -10.015509 -9.7234106 -8.9571781 -9.5983353 -9.4234867 -7.436554 -5.7839203][-7.3790617 -7.7723351 -7.8308678 -7.3471251 -8.4202442 -8.6790085 -8.6736441 -8.0411081 -7.5714316 -8.05243 -8.6214952 -9.4094706 -9.2888422 -7.4834313 -5.8809333][-8.1778555 -8.00783 -6.6435728 -5.9438519 -7.2443314 -7.1976709 -6.3928604 -6.4920716 -7.0099564 -6.6140704 -6.6171718 -8.72622 -8.9074926 -7.358151 -5.8585844][-10.601469 -10.084481 -7.9334464 -5.4216552 -4.7743726 -3.2687893 -2.7630556 -3.9693685 -5.0136323 -5.3851929 -5.5701528 -7.0850253 -7.6413279 -7.1260924 -5.8218918][-12.876234 -11.293552 -8.6975784 -5.6987658 -2.8207192 0.099648476 1.4954782 1.0226088 0.36088133 -2.1570363 -4.5898032 -6.4595289 -6.7401953 -6.0793829 -4.8679237][-12.386091 -10.903718 -7.6906204 -3.4632156 -0.95615005 1.5536761 3.4991031 3.8342805 3.6662221 0.98214388 -1.501195 -4.4085855 -5.8044934 -4.762248 -3.729805][-12.03201 -9.6092377 -7.1718183 -2.4515402 0.83924389 3.6952615 5.1046071 3.8491893 3.63347 2.5994563 1.3140306 -3.0787067 -5.6555438 -4.7105365 -4.2249432][-10.508911 -8.16295 -6.1334848 -1.4424391 1.1395593 3.3808722 4.5145736 3.9175935 3.2827716 1.3155046 0.051252365 -3.0367711 -5.081306 -4.8390064 -4.8525314][-8.4245567 -6.7272491 -5.1226125 -1.54775 0.36814356 1.1020064 1.8807726 1.8783803 1.4241538 0.248065 -0.50490808 -3.4419427 -4.8969808 -4.823822 -5.6861372][-9.0207405 -7.622108 -6.1994243 -3.0578895 -1.915235 -1.5175233 -0.82210827 -0.62094831 -0.60063934 -1.0399332 -2.3466725 -5.4415984 -6.5856133 -6.2977219 -6.1831951][-11.136787 -10.163198 -8.2266388 -6.821126 -6.9652858 -6.1537838 -5.3552322 -5.3976912 -5.5499687 -5.6049676 -6.1793866 -7.8467393 -8.6792107 -8.1805534 -7.7306657][-12.92547 -11.948314 -10.288295 -7.9876719 -7.1186948 -6.6791115 -7.0548425 -6.8001738 -6.7849197 -7.1754689 -7.2456918 -7.1213455 -6.87204 -6.2567811 -6.2489977][-11.036731 -10.041879 -8.6121979 -7.3643532 -6.3019872 -6.05294 -5.9187174 -6.0681272 -6.1991396 -6.472043 -6.1552348 -5.2841415 -4.8327174 -3.1273673 -2.7785227][-7.433713 -6.5113139 -5.1118803 -4.31882 -3.4150808 -3.0022411 -3.3586466 -3.9599204 -4.6265674 -4.7710781 -4.5394049 -5.1247778 -5.6468143 -4.4757175 -3.7202177]]...]
INFO - root - 2017-12-15 14:49:53.296679: step 24610, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 58h:21m:17s remains)
INFO - root - 2017-12-15 14:49:59.910262: step 24620, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 57h:25m:33s remains)
INFO - root - 2017-12-15 14:50:06.536064: step 24630, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.681 sec/batch; 58h:13m:55s remains)
INFO - root - 2017-12-15 14:50:13.053591: step 24640, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 54h:57m:43s remains)
INFO - root - 2017-12-15 14:50:19.692004: step 24650, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 55h:20m:30s remains)
INFO - root - 2017-12-15 14:50:26.427031: step 24660, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 58h:04m:30s remains)
INFO - root - 2017-12-15 14:50:33.106753: step 24670, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 56h:02m:55s remains)
INFO - root - 2017-12-15 14:50:39.665280: step 24680, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 55h:13m:22s remains)
INFO - root - 2017-12-15 14:50:46.302999: step 24690, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 55h:05m:02s remains)
INFO - root - 2017-12-15 14:50:52.942044: step 24700, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 58h:02m:20s remains)
2017-12-15 14:50:53.420802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.88698 -6.4285784 -6.4104795 -5.2423053 -5.493959 -6.4786353 -6.8997755 -7.3026462 -7.9169359 -7.7234707 -6.5197625 -7.5925822 -7.6798158 -6.748528 -5.8890004][-6.5978312 -6.0437579 -4.4274778 -3.4421921 -4.017735 -4.8084273 -5.9301672 -6.6816835 -7.2059236 -6.2883191 -5.3593397 -6.2537909 -6.1254854 -6.6459813 -6.0734916][-3.8554008 -5.2833672 -5.45549 -3.6484258 -3.979923 -4.6946692 -5.1666965 -6.0176878 -7.1485868 -7.0530386 -6.3410664 -7.4436197 -7.41493 -7.9333858 -8.0886154][-5.174922 -5.9688573 -6.2134724 -5.1726284 -4.7670007 -3.658999 -3.5049779 -4.4745612 -5.2807031 -5.1853404 -5.3771992 -7.3705416 -7.4109006 -8.1772175 -8.817194][-7.4521646 -8.1388292 -7.612402 -6.2519865 -4.9034996 -2.8618333 -1.4132442 -2.5373259 -3.2600789 -3.2197402 -4.5715981 -7.327569 -8.1730165 -8.5067244 -8.7315311][-10.932331 -10.361238 -8.27836 -5.57424 -2.1055939 1.1046791 2.5564208 1.8093657 0.59489489 -1.088769 -2.6147063 -5.003129 -6.3113179 -7.1142054 -6.8255606][-12.742001 -10.365572 -7.1169949 -3.0886302 0.11034679 3.6019673 6.6855845 6.2672257 3.9820323 0.74387836 -1.9377232 -5.3979816 -7.858779 -8.1143818 -7.8987741][-12.781525 -10.776155 -7.7039242 -2.1048591 1.6707034 4.1954913 6.8741403 6.6504149 5.5432982 3.0247064 -1.4205017 -6.4095106 -8.5891247 -9.67696 -9.5035343][-10.896467 -9.0070763 -6.2689686 -2.1942117 1.0269165 3.2819791 4.518312 3.4940724 2.8579459 1.2073636 -1.7716584 -6.8454742 -9.844532 -11.632524 -12.231112][-8.9410667 -6.8565989 -4.841958 -2.3581364 -1.3439293 -0.062585831 1.4755177 0.61014557 -0.89617634 -2.9289474 -5.1204109 -8.6640625 -10.137567 -11.630659 -12.291936][-12.722387 -10.29003 -8.8846083 -6.1621475 -5.8584232 -6.2749853 -5.1674423 -5.5666409 -6.1276145 -7.4144359 -9.1449556 -12.372354 -13.695627 -13.335247 -13.064013][-14.433136 -13.120138 -12.119738 -10.32379 -10.193951 -9.8232145 -9.6989393 -9.7926025 -8.9656067 -9.8852673 -11.275681 -12.428338 -12.56277 -13.339851 -12.149281][-16.702621 -14.505749 -13.22271 -12.350867 -12.197599 -12.048918 -12.042164 -11.97354 -11.723154 -11.242473 -11.154303 -11.838853 -10.434 -9.7397671 -9.0062466][-13.29447 -11.392792 -9.8409777 -7.6516795 -7.06126 -7.2407308 -8.0365572 -8.395565 -8.7073383 -9.0237885 -9.1084633 -8.27766 -7.8296738 -7.2022934 -5.932323][-8.0001745 -6.8096523 -6.0079823 -4.5350351 -2.8779063 -2.6278708 -2.7416816 -3.527283 -4.1180859 -4.4253993 -4.9716554 -5.614615 -5.3241777 -4.9974256 -5.4348783]]...]
INFO - root - 2017-12-15 14:51:00.012433: step 24710, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 55h:20m:55s remains)
INFO - root - 2017-12-15 14:51:06.676869: step 24720, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 56h:49m:56s remains)
INFO - root - 2017-12-15 14:51:13.301488: step 24730, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 55h:08m:12s remains)
INFO - root - 2017-12-15 14:51:19.854536: step 24740, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 56h:43m:33s remains)
INFO - root - 2017-12-15 14:51:26.430642: step 24750, loss = 0.18, batch loss = 0.13 (11.5 examples/sec; 0.694 sec/batch; 59h:19m:29s remains)
INFO - root - 2017-12-15 14:51:33.028602: step 24760, loss = 0.22, batch loss = 0.17 (12.5 examples/sec; 0.639 sec/batch; 54h:38m:45s remains)
INFO - root - 2017-12-15 14:51:39.709873: step 24770, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 54h:56m:43s remains)
INFO - root - 2017-12-15 14:51:46.277791: step 24780, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.634 sec/batch; 54h:09m:51s remains)
INFO - root - 2017-12-15 14:51:52.825142: step 24790, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 57h:41m:21s remains)
INFO - root - 2017-12-15 14:51:59.369902: step 24800, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 54h:59m:02s remains)
2017-12-15 14:51:59.864652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3307905 -6.4951572 -5.8826923 -5.7817783 -5.9187112 -5.846664 -5.550364 -4.6351933 -4.0594735 -4.211978 -4.0540347 -5.3751936 -7.3172684 -7.2075934 -6.7676992][-6.1153097 -5.6713147 -5.9427328 -5.773509 -6.1879578 -6.2629766 -5.7496638 -4.8513794 -4.4750586 -4.4208927 -4.6395073 -6.1631265 -9.0168409 -9.2510061 -8.3580742][-4.931015 -5.4911714 -5.4273663 -4.79091 -5.5752707 -5.6627564 -4.9517584 -4.2209167 -4.3505182 -4.1379881 -4.5373173 -5.9793768 -9.1773443 -10.000616 -9.8891048][-4.7066221 -4.6033154 -4.3255677 -4.2622676 -5.3340874 -5.61369 -4.7322721 -4.2174306 -3.8752446 -3.9665725 -4.1271038 -5.3963723 -8.3426914 -8.42454 -7.8463111][-4.72652 -5.0606894 -5.1527882 -4.053299 -4.6326184 -3.805475 -2.9159276 -3.5422513 -4.054595 -3.935688 -4.1731553 -5.1674318 -7.06481 -6.4242682 -6.5395555][-5.6821918 -5.9559712 -4.4891591 -3.4050534 -2.8086786 -1.0299773 0.39630795 0.42607784 0.36128092 -1.0710201 -2.2806141 -3.3361292 -5.6415138 -5.8551593 -5.8462839][-7.0024848 -6.7477474 -6.03625 -3.3038015 -1.2453446 0.93465567 2.2798638 2.7901368 3.6901164 2.0071931 -0.043363094 -1.9637232 -4.6113605 -5.0191488 -4.8471494][-5.833899 -5.2656918 -3.6742325 -0.95769215 0.68320036 2.8255486 3.5775685 3.9476266 3.7398925 2.0906873 0.96806 -1.0063348 -3.5463336 -3.9988096 -3.2590642][-3.2530515 -2.7321906 -1.1603756 0.69767475 2.2669907 3.698051 3.5183215 3.2331634 1.9692888 0.72792578 -0.32622004 -2.1341321 -3.8925824 -3.2386026 -1.5662704][-2.1991174 -2.1549275 -1.1060266 -0.15038443 0.98770809 1.5790267 1.80932 0.86972332 -0.33403826 -1.2511315 -2.2278328 -2.9720712 -4.4497633 -3.2766623 -2.313405][-7.2327352 -6.4649816 -4.8158092 -2.9919789 -2.546447 -2.2033637 -2.3583183 -3.0645959 -3.2974787 -3.8932724 -4.1879377 -5.2877092 -5.78864 -5.05725 -3.3921311][-12.623299 -11.72172 -10.242279 -7.9550018 -6.5723543 -5.8098688 -6.9080472 -7.6276569 -8.379962 -8.7221632 -7.6297293 -7.0104179 -5.6826234 -4.4895616 -2.6757593][-12.420512 -11.86062 -10.612797 -8.4439335 -7.284276 -6.6951733 -8.0710754 -9.6989746 -11.133286 -10.819427 -9.6046734 -8.0488291 -6.2889686 -4.386332 -2.3708243][-9.0483131 -8.4456139 -7.0104127 -5.8979897 -5.2289147 -5.2057667 -6.651619 -7.9083967 -9.4968252 -9.63027 -10.056431 -7.8567157 -6.4629188 -3.7902646 -2.613596][-5.4631658 -4.8530359 -3.3950644 -3.8855047 -3.10622 -3.3351486 -3.6593802 -5.0073838 -5.8765297 -6.1848235 -6.3111806 -5.3571134 -5.7125869 -5.409225 -5.0870118]]...]
INFO - root - 2017-12-15 14:52:06.519193: step 24810, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.675 sec/batch; 57h:40m:18s remains)
INFO - root - 2017-12-15 14:52:13.183203: step 24820, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.692 sec/batch; 59h:08m:19s remains)
INFO - root - 2017-12-15 14:52:19.824650: step 24830, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.629 sec/batch; 53h:44m:16s remains)
INFO - root - 2017-12-15 14:52:26.372504: step 24840, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 56h:51m:51s remains)
INFO - root - 2017-12-15 14:52:32.935410: step 24850, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 55h:53m:56s remains)
INFO - root - 2017-12-15 14:52:39.479043: step 24860, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.685 sec/batch; 58h:31m:32s remains)
INFO - root - 2017-12-15 14:52:46.024759: step 24870, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 56h:41m:32s remains)
INFO - root - 2017-12-15 14:52:52.600811: step 24880, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.684 sec/batch; 58h:24m:30s remains)
INFO - root - 2017-12-15 14:52:59.186869: step 24890, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 55h:35m:50s remains)
INFO - root - 2017-12-15 14:53:05.773740: step 24900, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 56h:47m:05s remains)
2017-12-15 14:53:06.302039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3639469 -4.48676 -3.2231829 -2.8901381 -3.6856954 -4.2477632 -4.237565 -3.8793762 -3.3854625 -3.4907234 -3.4694564 -4.9460979 -7.2321815 -8.1978531 -8.7607822][-6.4240584 -4.4150095 -2.5481765 -1.956445 -2.8664379 -4.3808546 -4.6234331 -4.2859573 -3.7364869 -3.4938204 -4.064949 -5.8375392 -7.8798952 -7.8879838 -7.1618352][-5.0037146 -3.9532526 -3.2684662 -2.0052221 -1.8657854 -2.2245374 -2.5126407 -2.5835068 -2.267658 -2.6243269 -3.1649199 -5.2069449 -7.4277043 -7.7932472 -7.8738403][-5.7607608 -4.7236319 -4.4492435 -3.6379316 -3.4163983 -3.0168507 -2.5470853 -2.4104495 -2.4381309 -2.6035821 -3.117873 -4.877213 -6.5790544 -7.3601618 -6.9524984][-5.7614121 -5.6704149 -5.4670658 -3.7682235 -2.8888068 -1.6013765 -0.97994375 -2.4937947 -3.8064349 -3.6013658 -3.7595158 -5.4709845 -7.3840375 -8.2338276 -6.7900105][-6.8513708 -5.7830672 -4.0059829 -0.63207626 1.9056907 4.2369876 5.5472264 2.9388452 0.89850521 -1.3125763 -3.4703858 -3.7606082 -5.2760053 -6.4311805 -5.6120715][-10.084626 -7.9060736 -5.1493225 0.020596027 3.1251826 5.7777133 8.2383938 7.5486751 6.1290755 1.7186651 -2.3069394 -4.0475674 -6.1692023 -6.542654 -5.8650908][-11.487471 -9.2243881 -6.4236379 -0.79082632 2.7852836 5.8642259 8.518734 7.9521346 7.39131 3.6600995 -0.75290155 -4.3676457 -7.8689451 -8.2143764 -6.5720778][-7.8759003 -8.3382616 -7.4282889 -3.8096609 -0.798522 3.3969178 6.2335114 4.5571313 3.1077228 0.53041553 -2.3584671 -6.068964 -10.122412 -11.726128 -10.950348][-8.3696976 -8.6053047 -9.2050495 -6.6484976 -4.5012593 -2.4748757 -0.76663637 -0.11004353 0.094082355 -2.2468734 -4.8889966 -7.2096615 -10.536816 -12.351244 -12.532019][-13.246699 -13.432051 -13.51145 -11.083509 -9.7201471 -9.0632877 -8.3249741 -7.1855669 -6.9156404 -7.66541 -8.7059269 -11.147793 -13.729486 -13.947947 -12.899025][-16.751905 -16.821163 -15.890463 -13.654947 -13.838997 -13.021639 -12.345837 -11.934504 -11.637579 -10.8883 -10.15158 -11.606368 -13.405661 -14.098036 -13.408246][-16.647514 -15.491188 -14.240679 -12.585169 -12.408295 -12.293082 -12.797133 -13.188492 -12.79449 -11.603762 -10.785978 -10.25218 -10.355324 -10.892673 -10.01478][-13.106524 -12.976055 -11.332399 -8.4010525 -7.1673574 -8.371109 -10.076799 -9.9661627 -9.8325443 -9.0943813 -8.7051716 -7.5733223 -7.2712469 -6.7483196 -6.2167153][-9.7350674 -9.0207291 -8.2666683 -4.75364 -4.1757755 -5.1521277 -5.121253 -6.2854552 -7.5654039 -6.1234655 -5.101469 -4.9104857 -5.4232655 -6.33706 -6.445837]]...]
INFO - root - 2017-12-15 14:53:12.861157: step 24910, loss = 0.22, batch loss = 0.17 (12.2 examples/sec; 0.658 sec/batch; 56h:14m:03s remains)
INFO - root - 2017-12-15 14:53:19.390489: step 24920, loss = 0.11, batch loss = 0.07 (11.6 examples/sec; 0.688 sec/batch; 58h:46m:58s remains)
INFO - root - 2017-12-15 14:53:25.941091: step 24930, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 56h:46m:06s remains)
INFO - root - 2017-12-15 14:53:32.559127: step 24940, loss = 0.19, batch loss = 0.14 (11.7 examples/sec; 0.686 sec/batch; 58h:35m:36s remains)
INFO - root - 2017-12-15 14:53:39.005361: step 24950, loss = 0.15, batch loss = 0.11 (12.8 examples/sec; 0.627 sec/batch; 53h:35m:29s remains)
INFO - root - 2017-12-15 14:53:45.498395: step 24960, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 58h:56m:34s remains)
INFO - root - 2017-12-15 14:53:52.066433: step 24970, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 58h:29m:17s remains)
INFO - root - 2017-12-15 14:53:58.644719: step 24980, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 55h:53m:57s remains)
INFO - root - 2017-12-15 14:54:05.195586: step 24990, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 56h:12m:36s remains)
INFO - root - 2017-12-15 14:54:11.839267: step 25000, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.647 sec/batch; 55h:18m:21s remains)
2017-12-15 14:54:12.364533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1383772 -7.5310583 -7.6423464 -7.2432022 -6.9339871 -7.1936183 -8.1702452 -8.9524212 -8.4203911 -7.3251462 -5.996726 -6.4700103 -8.23356 -7.9380937 -7.8387518][-4.9026155 -5.8879442 -5.7359529 -5.5632868 -6.1332545 -6.4727488 -7.6771417 -8.8167315 -8.806654 -7.2781467 -5.5857592 -5.7590637 -8.4129 -9.7558537 -9.48757][-3.0276482 -3.9753513 -4.3937473 -5.3327789 -5.7348886 -6.0265064 -6.533186 -6.8727455 -7.0796227 -6.8026943 -6.0651865 -6.1329875 -8.4097977 -9.0653868 -9.274251][-3.268424 -4.0639219 -4.6782017 -5.225554 -5.1633992 -5.2081928 -5.3055716 -5.1889877 -4.4828849 -4.4389238 -5.0607643 -5.613306 -8.0614138 -8.3794 -7.8473063][-4.1750917 -5.4858065 -6.3259635 -5.6697254 -4.5825667 -3.3194344 -2.3113191 -2.7563927 -3.3740535 -3.5604994 -3.7244873 -4.907959 -7.0572572 -6.9090328 -6.5231113][-4.6720543 -5.1545167 -5.286171 -4.5986862 -2.68189 -0.39863253 0.74989748 0.15739012 -0.41255522 -0.9189167 -1.9180105 -3.0733044 -5.626688 -5.6411467 -5.1348224][-5.1744423 -4.6055164 -3.7718196 -2.1201315 -0.41901731 1.237062 3.1260915 3.3813634 2.4830403 0.68501186 -2.0963254 -3.8991098 -6.5781693 -7.0263038 -6.5817218][-6.171608 -4.6257133 -3.397656 -1.7272611 -0.1112113 1.6712675 3.2730451 2.7830968 2.8201165 1.9329038 0.16915035 -3.3870106 -8.8011475 -9.205369 -7.7858105][-6.9681134 -5.4569211 -3.5624945 -1.2007408 -0.64122343 1.1270747 2.5725188 2.1262093 2.6858296 1.996944 0.83710766 -2.2354407 -7.1010456 -9.3393326 -10.76663][-7.174417 -6.3264685 -4.7278562 -2.1861002 -1.5560527 -0.36684036 0.55337906 0.70268583 0.88891172 1.3794045 1.2392106 -1.3540182 -6.1076779 -9.2643108 -11.298014][-9.6547661 -9.1683083 -8.49087 -6.2325182 -5.6462021 -4.6564727 -3.4155195 -2.9237447 -2.7638469 -2.8151131 -3.2276878 -5.0168624 -8.5714645 -10.458818 -11.685774][-12.133512 -12.05262 -12.1272 -11.374445 -11.389885 -9.6708241 -8.2121439 -7.673162 -7.0968709 -6.8052216 -6.7518058 -9.3459663 -12.234245 -12.872473 -12.278048][-13.31041 -13.248918 -12.696471 -11.726643 -12.224387 -11.96805 -11.543644 -9.507638 -7.7503281 -7.0668159 -7.2836847 -10.378439 -12.650272 -14.257956 -12.974623][-12.903522 -12.921943 -12.361303 -11.307468 -11.547595 -12.068455 -11.128233 -9.8212166 -9.3322258 -8.4940252 -8.7015991 -9.8646564 -10.795235 -12.481514 -12.447063][-9.6773405 -10.477795 -10.892683 -9.8260431 -8.8062134 -9.0204391 -7.9887547 -7.9411993 -8.19712 -8.6278057 -9.9341812 -11.236984 -11.162294 -11.614296 -11.767256]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 14:54:19.710592: step 25010, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 54h:45m:17s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 14:54:26.263615: step 25020, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 55h:26m:46s remains)
INFO - root - 2017-12-15 14:54:32.758716: step 25030, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 55h:33m:48s remains)
INFO - root - 2017-12-15 14:54:39.249679: step 25040, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 55h:08m:54s remains)
INFO - root - 2017-12-15 14:54:45.820225: step 25050, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 58h:01m:21s remains)
INFO - root - 2017-12-15 14:54:52.450340: step 25060, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 55h:20m:31s remains)
INFO - root - 2017-12-15 14:54:58.953234: step 25070, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 56h:04m:05s remains)
INFO - root - 2017-12-15 14:55:05.551078: step 25080, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 55h:30m:12s remains)
INFO - root - 2017-12-15 14:55:12.143002: step 25090, loss = 0.16, batch loss = 0.11 (11.5 examples/sec; 0.695 sec/batch; 59h:19m:33s remains)
INFO - root - 2017-12-15 14:55:18.769884: step 25100, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 55h:06m:16s remains)
2017-12-15 14:55:19.263967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-12.831606 -13.498924 -12.915199 -12.901587 -13.655077 -13.660582 -12.509271 -9.944582 -8.44314 -8.1978264 -9.3011589 -11.408483 -13.877127 -14.049423 -11.556804][-10.748237 -10.493957 -10.693767 -11.087982 -12.121904 -12.925507 -12.591068 -11.227945 -10.433393 -9.17369 -9.7864685 -11.396836 -13.728533 -14.460344 -13.192047][-4.8741846 -6.0875945 -7.5094948 -9.1662741 -10.690263 -10.546343 -9.7227993 -8.9309187 -8.3197708 -7.2673807 -6.9046092 -7.9958048 -11.257241 -12.968984 -13.12648][-2.6394448 -2.8300598 -3.7495356 -5.4094105 -6.6305685 -6.3285584 -5.4181991 -5.6237903 -6.4447904 -6.7312346 -6.6893921 -7.2596288 -9.1990337 -10.46231 -10.985399][-4.2154951 -4.5125732 -5.5810356 -4.4271917 -3.4167104 -1.6905532 -0.010419369 -1.4835515 -3.2940764 -3.8582418 -4.7819462 -6.4617934 -8.9400988 -10.447226 -9.7463131][-6.977272 -6.9434509 -6.69746 -4.906033 -3.3322556 0.3587923 3.7738423 3.51084 1.9908051 -0.29001141 -2.1620994 -3.8830581 -6.8752956 -8.2933035 -7.9325695][-8.6355286 -8.4592323 -7.408577 -4.6744037 -1.6851449 2.03902 5.2884936 6.0604024 6.8098712 4.29386 1.0599818 -1.661561 -5.22267 -7.1575804 -6.5744758][-7.8442659 -7.2670074 -6.1373382 -3.3275483 -0.42041254 2.7512259 5.4111505 5.7203431 5.8224187 4.1548142 1.4226928 -1.4190722 -4.5792475 -5.7492652 -4.5778551][-4.7804604 -3.8754921 -3.1713924 -1.1888618 0.010007858 1.5857439 2.8924317 3.95079 4.3094525 2.6117768 0.3621707 -2.6210296 -5.4819217 -6.2341609 -4.69759][-4.3222561 -3.0385766 -1.2668109 -0.45513582 -0.19994879 0.64934683 1.6141829 1.7400675 1.0873446 -0.44657564 -1.8002241 -3.7465956 -6.1635532 -6.67271 -5.522089][-7.6593056 -5.3820043 -4.7031727 -4.4417534 -4.5436511 -3.3303583 -1.3978772 -0.7867012 -2.030684 -4.0765991 -4.644187 -6.4608226 -8.3050261 -8.1788206 -6.2177186][-10.15506 -8.26345 -7.1237488 -6.9120479 -7.2177949 -6.0175662 -4.8697076 -4.3194613 -5.1774316 -5.7545033 -5.7286544 -6.0158114 -6.6757212 -7.1299982 -6.5254331][-9.4462938 -9.2126532 -8.5166044 -8.1134071 -7.1834025 -5.5129232 -4.3692417 -4.1955228 -4.9204245 -6.1041803 -6.3675113 -6.3600516 -6.3152862 -5.9242926 -4.8485241][-8.7140989 -7.9752536 -8.5458622 -7.7805347 -7.220612 -5.6647453 -4.6976032 -4.3812027 -4.9847012 -5.60032 -5.945241 -5.16619 -4.700357 -4.9246249 -5.0019116][-6.62968 -5.948256 -5.3123527 -5.364471 -4.6172867 -3.66516 -3.0821502 -3.4942553 -4.801702 -5.9252105 -6.3792748 -7.3429036 -7.9220233 -7.8430233 -7.0851479]]...]
INFO - root - 2017-12-15 14:55:25.795982: step 25110, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 56h:26m:35s remains)
INFO - root - 2017-12-15 14:55:32.301111: step 25120, loss = 0.22, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 55h:49m:23s remains)
INFO - root - 2017-12-15 14:55:38.912392: step 25130, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 55h:45m:50s remains)
INFO - root - 2017-12-15 14:55:45.508056: step 25140, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 57h:36m:10s remains)
INFO - root - 2017-12-15 14:55:52.118030: step 25150, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 55h:04m:38s remains)
INFO - root - 2017-12-15 14:55:58.745143: step 25160, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 56h:44m:43s remains)
INFO - root - 2017-12-15 14:56:05.308716: step 25170, loss = 0.11, batch loss = 0.06 (12.3 examples/sec; 0.653 sec/batch; 55h:42m:35s remains)
INFO - root - 2017-12-15 14:56:11.948157: step 25180, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 57h:53m:21s remains)
INFO - root - 2017-12-15 14:56:18.613812: step 25190, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 55h:11m:44s remains)
INFO - root - 2017-12-15 14:56:25.227214: step 25200, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 54h:02m:04s remains)
2017-12-15 14:56:25.781675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7356567 -7.5155358 -8.7941875 -9.5085583 -10.524423 -11.036529 -10.478745 -8.347456 -7.5869889 -8.12071 -8.1470146 -10.443887 -11.790501 -12.122627 -11.931538][-7.6082973 -7.3572083 -7.2875357 -8.29042 -9.9610567 -10.615869 -10.426535 -10.304689 -9.8071747 -8.6502924 -8.0127621 -10.93622 -12.152937 -12.776643 -13.47711][-5.7825918 -7.7813492 -8.9979887 -8.7014437 -9.4469767 -10.658857 -10.790884 -9.3187523 -8.44929 -9.17935 -9.4138117 -11.244245 -12.517074 -12.928455 -12.113211][-7.9320574 -8.3561792 -7.9794683 -7.1579833 -7.1805882 -7.3525248 -7.3444166 -8.3678246 -8.9469042 -7.952352 -6.9745193 -10.379587 -12.542095 -12.768982 -13.079773][-9.8734722 -10.503167 -10.628704 -8.3638325 -6.2599998 -4.4660435 -2.7772136 -4.524312 -7.5122609 -7.2835212 -6.9109921 -9.767086 -11.391901 -13.145063 -13.752544][-11.165921 -10.613816 -10.515806 -7.584651 -3.5410759 0.27497768 3.1275916 1.9249506 -0.98207712 -4.0015011 -7.47126 -9.6983919 -10.528269 -12.014223 -12.495686][-11.003564 -11.374313 -9.2722616 -3.9870348 -0.0097036362 2.234478 5.6269155 7.1096129 5.11083 -0.5760026 -5.6588154 -8.9554367 -11.701819 -12.377857 -12.011591][-11.438204 -10.300414 -8.167695 -3.1889589 0.6912179 5.150908 8.5430984 6.0604367 4.6032014 2.2824016 -2.8996246 -8.3061523 -11.029011 -12.559166 -13.022165][-7.4670191 -8.0093813 -7.0203619 -3.4572034 -0.77726221 4.2736773 7.1504083 5.9232106 5.601335 1.247705 -3.5547783 -8.8744965 -12.499424 -13.610459 -13.129287][-5.7901754 -5.4183178 -5.6428795 -3.6224484 -1.3548522 0.47992516 1.7109194 2.7801967 2.9907174 -0.53232622 -3.525574 -9.3338375 -13.556337 -14.010399 -13.86854][-9.8623056 -10.427486 -10.18927 -7.4920826 -6.1701803 -5.1329308 -3.4740727 -3.0762434 -3.5364215 -4.6438522 -5.8148561 -12.07013 -14.990337 -15.405922 -14.340971][-13.905802 -12.875956 -13.106555 -12.751935 -12.286049 -9.6996765 -7.9431973 -9.1060085 -9.3777752 -10.079487 -10.441225 -12.842055 -14.010799 -15.601521 -14.222441][-15.375271 -14.944635 -15.339676 -15.491041 -14.544531 -12.510777 -11.724722 -11.972795 -11.895264 -11.321476 -11.156471 -13.664391 -14.090395 -13.578991 -11.593966][-14.049318 -12.896076 -12.325096 -11.801493 -11.371656 -12.069221 -12.510918 -10.977957 -10.799633 -11.796177 -11.986942 -12.101202 -12.005697 -12.500118 -9.94001][-11.367254 -11.316347 -10.2098 -8.8382339 -8.0569429 -7.7737441 -8.1687231 -9.1220875 -10.485306 -10.140512 -9.6192865 -11.884193 -12.864353 -11.89187 -10.476876]]...]
INFO - root - 2017-12-15 14:56:32.323514: step 25210, loss = 0.16, batch loss = 0.11 (13.0 examples/sec; 0.618 sec/batch; 52h:43m:38s remains)
INFO - root - 2017-12-15 14:56:38.865748: step 25220, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 55h:11m:32s remains)
INFO - root - 2017-12-15 14:56:45.450754: step 25230, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 54h:44m:44s remains)
INFO - root - 2017-12-15 14:56:52.026281: step 25240, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 55h:07m:03s remains)
INFO - root - 2017-12-15 14:56:58.544947: step 25250, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 54h:56m:21s remains)
INFO - root - 2017-12-15 14:57:05.156244: step 25260, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.686 sec/batch; 58h:33m:18s remains)
INFO - root - 2017-12-15 14:57:11.807302: step 25270, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 56h:22m:04s remains)
INFO - root - 2017-12-15 14:57:18.360923: step 25280, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 56h:17m:19s remains)
INFO - root - 2017-12-15 14:57:24.930981: step 25290, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 56h:08m:23s remains)
INFO - root - 2017-12-15 14:57:31.526335: step 25300, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 55h:03m:37s remains)
2017-12-15 14:57:32.019477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5322936 -4.0659246 -4.6022716 -4.8070889 -4.9625931 -4.3122258 -3.5082729 -2.1343808 -1.056284 -1.1011305 -1.3014126 -4.0047646 -5.1468816 -6.935318 -7.3758683][-4.0751171 -4.3206015 -4.3409567 -4.4838758 -4.829186 -4.2905507 -3.0505557 -1.443615 -0.45311308 0.27064228 0.15203953 -3.7085187 -4.9887147 -7.4552484 -8.2641563][-1.5817313 -2.3946764 -3.3198304 -2.9965403 -3.2282064 -3.0325913 -2.7418959 -1.246264 -0.17977571 -0.35287952 -0.45339441 -3.0072448 -4.2183518 -7.0855808 -7.8642244][-1.9242599 -2.864908 -3.5148647 -2.9351921 -2.9157834 -2.2862463 -1.6992421 -1.295527 -0.98030472 -0.47129774 -0.3593502 -3.712075 -5.0736237 -7.4097629 -8.0271444][-2.5605202 -4.4322095 -4.9289689 -3.8942525 -3.9160304 -2.1848617 -1.0072355 -1.1148963 -1.3361712 -0.90653515 -0.52997732 -3.4068973 -4.682528 -7.2023525 -7.773283][-4.4114828 -4.9894161 -5.6794219 -3.7022948 -1.5360341 0.91840076 1.9634719 1.3433208 1.0215259 0.13032341 -1.2328353 -3.4616234 -3.9380214 -6.3024659 -6.900692][-6.0935736 -5.9066148 -4.9290037 -2.5778124 0.30975008 3.083878 4.2312903 4.1183219 3.7738976 1.3959441 -0.82209921 -3.5339832 -4.748426 -7.0463924 -7.4503007][-7.2600269 -6.4997287 -4.9906774 -1.7433996 1.1476145 4.5038457 7.1574349 6.14671 4.7608848 2.7847295 1.0594831 -2.7934866 -5.0806208 -7.8693151 -8.7921333][-5.8178988 -5.3264217 -4.6795239 -2.3853555 -0.473938 2.5791535 5.1045976 5.2051206 4.3520074 2.103054 0.46319675 -3.3937504 -5.4181843 -7.9905167 -9.0133867][-5.6525855 -5.0802507 -4.6709495 -2.0058622 -1.0250068 0.25386763 2.2137661 2.5540347 2.4586129 0.44902802 -1.519578 -4.9856844 -6.5360103 -8.7605457 -9.4981117][-7.8015404 -7.4087496 -7.1952786 -5.3147955 -4.1851969 -3.7147169 -2.9563341 -1.9058499 -1.4663653 -2.7271268 -3.8491626 -7.6819792 -9.3789091 -10.2386 -9.9633522][-10.982527 -10.061367 -9.4803848 -8.5336037 -7.9573593 -7.0671029 -6.829205 -7.2304869 -6.7512717 -6.6301951 -7.2567253 -9.4991655 -10.386335 -11.657637 -11.185748][-13.376206 -12.37405 -11.546705 -10.653624 -11.11578 -10.653486 -10.084105 -10.181059 -10.583334 -10.189184 -9.7488861 -10.886189 -10.933941 -11.335376 -10.509392][-11.291735 -11.19813 -10.83539 -10.210117 -9.9363346 -9.7061167 -10.196145 -9.8484077 -9.9734964 -10.46209 -10.156939 -9.5211143 -8.9538574 -9.8407288 -9.6206417][-8.2014532 -8.3809452 -8.1877651 -7.0790248 -6.9217558 -6.9687624 -7.3533454 -7.8083344 -8.2917261 -8.345396 -8.2542267 -8.8787537 -9.1301289 -8.8582573 -8.787034]]...]
INFO - root - 2017-12-15 14:57:38.633284: step 25310, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 55h:53m:27s remains)
INFO - root - 2017-12-15 14:57:45.183149: step 25320, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 55h:49m:34s remains)
INFO - root - 2017-12-15 14:57:51.755975: step 25330, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 55h:28m:13s remains)
INFO - root - 2017-12-15 14:57:58.384107: step 25340, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 55h:20m:31s remains)
INFO - root - 2017-12-15 14:58:05.006151: step 25350, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 56h:20m:21s remains)
INFO - root - 2017-12-15 14:58:11.661370: step 25360, loss = 0.16, batch loss = 0.11 (11.5 examples/sec; 0.695 sec/batch; 59h:18m:07s remains)
INFO - root - 2017-12-15 14:58:18.307066: step 25370, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 57h:01m:41s remains)
INFO - root - 2017-12-15 14:58:24.888770: step 25380, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 55h:39m:14s remains)
INFO - root - 2017-12-15 14:58:31.524006: step 25390, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 54h:41m:38s remains)
INFO - root - 2017-12-15 14:58:38.072148: step 25400, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 57h:38m:55s remains)
2017-12-15 14:58:38.586515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.20152 -5.032445 -4.954596 -5.088212 -5.9712315 -6.9224811 -7.7296624 -8.2061949 -8.3879356 -9.0530462 -9.1358356 -11.14337 -12.89941 -13.465161 -11.560701][-7.4029274 -7.249136 -6.6849666 -6.6359239 -6.9467297 -7.9802632 -8.9561377 -9.8543015 -10.408511 -10.187046 -10.347113 -12.693056 -14.263447 -15.268337 -12.825589][-5.3140106 -6.1594672 -6.9783869 -6.3031178 -6.8083344 -8.2224483 -9.0097942 -9.1033354 -9.0645866 -9.541 -10.24692 -12.379948 -14.567745 -16.507439 -15.484516][-7.5497866 -7.387114 -7.3516393 -6.5784674 -6.9028115 -6.8458347 -6.8490176 -7.8296108 -8.0595455 -7.3327522 -7.5086222 -11.047608 -14.13493 -16.060314 -15.098526][-10.40358 -11.184757 -9.8388863 -6.74866 -5.2038503 -3.616179 -2.9112558 -3.7992527 -4.9865694 -6.0248795 -7.4854474 -10.052319 -12.197536 -15.345121 -14.897898][-12.142037 -11.332453 -9.16815 -5.630302 -2.4873002 0.82508421 3.6734843 3.5414844 2.3651757 -0.76025009 -5.1316872 -8.9261789 -11.576042 -13.472542 -13.16099][-13.615803 -12.454643 -9.6120176 -4.7963748 -1.3325253 3.091783 7.4217944 7.9378905 7.7328706 2.932158 -2.762084 -7.7532187 -11.898438 -13.63145 -12.872692][-13.404116 -11.474735 -9.15988 -4.7044058 -1.1492686 4.51374 8.3585873 8.4511242 8.9038181 4.3741088 -1.4094772 -7.2162418 -12.124487 -14.59606 -13.842562][-10.759107 -9.4248314 -8.1278448 -3.9821305 -0.83972454 3.3459573 6.4918256 8.3697243 9.0896912 3.9347653 -1.301847 -7.9141631 -12.911755 -14.494591 -13.856291][-8.2296686 -6.9577971 -5.9777842 -3.7217102 -2.4596946 0.3734417 3.1904674 5.33625 5.6309 2.2391295 -2.0323646 -7.9521213 -12.524767 -15.362272 -14.768929][-10.695034 -10.138585 -8.7662725 -7.4324465 -7.106595 -5.4647775 -4.2251344 -3.2355618 -2.778409 -4.19892 -6.9399223 -12.125843 -14.693235 -15.755209 -13.822653][-16.929522 -14.904516 -13.266718 -12.544267 -12.182577 -11.200857 -10.996229 -10.410143 -9.9043341 -10.721227 -12.143988 -13.932711 -14.02161 -15.142405 -13.401016][-15.418732 -14.507566 -14.424427 -14.142984 -13.977909 -13.1059 -12.348616 -11.796606 -12.620541 -12.66649 -12.585646 -13.53837 -12.872013 -11.6414 -9.6400394][-11.5245 -10.798841 -10.447021 -9.3448944 -8.751544 -9.7179794 -10.535345 -10.018217 -10.309948 -10.407824 -11.011812 -10.623007 -9.3937492 -9.06686 -8.2479763][-8.7844191 -6.8123193 -4.7913203 -4.3834848 -4.0814013 -4.0200944 -4.175467 -5.4223013 -7.0319195 -6.6204181 -7.0691757 -8.2544308 -8.5369453 -8.5419006 -8.8638535]]...]
INFO - root - 2017-12-15 14:58:45.184533: step 25410, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 57h:06m:10s remains)
INFO - root - 2017-12-15 14:58:51.807843: step 25420, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 56h:00m:57s remains)
INFO - root - 2017-12-15 14:58:58.452616: step 25430, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 56h:59m:56s remains)
INFO - root - 2017-12-15 14:59:05.119912: step 25440, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.663 sec/batch; 56h:31m:14s remains)
INFO - root - 2017-12-15 14:59:11.658956: step 25450, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 54h:45m:43s remains)
INFO - root - 2017-12-15 14:59:18.201137: step 25460, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 55h:34m:34s remains)
INFO - root - 2017-12-15 14:59:24.821169: step 25470, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 56h:33m:04s remains)
INFO - root - 2017-12-15 14:59:31.380775: step 25480, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 56h:54m:48s remains)
INFO - root - 2017-12-15 14:59:37.946887: step 25490, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 55h:34m:16s remains)
INFO - root - 2017-12-15 14:59:44.477373: step 25500, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 55h:03m:22s remains)
2017-12-15 14:59:44.973307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.0482731 -7.1518793 -6.3530693 -5.2297306 -5.8766589 -6.6203618 -7.2902594 -6.5440583 -6.3947544 -5.4034772 -4.7148256 -7.3794332 -10.553078 -10.436838 -8.3493261][-7.6066909 -6.2013383 -6.2650528 -5.7571211 -5.3773565 -5.6415458 -6.0966854 -5.8312569 -4.32061 -2.9252684 -2.5700974 -4.8817716 -7.622602 -8.5354881 -8.2576923][-3.908916 -5.2475672 -5.5516229 -5.2396975 -4.6398687 -4.4174995 -4.3652687 -3.948319 -3.1724613 -2.3786585 -1.5273919 -3.6283817 -6.1832147 -6.9247985 -7.1736364][-3.72951 -4.2090716 -3.7941184 -2.561136 -2.641592 -1.959702 -1.6557093 -1.8522983 -1.5783677 -0.50408173 0.10046959 -2.0452507 -4.9996338 -6.9104748 -7.7244873][-2.1768811 -3.3277619 -2.965672 -1.2882214 -0.45206594 0.39073277 0.23134804 0.31771803 -0.13157654 -0.058830738 0.52270794 -1.5064259 -3.9177978 -5.7159581 -5.38269][-5.4882836 -4.70315 -2.5494545 -0.58334064 1.2477789 1.9822192 1.6822252 1.0050788 0.87414122 0.76379204 0.43980646 -1.6006432 -4.313303 -5.5832868 -6.0580649][-5.88553 -4.726727 -3.0647039 0.074011326 1.2425575 2.294847 1.5061383 1.0006723 0.19766378 -0.16226244 0.5092926 -1.3130312 -3.5998583 -5.0171418 -5.0732884][-4.6718693 -4.6808839 -3.5690141 -1.0410123 1.2025514 2.6173053 2.1767249 0.75849295 -0.4588933 -0.72070885 -0.39319897 -1.4728503 -3.6235209 -5.0080776 -4.5246177][-2.8575766 -2.8217287 -2.4178407 -0.73117971 -0.26943827 0.24156332 -0.15729952 -0.65191078 -1.3839312 -1.6297178 -1.0754828 -2.7504857 -5.0328298 -5.6882739 -4.7424049][-3.0854797 -2.6792798 -2.7890239 -2.5821135 -2.2403817 -0.66016674 -0.94811869 -1.5881171 -2.4066267 -2.3727345 -1.7653093 -2.8596554 -4.7149715 -4.9761629 -3.9407904][-6.841188 -7.2923088 -6.107255 -4.484066 -3.3389199 -2.9696102 -3.6461673 -4.2524748 -5.0574851 -4.9298415 -4.1429052 -4.7769313 -5.3608742 -5.1478648 -3.8574648][-12.097193 -10.704511 -9.7354631 -7.3921685 -6.5882716 -6.0219207 -6.3909912 -6.4627714 -6.95738 -7.0963526 -6.6125441 -6.1818051 -5.9409342 -4.936327 -3.4904609][-12.395931 -12.243519 -10.729738 -8.0360622 -6.2668238 -6.142427 -6.570796 -6.5127244 -6.4198523 -6.3762484 -5.2925673 -4.7422071 -4.7940226 -4.9469976 -4.2457428][-10.715666 -10.836476 -8.700222 -6.6301813 -6.2215524 -5.5889335 -5.8416586 -6.5222049 -6.1935649 -6.1947055 -5.8063626 -4.6937389 -4.2938776 -4.7956958 -5.1095428][-5.2991905 -5.1322842 -4.1173925 -4.1022892 -3.3488083 -4.0273886 -4.85372 -6.1088648 -6.4035482 -5.5434332 -4.7218909 -4.7939286 -4.8241091 -6.1140432 -6.9116611]]...]
INFO - root - 2017-12-15 14:59:51.502442: step 25510, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 55h:01m:52s remains)
INFO - root - 2017-12-15 14:59:58.069525: step 25520, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 57h:16m:29s remains)
INFO - root - 2017-12-15 15:00:04.639774: step 25530, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 55h:15m:46s remains)
INFO - root - 2017-12-15 15:00:11.287590: step 25540, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 56h:38m:17s remains)
INFO - root - 2017-12-15 15:00:17.886344: step 25550, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 57h:00m:46s remains)
INFO - root - 2017-12-15 15:00:24.432501: step 25560, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 55h:15m:38s remains)
INFO - root - 2017-12-15 15:00:31.065497: step 25570, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 56h:22m:10s remains)
INFO - root - 2017-12-15 15:00:37.710591: step 25580, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 56h:37m:02s remains)
INFO - root - 2017-12-15 15:00:44.296499: step 25590, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 56h:58m:44s remains)
INFO - root - 2017-12-15 15:00:50.915062: step 25600, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 56h:16m:57s remains)
2017-12-15 15:00:51.504911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5850191 -4.4935479 -3.336319 -2.3177285 -3.3041749 -5.4151845 -7.7034063 -9.2023172 -9.1988449 -7.5790377 -5.9859519 -6.9321103 -6.7914538 -6.3134141 -5.41587][-4.0388536 -2.7576623 -0.74511957 -0.25580931 -2.6792367 -5.6049466 -8.2050447 -10.070568 -10.328497 -9.1419468 -7.5911908 -7.7455883 -8.0703 -8.22481 -6.7492752][-0.68288755 -1.3910246 -2.0166798 -1.6341705 -3.296937 -5.844161 -7.8542633 -8.7426882 -9.0271425 -8.8034821 -8.1772842 -9.3894682 -10.142099 -9.9662266 -9.3313026][-3.9272656 -3.7845087 -3.0200043 -3.4461598 -4.8093653 -5.5458808 -6.3905349 -7.3256054 -7.183424 -6.431921 -6.3016472 -8.9360113 -11.265347 -11.610458 -11.096545][-5.9336419 -6.6541095 -6.2394328 -4.6619725 -4.0264711 -2.6195629 -1.4694386 -2.5946422 -3.9771338 -4.7540884 -5.3769388 -8.2734566 -11.105412 -12.321924 -11.901934][-7.916729 -7.8229613 -6.7146082 -4.5526667 -3.1743383 -0.33163357 2.2222886 1.9175563 1.5860105 -0.89856815 -3.7953262 -6.9019318 -10.032299 -11.524274 -11.598007][-9.9033623 -8.9319706 -7.2254257 -4.181325 -1.7834387 1.329483 4.6672225 4.816009 4.9272771 2.4408374 -0.78407955 -4.4018183 -7.9123011 -8.7736483 -8.22832][-10.96168 -9.516592 -7.6062942 -2.8103361 0.9024725 4.0101285 7.1656127 6.6670308 6.6035285 3.8538861 1.4554715 -1.8180935 -5.9541011 -6.97143 -5.8010874][-9.858984 -8.27239 -5.9976296 -2.0627058 0.54737425 2.8731151 5.5668454 6.1075673 6.3580642 3.9444289 1.773356 -2.3305857 -6.3851209 -6.3951945 -5.1642084][-9.0616636 -7.5672278 -6.1682673 -3.2050145 -1.9214759 -0.025418758 2.1134653 2.9980407 3.3937888 1.55333 -0.14236784 -3.5692341 -6.5831933 -7.00998 -6.5744667][-12.26053 -11.967005 -10.603629 -7.789753 -7.6038175 -5.766871 -3.8353417 -3.3392224 -3.3023081 -3.5643988 -4.6235275 -8.287817 -10.300791 -9.5551214 -7.9564734][-16.957193 -16.997169 -15.07305 -12.190866 -11.816141 -10.636753 -10.537575 -10.045732 -9.3326836 -9.236371 -9.5689163 -11.726849 -12.646596 -11.60099 -9.8356209][-15.5825 -14.660545 -13.757471 -11.958439 -10.988403 -10.008461 -9.1571617 -9.164422 -10.040134 -10.408047 -11.237385 -11.706985 -11.473879 -10.837968 -9.3503237][-12.092026 -11.413114 -11.26671 -9.5699472 -8.1955357 -8.3721523 -8.2615137 -8.0451 -8.1032076 -8.5766449 -9.552866 -10.00333 -9.74 -8.9114695 -7.8588142][-10.884694 -8.9699354 -7.8106966 -6.5436783 -6.0727773 -5.1017284 -4.2264605 -4.9152637 -5.7837586 -6.1221814 -6.9701142 -7.8980918 -8.1478777 -8.5503931 -8.6999941]]...]
INFO - root - 2017-12-15 15:00:58.039603: step 25610, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 55h:34m:20s remains)
INFO - root - 2017-12-15 15:01:04.547499: step 25620, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 55h:51m:46s remains)
INFO - root - 2017-12-15 15:01:11.080769: step 25630, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 55h:43m:01s remains)
INFO - root - 2017-12-15 15:01:17.652756: step 25640, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 54h:22m:46s remains)
INFO - root - 2017-12-15 15:01:24.273240: step 25650, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.668 sec/batch; 56h:56m:29s remains)
INFO - root - 2017-12-15 15:01:30.812120: step 25660, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 54h:36m:57s remains)
INFO - root - 2017-12-15 15:01:37.316569: step 25670, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.632 sec/batch; 53h:52m:42s remains)
INFO - root - 2017-12-15 15:01:43.948688: step 25680, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 54h:25m:28s remains)
INFO - root - 2017-12-15 15:01:50.536987: step 25690, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.684 sec/batch; 58h:19m:22s remains)
INFO - root - 2017-12-15 15:01:57.136343: step 25700, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.658 sec/batch; 56h:02m:16s remains)
2017-12-15 15:01:57.648578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6599572 -2.9293427 -2.5308437 -2.4786777 -2.7862997 -3.9897196 -4.9556894 -6.3101921 -7.1664362 -6.9062343 -6.5747476 -7.2766519 -7.8629446 -9.8291359 -10.813484][-3.9384463 -3.3745227 -3.5961428 -3.610435 -3.4828684 -3.84849 -4.9134941 -6.0549188 -6.9177084 -7.8340483 -7.5408282 -8.5982647 -9.1246529 -10.632931 -11.492075][-3.2713151 -3.3518398 -3.821532 -3.8365135 -4.4175358 -5.1116118 -5.5458627 -6.5364242 -7.1157179 -7.3601513 -6.9981551 -7.8191538 -8.789258 -9.9672766 -10.700708][-2.2783661 -2.4997036 -2.5281115 -3.0427752 -3.9272847 -4.1406527 -4.529551 -4.88123 -5.1505594 -5.1583109 -5.1793432 -6.9917188 -9.3351192 -10.663625 -10.076235][-4.5323129 -3.8531094 -3.8096814 -2.9352703 -3.4899821 -3.2325003 -2.8445089 -2.9342313 -3.3049574 -3.2018733 -3.6302562 -5.5851631 -7.8216953 -9.8167143 -10.865044][-6.5388403 -6.2649388 -4.8389583 -4.0066428 -3.4604843 -2.417995 -1.0842662 0.29011154 0.75786734 -0.86192751 -2.8999164 -4.2741232 -6.69039 -9.0954952 -9.901413][-7.3323965 -7.1851063 -6.5053267 -4.3050809 -2.8503714 -0.9068265 0.418643 1.5576038 3.0700812 2.2387629 0.60554028 -2.1232588 -5.8598237 -8.4275684 -8.9672546][-6.1828432 -6.8831325 -6.0113063 -3.8454614 -1.9867411 1.2145839 3.7936149 4.2000422 4.521935 3.7415471 2.5209928 -0.88535023 -4.75837 -6.3623829 -6.9005108][-5.2504258 -5.4309483 -4.4871793 -2.4750175 -1.5162477 0.96863604 3.4470382 5.7295909 6.5966611 4.4717145 2.8931203 0.22629547 -2.64057 -5.58272 -6.1792784][-5.2421813 -5.7659817 -4.7742224 -1.9838023 -0.28061962 1.4678831 2.5538783 3.26331 2.9458938 1.445478 -0.60152197 -2.8013804 -4.9826283 -6.24004 -6.6899738][-8.8647709 -8.3182516 -6.8702688 -4.5549231 -2.9988616 -2.1660159 -2.2529905 -2.8702111 -2.9641635 -3.305634 -4.0066762 -7.4987869 -10.453121 -10.330797 -8.897522][-11.149149 -11.349964 -10.087728 -7.1909752 -6.1854463 -6.3519578 -7.2714777 -7.9348211 -8.8440456 -9.1024418 -8.9856205 -10.009957 -11.060982 -11.218885 -10.676619][-13.741535 -13.185815 -10.927225 -10.276294 -10.926023 -10.736221 -10.60624 -11.318066 -11.276016 -10.082107 -8.5083122 -8.5789471 -9.7918911 -9.3684311 -8.4164324][-12.745439 -12.545008 -11.509505 -11.386265 -11.438727 -10.57069 -10.741424 -10.099628 -8.5780621 -7.4509926 -7.6744246 -6.9037642 -6.1171021 -5.5903983 -5.8212352][-9.0724669 -9.9236908 -9.8063259 -9.0089054 -8.19762 -7.8924041 -7.3319988 -6.7181721 -6.0267472 -4.6867509 -3.3726227 -3.61232 -5.728457 -5.8853641 -6.182425]]...]
INFO - root - 2017-12-15 15:02:04.152330: step 25710, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 57h:45m:36s remains)
INFO - root - 2017-12-15 15:02:10.724982: step 25720, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.671 sec/batch; 57h:09m:53s remains)
INFO - root - 2017-12-15 15:02:17.257063: step 25730, loss = 0.20, batch loss = 0.16 (12.7 examples/sec; 0.628 sec/batch; 53h:30m:08s remains)
INFO - root - 2017-12-15 15:02:23.871055: step 25740, loss = 0.17, batch loss = 0.13 (11.6 examples/sec; 0.690 sec/batch; 58h:45m:58s remains)
INFO - root - 2017-12-15 15:02:30.463479: step 25750, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 55h:59m:20s remains)
INFO - root - 2017-12-15 15:02:37.061758: step 25760, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 55h:34m:26s remains)
INFO - root - 2017-12-15 15:02:43.652377: step 25770, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 56h:35m:40s remains)
INFO - root - 2017-12-15 15:02:50.227936: step 25780, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 55h:11m:35s remains)
INFO - root - 2017-12-15 15:02:56.838185: step 25790, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 56h:45m:21s remains)
INFO - root - 2017-12-15 15:03:03.360906: step 25800, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 54h:04m:36s remains)
2017-12-15 15:03:03.889670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.60643 -8.9573784 -8.1289864 -7.3088174 -6.843298 -6.5440321 -6.3697186 -5.5787282 -4.9310417 -5.4765635 -6.4614587 -8.9523335 -10.153039 -10.505632 -10.485219][-9.267333 -8.7769184 -7.9809461 -7.0663133 -6.7867026 -6.3671031 -5.6998677 -4.9203892 -4.5651169 -5.0506897 -5.4083881 -8.10457 -8.7362 -9.1577168 -9.1240587][-8.0010319 -8.7066174 -8.8317251 -8.2150459 -8.0202389 -7.126339 -6.2128491 -4.9089379 -3.7283735 -3.7770255 -3.99162 -6.4475975 -7.6939459 -8.7951593 -8.7758274][-7.93976 -7.65885 -7.473423 -7.53187 -8.0326939 -6.2972927 -4.6719861 -3.594197 -3.11421 -3.3118119 -3.3621876 -6.0426197 -6.8763022 -8.1488819 -8.8020439][-6.6720896 -8.0516319 -7.6212206 -5.7157426 -5.2012353 -4.21616 -2.6762922 -1.4317861 -1.5502763 -2.3714247 -2.9646883 -5.1329737 -5.8324962 -7.4938784 -8.2257252][-6.2836633 -7.0889034 -6.4654188 -3.8137677 -2.0176456 -0.51621008 1.4497681 1.9531164 1.1596584 -0.6148448 -2.2508621 -4.2605777 -4.9846616 -6.7740307 -7.4642525][-5.3335919 -4.8338556 -4.8386979 -2.713644 0.0083384514 2.3902917 4.4857545 5.0716214 4.722322 1.8029251 -1.2884712 -4.4627771 -5.5703907 -7.2794724 -7.9748559][-6.2479734 -5.0908632 -4.1540236 -2.3189564 -0.93988609 2.9854188 6.3251624 5.8445 4.9414792 2.673676 0.1291995 -3.8512468 -6.6278648 -9.260829 -9.7683716][-6.529067 -5.46717 -3.9672418 -2.3062308 -2.3935807 0.67294073 4.216105 4.5111423 3.9203248 2.0320377 -0.16782665 -4.5721693 -8.271183 -11.27599 -12.381594][-5.5329938 -6.227385 -5.4996858 -2.6651208 -2.1001315 -1.0094919 1.5235014 2.8577704 2.3963714 0.80834484 -1.0525537 -5.2444987 -8.7297134 -13.379421 -15.194632][-6.8730855 -6.8553634 -7.4003725 -5.1956444 -4.2004714 -4.5183196 -3.1903358 -1.9263101 -1.6733332 -2.1837192 -4.0372663 -8.045516 -11.318449 -14.36146 -15.075569][-9.992857 -9.0534716 -9.2015629 -8.73052 -8.6805277 -8.2910652 -7.3179293 -6.818984 -6.302701 -6.5441828 -7.7691369 -10.348814 -12.590435 -14.196955 -14.343636][-13.428425 -11.825829 -10.456018 -10.620461 -11.076843 -9.9948912 -9.5678892 -9.4655762 -9.2614088 -9.3476515 -10.147212 -10.874741 -11.920048 -12.59103 -12.179877][-11.403954 -11.003656 -9.7615757 -8.6221323 -8.5746679 -8.7183352 -8.9165745 -8.8369 -9.3785686 -8.7368326 -8.7307405 -7.9560947 -7.5270386 -8.9596233 -9.4268255][-6.5758486 -8.0465717 -6.8966393 -4.920958 -4.1354194 -5.0612211 -5.2804074 -5.7602329 -6.8458514 -6.7056775 -7.1007085 -7.6946349 -8.5198746 -8.0136576 -7.949717]]...]
INFO - root - 2017-12-15 15:03:10.474576: step 25810, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.681 sec/batch; 58h:00m:21s remains)
INFO - root - 2017-12-15 15:03:17.007862: step 25820, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 55h:17m:49s remains)
INFO - root - 2017-12-15 15:03:23.640945: step 25830, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 54h:27m:15s remains)
INFO - root - 2017-12-15 15:03:30.298150: step 25840, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 57h:07m:57s remains)
INFO - root - 2017-12-15 15:03:36.852244: step 25850, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 55h:40m:20s remains)
INFO - root - 2017-12-15 15:03:43.457091: step 25860, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 55h:26m:54s remains)
INFO - root - 2017-12-15 15:03:49.959444: step 25870, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 56h:34m:12s remains)
INFO - root - 2017-12-15 15:03:56.599885: step 25880, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 56h:14m:10s remains)
INFO - root - 2017-12-15 15:04:03.238330: step 25890, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.689 sec/batch; 58h:39m:20s remains)
INFO - root - 2017-12-15 15:04:09.806331: step 25900, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 54h:30m:42s remains)
2017-12-15 15:04:10.298250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0095162 -6.3549018 -6.2981977 -6.6466026 -7.2335033 -6.8962784 -6.7543859 -6.2822514 -5.9252505 -5.17324 -4.5199475 -5.3731546 -7.6098962 -8.3006544 -8.4321346][-6.3293223 -6.4849181 -6.84912 -6.889081 -7.4680839 -7.45561 -8.1439934 -8.0994673 -7.6404982 -6.6127 -5.9898252 -6.647584 -8.43291 -8.8860569 -8.7832317][-3.5073326 -4.6357107 -6.6149769 -6.4790778 -6.8608327 -7.4422383 -7.5437412 -7.8094149 -7.5277443 -7.0460153 -7.176959 -8.3864918 -9.90023 -10.797693 -10.562664][-2.4581239 -3.5334642 -4.0885973 -4.0825634 -5.3338389 -5.5169911 -5.371088 -5.748961 -6.4303293 -6.6712413 -7.2313986 -8.9079666 -11.657061 -12.401403 -11.727522][-2.4462428 -3.7718062 -4.142292 -3.0944812 -3.4802177 -2.9205992 -2.4766278 -3.0138245 -3.9521666 -4.689249 -5.8992167 -7.947753 -11.910305 -13.515084 -12.938894][-4.9171996 -4.9820342 -3.5026097 -2.8347697 -2.3467846 -0.84656286 0.032739162 0.63261127 0.25818539 -1.9658873 -4.0204425 -7.0281153 -11.103815 -12.4534 -12.156359][-5.5032635 -5.5049682 -4.1179552 -2.4452386 -1.2121224 0.7296052 2.4003978 2.9508424 2.9790759 1.9038572 -0.48924351 -4.5718594 -9.0286636 -10.8605 -11.517109][-5.4755607 -5.0194626 -4.0952311 -2.1139264 0.063771725 1.9054203 2.759316 3.6901927 4.1762042 3.8168492 2.2470403 -1.5267038 -6.33469 -9.8349743 -10.286152][-3.6942322 -3.6818237 -3.0396779 -0.90952015 0.721714 2.2183022 3.605659 3.8100533 3.7854447 4.090919 3.6860309 0.0036711693 -4.5827947 -6.9196453 -8.1973743][-4.261693 -3.9163742 -2.8602073 -1.0482965 0.4283638 1.6694837 2.1799784 3.0758462 3.7807088 3.9225516 3.7957788 1.1741056 -3.1492021 -5.6928806 -6.8661408][-7.12207 -6.2251606 -4.0459752 -1.6819396 -1.1494126 -0.32747602 -0.35211945 -0.41790915 -0.26430511 0.010545731 0.10444927 -1.8113294 -3.9863868 -5.422667 -5.494803][-11.047967 -9.4907007 -7.1030259 -4.3483295 -2.925925 -2.6719663 -3.1646044 -3.113498 -3.135092 -3.6711273 -3.6439166 -3.875391 -5.1886086 -5.9394031 -5.4626164][-10.346678 -8.89782 -7.3844953 -6.2890887 -5.7941747 -5.0488524 -4.8584614 -5.51669 -5.7722473 -5.8015714 -5.6410685 -5.9134021 -6.6198907 -6.1459837 -5.8679066][-9.3181486 -8.4591742 -6.8860092 -5.5839081 -5.5938416 -6.3241129 -6.8549881 -5.5579681 -5.8406777 -6.7677135 -6.848382 -6.4634619 -6.8741961 -6.5073948 -5.9065452][-7.3426418 -7.3603163 -5.801661 -5.4358058 -5.9496822 -6.00725 -6.5774736 -6.6463237 -6.59644 -5.94466 -6.5111575 -7.6279159 -8.8453646 -10.121529 -9.66918]]...]
INFO - root - 2017-12-15 15:04:16.872800: step 25910, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 56h:47m:10s remains)
INFO - root - 2017-12-15 15:04:23.417933: step 25920, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 55h:19m:36s remains)
INFO - root - 2017-12-15 15:04:29.977061: step 25930, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 55h:58m:37s remains)
INFO - root - 2017-12-15 15:04:36.532757: step 25940, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 56h:41m:38s remains)
INFO - root - 2017-12-15 15:04:43.116582: step 25950, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 55h:37m:52s remains)
INFO - root - 2017-12-15 15:04:49.714718: step 25960, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 53h:58m:36s remains)
INFO - root - 2017-12-15 15:04:56.321301: step 25970, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 56h:08m:19s remains)
INFO - root - 2017-12-15 15:05:02.942474: step 25980, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 57h:15m:14s remains)
INFO - root - 2017-12-15 15:05:09.468319: step 25990, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 55h:40m:54s remains)
INFO - root - 2017-12-15 15:05:16.003185: step 26000, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 55h:28m:48s remains)
2017-12-15 15:05:16.535303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6552725 -6.3671269 -6.2429466 -6.033967 -5.86275 -5.1773052 -4.8914714 -4.6378222 -4.4212294 -4.7339211 -4.8920269 -6.0704618 -7.4187746 -7.9968424 -7.5951056][-6.9258709 -5.8074527 -4.6411252 -3.6393645 -3.1337991 -3.5837791 -3.5269289 -3.1881557 -3.3374972 -3.04758 -1.920866 -3.2677345 -5.1318274 -5.2941885 -5.5244188][-2.8370295 -4.8639197 -4.7734632 -2.5323529 -2.0562589 -2.1522253 -2.6024437 -3.2868595 -3.5658777 -2.6981757 -2.0052972 -2.9066694 -3.6203837 -4.7721143 -5.8294611][-4.6877618 -4.857048 -3.6574571 -2.6300905 -2.1216676 -1.8498421 -2.3431168 -3.1561513 -3.4892356 -2.8331892 -2.5058889 -3.860744 -5.1863704 -6.5297389 -6.7590251][-4.8926244 -5.0285 -4.4382844 -2.6471982 -1.4066734 -0.55408764 -0.01240778 -1.3550129 -2.8558807 -2.2275295 -1.8394792 -3.2473259 -4.6119661 -6.3795319 -7.3086886][-5.6942725 -4.6952248 -2.3341923 1.2096262 2.5386338 2.9999776 3.7180791 3.0386472 1.6087523 0.11282539 -0.902823 -2.1767454 -3.57852 -5.4290786 -6.7696404][-8.4323845 -6.3850875 -3.3273821 0.8955698 3.1757426 4.7742572 6.9701009 6.449429 5.2188106 2.5894036 -0.4766264 -2.9720554 -5.774045 -6.7995014 -7.6854277][-11.227171 -8.7742119 -7.3368464 -3.3752341 -0.93569613 2.2612319 5.2569995 4.5475326 4.134974 2.5412679 -0.60096025 -4.573597 -8.0606871 -9.4194164 -10.204297][-10.756738 -9.8591232 -7.825573 -4.3019342 -2.5933948 0.14522076 2.3920665 2.1033173 2.1144934 0.14008427 -2.1789896 -6.3349342 -9.4357815 -10.328354 -10.553902][-10.452016 -9.29693 -8.4039478 -5.3474855 -4.2370625 -3.3449919 -0.84730005 0.41143036 -0.015906811 -1.8867815 -3.4894779 -6.0947566 -8.4115839 -10.152218 -10.720736][-14.545679 -14.145794 -12.911745 -10.093431 -8.7082624 -8.04548 -6.0512862 -4.943769 -4.6458259 -5.1409521 -6.4991417 -9.0267239 -10.134975 -10.459503 -9.5056992][-16.706697 -15.965794 -14.645311 -13.615088 -12.687433 -10.794554 -10.078907 -10.301134 -9.2278175 -8.9570675 -9.6067276 -10.166521 -10.66006 -11.197845 -10.705341][-15.449247 -14.70685 -12.897549 -11.98353 -12.70154 -12.628468 -12.128704 -11.996607 -11.647698 -10.636074 -10.552484 -11.34171 -11.139591 -9.8781528 -9.0751553][-12.276588 -11.322693 -12.008068 -10.312408 -8.7571373 -10.426939 -11.471208 -11.01786 -10.358755 -10.518179 -10.826455 -10.61643 -9.1386957 -9.08276 -9.6541548][-9.6413918 -9.4650755 -9.1021881 -7.2754035 -7.7767372 -7.6763277 -6.8652444 -7.7427621 -7.6231437 -7.6288829 -7.9929132 -8.1615524 -9.011754 -9.8356209 -10.111204]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 15:05:23.109782: step 26010, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 56h:10m:29s remains)
INFO - root - 2017-12-15 15:05:29.704256: step 26020, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 56h:35m:50s remains)
INFO - root - 2017-12-15 15:05:36.283038: step 26030, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 58h:19m:54s remains)
INFO - root - 2017-12-15 15:05:42.841283: step 26040, loss = 0.21, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 54h:25m:19s remains)
INFO - root - 2017-12-15 15:05:49.405903: step 26050, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 57h:06m:00s remains)
INFO - root - 2017-12-15 15:05:56.006101: step 26060, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 56h:39m:16s remains)
INFO - root - 2017-12-15 15:06:02.588880: step 26070, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 55h:33m:08s remains)
INFO - root - 2017-12-15 15:06:09.201190: step 26080, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 56h:13m:51s remains)
INFO - root - 2017-12-15 15:06:15.766619: step 26090, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.692 sec/batch; 58h:56m:23s remains)
INFO - root - 2017-12-15 15:06:22.377740: step 26100, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 57h:16m:20s remains)
2017-12-15 15:06:22.955673: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.846693 -6.0773067 -6.7803173 -6.4335747 -6.6059 -7.1136742 -6.2154708 -5.8245978 -4.9144297 -3.8064215 -3.5414782 -6.2018518 -8.4078121 -10.253521 -10.888158][-3.3796666 -3.9345393 -3.4036336 -3.7224662 -4.2875528 -5.1912603 -5.6151795 -4.9444761 -4.0656366 -3.1575453 -1.8054018 -5.2096376 -6.5081725 -9.7186584 -11.502789][-1.7073946 -2.2889016 -3.0717518 -4.0152121 -4.3399439 -4.89664 -4.7127895 -4.1447577 -2.8111179 -1.9371941 -1.7468364 -4.700078 -7.5600019 -10.184704 -11.815109][-2.4498875 -3.3745427 -3.4749525 -4.6372795 -4.8700442 -4.2693253 -3.8923798 -3.8555119 -2.9674058 -2.2365944 -2.1338537 -5.4712358 -7.5538516 -11.052378 -13.265585][-3.2994349 -4.5986323 -5.6401563 -6.0472717 -5.442667 -4.3452358 -1.98738 -1.8754165 -2.4136634 -2.3333414 -2.1671779 -5.4566388 -7.009233 -9.6176929 -12.302092][-4.3919344 -5.0450687 -5.9592605 -4.7297535 -3.6869407 -2.1662498 1.1520238 1.776257 0.36070728 -0.85719013 -1.6031432 -5.1620183 -6.5789828 -8.6338263 -10.558826][-5.6823206 -5.807446 -5.8528228 -3.3920794 -1.071239 0.955194 4.0971742 4.5352159 4.8049388 3.2458758 0.18565178 -3.9477978 -6.6166811 -8.3434286 -9.538723][-5.0117307 -5.3970394 -4.5090475 -2.0121117 0.53068495 4.5367866 7.3809552 6.7405934 7.5977588 5.9001412 2.5531793 -2.8877509 -7.2923851 -8.9936314 -9.41747][-5.5944662 -4.903244 -4.3824372 -2.8592646 -0.37536144 3.5815988 5.8849473 5.9114146 6.1830964 3.8494763 2.7769837 -2.837527 -7.568531 -10.277929 -12.384198][-5.7080569 -6.1271114 -4.6071706 -3.8290038 -2.9479506 0.52251911 2.7777772 3.004107 1.88906 0.5899477 -1.0033941 -6.9391212 -10.676556 -13.188343 -15.946672][-8.9556618 -9.40229 -9.0797319 -8.0915565 -6.1749482 -4.4322519 -2.9581494 -2.37719 -3.9747729 -5.2690372 -5.5849242 -10.613028 -15.377451 -16.6196 -17.346416][-12.342883 -12.524404 -11.131113 -10.78955 -9.9346075 -8.1531591 -7.4601889 -7.4177661 -7.982399 -9.7678986 -10.532951 -12.982006 -14.674181 -16.46542 -16.540791][-16.66851 -16.666889 -15.799885 -14.713139 -14.214336 -13.233379 -12.341639 -11.523722 -12.747787 -13.587267 -13.39325 -14.343361 -14.09997 -14.726273 -12.847498][-16.002768 -16.339046 -14.894173 -15.547789 -14.061815 -12.939764 -13.25065 -12.166071 -12.255853 -11.237045 -11.852474 -12.880164 -10.938726 -11.567042 -10.630831][-12.230548 -13.01388 -11.889256 -10.759269 -9.6457825 -11.336447 -11.679554 -12.141832 -12.073317 -9.9178457 -9.3281431 -10.163094 -10.926889 -11.008017 -10.589338]]...]
INFO - root - 2017-12-15 15:06:29.550380: step 26110, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 55h:34m:27s remains)
INFO - root - 2017-12-15 15:06:36.181078: step 26120, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 56h:13m:57s remains)
INFO - root - 2017-12-15 15:06:42.648046: step 26130, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 54h:25m:33s remains)
INFO - root - 2017-12-15 15:06:49.294822: step 26140, loss = 0.18, batch loss = 0.13 (11.6 examples/sec; 0.691 sec/batch; 58h:49m:25s remains)
INFO - root - 2017-12-15 15:06:55.898796: step 26150, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 55h:57m:34s remains)
INFO - root - 2017-12-15 15:07:02.523212: step 26160, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 55h:46m:40s remains)
INFO - root - 2017-12-15 15:07:09.071984: step 26170, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 54h:49m:56s remains)
INFO - root - 2017-12-15 15:07:15.669413: step 26180, loss = 0.11, batch loss = 0.07 (11.9 examples/sec; 0.673 sec/batch; 57h:17m:54s remains)
INFO - root - 2017-12-15 15:07:22.206910: step 26190, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 54h:44m:43s remains)
INFO - root - 2017-12-15 15:07:28.795248: step 26200, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 54h:11m:50s remains)
2017-12-15 15:07:29.325185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.060256 -5.9933805 -6.7755666 -7.7112374 -8.9029465 -9.2779694 -9.3640194 -9.0598717 -8.47711 -8.108016 -7.61902 -8.394681 -10.362267 -12.274336 -12.554701][-6.096498 -6.1568913 -6.2829466 -6.9308343 -8.6256552 -9.7240467 -10.179775 -9.634675 -8.9671078 -7.9449911 -7.641499 -7.9622803 -9.2488909 -12.671917 -13.51561][-4.2471452 -5.2491169 -6.4663162 -7.4385581 -8.9014778 -9.61965 -9.3833532 -9.2588654 -9.4545183 -8.9952087 -8.8368187 -8.7708111 -9.8882561 -11.990924 -12.886751][-4.4230118 -4.7853165 -5.3059454 -6.4005909 -8.1607933 -7.7990861 -6.7885351 -6.7721357 -7.9899168 -8.1447859 -8.2301016 -8.1921215 -8.8234177 -11.304789 -12.237162][-4.8417335 -6.6218448 -7.568737 -7.1328592 -7.4136205 -4.4040976 -1.1203308 -3.0505562 -6.6870389 -7.0525908 -7.0435615 -6.9266496 -8.0387421 -10.634649 -12.040261][-7.2646837 -8.5909424 -8.5694132 -7.3612585 -5.7641497 -1.0891523 4.406384 3.9686074 0.52468586 -3.3396842 -6.1651087 -5.55777 -6.6574845 -9.7952785 -11.406183][-8.7327309 -9.1344433 -7.7463679 -5.1698413 -2.7722864 0.67785835 6.6487184 8.56333 7.7833123 2.1947231 -3.4316854 -4.8130641 -6.4126921 -9.345274 -10.70542][-8.3043861 -8.7838688 -8.9208174 -5.0284777 -1.3134737 4.4591613 10.079638 10.607584 10.119997 5.4342866 0.083706379 -3.6492562 -6.8129449 -9.2207022 -9.7877026][-5.3483238 -5.8776894 -6.2490149 -4.8205724 -5.4947634 -0.2589426 6.5508695 8.1894436 7.3933043 2.9510036 -0.699276 -3.6289926 -6.811038 -10.06703 -10.237474][-4.4939628 -4.5360575 -5.05749 -4.3578825 -5.574625 -3.6957862 0.11086798 3.5601764 3.5512433 -1.7318814 -6.5006809 -8.269865 -9.9913807 -11.439114 -11.900135][-8.04128 -7.6621532 -7.07703 -7.0818944 -8.1128654 -7.8152828 -6.8413744 -5.2787528 -4.2051344 -5.5431252 -9.6807461 -11.364285 -12.625264 -13.776621 -13.192822][-11.229691 -10.48595 -10.270279 -9.4229126 -9.177824 -9.4116154 -9.7479324 -9.7946882 -9.4458551 -10.665974 -12.351889 -13.13203 -13.238632 -12.90724 -11.716052][-13.263092 -11.929653 -10.098188 -9.2713346 -8.8962774 -9.4730272 -9.30094 -9.6739044 -9.7212315 -9.934248 -10.721849 -10.757517 -10.468176 -11.260352 -10.188805][-11.675838 -10.478659 -9.10302 -7.3592992 -6.8887076 -7.2264814 -7.3855095 -7.6994753 -7.3304353 -8.159873 -8.6292515 -7.8924351 -6.9414635 -7.3105822 -7.0898371][-7.3394256 -7.268084 -6.4320674 -5.0559435 -4.4721518 -4.908144 -5.3479257 -4.8335638 -4.572309 -5.1530523 -5.7695565 -6.0783811 -6.4579306 -6.0936332 -7.1568861]]...]
INFO - root - 2017-12-15 15:07:35.886745: step 26210, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 56h:04m:42s remains)
INFO - root - 2017-12-15 15:07:42.527519: step 26220, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.671 sec/batch; 57h:03m:55s remains)
INFO - root - 2017-12-15 15:07:49.012416: step 26230, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 55h:29m:23s remains)
INFO - root - 2017-12-15 15:07:55.518540: step 26240, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 56h:54m:08s remains)
INFO - root - 2017-12-15 15:08:02.061266: step 26250, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 55h:51m:58s remains)
INFO - root - 2017-12-15 15:08:08.686366: step 26260, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 56h:09m:43s remains)
INFO - root - 2017-12-15 15:08:15.259574: step 26270, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 56h:28m:47s remains)
INFO - root - 2017-12-15 15:08:21.723875: step 26280, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 55h:18m:45s remains)
INFO - root - 2017-12-15 15:08:28.288699: step 26290, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 55h:58m:50s remains)
INFO - root - 2017-12-15 15:08:34.901117: step 26300, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 57h:43m:46s remains)
2017-12-15 15:08:35.386758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7997184 -6.9896069 -8.1467113 -9.4443865 -11.257175 -11.719939 -11.78598 -10.233932 -9.4260807 -9.81349 -9.4853334 -11.524681 -13.113081 -14.948915 -13.560133][-6.7313843 -8.0838814 -8.8179407 -9.54295 -11.001331 -12.052126 -12.53507 -12.844733 -12.555563 -11.57376 -10.535391 -13.84589 -15.191837 -16.281322 -15.690544][-5.9676342 -7.6402121 -9.6725826 -9.5766373 -9.5916557 -10.956318 -11.823026 -10.992437 -11.381001 -12.175682 -11.569229 -14.160681 -15.58305 -17.224531 -17.067888][-8.06436 -9.0224724 -9.0384722 -7.5538182 -8.2657852 -8.3935242 -8.6843557 -10.116343 -10.807303 -10.267548 -9.0929165 -12.713958 -15.249359 -17.55843 -16.421007][-9.0326929 -11.657498 -13.185432 -10.067924 -6.2106457 -4.0634651 -3.8938808 -5.8365493 -9.3284769 -9.7743931 -9.8315525 -12.505226 -13.926708 -17.163195 -17.265427][-11.537867 -12.911127 -13.207075 -10.082338 -5.7309527 -0.70677233 3.7374272 1.2757893 -3.6246166 -6.8859615 -9.6238213 -11.915623 -13.48905 -15.156742 -15.377861][-11.096615 -12.998446 -12.247431 -7.843142 -3.7790649 1.4391789 6.4243083 7.0178142 4.9964423 -1.5661054 -7.8992481 -11.035164 -12.446514 -14.710953 -14.598328][-11.050479 -11.075092 -9.9062757 -5.8795562 -1.4467869 4.5256534 7.8615594 6.9868197 6.4719062 2.6682096 -2.6576447 -9.2059431 -12.935894 -14.928339 -14.158794][-7.0254135 -8.4130926 -9.8056993 -6.8260689 -2.1874385 3.6706729 6.6628623 6.4098392 6.1791778 1.3359799 -2.865787 -8.92771 -13.335796 -15.795582 -15.339485][-6.1008468 -6.5386114 -8.0434685 -6.2868896 -4.80784 -1.2703643 2.7273688 3.9556022 3.1259742 0.41272783 -2.789727 -9.3571205 -13.903128 -16.176413 -16.423021][-7.3625126 -9.4230366 -10.756646 -8.905138 -7.8532119 -5.6584806 -3.0619335 -1.6596508 -1.2119184 -2.682662 -4.1404319 -10.809939 -14.852655 -17.748756 -16.782898][-14.047453 -13.308603 -14.251257 -14.321299 -14.381536 -11.632717 -9.9112082 -9.9974661 -8.9010658 -8.958827 -9.4923916 -12.248703 -13.69143 -17.616402 -17.817064][-15.109945 -15.20879 -16.108374 -17.089714 -16.96479 -15.230335 -14.566856 -13.637455 -13.34586 -12.069695 -11.495379 -13.82719 -15.237503 -15.75366 -14.59971][-14.415606 -13.621633 -13.514908 -13.793749 -13.874702 -14.63064 -14.136406 -12.790268 -12.95106 -13.263305 -13.15008 -13.137104 -12.962154 -13.853727 -13.139429][-10.436761 -10.151806 -9.6297379 -9.2347813 -9.1507454 -9.0787315 -9.41307 -10.427478 -11.211668 -10.853636 -10.904616 -12.458066 -12.983344 -13.085361 -13.330692]]...]
INFO - root - 2017-12-15 15:08:41.978051: step 26310, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 57h:33m:11s remains)
INFO - root - 2017-12-15 15:08:48.632206: step 26320, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 56h:02m:25s remains)
INFO - root - 2017-12-15 15:08:55.198659: step 26330, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 55h:31m:25s remains)
INFO - root - 2017-12-15 15:09:01.744464: step 26340, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 54h:13m:47s remains)
INFO - root - 2017-12-15 15:09:08.333133: step 26350, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 54h:45m:36s remains)
INFO - root - 2017-12-15 15:09:14.924499: step 26360, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 54h:56m:14s remains)
INFO - root - 2017-12-15 15:09:21.529814: step 26370, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 57h:00m:08s remains)
INFO - root - 2017-12-15 15:09:28.115546: step 26380, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 55h:26m:14s remains)
INFO - root - 2017-12-15 15:09:34.635780: step 26390, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 56h:14m:00s remains)
INFO - root - 2017-12-15 15:09:41.274667: step 26400, loss = 0.13, batch loss = 0.08 (11.5 examples/sec; 0.698 sec/batch; 59h:21m:04s remains)
2017-12-15 15:09:41.830569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.383265 -5.4157248 -4.7203383 -3.1634097 -2.6980662 -2.5886297 -2.6072493 -2.7723162 -2.9175413 -2.7614393 -2.7133229 -5.2722445 -6.4378953 -6.485939 -5.7294669][-4.4002204 -4.1376567 -3.9728744 -3.7219191 -3.6788094 -3.6220655 -3.4840753 -3.4844453 -3.6008255 -4.0260763 -4.3084059 -7.1014004 -8.7799387 -7.9944234 -7.4929881][-3.4264226 -4.0047154 -3.6016872 -3.1949461 -4.4905005 -4.1844759 -3.9352303 -4.3701954 -4.9228988 -5.0340743 -5.2074461 -8.3766594 -9.5206614 -8.7951794 -7.9146905][-2.84512 -3.3665051 -3.7046785 -3.7277007 -3.9824884 -4.1653428 -4.6752024 -4.5960636 -4.7402954 -4.1166873 -4.0321817 -7.4673114 -8.876317 -8.8150282 -8.423521][-4.2529 -4.6491876 -5.655828 -4.6331654 -3.4192982 -3.0490508 -3.0141659 -2.9872849 -2.9536009 -2.617449 -2.8647332 -5.7117224 -7.475574 -7.5877695 -7.7037592][-5.8059297 -6.0926137 -5.6095114 -4.8286037 -2.9638417 -1.1001258 0.5568347 0.36386156 0.36174488 0.2966094 -0.15293598 -3.5824027 -5.2024922 -6.2773862 -6.6400957][-5.9967451 -6.3329344 -6.0552816 -3.6133127 -1.7442384 0.702929 2.6280351 2.9217486 2.7704921 2.1092997 1.0938568 -2.1533868 -3.9655275 -4.8061681 -5.2648954][-4.3207369 -4.0329533 -3.9168763 -2.0126228 -0.35576534 1.4364848 2.6519237 3.1208177 3.7495503 2.5929017 1.5851164 -1.7666574 -4.2148089 -4.498033 -4.296505][-3.7488523 -2.5004313 -1.9874945 -0.187253 0.56689978 1.143971 1.8341293 2.2477798 2.7220616 2.6187282 2.1423216 -1.4890385 -3.4496441 -3.9938478 -3.6794529][-4.0414782 -3.6851854 -2.6542127 -0.52233124 -0.088462353 0.61625576 1.1561408 0.792624 1.1891937 1.4621172 1.3379316 -2.0918381 -3.7705064 -4.353723 -3.7258546][-7.3408022 -6.7758751 -5.7010355 -4.3572073 -2.8316061 -2.264554 -2.4719515 -1.9984021 -1.17241 -1.2632666 -1.5244117 -4.4222894 -5.2981882 -5.4359531 -3.4064887][-11.036457 -9.6741982 -8.3811436 -6.921855 -5.6699853 -4.9027419 -4.2290478 -3.8426304 -3.4062359 -3.0046723 -3.1922102 -5.3986545 -5.4019961 -4.8573904 -2.6094415][-11.550287 -10.576767 -9.1030436 -7.6613164 -6.9920888 -6.2662544 -5.2365766 -4.2635365 -4.2572126 -4.2299428 -4.4184361 -5.22681 -5.0844307 -4.5915914 -2.4922645][-7.7023349 -7.6269383 -7.6525025 -5.88654 -5.0561996 -5.2953954 -5.405376 -4.99002 -4.0002728 -3.8890615 -4.178216 -4.751092 -4.8841496 -4.1394472 -2.7201936][-4.519423 -4.6771531 -5.24732 -4.9336791 -4.27968 -3.8496003 -3.4711781 -3.2338235 -3.6845539 -4.7258492 -5.0249214 -5.1544981 -4.867825 -4.8200831 -4.7199912]]...]
INFO - root - 2017-12-15 15:09:48.386976: step 26410, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 55h:26m:25s remains)
INFO - root - 2017-12-15 15:09:54.971041: step 26420, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 54h:18m:53s remains)
INFO - root - 2017-12-15 15:10:01.497645: step 26430, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 55h:56m:21s remains)
INFO - root - 2017-12-15 15:10:08.024197: step 26440, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 55h:08m:57s remains)
INFO - root - 2017-12-15 15:10:14.535416: step 26450, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 55h:36m:16s remains)
INFO - root - 2017-12-15 15:10:21.039263: step 26460, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.684 sec/batch; 58h:09m:57s remains)
INFO - root - 2017-12-15 15:10:27.665838: step 26470, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 55h:00m:56s remains)
INFO - root - 2017-12-15 15:10:34.202153: step 26480, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 54h:32m:31s remains)
INFO - root - 2017-12-15 15:10:40.800653: step 26490, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 56h:46m:16s remains)
INFO - root - 2017-12-15 15:10:47.392995: step 26500, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 55h:29m:17s remains)
2017-12-15 15:10:47.879814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4075665 -5.3100615 -4.7376461 -4.5620136 -5.09159 -5.7362752 -6.4141841 -6.9402142 -7.1852565 -8.1930723 -8.6271534 -10.837357 -12.69139 -13.054541 -11.306501][-7.4058847 -7.3122015 -6.6242843 -6.3155441 -6.5745993 -7.5798492 -8.5199928 -9.4927807 -10.161236 -10.213832 -10.566023 -12.849545 -14.223904 -14.751659 -12.382046][-5.5479674 -6.1305132 -6.5192256 -5.8717861 -6.4674697 -7.9280167 -8.8201046 -8.9739485 -8.9558363 -9.5732727 -10.271612 -12.533064 -14.435984 -15.995907 -14.906488][-6.8674011 -6.5292869 -6.16203 -5.2571416 -5.7954607 -6.0748186 -6.4591384 -7.6124082 -7.897687 -7.3136134 -7.5668945 -11.020279 -13.924635 -15.816513 -14.696178][-9.9963875 -10.376331 -8.7253189 -5.7552118 -4.4308157 -3.2013772 -2.7839293 -3.578706 -4.5646672 -5.4870939 -6.8063364 -9.3684587 -11.594791 -14.620447 -14.356216][-11.991674 -11.030261 -8.6313381 -5.1212611 -1.9907057 1.052063 3.4990697 3.4585309 2.4894347 -0.20611095 -4.1232281 -7.9854293 -10.899846 -12.500022 -12.253023][-13.060919 -11.893003 -8.9485064 -4.433567 -1.0209713 2.9973922 6.9098163 7.4722238 7.4083695 2.9549718 -2.3618035 -7.1080432 -11.110977 -12.768017 -12.336123][-12.882519 -11.372797 -9.0712776 -4.5452123 -1.2820954 3.8736539 7.7395082 7.85899 8.2909393 4.2588239 -1.2508678 -7.0107765 -11.96557 -14.224497 -13.448128][-10.912989 -9.3821945 -8.2260227 -4.4018741 -1.4323888 2.8267817 5.7348447 7.5459924 8.524971 3.8501868 -1.105082 -7.7853184 -12.846272 -14.338802 -13.832277][-7.909297 -7.1068821 -6.054471 -3.739522 -2.5438406 0.36842775 3.0114856 5.1130309 5.3856406 2.3355708 -1.6090622 -7.8184295 -12.665428 -15.516685 -15.283669][-9.9692163 -9.2408781 -7.5878096 -6.5341415 -6.3850088 -5.0454588 -4.0362191 -3.0209343 -2.6218135 -3.8120787 -6.4300103 -11.636707 -14.483953 -15.551849 -13.964695][-15.528284 -13.660753 -11.695065 -11.437614 -11.191278 -10.463129 -10.334464 -10.118877 -9.8553486 -10.487169 -11.837461 -13.561226 -13.70176 -14.797573 -13.355234][-15.433392 -14.195082 -14.170904 -13.425583 -13.104198 -12.648291 -12.041183 -11.620798 -12.50201 -12.879736 -12.91506 -13.559639 -13.016624 -11.790876 -9.6123657][-12.14522 -11.244762 -10.512669 -9.5559511 -9.0781107 -9.723053 -10.546494 -10.346659 -10.588404 -10.716878 -11.52741 -10.923951 -9.7135391 -9.2109251 -8.5969191][-8.9167538 -7.237112 -5.2849379 -4.4321294 -3.9928446 -4.1014419 -4.362545 -5.6230736 -7.0296812 -6.945076 -7.369781 -8.2944546 -8.9294825 -8.72853 -8.809514]]...]
INFO - root - 2017-12-15 15:10:54.436155: step 26510, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.687 sec/batch; 58h:23m:27s remains)
INFO - root - 2017-12-15 15:11:00.991826: step 26520, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 57h:13m:04s remains)
INFO - root - 2017-12-15 15:11:07.540978: step 26530, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 55h:50m:48s remains)
INFO - root - 2017-12-15 15:11:14.081841: step 26540, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 56h:50m:35s remains)
INFO - root - 2017-12-15 15:11:20.670000: step 26550, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 55h:44m:58s remains)
INFO - root - 2017-12-15 15:11:27.223373: step 26560, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 57h:30m:43s remains)
INFO - root - 2017-12-15 15:11:33.826441: step 26570, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 56h:11m:42s remains)
INFO - root - 2017-12-15 15:11:40.347072: step 26580, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 54h:13m:59s remains)
INFO - root - 2017-12-15 15:11:46.890782: step 26590, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 54h:27m:00s remains)
INFO - root - 2017-12-15 15:11:53.521865: step 26600, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 56h:33m:36s remains)
2017-12-15 15:11:54.053554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.7211304 -7.6962647 -6.7141867 -5.9077892 -5.9227357 -6.1329937 -7.1189985 -7.2679558 -8.0643749 -7.7123594 -7.7435484 -9.1762486 -10.871469 -10.687006 -9.0739355][-8.5135651 -8.0558567 -7.2837739 -6.952826 -6.439105 -6.1730771 -6.2251587 -6.8797388 -7.6997356 -7.1806483 -6.9285984 -8.9475269 -10.826187 -9.9299068 -8.9737444][-3.6501286 -5.0489283 -6.3753004 -5.7833595 -5.718996 -5.4230442 -4.9428658 -5.3290987 -5.623539 -5.4033442 -5.493916 -7.3134704 -9.2853241 -8.8470154 -9.223629][-3.5009749 -4.0450759 -4.4993577 -3.9107656 -3.9232192 -3.7044296 -2.5857074 -2.5384159 -2.0347805 -1.3914375 -1.1588998 -4.4511237 -7.4064159 -7.9348955 -8.9593906][-1.766844 -3.6494281 -4.1140075 -3.2256541 -2.2539032 -0.40289307 0.7907815 0.97736073 1.829309 1.7243528 1.8274903 -1.4443402 -4.0006456 -6.2073541 -8.4606094][-4.4160509 -4.4083796 -2.6510248 -1.6496234 -0.11572742 1.6644154 3.009726 3.4399877 3.9823203 3.9651027 3.1775327 -0.48494053 -3.8305142 -6.1026025 -7.7694006][-6.1309562 -5.2066507 -3.6009717 -0.77009344 1.5909662 3.0013709 4.7806621 4.7524962 4.6886678 3.5083451 2.6366754 0.020143986 -3.2547143 -5.6655784 -6.7130203][-4.2597466 -4.2163687 -2.5456481 -0.57906914 1.3953538 3.8770871 5.250504 5.3743224 4.9074521 3.0242705 1.6717777 -0.92113924 -3.2075121 -4.5393882 -5.9164248][-1.9117978 -2.2030659 -2.1650109 -0.98261309 0.30996895 2.6411214 3.9131875 3.8511052 2.8359199 0.822968 -0.6104002 -3.281662 -4.8115706 -4.8433757 -4.9205689][-0.63469982 -1.3530765 -1.9125397 -1.5286536 -1.6035509 -0.1747613 0.76243877 0.94054747 0.12756062 -1.5288506 -3.2937 -6.1260777 -7.9433384 -7.5265446 -6.170753][-3.2627199 -3.4281194 -3.5908105 -3.0482132 -3.0549064 -2.6225438 -2.9376366 -3.0538471 -4.2276187 -6.0081687 -7.1106977 -9.6275406 -9.6368771 -8.7057066 -7.1657915][-8.6719437 -7.55597 -6.46005 -5.59338 -5.1446261 -4.7878208 -5.7382536 -6.2165613 -6.9012475 -8.1214752 -9.3918905 -10.821222 -10.666989 -8.9890213 -6.4821177][-11.747232 -9.8439674 -8.3664293 -6.8830256 -5.9582071 -5.5049014 -6.4431443 -7.256197 -8.1172428 -9.1345978 -9.732729 -9.8721371 -9.5697041 -8.2820988 -6.4666452][-9.5678511 -9.06436 -8.0396 -6.0195718 -5.2834148 -4.8915949 -5.3786693 -6.3753066 -7.5102272 -8.1762142 -7.6969609 -6.8655834 -6.5378728 -5.4263592 -4.4427443][-7.2167954 -6.6416082 -6.1085677 -4.4488583 -3.1128592 -3.0873826 -3.5645196 -4.5784187 -5.6252661 -6.1323652 -6.3626533 -7.32935 -7.5060358 -6.698597 -6.1481004]]...]
INFO - root - 2017-12-15 15:12:00.568029: step 26610, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 54h:45m:33s remains)
INFO - root - 2017-12-15 15:12:07.159487: step 26620, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.686 sec/batch; 58h:18m:26s remains)
INFO - root - 2017-12-15 15:12:13.654029: step 26630, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 55h:10m:58s remains)
INFO - root - 2017-12-15 15:12:20.190016: step 26640, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 54h:57m:46s remains)
INFO - root - 2017-12-15 15:12:26.792611: step 26650, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 58h:13m:35s remains)
INFO - root - 2017-12-15 15:12:33.394929: step 26660, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 57h:04m:38s remains)
INFO - root - 2017-12-15 15:12:40.016688: step 26670, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 56h:59m:31s remains)
INFO - root - 2017-12-15 15:12:46.545165: step 26680, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 55h:44m:35s remains)
INFO - root - 2017-12-15 15:12:53.138478: step 26690, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 53h:56m:14s remains)
INFO - root - 2017-12-15 15:12:59.756251: step 26700, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 54h:43m:37s remains)
2017-12-15 15:13:00.269233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8301077 -7.7877865 -6.1322012 -4.163342 -3.3027778 -2.4261703 -2.7408979 -2.5840764 -2.3407321 -2.395067 -2.4642482 -4.9266191 -5.8113346 -5.6600914 -4.6580944][-5.9033256 -5.289742 -3.7025819 -1.6481385 -1.6013365 -2.0670116 -3.2953506 -4.1652346 -5.1123629 -4.7492042 -3.6892214 -6.0481896 -7.7500558 -7.4943066 -6.3103251][-4.2640219 -3.571367 -2.5824685 -0.65084934 -0.079741955 -0.58035994 -1.89749 -3.6117563 -4.9378762 -5.3723917 -5.1619468 -7.0954652 -8.4875622 -8.4980984 -7.6435828][-4.8817921 -3.509284 -1.8570864 -0.92117357 -0.89544106 -1.4019856 -2.5939994 -3.6659133 -4.5239964 -5.0309472 -5.3468437 -8.1137829 -10.053938 -10.178684 -9.77841][-4.9605269 -4.1701779 -2.8187716 -0.95629454 -0.36471558 -0.016928196 -0.90138245 -2.3133543 -3.49854 -3.707994 -4.3366814 -7.46064 -9.7234573 -10.515092 -10.304865][-5.4015608 -4.5647616 -3.1474965 -0.66172552 0.35583544 0.89809227 0.96541214 -0.10630178 -1.312911 -2.6442039 -3.8612165 -7.1923008 -9.2862225 -10.814091 -10.746519][-6.1215782 -4.3163109 -2.7954829 -0.280488 0.90394068 1.6561255 1.342205 0.9610734 0.52244186 -1.2549591 -2.7435317 -6.4705076 -9.1402836 -9.9335108 -9.52181][-6.6648769 -5.1320662 -2.6032848 0.48374271 2.0980239 3.7595067 3.4851441 2.05345 1.4432211 -0.16588116 -1.3129196 -4.922987 -7.2860112 -8.3697557 -6.9265471][-5.7081275 -4.6002212 -1.776823 1.855382 3.1456885 4.3423352 3.5328717 1.7524428 1.2347016 0.11017847 -0.35555458 -3.6560874 -6.1824231 -6.6802597 -5.111743][-5.4104848 -3.9707615 -2.6814823 0.73181009 2.4985108 2.9750152 3.223207 1.9967809 0.97606564 0.81831789 1.0134459 -1.1657195 -2.7092044 -3.2620401 -2.3611133][-10.305847 -9.327055 -6.7707014 -3.8526859 -2.0913806 -0.5214 0.57049942 0.25063419 -0.39175272 -0.075283051 0.60735273 -1.6955843 -2.3965774 -2.6005197 -1.8468034][-13.074241 -13.637118 -11.632157 -9.01902 -8.1451616 -6.0870557 -4.8094416 -3.9326191 -3.5357845 -3.7678676 -3.1451755 -4.142796 -4.3480062 -4.7294226 -4.3979688][-12.807726 -11.877398 -10.948393 -9.2744389 -7.7181253 -7.0029616 -6.3175983 -5.3525367 -4.850121 -4.6293573 -4.3767972 -5.1575818 -4.7427053 -4.3267221 -4.4593][-10.908899 -10.573373 -9.6115017 -9.1527014 -8.2293081 -7.9279051 -7.8148222 -6.2380562 -5.18566 -4.7809148 -4.469624 -5.0376906 -4.9640722 -4.1243005 -4.0526791][-8.4191494 -9.5016489 -9.88794 -8.9465847 -8.3600407 -7.0421438 -5.9181356 -5.8302746 -5.6755643 -4.9325781 -4.5388527 -5.47056 -6.5059562 -7.2675004 -7.3113246]]...]
INFO - root - 2017-12-15 15:13:06.896073: step 26710, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 56h:03m:31s remains)
INFO - root - 2017-12-15 15:13:13.530079: step 26720, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 56h:32m:29s remains)
INFO - root - 2017-12-15 15:13:20.201941: step 26730, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.688 sec/batch; 58h:26m:48s remains)
INFO - root - 2017-12-15 15:13:26.700078: step 26740, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 55h:10m:40s remains)
INFO - root - 2017-12-15 15:13:33.281116: step 26750, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 55h:53m:17s remains)
INFO - root - 2017-12-15 15:13:39.778075: step 26760, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 55h:03m:56s remains)
INFO - root - 2017-12-15 15:13:46.355881: step 26770, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 56h:55m:47s remains)
INFO - root - 2017-12-15 15:13:52.873894: step 26780, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 57h:16m:20s remains)
INFO - root - 2017-12-15 15:13:59.415096: step 26790, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 54h:40m:40s remains)
INFO - root - 2017-12-15 15:14:05.973954: step 26800, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 54h:01m:30s remains)
2017-12-15 15:14:06.477739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4768558 -3.1482916 -2.2716575 -2.4257274 -3.552495 -4.7408566 -5.6686625 -6.9105163 -7.6416826 -7.6324091 -6.9056058 -6.9383492 -8.516717 -9.0022869 -8.9585228][-3.6894848 -3.2468905 -3.5864692 -3.6072924 -4.4784393 -6.0361562 -6.2490187 -6.8026953 -7.7173848 -7.4350505 -7.1148005 -7.2740717 -8.7153006 -9.0786018 -8.64602][-3.0991485 -2.755517 -3.3831322 -4.5454893 -5.7830806 -6.0832291 -6.4981666 -7.1482677 -6.9280825 -6.6135645 -6.7896147 -7.1454649 -8.7938881 -9.5417795 -9.09648][-3.6865492 -3.539542 -3.2826374 -3.9271276 -5.0739012 -5.0890393 -4.7568889 -5.0736403 -5.9716687 -6.1644158 -6.4175897 -7.7966757 -10.651323 -10.244432 -8.0461674][-6.1756668 -5.8741803 -5.193737 -4.3780518 -3.830642 -2.9605467 -2.6243739 -2.6408381 -3.2746696 -4.6386514 -5.9934506 -7.2867422 -9.7998123 -11.308937 -10.38027][-7.5690584 -7.5891075 -6.2932487 -4.4175425 -3.5496612 -1.5875664 -0.14659166 0.66082048 0.33054638 -1.8422127 -4.3636065 -6.6731539 -10.037018 -10.09071 -9.369][-7.6622171 -7.6615772 -6.8091688 -4.260489 -2.579216 -0.59060907 1.3210716 2.8168597 3.3590055 1.7012949 -1.11128 -3.8679576 -7.2165065 -8.6352081 -8.4825974][-7.4592805 -7.1139784 -5.258678 -3.2190881 -2.2162204 0.34058809 2.1018724 3.3322368 4.176373 3.1585956 0.928175 -2.3922424 -6.7460933 -7.7304358 -7.6358786][-7.1187592 -6.3503418 -4.3062015 -1.9695187 -1.030283 0.74596024 2.1452842 3.5123162 4.5195136 4.1609273 3.0375857 0.307343 -4.1347704 -6.3075109 -6.4861765][-6.762279 -5.7528648 -3.7993622 -1.6917148 -0.8737936 0.24971008 1.2561107 2.3704572 2.749064 1.5611358 0.65969706 -1.0781174 -4.6160612 -6.7410007 -6.6444335][-10.868372 -9.04479 -7.007226 -4.3667297 -2.9520085 -2.6757705 -3.2303157 -2.4441135 -2.1407843 -2.2953362 -3.1842852 -5.510613 -8.7223511 -8.3004637 -7.1013155][-14.04421 -12.541069 -10.441589 -7.9915047 -6.7538261 -6.220623 -6.6060891 -6.5469971 -6.8750267 -7.5647821 -8.0036316 -8.5002012 -10.297749 -9.5540733 -8.0887594][-12.968294 -12.297093 -10.626673 -9.1525688 -8.6270485 -7.5368433 -7.3151507 -7.2234235 -7.1624718 -7.41455 -7.9078093 -8.5843668 -9.9513083 -9.4979677 -8.4681759][-11.362404 -11.349258 -10.456797 -8.3998127 -6.847167 -6.5140038 -6.874753 -6.7525482 -6.2459612 -6.5140381 -7.2492776 -6.8418584 -7.2900572 -7.366847 -7.358995][-6.8974872 -6.7645411 -6.2929544 -5.4844456 -4.9796333 -4.53859 -4.5001726 -4.5067749 -4.939322 -4.6554317 -3.9702833 -5.2324405 -6.6574807 -6.7139258 -7.061481]]...]
INFO - root - 2017-12-15 15:14:13.087273: step 26810, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.664 sec/batch; 56h:24m:21s remains)
INFO - root - 2017-12-15 15:14:19.700258: step 26820, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 54h:32m:11s remains)
INFO - root - 2017-12-15 15:14:26.289843: step 26830, loss = 0.20, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 55h:25m:24s remains)
INFO - root - 2017-12-15 15:14:32.837669: step 26840, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.642 sec/batch; 54h:30m:47s remains)
INFO - root - 2017-12-15 15:14:39.355905: step 26850, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 55h:44m:01s remains)
INFO - root - 2017-12-15 15:14:45.946870: step 26860, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.676 sec/batch; 57h:25m:56s remains)
INFO - root - 2017-12-15 15:14:52.590914: step 26870, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 57h:38m:32s remains)
INFO - root - 2017-12-15 15:14:59.240863: step 26880, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 55h:11m:34s remains)
INFO - root - 2017-12-15 15:15:05.835353: step 26890, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 55h:55m:17s remains)
INFO - root - 2017-12-15 15:15:12.412032: step 26900, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 54h:55m:14s remains)
2017-12-15 15:15:12.946129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.876472 -9.3503361 -8.5922222 -7.2451963 -7.4295869 -7.5131774 -6.9908285 -6.2281342 -4.4997749 -4.4410243 -4.4668608 -4.0507517 -5.3087234 -7.0151911 -7.9719305][-9.1821547 -9.4186687 -8.7606277 -6.9964809 -6.8644767 -6.7966566 -6.5246429 -5.6186156 -4.2556925 -3.5404012 -3.2739117 -4.1910253 -4.931344 -6.6259832 -8.1425257][-7.1607509 -8.219471 -8.2120152 -6.9914613 -7.0851212 -6.4408875 -5.9200854 -5.198853 -4.28094 -3.6946807 -2.9076264 -3.7670398 -4.723875 -6.6896143 -8.2886686][-6.777597 -6.27046 -6.0940204 -5.7989492 -5.9899111 -5.3764591 -4.93517 -4.2252259 -3.4120526 -2.6458244 -2.5097332 -4.0199351 -5.2062635 -7.2509356 -7.9818029][-6.5792584 -7.6882725 -6.5398092 -4.6188951 -4.9138503 -4.302578 -3.8574619 -3.4495418 -2.78259 -2.53782 -2.3919134 -3.8539636 -4.9938326 -7.794868 -9.7290249][-8.2062483 -7.416666 -5.501883 -4.0525832 -3.4542649 -2.5994031 -2.6766229 -2.4031429 -2.0415792 -1.8944426 -2.0390472 -3.3187437 -4.9730086 -7.4846764 -8.9075565][-6.78806 -6.1270385 -4.5249391 -2.2857389 -1.5298386 -0.54855442 -0.88470936 -0.9421649 -0.33297443 -0.88967371 -1.7024035 -2.98117 -4.5394554 -6.3684888 -7.25303][-6.0704489 -5.1111169 -2.8478403 -1.0557165 -0.94815111 -0.35713959 0.015591145 -0.11662388 0.0036330223 0.73175526 0.41526651 -1.5966697 -3.8303046 -6.3016009 -7.4199262][-5.9154105 -4.8705897 -2.4842234 -0.53275537 -0.23885679 0.30587721 0.076778889 -0.26883268 -0.21309948 -0.24236679 0.36832762 -0.9007535 -2.9920444 -5.6195526 -7.4135704][-3.5960004 -3.8546944 -2.4779744 0.32536936 -0.034797668 -0.91810751 -1.0564232 -1.0250025 -1.1131372 -0.81193638 -1.2791452 -1.9551854 -2.4831524 -5.1965151 -7.3019743][-5.3583097 -4.1688929 -3.8077307 -2.3086538 -2.4288049 -3.1136918 -3.7769861 -4.3101659 -3.862288 -3.3959856 -3.1521873 -4.56627 -5.3596711 -6.2679987 -6.8281198][-8.6328621 -6.30413 -4.3202763 -3.5599561 -4.6970649 -5.5412006 -5.5532084 -5.7029428 -6.2464304 -6.7652793 -7.040009 -6.7011452 -7.677845 -8.4400063 -8.0177078][-10.095684 -8.3633242 -6.4306607 -5.6721854 -6.6247454 -6.947854 -5.4412165 -5.5209908 -5.8847437 -6.9781857 -7.92448 -7.9429054 -7.83045 -8.2826538 -9.0260935][-9.98108 -8.5644932 -7.6468983 -5.9367995 -4.9879417 -5.9964828 -6.8055892 -6.3282728 -4.5956087 -5.5258389 -6.4780097 -6.6376896 -6.2981396 -6.5321674 -8.5345783][-6.9722056 -7.7256093 -7.3182583 -5.6994786 -5.0418453 -4.2832909 -4.417429 -4.7611117 -4.6352696 -5.2990046 -5.9778576 -6.9256692 -7.3774166 -7.1467428 -8.0247126]]...]
INFO - root - 2017-12-15 15:15:19.565859: step 26910, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 55h:43m:47s remains)
INFO - root - 2017-12-15 15:15:26.231181: step 26920, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 56h:22m:32s remains)
INFO - root - 2017-12-15 15:15:32.702160: step 26930, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 54h:17m:35s remains)
INFO - root - 2017-12-15 15:15:39.335661: step 26940, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 56h:16m:38s remains)
INFO - root - 2017-12-15 15:15:45.867501: step 26950, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 56h:19m:59s remains)
INFO - root - 2017-12-15 15:15:52.448935: step 26960, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.686 sec/batch; 58h:11m:49s remains)
INFO - root - 2017-12-15 15:15:59.112857: step 26970, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 58h:06m:41s remains)
INFO - root - 2017-12-15 15:16:05.784246: step 26980, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 55h:09m:03s remains)
INFO - root - 2017-12-15 15:16:12.468232: step 26990, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.637 sec/batch; 54h:03m:14s remains)
INFO - root - 2017-12-15 15:16:19.065169: step 27000, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 54h:27m:12s remains)
2017-12-15 15:16:19.617221: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2347794 -3.7377453 -3.2705917 -3.2831764 -3.1909516 -3.3902357 -3.88474 -4.4895644 -4.6929288 -3.783757 -2.5266407 -3.7393522 -4.6730733 -7.5446548 -7.6405249][-5.0274158 -5.7767472 -5.9710679 -5.3042841 -5.0307703 -5.562819 -6.0680084 -5.9355025 -5.289813 -3.9508076 -2.4030247 -3.8362718 -6.0197406 -8.0654068 -7.9332066][-4.6264935 -5.6803379 -6.2482762 -6.0781336 -5.4175038 -5.5647364 -6.3931646 -6.2169971 -5.5951424 -4.5305023 -3.3629694 -4.711781 -6.3844843 -8.0250263 -8.1859493][-6.7788863 -7.5883474 -7.1187444 -6.5589294 -6.3703079 -4.8884587 -4.2726545 -4.7075171 -4.9896827 -3.72945 -2.8579929 -4.884975 -6.8875561 -9.4884758 -9.0709209][-7.1267905 -8.25941 -8.9279642 -7.2196431 -5.3825235 -3.1871438 -2.4139378 -2.3959851 -2.4448659 -2.6473968 -3.1460106 -5.1914086 -6.8006325 -10.455874 -11.160035][-10.143233 -10.265875 -8.0051632 -5.6888041 -3.4309671 -0.036252975 1.4429531 1.9466286 1.8400273 0.24381208 -1.7817361 -5.2148867 -7.6290197 -10.01847 -10.05212][-8.6404772 -7.442935 -5.1438274 -3.1407161 -0.6579752 2.7749362 4.701654 5.06457 4.2963996 1.7944603 -0.20637035 -4.0587492 -7.3172183 -10.072647 -9.8453751][-6.9717426 -6.5218887 -4.5364561 -0.49603796 2.6501756 6.2727771 8.17832 7.0983338 5.3983073 3.5317454 2.3634048 -1.270081 -4.2068267 -6.8846951 -7.0537415][-6.3683357 -5.3348246 -3.86558 -0.44063854 1.7332115 3.9143529 5.6376309 5.53875 4.8806129 3.3953023 2.8342242 0.12168264 -2.3549221 -4.6880927 -5.1203375][-6.3521557 -5.7587504 -4.0808 -1.8587439 -1.2185497 1.1313987 2.846786 3.0847468 3.0293078 1.986516 1.9109898 -0.16170597 -1.8139997 -4.3039651 -5.3355417][-8.6735382 -8.9317589 -8.7445183 -6.4790168 -4.727839 -3.2301078 -2.9835141 -2.9221244 -2.7872007 -2.7599776 -2.6367991 -5.02697 -5.5335684 -6.0526624 -6.0077248][-11.535418 -12.009887 -10.362266 -8.5858631 -8.2302055 -7.6550031 -8.4248791 -9.0313129 -8.9619179 -8.2185354 -7.4111881 -8.2884521 -7.9910526 -8.24497 -7.6505842][-11.313076 -10.360186 -8.9178467 -8.4571905 -8.1958723 -7.5870361 -7.4302068 -7.8116393 -8.8865414 -8.1439333 -7.1934953 -6.9968987 -6.3892174 -7.0415406 -6.7817025][-8.8128185 -9.6030636 -7.9889116 -6.2633715 -5.548594 -5.48089 -5.6173697 -5.19297 -5.1618485 -5.2730856 -5.4747453 -4.5714841 -4.4167757 -5.4872966 -4.9880614][-6.1477866 -6.600564 -5.6565976 -4.4426575 -4.1594205 -4.1766467 -4.1033969 -4.09146 -3.9234571 -3.0577271 -3.130723 -3.9406447 -3.5134735 -4.9073763 -5.9833345]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 15:16:26.218303: step 27010, loss = 0.33, batch loss = 0.28 (12.6 examples/sec; 0.636 sec/batch; 53h:57m:28s remains)
INFO - root - 2017-12-15 15:16:32.785168: step 27020, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 56h:59m:59s remains)
INFO - root - 2017-12-15 15:16:39.384109: step 27030, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 54h:49m:36s remains)
INFO - root - 2017-12-15 15:16:45.889931: step 27040, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 54h:21m:12s remains)
INFO - root - 2017-12-15 15:16:52.470685: step 27050, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 56h:40m:40s remains)
INFO - root - 2017-12-15 15:16:59.174337: step 27060, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 56h:00m:08s remains)
INFO - root - 2017-12-15 15:17:05.806291: step 27070, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.684 sec/batch; 58h:04m:03s remains)
INFO - root - 2017-12-15 15:17:12.390209: step 27080, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 54h:18m:59s remains)
INFO - root - 2017-12-15 15:17:18.922131: step 27090, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 55h:36m:22s remains)
INFO - root - 2017-12-15 15:17:25.512190: step 27100, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 55h:36m:32s remains)
2017-12-15 15:17:26.017731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2154779 -3.1696317 -3.5753329 -3.6899958 -4.4701924 -5.0640416 -5.524333 -5.889472 -6.2768488 -7.1377048 -7.3735 -7.4530482 -8.352623 -9.2257357 -8.0248947][-4.2949777 -3.1366036 -3.4743869 -3.9886127 -4.7422395 -5.2095628 -5.4130278 -6.0196123 -5.9285412 -6.3198085 -6.6394105 -7.9807873 -9.0889 -10.155495 -8.8818636][-4.2964144 -4.4576378 -5.2177238 -4.9033909 -4.731463 -4.8531203 -5.4037795 -6.2053456 -6.1820917 -5.738256 -5.3218346 -5.6282458 -7.2527981 -10.439581 -9.9444838][-5.2038426 -6.5866652 -7.6796894 -8.0307312 -8.2676868 -6.9826264 -5.6856537 -5.2232661 -5.2883511 -5.4441571 -5.685245 -5.4951653 -6.5433969 -8.3303623 -8.2453384][-7.1366439 -9.4808464 -10.938625 -10.738338 -9.7589912 -6.8485718 -4.6630073 -4.5044837 -4.340055 -4.5727515 -5.0092955 -5.481729 -6.5891786 -7.4360595 -6.5782566][-7.1807303 -9.3344851 -10.639174 -10.499016 -8.8211441 -3.4455116 0.19380426 0.59752369 0.13936472 -1.2812705 -2.627681 -3.0163403 -4.8991857 -6.2998605 -5.015749][-6.4796829 -7.0207491 -8.4288607 -8.6786947 -7.1467223 -1.8033619 2.9233594 4.5686908 4.3553758 0.92972374 -1.9943745 -2.8863704 -5.0590463 -6.7618752 -6.2116957][-5.2935033 -5.4613228 -6.3023643 -6.1589603 -4.4992528 -0.52333069 4.0589747 6.6706223 6.441504 2.297091 -1.0879421 -3.5256608 -6.4038062 -8.02712 -7.7287035][-4.3249235 -4.0965738 -5.1087055 -4.5930905 -4.1154709 -1.7092953 1.6839633 4.09057 4.7741094 2.74748 0.24908257 -3.747313 -7.8513813 -10.396486 -10.151654][-5.462429 -5.9202809 -6.4762712 -5.4822621 -5.5959048 -3.5811243 -1.7872214 -0.84803438 -0.81901455 -1.5607967 -2.445524 -5.3075724 -8.9379692 -12.016289 -12.354694][-9.2919617 -9.6602421 -10.513723 -9.43255 -9.22377 -7.9493961 -6.8798037 -6.0807524 -6.3800473 -7.0030956 -7.2770462 -8.8787212 -11.517397 -13.160365 -12.891445][-13.124603 -13.293974 -13.032219 -11.946274 -11.448303 -10.857267 -10.933062 -10.419439 -10.6528 -10.764902 -11.214141 -11.967965 -12.410778 -12.868299 -11.695501][-14.102064 -13.34425 -12.514338 -11.09402 -10.701862 -9.5695 -9.901659 -10.543694 -11.125869 -11.419518 -11.620174 -12.729328 -13.315922 -12.005047 -9.1264763][-10.483978 -10.710093 -10.020803 -8.9383535 -8.2750168 -7.8555336 -7.3428488 -7.6714134 -8.6108494 -9.7639771 -10.141677 -10.294183 -10.37279 -9.9556541 -8.6405106][-7.3041558 -7.2742333 -6.2269945 -5.2471366 -4.050344 -4.7406158 -4.7107973 -4.0318451 -4.3916059 -5.2253342 -6.4565573 -7.877861 -9.1649 -9.0389252 -7.3720779]]...]
INFO - root - 2017-12-15 15:17:32.618070: step 27110, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 56h:10m:48s remains)
INFO - root - 2017-12-15 15:17:39.217434: step 27120, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 57h:14m:45s remains)
INFO - root - 2017-12-15 15:17:45.808898: step 27130, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 54h:43m:33s remains)
INFO - root - 2017-12-15 15:17:52.476181: step 27140, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 55h:15m:06s remains)
INFO - root - 2017-12-15 15:17:59.067267: step 27150, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 56h:12m:57s remains)
INFO - root - 2017-12-15 15:18:05.737352: step 27160, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.682 sec/batch; 57h:49m:37s remains)
INFO - root - 2017-12-15 15:18:12.339097: step 27170, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 55h:49m:12s remains)
INFO - root - 2017-12-15 15:18:18.883405: step 27180, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 56h:09m:16s remains)
INFO - root - 2017-12-15 15:18:25.449272: step 27190, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 56h:01m:50s remains)
INFO - root - 2017-12-15 15:18:31.978385: step 27200, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 55h:12m:00s remains)
2017-12-15 15:18:32.505994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2259727 -7.7521157 -9.3639622 -10.793073 -12.533337 -11.826506 -10.336782 -8.610157 -8.3543549 -9.6478539 -10.847292 -13.893503 -17.191444 -19.185661 -17.271502][-6.2424722 -6.9296937 -7.8286514 -9.3657207 -11.024467 -11.43153 -10.580381 -9.3061676 -9.17343 -9.0564766 -9.3725529 -10.844198 -14.731852 -17.479233 -16.445366][-4.1187825 -5.2492824 -7.3391323 -9.29534 -10.914592 -11.519979 -10.756815 -9.0637836 -7.6867204 -7.8282456 -8.9078941 -10.530267 -12.504779 -13.556099 -13.41781][-4.8238897 -5.2056193 -6.440546 -7.1332541 -8.2119122 -7.8057909 -7.5911794 -7.7223639 -7.5804625 -6.7948928 -6.2423334 -8.6966667 -11.059709 -13.186659 -11.116884][-4.3013306 -5.0726748 -6.2380972 -5.8454089 -5.8131309 -4.0084209 -2.6510854 -3.0882885 -3.6046603 -3.6525059 -4.04865 -5.5395832 -8.51862 -13.718248 -13.536751][-5.43779 -4.571856 -5.1528249 -3.3798387 -1.4161243 1.2411547 3.5730224 3.9301124 2.1814947 0.056699276 -1.932584 -2.5223191 -6.2930489 -9.8373566 -13.215626][-5.5013132 -4.28504 -3.2767 -2.0345125 0.038774967 3.3425279 6.8097014 8.2599926 6.8819156 3.7748742 0.90223026 -0.80712366 -2.7790997 -7.4024467 -9.1969538][-5.5400639 -3.7577939 -3.3175066 -1.4672441 0.733233 4.9473319 8.0584106 8.3339157 7.2737613 5.6417737 3.3647628 -0.12783957 -4.3615365 -7.6447549 -7.2686939][-5.2369504 -3.3847463 -3.0325017 -1.379848 -0.77317667 2.110723 6.4194255 7.5403695 7.4687667 4.1861749 0.59622526 -2.0203922 -7.8525381 -10.756994 -11.011379][-5.0033321 -4.9583015 -4.1718631 -2.7743962 -2.3794465 -0.035423279 2.3331065 3.8754334 2.650537 1.6131244 -0.49672318 -5.9625754 -10.873341 -15.80023 -17.495819][-10.183302 -9.3465691 -8.5263023 -7.0581083 -5.898006 -5.3207626 -4.7477903 -3.9678278 -4.1435328 -4.8612857 -6.42607 -10.663203 -16.405886 -19.584419 -19.927687][-12.523357 -11.100992 -10.646778 -10.428562 -9.0365467 -7.4165759 -7.7329664 -8.8014975 -10.571877 -10.367261 -11.354959 -13.022324 -15.093912 -19.737352 -18.838984][-14.770266 -12.758373 -12.149343 -10.308075 -10.785271 -10.069927 -9.2966938 -9.6850042 -9.95779 -10.475063 -12.126999 -12.501863 -15.800329 -16.801134 -14.525763][-12.776669 -12.668853 -11.405512 -9.7126913 -10.371887 -10.134354 -9.3241425 -7.7048016 -6.7900739 -7.7267303 -8.6995544 -9.3438311 -9.953371 -9.5171661 -10.223888][-8.2046509 -7.6894627 -7.67845 -7.5226784 -5.7687683 -6.8245807 -8.0391035 -9.384284 -9.2277 -8.5878544 -7.4476357 -7.1258612 -8.2713537 -8.1912451 -8.120409]]...]
INFO - root - 2017-12-15 15:18:39.069002: step 27210, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.687 sec/batch; 58h:17m:18s remains)
INFO - root - 2017-12-15 15:18:45.677001: step 27220, loss = 0.11, batch loss = 0.06 (11.8 examples/sec; 0.681 sec/batch; 57h:43m:47s remains)
INFO - root - 2017-12-15 15:18:52.185954: step 27230, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 56h:01m:44s remains)
INFO - root - 2017-12-15 15:18:58.732162: step 27240, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 55h:39m:36s remains)
INFO - root - 2017-12-15 15:19:05.369888: step 27250, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 56h:35m:16s remains)
INFO - root - 2017-12-15 15:19:11.954858: step 27260, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 56h:14m:42s remains)
INFO - root - 2017-12-15 15:19:18.608781: step 27270, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.681 sec/batch; 57h:44m:18s remains)
INFO - root - 2017-12-15 15:19:25.227202: step 27280, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.687 sec/batch; 58h:16m:41s remains)
INFO - root - 2017-12-15 15:19:31.881831: step 27290, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 56h:31m:28s remains)
INFO - root - 2017-12-15 15:19:38.520989: step 27300, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 55h:45m:04s remains)
2017-12-15 15:19:39.230234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.758213 -4.9231572 -4.6959167 -4.5039196 -5.3710566 -5.8442974 -6.6049371 -7.2147756 -7.0979147 -7.2787337 -6.078362 -5.7250075 -4.961081 -4.8490858 -3.6908193][-4.1998696 -3.4752228 -2.7363567 -3.3664236 -4.6710653 -5.3858604 -6.4999046 -7.9472408 -7.9345908 -7.553051 -5.7745123 -4.3266764 -4.0932584 -5.3554959 -3.8885441][-3.0825131 -3.3763707 -4.0547833 -4.2641106 -5.4616504 -6.344183 -6.3692913 -6.6121225 -6.7160068 -5.8867445 -4.3463 -3.8493669 -4.9827323 -6.3613014 -4.7058558][-3.4412239 -4.2271976 -6.1753826 -6.522244 -6.9648447 -6.6956944 -6.4880128 -7.2035117 -7.0491567 -5.5201926 -3.6927037 -3.3375244 -4.416028 -6.1490564 -6.3941531][-6.7266083 -7.420805 -8.0905972 -8.1104088 -7.4980249 -5.5117326 -3.2781825 -4.0453424 -4.6797948 -3.628871 -1.6697602 -1.2922945 -3.8335264 -5.9131169 -5.2149992][-7.055058 -7.7707787 -8.3739882 -7.8116655 -7.5420241 -2.8702869 1.3061194 1.3577309 0.20963526 -1.5156307 -1.6504788 -2.2566452 -3.7691529 -5.5675135 -4.472579][-6.9424334 -7.7036405 -8.4715929 -7.0413036 -4.7189193 -0.44761753 4.1091657 6.3092284 6.0705333 1.4685664 -1.528604 -2.7893755 -5.2036362 -6.4069 -4.3729792][-7.2047906 -6.8886137 -6.3438873 -5.4965587 -4.2806959 0.35101461 4.4054465 6.5038047 6.2912126 1.9933753 -0.863678 -4.382381 -7.3832703 -7.8648467 -4.9367776][-8.5416708 -8.4326992 -6.9495268 -4.4167285 -3.7016821 -3.2965546 -1.6687393 0.57491446 1.8010049 -0.11367893 -2.4475574 -5.2551503 -8.0372629 -10.054537 -8.2506189][-11.033353 -11.955313 -11.833617 -8.1606464 -6.1661239 -3.7776728 -3.214695 -2.5805583 -1.9808576 -1.704555 -2.3093445 -5.5015373 -9.2885675 -11.928362 -10.098185][-14.532772 -15.445858 -15.555269 -14.21262 -12.743126 -8.1365013 -6.6301022 -5.2850819 -4.0775189 -4.3776512 -4.3156023 -6.1524539 -9.2659454 -11.443918 -10.094492][-17.000652 -18.370998 -17.517426 -15.877546 -15.315714 -13.119055 -11.740677 -8.0381289 -6.2382421 -4.1054296 -2.6505563 -4.4194331 -7.2871881 -8.9299221 -8.1780872][-16.789703 -17.116461 -15.626968 -13.411696 -13.098295 -12.394899 -10.424841 -7.7230768 -5.9070492 -2.8993821 -2.4633355 -3.5223982 -4.3142385 -4.6027727 -4.5266085][-11.14205 -12.506539 -13.614281 -12.397918 -11.024626 -10.424936 -9.8504677 -8.4303532 -6.5066357 -3.502068 -3.0113616 -1.3384695 -1.1221428 -2.0427625 -1.9088585][-8.7949677 -8.3933592 -7.5560317 -8.2326393 -9.3869362 -8.444953 -7.6615462 -7.0709348 -5.6964827 -4.7455378 -4.0554709 -3.4235477 -2.6780012 -2.3101976 -1.2943821]]...]
INFO - root - 2017-12-15 15:19:45.766407: step 27310, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 57h:03m:22s remains)
INFO - root - 2017-12-15 15:19:52.300296: step 27320, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 54h:47m:39s remains)
INFO - root - 2017-12-15 15:19:59.022091: step 27330, loss = 0.14, batch loss = 0.10 (11.4 examples/sec; 0.704 sec/batch; 59h:38m:18s remains)
INFO - root - 2017-12-15 15:20:05.585028: step 27340, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.663 sec/batch; 56h:14m:06s remains)
INFO - root - 2017-12-15 15:20:12.257182: step 27350, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.689 sec/batch; 58h:21m:37s remains)
INFO - root - 2017-12-15 15:20:18.913685: step 27360, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 54h:51m:33s remains)
INFO - root - 2017-12-15 15:20:25.489751: step 27370, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 54h:27m:01s remains)
INFO - root - 2017-12-15 15:20:32.089091: step 27380, loss = 0.21, batch loss = 0.17 (11.6 examples/sec; 0.687 sec/batch; 58h:12m:46s remains)
INFO - root - 2017-12-15 15:20:38.707991: step 27390, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 56h:41m:34s remains)
INFO - root - 2017-12-15 15:20:45.301342: step 27400, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 55h:18m:43s remains)
2017-12-15 15:20:45.815820: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.2543373 -8.3846912 -9.5359325 -10.001278 -10.896696 -10.891964 -9.3808031 -7.742517 -6.4513097 -6.4601116 -6.5251837 -8.30143 -12.043774 -11.21578 -9.8665733][-8.1430206 -6.4252162 -5.6986403 -6.6559734 -8.3453312 -8.5940342 -7.7852135 -6.9598937 -6.2449436 -5.8732843 -5.8495188 -8.8081608 -12.787974 -12.80533 -12.712307][-6.0298653 -6.2572131 -7.2212968 -6.6174207 -7.9677496 -8.73264 -8.4006033 -7.4120851 -6.6103845 -6.4600363 -6.0288081 -9.1806507 -13.683674 -13.971607 -13.79454][-6.0199523 -5.8382144 -6.3036757 -6.6133332 -7.7351303 -6.8608055 -6.6882949 -7.1798229 -6.8922563 -5.6275182 -5.3340125 -8.596014 -12.846762 -14.053171 -14.657883][-8.0160818 -8.7803316 -9.8515863 -7.4533229 -5.9634533 -3.4801345 -1.3953047 -4.159193 -8.2736731 -7.3450809 -5.8785706 -8.084568 -12.184038 -13.351054 -14.287348][-9.9845953 -9.1670866 -10.013831 -7.2082863 -2.9410763 0.69892454 3.9965835 2.3313603 -1.3910966 -4.234385 -6.7699366 -7.7154627 -9.8453388 -11.283144 -13.042586][-12.054347 -10.869846 -8.3066063 -4.3852348 -1.269999 3.5436444 7.7942863 7.1643548 4.2488465 -0.28042936 -5.0227013 -7.4430804 -11.334758 -12.305565 -13.223145][-12.617357 -11.373018 -9.7403622 -5.032167 -1.3573785 3.8835864 8.3770294 6.551064 4.2879252 1.5830336 -2.5553758 -6.2916093 -11.322906 -12.799706 -13.904757][-8.3992462 -8.0494347 -8.5978928 -6.9453068 -3.7613392 0.99360085 5.3485951 4.7471538 3.7587848 0.98314285 -2.8147697 -6.8287978 -11.918095 -13.095291 -14.50666][-7.453177 -6.4955893 -7.8112307 -6.997508 -4.4907379 -2.8919954 0.7934618 1.8050551 0.63902092 -2.1559436 -4.6152425 -7.0656028 -11.801039 -13.54572 -16.06082][-13.766575 -12.297389 -11.6423 -10.730192 -9.8128109 -9.7676535 -6.9801321 -5.7253747 -6.2871256 -6.1859155 -7.7770863 -10.998659 -13.609346 -14.080168 -15.563663][-18.781889 -17.716549 -17.113924 -15.485615 -15.148876 -14.347652 -13.190113 -13.064665 -11.788364 -10.915105 -11.988792 -12.50527 -14.193146 -15.313778 -15.107479][-20.043734 -19.582949 -19.371391 -17.824875 -17.274307 -16.017492 -15.700468 -16.120876 -16.164564 -15.278999 -14.491739 -13.638459 -13.743523 -13.184313 -13.115946][-16.402321 -16.006025 -16.02161 -14.705133 -13.270386 -13.025703 -13.207503 -13.093559 -13.505845 -13.572074 -13.239389 -11.926226 -12.176882 -11.605905 -11.522276][-12.69213 -12.273381 -11.219388 -10.252314 -9.3897057 -9.2198067 -8.1208458 -9.165658 -10.062555 -10.147797 -10.565035 -11.16301 -12.678247 -11.806689 -12.698397]]...]
INFO - root - 2017-12-15 15:20:52.379004: step 27410, loss = 0.11, batch loss = 0.07 (12.7 examples/sec; 0.628 sec/batch; 53h:13m:10s remains)
INFO - root - 2017-12-15 15:20:58.897619: step 27420, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 55h:47m:33s remains)
INFO - root - 2017-12-15 15:21:05.425594: step 27430, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 56h:20m:01s remains)
INFO - root - 2017-12-15 15:21:12.062101: step 27440, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 54h:21m:57s remains)
INFO - root - 2017-12-15 15:21:18.643485: step 27450, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 56h:23m:21s remains)
INFO - root - 2017-12-15 15:21:25.197852: step 27460, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 56h:03m:26s remains)
INFO - root - 2017-12-15 15:21:31.810757: step 27470, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 54h:18m:01s remains)
INFO - root - 2017-12-15 15:21:38.443486: step 27480, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 57h:33m:11s remains)
INFO - root - 2017-12-15 15:21:45.001750: step 27490, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 56h:22m:48s remains)
INFO - root - 2017-12-15 15:21:51.612906: step 27500, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 54h:18m:26s remains)
2017-12-15 15:21:52.132832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3754368 -5.2421227 -4.203795 -3.3737557 -3.5628691 -4.1635609 -4.9959373 -5.8840246 -6.1332135 -5.5631237 -4.89581 -7.2189646 -8.8151016 -8.0721912 -7.7451377][-4.8678136 -4.4196906 -3.8004169 -3.2428961 -3.4681454 -4.7688708 -5.5581059 -6.2979584 -6.8849516 -6.8159752 -6.0675211 -8.49707 -11.175511 -10.478734 -9.9528666][-2.9480734 -3.1256576 -2.0706153 -1.3438072 -2.5045533 -3.2920456 -4.5517759 -5.4335041 -5.8231626 -6.0271091 -5.6591954 -8.3022938 -10.924172 -11.007517 -10.324896][-3.0746722 -2.252569 -0.53722811 0.30685091 -0.80464554 -1.4137206 -2.6037688 -3.6995351 -4.3737483 -3.5161271 -3.1665008 -6.1435623 -9.2049713 -9.3654537 -9.2303047][-3.7002478 -2.7560387 -1.2296233 0.49253607 0.38748264 0.076663017 -0.5801053 -1.3191285 -1.5105844 -1.2448807 -0.94198895 -3.8122828 -6.8507214 -7.2980833 -8.026619][-5.9037547 -4.5114636 -2.4326859 -0.56654119 0.62885666 1.2450361 1.1527262 0.58126259 0.86041451 1.3672729 1.0279117 -2.142375 -5.2638235 -6.1761751 -7.606226][-6.7815733 -5.691432 -3.869859 -0.51211071 1.2804651 2.3532662 3.1252027 3.5806165 4.1960292 3.8315463 3.3171067 -0.33279657 -3.8209748 -4.4783874 -5.8196607][-4.6758671 -3.0731804 -1.4721441 1.3610811 2.4453292 3.0187907 3.951324 4.4143987 5.0952363 5.4590154 4.9878058 1.5787473 -2.2014055 -3.5309188 -4.1853046][-4.2287288 -2.5130355 -0.53257084 1.7810116 2.0939631 2.4696174 3.2848897 4.3893256 5.059629 4.9738765 4.5585952 1.586328 -2.0168967 -2.2293749 -3.1255198][-3.8519545 -2.7749343 -1.0171919 0.49523878 0.64150286 0.53338909 0.92177868 1.4176674 2.1228995 2.769712 2.264791 0.18293619 -1.8525782 -2.2337186 -2.8558769][-7.0718174 -5.7648349 -4.7929487 -3.5253701 -3.4195259 -4.0461416 -3.7737823 -2.850405 -1.3484907 -0.75771427 -1.155829 -4.1852217 -5.4874139 -3.6599128 -3.3268003][-10.814227 -9.8614006 -9.3374758 -8.3676224 -7.8642006 -7.3906355 -7.2479119 -6.4789376 -5.2535129 -4.4663019 -4.5702276 -6.1696544 -6.5506034 -5.3454046 -4.9351263][-12.261818 -11.718237 -10.509094 -10.010847 -9.634409 -8.6065779 -7.6444931 -6.7756042 -6.5462251 -6.0548306 -5.9522424 -6.7186646 -6.6647539 -4.5141215 -3.4157567][-9.5816727 -9.695878 -9.5773764 -8.7635746 -7.7582531 -6.518589 -5.6623812 -5.05831 -5.2636919 -6.0531645 -5.3659654 -4.9048138 -5.0141826 -4.2628565 -3.7801275][-6.2515807 -6.6619563 -7.3342886 -6.107162 -5.0263758 -4.4994626 -2.8373015 -2.693815 -2.8775718 -2.1706545 -2.5408089 -3.4563661 -3.7509851 -4.0490904 -4.4462681]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 15:21:59.859312: step 27510, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 55h:19m:41s remains)
INFO - root - 2017-12-15 15:22:06.463245: step 27520, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 56h:24m:38s remains)
INFO - root - 2017-12-15 15:22:13.054873: step 27530, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 55h:51m:15s remains)
INFO - root - 2017-12-15 15:22:19.630230: step 27540, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 56h:10m:10s remains)
INFO - root - 2017-12-15 15:22:26.190982: step 27550, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 54h:50m:37s remains)
INFO - root - 2017-12-15 15:22:32.867906: step 27560, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 55h:55m:22s remains)
INFO - root - 2017-12-15 15:22:39.434961: step 27570, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 54h:34m:01s remains)
INFO - root - 2017-12-15 15:22:45.918859: step 27580, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.636 sec/batch; 53h:49m:52s remains)
INFO - root - 2017-12-15 15:22:52.474933: step 27590, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 56h:06m:51s remains)
INFO - root - 2017-12-15 15:22:59.063493: step 27600, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 54h:34m:39s remains)
2017-12-15 15:22:59.609893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9591374 -5.7994823 -7.5110908 -8.1547356 -8.4861832 -8.641923 -8.80445 -8.9792566 -9.1003151 -9.3431749 -9.3412285 -9.0770683 -11.527908 -11.754086 -11.097359][-3.3651478 -4.898797 -7.1711807 -8.5511875 -8.8999319 -7.8581276 -7.1745605 -7.4776258 -8.2214308 -8.6545343 -8.5801048 -9.34926 -11.918036 -11.926293 -11.050941][-2.2394607 -5.0207577 -8.2947912 -8.8775005 -8.7122135 -7.8930054 -7.0732431 -6.4651451 -8.1012621 -9.04793 -8.4320755 -8.3374434 -11.663387 -12.248259 -11.226732][-5.1363211 -6.4169884 -9.4144783 -10.715477 -10.719346 -7.4533615 -5.1642756 -5.9102039 -7.3275776 -8.5297279 -9.3494492 -9.5758286 -11.984983 -13.166309 -12.871809][-8.253 -11.411135 -13.831762 -14.338694 -12.024235 -5.118588 -0.65882158 -3.4185743 -8.3054018 -8.6313095 -8.7303085 -10.124962 -13.332817 -14.494209 -14.898592][-10.280947 -13.539533 -15.500772 -13.547507 -10.045612 -2.7539663 3.7038569 3.4825053 -0.53486347 -5.57018 -10.007812 -8.9422731 -11.561068 -13.971642 -14.505806][-10.67309 -11.399471 -12.155636 -8.9752216 -3.16803 3.130899 9.1162071 10.224237 8.1509571 -0.25973034 -7.2593684 -7.2919145 -11.072287 -13.481913 -13.911455][-10.463814 -11.453482 -12.12251 -8.2013712 -0.99637318 7.636188 13.800047 12.574001 10.241554 3.7264419 -2.2083952 -5.8722558 -11.423866 -12.383834 -12.47785][-7.8644786 -9.1810875 -10.318495 -8.7225885 -3.553643 4.6381364 11.747406 11.386946 6.8618817 0.67936039 -3.8670273 -7.3604045 -12.212833 -13.0871 -13.115013][-6.8004727 -8.0435724 -9.0353432 -9.2833319 -8.1391926 -2.7815289 3.7910695 6.1719432 3.69239 -2.9300213 -7.58786 -9.50709 -13.926628 -15.114552 -14.580153][-9.199913 -10.495064 -12.460624 -12.678162 -11.0968 -10.760541 -8.1820135 -5.640873 -5.7057834 -7.4217844 -10.429811 -13.419904 -15.555275 -15.583488 -14.704466][-14.776932 -15.166883 -15.647987 -15.319506 -14.417053 -14.701532 -14.970078 -14.390676 -12.918997 -13.221539 -15.027504 -15.583902 -16.187044 -15.155811 -14.218632][-16.2642 -16.834923 -16.893396 -16.260668 -15.484932 -15.146282 -15.032152 -15.606573 -16.148417 -14.444927 -13.330482 -14.066425 -15.418404 -14.310316 -12.157831][-15.713627 -15.895506 -15.80308 -14.691759 -13.138048 -12.499288 -12.946201 -13.044531 -12.968838 -12.832508 -13.128672 -11.79262 -12.035635 -11.802067 -10.568522][-10.778784 -10.971411 -9.7949867 -8.7915144 -8.2098284 -7.6005716 -7.8307633 -7.6002059 -8.3862228 -8.7825451 -9.2346115 -10.105728 -11.050534 -11.566292 -12.13228]]...]
INFO - root - 2017-12-15 15:23:06.248944: step 27610, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 55h:57m:00s remains)
INFO - root - 2017-12-15 15:23:12.939820: step 27620, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 54h:26m:38s remains)
INFO - root - 2017-12-15 15:23:19.587667: step 27630, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 55h:28m:03s remains)
INFO - root - 2017-12-15 15:23:26.157979: step 27640, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 55h:57m:23s remains)
INFO - root - 2017-12-15 15:23:32.769287: step 27650, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 55h:18m:29s remains)
INFO - root - 2017-12-15 15:23:39.425560: step 27660, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 55h:51m:59s remains)
INFO - root - 2017-12-15 15:23:45.974097: step 27670, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 55h:38m:45s remains)
INFO - root - 2017-12-15 15:23:52.531295: step 27680, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 57h:21m:21s remains)
INFO - root - 2017-12-15 15:23:59.164442: step 27690, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 56h:26m:37s remains)
INFO - root - 2017-12-15 15:24:05.743897: step 27700, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 55h:47m:01s remains)
2017-12-15 15:24:06.257070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9112706 -8.5595989 -8.4909668 -8.4417686 -9.0864773 -10.141237 -10.309954 -9.2236366 -8.155426 -6.8272686 -5.6192927 -6.3966408 -8.2873592 -9.1474762 -9.0291739][-7.5657463 -9.153676 -8.703537 -8.1928453 -8.8339672 -10.677032 -11.35171 -10.997206 -10.454345 -9.5367517 -8.8928089 -10.24544 -11.184323 -10.699192 -9.4964542][-6.81216 -8.0025654 -8.1701927 -8.0128536 -8.7965937 -9.1675634 -9.6959257 -9.9490414 -9.766078 -9.5072241 -9.3654861 -11.072108 -13.114771 -12.95978 -11.603012][-9.8034039 -10.875278 -10.378787 -9.2866917 -8.7240734 -8.8246574 -8.6644983 -8.40185 -8.1632843 -8.7300339 -9.9687824 -12.213546 -14.056076 -13.527257 -12.218998][-11.256508 -13.804245 -14.082466 -9.9965935 -6.060976 -4.6083646 -4.3328509 -5.4481125 -6.8809805 -7.4936242 -8.0026236 -10.478862 -13.470342 -13.900862 -13.14811][-13.717916 -14.567989 -13.648026 -10.63747 -5.2956333 1.2700286 5.1177535 2.2416744 -1.60108 -3.7311966 -6.1075439 -9.4599724 -12.449476 -12.718651 -12.250342][-12.69745 -12.942893 -11.022429 -7.1531205 -2.6781588 3.9759507 9.2954941 8.3413773 5.2122808 -0.70217991 -5.8491383 -7.5935559 -10.016665 -11.41182 -10.781212][-13.80076 -12.860861 -10.40213 -6.0108418 -0.040689945 5.2479892 9.720623 10.192234 7.8365035 1.2293038 -4.7806268 -8.5275679 -11.171469 -9.99421 -8.2861557][-12.499923 -12.784388 -10.347736 -6.7775741 -2.8068433 2.47051 6.0226407 6.1599917 3.9339309 -1.1700416 -5.2038145 -9.07171 -12.374498 -10.861726 -8.3047132][-10.372159 -10.630705 -10.497702 -8.6867752 -5.9931607 -2.0822649 1.6845765 2.3694434 0.073417187 -4.0191603 -6.7065744 -10.686596 -13.439138 -12.702019 -11.420996][-12.49543 -14.034237 -14.034924 -12.38969 -10.650549 -7.6996431 -4.9190044 -4.2072153 -4.9955268 -7.368207 -9.5685825 -12.766161 -14.162489 -13.174641 -11.764691][-15.473722 -17.006926 -16.491453 -14.697039 -13.051538 -11.420471 -10.120008 -8.6322479 -8.2630577 -10.25205 -12.034706 -12.987898 -13.885468 -13.552207 -11.959723][-17.849052 -18.0311 -17.514811 -16.517754 -14.945969 -13.770054 -11.914793 -10.796482 -10.849216 -11.048073 -11.925676 -13.047461 -13.075788 -12.219196 -10.786068][-13.368914 -13.708247 -13.779289 -13.367161 -12.341583 -11.657786 -11.73028 -11.577845 -10.773288 -10.524429 -11.060385 -10.378982 -10.975975 -11.180025 -10.687726][-8.1481934 -8.4495087 -7.7979569 -7.1510372 -6.8020144 -6.236021 -6.1499796 -6.9565477 -8.0602446 -8.9973345 -9.754878 -10.262843 -11.323565 -12.064236 -12.417776]]...]
INFO - root - 2017-12-15 15:24:12.877596: step 27710, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 54h:40m:35s remains)
INFO - root - 2017-12-15 15:24:19.482422: step 27720, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 54h:59m:27s remains)
INFO - root - 2017-12-15 15:24:26.084520: step 27730, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 54h:24m:07s remains)
INFO - root - 2017-12-15 15:24:32.740757: step 27740, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 56h:23m:51s remains)
INFO - root - 2017-12-15 15:24:39.468720: step 27750, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 56h:30m:33s remains)
INFO - root - 2017-12-15 15:24:46.108499: step 27760, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 55h:56m:55s remains)
INFO - root - 2017-12-15 15:24:52.705019: step 27770, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 54h:45m:21s remains)
INFO - root - 2017-12-15 15:24:59.277441: step 27780, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 55h:34m:10s remains)
INFO - root - 2017-12-15 15:25:05.959113: step 27790, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 56h:30m:42s remains)
INFO - root - 2017-12-15 15:25:12.593312: step 27800, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 57h:02m:47s remains)
2017-12-15 15:25:13.101523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8232379 -5.6657019 -5.4414339 -5.4375453 -6.5038362 -7.9075093 -8.6650991 -8.5278091 -7.033505 -5.0997148 -3.5320344 -5.03359 -5.6908779 -6.3532033 -7.5069437][-3.9164658 -4.2513189 -5.1213279 -6.0278411 -7.6255908 -9.3099 -11.089981 -12.172019 -11.598324 -10.046777 -7.6810074 -7.5327535 -7.9605684 -8.4559231 -8.3736019][-2.2247276 -4.7517252 -6.3033447 -6.8408976 -9.4473362 -10.451269 -11.341133 -11.964813 -12.23282 -12.116125 -9.9949627 -11.259051 -12.158827 -10.495035 -10.341593][-3.9851508 -5.2696581 -6.1561556 -6.7929735 -8.0450478 -8.623395 -9.0495777 -10.164125 -10.716749 -10.168137 -9.5365658 -11.73371 -12.718719 -12.820856 -13.267921][-5.3335166 -8.23353 -9.2985935 -7.4414463 -6.7784042 -4.4983826 -3.3544192 -5.1554356 -6.5671015 -7.1726832 -6.8203325 -8.5927658 -11.089162 -12.861971 -13.884996][-6.4875169 -8.5055637 -8.4417686 -6.5095448 -4.1435432 -0.58297539 2.0199971 1.4903307 0.45830822 -1.5922852 -3.1087985 -5.5864716 -8.8871117 -10.899799 -12.555019][-6.1636081 -7.7430296 -7.6369362 -5.0840688 -2.4018557 1.7649174 5.5339017 6.077024 6.4383731 2.6606441 -0.94924259 -4.370182 -8.0454445 -10.198975 -12.194979][-6.4705153 -6.8491063 -7.1169157 -4.356883 -1.7423089 2.9051671 6.9328647 6.9378781 6.5553184 3.9479146 0.52800083 -4.5814638 -9.3026667 -11.591823 -11.563574][-6.1534376 -6.4934425 -7.1476731 -5.1752796 -3.0990889 0.87706184 4.7510324 5.6647277 5.6120677 2.9714561 -0.26053047 -6.2847309 -11.887192 -12.99456 -12.835168][-6.2867584 -6.3460751 -5.8808775 -3.7445512 -2.0010607 -0.73719788 1.1206102 2.4628005 2.8936925 0.86863804 -2.2236238 -7.9922261 -12.219934 -13.341928 -14.068203][-9.30276 -8.2848177 -7.2830882 -5.4353352 -4.2666712 -3.0782514 -1.7048621 -1.516511 -2.7981048 -4.8343334 -6.7185822 -11.6852 -14.073866 -14.043434 -13.216894][-13.869894 -12.024617 -9.5786362 -7.4857903 -6.3795438 -4.9453812 -3.7872007 -5.013165 -6.9904766 -8.2815046 -10.272699 -13.705398 -14.625462 -13.348053 -12.405028][-15.552202 -13.923233 -10.87034 -8.0521049 -7.1849308 -5.7332435 -5.4056368 -6.5485168 -8.006134 -9.7912292 -11.400258 -12.632847 -12.269438 -10.888323 -9.7535973][-13.737387 -13.221981 -11.14815 -8.418788 -6.5651441 -6.7417936 -7.5651045 -7.28343 -7.5888748 -9.4619608 -10.444967 -10.422777 -10.424017 -9.38168 -8.6797085][-10.102362 -9.2307892 -8.1690235 -6.4610767 -6.1710529 -5.5392008 -5.1052432 -5.6427927 -7.1283541 -8.2165442 -8.7728605 -10.85257 -11.292122 -8.8346491 -8.1225138]]...]
INFO - root - 2017-12-15 15:25:19.704972: step 27810, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 54h:59m:48s remains)
INFO - root - 2017-12-15 15:25:26.329022: step 27820, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 55h:46m:01s remains)
INFO - root - 2017-12-15 15:25:32.932513: step 27830, loss = 0.20, batch loss = 0.16 (11.9 examples/sec; 0.673 sec/batch; 56h:57m:25s remains)
INFO - root - 2017-12-15 15:25:39.436960: step 27840, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 54h:36m:13s remains)
INFO - root - 2017-12-15 15:25:46.091913: step 27850, loss = 0.26, batch loss = 0.22 (12.5 examples/sec; 0.638 sec/batch; 53h:59m:57s remains)
INFO - root - 2017-12-15 15:25:52.689373: step 27860, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 56h:31m:46s remains)
INFO - root - 2017-12-15 15:25:59.219240: step 27870, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 56h:14m:02s remains)
INFO - root - 2017-12-15 15:26:05.838766: step 27880, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 55h:40m:57s remains)
INFO - root - 2017-12-15 15:26:12.372033: step 27890, loss = 0.20, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 53h:54m:02s remains)
INFO - root - 2017-12-15 15:26:18.843166: step 27900, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 54h:42m:13s remains)
2017-12-15 15:26:19.345630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8611856 -5.0122719 -4.2625 -3.4542451 -3.3058758 -4.491024 -6.1188865 -7.3722115 -7.6049185 -7.2037477 -6.3183079 -6.5885472 -7.2529879 -5.8814783 -4.70535][-3.9681544 -3.1516898 -2.5730751 -2.3461523 -3.0450857 -4.3007717 -5.3153124 -6.5071812 -6.8267794 -5.9394422 -4.692071 -5.2885432 -6.7151909 -6.2080235 -5.7575445][-1.1354842 -2.0287974 -3.1306336 -2.3453074 -2.3301189 -2.8698363 -4.0635338 -5.2631969 -5.4872785 -5.0142727 -4.4283342 -5.6736917 -6.6100373 -6.6708412 -7.1321917][-2.3352573 -2.9964881 -4.2641273 -4.2562766 -4.1359539 -3.146904 -2.7902346 -3.7827802 -3.9634924 -3.4825053 -3.1437464 -3.8974204 -5.5919867 -6.5768538 -6.2165875][-2.4686737 -4.4231472 -5.5522218 -4.5280428 -3.6783237 -1.202322 0.56111622 -0.33386993 -1.6683307 -2.2294686 -2.6456683 -3.4185147 -5.2133584 -6.105535 -5.8127251][-4.2107515 -5.7294478 -5.4157228 -3.2728555 -0.49409008 2.5920892 4.9431691 4.9309859 4.6424661 2.1731009 -0.61845684 -1.9344916 -4.2644186 -4.7093744 -3.9333987][-6.063364 -5.9497828 -4.4581022 -2.1280484 0.055413246 3.9683013 7.99688 8.3082523 7.8524995 4.7789969 1.16294 -1.9283645 -5.7862191 -5.9894581 -5.3653259][-5.764771 -5.6029034 -5.1714425 -3.3624203 -1.2776618 2.6469846 6.2995963 6.6528363 6.4476724 3.8945088 0.90938425 -2.8754733 -7.13076 -7.7102418 -7.1872587][-5.014102 -5.181807 -5.1112194 -3.4863827 -1.4238639 0.48199749 2.0184069 3.2128348 3.5795703 1.1339684 -1.582324 -5.1405468 -8.7752962 -9.156826 -8.34012][-4.8999395 -4.9910464 -4.9404869 -3.9425616 -2.381618 -1.9757578 -0.55073786 1.3802075 0.714262 -1.6557016 -3.756021 -6.7904835 -10.017872 -11.08532 -10.784957][-10.169453 -9.8506517 -9.13809 -8.1325417 -7.21827 -6.4854727 -4.9311728 -4.9064326 -5.8758659 -6.1097627 -7.72022 -10.628283 -12.605606 -12.697015 -11.624102][-14.435766 -14.05348 -12.857595 -11.701608 -11.410057 -10.512287 -9.4014521 -9.6540852 -9.6206923 -10.202139 -11.248585 -12.238827 -12.820684 -12.373636 -11.561836][-12.96356 -12.084397 -12.224859 -11.791336 -11.082971 -9.9200745 -9.77703 -10.059121 -10.069967 -10.358496 -10.674026 -10.294628 -9.8596516 -8.6337976 -7.0940814][-10.191628 -9.9213018 -9.530077 -7.6595306 -7.8168 -8.0864048 -7.5785704 -6.8006444 -7.0456142 -7.705864 -7.8580637 -8.02921 -7.65328 -6.1313252 -5.79354][-8.815176 -9.2172318 -8.5087729 -6.5983648 -5.8197064 -4.7053165 -3.8480377 -4.9385552 -5.8061032 -4.9744272 -5.2016239 -6.01661 -6.2748327 -5.8312068 -6.0063314]]...]
INFO - root - 2017-12-15 15:26:25.837587: step 27910, loss = 0.28, batch loss = 0.24 (12.4 examples/sec; 0.644 sec/batch; 54h:31m:03s remains)
INFO - root - 2017-12-15 15:26:32.301520: step 27920, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 55h:16m:38s remains)
INFO - root - 2017-12-15 15:26:38.913632: step 27930, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 55h:22m:44s remains)
INFO - root - 2017-12-15 15:26:45.397576: step 27940, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 53h:58m:28s remains)
INFO - root - 2017-12-15 15:26:51.950043: step 27950, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 55h:19m:25s remains)
INFO - root - 2017-12-15 15:26:58.470991: step 27960, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 54h:37m:56s remains)
INFO - root - 2017-12-15 15:27:05.018289: step 27970, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 54h:14m:52s remains)
INFO - root - 2017-12-15 15:27:11.542636: step 27980, loss = 0.29, batch loss = 0.25 (12.2 examples/sec; 0.656 sec/batch; 55h:31m:30s remains)
INFO - root - 2017-12-15 15:27:18.099980: step 27990, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 56h:23m:06s remains)
INFO - root - 2017-12-15 15:27:24.681802: step 28000, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 55h:52m:16s remains)
2017-12-15 15:27:25.196452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8531475 -7.7067108 -7.1019826 -6.5462117 -6.3208036 -6.4375567 -6.2902761 -5.55184 -4.8254952 -5.5815616 -6.7096558 -6.7917671 -8.78732 -8.944684 -9.5964394][-7.1120806 -7.128653 -6.9009275 -6.71605 -6.477551 -6.2011018 -5.7210064 -5.2621527 -4.5291834 -4.6438665 -4.7389154 -5.3131657 -7.4087839 -7.55917 -8.0673628][-5.6486135 -6.7734509 -7.659133 -7.4643793 -7.7439766 -7.613739 -6.701827 -5.6029706 -4.3952236 -4.1806841 -4.2629538 -4.2215137 -6.7609587 -7.5829182 -8.1656246][-5.20773 -5.7082868 -6.3443031 -6.6682854 -7.2543797 -6.5700045 -5.5702891 -4.998661 -4.1097031 -3.8282232 -3.8459134 -3.595058 -5.6205807 -6.554987 -7.5895109][-5.2435479 -6.2266417 -6.2255621 -5.4827948 -5.2979913 -4.7845054 -4.03551 -2.9362102 -2.8940115 -3.0977838 -3.2908626 -3.2398939 -5.0941472 -6.5586309 -7.8261919][-4.9910612 -5.268816 -4.7584443 -3.3089123 -1.9535542 -0.90209961 0.6430521 1.3626933 0.038822651 -1.3551641 -3.0143144 -2.8759332 -4.4422369 -5.9051208 -7.562665][-3.7751484 -3.5313756 -4.0212 -2.4228957 -0.22951126 1.5565305 4.0416884 4.8437438 3.8948636 1.0722728 -2.806077 -3.3980308 -5.0149374 -5.6002564 -6.3673162][-3.665277 -3.333353 -3.5412087 -2.5816088 -1.1176805 2.0924592 5.47502 5.5780826 4.7088017 2.6352611 -0.67909813 -2.5990055 -6.8091803 -8.1504936 -8.4420891][-5.0504284 -3.9927521 -3.6298451 -2.7641551 -2.5418561 -0.12526417 3.0475984 4.0234485 3.6697984 1.9352012 -0.58921432 -2.9391038 -7.995266 -10.50886 -12.113245][-5.018023 -5.5454578 -5.3142314 -3.4870443 -2.9831653 -1.6132984 0.63751888 2.223567 1.5631342 -0.17644882 -1.9738836 -3.4901147 -7.8402891 -11.994327 -14.741337][-6.3750844 -6.8332705 -7.655077 -6.3800726 -5.8095889 -5.80818 -4.8463993 -3.2582517 -2.9963288 -3.3760858 -5.050169 -6.6694 -10.765576 -13.236029 -15.149067][-9.5593319 -8.8638144 -9.315073 -9.0623455 -9.5116386 -9.5681286 -8.8670721 -8.0818424 -7.645236 -7.2954159 -8.268671 -9.3527451 -12.168955 -14.239435 -14.860409][-12.520023 -11.278602 -10.690584 -10.421057 -10.941026 -10.422906 -10.15695 -10.365829 -10.499259 -10.343363 -10.171867 -10.606623 -12.083408 -12.617517 -12.549507][-11.31093 -10.759068 -9.7535009 -8.4240637 -9.1448574 -9.1679831 -9.7559452 -9.9454966 -10.399334 -9.5919037 -9.0334854 -8.1951866 -7.8853426 -9.3682976 -10.02501][-6.5972633 -7.3432131 -6.1583056 -4.8209825 -4.2103591 -5.2314353 -5.8629651 -6.2869129 -7.5622387 -7.7491417 -8.2673512 -7.3240037 -7.6588621 -7.7854519 -7.8539858]]...]
INFO - root - 2017-12-15 15:27:31.841118: step 28010, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 56h:03m:17s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 15:27:38.385750: step 28020, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 53h:49m:27s remains)
INFO - root - 2017-12-15 15:27:45.013830: step 28030, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 54h:25m:58s remains)
INFO - root - 2017-12-15 15:27:51.594085: step 28040, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 56h:24m:06s remains)
INFO - root - 2017-12-15 15:27:58.251851: step 28050, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 54h:08m:12s remains)
INFO - root - 2017-12-15 15:28:04.842962: step 28060, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 54h:40m:06s remains)
INFO - root - 2017-12-15 15:28:11.440805: step 28070, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 55h:37m:12s remains)
INFO - root - 2017-12-15 15:28:17.983728: step 28080, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 54h:24m:09s remains)
INFO - root - 2017-12-15 15:28:24.548096: step 28090, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 55h:49m:24s remains)
INFO - root - 2017-12-15 15:28:31.069842: step 28100, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 55h:56m:58s remains)
2017-12-15 15:28:31.599552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.484314 -8.5276451 -9.48947 -8.3695316 -7.2633581 -6.9180794 -5.68678 -5.7111273 -6.3015647 -5.9113207 -4.4788342 -4.4592047 -6.8575959 -8.40344 -9.280674][-4.7814941 -6.5171747 -7.3324213 -7.3730168 -7.4319739 -4.8675137 -3.4792123 -3.4366195 -3.6769342 -4.0292015 -3.8869338 -4.522953 -7.3163986 -9.3494329 -10.361366][-4.856636 -6.7309403 -7.9852457 -7.1367207 -6.9482307 -5.69364 -3.8779831 -2.5053732 -3.4931583 -4.0847797 -3.6169803 -4.3168654 -6.9711571 -8.9850006 -10.044825][-7.5229855 -9.10726 -8.3293829 -7.1399393 -6.1760263 -5.030767 -4.5897465 -3.6153045 -2.7158933 -2.98674 -3.3483796 -4.0560551 -6.1476684 -8.5297413 -9.82605][-10.490256 -12.1559 -12.016951 -9.1688929 -6.067728 -4.318028 -3.5173044 -3.2122567 -3.6328137 -2.8685515 -1.8561256 -3.563832 -7.2534246 -10.11985 -10.275553][-9.3968649 -10.150215 -10.040869 -7.3046117 -4.0944991 -0.82151175 1.3841028 0.98648262 -0.10507202 -0.72084379 -1.7792752 -2.8967376 -5.2949681 -8.5923538 -10.591997][-8.5303 -8.3113708 -7.2376504 -3.7315605 -1.1716065 2.5867047 5.3595023 5.14663 5.0629964 3.4449 0.97448158 -0.87618303 -4.6886415 -7.5185547 -8.7271318][-6.3060441 -5.4545693 -4.0522718 -0.63203859 0.94275618 4.4986224 6.5767159 6.6554103 7.3212113 5.2034059 2.9056792 0.367486 -3.5678482 -6.9218512 -8.4470005][-3.9297094 -4.3714004 -3.225327 -0.65541744 0.680305 2.9163909 4.0383258 5.5814233 6.1987262 4.0351033 3.1733003 0.5182972 -4.5882878 -7.373754 -8.0995026][-4.6551971 -4.6167955 -4.3556571 -2.4755647 -1.096355 0.42124176 1.5767994 2.8301616 2.0205221 0.87367916 0.25092983 -2.1849601 -5.1996794 -8.0394 -9.6712208][-5.1614771 -7.3392191 -6.7414904 -5.6622987 -5.4348216 -3.9029236 -2.5446012 -1.9625223 -2.2500408 -2.9919426 -3.8061547 -5.6111274 -7.2864809 -10.18355 -11.765366][-7.9139967 -9.2296772 -8.7932873 -7.4738832 -8.20599 -8.395751 -6.424716 -4.8826575 -5.6056862 -5.6527019 -6.7785115 -7.508821 -8.3039837 -10.685278 -11.583749][-12.127598 -12.171072 -10.459275 -9.6546822 -9.7776871 -8.2480507 -8.5817108 -8.1429491 -7.3366146 -8.0776148 -9.9252739 -9.1326427 -9.6007824 -10.874311 -11.255762][-9.3862581 -9.10924 -7.2508855 -7.1523609 -6.185513 -5.9061131 -6.1755133 -5.5323668 -5.7371812 -6.5160432 -7.1020913 -6.0656428 -7.4734793 -8.893446 -7.8554883][-8.474081 -8.373991 -7.7015781 -6.4226141 -4.6757145 -4.452971 -5.2067146 -4.9654493 -4.9310894 -5.2437525 -5.6253986 -4.7052135 -5.7059088 -6.18824 -6.7475319]]...]
INFO - root - 2017-12-15 15:28:38.144125: step 28110, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 55h:41m:31s remains)
INFO - root - 2017-12-15 15:28:44.728778: step 28120, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 56h:05m:53s remains)
INFO - root - 2017-12-15 15:28:51.217936: step 28130, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 55h:20m:00s remains)
INFO - root - 2017-12-15 15:28:57.793440: step 28140, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 56h:05m:19s remains)
INFO - root - 2017-12-15 15:29:04.344261: step 28150, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 54h:29m:51s remains)
INFO - root - 2017-12-15 15:29:10.943077: step 28160, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 55h:59m:35s remains)
INFO - root - 2017-12-15 15:29:17.543397: step 28170, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 55h:08m:13s remains)
INFO - root - 2017-12-15 15:29:24.146149: step 28180, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.664 sec/batch; 56h:07m:51s remains)
INFO - root - 2017-12-15 15:29:30.675763: step 28190, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 56h:27m:31s remains)
INFO - root - 2017-12-15 15:29:37.255446: step 28200, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.661 sec/batch; 55h:51m:00s remains)
2017-12-15 15:29:37.763683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8973761 -6.7106056 -8.1406174 -8.6246834 -9.3556232 -10.62812 -11.684324 -11.147999 -10.721383 -10.361504 -9.3001881 -11.253679 -13.266401 -13.828592 -14.526484][-7.3295 -8.30715 -9.3374615 -10.057913 -10.445353 -11.31287 -12.2222 -13.41689 -13.85137 -12.275599 -11.033046 -13.008604 -14.450594 -14.686815 -14.576769][-4.626255 -7.5299044 -10.350973 -10.207272 -9.7871981 -11.257008 -12.862186 -12.957775 -12.388832 -12.613462 -12.509751 -13.028584 -13.903336 -14.742306 -15.296986][-8.6476135 -11.073709 -12.153898 -10.846695 -10.010849 -9.8092575 -9.6700954 -11.931854 -13.25001 -11.448128 -9.8085308 -13.26842 -16.120922 -15.50898 -14.909296][-9.7289772 -12.915523 -15.166576 -13.440816 -10.064823 -4.8097758 -2.3809354 -6.7427168 -11.052098 -11.513455 -11.246461 -12.104387 -13.39979 -15.666529 -16.957075][-13.611484 -15.034689 -14.949654 -12.39882 -8.5984955 -2.627203 2.5233884 1.7742338 -0.99574232 -6.8200817 -11.8925 -12.261634 -13.194538 -14.354292 -15.371582][-16.241375 -16.396259 -14.595589 -9.24591 -2.8348968 2.2071619 6.3886914 6.2476134 4.7118011 -1.6269708 -8.2273626 -11.657986 -14.320972 -14.221054 -14.765642][-15.146107 -14.667236 -13.761881 -8.8180628 -1.7852321 5.8115611 11.585001 8.4708481 4.8619075 -0.049859047 -5.6645646 -9.9397278 -13.206054 -14.925108 -16.740685][-12.141948 -11.506103 -12.059057 -8.6736774 -3.650841 2.681664 8.79867 9.30481 6.1049571 -1.5805345 -7.5665789 -11.449641 -15.201313 -15.960249 -16.62639][-9.9594679 -10.12937 -10.168961 -7.0835171 -4.2529669 -1.7441733 2.471622 3.9587359 2.5603995 -2.2067254 -7.7171845 -12.742916 -16.276272 -17.264317 -18.433987][-11.38829 -12.877565 -14.236786 -12.274832 -9.8176689 -7.1142197 -3.7735424 -4.1086884 -4.9308786 -6.161325 -9.5430155 -15.045746 -17.633318 -17.476965 -17.355682][-16.62826 -16.259304 -16.724272 -16.230253 -15.070055 -12.260754 -10.994576 -11.784231 -11.596766 -12.152672 -13.737852 -15.705217 -16.595623 -16.797031 -16.710594][-17.120253 -15.213478 -15.083773 -17.045185 -17.448994 -15.385529 -13.709755 -13.604799 -14.376585 -14.138672 -13.903938 -15.288813 -15.398672 -14.229023 -14.212145][-14.177626 -13.550692 -13.153399 -11.757053 -11.843533 -13.422074 -13.851587 -12.995279 -12.351385 -12.520267 -13.089695 -13.380501 -13.300673 -12.29243 -11.96142][-10.011652 -9.078434 -8.34877 -7.3142352 -7.3678093 -6.9203024 -7.5099621 -9.50598 -10.309278 -9.7355251 -9.8154087 -11.913319 -12.554873 -12.953476 -13.876698]]...]
INFO - root - 2017-12-15 15:29:44.363509: step 28210, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.668 sec/batch; 56h:29m:21s remains)
INFO - root - 2017-12-15 15:29:50.937244: step 28220, loss = 0.19, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 53h:30m:44s remains)
INFO - root - 2017-12-15 15:29:57.536013: step 28230, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 57h:01m:58s remains)
INFO - root - 2017-12-15 15:30:04.063176: step 28240, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 54h:24m:59s remains)
INFO - root - 2017-12-15 15:30:10.612006: step 28250, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 55h:12m:06s remains)
INFO - root - 2017-12-15 15:30:17.174988: step 28260, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 54h:31m:17s remains)
INFO - root - 2017-12-15 15:30:23.709368: step 28270, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 55h:22m:00s remains)
INFO - root - 2017-12-15 15:30:30.339189: step 28280, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 56h:00m:12s remains)
INFO - root - 2017-12-15 15:30:36.945988: step 28290, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 54h:36m:47s remains)
INFO - root - 2017-12-15 15:30:43.541688: step 28300, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 53h:52m:20s remains)
2017-12-15 15:30:44.135096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9352994 -9.4372492 -10.439743 -10.434361 -10.448654 -9.1803417 -8.1375036 -8.17311 -8.2625685 -9.2400036 -10.592269 -12.965359 -16.812471 -17.319008 -14.543823][-6.3445477 -7.2983265 -8.23431 -9.5620537 -10.319187 -10.207436 -9.3027334 -8.1416731 -7.7858305 -8.1306372 -8.01883 -9.7373638 -13.877409 -16.860281 -15.493647][-4.7129927 -6.5195651 -8.68915 -9.8416681 -10.821085 -11.104826 -10.531637 -9.3870335 -8.4729919 -8.458477 -8.5033474 -9.4406242 -12.388193 -13.319283 -12.074083][-3.5787361 -6.0388179 -8.13652 -8.71501 -9.4516764 -8.768446 -7.4173722 -7.5828857 -8.1141806 -7.1714735 -7.2887564 -8.6371031 -11.970139 -13.292055 -11.574086][-4.7214575 -6.8351421 -8.7230644 -8.025815 -6.8068171 -4.0589085 -2.4188309 -3.1534336 -4.4455724 -5.289968 -5.361485 -6.6837053 -10.724745 -14.4198 -14.410568][-6.9560647 -7.1098185 -7.7068624 -6.1671305 -3.0890915 0.60956478 3.9708982 4.5522027 2.0580454 -1.3181791 -3.1609063 -4.5203552 -7.8441091 -11.818134 -14.095266][-7.8103147 -7.0674987 -6.8326225 -4.7677846 -0.78325176 3.3741326 7.6092048 9.368576 7.5766244 2.7465892 -2.4805086 -3.7184463 -5.5900788 -8.589221 -10.047617][-7.642477 -7.4715376 -6.9628716 -4.0897989 -0.63260317 4.8725371 9.4901314 9.9603672 8.4747181 5.3429742 1.2554049 -1.8501048 -5.9011827 -7.0012608 -7.3907695][-6.7926364 -7.053185 -6.4843764 -4.6302347 -2.2940369 2.5544839 7.1235957 9.0168056 7.7880483 2.9768395 -0.93991852 -4.0783787 -9.0095406 -10.979633 -11.120062][-5.3846993 -6.1220384 -6.5579896 -5.03088 -3.5073237 -1.1675262 2.3732982 5.192039 3.9829841 0.63728142 -3.630995 -9.1348276 -13.144512 -15.683008 -16.353167][-9.5800667 -9.2216654 -9.687809 -8.4043474 -7.2034483 -6.3220444 -4.7099648 -2.8134625 -3.6851163 -5.9870853 -10.057908 -14.384779 -19.340775 -21.512909 -19.89472][-11.952895 -11.583731 -10.669176 -8.8809042 -7.4673777 -6.8921442 -6.5881753 -7.607986 -9.543294 -11.266815 -13.924761 -15.548616 -18.384338 -21.308649 -19.359295][-14.689718 -13.217891 -10.929646 -8.8576336 -9.2147551 -8.6391392 -7.7985516 -7.8992071 -9.149807 -12.195408 -14.22896 -14.787542 -16.634655 -17.751177 -15.172066][-12.633564 -11.913994 -9.4154625 -7.7811737 -7.5201368 -7.5694575 -7.88885 -7.2447042 -7.1999712 -9.0407009 -10.660322 -11.10985 -10.151464 -10.880381 -10.186161][-8.5098858 -8.3182554 -6.6611862 -5.5471873 -4.283823 -5.3208489 -7.3096995 -8.3911982 -9.4806767 -9.1617546 -8.9679222 -9.0009327 -9.0065842 -7.887701 -8.24695]]...]
INFO - root - 2017-12-15 15:30:50.784113: step 28310, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 56h:05m:18s remains)
INFO - root - 2017-12-15 15:30:57.468430: step 28320, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 54h:00m:05s remains)
INFO - root - 2017-12-15 15:31:04.172185: step 28330, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 57h:20m:32s remains)
INFO - root - 2017-12-15 15:31:10.783309: step 28340, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 56h:13m:23s remains)
INFO - root - 2017-12-15 15:31:17.279778: step 28350, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 53h:50m:03s remains)
INFO - root - 2017-12-15 15:31:23.892783: step 28360, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 55h:36m:13s remains)
INFO - root - 2017-12-15 15:31:30.488413: step 28370, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 54h:53m:15s remains)
INFO - root - 2017-12-15 15:31:37.080727: step 28380, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 54h:50m:14s remains)
INFO - root - 2017-12-15 15:31:43.692195: step 28390, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 55h:29m:27s remains)
INFO - root - 2017-12-15 15:31:50.255539: step 28400, loss = 0.14, batch loss = 0.09 (11.3 examples/sec; 0.708 sec/batch; 59h:47m:31s remains)
2017-12-15 15:31:50.727273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9897404 -7.4829073 -8.0072193 -8.5638962 -9.2073584 -9.5026207 -9.5547781 -8.589921 -7.1550245 -5.7491693 -3.9231403 -4.9722557 -8.5224686 -10.88828 -12.716156][-7.24393 -7.3868151 -6.6364441 -6.9671273 -7.9156871 -8.7187176 -9.4343033 -9.0223522 -8.3594723 -6.6560078 -4.0584254 -5.5533557 -8.6615734 -10.867708 -14.169239][-6.0244966 -6.4625111 -6.7048512 -6.2722421 -7.0749788 -8.0207577 -8.4351482 -7.8590746 -7.781354 -6.8155327 -4.4937 -5.5918703 -8.9836674 -11.365828 -14.05653][-6.0165277 -6.1055889 -5.93915 -6.1675858 -6.5451479 -5.6615062 -5.4012618 -6.6397424 -7.6751323 -6.0851011 -3.4055476 -4.8203249 -7.7980938 -11.243322 -14.751379][-7.2851496 -8.7359762 -9.4092569 -8.0018883 -7.1449752 -4.1197672 -0.89048004 -3.1066797 -6.5162139 -5.9425263 -3.601727 -3.6251783 -5.5958304 -8.2603741 -11.827894][-7.6008821 -8.0788813 -8.4642553 -7.8186474 -4.9057932 -0.07118988 4.229012 2.722218 0.17394209 -1.6477656 -3.1530097 -3.1476631 -4.6842222 -6.9601803 -9.89373][-9.6316843 -8.8083153 -8.1176243 -4.9896789 -1.31179 3.664392 8.0844574 8.395792 6.0412841 0.21786642 -3.6867218 -3.7449346 -5.6539607 -7.585413 -10.605702][-10.804518 -8.6299419 -6.4943876 -3.5436189 -0.87192154 4.5928025 10.198496 10.689335 7.7368531 2.404326 -2.0125434 -5.4266663 -9.3229752 -10.467591 -12.627296][-8.3942356 -7.4351149 -5.5018969 -3.286612 -1.4998641 1.3738918 5.52564 7.393805 6.0674548 2.246552 -1.6041923 -7.3197541 -12.483579 -13.971235 -15.317257][-7.1152306 -5.6944084 -4.4881616 -2.4604478 -1.2822065 -1.5958161 0.68667507 2.4271264 1.6963773 0.92930937 -1.7235606 -7.8189354 -12.951389 -16.26185 -18.715277][-9.4214134 -9.1843739 -8.2585583 -6.1357017 -5.1262035 -5.1171255 -3.4728422 -3.7323813 -4.1674314 -4.4726171 -6.9643779 -10.735697 -13.831173 -16.561012 -18.584894][-13.920931 -13.434509 -12.513461 -11.103215 -10.495407 -9.7605352 -9.0914078 -10.556416 -9.9717464 -9.1971045 -10.06449 -11.959438 -13.815697 -15.209263 -16.2374][-16.208853 -15.744623 -13.883986 -12.48722 -12.074018 -12.402719 -12.86082 -13.172393 -13.173876 -11.561611 -10.236511 -10.83983 -11.61397 -11.652653 -11.534703][-15.05545 -14.138086 -12.430764 -9.79118 -9.1573515 -10.057329 -11.327742 -10.506086 -9.9105377 -10.000755 -10.139444 -9.1156855 -8.449667 -8.4284573 -8.3658819][-10.896935 -10.755764 -9.0432911 -6.8772297 -5.6752906 -5.34521 -5.5372105 -6.4589276 -7.6226721 -7.5514402 -7.3945565 -8.2923021 -8.8866463 -7.9717484 -7.7084112]]...]
INFO - root - 2017-12-15 15:31:57.280674: step 28410, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 54h:33m:53s remains)
INFO - root - 2017-12-15 15:32:03.849521: step 28420, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 54h:16m:52s remains)
INFO - root - 2017-12-15 15:32:10.479901: step 28430, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 57h:48m:20s remains)
INFO - root - 2017-12-15 15:32:17.006034: step 28440, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.643 sec/batch; 54h:16m:14s remains)
INFO - root - 2017-12-15 15:32:23.649839: step 28450, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 56h:11m:25s remains)
INFO - root - 2017-12-15 15:32:30.171426: step 28460, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 56h:42m:15s remains)
INFO - root - 2017-12-15 15:32:36.718387: step 28470, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 55h:57m:16s remains)
INFO - root - 2017-12-15 15:32:43.405748: step 28480, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 55h:37m:01s remains)
INFO - root - 2017-12-15 15:32:50.020204: step 28490, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 56h:29m:45s remains)
INFO - root - 2017-12-15 15:32:56.550818: step 28500, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 54h:47m:16s remains)
2017-12-15 15:32:57.050662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4591179 -7.1250539 -7.0178075 -5.7111821 -5.9099655 -6.0323987 -6.6063385 -7.0207276 -6.7172322 -6.5320673 -6.6323204 -7.3386269 -6.6284318 -4.734293 -3.2404716][-7.207489 -6.332993 -5.5576096 -4.7707996 -4.7648726 -4.5337992 -5.0677233 -5.5437732 -5.3336782 -5.0687919 -4.4685659 -5.140151 -4.9198 -3.5065281 -1.8641982][-4.8852429 -5.5146308 -5.3251023 -4.2850552 -5.1102481 -4.6634908 -4.2683434 -3.7567143 -3.4479373 -3.8830123 -3.9968452 -4.7901659 -4.7247882 -4.0666366 -2.6376209][-5.0999975 -5.6105037 -6.1461053 -5.7996821 -6.4683952 -4.9734392 -4.3586779 -4.6425982 -3.6242225 -3.3046491 -3.4697452 -4.6537342 -5.1451511 -4.8857784 -3.0177085][-8.4231806 -9.2825241 -9.13334 -7.5775728 -7.6691561 -4.8193674 -2.6974089 -3.0987248 -3.1385055 -3.3763173 -4.0885668 -5.1675839 -5.8288279 -6.3856888 -5.023757][-8.7542906 -8.5217514 -7.7063584 -5.6955934 -4.3276119 -0.56564474 2.9614282 2.9749093 2.1796441 -0.66766119 -3.296927 -4.0849409 -5.6848416 -6.8123608 -6.0136118][-10.321592 -9.289464 -8.5293865 -5.0599041 -3.3701639 0.24924564 5.0912805 4.962316 3.5843511 -0.16069889 -3.6538148 -5.394393 -7.3079782 -7.3560953 -6.417201][-11.42775 -10.720169 -10.281651 -5.4745536 -1.7850764 2.1678805 6.1167531 6.3451829 6.1290812 1.7463632 -2.5651712 -5.1386542 -8.1609478 -8.56187 -7.0482154][-9.2032652 -7.8324862 -7.0213385 -3.1409814 -1.3400378 2.2382135 5.3067203 4.9662766 4.5744643 0.61303377 -3.0044782 -6.3189697 -8.7455473 -8.2041264 -7.3113623][-8.3782129 -7.6182508 -6.7357521 -3.6847768 -2.7997983 -0.22176027 1.4327555 0.79348469 0.57237482 -1.8258379 -3.6172574 -6.4004726 -8.2370834 -8.7716331 -7.7049584][-8.1197929 -7.7243156 -6.9100595 -5.7005262 -5.9113078 -4.3654509 -3.9735351 -4.5036736 -4.3212276 -4.7911406 -5.2965441 -8.33824 -10.011644 -10.405997 -9.5896854][-14.098963 -13.749943 -12.670686 -11.057952 -11.029858 -10.876303 -11.017586 -10.976604 -11.269016 -11.784528 -11.43473 -11.757638 -11.200932 -10.731089 -9.9243221][-13.671469 -13.045475 -12.124862 -11.828146 -11.74243 -11.366359 -11.733648 -12.030684 -11.918484 -11.032883 -9.83791 -10.681036 -10.291683 -9.1535625 -7.6837807][-9.7957067 -8.6223612 -8.6862488 -8.4156742 -8.3273325 -9.1705837 -9.9273872 -9.6973457 -9.4438324 -8.9702 -8.6041946 -7.79988 -6.6506925 -5.9577837 -5.5674763][-10.708076 -9.433918 -7.8026686 -7.5187378 -7.1611891 -6.3608294 -6.4272428 -7.2832718 -8.0804787 -7.2549362 -6.2703071 -5.9938407 -6.3951254 -7.1727242 -6.7878418]]...]
INFO - root - 2017-12-15 15:33:03.568802: step 28510, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 55h:10m:21s remains)
INFO - root - 2017-12-15 15:33:10.226417: step 28520, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 55h:05m:38s remains)
INFO - root - 2017-12-15 15:33:16.804783: step 28530, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 54h:00m:29s remains)
INFO - root - 2017-12-15 15:33:23.431835: step 28540, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 56h:41m:08s remains)
INFO - root - 2017-12-15 15:33:30.030980: step 28550, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 55h:41m:09s remains)
INFO - root - 2017-12-15 15:33:36.602073: step 28560, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.636 sec/batch; 53h:40m:39s remains)
INFO - root - 2017-12-15 15:33:43.097452: step 28570, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 54h:08m:54s remains)
INFO - root - 2017-12-15 15:33:49.607673: step 28580, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 56h:18m:11s remains)
INFO - root - 2017-12-15 15:33:56.282441: step 28590, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 56h:17m:01s remains)
INFO - root - 2017-12-15 15:34:02.975484: step 28600, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 54h:45m:56s remains)
2017-12-15 15:34:03.497382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7921507 -2.5769658 -2.5738168 -1.9133396 -3.1548741 -2.971827 -2.73674 -2.7344913 -2.7586114 -2.0006466 -1.5332799 -5.349268 -6.6803985 -8.5470009 -8.0018568][-2.6205642 -3.2026038 -4.0657859 -4.0477 -4.2830591 -4.7952266 -5.4437952 -5.2035561 -4.9758439 -4.0297604 -3.2857015 -6.61416 -7.410718 -9.1385117 -8.03602][-1.7143431 -3.0850043 -3.9865568 -3.7576187 -4.3185334 -5.50399 -5.8383417 -5.8174644 -5.9520569 -5.7085752 -4.6269946 -7.4233556 -8.05047 -8.7410355 -7.1933975][-2.9813292 -3.0379267 -3.3351779 -3.3145142 -3.4677713 -3.442488 -3.623559 -5.087141 -5.6405263 -5.3067107 -5.2192607 -7.8589163 -8.1199141 -9.2391949 -7.9486046][-3.8829842 -4.0917435 -4.948956 -3.25097 -1.9062541 -1.6688166 -1.8643067 -3.3137591 -4.1472421 -4.5076079 -4.556582 -7.1342831 -7.9398751 -9.5035686 -8.43301][-6.831212 -6.6819015 -6.180944 -3.1194108 -0.96038437 1.2427187 2.1419525 0.055881977 -1.3932986 -2.2511885 -2.1477451 -5.2994676 -6.3370037 -7.37095 -7.0162849][-9.3527336 -8.8751011 -7.8017092 -4.2617874 -0.68539715 3.6307807 5.8147178 4.3004527 2.7339482 0.64088678 -0.046608448 -3.1336634 -4.0279245 -5.8867774 -6.38782][-9.3975649 -9.4502468 -8.5818129 -5.0348892 -1.3488312 3.0583625 5.0571885 4.3936238 3.7787528 2.1960211 1.9682674 -1.4278865 -2.8407652 -5.0936646 -5.4127321][-8.39309 -9.0283747 -8.799078 -5.3931761 -1.9031622 2.1520038 3.589191 3.4777064 3.1541657 2.6694131 3.416471 -0.61660528 -2.6047311 -5.901607 -7.55582][-8.3386469 -9.07049 -8.7326126 -6.3901381 -3.8741035 -0.0070819855 1.9307904 2.5025306 2.2390127 2.2928367 3.5000052 0.12684441 -1.9422474 -6.7956657 -9.8814335][-12.073611 -12.392656 -12.190086 -10.137642 -7.7722263 -5.0194221 -3.4697065 -3.3398602 -3.1087527 -2.0638714 -0.68280363 -3.0260656 -4.325706 -7.9695034 -10.445175][-15.832085 -16.181005 -15.129129 -12.90709 -10.736938 -8.4731331 -7.4802709 -7.09194 -6.8524394 -6.22631 -4.9368763 -6.2531857 -6.7763386 -9.6056709 -11.328816][-15.433197 -15.820068 -14.944944 -14.606449 -13.597345 -12.017555 -11.576948 -10.694624 -10.468738 -9.564003 -7.881959 -8.3303537 -8.3503771 -10.22226 -10.554014][-12.951565 -13.102246 -12.30991 -12.068701 -11.69582 -11.866026 -11.949606 -11.74437 -11.388432 -10.350463 -9.5055676 -9.9356174 -9.6767282 -10.15521 -10.400114][-8.9179144 -9.6100769 -8.3570251 -7.2122879 -7.0365019 -7.4164047 -7.5973606 -8.6364279 -9.2583027 -8.7679768 -8.5626526 -9.4250546 -9.9353962 -10.826123 -11.17807]]...]
INFO - root - 2017-12-15 15:34:10.114605: step 28610, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 54h:26m:22s remains)
INFO - root - 2017-12-15 15:34:16.607668: step 28620, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 55h:37m:40s remains)
INFO - root - 2017-12-15 15:34:23.152581: step 28630, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 54h:16m:10s remains)
INFO - root - 2017-12-15 15:34:29.794628: step 28640, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 55h:28m:10s remains)
INFO - root - 2017-12-15 15:34:36.408818: step 28650, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 55h:49m:02s remains)
INFO - root - 2017-12-15 15:34:43.050433: step 28660, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.694 sec/batch; 58h:32m:19s remains)
INFO - root - 2017-12-15 15:34:49.677004: step 28670, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 56h:40m:02s remains)
INFO - root - 2017-12-15 15:34:56.313026: step 28680, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 57h:08m:48s remains)
INFO - root - 2017-12-15 15:35:02.884848: step 28690, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.636 sec/batch; 53h:40m:10s remains)
INFO - root - 2017-12-15 15:35:09.555943: step 28700, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 56h:04m:14s remains)
2017-12-15 15:35:10.137871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6693692 -7.2134666 -7.3335128 -7.0007324 -7.1256361 -7.2290912 -7.3965197 -6.5729742 -6.1696582 -5.6742964 -4.6017389 -3.8772056 -3.750325 -3.4986103 -2.8574159][-5.7393146 -5.0992341 -4.3321667 -4.2943387 -4.6565876 -4.7734551 -5.0952873 -5.0275245 -4.6874075 -3.8724477 -3.2492266 -3.9208369 -4.367825 -4.3002052 -3.149678][-2.4959016 -3.330219 -3.4491575 -2.4074445 -2.4686117 -2.7402864 -3.9770038 -4.0984888 -3.7479887 -4.1290913 -4.2178583 -4.4126768 -5.9536862 -7.0350366 -6.4225612][-2.4032397 -3.7340662 -4.645483 -3.0424387 -2.5102835 -1.9764309 -2.3764958 -2.8710086 -3.2484818 -3.0206418 -2.4867673 -3.8864713 -7.0141292 -8.5987415 -8.9069481][-4.6853509 -5.9265141 -5.7735648 -4.22667 -3.121347 -0.50579262 0.39897871 -1.4246454 -2.5926166 -2.3758337 -2.684236 -4.878849 -7.602674 -9.741086 -10.712814][-7.1752963 -6.9066162 -5.393281 -2.7997453 -0.7352066 2.2966733 3.893044 2.7079291 1.5423522 -0.56506634 -2.1719296 -3.9005911 -7.1959972 -9.9020042 -10.276764][-10.137245 -8.1717138 -5.7718959 -2.481904 0.035686016 2.845367 5.3936543 5.5019078 4.8512692 1.2438412 -2.4983704 -5.0600195 -8.7291946 -10.583057 -9.73754][-9.0486078 -7.6058364 -5.9882536 -3.4374692 -1.3442893 1.8518534 4.3067555 4.8490176 5.4483943 2.73202 -0.41511059 -4.0702381 -8.2084246 -10.09439 -9.3203354][-7.7918749 -7.0445256 -5.94979 -3.7338076 -2.6879616 -0.95518541 0.7861371 1.678833 2.2367229 0.65425348 -1.183866 -4.7671275 -8.9358444 -10.981735 -10.650622][-6.5050721 -6.4353046 -6.5728359 -4.5394659 -3.9124184 -3.6753535 -2.085284 -0.77920246 -1.142158 -2.2327986 -3.4229271 -6.3379431 -9.8966141 -11.992785 -11.640213][-10.968037 -10.74839 -10.614723 -9.0835533 -9.116045 -8.7337055 -7.2595873 -6.8971362 -6.6688395 -6.9916263 -7.8953595 -10.485379 -12.292852 -13.019455 -11.70356][-15.44138 -14.814888 -13.960989 -13.2624 -13.547644 -13.213263 -13.261893 -13.13558 -11.803296 -11.102226 -11.448706 -12.684971 -13.099047 -11.512686 -9.2463388][-14.033421 -14.202475 -13.651335 -12.278127 -12.288136 -11.873394 -11.968771 -11.598299 -11.142289 -10.656557 -9.96851 -10.096241 -9.6554832 -8.1194954 -6.1445885][-10.972746 -10.878107 -9.6921587 -8.1627407 -7.5487175 -7.8090382 -8.6645451 -8.31443 -8.2731285 -8.0350084 -7.6212206 -7.1101832 -6.3005772 -4.7631578 -3.8228707][-9.7256832 -9.63347 -8.6792374 -7.2352538 -6.4571118 -5.3527284 -5.5081878 -5.4315267 -5.3716664 -4.5934935 -4.4870105 -5.2713113 -5.6189322 -5.5184574 -5.3408275]]...]
INFO - root - 2017-12-15 15:35:16.718902: step 28710, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.635 sec/batch; 53h:32m:44s remains)
INFO - root - 2017-12-15 15:35:23.334112: step 28720, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.653 sec/batch; 55h:08m:15s remains)
INFO - root - 2017-12-15 15:35:29.843475: step 28730, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 54h:05m:56s remains)
INFO - root - 2017-12-15 15:35:36.483844: step 28740, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 54h:19m:49s remains)
INFO - root - 2017-12-15 15:35:43.157502: step 28750, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 53h:52m:18s remains)
INFO - root - 2017-12-15 15:35:49.774903: step 28760, loss = 0.13, batch loss = 0.08 (12.8 examples/sec; 0.624 sec/batch; 52h:37m:08s remains)
INFO - root - 2017-12-15 15:35:56.448188: step 28770, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 55h:10m:01s remains)
INFO - root - 2017-12-15 15:36:03.058671: step 28780, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 55h:55m:59s remains)
INFO - root - 2017-12-15 15:36:09.663711: step 28790, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 57h:19m:20s remains)
INFO - root - 2017-12-15 15:36:16.280510: step 28800, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 55h:56m:51s remains)
2017-12-15 15:36:16.837942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5001984 -5.8946619 -5.9601412 -6.3405647 -7.5575113 -7.8750405 -7.6929116 -7.0698795 -6.4675651 -5.4415421 -5.0706744 -5.795218 -7.9842296 -9.3378706 -8.4118366][-6.6506305 -7.681035 -8.6148748 -8.2213936 -8.2966356 -8.9147358 -9.3201294 -9.2616329 -9.1171522 -8.2724419 -7.6597834 -8.1808987 -9.3265562 -8.7479515 -7.4839039][-6.9155622 -7.6862025 -8.47379 -8.1013117 -7.2424049 -7.4071617 -7.57258 -8.4887476 -9.2396383 -8.4052906 -7.7727032 -8.4056091 -9.5220356 -9.0110655 -7.5520697][-5.150569 -5.9458075 -6.2431197 -6.0081911 -6.1363158 -6.0700788 -5.6050458 -6.1316094 -6.70189 -6.7868342 -6.9167156 -8.1047163 -9.597887 -8.7837572 -7.4689655][-5.6723928 -5.5711007 -5.3865709 -4.013937 -2.9414647 -2.1791532 -2.0346916 -3.3994596 -4.0840163 -4.43474 -4.3481674 -6.4321094 -9.2228222 -8.8875122 -7.5824757][-7.9543505 -6.9812212 -5.0921292 -3.1073265 -0.92165804 1.2577014 2.2893205 1.0169487 -0.58762646 -2.2159069 -3.2047935 -5.36078 -7.41111 -7.678627 -6.9529386][-9.352479 -8.2731848 -5.8978481 -2.4053524 -0.55958176 2.2190642 4.6126933 4.4223943 3.3941627 0.29837418 -2.4071946 -4.8494782 -7.2878704 -7.2439971 -5.9141011][-8.8747311 -7.78261 -5.8171468 -2.1123359 1.0060549 3.9338126 5.3673825 5.7138219 5.7903161 2.2207737 -0.93815088 -4.35385 -8.1050253 -8.2278023 -6.3933682][-6.6413007 -6.2966089 -4.5775051 -1.117321 1.1765971 4.5887771 5.6276174 5.8775163 5.404058 2.7595153 0.46880293 -3.9559283 -8.5961132 -8.8601971 -7.8914723][-4.1197081 -4.3360596 -3.5060229 -0.40188265 1.3935003 3.2043433 3.8974633 4.5171285 3.8617272 2.2248502 0.48645544 -3.4659047 -7.936955 -9.3465309 -9.8118267][-6.3574429 -5.8411474 -4.0884056 -1.8915017 -1.1790261 -0.51474857 -0.023337364 0.022078514 -0.54050636 -1.9568918 -3.8409705 -7.2218919 -9.3641787 -10.256966 -9.4084864][-9.791728 -9.2489519 -6.6730103 -4.2114468 -3.3356459 -3.533582 -4.1649947 -4.3670025 -4.0117846 -5.2061157 -6.9010997 -10.288571 -11.696253 -12.406375 -10.617234][-10.450937 -10.015733 -8.4080772 -6.8534575 -5.5314479 -5.2525816 -6.0549183 -6.8412323 -7.7225561 -8.193325 -8.6981583 -10.215666 -10.782707 -10.823738 -9.6999378][-9.3455849 -8.4522753 -7.4537587 -6.8557906 -5.7872248 -6.3928318 -7.3899889 -7.5895839 -8.3355045 -8.9980536 -9.9606838 -10.349722 -9.7865429 -8.6732349 -7.5616055][-6.4957433 -6.0543427 -5.9509864 -5.4002228 -4.9190025 -5.8014379 -5.6062393 -5.81311 -6.4717073 -6.7643824 -7.8581676 -9.187479 -9.3022633 -7.9187684 -6.7070422]]...]
INFO - root - 2017-12-15 15:36:23.426582: step 28810, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 55h:47m:33s remains)
INFO - root - 2017-12-15 15:36:29.967560: step 28820, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 54h:10m:55s remains)
INFO - root - 2017-12-15 15:36:36.611798: step 28830, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 54h:03m:29s remains)
INFO - root - 2017-12-15 15:36:43.179628: step 28840, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 56h:52m:57s remains)
INFO - root - 2017-12-15 15:36:49.703568: step 28850, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 55h:25m:55s remains)
INFO - root - 2017-12-15 15:36:56.347012: step 28860, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 56h:19m:09s remains)
INFO - root - 2017-12-15 15:37:02.950743: step 28870, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 56h:15m:54s remains)
INFO - root - 2017-12-15 15:37:09.521919: step 28880, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.650 sec/batch; 54h:49m:18s remains)
INFO - root - 2017-12-15 15:37:16.115882: step 28890, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 55h:17m:43s remains)
INFO - root - 2017-12-15 15:37:22.759430: step 28900, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 56h:09m:27s remains)
2017-12-15 15:37:23.262103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6228943 -2.7807283 -1.0354919 -0.65109539 -1.0843296 -1.5378056 -2.1638877 -2.2397757 -2.1069758 -2.4817824 -3.4497476 -7.8635321 -10.519164 -10.084913 -11.101398][-3.3647792 -4.7313585 -3.9589248 -3.2903807 -3.3016598 -3.740422 -4.0945091 -3.9270153 -3.9688945 -4.0415654 -4.8991771 -9.3121223 -11.973917 -11.68622 -11.573931][-4.0576768 -4.5871291 -4.3256369 -4.1455851 -4.5728197 -5.1874065 -5.4859567 -5.2951488 -4.4314461 -4.7615271 -5.8556952 -9.7659473 -11.88916 -11.185551 -11.198467][-6.269834 -6.0913749 -5.7780333 -4.9778414 -5.1219316 -4.5879045 -4.1810617 -4.5500116 -5.0555315 -4.7892523 -5.0791526 -9.2676563 -11.911802 -11.129452 -11.094727][-7.4942236 -8.651371 -8.36152 -6.4828591 -4.7147837 -2.7799075 -2.0297716 -3.0784261 -4.8003507 -4.8404007 -5.0544395 -8.7096758 -10.767749 -11.150204 -11.781242][-8.7936506 -9.239315 -7.8406696 -4.6013007 -1.9234989 0.54612494 2.7764468 2.2797256 -0.27235889 -2.8128519 -5.7978749 -8.6590986 -10.137178 -10.257721 -11.378836][-10.973886 -10.47993 -8.48578 -4.4168935 0.0010666847 3.8952498 6.6949363 5.386826 3.5420384 -0.046981335 -4.6719456 -8.52917 -10.938506 -10.6166 -12.060946][-11.061199 -9.3802261 -6.6879654 -2.6082458 1.0150504 4.8435349 9.1483593 7.6807933 5.8899817 1.8677425 -2.5770752 -7.6946845 -10.522453 -9.9802647 -11.328232][-9.3180094 -7.6458178 -6.3264546 -2.7141445 0.79635429 3.1846442 6.0001321 6.6039577 5.7677264 1.1835208 -2.6987009 -7.0733457 -10.338855 -10.194124 -10.819101][-8.161026 -6.868618 -4.9490204 -2.2526457 -0.69849205 0.19727421 1.9143658 2.5688996 2.630259 0.055127144 -2.9238191 -7.5488405 -10.236208 -9.8749542 -11.389305][-9.51699 -9.4128895 -7.3474154 -5.4056692 -3.8755817 -2.8332675 -1.9975333 -2.7274046 -3.2144217 -3.7416382 -5.2006068 -9.4507427 -10.832151 -10.147619 -10.070799][-13.019127 -12.333094 -9.384057 -7.7610483 -6.753511 -5.7657347 -6.7271528 -7.4730921 -7.7109194 -8.3443842 -8.7826395 -9.97436 -9.63031 -10.014933 -10.33662][-12.029193 -10.950003 -9.0734205 -7.9583859 -7.0941782 -6.8354592 -7.7323008 -8.090683 -9.3822174 -9.82159 -9.95278 -9.8370953 -9.7681541 -8.2452831 -7.4669337][-10.541025 -9.1933174 -8.1352987 -6.17745 -4.49303 -5.4624104 -7.0577478 -7.9692116 -8.2485027 -8.5305891 -9.1894035 -8.411109 -8.2995815 -6.5607119 -6.4918456][-6.73435 -5.9030113 -4.3216567 -3.225518 -3.2933717 -3.1394687 -4.090611 -6.0592523 -6.9060054 -7.0894575 -6.4778056 -7.2934074 -8.24873 -7.3763905 -7.7667694]]...]
INFO - root - 2017-12-15 15:37:29.883018: step 28910, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.633 sec/batch; 53h:24m:16s remains)
INFO - root - 2017-12-15 15:37:36.526825: step 28920, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.681 sec/batch; 57h:27m:58s remains)
INFO - root - 2017-12-15 15:37:43.076881: step 28930, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 54h:53m:02s remains)
INFO - root - 2017-12-15 15:37:49.721483: step 28940, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 55h:27m:28s remains)
INFO - root - 2017-12-15 15:37:56.271314: step 28950, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 54h:35m:46s remains)
INFO - root - 2017-12-15 15:38:02.821255: step 28960, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 54h:44m:23s remains)
INFO - root - 2017-12-15 15:38:09.407417: step 28970, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 54h:49m:33s remains)
INFO - root - 2017-12-15 15:38:16.075590: step 28980, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 56h:14m:26s remains)
INFO - root - 2017-12-15 15:38:22.717292: step 28990, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 56h:50m:03s remains)
INFO - root - 2017-12-15 15:38:29.382553: step 29000, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.690 sec/batch; 58h:09m:59s remains)
2017-12-15 15:38:29.875302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.577071 -5.4146981 -6.5294566 -7.1779089 -8.1532726 -8.6878185 -9.5244617 -10.022207 -10.533016 -10.780949 -9.6525221 -11.788085 -13.797527 -13.0795 -13.004504][-5.3762221 -7.656539 -8.2708778 -8.3757973 -8.4288216 -8.0059738 -8.6684923 -10.328016 -11.662567 -11.604479 -10.399895 -13.271297 -15.149446 -15.467354 -15.426987][-6.802218 -9.2099028 -10.270998 -9.8757019 -9.093195 -8.1969719 -8.3851328 -9.224822 -10.453539 -11.476271 -11.09096 -13.375045 -14.858006 -15.095757 -15.569916][-8.8278351 -10.649901 -11.312933 -9.9691486 -8.636116 -7.1935682 -6.1019154 -7.4659691 -9.4314575 -9.6541224 -8.6850958 -11.36815 -13.542616 -14.589863 -15.230976][-9.5605307 -12.374903 -13.257631 -10.053135 -6.5189013 -2.8892581 -1.1257048 -4.4399233 -7.8312788 -8.06321 -7.761447 -9.8996983 -10.982288 -12.298748 -13.476192][-9.7544632 -11.615616 -11.238901 -8.3951015 -4.0070362 1.8174438 5.1569743 2.7110248 -0.73280525 -4.1838961 -6.5405641 -7.8180456 -8.5498295 -9.5381565 -11.015715][-9.0619221 -10.795675 -9.3672352 -5.8340468 -1.2482224 4.4860721 8.2047672 7.2995248 5.273726 -0.4481864 -5.2863317 -8.0572224 -9.4379482 -9.6672964 -9.64727][-9.4029865 -9.5898314 -8.11345 -5.0320382 -0.48471212 5.1036849 8.6488419 7.6872106 6.3377595 2.5856309 -1.3515925 -7.3119292 -11.138369 -11.228399 -11.676888][-8.2053413 -8.9585428 -8.4762239 -6.74577 -2.6734843 1.8019371 4.2518706 5.1676068 4.7392783 1.3622766 -0.9200573 -6.77203 -12.241227 -13.973503 -14.943714][-5.3626094 -7.2024293 -8.0036631 -7.4160357 -4.9705677 -2.2772474 0.42131281 2.4795632 2.1994076 0.23373461 -1.413702 -7.6581826 -12.658152 -14.862093 -17.103691][-9.1929579 -10.336453 -11.556301 -10.830803 -9.3092518 -7.6387959 -5.2561307 -4.2454038 -4.3446131 -4.8039985 -6.6477947 -10.921409 -14.508553 -15.787033 -16.447927][-14.255556 -14.341972 -13.679419 -13.597717 -13.366655 -11.46216 -10.030154 -10.056684 -10.024181 -9.8670511 -10.657774 -12.284345 -13.988655 -15.19978 -15.762573][-14.965672 -14.438782 -13.783577 -14.17865 -14.523449 -13.644987 -13.22134 -12.467934 -12.122847 -11.746604 -11.789817 -12.231468 -12.58252 -11.581482 -11.098376][-12.794769 -11.988856 -11.128757 -9.7228394 -9.7268162 -10.285702 -11.088476 -11.02124 -10.843512 -11.097153 -11.479652 -10.77136 -10.418392 -10.219522 -10.051957][-9.6400442 -9.3271694 -7.6249208 -5.4881282 -4.9290485 -4.6109409 -5.1777372 -6.6274867 -7.7994785 -8.3251352 -8.8215809 -9.835825 -9.9086323 -8.5292606 -7.601141]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 15:38:36.407972: step 29010, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 54h:39m:28s remains)
INFO - root - 2017-12-15 15:38:43.058993: step 29020, loss = 0.14, batch loss = 0.09 (12.9 examples/sec; 0.621 sec/batch; 52h:22m:04s remains)
INFO - root - 2017-12-15 15:38:49.668970: step 29030, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 56h:20m:06s remains)
INFO - root - 2017-12-15 15:38:56.258662: step 29040, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 55h:48m:23s remains)
INFO - root - 2017-12-15 15:39:02.774106: step 29050, loss = 0.16, batch loss = 0.11 (12.7 examples/sec; 0.632 sec/batch; 53h:16m:10s remains)
INFO - root - 2017-12-15 15:39:09.424932: step 29060, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 55h:37m:18s remains)
INFO - root - 2017-12-15 15:39:16.008714: step 29070, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 55h:31m:10s remains)
INFO - root - 2017-12-15 15:39:22.638090: step 29080, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 57h:32m:59s remains)
INFO - root - 2017-12-15 15:39:29.161717: step 29090, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 54h:33m:37s remains)
INFO - root - 2017-12-15 15:39:35.760032: step 29100, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 55h:49m:07s remains)
2017-12-15 15:39:36.243167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6385489 -6.2214203 -7.4170971 -8.3345566 -9.0963182 -9.2857752 -9.38694 -9.7301712 -10.730196 -10.995179 -10.19239 -10.759577 -11.964334 -10.732718 -6.8733082][-7.7963276 -7.8235579 -8.1995926 -9.34489 -10.47007 -10.749465 -10.397865 -10.11523 -11.027335 -11.208492 -10.582188 -10.343813 -10.67296 -11.414585 -9.627841][-5.8293571 -7.7416978 -10.452258 -10.260271 -10.434658 -10.711701 -10.069824 -9.6659527 -10.328398 -10.340503 -10.084003 -9.6636009 -10.320871 -10.493154 -8.5129681][-7.9053373 -8.4803514 -9.8123112 -10.316328 -10.497968 -8.3424463 -6.5086074 -7.5657077 -9.3353519 -9.8089447 -9.3755159 -9.1382122 -10.934601 -11.336125 -9.5975361][-7.4559507 -9.8333616 -12.576721 -11.92079 -10.132109 -5.5429573 -1.5974383 -2.7312908 -5.9290814 -7.9490786 -9.9680805 -9.7923584 -10.476557 -10.632977 -9.09483][-9.2408733 -10.344698 -10.99196 -10.315553 -8.4318724 -1.8734026 4.1126065 4.2373452 1.7830853 -2.3576024 -6.8566971 -7.7433262 -8.3414726 -9.05642 -8.4501762][-8.4782333 -9.7334061 -8.8562183 -6.6213217 -4.6220336 0.99557495 7.6518626 9.56653 8.2201557 2.9011807 -3.0008516 -5.7707224 -8.044857 -8.4785662 -6.7538271][-8.3101349 -10.700416 -10.711596 -5.9852409 -1.2013426 2.7870154 6.5542321 8.5386505 8.5743942 3.6785836 -1.9586375 -4.4803457 -7.3408394 -8.9149723 -7.7747965][-7.2483826 -8.0598211 -9.005065 -6.6058569 -2.6599119 2.5201449 5.9855819 5.4030795 5.9812713 3.0479741 -1.0918455 -4.377923 -8.3745365 -10.572325 -9.9253807][-5.9953947 -6.3180571 -7.6755695 -6.3192177 -4.6232805 -0.85548353 3.5811362 3.0559297 2.7604861 0.3097949 -2.5297143 -4.5561047 -7.7758584 -10.502385 -11.679825][-9.81342 -10.017139 -11.154953 -9.7100191 -8.7403107 -6.1790233 -3.0074115 -3.6576009 -4.0400829 -5.661869 -6.95412 -8.3399162 -11.078264 -12.888709 -12.217461][-13.260155 -13.168284 -13.487827 -12.458279 -11.260288 -9.86642 -9.1935873 -8.9823627 -8.08654 -9.4867811 -11.623975 -13.389412 -13.89382 -13.997908 -13.483933][-12.2638 -12.707291 -13.09067 -12.259195 -12.47876 -11.167397 -10.202154 -10.99043 -11.876609 -11.156012 -10.392126 -12.080994 -13.26876 -13.514108 -12.078081][-10.545629 -10.2996 -10.567017 -10.085224 -8.6697731 -7.9784479 -8.3835325 -9.3929653 -10.252601 -10.085478 -9.620966 -10.102917 -10.192852 -9.9646559 -9.4419756][-7.0343904 -7.7705216 -7.0398722 -6.8671947 -6.0763726 -5.1691427 -4.2410684 -5.1877494 -6.9606586 -8.1189833 -8.467185 -8.666935 -8.5476112 -9.1692982 -9.1905365]]...]
INFO - root - 2017-12-15 15:39:42.789318: step 29110, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 55h:38m:37s remains)
INFO - root - 2017-12-15 15:39:49.379417: step 29120, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.681 sec/batch; 57h:24m:43s remains)
INFO - root - 2017-12-15 15:39:55.984590: step 29130, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 55h:44m:41s remains)
INFO - root - 2017-12-15 15:40:02.532539: step 29140, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 55h:04m:48s remains)
INFO - root - 2017-12-15 15:40:09.143839: step 29150, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 55h:46m:53s remains)
INFO - root - 2017-12-15 15:40:15.756782: step 29160, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 57h:19m:26s remains)
INFO - root - 2017-12-15 15:40:22.384148: step 29170, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 55h:53m:52s remains)
INFO - root - 2017-12-15 15:40:29.018362: step 29180, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 55h:33m:08s remains)
INFO - root - 2017-12-15 15:40:35.614583: step 29190, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.636 sec/batch; 53h:33m:08s remains)
INFO - root - 2017-12-15 15:40:42.195898: step 29200, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.631 sec/batch; 53h:09m:41s remains)
2017-12-15 15:40:42.737334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3140693 -5.1195216 -5.1190825 -5.3583016 -5.4929986 -4.0344453 -3.5940638 -4.228508 -5.1498909 -5.5866594 -5.7371068 -6.5733218 -8.8728056 -9.0728931 -9.6454792][-4.6331978 -5.2611103 -5.6819205 -4.9288507 -4.5925317 -4.0039172 -3.9840357 -3.9418836 -5.0005884 -6.6956768 -6.7535567 -8.1231489 -10.885386 -11.866426 -11.490143][-4.4925795 -5.0773625 -5.1350303 -5.479116 -5.956336 -5.55435 -5.026238 -4.4755912 -4.949821 -6.1404638 -7.9358273 -9.4305811 -11.583302 -12.805748 -12.478893][-7.5413818 -7.1576443 -6.5887103 -6.5445518 -7.0004153 -7.0986691 -6.0732031 -5.12702 -5.3954515 -6.0837727 -6.6072154 -9.7518463 -13.110609 -13.000793 -12.116656][-8.7586746 -9.1809664 -9.4394054 -7.1241159 -5.6054645 -5.3696437 -4.4503155 -3.8088028 -4.4248285 -4.7396836 -5.2755866 -8.2405949 -11.072271 -12.833772 -13.532072][-6.7312446 -7.1827831 -7.2882371 -5.6632452 -3.8338127 -0.331738 1.966043 0.22322416 -1.1317019 -2.0444028 -4.0036039 -6.0592341 -8.8300772 -10.830967 -11.699789][-7.7869854 -7.341475 -6.635066 -5.1626439 -2.4191833 1.988647 5.3153768 5.7510142 5.0552869 1.3449435 -2.5898337 -3.9019303 -6.7310457 -9.1576929 -10.555771][-6.5156689 -6.9830637 -5.6030211 -4.4739022 -2.4455647 1.7017899 5.6367307 7.3356519 7.0929761 3.6902919 -0.023169518 -3.7726769 -7.5944443 -8.5889587 -9.2175236][-4.85087 -4.9548492 -4.5879602 -3.2332928 -2.3575928 0.32462454 3.9975705 5.6818318 5.4871716 2.5245547 -1.0443959 -5.192667 -9.8748989 -11.286869 -11.191768][-5.2282724 -4.7257776 -4.6740355 -3.196161 -1.933265 -1.0114841 0.41372585 1.5684934 1.8318796 -0.49071741 -3.4319162 -7.5825386 -12.403271 -14.025658 -14.846779][-8.3890743 -8.07842 -8.2502928 -6.1321168 -5.4559164 -5.0846033 -4.2126217 -4.1057844 -4.9389529 -6.036787 -8.05333 -11.535 -14.585033 -15.839672 -15.25045][-14.726431 -13.975651 -12.63987 -10.336311 -9.9302664 -9.4981689 -9.5769806 -10.422156 -10.449772 -11.138847 -12.414719 -13.773674 -15.507324 -15.768991 -15.221756][-15.379007 -14.078371 -11.966532 -10.765391 -11.153358 -10.165948 -10.147396 -10.502221 -10.818699 -11.166378 -11.76942 -13.700134 -13.77824 -12.465224 -10.556635][-14.360928 -13.732618 -12.217808 -10.106902 -9.8192463 -9.6267605 -10.600206 -11.123783 -10.833094 -10.8901 -10.509922 -10.387815 -9.9999094 -9.87627 -8.6406479][-10.065781 -8.8436413 -7.7014575 -7.3809185 -7.0087233 -6.9199953 -7.6989822 -8.130785 -8.9386473 -8.9604874 -8.8500576 -9.9312019 -9.4566984 -8.4545879 -8.2860317]]...]
INFO - root - 2017-12-15 15:40:49.309550: step 29210, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 56h:40m:45s remains)
INFO - root - 2017-12-15 15:40:55.859629: step 29220, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 55h:15m:41s remains)
INFO - root - 2017-12-15 15:41:02.414755: step 29230, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 53h:52m:11s remains)
INFO - root - 2017-12-15 15:41:09.067900: step 29240, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 56h:09m:32s remains)
INFO - root - 2017-12-15 15:41:15.647368: step 29250, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 54h:30m:58s remains)
INFO - root - 2017-12-15 15:41:22.202624: step 29260, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 54h:36m:44s remains)
INFO - root - 2017-12-15 15:41:28.839154: step 29270, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 55h:20m:15s remains)
INFO - root - 2017-12-15 15:41:35.421345: step 29280, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 57h:06m:15s remains)
INFO - root - 2017-12-15 15:41:41.985732: step 29290, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 53h:52m:18s remains)
INFO - root - 2017-12-15 15:41:48.578837: step 29300, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 55h:11m:23s remains)
2017-12-15 15:41:49.122683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.323143 -7.2377453 -8.7439842 -9.578433 -11.029051 -12.611745 -13.217596 -12.199142 -11.51532 -11.523143 -10.689915 -11.985831 -13.37994 -13.285097 -13.253435][-7.7871733 -8.4006453 -9.144413 -10.392621 -11.607387 -12.40396 -12.748131 -13.964527 -14.782423 -13.736568 -12.463766 -13.771833 -14.80022 -14.907331 -14.638609][-5.7819648 -8.1838036 -10.417226 -10.471008 -11.028696 -12.456968 -13.746365 -13.683868 -13.30653 -14.121454 -14.061081 -14.233997 -15.145517 -15.73716 -14.805826][-8.6251354 -10.730828 -11.297904 -10.06959 -10.14188 -10.096027 -9.83752 -11.882362 -13.623435 -12.764771 -11.484245 -14.014156 -16.087198 -15.563816 -14.314487][-10.822485 -13.254812 -14.614368 -12.867155 -9.4155865 -4.7147603 -2.8856585 -7.1371841 -11.768918 -12.896654 -12.722189 -13.060213 -13.61651 -15.270496 -15.694141][-14.48687 -15.478033 -15.402498 -12.61835 -8.4360561 -1.9315383 3.2538514 2.8507009 -0.80652809 -7.6669774 -12.636508 -12.643793 -13.000685 -13.703789 -14.220562][-16.63839 -16.801987 -14.629772 -8.562273 -2.7838855 2.0562325 6.65274 8.4649105 6.4727473 -1.3679676 -8.1588078 -12.310297 -15.05121 -13.974373 -13.400702][-15.199863 -15.278879 -14.295959 -8.2583189 -1.5438995 6.1153064 11.505335 9.389286 6.0384917 1.3773623 -4.5401483 -9.8616476 -12.801034 -14.472971 -15.391621][-11.857583 -11.82513 -12.110018 -8.7322206 -3.6262321 3.5267539 9.2327919 9.7041931 6.8010383 -0.75300074 -7.1450357 -11.725864 -15.435511 -16.219166 -15.891449][-10.647748 -10.808059 -10.512579 -7.6371779 -4.4384074 -1.7165589 1.6374407 3.4401793 2.7362809 -1.9869444 -7.5773716 -13.06176 -17.041735 -17.13105 -17.214481][-12.526071 -13.585095 -14.867222 -12.57279 -10.373148 -7.7805891 -4.8557906 -4.8883691 -5.13349 -6.2062364 -9.1427221 -14.841942 -17.972786 -17.993477 -17.070177][-17.758741 -17.776743 -18.015015 -17.263578 -16.615131 -14.414354 -12.561716 -12.620072 -12.268068 -12.56362 -13.728773 -15.634306 -16.618177 -16.951378 -16.468061][-19.557354 -17.46744 -17.979086 -19.529694 -19.670528 -17.85619 -16.489853 -16.106417 -16.160032 -15.421455 -15.064919 -15.411058 -15.517017 -14.711155 -13.607174][-15.707615 -14.948164 -13.657711 -12.474174 -12.793066 -14.5728 -15.562279 -14.550808 -13.720142 -13.797192 -14.007795 -13.119345 -12.451479 -11.592955 -11.204958][-10.744013 -10.091805 -8.3936825 -6.7042546 -6.81166 -7.2856989 -7.7657681 -9.3476887 -10.786104 -10.685728 -10.708149 -11.819122 -12.29212 -12.294422 -12.607244]]...]
INFO - root - 2017-12-15 15:41:55.765137: step 29310, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 54h:30m:42s remains)
INFO - root - 2017-12-15 15:42:02.336390: step 29320, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.687 sec/batch; 57h:53m:42s remains)
INFO - root - 2017-12-15 15:42:08.973286: step 29330, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 55h:49m:44s remains)
INFO - root - 2017-12-15 15:42:15.587545: step 29340, loss = 0.20, batch loss = 0.15 (11.8 examples/sec; 0.676 sec/batch; 56h:53m:20s remains)
INFO - root - 2017-12-15 15:42:22.178395: step 29350, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 54h:52m:18s remains)
INFO - root - 2017-12-15 15:42:28.868899: step 29360, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 56h:28m:51s remains)
INFO - root - 2017-12-15 15:42:35.473870: step 29370, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 54h:27m:03s remains)
INFO - root - 2017-12-15 15:42:42.103504: step 29380, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.692 sec/batch; 58h:17m:55s remains)
INFO - root - 2017-12-15 15:42:48.700680: step 29390, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 54h:30m:27s remains)
INFO - root - 2017-12-15 15:42:55.282225: step 29400, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 54h:37m:09s remains)
2017-12-15 15:42:55.840024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3823786 -6.737318 -6.3382831 -5.2031293 -4.4547186 -5.33753 -6.0163236 -6.3119097 -6.4337983 -5.7686272 -5.3316016 -6.1515336 -8.1877565 -6.6897874 -5.0360322][-6.6211219 -5.2918515 -5.2474937 -3.5627689 -3.5762665 -4.5962706 -5.317451 -5.3196955 -5.5072494 -5.7034283 -5.3363795 -5.5597038 -6.8122425 -6.0132761 -4.667479][-4.0908575 -3.4490592 -3.8202353 -1.6628919 -2.4459383 -3.2085052 -3.8618889 -4.1709476 -4.252902 -4.5073738 -4.6969948 -5.4071155 -7.1101151 -6.0980005 -4.5003757][-5.4611716 -4.5122046 -3.9982457 -2.379523 -2.6499236 -2.7895548 -3.4281623 -3.885344 -4.6687651 -4.9341154 -4.8019638 -5.7629519 -8.0263834 -6.4713492 -4.5832562][-5.1474156 -5.7643995 -5.3466363 -2.9510875 -2.5711374 -1.3891606 -1.2187605 -2.0177438 -3.1193066 -4.050065 -4.5659332 -5.0217891 -6.5403767 -5.7150359 -4.5107465][-7.2078838 -6.309473 -5.4196005 -3.15336 -0.92995644 1.2377067 1.4289875 1.2593722 0.1781106 -1.1047831 -1.9513986 -3.7571349 -6.1665673 -4.7671404 -3.5110481][-9.6511879 -8.217267 -6.2630043 -2.693476 0.26076221 3.3609738 4.4840817 3.657289 2.1801009 0.81835175 -0.2063756 -2.1661253 -5.3562446 -4.8865371 -3.8732517][-9.8551006 -7.7215538 -6.1783032 -2.2195072 1.3997817 3.8927808 5.7977986 5.0531659 2.8168693 1.2898726 -0.52746534 -3.1114523 -5.9777369 -5.26416 -4.747591][-9.377883 -7.6744852 -5.0972853 -2.3244405 -0.68140841 2.1611118 4.2451129 3.4407516 1.4338059 -0.60602808 -2.66723 -4.5273132 -6.961875 -6.967113 -6.1582685][-8.449852 -7.4836659 -5.5533948 -2.8798118 -1.5484428 0.16017532 1.2121663 0.34534216 -1.5171909 -2.4801986 -3.6810029 -5.865387 -9.0321436 -9.25699 -8.3296576][-10.016375 -9.4438934 -8.1361914 -6.0079994 -4.8122849 -3.527308 -2.4887576 -3.0202761 -4.4589548 -5.1382236 -6.1670337 -7.9881449 -10.535986 -10.405058 -9.51926][-12.725371 -11.645581 -10.342058 -9.0874443 -8.231163 -7.4474587 -7.050395 -7.0843153 -7.193769 -7.5631943 -8.8288851 -8.7239437 -10.49688 -10.528151 -9.066371][-13.051632 -12.038618 -10.213797 -9.2848177 -8.9042654 -9.05451 -8.9731693 -8.8699055 -8.7748737 -8.7959251 -9.2295237 -9.9489355 -10.280964 -8.4963312 -6.7272224][-10.165567 -9.2370377 -7.7803421 -6.7302656 -5.8922539 -6.4107614 -7.387651 -6.9306765 -7.2483606 -7.4001093 -7.035315 -6.859745 -7.5297513 -6.4995294 -4.8366418][-5.4672055 -4.7795892 -4.0782871 -2.8482518 -2.545383 -2.6603251 -3.0021565 -3.5144105 -3.8603451 -3.3929341 -3.9881756 -4.648674 -4.7098403 -4.7236023 -4.8541074]]...]
INFO - root - 2017-12-15 15:43:02.374980: step 29410, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 55h:11m:12s remains)
INFO - root - 2017-12-15 15:43:09.048537: step 29420, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 55h:44m:14s remains)
INFO - root - 2017-12-15 15:43:15.641073: step 29430, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 55h:22m:30s remains)
INFO - root - 2017-12-15 15:43:22.274895: step 29440, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 55h:38m:37s remains)
INFO - root - 2017-12-15 15:43:28.920823: step 29450, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 54h:02m:02s remains)
INFO - root - 2017-12-15 15:43:35.539506: step 29460, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 55h:02m:01s remains)
INFO - root - 2017-12-15 15:43:42.086069: step 29470, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 53h:22m:25s remains)
INFO - root - 2017-12-15 15:43:48.731478: step 29480, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 53h:51m:58s remains)
INFO - root - 2017-12-15 15:43:55.282431: step 29490, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 56h:54m:28s remains)
INFO - root - 2017-12-15 15:44:01.895331: step 29500, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 55h:19m:40s remains)
2017-12-15 15:44:02.426859: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6034689 -6.7234864 -6.42343 -6.714015 -8.4372406 -9.6966076 -10.348248 -9.88796 -8.77087 -7.0365248 -5.2906914 -4.8006783 -5.4660797 -4.28695 -2.4635992][-6.729311 -6.7657509 -5.8654747 -6.3009987 -7.76832 -9.3350115 -10.189949 -9.9014168 -8.5594368 -6.5992303 -4.9974174 -5.1436424 -5.849009 -4.5828805 -3.754931][-6.2228189 -6.5800958 -6.3314652 -5.8918438 -6.8692923 -8.1604815 -8.7906761 -8.0383263 -6.2785816 -5.1942005 -4.5436459 -4.8262491 -5.4129066 -4.3905487 -2.7326043][-7.1146545 -7.1571665 -6.7501221 -6.0952687 -5.7903342 -5.6095109 -5.3315816 -4.9585385 -4.6180849 -4.0083547 -2.8824914 -3.1222532 -3.6503789 -3.28839 -2.2490089][-8.2645864 -8.30357 -8.3660183 -6.0277948 -4.5304451 -3.5235891 -3.2462509 -3.1820233 -3.3915243 -2.9444695 -3.0790575 -3.6782906 -4.0407553 -4.0629077 -2.6919003][-9.4085369 -8.8337841 -7.0768414 -4.6146283 -2.3893762 -1.3461032 -0.62538719 -0.86155939 -1.8422718 -2.32089 -2.9642103 -3.6769023 -4.2083707 -3.5459032 -1.5366611][-9.6354961 -8.62082 -5.791245 -2.3598816 -0.28492451 1.4904752 1.994102 1.5000691 1.0104361 -0.076352119 -0.78708696 -1.7507536 -2.4498951 -1.0701098 0.54043007][-8.4450684 -7.3286834 -4.5551386 -0.22469044 2.6165576 4.2252021 4.2283921 3.3703046 2.2671208 0.81920528 0.78355217 0.24871397 0.39016247 1.6507211 3.2123389][-6.33442 -5.0753546 -3.2435291 -0.17855072 2.75448 4.17656 3.9865546 2.8343234 1.5115347 0.18100643 -0.036698818 -0.64745045 0.29303169 1.4696064 2.0390825][-5.7200289 -4.4769659 -2.9206331 -0.69349527 1.2973208 2.0571265 2.3096776 1.5397692 -0.36139011 -1.439322 -1.5664549 -2.3280873 -2.2844474 -0.89812136 0.41026211][-5.6758528 -5.0211835 -3.7302661 -2.6519618 -1.8395202 -0.71723127 0.36422396 0.21321106 -0.82811117 -2.1039276 -2.9202421 -3.9372594 -3.4816575 -2.4387944 -1.9938891][-7.23844 -6.4718723 -6.0801644 -4.8592172 -4.6136456 -3.9693246 -3.1322486 -3.2274437 -3.8762496 -4.0163026 -3.9629869 -4.8105974 -5.089859 -4.407145 -3.441401][-10.2794 -8.3258505 -6.3542194 -6.5386915 -6.6332068 -6.3299174 -6.3816376 -6.381577 -5.8899112 -6.0411549 -6.6345572 -6.2174673 -5.5675125 -4.4358168 -3.8566105][-7.9291806 -6.6986842 -5.0097814 -4.3741641 -5.1482811 -5.3525019 -5.3268247 -5.4341469 -5.7177415 -5.0923996 -4.5026679 -3.7445056 -4.4628935 -3.5302765 -2.9448409][-6.0593591 -4.1324787 -3.1512756 -2.7343872 -2.42181 -2.8291955 -3.5617566 -3.6255181 -3.4096937 -3.1431022 -3.5581663 -3.4181998 -3.3685105 -2.8260119 -3.4429419]]...]
INFO - root - 2017-12-15 15:44:09.085708: step 29510, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.697 sec/batch; 58h:41m:46s remains)
INFO - root - 2017-12-15 15:44:15.674197: step 29520, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 56h:15m:31s remains)
INFO - root - 2017-12-15 15:44:22.277934: step 29530, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 55h:15m:01s remains)
INFO - root - 2017-12-15 15:44:28.841666: step 29540, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.648 sec/batch; 54h:30m:10s remains)
INFO - root - 2017-12-15 15:44:35.452836: step 29550, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 57h:14m:35s remains)
INFO - root - 2017-12-15 15:44:41.996392: step 29560, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 54h:08m:21s remains)
INFO - root - 2017-12-15 15:44:48.602324: step 29570, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 57h:08m:42s remains)
INFO - root - 2017-12-15 15:44:55.258482: step 29580, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 55h:22m:32s remains)
INFO - root - 2017-12-15 15:45:01.927892: step 29590, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 53h:47m:06s remains)
INFO - root - 2017-12-15 15:45:08.610603: step 29600, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 55h:33m:46s remains)
2017-12-15 15:45:09.208843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.038347 -10.567772 -10.567207 -10.624782 -11.377887 -12.135086 -13.273669 -13.03269 -12.182039 -12.137187 -11.481043 -11.313692 -12.143366 -10.762135 -9.5448589][-9.7693338 -11.553345 -11.945332 -10.900505 -10.640818 -11.442333 -12.808748 -14.25436 -15.418083 -15.02492 -13.212568 -13.416988 -14.010983 -12.997999 -10.981517][-8.9257984 -10.202794 -11.58267 -11.077274 -10.731024 -11.644905 -12.536858 -13.232874 -14.146074 -14.567253 -13.644823 -12.712666 -12.896427 -12.699017 -11.885509][-8.9424171 -9.3266983 -9.7468681 -8.9737282 -9.4250164 -9.7930164 -9.6502905 -11.280002 -13.828249 -13.280678 -11.57865 -11.607632 -12.903843 -12.481981 -11.509191][-10.466251 -10.405346 -9.9143124 -8.356391 -6.0758109 -3.0583322 -1.5383296 -6.10051 -11.33213 -11.252163 -10.851578 -10.52921 -11.073904 -12.00392 -12.126629][-13.101961 -13.234843 -11.537189 -7.9359527 -3.7761593 1.3935261 6.4092145 2.8093238 -2.96089 -7.12599 -10.628723 -9.2857647 -9.2106962 -10.182665 -10.679214][-13.395453 -13.70832 -12.420533 -6.0420818 -2.1071291 2.7178407 8.95714 7.5564761 5.0939765 -1.7922263 -9.4726753 -9.3308334 -9.6513758 -10.33127 -10.641878][-12.804797 -12.656815 -10.470266 -5.2962255 -0.77204943 4.5121007 8.4784908 6.7172542 6.1291728 1.0334053 -4.5207653 -7.538847 -11.640928 -12.068634 -10.979506][-9.9788847 -9.6777916 -8.7607822 -5.6883421 -2.4420462 2.4378572 5.6970248 4.7481008 4.0723643 -0.34798193 -4.5851436 -8.42782 -12.86817 -13.817129 -13.273295][-8.7117367 -8.17408 -7.9949608 -6.0243778 -3.5827188 -1.64462 1.2610378 1.6869087 0.66802788 -2.2831647 -5.5899825 -8.9351034 -13.097553 -14.169891 -14.749559][-11.876074 -11.124128 -10.477727 -8.9078817 -7.9729624 -6.9627371 -5.2432613 -5.1253314 -5.6959424 -7.2476025 -9.1256342 -11.147396 -13.395525 -14.302006 -15.158243][-18.087627 -16.166763 -14.297586 -13.532934 -14.113909 -13.229527 -13.141443 -12.674654 -12.20817 -12.766425 -12.574226 -12.33396 -12.884552 -13.765045 -13.822266][-16.390394 -15.301743 -13.886593 -14.273113 -15.095074 -14.439137 -14.306221 -14.220865 -13.957319 -13.266117 -12.1102 -11.396366 -11.422819 -11.47921 -10.332272][-14.661785 -13.6858 -13.151957 -12.812672 -12.149454 -13.218515 -14.565218 -13.608025 -13.275211 -13.44466 -13.144602 -10.495187 -9.8421936 -8.4217358 -7.9131107][-10.063519 -8.6165848 -7.1043944 -6.6847758 -5.6109462 -6.2075963 -7.7099171 -8.501091 -9.08053 -8.7805328 -8.8647013 -8.8069963 -9.6514626 -9.5709829 -9.4681978]]...]
INFO - root - 2017-12-15 15:45:15.783784: step 29610, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.688 sec/batch; 57h:54m:06s remains)
INFO - root - 2017-12-15 15:45:22.336663: step 29620, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 54h:16m:04s remains)
INFO - root - 2017-12-15 15:45:28.910317: step 29630, loss = 0.27, batch loss = 0.23 (12.4 examples/sec; 0.645 sec/batch; 54h:13m:40s remains)
INFO - root - 2017-12-15 15:45:35.548738: step 29640, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 55h:20m:31s remains)
INFO - root - 2017-12-15 15:45:42.141652: step 29650, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 56h:34m:09s remains)
INFO - root - 2017-12-15 15:45:48.790052: step 29660, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 56h:00m:29s remains)
INFO - root - 2017-12-15 15:45:55.362850: step 29670, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 56h:02m:37s remains)
INFO - root - 2017-12-15 15:46:01.891179: step 29680, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 54h:54m:07s remains)
INFO - root - 2017-12-15 15:46:08.545331: step 29690, loss = 0.18, batch loss = 0.14 (11.5 examples/sec; 0.698 sec/batch; 58h:45m:01s remains)
INFO - root - 2017-12-15 15:46:15.111130: step 29700, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 56h:20m:47s remains)
2017-12-15 15:46:15.627514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1582155 -3.7861834 -2.8697362 -2.0192785 -2.9418607 -4.5054569 -5.9158826 -6.5398188 -6.2076974 -6.4375353 -6.2964578 -8.1392174 -9.3132572 -9.5024776 -8.963706][-2.9685464 -1.309669 0.5245266 -0.0051207542 -1.5945582 -3.1306751 -4.2109795 -4.8336339 -4.9923425 -4.9979324 -4.5670147 -6.5085773 -8.4434109 -9.0787258 -8.8352566][-1.1905675 -1.0100398 -0.17989683 0.55933237 -0.79884958 -2.7918763 -3.7687502 -4.1276484 -3.5153534 -3.8660808 -4.1040564 -5.0860596 -7.4858727 -9.7175322 -10.441284][-2.3643095 -2.7544177 -2.2253118 -1.92487 -2.7027845 -3.7167768 -3.896781 -3.6841693 -2.9566972 -1.9444313 -1.5929885 -4.3024588 -7.1990519 -9.09034 -10.141][-1.7431409 -3.4564066 -3.9100251 -2.821106 -2.7926111 -2.3957567 -1.8536937 -1.9756052 -1.8077571 -1.8666816 -2.101542 -3.729708 -6.7250504 -9.3594542 -10.36006][-3.5340481 -4.1920347 -4.378973 -2.7266171 -1.2067385 0.70511866 2.6040187 2.1067595 1.4505167 0.54025221 -0.967258 -3.3333642 -5.9350572 -8.87802 -10.310884][-6.3179889 -5.8268409 -5.1828232 -2.7157662 -0.30182266 2.3973441 5.4694743 5.5253472 4.0409627 2.3263412 0.54260969 -2.319392 -5.915803 -7.9776506 -9.3126335][-7.3567371 -6.5066128 -4.9564228 -2.2792621 0.2537384 3.0009818 6.3026328 6.8925157 6.3100619 4.0282617 1.3521018 -2.2247925 -5.8172703 -7.8285708 -8.5500278][-5.8463635 -5.4666967 -4.9546795 -2.4452195 0.099467278 1.4124866 3.5002704 5.071301 4.9230075 3.1522336 0.89158154 -2.9373891 -7.1744542 -8.9168053 -9.0706711][-6.344243 -6.5199833 -5.3774047 -3.668745 -2.3807087 -0.73799896 1.6454325 2.6613898 2.4108815 1.1665936 -1.0987244 -5.3897591 -9.5172253 -11.408989 -11.074139][-9.63643 -8.136734 -6.3818541 -4.8654232 -4.0521784 -3.4285 -2.2647169 -1.3369069 -1.7801702 -2.9566243 -5.0467844 -9.2540913 -12.747763 -13.456631 -12.37331][-11.537344 -10.370394 -9.2190914 -6.8552976 -5.9025683 -5.677989 -5.0281396 -4.9655809 -5.4269733 -6.7301917 -8.72626 -11.468223 -13.232735 -13.167753 -12.074312][-11.805763 -9.7062836 -8.3731518 -7.0712633 -7.8703818 -7.0278692 -6.2518449 -6.2478733 -6.4944816 -7.9068785 -9.2401867 -10.645015 -10.903582 -9.8858852 -8.2788868][-11.790497 -10.262842 -9.3161116 -7.9821448 -7.128593 -7.3141141 -7.0949278 -5.8421855 -5.5532951 -7.0257473 -7.9786091 -7.7765093 -7.3737278 -6.45144 -4.9843836][-8.948041 -9.2764854 -8.8666449 -7.1310263 -5.8549209 -5.8284073 -5.5149879 -5.8323617 -5.9762669 -5.8099031 -6.1272249 -6.9695139 -6.864315 -6.43483 -6.4111695]]...]
INFO - root - 2017-12-15 15:46:22.226770: step 29710, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 55h:41m:59s remains)
INFO - root - 2017-12-15 15:46:28.806625: step 29720, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 55h:30m:31s remains)
INFO - root - 2017-12-15 15:46:35.437143: step 29730, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 57h:04m:54s remains)
INFO - root - 2017-12-15 15:46:42.091480: step 29740, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 54h:48m:15s remains)
INFO - root - 2017-12-15 15:46:48.660881: step 29750, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 55h:54m:08s remains)
INFO - root - 2017-12-15 15:46:55.248769: step 29760, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 53h:37m:39s remains)
INFO - root - 2017-12-15 15:47:01.766351: step 29770, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 54h:28m:42s remains)
INFO - root - 2017-12-15 15:47:08.367143: step 29780, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 55h:36m:40s remains)
INFO - root - 2017-12-15 15:47:15.029237: step 29790, loss = 0.22, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 55h:08m:43s remains)
INFO - root - 2017-12-15 15:47:21.645887: step 29800, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 55h:18m:44s remains)
2017-12-15 15:47:22.214203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3380594 -6.9077635 -6.7614913 -6.8597345 -7.8229055 -8.29494 -8.3675451 -7.899436 -7.9597874 -8.4452019 -8.9202175 -10.831167 -12.653679 -11.990992 -9.1147614][-6.5811987 -6.7942271 -7.370759 -7.5559206 -7.5972919 -8.2300043 -8.4663773 -7.9760113 -7.5545444 -7.6077619 -8.1485958 -9.57212 -11.037735 -11.346048 -9.8350859][-2.6296136 -4.2743435 -7.0466371 -7.6553111 -7.5742211 -7.4942131 -7.2618651 -7.3433981 -7.4590435 -7.9491792 -8.4018517 -9.9203863 -11.691465 -11.973345 -10.348212][-3.2764673 -3.6666996 -4.6414223 -5.2490125 -5.7628245 -5.1553354 -4.4045019 -4.6006775 -5.6040092 -6.8746738 -8.03216 -10.206132 -12.57366 -13.171268 -11.16903][-3.1886959 -4.057735 -4.4312377 -4.0055208 -3.2878256 -1.4203506 -0.16901159 -1.9407995 -4.1740985 -4.9979615 -6.1133666 -9.1370373 -13.033457 -13.378126 -12.76556][-5.00992 -3.9901693 -2.9663539 -1.1893239 -0.1559 1.0892339 2.821753 2.7613654 0.65723324 -3.1733308 -5.7523384 -7.7267647 -11.064077 -12.076104 -11.722143][-7.141109 -5.7026062 -3.9420505 -1.3609176 0.8357687 2.8250546 5.0064254 5.7371345 5.2507024 0.55246973 -4.2299085 -7.996521 -10.968048 -11.247671 -10.825728][-6.7058926 -5.2067614 -3.7273369 -1.0526562 1.0369968 3.7715812 5.6895213 5.5727715 5.9698873 3.8263507 0.763185 -5.034719 -9.6095028 -10.507049 -10.677868][-3.5919142 -3.188658 -2.6778784 -1.6950316 -1.0373478 2.3946462 5.0063968 4.5202947 3.5801425 2.0157151 -0.049338341 -5.1240849 -9.8212643 -12.122839 -12.260196][-2.0796432 -2.7038343 -3.4830222 -2.87096 -3.2296851 -1.786221 0.27801609 1.6435666 1.6880379 0.26971436 -1.4153094 -5.5803113 -9.3313856 -12.280413 -13.846064][-5.8306274 -7.6395917 -8.6944427 -7.5960646 -7.7056112 -6.9140167 -6.3902841 -5.1682515 -3.8009496 -4.4048004 -6.2644176 -9.9793606 -11.88472 -12.435806 -11.72445][-11.289998 -12.134912 -11.977289 -11.082058 -10.522831 -10.277121 -11.241743 -10.58021 -9.1909637 -8.9354153 -9.90682 -12.110022 -13.414806 -13.338474 -11.866405][-14.124489 -12.49441 -10.564964 -10.651016 -10.933189 -10.471041 -10.652828 -10.419546 -10.454783 -10.448592 -10.989464 -11.970427 -12.21519 -11.35186 -10.441437][-12.057732 -10.668016 -8.9679546 -7.4792991 -6.9158845 -7.234314 -7.7289782 -8.395133 -8.9089279 -9.3182144 -9.2509022 -9.3399048 -8.8977966 -8.7314329 -8.4639664][-7.8313017 -7.6802726 -6.524014 -3.9409513 -2.75718 -3.7626722 -4.5362334 -4.5782433 -4.7872925 -4.8967113 -6.0022874 -6.6894984 -7.320816 -7.5836334 -7.2130876]]...]
INFO - root - 2017-12-15 15:47:28.871042: step 29810, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.685 sec/batch; 57h:34m:05s remains)
INFO - root - 2017-12-15 15:47:35.525660: step 29820, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.650 sec/batch; 54h:38m:55s remains)
INFO - root - 2017-12-15 15:47:42.140956: step 29830, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 56h:13m:05s remains)
INFO - root - 2017-12-15 15:47:48.685691: step 29840, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 54h:15m:42s remains)
INFO - root - 2017-12-15 15:47:55.364980: step 29850, loss = 0.11, batch loss = 0.06 (12.0 examples/sec; 0.667 sec/batch; 56h:03m:42s remains)
INFO - root - 2017-12-15 15:48:02.028891: step 29860, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 56h:33m:49s remains)
INFO - root - 2017-12-15 15:48:08.658115: step 29870, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 53h:58m:17s remains)
INFO - root - 2017-12-15 15:48:15.179464: step 29880, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 57h:11m:29s remains)
INFO - root - 2017-12-15 15:48:21.852393: step 29890, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 54h:20m:00s remains)
INFO - root - 2017-12-15 15:48:28.413699: step 29900, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 56h:21m:23s remains)
2017-12-15 15:48:28.917665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.716835 -8.503294 -8.2435179 -7.4831934 -7.5041981 -6.6248894 -5.8030567 -4.8532143 -4.0319514 -2.6042747 -1.2998276 -1.3248181 -4.3228917 -5.859962 -7.3594084][-7.8364859 -8.8162632 -7.6974711 -6.9106965 -6.9631152 -7.01712 -7.0815248 -5.8631263 -4.7718716 -3.1859832 -1.0332112 -0.95714188 -4.3301158 -6.2511721 -6.9455781][-7.0669155 -7.6943612 -7.2558694 -6.501637 -6.207561 -6.3938317 -6.5801654 -6.3172674 -5.3212843 -3.5068319 -1.6129518 -2.2903044 -5.6216984 -6.3900638 -7.220819][-7.9055476 -7.8137865 -6.8143153 -5.6839786 -4.8879938 -4.3281217 -3.7195563 -4.63475 -4.7458773 -3.8376074 -2.896102 -3.4464817 -6.5449529 -7.3733416 -8.5678558][-9.2100372 -9.2434235 -9.0303993 -6.6716185 -4.2348943 -1.9323773 -0.24407196 -1.387291 -2.3016713 -2.5998173 -3.1060355 -3.9386392 -7.2012696 -8.4399319 -8.5308342][-11.004886 -10.521459 -8.98578 -6.45932 -3.5158949 0.61208248 3.8123536 3.434361 2.0940247 -0.38515902 -1.6372519 -2.5065091 -6.0036726 -6.853363 -7.1512508][-11.890068 -11.054857 -9.0907459 -5.4185882 -1.6735325 2.3923907 6.6066957 7.7837558 6.4854865 2.5377498 -0.88564062 -2.7866826 -6.2174149 -6.48695 -6.9432554][-10.684502 -10.256386 -8.4626293 -4.5218372 -0.49300909 4.4119773 8.4123478 9.1619225 8.7867756 5.4565778 0.31428385 -3.729495 -8.3997784 -8.7940731 -8.2640562][-9.41755 -9.3413754 -7.4392328 -3.9397149 -0.97059965 3.5777488 6.5352845 7.0817103 6.7918916 3.5081458 -0.13155413 -4.1329346 -10.11237 -10.842362 -10.760173][-8.695219 -8.24307 -7.3971453 -5.1144104 -2.8211591 0.14758921 2.8353295 4.4030442 3.6940742 1.4221606 -1.3121295 -5.8069253 -10.771984 -12.231957 -12.976669][-11.637293 -10.17429 -8.1311865 -6.5631108 -5.9224744 -4.3214436 -2.3702092 -1.1220651 -1.1194234 -2.061228 -4.7073884 -8.7321453 -12.039604 -12.948015 -12.604523][-15.334925 -12.26871 -9.3285122 -7.610599 -7.0796218 -5.9677978 -5.1379929 -4.6829658 -4.7255731 -5.8947892 -8.1206284 -11.245388 -13.254408 -13.197696 -11.434972][-13.543556 -11.299653 -8.1738834 -7.006494 -6.9063668 -6.1874447 -6.0940361 -5.4276471 -5.5812879 -6.4653821 -8.18536 -10.899614 -11.735895 -10.947489 -8.592411][-10.147467 -7.8755159 -5.7080503 -5.0509124 -5.1284533 -5.5635657 -5.9550724 -5.3118939 -5.5196457 -6.5629697 -7.7626791 -8.5169983 -8.7040977 -8.1462774 -6.7341805][-6.3264904 -5.0408306 -3.43575 -1.9172175 -1.107583 -2.1142137 -2.8066149 -2.6488972 -3.65909 -3.996155 -4.8917718 -6.3587132 -6.9934521 -6.6251221 -5.9744973]]...]
INFO - root - 2017-12-15 15:48:35.603313: step 29910, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.698 sec/batch; 58h:39m:13s remains)
INFO - root - 2017-12-15 15:48:42.258000: step 29920, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.669 sec/batch; 56h:16m:08s remains)
INFO - root - 2017-12-15 15:48:48.905777: step 29930, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.656 sec/batch; 55h:07m:57s remains)
INFO - root - 2017-12-15 15:48:55.489806: step 29940, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 55h:48m:40s remains)
INFO - root - 2017-12-15 15:49:02.124148: step 29950, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 55h:29m:49s remains)
INFO - root - 2017-12-15 15:49:08.756756: step 29960, loss = 0.19, batch loss = 0.14 (11.2 examples/sec; 0.713 sec/batch; 59h:54m:31s remains)
INFO - root - 2017-12-15 15:49:15.374470: step 29970, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 55h:41m:48s remains)
INFO - root - 2017-12-15 15:49:22.033993: step 29980, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.666 sec/batch; 55h:58m:19s remains)
INFO - root - 2017-12-15 15:49:28.659810: step 29990, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 55h:42m:16s remains)
INFO - root - 2017-12-15 15:49:35.287126: step 30000, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 55h:57m:51s remains)
2017-12-15 15:49:35.898349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0757356 -4.8436 -4.189579 -4.0950575 -3.9951565 -4.0076847 -4.6136346 -5.1276388 -5.1311779 -5.3940191 -5.652854 -8.5731792 -11.730048 -11.962299 -12.700182][-3.6883674 -4.442677 -5.0096254 -5.1409931 -5.2531238 -5.3280721 -5.3163013 -5.9685864 -6.8386326 -6.8775578 -6.405304 -8.6514473 -11.313185 -12.024817 -13.438572][-2.8083854 -4.4622917 -5.5713468 -5.2785983 -5.5330729 -6.0969615 -6.3529782 -6.4685469 -6.6048231 -6.5188293 -6.0966344 -8.27372 -10.702307 -12.002007 -14.060476][-4.4652724 -5.162488 -5.1683187 -5.2771726 -5.4592309 -4.7202697 -4.2204738 -4.378479 -4.9817557 -4.7321515 -4.3722715 -7.3890996 -10.671938 -12.439477 -13.857616][-5.5972495 -7.3536134 -7.5873675 -6.4508038 -5.54809 -3.939414 -2.6285145 -3.1838865 -4.2843132 -4.2703562 -4.1743879 -7.0619764 -9.7957458 -11.626468 -13.383152][-7.6453066 -7.822084 -5.9551625 -4.4652128 -2.8134983 -0.58834934 0.95474434 1.5779028 0.84917831 -1.0750628 -3.2113855 -6.0772958 -8.6834946 -10.411598 -12.13758][-9.2047014 -9.6081171 -8.15523 -4.3525891 -0.9939332 2.4980645 4.4140677 4.5191531 4.1894679 1.6053147 -1.7333393 -5.3907671 -8.84374 -9.4492455 -11.039989][-9.2867928 -9.4319191 -7.3833694 -3.6592162 -0.046336651 4.3223233 7.2571359 7.3604379 6.4546437 4.0269551 0.72693491 -3.919574 -7.7852297 -8.9054575 -10.769769][-7.9988732 -8.1462669 -7.0127373 -3.8810871 -0.48233318 3.5212321 6.1043763 6.2698588 6.022697 3.5415053 0.36263371 -4.6142092 -8.7988033 -9.3615618 -10.296968][-6.5421 -8.327136 -7.4461474 -4.7565961 -2.9425931 0.084170341 2.3640161 3.2031693 3.2927308 1.5588636 -0.73138237 -5.8891621 -10.052192 -10.410883 -11.256006][-8.853075 -10.072176 -9.4931049 -7.6667595 -5.678679 -3.6492014 -2.7317808 -2.1345558 -2.1765454 -3.2693627 -4.923686 -9.1890841 -11.776627 -12.645888 -11.904051][-12.925647 -12.685833 -11.285173 -10.210455 -9.2592125 -7.8836708 -7.7065725 -7.240015 -7.1983113 -8.2777739 -8.9979019 -10.419724 -10.546664 -11.304314 -10.86462][-13.47748 -12.389555 -11.115436 -10.388552 -10.595591 -9.6480865 -9.2558069 -9.2027 -10.093151 -10.034369 -10.750957 -11.463718 -11.888299 -10.658368 -7.9965019][-11.124875 -10.480505 -10.473621 -9.1503267 -8.2598429 -8.6064939 -8.7508793 -9.0915861 -9.453516 -9.8491421 -11.318123 -11.014311 -11.175304 -9.6284924 -8.6324329][-7.7257338 -8.0860329 -7.7733746 -7.0168562 -6.5042105 -5.8557096 -6.024169 -6.753129 -7.6139479 -8.4911013 -8.0865974 -10.022064 -11.418839 -10.364271 -8.3465948]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 15:49:43.600129: step 30010, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 55h:33m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 15:49:50.215565: step 30020, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.663 sec/batch; 55h:42m:38s remains)
INFO - root - 2017-12-15 15:49:56.878837: step 30030, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 55h:07m:10s remains)
INFO - root - 2017-12-15 15:50:03.424004: step 30040, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 55h:38m:05s remains)
INFO - root - 2017-12-15 15:50:10.127728: step 30050, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.680 sec/batch; 57h:10m:10s remains)
INFO - root - 2017-12-15 15:50:16.785097: step 30060, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 55h:47m:44s remains)
INFO - root - 2017-12-15 15:50:23.484516: step 30070, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 57h:19m:22s remains)
INFO - root - 2017-12-15 15:50:30.121815: step 30080, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 55h:39m:26s remains)
INFO - root - 2017-12-15 15:50:36.901464: step 30090, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 56h:29m:26s remains)
INFO - root - 2017-12-15 15:50:43.505221: step 30100, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.675 sec/batch; 56h:42m:25s remains)
2017-12-15 15:50:44.035273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6207137 -1.7207584 -1.4870477 -0.86436558 -1.2787547 -1.7949953 -2.4130383 -2.9429922 -3.9152868 -4.218955 -4.5542045 -6.3249149 -8.2197247 -9.2005138 -7.917244][-1.0294695 -0.3266058 0.17384434 0.01914072 -0.31727457 -0.86060667 -1.3775506 -1.7824111 -2.3702512 -3.1604118 -3.8574488 -5.4064722 -7.2133665 -8.2985268 -7.4547844][-0.55610847 -0.60873842 -0.89846134 -0.19286537 -0.71095848 -1.1950383 -1.7791908 -1.9720893 -1.7450318 -2.069669 -2.1479006 -3.6845953 -5.5045209 -7.200901 -6.6509361][-2.7267857 -1.8750279 -1.869472 -1.3270659 -1.4778895 -1.3489065 -1.3372412 -1.0794382 -0.86271667 -0.97556305 -1.0028343 -2.3848994 -4.2447481 -6.1394897 -6.4217877][-3.2394938 -3.7200096 -4.1047487 -2.1762254 -2.0064783 -1.0894313 -0.49703789 -0.38948202 -0.040223122 0.32258034 0.46869659 -1.5591745 -3.9634023 -6.3947935 -6.1194363][-4.7667489 -4.2631583 -3.5871024 -1.4148831 -0.0847106 1.2530775 2.4698415 2.1084223 1.4821906 0.93864822 1.0760069 -0.45359516 -2.6152225 -4.5403404 -4.1510458][-4.7742167 -4.0250392 -2.3216355 0.1901741 1.5665531 3.09874 4.01866 3.8300328 3.6179748 2.6150956 1.8950171 0.12023067 -2.4589157 -3.63501 -3.0947592][-5.1253977 -3.8884907 -2.0765126 0.95958996 3.049789 4.7534881 5.6065793 4.8978772 4.4002709 3.2942996 2.1629496 0.48778963 -1.7098579 -2.7076952 -2.0784178][-5.1534534 -3.4771507 -1.5339823 1.1114817 2.4517922 3.1177716 3.7402015 3.7437263 3.4985185 2.293797 1.3578405 -0.80352879 -3.5409906 -4.3129683 -2.9221416][-5.1781535 -4.4576159 -3.6386724 -0.50452566 0.67876482 1.1611362 1.4656644 0.7388196 -0.0038795471 -1.2915931 -2.1254773 -4.0563493 -5.9471636 -6.253365 -3.885107][-9.3746777 -9.2085991 -8.0219154 -5.6755071 -5.3451319 -4.7431951 -4.26745 -4.5050898 -4.18541 -4.5536919 -5.5245805 -8.5205193 -9.4574347 -8.1809216 -5.2019162][-12.861368 -12.567139 -11.708166 -9.1935062 -8.4924812 -7.5583487 -7.2076993 -7.2757206 -7.37547 -7.4180088 -8.0545559 -9.0879812 -9.0023594 -8.2733622 -5.4383426][-13.134117 -11.819641 -10.561725 -9.4828424 -8.9643011 -7.5555134 -7.2013273 -7.0789065 -7.38443 -7.8897481 -8.5978012 -8.5963383 -9.2172852 -8.1102276 -5.1457338][-8.972291 -8.1247444 -6.4944663 -5.7976346 -4.8099613 -4.3847017 -4.2233367 -4.2640595 -4.8889594 -6.0335164 -7.928875 -7.8843441 -7.85662 -6.7672505 -5.4430647][-5.674902 -5.3285265 -3.6061347 -1.335156 -0.89581537 -1.3642106 -0.89087057 -0.99953556 -1.2998209 -2.7507217 -4.2344375 -5.6681442 -6.9701772 -6.2081437 -5.8910122]]...]
INFO - root - 2017-12-15 15:50:50.718788: step 30110, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.633 sec/batch; 53h:09m:36s remains)
INFO - root - 2017-12-15 15:50:57.328136: step 30120, loss = 0.19, batch loss = 0.14 (11.4 examples/sec; 0.699 sec/batch; 58h:44m:02s remains)
INFO - root - 2017-12-15 15:51:03.953880: step 30130, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 55h:39m:34s remains)
INFO - root - 2017-12-15 15:51:10.661498: step 30140, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 56h:18m:12s remains)
INFO - root - 2017-12-15 15:51:17.328187: step 30150, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.639 sec/batch; 53h:39m:29s remains)
INFO - root - 2017-12-15 15:51:24.004528: step 30160, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.654 sec/batch; 54h:57m:07s remains)
INFO - root - 2017-12-15 15:51:30.629082: step 30170, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 54h:51m:33s remains)
INFO - root - 2017-12-15 15:51:37.318300: step 30180, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 56h:19m:48s remains)
INFO - root - 2017-12-15 15:51:43.971111: step 30190, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 57h:02m:15s remains)
INFO - root - 2017-12-15 15:51:50.472034: step 30200, loss = 0.11, batch loss = 0.06 (11.9 examples/sec; 0.674 sec/batch; 56h:38m:09s remains)
2017-12-15 15:51:50.971053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1736503 -7.5670657 -8.7176895 -9.6895428 -10.10829 -10.204891 -10.450039 -11.355749 -12.581922 -12.607581 -11.827232 -10.118031 -9.764389 -7.9230185 -5.8057184][-8.1330948 -8.6213465 -9.0583153 -10.044042 -10.971993 -11.386396 -10.77532 -10.275866 -11.22644 -11.761621 -11.592642 -10.617044 -10.102542 -8.4888163 -8.2703295][-6.3386927 -8.352355 -10.293407 -9.8464413 -10.571124 -11.277241 -10.801302 -10.475033 -10.284375 -10.220541 -10.764553 -9.7437735 -9.4010448 -9.150176 -8.58927][-8.2069769 -9.20355 -9.9062519 -9.8598433 -9.5753317 -8.0549812 -7.1121216 -8.5501633 -9.8825006 -9.6156092 -9.559845 -8.7179661 -9.3046436 -8.9457788 -8.9534674][-8.0123453 -10.112998 -11.933622 -11.097358 -9.308013 -5.1236558 -1.2723923 -2.8961351 -6.3153224 -7.8314214 -10.077113 -9.032052 -8.6763144 -8.5242138 -9.0699549][-9.80591 -10.307268 -10.287683 -9.0777111 -7.0809727 -2.1117501 3.686955 4.2146935 2.3594966 -1.8910375 -7.4155483 -6.4667387 -6.2563658 -7.1024485 -7.3053508][-9.1921158 -10.789531 -9.044096 -5.966732 -3.495883 1.0958629 7.1290345 8.7003288 7.6915212 2.4843984 -3.6490653 -4.6698871 -5.9270272 -6.2035422 -5.8801146][-7.9766665 -10.191192 -9.9773827 -5.8840241 -1.31566 2.643105 7.274426 8.3105583 7.9307084 3.4001241 -1.7002783 -3.7984025 -6.3782678 -7.0807652 -7.391386][-6.7566614 -7.4184523 -8.5575638 -6.5891685 -3.1394227 1.1372328 4.7097869 4.9296594 6.2061639 3.3199277 -1.0838466 -3.4064958 -6.8047981 -8.0913916 -9.0671005][-6.2889438 -5.8105149 -7.3037634 -7.0135131 -5.2817116 -1.6392636 1.6430492 1.5540237 2.0054903 -0.12470102 -2.3550959 -3.6167033 -6.6914768 -8.7329807 -11.04932][-9.4417419 -10.473461 -11.733266 -10.65647 -9.2661123 -7.3283615 -4.1684322 -4.0868616 -4.953105 -6.0804234 -7.5468621 -8.4634991 -9.7560863 -11.033484 -12.789188][-13.235554 -12.801889 -13.322489 -12.872419 -12.481873 -11.644192 -11.270203 -10.455349 -9.7988682 -9.5753012 -10.771847 -12.163146 -11.969095 -13.396059 -13.327473][-12.128548 -12.14511 -12.340213 -11.791949 -12.443638 -11.444287 -11.41934 -12.325089 -12.622559 -11.135442 -10.045134 -10.530138 -10.451565 -11.839324 -11.667392][-9.8479395 -9.7717762 -9.7930574 -9.9898834 -8.6132479 -8.4491768 -8.9095049 -9.3695564 -10.707105 -11.030806 -10.175206 -8.6879635 -8.5803785 -8.381093 -8.2223492][-6.2020135 -6.8374648 -6.701673 -6.7425056 -5.3829384 -5.0326738 -4.8281388 -5.0470543 -6.9649782 -7.8105016 -8.2530508 -6.9454756 -6.9188385 -7.2858667 -7.6012359]]...]
INFO - root - 2017-12-15 15:51:57.510866: step 30210, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 54h:29m:11s remains)
INFO - root - 2017-12-15 15:52:04.157991: step 30220, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 55h:13m:05s remains)
INFO - root - 2017-12-15 15:52:10.723831: step 30230, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 54h:27m:21s remains)
INFO - root - 2017-12-15 15:52:17.309145: step 30240, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 55h:49m:11s remains)
INFO - root - 2017-12-15 15:52:23.949040: step 30250, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 55h:57m:04s remains)
INFO - root - 2017-12-15 15:52:30.590212: step 30260, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 55h:59m:58s remains)
INFO - root - 2017-12-15 15:52:37.323980: step 30270, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 55h:08m:03s remains)
INFO - root - 2017-12-15 15:52:44.033217: step 30280, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 55h:21m:24s remains)
INFO - root - 2017-12-15 15:52:50.693153: step 30290, loss = 0.12, batch loss = 0.07 (11.5 examples/sec; 0.698 sec/batch; 58h:37m:31s remains)
INFO - root - 2017-12-15 15:52:57.289556: step 30300, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 53h:59m:00s remains)
2017-12-15 15:52:57.808154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0075073 -4.9868412 -4.8809156 -5.4899068 -6.8231006 -7.1267791 -7.0984793 -7.5076952 -8.2676449 -8.0933933 -7.4868159 -6.4282613 -7.0988755 -6.5956345 -5.2416968][-5.6917715 -6.2524037 -5.8741951 -5.3398833 -5.8540707 -7.2073355 -7.7732553 -7.6813612 -8.1555185 -8.656786 -8.4504452 -7.24072 -7.200388 -7.3209352 -6.3535395][-3.5400965 -5.3612843 -6.5036511 -6.0135603 -6.21698 -6.6400976 -6.2822018 -7.0535083 -7.5282011 -7.811512 -8.3893585 -7.1867285 -7.3000793 -7.8117032 -8.3172741][-3.988771 -5.4257393 -5.4067373 -4.8851638 -5.3826394 -5.5313945 -5.1094179 -5.9446745 -7.2615128 -7.7920337 -7.7953725 -7.5441766 -8.9544992 -9.4870014 -9.7528639][-5.4655175 -6.3192725 -6.3083935 -4.9840755 -3.4550769 -1.4057202 0.41579151 -1.6433897 -3.3448431 -4.6757364 -6.3384533 -5.1517158 -6.4021087 -8.0224762 -8.5220194][-7.1451192 -6.3086119 -5.2836823 -4.082108 -1.6072955 1.578599 4.44399 4.5440526 3.7118411 -0.30159235 -4.7393541 -4.5596204 -5.6670933 -6.0234308 -6.3294511][-7.6586342 -6.6943412 -5.2912984 -2.5690525 -0.28191662 2.4571691 5.9165425 6.85889 6.8337646 1.3479462 -4.3437133 -4.3076057 -7.1567421 -7.8509259 -6.2414103][-8.8378649 -7.4568238 -6.3873911 -3.9354403 -0.8862772 2.9517846 6.2925 7.2000327 7.0720716 2.7240634 -1.3131647 -4.0128479 -8.4024353 -8.7925014 -8.2251358][-6.9557018 -6.5908957 -6.6468687 -5.4826469 -3.8487396 -0.35434294 3.1457114 4.5082927 4.6475215 1.4982982 -1.6347933 -4.7631111 -9.6344948 -10.902169 -10.100538][-5.8439312 -6.1014471 -5.8520861 -5.558743 -5.9318857 -3.8486915 -1.8994045 -0.23918247 1.1677179 -1.2046099 -4.4241281 -6.7620592 -9.4006252 -11.017984 -11.272736][-9.5561028 -10.251682 -9.8518934 -9.824934 -9.7613373 -8.8774166 -8.389349 -7.244113 -6.6948318 -7.0088139 -7.8810205 -10.078039 -12.722885 -11.804001 -10.281782][-13.708874 -13.91946 -13.384961 -12.809387 -12.359406 -12.829258 -13.33527 -12.896263 -11.992931 -11.600067 -11.494814 -11.964361 -12.08068 -12.680729 -12.214037][-12.18993 -12.315372 -11.915491 -11.892878 -12.117764 -11.175902 -10.77532 -11.534609 -11.77076 -11.347198 -11.105268 -11.292534 -9.9795818 -9.07375 -8.3732109][-11.008747 -10.421345 -9.2191563 -8.9151087 -8.9860659 -8.6988811 -8.9484453 -8.7480516 -8.9642429 -9.8368683 -10.119146 -8.8067656 -7.8033113 -7.3365664 -5.5877972][-8.1917028 -7.6220317 -6.4948082 -5.2479939 -3.8000109 -3.3235936 -4.3396893 -5.1065722 -5.4695129 -5.440136 -6.0380583 -6.8047638 -6.6951356 -6.1683021 -6.0159187]]...]
INFO - root - 2017-12-15 15:53:04.293693: step 30310, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 54h:29m:48s remains)
INFO - root - 2017-12-15 15:53:10.850609: step 30320, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 54h:52m:16s remains)
INFO - root - 2017-12-15 15:53:17.298784: step 30330, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 54h:54m:02s remains)
INFO - root - 2017-12-15 15:53:23.818968: step 30340, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 54h:18m:45s remains)
INFO - root - 2017-12-15 15:53:30.471316: step 30350, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 54h:01m:52s remains)
INFO - root - 2017-12-15 15:53:37.159670: step 30360, loss = 0.17, batch loss = 0.13 (11.6 examples/sec; 0.692 sec/batch; 58h:05m:31s remains)
INFO - root - 2017-12-15 15:53:43.760444: step 30370, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 54h:59m:57s remains)
INFO - root - 2017-12-15 15:53:50.416631: step 30380, loss = 0.14, batch loss = 0.09 (11.4 examples/sec; 0.703 sec/batch; 58h:59m:34s remains)
INFO - root - 2017-12-15 15:53:57.134487: step 30390, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 55h:14m:26s remains)
INFO - root - 2017-12-15 15:54:03.732385: step 30400, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 54h:15m:34s remains)
2017-12-15 15:54:04.216295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.92445 -4.9173641 -5.5339603 -5.993412 -7.2450175 -7.6966214 -8.2850838 -8.7254639 -9.05205 -9.5822868 -9.5371914 -9.6840782 -10.546974 -9.7938747 -6.278523][-7.255847 -6.7529211 -6.5531874 -6.9404821 -8.0940065 -9.3398409 -9.971693 -10.027304 -10.166989 -10.066481 -9.6848221 -9.9192162 -10.843287 -10.830402 -7.7277727][-5.9246817 -6.3437958 -7.259346 -7.4047732 -7.7546721 -8.66149 -9.3163013 -9.3290052 -9.2149086 -9.2389593 -9.2577305 -8.8928423 -9.68035 -10.140632 -7.9667397][-6.9180393 -6.8386965 -7.2994156 -7.5802178 -8.3500576 -8.1932335 -8.1205025 -9.0103722 -9.3102694 -8.6914968 -8.5475407 -8.4639072 -8.9102631 -8.6419554 -6.969203][-7.1012506 -8.16353 -8.8682346 -8.4069386 -7.4997025 -6.3622112 -5.6898718 -6.6370072 -7.0514107 -7.3313379 -7.6865005 -7.7362118 -9.11908 -9.2033367 -6.3176742][-7.5887828 -7.2052956 -6.37323 -5.7454433 -4.6355577 -0.99546051 1.743505 0.62287521 -0.59153175 -2.3685684 -4.5208893 -5.4473171 -7.1878247 -7.7101269 -6.4176245][-8.80308 -8.5063572 -7.3279786 -5.5514674 -2.7888358 0.60040379 4.902308 6.3059077 6.0418496 1.7846928 -2.5160151 -3.2961414 -6.0575609 -7.6661677 -6.8156834][-8.96328 -8.9793587 -7.8055325 -4.715795 -0.96015787 2.8990941 6.1925387 8.0422764 9.2202778 3.5774484 -2.8174231 -4.7258711 -6.9610529 -8.110858 -7.57829][-6.357192 -6.9653683 -7.178545 -4.7974324 -1.8440945 1.3531656 4.4228053 6.4735131 7.141542 2.934689 -1.6835456 -4.8223033 -9.1297226 -10.139726 -8.4236679][-4.3558087 -4.1427813 -4.4396639 -3.2360473 -1.8065965 0.17512751 1.4738426 2.6716542 2.8628316 -0.70437908 -4.0946689 -6.9924412 -10.929033 -12.744028 -11.737454][-6.19689 -5.3657475 -4.9982376 -4.5147762 -4.0778918 -3.638459 -3.5636604 -4.217658 -5.4254317 -6.9955578 -8.7038527 -11.226236 -13.892591 -14.464783 -12.145286][-11.424568 -10.609415 -10.050194 -8.5287094 -7.8065224 -7.8390837 -8.8899651 -9.6963863 -10.697191 -11.813522 -13.195181 -13.886387 -14.434856 -13.694154 -10.97362][-11.437149 -10.575106 -9.2526169 -8.3066521 -9.1141138 -9.2370462 -8.8967152 -10.162775 -11.199657 -11.580508 -11.701574 -12.767565 -12.80644 -11.028833 -7.2730293][-10.838513 -9.3981247 -8.5417309 -8.266181 -8.6832819 -8.663518 -8.8785076 -8.9305677 -8.7863016 -9.1318789 -9.7624865 -9.8428974 -9.3710976 -8.9857121 -6.4286089][-8.735507 -7.8166676 -6.1136842 -4.784359 -5.48622 -5.25084 -5.2312474 -5.05744 -5.88503 -6.1507382 -6.5377378 -7.7210026 -8.0882187 -7.2531271 -5.5151882]]...]
INFO - root - 2017-12-15 15:54:10.859700: step 30410, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.633 sec/batch; 53h:05m:15s remains)
INFO - root - 2017-12-15 15:54:17.420572: step 30420, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 55h:34m:45s remains)
INFO - root - 2017-12-15 15:54:23.946096: step 30430, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 54h:56m:01s remains)
INFO - root - 2017-12-15 15:54:30.539520: step 30440, loss = 0.21, batch loss = 0.17 (11.9 examples/sec; 0.672 sec/batch; 56h:25m:19s remains)
INFO - root - 2017-12-15 15:54:37.091920: step 30450, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 53h:48m:43s remains)
INFO - root - 2017-12-15 15:54:43.674722: step 30460, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 53h:54m:50s remains)
INFO - root - 2017-12-15 15:54:50.347641: step 30470, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.633 sec/batch; 53h:06m:59s remains)
INFO - root - 2017-12-15 15:54:56.983393: step 30480, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 55h:41m:22s remains)
INFO - root - 2017-12-15 15:55:03.604070: step 30490, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 53h:54m:49s remains)
INFO - root - 2017-12-15 15:55:10.277108: step 30500, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 55h:19m:25s remains)
2017-12-15 15:55:10.828454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5491991 -6.3724108 -5.1397138 -4.0364785 -3.4880614 -3.3051991 -3.8804407 -4.7247338 -5.4834838 -5.5364833 -5.7983365 -8.64295 -10.972689 -10.516619 -9.885479][-5.259995 -4.6685658 -4.7468195 -2.9640591 -2.816242 -2.9664578 -3.7210889 -4.39453 -5.2662039 -5.9415584 -5.8245721 -8.6279678 -11.866003 -11.99214 -12.058636][-2.7481921 -2.7809355 -2.101083 -0.81880856 -1.3224478 -1.7141924 -2.6876082 -3.7244818 -4.3270512 -4.591701 -4.4785662 -7.1559873 -10.251667 -11.117484 -11.234814][-3.0536757 -1.9941888 -0.4731698 0.66790152 -0.091803551 -0.67553473 -1.5144429 -1.9973686 -2.9283519 -2.8500862 -1.8268249 -5.0040159 -8.3203068 -8.8580551 -9.0463219][-3.9355116 -3.4985456 -1.9570737 -0.16694021 0.49549055 1.4779153 0.72246695 -0.51013327 -1.3898659 -1.7497289 -1.0784221 -3.2139068 -6.2533636 -7.1645145 -8.5396347][-5.8812475 -4.9505668 -3.3510568 -0.76320124 0.6948657 1.9663405 2.0140219 1.0542459 0.28150892 0.34058332 0.35289478 -2.3854632 -5.1419091 -5.6983109 -7.0045204][-5.3003645 -4.6857853 -3.7360053 -1.4291306 0.61266756 2.0025601 2.297008 2.620913 2.3842626 2.46495 2.656281 -0.53292751 -4.0475283 -4.5478144 -5.7370305][-4.4069982 -2.8309164 -1.6209431 1.2638493 2.4183764 2.6429248 2.2021399 2.643703 3.235271 3.2516637 3.013989 0.31872606 -2.8207474 -3.8649588 -4.7623982][-4.6136532 -3.0127797 -1.5970731 0.82671738 1.7034006 1.7705126 1.5064106 1.8101664 2.0593743 1.7599821 1.6968374 -0.32465029 -2.9909382 -3.056988 -3.3598177][-4.3186245 -3.6587627 -2.2752666 -0.014164925 0.3132844 -0.14631224 -0.4951663 -1.14428 -1.2670584 -0.6084857 -0.36860037 -2.5191922 -3.9963636 -3.0016351 -3.2921166][-7.089365 -6.4155006 -5.1207666 -3.439853 -3.4879754 -4.4197192 -5.8489518 -5.4410586 -4.4814034 -3.3912094 -2.6073549 -5.0216742 -6.0283871 -4.6447258 -3.7912462][-10.366964 -8.881093 -7.9570808 -7.111496 -7.3589354 -7.9739857 -8.7628775 -8.7000237 -7.8579106 -6.5396528 -5.5544596 -6.4478192 -6.8453159 -5.0295038 -4.3943415][-10.817223 -9.9477787 -8.5476732 -8.1961765 -8.37915 -8.8635635 -9.1298618 -8.7416477 -8.2092667 -6.8642631 -6.2977057 -6.5102119 -5.76113 -3.75266 -2.4512086][-8.7820749 -8.1494093 -7.4328971 -6.7444897 -6.5587072 -7.0321593 -6.9268994 -6.3065596 -5.7904344 -5.594841 -4.6962061 -3.5048831 -3.4090643 -1.8847127 -1.1374598][-6.1181989 -6.3029256 -5.9689417 -5.2793183 -4.3193965 -3.9884284 -3.470788 -3.0534012 -3.063725 -2.4290657 -1.7537053 -3.124233 -3.1195979 -1.8592951 -2.2332091]]...]
INFO - root - 2017-12-15 15:55:17.459284: step 30510, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 55h:50m:12s remains)
INFO - root - 2017-12-15 15:55:23.969045: step 30520, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 56h:34m:12s remains)
INFO - root - 2017-12-15 15:55:30.522203: step 30530, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 55h:26m:12s remains)
INFO - root - 2017-12-15 15:55:37.097279: step 30540, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 56h:04m:31s remains)
INFO - root - 2017-12-15 15:55:43.737658: step 30550, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 55h:28m:01s remains)
INFO - root - 2017-12-15 15:55:50.338318: step 30560, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 54h:06m:52s remains)
INFO - root - 2017-12-15 15:55:56.942634: step 30570, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 55h:31m:48s remains)
INFO - root - 2017-12-15 15:56:03.529617: step 30580, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 55h:25m:17s remains)
INFO - root - 2017-12-15 15:56:10.035881: step 30590, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 54h:20m:14s remains)
INFO - root - 2017-12-15 15:56:16.596093: step 30600, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 55h:11m:23s remains)
2017-12-15 15:56:17.083913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4613934 -3.4160981 -3.7852716 -3.6075668 -4.4320879 -4.8676538 -4.9807892 -4.5416384 -3.2586112 -1.7700415 -1.3845816 -3.7808576 -6.249619 -7.9644547 -8.2830257][-3.0574121 -3.3554213 -2.8561449 -3.2933645 -4.3690515 -5.2950377 -5.2973208 -4.2856855 -3.3303668 -2.1555247 -1.3545547 -3.0941522 -5.875814 -7.7674475 -7.924191][-2.5964949 -3.3485746 -4.0427446 -4.0911355 -4.9880624 -5.3657808 -5.0432034 -4.3929672 -3.4051292 -2.9867747 -2.7402532 -4.6509914 -6.9766483 -8.2351866 -8.7091808][-2.4988556 -2.9585977 -3.2963459 -3.9719129 -4.3273621 -4.6423597 -4.327425 -4.241394 -4.0325842 -3.2159827 -2.1269939 -3.69098 -5.7310376 -7.1078143 -7.9112072][-3.1098948 -4.0320706 -5.0505939 -4.1605992 -3.296911 -2.7240148 -2.1069031 -2.3137159 -2.7074373 -2.6508307 -2.4123383 -3.4066095 -4.5866265 -5.47662 -6.1287451][-3.8181882 -3.6606066 -3.4364884 -3.1267552 -1.2547879 1.2554069 2.4887843 1.5354762 0.59840965 -0.32254744 -0.70573473 -1.7372515 -3.3711751 -4.3563709 -4.811419][-3.5974097 -3.6398649 -3.3978531 -1.9990792 -0.26437664 2.4582353 4.9198394 4.59648 3.7548575 1.4637809 -0.430161 -2.163739 -4.1771421 -4.5247984 -4.6900582][-3.7770309 -3.1048157 -1.9133363 -1.4552426 -0.4159956 2.4667115 4.9417272 5.0681491 4.6077323 2.2865167 -0.20060444 -2.3909714 -4.6927652 -5.2403512 -5.2676444][-2.9938378 -2.72914 -2.0170145 -0.82366943 -0.19124985 1.420259 2.9043536 3.8759837 3.6684566 1.6633635 -0.20013237 -3.7549577 -7.4422588 -7.6814623 -6.45637][-3.1642478 -2.4366605 -1.622818 -1.0727334 -0.66749573 0.74403906 1.9066224 2.1347756 1.9553313 -0.3377924 -2.4147463 -5.6890211 -8.980546 -9.8824539 -9.5540905][-5.8477659 -5.9846182 -4.7840204 -3.4354973 -3.3411145 -2.7417479 -2.3438306 -2.5062728 -3.1396186 -4.3466949 -5.497725 -9.18074 -11.621786 -11.818846 -10.988699][-9.5938406 -9.24646 -8.22057 -7.1800504 -6.7043066 -5.072814 -5.1542292 -5.8192344 -6.6306562 -7.9451642 -9.2524986 -10.840899 -11.14977 -12.155723 -12.038425][-12.073275 -11.666967 -10.111471 -9.087676 -8.0799837 -7.1731658 -7.1989789 -7.7989254 -9.2890377 -10.007793 -10.675541 -11.562199 -11.58579 -11.165606 -10.492889][-9.9480724 -9.8131781 -9.0441723 -8.2440319 -6.7281766 -6.801415 -7.3833647 -7.6627345 -8.0437508 -8.8324413 -9.5211039 -9.0403461 -8.7485847 -8.9696445 -9.2706146][-6.6951675 -6.4576473 -5.7908235 -5.0236697 -4.7405319 -4.94284 -4.6212053 -4.8285131 -5.8379636 -6.0946522 -6.3616076 -6.9649167 -7.0887809 -7.4892988 -9.3319712]]...]
INFO - root - 2017-12-15 15:56:23.705538: step 30610, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 55h:04m:06s remains)
INFO - root - 2017-12-15 15:56:30.206002: step 30620, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 54h:17m:45s remains)
INFO - root - 2017-12-15 15:56:36.789237: step 30630, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 54h:04m:45s remains)
INFO - root - 2017-12-15 15:56:43.350006: step 30640, loss = 0.21, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 56h:04m:30s remains)
INFO - root - 2017-12-15 15:56:49.905731: step 30650, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 54h:40m:31s remains)
INFO - root - 2017-12-15 15:56:56.575265: step 30660, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 54h:45m:41s remains)
INFO - root - 2017-12-15 15:57:03.181633: step 30670, loss = 0.27, batch loss = 0.22 (11.6 examples/sec; 0.690 sec/batch; 57h:51m:14s remains)
INFO - root - 2017-12-15 15:57:09.725697: step 30680, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 54h:18m:18s remains)
INFO - root - 2017-12-15 15:57:16.245106: step 30690, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 56h:23m:17s remains)
INFO - root - 2017-12-15 15:57:22.878271: step 30700, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 57h:13m:09s remains)
2017-12-15 15:57:23.448857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6180775 -2.9712563 -2.523715 -2.0454254 -1.9395099 -1.7672637 -1.813798 -1.8385565 -1.7023282 -1.9759631 -2.0428624 -4.3335133 -6.7093716 -7.8707547 -7.9574795][-1.0552011 -1.1376123 -0.55539131 -0.91105127 -1.0471482 -0.36183357 0.23298359 0.13756418 -0.61374855 -0.63665485 -0.29212952 -2.1239593 -3.4372776 -5.599884 -5.8663692][0.3386178 -0.19309044 -0.67311764 -0.69230127 -0.58177805 0.18522787 0.38741207 -0.01046133 -0.65886068 -1.1318612 -0.83490086 -2.0194852 -2.7507362 -4.1906147 -5.7586517][-1.8843708 -2.4177563 -1.8132801 -2.1899333 -1.8751259 -1.1987791 -1.164669 -2.0436966 -2.6629148 -2.083349 -1.0873308 -2.4994557 -4.0583773 -5.9281178 -7.1082649][-2.5393059 -4.0109968 -3.9794991 -2.1756053 -1.7245581 -1.0204344 -0.27359915 -1.3574328 -3.2961428 -3.1727247 -1.9862778 -3.0668497 -4.57471 -7.511548 -8.7646151][-5.7713027 -5.3643122 -3.2292836 -1.2099628 1.5628285 2.4998937 2.2428904 0.98806524 -1.4531984 -2.8858294 -2.9454362 -4.0626392 -4.9100904 -6.8783159 -7.8939447][-7.1422381 -6.9532146 -4.565032 -0.86249733 1.8622642 4.3332009 5.7873921 3.4502625 0.6666975 -1.8257127 -3.9328713 -5.5876808 -5.9213724 -6.8698545 -8.0520191][-7.4150949 -7.6983762 -5.959506 -1.8013284 1.4191284 4.2491565 6.2270017 4.2132277 0.85512352 -2.2631834 -4.3238711 -6.4346147 -7.1910362 -8.5099812 -9.4260216][-8.15447 -7.0601091 -4.8454585 -2.6897967 -1.3750815 1.4903088 3.2846923 2.2099066 0.38237524 -3.1095142 -5.3180251 -8.2682228 -9.7900944 -10.851639 -11.436501][-7.5425572 -7.7269926 -6.5905056 -4.4938688 -3.485388 -2.1947708 -0.015951157 0.099999428 -1.2852149 -3.5543036 -5.7320981 -9.085721 -11.004598 -12.712109 -12.809084][-10.1571 -10.894324 -9.98643 -8.5886078 -7.3228636 -5.4394217 -4.7887187 -4.3022466 -3.9703131 -5.5323658 -6.8264828 -9.8267479 -11.73364 -12.865042 -12.270496][-13.79355 -13.631201 -12.160451 -10.603001 -9.4162064 -8.554657 -8.3469658 -8.0361767 -7.6467333 -7.9082642 -8.2348108 -9.616909 -10.186727 -11.541063 -11.946968][-12.718855 -11.142765 -9.9284353 -9.8315258 -9.8942394 -9.1829576 -8.6502752 -8.7932072 -9.2903214 -9.3935375 -8.9110441 -9.3301649 -9.3893795 -9.7107573 -9.27818][-10.702976 -9.7621717 -8.5035315 -7.3323565 -7.126977 -7.7763529 -8.93231 -8.828207 -8.75848 -9.0857315 -9.4898052 -8.981122 -7.8972549 -8.5954933 -8.2146015][-7.6609015 -7.4004216 -6.57493 -5.1023464 -4.4378929 -4.3324776 -4.6646166 -5.9419785 -6.8799906 -6.670577 -6.739954 -8.1293144 -8.87891 -8.811883 -8.6231785]]...]
INFO - root - 2017-12-15 15:57:30.003264: step 30710, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 55h:38m:01s remains)
INFO - root - 2017-12-15 15:57:36.512422: step 30720, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 54h:56m:05s remains)
INFO - root - 2017-12-15 15:57:43.133822: step 30730, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.687 sec/batch; 57h:34m:25s remains)
INFO - root - 2017-12-15 15:57:49.718915: step 30740, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 55h:35m:34s remains)
INFO - root - 2017-12-15 15:57:56.236640: step 30750, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 55h:25m:00s remains)
INFO - root - 2017-12-15 15:58:02.800703: step 30760, loss = 0.18, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 53h:20m:43s remains)
INFO - root - 2017-12-15 15:58:09.329010: step 30770, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 55h:13m:48s remains)
INFO - root - 2017-12-15 15:58:15.919617: step 30780, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 54h:59m:58s remains)
INFO - root - 2017-12-15 15:58:22.514710: step 30790, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 55h:13m:11s remains)
INFO - root - 2017-12-15 15:58:29.058025: step 30800, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 56h:02m:22s remains)
2017-12-15 15:58:29.612133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8278923 -6.8542318 -6.1538739 -5.2632651 -5.9968 -6.5350647 -6.9020643 -7.1601396 -6.9778342 -6.7867169 -6.7204633 -5.79122 -7.1530972 -8.134058 -4.856987][-8.2200432 -7.2564087 -6.1872916 -5.4350166 -5.607615 -6.3967867 -6.8187213 -6.9475312 -6.87906 -6.96871 -7.0533714 -5.9429641 -7.1545591 -8.6047554 -5.8164353][-6.7030416 -6.2258906 -6.1857281 -5.3653522 -5.7130227 -6.0719256 -6.1940341 -6.2949629 -6.1536045 -6.4986157 -7.4618015 -6.8529744 -7.6911883 -9.1263561 -6.8759851][-5.9824038 -5.2561183 -5.2897062 -5.5266786 -6.275948 -5.9082112 -5.1036735 -5.2842736 -5.6440587 -6.2063303 -6.9427419 -6.8012247 -8.27022 -9.1299095 -6.0520849][-6.99967 -5.6789284 -5.7677903 -5.6173019 -5.3025908 -3.8128867 -2.3227472 -3.2148187 -4.3362103 -5.760397 -7.0118718 -6.5208268 -7.5994749 -9.2669926 -6.5970964][-9.5140533 -7.3777771 -6.5559654 -5.329885 -3.8274989 -1.002665 1.5231447 1.3814907 -0.22237635 -3.3787262 -5.8958778 -5.0262957 -6.4394808 -8.6047363 -6.1530271][-11.008659 -8.9007463 -7.8816118 -5.1718478 -2.7007449 0.50102472 4.1131244 5.1509757 4.3924289 0.12888908 -4.4353027 -4.497982 -6.04116 -7.4061413 -5.2444634][-11.510778 -9.273263 -8.1652308 -4.6777339 -1.49016 2.8907094 6.2716289 6.9198356 6.5631747 2.8456063 -1.5726519 -3.390759 -6.5460539 -8.4137 -6.2412066][-10.280789 -8.3953753 -7.680047 -5.1872668 -2.9874058 1.3398128 4.6951327 5.5961328 4.5772185 1.101687 -1.7217696 -3.2221897 -6.6098704 -9.2834091 -7.1601353][-8.6779833 -7.2329593 -6.927268 -5.64652 -4.737761 -1.5126696 1.0633893 2.4466214 1.7505836 -1.6222134 -4.327003 -4.8949661 -6.8340073 -9.4441843 -7.8938632][-9.6486025 -7.8381643 -7.3297005 -6.4073358 -6.3574624 -5.1555305 -4.42451 -2.7996702 -2.5234792 -4.4872494 -6.5903449 -7.0624065 -8.2618542 -9.92079 -7.4093752][-13.24482 -11.296104 -10.866567 -9.5510187 -9.2558336 -8.9989548 -8.8545666 -7.959712 -7.6506214 -8.3749352 -9.0165777 -8.57745 -8.5660982 -9.4924841 -6.6680889][-12.806079 -10.900745 -10.55844 -9.3735313 -9.6858673 -8.9437275 -8.91643 -8.7568455 -8.8694859 -8.9171343 -8.7954149 -7.5366578 -8.0377932 -8.7884254 -6.0335493][-10.051584 -8.7814789 -9.1482344 -8.7714157 -8.0272741 -7.4442868 -7.7723923 -7.4555759 -7.2457614 -7.7063217 -8.0131645 -6.1339779 -5.926177 -6.3336434 -4.5249476][-7.0520778 -5.6068621 -5.1634164 -4.8541265 -4.8795404 -5.3583217 -5.1974554 -4.5462561 -4.507585 -4.7452273 -5.048563 -4.1958618 -5.1721864 -5.7971268 -5.0379357]]...]
INFO - root - 2017-12-15 15:58:36.158698: step 30810, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 56h:58m:14s remains)
INFO - root - 2017-12-15 15:58:42.765169: step 30820, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.655 sec/batch; 54h:54m:16s remains)
INFO - root - 2017-12-15 15:58:49.338775: step 30830, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 53h:49m:31s remains)
INFO - root - 2017-12-15 15:58:55.906949: step 30840, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 53h:30m:49s remains)
INFO - root - 2017-12-15 15:59:02.494373: step 30850, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.634 sec/batch; 53h:08m:34s remains)
INFO - root - 2017-12-15 15:59:09.094291: step 30860, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 54h:49m:31s remains)
INFO - root - 2017-12-15 15:59:15.708880: step 30870, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 55h:06m:45s remains)
INFO - root - 2017-12-15 15:59:22.246378: step 30880, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 53h:32m:53s remains)
INFO - root - 2017-12-15 15:59:28.822821: step 30890, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 55h:59m:03s remains)
INFO - root - 2017-12-15 15:59:35.502346: step 30900, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 56h:20m:05s remains)
2017-12-15 15:59:36.084561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4859381 -6.6210651 -6.2070508 -6.5630856 -8.0650425 -9.2126818 -9.9451008 -9.3580437 -8.8083735 -8.8928432 -8.0106058 -7.3512692 -8.1979923 -7.0275598 -5.4085269][-6.8196564 -6.4230452 -5.8014 -5.8629608 -6.3782215 -7.3447061 -8.12215 -8.4680195 -8.2911673 -6.957448 -5.81815 -5.7187548 -6.455883 -6.0800943 -5.4144683][-3.6145723 -4.3963346 -6.2390704 -6.008481 -5.6121407 -6.2944078 -6.7766323 -7.1324291 -7.0726404 -6.0128441 -4.7898788 -4.7265248 -6.0258303 -6.0973916 -6.2130742][-2.2522066 -2.8161247 -4.0024772 -4.9570317 -5.8718195 -5.3579388 -4.6831665 -5.2846708 -5.8869519 -5.3901806 -4.27674 -3.8452821 -5.5054626 -7.0546017 -7.6622853][-2.1836383 -3.0661879 -3.2999992 -3.0534554 -2.7522371 -1.4275827 -0.48182392 -2.3707454 -4.8111911 -4.1547141 -3.5072908 -4.1139331 -6.236269 -7.0961256 -7.7538271][-4.1457806 -2.7812035 -1.967078 -1.8886068 0.03915596 2.997375 5.1004624 3.4128566 0.46579027 -2.0401897 -4.4601688 -3.7483306 -4.8505187 -6.4032893 -7.2774372][-5.8973727 -4.23454 -1.8209074 -1.0642543 0.19734192 3.0823789 6.3497872 6.5966754 4.9447284 0.53275394 -3.660042 -4.2005672 -6.0482221 -6.621089 -7.8995619][-6.8939414 -5.4095788 -3.9520917 -1.672327 0.72999954 2.7798991 3.8857903 4.0558019 3.789763 1.3845668 -1.4131603 -3.741137 -7.1037836 -7.5596991 -7.2421341][-6.5152531 -6.2095423 -5.3428869 -2.8975627 -0.17342234 1.950511 3.3300347 2.9292798 0.72968292 -1.4051638 -3.2425597 -5.393023 -8.3171835 -9.2660608 -8.8966274][-6.7216034 -7.4813042 -6.8052287 -4.2330828 -2.6005418 -1.6081972 0.23648405 1.4055119 -0.4541769 -3.4190731 -5.5077958 -6.7618079 -9.2860556 -10.323843 -10.090573][-11.846795 -12.129907 -11.3451 -8.8106241 -7.9804344 -7.6326332 -6.4706192 -6.6452045 -7.0808716 -7.1594291 -8.2512178 -10.469208 -12.361965 -11.644487 -10.13728][-14.263266 -14.151678 -13.19692 -10.799774 -10.814701 -10.903133 -11.549996 -13.267925 -13.130623 -12.665461 -12.505436 -13.455851 -14.415258 -13.80995 -12.681994][-14.333818 -13.105332 -11.264312 -10.377394 -10.141024 -10.235847 -11.402876 -12.473872 -13.160109 -13.607743 -13.764317 -14.665503 -14.873344 -12.089921 -9.6780224][-10.680447 -9.90069 -8.9456463 -7.9987421 -8.0702972 -10.079762 -10.664238 -10.082193 -9.8722658 -9.8565884 -10.706549 -11.26857 -10.928065 -9.5074129 -8.3350458][-9.1490488 -8.8541183 -8.4648628 -5.717566 -3.6720805 -6.6433744 -7.9957047 -8.5719357 -8.514081 -7.2419829 -7.0671444 -7.3744531 -8.2682228 -7.84128 -6.7421122]]...]
INFO - root - 2017-12-15 15:59:42.730052: step 30910, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 55h:30m:25s remains)
INFO - root - 2017-12-15 15:59:49.224485: step 30920, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.653 sec/batch; 54h:43m:50s remains)
INFO - root - 2017-12-15 15:59:55.748499: step 30930, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 53h:51m:12s remains)
INFO - root - 2017-12-15 16:00:02.323250: step 30940, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 54h:52m:00s remains)
INFO - root - 2017-12-15 16:00:08.819129: step 30950, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 56h:01m:10s remains)
INFO - root - 2017-12-15 16:00:15.400054: step 30960, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 54h:09m:27s remains)
INFO - root - 2017-12-15 16:00:21.952837: step 30970, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 53h:31m:03s remains)
INFO - root - 2017-12-15 16:00:28.592019: step 30980, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 54h:49m:48s remains)
INFO - root - 2017-12-15 16:00:35.148365: step 30990, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 56h:16m:56s remains)
INFO - root - 2017-12-15 16:00:41.788994: step 31000, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 56h:01m:22s remains)
2017-12-15 16:00:42.348186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9224682 -3.3541825 -2.3172498 -1.922884 -2.3301251 -2.6990685 -3.012922 -3.7511237 -3.8733566 -4.2552819 -4.320035 -4.5150127 -5.453476 -6.1271753 -6.1067061][-3.2697942 -3.9865384 -3.6344628 -3.197922 -2.3431 -2.8104126 -3.3122351 -3.3913612 -2.8168983 -2.0495172 -1.8620026 -2.6815128 -4.5018926 -6.697567 -7.6027784][-1.6403337 -3.0430059 -3.7159922 -3.9793191 -3.8590212 -3.4490507 -3.6512444 -3.9717431 -2.9549954 -2.1940138 -2.1267259 -3.5572498 -5.7732067 -6.4415259 -6.1588674][-2.6293201 -3.6411605 -4.0974369 -3.9068213 -3.5477147 -3.2494197 -3.3691008 -3.5638845 -2.7041535 -1.8114851 -1.3898969 -3.0667331 -5.6040998 -6.527318 -6.2024651][-4.8338909 -5.8789 -5.2602458 -3.3444939 -1.5396872 -0.73047018 -1.2279344 -2.2042127 -2.4961953 -1.1433592 -0.59541464 -2.8219249 -5.6317925 -7.7887421 -7.647078][-5.4572797 -5.2664413 -3.9935894 -2.1935947 -0.62159538 1.0150828 1.1974301 0.38400269 -0.34068203 -0.57840872 -0.70148993 -2.0782576 -5.1466765 -7.3010387 -7.6353321][-5.3927593 -4.7801027 -3.4369271 -1.4710207 0.15118313 2.0086861 2.4128175 2.5160651 2.4066892 1.7366524 0.89187622 -0.87618637 -4.1599288 -6.261168 -6.9919806][-4.7169228 -4.1934586 -2.8171415 -0.49815226 1.4098616 2.4029574 2.7987208 2.5605302 2.7911286 2.4676156 1.5427299 -0.76850653 -4.1817942 -5.8783789 -6.0772939][-3.5672739 -2.8726575 -1.7320251 -1.0137568 -0.588532 0.80756283 2.4330659 2.7508054 2.6684985 2.1802688 1.4446697 -0.81580734 -4.352953 -6.5013947 -6.4117794][-2.9022489 -2.6855326 -2.1082494 -1.7173176 -1.503685 -0.30754566 0.92645264 1.6724133 1.703022 0.18300104 -1.526032 -3.4634461 -6.7036643 -8.7533178 -8.5347443][-6.5908675 -6.4457483 -5.4948082 -4.0905371 -3.0736952 -2.1449287 -1.4157777 -1.2372451 -1.8774619 -3.9472375 -5.4003372 -7.1106114 -9.3684874 -10.910033 -10.991217][-10.896515 -10.000568 -8.0521927 -6.5046592 -5.5620837 -4.4262152 -4.667181 -5.564559 -6.2837806 -6.9542432 -7.7142353 -9.2050409 -10.573273 -11.030142 -10.277021][-11.461819 -11.150222 -9.3309708 -8.1420784 -7.4063449 -5.5099626 -4.2807579 -4.6177249 -5.6616397 -6.6178527 -7.7766118 -8.4795771 -9.3255272 -9.3321056 -8.3606739][-10.98859 -9.9799547 -8.2182379 -6.701601 -5.3980227 -5.4975123 -5.3613462 -4.2041306 -3.9956105 -4.6555414 -5.6342254 -5.2733493 -5.72251 -5.5310931 -5.0227814][-6.6767154 -6.474864 -5.6970863 -4.3308206 -3.3012371 -2.9607317 -2.9839563 -3.0864394 -2.8621018 -3.3578372 -4.4345922 -5.0108767 -5.2775059 -4.2132521 -3.452575]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 16:00:48.918715: step 31010, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 53h:55m:24s remains)
INFO - root - 2017-12-15 16:00:55.469556: step 31020, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 55h:36m:35s remains)
INFO - root - 2017-12-15 16:01:02.049840: step 31030, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 53h:55m:52s remains)
INFO - root - 2017-12-15 16:01:08.633052: step 31040, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 55h:54m:55s remains)
INFO - root - 2017-12-15 16:01:15.188194: step 31050, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 53h:52m:54s remains)
INFO - root - 2017-12-15 16:01:21.847353: step 31060, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 54h:04m:11s remains)
INFO - root - 2017-12-15 16:01:28.441662: step 31070, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 53h:51m:26s remains)
INFO - root - 2017-12-15 16:01:34.959093: step 31080, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 55h:09m:39s remains)
INFO - root - 2017-12-15 16:01:41.510053: step 31090, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.669 sec/batch; 56h:03m:02s remains)
INFO - root - 2017-12-15 16:01:48.066445: step 31100, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 54h:24m:54s remains)
2017-12-15 16:01:48.534728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7967329 -6.8816605 -4.9858332 -4.3379545 -4.6897879 -5.2574944 -5.7575178 -5.2189422 -4.3513274 -4.5731211 -5.0748568 -5.8877769 -6.99586 -5.8293142 -4.3610716][-4.4958558 -4.5839233 -3.5730612 -2.9900086 -3.1017132 -3.5006859 -4.7779546 -5.1092029 -5.2154245 -4.1842685 -3.7882442 -4.5850129 -7.0503564 -6.818119 -5.3291774][-3.3869617 -4.0107851 -3.6483657 -2.6196022 -2.1038156 -2.3437982 -3.1805408 -3.54701 -3.9178314 -3.2579496 -4.1031036 -4.9389868 -7.020833 -6.3846855 -5.608592][-4.3128309 -3.5791488 -2.3731399 -1.2987704 -0.75792551 -1.3173447 -2.3511457 -2.8941488 -3.23814 -3.5218577 -4.2158794 -5.1416812 -6.3823547 -5.6918864 -5.1674466][-5.9825535 -4.8842869 -3.9201698 -2.0094302 -0.7515583 -0.0569911 0.44433832 -1.5524583 -3.6135552 -3.7184734 -3.8265429 -4.9150295 -6.3680329 -6.153996 -4.955162][-8.642004 -6.7725849 -3.5856538 -0.57583666 1.4813657 2.4630866 3.6394114 2.5016541 1.5849414 -0.38456059 -1.4224725 -2.0580502 -3.3298378 -3.70834 -3.2395618][-10.660511 -8.0018559 -4.3850365 0.55218315 2.9569268 3.7891774 4.3855891 4.1650834 3.874248 2.0777063 0.31263018 -1.6005836 -3.5530384 -2.7741048 -2.1059437][-11.5156 -9.3636436 -5.8279977 -0.55134439 2.6851668 4.3877769 5.1776223 3.65212 2.6528258 0.92589188 -1.0585775 -2.5141132 -4.2156067 -3.9592175 -2.1779633][-10.651569 -8.4523048 -5.2849693 -1.301949 1.1426978 2.4571843 2.2097526 1.2437329 -0.095659256 -1.969383 -4.315166 -6.0410914 -7.7569051 -5.9143314 -3.7358475][-11.059645 -9.2972984 -7.2983551 -4.06023 -1.3459821 -0.22279739 -0.095518112 -1.6759877 -3.2216434 -4.6018205 -5.8674965 -7.44148 -9.4640064 -8.0694847 -6.065093][-15.667583 -14.184706 -10.868076 -7.94806 -6.60921 -5.7150908 -4.7766638 -5.1930261 -5.5637112 -6.46425 -8.0201435 -9.9056273 -10.938467 -9.9604282 -7.7775841][-17.810789 -16.367136 -13.723135 -10.350712 -9.0034485 -8.7815371 -8.6090212 -8.1732349 -7.8624725 -8.3907871 -8.8188152 -9.4031124 -9.2250862 -8.3075638 -6.3569422][-14.51689 -11.977386 -9.8900127 -9.3592424 -9.6722 -9.0529623 -8.8999939 -8.90759 -8.6422529 -8.78088 -9.4038258 -8.7186375 -7.0354567 -6.2501864 -3.7262754][-9.9481468 -9.1948929 -7.2574625 -6.2875595 -6.9471149 -7.5507932 -8.0905972 -7.1466985 -7.6835036 -7.2320418 -7.8027368 -7.0836458 -6.8084741 -4.572298 -3.1920073][-6.988378 -6.8891487 -6.915 -5.2384481 -4.5348878 -5.1996 -4.7033095 -4.66384 -4.7352815 -4.2930675 -5.1108561 -4.8091168 -6.278367 -5.8010864 -5.3279366]]...]
INFO - root - 2017-12-15 16:01:55.089300: step 31110, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.654 sec/batch; 54h:44m:56s remains)
INFO - root - 2017-12-15 16:02:01.739649: step 31120, loss = 0.29, batch loss = 0.24 (11.4 examples/sec; 0.702 sec/batch; 58h:46m:37s remains)
INFO - root - 2017-12-15 16:02:08.421979: step 31130, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.675 sec/batch; 56h:32m:21s remains)
INFO - root - 2017-12-15 16:02:15.024071: step 31140, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 54h:48m:27s remains)
INFO - root - 2017-12-15 16:02:21.562950: step 31150, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 56h:41m:34s remains)
INFO - root - 2017-12-15 16:02:28.170947: step 31160, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.653 sec/batch; 54h:40m:33s remains)
INFO - root - 2017-12-15 16:02:34.740871: step 31170, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 54h:07m:26s remains)
INFO - root - 2017-12-15 16:02:41.369271: step 31180, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 54h:31m:09s remains)
INFO - root - 2017-12-15 16:02:47.935988: step 31190, loss = 0.18, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 52h:50m:46s remains)
INFO - root - 2017-12-15 16:02:54.559751: step 31200, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 54h:53m:42s remains)
2017-12-15 16:02:55.073705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.898427 -8.5150442 -8.78367 -8.220252 -7.7175174 -7.17871 -6.7980413 -7.4728274 -7.1371202 -6.5819845 -5.143878 -7.6759496 -9.43936 -9.112504 -9.2067089][-8.539278 -9.76679 -10.335712 -10.271124 -9.8600693 -9.3192244 -9.1920671 -8.7937126 -8.7686977 -8.5191545 -7.700067 -9.7174988 -10.353262 -10.24609 -10.468596][-6.5939751 -8.177578 -9.4997416 -9.4165106 -9.2441807 -9.4142838 -9.189086 -9.1862926 -9.5396681 -9.494998 -8.9653883 -10.906002 -11.698811 -11.150204 -10.874655][-5.5413785 -6.2481518 -5.8738785 -6.8833981 -7.3521347 -6.7498994 -6.5866113 -7.2309532 -7.6197271 -7.7114096 -7.8669891 -10.698399 -12.522091 -12.056 -11.591849][-6.0519018 -6.2676516 -5.8411059 -4.9959173 -4.3357148 -3.2830429 -3.0460691 -3.8279018 -4.0145922 -4.5703521 -4.5310674 -7.4133067 -10.33498 -11.155918 -11.632052][-5.2002759 -5.87117 -4.6787124 -3.9581075 -3.4296169 -1.1481423 -0.093103409 0.20220041 0.21755695 -0.7667594 -2.0954971 -5.5114603 -8.5247784 -9.6615591 -9.7657318][-6.5525074 -6.129837 -5.0150161 -3.3281085 -2.0571635 -0.25253868 1.1436639 2.4573712 2.9235654 1.8240924 1.2946887 -2.9005818 -6.4440479 -7.9768867 -8.8470459][-6.2559867 -5.6007519 -3.9932432 -2.6151531 -0.73540497 1.6684585 2.7109466 3.1184011 3.6647077 3.8051791 2.4261537 -1.4514785 -4.7872529 -6.5439086 -7.4152837][-6.1754251 -5.6063137 -3.8386798 -2.3911905 -0.63421059 1.7511373 2.889338 4.0925679 4.8704896 3.5130858 2.0988755 -0.5551362 -3.5216279 -6.1346111 -6.8963165][-6.9709067 -6.7701139 -5.0524421 -2.6227694 -0.82891941 0.82190466 1.5398407 1.7819648 2.4495149 2.9455333 2.3609538 -1.4093351 -3.9965458 -4.5169816 -5.12195][-10.765245 -10.474776 -8.3524151 -5.2633028 -3.5295942 -1.81197 -0.8480711 -1.2076755 -1.5917053 -1.589901 -1.5937433 -4.4556746 -6.7386513 -6.7232313 -6.0164104][-12.959078 -11.826962 -9.9474983 -7.5535035 -5.1782579 -4.4157081 -4.2370028 -4.4786544 -4.6984215 -5.2282515 -5.7508292 -7.3180656 -7.9677014 -7.1936626 -6.3231316][-13.494209 -11.902052 -9.8288269 -8.4017029 -7.3735776 -5.9920321 -5.0905137 -4.9356475 -5.5404067 -5.691813 -6.4828329 -8.0166445 -8.9235 -7.5578246 -5.9704442][-12.421062 -10.488431 -8.7066116 -7.0094042 -6.3825526 -5.6558185 -5.3557906 -5.3629742 -5.0572515 -5.4064531 -6.89287 -7.3267617 -7.1729021 -6.5339956 -6.3991261][-8.6581478 -8.8201742 -7.7197027 -6.8791046 -6.4203277 -5.3331928 -4.4834986 -4.0410318 -3.9619832 -3.734786 -3.7619352 -6.2124143 -8.0497036 -6.8557582 -6.689568]]...]
INFO - root - 2017-12-15 16:03:01.622898: step 31210, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 53h:35m:14s remains)
INFO - root - 2017-12-15 16:03:08.192432: step 31220, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 55h:52m:04s remains)
INFO - root - 2017-12-15 16:03:14.777679: step 31230, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 56h:06m:10s remains)
INFO - root - 2017-12-15 16:03:21.327011: step 31240, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 55h:24m:25s remains)
INFO - root - 2017-12-15 16:03:27.870352: step 31250, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 53h:13m:51s remains)
INFO - root - 2017-12-15 16:03:34.439691: step 31260, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 53h:28m:06s remains)
INFO - root - 2017-12-15 16:03:40.996794: step 31270, loss = 0.13, batch loss = 0.08 (11.4 examples/sec; 0.699 sec/batch; 58h:28m:24s remains)
INFO - root - 2017-12-15 16:03:47.488611: step 31280, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 54h:40m:21s remains)
INFO - root - 2017-12-15 16:03:54.124135: step 31290, loss = 0.43, batch loss = 0.39 (11.6 examples/sec; 0.693 sec/batch; 57h:56m:42s remains)
INFO - root - 2017-12-15 16:04:00.773696: step 31300, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.671 sec/batch; 56h:08m:42s remains)
2017-12-15 16:04:01.260222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.426294 -9.765049 -8.50177 -6.1767149 -4.0779409 -1.9911642 -1.1171646 -2.5490086 -3.4704189 -4.607265 -4.9726872 -6.5440974 -7.7696714 -8.682188 -7.5520992][-9.458252 -9.2246666 -8.6853218 -5.7365642 -3.6753507 -2.7878871 -2.5784729 -2.3782442 -2.687175 -3.2355897 -3.8244593 -6.89619 -8.8832235 -9.5684385 -9.121974][-7.4537334 -7.551158 -6.5162153 -4.8504529 -3.8337193 -2.8270004 -3.2598968 -3.4127166 -4.23212 -5.2278318 -5.0464525 -7.0152106 -8.7095928 -8.7921352 -7.762599][-7.6966248 -7.7467604 -6.0166268 -5.5612545 -4.8689895 -2.7442381 -2.0194547 -2.7623329 -3.639684 -4.2881107 -4.4000292 -6.5006142 -7.9445276 -9.9128637 -9.1250706][-6.5820069 -6.7403555 -6.3589511 -4.5103283 -4.13883 -2.8484447 -2.0730808 -1.4375644 -3.5646026 -4.3621969 -5.304903 -7.1432662 -7.8039217 -9.5600338 -9.49534][-8.0426006 -6.4581656 -5.27967 -4.8143616 -3.6030512 -2.1519253 -1.8521707 -1.9477119 -1.7752743 -3.0182674 -4.0402312 -6.7448225 -8.4889011 -9.59469 -9.195858][-5.6568146 -5.0268984 -4.1342125 -2.9580667 -2.6633184 -1.9222693 -0.18167257 -0.39438915 -2.0758986 -2.3701878 -2.023886 -5.2359295 -7.2671595 -9.4105768 -9.6264315][-4.5105848 -3.5246949 -2.4685369 -2.2398124 -2.2757692 -2.4060814 -3.1002009 -1.2604136 0.36432266 0.033471107 -0.68148804 -3.7806463 -6.1558661 -8.7256317 -9.1666565][-3.2523205 -2.9216316 -2.0118215 -1.0811319 -2.3319876 -1.8404868 -1.1794424 -0.66276836 -1.957648 -0.28023291 0.55090284 -2.5478671 -4.9995832 -7.4072404 -7.8684864][-2.9965353 -1.8294914 -0.62816525 0.13256121 -0.60534954 -1.5840368 -2.2746441 -1.1268945 -1.0535779 -1.1922536 0.432055 -2.3175364 -5.6675844 -7.918581 -7.4516139][-6.2191973 -4.8896217 -3.226305 -4.148716 -4.6452522 -4.4200497 -3.7903197 -3.2058051 -3.1889842 -3.1820426 -3.3701966 -5.7959771 -7.4617662 -7.5682144 -6.8046522][-10.986582 -9.3514051 -7.8104353 -6.5270615 -6.4943104 -6.5303354 -7.4974818 -7.5621085 -7.3515558 -6.9107141 -6.0759134 -7.4773126 -9.2738867 -9.2318106 -6.1929779][-9.8615417 -8.1476583 -7.1083279 -6.8071394 -7.4838672 -5.8977203 -5.61109 -6.6259046 -7.326901 -7.8492212 -8.1194477 -8.8966141 -9.8954468 -9.1347923 -6.7845192][-7.5248289 -6.9406672 -6.1567965 -4.2269311 -3.1952758 -3.5798004 -4.5964308 -4.7319083 -5.3885627 -8.1054192 -8.7565193 -8.20476 -7.8362856 -8.0559893 -7.3199215][-5.3020244 -4.4815474 -4.0087514 -5.0652285 -5.8634453 -5.1642022 -5.11796 -6.51673 -7.6160574 -6.836297 -5.7155781 -8.3196878 -10.207804 -9.8478241 -8.2298164]]...]
INFO - root - 2017-12-15 16:04:07.866933: step 31310, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 54h:42m:46s remains)
INFO - root - 2017-12-15 16:04:14.524972: step 31320, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 57h:19m:51s remains)
INFO - root - 2017-12-15 16:04:21.144008: step 31330, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.672 sec/batch; 56h:13m:54s remains)
INFO - root - 2017-12-15 16:04:27.717893: step 31340, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 53h:24m:11s remains)
INFO - root - 2017-12-15 16:04:34.279538: step 31350, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 56h:09m:20s remains)
INFO - root - 2017-12-15 16:04:40.882185: step 31360, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 54h:22m:37s remains)
INFO - root - 2017-12-15 16:04:47.371208: step 31370, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 53h:41m:52s remains)
INFO - root - 2017-12-15 16:04:53.987041: step 31380, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 56h:46m:09s remains)
INFO - root - 2017-12-15 16:05:00.501403: step 31390, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 55h:11m:53s remains)
INFO - root - 2017-12-15 16:05:07.119339: step 31400, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 54h:43m:20s remains)
2017-12-15 16:05:07.636609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1154718 -3.3537805 -2.5566559 -2.6319051 -3.9099593 -5.0705614 -5.8031564 -6.1761351 -5.8634272 -5.9032731 -5.6579051 -3.8145771 -4.6584849 -5.5910478 -5.7026353][-4.4479332 -4.6209235 -3.5401385 -4.0118465 -4.743762 -6.4308753 -7.7649231 -7.9472933 -8.1773739 -7.8301139 -7.0330105 -5.0873089 -6.3347173 -7.0439596 -6.7368307][-3.1035681 -3.4596088 -4.4111805 -5.1655607 -6.3428679 -7.8001451 -8.8556395 -9.1123085 -8.1298018 -7.7790122 -7.6850996 -5.7739449 -6.6164732 -7.4499512 -7.5037193][-4.5116229 -4.0378804 -4.4533415 -5.7122388 -7.0591674 -8.0425711 -8.0686913 -8.3648186 -8.3509655 -7.8471928 -7.5450263 -6.2387023 -7.998167 -8.6201763 -8.1073284][-4.8483849 -6.2886868 -6.75239 -6.4930868 -7.0031815 -5.6489773 -4.4282904 -5.8018141 -6.2824788 -6.1452885 -6.7468004 -5.5004225 -7.3495412 -9.3320637 -9.3570986][-6.7948189 -6.3178949 -6.4805212 -6.950634 -5.9938636 -2.8447151 1.1302004 1.2217622 -1.1339884 -3.4907811 -5.6453671 -4.1374207 -6.3870144 -8.5028591 -9.5828714][-7.4709325 -7.0631561 -6.58753 -6.8695564 -4.7047367 -0.069060326 4.3283229 5.7903066 5.8700604 1.0064025 -4.6880283 -4.0488796 -5.6658916 -7.1342173 -8.0428381][-8.4733067 -6.8895283 -5.4523253 -5.225297 -2.6821146 0.8368926 4.7713103 7.4174275 7.7643323 4.2288747 -0.6683774 -2.0087881 -5.6179714 -7.2422166 -7.51477][-7.4794912 -6.52744 -5.2520914 -5.3734288 -3.5591218 0.37441158 4.2164731 6.9031692 6.6139312 3.3281322 0.32082844 -1.1019254 -5.2995262 -7.6331196 -8.0530262][-7.0242138 -6.2222872 -5.7427673 -5.3629861 -3.9079337 -1.7394431 1.4065132 4.0450168 3.9338899 1.6235247 -1.6258187 -2.0671589 -4.513073 -7.6582179 -9.9073992][-9.61048 -8.4119253 -8.1280785 -7.8571548 -6.969563 -6.1359167 -4.5046663 -2.336283 -1.1770163 -2.121469 -3.9736042 -5.1332555 -8.8414993 -9.3395 -8.7185745][-12.319065 -10.981927 -9.8182116 -9.0437737 -8.3742523 -8.2451477 -8.4281855 -7.3203177 -6.122942 -6.1819749 -7.0454354 -6.8970804 -8.1704617 -9.99314 -9.2860489][-11.637156 -10.526853 -9.5645237 -9.4991493 -9.4166794 -8.4909468 -8.3352184 -8.1150665 -8.0325918 -7.2369895 -6.7003288 -6.5737128 -8.613718 -8.7581587 -6.8909869][-7.5795031 -7.3126574 -7.4672775 -7.8447046 -7.7845383 -8.0711422 -8.3555231 -7.3359489 -6.8028555 -6.6146092 -6.7850995 -5.9601569 -5.6020489 -5.73598 -5.5272794][-3.4001038 -3.5626824 -3.4380555 -3.769505 -3.8535008 -4.7378397 -5.1495304 -5.2391152 -4.5235147 -3.8090072 -4.2503748 -4.5875049 -5.9139571 -5.7267852 -5.8600388]]...]
INFO - root - 2017-12-15 16:05:14.208463: step 31410, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 54h:37m:51s remains)
INFO - root - 2017-12-15 16:05:20.774143: step 31420, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.687 sec/batch; 57h:27m:55s remains)
INFO - root - 2017-12-15 16:05:27.372488: step 31430, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 55h:44m:08s remains)
INFO - root - 2017-12-15 16:05:33.904530: step 31440, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 53h:50m:06s remains)
INFO - root - 2017-12-15 16:05:40.488128: step 31450, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 56h:02m:14s remains)
INFO - root - 2017-12-15 16:05:47.059693: step 31460, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 55h:46m:07s remains)
INFO - root - 2017-12-15 16:05:53.599401: step 31470, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.661 sec/batch; 55h:16m:41s remains)
INFO - root - 2017-12-15 16:06:00.200068: step 31480, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 55h:09m:19s remains)
INFO - root - 2017-12-15 16:06:06.787834: step 31490, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 53h:59m:36s remains)
INFO - root - 2017-12-15 16:06:13.348047: step 31500, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 53h:18m:22s remains)
2017-12-15 16:06:13.847604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8126059 -5.2433796 -5.8834734 -6.5066161 -7.7291107 -8.1795845 -8.7157536 -8.8342762 -9.2646952 -9.9850121 -9.8509111 -9.55167 -10.821856 -10.235088 -6.2065306][-7.41698 -6.2613478 -6.4121261 -7.2542524 -8.0962772 -9.33078 -9.8900156 -9.6791534 -9.6989384 -9.6861553 -9.3915958 -9.7443008 -10.807974 -10.630257 -7.115057][-6.15447 -6.7759852 -7.8575048 -7.5122004 -8.0150433 -8.7200193 -9.2261086 -9.0906239 -9.0093107 -8.8896446 -8.98152 -8.8073463 -9.8777208 -10.495018 -7.8898296][-6.4982777 -6.6523037 -7.1638212 -7.2880082 -8.1289873 -7.9082394 -7.6775527 -8.3189621 -8.2884655 -8.0885477 -8.2030678 -7.9496074 -8.6409159 -8.6682405 -6.8468418][-7.0262842 -7.9864588 -8.4526424 -7.612782 -7.0404205 -5.7591004 -4.7996225 -5.6758075 -6.4080157 -6.6953654 -7.1328249 -7.1264191 -8.619482 -8.8330708 -6.0470085][-6.6778555 -6.3713651 -5.7422571 -5.0572243 -3.9997964 -0.55614376 2.1702557 1.5897717 0.5272789 -2.1488841 -4.4768915 -4.7352943 -6.3491445 -7.4385433 -6.11474][-7.9822912 -6.9503112 -6.2763228 -4.1587586 -1.9550877 1.3093214 5.5551496 7.6730819 7.2781758 2.13657 -2.0522962 -2.9984584 -5.505558 -7.21671 -6.1555696][-7.8727074 -7.7501297 -6.747159 -3.3234663 -0.21183872 3.4868789 6.3199315 8.6797619 9.5122185 3.8297791 -2.0921273 -4.1901298 -6.1630278 -7.0610375 -6.3664379][-4.8840079 -5.4223709 -5.8109188 -3.3889928 -1.599278 2.2179527 5.5139079 7.2219043 7.0691466 3.3646579 -0.90855932 -4.5742149 -8.3909435 -9.3237629 -7.5556421][-2.6278515 -2.3031814 -3.1883609 -2.8753507 -2.2274165 -0.18969631 1.5256586 3.1285071 3.0172944 -0.57849121 -3.6496131 -6.3457017 -9.6917667 -11.752375 -10.783123][-4.9901876 -3.9225223 -3.8526707 -3.7690108 -4.1740422 -4.0691595 -4.2156987 -3.9581594 -4.6641636 -6.6591687 -8.7155085 -10.497889 -12.921246 -13.867048 -11.693262][-9.5083008 -8.7779636 -8.6355276 -7.75758 -7.5849452 -8.676672 -9.7482529 -10.054129 -10.444936 -11.604407 -12.843193 -13.146128 -13.505617 -13.113461 -10.384945][-11.319945 -10.259442 -9.1071243 -8.1285954 -9.6015415 -9.9810181 -9.7994747 -10.829388 -11.706192 -12.166416 -11.978086 -12.091699 -11.965075 -10.827269 -7.4359379][-11.27713 -10.339676 -9.5157013 -8.8508987 -9.06253 -8.7755795 -9.4335308 -9.7198315 -9.2844553 -9.8248272 -10.056726 -9.1035137 -8.4931469 -8.6828613 -6.33412][-9.5587111 -9.0235605 -7.5393896 -6.0938816 -5.4185753 -5.5764928 -6.0982933 -5.9198275 -5.9488568 -6.5209479 -6.1318707 -6.616312 -6.9031525 -6.535111 -5.2835]]...]
INFO - root - 2017-12-15 16:06:20.407606: step 31510, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 53h:31m:44s remains)
INFO - root - 2017-12-15 16:06:26.953350: step 31520, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 53h:42m:50s remains)
INFO - root - 2017-12-15 16:06:33.510859: step 31530, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 53h:39m:01s remains)
INFO - root - 2017-12-15 16:06:40.152738: step 31540, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 56h:21m:13s remains)
INFO - root - 2017-12-15 16:06:46.708747: step 31550, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 53h:38m:14s remains)
INFO - root - 2017-12-15 16:06:53.335414: step 31560, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 55h:15m:43s remains)
INFO - root - 2017-12-15 16:06:59.976338: step 31570, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.688 sec/batch; 57h:31m:34s remains)
INFO - root - 2017-12-15 16:07:06.489565: step 31580, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 52h:59m:37s remains)
INFO - root - 2017-12-15 16:07:13.057593: step 31590, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 55h:27m:02s remains)
INFO - root - 2017-12-15 16:07:19.637642: step 31600, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 55h:49m:29s remains)
2017-12-15 16:07:20.131916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4428906 -6.982831 -8.1999083 -8.827755 -10.097611 -11.402557 -12.503069 -12.779698 -13.030663 -12.489428 -11.511019 -12.24682 -11.742285 -12.681143 -9.9642057][-10.079919 -9.0357246 -9.4873028 -9.4704552 -10.293753 -11.410376 -12.708527 -13.916914 -13.978739 -13.58575 -13.315672 -12.717083 -11.347433 -13.168496 -9.9832516][-8.5529137 -9.867094 -11.315704 -10.586481 -10.941633 -11.388382 -11.824658 -12.405365 -13.252517 -14.006491 -13.677584 -13.802393 -12.691088 -13.862692 -10.662199][-9.2965145 -9.665844 -11.672952 -11.712898 -11.724355 -9.7047329 -9.2944431 -10.967989 -11.369655 -11.470644 -12.440659 -13.231015 -11.985265 -13.30942 -10.320122][-10.758284 -11.580959 -13.027237 -12.718925 -11.292458 -7.2086854 -4.9561725 -7.3223381 -10.388007 -10.697984 -11.046041 -12.376599 -12.260975 -14.158503 -10.154623][-12.730892 -12.000946 -12.990476 -11.892651 -9.7585239 -3.5038254 1.9430614 1.1173959 -2.5586419 -7.1609926 -10.500317 -10.075871 -9.6465788 -12.801037 -11.334368][-15.319891 -13.259697 -11.724186 -9.0474615 -6.7228866 -1.1753139 5.4708104 7.9628081 6.6289039 -0.95632505 -8.4180775 -9.7703905 -9.7213516 -11.613159 -9.73131][-15.439991 -15.170214 -14.264774 -7.2750349 -1.7078452 2.9006658 7.0081429 8.7030468 9.2800541 3.0517716 -5.1406851 -9.4782639 -10.620482 -11.997166 -9.7868185][-12.444015 -11.770751 -13.101009 -9.4139881 -4.5996332 3.1848884 7.9103703 6.4960227 5.6422048 1.1316128 -5.3917813 -10.720744 -12.821775 -14.574661 -12.253363][-9.1008129 -8.2858543 -10.412569 -9.3623276 -8.8107395 -2.6038132 3.3384194 3.4615598 2.1396451 -3.788703 -9.1394892 -12.195585 -14.243389 -16.575144 -14.623388][-13.203074 -11.534761 -11.646467 -10.871174 -11.659369 -9.6562805 -7.1227818 -5.5960941 -4.9162736 -8.891921 -13.355305 -16.118832 -15.230335 -16.89867 -14.933247][-16.338253 -15.25971 -15.342245 -13.450479 -13.006804 -12.720176 -13.382915 -13.048038 -11.79409 -13.762589 -15.028635 -17.091112 -16.137888 -16.228552 -12.915499][-15.782064 -15.495903 -15.941467 -15.099163 -14.784418 -12.888235 -13.003506 -14.236641 -14.285267 -13.749626 -13.68762 -15.290693 -14.017054 -13.803307 -10.404021][-11.561089 -11.586487 -11.712669 -12.053232 -12.220192 -11.152597 -10.595261 -10.103077 -10.472994 -10.855712 -10.855742 -10.703149 -10.33062 -10.634739 -8.3769712][-8.2401114 -7.5842438 -6.5429025 -6.3015928 -6.1355591 -6.6672173 -7.1395516 -6.0570683 -5.736095 -6.0939341 -6.7202411 -7.8845434 -7.7698641 -8.3741226 -7.7945843]]...]
INFO - root - 2017-12-15 16:07:26.701788: step 31610, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.678 sec/batch; 56h:39m:15s remains)
INFO - root - 2017-12-15 16:07:33.299129: step 31620, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 55h:47m:51s remains)
INFO - root - 2017-12-15 16:07:39.901488: step 31630, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 56h:28m:53s remains)
INFO - root - 2017-12-15 16:07:46.430007: step 31640, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.675 sec/batch; 56h:26m:11s remains)
INFO - root - 2017-12-15 16:07:53.020453: step 31650, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 54h:38m:16s remains)
INFO - root - 2017-12-15 16:07:59.518785: step 31660, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 53h:23m:05s remains)
INFO - root - 2017-12-15 16:08:06.041991: step 31670, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 55h:40m:37s remains)
INFO - root - 2017-12-15 16:08:12.607467: step 31680, loss = 0.21, batch loss = 0.17 (12.5 examples/sec; 0.641 sec/batch; 53h:34m:49s remains)
INFO - root - 2017-12-15 16:08:19.257716: step 31690, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 56h:31m:36s remains)
INFO - root - 2017-12-15 16:08:25.831552: step 31700, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.629 sec/batch; 52h:34m:51s remains)
2017-12-15 16:08:26.436858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7844415 -5.1599627 -4.0817347 -2.7308087 -2.2186751 -2.1040685 -3.09822 -3.43321 -3.4238989 -3.0224986 -2.9952092 -5.5303726 -6.4435844 -10.17322 -10.776627][-6.5352654 -5.7911434 -3.9071994 -2.5120881 -2.017679 -2.2013586 -3.4437976 -3.4391835 -3.5067737 -3.0316527 -2.3770323 -4.3261142 -5.0319414 -8.7500648 -9.839798][-5.2720075 -5.1682673 -4.15133 -2.4113488 -2.1298945 -2.4843855 -3.1808329 -4.075716 -3.5554266 -2.6779251 -2.3965719 -4.146431 -4.6582937 -7.3523879 -7.2701507][-3.8519969 -3.5992169 -2.7930048 -1.4049039 -1.4940586 -1.6818438 -2.4848347 -3.2969432 -2.0780098 -0.84071112 -0.5841074 -2.9768879 -4.43767 -7.2117453 -6.9349732][-3.4944205 -3.0635331 -1.5414186 0.00091171265 -0.074518204 -0.048135757 -0.85182858 -1.384244 -0.52895832 0.49845076 0.57841063 -1.8977933 -2.9016495 -6.2477903 -7.25269][-4.7470517 -3.1922903 -0.74288034 1.1185327 1.850769 2.2630572 1.8765297 1.9772816 2.4723144 2.5318942 2.6062284 -0.25138044 -2.2217414 -5.8253565 -6.6162953][-4.57292 -3.7530403 -2.5014205 -0.1665473 0.75281191 2.2252336 3.6799054 3.8825173 4.1790423 3.1400323 1.1604753 -0.843575 -2.1241717 -5.6627336 -7.2401114][-5.7987304 -5.2127337 -3.8277636 -0.39067698 1.6589541 2.7218175 3.7515512 4.0958257 5.6012568 4.3868051 1.351933 -1.8945901 -3.5099988 -6.2630405 -6.7192516][-5.2276945 -4.8032322 -3.8842342 -1.6739187 0.23101711 2.8095059 3.6406074 2.6052127 2.679852 2.0434141 0.69234943 -2.1165471 -3.0774152 -5.5818834 -5.829021][-4.6738896 -3.7964363 -2.7071662 -1.3700604 -0.89104366 0.65245628 1.7872238 1.578136 1.2586603 0.19116831 -0.75901413 -2.1555462 -2.9274249 -6.2155476 -6.7279015][-5.9049087 -4.6106806 -3.8630147 -2.6944306 -2.104758 -1.8663733 -1.318573 -0.86189413 -0.54785919 -2.0714686 -3.8419819 -5.0106206 -5.1267729 -7.1060348 -6.8878212][-9.18129 -8.2434216 -7.1569171 -5.3097839 -4.2421989 -4.2531376 -5.1277294 -5.3292265 -4.9372663 -5.040844 -5.558444 -6.0758138 -5.5756569 -7.1127768 -7.0618725][-12.224203 -11.157272 -10.036041 -9.1126747 -7.7797394 -7.0232286 -7.33426 -7.5343943 -8.1462135 -7.9798441 -7.4012971 -7.5030904 -6.7133756 -7.1041203 -6.2785988][-8.8136673 -9.1473341 -8.9389629 -7.7692785 -6.4830527 -5.55927 -5.7751927 -6.2725773 -6.7303939 -6.8830404 -6.9117017 -6.4956255 -5.584228 -5.0129395 -5.1529126][-3.9768686 -3.7810323 -2.8577044 -1.9770677 -1.675456 -1.1870751 -1.5161734 -1.6514401 -1.9076028 -2.6604052 -3.4733059 -4.1966953 -5.1154656 -5.7113781 -6.2918029]]...]
INFO - root - 2017-12-15 16:08:32.971077: step 31710, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 55h:30m:35s remains)
INFO - root - 2017-12-15 16:08:39.611611: step 31720, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 54h:01m:16s remains)
INFO - root - 2017-12-15 16:08:46.294692: step 31730, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 54h:48m:59s remains)
INFO - root - 2017-12-15 16:08:52.915334: step 31740, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 53h:56m:15s remains)
INFO - root - 2017-12-15 16:08:59.578720: step 31750, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 54h:57m:32s remains)
INFO - root - 2017-12-15 16:09:06.156968: step 31760, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 54h:40m:17s remains)
INFO - root - 2017-12-15 16:09:12.678661: step 31770, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 55h:54m:41s remains)
INFO - root - 2017-12-15 16:09:19.207922: step 31780, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 53h:32m:18s remains)
INFO - root - 2017-12-15 16:09:25.841339: step 31790, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.697 sec/batch; 58h:14m:00s remains)
INFO - root - 2017-12-15 16:09:32.423532: step 31800, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 56h:17m:45s remains)
2017-12-15 16:09:32.925022: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8873386 -6.4575281 -5.5535979 -4.2582426 -4.1662326 -3.4156795 -2.5276816 -1.2659097 -0.504797 0.18197012 -1.0493307 -4.5463967 -8.6964025 -10.066928 -8.58492][-6.6002889 -8.01812 -8.4285145 -7.2551436 -5.608407 -4.3826742 -2.5181861 -0.85923386 -0.84912634 -1.4429398 -1.5322781 -4.8049264 -9.08034 -9.6981916 -9.7121248][-5.1852775 -6.4528508 -8.365695 -7.9681768 -7.8611097 -5.7790585 -2.8516848 -1.2947712 -1.114841 -2.1590607 -3.5969822 -7.2397184 -10.461967 -11.387113 -9.6994448][-4.0190964 -5.5520673 -6.635128 -6.2793527 -6.8129621 -5.85766 -4.5923219 -2.9005189 -1.8917778 -2.6818318 -4.2840428 -9.0575485 -13.025875 -12.016702 -9.7148457][-5.3370771 -5.2694163 -5.4491606 -4.6867561 -4.8580728 -3.1868262 -1.6158161 -2.3631077 -2.8552449 -3.2926011 -4.3747211 -8.3091278 -12.421705 -12.495638 -10.208229][-8.303175 -6.4443083 -6.0626864 -4.3199015 -3.7846041 -1.063118 0.82345247 0.41226006 -0.72990084 -3.1725662 -5.1169209 -8.4162054 -11.789337 -12.776114 -10.670307][-8.6942186 -7.5870428 -7.217854 -4.8142123 -3.6056619 -0.28313637 2.7125411 3.6971202 2.9331937 -0.82973051 -4.878408 -7.8087845 -10.158932 -10.671629 -10.194756][-7.1494236 -6.5814805 -6.5806842 -3.7178216 -1.7111764 1.2988148 4.117414 5.045486 4.7814918 1.8717942 -1.6255927 -7.0421658 -11.132064 -10.83758 -9.4838753][-6.4522243 -5.3345518 -4.8360386 -2.5199294 -1.7075815 1.5753593 3.9102387 5.2210584 5.026072 1.559176 -1.5239196 -6.6394143 -11.287347 -12.385139 -10.888449][-5.751483 -5.1297641 -4.4621358 -2.5884786 -1.7286494 -0.12326431 0.81705046 2.6512122 3.0407224 1.493176 -1.0091581 -6.6861372 -11.368446 -12.592688 -11.343081][-6.6735888 -7.5627046 -7.7468095 -6.5827613 -4.8317108 -3.5963764 -2.2763548 -1.1597815 -1.1537294 -1.4805837 -2.4624081 -7.6219339 -11.521588 -13.35207 -12.287899][-10.464397 -8.8188438 -8.0334692 -7.2908082 -7.331655 -6.6271291 -6.6179996 -7.0209 -7.0813222 -6.3881779 -6.74514 -8.9567652 -10.675484 -11.034046 -9.9697437][-13.555739 -11.624788 -8.6153069 -8.2885046 -8.5361938 -8.8130188 -9.3230448 -8.6831188 -8.2646923 -7.8152332 -7.7312069 -9.1318073 -11.413385 -10.737309 -8.5602369][-12.853767 -11.190007 -8.6366243 -7.7136436 -7.6086636 -6.8648715 -7.0644884 -7.6429114 -7.3223677 -8.0667009 -7.98833 -6.6398587 -6.879046 -8.5953722 -8.9341679][-9.0898647 -7.4868994 -6.6138525 -5.3297796 -5.4341893 -6.2774096 -6.225194 -5.301868 -5.1059365 -5.83399 -6.6020861 -8.811903 -9.4197025 -8.5161934 -9.1036205]]...]
INFO - root - 2017-12-15 16:09:39.587033: step 31810, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.690 sec/batch; 57h:38m:58s remains)
INFO - root - 2017-12-15 16:09:46.251221: step 31820, loss = 0.14, batch loss = 0.10 (11.4 examples/sec; 0.699 sec/batch; 58h:25m:11s remains)
INFO - root - 2017-12-15 16:09:52.973747: step 31830, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 56h:31m:45s remains)
INFO - root - 2017-12-15 16:09:59.547246: step 31840, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 56h:32m:55s remains)
INFO - root - 2017-12-15 16:10:06.189713: step 31850, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 56h:06m:37s remains)
INFO - root - 2017-12-15 16:10:12.853373: step 31860, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 54h:21m:55s remains)
INFO - root - 2017-12-15 16:10:19.377668: step 31870, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 54h:40m:51s remains)
INFO - root - 2017-12-15 16:10:25.898074: step 31880, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 54h:35m:55s remains)
INFO - root - 2017-12-15 16:10:32.478025: step 31890, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 54h:39m:20s remains)
INFO - root - 2017-12-15 16:10:39.126557: step 31900, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 54h:40m:05s remains)
2017-12-15 16:10:39.616579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.5415421 -8.2226677 -7.5629578 -6.2126684 -6.031641 -4.7657065 -3.2819598 -1.4239445 -0.834198 -0.061519146 0.010618687 -1.1441488 -3.7102661 -5.2061906 -5.9182639][-5.9366932 -7.1097784 -6.4812903 -5.855022 -6.0415244 -5.4960628 -4.6306996 -3.6112678 -3.30326 -2.57939 -2.1505089 -1.7243514 -3.7263041 -4.8490343 -4.9779124][-3.2175026 -5.1268706 -6.0049815 -5.3187046 -5.2514029 -5.8470817 -5.4269958 -4.610764 -4.3306322 -3.4763222 -3.1366622 -4.5091095 -6.9175205 -6.508986 -7.0670428][-3.3659074 -4.7111192 -5.3959241 -4.7311683 -5.4884639 -5.2753234 -4.5924106 -4.3462334 -3.7695565 -2.9562287 -2.5478172 -4.4251719 -7.6173716 -8.93135 -9.22037][-4.8512468 -6.18865 -6.3505945 -5.1487842 -5.2142358 -4.1292105 -3.3959391 -3.9019613 -3.3406365 -1.4663563 -1.1311626 -3.4544764 -6.2590365 -8.415637 -9.6784143][-6.9521341 -7.4694252 -6.349227 -4.7252216 -3.2205472 -0.94621563 0.13056564 -0.30543709 -0.35464954 -0.64234686 -1.0791039 -1.6863098 -4.8744745 -7.4541817 -9.08707][-6.8776803 -6.0181422 -4.3939342 -2.2319152 -0.3178072 1.1812677 1.9573226 2.7460189 3.2735267 1.6073508 0.63050127 0.17701006 -3.7001271 -6.3578806 -9.09713][-5.1782808 -4.1870742 -3.3882103 -0.787323 1.2674398 2.9048505 3.11416 3.834168 4.5796418 3.4823117 2.5975146 -0.020416737 -4.2421484 -6.4540005 -8.076704][-2.4682791 -2.8418739 -1.9807541 -0.079896927 1.0348268 1.7731805 1.8597722 2.44198 3.2277942 4.1889014 4.9627786 1.3999705 -3.3914485 -5.9260125 -6.7872105][-2.1880672 -3.0760782 -2.7039607 -1.0060143 -0.008913517 -0.29196215 -0.1518631 1.7228165 2.5989738 3.4639153 4.5067773 2.1409006 -2.7906823 -5.4889636 -6.8994961][-6.6435184 -5.6825237 -4.9738631 -3.7350311 -2.4922667 -2.8706524 -2.2496383 -1.3584595 -0.84678268 0.54988909 0.97493982 -0.21310902 -3.018748 -4.6116033 -5.5755692][-10.862463 -10.269773 -9.3749943 -7.084414 -5.4489484 -4.7698956 -4.1220922 -4.4810867 -3.9470487 -3.433567 -3.0385742 -4.2100563 -6.1034422 -6.0279059 -6.0343509][-12.033497 -11.366821 -10.060421 -8.2726 -7.1652284 -6.488914 -5.6031194 -5.7708592 -5.9179506 -5.998672 -5.8328214 -6.5188632 -7.2715359 -5.5807366 -4.5273075][-10.379675 -9.1574993 -7.4126892 -6.8582778 -6.6588187 -6.8878326 -6.6871467 -6.1084566 -5.8499265 -6.7868762 -7.7235603 -7.7454324 -8.1173525 -7.5878654 -6.4799914][-7.2870636 -7.1855135 -6.4010372 -5.4757204 -5.289433 -5.2266936 -5.431489 -5.4881411 -5.28836 -5.9335041 -7.2054644 -8.7124405 -10.255014 -10.435328 -10.116677]]...]
INFO - root - 2017-12-15 16:10:46.186909: step 31910, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 56h:13m:27s remains)
INFO - root - 2017-12-15 16:10:52.742601: step 31920, loss = 0.14, batch loss = 0.10 (12.8 examples/sec; 0.627 sec/batch; 52h:22m:57s remains)
INFO - root - 2017-12-15 16:10:59.313896: step 31930, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 55h:23m:21s remains)
INFO - root - 2017-12-15 16:11:05.910030: step 31940, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.643 sec/batch; 53h:38m:32s remains)
INFO - root - 2017-12-15 16:11:12.563385: step 31950, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 53h:50m:53s remains)
INFO - root - 2017-12-15 16:11:19.186356: step 31960, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.675 sec/batch; 56h:18m:58s remains)
INFO - root - 2017-12-15 16:11:25.697351: step 31970, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 53h:40m:49s remains)
INFO - root - 2017-12-15 16:11:32.243292: step 31980, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 55h:14m:11s remains)
INFO - root - 2017-12-15 16:11:38.841390: step 31990, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 56h:46m:35s remains)
INFO - root - 2017-12-15 16:11:45.433552: step 32000, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 53h:52m:25s remains)
2017-12-15 16:11:45.931871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1377153 -7.4392042 -7.0745583 -6.8192244 -7.4606385 -7.7579775 -7.8187351 -7.0434675 -6.2999144 -6.2155695 -5.918715 -6.2386227 -8.7224779 -7.8350096 -6.9688973][-6.6102328 -6.1124215 -5.5735106 -5.621254 -6.0419555 -6.4017849 -6.3293886 -6.2390509 -6.0233836 -5.1575155 -4.7564974 -5.90264 -8.1104164 -9.3839512 -9.5598412][-4.6089444 -5.6638041 -5.8286257 -4.800106 -5.1894321 -5.4156551 -5.5325527 -5.1606178 -4.6565347 -4.8068762 -5.2837706 -6.9349585 -9.7964458 -9.900528 -9.5592861][-6.2920885 -7.1700449 -6.5270576 -5.0800452 -4.5189381 -3.71392 -3.4941585 -3.7987461 -4.0348148 -4.3362112 -5.0570951 -6.5023832 -9.4631233 -10.38604 -9.7569208][-7.2401314 -9.4703226 -9.2158022 -7.1569152 -4.4977894 -1.4461012 0.025380135 -0.8108325 -2.1849079 -3.922749 -5.1959486 -6.4397659 -8.4942226 -8.5417309 -8.2800512][-8.8114624 -10.143125 -9.3281507 -6.3339829 -2.5373821 1.3061061 3.9967856 3.7704663 1.4224148 -1.0149465 -2.4455478 -2.9352741 -5.2138519 -5.8865247 -6.1308832][-9.1265421 -9.29177 -7.6634197 -4.4111185 -1.297617 2.7864509 6.8632941 8.2937393 6.5488486 1.7297974 -1.9649823 -3.437392 -5.7727633 -5.6957145 -5.2295718][-10.028359 -8.976429 -5.8766556 -1.263483 2.1300387 4.8003097 7.5300756 8.1961517 7.9229274 4.8987441 0.89334488 -2.5899477 -7.0346408 -8.3026628 -8.0368624][-7.0790005 -7.0058632 -5.7603521 -2.2045581 0.35900116 3.1122279 4.7305779 4.9955831 5.2330632 2.9553037 -0.25778198 -3.6735981 -8.0297642 -9.575429 -10.147544][-5.7395606 -5.3069491 -4.6674557 -3.1670349 -1.0599461 1.2137194 1.632319 1.0798359 0.46602297 -1.0038352 -2.3055654 -4.6492667 -9.0036125 -11.110126 -12.771849][-9.87248 -9.5303326 -9.5322952 -8.1904011 -6.5565062 -5.11948 -4.0230336 -3.836616 -4.6714249 -5.3070951 -5.9347181 -8.3544273 -10.78451 -11.081083 -11.849022][-12.843787 -10.652063 -10.288314 -10.405548 -10.497654 -9.2483883 -7.9263859 -7.7940521 -8.1270208 -7.9918661 -8.092782 -9.7036781 -10.758587 -11.325436 -11.518156][-14.474201 -12.771313 -11.814825 -11.650639 -11.201233 -10.580655 -9.9150658 -9.3137379 -9.0773611 -8.9307928 -9.1272335 -9.85779 -10.032932 -8.7112427 -7.58272][-12.780048 -11.390779 -9.5772057 -8.5124779 -7.6887293 -7.6372695 -7.5774479 -7.2522411 -7.008615 -7.1595545 -7.4189935 -7.2657046 -7.1079712 -7.4222078 -7.2605295][-8.9833927 -7.8074837 -6.5905223 -4.8387194 -4.13148 -3.5918608 -3.6250114 -4.0640192 -5.0737567 -5.2034674 -5.9298773 -6.6532555 -6.6063919 -5.7755356 -6.3308544]]...]
INFO - root - 2017-12-15 16:11:52.682946: step 32010, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 54h:20m:28s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 16:11:59.329621: step 32020, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 53h:57m:47s remains)
INFO - root - 2017-12-15 16:12:05.930292: step 32030, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 53h:57m:53s remains)
INFO - root - 2017-12-15 16:12:12.520383: step 32040, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 54h:47m:36s remains)
INFO - root - 2017-12-15 16:12:19.101341: step 32050, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 55h:50m:16s remains)
INFO - root - 2017-12-15 16:12:25.688336: step 32060, loss = 0.11, batch loss = 0.07 (11.9 examples/sec; 0.673 sec/batch; 56h:07m:25s remains)
INFO - root - 2017-12-15 16:12:32.301669: step 32070, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 56h:15m:37s remains)
INFO - root - 2017-12-15 16:12:38.930655: step 32080, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 56h:11m:46s remains)
INFO - root - 2017-12-15 16:12:45.503548: step 32090, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 53h:54m:06s remains)
INFO - root - 2017-12-15 16:12:52.128520: step 32100, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.687 sec/batch; 57h:17m:40s remains)
2017-12-15 16:12:52.626096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5017962 -5.8311005 -5.0982695 -4.1635551 -4.6867642 -4.62002 -4.3569555 -3.8349872 -3.2414525 -2.5704205 -1.9017775 -5.4108047 -7.316503 -8.108777 -8.7029171][-5.0179739 -4.7295675 -3.363574 -2.4875724 -3.2416248 -4.1079874 -3.8333468 -3.0994823 -3.1049662 -2.6971385 -2.0159979 -4.4276795 -5.9154263 -6.7859364 -8.0476036][-2.2989557 -3.1023095 -3.2565229 -3.1378531 -3.1685393 -3.2674334 -2.9945555 -3.0820608 -3.1446357 -2.5935607 -2.4740751 -5.5566173 -7.6390467 -8.6147175 -9.1890774][-2.5407619 -3.027437 -2.8109114 -2.4059432 -3.0066516 -3.5021038 -3.1572936 -2.4509959 -1.956424 -2.3358827 -2.52383 -5.6216574 -7.4327974 -8.6371689 -10.15448][-1.2493505 -2.9647727 -3.9015641 -3.8499064 -3.2798612 -2.3375022 -2.286226 -2.5539303 -1.9943948 -1.5847507 -1.0467854 -5.0670691 -7.80884 -8.5239449 -9.2919044][-3.1930389 -4.0852828 -3.4747221 -1.6650238 -0.85104465 -0.406116 0.063816071 0.87325907 0.8584671 -0.858799 -1.4495888 -4.955987 -7.185586 -8.7140827 -9.6302624][-3.7223792 -4.2093253 -3.8105602 -1.5346794 1.0491643 2.0462575 1.9084373 2.3910413 3.1922212 2.4164815 0.95216846 -4.495554 -7.1533432 -7.5262489 -8.1144638][-2.4800389 -2.4188037 -1.0853367 0.15716934 1.7410407 3.2114587 3.6349444 3.3421464 2.980845 2.5282445 1.8848243 -3.1006877 -6.3069491 -7.0810976 -6.7199979][-3.0046897 -3.8121991 -2.8207188 -0.24443865 1.2282524 1.5338688 2.2906241 2.727674 2.4907165 1.9747219 1.2473531 -3.4657254 -5.8500752 -5.9662437 -6.0884867][-2.5669684 -4.1159105 -4.4023085 -2.6009898 -1.4861321 -0.79953289 -0.8328433 -0.49752092 0.32577181 0.37453842 0.19605827 -4.5648308 -6.9384751 -6.6546283 -6.3820343][-4.4684114 -5.2430644 -5.9737864 -5.35601 -4.3548803 -3.408035 -4.5509763 -5.1691389 -5.4239779 -5.15555 -4.7603292 -7.7568569 -8.4722128 -7.490201 -6.6355114][-8.9093609 -7.2625933 -6.3376031 -6.328577 -6.4065361 -5.2248797 -5.7020106 -7.5266714 -9.3800812 -9.4212008 -8.6970387 -9.5608177 -10.300418 -9.726326 -8.1907425][-11.571791 -9.93475 -7.6212473 -6.3038239 -6.3882813 -5.9378042 -6.8658466 -8.2653885 -9.3142729 -9.6218739 -9.9017258 -9.6621246 -9.3392057 -8.3112392 -6.4304738][-10.200884 -8.8158836 -7.6485815 -7.4584379 -7.2694321 -6.4821472 -6.2397089 -6.5050044 -5.9672351 -6.7064843 -7.1727057 -6.5222354 -6.6655416 -6.3792524 -5.8880024][-6.8891859 -5.9794574 -5.6046286 -5.9747834 -6.3831825 -6.3694334 -6.4363751 -6.1471949 -5.0754128 -4.3692603 -3.6006796 -3.6356959 -5.320044 -6.7368221 -6.6397018]]...]
INFO - root - 2017-12-15 16:12:59.148833: step 32110, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 53h:48m:40s remains)
INFO - root - 2017-12-15 16:13:05.691866: step 32120, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 53h:15m:18s remains)
INFO - root - 2017-12-15 16:13:12.294077: step 32130, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 54h:48m:13s remains)
INFO - root - 2017-12-15 16:13:18.915240: step 32140, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 54h:01m:05s remains)
INFO - root - 2017-12-15 16:13:25.595850: step 32150, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 56h:49m:49s remains)
INFO - root - 2017-12-15 16:13:32.218995: step 32160, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 54h:10m:27s remains)
INFO - root - 2017-12-15 16:13:38.753293: step 32170, loss = 0.27, batch loss = 0.23 (12.3 examples/sec; 0.650 sec/batch; 54h:15m:16s remains)
INFO - root - 2017-12-15 16:13:45.280587: step 32180, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 54h:34m:28s remains)
INFO - root - 2017-12-15 16:13:51.781207: step 32190, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 53h:19m:50s remains)
INFO - root - 2017-12-15 16:13:58.432627: step 32200, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.648 sec/batch; 54h:00m:48s remains)
2017-12-15 16:13:58.941441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4766238 -2.3553436 -1.9445868 -1.4284921 -1.9992614 -2.677763 -3.2670734 -3.6043136 -3.68604 -2.8785281 -2.1095867 -3.5774641 -4.4461012 -5.2154922 -4.6824913][-5.093082 -2.9900808 -2.4223113 -2.1610792 -2.7060051 -3.4075437 -4.0041571 -4.6430559 -4.3520541 -3.9542756 -3.3092248 -4.5245366 -5.1116343 -4.8771558 -4.5083566][-2.7913842 -3.2106795 -3.7339964 -3.2107084 -3.8988032 -4.4395695 -4.6895084 -4.8897138 -4.9416552 -4.488163 -4.09879 -5.80166 -6.7650843 -6.9872684 -6.5400534][-5.2359853 -4.8001156 -4.3497095 -3.2059388 -2.9840388 -2.7417517 -2.949203 -4.0896053 -4.5914316 -4.4883814 -5.0057921 -6.8418875 -7.597929 -7.530489 -6.7489371][-5.8934436 -6.2919331 -5.2684855 -3.4151726 -2.8095069 -1.7032099 -1.169415 -1.3132753 -2.0652943 -2.0362542 -2.603399 -5.6038246 -7.2583842 -8.165741 -7.755702][-7.9749041 -7.7392569 -6.4460535 -4.6229467 -2.3324463 -0.063589096 1.4726081 1.8925924 1.2806859 -0.17256212 -1.6244936 -4.2993126 -6.2065616 -7.5980129 -7.2439818][-8.7988491 -7.3182921 -5.1921053 -2.3435359 0.13795233 2.8584323 4.4407516 5.140779 4.6903577 2.7997823 0.75166845 -3.0758293 -5.4748549 -7.1399059 -6.8289461][-7.1097722 -6.4137573 -4.7819915 -1.7582557 1.2585764 3.8700871 5.2956328 5.0312047 4.442884 3.3644137 1.6679487 -1.8955567 -4.512929 -5.9790998 -5.532958][-6.9124722 -6.4979982 -5.2330589 -1.4519019 1.125895 3.40313 5.1936803 5.5246892 5.0607429 3.8792377 2.4395967 -2.1061606 -4.7362833 -5.6006274 -4.7892213][-5.769691 -6.1451244 -4.7700129 -1.5796881 -0.49601221 1.259335 2.2871528 2.6583982 2.7629743 1.6100168 0.2238431 -3.2844026 -5.5288916 -5.730916 -4.6012912][-9.541728 -9.0700293 -7.8966942 -6.041923 -4.7036319 -2.8346295 -2.2896256 -1.8405831 -1.6138706 -2.1726286 -3.1625292 -6.4370928 -8.1987209 -7.7240639 -4.8474078][-15.14255 -12.5546 -9.5195656 -7.7024927 -7.033227 -6.4958043 -6.2565823 -6.0794072 -5.9534225 -6.5585246 -7.369741 -8.8419 -9.5614929 -7.8964329 -5.4568496][-13.420573 -12.37618 -10.041787 -8.1633959 -7.3322773 -7.0495305 -6.602294 -7.0500298 -7.773551 -7.5155077 -7.1719456 -8.3708229 -9.2652845 -7.3585377 -4.4160204][-10.200746 -8.863534 -6.8255415 -5.1615205 -4.4583936 -4.7883549 -5.6046662 -5.7159619 -5.2370896 -6.0676146 -6.8682852 -6.5569086 -6.385993 -5.0063448 -4.0522671][-5.50059 -5.159225 -3.9787765 -3.0553806 -2.7058122 -3.1945779 -3.3007512 -3.9613471 -4.3959618 -4.3094726 -4.0581231 -4.7225609 -5.7333174 -6.3011665 -5.8829165]]...]
INFO - root - 2017-12-15 16:14:05.539057: step 32210, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.678 sec/batch; 56h:34m:35s remains)
INFO - root - 2017-12-15 16:14:12.164935: step 32220, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 56h:06m:16s remains)
INFO - root - 2017-12-15 16:14:18.714695: step 32230, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 56h:00m:25s remains)
INFO - root - 2017-12-15 16:14:25.278461: step 32240, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 54h:50m:45s remains)
INFO - root - 2017-12-15 16:14:31.904244: step 32250, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 53h:55m:24s remains)
INFO - root - 2017-12-15 16:14:38.507354: step 32260, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 55h:12m:41s remains)
INFO - root - 2017-12-15 16:14:45.149496: step 32270, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 54h:41m:06s remains)
INFO - root - 2017-12-15 16:14:51.726520: step 32280, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 53h:22m:09s remains)
INFO - root - 2017-12-15 16:14:58.270976: step 32290, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.653 sec/batch; 54h:28m:44s remains)
INFO - root - 2017-12-15 16:15:04.850560: step 32300, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 54h:17m:08s remains)
2017-12-15 16:15:05.365737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6933689 -5.83157 -4.9867091 -3.9288266 -3.9337606 -4.1518087 -4.7285352 -4.7132425 -4.852109 -5.1630578 -5.6377363 -7.8486586 -9.6727123 -10.370005 -8.9413691][-5.6746545 -5.6933494 -5.8032742 -4.9284005 -4.5173974 -4.092371 -4.2031384 -3.9636655 -3.8434339 -4.1760921 -4.8377132 -7.1053486 -8.0617266 -9.2622013 -9.0270109][-6.2539024 -5.6772027 -5.1857371 -3.8674827 -3.364208 -2.7423611 -2.5472765 -2.4253621 -2.4099121 -2.8559554 -3.5193713 -5.6429753 -6.5849252 -7.3752422 -7.6197982][-6.1673756 -5.3103633 -4.0999746 -2.6926205 -2.8145626 -2.7127314 -2.2984681 -1.910655 -1.6260595 -1.7762957 -2.3572383 -4.4370036 -5.2651114 -6.1708789 -5.5819359][-6.5570712 -5.6356039 -4.2247686 -2.140039 -1.6485686 -0.73771667 -0.69891691 -0.33121586 -0.48249197 -0.57184792 -0.43176031 -2.3350964 -2.8342681 -4.1057515 -4.3070264][-6.85369 -5.2566066 -3.1341741 -0.46647549 0.50051689 1.2144732 1.5249348 1.5355 0.96445274 0.12440062 -0.24909973 -1.7611959 -2.6667025 -4.2145629 -3.9233291][-7.5920439 -5.8360538 -3.5122259 -0.33507633 0.78164625 2.4769015 3.1513333 3.299253 3.1014276 1.9581375 0.54424143 -1.5464287 -2.5009508 -3.7884934 -3.9948626][-7.1847081 -6.4306712 -4.6366477 -1.1064639 1.1902747 3.3562207 4.1992755 4.4319882 4.019279 3.0867333 1.7427917 -0.76570177 -2.1446154 -3.3269272 -3.7870207][-7.2909675 -6.418817 -4.6024776 -1.3455009 0.566432 3.1899276 3.9233356 4.2464061 4.7897849 3.6947713 2.3428082 -0.68734741 -2.9426513 -4.5327673 -4.5851192][-5.2534633 -5.0470824 -4.2333751 -1.5574818 -0.33555746 1.9683104 2.7613702 2.970593 2.8399472 2.392015 1.4856715 -1.2251287 -3.2049134 -5.2698593 -5.9986691][-7.1726513 -5.7977915 -4.2915306 -2.9379652 -2.4865534 -1.6425662 -1.8953767 -1.9040329 -1.8547859 -2.0389109 -1.9859352 -4.5875435 -6.0720692 -7.2512317 -6.8105536][-9.0426626 -7.614356 -6.1327028 -5.0263181 -4.7960854 -4.6748843 -5.9585309 -6.7058187 -6.7602077 -6.767004 -6.7506208 -7.7150903 -7.4973407 -8.2260818 -7.6828461][-10.796579 -9.0609713 -7.4824748 -7.2639956 -7.2042456 -6.5828109 -6.8528447 -7.7729316 -9.13036 -9.1499195 -8.4406395 -7.9932117 -7.2215271 -7.0889244 -6.14096][-9.7090263 -9.6072969 -8.7034988 -7.4805455 -6.2678528 -5.8311591 -6.4461975 -6.5534649 -6.564301 -6.6464229 -6.2284131 -5.1856284 -4.5558853 -4.9438133 -4.9606991][-9.3223953 -9.8357792 -9.5197659 -8.2311115 -6.6728935 -5.8718443 -5.6126804 -5.4215517 -5.3275919 -4.8715782 -4.1976781 -4.0317464 -4.2740588 -3.5318398 -3.6305664]]...]
INFO - root - 2017-12-15 16:15:11.998333: step 32310, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 53h:53m:37s remains)
INFO - root - 2017-12-15 16:15:18.676438: step 32320, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 55h:11m:11s remains)
INFO - root - 2017-12-15 16:15:25.212044: step 32330, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 53h:40m:56s remains)
INFO - root - 2017-12-15 16:15:31.796125: step 32340, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 54h:26m:23s remains)
INFO - root - 2017-12-15 16:15:38.368612: step 32350, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 55h:05m:59s remains)
INFO - root - 2017-12-15 16:15:44.869792: step 32360, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 54h:07m:03s remains)
INFO - root - 2017-12-15 16:15:51.388197: step 32370, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 54h:17m:15s remains)
INFO - root - 2017-12-15 16:15:57.995682: step 32380, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 54h:57m:43s remains)
INFO - root - 2017-12-15 16:16:04.592227: step 32390, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.692 sec/batch; 57h:39m:58s remains)
INFO - root - 2017-12-15 16:16:11.295114: step 32400, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 55h:08m:14s remains)
2017-12-15 16:16:11.848617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9742112 -4.4205389 -4.3936625 -3.6240604 -4.0136061 -4.4334774 -4.6845026 -5.6838808 -6.5307469 -7.7686739 -7.6626692 -6.4177766 -7.3384361 -7.840044 -7.4649844][-5.099071 -5.1588254 -5.2225933 -4.9109788 -3.7429862 -3.8117161 -5.0563636 -5.6166849 -6.4495597 -6.4081068 -7.0259933 -5.6322966 -7.1315136 -7.2563853 -7.343997][-3.4594629 -4.6618729 -6.1141682 -5.6908612 -5.0887661 -5.1730328 -5.44724 -6.4142337 -6.9130111 -7.6689715 -7.4795156 -4.6941576 -5.8833814 -5.5613823 -5.4625516][-3.2777073 -4.782392 -4.7833185 -5.5595865 -6.1515436 -5.5055051 -5.005353 -6.2595191 -6.6508436 -7.1553011 -7.2105384 -4.3359284 -5.9537764 -5.6984072 -5.8076968][-3.5527706 -5.7571611 -6.3749433 -5.2305088 -5.0295405 -3.93396 -2.8226717 -3.6349058 -5.2068648 -6.381916 -6.497839 -4.1291065 -4.7329321 -5.8521986 -6.1742492][-5.52073 -5.9030275 -6.2663431 -5.7054877 -4.8918242 -3.2305262 -0.905231 -0.53433275 -1.2168522 -3.7838221 -5.7377806 -3.3081872 -4.8081236 -5.8668633 -6.4786668][-5.6841192 -5.361321 -5.6469388 -4.923893 -2.1853518 -0.16549778 1.7493434 2.7582116 2.7818475 0.67509174 -2.0888455 -1.8081453 -3.6464629 -4.7795343 -6.1143241][-6.844018 -6.0376878 -5.2937994 -4.1777325 -2.0124106 1.4597473 3.8580117 3.7454934 3.0826364 2.0528522 0.14585495 -0.20780563 -2.9062874 -4.6442595 -5.9887304][-6.1185822 -5.31382 -5.7174978 -4.1728778 -3.2316384 -1.2008691 2.7103906 2.7013774 0.57398558 -0.53160334 -0.58484554 -0.010432243 -2.7566264 -4.892251 -5.2147541][-7.3957205 -6.6518 -5.7557697 -4.4066048 -3.6749685 -1.4906025 0.15816784 0.40653086 -1.0994091 -2.9163079 -3.5711124 -2.0465171 -3.3664625 -4.85862 -5.9800973][-11.220757 -9.8320522 -9.2529793 -7.6829176 -5.7491436 -4.4278626 -3.1517947 -4.101512 -4.508152 -5.8655496 -5.645647 -4.7402921 -6.5619946 -6.3097272 -5.817348][-13.983013 -11.967601 -9.8945627 -8.3653717 -5.8303742 -3.960284 -3.6331232 -4.8849592 -6.3862996 -6.5818143 -5.578115 -4.6276956 -6.435092 -6.4407949 -4.5521784][-13.197235 -11.981693 -9.68627 -6.86979 -5.0624428 -2.5371478 -1.8358052 -3.5097356 -5.0634265 -6.3990011 -5.2014618 -3.7963133 -4.7643738 -5.1665282 -5.5735984][-10.440573 -9.2644053 -7.4779882 -4.3913579 -2.9388025 -2.2312019 -2.4694664 -3.0168536 -3.7279587 -4.5020847 -4.08712 -3.838706 -3.9247909 -2.459938 -2.5720608][-7.8289881 -9.3228607 -6.1212091 -2.353683 -2.3256781 -2.720891 -3.5302022 -4.5487032 -4.478055 -4.3619347 -3.2106543 -3.4623764 -4.2320938 -3.5380521 -3.7336955]]...]
INFO - root - 2017-12-15 16:16:18.368052: step 32410, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 55h:17m:25s remains)
INFO - root - 2017-12-15 16:16:24.913721: step 32420, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 55h:11m:37s remains)
INFO - root - 2017-12-15 16:16:31.497129: step 32430, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 54h:58m:57s remains)
INFO - root - 2017-12-15 16:16:38.084340: step 32440, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 55h:16m:49s remains)
INFO - root - 2017-12-15 16:16:44.740588: step 32450, loss = 0.20, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 54h:13m:43s remains)
INFO - root - 2017-12-15 16:16:51.332301: step 32460, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 53h:01m:52s remains)
INFO - root - 2017-12-15 16:16:57.922320: step 32470, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 53h:23m:28s remains)
INFO - root - 2017-12-15 16:17:04.519860: step 32480, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 54h:09m:57s remains)
INFO - root - 2017-12-15 16:17:11.122766: step 32490, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 55h:22m:56s remains)
INFO - root - 2017-12-15 16:17:17.803856: step 32500, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 56h:30m:30s remains)
2017-12-15 16:17:18.312501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5485456 -4.2636366 -3.9410307 -3.5222611 -3.7731185 -4.2837543 -4.9606571 -4.24498 -3.814291 -3.296371 -2.4161921 -2.5936813 -2.5713761 -4.1429558 -3.7123129][-2.3850362 -2.7382786 -3.384726 -3.2535257 -3.5583329 -4.1443038 -4.17726 -3.4092751 -2.3301818 -1.7264001 -0.88009834 -0.86193943 -0.79456329 -3.45323 -3.5159736][-2.6795208 -2.8828855 -3.6060276 -2.9009392 -3.6073508 -4.3284712 -4.1822472 -2.9286537 -2.1287889 -2.1366203 -1.2787738 -0.79984331 0.13854456 -2.4614363 -1.941705][-4.0553718 -5.1552625 -5.5922165 -4.8263755 -5.550415 -4.4128146 -3.3622289 -2.7287493 -1.8297641 -1.2460856 -1.4248567 -1.9271944 -1.0269523 -2.4641309 -1.52703][-5.24546 -6.4046578 -7.1190252 -5.7728677 -4.8252339 -3.0359731 -2.0241611 -0.58439541 0.4461937 -0.050709724 -0.81207371 -1.7965872 -2.1610465 -4.3760571 -3.1948812][-5.177 -5.4161067 -5.9276195 -4.5025549 -2.7888005 0.87834978 3.0291638 3.2692132 3.1303039 1.2820306 0.063314915 -0.92662287 -1.7344079 -4.6316438 -3.8274362][-5.2495594 -4.2969513 -3.0608552 -1.8182249 -1.4271646 2.728683 6.1471667 7.1309447 6.8445659 3.8149552 1.7233596 -0.69562387 -2.4050207 -5.0537233 -3.9738014][-4.8176994 -4.6649518 -3.8955059 -1.0426149 0.80521822 4.3718381 6.799561 8.1565247 8.2208271 5.0095391 1.6195617 -1.264204 -2.1467905 -5.4426036 -5.0375643][-6.6331429 -6.5317845 -5.2172494 -2.8878419 -1.8202 2.0379553 4.3103728 6.037374 6.4494262 3.7950187 1.6520734 -1.974931 -3.7514286 -5.4418187 -4.3957133][-5.9597836 -7.1230316 -7.1998019 -4.6303759 -4.1795135 -0.8726511 1.5092177 3.1025977 3.0204492 0.272182 -2.330282 -4.796175 -5.2606993 -7.8613787 -6.7691154][-9.752574 -9.6702309 -9.1347036 -7.1977673 -6.0633717 -4.7700968 -4.7190585 -3.5746894 -3.169934 -3.5645974 -4.6541438 -7.7989969 -8.8646164 -9.7329292 -7.336205][-10.811035 -10.323615 -9.8256016 -8.4006824 -7.819077 -7.1035295 -6.6757479 -6.0571489 -6.5726295 -7.367218 -7.6929989 -7.8833036 -7.8129897 -8.7454643 -7.6941323][-11.436722 -10.700214 -9.3757954 -7.9416986 -7.6131868 -5.7492476 -5.6295643 -6.730207 -7.1396713 -6.1696014 -5.245574 -6.0388279 -5.8013854 -7.3399405 -6.3275647][-8.6295166 -8.4934072 -7.9837675 -7.1170435 -6.1697941 -5.6450996 -5.8387427 -4.8609471 -4.7639246 -4.5075436 -4.0932455 -3.2797196 -2.2599289 -4.2409129 -4.5662937][-6.123858 -5.5153055 -5.3232226 -5.5208836 -5.3099432 -4.7500157 -3.9948771 -3.8859696 -3.5735264 -3.5201213 -3.437706 -3.6283212 -3.7770386 -4.3898592 -5.186192]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 16:17:25.698962: step 32510, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 54h:33m:24s remains)
INFO - root - 2017-12-15 16:17:32.337174: step 32520, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 54h:39m:07s remains)
INFO - root - 2017-12-15 16:17:38.783590: step 32530, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.640 sec/batch; 53h:20m:44s remains)
INFO - root - 2017-12-15 16:17:45.346778: step 32540, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 55h:16m:04s remains)
INFO - root - 2017-12-15 16:17:51.885421: step 32550, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 55h:07m:10s remains)
INFO - root - 2017-12-15 16:17:58.394347: step 32560, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.636 sec/batch; 52h:57m:10s remains)
INFO - root - 2017-12-15 16:18:04.937877: step 32570, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 53h:22m:21s remains)
INFO - root - 2017-12-15 16:18:11.418998: step 32580, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 53h:37m:42s remains)
INFO - root - 2017-12-15 16:18:18.003812: step 32590, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 54h:41m:41s remains)
INFO - root - 2017-12-15 16:18:24.599297: step 32600, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 54h:25m:57s remains)
2017-12-15 16:18:25.115526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3902423 -3.367975 -3.1404583 -2.414778 -2.6216414 -3.50838 -4.3633323 -5.3627238 -6.0509586 -5.774116 -5.5035615 -4.6449881 -5.7737818 -8.1062956 -8.3892555][-6.5635166 -5.7644224 -4.789475 -4.485034 -4.8104696 -5.8393431 -7.0504031 -7.5064383 -7.2931075 -6.9862227 -6.6438694 -6.6175947 -8.5557871 -9.3551292 -8.1305637][-6.6411819 -7.0053515 -6.7452 -6.4330392 -7.2736034 -7.1009741 -7.2005386 -7.4561167 -7.6485043 -7.5812945 -7.4196439 -6.2482033 -6.7167087 -7.5375738 -7.2461543][-4.6726823 -5.5306315 -5.8593516 -6.8249617 -7.7572393 -7.9170809 -7.9975758 -8.0947132 -7.3837762 -6.226336 -5.7027497 -5.7556338 -7.2757463 -7.4283452 -7.1332073][-6.0861025 -6.2359123 -6.3260903 -6.4071231 -6.1706491 -5.2479811 -4.0909872 -4.2334542 -4.7037163 -4.4800673 -4.1820431 -4.1041913 -4.8364916 -6.3273149 -7.234859][-7.106113 -6.2357869 -4.3446641 -3.8553534 -3.5660598 -1.8574464 0.32226086 0.80495596 1.1776156 0.047661304 -1.6582298 -2.2759981 -4.5790935 -6.6450987 -8.3525515][-7.0395374 -7.8129921 -6.0320148 -4.7548909 -2.936094 -0.95466566 1.8679795 4.9947419 6.6037517 3.9805045 0.35367966 -1.8878293 -4.6102247 -6.7954817 -7.1923141][-6.9744439 -8.2729607 -6.7895713 -4.7363863 -2.6847606 0.37598324 3.5460858 6.5702891 7.9432578 5.3684611 1.5943518 -2.171953 -6.5066986 -8.1501465 -8.5097218][-7.3262229 -8.5746965 -6.8490744 -4.90862 -3.2752697 -0.040445805 3.1477494 5.5609536 6.3443656 4.6208043 0.94201279 -3.2846761 -7.33763 -9.2765732 -9.2709484][-6.4484234 -7.2455382 -5.9003067 -4.6466255 -2.5941918 -0.42704964 1.4224248 3.3356614 4.3309674 2.4227042 -1.0272779 -3.6801674 -7.0433588 -9.5227442 -11.133258][-8.2163458 -7.8132954 -6.2782326 -5.0373626 -4.0142145 -3.1373026 -2.3597574 -1.0557213 -0.24031115 -2.0332971 -4.9450569 -6.3015013 -8.7825871 -8.5531006 -7.656456][-9.1303577 -9.23224 -8.4093971 -7.0733614 -6.0822506 -6.0917563 -5.9745369 -5.5468946 -5.3524265 -6.227128 -7.7093363 -8.4336109 -8.78462 -8.6845551 -8.4154854][-10.019544 -9.44593 -8.97403 -8.0094652 -7.3886185 -6.5202723 -6.8506346 -7.5809093 -8.2422771 -8.7458124 -9.5359936 -10.450787 -10.591256 -8.5546656 -7.8381481][-7.5664339 -7.8594093 -7.7702756 -7.8832417 -7.30069 -7.2756019 -7.9194746 -8.6455011 -8.3764229 -8.7957373 -9.4246244 -8.7353115 -8.2454252 -7.7517242 -8.2302418][-5.8482208 -7.3417611 -7.9313602 -8.1550579 -7.4131064 -6.567677 -6.4316545 -6.3151617 -6.5426722 -6.9164071 -8.3267136 -8.5743427 -8.3917875 -8.1025972 -8.439271]]...]
INFO - root - 2017-12-15 16:18:31.672165: step 32610, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 56h:24m:10s remains)
INFO - root - 2017-12-15 16:18:38.231885: step 32620, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 55h:34m:59s remains)
INFO - root - 2017-12-15 16:18:44.907823: step 32630, loss = 0.13, batch loss = 0.09 (11.4 examples/sec; 0.703 sec/batch; 58h:34m:22s remains)
INFO - root - 2017-12-15 16:18:51.484326: step 32640, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.665 sec/batch; 55h:21m:39s remains)
INFO - root - 2017-12-15 16:18:58.068485: step 32650, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 55h:05m:09s remains)
INFO - root - 2017-12-15 16:19:04.648895: step 32660, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.682 sec/batch; 56h:50m:00s remains)
INFO - root - 2017-12-15 16:19:11.221109: step 32670, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 53h:16m:49s remains)
INFO - root - 2017-12-15 16:19:17.807309: step 32680, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 54h:57m:31s remains)
INFO - root - 2017-12-15 16:19:24.372557: step 32690, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 53h:05m:12s remains)
INFO - root - 2017-12-15 16:19:30.895643: step 32700, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 56h:07m:42s remains)
2017-12-15 16:19:31.433345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.41844 -6.0508838 -5.2415004 -4.7941575 -5.2830925 -5.4100685 -5.0841908 -4.5734305 -4.496768 -4.5917616 -5.0398083 -7.205883 -9.462574 -9.564002 -9.1114206][-5.9432 -5.6082549 -5.1335053 -4.4889374 -5.0021152 -5.0465689 -4.6670065 -4.1805172 -3.6736648 -3.4166079 -3.4410384 -5.30708 -7.9104424 -8.4447155 -8.6961012][-3.6623826 -4.0954704 -3.783227 -3.7745242 -4.65695 -4.7355976 -4.8038368 -4.572319 -4.1986294 -3.9629049 -3.8933702 -5.7926555 -8.1269751 -7.9467778 -8.063118][-3.5665934 -3.5786922 -3.2069407 -3.0405903 -3.8278353 -3.8280566 -3.7345438 -3.9316311 -3.7316098 -3.6104488 -3.501055 -5.500124 -7.8590684 -8.0665112 -7.8263521][-3.1115863 -3.565552 -3.2871156 -2.535202 -2.627207 -2.1260633 -2.0019326 -1.8865392 -2.164196 -2.8591204 -3.01374 -5.1986141 -8.2148514 -8.3355656 -7.8127742][-4.7111912 -4.3877106 -2.8337662 -0.97382021 -0.055804253 1.0543237 1.2513022 1.0196457 0.05551672 -0.74384212 -1.6598616 -4.579855 -7.3289518 -7.3233247 -7.8167429][-6.31663 -4.880518 -2.8419759 -0.67815876 0.75429726 2.4098659 3.1836858 2.609488 1.649066 0.70435858 -0.47605181 -3.6912589 -7.0600848 -7.1753297 -7.324553][-6.5338097 -5.9880438 -4.23042 -1.0655179 0.63259172 2.4080262 3.0497832 2.5876422 2.3950868 1.7480907 0.23890638 -3.2601902 -6.6983414 -7.0792365 -7.4066405][-6.5424705 -6.1252313 -4.3954673 -1.5229216 0.20220757 1.8347826 2.2940545 2.3537917 1.7583137 0.85782957 0.21123075 -2.9762738 -6.6907043 -7.0467234 -6.98715][-6.2745991 -6.152791 -4.258316 -1.4629121 -0.1577158 1.2315054 1.1106215 0.31318378 -0.20911407 -0.54583073 -1.6533237 -4.9076695 -8.0076866 -8.1313419 -8.418231][-9.4455137 -7.2239304 -4.7120714 -2.6373568 -2.1111066 -1.608737 -2.2804656 -3.7672415 -4.9616194 -5.616509 -6.2823572 -8.8533516 -10.032842 -9.6951866 -9.2757139][-10.98909 -8.9107857 -6.6022029 -4.2931218 -3.9012823 -4.4777822 -6.1191931 -8.0968485 -8.7943716 -9.26108 -10.098621 -11.507904 -12.12838 -11.0826 -9.9479361][-10.890026 -8.4164639 -6.2591858 -5.2098713 -5.4197612 -6.41682 -7.30229 -8.44801 -9.79204 -10.175299 -9.8027973 -10.201614 -10.2335 -8.7156963 -8.142519][-10.266681 -8.9092979 -7.55106 -6.6958447 -6.431819 -6.914391 -7.5196185 -7.9205 -7.5397649 -7.5607905 -7.8137779 -7.5896955 -7.324688 -6.8396573 -6.3409519][-9.4318933 -9.4496641 -8.88038 -8.0791111 -8.3793821 -7.9998717 -7.7053957 -7.4690475 -6.4282441 -5.9605441 -5.7710376 -5.9446087 -5.8904867 -6.1797938 -6.5114493]]...]
INFO - root - 2017-12-15 16:19:37.992801: step 32710, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 56h:44m:49s remains)
INFO - root - 2017-12-15 16:19:44.660909: step 32720, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 56h:21m:02s remains)
INFO - root - 2017-12-15 16:19:51.253509: step 32730, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 55h:34m:10s remains)
INFO - root - 2017-12-15 16:19:57.824106: step 32740, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 54h:59m:43s remains)
INFO - root - 2017-12-15 16:20:04.344815: step 32750, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 54h:37m:56s remains)
INFO - root - 2017-12-15 16:20:10.952503: step 32760, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 55h:38m:58s remains)
INFO - root - 2017-12-15 16:20:17.564015: step 32770, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.671 sec/batch; 55h:51m:12s remains)
INFO - root - 2017-12-15 16:20:24.222964: step 32780, loss = 0.17, batch loss = 0.12 (11.6 examples/sec; 0.689 sec/batch; 57h:23m:07s remains)
INFO - root - 2017-12-15 16:20:30.783228: step 32790, loss = 0.18, batch loss = 0.13 (12.8 examples/sec; 0.627 sec/batch; 52h:13m:10s remains)
INFO - root - 2017-12-15 16:20:37.330692: step 32800, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 53h:41m:38s remains)
2017-12-15 16:20:37.834000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.396426 -3.5720422 -3.686626 -3.2512653 -3.7814455 -4.0950212 -4.2392073 -4.18463 -3.8591533 -3.9203334 -3.458642 -6.0923271 -7.8093576 -9.5038395 -8.92271][-4.97997 -4.8692422 -4.6741524 -4.2156296 -4.682992 -5.1113272 -5.3590479 -5.5976915 -5.4960294 -4.9201112 -4.2691803 -6.5580907 -7.609026 -9.722168 -10.076777][-4.352356 -5.0235662 -5.7184343 -5.3203568 -5.8119965 -6.0515943 -6.4331841 -6.2937889 -6.0810885 -5.9934769 -5.5423226 -7.6472015 -8.8378115 -10.462088 -10.636075][-4.9434357 -5.7322793 -5.7875357 -5.3181672 -5.8841891 -5.6057253 -5.4332819 -5.4825144 -5.3939266 -4.9766116 -4.7504263 -7.3318348 -8.9961891 -11.026465 -11.15766][-6.045085 -7.1100969 -7.5426722 -5.8994565 -5.3960629 -4.4943523 -4.4057841 -4.4061823 -4.1046209 -3.6487606 -3.1548193 -6.0825095 -8.315237 -10.920175 -11.621292][-7.1613779 -7.6884041 -7.2179317 -4.94133 -2.7642572 -0.6867547 0.032421112 -0.30428267 -1.0225544 -1.4702115 -1.459415 -4.1451259 -6.29399 -9.1386471 -10.314734][-8.394268 -8.2826214 -6.7397842 -3.3929336 -0.73625469 1.2360387 2.6674104 3.3606639 3.2083726 1.2807708 -0.21077013 -3.4231615 -5.6396461 -8.3375731 -9.2851028][-7.8701482 -7.8558068 -6.0804973 -2.2692332 0.44106436 3.2378011 4.9182849 4.2714763 4.0203977 2.7443175 0.63228273 -3.6274679 -6.2209477 -8.7637405 -9.3502979][-5.7514472 -5.70907 -4.7542114 -2.5528538 -0.91662312 2.404779 4.0617309 4.1966691 4.3398957 2.5127568 0.72848272 -3.8087919 -7.0514822 -9.58214 -9.6644][-4.7034664 -3.7114162 -3.2312264 -2.1210158 -1.3376346 0.015163898 0.80433989 2.0808864 2.2283468 0.77797651 -0.58340216 -5.0791683 -8.14705 -10.647676 -11.382534][-5.6898074 -4.8813968 -4.155663 -3.2088981 -3.4697099 -3.1743169 -3.2013993 -2.8262322 -2.9621658 -3.2992849 -4.0201635 -8.1307812 -10.907959 -12.574999 -12.257296][-8.8016863 -7.2527027 -5.92644 -6.0480323 -6.0257273 -6.0224833 -6.940021 -7.6493974 -8.0785589 -8.0665646 -8.3461 -9.8005028 -10.796248 -12.267897 -11.660192][-11.61598 -10.405169 -8.9888239 -8.45138 -8.9223356 -8.645401 -8.809866 -9.334795 -10.380019 -10.819773 -10.681394 -10.946873 -11.234835 -11.318254 -9.6841679][-11.349515 -11.060868 -9.9676914 -9.2623692 -8.8916245 -8.867362 -9.0138426 -9.1330318 -9.5012465 -10.290176 -10.381059 -9.1879663 -8.3894033 -9.2428236 -8.8996372][-8.6508951 -9.0602884 -8.641922 -7.3441372 -6.8450923 -6.6392555 -7.0983968 -7.5802045 -7.7796049 -7.803196 -7.89841 -8.7222567 -9.5000734 -8.7817955 -8.3649731]]...]
INFO - root - 2017-12-15 16:20:44.396347: step 32810, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 53h:57m:11s remains)
INFO - root - 2017-12-15 16:20:51.019112: step 32820, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 54h:03m:52s remains)
INFO - root - 2017-12-15 16:20:57.553176: step 32830, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 54h:18m:43s remains)
INFO - root - 2017-12-15 16:21:04.062622: step 32840, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 54h:35m:01s remains)
INFO - root - 2017-12-15 16:21:10.654628: step 32850, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 55h:05m:18s remains)
INFO - root - 2017-12-15 16:21:17.123054: step 32860, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 54h:54m:34s remains)
INFO - root - 2017-12-15 16:21:23.694232: step 32870, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 55h:30m:31s remains)
INFO - root - 2017-12-15 16:21:30.278692: step 32880, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 54h:10m:57s remains)
INFO - root - 2017-12-15 16:21:36.868429: step 32890, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 55h:21m:10s remains)
INFO - root - 2017-12-15 16:21:43.492808: step 32900, loss = 0.20, batch loss = 0.16 (12.0 examples/sec; 0.667 sec/batch; 55h:28m:53s remains)
2017-12-15 16:21:44.078752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4397144 -6.4766693 -5.974227 -5.8698182 -7.2889605 -7.6985598 -8.6202755 -9.2389193 -9.0317974 -8.7113209 -7.8436813 -8.92516 -9.808609 -10.547455 -9.2415342][-6.475143 -6.0136862 -4.804512 -4.2533321 -5.2884398 -6.8953176 -8.7008486 -9.7247543 -10.7725 -10.366445 -9.3371935 -11.00527 -11.626294 -11.553562 -10.84477][-3.3874605 -4.7232819 -5.7764568 -4.3786721 -4.8451552 -5.5719595 -6.7396941 -8.1155777 -8.799633 -9.7148 -10.085636 -11.000842 -11.362616 -11.954538 -11.302216][-6.2897544 -5.99035 -5.9973707 -5.7780433 -5.943254 -5.996201 -6.420248 -7.5617609 -8.9313545 -8.7247753 -8.0428772 -10.439932 -11.131516 -11.615784 -12.411978][-6.8264437 -7.6901674 -7.6814375 -6.1273189 -5.3463936 -3.7921064 -2.9849927 -4.8157535 -7.7001281 -7.0776358 -6.5735431 -9.0325623 -10.376554 -11.958886 -12.073933][-9.5239811 -10.340271 -8.9562378 -5.2895441 -2.6374075 -0.67769718 1.5336108 0.70817661 -1.4327993 -3.7473435 -6.1630583 -7.4339366 -8.2976456 -9.64693 -10.12138][-12.669956 -11.61478 -9.3079729 -4.6153989 -1.1824689 2.6284213 7.1381288 5.7159095 4.4174209 1.2107701 -3.609621 -6.4525213 -8.1926546 -9.1107149 -9.33308][-14.46797 -14.079277 -11.806383 -6.2459769 -2.0634851 4.5007472 9.8535061 7.8669839 6.3981538 2.9830337 -1.1025786 -4.2948613 -7.3295612 -8.8378391 -9.4465666][-12.327397 -12.071976 -10.657209 -5.561306 -1.800637 3.0488973 6.4872708 6.0211854 5.9216981 2.1710038 -1.477149 -5.5925016 -9.06502 -9.9030437 -10.319214][-9.01864 -8.598074 -9.0558758 -5.9549651 -3.9299586 -0.54870319 3.3442121 3.776238 3.9345136 0.14341068 -3.1955857 -6.5443673 -9.7019272 -10.652477 -11.319534][-14.656448 -13.429718 -13.264709 -9.9021511 -8.3038044 -5.2163134 -1.7083125 -2.1350765 -1.8752761 -3.0937684 -4.9613962 -8.19936 -9.5892658 -10.588463 -9.9305763][-19.343254 -19.048601 -18.874762 -15.648172 -13.936708 -10.717038 -8.4425468 -7.3698993 -6.8057818 -7.5749903 -8.4709873 -10.03475 -11.2718 -11.420852 -10.881337][-16.993256 -16.32292 -15.980326 -13.746977 -13.615017 -11.707733 -11.031305 -10.29718 -8.95583 -7.6054168 -7.950304 -9.815382 -10.973804 -11.798365 -10.766012][-13.501873 -13.223009 -13.481741 -12.098906 -10.708656 -11.393579 -11.408443 -10.225781 -9.9040833 -8.5197735 -7.7299662 -9.0212679 -10.04465 -9.5517988 -9.5952406][-12.177024 -12.108727 -10.793274 -7.96998 -7.0900736 -7.2882228 -7.7102585 -7.8746572 -7.8643236 -7.2313962 -6.8777523 -6.8711457 -8.4094486 -10.501528 -11.739897]]...]
INFO - root - 2017-12-15 16:21:50.641093: step 32910, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 54h:12m:18s remains)
INFO - root - 2017-12-15 16:21:57.312858: step 32920, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.682 sec/batch; 56h:46m:48s remains)
INFO - root - 2017-12-15 16:22:03.922023: step 32930, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 54h:50m:24s remains)
INFO - root - 2017-12-15 16:22:10.549513: step 32940, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 55h:27m:22s remains)
INFO - root - 2017-12-15 16:22:17.050329: step 32950, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 53h:49m:38s remains)
INFO - root - 2017-12-15 16:22:23.681514: step 32960, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 55h:56m:34s remains)
INFO - root - 2017-12-15 16:22:30.292404: step 32970, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 53h:46m:50s remains)
INFO - root - 2017-12-15 16:22:36.922785: step 32980, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 53h:32m:31s remains)
INFO - root - 2017-12-15 16:22:43.448413: step 32990, loss = 0.19, batch loss = 0.14 (11.8 examples/sec; 0.680 sec/batch; 56h:35m:03s remains)
INFO - root - 2017-12-15 16:22:49.952518: step 33000, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 54h:09m:03s remains)
2017-12-15 16:22:50.452946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.38535 -4.7805533 -5.1116638 -4.5291615 -3.9852254 -4.1962404 -4.0338435 -3.0935252 -2.1878877 -1.7790115 -1.713212 -3.2485833 -5.4957728 -7.5049171 -6.5095649][-3.7486022 -3.9953368 -3.2593441 -3.2599978 -3.6546631 -3.9412761 -3.608752 -3.0037634 -2.3651166 -1.8627741 -2.3338723 -3.3542545 -4.7742915 -6.8927517 -6.5195012][-2.5553765 -2.3263521 -2.8182013 -3.0683749 -3.1257024 -3.2957187 -3.278651 -2.9647946 -1.8755522 -1.3411207 -2.1249297 -4.1569195 -6.4082317 -8.710043 -8.1802368][-2.5452809 -2.9319239 -2.9901619 -2.9740531 -3.0117652 -3.0612772 -2.9542253 -3.1648133 -2.7517469 -1.9373991 -1.9209564 -3.8048234 -6.5758281 -8.5833883 -8.3200378][-2.9717987 -3.1371348 -2.9536042 -2.3084326 -1.8632016 -1.4548111 -1.6304069 -2.5249789 -1.959975 -1.9358876 -2.4431932 -3.9320948 -6.4762983 -9.3289747 -9.3302813][-3.5549617 -3.1275556 -2.5268886 -0.75306273 0.041712284 0.21004295 0.19687891 -0.27948618 -0.5179801 -1.1952906 -1.7781985 -3.5713689 -6.377347 -9.1059914 -9.5568018][-3.6163425 -3.2418892 -2.5119724 -0.13341713 0.7065897 1.6563129 2.74851 2.3091359 2.056325 0.81488562 -0.23005581 -2.3132124 -5.2842951 -8.4928493 -8.5770674][-3.9114437 -3.4366777 -1.9069593 0.038309097 0.26009417 1.6607623 3.3913541 3.4463983 3.5709853 1.82159 -0.16102219 -2.2222967 -4.7445331 -7.5336423 -7.6649518][-2.8802171 -3.1297092 -1.6059012 -0.79150963 -0.38055611 1.4939413 2.0375566 2.069304 2.8012843 2.1666813 1.6712112 -1.2174535 -4.1136427 -6.4312382 -5.7815948][-2.2264094 -2.2486863 -1.9016118 -1.0095291 -0.23237371 0.24149466 0.53896761 1.481905 2.3819661 2.1254902 1.4365129 -0.67977619 -3.2764268 -6.2456331 -5.9805436][-5.7892551 -5.7742224 -5.1912594 -4.8096552 -3.5037968 -3.1541915 -3.2136822 -2.10614 -1.5982881 -1.7224629 -1.4621015 -3.3410184 -4.591362 -5.9135165 -5.1415691][-7.2838993 -7.1414342 -6.8013315 -5.6826029 -5.3345113 -5.4314566 -5.0121942 -4.5980258 -4.4151363 -4.1109591 -4.188098 -4.2642083 -4.3376751 -6.4640326 -6.4831014][-10.950026 -9.7544956 -7.8743286 -7.3025188 -6.8665495 -6.535943 -7.1955175 -7.3549562 -7.6332779 -7.4905386 -6.69156 -6.2008305 -5.9383135 -5.9363775 -4.402638][-8.5374908 -8.31185 -8.1918344 -5.907495 -4.607285 -4.6216264 -4.9904389 -6.3412342 -7.5182776 -7.6901493 -7.4330769 -6.3623395 -5.9052844 -6.3121872 -5.5843945][-7.1682858 -6.1832871 -5.4306936 -4.9539175 -4.6282353 -4.298326 -4.4991045 -5.269907 -6.2040205 -6.605721 -6.8873272 -6.9733481 -6.8696604 -6.944849 -6.7833357]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 16:22:57.021208: step 33010, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 56h:15m:40s remains)
INFO - root - 2017-12-15 16:23:03.647677: step 33020, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 54h:47m:16s remains)
INFO - root - 2017-12-15 16:23:10.276521: step 33030, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 55h:19m:04s remains)
INFO - root - 2017-12-15 16:23:16.870434: step 33040, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 54h:28m:17s remains)
INFO - root - 2017-12-15 16:23:23.429985: step 33050, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.689 sec/batch; 57h:18m:47s remains)
INFO - root - 2017-12-15 16:23:30.085421: step 33060, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 54h:30m:29s remains)
INFO - root - 2017-12-15 16:23:36.694945: step 33070, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 55h:21m:16s remains)
INFO - root - 2017-12-15 16:23:43.299543: step 33080, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 55h:08m:38s remains)
INFO - root - 2017-12-15 16:23:49.814892: step 33090, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 54h:02m:30s remains)
INFO - root - 2017-12-15 16:23:56.407153: step 33100, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 54h:36m:27s remains)
2017-12-15 16:23:56.903754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.286799 -6.0834904 -5.5121183 -5.2973695 -6.0922523 -6.4227934 -6.829699 -6.1511965 -4.4959636 -3.1394732 -2.3874393 -4.0673828 -6.6189828 -5.6120963 -5.3221216][-5.1623335 -5.6196342 -4.8392563 -4.3989878 -4.7680736 -4.9991827 -4.8986511 -4.8885241 -4.1485958 -3.154233 -2.4753628 -3.8677595 -5.9181943 -6.0254326 -6.6202207][-2.7800794 -3.6320212 -3.5901515 -2.5093093 -1.8748715 -2.3129823 -2.8702056 -2.7951121 -1.9848771 -1.6074176 -1.4567742 -3.9097044 -6.9515581 -7.0307217 -6.9462662][-5.4526377 -5.3800764 -4.3827195 -3.1985383 -2.668057 -2.0624144 -1.6674695 -2.0731037 -2.1588106 -1.8614736 -1.5876493 -3.280128 -5.9658179 -7.0756226 -7.9152727][-5.50812 -6.7987165 -6.363039 -4.0805225 -1.5976462 0.053572178 0.47306204 -0.18536282 -1.185451 -1.2556348 -1.0740676 -3.5238378 -6.6711111 -7.13934 -7.4034481][-6.978931 -6.4957094 -4.2794847 -1.6937966 0.73438644 3.4887614 4.5256152 3.7817893 2.7839322 0.85957384 -1.2223191 -3.270124 -5.7839403 -6.0913115 -6.3468518][-8.7971716 -7.6956091 -4.4401731 -1.4624038 0.7475276 3.3017945 4.9821677 4.9620652 4.193315 2.3475385 -0.29484844 -3.373219 -6.27784 -6.724741 -6.6428714][-8.6949177 -6.9140682 -4.0734119 -1.366138 0.27088881 1.984952 2.8164611 3.3306737 3.5563626 2.0111012 0.32768106 -3.2473958 -7.0735683 -7.6539078 -7.1576328][-7.2232566 -5.9405475 -3.754252 -1.6021433 -0.32810688 0.36855125 0.97890377 1.5418611 1.7592235 0.50011158 -0.53795004 -3.7777476 -7.5714746 -8.02036 -8.5094662][-5.4400883 -4.5750508 -3.3852549 -1.4429522 -0.14531136 0.0075573921 0.12247372 0.26806879 -0.19517899 -0.73043537 -1.1429782 -4.4018745 -8.0425262 -8.9619026 -9.3046856][-9.1161795 -8.3378658 -6.9259152 -5.0109296 -4.4119964 -4.3646326 -3.9677863 -4.4724541 -5.0345206 -5.2933521 -5.8978992 -8.2579308 -9.7831669 -10.003496 -10.113991][-13.832256 -12.993179 -10.587343 -8.9703627 -9.0866108 -8.6117191 -7.9298096 -8.32483 -8.9608183 -9.1839428 -9.5262365 -10.730115 -11.391315 -10.552071 -9.8449326][-13.10088 -12.554193 -11.627762 -10.025335 -8.4475756 -8.2837429 -8.982914 -9.4752207 -9.1682854 -8.6553316 -8.1450577 -8.6012526 -8.9732418 -7.9126844 -6.5265055][-12.050581 -11.209677 -9.730484 -7.8276238 -7.08259 -7.8294086 -8.1928616 -7.1343641 -6.8361597 -7.1260567 -7.2780752 -7.4645238 -6.8150067 -5.7773242 -5.3393831][-9.4775925 -10.010316 -9.7296314 -6.7371025 -4.8700027 -4.2271185 -4.4126582 -4.9177022 -5.028049 -4.0691032 -3.8767753 -5.1588063 -6.6062975 -6.8120279 -6.2974091]]...]
INFO - root - 2017-12-15 16:24:03.468802: step 33110, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 55h:07m:20s remains)
INFO - root - 2017-12-15 16:24:10.095679: step 33120, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 54h:14m:48s remains)
INFO - root - 2017-12-15 16:24:16.664729: step 33130, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 54h:21m:16s remains)
INFO - root - 2017-12-15 16:24:23.195335: step 33140, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 54h:33m:40s remains)
INFO - root - 2017-12-15 16:24:29.795210: step 33150, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 53h:29m:45s remains)
INFO - root - 2017-12-15 16:24:36.441769: step 33160, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 56h:19m:21s remains)
INFO - root - 2017-12-15 16:24:43.156545: step 33170, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 56h:17m:48s remains)
INFO - root - 2017-12-15 16:24:49.754500: step 33180, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 55h:40m:50s remains)
INFO - root - 2017-12-15 16:24:56.401436: step 33190, loss = 0.22, batch loss = 0.17 (11.9 examples/sec; 0.670 sec/batch; 55h:41m:32s remains)
INFO - root - 2017-12-15 16:25:02.966706: step 33200, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 55h:26m:49s remains)
2017-12-15 16:25:03.441089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.2696609 -9.4094791 -10.086967 -10.50452 -11.61418 -11.475718 -11.900475 -11.804955 -11.251634 -10.458721 -8.5696392 -7.1830149 -7.126 -5.8865881 -3.2377162][-8.1056051 -8.3369827 -9.18563 -9.477581 -11.114061 -11.255644 -11.474394 -11.36935 -10.973494 -9.5437326 -7.5824451 -5.8545556 -4.8775439 -5.05178 -3.5384912][-5.9117851 -7.6118083 -9.5580387 -9.2217035 -10.063042 -9.2282476 -9.1196346 -9.1922264 -9.1563416 -8.8843975 -7.8296337 -5.9146729 -5.0459509 -5.5122619 -4.743238][-5.8355379 -6.6890516 -8.7525082 -9.2008362 -10.064558 -8.0682917 -6.521369 -7.7294188 -8.7126369 -8.5749769 -8.4756165 -7.8949261 -8.1532574 -8.7304363 -7.4464993][-6.2190065 -8.2711792 -10.635487 -9.5320559 -8.2954445 -4.0669594 -1.5774174 -4.0018673 -6.5048347 -7.9531393 -9.5866432 -9.0364475 -9.1171312 -9.7858543 -8.3996563][-7.6185222 -7.8419151 -9.0849972 -8.5480938 -6.6776476 -0.74765205 3.7080483 2.858089 0.16989326 -4.8555937 -8.60444 -7.7712288 -7.6452379 -8.4890575 -7.4151144][-9.45735 -8.4445028 -7.408576 -5.127954 -2.9531708 2.1163073 6.9206462 7.098166 5.1415114 -1.4944797 -6.8624096 -7.138217 -8.1512451 -8.7041693 -6.6838617][-12.051178 -10.419958 -8.0861979 -3.7831008 -0.23954868 4.6489959 8.8389378 8.3787823 6.4534745 0.95066929 -4.56925 -6.073648 -7.1726208 -8.2841825 -6.4993587][-12.459101 -11.408245 -9.0590849 -5.5611944 -2.9989343 1.634975 5.5030732 5.1039329 2.8774581 -1.7139692 -5.6207948 -7.9941545 -10.294086 -10.680635 -9.0166969][-12.788762 -11.904844 -10.737965 -7.2426529 -5.3519707 -2.746762 0.5511322 0.97562027 -0.833251 -4.5343232 -8.4376059 -10.041943 -12.209024 -14.829912 -13.440551][-16.190449 -15.486557 -13.538965 -10.649285 -8.9493876 -6.6856518 -4.3032708 -4.4542251 -5.880918 -8.5155325 -11.378781 -12.888662 -13.512941 -14.515892 -13.846426][-19.62816 -18.727957 -15.856379 -11.874065 -9.3703146 -7.0493493 -6.1495905 -6.3013692 -7.0350618 -9.8048477 -12.500406 -13.675953 -13.970913 -14.306311 -12.543652][-18.388103 -17.534454 -15.036839 -11.637136 -9.6194086 -7.1797519 -5.4300427 -6.2508645 -7.1144991 -8.060523 -9.7356024 -11.989701 -12.756229 -11.89633 -9.9455452][-15.47501 -15.295675 -13.452364 -9.813446 -7.6121974 -7.0802641 -6.542202 -5.9797759 -6.4030328 -7.0243578 -7.9471073 -8.4193459 -9.408987 -9.4707594 -8.154233][-10.386975 -10.131501 -9.7183361 -7.6729236 -6.1322913 -5.5196295 -5.6003 -6.782073 -6.5281625 -5.9836826 -6.3189888 -7.0199752 -6.9394269 -7.3170919 -7.0916152]]...]
INFO - root - 2017-12-15 16:25:09.970196: step 33210, loss = 0.18, batch loss = 0.14 (12.6 examples/sec; 0.636 sec/batch; 52h:54m:53s remains)
INFO - root - 2017-12-15 16:25:16.516270: step 33220, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 54h:49m:12s remains)
INFO - root - 2017-12-15 16:25:23.086797: step 33230, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 54h:28m:18s remains)
INFO - root - 2017-12-15 16:25:29.715759: step 33240, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 55h:35m:04s remains)
INFO - root - 2017-12-15 16:25:36.301095: step 33250, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 54h:45m:00s remains)
INFO - root - 2017-12-15 16:25:42.851003: step 33260, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 55h:30m:14s remains)
INFO - root - 2017-12-15 16:25:49.433569: step 33270, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 55h:53m:40s remains)
INFO - root - 2017-12-15 16:25:56.081294: step 33280, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.686 sec/batch; 57h:00m:50s remains)
INFO - root - 2017-12-15 16:26:02.813200: step 33290, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 55h:51m:08s remains)
INFO - root - 2017-12-15 16:26:09.442516: step 33300, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 52h:47m:04s remains)
2017-12-15 16:26:09.971660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1991453 -1.8624852 -3.2332251 -3.524533 -4.51599 -5.4725561 -5.8078723 -6.37268 -7.4929543 -6.9260182 -5.8874335 -6.5455222 -8.1143684 -8.7674694 -8.3109875][-3.3904328 -4.4386911 -5.1027169 -5.1636181 -6.34622 -7.3905163 -8.1391525 -8.8737125 -9.1005192 -8.5041323 -7.3894372 -6.45129 -7.6861148 -9.15097 -8.27009][-5.09336 -6.96698 -8.5071783 -9.0329571 -9.1497364 -8.636137 -8.2302456 -8.7333851 -9.0793991 -8.4475374 -7.9469209 -8.1632214 -9.5082455 -9.43965 -9.0628939][-7.3523273 -8.8906488 -10.486217 -10.202995 -8.876483 -7.2252855 -5.5691938 -5.8378215 -6.8111467 -6.4928703 -6.1236854 -7.19695 -9.737483 -10.6644 -10.597628][-7.9435153 -9.8863163 -11.227325 -9.2543974 -7.7579508 -4.7645054 -2.612838 -2.5884395 -3.0456698 -3.6191566 -3.8715489 -5.1773729 -8.0419264 -10.457417 -10.547945][-7.1479774 -8.0527706 -8.5033054 -6.8003378 -4.7902989 -0.92482281 1.3192863 1.2813139 0.99529648 -0.18844986 -1.154634 -1.7164311 -5.0494428 -8.0732288 -9.3953056][-6.2697945 -7.1610603 -6.82465 -4.5776672 -2.1587493 0.98357677 3.8228078 4.5467381 4.8098311 2.7308841 0.9106369 -0.72116852 -4.2788048 -6.375679 -8.1312428][-5.6347528 -5.8513846 -5.7881813 -3.3518608 -1.0715051 1.818182 4.3551192 4.4784074 4.32921 2.6183877 1.0288739 -0.22892809 -3.9354196 -6.574604 -7.2713737][-4.4873333 -5.1245213 -5.8341117 -4.1588783 -2.7016082 0.35833836 2.3328452 2.852181 3.0688148 1.8555198 0.78741074 -0.85479307 -4.5555944 -6.5915279 -7.4999781][-3.1115291 -4.238308 -4.9877439 -3.6074255 -2.5297263 -1.0790586 0.27905512 0.37435007 0.31977749 0.26234293 -0.251976 -1.2996125 -4.0758219 -5.5163646 -7.7583036][-7.7222452 -7.7861528 -7.5326881 -6.595108 -5.1886687 -4.1508226 -3.1839612 -2.9132986 -2.583545 -2.9565163 -3.6100271 -4.6598063 -5.9385834 -7.6151133 -7.7637224][-12.0695 -11.022497 -8.36618 -6.1940479 -4.6819515 -4.3370438 -5.0410337 -6.1318994 -6.3752441 -6.7133255 -7.015336 -8.2046909 -8.1437874 -8.8309669 -9.0769119][-11.630774 -10.601194 -8.7383785 -7.0000348 -6.3764234 -5.623086 -6.7807274 -7.6561503 -7.7153611 -8.8961878 -9.2949581 -9.2452459 -9.1829519 -8.1243191 -6.7620363][-9.1985693 -7.4532318 -6.4213705 -4.8726673 -4.3496866 -5.4915843 -6.7621274 -6.5699415 -7.4704523 -7.6504874 -8.045063 -8.57818 -8.03553 -7.683219 -7.6356945][-4.9292951 -4.72599 -4.0114021 -2.7434726 -2.2535563 -2.9245281 -3.95826 -5.4130125 -6.1267705 -6.6651583 -7.4043369 -8.1472721 -8.7194166 -7.4665856 -6.4073744]]...]
INFO - root - 2017-12-15 16:26:16.455701: step 33310, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 56h:28m:38s remains)
INFO - root - 2017-12-15 16:26:23.046996: step 33320, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 55h:32m:27s remains)
INFO - root - 2017-12-15 16:26:29.589595: step 33330, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 53h:52m:35s remains)
INFO - root - 2017-12-15 16:26:36.215079: step 33340, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 54h:19m:59s remains)
INFO - root - 2017-12-15 16:26:42.866806: step 33350, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.696 sec/batch; 57h:52m:00s remains)
INFO - root - 2017-12-15 16:26:49.411754: step 33360, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 55h:11m:21s remains)
INFO - root - 2017-12-15 16:26:55.999425: step 33370, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 55h:44m:53s remains)
INFO - root - 2017-12-15 16:27:02.556007: step 33380, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 55h:32m:17s remains)
INFO - root - 2017-12-15 16:27:09.174734: step 33390, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.681 sec/batch; 56h:33m:25s remains)
INFO - root - 2017-12-15 16:27:15.788509: step 33400, loss = 0.21, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 53h:25m:52s remains)
2017-12-15 16:27:16.288872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0818081 -4.2266083 -3.984024 -2.9906313 -3.5588408 -3.6038635 -3.9164264 -3.5720761 -3.2827327 -3.2186472 -3.1219945 -5.3100543 -6.6598825 -8.5424614 -8.4871893][-5.3228512 -4.7289448 -4.6223989 -3.9648962 -3.8556411 -3.9063258 -3.7767019 -3.1358962 -2.697921 -2.3185177 -2.6087451 -4.8160176 -6.4037519 -9.08723 -9.3314161][-4.2684937 -4.523293 -4.499454 -3.9379997 -3.9352863 -3.465323 -2.9002831 -1.9969971 -1.5983777 -2.3484006 -2.8869271 -4.6022453 -5.7710342 -8.48417 -8.56745][-5.0062351 -5.1619306 -4.3003283 -3.3441241 -3.6418076 -2.848531 -1.8579264 -1.2436805 -1.3773847 -1.9789817 -2.4154544 -4.387578 -5.7402844 -8.2207451 -7.8496408][-4.6348562 -5.0385056 -4.8557673 -3.7622154 -3.1703825 -1.3028135 0.12293863 0.5956068 0.76258755 -0.42151213 -1.1753006 -3.4959989 -4.7254696 -7.2137375 -7.2934623][-5.2924962 -5.5468931 -4.446681 -1.8104804 -0.036344528 1.6887102 2.6545367 3.2147059 3.0197263 1.1258645 -0.22658539 -2.0161419 -3.5078328 -6.7296686 -6.361639][-6.4968739 -5.961504 -4.6713524 -1.7997196 0.58130169 3.4094071 5.7612023 6.4437652 5.9245839 3.9255462 1.6036286 -1.8351741 -3.80767 -6.6015282 -6.4532342][-6.3420367 -6.2994132 -5.0269151 -1.9065461 0.6674614 4.8952508 6.9876208 6.75114 6.4869008 4.8462949 2.7121539 -1.203886 -3.5605423 -6.872766 -7.0268888][-5.5082054 -5.4085474 -4.268569 -1.5501981 0.76135492 3.8325019 5.0438342 6.0437236 6.286531 4.1466594 2.1796975 -1.4422488 -4.5219612 -7.5468311 -7.5617285][-4.4135036 -4.2215486 -3.938422 -1.4008203 -0.00898695 2.1610923 3.6668763 4.4783692 4.1125083 2.4773059 1.2983499 -2.2263682 -5.0313005 -8.9100552 -9.8399963][-6.9309487 -5.9777331 -5.0725794 -3.3439686 -2.5939918 -1.4784646 -1.4487143 -1.2236981 -1.355032 -2.0735705 -3.0489852 -6.2625952 -7.764946 -10.068678 -9.85194][-9.6897907 -7.7256155 -6.4900136 -4.79172 -3.6301591 -3.5468535 -4.1206226 -4.67949 -5.253406 -6.3093257 -6.8874 -8.1126041 -8.71402 -10.847822 -10.49123][-9.3302717 -7.3949709 -5.8705382 -4.907464 -4.9128528 -4.6526446 -5.5765634 -6.2281294 -7.2291851 -8.2305241 -8.7864408 -9.3871861 -9.4365454 -10.470072 -8.6592083][-7.0921745 -7.0313454 -6.299355 -5.4503775 -4.9265051 -5.5312843 -6.37607 -6.5581756 -7.2247124 -8.1480389 -8.947298 -8.1897659 -7.2827082 -7.9512267 -6.8171577][-6.0956068 -4.7524576 -4.8685846 -4.1387773 -3.9307103 -4.5041356 -5.0536375 -6.40613 -7.6059165 -7.552248 -7.4454336 -8.1329975 -8.1142035 -7.710628 -7.1750283]]...]
INFO - root - 2017-12-15 16:27:22.934910: step 33410, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.690 sec/batch; 57h:20m:15s remains)
INFO - root - 2017-12-15 16:27:29.480104: step 33420, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 56h:07m:59s remains)
INFO - root - 2017-12-15 16:27:36.107014: step 33430, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 55h:37m:39s remains)
INFO - root - 2017-12-15 16:27:42.759088: step 33440, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 53h:04m:12s remains)
INFO - root - 2017-12-15 16:27:49.309400: step 33450, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 53h:57m:26s remains)
INFO - root - 2017-12-15 16:27:55.931404: step 33460, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 54h:03m:05s remains)
INFO - root - 2017-12-15 16:28:02.548044: step 33470, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.686 sec/batch; 56h:58m:24s remains)
INFO - root - 2017-12-15 16:28:09.092861: step 33480, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.694 sec/batch; 57h:37m:48s remains)
INFO - root - 2017-12-15 16:28:15.729420: step 33490, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 54h:50m:00s remains)
INFO - root - 2017-12-15 16:28:22.294326: step 33500, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.686 sec/batch; 56h:56m:25s remains)
2017-12-15 16:28:22.787550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9381452 -5.7695594 -5.9617758 -5.4043875 -5.3194494 -5.6723814 -4.7729635 -5.1285949 -6.5044093 -8.1099119 -8.4352341 -7.9629517 -7.6828356 -7.7916656 -8.4416132][-5.1086035 -5.4615235 -5.1736512 -5.1125894 -4.9657183 -5.32833 -5.4500217 -5.455689 -5.8247633 -5.965312 -6.7844648 -6.4351177 -6.7088408 -6.4969845 -7.2648277][-3.4385421 -3.3558803 -4.6764894 -5.4240232 -5.4210529 -6.2125583 -6.0363259 -5.9192467 -6.4622726 -6.6266575 -6.0960655 -5.5321932 -6.0106483 -5.3331752 -5.693305][-3.7756042 -4.8968487 -3.7011232 -4.6584735 -5.8973641 -5.3094039 -4.9337454 -5.7871747 -6.0132408 -5.7609982 -5.296102 -4.4321938 -5.2372894 -5.831912 -6.6158404][-2.9228923 -4.8180127 -5.7428508 -5.4144497 -5.4093204 -4.7314191 -2.8538849 -3.0141907 -4.9765878 -5.0368223 -5.0715218 -4.1817842 -4.3359385 -5.6326795 -6.8121529][-5.6934171 -5.9600554 -6.1126833 -4.7371197 -3.8994188 -2.4489446 -0.33008718 -0.60632992 -0.93492413 -3.3684325 -5.141346 -4.4351015 -4.9668236 -5.7377028 -6.6416326][-5.9575586 -4.4618564 -4.6397114 -3.1257794 -0.93975067 1.6104817 3.3634019 3.4515824 2.4495893 1.1585131 -1.862675 -3.8500862 -3.9958801 -4.8077726 -6.4099631][-5.9733062 -4.8405375 -3.8674963 -2.2930253 -0.48796463 1.9335232 4.2806878 4.6498408 3.2907233 2.420857 0.29650211 -1.7806039 -3.761065 -5.0115662 -5.6511512][-5.2111397 -4.7655392 -4.2536387 -2.0181718 -1.2132454 0.070003033 2.2411284 2.318213 1.2491097 0.77084494 0.7714324 -0.69821882 -2.4857156 -4.1259222 -5.4140987][-6.4498348 -5.6860828 -4.877099 -3.6246743 -2.4463525 -1.0170679 -0.65867853 -1.3894739 -2.7547996 -2.5125144 -1.4875507 -2.7069705 -2.9151211 -4.0242095 -5.2554617][-11.282776 -9.7095413 -8.9877472 -6.7442307 -5.7462139 -4.9945636 -4.373435 -6.0864768 -6.8184543 -6.2428279 -4.6284618 -4.6633325 -5.687294 -5.618093 -5.4088659][-14.842783 -13.429749 -11.050133 -9.29887 -7.5834484 -6.444838 -6.0700135 -7.2013941 -8.7931318 -8.22188 -6.2244992 -4.8485317 -6.360075 -6.442246 -4.8889294][-12.972547 -12.164112 -9.4920263 -7.1394014 -6.0027356 -4.5707541 -4.6892772 -6.2526455 -7.0454445 -7.0105143 -5.3113546 -4.018127 -4.222929 -4.493618 -4.7358055][-10.345196 -8.7915173 -6.8463774 -3.8835931 -2.8804841 -3.6596036 -4.3587456 -4.2755666 -4.5504045 -5.4420986 -3.260283 -3.0749559 -3.2560678 -1.9739316 -1.70403][-7.7616014 -9.2389107 -7.0388765 -3.7134628 -3.9795916 -3.5813191 -4.9895639 -6.6023812 -6.5241661 -5.2648091 -4.18728 -4.2713575 -5.2085233 -4.0448422 -3.3488104]]...]
INFO - root - 2017-12-15 16:28:29.336167: step 33510, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 55h:55m:29s remains)
INFO - root - 2017-12-15 16:28:35.822018: step 33520, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 55h:11m:11s remains)
INFO - root - 2017-12-15 16:28:42.390920: step 33530, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 54h:08m:40s remains)
INFO - root - 2017-12-15 16:28:49.025643: step 33540, loss = 0.19, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 52h:40m:40s remains)
INFO - root - 2017-12-15 16:28:55.581343: step 33550, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 56h:43m:15s remains)
INFO - root - 2017-12-15 16:29:02.159403: step 33560, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 54h:41m:28s remains)
INFO - root - 2017-12-15 16:29:08.717007: step 33570, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 54h:08m:19s remains)
INFO - root - 2017-12-15 16:29:15.298365: step 33580, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.669 sec/batch; 55h:34m:03s remains)
INFO - root - 2017-12-15 16:29:21.924507: step 33590, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.635 sec/batch; 52h:44m:55s remains)
INFO - root - 2017-12-15 16:29:28.476859: step 33600, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.652 sec/batch; 54h:09m:50s remains)
2017-12-15 16:29:29.006841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-11.85239 -12.902527 -13.120501 -13.723225 -14.357058 -14.123407 -13.234483 -11.180699 -9.46192 -8.8052559 -9.3807993 -10.470921 -12.287573 -11.984559 -9.8442364][-11.040712 -10.721883 -10.372362 -11.409011 -12.518211 -13.536695 -13.304567 -12.506844 -11.976439 -10.807913 -10.420727 -11.302755 -12.548307 -12.672203 -11.726938][-5.0632849 -6.0832434 -7.6640658 -10.041018 -11.172436 -11.219713 -10.517578 -9.613102 -9.5161829 -9.039113 -8.2906408 -8.7641487 -10.422602 -11.545164 -11.55911][-3.33978 -2.5987351 -3.3643358 -4.9898434 -6.6181045 -6.8053584 -6.042419 -6.25325 -7.0499763 -7.3092546 -7.4061389 -7.8497419 -8.6751938 -9.616725 -9.6981869][-3.8353295 -4.2115664 -4.9008045 -3.8677893 -3.4418831 -1.989429 -0.42059517 -1.8934426 -3.7364779 -4.1721106 -4.8971362 -6.6336451 -8.5385494 -9.9565945 -9.1303139][-6.1893845 -6.2982235 -6.0114164 -4.5651975 -3.2040131 0.62366343 3.6516805 2.7757773 1.7185793 -0.4565258 -2.1949391 -3.602205 -6.0301242 -7.4758387 -7.1886506][-8.2091208 -8.4816484 -7.5175 -4.6680732 -2.0515115 1.7157373 4.8606906 5.7976527 6.6205764 4.3729014 1.3657222 -1.0218787 -4.3144817 -6.1703029 -5.6736689][-7.6257772 -7.8563852 -6.9995627 -4.1612396 -0.4809761 3.0119987 5.3915277 5.9476247 6.4402051 4.6052804 1.8642907 -0.97695732 -4.1921196 -5.0634465 -4.2326317][-4.96182 -4.9082365 -4.5403781 -3.056175 -0.64621353 2.0771089 3.6646791 4.7334819 4.9666028 3.6583257 1.7688622 -1.323194 -4.4893351 -5.8649735 -4.8113875][-4.4084845 -3.5489 -2.1688557 -1.5210276 -0.78979778 0.47690105 1.6259332 2.0779142 1.1503968 0.011962414 -0.71442556 -2.5081537 -4.6540966 -5.0011406 -4.2244968][-6.6605248 -4.8430619 -4.3182106 -4.231009 -3.9247255 -2.5726111 -0.88519096 0.29444075 -1.0101209 -2.8378909 -3.640058 -4.9869456 -7.19477 -7.30827 -5.4851403][-9.445672 -7.5506163 -6.3192439 -6.1525593 -6.4592581 -5.7655511 -4.6663556 -3.932277 -4.35789 -4.8837843 -4.7179842 -4.8816872 -5.7591786 -6.1408987 -5.9061427][-10.368439 -9.44062 -8.7477264 -8.010376 -7.2439156 -5.7786307 -4.8096547 -4.470118 -5.0080867 -5.6879492 -5.7313762 -5.8254824 -6.2641287 -6.0162258 -4.7403941][-9.0419807 -8.3451881 -8.15896 -7.5544238 -7.1479945 -5.9581342 -5.189362 -5.1812472 -5.4238038 -5.1944861 -5.314146 -4.9073057 -4.3634214 -4.3085933 -4.6446586][-6.6603251 -5.6750479 -5.2313557 -5.0146942 -4.1215334 -3.5298645 -3.5448308 -3.9886785 -5.1035829 -5.8587723 -6.2171 -6.8849897 -8.0594311 -7.9460554 -6.7914839]]...]
INFO - root - 2017-12-15 16:29:35.642635: step 33610, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 55h:14m:37s remains)
INFO - root - 2017-12-15 16:29:42.215938: step 33620, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 55h:37m:05s remains)
INFO - root - 2017-12-15 16:29:48.820903: step 33630, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 55h:27m:53s remains)
INFO - root - 2017-12-15 16:29:55.495803: step 33640, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 54h:44m:55s remains)
INFO - root - 2017-12-15 16:30:02.022244: step 33650, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.649 sec/batch; 53h:51m:41s remains)
INFO - root - 2017-12-15 16:30:08.552615: step 33660, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 55h:27m:47s remains)
INFO - root - 2017-12-15 16:30:15.079659: step 33670, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 55h:11m:45s remains)
INFO - root - 2017-12-15 16:30:21.644191: step 33680, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 56h:34m:14s remains)
INFO - root - 2017-12-15 16:30:28.154852: step 33690, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.637 sec/batch; 52h:53m:05s remains)
INFO - root - 2017-12-15 16:30:34.694197: step 33700, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 54h:33m:50s remains)
2017-12-15 16:30:35.202539: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9688911 -6.0546122 -5.9016223 -5.7878928 -6.7772303 -7.7074132 -8.3413143 -8.4095325 -8.53908 -8.7637806 -8.9915686 -11.012598 -13.369243 -13.332443 -13.732769][-5.3765106 -5.9656549 -5.4221382 -5.78894 -6.8526788 -8.2061806 -9.5024443 -10.360798 -10.809418 -10.72963 -10.710896 -12.284218 -13.635894 -13.741636 -14.609783][-4.4654894 -5.9683161 -6.4672346 -6.6119823 -7.4687634 -8.6153812 -9.7109394 -10.317913 -11.094574 -11.794993 -11.785219 -13.093088 -14.655153 -13.926924 -14.123833][-3.010407 -4.1986952 -4.9470863 -5.5578322 -6.2599607 -6.7216082 -7.1343656 -7.7272959 -8.2459469 -8.2715759 -8.6849871 -10.781331 -12.590261 -12.429371 -13.524563][-4.1746397 -5.4525805 -5.3957868 -5.4297485 -5.0392718 -4.6734467 -4.3124862 -4.3526077 -4.9621525 -4.9501238 -4.9624758 -6.6927371 -8.88657 -9.2169819 -10.616313][-5.0164671 -6.29597 -5.5507483 -4.6126695 -3.2796772 -1.3741884 0.85371733 0.67425346 -0.16574764 -1.0519071 -1.6131659 -3.7181053 -6.3403673 -6.7828937 -8.0048237][-6.7969232 -7.1255531 -5.7267127 -3.2966559 -1.4550271 0.90760374 3.7922025 4.9092021 4.7056527 2.516645 0.31094503 -2.208842 -4.6972156 -5.1095419 -7.0024176][-7.3248043 -6.9792676 -5.7193842 -2.516701 0.33991861 3.7419686 5.7243028 5.8364959 6.0596662 5.313827 2.9570775 -1.3114009 -5.6496673 -5.829145 -7.5307751][-5.0469503 -5.5050054 -4.9593439 -2.6821415 -0.60524988 2.3891244 4.238605 4.9688268 5.3065467 4.1812949 2.4582648 -1.9912007 -7.2544537 -8.7738819 -10.858444][-2.5924127 -3.3855503 -3.3451064 -2.3265121 -1.1779227 -0.2568202 0.44741726 0.54152775 0.7141118 0.82610989 0.925354 -2.8430121 -7.4432039 -9.5744429 -12.8456][-4.8860273 -5.6988654 -6.0028987 -5.09689 -4.6322932 -4.539578 -4.1474738 -4.19116 -4.424777 -4.9541068 -5.0378723 -7.6873426 -9.6130714 -11.666105 -13.405742][-8.1396427 -8.7927408 -10.274428 -10.378534 -10.076511 -9.8207951 -9.8213367 -10.377512 -10.221082 -9.4431858 -9.4024591 -11.286831 -12.16669 -13.125771 -13.692873][-13.061319 -12.784411 -13.4778 -13.256971 -13.191837 -13.193073 -13.281067 -13.552599 -13.316424 -12.720673 -12.110963 -12.060099 -12.774177 -12.665236 -12.714808][-13.550449 -13.548124 -12.574588 -11.323082 -11.456744 -11.731569 -12.289915 -12.537677 -12.317412 -11.409311 -10.233984 -9.3368893 -9.5978985 -9.8661184 -11.160691][-9.9720135 -10.548656 -9.5741024 -7.6858473 -6.2363191 -6.0865173 -7.120903 -8.0580235 -8.4717274 -8.3295746 -7.67615 -7.5792537 -7.469265 -7.6380262 -8.5452518]]...]
INFO - root - 2017-12-15 16:30:41.825067: step 33710, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 53h:54m:55s remains)
INFO - root - 2017-12-15 16:30:48.425575: step 33720, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 53h:46m:23s remains)
INFO - root - 2017-12-15 16:30:54.951805: step 33730, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 54h:06m:17s remains)
INFO - root - 2017-12-15 16:31:01.441266: step 33740, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.658 sec/batch; 54h:38m:52s remains)
INFO - root - 2017-12-15 16:31:07.969108: step 33750, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 53h:52m:09s remains)
INFO - root - 2017-12-15 16:31:14.487897: step 33760, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 53h:21m:49s remains)
INFO - root - 2017-12-15 16:31:21.112337: step 33770, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 54h:17m:46s remains)
INFO - root - 2017-12-15 16:31:27.688105: step 33780, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 53h:20m:02s remains)
INFO - root - 2017-12-15 16:31:34.209028: step 33790, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.670 sec/batch; 55h:37m:06s remains)
INFO - root - 2017-12-15 16:31:40.788876: step 33800, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 54h:03m:55s remains)
2017-12-15 16:31:41.306424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5546436 -1.4360962 -1.3200521 -0.70796633 -0.73495674 -1.3571258 -2.0289948 -2.6202111 -3.1024795 -3.4170456 -3.2510979 -5.5600843 -6.8020368 -9.1809406 -9.7974825][-2.4554782 -1.6218925 -0.81276083 0.076483727 -0.25719881 -1.1675467 -2.4318933 -2.4722903 -3.1742923 -3.3674681 -2.8611732 -6.2066083 -7.4161744 -9.3997574 -10.68561][-0.94025469 -1.7224805 -1.9516995 -1.2895126 -1.5470171 -1.300118 -0.7955904 -1.5545721 -2.3664279 -3.1087732 -4.4115582 -6.3758121 -6.9691658 -8.3311338 -8.8314533][-1.0824461 -1.3827806 -1.008255 -0.74684429 -1.5173559 -1.7574992 -1.7696676 -1.6618652 -2.150219 -2.9276023 -3.9004316 -7.0272713 -8.0089493 -9.19669 -8.7490444][-1.8018589 -2.3980525 -2.0064797 -0.46858406 -0.39983368 0.29089642 0.72858572 -1.0866404 -2.2489102 -2.8478084 -3.5858319 -6.4893861 -7.1184788 -8.6713657 -9.1764927][-2.8112216 -2.8108423 -1.8044848 0.42718172 0.622149 1.959682 3.13246 2.2983289 0.49962521 -1.5020447 -2.7774317 -5.5154634 -7.1862688 -9.3092041 -10.042948][-4.0453477 -3.7251625 -2.4679523 0.46747971 2.1343412 3.2565484 3.2478261 3.962069 4.0449195 1.3966632 -1.0816412 -4.462801 -6.294054 -8.9790592 -10.386667][-3.5298262 -3.8616767 -3.2148085 -1.1352086 1.2716732 4.5215707 5.547111 5.0265317 4.6572909 2.686645 1.6284733 -2.3562546 -5.0975838 -7.879385 -9.8759][-2.4993935 -2.389972 -2.0813093 -1.0175681 0.0103755 1.9805446 3.3160958 4.05533 3.9628568 2.7552962 1.6955576 -1.75944 -3.9651392 -7.0905523 -8.5821323][-2.1658304 -2.7434688 -3.0289648 -1.1902952 -0.28008175 0.047987461 0.69933939 2.1034226 2.3356185 1.4575105 0.99518585 -2.5589314 -4.0241675 -6.3607941 -8.5363626][-5.4983106 -5.487113 -5.3502045 -3.5722356 -2.9597538 -2.6890647 -3.2467742 -2.90824 -1.729562 -1.2980199 -1.6748095 -5.1868386 -6.5208359 -7.1205463 -7.5074329][-9.6556892 -8.7002945 -7.4179831 -6.3074827 -6.2885971 -5.7374673 -5.8931379 -5.8596087 -5.0018072 -4.0468483 -4.0760608 -6.1779976 -7.5019836 -8.6910267 -8.1755791][-11.808119 -11.743216 -9.8177948 -8.1411295 -7.693778 -8.1438141 -8.4911089 -8.5939 -8.0143337 -6.9618669 -5.7549243 -6.9115572 -8.7805262 -8.8639488 -7.6755233][-10.439528 -10.438242 -9.6025095 -8.8064423 -8.4634418 -8.4262152 -8.03675 -7.354136 -7.0925832 -7.0738049 -6.3339124 -5.7132277 -6.3716965 -7.3481083 -6.8915815][-7.992197 -8.5022631 -8.17819 -7.8775291 -7.6377454 -7.5329785 -7.0849705 -6.7301235 -7.049675 -7.207849 -6.6877937 -7.6893826 -8.3898678 -7.9201727 -8.2799015]]...]
INFO - root - 2017-12-15 16:31:47.920117: step 33810, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 54h:38m:14s remains)
INFO - root - 2017-12-15 16:31:54.512622: step 33820, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 54h:34m:46s remains)
INFO - root - 2017-12-15 16:32:01.109941: step 33830, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 54h:48m:06s remains)
INFO - root - 2017-12-15 16:32:07.736668: step 33840, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 55h:32m:40s remains)
INFO - root - 2017-12-15 16:32:14.249285: step 33850, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 52h:54m:33s remains)
INFO - root - 2017-12-15 16:32:20.855969: step 33860, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 54h:49m:19s remains)
INFO - root - 2017-12-15 16:32:27.465234: step 33870, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 54h:43m:33s remains)
INFO - root - 2017-12-15 16:32:34.163460: step 33880, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 55h:40m:01s remains)
INFO - root - 2017-12-15 16:32:40.727439: step 33890, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 56h:16m:49s remains)
INFO - root - 2017-12-15 16:32:47.282076: step 33900, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.636 sec/batch; 52h:47m:34s remains)
2017-12-15 16:32:47.780974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3279924 -3.3228924 -3.4042344 -4.0285215 -4.8557463 -5.3122845 -6.1867261 -5.8867917 -5.8173823 -6.4255657 -7.2785378 -8.666832 -10.923923 -9.5260887 -7.826128][-2.7035096 -3.029599 -3.171263 -3.5637488 -3.6633182 -3.95081 -3.2847846 -3.5659678 -3.7227952 -4.3629165 -4.3391781 -5.8390427 -7.9601965 -7.6649289 -6.7183976][-2.6395006 -3.4470975 -4.2703333 -3.7531452 -4.8138728 -5.1922288 -4.8582029 -3.8457279 -3.1126397 -4.1076241 -3.5535696 -4.7508097 -6.4758606 -5.7835846 -4.6203508][-4.5374146 -4.6854463 -5.572691 -5.1898308 -5.0986962 -4.9247937 -4.16734 -3.9147539 -3.6555538 -3.0461395 -2.5976508 -3.7264223 -5.9903803 -5.5187726 -4.7754383][-5.1831684 -6.6634226 -7.0401964 -6.9153481 -6.2646346 -4.7699342 -3.5155177 -2.6666653 -2.0528581 -1.7723835 -0.72344732 -3.1111646 -5.3041072 -5.4813857 -5.6500554][-5.4088745 -5.2507982 -5.1946025 -3.534193 -2.455406 -0.73671818 0.8604908 1.3736615 0.47560453 -0.10271931 -0.656939 -2.1397665 -4.94382 -5.6954818 -6.1038218][-4.8757668 -5.0671411 -3.9482775 -1.0136847 1.0905404 2.9061942 4.4648013 4.9180655 4.5347533 2.2864833 0.85867691 -1.7578487 -4.9369044 -5.8573251 -6.078248][-3.5167062 -3.190383 -3.2409163 -0.54420996 2.1443005 4.4765544 6.6007972 6.4513354 5.7275386 3.8302341 2.0007505 -0.82850504 -3.5873246 -4.5970869 -4.9298162][-4.04058 -3.0146489 -1.9449584 -0.42042446 0.63315248 2.692472 4.7452826 5.4195132 4.3080478 2.8206267 1.1810379 -1.6194954 -4.8808904 -4.9228182 -4.9547019][-5.5325627 -4.4259419 -3.3169856 -0.75355148 -0.33656883 1.090065 2.8077998 2.8874774 1.9848828 0.68031979 -0.16042757 -2.8427756 -5.8036413 -6.3237619 -6.0758519][-8.5733051 -7.6473985 -7.153193 -5.0842333 -3.8708689 -2.5208828 -1.9634366 -1.7616546 -2.202672 -3.1881342 -4.093143 -6.3197613 -7.58375 -7.634923 -6.929925][-11.245056 -10.126657 -8.7966194 -7.5716829 -7.0111737 -5.9153657 -5.4832435 -6.5021305 -6.7295051 -7.0590439 -7.4548025 -8.3711872 -8.307375 -7.9619646 -6.8302474][-12.080339 -10.588537 -8.9688921 -7.4186821 -7.385726 -6.7241216 -7.195415 -7.6793618 -8.31518 -8.7403126 -8.8607569 -9.0289993 -8.5176029 -7.5398932 -5.910913][-9.5585537 -8.7957077 -7.3803916 -5.8287673 -5.2428789 -5.4254045 -5.5710497 -5.9913735 -6.4613724 -6.7679338 -6.9794245 -6.5096726 -6.5651526 -6.0096669 -5.18435][-6.4732323 -6.5791831 -6.3497429 -5.2704005 -3.6495769 -3.8598194 -3.5222549 -3.6837239 -4.0038042 -4.40293 -4.3156605 -5.4790711 -6.0630655 -6.1767535 -6.1097984]]...]
INFO - root - 2017-12-15 16:32:54.332394: step 33910, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 55h:40m:09s remains)
INFO - root - 2017-12-15 16:33:00.880101: step 33920, loss = 0.17, batch loss = 0.12 (12.8 examples/sec; 0.624 sec/batch; 51h:47m:12s remains)
INFO - root - 2017-12-15 16:33:07.506646: step 33930, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 54h:20m:08s remains)
INFO - root - 2017-12-15 16:33:14.073183: step 33940, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 55h:20m:35s remains)
INFO - root - 2017-12-15 16:33:20.659984: step 33950, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 55h:35m:01s remains)
INFO - root - 2017-12-15 16:33:27.195870: step 33960, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 54h:17m:51s remains)
INFO - root - 2017-12-15 16:33:33.766154: step 33970, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.658 sec/batch; 54h:36m:11s remains)
INFO - root - 2017-12-15 16:33:40.288902: step 33980, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 54h:50m:56s remains)
INFO - root - 2017-12-15 16:33:46.899685: step 33990, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 54h:18m:36s remains)
INFO - root - 2017-12-15 16:33:53.547642: step 34000, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 54h:48m:47s remains)
2017-12-15 16:33:54.113582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7110577 -6.7049537 -5.8368897 -4.1697669 -4.0389085 -3.6619794 -2.7531712 -1.4480028 -0.98621416 -0.080424309 -0.6296587 -4.6721373 -9.1310263 -10.533504 -8.85654][-6.5586486 -7.9987297 -8.4866648 -7.4614358 -5.6167397 -3.9247682 -2.6297166 -1.0552282 -0.93907309 -1.3324575 -1.8821526 -5.054493 -9.8253365 -10.186023 -9.6726332][-5.0671277 -6.4658923 -8.3324356 -7.8532677 -7.4490633 -5.521565 -2.550885 -1.4103489 -1.449069 -2.4992728 -3.8968596 -7.4692636 -10.904347 -11.571727 -9.9001732][-3.960943 -5.3961973 -6.4392838 -6.0815973 -6.7058125 -5.6036711 -4.2138948 -2.601681 -1.8000984 -2.8902822 -4.6296926 -9.2843876 -13.23778 -12.11541 -9.8505678][-5.1591291 -5.2044539 -5.4076715 -4.5694976 -4.5639753 -2.933455 -1.5776467 -2.2748098 -2.657299 -3.1499724 -4.5431361 -8.4727182 -12.580875 -12.279629 -10.040085][-8.38412 -6.3851032 -6.0380397 -4.5230107 -3.5958896 -0.7922821 0.99587393 0.41789198 -0.84720659 -3.2360735 -5.1802325 -8.4432888 -11.866428 -12.489451 -10.33125][-8.76498 -7.6479316 -7.2175035 -5.1604772 -3.4363832 -0.24625063 2.8877845 3.8513122 2.7168193 -1.032402 -4.9656172 -7.7155304 -10.214405 -10.315735 -9.8770094][-6.9407954 -6.4907365 -6.7376633 -4.0763931 -1.7301469 1.1498413 4.0753722 5.0129743 4.6495442 1.8370562 -1.6966171 -6.7822957 -11.085113 -10.824844 -9.4990234][-6.196269 -4.8013663 -4.4479113 -2.3989272 -1.7704349 1.2745404 3.7263217 5.2204537 4.9407792 1.334332 -1.7052312 -6.9050851 -11.631766 -12.397826 -11.0215][-5.4083605 -5.0549555 -4.0372515 -2.2449353 -1.4849553 -0.188066 0.73596525 2.5367951 2.8635669 1.328464 -1.1076717 -6.752697 -11.50448 -12.740431 -11.280907][-6.4962339 -7.2632389 -7.4956751 -6.2014976 -4.4996643 -3.6061649 -2.3965769 -1.2871151 -1.174953 -1.5226126 -2.3606312 -7.7732158 -11.822912 -13.645247 -12.471647][-10.342856 -8.7079411 -7.7012649 -7.1302347 -7.1773529 -6.5595717 -6.575491 -6.9418631 -6.8876476 -6.2743492 -6.6731219 -8.70607 -10.368501 -11.101894 -9.9705944][-13.263571 -11.233475 -8.2547235 -7.8132243 -8.0958624 -8.4975529 -9.2381134 -8.5069122 -8.0502119 -7.6022735 -7.6901026 -9.2958212 -11.617316 -10.868591 -8.8143253][-12.320099 -10.323029 -7.8743806 -7.3703766 -7.2805872 -6.4950905 -6.757493 -7.4563894 -7.2041259 -7.9500237 -7.9498472 -6.5917687 -7.0025644 -8.9020033 -9.2869139][-8.5210228 -6.7159133 -5.9419236 -5.2087917 -5.4945288 -6.2744055 -6.2528834 -5.4077368 -5.1910615 -6.0365725 -6.691906 -8.7868137 -9.2713413 -8.2763977 -8.9912539]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 16:34:00.718476: step 34010, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 55h:20m:54s remains)
INFO - root - 2017-12-15 16:34:07.309171: step 34020, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 54h:47m:11s remains)
INFO - root - 2017-12-15 16:34:13.959548: step 34030, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 54h:16m:02s remains)
INFO - root - 2017-12-15 16:34:20.608637: step 34040, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 55h:01m:08s remains)
INFO - root - 2017-12-15 16:34:27.253137: step 34050, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 53h:36m:47s remains)
INFO - root - 2017-12-15 16:34:33.825199: step 34060, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 54h:33m:08s remains)
INFO - root - 2017-12-15 16:34:40.377228: step 34070, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 54h:56m:34s remains)
INFO - root - 2017-12-15 16:34:47.026310: step 34080, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 53h:40m:03s remains)
INFO - root - 2017-12-15 16:34:53.601772: step 34090, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 53h:35m:01s remains)
INFO - root - 2017-12-15 16:35:00.152023: step 34100, loss = 0.15, batch loss = 0.10 (12.8 examples/sec; 0.627 sec/batch; 51h:56m:45s remains)
2017-12-15 16:35:00.634226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8506222 -8.399437 -7.0602164 -6.1300864 -5.9551072 -5.9228311 -5.7153287 -6.4828091 -6.8130994 -7.0139189 -6.9207358 -7.9465737 -8.6538324 -9.0538836 -8.7849789][-7.0130954 -6.6449375 -5.6868715 -5.3607569 -5.9922047 -6.2490768 -6.4293761 -7.7756577 -8.7752934 -10.112597 -10.836203 -11.973654 -12.631866 -12.353601 -10.880432][-4.4369216 -5.3231549 -5.9110188 -4.870338 -5.389111 -5.8298779 -6.6732025 -8.1620121 -9.3016472 -10.775774 -12.162185 -13.496331 -14.559986 -14.609104 -13.001325][-5.3873119 -5.0204163 -5.0568609 -5.0897245 -5.2341928 -4.7077861 -4.6816874 -6.1669173 -7.8358374 -8.754097 -9.0683889 -11.84528 -14.506428 -14.834707 -14.193296][-5.0651412 -5.9336338 -5.8496656 -5.7380066 -5.5447469 -3.0183108 -1.1739979 -3.7644694 -6.1086845 -7.5032196 -8.1379843 -8.8274269 -10.840807 -13.35587 -13.925207][-5.3835368 -5.9106145 -6.2427773 -4.9879551 -3.3667369 -0.41313839 2.6906123 1.8204074 0.277133 -3.0028059 -4.9661818 -6.6509047 -8.6082907 -10.626694 -12.253462][-5.5961285 -5.8750005 -5.2000985 -2.2895021 -1.3633561 1.3567905 4.8544974 4.9969878 4.9702649 1.1991386 -2.4872229 -5.3260741 -8.1537647 -10.000593 -10.570086][-5.6800818 -5.2889771 -4.8087268 -1.3610187 -1.1324263 2.0885572 5.6177268 4.5062003 5.0239329 2.8534036 -0.17559624 -4.8454409 -9.13594 -11.355324 -11.716806][-5.4989014 -4.5211067 -3.4782884 -1.643405 -1.3408203 1.7442369 3.4785943 3.2456756 3.5278897 0.25234652 -1.1159625 -4.9568329 -10.128296 -12.308531 -12.204823][-5.4266162 -4.1079683 -2.4214256 -1.0789852 -1.0911207 0.37345266 3.0991263 3.5719333 2.5581889 0.26691341 -1.2668962 -5.7888789 -9.559907 -11.922606 -13.167109][-10.03265 -7.380537 -5.0697613 -3.2011147 -2.1658509 -1.0891428 0.83969164 0.745358 -0.39021683 -2.1203523 -4.3238745 -8.6113634 -11.41501 -12.832043 -12.222417][-14.916498 -11.637573 -8.32498 -6.6836019 -5.5101128 -3.9961483 -3.1136382 -3.286418 -3.9939196 -5.4472246 -7.8336315 -10.630388 -11.592691 -12.845119 -12.323475][-14.846621 -12.80113 -10.999203 -9.2436972 -7.1152415 -6.0612965 -5.8774848 -5.6621737 -7.3496075 -7.8860703 -8.3743353 -9.7848568 -10.216017 -9.8219261 -8.5147057][-12.377947 -11.151915 -9.01381 -8.4046373 -7.0748672 -7.3458757 -6.8987136 -6.4106665 -6.7636747 -7.7359591 -9.2598515 -8.4268847 -7.8254709 -7.8990059 -7.3962054][-9.4565067 -8.257823 -5.9890552 -4.4066892 -2.6551232 -3.587014 -3.9126883 -4.9848146 -6.9966688 -6.9695582 -7.6763525 -9.1379776 -9.5190506 -9.5277 -9.5012255]]...]
INFO - root - 2017-12-15 16:35:07.255566: step 34110, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.688 sec/batch; 57h:01m:06s remains)
INFO - root - 2017-12-15 16:35:13.875371: step 34120, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 53h:09m:24s remains)
INFO - root - 2017-12-15 16:35:20.389433: step 34130, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 53h:53m:13s remains)
INFO - root - 2017-12-15 16:35:26.964510: step 34140, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 55h:00m:35s remains)
INFO - root - 2017-12-15 16:35:33.531036: step 34150, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 52h:55m:17s remains)
INFO - root - 2017-12-15 16:35:40.216084: step 34160, loss = 0.24, batch loss = 0.20 (11.5 examples/sec; 0.697 sec/batch; 57h:45m:34s remains)
INFO - root - 2017-12-15 16:35:46.824268: step 34170, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 55h:46m:45s remains)
INFO - root - 2017-12-15 16:35:53.348746: step 34180, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.675 sec/batch; 55h:58m:07s remains)
INFO - root - 2017-12-15 16:35:59.985173: step 34190, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 55h:32m:32s remains)
INFO - root - 2017-12-15 16:36:06.537052: step 34200, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.638 sec/batch; 52h:51m:55s remains)
2017-12-15 16:36:07.049189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.65764046 -0.6766777 -0.13962889 0.87966776 0.49096394 -0.89418554 -3.5038047 -5.7157626 -6.9776287 -7.6578064 -7.7977724 -9.9199686 -12.85813 -12.511476 -10.56593][-1.2388492 1.3347902 3.4668508 3.9387574 2.6761279 0.18723059 -2.482425 -4.4908495 -6.7261758 -8.0524254 -9.1362743 -12.04485 -14.862442 -14.822062 -14.006773][-0.14114428 0.51333857 0.62863255 2.9740624 3.3989263 2.0729814 0.2874279 -2.0033338 -4.4801469 -6.7759976 -8.6445885 -11.064348 -13.796383 -15.009771 -15.09153][-1.7722867 -0.78999853 0.8500843 2.9769607 1.8300061 0.68329 -0.45780993 -1.7129183 -3.346719 -4.7294188 -5.868433 -8.6828728 -12.016546 -12.399696 -12.194468][-1.0849552 -1.4178286 -0.93935633 0.82527494 1.0207205 2.0580292 2.1097183 0.3093586 -1.404058 -2.6513567 -3.9922483 -7.382926 -10.877993 -11.904495 -11.915541][-3.3724561 -3.4248366 -2.4214969 0.82772636 2.2697496 4.0546594 4.6315589 3.8881221 3.0411897 1.3171086 -1.0955129 -3.5487034 -6.5721273 -7.7248521 -7.8072014][-4.3281159 -3.9508014 -2.3558409 0.087245464 1.8701253 4.6450524 6.1639037 6.6046309 6.3072572 3.8107495 1.0004973 -2.1323812 -5.9147134 -6.7554364 -7.0138659][-7.6345196 -6.5480814 -4.4521561 -0.34423542 2.2532334 4.6860747 6.3606114 6.4838204 6.1167789 4.67166 2.4674067 -1.4533072 -6.3716173 -8.4079437 -8.4995289][-8.4277258 -7.1184416 -4.7973056 -1.4047589 0.8983717 3.7198548 5.0418324 4.7183671 4.50923 3.5990644 1.8953733 -1.8062434 -6.2851853 -7.7024522 -7.7213273][-9.1770983 -8.0356007 -5.8803921 -2.3035877 0.28058863 2.3607697 2.8507857 2.9009061 3.2179828 2.0127563 -0.12407017 -3.3132322 -7.1951275 -8.4769773 -8.7807846][-13.459513 -12.213653 -10.405183 -6.5059805 -4.3619509 -2.4439483 -1.2513728 -1.1300206 -1.1089077 -1.4223738 -1.9206629 -5.1012068 -7.9909658 -8.1829758 -7.10971][-15.828876 -15.502832 -13.801754 -10.554783 -9.1222553 -7.3039351 -6.9934373 -6.6455011 -6.4457026 -6.2131729 -5.9904675 -6.9530907 -7.5902653 -7.2819171 -6.6969919][-15.865187 -15.677603 -13.800615 -11.658683 -10.469709 -8.330862 -8.2386093 -9.0299568 -9.956418 -9.1201916 -8.214076 -7.9112525 -7.9494362 -6.9166088 -5.3176847][-11.565701 -11.454723 -11.306005 -9.6540356 -8.402029 -7.7807093 -8.3964462 -7.9474144 -7.1914029 -7.6925035 -8.1305447 -6.8721151 -5.7203455 -4.8416266 -4.9791551][-8.91235 -8.13842 -6.7783322 -5.5960865 -5.735589 -4.7705836 -3.9070706 -4.1665659 -5.4859595 -5.7039661 -5.3752871 -5.8541355 -6.4801373 -6.3336844 -6.3115759]]...]
INFO - root - 2017-12-15 16:36:13.575238: step 34210, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 54h:06m:38s remains)
INFO - root - 2017-12-15 16:36:20.143416: step 34220, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 53h:14m:03s remains)
INFO - root - 2017-12-15 16:36:26.797962: step 34230, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.648 sec/batch; 53h:39m:10s remains)
INFO - root - 2017-12-15 16:36:33.420877: step 34240, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 53h:41m:02s remains)
INFO - root - 2017-12-15 16:36:39.982604: step 34250, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 54h:50m:45s remains)
INFO - root - 2017-12-15 16:36:46.540184: step 34260, loss = 0.11, batch loss = 0.06 (12.5 examples/sec; 0.640 sec/batch; 53h:02m:00s remains)
INFO - root - 2017-12-15 16:36:53.177464: step 34270, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 54h:35m:21s remains)
INFO - root - 2017-12-15 16:36:59.789861: step 34280, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 53h:28m:19s remains)
INFO - root - 2017-12-15 16:37:06.406531: step 34290, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 54h:57m:35s remains)
INFO - root - 2017-12-15 16:37:13.080865: step 34300, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 56h:14m:49s remains)
2017-12-15 16:37:13.575374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1710205 -3.3312585 -5.550878 -7.0462618 -6.9581771 -7.182806 -8.3141594 -9.1780787 -8.96543 -8.4367561 -8.682621 -10.682239 -13.048635 -14.062021 -13.819881][-2.5026026 -3.74245 -5.4416542 -5.5390439 -6.0430708 -5.4892926 -4.768712 -4.8497734 -5.5179038 -6.0261593 -7.1790748 -9.3754349 -11.010404 -13.255326 -13.936764][-2.2831 -3.0798049 -4.907074 -6.0700393 -6.4126763 -6.1638956 -5.4713759 -4.3735485 -3.773392 -4.6087227 -5.59728 -7.8033762 -10.287032 -11.766972 -11.8262][-5.2281017 -5.8925257 -6.9219513 -7.4499512 -7.3805418 -5.3731494 -4.07713 -2.9076192 -1.887954 -2.709305 -3.8745368 -6.0061541 -8.6665869 -9.6973963 -9.90914][-7.0336947 -8.7353611 -10.089556 -10.093191 -8.6431971 -4.8583722 -2.0530534 0.19419479 -0.40351343 -1.6373601 -3.1605706 -5.6260171 -7.1926942 -7.3022323 -7.81639][-8.5291529 -9.1126165 -8.3797684 -7.8650565 -6.5780139 -3.7068529 0.29361248 2.5070329 1.5500379 -0.68404055 -3.3305697 -4.60281 -5.0268717 -5.7403836 -5.7791696][-8.0824413 -8.6621046 -8.2564249 -6.0206366 -2.4957204 1.0793905 4.2358594 5.3739219 5.1627431 2.9246917 -1.1191344 -3.5343821 -4.7273569 -4.2202668 -3.2379963][-7.7199993 -7.6350756 -7.4248743 -6.555335 -3.9771 1.4116979 6.4994817 7.6918178 7.5817513 5.2482457 2.6082501 -0.37761688 -3.4828291 -3.6323514 -3.859066][-9.9163628 -9.0957718 -7.2550292 -5.5402327 -3.5692394 -0.86528015 2.7888627 5.2760863 6.0863709 4.5776582 2.9327693 0.01674366 -3.9140224 -6.1229081 -6.2996564][-9.42644 -10.689114 -9.4212685 -7.6657357 -4.8512335 -1.7503302 0.529511 2.0296841 2.9190345 2.2306929 1.8984714 -1.275404 -5.1460195 -8.479023 -10.331252][-12.563969 -11.202148 -10.937381 -10.342503 -9.02655 -6.6766491 -4.0856318 -1.9843299 -1.5163913 -2.0298543 -2.0964141 -5.3172855 -9.5428143 -11.049904 -11.705883][-12.936373 -11.446026 -10.345621 -10.224351 -9.6134691 -9.0382509 -7.9010506 -6.4013114 -6.2169166 -6.0107412 -5.2776756 -7.9366322 -10.620588 -12.344466 -12.934259][-11.589642 -10.23192 -7.937624 -7.015388 -8.265605 -7.9424052 -7.4065895 -7.1733785 -6.8561611 -6.7698507 -6.661489 -8.1633091 -10.132986 -10.677713 -11.40607][-10.939358 -9.565671 -7.9326973 -7.0209532 -6.5892029 -6.5737257 -6.82673 -5.47188 -5.0400362 -4.9895134 -4.5280042 -4.9563289 -6.8533106 -7.625515 -7.9362097][-8.70545 -8.3568678 -7.672338 -6.0687847 -4.9885287 -5.3717017 -5.5212288 -4.7951193 -4.0431318 -3.9333725 -4.0011234 -5.4125013 -6.2448335 -7.0176053 -7.3997178]]...]
INFO - root - 2017-12-15 16:37:20.105022: step 34310, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 52h:49m:05s remains)
INFO - root - 2017-12-15 16:37:26.665862: step 34320, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 53h:18m:27s remains)
INFO - root - 2017-12-15 16:37:33.279079: step 34330, loss = 0.11, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 53h:25m:24s remains)
INFO - root - 2017-12-15 16:37:39.801814: step 34340, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 53h:32m:04s remains)
INFO - root - 2017-12-15 16:37:46.316933: step 34350, loss = 0.10, batch loss = 0.06 (12.2 examples/sec; 0.655 sec/batch; 54h:15m:30s remains)
INFO - root - 2017-12-15 16:37:52.894997: step 34360, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 55h:24m:07s remains)
INFO - root - 2017-12-15 16:37:59.461569: step 34370, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.681 sec/batch; 56h:24m:32s remains)
INFO - root - 2017-12-15 16:38:06.060277: step 34380, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.657 sec/batch; 54h:26m:45s remains)
INFO - root - 2017-12-15 16:38:12.744775: step 34390, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 54h:43m:29s remains)
INFO - root - 2017-12-15 16:38:19.353307: step 34400, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.675 sec/batch; 55h:53m:20s remains)
2017-12-15 16:38:19.841258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5118818 -6.5681744 -6.5645423 -6.2553391 -6.8290844 -7.3643 -6.9170785 -5.6780505 -4.609828 -3.2468705 -1.9070895 -4.3017387 -6.0742159 -6.95481 -7.3803344][-5.9552608 -6.1313915 -6.1628008 -6.588347 -8.0688257 -8.9732456 -8.924798 -7.4581003 -6.0150938 -4.911694 -4.0439749 -5.5531278 -6.9018879 -7.6811819 -7.8926725][-5.5856757 -6.4658785 -7.0481434 -7.5244312 -8.5461226 -8.71443 -8.7289314 -8.1309042 -7.4568281 -6.7780752 -6.0833306 -8.2489414 -9.6827965 -9.2483664 -8.806531][-6.2278314 -6.0718422 -6.3082223 -7.128592 -7.6266918 -7.0908589 -6.9290948 -6.64737 -6.9743977 -7.3493342 -7.7012768 -9.75775 -11.215643 -10.605623 -10.128675][-6.2699065 -7.1900873 -6.94775 -5.6648622 -5.378139 -4.1424875 -3.1674178 -3.7658725 -5.0020347 -6.1984482 -7.2426414 -9.4796124 -10.572165 -10.00921 -9.3344584][-5.9546905 -6.1583452 -4.6823139 -3.3250952 -2.6472898 -0.39569235 0.53571796 0.40185976 -0.011838436 -2.6037035 -4.4214239 -6.25453 -7.9239817 -8.1375542 -8.4514885][-5.8141308 -4.8266182 -3.2053418 -1.9036145 -0.85044909 1.3293967 3.0947843 3.6868711 3.5517039 1.3060708 -0.72978783 -3.621011 -5.7774572 -5.6582913 -5.9841514][-6.31168 -5.039412 -2.4456756 0.09053278 1.4689541 3.5669913 4.918982 4.9612689 5.2219663 3.3758206 1.2030754 -1.6784077 -4.19262 -4.661653 -4.5491223][-4.6523657 -3.6475904 -1.6749296 0.71257305 2.0865569 4.0726476 5.0504804 4.9838567 4.5400405 2.3318458 0.72528553 -1.9129629 -4.5466285 -4.4856234 -4.2528362][-4.5656204 -3.3146021 -1.8575299 0.82467747 2.2060728 2.809689 3.7837911 3.9319367 3.07906 0.90067482 -0.71377993 -3.3933079 -5.7450724 -6.0271497 -5.7713828][-9.37101 -8.0564728 -5.5797739 -2.8563838 -1.4936228 -0.19766951 0.63261175 -0.092890739 -0.94285679 -2.140003 -4.0775495 -7.32353 -8.2833033 -7.0510592 -5.46294][-13.108202 -11.686483 -9.2429142 -6.277637 -4.4448905 -2.9066858 -2.4023571 -3.285573 -4.5588274 -6.3181534 -7.8242598 -8.6625595 -8.6893539 -7.2762575 -5.72175][-11.778521 -10.352729 -7.9153509 -5.5865946 -4.2647748 -3.0441077 -2.9048586 -4.0826626 -5.5670576 -6.4446125 -7.5516758 -8.5323563 -8.69458 -7.3711753 -6.3605037][-10.860466 -9.0109768 -6.6229429 -4.6393461 -3.102952 -2.7684236 -3.3990579 -3.6539352 -4.648056 -5.9733825 -7.1119032 -7.4867964 -7.6324611 -6.3977976 -5.8661513][-7.5614738 -6.9261436 -5.668869 -4.8260021 -3.4679265 -3.6014748 -4.1542673 -4.3183188 -4.9143987 -5.4036646 -6.4691949 -7.6997771 -8.267127 -8.567421 -8.4631567]]...]
INFO - root - 2017-12-15 16:38:26.350941: step 34410, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 53h:39m:03s remains)
INFO - root - 2017-12-15 16:38:32.986027: step 34420, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 54h:03m:00s remains)
INFO - root - 2017-12-15 16:38:39.595892: step 34430, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 54h:49m:23s remains)
INFO - root - 2017-12-15 16:38:46.115942: step 34440, loss = 0.11, batch loss = 0.07 (12.0 examples/sec; 0.665 sec/batch; 55h:02m:46s remains)
INFO - root - 2017-12-15 16:38:52.809869: step 34450, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 54h:57m:32s remains)
INFO - root - 2017-12-15 16:38:59.342353: step 34460, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 53h:25m:27s remains)
INFO - root - 2017-12-15 16:39:06.006140: step 34470, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 54h:42m:06s remains)
INFO - root - 2017-12-15 16:39:12.612572: step 34480, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 56h:12m:34s remains)
INFO - root - 2017-12-15 16:39:19.222431: step 34490, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 54h:38m:33s remains)
INFO - root - 2017-12-15 16:39:25.803283: step 34500, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 53h:28m:20s remains)
2017-12-15 16:39:26.325690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.5102339 -6.7050538 -4.7418909 -3.9083796 -4.2501068 -3.6838639 -2.2997229 -2.2466273 -1.8371487 -2.0728548 -2.0526574 -3.5993593 -6.5640216 -8.1381111 -7.9310164][-7.1736617 -6.4174747 -5.3832226 -4.2710304 -4.2999048 -4.9409294 -4.5158248 -3.7700465 -2.9509914 -2.3484144 -2.3967907 -4.7847762 -7.26472 -8.4753351 -9.0921][-4.5306864 -4.7622309 -4.3165717 -3.2333579 -3.4839087 -4.0676107 -4.4134641 -4.6290731 -4.30575 -3.73345 -3.3356359 -4.148221 -5.9179 -8.2597761 -7.9954071][-3.6837146 -3.2399497 -2.4035501 -1.7343678 -2.6057811 -2.5192332 -2.3318343 -3.3123043 -3.9014382 -3.6851079 -4.0180836 -5.9142079 -7.7739544 -7.6890326 -6.7966151][-5.5217319 -4.6727195 -3.485074 -1.6708045 -1.8256168 -1.3010221 -0.44386959 -1.011363 -2.2640789 -3.0485785 -3.8855083 -6.0464377 -7.901845 -8.39755 -7.5900598][-6.1350865 -5.5216203 -4.0149589 -1.9510715 -0.94655895 1.0242758 2.4153657 2.4988475 1.875505 -0.32557774 -3.38875 -5.6995053 -7.4893227 -8.41331 -8.0209608][-5.8707457 -5.292881 -4.5800743 -2.4696662 -0.98736906 1.4913783 4.1272321 5.4217086 4.9077535 1.7900982 -1.3772869 -4.5000529 -6.7308512 -7.1143866 -6.5388689][-7.7840929 -6.5470061 -4.3146372 -1.0869136 0.44686842 3.0227513 5.5428967 5.6642966 5.6054626 4.4883294 1.9634795 -2.6212611 -6.4323587 -7.8363404 -7.62815][-9.0678778 -7.4802704 -5.2994385 -2.8713706 -0.97818804 2.1498346 4.5562263 5.9417367 6.9104028 4.4703574 1.2073188 -2.4804041 -6.5087442 -8.2896719 -8.311779][-6.9643626 -6.2784796 -6.1946311 -4.4043279 -2.27113 -0.13606167 2.1125746 4.0592265 5.4339881 3.8780646 0.90011883 -3.2259495 -6.7686396 -9.5520153 -10.369428][-9.8097391 -8.2630081 -6.8579249 -5.0586205 -5.2457471 -4.9453588 -3.2393758 -1.6080956 -0.91956186 -1.1064444 -2.0613236 -6.9138165 -10.433757 -11.014042 -10.094587][-13.653067 -12.934587 -11.181173 -9.4520521 -9.3203459 -8.397893 -7.5498457 -7.1166043 -6.6235414 -6.2358403 -6.9424105 -9.0767927 -10.807234 -12.722076 -13.352589][-12.787285 -11.460955 -11.099115 -10.467118 -9.9834194 -9.1587877 -8.8109541 -8.9561939 -8.8171635 -8.5862112 -8.9733524 -9.0097885 -8.8963976 -9.2574148 -9.2619295][-10.120584 -9.7042246 -8.5990124 -6.7263117 -7.1729469 -9.0021667 -9.5446138 -8.6468658 -8.26102 -8.2736387 -8.9356461 -8.4933472 -8.5347824 -8.0927191 -7.7210479][-7.9718442 -7.8459568 -7.2247553 -5.5887575 -5.5849075 -6.3579707 -6.9557905 -7.5957813 -7.7235446 -6.4692903 -6.2395029 -7.6569142 -8.267971 -8.0555315 -7.997242]]...]
INFO - root - 2017-12-15 16:39:32.889632: step 34510, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 55h:12m:50s remains)
INFO - root - 2017-12-15 16:39:39.480239: step 34520, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 54h:55m:42s remains)
INFO - root - 2017-12-15 16:39:46.064073: step 34530, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 54h:56m:19s remains)
INFO - root - 2017-12-15 16:39:52.622946: step 34540, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 52h:48m:56s remains)
INFO - root - 2017-12-15 16:39:59.208533: step 34550, loss = 0.20, batch loss = 0.16 (11.7 examples/sec; 0.684 sec/batch; 56h:36m:24s remains)
INFO - root - 2017-12-15 16:40:05.886801: step 34560, loss = 0.17, batch loss = 0.13 (11.4 examples/sec; 0.705 sec/batch; 58h:19m:00s remains)
INFO - root - 2017-12-15 16:40:12.438070: step 34570, loss = 0.18, batch loss = 0.13 (11.6 examples/sec; 0.689 sec/batch; 57h:00m:33s remains)
INFO - root - 2017-12-15 16:40:19.049783: step 34580, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 53h:49m:49s remains)
INFO - root - 2017-12-15 16:40:25.680435: step 34590, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 54h:04m:17s remains)
INFO - root - 2017-12-15 16:40:32.266463: step 34600, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 53h:54m:48s remains)
2017-12-15 16:40:32.743533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.49441 -6.1623015 -6.186883 -6.2942162 -7.0774708 -7.7038293 -7.7714472 -7.36941 -7.65825 -8.0949812 -7.6706419 -8.3085365 -8.7469873 -9.27663 -9.2235928][-5.7352548 -5.5321541 -5.8430429 -6.7068696 -7.5364432 -8.2181015 -7.84006 -6.9523787 -6.8745618 -7.5326314 -7.6583796 -9.1023331 -10.161015 -10.858583 -9.8743343][-4.2617922 -4.5723152 -6.0376649 -6.2210841 -6.4161243 -6.7423463 -7.9326744 -8.0137005 -7.3783836 -8.1474133 -8.2332325 -9.0548744 -10.154564 -11.147503 -9.5261478][-5.3422012 -5.3209805 -4.9415989 -5.7294593 -7.6572618 -7.07959 -5.6576266 -6.0304303 -6.1599517 -5.4156327 -6.5110669 -8.78853 -10.302647 -12.598774 -11.961092][-4.2731628 -5.7144494 -6.28719 -4.9284267 -5.4094296 -4.063745 -2.2166424 -2.4670527 -3.576937 -3.4951875 -4.4806352 -6.7105465 -8.8678236 -11.437027 -12.456959][-5.924077 -5.6327009 -5.4355273 -4.4642105 -3.3254213 -0.668571 1.6560731 1.6685963 0.41196823 -1.7249365 -2.3403435 -3.8069358 -6.7854791 -10.265407 -11.595404][-5.6290216 -4.8903322 -5.0580211 -3.1044939 0.21038008 3.5120444 5.5456367 4.8632731 4.0103 1.0794125 -0.3170867 -1.7014866 -4.8056087 -7.7285242 -9.1341352][-6.1386681 -4.6205206 -3.6192312 -1.5418763 1.0312347 4.2110934 7.4040751 7.7587857 6.68372 4.8214946 2.1069388 -0.73037863 -3.1476123 -6.6401677 -8.3289185][-5.4819894 -5.0918307 -4.6174607 -2.5206189 -0.16787672 2.2526517 6.1146216 7.6714034 6.0533347 4.0590777 2.3894639 -0.11291218 -3.6037109 -6.9625134 -7.5290203][-6.7544293 -6.3098068 -5.1955323 -3.2879267 -2.4617794 0.47084618 3.7704406 5.4020486 4.3931241 1.940619 0.35092211 -2.4157073 -4.3803072 -8.1060944 -10.237539][-11.323603 -10.165428 -8.7387419 -6.8233271 -5.1502571 -3.027029 -1.883327 -1.1712046 -0.90311193 -2.0206683 -3.6812973 -6.2829962 -9.2168865 -10.205208 -9.3779144][-16.059914 -12.76914 -9.8325233 -8.6267395 -6.9781704 -5.8747406 -5.5366335 -4.5980587 -4.1615348 -4.6817021 -5.4236054 -6.56845 -9.13915 -10.470116 -9.5410042][-14.40052 -12.719093 -8.7178917 -6.6164465 -5.7818937 -4.8756366 -4.7629957 -4.3198662 -4.3455849 -4.3018236 -4.4310751 -5.5206852 -7.4724021 -8.3330822 -6.9671535][-8.992198 -7.803194 -5.673377 -4.1441717 -2.4814203 -2.5294998 -2.8857377 -2.1769462 -1.7001038 -1.6500602 -2.6780691 -3.3792698 -4.2938042 -5.6738038 -6.3103652][-5.043447 -4.6964426 -3.6043484 -1.9810941 -1.8802302 -1.1302147 -0.710731 -0.82760906 -1.2475739 -1.2017117 -0.76885366 -2.4142873 -5.0366917 -5.5607643 -5.8040781]]...]
INFO - root - 2017-12-15 16:40:39.330361: step 34610, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.684 sec/batch; 56h:35m:33s remains)
INFO - root - 2017-12-15 16:40:45.933117: step 34620, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 54h:07m:38s remains)
INFO - root - 2017-12-15 16:40:52.499690: step 34630, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 53h:07m:57s remains)
INFO - root - 2017-12-15 16:40:59.169037: step 34640, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 56h:01m:58s remains)
INFO - root - 2017-12-15 16:41:05.797202: step 34650, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 56h:30m:28s remains)
INFO - root - 2017-12-15 16:41:12.399083: step 34660, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 54h:44m:00s remains)
INFO - root - 2017-12-15 16:41:19.074067: step 34670, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.697 sec/batch; 57h:38m:08s remains)
INFO - root - 2017-12-15 16:41:25.676712: step 34680, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 55h:43m:47s remains)
INFO - root - 2017-12-15 16:41:32.296422: step 34690, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 55h:03m:17s remains)
INFO - root - 2017-12-15 16:41:38.823681: step 34700, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 54h:55m:18s remains)
2017-12-15 16:41:39.369947: I tensorflow/core/kernels/logging_ops.cc:79] [[[-11.968849 -12.935602 -13.935595 -14.419069 -14.831861 -14.12562 -12.766634 -10.876215 -8.8437233 -7.9837589 -8.7644815 -10.463232 -12.338475 -12.766771 -10.329851][-11.171703 -10.95057 -10.844304 -11.952508 -12.718546 -13.364454 -12.841813 -11.819213 -11.218603 -10.176421 -9.716877 -11.096517 -12.939669 -12.824611 -11.532151][-5.4250393 -6.7551107 -8.405241 -10.428181 -11.47156 -11.274509 -10.555956 -9.5827761 -9.5026 -8.7463121 -7.9804964 -9.30056 -11.325151 -12.014646 -12.385862][-3.5590296 -3.1034555 -3.6664202 -5.3473139 -6.8086905 -6.4857144 -6.0308576 -5.9661088 -6.5124936 -6.4576921 -6.8423204 -8.3185863 -9.3051071 -10.610295 -10.707659][-3.9840493 -4.64128 -5.2001405 -4.3791046 -3.3851464 -1.8408401 -0.65980911 -1.8393035 -3.1877687 -3.5715654 -4.3416896 -6.9787273 -8.9793 -10.479979 -9.6952343][-6.3517694 -6.8322077 -6.5136638 -4.7952867 -2.9124579 0.7208147 3.5467782 2.9159636 1.6309643 -0.63988686 -2.2328451 -4.3238225 -6.6931005 -8.0432177 -7.6018319][-8.4663534 -8.8610773 -7.7761583 -4.66086 -1.9915774 1.6572661 5.0041108 6.0209727 6.6772847 4.4521413 1.6377125 -1.4076891 -4.8575559 -6.8621116 -6.4036956][-7.7975626 -7.2532516 -6.2793355 -3.7998393 -0.63592863 3.1929069 5.9855 6.2565894 6.4934077 4.4665895 1.9677172 -1.3498435 -4.1823139 -5.3364668 -4.5089111][-5.1856966 -3.9396477 -3.6069753 -2.3234127 -0.26280642 2.50489 4.0582337 5.137054 5.2293696 3.4976516 1.6674199 -2.0231767 -5.046948 -6.1425581 -5.0902777][-4.2364531 -2.9205635 -1.2925978 -0.79774046 -0.23263836 0.46927166 1.5329657 1.9997301 0.96080017 -0.015603065 -0.70730352 -3.2241967 -5.1302404 -5.1189375 -4.3166337][-6.4592576 -4.5297432 -3.4538825 -3.266865 -3.0008132 -2.5458498 -0.75725937 0.29601479 -0.92355919 -2.2727149 -3.156949 -5.3428369 -7.5536447 -7.6710081 -5.8550267][-9.1170044 -7.4650493 -6.5071173 -6.1999526 -6.3946519 -5.5279241 -4.278574 -3.6535716 -3.9950476 -4.5361643 -4.1500616 -5.3059278 -6.172565 -6.3146458 -5.9116082][-10.259242 -9.5906563 -8.8996172 -8.5123787 -7.6201005 -5.5790296 -4.718854 -4.3596816 -4.5797873 -5.2314715 -5.2261119 -5.7164054 -6.4536324 -6.3634763 -5.1600027][-9.2206669 -8.4585857 -8.52661 -7.513875 -6.8725538 -5.6076713 -4.9270754 -4.7856078 -4.7767606 -4.653903 -4.9658203 -4.9373078 -4.7601652 -4.868454 -5.4060917][-6.9884543 -5.9218211 -5.6360292 -5.4747286 -4.18534 -3.7954564 -3.7260215 -4.1792679 -5.0486684 -5.280858 -5.5167136 -6.6296535 -7.9556589 -8.540904 -7.6976056]]...]
INFO - root - 2017-12-15 16:41:45.982086: step 34710, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 54h:17m:17s remains)
INFO - root - 2017-12-15 16:41:52.589356: step 34720, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 53h:25m:04s remains)
INFO - root - 2017-12-15 16:41:59.346064: step 34730, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 55h:48m:38s remains)
INFO - root - 2017-12-15 16:42:06.003835: step 34740, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 55h:42m:00s remains)
INFO - root - 2017-12-15 16:42:12.568765: step 34750, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 53h:05m:50s remains)
INFO - root - 2017-12-15 16:42:19.184203: step 34760, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 53h:02m:36s remains)
INFO - root - 2017-12-15 16:42:25.798223: step 34770, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 54h:20m:04s remains)
INFO - root - 2017-12-15 16:42:32.391983: step 34780, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 53h:33m:04s remains)
INFO - root - 2017-12-15 16:42:39.071405: step 34790, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 55h:10m:57s remains)
INFO - root - 2017-12-15 16:42:45.706236: step 34800, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 54h:19m:31s remains)
2017-12-15 16:42:46.225636: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1849871 -6.8382864 -6.2458062 -5.9028497 -6.4463396 -6.9908237 -7.467988 -6.7927513 -6.4301643 -6.5148273 -6.8577137 -10.639482 -13.370539 -12.473507 -11.944524][-7.3671985 -7.8904161 -7.2930017 -7.1851668 -6.8411574 -7.7534571 -8.7047729 -9.0552435 -9.3233767 -8.9530382 -9.171277 -12.888962 -14.651917 -13.309474 -12.632453][-6.6853538 -8.4204655 -8.7058992 -7.8769789 -7.7231331 -8.3062887 -8.5653391 -8.6071758 -9.1072235 -9.7440052 -10.201448 -13.799397 -16.091839 -14.285368 -13.129501][-6.4557829 -7.5323386 -6.9502754 -6.3692141 -6.3899217 -6.7282796 -6.7239923 -6.9687715 -6.991581 -7.3386106 -7.8296018 -11.7491 -14.188774 -13.412201 -13.161655][-7.6378236 -8.682848 -7.5296526 -5.4561853 -4.3861403 -3.2890339 -2.9302218 -4.1497469 -5.0066223 -4.5951986 -4.3079829 -8.301342 -11.132299 -10.843334 -11.142451][-7.4301257 -8.39719 -6.2686915 -3.8977494 -1.6057878 1.4098406 2.6416678 0.92883825 -0.58357859 -1.8892894 -2.6407125 -6.1711259 -8.3577595 -7.9491739 -8.306015][-9.7575035 -8.9675007 -6.1362004 -2.9480002 0.50794506 3.9221892 5.6194177 5.1469254 3.8010011 0.86003017 -0.96237993 -4.2698298 -6.5104384 -6.0369344 -6.6395493][-10.236383 -9.0255547 -5.9575853 -2.164854 1.3667626 5.3697867 7.1749291 7.0001206 6.524199 4.4997544 2.0144634 -3.5733819 -7.4862075 -7.2601767 -7.3322835][-7.0779686 -6.9941411 -4.985692 -1.7138944 0.82615376 3.1847415 4.3584323 4.4931493 4.800612 4.2144132 2.7621026 -3.2119708 -8.2591887 -8.8871756 -9.7511768][-4.0859728 -4.8253927 -4.4585743 -2.6023152 -0.2440486 0.051522732 -0.25508261 0.38204861 1.5604367 1.9928021 1.4679136 -3.845952 -8.3573351 -10.858105 -12.879463][-4.7389011 -5.799572 -6.2254171 -4.9241319 -4.1612029 -4.4014845 -4.1533747 -4.404995 -4.7087212 -4.2531261 -3.9135647 -6.33508 -8.8848276 -11.244986 -13.496967][-8.1331434 -8.7010269 -8.7921848 -9.142293 -8.9355316 -8.4347649 -8.5271921 -8.9516382 -8.9492388 -8.06173 -7.8681016 -9.6246338 -10.726189 -11.17415 -12.224333][-13.245417 -12.29433 -11.656087 -10.251942 -10.89092 -11.969826 -12.039486 -11.550547 -11.202498 -11.174585 -10.634567 -11.017123 -11.27233 -10.154276 -10.064388][-12.214972 -12.402137 -11.629734 -9.9660358 -10.188974 -10.003916 -10.800888 -10.687907 -10.39748 -9.9900265 -9.5980873 -9.503253 -9.1631765 -8.4975462 -8.546423][-10.067984 -9.8706436 -9.1184769 -7.8349409 -5.7646236 -5.5981584 -6.5447702 -6.1593719 -7.0777273 -7.943429 -8.06544 -8.3994226 -8.8145523 -8.2192688 -7.9230623]]...]
INFO - root - 2017-12-15 16:42:52.812694: step 34810, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 53h:01m:27s remains)
INFO - root - 2017-12-15 16:42:59.530370: step 34820, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 56h:00m:40s remains)
INFO - root - 2017-12-15 16:43:06.172458: step 34830, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 55h:07m:33s remains)
INFO - root - 2017-12-15 16:43:12.681067: step 34840, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 53h:21m:30s remains)
INFO - root - 2017-12-15 16:43:19.167464: step 34850, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 53h:06m:23s remains)
INFO - root - 2017-12-15 16:43:25.646991: step 34860, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 52h:44m:41s remains)
INFO - root - 2017-12-15 16:43:32.138136: step 34870, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 52h:55m:42s remains)
INFO - root - 2017-12-15 16:43:38.743879: step 34880, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 55h:32m:53s remains)
INFO - root - 2017-12-15 16:43:45.391828: step 34890, loss = 0.27, batch loss = 0.22 (12.0 examples/sec; 0.668 sec/batch; 55h:15m:30s remains)
INFO - root - 2017-12-15 16:43:51.990952: step 34900, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 55h:31m:53s remains)
2017-12-15 16:43:52.485469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0153515 -1.8785524 -2.097959 -2.1422839 -2.3006895 -2.6244164 -2.8993695 -3.2978513 -4.3353472 -4.4993448 -5.1514287 -8.09049 -9.34454 -11.968277 -11.499337][-3.7095919 -2.7761726 -2.1071348 -1.6101189 -2.0566931 -2.9627116 -4.2821555 -4.2328057 -4.6626496 -4.9975681 -4.3331947 -8.1041508 -10.220695 -12.260734 -12.613316][-2.0184448 -2.6919088 -2.9295022 -2.4267702 -2.9737289 -3.1996768 -2.8585746 -3.3862519 -3.7929893 -4.5108767 -5.6843429 -7.9052014 -9.195241 -11.302917 -11.124353][-2.1272657 -2.7568216 -2.6577361 -2.5586634 -3.1658468 -3.6573021 -3.7470047 -3.6914546 -3.948226 -4.2613511 -4.9191132 -8.3891516 -9.9312353 -11.978155 -11.489405][-3.1676006 -4.1746292 -3.48664 -1.9984987 -2.1150739 -1.3547359 -0.73164177 -2.4721849 -3.8007398 -4.2750192 -4.6600466 -7.6693115 -9.049963 -11.459519 -11.752904][-3.8321066 -3.7153871 -2.9964242 -1.0169649 -0.15852737 1.4961824 2.3395514 1.6911049 -0.17237473 -2.5576754 -4.302639 -7.2889347 -9.0837774 -11.934338 -12.678907][-4.7906518 -4.1254029 -3.1797869 -0.19667149 1.828248 3.0323644 3.3392825 3.9155412 3.4815106 0.45224905 -1.9530239 -5.825088 -8.2377176 -11.248405 -11.939754][-3.9294257 -3.91565 -3.5004935 -1.2244177 1.524147 4.8579564 5.9259744 5.4321713 4.6842046 2.4381518 1.07691 -3.173449 -6.3943968 -9.8241425 -11.505968][-2.3929744 -2.6604671 -2.1039832 -1.1007528 -0.42118931 2.165319 3.9396605 4.5617766 4.398726 3.047956 1.5816274 -2.5260429 -5.2963428 -9.1157188 -10.287091][-2.1496966 -3.5098944 -3.8728614 -2.3365629 -1.5670943 -0.93661022 -0.16901827 1.5473022 2.3693166 1.6279564 0.745491 -3.6120341 -5.8665018 -8.9004011 -10.117792][-5.9438591 -6.2743459 -6.825573 -5.3973374 -5.273603 -4.6843534 -4.6828256 -3.8699687 -2.5457423 -1.9380267 -2.0577483 -6.1405873 -8.42357 -9.7647 -9.4356642][-10.831455 -10.342379 -9.8654795 -9.2584944 -9.4300756 -8.8436613 -8.5016279 -8.0595207 -7.2089567 -6.1226778 -5.6820316 -7.8803086 -9.6331444 -10.948629 -9.9521055][-12.888136 -12.922388 -11.53371 -10.586956 -10.843589 -11.122007 -10.988125 -10.80905 -10.037218 -8.9717236 -7.993288 -8.9323244 -10.03075 -10.617424 -9.3911285][-11.362017 -11.434628 -10.882557 -10.278578 -10.486399 -10.855134 -10.485881 -9.5405455 -9.1458054 -9.0288725 -8.1356087 -7.2513494 -7.5886173 -8.80393 -8.2987442][-8.869442 -8.903266 -8.8360672 -8.7992439 -8.972434 -9.08112 -9.1255188 -9.0050907 -8.86706 -8.6272659 -8.135273 -9.0443554 -9.5278206 -9.49037 -9.6619148]]...]
INFO - root - 2017-12-15 16:43:59.065334: step 34910, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 53h:38m:44s remains)
INFO - root - 2017-12-15 16:44:05.594653: step 34920, loss = 0.31, batch loss = 0.26 (12.2 examples/sec; 0.658 sec/batch; 54h:21m:36s remains)
INFO - root - 2017-12-15 16:44:12.172568: step 34930, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 55h:24m:08s remains)
INFO - root - 2017-12-15 16:44:18.730072: step 34940, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 55h:46m:39s remains)
INFO - root - 2017-12-15 16:44:25.297045: step 34950, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 55h:40m:10s remains)
INFO - root - 2017-12-15 16:44:31.852506: step 34960, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 54h:12m:33s remains)
INFO - root - 2017-12-15 16:44:38.449023: step 34970, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 55h:06m:34s remains)
INFO - root - 2017-12-15 16:44:45.046685: step 34980, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 55h:17m:55s remains)
INFO - root - 2017-12-15 16:44:51.554779: step 34990, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 55h:13m:18s remains)
INFO - root - 2017-12-15 16:44:58.194622: step 35000, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 54h:08m:16s remains)
2017-12-15 16:44:58.686555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4659476 -3.6669612 -3.8184617 -3.3975635 -3.9247696 -4.2373786 -4.3886719 -4.3746853 -4.0693336 -4.1521225 -3.6886544 -6.3123641 -7.9772992 -9.6500168 -9.02453][-5.1081848 -5.0295849 -4.8513279 -4.393095 -4.8695536 -5.2919416 -5.5269794 -5.7780466 -5.7077703 -5.1325388 -4.4674544 -6.7738514 -7.7733583 -9.8510494 -10.15729][-4.5286508 -5.2526245 -5.94827 -5.5166306 -5.9711041 -6.2001405 -6.6066022 -6.4677205 -6.2642927 -6.1713004 -5.6967459 -7.7939205 -8.9541035 -10.57499 -10.673235][-5.18059 -6.0117903 -6.0601439 -5.5502987 -6.0597239 -5.727982 -5.5753603 -5.6371512 -5.5702672 -5.1626639 -4.895977 -7.4764566 -9.1193428 -11.160986 -11.215283][-6.2606664 -7.3540106 -7.7517309 -6.0494728 -5.5073128 -4.5750847 -4.4772973 -4.5346508 -4.2583466 -3.7995973 -3.3031235 -6.23705 -8.4543114 -11.062737 -11.704987][-7.4065 -7.9193306 -7.4333887 -5.0344334 -2.7840586 -0.68727207 0.030379295 -0.38854265 -1.1553531 -1.6257133 -1.6026258 -4.3023958 -6.47939 -9.3127575 -10.44734][-8.578167 -8.4594078 -6.854876 -3.3811409 -0.6807909 1.2946811 2.7192998 3.3499818 3.1473727 1.1563869 -0.37964249 -3.6161745 -5.8176236 -8.51015 -9.4072533][-8.0776405 -8.0642824 -6.2631178 -2.3161194 0.49381971 3.3643546 5.0524154 4.3170133 4.0654902 2.7175756 0.52969122 -3.8074832 -6.400979 -8.9003773 -9.4580164][-5.93911 -5.8958716 -4.923645 -2.6738982 -1.0126147 2.4302917 4.0874925 4.177321 4.3187203 2.4750857 0.64841747 -3.9499412 -7.208076 -9.7278776 -9.7284451][-4.9225068 -3.9538686 -3.4819303 -2.3095026 -1.4728818 -0.087077618 0.72691727 2.0078225 2.1456771 0.62734985 -0.72335291 -5.2519283 -8.3592529 -10.820976 -11.500795][-5.8607626 -5.1115632 -4.4238276 -3.4401438 -3.6957717 -3.4122913 -3.4311249 -2.9704623 -3.0736461 -3.427943 -4.1584334 -8.3228407 -11.087799 -12.753815 -12.5959][-8.9785194 -7.4345827 -6.0688362 -6.1788254 -6.1488876 -6.1790438 -7.1589451 -7.8771286 -8.2304449 -8.1447945 -8.4399109 -9.955616 -10.960251 -12.409688 -11.928745][-11.990877 -10.595551 -9.077137 -8.4684477 -8.9333725 -8.68548 -8.9165077 -9.4855137 -10.503193 -10.887623 -10.732057 -11.030079 -11.318602 -11.404594 -9.8230591][-11.83532 -11.448764 -10.082758 -9.1985912 -8.8487644 -8.8586864 -9.0227509 -9.1743088 -9.5820675 -10.354198 -10.420898 -9.2425966 -8.4647636 -9.3047295 -8.7961273][-9.1592312 -9.7006493 -9.0826864 -7.3875294 -6.8518953 -6.6548066 -7.0711541 -7.559895 -7.77311 -7.8086143 -7.9233274 -8.7881069 -9.5785923 -8.8552427 -8.4277916]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 16:45:06.323229: step 35010, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 55h:54m:21s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 16:45:12.911303: step 35020, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 52h:45m:03s remains)
INFO - root - 2017-12-15 16:45:19.466105: step 35030, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 53h:36m:59s remains)
INFO - root - 2017-12-15 16:45:26.054097: step 35040, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 53h:19m:29s remains)
INFO - root - 2017-12-15 16:45:32.567955: step 35050, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 55h:32m:35s remains)
INFO - root - 2017-12-15 16:45:39.121269: step 35060, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 52h:56m:39s remains)
INFO - root - 2017-12-15 16:45:45.637968: step 35070, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.663 sec/batch; 54h:47m:11s remains)
INFO - root - 2017-12-15 16:45:52.134634: step 35080, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 54h:20m:26s remains)
INFO - root - 2017-12-15 16:45:58.743678: step 35090, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 55h:12m:14s remains)
INFO - root - 2017-12-15 16:46:05.330877: step 35100, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 54h:31m:51s remains)
2017-12-15 16:46:05.843686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.724875 -5.8585763 -5.5047574 -4.3909631 -4.3517561 -3.8230348 -2.4519255 -1.2410231 -1.0353699 -0.19309616 -1.3165641 -5.2065024 -7.7728162 -9.0282154 -7.6266494][-6.4199777 -7.3442636 -8.1388254 -6.8865757 -5.6275711 -4.6694527 -2.9602854 -1.0060191 -1.2001743 -1.7360837 -2.1274798 -5.7909627 -8.5014048 -8.7836132 -8.5514631][-5.1066818 -6.1741142 -6.9745588 -7.0507817 -7.3000913 -5.5311389 -3.1331272 -1.9328508 -1.9257586 -2.7041242 -3.7836142 -7.6546326 -9.8213568 -9.8811073 -8.6619606][-4.6071167 -4.4797354 -4.9707165 -5.1791611 -5.840519 -4.9936023 -3.6895103 -2.6332176 -2.5273035 -2.9634297 -4.0447445 -9.1539507 -11.59726 -10.438013 -8.5500975][-5.0367622 -4.744596 -4.5216484 -4.0235348 -4.2923288 -2.5211296 -1.1770711 -2.1100895 -2.8193729 -3.7031183 -4.7956452 -8.0295792 -10.517303 -10.761874 -8.8727617][-8.1612873 -6.105619 -5.3695507 -3.7360454 -2.8962836 -1.0324197 0.45839691 0.30649328 -0.45811367 -3.0654662 -4.6929197 -8.1219425 -9.9908066 -10.061598 -8.5557795][-8.0081186 -7.1493506 -6.3872581 -3.69665 -2.6218662 0.33483744 3.076158 3.4909415 2.3061681 -0.70216751 -3.701721 -7.5036058 -8.9308481 -8.6146955 -8.2963409][-6.637815 -5.9321651 -5.7558913 -2.8490279 -1.3314729 1.5030179 4.2272582 4.5232863 3.9903827 1.61058 -1.5975084 -6.8465776 -9.522377 -8.9366407 -7.3990765][-5.3050194 -4.1153827 -3.6646762 -1.4603825 -0.62140751 2.1337223 4.2172332 4.4474092 4.4427342 1.3224759 -1.2042446 -6.0011125 -9.751502 -10.478291 -8.8270235][-4.5119324 -3.8199203 -3.48172 -1.6802969 -1.1528091 1.0902672 1.5911937 1.9165878 2.1530108 0.7078352 -1.285522 -6.4558725 -10.117498 -10.759609 -9.9104834][-6.1478772 -6.0494514 -6.1857777 -5.1326208 -4.3431087 -3.0743196 -2.055671 -1.7942898 -2.115989 -2.8108096 -3.463351 -8.2801 -10.594286 -10.97979 -10.273569][-10.547165 -9.3080711 -8.21637 -7.6588688 -7.3682637 -6.9560981 -7.2955666 -7.5473337 -7.6227503 -7.4683619 -7.1238441 -8.5113869 -8.9483881 -9.9113388 -9.0160379][-14.191813 -12.521868 -10.005313 -9.209815 -9.1344728 -9.03481 -9.3912888 -8.2183228 -8.1506014 -8.2804127 -8.511816 -9.4807453 -9.9505177 -9.7199345 -7.9029036][-12.189156 -10.980457 -8.5421715 -7.2796588 -6.8298216 -6.8490825 -7.2795606 -7.4942722 -7.1514492 -7.9198766 -8.5385981 -7.2555351 -6.6113825 -8.661581 -8.5243425][-7.8340673 -6.79922 -5.961741 -5.2040043 -5.2355661 -5.7782426 -5.4281621 -5.0610838 -4.9917808 -5.3689013 -5.9381785 -8.8115835 -9.0105753 -8.00145 -8.3975277]]...]
INFO - root - 2017-12-15 16:46:12.406902: step 35110, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 54h:38m:44s remains)
INFO - root - 2017-12-15 16:46:18.965086: step 35120, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 56h:33m:59s remains)
INFO - root - 2017-12-15 16:46:25.559480: step 35130, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 52h:23m:38s remains)
INFO - root - 2017-12-15 16:46:32.227969: step 35140, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 54h:19m:39s remains)
INFO - root - 2017-12-15 16:46:38.863615: step 35150, loss = 0.11, batch loss = 0.06 (12.1 examples/sec; 0.660 sec/batch; 54h:29m:18s remains)
INFO - root - 2017-12-15 16:46:45.416918: step 35160, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 53h:41m:16s remains)
INFO - root - 2017-12-15 16:46:51.980707: step 35170, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 54h:19m:09s remains)
INFO - root - 2017-12-15 16:46:58.609315: step 35180, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 55h:40m:52s remains)
INFO - root - 2017-12-15 16:47:05.125158: step 35190, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.648 sec/batch; 53h:29m:11s remains)
INFO - root - 2017-12-15 16:47:11.646728: step 35200, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 52h:41m:48s remains)
2017-12-15 16:47:12.148903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.0387449 -6.9706068 -5.8854804 -5.2684636 -6.2909126 -6.3774042 -5.9967718 -4.9596577 -3.6409554 -2.9085398 -2.0246625 -4.3976445 -6.119453 -6.567152 -5.6038237][-7.5655212 -7.1622729 -6.3038726 -5.9457121 -6.9501839 -7.3946309 -7.3867912 -6.6250181 -5.5408673 -4.7010059 -4.2692676 -6.7708359 -8.953413 -9.4555693 -7.6498928][-5.1283274 -5.6369333 -6.037683 -5.4879031 -6.032424 -6.4714065 -6.3673916 -5.9114294 -5.3941154 -5.3413191 -5.7156181 -8.4364843 -10.76445 -11.130428 -9.4637871][-6.2786136 -6.2382059 -5.9247265 -5.7296195 -6.6660776 -6.7351 -6.0048113 -5.7570443 -5.8391004 -5.7385325 -6.2935572 -9.5280342 -12.422097 -13.208408 -11.550572][-5.6596961 -6.629756 -6.52157 -5.4259372 -4.7234039 -3.0459256 -1.7656164 -2.6037352 -3.7859538 -4.3093071 -5.086153 -8.8774033 -12.368239 -14.11973 -12.931393][-8.3441029 -8.02293 -6.3080411 -4.1181803 -2.3572772 0.16105223 2.3892274 2.1508284 1.0305157 -1.6045308 -3.9979482 -7.7304764 -10.924597 -13.230766 -12.780615][-9.8113461 -8.3735933 -6.7100706 -3.0874462 -0.45655441 2.231596 5.4150395 5.4507146 4.3770757 0.59116554 -3.0371878 -7.149559 -10.436935 -11.343042 -10.079895][-7.6781111 -6.9333191 -5.9287791 -1.1001444 1.9702344 4.7152381 6.9756131 6.6455235 6.1443839 2.3133655 -1.0423913 -5.2417359 -8.9671831 -9.3197308 -8.1286516][-5.5133638 -4.9692464 -3.9583259 -1.1448383 0.54159832 3.6324668 5.7784295 5.2689042 4.3537688 2.2393255 0.36503458 -3.8536048 -7.2004762 -7.161458 -5.1870289][-5.0470924 -5.0508275 -4.2676072 -2.2982898 -0.76957512 0.96938896 2.6384683 3.27739 2.3705773 1.7841954 0.92310095 -2.7329431 -5.5224071 -6.4461889 -5.4115553][-9.4814377 -8.9691572 -7.5944042 -5.9533987 -5.3084431 -4.8189974 -3.677036 -2.5029008 -1.7565358 -1.5030904 -2.4405038 -5.1967316 -6.2876816 -5.9156375 -4.8674159][-14.460426 -14.480207 -12.585756 -10.886296 -10.758208 -10.134791 -9.5868988 -8.6110249 -7.1886816 -6.5646987 -6.2301097 -6.76414 -6.7991714 -6.2886953 -5.65][-15.054474 -14.519308 -13.17985 -12.415087 -11.571676 -10.530231 -10.561675 -10.320574 -9.4208965 -8.0612125 -7.2099304 -7.1515317 -6.8908658 -5.9835167 -4.5769615][-11.195862 -10.672846 -9.8681307 -9.4876413 -9.1208725 -9.4341717 -9.5833139 -9.01696 -9.0258636 -8.446909 -7.6670394 -7.0080771 -6.5109544 -5.980679 -5.2177672][-8.2089663 -7.8958941 -7.3487458 -6.1192021 -5.605814 -5.9623642 -5.81516 -6.3426175 -6.1946507 -5.3030629 -5.1585956 -5.8425922 -6.3218441 -5.7529831 -5.5666513]]...]
INFO - root - 2017-12-15 16:47:18.745976: step 35210, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.685 sec/batch; 56h:31m:58s remains)
INFO - root - 2017-12-15 16:47:25.366527: step 35220, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.685 sec/batch; 56h:31m:35s remains)
INFO - root - 2017-12-15 16:47:32.026897: step 35230, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 52h:50m:53s remains)
INFO - root - 2017-12-15 16:47:38.606587: step 35240, loss = 0.22, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 54h:00m:43s remains)
INFO - root - 2017-12-15 16:47:45.088349: step 35250, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 55h:25m:59s remains)
INFO - root - 2017-12-15 16:47:51.611312: step 35260, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.675 sec/batch; 55h:42m:46s remains)
INFO - root - 2017-12-15 16:47:58.190781: step 35270, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 54h:29m:44s remains)
INFO - root - 2017-12-15 16:48:04.813140: step 35280, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 53h:19m:03s remains)
INFO - root - 2017-12-15 16:48:11.381309: step 35290, loss = 0.26, batch loss = 0.21 (12.6 examples/sec; 0.636 sec/batch; 52h:30m:47s remains)
INFO - root - 2017-12-15 16:48:17.929488: step 35300, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 53h:16m:14s remains)
2017-12-15 16:48:18.458102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0101023 -4.5137925 -3.7059522 -2.4504321 -2.1209598 -2.1888714 -2.8187449 -3.3502343 -3.8366151 -4.0085149 -3.7563953 -5.12602 -7.1527505 -6.329103 -5.6224022][-4.9266143 -4.9385509 -4.861166 -3.9319272 -3.5275111 -3.3490853 -3.5150847 -4.3110161 -5.1834464 -5.2646904 -5.0190964 -5.8087239 -7.33953 -6.9886713 -7.2556553][-2.7943244 -3.5534797 -4.2137246 -3.5815275 -3.6663549 -3.1339569 -3.2174621 -3.2059627 -3.5432179 -4.3411269 -5.0871544 -6.1828594 -7.9457269 -7.2228355 -6.5195584][-3.5174725 -3.389775 -2.4367123 -1.5209522 -1.3235254 -0.38928175 -0.15692425 -0.51740742 -1.2424479 -1.9455402 -2.6239128 -4.4378309 -7.123661 -7.1618834 -7.0610127][-4.3461704 -4.0741019 -3.6749275 -1.5488296 0.33623457 1.293674 1.496984 1.1093965 0.26766825 -0.62408924 -1.6746244 -3.7765188 -6.4318538 -6.6732812 -6.5279241][-4.8420887 -4.1071334 -2.3539798 -0.85079384 -0.04739666 1.3759995 1.8965406 1.9452796 1.5188169 0.097191811 -1.2772303 -3.0060403 -5.3729267 -5.3266931 -5.0791593][-7.1548896 -5.8063207 -3.6021571 -1.2939172 0.16827583 0.98602819 1.7202301 2.61204 2.6436915 1.7073307 0.53783846 -1.6937137 -4.7559772 -4.1837668 -3.7460537][-7.015224 -5.4764681 -3.4139884 -1.3067555 -0.56489086 0.881691 2.1598597 2.4792829 2.552074 2.9882388 2.8355069 -0.30284119 -4.197866 -4.0993223 -4.0281196][-7.5251923 -5.7430892 -3.3036866 -1.6465693 -0.68213034 0.14880657 0.65473652 1.8467102 2.9002395 2.9639745 2.4217725 -0.37000751 -3.920959 -4.6541634 -5.784802][-6.3491926 -4.9222069 -3.6466565 -1.4233613 -0.513865 -0.29515123 0.46990776 1.3990259 2.250926 2.7341342 2.2565813 -0.670701 -3.9635429 -4.4370852 -4.983532][-5.4170485 -4.9731536 -3.8778896 -2.1401694 -1.5333385 -1.1811152 -0.9394021 -0.084358692 0.47032642 0.34686852 -0.52511597 -2.8213031 -4.5838447 -4.9980078 -5.0927606][-9.1591463 -8.2300568 -6.5586367 -4.7741189 -4.5067344 -3.972229 -3.3777936 -3.3667939 -3.4475257 -3.6813722 -4.2250977 -5.1204824 -6.2214975 -6.4371009 -6.3729534][-11.144236 -9.7979317 -7.8116484 -6.7564483 -6.2734838 -5.7298188 -5.3539009 -5.0983319 -5.2703571 -6.243856 -6.8858109 -6.6915603 -6.3741107 -5.2404413 -3.867578][-7.4963746 -6.7697978 -5.2093749 -3.6910827 -3.1006331 -3.3648534 -3.5956478 -3.8982892 -4.4782124 -4.6075149 -4.5707645 -4.410562 -4.2697468 -2.9906397 -1.9785335][-5.6644268 -4.5330873 -3.2134233 -2.1549277 -1.6904836 -1.7776878 -2.1235201 -2.5328665 -3.0109866 -3.3721619 -3.7384543 -3.9271641 -4.29173 -3.9155748 -3.6891561]]...]
INFO - root - 2017-12-15 16:48:25.061563: step 35310, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 55h:37m:29s remains)
INFO - root - 2017-12-15 16:48:31.572821: step 35320, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 55h:12m:52s remains)
INFO - root - 2017-12-15 16:48:38.049898: step 35330, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 53h:41m:45s remains)
INFO - root - 2017-12-15 16:48:44.631764: step 35340, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 54h:37m:44s remains)
INFO - root - 2017-12-15 16:48:51.198460: step 35350, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 53h:33m:57s remains)
INFO - root - 2017-12-15 16:48:57.769892: step 35360, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.682 sec/batch; 56h:16m:34s remains)
INFO - root - 2017-12-15 16:49:04.326594: step 35370, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 54h:30m:05s remains)
INFO - root - 2017-12-15 16:49:10.904748: step 35380, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.697 sec/batch; 57h:33m:47s remains)
INFO - root - 2017-12-15 16:49:17.481448: step 35390, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 55h:09m:54s remains)
INFO - root - 2017-12-15 16:49:24.021946: step 35400, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 55h:28m:03s remains)
2017-12-15 16:49:24.532024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5647888 -3.4019234 -3.9254823 -4.1987476 -4.5644574 -4.8700795 -4.9673991 -5.076529 -5.1254511 -4.6217861 -3.6650383 -4.349669 -5.4756742 -6.8868313 -7.51626][-4.5120006 -3.1360292 -2.4103267 -2.786335 -4.1726513 -4.3231931 -4.3561349 -4.0555 -3.3785946 -3.0337777 -2.1856494 -3.44227 -4.7979875 -6.6016445 -8.0300617][-1.929004 -1.7659938 -2.5679529 -2.4832063 -2.955287 -3.5648856 -3.5489125 -3.90866 -4.4165049 -3.800983 -2.8482022 -3.5635703 -4.6136732 -6.178792 -7.3296366][-1.4000144 -1.7719646 -2.4361877 -2.6821711 -3.3779826 -3.2558646 -3.0899193 -3.6995237 -3.643259 -3.5995347 -3.4802155 -4.7712016 -5.7913814 -6.7436142 -7.2392378][-2.2507956 -2.2382381 -2.7977152 -2.7534261 -2.6949489 -2.4455256 -1.9797463 -2.5306728 -2.5661092 -2.9316077 -3.8168998 -5.259007 -6.5614719 -8.4667635 -9.3846884][-3.9033902 -3.8902168 -3.3414068 -2.0008454 -1.2157125 0.81155872 1.3024731 1.0734529 0.89079189 -0.63723516 -1.6773672 -3.779305 -6.037446 -8.2149067 -9.57364][-5.9889851 -4.847158 -3.2515326 -1.295692 -0.083762169 1.8671832 4.0410237 4.2436996 3.4902177 1.6695852 -0.11756039 -3.2033353 -5.3400941 -6.721674 -7.3618646][-6.4220285 -6.0949903 -3.7016339 -1.7874751 -0.33937836 2.4747119 3.667994 4.2653518 5.3530087 3.7724252 1.5491562 -1.7001982 -4.331521 -5.9053941 -6.189239][-6.2261291 -5.512517 -4.6074438 -2.0619564 -1.4199481 0.31816149 1.9530602 3.1469445 4.3687024 3.5974288 2.0080967 -2.0236938 -5.2655692 -6.6814055 -6.9371252][-6.5271912 -5.9249797 -4.7264991 -4.0824237 -2.8642356 -1.9872396 -1.0182762 0.58784914 1.2577934 1.1045208 0.98827887 -2.4599204 -5.5154362 -7.4892583 -9.3276157][-7.64093 -7.0114193 -6.116631 -5.2599769 -4.9830489 -4.404624 -3.5097213 -2.9370697 -2.9468763 -3.1533771 -3.9311166 -6.1515021 -7.8260727 -8.7308378 -8.9834557][-11.154062 -9.68484 -8.376379 -7.1860919 -5.9883647 -5.8700619 -5.2659173 -5.4122963 -5.9475031 -6.5298505 -7.301434 -7.7524195 -7.7678185 -7.9110894 -8.5103941][-11.561444 -10.550755 -8.417058 -6.5279574 -6.7371273 -6.5453925 -6.5189881 -7.1527214 -8.0031366 -8.6827736 -9.06633 -8.4519978 -8.7521019 -8.3109589 -7.4107332][-10.318317 -8.2836218 -7.655942 -6.3812022 -5.0177083 -5.29824 -5.0314183 -5.7568493 -7.0775614 -7.4518738 -7.7390432 -7.0392962 -6.8176322 -7.3019028 -6.9781871][-6.5501218 -6.1288133 -5.6684356 -4.8041553 -4.4811277 -4.3929114 -4.5040641 -4.8925481 -5.5577168 -5.9826593 -6.0341239 -7.0436153 -8.2302494 -7.8348761 -8.4412527]]...]
INFO - root - 2017-12-15 16:49:30.994461: step 35410, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 53h:04m:45s remains)
INFO - root - 2017-12-15 16:49:37.547782: step 35420, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 54h:48m:55s remains)
INFO - root - 2017-12-15 16:49:44.100522: step 35430, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 55h:09m:35s remains)
INFO - root - 2017-12-15 16:49:50.712971: step 35440, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.692 sec/batch; 57h:04m:07s remains)
INFO - root - 2017-12-15 16:49:57.351512: step 35450, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 55h:02m:55s remains)
INFO - root - 2017-12-15 16:50:03.963793: step 35460, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 54h:27m:28s remains)
INFO - root - 2017-12-15 16:50:10.518693: step 35470, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 52h:40m:29s remains)
INFO - root - 2017-12-15 16:50:17.096541: step 35480, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 53h:55m:37s remains)
INFO - root - 2017-12-15 16:50:23.664075: step 35490, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 53h:58m:40s remains)
INFO - root - 2017-12-15 16:50:30.246463: step 35500, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 53h:35m:41s remains)
2017-12-15 16:50:30.752470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1372428 -5.7899246 -5.4014106 -5.4012322 -5.9062705 -5.4066405 -4.750001 -4.3193388 -3.6088431 -3.276454 -3.3500302 -4.1948657 -6.5100346 -7.3546305 -6.4004526][-6.5610485 -6.264082 -5.0295448 -5.3637476 -6.0517406 -5.6431513 -5.2779226 -5.0331125 -4.322753 -3.9723392 -4.33966 -4.9292231 -7.5718589 -9.1237059 -7.82038][-4.9379663 -5.0151 -5.497344 -5.82068 -5.7267261 -5.29846 -4.5145731 -3.8114605 -3.9133801 -4.1552348 -4.4214411 -5.6010852 -8.4958181 -9.5024385 -9.2034178][-5.3017211 -4.8775697 -4.2055526 -4.9848022 -6.1007686 -5.9974694 -5.0149808 -4.4436703 -3.9786956 -3.9042819 -4.3213034 -5.0890379 -7.7293091 -8.56518 -8.0193844][-4.7657266 -5.3665171 -4.6772256 -4.2076731 -4.2129192 -3.1060512 -2.1070552 -2.9263597 -4.0458231 -4.0081329 -4.3805542 -4.6169324 -6.41637 -6.8318992 -6.4937181][-6.2546139 -5.819047 -4.1297321 -3.5426552 -2.5361345 -0.6564374 1.0095372 0.78213263 -0.00051546097 -1.1485543 -2.14153 -2.5160148 -4.7207375 -5.5746703 -5.7115517][-7.2718906 -6.66139 -5.1888733 -2.9686074 -1.083704 0.87130594 2.9290738 3.1438041 3.0069718 1.3759422 -0.9770546 -1.5873728 -3.2353091 -3.88904 -4.0563669][-6.2161274 -5.64136 -3.3095398 -0.31157732 1.5950551 3.5123391 4.4625907 4.3428607 3.874495 2.1381245 0.10420132 -1.2461572 -3.2157087 -3.2082572 -1.7728775][-4.0623193 -4.0224352 -2.5568686 -0.0773716 1.9192505 3.6071382 4.1697831 3.5082011 2.3148475 0.49651003 -0.67599297 -1.1259756 -2.6275351 -2.4875474 -0.65332317][-2.4224055 -2.7953489 -2.4112136 -1.3943954 -0.3230319 0.40091133 0.83416939 0.49078369 -0.85933781 -2.2352629 -3.0411463 -2.9703398 -3.7464406 -2.9633269 -1.0590506][-8.3348351 -8.036459 -6.6929994 -5.1824174 -3.9680886 -4.0247669 -4.4304304 -4.8164673 -5.24303 -5.6187596 -5.8566179 -5.7143455 -5.8384628 -4.5175519 -2.0096135][-12.230364 -11.982401 -9.8857594 -7.9711294 -6.375062 -6.3921447 -7.7581606 -8.8112335 -9.3443155 -9.5825806 -8.1780949 -6.2725172 -5.2326531 -3.7756805 -1.8658433][-11.186695 -10.6196 -9.0717773 -7.0776997 -5.8816142 -5.68606 -7.167768 -8.7584229 -10.09048 -9.8632069 -8.0543842 -6.5399013 -5.0577335 -3.660573 -1.7204149][-8.52314 -7.7715445 -6.3815594 -5.24151 -4.4645782 -4.5322552 -5.8703103 -6.7954307 -8.2953005 -8.6638327 -8.0525761 -5.8636303 -3.9262609 -3.1075377 -2.1061215][-6.386972 -5.5498142 -4.565217 -3.9513426 -3.1882963 -2.7065322 -3.2046132 -4.26894 -6.0183525 -6.0990534 -5.9515519 -5.3449068 -4.65449 -3.9927824 -4.1922455]]...]
INFO - root - 2017-12-15 16:50:37.296848: step 35510, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 53h:45m:37s remains)
INFO - root - 2017-12-15 16:50:43.941313: step 35520, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 55h:15m:01s remains)
INFO - root - 2017-12-15 16:50:50.524784: step 35530, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 55h:04m:56s remains)
INFO - root - 2017-12-15 16:50:57.077371: step 35540, loss = 0.21, batch loss = 0.16 (11.8 examples/sec; 0.679 sec/batch; 55h:59m:07s remains)
INFO - root - 2017-12-15 16:51:03.652762: step 35550, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 54h:50m:23s remains)
INFO - root - 2017-12-15 16:51:10.175002: step 35560, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.688 sec/batch; 56h:45m:05s remains)
INFO - root - 2017-12-15 16:51:16.733376: step 35570, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 54h:21m:10s remains)
INFO - root - 2017-12-15 16:51:23.260151: step 35580, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 53h:33m:12s remains)
INFO - root - 2017-12-15 16:51:29.817049: step 35590, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 52h:15m:49s remains)
INFO - root - 2017-12-15 16:51:36.411840: step 35600, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.681 sec/batch; 56h:07m:46s remains)
2017-12-15 16:51:36.933975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9482718 -2.3029804 -1.7049437 -1.0188665 -1.7558658 -2.5204294 -2.7366707 -2.6471713 -1.4054518 0.84674931 2.7103467 1.2962828 1.9572873 -0.57559347 -2.4244239][-3.5205 -3.2193079 -3.0353055 -3.1225047 -3.7494774 -5.0878348 -4.8847065 -3.8251731 -2.2537355 -0.29428434 1.8318534 0.75915432 1.2519155 -0.19655418 -0.73330927][-3.5014935 -3.3948126 -2.6580689 -2.7125249 -3.9450619 -4.803091 -5.3837891 -5.0722857 -2.7959347 -1.7894938 -1.1854453 -2.0077751 -1.5744133 -2.5640388 -2.6230059][-5.8227286 -5.3272519 -4.4157791 -4.4341011 -4.7005205 -5.3061905 -4.9762168 -4.5046945 -3.5992997 -2.373781 -1.448524 -4.2084761 -4.6966591 -5.1697764 -4.7348022][-7.83711 -6.8631058 -5.9770341 -4.8563337 -3.7237618 -2.7851439 -2.6556528 -3.8182588 -4.4749856 -4.2724233 -3.5544631 -5.6865811 -6.0572004 -6.927402 -6.8634105][-9.9277163 -8.2197485 -5.8081274 -3.0241606 -0.76154184 1.3729439 2.764318 1.7922702 0.35440874 -2.1604939 -4.1867394 -6.5586267 -6.4627509 -7.4387488 -7.7536311][-11.649029 -10.007902 -6.7497349 -2.410995 0.57413197 3.3519359 5.8544602 5.4890647 4.3236003 0.38393497 -3.3371959 -6.23248 -6.6306129 -7.6944494 -7.15739][-12.805079 -11.179981 -8.060215 -3.1290729 0.14496136 4.1443086 7.2784648 6.7475371 6.0907826 1.833312 -2.8191464 -7.2262549 -8.4348726 -9.0310926 -7.5754433][-11.441015 -10.579042 -8.1332865 -4.6669636 -2.043072 1.9583364 3.9634585 3.9003081 4.069859 0.52431583 -2.7836668 -7.747859 -9.77724 -10.207558 -8.514123][-9.80138 -9.3451662 -7.5681248 -5.3158932 -4.0820637 -1.2003603 0.30964947 0.26033878 0.21050739 -2.643255 -5.2737126 -9.4479084 -10.59611 -11.506718 -11.390539][-12.037775 -11.844778 -9.6312647 -7.4693737 -6.4321518 -4.9145055 -4.8424187 -5.3917284 -5.9486032 -7.4841051 -8.4508476 -11.895926 -13.176964 -12.926932 -11.942364][-16.910465 -16.835409 -14.86511 -12.433029 -11.290905 -10.38932 -10.890781 -10.780205 -10.543871 -11.350746 -11.652537 -12.721958 -12.197328 -11.774611 -11.050838][-18.048145 -17.189011 -15.285519 -14.073206 -14.131636 -12.791765 -12.194672 -12.503981 -12.921824 -12.413855 -11.938919 -12.046143 -10.97633 -9.2808113 -7.7301598][-14.896915 -14.712133 -14.551203 -13.123747 -11.9816 -12.16704 -12.234609 -11.128092 -10.216548 -10.565969 -10.837756 -9.7619867 -8.5561609 -7.4945498 -6.534358][-9.6304445 -9.21352 -8.750288 -8.28665 -7.6708484 -6.67228 -6.4189234 -7.1470571 -7.4627404 -6.9541173 -6.8493547 -7.7552271 -8.6343184 -8.4526234 -8.0978985]]...]
INFO - root - 2017-12-15 16:51:43.584242: step 35610, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 55h:13m:02s remains)
INFO - root - 2017-12-15 16:51:50.035698: step 35620, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 54h:16m:23s remains)
INFO - root - 2017-12-15 16:51:56.671590: step 35630, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 55h:37m:15s remains)
INFO - root - 2017-12-15 16:52:03.333724: step 35640, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 55h:33m:52s remains)
INFO - root - 2017-12-15 16:52:09.875544: step 35650, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 54h:44m:09s remains)
INFO - root - 2017-12-15 16:52:16.451647: step 35660, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 53h:40m:21s remains)
INFO - root - 2017-12-15 16:52:23.037019: step 35670, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 53h:49m:45s remains)
INFO - root - 2017-12-15 16:52:29.582054: step 35680, loss = 0.17, batch loss = 0.12 (12.7 examples/sec; 0.632 sec/batch; 52h:05m:03s remains)
INFO - root - 2017-12-15 16:52:36.073072: step 35690, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 54h:08m:27s remains)
INFO - root - 2017-12-15 16:52:42.695409: step 35700, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 53h:52m:39s remains)
2017-12-15 16:52:43.216327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0745683 -6.738812 -7.2808275 -7.4968061 -7.3505888 -7.5039148 -7.8227482 -7.5817938 -6.7344985 -5.92281 -5.1605268 -7.6294847 -8.4341164 -8.8620682 -10.08293][-2.9438434 -4.4953947 -5.7622566 -5.1014633 -5.7516794 -6.1691394 -6.9478755 -7.1186228 -6.8417115 -6.330904 -5.1222544 -7.8863173 -9.3774614 -9.5113716 -10.242685][-2.4326015 -3.8250031 -4.2304878 -4.4193177 -5.5011106 -5.6736345 -6.118073 -5.9850526 -6.1568871 -6.6995764 -7.2404 -9.7810574 -10.843355 -11.029793 -12.354555][-2.7540169 -3.1469326 -3.3906064 -4.2774758 -4.8884821 -4.695518 -4.597609 -5.3171272 -5.0210505 -4.8313975 -4.6880703 -8.5409155 -10.860876 -10.718517 -11.530495][-2.6176927 -4.2080784 -4.906239 -4.1820364 -4.0886488 -3.7755408 -2.9563303 -3.7964721 -4.319459 -4.5519462 -3.8091407 -6.6572757 -8.4605961 -8.90103 -10.596132][-3.4223812 -4.0963945 -4.9876561 -3.8709586 -2.3480272 -1.0822673 0.15593672 -0.35852003 -1.6380725 -2.3070364 -2.2591093 -5.0843658 -6.7030363 -7.5093942 -9.0654869][-3.2763979 -3.9781365 -4.0434265 -1.4471493 0.17420673 1.3848004 3.3101401 3.6924815 2.8554959 0.74881744 -1.7959321 -4.9539332 -6.588521 -6.5580883 -8.3559895][-3.5137551 -3.0645895 -2.9629359 -0.700315 0.78407145 2.2029557 3.5099397 3.4669137 3.6009984 1.9087915 0.15012646 -4.2620244 -7.7501507 -8.2379322 -9.6346979][-4.7764359 -3.7695146 -3.3548055 -1.9027107 -1.4480753 0.4121623 2.3258986 2.8903508 3.4556546 2.5171952 1.2467885 -4.6054039 -9.9951763 -12.580141 -13.916843][-5.4882946 -5.4737868 -4.7279663 -2.6482275 -2.3727596 -1.1492019 0.341506 0.35084915 1.4706302 2.1963615 2.0315824 -4.073596 -8.6790066 -12.094872 -15.935467][-7.372067 -7.5280838 -7.4592957 -5.2812176 -4.3079557 -3.5440841 -2.3070374 -2.0693755 -1.7456243 -1.4375587 -2.1848161 -7.8597422 -11.379655 -14.662268 -17.038013][-10.575615 -10.367081 -10.829579 -9.7525826 -9.6930761 -9.1042747 -8.3417053 -7.2304068 -5.961874 -5.2446256 -5.6395826 -10.517958 -13.874001 -16.552614 -17.26605][-14.493214 -14.052389 -14.09273 -13.065239 -13.174215 -12.664531 -12.803471 -12.274073 -10.693887 -9.1689005 -8.7699556 -11.654259 -13.667295 -15.600965 -16.229254][-14.799614 -15.161747 -14.086987 -12.236136 -12.394555 -13.404249 -13.574205 -11.919062 -11.016766 -10.104198 -9.8692036 -10.992191 -11.05348 -13.226074 -13.710842][-11.959853 -12.257399 -12.055282 -11.013615 -10.16249 -9.8265171 -10.241722 -10.459402 -10.576077 -9.674427 -9.3400106 -10.457393 -11.084996 -13.042286 -13.187986]]...]
INFO - root - 2017-12-15 16:52:49.817845: step 35710, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.649 sec/batch; 53h:30m:34s remains)
INFO - root - 2017-12-15 16:52:56.334318: step 35720, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 53h:34m:06s remains)
INFO - root - 2017-12-15 16:53:02.932881: step 35730, loss = 0.28, batch loss = 0.23 (12.2 examples/sec; 0.658 sec/batch; 54h:14m:31s remains)
INFO - root - 2017-12-15 16:53:09.476796: step 35740, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 54h:35m:05s remains)
INFO - root - 2017-12-15 16:53:16.041314: step 35750, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 54h:45m:15s remains)
INFO - root - 2017-12-15 16:53:22.623499: step 35760, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 53h:08m:30s remains)
INFO - root - 2017-12-15 16:53:29.310722: step 35770, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 53h:34m:50s remains)
INFO - root - 2017-12-15 16:53:35.915081: step 35780, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.637 sec/batch; 52h:28m:13s remains)
INFO - root - 2017-12-15 16:53:42.431975: step 35790, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 53h:49m:41s remains)
INFO - root - 2017-12-15 16:53:48.981829: step 35800, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 53h:59m:33s remains)
2017-12-15 16:53:49.523143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3880076 -5.2200594 -5.0545864 -4.9016714 -5.4965224 -6.1230383 -6.8344107 -6.599493 -6.1640668 -5.6765075 -4.4007483 -6.0423651 -8.7277708 -9.7332754 -8.5110569][-4.1746273 -4.9410315 -4.5094333 -4.62967 -5.4536471 -6.3361859 -6.4379239 -6.8582611 -6.4638305 -5.2975726 -4.4410748 -5.6870561 -6.5565033 -8.4349871 -9.2131615][-3.4837594 -4.8035612 -6.0297 -6.0307503 -5.7129498 -5.9760942 -6.3341904 -5.6524878 -5.1893396 -5.3203197 -4.46445 -6.6759291 -9.2543535 -8.8736029 -8.7817974][-5.080441 -5.8654337 -6.1730828 -5.9526386 -5.9163971 -5.547677 -5.1948323 -5.3043094 -5.1062875 -4.4520626 -3.9605772 -6.0062647 -8.0833206 -10.269171 -10.712526][-6.1123061 -7.8276567 -8.96767 -7.5664034 -5.8381228 -3.9297194 -3.0601194 -2.1225586 -1.8585501 -3.03323 -3.0808525 -5.4604006 -8.4196939 -10.395164 -10.632932][-7.2355323 -8.141161 -8.5192251 -6.5066061 -3.8566489 -0.76779222 1.7006431 2.0536079 1.7322602 1.0309634 -0.18411875 -3.0870864 -5.9318419 -8.0231237 -9.6505394][-6.8570628 -7.9121943 -7.0853586 -4.7340689 -1.6282754 2.2222056 5.1860032 5.5659003 5.0187888 2.4540277 0.59390974 -2.8895879 -6.8068476 -8.359869 -8.7403145][-5.522367 -6.5740705 -5.9037123 -3.3868539 0.065986156 4.1086669 7.6142554 6.7935138 5.3869748 3.8979373 1.9607425 -2.7067947 -6.6297979 -9.1184626 -10.253201][-5.5255847 -6.3087478 -5.7658553 -3.9955707 -2.1652248 1.2450633 3.6901145 4.6670194 4.3159118 1.5250797 -0.19695187 -4.2086368 -8.9486828 -11.603928 -12.881609][-4.93791 -6.2167296 -7.2829161 -5.2913041 -4.5153112 -2.8242416 -0.3534584 0.70635414 0.18506908 -1.174695 -3.2831869 -7.5737281 -10.861628 -13.347383 -14.959824][-8.2779026 -8.85424 -8.9103317 -7.6464181 -7.3310404 -6.5400214 -6.0219746 -5.4207487 -5.6088891 -6.6071339 -7.6314373 -11.251561 -14.168507 -15.887323 -15.653084][-11.099531 -11.04295 -10.360503 -8.3071156 -7.3059177 -6.70194 -7.1509256 -9.1142073 -9.72463 -9.6887226 -10.769262 -11.446904 -11.595997 -13.952881 -13.920509][-13.980225 -13.445242 -10.892841 -9.5502586 -8.96446 -8.5137339 -9.2501287 -10.253481 -11.30183 -11.505272 -10.477846 -10.375148 -10.172271 -10.329861 -9.8540154][-13.402241 -12.976672 -11.482615 -8.7241592 -7.1467228 -6.66675 -6.7094707 -7.5995269 -8.1510344 -8.8547955 -8.1241817 -6.026248 -5.8994122 -5.8474522 -5.4706297][-10.270081 -10.532091 -8.9663553 -6.746016 -5.0708237 -5.8355808 -6.566 -7.0162191 -8.05808 -7.2984562 -6.702785 -6.3712668 -5.13937 -4.8333344 -4.7087932]]...]
INFO - root - 2017-12-15 16:53:56.107864: step 35810, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 53h:35m:22s remains)
INFO - root - 2017-12-15 16:54:02.663780: step 35820, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 53h:49m:27s remains)
INFO - root - 2017-12-15 16:54:09.300539: step 35830, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 55h:02m:42s remains)
INFO - root - 2017-12-15 16:54:15.860208: step 35840, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 54h:09m:58s remains)
INFO - root - 2017-12-15 16:54:22.404712: step 35850, loss = 0.17, batch loss = 0.12 (12.8 examples/sec; 0.627 sec/batch; 51h:40m:10s remains)
INFO - root - 2017-12-15 16:54:28.957771: step 35860, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 53h:12m:23s remains)
INFO - root - 2017-12-15 16:54:35.530526: step 35870, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 53h:41m:22s remains)
INFO - root - 2017-12-15 16:54:42.084772: step 35880, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 54h:13m:37s remains)
INFO - root - 2017-12-15 16:54:48.685651: step 35890, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 56h:20m:09s remains)
INFO - root - 2017-12-15 16:54:55.246198: step 35900, loss = 0.22, batch loss = 0.17 (12.6 examples/sec; 0.634 sec/batch; 52h:15m:34s remains)
2017-12-15 16:54:55.719909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8855667 -8.3179817 -8.148777 -8.8447752 -9.5741673 -9.9561367 -9.2745514 -7.4367151 -5.6007667 -5.0082359 -4.5733314 -6.614224 -8.0183039 -6.9737749 -5.82721][-8.3267956 -7.6033449 -6.5851369 -7.0845332 -8.0688229 -8.1701136 -7.6588569 -6.7849469 -6.1673307 -4.9522686 -4.6239533 -6.9541335 -8.64289 -7.3485627 -6.4256506][-6.5900917 -6.3649344 -5.5242977 -4.9839344 -6.143981 -7.0020947 -6.493773 -5.4347696 -4.6217775 -4.4936543 -5.1454992 -7.3455791 -8.6264505 -7.661284 -8.0309315][-8.35429 -7.3716135 -5.0220852 -4.42777 -5.6322446 -5.9862738 -5.6596279 -5.51269 -4.9759178 -4.31082 -4.6611238 -7.7359667 -9.9475317 -9.1037388 -9.9404173][-10.575128 -9.8350706 -7.499227 -5.6513915 -4.6028457 -4.1795373 -3.4676378 -3.0822778 -3.1191456 -3.2480383 -3.9200528 -6.8041682 -8.8601608 -8.521471 -9.3109226][-13.024608 -9.7536163 -6.2749152 -2.8074112 -1.2194271 0.499475 2.2805176 1.5699453 0.914413 -0.35943127 -2.5440505 -4.853663 -6.7891159 -7.51954 -7.7729368][-12.495955 -10.393358 -5.1841912 -0.35026026 2.6859584 4.7603641 6.9300427 7.0987735 6.0274472 2.4021916 -1.7316189 -4.7647171 -7.4924011 -8.0759668 -7.9998922][-11.835772 -9.3896809 -4.9962244 -0.30110407 1.8596878 4.7412534 7.8517184 7.2792439 6.2123437 2.9129376 -0.38121367 -5.3924413 -9.6408739 -9.9562082 -10.151417][-8.9358521 -8.3441639 -5.500628 -1.7517965 0.13006592 2.7728944 5.3307109 5.781445 5.6547551 2.1082025 -0.93172932 -6.0971508 -10.702538 -10.817411 -11.265914][-9.315733 -8.2708282 -5.4990129 -2.53346 -0.30076933 1.6453271 3.2439122 3.6811538 4.0452905 1.5105815 -1.4382987 -6.9916105 -11.063322 -11.744259 -13.085451][-11.523993 -10.118055 -9.1470013 -6.7782087 -5.3924055 -4.0252004 -3.14277 -2.8528008 -2.9295058 -3.8094831 -5.0873713 -10.917692 -13.960394 -13.324601 -12.329561][-16.119776 -15.154596 -12.700579 -11.437113 -11.201049 -9.8084755 -8.9381638 -10.042284 -9.80975 -10.13372 -11.158871 -14.090158 -14.929766 -14.887737 -14.152683][-17.333838 -15.964781 -15.229729 -13.875927 -12.4562 -11.669165 -11.066393 -11.414832 -11.965654 -11.899845 -11.691738 -12.707341 -13.133163 -11.979368 -9.9175262][-15.5273 -14.37496 -12.581184 -11.956248 -10.975746 -9.8865623 -10.306894 -9.8743839 -9.5623217 -9.774272 -10.277799 -10.368357 -9.2583561 -8.8052788 -8.0689411][-11.123266 -10.086834 -8.59339 -6.9499679 -6.2136655 -6.281527 -5.8439217 -6.2651491 -7.433116 -6.8623023 -7.1376109 -8.3770285 -9.75511 -8.7439871 -7.9497576]]...]
INFO - root - 2017-12-15 16:55:02.321987: step 35910, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 52h:41m:44s remains)
INFO - root - 2017-12-15 16:55:08.952164: step 35920, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 55h:13m:59s remains)
INFO - root - 2017-12-15 16:55:15.550685: step 35930, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.653 sec/batch; 53h:49m:21s remains)
INFO - root - 2017-12-15 16:55:22.088144: step 35940, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 53h:06m:09s remains)
INFO - root - 2017-12-15 16:55:28.684937: step 35950, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 55h:59m:11s remains)
INFO - root - 2017-12-15 16:55:35.388655: step 35960, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 54h:04m:19s remains)
INFO - root - 2017-12-15 16:55:41.941799: step 35970, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 53h:41m:57s remains)
INFO - root - 2017-12-15 16:55:48.497113: step 35980, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.655 sec/batch; 53h:54m:58s remains)
INFO - root - 2017-12-15 16:55:55.141155: step 35990, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.632 sec/batch; 52h:03m:47s remains)
INFO - root - 2017-12-15 16:56:01.704717: step 36000, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 53h:23m:26s remains)
2017-12-15 16:56:02.217831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4402022 -5.56409 -5.6195865 -5.0075312 -4.4422226 -3.8688841 -3.2724514 -2.5283079 -1.6145673 -1.6018362 -1.2297568 -3.9783225 -6.5017247 -7.1873503 -7.2462358][-4.8033118 -4.6512465 -4.5976105 -3.9432919 -3.4124017 -3.2760646 -2.6450968 -1.7278802 -0.7907877 0.027957439 0.36297655 -3.3745472 -5.9601936 -7.4443855 -8.0731573][-2.0247469 -3.5828259 -4.7502742 -3.767406 -3.6910372 -3.031049 -2.2254255 -1.3462996 -0.61319304 -0.81024885 -0.78643513 -3.5982943 -5.61638 -7.0071979 -8.121974][-2.7294672 -3.7806005 -4.5905838 -3.9310398 -4.5593915 -3.4689372 -2.2705259 -1.9346654 -1.5637517 -0.93575811 -0.85969782 -4.1696014 -6.5125 -7.4911556 -7.6554537][-4.2624445 -6.7617073 -7.6353469 -6.2675967 -5.2521672 -2.7644246 -1.3088756 -1.6832218 -2.0598264 -1.2830505 -1.2425585 -4.0336447 -6.0062537 -6.9793735 -7.665596][-5.4729033 -7.2440224 -7.1017032 -4.6626482 -2.8625007 0.031112194 2.2998252 1.484746 0.91021252 -0.22725534 -1.6469469 -3.7191076 -5.4044542 -6.2900033 -6.9863548][-7.8790607 -7.6088347 -6.0797548 -2.9607184 -0.58207512 2.9250684 5.7585044 5.1179976 3.9382215 1.3749113 -0.86295986 -3.5392666 -6.042026 -6.8236475 -7.6623335][-8.423068 -8.0864744 -6.6364717 -2.8995285 0.52738619 4.4342027 7.0937982 6.0727496 4.9503274 2.8816361 0.79301405 -3.3587039 -6.6118784 -7.7002697 -8.581665][-6.6256142 -6.5140414 -6.4063134 -3.3574574 -0.67115211 2.0588813 4.0404897 4.3463902 3.9751801 1.7834144 0.014529705 -4.2038708 -6.6231189 -7.3536325 -8.4191475][-7.0925369 -5.9356122 -5.2900324 -2.8072331 -1.4200354 -0.18340492 1.4191971 2.1026311 1.459609 -0.30900431 -1.9384851 -5.5972304 -7.556736 -8.2801971 -9.0705643][-9.2508259 -8.8271961 -8.2024364 -5.8616319 -5.0793815 -4.2528062 -3.3097639 -2.9468708 -3.3310623 -3.4182539 -4.3265929 -8.4042988 -10.040241 -9.7448311 -9.3820581][-12.141937 -11.548872 -10.916523 -9.3696947 -8.7217474 -7.3639436 -6.7637811 -7.2648034 -7.7215433 -7.3721504 -7.3936291 -9.4733839 -10.179071 -10.38919 -10.333889][-13.533183 -13.024846 -11.74582 -10.584902 -10.628736 -9.0740767 -8.9662762 -9.5418 -9.6983986 -9.5431652 -9.3763332 -10.301608 -10.522528 -9.9604578 -9.321043][-12.137545 -12.106116 -10.776886 -9.5131121 -8.8854055 -8.2476778 -8.4512777 -7.7725015 -8.6753016 -9.3075151 -9.5615273 -9.3421783 -8.8955135 -8.7032461 -8.6981945][-8.144434 -8.1283064 -7.3226418 -6.569232 -5.9396791 -5.4348826 -5.4904113 -5.6609836 -6.341342 -6.5372496 -7.3691497 -8.47818 -8.9427052 -9.0470476 -9.2525187]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 16:56:08.870438: step 36010, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 53h:38m:55s remains)
INFO - root - 2017-12-15 16:56:15.436006: step 36020, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 54h:37m:56s remains)
INFO - root - 2017-12-15 16:56:21.989122: step 36030, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.663 sec/batch; 54h:35m:31s remains)
INFO - root - 2017-12-15 16:56:28.646453: step 36040, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 54h:37m:47s remains)
INFO - root - 2017-12-15 16:56:35.213808: step 36050, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 54h:40m:01s remains)
INFO - root - 2017-12-15 16:56:41.813332: step 36060, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.686 sec/batch; 56h:30m:07s remains)
INFO - root - 2017-12-15 16:56:48.406902: step 36070, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 54h:03m:09s remains)
INFO - root - 2017-12-15 16:56:55.068480: step 36080, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 54h:56m:00s remains)
INFO - root - 2017-12-15 16:57:01.665082: step 36090, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.649 sec/batch; 53h:24m:51s remains)
INFO - root - 2017-12-15 16:57:08.305376: step 36100, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 55h:03m:33s remains)
2017-12-15 16:57:08.906521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6081977 -3.3857105 -2.5429294 -2.8837674 -4.425396 -5.1217079 -5.8747783 -4.9580946 -3.471005 -4.1083632 -4.7812366 -6.1066227 -7.7828822 -7.974009 -7.19997][-3.8841877 -3.3258944 -2.4682522 -2.4291334 -3.2075591 -3.4511464 -2.9355426 -2.0664315 -1.7652283 -2.217855 -2.9212909 -4.1716185 -5.4208417 -6.6910005 -6.6653428][-2.9668782 -3.2004576 -3.8787365 -3.3538775 -3.5892391 -3.3490434 -2.6181412 -1.694591 -1.0463557 -1.5562401 -2.0847182 -3.4036939 -4.6366396 -5.1417003 -5.0418386][-4.8382664 -4.4570537 -4.924952 -4.784997 -5.5111051 -4.0591936 -2.695266 -2.099411 -1.3148947 -0.96351719 -0.94902515 -2.8635321 -5.088212 -6.0160007 -4.8606668][-3.4502864 -4.9108667 -4.8502574 -4.5051589 -5.7833152 -4.3765273 -2.1661062 -1.0966196 -0.84743309 0.28578234 1.6503744 -0.49873495 -3.118026 -5.36929 -6.189168][-3.8621893 -3.8994751 -3.4481213 -2.4040904 -1.32019 -0.8099165 0.18372297 2.0140142 1.5451713 0.81444645 0.48451471 -1.0569706 -3.1718094 -4.8293347 -4.9361906][-3.366267 -2.5441976 -2.1570008 -0.54944992 1.4234409 3.3955102 4.2511191 4.1441073 3.7727771 2.4996524 0.93841457 -0.94663 -2.7054603 -4.9553027 -5.7430525][-2.8677855 -2.2604172 -2.6038644 0.42209005 3.7149529 5.378859 7.1715312 5.9704108 3.5998178 2.323051 1.6992383 -0.18625355 -2.0572617 -4.0842686 -4.7141609][-3.9363637 -2.5121958 -1.7581935 -0.32467556 1.1845307 4.440433 5.9497323 5.0316091 2.7776942 0.74533319 0.16619682 -1.0117764 -2.7538664 -4.2168202 -4.1856613][-4.32879 -3.960474 -2.2478402 -0.36117554 -0.2237215 1.4486561 3.2523551 3.1212287 1.4337201 -0.16373158 -0.87266779 -1.9083679 -2.8565249 -4.1109238 -4.6059117][-6.7810636 -6.0986538 -5.3430934 -4.0472231 -3.047111 -2.3285701 -2.160383 -1.6218052 -2.1675136 -3.0525551 -2.9466543 -3.7640989 -5.1657667 -5.7475548 -5.3229914][-10.330796 -8.5467367 -6.7497764 -5.6640735 -6.3284407 -5.9811773 -5.912518 -6.0358739 -6.1640439 -6.1186414 -5.7825732 -6.3155279 -7.2479439 -7.7439637 -5.8842568][-10.017836 -8.9699774 -8.0457916 -6.5675464 -6.8254328 -7.8775339 -8.031518 -7.7912817 -7.663784 -6.740881 -5.9836583 -6.5509176 -7.1171484 -6.1291294 -4.5754232][-9.0628223 -7.4675641 -6.3330545 -5.9287038 -5.823842 -6.4192472 -6.7331219 -6.4141221 -5.8590641 -6.3309951 -6.1877823 -4.4020538 -4.0018134 -3.9241357 -2.736238][-5.0317216 -5.5798855 -5.5500445 -5.2126665 -4.5014181 -4.5041838 -4.1740527 -4.5856924 -4.8998175 -4.0397739 -3.731313 -4.3489223 -4.5851364 -4.6201925 -4.978898]]...]
INFO - root - 2017-12-15 16:57:15.533408: step 36110, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.681 sec/batch; 56h:03m:26s remains)
INFO - root - 2017-12-15 16:57:22.127302: step 36120, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 53h:51m:03s remains)
INFO - root - 2017-12-15 16:57:28.681674: step 36130, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 52h:55m:00s remains)
INFO - root - 2017-12-15 16:57:35.254014: step 36140, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 52h:52m:33s remains)
INFO - root - 2017-12-15 16:57:41.862449: step 36150, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 53h:07m:02s remains)
INFO - root - 2017-12-15 16:57:48.456800: step 36160, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 53h:05m:52s remains)
INFO - root - 2017-12-15 16:57:54.974559: step 36170, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 53h:07m:31s remains)
INFO - root - 2017-12-15 16:58:01.505151: step 36180, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 53h:33m:13s remains)
INFO - root - 2017-12-15 16:58:08.044854: step 36190, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 53h:47m:41s remains)
INFO - root - 2017-12-15 16:58:14.654585: step 36200, loss = 0.16, batch loss = 0.12 (11.4 examples/sec; 0.699 sec/batch; 57h:33m:09s remains)
2017-12-15 16:58:15.155590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4999456 -6.2071419 -6.1135397 -4.71387 -3.3778505 -1.585495 -1.0930829 -1.6557188 -2.2039959 -3.7645321 -5.0267463 -6.1918941 -10.181581 -9.7761526 -8.56764][-3.6355517 -3.4105914 -3.1761613 -2.1271927 -1.8713758 -1.0999289 -0.91359711 -1.4829168 -2.5309651 -4.2939787 -5.6170721 -6.4927673 -8.8923445 -9.5945873 -9.9699192][-1.533648 -2.2009397 -2.5598896 -1.3685679 -1.096487 -0.64701366 -1.1085396 -1.860141 -2.9808815 -4.5530643 -5.6322231 -6.8319635 -9.1817055 -9.15118 -10.115382][-2.667686 -3.5940232 -4.2685757 -3.3703935 -2.8190441 -2.5289083 -2.8868442 -2.9111171 -3.8872092 -4.8580513 -5.2752986 -5.8423047 -8.6014137 -10.002893 -10.581118][-3.9762788 -5.74066 -7.241982 -6.8389044 -6.1172028 -3.951314 -2.4029653 -2.6460085 -2.9523656 -3.0034196 -3.6547415 -4.4490576 -7.2028646 -8.6194229 -9.6511478][-6.3220448 -7.9618258 -8.4298668 -7.660821 -7.0311337 -4.23737 -1.0351872 -0.34513187 -0.50269556 -1.6620622 -2.2828395 -2.4259255 -5.7104363 -7.8702183 -8.8746176][-7.15619 -7.4535732 -7.53138 -6.9005876 -5.9017324 -2.8514361 1.0285101 2.6487985 2.8721156 0.90452957 -1.7648797 -2.7307367 -6.1194735 -7.2918763 -7.9047389][-6.567945 -6.6567864 -6.8169975 -5.5861883 -4.272439 -1.3489113 2.2691631 4.40914 5.041513 3.0577769 0.66245365 -1.6244068 -6.0146384 -7.7372179 -9.2668982][-6.4879646 -6.4248319 -6.456821 -4.815032 -4.049283 -1.7396283 1.1039553 3.4779487 4.0629191 2.4314504 0.20932579 -2.7730427 -7.76315 -9.5765877 -10.429329][-7.3222561 -7.4845643 -8.3579664 -6.6374135 -5.014946 -2.3471916 -0.18473816 1.4984484 2.2350268 1.3594236 -1.1261663 -4.1835623 -9.3588438 -11.681722 -12.599165][-10.736061 -11.558992 -11.468302 -9.8939857 -8.9307213 -6.4912233 -3.3755677 -1.5386577 -1.7244768 -2.4900813 -4.2001195 -7.541369 -11.619473 -12.56813 -12.436405][-12.810764 -12.650172 -12.666517 -10.92572 -9.4375134 -7.6762066 -5.9239993 -4.3475542 -4.143281 -5.4679933 -7.5635834 -9.6846275 -11.486874 -11.820669 -11.094137][-12.609356 -11.424398 -10.528076 -9.1326342 -8.45452 -7.2580929 -6.35252 -5.880825 -5.6327167 -6.4213152 -7.8700638 -9.0997658 -10.491571 -9.898653 -9.0104256][-9.8630781 -8.885931 -7.568274 -7.1695285 -6.6240931 -5.9538403 -5.5020981 -5.1834803 -5.3321991 -5.8080411 -6.81715 -7.0339193 -7.9651365 -7.9297733 -8.0405293][-7.8921614 -6.5830259 -4.9405479 -2.9210331 -1.7489641 -2.8351145 -3.475281 -3.0874276 -3.597589 -4.8707247 -5.5229363 -6.0966253 -6.3473821 -6.7672353 -7.4582205]]...]
INFO - root - 2017-12-15 16:58:21.712682: step 36210, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 54h:34m:53s remains)
INFO - root - 2017-12-15 16:58:28.310796: step 36220, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 53h:48m:53s remains)
INFO - root - 2017-12-15 16:58:34.832136: step 36230, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 54h:04m:02s remains)
INFO - root - 2017-12-15 16:58:41.433044: step 36240, loss = 0.18, batch loss = 0.14 (11.6 examples/sec; 0.687 sec/batch; 56h:34m:28s remains)
INFO - root - 2017-12-15 16:58:48.047325: step 36250, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 53h:24m:54s remains)
INFO - root - 2017-12-15 16:58:54.592023: step 36260, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 53h:58m:35s remains)
INFO - root - 2017-12-15 16:59:01.161016: step 36270, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 54h:54m:31s remains)
INFO - root - 2017-12-15 16:59:07.781447: step 36280, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 54h:00m:12s remains)
INFO - root - 2017-12-15 16:59:14.340610: step 36290, loss = 0.22, batch loss = 0.17 (12.2 examples/sec; 0.653 sec/batch; 53h:46m:07s remains)
INFO - root - 2017-12-15 16:59:20.944936: step 36300, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 54h:07m:23s remains)
2017-12-15 16:59:21.468658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1944265 -3.7169907 -3.5330808 -2.2302732 -1.391922 -2.7523611 -3.7040589 -4.52096 -5.4016633 -4.8783984 -3.5724037 -4.4599791 -5.7208447 -8.2151384 -7.9518614][-3.8552177 -3.3157935 -2.4194152 -1.613306 -1.1579852 -1.8137503 -2.8083203 -4.3897648 -5.9939737 -7.5715332 -7.498497 -8.6530094 -9.32151 -9.7237425 -8.5946436][-0.909605 -1.0032678 -1.2701335 -0.85509682 -1.3440142 -2.1928053 -3.1436083 -4.21507 -5.2081347 -5.6228309 -6.0782995 -9.040309 -9.92921 -10.472177 -10.478577][-0.57395363 -1.2132087 -2.1667149 -3.0626969 -3.3075595 -3.1642244 -3.3974171 -4.151906 -5.1430559 -5.6012869 -5.5249643 -8.5180779 -10.480923 -10.512448 -9.5829926][-0.73625088 -1.9061561 -2.9989097 -3.457135 -3.5774429 -2.8426807 -2.4715166 -2.7305911 -3.0595369 -3.693789 -4.4500246 -7.5201769 -9.6352806 -10.981111 -10.947384][-2.1590564 -2.64551 -2.5309894 -1.7853265 -1.9205637 0.19574833 1.8154464 1.5831285 0.98452091 -1.4307556 -3.2350817 -4.9891992 -6.1634197 -8.1690817 -9.1168966][-3.7681377 -2.5417323 -1.0331421 -0.37181711 -0.42520523 1.4334016 2.3271375 3.2904077 4.282558 1.6629906 -0.595171 -3.868669 -5.6204367 -6.9641457 -6.837316][-5.1468849 -4.6494865 -2.1553116 0.413311 2.1139708 3.7544703 4.0215468 4.2976947 5.2430854 3.3672929 0.88613558 -2.9837561 -4.985136 -6.55682 -6.6901054][-5.5849385 -5.0972223 -3.061516 0.0057458878 1.7886558 3.9708123 4.5575194 4.0583005 3.6330514 3.3321891 2.6560569 -1.5136971 -4.4177527 -5.4761157 -4.5714312][-6.1393647 -6.0287318 -4.1652021 -1.0853133 0.39461136 1.8324652 2.936029 3.8466382 3.6977477 2.7574248 1.0368905 -2.535182 -4.2414422 -5.1266389 -4.6507058][-10.626879 -9.5474434 -6.6691613 -3.9186573 -2.183682 -1.189105 -0.86482477 -0.74108076 -1.0089517 -1.5650806 -2.5962746 -5.1394286 -6.20673 -6.3466573 -5.4174023][-12.5912 -12.184272 -9.6977386 -6.1578388 -4.1929126 -3.6707294 -4.188797 -4.592895 -3.9270039 -4.133698 -4.9255257 -6.1546531 -5.8016486 -6.3377748 -6.8374352][-13.988592 -12.026745 -9.5126114 -7.59647 -6.8480439 -5.7803493 -5.6365876 -5.8762846 -5.9184556 -5.7394295 -5.9147496 -6.9570675 -6.1300092 -4.8793597 -3.982343][-10.684888 -11.176323 -10.420879 -8.024435 -6.3167844 -6.5040841 -6.5019393 -5.401413 -4.9199076 -5.2984762 -5.4496942 -5.2222934 -5.309947 -5.259048 -4.9551158][-7.7066307 -8.116312 -7.7486944 -7.2943926 -6.8227568 -6.0895166 -5.86125 -5.8067055 -5.6309938 -5.8431311 -5.6766987 -6.32537 -6.7615767 -6.0866861 -6.1104274]]...]
INFO - root - 2017-12-15 16:59:28.010859: step 36310, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 55h:15m:00s remains)
INFO - root - 2017-12-15 16:59:34.588525: step 36320, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 55h:05m:50s remains)
INFO - root - 2017-12-15 16:59:41.133567: step 36330, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 53h:13m:59s remains)
INFO - root - 2017-12-15 16:59:47.673493: step 36340, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 53h:34m:53s remains)
INFO - root - 2017-12-15 16:59:54.243976: step 36350, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 53h:51m:13s remains)
INFO - root - 2017-12-15 17:00:00.802724: step 36360, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 53h:00m:46s remains)
INFO - root - 2017-12-15 17:00:07.477858: step 36370, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 55h:08m:44s remains)
INFO - root - 2017-12-15 17:00:14.032961: step 36380, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 54h:16m:56s remains)
INFO - root - 2017-12-15 17:00:20.622780: step 36390, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 53h:31m:49s remains)
INFO - root - 2017-12-15 17:00:27.113815: step 36400, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 52h:55m:37s remains)
2017-12-15 17:00:27.654331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3892593 -6.5380344 -5.4615612 -4.3343391 -4.1508112 -4.5599694 -5.1684813 -5.4539442 -5.7310472 -5.4841332 -5.0756516 -6.9471908 -7.6157703 -6.0195684 -3.7852192][-7.8750267 -7.3228989 -6.8913422 -5.6330142 -5.7447672 -6.058136 -6.2299352 -6.876533 -7.5465088 -7.4003067 -7.1957226 -8.5226841 -7.9494977 -6.68323 -6.2998409][-5.2322826 -6.0797858 -6.3634577 -5.0070324 -5.4993091 -5.6523709 -5.4956546 -5.8662791 -6.4195471 -6.5467663 -6.7647743 -8.5879583 -8.8169079 -6.7790408 -5.610992][-5.5234804 -5.3635364 -4.4870558 -3.4257326 -3.5892167 -2.6693428 -2.0837815 -2.7702587 -3.5035326 -3.9381449 -4.5752106 -6.5131106 -6.4669151 -5.6242223 -5.511868][-6.9017382 -7.2378206 -6.0715857 -3.6140716 -2.3794632 -0.61330795 -0.10876417 -1.0072479 -1.7830045 -2.7985656 -3.7800844 -5.8713546 -6.2729597 -5.7880945 -5.5261126][-9.1248989 -7.8067312 -5.423193 -3.4898388 -1.7521856 0.46776581 1.3330622 1.2152414 0.72077656 -0.6350174 -2.370055 -4.65125 -4.94068 -4.2880416 -4.314187][-9.5675325 -8.4914694 -6.7728844 -3.38326 -0.64502859 1.7988257 2.596292 2.7833285 3.2741113 2.0371299 0.69161367 -1.9790218 -3.156219 -3.2094364 -3.4709837][-10.061309 -8.409337 -6.8091054 -3.6500926 -0.61856508 2.7440619 4.3356471 4.2200656 4.2389121 3.7798018 2.85006 -1.0500569 -4.0223455 -4.1983438 -4.0278916][-10.126011 -7.8224411 -6.1947546 -3.4165802 -0.83026838 1.9115543 2.7402434 2.9737945 3.0771298 2.348249 1.2603869 -2.0823 -4.7983494 -6.246397 -6.6675558][-9.0131264 -7.1927934 -5.1877422 -2.4717689 -1.0899029 0.5375638 1.6223412 1.7984076 1.651845 1.367898 0.81278324 -2.3508537 -4.7905569 -5.7156653 -5.8486156][-7.3098588 -6.2633572 -4.3786955 -1.5293703 -0.98741388 -0.5266552 0.076461792 0.011174202 -0.39328241 -0.48146248 -1.2060523 -3.6279895 -5.1827068 -6.4732313 -6.7084589][-12.053737 -10.336795 -7.9043303 -5.701293 -4.9205065 -3.7073281 -3.8131375 -4.3475857 -4.4228168 -4.5682125 -4.9402485 -6.2950392 -7.2435384 -7.5824065 -7.5828152][-12.550084 -11.427431 -8.6783009 -6.7131438 -6.9128265 -6.8329735 -6.83568 -6.5243759 -6.8447189 -7.1988111 -6.4641266 -6.4658594 -6.3996153 -6.2727032 -6.3973775][-9.46703 -8.0409794 -6.0034008 -4.2494984 -4.1070361 -4.8816814 -5.9777803 -5.5489411 -5.2143164 -4.883739 -4.7640409 -4.021297 -2.6280546 -1.9406259 -2.4327307][-6.0583315 -5.2938223 -3.9301472 -2.4251306 -1.3617344 -1.4315615 -1.8623734 -2.1050847 -2.9740715 -2.4727345 -1.5336294 -2.5784667 -3.0780714 -2.30164 -2.3454182]]...]
INFO - root - 2017-12-15 17:00:34.236185: step 36410, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 54h:08m:35s remains)
INFO - root - 2017-12-15 17:00:40.889908: step 36420, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 54h:43m:19s remains)
INFO - root - 2017-12-15 17:00:47.505785: step 36430, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 53h:51m:35s remains)
INFO - root - 2017-12-15 17:00:54.096244: step 36440, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 52h:51m:23s remains)
INFO - root - 2017-12-15 17:01:00.714887: step 36450, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.646 sec/batch; 53h:08m:10s remains)
INFO - root - 2017-12-15 17:01:07.367617: step 36460, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 56h:18m:48s remains)
INFO - root - 2017-12-15 17:01:14.054815: step 36470, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 56h:03m:57s remains)
INFO - root - 2017-12-15 17:01:20.584064: step 36480, loss = 0.11, batch loss = 0.07 (12.5 examples/sec; 0.640 sec/batch; 52h:37m:01s remains)
INFO - root - 2017-12-15 17:01:27.173461: step 36490, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 53h:54m:40s remains)
INFO - root - 2017-12-15 17:01:33.799975: step 36500, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 55h:02m:04s remains)
2017-12-15 17:01:34.318110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.476601 -10.050178 -9.5945816 -7.8193645 -7.7399178 -7.7225208 -7.83733 -7.2715478 -6.5652852 -6.7786059 -6.431169 -8.14237 -9.7077837 -9.6992378 -8.8844891][-9.947998 -9.8742256 -8.5312939 -7.916532 -7.902729 -7.5481339 -6.8259673 -5.8754005 -5.463295 -5.3508024 -5.6095576 -7.69293 -8.7379637 -9.0632534 -8.7197838][-6.6657419 -8.0646639 -8.4963017 -7.877264 -7.5152912 -6.9245224 -6.4023838 -5.9013047 -5.0773678 -5.2924938 -4.9303226 -7.310524 -8.482769 -8.1864338 -8.2415829][-6.0155115 -7.0392084 -6.7791224 -6.1221704 -6.0809808 -5.7426896 -4.8478093 -3.8925846 -3.4597049 -3.9670987 -4.3732672 -6.7068954 -7.9791927 -8.1964264 -7.2905512][-6.0548968 -7.3380117 -7.1039147 -5.4859123 -4.5174608 -3.0027864 -2.2542663 -2.6241887 -2.7813723 -3.2572405 -3.5522318 -5.9085875 -7.4742827 -7.9379063 -7.4738846][-8.3678322 -7.3210793 -5.9996924 -4.4534216 -2.5563192 -0.44480753 1.0425677 1.0800729 0.53341866 -1.1440601 -2.5754912 -4.8661146 -6.4922361 -7.5401645 -7.43773][-7.4034157 -6.1570034 -4.9299822 -2.4613352 -0.12691879 2.2237358 3.8388171 3.4114633 2.5870624 0.86096907 -0.58678389 -3.3740344 -5.4752541 -6.7303925 -6.3647842][-6.3596854 -5.4199677 -3.3695307 -1.0964684 -0.18833971 2.3047662 4.521728 4.2918057 3.4603372 1.7850838 0.58999538 -2.0779946 -4.9138203 -6.2702055 -6.3156271][-6.1246486 -4.5715847 -2.7817791 -0.71560764 -0.063692093 1.5759306 3.3941722 3.2502828 1.8269668 0.21811819 -0.083851337 -2.7443998 -5.3624272 -6.9707208 -7.1977549][-4.5010014 -4.3098426 -2.9789259 -1.1395435 -0.52232361 0.30105019 1.6715779 1.9203033 0.11676884 -1.059094 -1.6082745 -3.7837563 -5.2983809 -7.7953057 -8.0177927][-6.6484747 -5.8678379 -4.4735889 -2.7723904 -2.81801 -2.6251853 -1.7803085 -1.5382819 -2.0765295 -2.8145006 -3.9681892 -6.2030544 -7.8413553 -9.4470234 -10.004394][-9.4469566 -7.8609447 -6.311317 -5.2588148 -5.9495406 -5.8889923 -5.2338285 -4.78019 -5.2754498 -6.043766 -6.1327395 -7.2712011 -8.5332079 -9.5269794 -9.7029][-12.820956 -11.66369 -9.9265366 -8.1115942 -8.0076275 -7.528409 -7.2955322 -6.8004146 -6.3875737 -7.2143 -7.8140836 -8.2558641 -8.681531 -8.6982069 -9.0154295][-10.486379 -9.9394531 -8.6921358 -6.7635646 -5.3560252 -5.7801847 -6.2982588 -5.7357907 -5.9390655 -5.9760141 -6.4284277 -6.5698977 -5.65649 -6.4008493 -7.5502024][-6.4690304 -6.5140533 -6.6191349 -4.69267 -3.3629725 -3.593158 -3.5248141 -3.2338905 -3.5853703 -5.0629811 -5.6112447 -5.8377156 -6.8111792 -7.3043804 -7.0627084]]...]
INFO - root - 2017-12-15 17:01:40.851992: step 36510, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 54h:06m:12s remains)
INFO - root - 2017-12-15 17:01:47.398937: step 36520, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 53h:09m:33s remains)
INFO - root - 2017-12-15 17:01:53.918819: step 36530, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 54h:51m:21s remains)
INFO - root - 2017-12-15 17:02:00.460180: step 36540, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.689 sec/batch; 56h:39m:15s remains)
INFO - root - 2017-12-15 17:02:07.139090: step 36550, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.695 sec/batch; 57h:07m:50s remains)
INFO - root - 2017-12-15 17:02:13.745516: step 36560, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 55h:03m:52s remains)
INFO - root - 2017-12-15 17:02:20.357882: step 36570, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 54h:51m:22s remains)
INFO - root - 2017-12-15 17:02:26.970196: step 36580, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 54h:38m:31s remains)
INFO - root - 2017-12-15 17:02:33.587665: step 36590, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.675 sec/batch; 55h:30m:26s remains)
INFO - root - 2017-12-15 17:02:40.178768: step 36600, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 54h:20m:20s remains)
2017-12-15 17:02:40.691398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0602927 -5.4769 -4.9170961 -4.5811615 -5.3682384 -5.92478 -6.4233713 -6.7839432 -6.8887925 -7.1910176 -7.0221634 -8.14193 -10.29521 -10.77766 -10.889413][-4.8874078 -5.2976618 -4.9523883 -4.5848675 -5.1318703 -6.7007065 -7.7714252 -8.4084492 -9.0372324 -9.0537605 -9.2969 -10.349067 -11.246122 -11.276407 -12.249596][-3.7057903 -4.6414804 -5.4839377 -5.1826978 -5.5183463 -6.0986209 -6.7985215 -7.8142834 -8.22817 -8.7536755 -9.0245008 -9.6202641 -11.289597 -11.92824 -12.446695][-4.7455511 -5.6605182 -5.6249857 -5.7831221 -6.0358477 -5.7161932 -5.507875 -6.5547051 -7.2406573 -7.59348 -7.9440045 -8.5869036 -10.620787 -12.155811 -12.852186][-5.5821409 -6.619627 -7.1792345 -6.0556068 -5.2930837 -3.373739 -2.5558732 -3.7031627 -4.9733543 -5.4803348 -6.1343961 -7.6958089 -9.9344969 -10.8946 -12.255541][-7.4276028 -7.2824821 -5.8698535 -4.2401037 -3.1758492 -0.97614431 0.95701027 1.4706078 0.47531748 -1.8375854 -4.1408663 -5.3866286 -8.00181 -9.9971657 -11.549789][-8.1474628 -8.4044228 -7.2013936 -4.3404932 -2.2697527 1.1399655 4.6726127 5.0157962 4.4084697 1.8807545 -1.688004 -4.480382 -7.8767056 -9.03881 -10.109682][-8.02935 -7.7583418 -6.877779 -3.5453708 -1.037827 2.0442171 5.3682771 6.3138595 6.4579234 3.3778634 -0.69562483 -3.6242471 -7.4067659 -8.9942245 -9.9820414][-5.871922 -6.422143 -5.7590303 -2.6234813 0.0294137 2.1415629 3.2991319 3.8091712 4.3223548 2.0771823 -1.1912398 -5.1650586 -9.37845 -9.91308 -9.9544811][-3.8707337 -4.7795253 -5.03539 -2.8807964 -1.4902163 -0.86111307 1.2829003 2.2970929 1.2776713 -0.71433496 -2.5976996 -5.6249962 -9.0568056 -9.7386227 -10.556047][-6.6945543 -7.0966625 -6.828238 -6.0978832 -5.6825929 -5.0576625 -3.7798767 -3.9813871 -4.1817832 -4.41478 -5.9520979 -8.6343822 -9.9703617 -9.9942217 -10.13509][-11.330842 -10.995062 -10.24933 -9.2680836 -8.6108046 -7.6865182 -7.58163 -8.4378033 -8.0162811 -7.8794661 -8.5183687 -8.5614758 -9.17952 -9.1832056 -9.2539892][-10.27504 -9.349926 -8.375699 -8.1218224 -7.6423297 -7.5990572 -7.9901609 -7.9101467 -7.9613943 -7.5851126 -7.304358 -7.763988 -8.0855732 -7.6778584 -7.5931358][-8.9347048 -8.1992512 -7.16977 -6.9538064 -6.2665181 -6.5332446 -7.1877437 -7.2784648 -7.2675257 -7.2771945 -7.3490844 -7.01479 -7.1693048 -6.7763028 -7.3145967][-7.4079628 -6.7849693 -6.5361342 -6.4781175 -5.65722 -5.7035623 -5.3662429 -5.3170991 -5.7255745 -5.3792276 -5.6748724 -6.8571177 -7.3233094 -7.607389 -8.65957]]...]
INFO - root - 2017-12-15 17:02:47.321841: step 36610, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 54h:41m:00s remains)
INFO - root - 2017-12-15 17:02:53.840272: step 36620, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 53h:50m:44s remains)
INFO - root - 2017-12-15 17:03:00.473187: step 36630, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 53h:50m:20s remains)
INFO - root - 2017-12-15 17:03:07.090261: step 36640, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 54h:33m:52s remains)
INFO - root - 2017-12-15 17:03:13.735210: step 36650, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 54h:05m:55s remains)
INFO - root - 2017-12-15 17:03:20.294779: step 36660, loss = 0.25, batch loss = 0.20 (12.0 examples/sec; 0.665 sec/batch; 54h:39m:04s remains)
INFO - root - 2017-12-15 17:03:26.933774: step 36670, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 53h:55m:20s remains)
INFO - root - 2017-12-15 17:03:33.526778: step 36680, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 53h:59m:41s remains)
INFO - root - 2017-12-15 17:03:40.076095: step 36690, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.637 sec/batch; 52h:21m:56s remains)
INFO - root - 2017-12-15 17:03:46.703732: step 36700, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.684 sec/batch; 56h:10m:05s remains)
2017-12-15 17:03:47.294883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3090343 -5.9771152 -5.7700758 -5.6651521 -6.087235 -4.64512 -3.7642078 -3.4824719 -2.7375686 -1.7599387 -0.777194 -1.0880213 -2.4978991 -3.9997697 -5.2631717][-4.4515133 -3.9183235 -2.2022812 -1.9752841 -2.5391262 -1.7621543 -0.99544287 -0.58606434 -0.72780323 -0.14936447 1.0472679 0.28506184 -1.7330503 -3.5416439 -4.6706223][-2.567318 -2.6747169 -1.7794943 -1.3894267 -0.95228767 -0.46420479 -0.72702217 -0.6600852 -0.27970886 -0.32376242 -0.23719501 -1.611156 -3.6826041 -5.9196253 -7.4127169][-5.060009 -4.9893527 -3.0282688 -1.933136 -1.8391619 -1.4955349 -1.2056775 -1.0634837 -1.4487801 -1.2618747 -0.75094652 -2.6487632 -5.5734634 -8.0028982 -8.7705994][-6.3261881 -6.2112126 -4.7014847 -2.8508396 -1.376905 -0.22648907 0.40388107 -0.4637084 -1.9832578 -1.7986116 -1.4405127 -3.7281585 -6.3545237 -8.3377228 -9.72488][-8.2392788 -7.6253905 -4.451458 -0.60101509 1.1516743 2.833468 4.1878486 3.9080815 2.6718984 0.85987616 -1.0112538 -2.2803278 -4.2517729 -7.2440753 -8.5204229][-8.3588772 -7.4984989 -5.2071233 -2.422802 0.15136433 3.1362824 5.5137124 5.8694177 5.1917062 2.2381186 -1.2216482 -3.5754066 -5.887279 -7.7484818 -9.3548908][-8.8473463 -8.0858212 -6.096818 -3.0987902 -1.1564589 2.33425 4.5744309 4.6501832 4.260057 2.0816598 -0.21548986 -2.2684417 -5.2202635 -7.329227 -8.5678272][-7.2818666 -6.95464 -5.48043 -3.9451284 -2.5398641 0.61692333 3.2208371 4.2133117 3.6575408 0.82823658 -1.6799531 -4.0420451 -7.0648208 -8.6374216 -9.0602455][-6.9386578 -6.4664421 -6.4396639 -4.9027252 -3.7182984 -2.2308207 0.019214153 2.4787726 2.5621438 -0.098487854 -2.9523458 -5.0141082 -7.3123012 -9.5224056 -10.957589][-11.787895 -10.591097 -9.5242233 -9.2603874 -8.4424124 -6.8953514 -5.0414243 -3.6149361 -2.571764 -2.5938342 -4.3523574 -7.8020544 -10.666422 -11.015986 -10.563986][-15.242704 -14.767838 -12.960857 -11.735064 -10.457921 -9.3707485 -9.133564 -8.70695 -7.6034985 -7.5390286 -8.0318251 -8.6299543 -9.3428373 -10.551765 -10.913357][-14.790989 -13.422735 -11.206265 -11.127529 -10.519651 -9.0700579 -8.0143547 -8.024353 -8.65378 -8.4562 -8.1171227 -8.5456648 -8.7001581 -8.3614035 -7.8979015][-12.510666 -11.763235 -9.9063072 -7.8481531 -6.4762888 -7.2244244 -7.9994907 -7.0125327 -6.804697 -7.4974861 -8.7135658 -8.14818 -7.2214255 -6.9468431 -6.4275093][-9.3012552 -7.8825684 -6.3491344 -4.6352077 -3.5388231 -3.0143247 -2.8264637 -3.6641874 -4.7656808 -4.7059507 -4.895649 -6.0872297 -7.0930462 -7.4834557 -7.2378674]]...]
INFO - root - 2017-12-15 17:03:53.965550: step 36710, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 55h:43m:47s remains)
INFO - root - 2017-12-15 17:04:00.576605: step 36720, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 53h:14m:21s remains)
INFO - root - 2017-12-15 17:04:07.135737: step 36730, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.676 sec/batch; 55h:32m:31s remains)
INFO - root - 2017-12-15 17:04:13.820630: step 36740, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 55h:04m:52s remains)
INFO - root - 2017-12-15 17:04:20.409936: step 36750, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.692 sec/batch; 56h:51m:36s remains)
INFO - root - 2017-12-15 17:04:27.053922: step 36760, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 53h:00m:21s remains)
INFO - root - 2017-12-15 17:04:33.685574: step 36770, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 55h:59m:31s remains)
INFO - root - 2017-12-15 17:04:40.275757: step 36780, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.684 sec/batch; 56h:13m:19s remains)
INFO - root - 2017-12-15 17:04:46.771214: step 36790, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 53h:05m:00s remains)
INFO - root - 2017-12-15 17:04:53.405444: step 36800, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 56h:03m:16s remains)
2017-12-15 17:04:54.008629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3702917 -5.8737712 -6.9909739 -6.3478456 -5.9752278 -6.6504641 -5.9969025 -5.4760094 -4.5221696 -4.0897169 -3.6979198 -5.0239496 -7.5199909 -7.4516768 -9.543932][-4.4246259 -3.4913464 -3.2332826 -3.4166384 -3.80019 -4.8217087 -5.4647932 -5.1365013 -4.20185 -3.1799111 -2.6545587 -3.9475865 -5.51366 -7.3990541 -9.9943371][-1.4444227 -2.33351 -3.0084715 -3.5533798 -4.1360669 -4.8529882 -4.3314166 -3.7397776 -2.6676633 -2.0770071 -1.3886909 -3.8383882 -6.5604954 -8.4838257 -10.316001][-2.8713503 -3.2367747 -3.2334812 -3.9615638 -4.4226665 -4.738574 -4.2398658 -3.4966195 -2.749433 -1.9980443 -1.9277751 -3.4568753 -6.0050344 -8.6690426 -11.465303][-3.1456082 -4.2913346 -5.860456 -5.6725144 -4.8321486 -4.2644672 -2.1324966 -1.8532543 -2.351445 -2.0283828 -1.9781375 -4.0690374 -5.2024083 -6.6124988 -9.9056988][-3.9181809 -4.5933318 -5.1358337 -4.3561006 -2.8613977 -1.1749792 1.7180882 1.8284788 0.19548035 -1.0112996 -1.7565207 -3.4283605 -5.1146803 -5.9905481 -8.237752][-4.6647339 -4.4039497 -4.4493217 -3.1244888 -1.6312022 1.2281761 4.4502654 4.6605153 4.6900868 2.3358521 -0.47412586 -2.4542677 -4.4612126 -6.1568809 -7.79348][-3.9107637 -4.1693115 -3.2633679 -1.8124146 -0.17963839 3.3724818 6.9003921 6.9199071 7.5611482 5.7615752 2.0418358 -1.697907 -5.8455667 -5.9737248 -7.8419194][-4.2567844 -3.7331827 -3.3419373 -1.9274721 -0.22036886 2.336184 4.8373666 5.7432923 5.6145959 3.5255284 2.6577048 -1.5318451 -6.2724791 -7.326427 -10.000811][-5.204937 -4.5575161 -3.2337556 -2.6906443 -1.8776608 0.30171347 2.3598361 2.9943061 2.0604978 0.5359087 -0.98724079 -5.1038141 -8.7451859 -10.66169 -13.718021][-8.1840944 -8.0940657 -8.2223969 -6.9391413 -4.833847 -3.5815859 -2.3773453 -2.2299504 -3.4783425 -3.8559437 -4.990128 -8.3494606 -12.258762 -13.095486 -14.117016][-11.578148 -11.113585 -10.212559 -9.6072121 -8.6209192 -6.9587049 -6.3273945 -6.5254006 -7.1627407 -8.2533388 -9.5729246 -10.410482 -12.268894 -12.769717 -13.589031][-15.046776 -15.50589 -14.656513 -12.814089 -12.405651 -11.487569 -10.845288 -10.551136 -11.396351 -12.224377 -11.754376 -12.615662 -12.461063 -11.121248 -10.303418][-13.886192 -13.530098 -12.823284 -13.34201 -12.240948 -11.160519 -11.111013 -10.544455 -10.628689 -9.8846607 -10.238758 -10.948065 -9.577323 -9.0482063 -8.2886686][-10.246624 -10.023661 -9.4851856 -8.9203911 -7.9908266 -9.1327629 -9.7621174 -10.106276 -10.1772 -9.1004009 -7.7877588 -8.3187685 -9.3804626 -9.8935375 -9.6106234]]...]
INFO - root - 2017-12-15 17:05:00.611691: step 36810, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 54h:19m:41s remains)
INFO - root - 2017-12-15 17:05:07.159901: step 36820, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 54h:09m:19s remains)
INFO - root - 2017-12-15 17:05:13.759197: step 36830, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 54h:22m:27s remains)
INFO - root - 2017-12-15 17:05:20.412373: step 36840, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.675 sec/batch; 55h:27m:45s remains)
INFO - root - 2017-12-15 17:05:27.036191: step 36850, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 53h:15m:52s remains)
INFO - root - 2017-12-15 17:05:33.661386: step 36860, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 54h:26m:51s remains)
INFO - root - 2017-12-15 17:05:40.291129: step 36870, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 55h:38m:31s remains)
INFO - root - 2017-12-15 17:05:46.915657: step 36880, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 53h:15m:35s remains)
INFO - root - 2017-12-15 17:05:53.515727: step 36890, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 54h:26m:50s remains)
INFO - root - 2017-12-15 17:06:00.103684: step 36900, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 55h:59m:28s remains)
2017-12-15 17:06:00.615802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2636223 -6.2798419 -6.0594783 -7.1899004 -8.192709 -8.4014721 -8.0920944 -7.5221324 -6.9091024 -5.83216 -3.9426372 -3.9293573 -5.81674 -7.2064638 -7.2743783][-6.9843121 -8.030714 -8.2311277 -9.2331581 -10.222731 -10.248647 -9.4485731 -8.4673023 -8.1950016 -7.2042341 -5.7581944 -5.5896711 -7.1717443 -8.356864 -8.3767776][-4.9713235 -6.9827843 -9.2301 -11.012038 -11.329794 -11.044818 -10.281054 -9.0784264 -8.8893518 -8.369545 -7.2506638 -7.9347 -10.047421 -10.593393 -10.294308][-7.03154 -8.12141 -9.6435366 -11.806425 -11.359543 -9.5340452 -8.2748547 -8.1143141 -8.3598118 -8.3572664 -7.9536352 -8.8519993 -11.161312 -11.941149 -11.858328][-8.6812162 -10.443975 -11.846979 -11.49627 -9.0394039 -5.5673294 -3.3253059 -5.1271591 -7.7434087 -7.4440603 -5.9499273 -7.4229679 -10.651758 -12.113144 -12.558619][-10.754831 -11.836885 -12.241463 -10.612178 -6.7328453 -0.99022532 4.3475842 2.4798102 -2.5131662 -4.4687271 -5.2257357 -6.4533958 -8.5385313 -10.754535 -11.734028][-10.993792 -11.439947 -10.17551 -6.97924 -3.1224334 1.9947033 8.5961094 8.2747135 4.6688294 -1.3236766 -5.4723983 -6.4166574 -7.8604031 -9.3397846 -9.7928934][-11.307657 -11.849771 -10.361441 -6.3608918 -0.75183058 6.0287061 11.487606 10.404736 7.5365176 1.0648351 -4.049737 -7.1088552 -9.7300406 -10.072706 -9.0028706][-9.2696018 -9.3539667 -9.12537 -7.0814943 -4.2269864 2.0432925 7.6188712 6.0555625 3.5701852 -0.44505644 -4.0940328 -7.7587624 -10.941882 -11.485054 -10.294268][-7.0715952 -6.8262434 -8.4913216 -8.0491362 -6.4292269 -2.2977726 1.2764807 1.9993339 0.20383215 -4.0467196 -6.6273465 -9.6489811 -12.736257 -13.797598 -13.325794][-10.23718 -9.8543558 -9.943469 -10.21986 -10.073807 -8.4874554 -7.1666741 -6.2305622 -6.4636273 -8.2128668 -10.136122 -12.363611 -14.253023 -14.893553 -14.054108][-13.884897 -13.288969 -13.009153 -12.570871 -12.359337 -11.889208 -11.359901 -11.230108 -11.482498 -12.785259 -13.62933 -13.506021 -13.488304 -13.566001 -12.489278][-14.346992 -12.821829 -11.636526 -12.008452 -11.566445 -11.28105 -11.352262 -11.700575 -12.199978 -11.78052 -11.390781 -12.189803 -12.134005 -11.632307 -10.113061][-12.473804 -11.284794 -10.309385 -10.830183 -11.037469 -10.977888 -10.997705 -10.719036 -10.516067 -10.53927 -10.076963 -9.6329851 -9.2997723 -9.3387051 -8.8067169][-8.4856863 -7.9250956 -6.9520116 -6.2565203 -6.3514118 -6.9320993 -7.988626 -8.4072437 -8.8516922 -8.8951883 -8.5080462 -8.8561354 -9.7805882 -9.73309 -10.130866]]...]
INFO - root - 2017-12-15 17:06:07.168026: step 36910, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 54h:53m:31s remains)
INFO - root - 2017-12-15 17:06:13.763490: step 36920, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 54h:26m:20s remains)
INFO - root - 2017-12-15 17:06:20.304305: step 36930, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 52h:55m:43s remains)
INFO - root - 2017-12-15 17:06:26.882603: step 36940, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 53h:28m:01s remains)
INFO - root - 2017-12-15 17:06:33.493133: step 36950, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 53h:32m:07s remains)
INFO - root - 2017-12-15 17:06:40.047820: step 36960, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 53h:36m:57s remains)
INFO - root - 2017-12-15 17:06:46.683695: step 36970, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 55h:15m:29s remains)
INFO - root - 2017-12-15 17:06:53.323923: step 36980, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 56h:14m:54s remains)
INFO - root - 2017-12-15 17:06:59.878876: step 36990, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 53h:42m:23s remains)
INFO - root - 2017-12-15 17:07:06.514043: step 37000, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 54h:51m:48s remains)
2017-12-15 17:07:07.019075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2384934 -8.8249331 -9.1765671 -9.3072128 -10.389071 -10.854675 -11.667453 -11.61038 -11.224366 -10.682531 -9.6152525 -8.94858 -10.203399 -10.703053 -9.1217442][-9.7718449 -10.814421 -10.078644 -9.92071 -10.534374 -11.657574 -13.046312 -13.302468 -13.289104 -12.123308 -11.026811 -11.068513 -12.349627 -12.999823 -11.53533][-7.6810484 -10.043589 -11.067247 -10.935924 -10.969767 -12.147486 -13.165954 -13.0803 -12.477359 -11.376719 -11.181468 -11.571257 -13.273197 -13.722204 -13.34387][-8.4431152 -10.821679 -12.169274 -11.183266 -10.270345 -9.8109589 -10.051758 -10.82311 -10.841052 -9.82085 -9.949646 -11.000136 -13.061771 -13.300557 -11.753696][-10.068485 -13.357841 -15.545132 -14.005104 -10.409649 -5.4644613 -3.0388079 -5.5034232 -8.6067905 -8.5226116 -8.6388111 -9.4467258 -11.753353 -13.200598 -11.990339][-10.945248 -12.86266 -14.369024 -14.027912 -10.031923 -2.3052437 4.3772254 3.92481 -0.53289652 -4.9648786 -8.3327236 -7.9297152 -9.1707726 -11.741833 -12.221607][-12.956345 -13.074724 -11.221171 -9.6329031 -6.7710495 -0.64772034 6.3535743 9.3971462 7.6216168 -0.91114712 -8.6996889 -8.5693378 -10.408396 -11.800978 -11.092925][-13.783831 -13.711088 -11.631935 -6.5444279 -1.1696453 3.3115335 7.1200814 8.7647152 8.8427353 2.2131214 -4.8846354 -7.4154434 -12.129412 -13.45838 -11.360529][-12.191095 -12.3862 -12.763461 -9.3718843 -3.1855671 3.0173373 7.2777591 7.1662173 5.5945811 -0.30545664 -5.4520745 -7.1682949 -11.722956 -14.755917 -14.17][-8.6509495 -9.5709229 -11.560547 -9.8383389 -6.7663064 -2.2645218 2.8171058 4.3253665 2.2521577 -3.3070967 -7.6465154 -9.8287735 -12.715693 -14.739723 -15.222157][-12.284618 -13.269045 -13.791145 -11.691099 -10.469775 -8.1643925 -5.9994802 -4.9454989 -5.5496488 -7.9524317 -10.753377 -13.023531 -14.532312 -15.024817 -13.948557][-16.285248 -17.264595 -16.851881 -15.069954 -13.830381 -12.402264 -11.957461 -12.181072 -12.926031 -13.718031 -13.77928 -14.61515 -14.61454 -15.331264 -13.348049][-14.467386 -14.472141 -15.007189 -14.425436 -14.141615 -13.300044 -12.146924 -12.864215 -13.683506 -13.253492 -12.377233 -13.305788 -13.401682 -13.262915 -11.088816][-12.480906 -11.683851 -11.241859 -9.9427109 -10.566217 -11.599641 -11.076412 -10.985992 -10.998499 -11.414856 -11.738015 -11.363392 -10.574516 -10.553713 -9.6977262][-9.144454 -8.8472614 -7.26549 -5.248014 -4.4154849 -5.3762736 -6.8659945 -7.2353396 -7.7817559 -7.9085007 -8.1429148 -9.2914352 -10.692699 -11.447599 -10.440271]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 17:07:13.653116: step 37010, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.694 sec/batch; 56h:55m:38s remains)
INFO - root - 2017-12-15 17:07:20.253283: step 37020, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 54h:59m:27s remains)
INFO - root - 2017-12-15 17:07:26.884630: step 37030, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 54h:40m:59s remains)
INFO - root - 2017-12-15 17:07:33.440166: step 37040, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 53h:01m:26s remains)
INFO - root - 2017-12-15 17:07:40.056429: step 37050, loss = 0.27, batch loss = 0.22 (11.8 examples/sec; 0.677 sec/batch; 55h:31m:36s remains)
INFO - root - 2017-12-15 17:07:46.622425: step 37060, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 52h:42m:53s remains)
INFO - root - 2017-12-15 17:07:53.272556: step 37070, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.686 sec/batch; 56h:17m:20s remains)
INFO - root - 2017-12-15 17:07:59.819802: step 37080, loss = 0.19, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 52h:25m:42s remains)
INFO - root - 2017-12-15 17:08:06.437978: step 37090, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 53h:01m:04s remains)
INFO - root - 2017-12-15 17:08:12.996830: step 37100, loss = 0.20, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 53h:24m:36s remains)
2017-12-15 17:08:13.518875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3442984 -4.872736 -5.2920089 -4.2701874 -3.9111342 -3.6116922 -3.5277591 -2.8183763 -1.8052175 -1.5900507 -1.3128748 -4.5476327 -6.6831546 -8.19415 -7.9260845][-4.4572735 -4.3660393 -4.1932125 -3.5810344 -3.0283928 -2.6243265 -2.5636282 -1.6629186 -0.410954 0.39745951 0.47873449 -3.4356651 -5.3938875 -7.8904223 -8.4449368][-2.7032869 -3.9226613 -4.5743694 -3.2565253 -2.9941897 -2.2362919 -1.7256038 -0.78255033 0.27188778 0.21851015 0.016679764 -3.2217259 -4.6250124 -6.6047277 -7.818038][-4.13102 -4.6706867 -4.7130871 -3.27843 -3.2719276 -2.1513436 -1.3356938 -0.88385534 -0.067334652 0.71298552 0.61021233 -3.1145027 -4.9551368 -7.05193 -7.5310068][-4.7783594 -7.220933 -7.56451 -5.5129113 -4.5723267 -1.8875871 -0.35152245 -1.0911775 -1.1725769 -0.27915192 -0.16991663 -3.1554823 -4.6076636 -6.7099657 -7.7031646][-5.4247103 -7.2484727 -7.0248179 -4.1303654 -2.1477938 1.1209574 3.044107 1.9836078 1.1694779 0.20834112 -1.1529303 -3.3991165 -4.2856336 -6.3483896 -7.1438718][-7.5463276 -7.2833548 -5.6462955 -2.4745169 -0.46606159 3.1592765 5.692945 4.6850896 3.5606122 0.85774136 -1.5746374 -3.9704509 -5.2221723 -6.7219977 -7.594635][-8.3465557 -7.3802128 -6.1924438 -2.4722993 0.78622675 4.7124619 7.1526265 5.6599374 4.4341426 2.4795995 0.20199156 -3.6765938 -5.7045155 -7.320621 -8.8083048][-7.5005736 -6.3163486 -5.2188549 -2.6239309 -0.58515692 2.5784993 4.763618 4.2323527 3.2892432 1.2147603 -0.2543788 -4.3308649 -5.9397078 -7.3886137 -8.1580439][-7.9272738 -6.5871172 -5.5042524 -2.2690227 -1.6207528 0.16268873 2.166513 2.3046565 1.3314281 -0.70665646 -2.3866208 -5.643364 -6.5633121 -7.8217916 -8.6828][-9.9791412 -8.995513 -8.1602392 -5.2187381 -4.7602057 -3.6059344 -2.4198136 -1.938447 -2.3501942 -3.0856154 -4.1761088 -7.8417168 -9.0266171 -9.26978 -9.2261314][-12.488333 -12.140131 -11.38592 -9.3295956 -8.7093906 -6.6743937 -6.0501361 -6.6692104 -6.8329725 -6.6537132 -7.0414991 -9.2193165 -9.558814 -10.171757 -10.219357][-12.775827 -12.852724 -11.834011 -11.055839 -10.838114 -8.8883514 -8.3119316 -9.2101555 -9.4493141 -8.9483833 -8.5648041 -9.5989475 -9.7700348 -9.4490929 -8.6631489][-11.632784 -11.433945 -10.776804 -9.66243 -9.1615753 -8.5758028 -8.7828512 -8.3344154 -8.6600485 -9.0850239 -9.1661263 -8.9361429 -7.9727759 -7.99835 -7.9589686][-8.0191879 -8.0857105 -7.5505381 -5.9357247 -5.9935613 -5.7927217 -5.9752665 -6.2251596 -6.6661882 -6.6215162 -6.9294729 -7.8678937 -7.999723 -8.5530014 -8.5538807]]...]
INFO - root - 2017-12-15 17:08:20.126044: step 37110, loss = 0.22, batch loss = 0.17 (11.9 examples/sec; 0.672 sec/batch; 55h:09m:51s remains)
INFO - root - 2017-12-15 17:08:26.823689: step 37120, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 55h:00m:10s remains)
INFO - root - 2017-12-15 17:08:33.518300: step 37130, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 52h:41m:47s remains)
INFO - root - 2017-12-15 17:08:40.057084: step 37140, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 55h:38m:03s remains)
INFO - root - 2017-12-15 17:08:46.700446: step 37150, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.688 sec/batch; 56h:25m:30s remains)
INFO - root - 2017-12-15 17:08:53.236069: step 37160, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 52h:52m:10s remains)
INFO - root - 2017-12-15 17:08:59.763194: step 37170, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 55h:19m:59s remains)
INFO - root - 2017-12-15 17:09:06.350029: step 37180, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 55h:10m:45s remains)
INFO - root - 2017-12-15 17:09:12.978490: step 37190, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 53h:34m:31s remains)
INFO - root - 2017-12-15 17:09:19.527219: step 37200, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 52h:20m:28s remains)
2017-12-15 17:09:20.081360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2484047 -3.3533037 -3.3172357 -3.434612 -4.4611616 -5.2452927 -5.8490677 -5.5003142 -5.6572847 -6.1457057 -6.1950746 -7.0560312 -8.3862839 -8.044384 -8.2300377][-3.5221105 -2.2072866 -1.4815001 -1.8788347 -3.002223 -4.0761061 -4.3530664 -4.5431719 -5.6379309 -5.788033 -5.6832943 -7.0909572 -8.57601 -8.7579412 -9.5049334][0.72702789 -0.18851805 -2.2027969 -1.650949 -2.3150096 -3.3442845 -3.5257733 -3.4619217 -3.8688769 -4.5189524 -5.23411 -7.07366 -8.534524 -9.1929579 -10.021464][-1.6207271 -2.0295365 -1.7482228 -1.4104075 -2.7854905 -3.025054 -2.5735552 -3.1401708 -4.3051853 -4.1545863 -4.1093168 -6.221467 -8.2022276 -9.2643557 -10.078097][-1.6801019 -2.8507102 -3.4992783 -2.5331297 -2.0244963 -0.92025137 0.0677042 -1.5846343 -4.0783911 -4.1498919 -3.99715 -5.4654603 -7.3083782 -8.8360329 -9.7488346][-4.7324586 -4.601872 -3.6107569 -0.91358805 1.1109791 2.4315133 3.411201 2.3747106 0.77297974 -0.92521381 -2.7096205 -3.7343216 -5.5023408 -7.2868605 -8.3561087][-6.8235459 -6.0038304 -3.8641613 -0.10284519 1.8719568 4.727839 6.7442966 6.2861218 5.0715775 2.49482 -0.13479662 -1.8395662 -4.3496771 -6.192462 -7.1253471][-7.5004973 -6.897697 -4.5558486 -0.58111906 2.1267743 5.9757304 8.1862774 7.3551478 5.9645076 4.2549491 2.073359 -1.2561355 -4.4736362 -5.8754077 -6.6888905][-5.9065976 -6.1450672 -4.8323021 -1.9018319 0.85454226 3.6707187 5.1898856 5.185308 4.8952107 3.1541972 0.58417416 -2.8423905 -6.1845026 -7.7385798 -8.5256023][-4.4089289 -5.3425083 -5.4078965 -3.4270189 -1.7436726 0.42782879 2.185236 2.615253 2.2202315 0.69952393 -1.1947694 -4.0742803 -7.3128395 -8.7738552 -9.4184923][-8.3247986 -8.8101826 -8.3467884 -7.3875046 -7.1554441 -5.9448123 -4.3247361 -3.2658508 -3.1238573 -3.9153557 -4.7873354 -7.4079084 -9.4002876 -10.206577 -9.8932972][-11.606071 -11.930571 -12.051853 -11.171751 -10.827732 -9.9151669 -9.2722845 -8.3589449 -7.8338857 -7.8684969 -8.17878 -8.9248877 -9.4956913 -9.4925346 -9.3230124][-11.706083 -10.403086 -9.4186554 -9.6922054 -11.437653 -11.335728 -10.468401 -10.247478 -10.287593 -9.5131025 -9.1837769 -9.2459507 -9.7679958 -8.7005739 -8.7196665][-8.0719337 -7.6242156 -6.7883625 -6.0931034 -6.0137272 -7.3625636 -8.90427 -8.803194 -8.2425184 -8.3763065 -8.6586685 -7.9661713 -8.2350836 -8.0910664 -7.7892542][-6.3478432 -6.2175484 -5.2961874 -4.3165393 -3.3492088 -4.1501207 -4.80494 -5.2321177 -6.0070009 -5.9459486 -5.8631163 -6.6667614 -7.977129 -8.5193748 -8.0667019]]...]
INFO - root - 2017-12-15 17:09:26.676200: step 37210, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.657 sec/batch; 53h:54m:49s remains)
INFO - root - 2017-12-15 17:09:33.237575: step 37220, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 54h:25m:09s remains)
INFO - root - 2017-12-15 17:09:39.805542: step 37230, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 53h:03m:47s remains)
INFO - root - 2017-12-15 17:09:46.350894: step 37240, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 53h:41m:16s remains)
INFO - root - 2017-12-15 17:09:52.902994: step 37250, loss = 0.33, batch loss = 0.29 (12.0 examples/sec; 0.665 sec/batch; 54h:32m:41s remains)
INFO - root - 2017-12-15 17:09:59.396688: step 37260, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 53h:52m:36s remains)
INFO - root - 2017-12-15 17:10:06.072182: step 37270, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 55h:32m:38s remains)
INFO - root - 2017-12-15 17:10:12.662336: step 37280, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 54h:15m:24s remains)
INFO - root - 2017-12-15 17:10:19.269079: step 37290, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 55h:05m:12s remains)
INFO - root - 2017-12-15 17:10:25.827494: step 37300, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 53h:40m:43s remains)
2017-12-15 17:10:26.392692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3031249 -5.6788287 -4.7746983 -2.3494124 0.10043859 -0.48887491 -2.2440872 -3.56571 -4.9482532 -5.2860494 -5.229301 -7.3934231 -9.9392529 -11.466089 -10.122062][-4.5358844 -4.4026022 -5.8577652 -5.3567586 -4.2416873 -2.492177 -0.4837389 -1.511445 -3.4202483 -5.6347971 -8.1121635 -10.545069 -10.992109 -12.313096 -11.166239][-3.1291718 -4.4350882 -6.076323 -4.606555 -5.0111971 -4.5996137 -3.8095481 -3.5870461 -3.4510736 -3.8692241 -5.1367435 -8.6363258 -10.669029 -11.501503 -10.364061][-2.8778503 -4.2898149 -6.4148483 -7.3484445 -8.0075121 -5.6917295 -3.0732439 -1.6928296 -1.9001501 -4.5382004 -7.918457 -11.340622 -11.704608 -12.152142 -10.043903][-5.1313705 -5.9200144 -7.2723393 -7.5702586 -6.5974736 -3.7950873 -2.5359237 -2.5040545 -2.5407693 -2.7850215 -4.6551208 -9.7850237 -12.630767 -14.027878 -12.211248][-7.3312426 -5.55097 -5.4855061 -5.8302689 -4.1275749 0.70040464 5.7471423 5.4311576 1.9245954 -2.6725292 -5.8432255 -8.4191475 -10.691477 -13.365836 -12.970224][-8.0359383 -7.8998 -8.5736828 -6.1950207 -2.7669759 2.8251595 7.3204741 8.4239521 7.123158 0.86851597 -4.5090733 -6.7963829 -8.8723907 -11.392072 -12.000532][-8.0715532 -9.0363445 -9.329977 -5.83257 -2.1915751 3.6452413 8.8878384 9.5721493 8.4210529 2.7807822 -3.1182916 -7.5400858 -9.7553082 -9.9751234 -9.5817146][-6.78804 -10.069672 -9.784091 -6.8929672 -4.04232 1.8123226 6.10757 7.2765059 6.9297786 0.97820139 -3.2930129 -8.3158464 -11.777977 -13.114656 -11.39588][-6.7359157 -7.5165949 -7.6843596 -6.5459471 -4.9506969 -1.1776371 1.7859483 3.2561793 2.8574739 -2.3683605 -7.4911647 -12.671076 -15.721802 -16.031769 -14.130171][-9.8488426 -11.517509 -10.540741 -7.6850677 -6.445838 -4.7868876 -4.3184032 -3.745466 -5.2073336 -8.5925169 -11.016939 -14.302885 -17.42275 -18.044796 -15.44846][-12.976049 -13.678901 -13.754322 -11.19225 -8.0338058 -6.2335882 -6.7515893 -7.3870535 -8.8529329 -11.727839 -13.99683 -15.897371 -15.948004 -14.868036 -13.625273][-14.762369 -14.519037 -12.968515 -12.331537 -11.823925 -10.399963 -9.27806 -9.6017361 -10.529772 -10.801197 -11.426222 -12.419935 -13.041416 -11.492628 -9.4916458][-10.625172 -9.8582706 -10.127352 -10.634233 -10.826739 -10.114508 -9.5820475 -9.5489426 -9.4725971 -10.385761 -10.346709 -8.6937037 -8.374733 -7.5312061 -7.29948][-6.0600863 -5.8416147 -4.9542074 -4.6740422 -5.9705787 -7.75955 -8.3647251 -7.6953936 -6.9629359 -7.0341206 -6.9958105 -7.6584225 -8.2786045 -7.157773 -6.6050282]]...]
INFO - root - 2017-12-15 17:10:32.918908: step 37310, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 54h:15m:31s remains)
INFO - root - 2017-12-15 17:10:39.551512: step 37320, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 54h:54m:24s remains)
INFO - root - 2017-12-15 17:10:46.157225: step 37330, loss = 0.22, batch loss = 0.17 (11.8 examples/sec; 0.681 sec/batch; 55h:49m:22s remains)
INFO - root - 2017-12-15 17:10:52.710234: step 37340, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 52h:11m:10s remains)
INFO - root - 2017-12-15 17:10:59.284563: step 37350, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 53h:22m:57s remains)
INFO - root - 2017-12-15 17:11:05.950004: step 37360, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 53h:15m:04s remains)
INFO - root - 2017-12-15 17:11:12.545193: step 37370, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 54h:01m:44s remains)
INFO - root - 2017-12-15 17:11:19.134362: step 37380, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.653 sec/batch; 53h:33m:44s remains)
INFO - root - 2017-12-15 17:11:25.792353: step 37390, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 53h:17m:31s remains)
INFO - root - 2017-12-15 17:11:32.346991: step 37400, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.691 sec/batch; 56h:36m:49s remains)
2017-12-15 17:11:32.845193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.6165981 -7.3987608 -6.1829047 -5.4135089 -5.9849172 -6.5127268 -6.6284976 -5.6277547 -4.5823064 -3.7646565 -3.11586 -3.5325606 -4.2971225 -5.2198811 -4.9497533][-7.1262283 -6.671545 -5.3101192 -5.2775726 -6.4374971 -6.8756266 -6.9240928 -6.4671845 -5.5963244 -4.81167 -4.5281773 -5.3435631 -6.5260315 -7.5061722 -6.7176204][-4.6422992 -5.0520782 -5.5449028 -5.0541043 -5.6651378 -6.3636913 -6.752367 -5.8671703 -5.3464141 -5.435441 -5.9342031 -7.0295773 -8.3385658 -8.6126585 -8.2987652][-6.1354938 -6.25963 -5.7737813 -5.8057008 -6.7287531 -6.6801195 -5.9081688 -5.8157253 -5.8755264 -5.2896152 -5.7157483 -7.3634415 -9.4380751 -10.085491 -9.8030834][-6.6233659 -7.3808041 -6.9920616 -5.789453 -5.4695635 -3.8103952 -1.5348306 -2.3232582 -3.6946347 -4.0423784 -4.932569 -6.6852746 -9.1098223 -10.698927 -10.893358][-8.8470831 -9.0006914 -7.4410934 -5.4876232 -3.0412393 -0.57304335 1.1561184 1.1004605 0.48128939 -1.8600602 -4.39221 -6.2265105 -8.7980108 -10.61409 -11.092106][-10.313719 -9.3319645 -6.7020817 -3.995728 -1.8617644 1.1984663 4.2009215 4.2838531 2.7113376 -0.71278715 -3.7181122 -5.9431157 -8.6907825 -9.6032867 -8.81247][-7.9478536 -6.947506 -6.0842528 -2.26481 -0.054048061 2.6205039 4.6814103 4.6028724 3.8740773 0.12412357 -2.9081404 -5.0509062 -7.0402851 -8.1842289 -7.3846359][-5.8320661 -5.494576 -4.0737524 -1.4303055 -0.06545496 1.7916918 3.0311694 3.290277 2.6518512 0.31741858 -1.5804715 -4.1460743 -5.8325543 -6.0184784 -4.7255797][-5.1007743 -5.9005303 -4.7811518 -2.3828058 -1.5463672 -1.2148223 0.089009285 1.4547548 1.1249514 -0.00995779 -0.88036823 -3.059114 -4.5519371 -5.2887516 -5.4147868][-10.825032 -10.021475 -8.4736118 -7.4229345 -6.948246 -6.1611276 -5.2434468 -5.3862324 -3.9922886 -3.0007136 -3.9236922 -5.6561117 -5.8663058 -5.439085 -4.5112572][-13.668321 -13.835033 -12.546292 -11.750489 -11.126877 -10.202206 -10.070784 -9.90456 -8.6490784 -7.4562531 -7.0304289 -6.3367991 -6.3781071 -6.2507758 -5.6299014][-13.690266 -13.178043 -12.788094 -12.388141 -11.110861 -10.497465 -10.496145 -10.206633 -9.6113548 -8.0613384 -7.45135 -7.3902836 -6.3765426 -5.2498717 -3.714915][-10.561539 -10.824709 -9.8460007 -8.8541317 -8.4650335 -9.1736012 -9.3996735 -8.7979231 -8.4211731 -7.8150587 -7.1789365 -6.605629 -5.6416569 -4.9451776 -4.3180242][-7.8375421 -7.4912038 -7.5730824 -6.92928 -5.9347591 -6.02224 -6.3516049 -6.8811574 -6.6691184 -5.6355505 -4.9592848 -5.2503052 -5.6008058 -5.3882713 -5.6939478]]...]
INFO - root - 2017-12-15 17:11:39.431694: step 37410, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.658 sec/batch; 53h:55m:58s remains)
INFO - root - 2017-12-15 17:11:45.964518: step 37420, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 52h:42m:56s remains)
INFO - root - 2017-12-15 17:11:52.576021: step 37430, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 54h:14m:09s remains)
INFO - root - 2017-12-15 17:11:59.135649: step 37440, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 54h:06m:21s remains)
INFO - root - 2017-12-15 17:12:05.703843: step 37450, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 56h:05m:47s remains)
INFO - root - 2017-12-15 17:12:12.206183: step 37460, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 54h:09m:49s remains)
INFO - root - 2017-12-15 17:12:18.745084: step 37470, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 52h:37m:24s remains)
INFO - root - 2017-12-15 17:12:25.364182: step 37480, loss = 0.21, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 53h:24m:31s remains)
INFO - root - 2017-12-15 17:12:31.912724: step 37490, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 52h:58m:09s remains)
INFO - root - 2017-12-15 17:12:38.462246: step 37500, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 54h:04m:50s remains)
2017-12-15 17:12:39.053949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5640631 -3.887917 -3.5525939 -3.8278463 -4.4416704 -4.8398342 -5.5479684 -6.1375842 -6.5770659 -7.2220278 -7.2145581 -7.4079638 -8.8007679 -6.6927619 -5.2930889][-4.8999853 -3.7821527 -3.4393408 -2.5655077 -3.3729 -4.249526 -4.5792317 -5.7838335 -6.4684668 -6.6538281 -6.0637951 -7.0258665 -8.0849533 -6.8989491 -6.5423307][-2.9315732 -2.7806194 -3.052871 -2.7360477 -3.0882621 -3.1057119 -3.4525938 -3.6891527 -4.7036695 -5.879797 -5.7381625 -6.4749355 -7.619482 -7.2750039 -6.7170215][-4.232707 -4.2692471 -3.9420643 -3.5249865 -4.0228529 -3.2385437 -2.2658978 -3.0293386 -4.2165027 -4.4102316 -4.24463 -5.0462351 -6.5130491 -6.7916708 -6.2448545][-4.5882187 -4.6923943 -5.5006528 -3.6706026 -2.6740305 -1.1463814 0.43506098 -0.43163586 -2.1372395 -3.1346381 -3.6552181 -3.8472047 -5.27452 -5.71677 -5.2008619][-6.4922929 -5.7567182 -4.6987429 -2.5690517 0.068329811 3.0698924 5.2852445 4.9442687 4.190536 1.5837398 -1.4575124 -2.0994325 -3.9977181 -4.7125359 -4.1771278][-7.4341483 -6.4404721 -4.5798845 -1.1440177 1.097755 4.4654918 8.0235729 7.7926307 7.089932 4.1647277 0.94561338 -1.6455688 -5.9145555 -5.9875307 -5.5531149][-8.7241077 -6.9491367 -5.5468941 -2.8000584 -0.55941391 2.9625516 6.2222085 6.0404391 5.8538594 3.3726401 0.081298828 -3.2773116 -7.5557795 -8.502512 -8.05232][-7.3765292 -6.1285391 -5.3486814 -3.8875108 -1.1224489 1.919343 3.4185605 3.880631 3.0749383 0.3233099 -1.9072506 -5.2018461 -9.0642643 -9.7243843 -9.4921274][-6.3331261 -5.8677535 -5.449707 -3.4121413 -1.1970458 -0.28027678 0.44523668 1.8195443 1.0147495 -1.0857577 -2.8915021 -5.9692326 -9.1628036 -10.525949 -10.919662][-10.694922 -9.7947111 -8.4942207 -7.44153 -6.7018733 -5.4683166 -3.1353505 -2.8764973 -4.542017 -4.9212813 -6.7140369 -9.4298868 -10.731848 -11.181219 -10.700871][-13.798813 -12.853727 -11.379051 -10.28257 -9.3274117 -8.2951279 -7.0236783 -7.121829 -7.492085 -7.7074461 -8.5144043 -9.5541563 -10.737579 -11.914494 -11.417869][-13.714766 -11.745508 -10.335817 -9.3358536 -8.7067623 -7.6256514 -6.9897962 -7.5751748 -8.9190245 -9.2403069 -9.1308317 -8.9745359 -9.5222092 -7.9877729 -6.2501326][-11.597073 -10.064854 -9.3874512 -7.7582569 -6.033783 -5.8328147 -6.2893858 -5.8645892 -6.1245785 -6.9008904 -7.8331966 -6.9824576 -6.303071 -5.6594791 -6.5009837][-8.5178528 -8.9805088 -8.1498957 -6.0767727 -4.1258087 -2.8487844 -2.076371 -3.0334003 -4.0738544 -4.24066 -4.0645757 -4.8862534 -6.0857725 -5.4592242 -4.9790778]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 17:12:46.471818: step 37510, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 54h:42m:43s remains)
INFO - root - 2017-12-15 17:12:52.993953: step 37520, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 54h:43m:35s remains)
INFO - root - 2017-12-15 17:12:59.589331: step 37530, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 54h:05m:07s remains)
INFO - root - 2017-12-15 17:13:06.238160: step 37540, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 53h:41m:35s remains)
INFO - root - 2017-12-15 17:13:12.822895: step 37550, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 52h:37m:57s remains)
INFO - root - 2017-12-15 17:13:19.338161: step 37560, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 52h:47m:16s remains)
INFO - root - 2017-12-15 17:13:25.906978: step 37570, loss = 0.34, batch loss = 0.30 (12.4 examples/sec; 0.643 sec/batch; 52h:39m:56s remains)
INFO - root - 2017-12-15 17:13:32.468964: step 37580, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 53h:42m:53s remains)
INFO - root - 2017-12-15 17:13:39.063899: step 37590, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 54h:33m:52s remains)
INFO - root - 2017-12-15 17:13:45.646772: step 37600, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 55h:44m:32s remains)
2017-12-15 17:13:46.181876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.9884357 -9.5994329 -9.2192307 -7.6296825 -7.9115868 -7.8670859 -7.5567217 -7.6368971 -7.8979292 -7.1901536 -6.3777857 -6.954464 -7.8767233 -8.7228088 -6.3863368][-7.8259315 -7.2503047 -4.9694238 -4.6893096 -6.2973309 -6.7417064 -6.5003757 -5.9645376 -6.287746 -6.5839682 -6.566524 -7.0790381 -7.6887069 -8.88871 -6.7896643][-3.9281809 -5.2263269 -5.1794729 -4.3732038 -4.6064682 -4.9460344 -4.7265935 -4.68221 -5.6305728 -6.0247107 -6.51668 -7.6708164 -9.3421173 -10.943983 -10.21911][-5.3552184 -5.7929883 -5.5930643 -5.1163678 -5.13541 -4.8570313 -4.3377829 -3.9675388 -4.3155208 -5.4728479 -7.0804024 -8.8705635 -10.664848 -12.283598 -10.950117][-6.3148327 -6.9358177 -5.9734621 -4.2229195 -4.2501416 -2.9465311 -1.2411451 -1.1639042 -1.8434985 -3.1731982 -5.7220244 -8.58023 -10.761806 -12.979562 -12.141277][-7.8411741 -7.8815694 -5.8064661 -3.8187246 -2.1241937 0.50213718 3.0350566 3.1687379 2.0657601 -1.2703738 -4.9726639 -7.2183561 -9.4389277 -11.685333 -10.332815][-10.532633 -9.0556583 -5.0959945 -2.5647216 -1.1272388 1.9962139 6.0406604 6.7925363 5.7752738 1.3603921 -3.3799465 -6.3828621 -8.51034 -10.372343 -8.345541][-11.072946 -9.3069944 -6.186173 -2.0041721 0.88082075 3.4627318 6.0842996 7.3528924 7.9406466 3.6574426 -1.4467025 -5.3010464 -7.9518385 -9.8682556 -8.2733822][-9.8849468 -8.8370724 -6.1990418 -2.2428679 -0.25654459 1.687151 4.3020425 4.9654622 5.29991 2.1798348 -1.0620484 -5.1236053 -8.8364391 -11.362961 -10.208057][-9.1001377 -7.4927669 -5.2682157 -2.3328044 -1.7693377 -0.6353569 0.68514013 0.61850643 1.1527534 -0.64075327 -3.2473977 -6.9762917 -10.080475 -12.905241 -12.246427][-12.005835 -10.946457 -8.6634054 -5.6745257 -5.754169 -5.4840775 -5.1015325 -5.2026033 -4.9691119 -6.0109882 -8.3877678 -11.190266 -13.131769 -15.640741 -14.641008][-14.188499 -14.186462 -13.442875 -11.704603 -10.939142 -10.161867 -10.362757 -10.45737 -10.250103 -11.126554 -12.446186 -13.407665 -13.663923 -15.182257 -13.804192][-14.070091 -13.449939 -11.736122 -11.209904 -10.526379 -9.8554983 -10.065359 -10.336054 -10.530425 -11.217138 -12.086947 -13.521453 -12.783978 -12.901714 -10.280603][-12.179255 -12.382505 -11.322065 -8.842164 -8.0753088 -8.0364342 -8.7102385 -8.5834761 -9.2391319 -9.8983107 -10.529589 -10.766256 -9.823555 -8.6798887 -7.1842108][-9.42148 -9.759552 -9.7608051 -8.0519571 -5.7168369 -4.9691014 -5.1014533 -5.2502136 -5.5088921 -5.3953962 -5.8052936 -6.1672897 -6.1464443 -6.0209117 -6.0523267]]...]
INFO - root - 2017-12-15 17:13:52.730480: step 37610, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 53h:20m:18s remains)
INFO - root - 2017-12-15 17:13:59.332332: step 37620, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.679 sec/batch; 55h:38m:29s remains)
INFO - root - 2017-12-15 17:14:05.932261: step 37630, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 54h:42m:35s remains)
INFO - root - 2017-12-15 17:14:12.544470: step 37640, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 54h:13m:51s remains)
INFO - root - 2017-12-15 17:14:19.127267: step 37650, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 55h:12m:02s remains)
INFO - root - 2017-12-15 17:14:25.752371: step 37660, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 52h:49m:57s remains)
INFO - root - 2017-12-15 17:14:32.340201: step 37670, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 53h:46m:21s remains)
INFO - root - 2017-12-15 17:14:38.892441: step 37680, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 54h:28m:38s remains)
INFO - root - 2017-12-15 17:14:45.438648: step 37690, loss = 0.20, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 53h:18m:35s remains)
INFO - root - 2017-12-15 17:14:52.016770: step 37700, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 53h:59m:08s remains)
2017-12-15 17:14:52.530821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8486495 -6.4282813 -5.7702317 -5.1737404 -5.383482 -5.8357038 -6.1251621 -6.5523558 -7.2393475 -7.3260245 -6.8754091 -7.6511641 -9.4052124 -9.9282417 -8.3640156][-5.9963303 -6.0125914 -5.8741703 -5.4046254 -5.4706278 -6.4249544 -6.8655696 -7.1618748 -7.7093339 -8.186409 -8.5310087 -8.80495 -9.4756832 -10.3354 -9.6475868][-4.6074381 -4.0068383 -4.411716 -3.7883942 -4.4066963 -4.8365 -4.7269616 -5.4243221 -6.0059934 -5.9322267 -5.5809331 -6.3516178 -8.1875172 -8.4307489 -7.6007814][-5.4886441 -5.1244588 -4.94631 -4.2723551 -4.8499994 -4.194037 -3.4380908 -3.629642 -3.8348203 -4.1252913 -4.167079 -4.9426155 -6.0472317 -6.9549365 -7.0582962][-6.015995 -5.473835 -5.824111 -4.8894496 -4.0642738 -1.6548328 -0.17606735 -0.99817467 -2.1271434 -2.053618 -2.569289 -3.8658621 -4.5700183 -5.441628 -5.3284082][-7.5739441 -6.80054 -6.0903258 -3.8348329 -1.2907872 1.3947768 2.8845448 2.5392432 1.5582972 -0.185287 -1.8259289 -2.5486319 -4.1409535 -5.1286116 -4.9660931][-8.9222383 -7.9028111 -6.661149 -4.0162735 -1.5952635 2.953999 6.5532136 5.5857615 3.465878 1.1194749 -1.0711675 -2.2557044 -3.4249787 -4.303113 -3.8303354][-7.7986746 -7.1860995 -5.3275528 -2.076575 -0.0094685555 2.9490409 5.88856 5.933599 4.5520597 1.0701327 -1.8722796 -2.9133241 -3.4091086 -3.5101585 -2.7503817][-7.2722812 -6.9799356 -5.9557152 -2.7925022 -0.48578882 1.513515 2.3279338 2.8382993 3.2802711 0.44407082 -2.342556 -3.8745649 -4.6902342 -3.9617882 -2.2820952][-6.4544516 -5.8609443 -5.5149913 -3.800806 -3.4443572 -0.92362833 1.1329455 1.5390182 0.54199076 -1.8561769 -3.6351225 -4.3551435 -4.5846777 -4.5342851 -3.4109371][-8.796628 -9.1612892 -8.5514488 -6.4693274 -5.3674035 -4.8640275 -4.3784781 -3.089282 -2.5891204 -4.1773486 -5.4047952 -6.1780639 -6.51439 -6.6633329 -4.8910828][-10.538873 -11.009646 -10.977533 -8.6478357 -6.9765434 -6.3237648 -6.6727648 -6.0579753 -4.8147049 -5.2618885 -6.633883 -7.1378078 -6.5172334 -7.2801442 -6.2043004][-10.202448 -8.6603127 -7.7586451 -7.0337577 -7.5670071 -6.9444313 -6.6050386 -5.99469 -5.5913429 -5.3537388 -5.5976853 -5.7402349 -6.3866177 -6.7944307 -5.6646681][-6.6355715 -6.7494907 -7.4694338 -6.4980659 -5.3691559 -6.0253143 -7.0336294 -6.2014389 -5.1018839 -5.0767455 -5.1906347 -4.8199048 -5.0114074 -5.2636924 -4.9265003][-4.5083971 -3.5641971 -3.2370772 -4.4023447 -4.4627857 -4.2205725 -3.3109579 -3.1755497 -3.3603518 -3.4878666 -3.855984 -4.210835 -4.7905512 -5.4249659 -5.9100361]]...]
INFO - root - 2017-12-15 17:14:59.122960: step 37710, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 53h:54m:49s remains)
INFO - root - 2017-12-15 17:15:05.701299: step 37720, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 53h:35m:16s remains)
INFO - root - 2017-12-15 17:15:12.343168: step 37730, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.671 sec/batch; 54h:55m:22s remains)
INFO - root - 2017-12-15 17:15:18.981748: step 37740, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 52h:24m:28s remains)
INFO - root - 2017-12-15 17:15:25.573402: step 37750, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 53h:47m:49s remains)
INFO - root - 2017-12-15 17:15:32.178558: step 37760, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.680 sec/batch; 55h:40m:45s remains)
INFO - root - 2017-12-15 17:15:38.726748: step 37770, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 54h:31m:46s remains)
INFO - root - 2017-12-15 17:15:45.233775: step 37780, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 52h:49m:04s remains)
INFO - root - 2017-12-15 17:15:51.867503: step 37790, loss = 0.22, batch loss = 0.18 (12.8 examples/sec; 0.623 sec/batch; 50h:58m:05s remains)
INFO - root - 2017-12-15 17:15:58.491316: step 37800, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 54h:47m:29s remains)
2017-12-15 17:15:59.010588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.6044989 -7.4461374 -6.456419 -5.2476969 -4.9745493 -5.0956988 -5.1864543 -5.3542962 -5.2312655 -4.44506 -4.4714742 -5.9703822 -7.63943 -8.4467564 -8.5978661][-5.2907186 -4.84153 -3.812253 -2.710578 -2.3715298 -1.9307642 -1.6047578 -2.0346673 -2.8957882 -3.7607043 -4.1104331 -5.2455258 -7.1689758 -8.0564432 -7.9022164][-3.8164124 -3.4978504 -2.4839585 -1.6410971 -1.4071941 -0.8237648 -0.57727575 -0.55639458 -1.006196 -1.6570115 -1.9363644 -3.1114769 -4.9979887 -5.7461958 -5.9189229][-2.6601744 -2.6285968 -2.2469778 -1.1390333 -0.75925827 -0.11446905 -0.039404392 -0.35467863 -0.793221 -1.3619485 -1.8852394 -3.2956738 -4.6782284 -5.1593585 -4.70152][-2.733278 -3.5100369 -3.4284701 -1.7711833 -1.0887284 -0.14070034 -0.2293005 -0.7218914 -1.194397 -1.5953622 -1.8619256 -3.5528567 -5.4005628 -6.1155987 -6.0568047][-4.5993729 -4.5116887 -3.7600417 -1.8966269 -0.12901258 0.32941103 0.06555891 0.10316896 -0.048040867 -0.19135427 -0.2418437 -2.2284889 -4.468142 -5.4001446 -5.4621787][-5.947135 -5.6560092 -3.9448595 -1.2793837 -0.20507002 0.96221018 1.6706576 1.2104564 0.22767353 -0.4773221 -0.7464118 -2.4160469 -4.5606265 -5.8696117 -6.048512][-5.7881031 -4.7340465 -3.3162739 -1.5789633 -0.77340221 0.72657394 1.5166497 1.1574554 0.77776861 0.0041880608 -0.64691544 -2.8387547 -5.42181 -6.6821146 -6.4754882][-5.4917903 -5.4104633 -3.8857412 -1.3818026 -0.26218605 0.13561773 0.41531134 1.0317459 1.504178 0.73478079 -0.011926651 -2.6586506 -5.8635774 -7.3548388 -6.8815851][-4.2207346 -3.9590755 -2.67222 -1.0475688 -0.35464621 0.41089392 0.977499 0.674356 -0.2623229 -0.064686775 0.62615824 -2.2172804 -5.9958453 -7.8217754 -8.0520992][-6.4802017 -5.5851822 -4.6540308 -3.1314931 -2.7137661 -2.3520277 -2.2716215 -2.5797446 -3.0086849 -3.0991521 -3.0413115 -5.3535166 -7.0446062 -8.1611137 -8.1608219][-9.2352953 -8.4434242 -7.278934 -6.174305 -6.2241879 -6.3568854 -6.2969866 -5.8691325 -5.6183934 -5.6055684 -5.992084 -7.0357943 -7.651782 -8.1828623 -7.9492636][-10.481344 -9.7791386 -8.77478 -8.1313982 -8.2683039 -8.101656 -7.8745475 -7.4105406 -7.3837767 -7.5049229 -7.2279253 -7.147727 -8.1286774 -7.4223633 -6.577004][-11.000243 -10.308831 -9.0548706 -7.6190262 -7.6657548 -8.0228615 -7.4853821 -6.8753777 -6.9688206 -6.8418241 -6.9885874 -6.9612441 -6.8309212 -6.09797 -6.2222052][-8.2854948 -8.8013763 -8.9881058 -8.2946444 -7.4414015 -6.5204382 -5.9496164 -6.1315131 -6.3448806 -6.4127278 -6.8930378 -6.2247581 -6.39753 -7.4335017 -7.5974197]]...]
INFO - root - 2017-12-15 17:16:05.592228: step 37810, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.673 sec/batch; 55h:03m:51s remains)
INFO - root - 2017-12-15 17:16:12.280162: step 37820, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.694 sec/batch; 56h:50m:27s remains)
INFO - root - 2017-12-15 17:16:18.857772: step 37830, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.656 sec/batch; 53h:39m:59s remains)
INFO - root - 2017-12-15 17:16:25.499833: step 37840, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 54h:11m:24s remains)
INFO - root - 2017-12-15 17:16:32.071712: step 37850, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 54h:17m:45s remains)
INFO - root - 2017-12-15 17:16:38.654846: step 37860, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 53h:51m:45s remains)
INFO - root - 2017-12-15 17:16:45.185757: step 37870, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 52h:57m:43s remains)
INFO - root - 2017-12-15 17:16:51.755302: step 37880, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 53h:29m:17s remains)
INFO - root - 2017-12-15 17:16:58.324325: step 37890, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 53h:12m:29s remains)
INFO - root - 2017-12-15 17:17:04.885463: step 37900, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 53h:07m:09s remains)
2017-12-15 17:17:05.455745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.950037 -9.72307 -10.501659 -10.843338 -11.159539 -9.8652954 -8.3288708 -8.4993172 -8.2124557 -9.0982647 -10.077192 -12.105065 -15.34803 -17.311152 -15.431978][-5.9585733 -7.1480331 -7.8314791 -9.4520178 -10.288504 -10.443501 -9.3391142 -7.6864271 -7.6134844 -7.8605742 -7.739574 -9.0643263 -12.238521 -16.071438 -15.23646][-3.8156233 -5.3822794 -7.9538631 -9.626256 -10.786877 -11.122768 -10.7034 -9.3617249 -8.194622 -7.8849645 -8.0143929 -9.0136938 -10.765162 -13.088789 -12.573132][-3.7121506 -5.5168839 -7.2038455 -7.84542 -8.8462858 -8.3030567 -7.448307 -8.0383291 -8.1077671 -6.9599085 -6.3663144 -7.4417305 -10.392855 -13.166071 -11.688903][-4.4935331 -6.1729226 -7.6076083 -7.2683926 -6.2715955 -3.4040942 -2.1080339 -3.0940995 -4.5212626 -5.5427332 -5.4121504 -5.9903488 -9.1218052 -14.270985 -13.973206][-6.0763807 -6.038311 -6.7983618 -5.3013878 -2.8714881 0.59514093 4.1654038 4.9052987 2.176188 -1.178689 -3.0856283 -3.7867069 -6.032865 -10.370367 -13.186873][-7.0311365 -6.4568968 -5.8759766 -4.0258646 -0.8097682 2.9201341 7.2756696 9.2668419 7.4835334 2.6640325 -2.3767967 -3.5994172 -3.8545969 -7.0669341 -8.1212626][-8.3662367 -7.2847891 -6.6231709 -4.2178841 -1.0248113 4.8713527 9.1472836 9.8842735 8.5238342 5.1563334 0.76495457 -1.8954232 -5.2164168 -6.8859935 -6.0386553][-7.8400593 -7.1772413 -6.3229461 -4.5812864 -2.9845314 1.8056626 6.8438382 8.7565765 8.5638943 3.5398202 -0.95303154 -4.02054 -8.291893 -11.226294 -11.268418][-6.0814266 -7.1825423 -6.9109488 -5.4574413 -4.4401097 -1.8651226 1.5888853 4.5026593 4.0987573 1.1053295 -3.1034989 -9.3077641 -12.879261 -16.495745 -16.98193][-10.332529 -10.651354 -10.953617 -9.0686493 -7.6675563 -7.1237984 -5.8633947 -3.6219885 -3.7086089 -5.6261535 -9.3122568 -13.783552 -18.382263 -21.668232 -20.709126][-12.617451 -12.082985 -11.797158 -11.008196 -9.1234694 -8.1741762 -8.267889 -8.9312611 -10.054726 -11.374567 -13.462143 -14.962553 -16.786558 -21.124016 -19.525412][-14.008806 -12.656734 -11.375311 -9.9657917 -10.593589 -9.5355988 -8.4316378 -8.3817511 -9.65287 -11.759437 -14.073614 -14.331745 -16.650261 -17.839472 -16.033609][-12.467094 -12.276431 -9.6564617 -8.2617512 -8.312355 -8.5161686 -8.2616558 -7.5879779 -6.9661865 -8.875762 -10.077047 -10.868408 -10.26227 -11.381699 -11.203669][-8.5757742 -8.7183857 -7.272388 -5.6249337 -4.0615091 -6.1423864 -8.045125 -9.08935 -9.7156925 -9.184454 -9.255373 -9.4672976 -9.492424 -8.3557158 -8.7938337]]...]
INFO - root - 2017-12-15 17:17:12.049677: step 37910, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 54h:15m:28s remains)
INFO - root - 2017-12-15 17:17:18.620975: step 37920, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.682 sec/batch; 55h:47m:45s remains)
INFO - root - 2017-12-15 17:17:25.286370: step 37930, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 55h:52m:37s remains)
INFO - root - 2017-12-15 17:17:31.883709: step 37940, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 54h:45m:26s remains)
INFO - root - 2017-12-15 17:17:38.501340: step 37950, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 52h:43m:41s remains)
INFO - root - 2017-12-15 17:17:45.055129: step 37960, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 54h:29m:49s remains)
INFO - root - 2017-12-15 17:17:51.538419: step 37970, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 53h:16m:36s remains)
INFO - root - 2017-12-15 17:17:58.120101: step 37980, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 53h:27m:04s remains)
INFO - root - 2017-12-15 17:18:04.715358: step 37990, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 53h:51m:55s remains)
INFO - root - 2017-12-15 17:18:11.337562: step 38000, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 53h:39m:49s remains)
2017-12-15 17:18:11.859944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6251316 -3.6533477 -4.6394463 -6.6528134 -8.2455711 -8.852169 -9.1261635 -8.6622925 -7.7509017 -7.7075043 -7.0785561 -7.02268 -7.6098266 -8.1435766 -6.4944959][-2.5427511 -2.2035666 -3.5021386 -5.6794353 -7.9879379 -9.7077656 -10.289786 -9.9852638 -10.096849 -9.8479862 -9.0992184 -9.4174328 -10.161174 -11.185377 -8.5664835][-4.044075 -3.6192389 -4.2731895 -5.6029811 -7.6704373 -9.4980373 -10.380825 -10.383217 -9.9958305 -9.776453 -10.088367 -10.362793 -10.712683 -11.630253 -8.7289143][-5.7414665 -7.207684 -7.9293785 -8.5228252 -10.63023 -10.376726 -9.2238312 -9.6854153 -10.52309 -9.7003059 -10.027919 -10.978018 -11.698932 -11.757021 -8.6939182][-8.7781334 -10.58964 -12.107191 -11.530113 -9.7121544 -7.2582574 -5.3879595 -5.9815831 -7.3724184 -7.651082 -8.7605648 -10.343985 -12.078625 -12.837108 -9.7881994][-8.1604013 -9.3472662 -10.071386 -10.065847 -9.2579069 -3.4354537 2.2495732 1.96451 -0.21282578 -3.2497787 -6.4303856 -8.7585354 -10.748569 -12.386789 -9.92501][-7.1651592 -7.3043447 -8.6196289 -7.292594 -4.2844911 1.831533 6.98364 7.748632 6.2230735 1.2860661 -3.904563 -6.4540644 -8.5767365 -10.732327 -8.8564291][-8.9402142 -8.0208483 -7.472312 -5.9036527 -2.7848797 4.3020682 10.95451 12.467474 10.291893 3.6512733 -1.6804671 -5.4767065 -8.7601748 -9.3786564 -7.1975417][-7.2477589 -8.3038254 -9.1799383 -6.5521736 -3.3788483 1.7518921 6.3133187 8.4582062 8.5567818 3.4582553 -1.5750713 -6.035521 -9.7010345 -10.946955 -8.6862211][-6.4384527 -7.1681962 -6.4411306 -5.1402645 -3.7475481 1.4207153 4.1909108 4.0338435 2.5938373 -2.1472564 -5.56183 -10.176449 -13.966724 -15.928583 -13.187843][-7.3055859 -6.4075255 -4.8486581 -2.5087039 -1.0715508 -0.049290657 -0.72640896 -0.53760862 -2.6341472 -6.3384142 -8.78656 -12.502779 -15.74439 -16.344923 -13.447271][-13.160583 -9.2421312 -6.2322311 -3.905139 -1.5728884 -1.3240638 -3.0004842 -5.0863886 -8.422699 -10.871353 -12.028908 -13.30661 -14.366425 -13.726721 -11.22027][-14.663343 -9.60075 -5.0312729 -2.0282624 -0.92436361 -0.23951817 -1.6004071 -4.9198956 -9.0305119 -11.035202 -10.9292 -12.098998 -11.252308 -9.6094818 -6.1852942][-14.366554 -9.8304958 -6.0700312 -3.8741763 -3.1121113 -3.1022639 -3.2849231 -4.0702667 -6.615449 -8.4929943 -8.2872686 -7.8596306 -6.2699885 -5.6292272 -2.874378][-11.557946 -8.231122 -4.2864037 -2.0713534 -2.4388816 -3.5388074 -3.9233184 -4.58486 -6.3896489 -7.0453534 -6.9161835 -7.2925544 -6.8276258 -5.9723468 -2.8733621]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 17:18:18.492094: step 38010, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 56h:01m:12s remains)
INFO - root - 2017-12-15 17:18:25.088538: step 38020, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 55h:00m:02s remains)
INFO - root - 2017-12-15 17:18:31.650089: step 38030, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 54h:09m:51s remains)
INFO - root - 2017-12-15 17:18:38.260766: step 38040, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 54h:43m:37s remains)
INFO - root - 2017-12-15 17:18:44.892083: step 38050, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 54h:10m:30s remains)
INFO - root - 2017-12-15 17:18:51.557683: step 38060, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 54h:06m:18s remains)
INFO - root - 2017-12-15 17:18:58.185108: step 38070, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 53h:14m:59s remains)
INFO - root - 2017-12-15 17:19:04.814225: step 38080, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 52h:55m:10s remains)
INFO - root - 2017-12-15 17:19:11.373539: step 38090, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.679 sec/batch; 55h:32m:24s remains)
INFO - root - 2017-12-15 17:19:17.862355: step 38100, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 54h:07m:08s remains)
2017-12-15 17:19:18.398500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.7964687 -9.8519983 -10.157066 -10.270088 -10.676451 -10.851772 -10.378193 -8.8587837 -8.0436325 -7.6838274 -7.4771414 -8.4026718 -10.719995 -8.4475451 -8.3903675][-8.3632584 -7.9990854 -7.041544 -7.1659818 -8.2853966 -8.7624464 -8.532114 -7.751009 -7.2595854 -6.7974548 -6.7855043 -8.4679089 -10.902829 -9.1028423 -9.7691231][-5.9564333 -6.817234 -7.128181 -6.4631257 -7.8842497 -8.686985 -8.8382664 -7.6649847 -6.7127748 -6.461175 -6.7136045 -7.9035382 -10.958659 -9.7269773 -10.635018][-6.8631306 -6.800148 -6.4740205 -7.0734367 -8.5668125 -7.73859 -7.2615337 -6.6465511 -5.9619417 -5.4175844 -5.7393909 -7.2524037 -10.2024 -9.2694473 -10.874353][-8.4475565 -9.6038513 -10.651176 -8.9663 -7.3181243 -5.063952 -2.6721458 -3.4888251 -5.8120542 -5.4609256 -5.6368933 -7.2244148 -10.2071 -9.103714 -10.685234][-10.231808 -9.8102 -10.199263 -7.1970105 -3.9733639 -0.70968819 3.4154973 2.6830564 -1.1675792 -3.1844509 -5.6146126 -6.567493 -9.3154964 -8.5666723 -10.383047][-10.086452 -10.032991 -8.8322048 -5.2491612 -1.4650087 2.3508544 7.7614741 6.6710067 3.4200969 -0.50418043 -5.1463456 -6.239109 -10.065626 -9.6652184 -11.261932][-10.841618 -9.74646 -9.1204529 -4.6189585 -0.83010006 4.2584233 8.7894669 6.9565673 4.7259755 0.86305428 -3.5927157 -6.5917459 -10.697745 -9.2226143 -10.805013][-8.6161346 -7.9946995 -7.564374 -4.4095459 -1.6872015 1.5134602 5.476768 4.7760091 2.6443124 -0.52443504 -4.2602081 -8.1474094 -12.146263 -9.939806 -11.124735][-7.0928626 -7.1985703 -7.5056896 -5.4789095 -3.2108972 -1.9236503 0.96347618 1.1247926 -0.72028017 -3.7296658 -6.4694347 -8.6348 -11.736167 -9.99897 -12.50841][-11.39288 -11.403795 -10.859674 -9.4936581 -8.7321434 -7.8406858 -6.1072831 -6.3911881 -6.4491682 -6.7105165 -8.29814 -10.79542 -12.389496 -11.494966 -11.853209][-15.5625 -15.52227 -15.166225 -14.892076 -14.624651 -13.288696 -12.464067 -12.563974 -11.47901 -11.139038 -11.3615 -11.516804 -11.844766 -11.653378 -11.850361][-17.0499 -16.380947 -17.029995 -16.566118 -16.118366 -15.171366 -15.002556 -14.660353 -14.258776 -13.354893 -12.332947 -12.74013 -12.592605 -11.461672 -10.61772][-14.438725 -14.41604 -14.362963 -13.629344 -11.949715 -13.269373 -12.88649 -13.170543 -12.585486 -12.165829 -11.658144 -10.993235 -10.982989 -10.200809 -9.4932156][-10.61562 -10.399738 -9.8644791 -9.5523472 -8.6205187 -8.6149769 -7.3732519 -8.3008575 -9.1272964 -8.9990673 -9.0154371 -9.477953 -10.103297 -9.8769579 -9.8170509]]...]
INFO - root - 2017-12-15 17:19:24.893288: step 38110, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 54h:42m:22s remains)
INFO - root - 2017-12-15 17:19:31.434627: step 38120, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 53h:01m:18s remains)
INFO - root - 2017-12-15 17:19:37.990792: step 38130, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.691 sec/batch; 56h:32m:08s remains)
INFO - root - 2017-12-15 17:19:44.573466: step 38140, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 52h:11m:33s remains)
INFO - root - 2017-12-15 17:19:51.103738: step 38150, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 54h:56m:17s remains)
INFO - root - 2017-12-15 17:19:57.755278: step 38160, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 52h:46m:45s remains)
INFO - root - 2017-12-15 17:20:04.398317: step 38170, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 52h:44m:42s remains)
INFO - root - 2017-12-15 17:20:11.064154: step 38180, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 54h:43m:31s remains)
INFO - root - 2017-12-15 17:20:17.627548: step 38190, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 54h:03m:38s remains)
INFO - root - 2017-12-15 17:20:24.227731: step 38200, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 55h:01m:46s remains)
2017-12-15 17:20:24.732282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7956233 -2.0722764 -2.0310156 -2.1811492 -3.2889545 -3.1449869 -4.0017891 -4.8475828 -5.1366615 -5.720561 -5.3965788 -7.4182138 -8.702755 -8.7736359 -7.959126][-1.8754008 -1.0355701 -1.1967564 -1.9515076 -3.6156025 -4.4024744 -5.0043645 -5.5644588 -5.9193721 -5.9385343 -6.0376382 -8.1894112 -9.1219654 -10.122158 -9.8229733][0.021844864 -0.13080454 -1.486208 -1.5978856 -3.1523914 -3.8592017 -4.257596 -4.4040742 -4.6763048 -5.91947 -6.3090096 -8.2045546 -8.5397644 -8.9867134 -8.47499][-2.2714353 -1.9354293 -1.6557832 -1.5183816 -2.6120734 -3.036108 -3.3384023 -3.9270382 -4.6941328 -4.438447 -4.3119907 -6.9050913 -8.1517124 -8.2938747 -7.4636173][-3.6760898 -4.3315892 -4.0607305 -2.7423894 -2.6463392 -1.5977163 -1.3783464 -1.9248383 -2.6258423 -3.3638182 -3.2922451 -5.4368391 -6.07954 -6.9756045 -7.155983][-5.6929693 -5.8118434 -4.9600034 -2.5138965 -1.0918422 0.43552494 0.88874006 0.24072409 -0.66597033 -1.2526064 -1.1517282 -2.5197239 -3.4470057 -5.0297623 -6.2452283][-7.3058958 -6.6297388 -5.0419378 -1.889612 0.06474638 2.006217 3.4370408 3.338697 3.0017362 1.7365484 0.56078339 -1.6474342 -3.0945032 -4.2990513 -4.4228425][-8.3222132 -6.97718 -5.7040429 -2.151906 0.69688511 2.7022843 4.1653695 3.6507516 3.2902865 2.7195468 1.7099252 -0.75950575 -1.7996416 -3.5914359 -4.6033578][-8.0045214 -6.9948349 -4.9247608 -1.7382638 -0.27944708 1.4540935 3.1145205 2.317389 1.990344 1.6224847 1.2488666 -0.97626781 -2.8337996 -4.4953156 -4.7746267][-8.3966808 -8.2180672 -6.577199 -3.5464487 -1.5286808 0.30718088 1.7858472 1.4714389 1.3287592 0.97695923 0.69731283 -1.683785 -2.7365766 -4.4511619 -5.7396159][-11.4584 -11.49107 -10.192783 -7.1187172 -5.9076242 -3.5896444 -1.7578506 -1.2526612 -0.731688 -0.40487576 -0.31097555 -2.7179832 -4.5494008 -4.9650168 -5.3363004][-15.496733 -15.503077 -13.808462 -11.364211 -9.2240744 -7.155098 -6.6791716 -5.3016562 -4.2993155 -3.7394214 -3.4486098 -4.3399849 -4.8791432 -5.4013038 -5.3023815][-15.429365 -15.637161 -14.17952 -12.210234 -11.017845 -8.8269911 -8.7437439 -8.64188 -7.4978132 -6.57715 -5.9726133 -6.3439078 -6.371305 -5.8585281 -5.3395076][-10.97046 -11.597695 -11.372923 -10.19001 -8.7012405 -7.6837835 -8.2490864 -7.2909088 -6.8207936 -6.084023 -5.7792206 -6.01041 -5.8162789 -5.92569 -6.0052357][-6.4673896 -6.7111435 -6.9808359 -6.3744631 -5.8754725 -4.8359413 -4.6104794 -4.794867 -4.6296086 -4.2700186 -3.344141 -4.024416 -5.0632443 -5.8290572 -6.5975]]...]
INFO - root - 2017-12-15 17:20:31.280996: step 38210, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 53h:27m:05s remains)
INFO - root - 2017-12-15 17:20:37.926758: step 38220, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 54h:10m:50s remains)
INFO - root - 2017-12-15 17:20:44.512784: step 38230, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 53h:39m:21s remains)
INFO - root - 2017-12-15 17:20:51.108193: step 38240, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.673 sec/batch; 54h:58m:18s remains)
INFO - root - 2017-12-15 17:20:57.684657: step 38250, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 54h:32m:18s remains)
INFO - root - 2017-12-15 17:21:04.266452: step 38260, loss = 0.19, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 52h:38m:15s remains)
INFO - root - 2017-12-15 17:21:10.842521: step 38270, loss = 0.11, batch loss = 0.06 (12.3 examples/sec; 0.650 sec/batch; 53h:07m:31s remains)
INFO - root - 2017-12-15 17:21:17.397660: step 38280, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 53h:02m:31s remains)
INFO - root - 2017-12-15 17:21:23.973442: step 38290, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 52h:32m:32s remains)
INFO - root - 2017-12-15 17:21:30.519163: step 38300, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 54h:29m:32s remains)
2017-12-15 17:21:31.035103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9170227 -7.07629 -7.3445611 -6.1434765 -6.299015 -5.8623028 -5.74774 -5.3373003 -5.2354617 -5.5099382 -5.1861782 -6.3609295 -8.3801126 -8.33721 -7.6900654][-6.1271362 -7.0133414 -6.2951555 -5.1757512 -5.3141937 -4.56713 -3.9142151 -4.0069971 -4.078948 -4.0400672 -4.3158588 -6.1291323 -7.990766 -8.7058916 -8.3191891][-4.5983377 -5.4136438 -5.6666613 -4.3349504 -4.0144062 -3.3062508 -3.2160046 -2.8248186 -2.9065354 -3.4175365 -3.8530951 -5.7319174 -7.8618279 -8.2017708 -8.4205313][-2.9424264 -4.4199581 -5.2824054 -4.6958728 -4.0986695 -3.1711977 -3.3466263 -3.2733715 -3.2826388 -3.3308842 -3.2159524 -4.9071083 -6.45078 -7.5736818 -7.7453279][-3.3666456 -4.839983 -4.8212771 -3.6027911 -2.8761954 -2.1331093 -1.7886829 -2.7480972 -3.9715316 -3.9055674 -3.8624916 -5.335772 -6.2836366 -6.0507412 -5.2667942][-4.3955216 -4.2988968 -4.1022124 -2.8642695 -1.3453317 0.66564512 1.2605972 0.274961 -0.74476576 -2.5098479 -3.5714836 -4.3321171 -5.5601044 -5.7537594 -4.4598765][-4.4158792 -4.3432193 -3.9220738 -1.7557635 -0.21743011 2.4424214 3.9469886 3.6831651 2.602037 0.87391615 0.11875725 -2.0706131 -4.1118369 -4.2660332 -4.3190455][-3.4953816 -3.098949 -1.5951276 0.43331861 1.3666506 3.4757743 4.4462981 3.8073878 3.2526717 2.1770563 1.5798421 -0.47624302 -2.3190081 -2.2435527 -1.7067075][-3.1619353 -2.0152156 -0.40716648 1.5935087 2.8983417 3.4265018 2.6566615 1.8772588 0.88621044 0.30954742 0.31327677 -0.78786373 -2.5584655 -2.7002742 -1.8124084][-1.7004352 -0.25282431 0.11830664 1.4928403 2.3823495 1.9432621 1.5079694 0.29246235 -1.6145554 -1.7527235 -1.0628109 -2.4811597 -3.9142561 -3.4326518 -2.7482457][-3.8168125 -3.5608108 -1.7908893 -0.88630152 -1.412528 -1.0235877 -0.81946754 -1.8525293 -2.8242078 -2.8009329 -2.7904925 -4.6895103 -5.364274 -5.2584987 -5.3665228][-8.5036583 -8.52699 -6.6980968 -5.1371818 -4.6170154 -4.4967957 -4.8732624 -4.8248973 -4.5267162 -4.6367431 -4.95823 -5.8214478 -5.9872026 -6.0192986 -5.1820631][-10.718891 -9.3079967 -7.9977913 -6.98612 -6.2087264 -6.2455354 -6.8549871 -7.017797 -7.3587923 -7.15912 -6.7716737 -6.9688673 -7.0137262 -6.4309297 -5.8085351][-7.2693672 -6.6634655 -6.5564218 -5.5626478 -4.4830813 -5.1313429 -6.0061831 -6.9361906 -8.0114861 -7.8355379 -8.0595779 -8.4206448 -7.9646668 -6.7711592 -6.9071465][-4.2917213 -3.8311107 -3.0201111 -2.5745704 -2.9974072 -3.081018 -2.9979539 -4.1670241 -5.1091752 -5.9283967 -6.2505875 -6.4624515 -6.6968241 -7.6123095 -7.8508186]]...]
INFO - root - 2017-12-15 17:21:37.606870: step 38310, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 52h:28m:39s remains)
INFO - root - 2017-12-15 17:21:44.193625: step 38320, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 52h:56m:46s remains)
INFO - root - 2017-12-15 17:21:50.770739: step 38330, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 54h:04m:11s remains)
INFO - root - 2017-12-15 17:21:57.386412: step 38340, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 55h:41m:29s remains)
INFO - root - 2017-12-15 17:22:03.940679: step 38350, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 53h:01m:57s remains)
INFO - root - 2017-12-15 17:22:10.505289: step 38360, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 52h:59m:35s remains)
INFO - root - 2017-12-15 17:22:17.117624: step 38370, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 52h:18m:25s remains)
INFO - root - 2017-12-15 17:22:23.735957: step 38380, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 53h:56m:50s remains)
INFO - root - 2017-12-15 17:22:30.291732: step 38390, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 52h:56m:13s remains)
INFO - root - 2017-12-15 17:22:36.953718: step 38400, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 53h:51m:58s remains)
2017-12-15 17:22:37.448180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6884179 -6.4895616 -4.4481821 -2.4582922 -1.076911 -1.7331071 -2.8738399 -4.0673351 -5.2526784 -6.3572588 -6.410017 -8.9959822 -11.716368 -11.072096 -9.6551781][-5.0070376 -5.176652 -5.548265 -3.9304588 -2.6738558 -2.2085075 -1.4223032 -2.7802444 -4.18785 -5.3211608 -6.7041311 -8.99706 -10.643824 -11.432793 -10.818115][-4.5232973 -5.7236285 -6.0635409 -4.2898312 -3.8560438 -2.6104038 -1.8402028 -2.3471839 -3.5898054 -4.1752076 -4.4337521 -7.6393538 -10.360675 -9.8844652 -10.174675][-3.490598 -5.8071055 -7.3970537 -7.0794873 -6.3748617 -4.3225346 -1.6881833 -0.89098787 -2.5328264 -4.9049296 -7.1032953 -9.95042 -11.144945 -10.84294 -9.9030428][-5.0169144 -6.1964703 -7.3474708 -7.3854313 -5.44769 -1.8970177 -0.0997076 -0.43781805 -1.842345 -3.1573708 -5.1204805 -9.4256544 -12.351328 -13.018929 -12.684135][-6.0757627 -5.26393 -5.0719967 -5.4252319 -2.6539123 1.8441377 6.0532746 5.8028903 1.8694963 -2.4300241 -6.1618333 -8.7915859 -10.834996 -12.759917 -13.209198][-7.466661 -7.4559269 -7.0575371 -4.1889591 -0.8502574 3.9742045 8.6380939 10.077049 7.5824847 1.3922286 -5.1707597 -7.2120891 -9.1660223 -10.519787 -11.907894][-7.23365 -8.9050274 -8.5029335 -4.3290014 0.30894232 5.6492982 10.411369 10.05689 7.5632291 3.1419873 -2.3430612 -7.5931611 -9.8022995 -9.0041294 -9.1023216][-6.9949341 -8.4117069 -7.3197083 -5.3555603 -2.9437423 3.5957379 8.2080784 8.5105934 6.1788707 0.14895439 -3.8701735 -8.6853495 -11.815048 -12.079896 -10.775562][-5.6355119 -6.960206 -7.1136408 -5.2858162 -3.248801 0.00078630447 1.9503789 3.671762 2.9010892 -2.6619694 -8.0190325 -12.586261 -14.6325 -13.782616 -12.859211][-9.33714 -10.624321 -9.68243 -7.5897608 -6.5494981 -4.6009769 -4.5949483 -3.8783686 -4.92414 -7.6452241 -10.589375 -14.473095 -17.776333 -16.243914 -14.145765][-13.157522 -13.195803 -12.974252 -10.735556 -8.3781738 -7.1556726 -7.4521623 -8.2890406 -9.7703314 -11.4062 -12.539208 -15.357988 -15.540981 -14.364847 -12.877573][-14.853268 -14.266472 -13.023077 -12.754131 -12.411758 -11.06641 -9.9738693 -10.17849 -10.454845 -10.966883 -11.15527 -12.107231 -12.820626 -11.433029 -9.1402521][-11.424252 -10.698905 -11.234192 -10.873343 -10.429422 -10.098853 -9.626152 -9.3194065 -9.0354433 -9.5018234 -9.4085674 -7.7570071 -7.5089226 -8.1052113 -8.0007477][-5.4488878 -6.3003931 -4.7244172 -4.5114603 -5.4360838 -6.8755198 -7.4502358 -6.700202 -6.3282809 -6.5571275 -6.1816316 -7.0582 -7.4708457 -6.8733072 -7.068398]]...]
INFO - root - 2017-12-15 17:22:43.983780: step 38410, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 53h:00m:31s remains)
INFO - root - 2017-12-15 17:22:50.590585: step 38420, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 54h:06m:53s remains)
INFO - root - 2017-12-15 17:22:57.143697: step 38430, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 52h:35m:57s remains)
INFO - root - 2017-12-15 17:23:03.767784: step 38440, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 54h:05m:58s remains)
INFO - root - 2017-12-15 17:23:10.348201: step 38450, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.668 sec/batch; 54h:32m:27s remains)
INFO - root - 2017-12-15 17:23:16.954905: step 38460, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 53h:37m:29s remains)
INFO - root - 2017-12-15 17:23:23.587795: step 38470, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 53h:22m:30s remains)
INFO - root - 2017-12-15 17:23:30.127756: step 38480, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 54h:08m:49s remains)
INFO - root - 2017-12-15 17:23:36.725077: step 38490, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 53h:51m:10s remains)
INFO - root - 2017-12-15 17:23:43.398397: step 38500, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 54h:03m:55s remains)
2017-12-15 17:23:43.865805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0079193 -5.6721873 -4.7089553 -4.1597395 -3.778743 -3.5941951 -2.5709741 -2.3719239 -0.77395487 2.9798121 4.8471169 2.7904153 -0.091263294 -3.3896644 -5.5571179][-4.7080293 -3.7525082 -3.8576438 -1.5732179 -0.40985203 -0.57162762 -0.47848415 0.48674107 1.2780757 0.97556114 1.3303227 -0.040820122 -3.4579706 -5.1903768 -6.0465217][-2.0234311 -1.9707675 -2.6996872 -2.095674 -2.093245 -0.75875139 0.67020035 2.1507592 2.8089757 2.7975326 1.4976125 -2.6278656 -6.8184342 -7.7378817 -8.5416231][-2.0754921 -2.7842708 -2.5100975 -2.0604069 -2.248616 -2.01411 -0.74104214 1.3891225 3.1290479 2.8717513 2.1790872 -1.9286571 -7.4034638 -9.3681564 -10.205164][-1.1201315 -1.3638973 -1.9249165 -1.9583797 -1.1641321 -0.99993896 -0.30930424 0.43271637 1.766191 1.9425516 1.3548012 -2.5796633 -6.6183367 -8.6031713 -10.799632][-2.6195829 -2.1992464 -1.1290979 0.55719662 1.0265951 1.4250293 1.4686174 0.706058 0.60004663 0.42862558 0.15989208 -2.5900545 -5.5158238 -7.9039907 -9.6918888][-3.3871117 -2.3923497 -1.1567931 1.7926569 2.9349971 3.9903979 3.6391749 3.7221227 2.65863 0.9433651 0.23364687 -1.6044488 -4.3160982 -7.0345855 -8.4632168][-4.0902734 -2.798063 -0.34510469 1.732554 3.2030759 5.1164832 6.2978225 6.5618625 4.3589034 2.53583 1.1627212 -2.1467674 -5.6685243 -6.5730734 -6.1263375][-3.9341974 -2.8629172 -1.7967668 0.36023426 2.5067286 4.4346204 5.6533647 5.6369567 4.5255408 2.4705272 0.8550539 -1.8848658 -4.5315809 -5.7001543 -6.2721515][-4.0044661 -3.9221048 -3.7149842 -2.1331291 -0.51730871 0.7783165 2.4288011 2.5495734 1.7383294 0.4193244 -0.93367958 -3.9274662 -6.3534608 -6.3020835 -6.2654343][-9.0466814 -8.5908566 -7.9778557 -6.3437634 -5.4293509 -5.3574467 -4.9230547 -5.4349461 -5.8182173 -6.2599154 -5.9666 -7.3253274 -8.2430553 -8.0746336 -8.2796211][-13.365667 -12.729283 -10.827223 -9.5536823 -10.063966 -9.5334263 -9.8427591 -10.712279 -12.18724 -12.333192 -11.24098 -10.731499 -10.206171 -8.9720917 -8.4793358][-14.08234 -13.568068 -12.247046 -10.501897 -10.005871 -10.170443 -10.819363 -12.263189 -14.838966 -15.471859 -14.476009 -12.364 -10.500568 -9.3712187 -8.8370857][-12.796622 -12.181229 -10.739939 -9.4040337 -8.925539 -9.7861595 -9.7559528 -10.55211 -11.523285 -12.342069 -12.81787 -10.888119 -8.6254177 -7.8568258 -8.0810289][-9.8280563 -10.182132 -9.0739279 -8.0559969 -7.1724081 -6.4873767 -5.6513114 -7.7629042 -8.3665771 -8.5399837 -9.0954151 -10.531533 -10.613655 -9.31701 -7.8993073]]...]
INFO - root - 2017-12-15 17:23:50.499533: step 38510, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 55h:05m:19s remains)
INFO - root - 2017-12-15 17:23:57.132186: step 38520, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 53h:19m:44s remains)
INFO - root - 2017-12-15 17:24:03.768120: step 38530, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 54h:17m:25s remains)
INFO - root - 2017-12-15 17:24:10.380322: step 38540, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 53h:28m:50s remains)
INFO - root - 2017-12-15 17:24:16.906297: step 38550, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 53h:56m:35s remains)
INFO - root - 2017-12-15 17:24:23.544705: step 38560, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 53h:30m:35s remains)
INFO - root - 2017-12-15 17:24:30.155467: step 38570, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 55h:32m:21s remains)
INFO - root - 2017-12-15 17:24:36.753540: step 38580, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.639 sec/batch; 52h:12m:37s remains)
INFO - root - 2017-12-15 17:24:43.303872: step 38590, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 51h:58m:53s remains)
INFO - root - 2017-12-15 17:24:50.010812: step 38600, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 53h:18m:01s remains)
2017-12-15 17:24:50.509032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5866024 -3.0001352 -1.9788666 -2.1223586 -3.5068054 -4.9502649 -5.7034168 -6.7796516 -7.5205941 -7.4292841 -6.7272763 -7.2002163 -8.0446014 -9.5112553 -10.151412][-3.1173489 -2.3609636 -2.6802971 -2.8774624 -3.9672232 -5.1863842 -5.7302337 -6.7254167 -7.5806551 -7.8764648 -7.3301849 -7.8284769 -8.9947567 -10.699536 -11.058035][-2.5336823 -2.7619226 -3.3615921 -3.3404686 -4.361845 -5.1353092 -5.7416487 -6.7076187 -7.0654078 -7.1470485 -6.3821034 -7.2163424 -8.4518719 -9.7988873 -10.705826][-1.0036221 -1.5446916 -1.9630804 -2.6903431 -4.1423092 -4.5560703 -4.5988054 -4.9201345 -5.5468278 -5.3346481 -4.9613338 -6.4824238 -9.37722 -11.037227 -10.374613][-4.090126 -3.9804425 -3.6928241 -2.9649789 -3.7993622 -3.3320115 -2.4707208 -2.4789016 -2.7906272 -2.9309647 -3.5939863 -5.9248867 -8.6114931 -10.599886 -11.308811][-5.893218 -6.131609 -4.794632 -3.7942724 -3.09726 -2.5869896 -1.1266408 0.50293922 0.94676256 -0.6535573 -3.2234166 -4.7229285 -7.64352 -10.061506 -10.533661][-7.2361169 -7.5335164 -6.7870836 -4.23005 -2.4213059 -0.88160324 0.35802412 1.9624543 3.2804494 2.430943 0.65648985 -2.8420112 -6.8046474 -9.0164185 -9.1842489][-5.8816595 -6.7256064 -5.6127038 -3.5953803 -1.7924516 0.50741148 2.813005 3.5693936 4.3698487 4.5737805 3.0746932 -0.75762749 -4.8661423 -6.8683691 -7.7204676][-4.7696729 -5.2632637 -3.8693595 -1.8184776 -0.65833092 1.2687426 2.8271689 4.6214585 5.339541 4.309607 3.6126256 0.91497564 -2.2437088 -5.8051777 -6.6177773][-5.3029256 -5.332448 -3.8833156 -1.1410284 0.28375769 0.95765734 1.9464245 2.4632144 2.5523686 1.9310102 0.69361973 -1.5807595 -4.5686483 -6.616128 -7.4660416][-9.0904579 -8.0163784 -5.9926696 -4.0121584 -2.9218256 -2.4851794 -2.713671 -2.5441468 -2.0357215 -2.197413 -3.45891 -7.198947 -10.81515 -10.499108 -9.1562691][-12.315291 -11.564598 -9.8878536 -7.3046837 -6.2927933 -6.3086638 -7.5471435 -8.1105261 -7.9502048 -7.6578856 -7.8919935 -10.250975 -12.039634 -12.304941 -10.627432][-13.752625 -13.871239 -11.987923 -10.92923 -11.062302 -10.232863 -9.411828 -9.5269089 -9.6047573 -8.8327866 -8.6558857 -9.6589394 -10.841431 -10.468205 -9.3708916][-12.911232 -13.355268 -13.162655 -11.624958 -10.153461 -9.9325314 -10.130013 -8.992897 -6.9512539 -7.1334229 -8.3504906 -7.8051939 -7.3970523 -7.120182 -7.2278242][-8.8490944 -10.050219 -10.425209 -9.512166 -8.2766237 -7.7470422 -6.9717493 -5.6371922 -5.3716426 -4.1242027 -3.8212276 -5.3656349 -7.2639132 -6.8687377 -6.9012003]]...]
INFO - root - 2017-12-15 17:24:57.094899: step 38610, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 54h:49m:28s remains)
INFO - root - 2017-12-15 17:25:03.683507: step 38620, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 53h:21m:29s remains)
INFO - root - 2017-12-15 17:25:10.380137: step 38630, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 54h:28m:07s remains)
INFO - root - 2017-12-15 17:25:16.970155: step 38640, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 55h:00m:22s remains)
INFO - root - 2017-12-15 17:25:23.637914: step 38650, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 53h:37m:36s remains)
INFO - root - 2017-12-15 17:25:30.242792: step 38660, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 54h:01m:40s remains)
INFO - root - 2017-12-15 17:25:36.893033: step 38670, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 51h:53m:11s remains)
INFO - root - 2017-12-15 17:25:43.462959: step 38680, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.665 sec/batch; 54h:16m:53s remains)
INFO - root - 2017-12-15 17:25:50.048397: step 38690, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.634 sec/batch; 51h:42m:36s remains)
INFO - root - 2017-12-15 17:25:56.534103: step 38700, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.656 sec/batch; 53h:33m:05s remains)
2017-12-15 17:25:57.077660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1159272 -8.4183807 -8.5582647 -8.5187473 -9.3606644 -10.129215 -10.019991 -9.4656391 -8.8061476 -9.0775795 -9.0307045 -9.8299675 -11.294011 -10.911568 -9.2433414][-8.306263 -9.5826044 -9.7416916 -9.1817465 -9.24263 -9.1856785 -9.378046 -8.9605093 -8.1058931 -7.1682997 -6.9184537 -8.6083727 -10.379826 -10.763209 -10.992472][-7.5357056 -9.5223885 -10.813339 -10.780427 -10.1305 -8.4608889 -7.9316583 -7.444901 -7.4184027 -7.765377 -7.5851483 -9.5012684 -11.440891 -11.623087 -11.685231][-8.4831543 -10.055597 -10.257239 -9.8467617 -8.8389168 -7.0424337 -6.1894603 -6.279253 -7.1954608 -7.9364829 -7.8097191 -9.6730967 -11.785534 -13.642279 -14.949697][-9.8366623 -12.149088 -12.873119 -9.78273 -6.054441 -2.4821804 -0.32264328 -1.8196638 -5.1364951 -6.4543819 -7.4092226 -9.9916382 -12.71347 -14.787159 -15.886875][-10.191128 -12.463279 -12.248541 -7.5499268 -2.8215389 2.6960311 6.7411313 5.2812943 2.3484082 -1.9624767 -5.5748324 -7.8954124 -10.727132 -13.256097 -13.73712][-9.1304874 -10.73221 -9.70186 -4.8135848 -0.012526035 6.3607669 10.658751 9.9749393 7.5941205 1.8289175 -3.3807752 -6.3957334 -9.158165 -11.219198 -11.410306][-7.2128086 -8.5025005 -6.874896 -2.9217255 0.98325253 7.4780498 11.575388 10.642288 9.266304 3.5957837 -0.97814989 -4.6929502 -9.168313 -10.261692 -10.003811][-4.9409676 -6.6259222 -6.6371808 -4.3353796 -1.3451648 3.0939097 6.110558 5.9257751 5.4681439 2.2900043 -0.57524443 -5.4106374 -9.9633522 -11.232271 -12.17071][-4.83525 -5.4908276 -6.1052318 -4.883234 -3.7886417 -2.2023644 -0.14361858 0.25586891 -0.33381319 -2.5618396 -3.9699953 -8.6522827 -13.09717 -14.790066 -15.394842][-7.2040734 -8.7070007 -9.34651 -8.5430679 -7.9742384 -7.6042027 -7.527987 -7.7209687 -7.6122832 -8.0638828 -8.75102 -13.156887 -15.51498 -16.450884 -16.31085][-10.427485 -11.741653 -12.486429 -11.925182 -12.153566 -12.413367 -12.557443 -13.459589 -13.489579 -12.64811 -12.802964 -14.285101 -14.798574 -16.010792 -16.457954][-12.238066 -11.474058 -12.757135 -14.202927 -14.693596 -13.606151 -13.434006 -14.072805 -14.90708 -14.028139 -12.353399 -12.832624 -14.009527 -13.519949 -12.55702][-14.944023 -14.283215 -13.04623 -12.62244 -12.719484 -12.988197 -12.98436 -11.905265 -11.12504 -10.905455 -11.000046 -10.552804 -10.162424 -10.588634 -9.7467766][-12.48699 -11.951033 -10.640042 -10.410525 -9.9410458 -10.079706 -9.5689173 -8.70934 -8.5585251 -7.62274 -7.6121054 -8.58757 -9.3544159 -9.1378193 -8.4420509]]...]
INFO - root - 2017-12-15 17:26:03.615019: step 38710, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 53h:51m:20s remains)
INFO - root - 2017-12-15 17:26:10.112240: step 38720, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 53h:54m:04s remains)
INFO - root - 2017-12-15 17:26:16.671449: step 38730, loss = 0.22, batch loss = 0.18 (12.4 examples/sec; 0.647 sec/batch; 52h:47m:36s remains)
INFO - root - 2017-12-15 17:26:23.218382: step 38740, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 52h:14m:31s remains)
INFO - root - 2017-12-15 17:26:29.808478: step 38750, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 53h:45m:56s remains)
INFO - root - 2017-12-15 17:26:36.403623: step 38760, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 53h:33m:00s remains)
INFO - root - 2017-12-15 17:26:43.009653: step 38770, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 53h:16m:28s remains)
INFO - root - 2017-12-15 17:26:49.550801: step 38780, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 53h:51m:26s remains)
INFO - root - 2017-12-15 17:26:56.109847: step 38790, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 54h:10m:05s remains)
INFO - root - 2017-12-15 17:27:02.681231: step 38800, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 52h:13m:27s remains)
2017-12-15 17:27:03.190338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6965308 -6.1597538 -8.37268 -8.1410151 -8.2942219 -6.7383509 -5.8715544 -6.095891 -5.9508371 -6.7641935 -7.6697011 -8.8290672 -12.414955 -12.393859 -11.277755][-4.9422784 -5.0752335 -4.693862 -6.1319785 -7.322999 -6.3333373 -4.91539 -4.5148807 -4.7289844 -5.668725 -6.338315 -8.7769651 -11.993239 -13.521057 -13.962179][-4.1240683 -4.1000285 -5.1445861 -4.7639809 -6.2747355 -5.9416008 -5.2743244 -4.539505 -4.8002911 -5.5381789 -6.4159451 -8.4250746 -11.866849 -14.323103 -15.155224][-6.0597968 -5.80064 -5.7214589 -5.9881477 -6.6840076 -5.8162408 -5.4865589 -5.1926036 -4.8902774 -4.509758 -4.7063117 -6.7390938 -10.453629 -12.999308 -13.739019][-6.4840584 -7.2317109 -8.2225647 -7.4017024 -5.8600311 -3.6335628 -2.4818065 -3.7807295 -4.398643 -4.3582354 -4.5404744 -5.57541 -9.4642172 -12.328245 -13.609692][-6.27859 -6.339129 -7.7060947 -6.0651298 -3.7083726 -0.91534948 2.9310193 3.1755366 1.2159252 -0.9446764 -2.534164 -3.0580213 -7.090097 -10.22686 -11.589703][-4.8655357 -5.2530065 -5.6034179 -4.298234 -3.6571088 0.13101721 5.3218694 6.8349013 5.89374 2.2141323 -0.78474283 -2.3495259 -6.9091244 -9.159174 -10.956017][-3.8927722 -2.4901977 -2.908093 -2.3243372 -2.3023639 1.0163765 5.4573236 7.8343806 7.888864 4.740025 0.95329046 -3.3328803 -7.7896328 -9.9256334 -10.659138][-3.1147175 -1.8598301 -0.78974104 0.42431068 -0.0543952 1.1835532 3.1031594 4.2831483 4.2685351 2.0251975 -0.72273588 -3.9196546 -8.6426353 -10.988311 -11.487038][-2.2957089 -1.847568 -0.7973032 0.30087233 0.072327137 0.57010174 1.9354248 1.7695351 -0.18377876 -1.6060867 -3.0405974 -6.0560555 -10.064405 -11.926707 -13.344956][-3.8713903 -4.199017 -4.5040426 -3.5230169 -3.6170857 -3.4867547 -3.5541058 -3.9833343 -5.159791 -6.0557976 -7.4930549 -9.5503426 -10.942753 -12.664419 -13.113038][-8.9401684 -8.4883814 -8.0068674 -7.8460693 -8.0438557 -8.1351871 -7.9486256 -8.9322271 -9.6514263 -9.5587959 -9.7570229 -9.7865181 -10.659199 -12.560802 -12.928387][-12.278926 -12.769705 -11.735327 -10.324186 -10.018448 -10.244736 -11.059724 -10.665844 -9.928957 -10.46253 -10.430558 -9.8464289 -10.50966 -11.456671 -11.377594][-10.331514 -10.909275 -11.39156 -9.4586649 -9.4487839 -9.2270422 -9.72813 -9.5526781 -9.7993612 -9.5441513 -9.1205139 -7.7722139 -8.057745 -9.6268377 -9.6157284][-8.4192238 -7.974493 -8.417182 -7.9261532 -6.7846642 -5.794045 -5.5740862 -6.1213675 -6.5696273 -6.6536093 -7.8165607 -7.2379689 -7.8000031 -7.2869015 -7.7358847]]...]
INFO - root - 2017-12-15 17:27:09.871331: step 38810, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 52h:45m:40s remains)
INFO - root - 2017-12-15 17:27:16.375685: step 38820, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 52h:52m:15s remains)
INFO - root - 2017-12-15 17:27:22.895667: step 38830, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.687 sec/batch; 56h:01m:07s remains)
INFO - root - 2017-12-15 17:27:29.510346: step 38840, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.637 sec/batch; 52h:00m:01s remains)
INFO - root - 2017-12-15 17:27:36.044484: step 38850, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.633 sec/batch; 51h:38m:37s remains)
INFO - root - 2017-12-15 17:27:42.630523: step 38860, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 54h:43m:59s remains)
INFO - root - 2017-12-15 17:27:49.304256: step 38870, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 54h:42m:24s remains)
INFO - root - 2017-12-15 17:27:55.906684: step 38880, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 54h:22m:42s remains)
INFO - root - 2017-12-15 17:28:02.545682: step 38890, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.684 sec/batch; 55h:47m:45s remains)
INFO - root - 2017-12-15 17:28:09.147909: step 38900, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 53h:37m:57s remains)
2017-12-15 17:28:09.659770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2228189 -4.2908878 -4.278266 -4.0600238 -4.2524152 -4.7443976 -4.5587482 -4.8081746 -4.9152112 -5.0129747 -4.7680511 -6.8064651 -9.8142776 -11.127018 -10.721518][-4.0344415 -4.5410156 -4.505403 -4.8769894 -4.7159834 -5.0682087 -4.992806 -5.1751356 -5.7156796 -5.8832088 -5.9691615 -7.5549622 -10.025947 -11.388311 -11.693975][-2.7254622 -3.828311 -5.0709505 -4.9060354 -5.4488754 -5.5897961 -5.3402357 -5.8154564 -6.1485085 -6.34388 -6.0218062 -7.7905655 -10.787389 -11.801853 -11.932844][-4.0185976 -5.6455612 -6.7212067 -6.781106 -6.707644 -5.1009674 -4.51597 -4.7581706 -4.9515796 -5.3345385 -5.8196974 -7.5600982 -10.409956 -11.274055 -10.992754][-6.1497273 -8.4804325 -9.4091988 -8.6317329 -7.3383985 -3.8855247 -2.1038854 -2.6108897 -2.8342483 -3.1430056 -3.6420677 -5.7011161 -8.775692 -9.9674768 -9.6246052][-8.2817869 -10.117057 -10.280504 -8.4910364 -5.4983034 -0.70470381 1.9330521 1.6669521 0.9379425 -0.46360111 -0.87273026 -2.2610028 -5.2650347 -6.6931539 -6.8778772][-7.4973264 -8.6050358 -7.69228 -5.313354 -2.7007959 0.66087055 3.8658967 4.9810176 5.3604856 2.380115 0.41218853 -0.66135311 -3.1344869 -4.8075533 -5.3935771][-6.0513506 -6.6090217 -6.2431765 -3.1366251 0.38952494 3.2582412 5.2178454 5.9079146 5.9450555 4.3514571 2.5246329 -0.28521156 -4.3077197 -5.1971464 -5.8517051][-3.3941369 -4.285306 -4.9314256 -3.22862 -1.5113416 1.5445504 4.5978761 5.0120177 5.3953357 3.7331843 1.962234 -1.260242 -5.0920277 -7.0696974 -7.9391122][-2.4845974 -2.8151131 -3.7688911 -3.2467585 -2.7690294 -0.84188557 1.3243051 2.05581 2.02452 1.364655 0.52629519 -2.5911529 -6.9932356 -8.9736023 -10.830664][-7.5166049 -7.6828804 -8.1967916 -7.2474165 -7.117825 -6.4399686 -5.4510345 -4.254827 -3.8956804 -4.9068208 -5.830091 -7.6213775 -10.626586 -11.81052 -12.354514][-10.977705 -11.071925 -10.350374 -9.5516109 -8.4769258 -7.8886995 -8.2196732 -8.7119331 -9.3484612 -9.25211 -9.4991665 -11.54357 -12.47159 -13.094902 -13.221842][-13.333317 -12.479607 -10.994473 -10.067332 -9.827672 -8.8833418 -9.0581818 -9.9258642 -10.866264 -11.581488 -11.643494 -12.073124 -12.234251 -11.769578 -10.981174][-11.724474 -11.268747 -9.6907177 -8.8304405 -7.1428552 -6.9158468 -7.7264972 -8.3481789 -9.1177731 -9.8761206 -9.3857937 -9.824234 -10.04908 -10.018005 -10.474192][-8.1842794 -7.6631956 -6.2978244 -5.224226 -4.7355905 -4.9448657 -5.4201479 -5.2104187 -5.7169094 -6.1482148 -7.3198729 -7.9459071 -8.3540745 -8.7992973 -8.6547165]]...]
INFO - root - 2017-12-15 17:28:16.275077: step 38910, loss = 0.21, batch loss = 0.17 (12.4 examples/sec; 0.646 sec/batch; 52h:42m:20s remains)
INFO - root - 2017-12-15 17:28:22.829346: step 38920, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 54h:56m:40s remains)
INFO - root - 2017-12-15 17:28:29.455724: step 38930, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 53h:00m:08s remains)
INFO - root - 2017-12-15 17:28:36.049621: step 38940, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 54h:42m:12s remains)
INFO - root - 2017-12-15 17:28:42.713507: step 38950, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.667 sec/batch; 54h:22m:20s remains)
INFO - root - 2017-12-15 17:28:49.442903: step 38960, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.680 sec/batch; 55h:28m:02s remains)
INFO - root - 2017-12-15 17:28:56.034346: step 38970, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 54h:05m:01s remains)
INFO - root - 2017-12-15 17:29:02.626123: step 38980, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.657 sec/batch; 53h:34m:09s remains)
INFO - root - 2017-12-15 17:29:09.211981: step 38990, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 53h:00m:36s remains)
INFO - root - 2017-12-15 17:29:15.770972: step 39000, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 51h:52m:04s remains)
2017-12-15 17:29:16.323428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.302474 -6.6251159 -6.4412332 -4.8234987 -4.5187769 -5.765121 -6.8171315 -7.5829811 -7.4811983 -6.5079889 -5.2486162 -6.9803996 -8.7554045 -8.0335941 -6.7142305][-6.5915222 -5.1326804 -4.7399774 -2.888262 -3.4516795 -5.070776 -6.1940269 -6.6560698 -6.5275388 -5.7561097 -4.8489137 -6.3640027 -7.8403978 -7.903913 -6.8566017][-4.6519156 -4.074851 -3.9441242 -1.8361697 -2.7348328 -3.7773056 -4.9282618 -5.20142 -5.269269 -5.0050235 -4.6164861 -6.4745669 -8.2715034 -7.9776592 -6.8443041][-6.0359707 -5.311533 -4.3658695 -2.3195341 -2.8625441 -3.0760634 -3.8124864 -4.0705409 -4.8549156 -4.7774887 -4.2998953 -6.5149422 -8.7243633 -8.5466032 -7.0517235][-6.2636118 -6.3245549 -6.3479309 -3.4316564 -2.9016068 -1.5670648 -1.3807197 -2.2180071 -3.2160327 -4.0326414 -4.7499204 -6.6875176 -7.7953148 -7.768199 -6.7114997][-8.2731133 -7.7083817 -6.7048364 -3.1643407 -1.1967301 0.80880976 1.3259916 1.0197306 -0.40884113 -1.6136551 -2.4557459 -5.2772827 -7.5156612 -7.0768538 -5.653336][-10.330456 -9.075798 -7.4758153 -2.9027083 0.26808929 3.0731759 4.5183845 3.8173919 2.0476718 1.0309467 -0.29007673 -3.8518364 -6.764617 -6.707952 -6.01736][-9.9829693 -8.4913788 -6.6515627 -2.2236125 1.0542808 4.0931878 6.0723443 5.1833377 3.2656951 1.6248603 -1.011694 -4.5993552 -7.1026034 -7.1948938 -6.4447126][-9.0862427 -7.545886 -5.5326443 -2.7900121 -0.66288376 2.4986033 4.5716968 3.4959369 1.8755074 -0.22842264 -2.7693691 -6.0799141 -8.8227587 -9.1771135 -8.266737][-8.4138937 -7.1461911 -5.8269877 -3.4013052 -1.7374842 -0.17744541 0.49692535 -0.29222631 -1.72438 -2.6548142 -3.7122972 -7.422173 -10.553762 -10.899256 -9.8746614][-10.21591 -8.7929268 -8.3137245 -6.6978812 -5.4610491 -4.1874628 -3.1931252 -3.8953733 -5.3052864 -5.9743805 -6.7608018 -9.6367159 -12.073027 -12.248928 -11.041149][-12.57126 -11.573721 -11.004684 -9.5856867 -8.9434242 -7.9938197 -7.392746 -7.814579 -8.14831 -8.60113 -9.5928745 -10.806209 -11.894932 -11.788541 -10.39963][-13.950462 -12.606263 -11.263588 -10.487576 -10.125874 -9.9421024 -10.282615 -10.126686 -9.8387165 -9.9089584 -10.174602 -10.506804 -11.161615 -10.026392 -8.1571436][-10.670956 -9.8597183 -8.435091 -7.2960863 -6.5818977 -7.3205695 -8.5865278 -8.0287428 -8.33438 -8.1794252 -7.5143161 -7.2199984 -8.0892954 -7.4100051 -5.8373337][-6.2039967 -5.2845225 -4.3233018 -3.3167007 -3.0881014 -3.7680166 -3.9916515 -4.2979541 -4.369709 -4.1862373 -4.3361654 -4.917769 -5.1584573 -5.6131744 -5.8422918]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 17:29:23.014592: step 39010, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 54h:00m:31s remains)
INFO - root - 2017-12-15 17:29:29.551187: step 39020, loss = 0.23, batch loss = 0.18 (12.4 examples/sec; 0.644 sec/batch; 52h:29m:02s remains)
INFO - root - 2017-12-15 17:29:36.171225: step 39030, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.693 sec/batch; 56h:28m:51s remains)
INFO - root - 2017-12-15 17:29:42.774418: step 39040, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 53h:27m:19s remains)
INFO - root - 2017-12-15 17:29:49.394023: step 39050, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.671 sec/batch; 54h:40m:37s remains)
INFO - root - 2017-12-15 17:29:55.969720: step 39060, loss = 0.22, batch loss = 0.18 (12.4 examples/sec; 0.646 sec/batch; 52h:38m:37s remains)
INFO - root - 2017-12-15 17:30:02.586352: step 39070, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 53h:11m:04s remains)
INFO - root - 2017-12-15 17:30:09.195860: step 39080, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 54h:18m:13s remains)
INFO - root - 2017-12-15 17:30:15.817747: step 39090, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 53h:25m:16s remains)
INFO - root - 2017-12-15 17:30:22.502871: step 39100, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.681 sec/batch; 55h:28m:00s remains)
2017-12-15 17:30:23.037914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.99588251 -2.7148046 -4.8880458 -6.3390789 -7.3449349 -8.13901 -8.1586809 -7.5166979 -7.2041931 -7.6712728 -7.3085108 -8.987093 -9.8090343 -9.814333 -9.8051634][-3.3039134 -5.1497035 -7.8349018 -9.4016047 -10.263079 -10.160307 -8.7861834 -8.3553381 -9.0014219 -8.9770384 -8.631218 -10.016613 -9.9555931 -10.938091 -10.423281][-3.8227789 -6.1562562 -9.10496 -10.775043 -11.446532 -11.370398 -9.9860554 -8.7997179 -8.415597 -8.6535778 -8.6617222 -10.238586 -10.853517 -10.989653 -10.464783][-4.5985012 -6.5582433 -8.854887 -10.502447 -11.78832 -9.9291611 -7.0161152 -7.1119709 -7.6227131 -6.6001496 -5.4814806 -7.5863285 -8.9805622 -10.434781 -11.500046][-5.4899125 -8.4924049 -11.499371 -11.44475 -11.552061 -8.7684622 -4.4451694 -4.97972 -5.6947494 -4.7498736 -4.3275132 -5.5461969 -5.7404795 -8.4102516 -10.291294][-6.4665737 -9.1071053 -11.280033 -10.670321 -8.567318 -4.6357403 -0.49469614 0.8567214 1.2888618 -1.1650162 -3.275655 -4.0389376 -4.9346919 -6.9903913 -7.62779][-5.8483038 -7.3241749 -8.5672474 -6.4295774 -3.5669837 0.001294136 4.3455415 6.5217566 7.328095 3.0960679 -0.6113162 -2.1286502 -2.5804136 -4.6784687 -7.3627858][-5.0360017 -5.6977463 -6.0960512 -3.7614079 -0.80328035 3.5771251 7.7691293 8.864851 8.7553253 5.2484269 2.3273549 -1.7835891 -4.0930252 -6.0771575 -7.7858019][-2.27706 -3.2696292 -5.1887493 -4.1497703 -1.9569607 1.0091429 4.7489839 6.0963235 7.0065522 4.1959653 1.3884392 -3.5672019 -7.1503305 -9.3531837 -10.625139][-0.61421824 -2.3328109 -5.8176231 -5.45288 -4.8146391 -2.6705198 0.21125984 1.6628947 2.5745387 2.190094 1.7131858 -4.0536442 -8.0164375 -11.461926 -14.358025][-2.5332582 -3.7525988 -6.7429705 -6.9126005 -7.271091 -6.585566 -5.1760683 -3.6423442 -2.7259867 -1.7960603 -1.8827727 -6.0615931 -8.733284 -11.851454 -14.084154][-8.5558033 -8.4490356 -9.6179266 -10.227449 -12.171182 -11.591495 -10.618744 -11.185859 -10.553074 -8.42677 -8.0365877 -10.860546 -11.034122 -11.74827 -12.899291][-10.755886 -10.470673 -10.911045 -11.333063 -14.184511 -14.907228 -14.977915 -14.034723 -13.356375 -13.104275 -12.768464 -13.060881 -11.607984 -11.359851 -10.844922][-10.371842 -8.9477444 -9.3580112 -11.087844 -12.238417 -13.791899 -14.849701 -14.225756 -13.634132 -12.756237 -12.627069 -12.224921 -11.044523 -10.586947 -9.9303551][-7.028873 -8.193903 -8.2430544 -7.1983132 -7.0114508 -8.2653866 -8.9317446 -9.1512117 -10.173983 -9.88159 -9.3093014 -10.164165 -10.782961 -10.720388 -10.072729]]...]
INFO - root - 2017-12-15 17:30:29.597190: step 39110, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 53h:10m:21s remains)
INFO - root - 2017-12-15 17:30:36.198627: step 39120, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 55h:10m:35s remains)
INFO - root - 2017-12-15 17:30:42.862165: step 39130, loss = 0.13, batch loss = 0.08 (11.5 examples/sec; 0.698 sec/batch; 56h:52m:29s remains)
INFO - root - 2017-12-15 17:30:49.461187: step 39140, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 52h:03m:41s remains)
INFO - root - 2017-12-15 17:30:56.024461: step 39150, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 53h:15m:30s remains)
INFO - root - 2017-12-15 17:31:02.597326: step 39160, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.693 sec/batch; 56h:27m:50s remains)
INFO - root - 2017-12-15 17:31:09.184074: step 39170, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 52h:40m:48s remains)
INFO - root - 2017-12-15 17:31:15.698407: step 39180, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 53h:59m:33s remains)
INFO - root - 2017-12-15 17:31:22.211776: step 39190, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 52h:42m:08s remains)
INFO - root - 2017-12-15 17:31:28.857433: step 39200, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 53h:23m:02s remains)
2017-12-15 17:31:29.391655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.6421223 -9.1676369 -10.131913 -10.360439 -10.628201 -10.524784 -9.0572748 -7.414484 -6.4957604 -6.8881478 -7.0773354 -9.27089 -13.066875 -13.594083 -12.754749][-8.757638 -8.3684206 -7.3199148 -7.5004587 -8.4608641 -8.6230736 -7.9421997 -7.3284221 -6.8734455 -6.136848 -5.7433634 -8.7437057 -12.105083 -13.560629 -13.8923][-5.2818027 -6.6011176 -7.9169822 -7.4439654 -8.2367144 -8.9554787 -8.48083 -7.4778032 -6.87755 -6.523912 -6.0586972 -8.3771553 -12.283741 -13.865421 -14.088543][-4.1969976 -5.6952062 -7.1632004 -8.2454834 -8.9673109 -7.9747486 -7.0106177 -7.1205568 -6.8610969 -5.5257154 -5.1757026 -8.4693165 -11.903498 -13.555895 -14.842596][-6.9926505 -7.8271308 -9.8063431 -9.3265209 -8.1232471 -5.1675725 -2.4384661 -3.5534568 -6.4019971 -5.8553896 -5.3338714 -7.8137088 -12.011428 -14.554569 -15.503723][-7.1943889 -7.5392056 -9.5862169 -8.9350395 -5.2615619 -1.0017924 3.102159 3.4963393 0.78272629 -2.4547503 -5.8739853 -8.2662773 -10.921038 -12.805273 -14.143808][-9.424962 -8.2742605 -7.3051667 -5.4737387 -3.3635623 1.2853613 7.056294 7.8391595 5.6767812 1.1118283 -4.4318094 -8.1194944 -12.348116 -14.048035 -14.156485][-9.4817295 -8.4435863 -7.5851679 -3.7847533 -1.6441932 2.5734496 7.8778491 7.6079278 6.2690873 2.587728 -2.9114795 -7.1644764 -12.284908 -13.707718 -13.527746][-6.0615687 -4.66338 -5.2499142 -2.9830976 -0.99753618 2.0316076 5.8996148 5.9567752 5.1234841 0.77668667 -4.5068054 -8.9004784 -14.112276 -14.024446 -12.849438][-5.4504342 -3.9227073 -3.2088056 -2.0679998 -1.0416918 0.53226805 3.3259444 3.6586547 1.9803929 -2.0856853 -6.1451583 -9.7797966 -14.47879 -15.222256 -14.98378][-10.751081 -8.7325077 -7.9728785 -6.635613 -6.218195 -5.8825893 -4.0333929 -3.7466135 -4.6087828 -5.7290411 -8.3515759 -12.783925 -15.46697 -14.849218 -14.667017][-14.538122 -14.370338 -13.973818 -12.284047 -11.672775 -11.518476 -11.005381 -11.372416 -10.845959 -10.278725 -11.617737 -13.185339 -14.71924 -15.538147 -14.969175][-16.872561 -17.10848 -16.588675 -15.190254 -15.119406 -14.160055 -13.764267 -14.627926 -14.927034 -14.303228 -13.869671 -13.806023 -13.793989 -13.667951 -13.362955][-14.055103 -13.169199 -13.986406 -13.869486 -13.556662 -13.914639 -13.102434 -12.200112 -12.545488 -12.675769 -12.81323 -12.354313 -12.839465 -12.305484 -11.07416][-10.494744 -10.019857 -9.5004387 -9.6090069 -9.5990515 -9.7971659 -9.4295893 -9.3221493 -9.8845406 -9.5818043 -9.94125 -10.760874 -12.071672 -11.122989 -11.540752]]...]
INFO - root - 2017-12-15 17:31:35.924831: step 39210, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 54h:07m:07s remains)
INFO - root - 2017-12-15 17:31:42.528781: step 39220, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 52h:36m:53s remains)
INFO - root - 2017-12-15 17:31:49.055336: step 39230, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 52h:19m:26s remains)
INFO - root - 2017-12-15 17:31:55.768104: step 39240, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 54h:12m:19s remains)
INFO - root - 2017-12-15 17:32:02.316890: step 39250, loss = 0.16, batch loss = 0.11 (12.7 examples/sec; 0.632 sec/batch; 51h:29m:50s remains)
INFO - root - 2017-12-15 17:32:08.909713: step 39260, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 53h:43m:44s remains)
INFO - root - 2017-12-15 17:32:15.420422: step 39270, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 53h:16m:23s remains)
INFO - root - 2017-12-15 17:32:21.963550: step 39280, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 52h:09m:57s remains)
INFO - root - 2017-12-15 17:32:28.572850: step 39290, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 54h:03m:06s remains)
INFO - root - 2017-12-15 17:32:35.123023: step 39300, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 53h:11m:42s remains)
2017-12-15 17:32:35.615444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1384068 -2.6830027 -2.7169907 -2.8803804 -4.0979376 -4.7427239 -4.8997068 -5.0291581 -4.9414029 -3.9507399 -3.2189329 -4.9306593 -8.1222725 -9.06908 -8.858861][-2.6057343 -2.9411054 -3.0642452 -3.2029943 -4.1891723 -5.1262646 -6.4969234 -7.2404585 -7.2927456 -6.3982649 -5.2286038 -5.8107729 -8.3971634 -9.610836 -8.941906][-1.1938739 -1.6252565 -2.2132745 -2.6988311 -3.7259257 -3.9905248 -5.2079854 -6.5296774 -6.9682045 -6.4446707 -5.647779 -6.243032 -8.079649 -8.5066319 -7.3428578][-1.6922793 -1.7647557 -2.1975763 -2.529083 -2.9430394 -2.8679304 -3.2267408 -4.8362813 -6.2374334 -6.3471909 -5.6739631 -6.3685822 -7.9549665 -7.7864847 -6.962039][-2.2047141 -2.7157876 -2.9096017 -1.9874349 -0.9559536 -0.10461378 0.006714344 -1.6112518 -3.6567914 -4.7577305 -5.3918004 -5.8529148 -6.7519665 -7.000865 -6.6621456][-4.8140054 -4.5463457 -3.9196513 -2.1486506 -0.14772129 2.2656069 3.408648 2.2847743 0.15380192 -1.632513 -3.021158 -4.1792088 -5.258709 -4.3961511 -3.8395233][-7.0371418 -6.638309 -5.5673103 -3.5285268 -0.48517942 3.607533 6.2442679 6.2121758 4.6001678 1.7336173 -0.84675884 -1.7594197 -2.6374826 -2.8062074 -2.902231][-7.882638 -7.8939734 -7.0720325 -4.4968514 -1.0814109 2.7335782 4.965652 5.4442239 4.9315619 2.4637055 0.91938114 -0.3676424 -2.3600614 -2.2509534 -2.8051088][-6.6776557 -7.1578531 -6.963479 -5.1007929 -2.6771975 0.3674922 2.4876609 3.3402085 2.9805684 1.8328371 1.8056488 0.70548391 -1.6023865 -2.5693548 -3.3004348][-5.9097905 -6.6488791 -6.9892168 -4.7703686 -2.8823879 -0.36038685 1.060473 1.3611617 0.84354782 0.71320677 1.4099917 0.51619768 -1.0284958 -2.3858166 -5.1261082][-9.0711441 -8.861846 -8.46858 -7.2688141 -5.8549976 -3.2265925 -1.7829173 -1.8965931 -2.5331268 -2.469352 -2.0627918 -2.8323221 -3.0554807 -3.6685238 -5.0827117][-12.258146 -12.035085 -10.947617 -10.054615 -9.0273647 -6.8863468 -5.9442296 -5.6645837 -6.1331687 -6.4986424 -6.1311417 -5.3471422 -5.2439685 -4.7747526 -5.1399632][-11.50081 -10.615287 -9.658824 -9.7798672 -10.027155 -9.2717209 -8.7986927 -7.9704247 -7.4199705 -7.0321732 -6.6845088 -6.726481 -6.28867 -5.140872 -4.4747715][-8.4319611 -8.1535912 -7.9791059 -8.0087128 -8.1143179 -8.3150883 -8.7882309 -8.85639 -8.616806 -7.8486524 -7.1849437 -6.9888988 -6.3289204 -6.38996 -5.9467278][-5.3015471 -5.1845608 -4.5544786 -3.6705594 -3.6042509 -4.0948091 -4.5906992 -5.4912581 -6.2872972 -6.5664377 -6.1034436 -6.8378639 -8.0062456 -8.3355579 -8.3341551]]...]
INFO - root - 2017-12-15 17:32:42.166564: step 39310, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 54h:10m:34s remains)
INFO - root - 2017-12-15 17:32:48.730983: step 39320, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 52h:42m:59s remains)
INFO - root - 2017-12-15 17:32:55.374247: step 39330, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 53h:22m:00s remains)
INFO - root - 2017-12-15 17:33:02.001174: step 39340, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.674 sec/batch; 54h:51m:51s remains)
INFO - root - 2017-12-15 17:33:08.538029: step 39350, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 53h:34m:48s remains)
INFO - root - 2017-12-15 17:33:15.010342: step 39360, loss = 0.14, batch loss = 0.10 (12.8 examples/sec; 0.627 sec/batch; 51h:03m:22s remains)
INFO - root - 2017-12-15 17:33:21.568177: step 39370, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 53h:41m:32s remains)
INFO - root - 2017-12-15 17:33:28.118484: step 39380, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 52h:27m:57s remains)
INFO - root - 2017-12-15 17:33:34.672556: step 39390, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 53h:56m:31s remains)
INFO - root - 2017-12-15 17:33:41.235887: step 39400, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 51h:43m:10s remains)
2017-12-15 17:33:41.762714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5466852 -5.6780419 -5.6660085 -5.7293873 -7.1717734 -8.0440388 -9.1315832 -9.5167284 -9.5123844 -9.6368389 -8.6923037 -8.0169258 -8.7073765 -8.8194332 -6.3309126][-7.0211725 -5.6056895 -5.3596554 -5.7088003 -6.3553076 -6.9428754 -8.0873623 -8.9124374 -9.0320005 -7.952 -6.0414386 -5.4759293 -6.207303 -7.1087484 -5.965765][-4.8307333 -5.376502 -6.6988668 -6.1532011 -6.0930777 -6.2909684 -7.2445803 -7.6182103 -7.5625329 -6.4154768 -5.3097463 -4.6812816 -5.1783867 -6.9817643 -6.5183034][-2.0231144 -2.4100659 -4.3354297 -4.6021118 -5.5023994 -5.1181865 -4.9957018 -6.1362486 -6.6183434 -5.7749777 -4.4699574 -3.4635594 -4.5201416 -6.7168441 -6.1192656][-2.1618609 -2.0989547 -2.1799881 -3.1174483 -4.5688424 -3.0681336 -1.4355116 -2.9515944 -4.3371887 -4.2812285 -3.9919133 -4.0006161 -5.3959546 -7.4506292 -7.5253267][-4.2454023 -2.4403944 -1.1970711 -0.96451187 -0.56475258 1.6805873 3.0785394 2.5500932 1.6045308 -1.0716977 -3.338568 -3.3771825 -4.9287515 -7.3352709 -7.40563][-6.1799397 -4.1450491 -2.20504 -0.82895136 0.74392223 3.0375638 4.9429526 6.3682685 6.0521655 1.2495151 -2.2231638 -3.4340374 -6.0752187 -7.8355956 -6.9735012][-6.2166429 -5.203558 -3.6548877 -1.6156483 0.2062335 1.9815621 4.0283494 5.6729331 5.8764272 2.8632016 -0.45279264 -3.0757234 -5.9331717 -8.0163412 -7.5038018][-5.6114664 -5.2812247 -4.4883218 -2.0558493 -0.19190407 0.16652393 1.3892384 2.0060802 1.2920523 -0.028102398 -1.8323035 -4.5361071 -7.2718821 -9.5545731 -8.8861866][-5.9563942 -5.9954991 -5.2398171 -2.6378136 -1.5750146 -0.78634977 0.35653639 -0.59729481 -2.2443662 -3.8682046 -4.6499829 -6.175869 -9.0873556 -10.968416 -10.48008][-9.9123211 -9.69158 -8.8984146 -7.4145737 -6.9858608 -5.9349017 -5.0698872 -5.9523067 -6.4960456 -7.8316622 -9.34915 -10.287157 -11.809223 -12.772055 -11.707773][-13.700527 -13.426441 -12.848196 -11.003374 -10.049236 -9.9983692 -10.270068 -11.163498 -11.055406 -11.69422 -12.575595 -12.766011 -13.736549 -13.360254 -12.119731][-14.529718 -13.373249 -11.352921 -10.474115 -10.291027 -10.542583 -11.69976 -12.656836 -13.796097 -13.57762 -13.143789 -13.84425 -14.278927 -12.913906 -11.305873][-10.249298 -9.9313 -8.5705481 -7.4374938 -6.8698068 -6.9025869 -8.2390633 -9.5470991 -10.582135 -10.85803 -10.957689 -11.297159 -10.693785 -9.3706827 -7.5272932][-7.5008926 -7.4471469 -7.6986895 -6.5685329 -4.6935234 -5.737299 -6.3338704 -6.2330294 -6.0793424 -5.7740011 -5.6535096 -6.932744 -7.6451373 -7.8830204 -6.8419347]]...]
INFO - root - 2017-12-15 17:33:48.437991: step 39410, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.688 sec/batch; 56h:00m:30s remains)
INFO - root - 2017-12-15 17:33:55.040427: step 39420, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 51h:48m:54s remains)
INFO - root - 2017-12-15 17:34:01.595366: step 39430, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 53h:52m:20s remains)
INFO - root - 2017-12-15 17:34:08.104377: step 39440, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 53h:21m:34s remains)
INFO - root - 2017-12-15 17:34:14.724437: step 39450, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 51h:53m:17s remains)
INFO - root - 2017-12-15 17:34:21.304036: step 39460, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 53h:06m:46s remains)
INFO - root - 2017-12-15 17:34:27.900331: step 39470, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 54h:08m:36s remains)
INFO - root - 2017-12-15 17:34:34.529807: step 39480, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 54h:08m:16s remains)
INFO - root - 2017-12-15 17:34:41.248838: step 39490, loss = 0.22, batch loss = 0.18 (12.1 examples/sec; 0.661 sec/batch; 53h:48m:52s remains)
INFO - root - 2017-12-15 17:34:47.843311: step 39500, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 52h:54m:40s remains)
2017-12-15 17:34:48.371561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.907464 -3.9817512 -2.6757777 -1.1127934 -1.2962875 -1.9279671 -2.0345764 -2.2347753 -2.9937408 -2.6338353 -2.6915276 -4.9560556 -5.744772 -7.8697667 -6.9339232][-4.1743193 -4.4254379 -3.6994717 -2.7189832 -2.3775918 -2.6948254 -3.0050306 -2.824661 -2.8014565 -2.8823981 -3.0576775 -5.053616 -5.7555132 -8.2022724 -7.6072006][-4.4972243 -4.4591408 -4.2639227 -3.6018631 -3.7812519 -3.8710785 -4.0707521 -4.5431218 -3.8716426 -3.4669278 -3.3934734 -5.0724587 -5.4961619 -7.1741209 -6.7516317][-4.8539286 -5.1345711 -4.5621309 -3.9890723 -4.1290607 -4.1470709 -4.0724883 -4.0849433 -3.7217302 -2.9123254 -2.1257913 -4.0674906 -5.0159168 -7.0993567 -6.3446932][-5.2827692 -4.81332 -4.0225329 -2.4320834 -1.676825 -1.7514002 -2.2468035 -2.7799389 -2.6899216 -2.1812737 -1.2751164 -2.3397365 -3.3981924 -6.1885505 -5.948832][-5.1661015 -3.7280629 -2.437988 -0.78815413 -0.33471823 -0.012638092 0.33819342 0.19745588 0.34388304 0.53003073 0.21681166 -1.8593657 -3.1113064 -5.0284371 -4.946486][-5.6624389 -4.0406122 -3.0364685 -1.5899029 0.077095985 2.0743937 2.805572 2.3533597 3.3456264 3.0017772 1.3406148 -0.90150166 -2.6870105 -5.6059217 -5.7114439][-5.5254683 -4.4400349 -2.6374581 -1.6028953 -0.54939318 1.1866946 2.6845756 3.313828 4.7397285 4.8189731 3.2407556 -0.51441765 -3.557385 -6.05574 -5.8922668][-5.7214842 -5.0148497 -4.2532787 -1.5479755 0.15588999 0.61844587 1.3740745 2.3804698 3.8968768 3.892736 4.0219045 0.45268583 -2.3465667 -5.3058648 -5.63693][-6.3707242 -5.6407719 -4.4714279 -2.2287159 -1.2962527 -0.88640356 -0.40341377 1.0921974 2.1552429 2.8036752 3.544311 -0.14441156 -2.5568924 -4.7564092 -4.6853838][-8.6962929 -8.2896414 -7.2505832 -4.8643556 -3.1526086 -2.2802713 -1.8621166 -0.9612174 -0.88456106 -0.31777096 0.18865108 -2.0997853 -3.4560282 -5.3451271 -4.1383677][-11.230537 -10.15458 -8.8221436 -7.3156829 -5.8836727 -3.8672891 -3.4552329 -3.6953123 -3.789628 -2.9021475 -2.6353261 -4.6608515 -5.3721914 -5.3011212 -3.7045584][-10.017608 -9.2176809 -8.3738823 -6.7401128 -6.0097842 -4.2755418 -2.7123396 -3.2910213 -4.387692 -3.738255 -3.2657282 -4.7970538 -4.8816743 -5.0139456 -3.9318314][-7.6990089 -7.1967549 -6.6262956 -5.8534417 -4.5966511 -4.053133 -3.8628001 -2.9801555 -2.8933394 -3.4936292 -3.3224325 -4.3250518 -5.1271262 -6.0479631 -5.7179365][-4.9024749 -5.1223063 -4.4718237 -3.7931433 -3.6270223 -2.9529593 -2.5814974 -2.2676253 -2.2261584 -2.6063328 -3.3565965 -4.372437 -5.4550467 -7.4880052 -7.8006721]]...]
INFO - root - 2017-12-15 17:34:54.987859: step 39510, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 54h:41m:57s remains)
INFO - root - 2017-12-15 17:35:01.605384: step 39520, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 53h:00m:51s remains)
INFO - root - 2017-12-15 17:35:08.250570: step 39530, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 54h:39m:47s remains)
INFO - root - 2017-12-15 17:35:14.851683: step 39540, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.641 sec/batch; 52h:08m:04s remains)
INFO - root - 2017-12-15 17:35:21.430968: step 39550, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 54h:32m:17s remains)
INFO - root - 2017-12-15 17:35:27.985083: step 39560, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 52h:33m:00s remains)
INFO - root - 2017-12-15 17:35:34.470660: step 39570, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 53h:16m:17s remains)
INFO - root - 2017-12-15 17:35:41.110585: step 39580, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 53h:33m:20s remains)
INFO - root - 2017-12-15 17:35:47.689077: step 39590, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 55h:01m:47s remains)
INFO - root - 2017-12-15 17:35:54.427197: step 39600, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 55h:03m:59s remains)
2017-12-15 17:35:54.972067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9847479 -5.2124991 -5.1148791 -4.37603 -4.9335761 -5.3152919 -5.4796324 -5.784245 -6.0743694 -5.9804654 -5.8011212 -8.657505 -9.5506153 -9.512537 -9.1121721][-6.6547313 -6.3671765 -6.9916058 -6.8991942 -7.4207544 -7.8443537 -8.1983776 -8.1976585 -8.1385336 -7.6860609 -6.8622274 -9.5646858 -10.706016 -11.424461 -11.373062][-5.7246861 -6.4980421 -7.6451926 -7.0767527 -7.3346362 -7.4867516 -7.3706274 -7.28996 -7.578084 -7.2109427 -6.6753488 -9.6029816 -11.109197 -12.560153 -13.701494][-5.398006 -5.3572731 -5.7503781 -5.7129779 -6.5850492 -6.6102052 -5.9695215 -5.3720875 -5.1184115 -4.8316455 -5.1743116 -8.6230831 -10.559473 -12.664585 -13.740574][-7.6357746 -8.4371223 -8.3728371 -6.7728105 -6.2040014 -4.0182509 -2.6491249 -2.6337454 -2.6249149 -2.5371616 -3.5376451 -7.7617989 -9.5771532 -11.193445 -12.187484][-10.078322 -9.600976 -8.544013 -7.1254988 -4.8090653 -1.3463044 0.96319771 1.1499419 1.0299816 -0.29695463 -1.9509218 -5.4449883 -8.0049028 -9.56968 -9.5601654][-11.079054 -10.313387 -9.1113024 -5.5163584 -3.0126128 -0.4299469 1.9023285 3.36832 4.1352067 1.719192 -0.48784161 -4.3889842 -5.9787464 -7.3885694 -7.4809232][-9.4832125 -8.8257647 -7.3711 -3.665478 -1.1523376 1.0509524 2.3003755 3.0594544 4.0974078 1.8599977 -0.7187047 -4.9820547 -6.3377504 -6.9925261 -5.7633972][-7.1452847 -5.958385 -5.24572 -2.8040164 -1.5345426 -0.13688707 1.2636781 2.2143207 2.8934426 0.95245743 -1.3940673 -6.029623 -8.2973394 -8.5997314 -7.1759763][-5.5188837 -4.7047892 -3.4507594 -1.0065956 -0.83216095 -0.10365105 1.4737554 2.0565891 1.7983541 -0.50353718 -2.5539408 -6.8338766 -8.2675171 -7.670207 -7.1504164][-7.0470738 -5.9395151 -4.0952792 -2.7053332 -2.2120876 -1.808989 -1.4417691 -1.7988806 -2.25233 -3.0372612 -4.2702618 -8.3431168 -9.2290192 -8.6782837 -7.7619019][-12.451645 -10.349652 -8.5087433 -6.7750921 -5.0728612 -3.854835 -3.4898026 -4.5526605 -5.2865481 -5.889781 -6.8403974 -8.480258 -8.6390228 -7.1888971 -6.1237335][-14.975868 -12.489834 -10.358276 -8.4714575 -7.6948223 -6.0455484 -4.9891233 -5.2298374 -5.5063529 -6.4752822 -7.3079863 -7.8192072 -6.8019152 -5.1676741 -4.5562572][-11.66604 -10.637316 -9.1850262 -7.8715277 -6.8717566 -6.1329951 -5.6086688 -4.8324523 -4.4605474 -5.7086859 -6.9070573 -7.179894 -6.2346029 -4.3037872 -3.0583556][-8.2592812 -7.4462166 -5.8272161 -4.289835 -2.9426785 -2.6309946 -3.1806829 -3.8366921 -4.4658861 -5.1779509 -5.5644336 -6.5785851 -6.4544625 -5.803606 -5.1833553]]...]
INFO - root - 2017-12-15 17:36:01.566181: step 39610, loss = 0.11, batch loss = 0.07 (12.5 examples/sec; 0.638 sec/batch; 51h:52m:11s remains)
INFO - root - 2017-12-15 17:36:08.139777: step 39620, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 53h:10m:52s remains)
INFO - root - 2017-12-15 17:36:14.795476: step 39630, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.689 sec/batch; 56h:01m:57s remains)
INFO - root - 2017-12-15 17:36:21.382843: step 39640, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 53h:15m:13s remains)
INFO - root - 2017-12-15 17:36:28.001834: step 39650, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 54h:10m:44s remains)
INFO - root - 2017-12-15 17:36:34.539900: step 39660, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 52h:48m:34s remains)
INFO - root - 2017-12-15 17:36:41.124243: step 39670, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 52h:59m:42s remains)
INFO - root - 2017-12-15 17:36:47.708201: step 39680, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 54h:19m:19s remains)
INFO - root - 2017-12-15 17:36:54.326250: step 39690, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 52h:10m:40s remains)
INFO - root - 2017-12-15 17:37:00.864803: step 39700, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 51h:33m:34s remains)
2017-12-15 17:37:01.431205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4406033 -6.4219255 -6.35559 -6.2524886 -6.761138 -7.3897223 -7.83257 -7.4599133 -7.5340409 -7.049036 -6.5162735 -8.8226681 -11.565489 -12.337562 -13.090117][-6.1932912 -7.6137295 -7.700057 -7.1637511 -6.9001184 -7.2527905 -7.949791 -8.8696947 -9.3576765 -8.3013973 -7.7231889 -10.452396 -12.701897 -14.420877 -15.320696][-5.08868 -7.2011991 -8.5517416 -8.1791372 -7.6613846 -7.9523697 -8.7315931 -8.9139805 -9.0116091 -9.1944 -9.0626783 -10.961784 -13.054762 -14.511944 -16.18815][-7.453618 -8.9851055 -9.6024485 -8.9182177 -8.3226938 -7.1771116 -6.9511361 -8.1208563 -9.1374083 -8.2823534 -7.3481975 -10.493891 -13.129622 -14.315304 -15.453424][-9.4648037 -12.108444 -12.875564 -10.630945 -7.8738422 -4.2827339 -2.1111293 -3.3486028 -5.4277267 -6.1804042 -6.8867493 -8.7873516 -10.678583 -12.665725 -14.807259][-11.627144 -13.281326 -12.159592 -9.621707 -5.7854071 -0.83695126 2.6106191 2.3719277 1.5194016 -1.0270972 -4.3395429 -6.9571347 -9.3293619 -10.481517 -11.883054][-11.320561 -12.581343 -10.866501 -7.2631969 -2.9129026 2.2999978 6.2615895 6.7899642 5.99871 1.9677005 -2.0403087 -5.5985579 -9.54129 -10.642115 -11.531839][-11.330403 -11.638109 -10.122928 -6.4331493 -1.549716 4.1780744 8.4859562 8.2993126 7.8635669 4.46897 -0.12449598 -5.6654744 -10.705862 -12.083557 -13.294102][-9.6233625 -10.377843 -9.7609787 -6.5304689 -1.9248626 2.4252868 5.5354877 6.8122277 6.5098042 2.7972589 -0.87359905 -6.0869751 -11.749059 -13.571884 -14.399511][-8.9643221 -8.9990921 -9.01313 -7.0666404 -3.4992092 -0.15743446 3.066134 3.7368102 2.6077666 0.64517212 -2.0648391 -7.3335185 -12.437191 -15.052364 -16.55508][-11.308149 -11.725956 -11.712868 -9.7094212 -7.7922511 -5.3305154 -2.2462313 -2.2652478 -3.0776029 -4.3618183 -6.883009 -11.706762 -14.387341 -15.292675 -16.135744][-15.449089 -15.07012 -13.629186 -12.76149 -11.804111 -9.4202309 -8.5827627 -9.23976 -9.0755148 -9.6602182 -11.177513 -13.56416 -14.942892 -15.851456 -15.84623][-16.043879 -15.087208 -13.76375 -13.112635 -12.320246 -11.676699 -11.086415 -10.732166 -10.722746 -10.827953 -11.437181 -13.013777 -13.636217 -12.338012 -11.809443][-14.097555 -12.718979 -11.188594 -9.7921791 -9.1148539 -9.5488186 -9.9614487 -9.8528 -9.5557051 -9.5169153 -10.520454 -10.314369 -10.37638 -10.349411 -10.068972][-10.986614 -10.552143 -8.2726135 -6.2459011 -5.4122834 -5.2473841 -5.3423581 -6.6733103 -7.5769358 -7.9349213 -8.390172 -9.5861111 -10.595814 -9.1473122 -8.6931171]]...]
INFO - root - 2017-12-15 17:37:07.970472: step 39710, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 53h:13m:34s remains)
INFO - root - 2017-12-15 17:37:14.577915: step 39720, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 52h:05m:05s remains)
INFO - root - 2017-12-15 17:37:21.170630: step 39730, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 53h:44m:35s remains)
INFO - root - 2017-12-15 17:37:27.678385: step 39740, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 54h:09m:02s remains)
INFO - root - 2017-12-15 17:37:34.177831: step 39750, loss = 0.12, batch loss = 0.08 (12.7 examples/sec; 0.631 sec/batch; 51h:19m:26s remains)
INFO - root - 2017-12-15 17:37:40.789566: step 39760, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 53h:55m:58s remains)
INFO - root - 2017-12-15 17:37:47.344238: step 39770, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 53h:17m:11s remains)
INFO - root - 2017-12-15 17:37:53.944794: step 39780, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 51h:53m:57s remains)
INFO - root - 2017-12-15 17:38:00.527543: step 39790, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 53h:21m:36s remains)
INFO - root - 2017-12-15 17:38:07.105037: step 39800, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 54h:02m:07s remains)
2017-12-15 17:38:07.620181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5843086 -6.3516197 -6.6249475 -6.4558277 -7.1505413 -7.9625063 -7.7386341 -7.016036 -6.7747736 -7.094739 -6.798418 -6.6725268 -7.8399186 -8.3780394 -8.1874342][-5.7389612 -6.4481478 -7.4326406 -6.9593062 -7.2589593 -7.0378151 -6.1956372 -5.6477633 -6.0958 -5.9178319 -4.4945455 -6.2107215 -8.1407518 -8.3713131 -8.3398829][-5.2387586 -6.0729113 -6.421443 -5.4854355 -6.5949097 -7.4913826 -7.2629724 -6.1833658 -4.71899 -3.9352307 -3.812948 -4.765852 -6.803546 -8.7405138 -8.3193359][-4.3282051 -6.4828491 -6.702868 -6.0511847 -7.4118423 -7.72832 -7.366775 -5.6671796 -4.0739155 -2.480967 -2.6248639 -4.5952907 -5.7250509 -6.293961 -5.7738566][-5.3645773 -5.8888073 -6.3215213 -7.1689243 -7.290123 -5.3944521 -3.7362666 -3.0311174 -2.2163773 -1.2856107 -1.663229 -3.1196361 -4.9220023 -5.7600408 -5.2307148][-6.6072626 -6.4505525 -6.52276 -5.1649647 -4.3357244 -0.50658941 2.1197305 1.8867798 1.6178803 -0.84156609 -2.7476158 -2.688448 -3.9478359 -5.3750954 -4.4831686][-5.7668438 -6.0717072 -5.5000296 -3.574219 -1.0256128 2.6630836 4.5852847 6.2081342 5.0636172 0.53041458 -2.3932683 -3.3640833 -4.5610418 -5.7379832 -5.2292509][-7.0412655 -6.0758004 -5.1596365 -2.6084557 -0.63008165 2.7308545 5.7895951 6.9214921 6.3038259 2.8245292 -0.58895731 -3.6448138 -5.861382 -5.8167806 -5.2412939][-7.0267797 -5.9328384 -4.3204508 -2.4760556 -2.0296433 1.465095 4.0678105 5.193109 5.0185542 2.0189633 -1.0513101 -3.8173518 -7.0298939 -8.3835 -7.4293871][-6.0048466 -6.1430306 -5.0300021 -4.091095 -2.7498517 -0.63513803 1.0895333 2.8697152 2.9075618 0.19743395 -3.4940274 -6.7068911 -9.9447212 -11.316476 -10.677639][-7.2174082 -7.3110714 -7.9656143 -6.3791676 -5.8276367 -4.2393646 -2.1123166 -1.7269664 -1.6780515 -2.5810618 -5.0127282 -8.6763506 -11.900838 -13.224844 -12.279742][-12.247286 -11.445341 -10.801172 -9.9399509 -9.7778893 -8.9844313 -7.95026 -7.38433 -7.3083897 -7.8713446 -8.9190512 -9.2968712 -10.924555 -11.957615 -11.77939][-13.64308 -12.978247 -11.828207 -10.550545 -10.734367 -10.655462 -10.793777 -10.434414 -9.1629782 -9.2615452 -10.14193 -10.5378 -10.769276 -10.483017 -9.4755468][-11.442301 -11.416083 -10.795453 -9.7676754 -9.2713261 -9.6750984 -10.257471 -10.165054 -10.415651 -9.7758932 -8.9353333 -8.1463318 -8.151659 -8.0774107 -7.4596415][-7.3610888 -7.0657825 -6.4360824 -6.787322 -6.9897141 -7.2069268 -7.5109682 -7.9726176 -8.4530144 -8.7624655 -8.6328773 -8.2431555 -7.7382116 -7.2696114 -7.4075785]]...]
INFO - root - 2017-12-15 17:38:14.170047: step 39810, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 53h:21m:22s remains)
INFO - root - 2017-12-15 17:38:20.670869: step 39820, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 54h:36m:18s remains)
INFO - root - 2017-12-15 17:38:27.270827: step 39830, loss = 0.15, batch loss = 0.10 (11.4 examples/sec; 0.702 sec/batch; 57h:02m:05s remains)
INFO - root - 2017-12-15 17:38:33.848245: step 39840, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.686 sec/batch; 55h:44m:08s remains)
INFO - root - 2017-12-15 17:38:40.428492: step 39850, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 53h:25m:05s remains)
INFO - root - 2017-12-15 17:38:46.983132: step 39860, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 51h:48m:17s remains)
INFO - root - 2017-12-15 17:38:53.510466: step 39870, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 53h:04m:06s remains)
INFO - root - 2017-12-15 17:39:00.091676: step 39880, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 52h:37m:33s remains)
INFO - root - 2017-12-15 17:39:06.677426: step 39890, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.653 sec/batch; 53h:05m:47s remains)
INFO - root - 2017-12-15 17:39:13.339331: step 39900, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 52h:28m:53s remains)
2017-12-15 17:39:13.817075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2113457 -5.0852232 -4.8331957 -5.3695 -6.7241631 -7.0656462 -7.3466439 -7.9006085 -8.9517775 -9.1353693 -8.3893833 -7.3874321 -7.8832145 -6.9970355 -5.7911024][-5.8662634 -6.5961709 -6.2106161 -5.6338525 -5.9649458 -7.4272051 -8.2849016 -8.4511509 -9.0545988 -9.8272781 -9.7677784 -8.1334095 -7.8875 -7.6167054 -7.0373397][-3.9489713 -5.62176 -6.8496008 -6.6261787 -6.6623759 -6.7631664 -6.4417109 -7.6078157 -8.2216015 -8.6197338 -9.1071444 -8.068306 -8.2475471 -8.5693188 -8.8441105][-4.2672238 -5.8819027 -6.0265241 -5.9101825 -6.2245789 -5.7573872 -5.3239665 -6.1490388 -7.4036465 -8.0545683 -8.1137314 -7.66599 -9.0504141 -9.7289619 -10.031162][-6.0146303 -6.6221619 -7.09829 -6.4738913 -4.7019458 -1.8962703 0.22245836 -1.7712669 -3.4715285 -4.817379 -6.3906207 -5.1769409 -6.352458 -7.83582 -8.7642889][-7.5484743 -6.8462381 -6.1106997 -5.140481 -2.5070925 0.80412865 4.0867286 4.6797767 3.8681455 -0.19157314 -4.6441174 -4.6836529 -5.9106393 -6.010365 -6.3071856][-6.8629 -6.7363777 -5.946238 -3.4341853 -0.84966421 2.5122361 6.1745067 6.8062816 6.8273549 1.280396 -4.5556469 -4.4342604 -7.5732508 -8.0201616 -6.2861366][-8.3019819 -7.7920313 -7.21076 -4.3536143 -0.76038551 2.9531312 6.8135009 7.9985156 7.4702163 2.796535 -1.3431311 -4.2554178 -8.71555 -8.7814312 -8.4247475][-7.0119948 -7.0829115 -7.22748 -6.4579616 -4.8414383 -0.83033943 3.5277886 5.37448 5.4039159 2.0148768 -1.3809371 -4.5814056 -10.218596 -11.711873 -10.532264][-5.7825322 -5.8958654 -5.9282112 -6.1139169 -6.7435889 -4.8266582 -1.9715116 0.13557625 1.6192994 -0.49730253 -4.3149672 -6.745718 -9.6009741 -11.410702 -12.132812][-9.5775166 -10.353408 -10.342819 -10.429264 -10.108326 -9.2339954 -8.84731 -7.5878277 -6.2707787 -6.4029512 -7.9844728 -10.360675 -13.758419 -13.164385 -11.628865][-13.887684 -14.412203 -14.50321 -13.467175 -13.03453 -13.644113 -14.120209 -13.564293 -12.539598 -11.772824 -11.774514 -12.28287 -12.610835 -13.596708 -13.556492][-12.754267 -13.216026 -12.674579 -12.728689 -13.284409 -12.419787 -12.110167 -13.326843 -12.954526 -11.97064 -11.795175 -11.726536 -10.418737 -9.3425236 -8.9435987][-11.672329 -10.769021 -9.600791 -9.3707581 -9.2457047 -9.631464 -10.436043 -10.173911 -10.372951 -10.830647 -10.564384 -9.321785 -8.133935 -7.3748922 -5.5818396][-8.2541513 -7.7873411 -5.7737117 -4.7599974 -3.7168412 -3.222116 -4.8297963 -6.1659164 -6.1018186 -6.024745 -6.2756157 -6.837965 -6.6528535 -6.3135138 -6.0689259]]...]
INFO - root - 2017-12-15 17:39:20.305147: step 39910, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 52h:33m:28s remains)
INFO - root - 2017-12-15 17:39:26.856270: step 39920, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.639 sec/batch; 51h:54m:01s remains)
INFO - root - 2017-12-15 17:39:33.345947: step 39930, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 52h:46m:50s remains)
INFO - root - 2017-12-15 17:39:39.893340: step 39940, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 53h:01m:39s remains)
INFO - root - 2017-12-15 17:39:46.488762: step 39950, loss = 0.19, batch loss = 0.14 (11.8 examples/sec; 0.676 sec/batch; 54h:53m:42s remains)
INFO - root - 2017-12-15 17:39:53.031898: step 39960, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 54h:55m:50s remains)
INFO - root - 2017-12-15 17:39:59.687397: step 39970, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 54h:07m:34s remains)
INFO - root - 2017-12-15 17:40:06.257765: step 39980, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 53h:20m:43s remains)
INFO - root - 2017-12-15 17:40:12.796633: step 39990, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 52h:34m:46s remains)
INFO - root - 2017-12-15 17:40:19.337646: step 40000, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.635 sec/batch; 51h:37m:33s remains)
2017-12-15 17:40:19.788841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5312662 -6.9438214 -7.7216816 -8.4217644 -9.7618217 -11.287462 -12.404167 -11.035035 -9.9577007 -10.173714 -9.4402218 -9.8598843 -11.718972 -11.308173 -11.991044][-8.5969734 -9.3661356 -9.38958 -10.23699 -10.812311 -11.372976 -12.021654 -12.844822 -13.646852 -12.664806 -11.175781 -11.914492 -13.033297 -12.456797 -12.721235][-5.9605031 -7.9721422 -10.14291 -9.7957983 -9.3237791 -11.449177 -12.527386 -12.114701 -11.85524 -12.615846 -12.653005 -12.208868 -13.099312 -13.293867 -13.576905][-8.2512188 -10.050734 -10.594292 -8.9599113 -9.0616159 -9.5664253 -8.8321075 -11.218277 -12.78904 -11.216297 -9.823988 -12.078176 -14.126516 -12.969481 -12.175842][-9.4922256 -12.442062 -13.155849 -10.829107 -8.1063557 -4.3879023 -2.3314047 -6.8140631 -11.65728 -11.99539 -11.638908 -11.618212 -12.400537 -12.901552 -13.309753][-12.702142 -14.315403 -13.441193 -10.79908 -7.0223413 -1.0761962 4.3267941 1.4739127 -2.1041961 -7.2999115 -12.874441 -11.853715 -11.400825 -11.240475 -12.241283][-16.186756 -15.482992 -13.140966 -8.4415569 -2.8402703 2.358645 6.4275441 6.5383372 6.0436854 -1.4958363 -9.276741 -10.911324 -13.477923 -11.777466 -11.17518][-16.018032 -15.077686 -13.221072 -7.8337197 -0.52656031 6.6007266 10.247833 7.1395135 4.8809543 -0.093195438 -5.6183786 -9.2863321 -12.560755 -12.306217 -13.127199][-12.995817 -12.159623 -12.608145 -8.4649277 -2.2770565 3.7937303 8.5632381 8.9194565 4.9027524 -2.546478 -7.5772238 -10.629204 -14.526381 -13.891523 -13.969877][-11.11419 -11.013895 -10.691332 -6.6277981 -3.4953983 -0.94908524 2.4729886 3.3595119 1.6159554 -2.9960763 -8.4011917 -12.407433 -15.424236 -14.830311 -15.546469][-11.446266 -12.619698 -12.702479 -10.053036 -8.0039444 -5.5010371 -2.7300146 -3.4417992 -4.7298169 -6.0226345 -8.6678028 -13.04504 -15.444012 -15.450041 -14.947582][-16.582243 -16.783358 -16.171492 -15.482428 -14.154509 -11.933119 -10.866047 -11.085829 -11.327736 -12.070946 -13.002733 -13.891823 -14.384237 -14.497852 -14.079897][-16.30765 -14.652672 -14.880651 -16.576551 -15.931217 -14.591518 -13.150354 -13.012993 -13.903257 -13.175393 -12.622112 -14.30382 -14.424295 -12.638062 -11.454618][-13.549374 -12.799872 -11.877711 -10.89516 -11.08507 -12.153097 -12.758619 -11.937765 -11.476989 -11.496576 -12.17617 -12.029074 -11.273105 -9.70459 -9.6899853][-9.7760773 -8.6721487 -6.9713373 -6.387044 -6.0241919 -6.2957964 -6.8983884 -8.1985073 -9.3781414 -8.9310951 -9.2120132 -10.483987 -10.583193 -10.487511 -10.956514]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 17:40:27.605208: step 40010, loss = 0.17, batch loss = 0.12 (11.4 examples/sec; 0.700 sec/batch; 56h:54m:09s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 17:40:34.209986: step 40020, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 54h:44m:59s remains)
INFO - root - 2017-12-15 17:40:40.794036: step 40030, loss = 0.11, batch loss = 0.07 (12.4 examples/sec; 0.643 sec/batch; 52h:14m:56s remains)
INFO - root - 2017-12-15 17:40:47.366377: step 40040, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 51h:46m:00s remains)
INFO - root - 2017-12-15 17:40:53.919615: step 40050, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 52h:41m:51s remains)
INFO - root - 2017-12-15 17:41:00.470177: step 40060, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 53h:18m:50s remains)
INFO - root - 2017-12-15 17:41:07.045224: step 40070, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 54h:54m:25s remains)
INFO - root - 2017-12-15 17:41:13.690555: step 40080, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.694 sec/batch; 56h:23m:15s remains)
INFO - root - 2017-12-15 17:41:20.331945: step 40090, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 54h:31m:22s remains)
INFO - root - 2017-12-15 17:41:26.856130: step 40100, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 53h:17m:48s remains)
2017-12-15 17:41:27.353155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2602129 -6.5494118 -6.034801 -6.5107727 -7.8486013 -9.2016544 -10.288786 -9.996603 -8.7708664 -7.7590103 -7.52367 -10.920112 -13.066555 -12.048071 -9.8858376][-9.3622684 -8.8612394 -7.958868 -8.07762 -9.2350445 -11.464323 -12.891048 -13.418215 -12.981503 -11.391321 -10.490636 -12.867576 -14.392363 -13.642125 -10.742895][-7.3392925 -8.0300369 -8.3178177 -8.5601234 -9.7154188 -12.221878 -13.786406 -13.563012 -12.334887 -12.16581 -12.101665 -14.399748 -15.536911 -14.412584 -12.233549][-7.4346232 -7.931736 -7.9288731 -7.8598328 -9.1040936 -10.082859 -10.865915 -12.780867 -12.731865 -11.019204 -10.095829 -13.95097 -16.053778 -14.834196 -11.791727][-8.6534023 -10.158741 -9.9947462 -8.2316074 -6.9992537 -5.9578543 -5.7482314 -8.1144295 -10.186458 -10.593785 -10.435078 -12.744079 -14.528137 -14.887041 -12.664132][-10.411137 -10.717009 -9.7207251 -6.8472843 -4.5715375 -1.193294 2.7552495 1.0159745 -1.9345279 -6.0450964 -9.7821589 -12.032539 -12.982765 -12.809343 -12.105739][-13.841393 -13.074731 -10.703096 -5.8963237 -1.8554671 2.6584496 7.8417535 7.9699321 5.9225793 0.11660767 -6.6785293 -10.98028 -13.214993 -13.05437 -11.565335][-13.612961 -13.196259 -11.426893 -6.2782836 -1.2043772 4.7360911 9.40703 9.8947926 9.6769733 3.7334428 -3.2429183 -9.4567509 -13.718695 -13.865812 -11.35854][-10.908805 -10.88052 -10.439997 -6.5585556 -2.5218511 2.444098 6.7608294 7.769546 7.7050624 2.4624009 -3.7637892 -10.557871 -15.499201 -14.947477 -12.697277][-7.8094721 -7.7633948 -7.9292269 -5.5079727 -3.9162507 -1.0849872 2.7607865 3.5401855 3.2957358 -0.57789183 -5.1833487 -11.702417 -17.392252 -18.355652 -17.108904][-9.2763338 -8.8907223 -9.06925 -8.3548641 -7.5698156 -6.0104165 -4.5971413 -4.2349176 -4.8211336 -6.5533848 -8.8792152 -14.576147 -17.745419 -17.534929 -15.182011][-15.245779 -13.9055 -13.903326 -13.466111 -13.295594 -12.926762 -12.607073 -12.569748 -12.262761 -13.715498 -14.655069 -16.828791 -17.899643 -16.749046 -14.37166][-15.817251 -15.003592 -14.805382 -15.234699 -15.35989 -14.351929 -13.744186 -14.458391 -15.022081 -14.839941 -14.890644 -16.09314 -15.952471 -13.980272 -10.477192][-12.31169 -11.969667 -11.310383 -10.873186 -12.348133 -13.254642 -13.356091 -12.493803 -12.023266 -12.690701 -13.003163 -12.358785 -11.758283 -10.725029 -9.2295551][-7.9853172 -6.3600349 -5.1172152 -5.3319383 -5.9328785 -6.3002253 -7.6829567 -8.7544937 -9.32117 -8.620532 -8.126771 -9.6317616 -9.8224583 -10.225044 -8.8924952]]...]
INFO - root - 2017-12-15 17:41:33.838238: step 40110, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 53h:24m:13s remains)
INFO - root - 2017-12-15 17:41:40.452069: step 40120, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 52h:26m:48s remains)
INFO - root - 2017-12-15 17:41:47.029764: step 40130, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 52h:16m:58s remains)
INFO - root - 2017-12-15 17:41:53.628602: step 40140, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 54h:04m:17s remains)
INFO - root - 2017-12-15 17:42:00.152011: step 40150, loss = 0.20, batch loss = 0.15 (11.7 examples/sec; 0.683 sec/batch; 55h:28m:53s remains)
INFO - root - 2017-12-15 17:42:06.658685: step 40160, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 53h:17m:30s remains)
INFO - root - 2017-12-15 17:42:13.169462: step 40170, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 53h:09m:54s remains)
INFO - root - 2017-12-15 17:42:19.692754: step 40180, loss = 0.21, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 52h:40m:38s remains)
INFO - root - 2017-12-15 17:42:26.271241: step 40190, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.633 sec/batch; 51h:23m:50s remains)
INFO - root - 2017-12-15 17:42:32.933874: step 40200, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 53h:45m:53s remains)
2017-12-15 17:42:33.502180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.283911 -4.2109265 -4.1313896 -3.8152313 -4.6890421 -5.1335649 -5.34873 -4.9988003 -3.7921555 -2.365612 -1.6045475 -3.6656234 -6.3701253 -8.1162682 -8.3258524][-3.673774 -3.7680397 -3.2812145 -3.3584294 -4.349472 -5.3470793 -5.491261 -4.7113805 -3.8105044 -2.7038486 -1.6311665 -3.2047689 -5.9362788 -7.783638 -7.8987918][-2.7259276 -3.6677079 -4.1239104 -3.8840222 -4.7818866 -5.2008505 -4.9323454 -4.4036751 -3.5909514 -3.2024579 -2.9136569 -4.5706253 -7.0062175 -8.4741611 -8.742444][-2.8927255 -3.5263572 -4.0483274 -4.0605469 -4.5072727 -4.5252838 -4.2742672 -4.2271748 -4.1505694 -3.4971182 -2.5284123 -3.7366233 -5.7224236 -7.13501 -7.83258][-4.054812 -5.1047249 -6.2232032 -4.8135371 -3.9038081 -2.5093744 -1.6329265 -1.959583 -2.3272254 -2.4807825 -2.4503143 -3.3062589 -4.3765974 -5.239212 -5.7545338][-4.1769419 -4.8897824 -4.9925041 -3.9060435 -1.7832868 1.5561681 3.0184751 1.8470759 0.73370075 -0.32262707 -0.83845329 -1.7319188 -3.4083338 -4.2974691 -4.5675569][-3.7778289 -4.2250738 -4.1656575 -2.5000594 -0.80298281 2.502687 5.27018 5.1968141 4.0797276 1.4170542 -0.45467997 -2.0214095 -4.2527456 -4.5854168 -4.5538492][-3.8197498 -3.1434474 -2.4449427 -1.3212185 -0.5603075 2.3056293 4.9524293 5.4264178 4.9781346 2.2995305 -0.36881828 -2.4597149 -4.9047074 -5.3784695 -5.1055207][-2.9220572 -2.6198595 -2.2631645 -0.71752834 0.08634901 1.161768 2.9360404 4.0160308 3.8584275 1.6905894 -0.29353666 -3.7113831 -7.4928675 -7.80982 -6.6590972][-3.5400355 -2.719614 -1.8777988 -0.79893494 -0.40428543 0.59707451 1.8329887 2.3401494 1.8766451 -0.63329554 -2.6321902 -5.8615727 -9.2862778 -10.164066 -9.6328363][-6.2201877 -6.1305089 -4.7906742 -3.4593866 -3.3971462 -2.6717234 -2.2734048 -2.3768029 -2.9493196 -4.3355279 -5.7116652 -9.3557129 -11.981176 -12.101309 -10.980026][-9.9442968 -9.4540138 -8.5166464 -7.1515245 -6.5932064 -5.0716624 -5.2780046 -5.8985839 -6.7159247 -7.8647661 -9.256053 -10.659127 -11.246847 -12.200508 -11.924486][-12.145321 -11.540722 -9.8683662 -8.6306028 -7.7319384 -6.9734931 -7.0181384 -7.74033 -9.1457586 -9.7812986 -10.442865 -11.207219 -11.252873 -10.839503 -10.372959][-9.66853 -9.5153217 -8.7446175 -7.6472406 -6.3103123 -6.2245188 -6.7713852 -7.531868 -8.2517376 -8.84758 -9.3753681 -8.6804085 -8.1529789 -8.18779 -8.7657013][-6.28287 -6.2593474 -5.4523773 -4.5084023 -4.257731 -4.552289 -4.2322564 -4.4137549 -5.4497695 -5.7369547 -6.0515161 -6.47262 -6.4372826 -6.9370875 -9.0506325]]...]
INFO - root - 2017-12-15 17:42:40.084036: step 40210, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 53h:49m:09s remains)
INFO - root - 2017-12-15 17:42:46.651345: step 40220, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 53h:00m:54s remains)
INFO - root - 2017-12-15 17:42:53.252320: step 40230, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 54h:36m:24s remains)
INFO - root - 2017-12-15 17:42:59.923129: step 40240, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 52h:47m:34s remains)
INFO - root - 2017-12-15 17:43:06.475229: step 40250, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 53h:13m:53s remains)
INFO - root - 2017-12-15 17:43:13.083434: step 40260, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 52h:33m:30s remains)
INFO - root - 2017-12-15 17:43:19.706493: step 40270, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 53h:27m:36s remains)
INFO - root - 2017-12-15 17:43:26.325791: step 40280, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 52h:23m:41s remains)
INFO - root - 2017-12-15 17:43:32.889007: step 40290, loss = 0.20, batch loss = 0.16 (12.0 examples/sec; 0.667 sec/batch; 54h:08m:43s remains)
INFO - root - 2017-12-15 17:43:39.553954: step 40300, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 54h:30m:45s remains)
2017-12-15 17:43:40.063067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.2120094 -8.8329344 -8.2997513 -7.0666447 -6.7884541 -6.8219404 -7.2265291 -7.3417449 -7.1364989 -7.5282049 -8.0948277 -7.9177151 -7.70693 -6.6641908 -3.2283854][-8.1135759 -7.2916651 -5.4733615 -4.059917 -4.3205929 -4.9688048 -5.7389851 -5.684453 -5.772265 -6.1814833 -6.401474 -5.9519081 -5.8685384 -5.4421215 -2.3948021][-5.0089493 -5.8788972 -5.4599819 -3.2585757 -2.9436727 -3.0893788 -2.8279412 -2.3802645 -3.0923772 -4.699523 -5.6983128 -5.0479846 -5.041698 -5.5253863 -2.9827445][-4.6337829 -5.9064536 -6.1076794 -4.9350681 -5.1886892 -4.5827484 -4.121942 -3.9003701 -4.348979 -5.064918 -5.7373443 -5.6308117 -5.40263 -5.6258945 -3.0889292][-7.1488776 -7.0685177 -6.459393 -6.110518 -6.2859974 -3.3931384 -1.4924312 -2.2684987 -4.0414171 -5.0607147 -5.83227 -5.5686579 -5.4328351 -5.888032 -3.7906272][-8.8190508 -7.2986364 -5.6455874 -3.6564524 -1.5412974 1.7196145 4.6412396 4.6454606 3.3195558 0.0019254684 -3.1275818 -3.3202446 -4.1461411 -5.4043961 -4.2331567][-11.195005 -8.728363 -6.1912427 -3.4837561 -1.4901443 2.6201034 7.2911382 7.7373748 6.2759585 1.5554023 -3.0337405 -3.8863571 -5.1349077 -5.7282939 -4.7367172][-10.691454 -9.5838213 -9.1254778 -6.1048651 -2.5080631 1.9281578 6.1509004 7.418282 6.8955245 2.2836585 -1.911278 -3.85254 -6.2476397 -6.8958073 -5.39308][-8.1215792 -7.4825778 -6.9070234 -4.802073 -2.9849973 0.598897 4.4337945 5.4603648 3.8416734 -0.66803885 -4.4324093 -6.04942 -7.9084578 -8.19153 -5.8932118][-6.755199 -6.09139 -6.0204206 -4.2711039 -2.9817295 -0.38480425 1.8802552 2.1405768 0.21118879 -3.0833731 -5.29691 -5.9870481 -7.1435356 -8.6316538 -7.0742521][-8.0618649 -7.364522 -6.7018738 -5.3048196 -4.6403894 -3.0830567 -1.9353938 -2.2401106 -3.5733042 -5.2319822 -6.3913059 -7.0868983 -7.9713669 -9.4476166 -8.042448][-13.695312 -12.824312 -12.271072 -11.249409 -10.256912 -9.4546137 -8.8908949 -8.7914295 -9.53821 -10.544109 -10.912259 -9.737278 -8.97679 -9.4516249 -8.4695606][-13.00283 -12.339891 -12.169567 -11.391348 -11.002628 -10.318628 -9.6418037 -10.072405 -10.671792 -10.277994 -9.4691429 -8.4752979 -7.853857 -8.4062023 -6.7755232][-9.322813 -9.2968264 -9.5411406 -8.6992188 -8.1146154 -7.6730947 -8.0211153 -7.7932677 -7.1743298 -6.9285073 -6.7955713 -4.9179497 -4.3849363 -4.8260117 -4.0451927][-7.5823236 -6.6044321 -5.9508328 -5.7854171 -5.645062 -5.3728566 -5.3662472 -5.0955076 -5.2961493 -4.8584766 -4.3284426 -4.0489407 -4.5015121 -4.9088583 -4.4372325]]...]
INFO - root - 2017-12-15 17:43:46.616852: step 40310, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 52h:32m:34s remains)
INFO - root - 2017-12-15 17:43:53.164262: step 40320, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 52h:18m:23s remains)
INFO - root - 2017-12-15 17:43:59.848591: step 40330, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 53h:31m:48s remains)
INFO - root - 2017-12-15 17:44:06.479062: step 40340, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 52h:58m:10s remains)
INFO - root - 2017-12-15 17:44:13.203930: step 40350, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 55h:09m:40s remains)
INFO - root - 2017-12-15 17:44:19.841152: step 40360, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 53h:40m:51s remains)
INFO - root - 2017-12-15 17:44:26.406043: step 40370, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 52h:27m:21s remains)
INFO - root - 2017-12-15 17:44:32.968565: step 40380, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 53h:07m:28s remains)
INFO - root - 2017-12-15 17:44:39.575921: step 40390, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 54h:38m:38s remains)
INFO - root - 2017-12-15 17:44:46.128570: step 40400, loss = 0.14, batch loss = 0.10 (11.2 examples/sec; 0.712 sec/batch; 57h:46m:04s remains)
2017-12-15 17:44:46.680368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.575161 -5.0019436 -5.0934052 -4.3118658 -4.820025 -5.2743998 -5.9947796 -6.6997137 -6.7298222 -6.6142969 -5.7845731 -7.7533531 -9.830534 -10.176825 -8.312417][-3.5031192 -3.1191618 -3.3636405 -2.3457294 -2.767617 -3.0719438 -3.7669992 -4.2171125 -4.7281494 -5.518537 -4.875051 -6.8066931 -8.3063564 -9.6048746 -9.4038477][-1.5257082 -1.6842108 -2.4235895 -1.2660661 -2.0313892 -2.3560984 -3.0589983 -3.7265368 -4.2030673 -4.9977751 -4.69108 -6.4782271 -8.02773 -9.38509 -10.297111][-1.9716332 -2.66887 -3.0185077 -1.8949587 -2.4017019 -2.2675006 -2.5517273 -3.6175559 -4.26632 -4.2758894 -3.3416021 -5.8493075 -7.7816391 -9.5209618 -10.619109][-2.6618874 -4.3618326 -5.033227 -3.16144 -2.4026506 -1.2065773 -0.87758207 -2.2523737 -3.6129632 -3.4226513 -2.8125944 -4.4210181 -6.7245007 -9.7283506 -10.916792][-4.4094944 -5.1954794 -5.3235888 -3.0911331 -1.1863246 1.0819335 2.6295733 1.4834375 -0.21181726 -1.8574436 -2.2301302 -4.1966062 -6.4472933 -9.5526876 -10.753338][-6.7748938 -7.1135406 -6.31688 -3.2511561 -0.4176631 2.8712134 5.7041593 4.733089 2.7050309 0.80466366 -0.75542688 -4.0115824 -7.1368876 -9.66642 -10.96208][-8.08677 -7.7823744 -6.0704179 -1.7036276 1.0427318 4.6202846 7.15583 6.0705619 4.3083711 2.490273 0.75395918 -3.3847327 -6.9316111 -9.5055428 -10.2913][-7.229218 -7.11143 -5.8681264 -1.8812361 1.146277 3.547894 5.2178779 4.6147065 3.1172357 1.8132467 0.83645821 -3.6140513 -7.2913036 -9.86475 -10.089114][-7.9380894 -7.398798 -6.0064068 -2.7240496 -0.92984581 0.97419167 2.881948 2.5582747 0.92928553 -0.62357378 -1.5201316 -5.001616 -8.6498842 -10.899626 -11.277353][-11.144155 -9.3303652 -7.4299622 -4.6120815 -3.9069085 -2.3168714 -1.0579281 -0.93593216 -1.5352197 -2.9410925 -4.8207803 -8.9974308 -11.43288 -12.456208 -11.865116][-14.679909 -12.744447 -10.817713 -7.800663 -6.1330357 -5.388361 -5.6349311 -5.6917057 -5.8511114 -7.4892073 -9.4131031 -11.225189 -12.224102 -12.370395 -11.259193][-14.407185 -13.577509 -12.018721 -10.136679 -8.8498478 -7.2899017 -7.4219327 -7.6066046 -7.9194841 -8.465291 -9.5156956 -11.158464 -11.709719 -10.273066 -8.3274374][-13.656804 -13.082013 -12.138275 -9.4964638 -8.0121679 -7.0728407 -7.7479553 -7.5564775 -7.1963835 -7.6150451 -8.60032 -8.6832485 -8.06883 -7.2607689 -6.0846996][-11.380562 -11.908032 -10.744339 -9.045742 -7.0520167 -6.2889652 -6.8066974 -7.06154 -6.6802678 -6.0194836 -5.8723426 -6.5808392 -7.76073 -7.4248195 -6.9989347]]...]
INFO - root - 2017-12-15 17:44:53.336652: step 40410, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 54h:07m:16s remains)
INFO - root - 2017-12-15 17:44:59.914370: step 40420, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 52h:07m:04s remains)
INFO - root - 2017-12-15 17:45:06.484782: step 40430, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 53h:28m:48s remains)
INFO - root - 2017-12-15 17:45:13.025436: step 40440, loss = 0.22, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 52h:48m:13s remains)
INFO - root - 2017-12-15 17:45:19.681724: step 40450, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 53h:35m:32s remains)
INFO - root - 2017-12-15 17:45:26.222137: step 40460, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 52h:36m:32s remains)
INFO - root - 2017-12-15 17:45:32.868945: step 40470, loss = 0.11, batch loss = 0.07 (11.7 examples/sec; 0.684 sec/batch; 55h:31m:18s remains)
INFO - root - 2017-12-15 17:45:39.470185: step 40480, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 53h:24m:58s remains)
INFO - root - 2017-12-15 17:45:46.078877: step 40490, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 55h:07m:49s remains)
INFO - root - 2017-12-15 17:45:52.721499: step 40500, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 52h:24m:18s remains)
2017-12-15 17:45:53.346744: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7844229 -5.2977071 -4.831686 -4.0992036 -4.8580294 -4.7916284 -4.2200384 -3.6666858 -3.2212656 -2.9460895 -2.2110724 -3.7874503 -5.68081 -7.1954613 -7.9764085][-3.845679 -4.0915241 -3.3182385 -2.7866766 -3.2197957 -2.8495564 -2.8682883 -2.9301636 -2.8573771 -2.6458127 -2.3482273 -4.090611 -5.579762 -7.6559181 -8.86692][-2.1392517 -2.9937737 -3.57525 -2.7371025 -2.3344152 -2.196857 -2.2671609 -2.1741536 -2.2788982 -2.7043524 -3.3201113 -5.6183391 -7.91914 -10.39366 -10.338211][-4.5839772 -4.8871813 -4.6841025 -4.0130033 -3.2522881 -2.2203484 -2.2282443 -2.0702586 -1.793865 -2.4819567 -2.9526324 -5.2767067 -8.44231 -11.164932 -11.92802][-6.2345014 -7.7010803 -7.1376181 -5.1159825 -3.8274086 -2.0939052 -1.2971244 -1.8224809 -2.7839215 -3.158494 -3.9863193 -6.7675505 -9.54711 -11.808342 -11.682696][-9.8198891 -9.2027454 -6.7228394 -4.218195 -2.4261215 0.59815645 1.5300531 1.0611467 0.381804 -1.3139334 -3.0326321 -5.7513409 -8.34552 -9.8703 -9.7797947][-11.410962 -10.414436 -6.5664248 -2.5648949 -0.59303093 1.5306897 2.8423352 2.838552 2.1500931 -0.47443295 -3.2663143 -6.1264791 -8.6539564 -9.8177738 -9.7790909][-12.549288 -10.288212 -6.1453142 -2.2602413 -0.54927063 1.4587765 2.5646195 2.8597493 3.4073386 0.94814968 -2.0442181 -5.9053054 -9.17824 -10.492423 -10.413872][-9.5200815 -8.1762028 -5.2388344 -2.2199783 -0.68557978 0.92206764 1.5951171 1.2774177 1.1643014 -0.23939991 -1.9801204 -6.1783829 -10.051138 -12.540302 -13.168453][-6.5510731 -5.4353561 -3.81464 -1.6949 -0.57717943 -0.18538094 0.063908577 -0.49447536 -0.95132637 -1.8216741 -2.6967294 -5.9787436 -10.147567 -12.623569 -14.555983][-9.8934727 -8.8083172 -7.7073727 -6.2397285 -5.1226778 -4.541151 -4.0251403 -4.8038883 -5.6169815 -6.1070027 -7.068655 -10.027187 -11.931175 -14.302423 -14.590614][-13.587189 -12.878003 -11.645527 -9.9946947 -8.7417488 -7.9004364 -7.9299331 -8.7901344 -8.6500139 -8.6175966 -9.2046318 -11.631208 -12.980903 -13.563181 -13.633698][-15.356308 -14.85165 -13.424799 -11.344858 -9.9398727 -9.0463552 -9.0007668 -9.6005774 -9.8648291 -9.2271042 -8.9757614 -10.153276 -10.633131 -10.823673 -9.6015835][-12.74442 -12.51071 -12.186859 -10.62408 -8.5467377 -8.0975981 -7.5804763 -7.6259441 -7.1629925 -7.5656266 -7.5904303 -7.9905415 -8.1416826 -7.5932541 -7.2756395][-10.573313 -11.815536 -12.024272 -10.353811 -9.337635 -7.4523897 -4.9033747 -4.2445321 -4.7123785 -4.3267274 -4.7605972 -6.110034 -7.4918566 -7.872961 -7.96039]]...]
INFO - root - 2017-12-15 17:45:59.908944: step 40510, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 54h:09m:09s remains)
INFO - root - 2017-12-15 17:46:06.566530: step 40520, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 55h:25m:09s remains)
INFO - root - 2017-12-15 17:46:13.180619: step 40530, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 55h:57m:40s remains)
INFO - root - 2017-12-15 17:46:19.727492: step 40540, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 52h:51m:34s remains)
INFO - root - 2017-12-15 17:46:26.286530: step 40550, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 53h:04m:34s remains)
INFO - root - 2017-12-15 17:46:32.931774: step 40560, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 54h:32m:34s remains)
INFO - root - 2017-12-15 17:46:39.623795: step 40570, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 52h:45m:33s remains)
INFO - root - 2017-12-15 17:46:46.275566: step 40580, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 53h:43m:54s remains)
INFO - root - 2017-12-15 17:46:52.869983: step 40590, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.630 sec/batch; 51h:03m:08s remains)
INFO - root - 2017-12-15 17:46:59.434160: step 40600, loss = 0.19, batch loss = 0.14 (12.6 examples/sec; 0.633 sec/batch; 51h:20m:23s remains)
2017-12-15 17:46:59.972207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8062973 -5.5603828 -4.272975 -3.3274853 -4.090519 -4.0155969 -4.2194853 -3.80228 -3.0745893 -2.4817023 -1.9445329 -4.2115874 -6.5185084 -6.4414182 -6.1554275][-7.3393078 -6.0168881 -4.5583587 -3.0748565 -3.167079 -3.889225 -4.4941254 -4.1845279 -3.4885554 -2.0860054 -1.3959665 -3.2936285 -5.5099254 -6.1465769 -6.0673904][-5.3265905 -5.5694609 -5.2118187 -3.301096 -2.236006 -2.6556742 -2.9079847 -3.0145738 -3.1036599 -2.5571985 -2.0483432 -2.9631374 -5.0445285 -5.82079 -6.4688883][-5.088346 -4.3393865 -3.868124 -2.3029718 -2.2479162 -1.5042958 -0.84211063 -1.4208803 -1.9467907 -2.024642 -1.9332604 -3.2443798 -5.356143 -5.9156628 -6.4714947][-5.4700928 -5.4218178 -4.2620935 -1.5670629 -0.74067736 0.51818275 0.89977074 0.33568287 -0.17963696 -1.0420475 -1.7915764 -3.3192275 -4.8364167 -5.5033236 -5.81612][-6.5987983 -6.0263634 -4.1675324 -1.1908765 1.0524111 2.652452 3.7419267 3.1830955 2.078702 0.94441938 -0.68939829 -2.6770692 -4.8607593 -4.7813048 -4.3318682][-8.3383121 -7.1423669 -4.6495166 -1.2080402 1.07899 3.9108768 6.0651946 5.5119262 4.3938317 2.134964 0.1802454 -2.2080843 -5.040473 -4.6997118 -3.9819937][-8.4735022 -6.682466 -4.3453517 -0.29948521 2.724853 5.6371112 6.6110253 6.0934272 5.7004209 3.3831639 0.96994305 -2.3606086 -5.4392419 -5.2988391 -5.2100592][-5.5790882 -4.43271 -2.8989172 0.3646965 2.4231982 4.6712089 5.5838552 5.3986878 4.965095 2.7459836 0.63445187 -2.908998 -6.1901259 -6.1658664 -5.9881239][-4.9039989 -2.9421058 -1.1590095 0.79204273 1.7925863 3.0269589 4.2386985 3.7553811 2.4677377 1.2452521 0.46724272 -3.0229573 -6.4269462 -7.2255945 -7.1614718][-7.9548531 -6.6962967 -4.3181319 -2.0327768 -1.3945813 -0.27146387 0.56538105 0.21440458 -0.44710493 -1.9685304 -3.0745139 -6.2981372 -8.1858921 -8.6571054 -8.3732328][-11.249221 -9.6790848 -7.9140072 -6.1605229 -5.5021672 -4.3622141 -3.9598207 -3.7782257 -4.0514655 -5.2993379 -6.7040982 -8.5959835 -9.3345547 -8.8149519 -7.9056015][-11.43329 -9.5112381 -8.4539242 -7.666317 -7.2550416 -6.5242348 -6.2781711 -6.1542115 -6.1586475 -6.5517554 -7.2687378 -8.2084827 -8.9598293 -8.1965008 -6.8822222][-8.8502388 -7.9293365 -6.6960306 -5.78094 -4.9102745 -5.3472123 -5.7147136 -5.5224118 -5.8668408 -6.2968054 -6.6305876 -6.5407128 -6.3391442 -5.7396483 -5.6838317][-6.6602197 -6.4763665 -5.9868684 -5.056592 -4.5895734 -4.2726588 -3.5911896 -3.5431073 -3.4189012 -3.659061 -3.9004977 -4.5644917 -5.188261 -5.602622 -5.7489748]]...]
INFO - root - 2017-12-15 17:47:06.558322: step 40610, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 53h:40m:24s remains)
INFO - root - 2017-12-15 17:47:13.230286: step 40620, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 52h:19m:32s remains)
INFO - root - 2017-12-15 17:47:19.882665: step 40630, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 53h:48m:48s remains)
INFO - root - 2017-12-15 17:47:26.445321: step 40640, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 52h:56m:04s remains)
INFO - root - 2017-12-15 17:47:32.970953: step 40650, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 54h:00m:51s remains)
INFO - root - 2017-12-15 17:47:39.572103: step 40660, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 53h:44m:58s remains)
INFO - root - 2017-12-15 17:47:46.197468: step 40670, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 54h:17m:49s remains)
INFO - root - 2017-12-15 17:47:52.807099: step 40680, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 51h:25m:44s remains)
INFO - root - 2017-12-15 17:47:59.529863: step 40690, loss = 0.15, batch loss = 0.11 (11.2 examples/sec; 0.712 sec/batch; 57h:42m:58s remains)
INFO - root - 2017-12-15 17:48:06.205626: step 40700, loss = 0.16, batch loss = 0.11 (11.2 examples/sec; 0.713 sec/batch; 57h:46m:13s remains)
2017-12-15 17:48:06.749439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7705362 -4.4328656 -4.5503693 -4.16286 -4.8888168 -5.3557296 -5.8238597 -5.46388 -4.7518516 -4.4298921 -4.3195381 -6.8405938 -10.221508 -9.7031879 -9.3153257][-4.8262515 -5.0930014 -5.1603775 -4.7223349 -5.1097913 -5.273056 -5.3194332 -5.2683725 -5.4775491 -5.2944736 -4.8409619 -7.4840274 -10.86585 -10.725668 -10.135199][-3.4018764 -4.84743 -5.8933587 -4.8658686 -4.7741528 -5.0390515 -4.8530183 -4.3385739 -3.872612 -4.0502777 -4.4188175 -7.8268371 -11.155409 -11.422594 -11.348197][-5.6775036 -5.4661326 -4.7784514 -4.0452623 -4.5443392 -4.2740269 -4.1152716 -4.384253 -4.632761 -4.0993795 -3.8853507 -7.6335821 -10.982793 -11.213268 -10.349356][-6.4700742 -7.2098074 -6.8475733 -5.04702 -4.2034 -2.9738817 -2.5908175 -3.8295484 -5.4225459 -5.1015859 -4.9373989 -8.2001295 -11.441887 -11.783154 -10.818165][-7.368124 -7.696969 -6.2280421 -3.6621578 -1.7234733 0.48558998 2.0492215 1.4300833 -0.87260246 -3.5456631 -5.7805061 -8.2707615 -10.990137 -11.607874 -11.347197][-8.5349789 -8.2049408 -6.4606595 -3.3523338 -0.94279575 2.5152907 4.9616971 4.1329904 2.2633452 -0.73941231 -4.1924286 -8.2015762 -11.267347 -11.260607 -11.523827][-8.5046844 -7.3814273 -5.0972047 -2.6466405 0.51969767 4.7774959 7.3447824 6.3555293 4.7368464 1.7209582 -1.2833962 -5.6374207 -9.1526642 -9.7865829 -10.305401][-7.7328081 -6.7197075 -5.1353812 -2.3347723 0.3319869 3.3183036 5.1062341 5.1442676 4.3858247 1.2936058 -1.0608459 -4.9789023 -8.3691711 -8.568306 -8.021965][-8.0349083 -7.2675843 -6.632422 -4.5143991 -2.899323 -0.44937754 1.5872641 2.3110251 1.6022491 -0.10080624 -1.7495682 -5.6534057 -8.8893309 -8.1841316 -8.2929478][-8.9338055 -8.62818 -7.4631805 -6.0837345 -4.984653 -3.3215818 -2.1120985 -1.6426806 -1.7075677 -1.6079588 -2.1963339 -6.2267013 -8.307682 -8.5984869 -9.1632614][-11.671177 -10.970622 -9.0048771 -7.8903627 -7.222331 -6.2428861 -6.0878291 -5.1900015 -5.0077167 -5.0741091 -5.5897694 -7.236361 -7.3041611 -7.4858503 -7.8342781][-11.505184 -10.202444 -8.5935783 -8.515976 -8.122283 -7.6359024 -7.4312906 -7.0732441 -7.8472295 -7.4677715 -7.393641 -8.0210629 -8.3117313 -6.7197719 -5.927784][-10.373638 -8.8293839 -7.844378 -6.6898446 -6.44637 -7.2074671 -7.3224697 -7.4340816 -7.2068362 -7.5944386 -8.479948 -7.9058638 -7.8073111 -6.5231495 -5.564013][-7.1969695 -6.0893974 -4.9125881 -4.3307085 -4.9044018 -5.1803188 -5.5440383 -6.2816839 -6.6411271 -6.3872118 -6.1777081 -7.5922484 -9.042592 -8.5721388 -8.7799053]]...]
INFO - root - 2017-12-15 17:48:13.285837: step 40710, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 51h:53m:35s remains)
INFO - root - 2017-12-15 17:48:19.856384: step 40720, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 51h:25m:11s remains)
INFO - root - 2017-12-15 17:48:26.509035: step 40730, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 53h:09m:23s remains)
INFO - root - 2017-12-15 17:48:33.148579: step 40740, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.657 sec/batch; 53h:15m:14s remains)
INFO - root - 2017-12-15 17:48:39.743770: step 40750, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.669 sec/batch; 54h:15m:23s remains)
INFO - root - 2017-12-15 17:48:46.381880: step 40760, loss = 0.12, batch loss = 0.08 (12.7 examples/sec; 0.632 sec/batch; 51h:11m:53s remains)
INFO - root - 2017-12-15 17:48:52.998263: step 40770, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 51h:27m:21s remains)
INFO - root - 2017-12-15 17:48:59.566519: step 40780, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 54h:56m:02s remains)
INFO - root - 2017-12-15 17:49:06.183913: step 40790, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.677 sec/batch; 54h:49m:18s remains)
INFO - root - 2017-12-15 17:49:12.775931: step 40800, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 54h:28m:59s remains)
2017-12-15 17:49:13.314962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4478488 -3.0426118 -2.5315785 -1.5272269 -2.238889 -2.5280333 -2.9717102 -3.247452 -3.4190066 -3.6918159 -3.8497586 -5.3372731 -6.4103389 -7.5064626 -5.928853][-2.6907117 -1.5177622 -0.089793205 -0.13318396 -1.5524192 -1.8855829 -1.9077246 -1.8227901 -1.929801 -2.3373802 -2.7313993 -4.1891122 -5.3384361 -6.0639753 -4.9474888][-1.3850875 -1.1788054 -1.114872 -0.61452532 -0.93666983 -1.4765253 -1.6745157 -1.3124385 -0.70888281 -0.79450846 -1.0433407 -2.8677304 -3.8871524 -5.3378448 -4.8866391][-2.7949004 -1.9937499 -1.3818111 -1.0204253 -1.0941687 -1.2842946 -1.3821573 -1.1678376 -0.75604343 -0.015409946 0.058068275 -1.7423456 -2.8559592 -4.4038558 -4.0577006][-3.416806 -3.7543843 -4.3685007 -3.1145933 -2.6387403 -2.0258665 -1.3224297 -1.3330135 -1.2889938 -0.56505442 0.015064716 -1.1842146 -2.1779156 -4.2735996 -4.378068][-4.8634911 -4.0955791 -3.126395 -1.1493888 -0.45793915 0.62311506 1.4992952 1.0808578 0.50439358 -0.002448082 -0.35505056 -1.0217948 -1.3542686 -3.0731659 -2.5915329][-4.9218092 -3.9401155 -2.0985267 0.064944267 0.95647764 2.1515346 3.1735044 2.9531541 2.6270604 1.6674147 0.94884491 -0.32522249 -1.3922253 -2.1780379 -1.4135675][-5.4525909 -4.1909122 -2.6118484 -0.059194088 1.6313605 3.1300225 4.2597346 3.9898677 3.2889123 2.1428876 1.4498296 0.01286459 -0.69335985 -1.4360094 -0.72384548][-5.0770049 -3.1168492 -1.8709872 0.40675735 1.0753403 2.3038173 3.0563149 3.1838155 3.3268352 1.9426823 0.76617479 -0.8470211 -1.541502 -2.518708 -1.4397759][-4.6483593 -3.8562977 -3.1720064 -0.43281031 0.52347994 0.90696239 1.524509 1.2011027 0.89424229 -0.341362 -1.644228 -3.2878079 -3.5850306 -3.8404083 -2.4179356][-7.4643159 -6.5572319 -5.5539455 -3.3708436 -3.4015355 -3.1486957 -2.8395569 -2.8629889 -2.3414841 -2.8920019 -3.8369861 -6.612112 -8.0881615 -6.8798265 -4.0919147][-10.111343 -9.9084024 -8.9353838 -7.0129161 -6.5538373 -5.7279925 -5.7959023 -5.7182322 -5.1934686 -5.3350043 -5.3854136 -6.2931132 -6.3296375 -5.8094287 -3.6203642][-11.814573 -10.598828 -9.0856915 -7.9960432 -7.750845 -7.0455623 -6.2251086 -5.5636964 -5.9144378 -6.3807669 -6.7579861 -6.889185 -7.3061843 -6.1986532 -3.7578197][-8.3156366 -6.8036079 -5.8870549 -4.6878042 -3.9701688 -3.7591982 -3.8688402 -4.0677271 -4.0000176 -4.6270332 -6.10279 -6.7121429 -6.3291092 -5.6735058 -4.391346][-3.8820603 -3.7492752 -3.0762508 -1.6046367 -1.2267981 -0.75582886 -1.2048111 -1.1435103 -1.1101942 -1.9195895 -3.2201965 -4.39555 -5.6637845 -5.4313445 -4.7962384]]...]
INFO - root - 2017-12-15 17:49:19.836686: step 40810, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 52h:27m:46s remains)
INFO - root - 2017-12-15 17:49:26.563189: step 40820, loss = 0.13, batch loss = 0.08 (11.4 examples/sec; 0.701 sec/batch; 56h:46m:29s remains)
INFO - root - 2017-12-15 17:49:33.134519: step 40830, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.671 sec/batch; 54h:23m:51s remains)
INFO - root - 2017-12-15 17:49:39.730741: step 40840, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 52h:26m:01s remains)
INFO - root - 2017-12-15 17:49:46.318920: step 40850, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 52h:02m:13s remains)
INFO - root - 2017-12-15 17:49:52.918204: step 40860, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 51h:45m:53s remains)
INFO - root - 2017-12-15 17:49:59.543319: step 40870, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 51h:47m:33s remains)
INFO - root - 2017-12-15 17:50:06.095318: step 40880, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 54h:16m:28s remains)
INFO - root - 2017-12-15 17:50:12.635987: step 40890, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 54h:56m:36s remains)
INFO - root - 2017-12-15 17:50:19.209210: step 40900, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.686 sec/batch; 55h:34m:27s remains)
2017-12-15 17:50:19.746294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0402083 -6.9586287 -7.59659 -7.9001875 -8.3927536 -8.9249353 -9.3693295 -10.36129 -11.001974 -10.982738 -10.634767 -10.861559 -9.8978748 -8.1235847 -5.4280081][-7.732646 -7.479763 -7.7654462 -8.6300888 -9.409071 -9.7077694 -9.469985 -9.5723915 -10.565719 -10.979157 -10.614168 -10.140632 -9.0279722 -8.53627 -8.29598][-6.3447475 -7.1510453 -8.7643986 -8.3799648 -9.0366621 -9.6264076 -9.3103333 -9.0190058 -9.3271856 -9.3813438 -9.9785709 -9.6315031 -7.8481884 -7.6630926 -7.2825966][-8.3246078 -8.7418718 -8.5835524 -8.010519 -8.530426 -7.656776 -6.1310167 -7.1249681 -8.38858 -8.1106691 -8.22179 -8.6103172 -7.9351206 -7.4726357 -7.3363433][-6.777771 -9.037261 -10.590839 -9.0364513 -8.0078678 -5.1908512 -1.2302947 -1.995641 -4.7906089 -6.3541222 -7.8060637 -7.9493504 -6.9915762 -7.0237217 -7.305089][-8.5946884 -9.3537512 -9.8728447 -8.1800108 -6.1054111 -1.7671795 3.1085057 3.6839986 2.9691224 -1.1190367 -5.7170582 -5.89064 -4.8676386 -5.4023156 -5.4362397][-8.6524639 -9.4438143 -8.4564886 -6.4879818 -3.848345 1.6125002 6.7189393 7.917582 8.0866566 3.4309583 -2.1956341 -4.5936337 -5.1293635 -5.1482778 -4.1961684][-7.8400917 -9.4219017 -8.7443876 -5.728436 -1.770318 2.3404784 6.3443093 8.085928 8.0680466 3.9762511 -0.63315296 -4.2968106 -5.3035946 -5.9412651 -5.9394007][-6.8370514 -6.65313 -7.306839 -5.3772464 -2.4177351 0.80126572 4.070724 4.8019576 5.5003905 3.1160188 -0.072202682 -3.7795224 -6.2009015 -7.5670996 -7.3485641][-6.14004 -5.2081409 -6.1161366 -5.0729403 -4.4633555 -1.5722251 1.0901799 1.3622971 1.9883003 0.47078085 -1.6892271 -4.3772817 -5.879282 -8.2912483 -9.7591372][-8.4021435 -8.4271784 -9.2603178 -7.8278141 -7.485568 -5.4502158 -3.5814028 -4.6796083 -4.9462967 -5.460638 -5.5212679 -7.51723 -9.1861706 -11.091388 -11.086737][-13.200778 -12.119168 -12.60373 -11.643672 -10.645916 -9.4204073 -9.6257887 -9.6393461 -9.6562958 -10.171379 -10.958662 -11.596312 -11.671464 -12.123077 -12.329095][-11.616674 -11.655451 -11.684315 -10.682018 -11.52195 -10.712306 -10.202517 -10.763127 -11.533586 -10.866198 -10.092901 -10.648985 -11.147509 -12.077202 -10.870109][-9.9953747 -9.8003492 -9.3962135 -9.4445477 -8.4753428 -8.4964781 -9.0325346 -9.5399685 -9.9751053 -9.9057684 -9.6354275 -9.3219624 -8.6205912 -7.8738708 -7.4320207][-6.0141168 -6.759366 -6.4596834 -5.5733805 -4.7100854 -3.9133604 -3.9947453 -4.7511268 -6.1413016 -6.975142 -6.9139361 -6.2885804 -6.7884164 -7.2656446 -7.0334024]]...]
INFO - root - 2017-12-15 17:50:26.365741: step 40910, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 54h:23m:32s remains)
INFO - root - 2017-12-15 17:50:32.972362: step 40920, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 53h:41m:22s remains)
INFO - root - 2017-12-15 17:50:39.620047: step 40930, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 55h:54m:38s remains)
INFO - root - 2017-12-15 17:50:46.246952: step 40940, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 53h:57m:49s remains)
INFO - root - 2017-12-15 17:50:52.749074: step 40950, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 52h:40m:35s remains)
INFO - root - 2017-12-15 17:50:59.286345: step 40960, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 52h:50m:00s remains)
INFO - root - 2017-12-15 17:51:05.856524: step 40970, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 52h:36m:42s remains)
INFO - root - 2017-12-15 17:51:12.454442: step 40980, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 52h:51m:25s remains)
INFO - root - 2017-12-15 17:51:19.085263: step 40990, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 52h:58m:14s remains)
INFO - root - 2017-12-15 17:51:25.724917: step 41000, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 53h:17m:05s remains)
2017-12-15 17:51:26.312412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3995819 -3.9578042 -3.2517025 -2.70749 -2.7951188 -2.6577313 -2.6125083 -4.2501307 -6.0395403 -6.3993192 -5.5040069 -6.0808587 -5.9450364 -7.2484035 -8.3215179][-4.0109873 -3.4022098 -2.9359906 -1.9137437 -1.5477314 -1.2234626 -1.4478426 -2.364538 -4.350862 -6.2216792 -6.0867319 -6.7724266 -7.32509 -8.6578979 -9.2527237][-3.5358093 -3.2884612 -3.27864 -3.2318308 -3.1950121 -2.7424846 -2.5693264 -2.4031341 -3.5707173 -6.095921 -7.246356 -8.2721529 -8.8668423 -10.384256 -11.038851][-5.0460052 -3.8377504 -3.120265 -2.8343556 -3.4760594 -3.7543294 -3.2558265 -2.8115094 -3.8571472 -5.3204494 -6.8991079 -9.8514118 -10.709661 -11.556627 -11.861068][-5.326869 -4.6662807 -4.6603441 -3.6949708 -3.0246248 -2.6556175 -2.2658288 -2.3345311 -3.0751295 -4.9937992 -6.1388836 -8.5163517 -9.43471 -12.075734 -13.122042][-5.6646562 -5.4117708 -4.5751309 -2.0582678 -1.0106983 0.918046 1.8853989 0.79211283 -1.4230032 -2.5894661 -3.1229939 -5.5678935 -7.3550267 -9.6648064 -11.318619][-6.1489344 -5.2901683 -3.5869637 -1.347548 -0.2940855 2.4181771 4.1373219 4.4132571 3.6343741 0.522717 -2.4520614 -4.1351376 -5.192018 -8.7627716 -10.177992][-6.4562678 -6.3816814 -4.2648792 -0.69332743 1.2065387 4.6202121 6.4723725 5.8056235 4.8978372 2.0352674 -0.87465477 -3.4788694 -5.1748085 -7.8480692 -9.0144434][-6.0332165 -5.6080132 -3.9282589 -1.6198583 0.29504919 3.0565 4.6378331 5.1596923 4.5598187 1.0508523 -1.4171405 -4.1790814 -6.2495966 -8.50224 -9.8538189][-5.4256043 -4.6884274 -3.222703 -0.78222942 -0.021754265 0.69838285 1.6125212 1.8276753 1.226264 -0.86292315 -2.1295233 -5.5496225 -8.28309 -11.104486 -12.55568][-8.0064 -7.9044905 -7.5198812 -4.6299596 -3.7178717 -3.1246357 -3.1166375 -3.1014631 -2.6615551 -3.5497952 -5.3197155 -9.331852 -11.577486 -12.86222 -13.087044][-12.405897 -12.228159 -11.666269 -10.265728 -8.7801056 -7.7299223 -7.5329318 -8.0051537 -8.2331924 -8.6536808 -9.5359039 -11.020069 -12.214164 -13.532084 -13.117468][-13.930445 -13.333431 -11.773925 -10.662069 -11.055199 -10.149593 -9.0539532 -9.2867727 -9.2746754 -9.7342682 -10.667618 -12.428296 -12.479565 -11.016621 -9.608942][-14.251387 -13.440804 -12.149101 -10.336819 -10.2245 -10.296511 -11.145565 -10.925505 -10.163708 -9.741312 -9.2837029 -9.7622986 -9.5084906 -8.9956665 -7.6364384][-8.9414558 -9.1952715 -9.1136 -9.3861313 -9.4323254 -8.0272789 -8.0520811 -8.576067 -9.1840324 -8.974946 -8.49153 -8.76561 -8.4510469 -7.8612947 -8.0275116]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 17:51:32.929188: step 41010, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 53h:09m:28s remains)
INFO - root - 2017-12-15 17:51:39.559512: step 41020, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 54h:21m:24s remains)
INFO - root - 2017-12-15 17:51:46.147644: step 41030, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 54h:44m:27s remains)
INFO - root - 2017-12-15 17:51:52.690236: step 41040, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 53h:07m:47s remains)
INFO - root - 2017-12-15 17:51:59.285417: step 41050, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 54h:36m:30s remains)
INFO - root - 2017-12-15 17:52:05.905724: step 41060, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 53h:33m:54s remains)
INFO - root - 2017-12-15 17:52:12.415445: step 41070, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 54h:18m:45s remains)
INFO - root - 2017-12-15 17:52:19.000586: step 41080, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 54h:06m:01s remains)
INFO - root - 2017-12-15 17:52:25.609760: step 41090, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 53h:15m:29s remains)
INFO - root - 2017-12-15 17:52:32.186432: step 41100, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 52h:14m:58s remains)
2017-12-15 17:52:32.720772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1875367 -5.8240585 -7.1574249 -8.0229893 -9.1093082 -10.512407 -11.67833 -10.993343 -10.582504 -10.260073 -9.3117647 -10.130628 -11.927763 -11.112357 -12.203251][-7.8553252 -8.944519 -9.1529827 -9.7982082 -10.184894 -10.942568 -12.200716 -13.320139 -14.000824 -12.488605 -11.192406 -11.945572 -13.186869 -12.119683 -12.431852][-4.66635 -7.16391 -9.9876261 -9.6967545 -9.1230364 -11.070648 -12.632936 -12.717297 -12.641159 -12.803405 -12.532321 -11.947531 -12.955311 -12.748871 -13.930275][-7.8941503 -9.8496819 -11.079879 -9.8157444 -9.5287056 -10.05566 -9.64611 -12.037746 -13.428076 -11.486092 -9.81662 -12.098433 -14.650978 -13.001776 -12.751825][-8.8056669 -12.054667 -13.953188 -11.867831 -9.4208126 -5.1816621 -3.0060749 -7.3994241 -12.084939 -12.006836 -11.494312 -11.170788 -12.3479 -12.861069 -14.012232][-12.141636 -13.934752 -13.577263 -11.260565 -8.300662 -2.8192961 2.8162408 0.90057087 -2.3713396 -7.412107 -12.420314 -11.411183 -11.875225 -11.948694 -13.449684][-15.418232 -14.808195 -12.983698 -9.2019024 -4.1344776 1.2402201 5.5906444 5.6156039 5.3275113 -1.3008289 -8.9913845 -10.391011 -13.103788 -12.041844 -12.58407][-15.202503 -14.559118 -13.140804 -8.4834518 -1.4849534 5.5374331 9.6466484 6.3663592 4.7565856 0.0053310394 -5.4334559 -8.6412029 -12.512311 -12.42679 -14.019798][-12.54328 -11.878969 -12.785971 -8.9574194 -3.4133289 2.5153871 8.0170345 8.7258606 4.9625525 -2.665951 -7.3411045 -9.7548895 -14.103037 -13.636051 -14.328299][-10.329111 -10.323174 -10.716591 -7.2144852 -4.9930248 -2.3802674 2.7957654 4.253736 2.2768998 -2.668494 -8.2367773 -11.832189 -14.816135 -14.313604 -16.08959][-10.644325 -12.029342 -12.528628 -10.720393 -9.0225067 -6.4809833 -3.191103 -3.5230188 -4.0701218 -5.295177 -8.68246 -13.132748 -15.444302 -15.371073 -15.334446][-16.816803 -16.136578 -16.138157 -15.66922 -14.470516 -11.377461 -10.832591 -11.56846 -11.224941 -11.745882 -13.29032 -14.075203 -14.721369 -14.656111 -14.443171][-15.955931 -14.487892 -14.525894 -16.532267 -15.908426 -14.575874 -12.523341 -12.400805 -13.760914 -13.272097 -12.953569 -14.803419 -14.838564 -13.228922 -12.869034][-13.029013 -12.355437 -13.083492 -11.450671 -11.289034 -12.079775 -12.625769 -11.950002 -11.339237 -11.334684 -12.172281 -12.468998 -12.220531 -10.616753 -10.912209][-9.477273 -8.29724 -7.2127757 -6.5486903 -6.5416908 -6.412509 -7.1810074 -9.023756 -9.4037523 -8.653553 -8.8388252 -10.757736 -10.933964 -11.186541 -12.370721]]...]
INFO - root - 2017-12-15 17:52:39.384361: step 41110, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 54h:31m:38s remains)
INFO - root - 2017-12-15 17:52:46.010590: step 41120, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 53h:39m:07s remains)
INFO - root - 2017-12-15 17:52:52.702190: step 41130, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 54h:49m:38s remains)
INFO - root - 2017-12-15 17:52:59.244029: step 41140, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 52h:09m:05s remains)
INFO - root - 2017-12-15 17:53:05.860517: step 41150, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 52h:53m:46s remains)
INFO - root - 2017-12-15 17:53:12.539838: step 41160, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 53h:57m:22s remains)
INFO - root - 2017-12-15 17:53:19.185062: step 41170, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 53h:37m:58s remains)
INFO - root - 2017-12-15 17:53:25.778100: step 41180, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 52h:07m:05s remains)
INFO - root - 2017-12-15 17:53:32.358716: step 41190, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 53h:12m:04s remains)
INFO - root - 2017-12-15 17:53:38.976883: step 41200, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 53h:40m:10s remains)
2017-12-15 17:53:39.488856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4324875 -7.7951345 -7.5376396 -7.3093643 -8.5585461 -10.028646 -11.455574 -10.989143 -9.8863678 -8.9413033 -8.3682823 -9.4527407 -11.733596 -10.57765 -8.0027008][-9.0308838 -6.7270575 -5.692595 -5.4982786 -6.5610986 -8.8345823 -10.85843 -11.289219 -10.549884 -8.7051868 -7.6190486 -9.46422 -11.327555 -10.768087 -9.515276][-6.1842551 -5.2936883 -5.2563457 -4.4666147 -4.7795024 -6.710639 -7.9528723 -8.9376354 -9.4439783 -9.04233 -8.6336365 -10.070006 -12.383081 -11.702158 -10.467518][-7.0050874 -5.8985291 -5.8378773 -3.7954762 -4.0706182 -4.9984941 -5.9813128 -7.2471776 -8.4637136 -8.82283 -8.9809113 -10.562229 -13.016975 -12.765793 -11.818117][-10.203148 -10.593794 -9.6398849 -6.1916027 -4.0827603 -2.1046379 -1.7919407 -5.4353118 -8.660203 -8.500598 -8.610281 -10.813914 -13.380832 -12.491157 -11.403265][-12.490894 -12.407556 -10.86014 -6.465332 -2.6822577 1.8527541 4.5917611 1.1067681 -2.4147637 -4.886344 -8.0487671 -9.3169918 -11.391588 -11.395338 -9.29071][-14.388376 -12.093454 -8.8764267 -4.4585848 0.28286076 4.3795962 7.8500228 7.5708528 5.1720357 -0.386158 -5.652154 -7.9300761 -11.546244 -11.022635 -8.6371355][-13.76042 -10.527574 -8.6693125 -2.8942235 2.626862 6.1749511 9.7987556 9.1631184 6.6278043 2.6721768 -2.2313726 -7.5487194 -13.241912 -13.135445 -11.143726][-10.262498 -9.59489 -8.1529245 -3.8303866 -0.15776539 4.1208987 7.55942 5.1587043 2.4514623 0.30661869 -2.9315197 -7.8743038 -13.878124 -15.706453 -14.516315][-9.2352591 -8.883522 -8.8016376 -4.8498716 -1.9382384 -0.50377464 0.88740349 -0.23681164 -2.2407241 -4.3650541 -6.8534126 -10.350863 -15.398806 -16.076557 -15.106243][-12.655021 -12.573973 -12.013616 -9.1832314 -7.8178816 -7.3013835 -6.4106255 -6.8145323 -7.6012449 -9.5762892 -11.502909 -14.519648 -17.275042 -16.937841 -15.063265][-17.483967 -16.285858 -15.263153 -13.292099 -13.076806 -12.967322 -12.818605 -13.379737 -13.405275 -13.642994 -14.264927 -17.217613 -18.477724 -16.829163 -14.364426][-18.141157 -16.786263 -16.372965 -14.898571 -13.934017 -13.764828 -14.048117 -15.030809 -15.460518 -14.531725 -13.617014 -15.364557 -15.676916 -14.203485 -11.625168][-13.023005 -12.591005 -11.706321 -10.934943 -11.212826 -11.922827 -12.982786 -12.470573 -12.135573 -12.147588 -11.995471 -11.961821 -11.639046 -10.968546 -9.0047264][-8.1219826 -6.451581 -5.3096933 -3.4744244 -4.0567288 -5.352087 -7.17119 -7.1455126 -7.3783913 -7.2151475 -7.69395 -9.0562973 -9.4430532 -8.8886509 -7.1735864]]...]
INFO - root - 2017-12-15 17:53:46.159174: step 41210, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.696 sec/batch; 56h:20m:03s remains)
INFO - root - 2017-12-15 17:53:52.820875: step 41220, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 53h:09m:07s remains)
INFO - root - 2017-12-15 17:53:59.449470: step 41230, loss = 0.21, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 52h:37m:24s remains)
INFO - root - 2017-12-15 17:54:06.108308: step 41240, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 53h:54m:57s remains)
INFO - root - 2017-12-15 17:54:12.770507: step 41250, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 53h:35m:18s remains)
INFO - root - 2017-12-15 17:54:19.372614: step 41260, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 53h:00m:54s remains)
INFO - root - 2017-12-15 17:54:26.010920: step 41270, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 51h:46m:52s remains)
INFO - root - 2017-12-15 17:54:32.517411: step 41280, loss = 0.18, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 51h:19m:03s remains)
INFO - root - 2017-12-15 17:54:39.139542: step 41290, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 54h:31m:31s remains)
INFO - root - 2017-12-15 17:54:45.817355: step 41300, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 53h:03m:54s remains)
2017-12-15 17:54:46.354068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5885973 -1.8863792 -0.45541573 -0.15975046 -0.56335592 -1.1716924 -1.5252619 -1.6756358 -1.8439319 -1.602829 -1.2844567 -3.8429265 -5.519547 -7.2446737 -7.7430077][-2.7005844 -1.975075 -1.0282197 -0.071327686 0.12195826 -0.57457113 -0.71560431 -0.7854681 -0.80018806 -1.5345826 -2.1132624 -4.852407 -5.7902932 -7.5961246 -8.4082518][-2.2768011 -1.8441219 -1.2412558 -0.089194775 -0.0042710304 -0.2001977 -0.25192928 -0.45223141 -0.07353878 0.13714075 -0.16504049 -3.3775289 -4.9538746 -6.68207 -8.1057091][-2.3852978 -2.6737232 -2.2975516 -0.93495655 -0.67314053 -0.55218267 -0.40758467 -0.68257666 -0.54871321 -0.18386793 -0.289145 -2.9367371 -4.5608072 -5.7547073 -6.5916252][-1.8097975 -1.9134989 -1.3451452 -0.46380234 -0.30345726 -0.0015454292 -0.21731901 -0.36269617 -0.39704943 -0.2420845 -0.66750574 -3.0035877 -4.0903935 -5.6894126 -6.9898262][-1.6134868 -1.2072792 -0.94129086 0.32818794 0.83175516 0.73040581 0.76614475 0.85037613 1.1213946 0.9131155 0.3077755 -1.9770119 -3.5749717 -5.1226878 -6.5510235][-3.0954461 -2.1354532 -0.90344667 1.1915054 1.553638 2.218699 2.558764 2.0484829 1.7758684 0.95243979 0.19284725 -2.1950216 -4.4384708 -5.8036175 -6.9565763][-2.8225403 -2.1389875 -0.60304546 1.0718517 2.1799731 2.9333749 3.2143006 2.4212661 2.1028171 0.66226482 -0.77954769 -3.5420542 -5.0219207 -6.2288623 -7.7405686][-2.9257474 -2.5160282 -1.1296353 0.27346563 1.1731811 2.2234969 3.186914 2.2255073 1.724143 0.16314173 -0.85688925 -3.7250426 -5.3924565 -7.0298781 -7.800807][-3.8142209 -3.3546956 -2.7046897 -1.3558922 -0.12385035 0.88319159 0.82790852 0.47452068 0.16104317 -1.2574148 -2.2259023 -4.5132132 -6.1123686 -7.7494936 -8.4234848][-6.9278231 -6.781539 -6.0678596 -4.6047111 -3.6462135 -2.7525282 -2.5660577 -2.5711005 -2.6464145 -3.2961717 -3.944021 -6.4632411 -8.2813263 -9.4063349 -9.4701881][-10.076389 -10.134836 -9.90454 -8.1642284 -6.6938963 -5.6348262 -5.4253449 -5.185873 -5.6408043 -6.3744197 -6.55968 -7.8285084 -8.496767 -10.071218 -10.906103][-10.35936 -9.9813986 -9.337636 -8.4129515 -8.2926416 -7.3191071 -6.36162 -6.4610143 -7.0758648 -7.2650871 -7.7825108 -8.5022812 -9.208477 -9.5306082 -9.241293][-10.435534 -9.5406942 -8.7518492 -8.0381575 -7.1878943 -7.7269225 -8.9439249 -8.1148119 -7.2475462 -7.6088266 -8.3766718 -8.2441931 -8.2956324 -8.315239 -8.3669491][-6.5603714 -6.1878462 -5.1737919 -4.6595488 -4.2217951 -4.911468 -4.932529 -5.493185 -6.55998 -6.2681017 -5.920383 -6.7610297 -8.1135626 -9.0512743 -9.6199608]]...]
INFO - root - 2017-12-15 17:54:52.914776: step 41310, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 51h:44m:08s remains)
INFO - root - 2017-12-15 17:54:59.475742: step 41320, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 53h:04m:30s remains)
INFO - root - 2017-12-15 17:55:06.070361: step 41330, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 53h:43m:28s remains)
INFO - root - 2017-12-15 17:55:12.735175: step 41340, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 54h:01m:27s remains)
INFO - root - 2017-12-15 17:55:19.283062: step 41350, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 52h:53m:42s remains)
INFO - root - 2017-12-15 17:55:25.824792: step 41360, loss = 0.14, batch loss = 0.09 (12.8 examples/sec; 0.627 sec/batch; 50h:41m:54s remains)
INFO - root - 2017-12-15 17:55:32.401139: step 41370, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 53h:08m:59s remains)
INFO - root - 2017-12-15 17:55:38.919736: step 41380, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 53h:11m:32s remains)
INFO - root - 2017-12-15 17:55:45.552184: step 41390, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 53h:57m:17s remains)
INFO - root - 2017-12-15 17:55:52.147238: step 41400, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 53h:52m:28s remains)
2017-12-15 17:55:52.629887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.19241 -8.7739258 -7.1610761 -5.7319918 -5.1798711 -5.2072673 -5.4954872 -6.5952148 -6.7694459 -6.2885056 -5.4110336 -5.816709 -7.5961967 -8.03911 -8.0340118][-7.6878 -7.2831841 -5.4419217 -3.8386979 -3.4029753 -3.4461389 -3.6627693 -4.4215355 -4.3649187 -4.3590131 -3.5119207 -4.2152672 -6.7511559 -7.2492433 -7.0072746][-5.2635274 -5.5759478 -5.3004584 -3.4886518 -2.2270169 -1.942255 -2.1199977 -2.696615 -2.9102776 -2.4991369 -2.1655462 -3.1503801 -5.5444918 -6.4372554 -6.4478631][-6.0520535 -6.2183967 -5.1586661 -3.7626605 -2.906419 -2.3210275 -2.1603646 -2.3372054 -2.1977489 -2.1222141 -1.7857699 -3.0524936 -6.7785625 -7.686512 -7.3877344][-6.8168039 -6.5869923 -5.8737884 -3.8606195 -2.427968 -0.57689476 0.35275364 -0.059188366 -0.71441031 -0.66055441 -0.71565485 -2.4827392 -5.627562 -6.8151836 -6.5355091][-6.8252778 -6.4200659 -5.3357339 -2.7304587 -0.65153408 1.2814484 3.258183 3.7059846 3.6066117 2.092042 0.20768881 -1.2165279 -4.191772 -5.448061 -6.1758232][-6.807929 -6.4529123 -4.9013381 -2.2358065 -0.44056988 1.9336495 4.5368943 5.13055 4.9743829 2.7827744 0.29711342 -1.8437481 -5.0422363 -5.9370174 -6.1358433][-7.4262466 -7.4311686 -5.5643134 -2.1475062 -0.43645573 1.6692657 3.7767205 3.992311 3.6215835 1.9316773 -0.30308056 -2.9096859 -6.8845153 -7.5823164 -7.4696465][-7.4564266 -6.5479851 -4.8666773 -1.7883923 -0.77873039 0.94048929 3.0766597 3.2453742 3.0625739 1.2953196 -0.90431213 -3.3014832 -7.6017537 -8.76669 -8.6083746][-6.4061766 -5.9325871 -4.551652 -2.0777674 -1.1489391 -0.21042061 1.0867434 1.5892572 2.4080472 0.82066774 -0.7644372 -3.2789419 -7.6199141 -9.435071 -10.404339][-9.3933887 -8.3291483 -6.6826096 -4.77604 -4.2326074 -3.360739 -2.1034381 -2.6393561 -3.3434775 -3.6402493 -3.7701893 -6.2578282 -9.1545706 -9.4044 -8.5684519][-13.471071 -12.395475 -9.9721718 -7.3506079 -6.1945233 -5.3205838 -5.2717543 -5.9597678 -6.0040064 -6.6341004 -6.9958272 -7.1614051 -8.3088884 -9.2516785 -9.6177483][-11.753477 -10.433376 -8.5058689 -7.26128 -6.5883503 -5.3976851 -5.3445807 -6.1098127 -6.6843634 -6.6999168 -6.6357913 -7.6198416 -7.8239851 -7.4768896 -6.9825649][-8.1834373 -7.6394958 -6.6935921 -4.7019005 -4.1749759 -4.9517107 -5.6893563 -5.4414363 -5.8885636 -6.6027708 -6.7853918 -6.6273451 -6.4244695 -6.8542981 -7.0119042][-6.5618529 -5.6002407 -4.1362343 -3.6766517 -2.6951959 -1.9570241 -2.0389707 -3.2818246 -4.5407524 -4.2649121 -4.7686381 -6.3305535 -7.0773029 -6.6843886 -6.2983327]]...]
INFO - root - 2017-12-15 17:55:59.212141: step 41410, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 54h:34m:02s remains)
INFO - root - 2017-12-15 17:56:05.821783: step 41420, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 53h:13m:25s remains)
INFO - root - 2017-12-15 17:56:12.335369: step 41430, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 51h:38m:54s remains)
INFO - root - 2017-12-15 17:56:19.033327: step 41440, loss = 0.17, batch loss = 0.12 (11.4 examples/sec; 0.700 sec/batch; 56h:33m:46s remains)
INFO - root - 2017-12-15 17:56:25.589364: step 41450, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 53h:17m:34s remains)
INFO - root - 2017-12-15 17:56:32.167208: step 41460, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 53h:31m:19s remains)
INFO - root - 2017-12-15 17:56:38.735231: step 41470, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 52h:00m:49s remains)
INFO - root - 2017-12-15 17:56:45.380810: step 41480, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 51h:30m:00s remains)
INFO - root - 2017-12-15 17:56:51.961845: step 41490, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 52h:46m:44s remains)
INFO - root - 2017-12-15 17:56:58.548202: step 41500, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 52h:34m:22s remains)
2017-12-15 17:56:59.102429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0887012 -6.4159541 -6.1336679 -5.1423182 -5.2790251 -5.627984 -5.4553828 -5.1445222 -5.0886364 -5.70453 -5.96282 -7.3961306 -8.5431452 -9.3301888 -8.3873577][-6.3277369 -5.6500688 -5.4268436 -5.49667 -5.431664 -5.683877 -5.7881055 -4.8702092 -4.899622 -5.5141263 -6.1509547 -7.8010821 -8.1782751 -9.4969587 -8.3111534][-5.1242089 -5.4138193 -5.8954115 -5.6219139 -6.1978955 -6.2934318 -6.0651278 -5.6141739 -5.0927672 -5.5380311 -6.7187004 -8.57426 -8.8845768 -9.3193932 -8.3115616][-4.8857112 -4.6953511 -5.387249 -5.7671552 -6.6934257 -6.6450953 -5.8669686 -5.34783 -5.3166642 -5.7502055 -6.1920943 -8.3577528 -9.062254 -9.3681679 -8.3190517][-5.1579847 -5.2634568 -5.5890827 -6.068059 -6.1964622 -4.8278632 -3.5224485 -3.9535751 -4.1556211 -4.6467261 -5.7819881 -7.6798086 -8.059865 -9.5413179 -8.4860125][-7.5913515 -6.3667855 -5.5617394 -5.08897 -3.9455843 -1.9453533 0.45313215 0.83374262 -0.091220379 -2.6314626 -5.2056756 -6.1047993 -7.0589523 -8.4316645 -7.9857621][-9.4952259 -7.8724136 -7.1911526 -4.7832155 -2.378042 0.55566454 4.0780015 5.3386092 5.2801185 1.1580405 -3.4643757 -6.1304445 -7.5460091 -8.5937052 -8.2483854][-10.049191 -8.5356293 -8.2708626 -5.2186856 -2.2800026 1.7619019 5.9448924 7.1446185 7.6571126 3.9630694 -1.0276432 -5.8113537 -8.1216974 -9.6900921 -8.6583176][-8.4644985 -7.5361338 -7.3545156 -5.4456224 -3.3921402 0.56566429 3.6997285 5.4129491 5.615994 2.0442581 -1.1152029 -5.5459404 -9.0166922 -10.906731 -9.6768064][-6.4971523 -6.0392685 -6.0047245 -4.6426878 -4.2865868 -2.285661 0.014820099 1.5275435 1.3931584 -0.81694984 -3.6904883 -8.2022676 -9.953867 -11.090292 -11.02276][-8.707695 -7.6464314 -7.1973066 -6.2424135 -6.1447449 -5.8150768 -5.7060146 -5.3057318 -5.3361683 -6.0459838 -7.3922172 -10.263649 -11.6953 -12.521656 -10.620989][-12.08923 -10.471778 -9.7810631 -8.8272724 -8.5844631 -8.1960058 -8.5243168 -9.4615536 -10.161692 -10.753988 -11.168884 -12.480772 -11.646473 -11.861328 -9.7181473][-13.191652 -11.617538 -10.798082 -10.276999 -10.59931 -9.8851089 -9.9792366 -10.849411 -11.326555 -10.942012 -10.87993 -11.20889 -10.738924 -10.082696 -7.8180504][-10.889838 -10.025859 -9.826807 -9.95314 -9.8987808 -9.5683889 -10.130032 -9.6444674 -9.2575884 -9.8099842 -9.8941441 -8.6686125 -7.8685188 -7.3780665 -6.501863][-7.6158581 -7.4493079 -6.7898684 -5.8045239 -6.2228861 -7.4496469 -7.0894861 -6.923357 -7.3878651 -6.8123589 -6.5684729 -7.3231258 -7.8435907 -7.6116538 -6.91022]]...]
INFO - root - 2017-12-15 17:57:05.713843: step 41510, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 52h:45m:02s remains)
INFO - root - 2017-12-15 17:57:12.361805: step 41520, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 54h:57m:24s remains)
INFO - root - 2017-12-15 17:57:18.944352: step 41530, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 51h:55m:23s remains)
INFO - root - 2017-12-15 17:57:25.449519: step 41540, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 52h:38m:23s remains)
INFO - root - 2017-12-15 17:57:32.062012: step 41550, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 53h:15m:21s remains)
INFO - root - 2017-12-15 17:57:38.614464: step 41560, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 52h:44m:08s remains)
INFO - root - 2017-12-15 17:57:45.193039: step 41570, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 52h:38m:06s remains)
INFO - root - 2017-12-15 17:57:51.747276: step 41580, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 52h:25m:06s remains)
INFO - root - 2017-12-15 17:57:58.340712: step 41590, loss = 0.22, batch loss = 0.17 (12.3 examples/sec; 0.653 sec/batch; 52h:45m:55s remains)
INFO - root - 2017-12-15 17:58:05.000497: step 41600, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 52h:29m:24s remains)
2017-12-15 17:58:05.531736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8705049 -1.9202871 -1.5049467 -1.4598222 -2.5608094 -3.4597352 -4.3086438 -4.0981455 -3.4297309 -3.8202283 -3.9665353 -7.45309 -10.304523 -11.052645 -10.202591][-0.44638109 -0.0863781 0.38393068 -0.32983828 -2.0708125 -3.4066787 -4.5764194 -5.2687011 -6.1824856 -6.5871639 -6.8830333 -9.9119482 -12.132513 -13.286173 -13.127895][-0.42263365 -0.23742247 0.76959372 1.3652792 0.659647 -0.82254648 -2.6074333 -4.4284716 -5.9503541 -6.9594555 -7.6321564 -10.3558 -12.531578 -13.172214 -12.496161][-1.7025375 -0.44527626 0.43799639 1.0678701 0.25877714 -0.672493 -1.2168384 -2.1970406 -3.2377787 -4.4282618 -5.668221 -8.6909084 -11.343674 -12.345547 -12.032139][-2.8042133 -2.8193293 -2.1728029 -0.42073679 -0.44081306 -0.17439508 -0.30898714 -1.3853865 -2.2002454 -3.0338724 -3.9580488 -6.7463455 -9.5132055 -10.96373 -10.708488][-4.2883911 -3.8126297 -2.8498385 -0.77564907 0.45776987 1.768383 2.3404384 1.9674873 1.1828198 -0.40715981 -1.9940257 -4.6767797 -7.7453952 -9.2191935 -9.3048935][-5.7125034 -5.097208 -3.4772863 -1.0437536 1.0559855 2.8811679 4.044107 4.3098292 3.9569907 1.888978 -0.50912285 -3.8602295 -7.7073741 -9.3725882 -9.5281591][-6.9066658 -6.1562586 -4.0756679 -1.2727532 0.66525221 2.7593274 4.2647815 4.4561076 4.8311009 3.6684508 1.8865142 -2.5152636 -7.3245339 -9.017993 -9.3714552][-7.2932281 -6.4099293 -4.9118409 -2.5705066 -0.60459852 1.1669421 2.4825187 2.6401229 2.9224668 2.2526937 1.2509127 -3.1304681 -7.384593 -8.7367582 -8.96358][-8.3783789 -7.5272 -5.715519 -3.6976483 -1.8814845 -0.42348289 0.65395164 1.1151056 1.0965662 0.35615396 -0.62103415 -4.0059705 -7.1279135 -7.859941 -7.3882375][-11.952368 -10.35672 -8.4650593 -6.0748248 -5.2564178 -4.6109757 -3.5022104 -3.0825236 -3.1272388 -3.4879382 -4.2124362 -7.2834821 -9.1236086 -9.4105015 -8.0877934][-14.71273 -13.290149 -11.317255 -9.4908257 -8.4131842 -7.8687387 -7.0723586 -6.7127419 -6.4113059 -6.6506367 -7.1766615 -8.7831831 -10.071162 -10.077962 -8.8518353][-15.580145 -14.10302 -12.589237 -11.379279 -10.911339 -10.433708 -9.9125576 -9.8561344 -9.7533951 -9.5592909 -9.5316591 -10.129002 -10.316368 -9.4135265 -8.4973707][-12.156854 -11.310213 -10.753219 -9.0344543 -8.0834827 -8.1477013 -8.3220053 -8.5224352 -8.78516 -9.0286417 -9.4155846 -9.1837006 -9.2671824 -8.5487309 -7.9187646][-8.6837549 -8.1169634 -6.6842365 -5.0528417 -4.4916763 -4.071723 -4.5010571 -5.1135826 -5.738236 -6.3537831 -7.5967412 -8.9631348 -10.167002 -10.368487 -10.054207]]...]
INFO - root - 2017-12-15 17:58:11.984444: step 41610, loss = 0.23, batch loss = 0.19 (12.6 examples/sec; 0.636 sec/batch; 51h:24m:02s remains)
INFO - root - 2017-12-15 17:58:18.537977: step 41620, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 54h:00m:48s remains)
INFO - root - 2017-12-15 17:58:25.034449: step 41630, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 52h:15m:04s remains)
INFO - root - 2017-12-15 17:58:31.635967: step 41640, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 52h:13m:43s remains)
INFO - root - 2017-12-15 17:58:38.200627: step 41650, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 53h:21m:39s remains)
INFO - root - 2017-12-15 17:58:44.735271: step 41660, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 52h:19m:23s remains)
INFO - root - 2017-12-15 17:58:51.224765: step 41670, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 52h:14m:11s remains)
INFO - root - 2017-12-15 17:58:57.767286: step 41680, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.643 sec/batch; 51h:54m:17s remains)
INFO - root - 2017-12-15 17:59:04.305877: step 41690, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 53h:29m:34s remains)
INFO - root - 2017-12-15 17:59:10.916486: step 41700, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 51h:56m:46s remains)
2017-12-15 17:59:11.416643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.0057373 -8.7700739 -8.9149065 -9.7464218 -11.273727 -11.877753 -11.287811 -9.8794012 -8.6194868 -7.3868251 -5.873291 -5.9574542 -7.79754 -8.1198626 -7.5064287][-7.3779578 -7.8125091 -7.8673062 -7.977272 -9.24296 -10.321033 -10.465759 -9.9354239 -9.2592659 -8.1585236 -6.9358749 -6.5851927 -8.33021 -9.75215 -9.0932207][-4.9642067 -5.8862777 -6.3582082 -6.495163 -7.3103852 -7.6474333 -7.5814767 -7.5966811 -8.1311207 -8.0068235 -6.9224992 -6.6364427 -8.0012188 -9.1261215 -9.5492525][-5.2921877 -5.0417814 -4.2586126 -3.3341622 -4.2552037 -4.8088946 -4.9379935 -5.183672 -5.6324558 -5.610507 -5.4699469 -5.7214851 -7.2368417 -7.9050932 -8.41234][-7.1524172 -6.0554361 -4.1951175 -0.99896574 1.1389856 0.542089 -1.3464441 -3.4049623 -4.5880742 -4.9390407 -4.7375717 -4.850728 -6.0961266 -7.0792193 -7.2599955][-8.79488 -7.3412561 -4.4117575 -1.1041856 0.99105358 2.8051991 2.890306 0.8092289 -0.89554548 -2.3617585 -3.2138572 -3.4424272 -4.8383517 -5.9493818 -6.631424][-8.270649 -5.9622884 -2.4931056 1.6215487 3.3400388 4.2297711 4.2021813 3.3938222 2.754601 1.0541039 -0.80089474 -2.3936079 -4.4222407 -4.73158 -5.0277338][-8.1178179 -6.380652 -2.6999764 1.3944879 3.0365434 4.6784177 4.8357129 4.1534057 3.7234693 2.7493806 1.4498558 -1.0336022 -4.3086648 -4.769268 -3.4654121][-8.1897049 -6.8978643 -3.9431639 0.016039371 1.083272 1.8807306 1.9519811 2.1172347 2.1502552 1.5363293 0.531373 -1.0518308 -3.3073089 -4.0004425 -2.6164455][-8.6304 -7.3919964 -5.008769 -2.541609 -2.7020593 -2.377301 -2.1077454 -1.6404958 -0.80957556 -0.68561411 -0.82067442 -1.9151087 -3.6414144 -3.7047353 -1.8454912][-12.587194 -11.438271 -9.8935909 -7.798315 -7.5420346 -7.6852784 -7.3887458 -5.6702509 -3.9513624 -3.0043533 -3.3128924 -4.2862015 -5.360847 -4.5674257 -1.9826961][-14.093163 -12.617682 -11.185919 -9.994236 -9.4919243 -8.2596807 -7.2227745 -6.4770961 -5.6254644 -4.99788 -5.3173432 -5.8391995 -5.7750025 -4.8466916 -3.0860672][-13.209017 -12.274975 -11.189024 -10.347116 -9.28609 -8.0105705 -6.44223 -5.0584803 -4.3111839 -4.2693844 -4.9618177 -6.3500395 -6.2411079 -4.618135 -1.9323189][-11.586842 -10.358132 -9.1485825 -8.6745825 -6.9643044 -5.9000926 -4.5594878 -2.8288324 -1.4391046 -1.7801032 -3.0080569 -3.4887941 -3.6479726 -3.6464624 -2.1749871][-8.0339413 -7.5969129 -6.7118874 -5.8976679 -5.0390811 -3.958267 -1.77105 -1.0090942 -0.6646266 -0.25772858 -0.95794487 -1.9910316 -2.2626252 -2.2776942 -2.2071421]]...]
INFO - root - 2017-12-15 17:59:17.950406: step 41710, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.644 sec/batch; 52h:01m:13s remains)
INFO - root - 2017-12-15 17:59:24.549181: step 41720, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.671 sec/batch; 54h:12m:59s remains)
INFO - root - 2017-12-15 17:59:31.099407: step 41730, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 53h:11m:53s remains)
INFO - root - 2017-12-15 17:59:37.654108: step 41740, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 53h:22m:52s remains)
INFO - root - 2017-12-15 17:59:44.260101: step 41750, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.656 sec/batch; 52h:59m:35s remains)
INFO - root - 2017-12-15 17:59:50.832244: step 41760, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 52h:43m:56s remains)
INFO - root - 2017-12-15 17:59:57.437103: step 41770, loss = 0.17, batch loss = 0.12 (11.3 examples/sec; 0.706 sec/batch; 57h:02m:39s remains)
INFO - root - 2017-12-15 18:00:04.066734: step 41780, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 54h:41m:48s remains)
INFO - root - 2017-12-15 18:00:10.621213: step 41790, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.656 sec/batch; 52h:57m:47s remains)
INFO - root - 2017-12-15 18:00:17.166421: step 41800, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 52h:22m:19s remains)
2017-12-15 18:00:17.667841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9113331 -4.47618 -3.8158505 -1.884711 -1.8682766 -2.588428 -2.8202004 -2.7106459 -2.3334744 -1.2248969 -0.056967735 -1.319602 -4.7660685 -7.6870174 -8.2652645][-2.8224339 -1.2081943 -0.37033415 0.14321709 -0.39816952 -0.69451284 -0.53157282 -0.36556339 0.067346096 0.5107584 1.2437758 -0.64487457 -4.5055618 -6.7095995 -7.0617886][-1.6486902 -1.1287074 -1.1163678 -0.61207914 -0.88282967 -1.4582787 -1.2235026 -0.31953478 0.37264156 1.1540017 1.4568257 -1.6746264 -5.84768 -7.9762878 -8.4400454][-4.3424497 -3.419172 -2.0483265 -2.3893354 -3.0378954 -3.3118486 -3.1737015 -1.8345501 -0.72267962 -0.123528 0.25421429 -2.7762234 -6.5679555 -8.2354288 -8.8709688][-4.7616119 -4.3665714 -3.737025 -2.0567069 -1.2319255 -0.79300547 0.056949615 0.46060991 0.56428862 0.96361732 0.36929131 -2.5143361 -5.8780785 -8.3357182 -9.2667227][-5.2302613 -4.0235906 -2.209511 -0.73085356 0.31750345 1.0258393 1.1439266 1.2310152 1.0684195 0.69664192 0.00890255 -2.3814988 -5.6489053 -7.536942 -8.1629906][-6.0972905 -4.4685431 -2.2070422 -0.029629707 0.75906849 1.7421165 2.6148009 2.1255493 1.1346197 0.091916084 -1.0540137 -3.0562787 -5.4068542 -6.7855425 -7.4988661][-5.5269356 -3.418278 -0.62244034 1.7196422 2.2669048 2.9282336 3.5591073 3.7986798 2.9733272 1.0137296 -0.77364922 -2.7991152 -5.3258343 -5.4970884 -4.4860687][-3.9780765 -2.3209784 -0.23027325 1.7641878 2.4869504 2.6660056 2.9375348 3.1645446 2.4140124 0.37003756 -1.7724378 -4.270237 -6.5193672 -5.8384333 -3.8211918][-5.4989305 -3.9934797 -2.2405071 -0.57707977 0.27637625 0.52430677 0.51468229 0.68039083 -0.21433067 -1.9097049 -3.446357 -5.4431124 -7.4617205 -6.7176127 -4.9179039][-11.02319 -10.045406 -8.3748932 -6.7535367 -5.177824 -4.1225257 -3.7313838 -4.2579556 -4.7275167 -5.3471012 -6.5679603 -8.8179455 -10.118223 -8.7553024 -6.0352006][-14.340981 -14.557199 -12.957425 -9.9311848 -8.443841 -7.0751863 -6.7825365 -7.7845106 -8.2638683 -8.4546585 -9.1390371 -10.63312 -10.994807 -9.7150583 -7.3547254][-14.771166 -13.740351 -12.52949 -10.727294 -8.9613724 -6.9907551 -7.1566248 -8.2980394 -8.93964 -9.4877033 -10.486219 -10.87406 -10.690027 -9.2978649 -6.8484859][-12.146368 -11.819561 -11.118637 -10.25062 -8.5063505 -6.85069 -6.3225608 -6.4506016 -7.0304575 -7.9621229 -9.11936 -9.2984858 -8.8853111 -7.7093463 -6.0758052][-9.3733826 -9.4008942 -8.4303961 -6.9201221 -5.7248797 -4.6879587 -3.4216189 -3.4894359 -3.735096 -4.1138835 -4.89319 -5.81015 -6.6454215 -7.075191 -7.3310032]]...]
INFO - root - 2017-12-15 18:00:24.214110: step 41810, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 53h:27m:47s remains)
INFO - root - 2017-12-15 18:00:30.796675: step 41820, loss = 0.24, batch loss = 0.19 (12.4 examples/sec; 0.647 sec/batch; 52h:15m:24s remains)
INFO - root - 2017-12-15 18:00:37.403328: step 41830, loss = 0.20, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 51h:20m:20s remains)
INFO - root - 2017-12-15 18:00:44.010215: step 41840, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 54h:46m:21s remains)
INFO - root - 2017-12-15 18:00:50.682849: step 41850, loss = 0.21, batch loss = 0.16 (11.7 examples/sec; 0.681 sec/batch; 55h:00m:40s remains)
INFO - root - 2017-12-15 18:00:57.327590: step 41860, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 54h:21m:18s remains)
INFO - root - 2017-12-15 18:01:03.964026: step 41870, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 52h:41m:37s remains)
INFO - root - 2017-12-15 18:01:10.619056: step 41880, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 54h:07m:28s remains)
INFO - root - 2017-12-15 18:01:17.228884: step 41890, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 51h:39m:08s remains)
INFO - root - 2017-12-15 18:01:23.812221: step 41900, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 54h:18m:35s remains)
2017-12-15 18:01:24.444208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.6539993 -8.7151423 -8.0902367 -7.5022273 -7.2989941 -7.4674768 -7.4574413 -6.5287333 -6.6298404 -7.0782976 -6.6276608 -9.124198 -10.312456 -9.6971779 -8.9311028][-9.6551952 -10.297114 -10.161295 -9.7250023 -9.3253107 -9.6716022 -9.5875139 -9.0172062 -8.3613329 -7.8381424 -6.9736843 -9.0463037 -9.5185909 -9.4033623 -9.3070412][-7.1614285 -8.7444658 -10.097824 -8.9965744 -8.8038845 -8.96574 -8.7537155 -8.8016663 -8.8094444 -8.1116734 -7.8543997 -10.096165 -10.206148 -9.8350124 -9.6713638][-4.6563373 -5.7041192 -5.6895566 -6.0580115 -7.0806646 -7.0913267 -6.8516045 -7.613389 -7.8421707 -7.4519596 -7.31989 -10.039025 -11.181966 -10.626766 -9.9241829][-5.4196067 -5.3962693 -5.4272809 -4.1625147 -3.3280053 -2.4526043 -2.6938729 -3.7934515 -4.8308544 -5.6170745 -5.4895453 -8.4076338 -10.262675 -10.629418 -10.708303][-5.3632984 -5.2209435 -3.8215272 -2.8137965 -2.381146 -0.68074989 0.25787497 0.0019340515 -1.3045526 -3.0530622 -4.034111 -6.6581755 -8.72351 -9.7524014 -9.894268][-6.8951168 -6.3517222 -4.8872566 -2.606802 -0.76534653 0.9674201 1.786438 1.8596301 1.4879508 0.56425858 -0.3070426 -4.5146651 -6.8726969 -8.1736507 -9.5270977][-6.5148721 -5.6279035 -3.9731574 -1.6508746 0.026696205 2.126699 3.4735541 3.2545466 2.4363742 2.1617613 1.7241216 -1.8004875 -4.7325521 -6.0643058 -7.9953365][-4.6803408 -4.3455238 -2.8830755 -0.46213675 0.68761635 2.1435246 3.4176593 4.7423606 4.4390082 3.3138356 2.8420634 -0.62755632 -2.8935511 -5.552279 -7.2261033][-3.1360643 -2.9322205 -1.9347224 -0.25603151 0.90516424 0.9968524 1.4913979 2.2908664 2.7750144 3.2635961 2.8638549 -0.70456266 -2.8424051 -5.0248575 -6.717732][-6.2032952 -5.3407946 -4.2858858 -2.7926707 -2.2675662 -1.962157 -2.0794857 -1.6238256 -1.8651309 -1.1847324 -1.1238532 -3.8658514 -5.633604 -6.4265633 -6.7721987][-9.4267969 -8.8348837 -8.1175785 -6.4950461 -6.1107655 -6.136735 -6.4089618 -6.4624252 -6.4690261 -6.5515776 -5.7625875 -7.6048756 -8.6371794 -8.2114372 -7.4343753][-9.9727144 -9.5899963 -8.9381762 -8.2199745 -7.7615023 -7.3908949 -7.5185823 -7.7649012 -8.2604008 -8.2363825 -8.2788963 -9.2961655 -9.4430857 -9.7955132 -8.96986][-8.7998991 -8.9245815 -8.5555468 -7.4869642 -6.659585 -7.100174 -7.9646463 -7.7781792 -8.0586529 -8.3105631 -8.6081791 -9.31932 -8.8286848 -8.5110912 -8.7096872][-7.3628678 -7.4539886 -7.9638224 -7.2863603 -6.0613332 -6.2000628 -6.5646496 -6.6650605 -7.0209289 -6.5961285 -6.1968784 -7.6501341 -8.9759636 -8.7203693 -8.72357]]...]
INFO - root - 2017-12-15 18:01:30.987027: step 41910, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 52h:45m:19s remains)
INFO - root - 2017-12-15 18:01:37.516203: step 41920, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 53h:01m:25s remains)
INFO - root - 2017-12-15 18:01:44.144535: step 41930, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 52h:37m:38s remains)
INFO - root - 2017-12-15 18:01:50.777680: step 41940, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 53h:17m:41s remains)
INFO - root - 2017-12-15 18:01:57.292942: step 41950, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.643 sec/batch; 51h:51m:23s remains)
INFO - root - 2017-12-15 18:02:03.803778: step 41960, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 52h:46m:56s remains)
INFO - root - 2017-12-15 18:02:10.430012: step 41970, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 53h:46m:25s remains)
INFO - root - 2017-12-15 18:02:16.988702: step 41980, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 51h:36m:32s remains)
INFO - root - 2017-12-15 18:02:23.630439: step 41990, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 55h:10m:24s remains)
INFO - root - 2017-12-15 18:02:30.239243: step 42000, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 52h:08m:58s remains)
2017-12-15 18:02:30.782259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.75229 -5.5054231 -7.031384 -7.4198461 -8.4267654 -9.5172749 -9.5602608 -10.473213 -10.077211 -10.915535 -11.224681 -12.720654 -14.401627 -15.921778 -15.721064][-6.2243271 -6.3061709 -7.385076 -8.5849848 -10.083024 -11.901434 -12.238709 -11.767285 -11.353488 -11.233843 -10.85598 -12.851625 -14.733448 -16.433916 -15.150696][-5.1708789 -5.7919555 -8.5656157 -10.366615 -12.213953 -13.286486 -12.81834 -11.636103 -9.8494186 -9.6058626 -9.1749153 -11.396448 -12.80867 -14.954435 -14.851858][-3.6217484 -5.7234621 -7.3435855 -8.7972116 -11.173588 -11.018482 -9.2645235 -9.0181932 -8.071414 -6.5083666 -7.5983229 -10.587439 -11.733602 -14.587561 -14.423655][-5.873086 -7.4588246 -9.3267879 -8.9847488 -8.8083048 -6.6762776 -4.4377155 -2.9889069 -3.0964925 -3.7887888 -4.020606 -7.1066837 -11.315091 -14.334146 -14.266022][-5.4670649 -6.4666462 -7.4807539 -7.8764105 -7.6445246 -3.9610577 0.18603182 1.4690962 2.512773 0.15990973 -3.2265198 -5.6585994 -9.1275682 -13.894995 -15.209284][-4.9653554 -5.3224211 -5.6230254 -5.5894489 -4.389801 -0.63211012 3.0551572 6.2554059 8.6162033 5.4862876 1.4389367 -3.2476976 -8.3967638 -12.112753 -13.830311][-3.1263087 -4.0805283 -5.2562671 -4.7852378 -3.5315731 1.0347848 6.7675014 9.1899891 10.484913 8.2890835 5.7370572 -0.78224087 -6.3573346 -11.383911 -13.490371][-4.6677828 -6.599144 -7.4387379 -5.2985468 -3.7805972 -1.4543447 3.6376004 8.0597324 9.4060307 5.7780662 2.6484113 -1.7418573 -7.0605774 -11.927216 -13.356687][-5.3204789 -6.78265 -7.7198048 -6.6028466 -5.2975516 -2.7607155 -0.3620677 2.2068996 3.3022027 1.3862786 -2.0732164 -7.5841427 -11.517624 -15.310127 -16.386292][-6.9854221 -9.5597277 -11.313587 -10.514757 -10.34448 -7.9746704 -5.6313548 -5.166472 -6.1670108 -8.0833492 -10.544309 -14.652012 -17.756544 -18.705147 -17.620201][-11.379654 -11.62101 -11.762465 -11.807193 -11.294395 -11.005451 -10.490349 -10.895004 -13.038114 -14.771002 -16.818745 -19.432934 -20.85784 -20.266254 -18.125149][-14.507818 -14.259281 -13.157662 -12.006465 -12.048729 -11.254917 -9.7826777 -10.060633 -11.423325 -14.396845 -16.871822 -17.07443 -17.604082 -16.507965 -15.223946][-15.23469 -13.541185 -12.047884 -11.410082 -10.612003 -10.055199 -9.1762838 -8.3887949 -9.0710936 -10.938198 -11.991408 -11.961855 -11.923661 -11.355154 -11.164852][-11.966549 -12.209921 -12.231466 -9.869091 -8.4272938 -8.0331039 -7.6653309 -7.7490339 -7.9688916 -7.5557852 -7.8721685 -9.2831 -9.7885008 -9.5231323 -9.15019]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 18:02:37.402189: step 42010, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 53h:42m:39s remains)
INFO - root - 2017-12-15 18:02:44.040816: step 42020, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 53h:07m:34s remains)
INFO - root - 2017-12-15 18:02:50.645598: step 42030, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 53h:39m:48s remains)
INFO - root - 2017-12-15 18:02:57.203062: step 42040, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 53h:11m:55s remains)
INFO - root - 2017-12-15 18:03:03.897516: step 42050, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 52h:21m:49s remains)
INFO - root - 2017-12-15 18:03:10.540587: step 42060, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 52h:20m:40s remains)
INFO - root - 2017-12-15 18:03:17.077764: step 42070, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.633 sec/batch; 51h:04m:13s remains)
INFO - root - 2017-12-15 18:03:23.662921: step 42080, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 52h:31m:41s remains)
INFO - root - 2017-12-15 18:03:30.219062: step 42090, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 52h:44m:07s remains)
INFO - root - 2017-12-15 18:03:36.780046: step 42100, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 54h:46m:47s remains)
2017-12-15 18:03:37.306333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2100191 -4.3775291 -4.5045061 -5.0234466 -6.684763 -7.5738325 -7.6815672 -7.3421407 -7.2851753 -7.3579912 -6.1959047 -6.0041623 -7.3001328 -9.1884985 -9.388154][-5.068532 -5.0518088 -5.2712736 -6.0372682 -7.5562692 -9.1327534 -9.7449837 -9.7408247 -9.1792059 -8.3957787 -7.1155806 -7.1966386 -8.5804453 -11.329535 -11.573722][-3.770833 -4.6383705 -6.7153807 -8.0102587 -9.5495176 -10.795431 -11.449786 -10.492435 -9.8114128 -9.2998753 -8.2911263 -7.9820051 -9.0940323 -11.863825 -11.467068][-4.4523554 -4.9131546 -6.4339356 -8.9287214 -11.330156 -10.850295 -9.6531105 -9.3069267 -8.7885456 -8.4274464 -8.3164825 -8.53993 -10.457474 -13.146439 -12.729053][-5.5239162 -6.7588959 -9.15722 -10.012932 -10.466606 -8.680831 -5.605988 -5.1017547 -6.0610023 -6.0795808 -5.8384304 -7.3884358 -9.7898788 -13.520567 -13.738958][-6.4957676 -7.9394073 -9.1913414 -9.4360189 -8.61924 -3.341404 1.9609566 2.504683 0.92296219 -2.5367885 -4.6657944 -5.2759686 -7.9962459 -12.669338 -13.781782][-8.138999 -8.8334675 -9.6201439 -8.9976158 -6.1017275 -0.4501915 5.53223 8.8697014 7.8872304 1.6298375 -3.4098749 -4.9461026 -7.3378258 -11.315399 -11.933352][-8.6586733 -8.6010189 -8.5753956 -7.2279429 -4.1064067 2.3836265 8.6437874 10.786348 10.677935 4.819963 -1.4883046 -5.3289781 -9.2174883 -11.618803 -11.712736][-6.5642238 -7.0563879 -7.0539732 -6.0992918 -4.0450068 1.0189805 6.355701 8.9824581 8.1449814 3.9077182 0.14813805 -4.5173798 -9.2234554 -12.935493 -12.625096][-5.9752774 -6.1371908 -6.2315507 -5.4160185 -4.297205 -0.80744791 2.0352359 4.3459878 4.3023629 0.20290852 -3.226676 -6.2377605 -10.519508 -14.356895 -14.871862][-8.7208881 -9.1884422 -9.3114586 -7.4561057 -6.4521561 -6.0422549 -5.4431462 -3.9452 -3.7502306 -4.95284 -6.6541629 -9.9269428 -12.623475 -14.821911 -14.491287][-13.117583 -12.158472 -11.580107 -10.519932 -9.5706692 -9.4136457 -9.975729 -10.180971 -10.629588 -10.957443 -11.542379 -11.474554 -12.811655 -14.261261 -13.026037][-14.374672 -13.126256 -10.970961 -9.9496622 -10.482407 -10.406584 -10.692742 -11.482628 -11.791809 -11.326638 -10.941934 -10.846962 -11.111681 -11.73213 -10.041719][-11.010448 -11.644694 -10.250732 -8.93207 -8.7132158 -9.6647654 -10.314974 -9.8628635 -9.6625862 -9.600934 -9.5695152 -8.3102875 -7.4838529 -8.3382816 -7.90273][-7.6501746 -7.471025 -6.8382578 -5.5099587 -5.0873661 -6.1085877 -6.8377175 -7.0042453 -6.9349508 -7.1428146 -7.1434321 -7.6910124 -8.6067591 -7.6585507 -7.1028233]]...]
INFO - root - 2017-12-15 18:03:43.781924: step 42110, loss = 0.15, batch loss = 0.10 (12.8 examples/sec; 0.627 sec/batch; 50h:35m:14s remains)
INFO - root - 2017-12-15 18:03:50.298468: step 42120, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.634 sec/batch; 51h:07m:59s remains)
INFO - root - 2017-12-15 18:03:56.830058: step 42130, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 51h:59m:05s remains)
INFO - root - 2017-12-15 18:04:03.375434: step 42140, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 52h:19m:28s remains)
INFO - root - 2017-12-15 18:04:09.938662: step 42150, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 53h:50m:03s remains)
INFO - root - 2017-12-15 18:04:16.579524: step 42160, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 54h:42m:05s remains)
INFO - root - 2017-12-15 18:04:23.142805: step 42170, loss = 0.15, batch loss = 0.11 (12.7 examples/sec; 0.630 sec/batch; 50h:50m:44s remains)
INFO - root - 2017-12-15 18:04:29.699921: step 42180, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 51h:48m:28s remains)
INFO - root - 2017-12-15 18:04:36.324349: step 42190, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 54h:03m:48s remains)
INFO - root - 2017-12-15 18:04:42.914012: step 42200, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 51h:34m:42s remains)
2017-12-15 18:04:43.420930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4212594 -4.7715878 -4.7113256 -3.6867177 -3.7802191 -4.5393991 -5.0133276 -5.8364034 -6.7590694 -7.2021561 -7.2374458 -5.8922262 -5.667345 -6.9913468 -7.7777281][-5.6051273 -5.3499827 -4.8716211 -4.9721227 -5.4560618 -6.4887257 -7.2375631 -7.4446616 -7.5328674 -7.3995132 -7.7390141 -8.0329533 -9.4176178 -7.7442904 -5.8531356][-6.7208242 -6.3597379 -6.2573566 -5.6773267 -6.1335888 -5.8756866 -6.02355 -6.8835659 -7.0032845 -7.1608815 -7.2549734 -6.4848909 -6.869493 -7.3825212 -6.3250203][-4.0360746 -4.2000961 -3.853704 -4.5156803 -6.0743027 -7.0135846 -7.4149971 -7.688077 -6.8710794 -5.9290395 -5.83313 -5.3249416 -6.5629916 -6.5471635 -5.8793025][-5.4752197 -5.6520209 -6.0159287 -5.86757 -5.23977 -4.0636573 -4.1113791 -4.0677729 -4.4638777 -4.142971 -3.8843849 -3.8168945 -5.2349434 -5.9361973 -6.2374058][-6.4217887 -5.7013812 -4.4755707 -3.4455311 -3.0710073 -1.5928693 0.79330158 0.74040556 -0.15520382 -1.0535693 -1.49329 -1.9094408 -4.1503305 -5.4568467 -6.8640285][-6.4270024 -7.1063442 -5.5667863 -4.2338982 -2.7019558 -0.66892958 2.1735263 5.0844178 5.4499469 2.7284713 -0.16135073 -1.978235 -4.3666615 -5.6222973 -6.4406796][-6.2274833 -7.1628323 -6.0385962 -3.4362659 -1.003727 1.8818035 3.892467 5.0714755 5.4559655 3.44109 0.21672535 -2.7886219 -6.3109775 -6.1241918 -6.704648][-6.7669973 -7.8143959 -6.4026113 -4.37138 -2.4508703 1.0838118 2.860496 3.8368134 4.1155505 2.6575484 0.67800236 -2.344013 -5.6399174 -6.8073997 -5.9762745][-5.6784253 -5.807405 -5.0684233 -3.8308637 -2.8979394 -0.15328598 2.0084581 2.8318563 1.8482943 -0.57358313 -2.5885806 -3.3555243 -4.8082514 -6.347209 -8.7575617][-8.0462589 -7.1321745 -5.7337837 -4.6065803 -3.844569 -3.0015452 -2.954792 -1.9206872 -1.9082892 -3.5688329 -5.6112852 -6.1321149 -7.4060121 -5.6288934 -4.45609][-8.5031681 -8.9702291 -8.5934029 -7.4633265 -5.9481978 -6.3415351 -6.2015653 -5.9188356 -6.5434837 -7.2701054 -8.4197216 -9.0795164 -8.5355053 -7.9052372 -6.4479837][-10.591618 -10.062283 -9.177619 -8.0747881 -7.6946864 -6.85975 -7.6966205 -8.6718 -9.6440115 -9.8190937 -9.3975821 -10.064779 -11.066412 -9.0044489 -6.5077715][-7.548646 -8.1422386 -8.1285257 -8.0743837 -7.787262 -8.2158241 -7.7097692 -7.7479858 -7.9735432 -8.5889721 -8.9944372 -7.897016 -7.8131166 -8.6766462 -8.6335659][-5.1226649 -7.2495589 -8.1622915 -7.6849012 -7.3127451 -6.5116649 -5.8770361 -5.087297 -4.8166828 -5.6065469 -6.2843866 -7.0733709 -7.7447634 -8.0186653 -8.7176418]]...]
INFO - root - 2017-12-15 18:04:49.931898: step 42210, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 52h:29m:14s remains)
INFO - root - 2017-12-15 18:04:56.473331: step 42220, loss = 0.22, batch loss = 0.18 (12.6 examples/sec; 0.636 sec/batch; 51h:18m:45s remains)
INFO - root - 2017-12-15 18:05:03.028145: step 42230, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 52h:12m:19s remains)
INFO - root - 2017-12-15 18:05:09.593255: step 42240, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 51h:53m:27s remains)
INFO - root - 2017-12-15 18:05:16.213745: step 42250, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 53h:07m:51s remains)
INFO - root - 2017-12-15 18:05:22.794315: step 42260, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 54h:23m:40s remains)
INFO - root - 2017-12-15 18:05:29.338377: step 42270, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.634 sec/batch; 51h:06m:39s remains)
INFO - root - 2017-12-15 18:05:35.913294: step 42280, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 51h:16m:21s remains)
INFO - root - 2017-12-15 18:05:42.637401: step 42290, loss = 0.17, batch loss = 0.12 (11.4 examples/sec; 0.703 sec/batch; 56h:38m:34s remains)
INFO - root - 2017-12-15 18:05:49.197455: step 42300, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 52h:27m:11s remains)
2017-12-15 18:05:49.689482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7938738 -6.4732323 -5.9676938 -4.4059811 -4.2092977 -4.1082468 -4.0238056 -3.7866468 -3.7954831 -4.0585213 -4.5386925 -7.0330658 -8.42022 -10.128539 -9.3457212][-5.9450788 -4.4125972 -3.8611884 -2.9408929 -3.8214993 -3.9229636 -4.1671419 -4.1908116 -3.4865355 -3.0149426 -2.9438794 -6.0596948 -7.4651375 -9.0096626 -8.6215677][-3.7567594 -3.2822292 -2.79393 -1.9701614 -2.9022295 -2.8249209 -3.0154009 -3.0705504 -2.8753495 -2.7217054 -2.8363454 -5.459301 -6.2538881 -7.6955066 -7.3173308][-1.6449132 -1.3446431 -1.7038722 -1.0010986 -2.1239917 -1.7842615 -1.7326863 -1.9632881 -1.82657 -1.9451449 -2.2898753 -5.184988 -6.5050611 -7.6068492 -6.7166138][-2.1418042 -2.061661 -1.3257194 -0.32477236 -1.3094139 -0.53666878 -0.73378372 -0.77476358 -0.81216908 -1.105536 -1.3935194 -4.325139 -5.6452336 -6.765944 -6.2932067][-3.3911154 -2.8804235 -1.8902001 -0.29342794 -0.5334444 0.49322414 0.71911764 0.85133457 0.93336153 1.0888095 0.81003618 -2.3805487 -3.9318419 -5.28864 -5.154016][-4.2701745 -4.0322409 -2.9259548 -1.0298295 0.075316906 1.7553372 1.6914792 2.0507703 2.0798249 1.5936785 1.6018467 -1.3404627 -3.405587 -4.9679823 -5.2206607][-4.5131383 -4.376338 -4.1062922 -1.5817966 -0.02224493 1.1696029 1.1168013 1.3448491 1.2250357 1.5987835 1.7965865 -1.2431803 -3.5333955 -5.5683112 -5.9118834][-4.0054874 -3.510591 -3.77456 -1.5413365 0.042516232 1.1854739 0.80831957 0.98226929 0.55249786 0.77286148 1.5104866 -0.80477238 -2.9151974 -4.7755322 -5.6392589][-3.9565725 -3.8290424 -3.6805246 -2.192354 -1.114162 -0.093901634 0.26601505 0.0075383186 -0.87220716 -0.41044903 0.85352373 -1.4413466 -3.4892483 -5.2530322 -5.7421293][-7.9180079 -7.1601191 -6.4960876 -4.13234 -2.9742424 -2.4133978 -2.3560443 -2.1188889 -2.6024265 -2.7201951 -2.6521542 -3.4305623 -4.2204323 -5.5332952 -4.9254131][-9.320837 -8.1105213 -6.4428911 -4.9257617 -3.9135528 -3.5831287 -4.1874828 -4.3587837 -4.86136 -4.4260626 -4.4101214 -5.3749647 -5.3662062 -5.1802278 -3.8333232][-9.9055147 -8.751297 -6.513124 -4.62991 -3.7093868 -3.9337754 -4.3411322 -4.642921 -4.6675091 -4.2527614 -3.802464 -4.574636 -5.423656 -5.4404769 -3.8760481][-8.0438623 -8.45475 -7.38736 -5.6473866 -3.9445519 -3.2742181 -3.1169271 -3.0902297 -3.2486563 -2.6149974 -2.696341 -3.2520075 -3.4743397 -4.2114711 -3.9040027][-7.0278292 -7.9792085 -8.1117058 -5.9816723 -4.6625648 -3.4636912 -3.0989034 -2.504724 -2.7099514 -3.0033681 -2.9642518 -3.0479519 -3.4765167 -4.2542267 -5.0007019]]...]
INFO - root - 2017-12-15 18:05:56.303754: step 42310, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 52h:07m:45s remains)
INFO - root - 2017-12-15 18:06:02.863199: step 42320, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.673 sec/batch; 54h:13m:55s remains)
INFO - root - 2017-12-15 18:06:09.541111: step 42330, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 52h:27m:11s remains)
INFO - root - 2017-12-15 18:06:16.186270: step 42340, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 53h:02m:37s remains)
INFO - root - 2017-12-15 18:06:22.743923: step 42350, loss = 0.18, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 50h:56m:20s remains)
INFO - root - 2017-12-15 18:06:29.345389: step 42360, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 53h:37m:46s remains)
INFO - root - 2017-12-15 18:06:36.006495: step 42370, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.689 sec/batch; 55h:32m:52s remains)
INFO - root - 2017-12-15 18:06:42.637119: step 42380, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 52h:42m:16s remains)
INFO - root - 2017-12-15 18:06:49.215501: step 42390, loss = 0.11, batch loss = 0.07 (11.9 examples/sec; 0.672 sec/batch; 54h:11m:36s remains)
INFO - root - 2017-12-15 18:06:55.817978: step 42400, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 52h:25m:07s remains)
2017-12-15 18:06:56.352474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5885377 -3.5271893 -2.3149173 -1.4261527 -1.6067839 -1.0401525 -1.0329723 -0.56305122 -0.23015499 -0.052754402 -0.1651516 -3.3608994 -5.484158 -8.1218262 -8.2547379][-5.7489028 -5.0093689 -4.2922297 -2.5301702 -1.9277234 -2.1989231 -2.8263216 -2.9527681 -2.8749719 -2.0578275 -1.5661793 -3.8964989 -5.2073369 -7.3600726 -7.6576672][-4.4128728 -4.5135474 -3.9256988 -3.0658667 -3.0981331 -3.3727305 -3.7713506 -3.7603338 -3.5750339 -2.7859714 -2.4067872 -4.3461094 -5.5494208 -7.1232839 -7.3204212][-3.4760919 -3.0422554 -3.1509731 -3.0769088 -3.447356 -3.4378259 -3.3677783 -3.389385 -2.956991 -1.9504919 -1.5337477 -3.4844515 -4.5773554 -6.2821093 -5.8079438][-3.1396399 -3.477685 -3.1307371 -1.9616129 -2.2232845 -1.9480457 -1.8677597 -2.2461476 -2.826762 -2.0416081 -1.9006748 -3.7578397 -4.7767568 -6.2436171 -5.69511][-3.8199742 -2.9842498 -2.1091537 -0.37606812 -0.16313648 -0.31304455 0.1315279 -0.4947381 -1.2281346 -1.4125423 -2.0509803 -3.8257978 -4.5381136 -5.9602413 -5.5458817][-4.2412877 -3.2760355 -2.2252166 -0.20962763 0.33009195 2.0089369 2.9817452 1.8303666 1.2464652 -0.29262733 -2.2536404 -3.8899031 -5.2331996 -6.47719 -5.8965688][-5.4692326 -4.2139778 -3.1705287 -0.2934041 1.4429622 3.418941 4.1073966 3.4159312 3.6250329 1.4941902 -0.65129757 -3.8320093 -5.408237 -6.5253129 -5.9241357][-5.8649888 -4.798408 -3.8054521 -1.9657917 -0.30483246 2.741508 3.8213782 3.6197867 3.3857265 1.8586283 0.027686596 -3.0347178 -5.0984344 -6.2038765 -5.2564459][-5.3451972 -4.8311968 -3.939141 -2.2935617 -1.4640064 0.50921059 1.6323538 2.905066 3.5752788 1.7011142 0.36515379 -2.8745325 -5.4029031 -6.5610123 -6.3260951][-3.268539 -2.8959503 -2.5896511 -1.8069856 -1.3624234 -0.26545858 0.1097455 1.3707485 2.2667542 1.4084806 0.71094084 -2.2149804 -4.7305322 -6.8416696 -6.5417562][-7.9436545 -7.4249144 -6.9915419 -5.5145788 -4.999011 -4.9568396 -4.5882006 -3.3191669 -2.4443197 -2.7217965 -2.7949359 -4.2379413 -5.5102024 -7.5650153 -7.7538061][-10.751398 -10.170183 -10.184504 -8.6521444 -7.9967794 -7.5236616 -7.2980056 -6.6253023 -6.1109529 -5.74606 -5.1069994 -5.2221127 -5.5818839 -6.7852826 -6.15142][-9.6568756 -9.7652636 -8.9891291 -7.749989 -6.8852692 -6.678196 -7.0597343 -6.7422214 -6.4057393 -6.443675 -6.3194218 -5.6413841 -5.046052 -5.3449597 -5.0832567][-4.6964226 -4.6340404 -4.520977 -3.3552325 -2.6887677 -2.3964789 -2.6084359 -3.1862826 -4.0796881 -4.1914511 -4.1344113 -4.787796 -5.5130873 -6.15409 -5.9948664]]...]
INFO - root - 2017-12-15 18:07:02.930459: step 42410, loss = 0.30, batch loss = 0.26 (12.2 examples/sec; 0.654 sec/batch; 52h:41m:11s remains)
INFO - root - 2017-12-15 18:07:09.542807: step 42420, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 51h:43m:11s remains)
INFO - root - 2017-12-15 18:07:16.090772: step 42430, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 52h:46m:39s remains)
INFO - root - 2017-12-15 18:07:22.703474: step 42440, loss = 0.11, batch loss = 0.07 (12.3 examples/sec; 0.648 sec/batch; 52h:12m:13s remains)
INFO - root - 2017-12-15 18:07:29.222961: step 42450, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 52h:34m:41s remains)
INFO - root - 2017-12-15 18:07:35.780947: step 42460, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 53h:19m:37s remains)
INFO - root - 2017-12-15 18:07:42.328653: step 42470, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 52h:08m:39s remains)
INFO - root - 2017-12-15 18:07:48.919466: step 42480, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 51h:44m:56s remains)
INFO - root - 2017-12-15 18:07:55.544275: step 42490, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 54h:03m:35s remains)
INFO - root - 2017-12-15 18:08:02.195541: step 42500, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 54h:10m:39s remains)
2017-12-15 18:08:02.735656: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1429958 -6.4265585 -6.4950881 -5.8020144 -6.5269189 -8.0995407 -9.3584137 -9.9121037 -9.9985714 -9.7581959 -8.6095018 -8.7264023 -9.5349665 -10.436562 -9.8695717][-5.2428546 -5.2267413 -4.9134221 -4.3946171 -5.3567686 -6.7478576 -7.971839 -9.7757225 -11.36936 -11.046688 -8.9815969 -9.3399525 -10.013514 -11.003281 -10.703169][-1.9475183 -3.6668596 -5.9208808 -5.4276853 -5.7962527 -6.9552684 -7.9258304 -8.8319492 -9.4602871 -10.187456 -9.6313248 -9.1775951 -8.7091656 -9.9528351 -9.8249416][-3.042444 -4.102941 -5.4241819 -5.5355473 -6.7402215 -6.7971125 -7.0044084 -8.6441011 -10.09774 -8.9660778 -7.4557514 -8.5092735 -9.5198936 -10.096205 -9.4068375][-5.2423906 -7.0322218 -8.144371 -6.5684404 -4.9836521 -2.1881852 -1.1995158 -4.9491329 -8.3268738 -8.0120125 -7.1635752 -7.3784242 -8.3385143 -10.219305 -10.219481][-8.5989428 -9.6925573 -8.9372082 -5.6110678 -2.5028405 2.0050864 6.1856732 4.3140922 1.1324248 -3.0958915 -7.4842768 -7.3460097 -7.7187929 -9.4257832 -9.8390388][-10.322071 -9.8015137 -7.9564486 -3.6685154 -1.1756964 3.4068704 9.0780773 8.7781658 8.6722717 2.6154246 -4.6174464 -7.0423632 -9.6858139 -10.543464 -9.5513306][-11.104162 -10.338336 -8.9198532 -4.407414 -1.382616 4.8325429 9.8530769 8.1365261 8.3499928 3.6603236 -1.2786794 -5.4568944 -10.341462 -11.398438 -11.164297][-10.093111 -9.8384371 -9.1422272 -5.3852315 -2.7843723 2.1569505 5.4626346 5.6151052 6.195951 0.75276089 -3.6637762 -7.0074019 -11.477856 -12.662565 -11.774438][-9.7228041 -10.246431 -9.7185669 -6.2270713 -4.8377995 -2.441556 0.42160225 1.8261971 2.0653362 -2.0221922 -5.1254249 -8.7677116 -12.121888 -13.382185 -13.184602][-13.257114 -13.095441 -13.285616 -10.476418 -8.5185146 -7.5983934 -7.0745707 -6.5787868 -6.0477557 -7.4111223 -8.1670837 -11.822864 -13.806213 -13.811283 -12.51581][-19.193996 -18.980738 -17.100807 -14.449327 -13.991243 -13.30835 -12.786613 -12.657728 -12.425894 -12.782484 -12.491867 -13.810785 -13.842831 -13.487628 -11.767496][-19.381697 -18.903725 -18.317099 -16.631199 -16.252359 -14.619473 -13.942518 -14.387033 -14.631149 -13.905418 -12.823283 -12.837101 -12.403353 -11.619194 -9.4684076][-17.381577 -16.610233 -16.107889 -13.185511 -11.605579 -12.122059 -12.419061 -11.68643 -11.981861 -12.254809 -12.500704 -11.534561 -10.450261 -9.10194 -7.5948744][-12.798903 -11.856133 -9.7868032 -7.438345 -6.1305346 -5.8206763 -6.2354474 -7.0425062 -8.1904211 -7.9760771 -8.2477064 -9.1794329 -9.5024385 -8.7661314 -8.3758945]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 18:08:10.156821: step 42510, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 53h:49m:41s remains)
INFO - root - 2017-12-15 18:08:16.721388: step 42520, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 51h:25m:59s remains)
INFO - root - 2017-12-15 18:08:23.335265: step 42530, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 54h:25m:00s remains)
INFO - root - 2017-12-15 18:08:29.987046: step 42540, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.692 sec/batch; 55h:42m:21s remains)
INFO - root - 2017-12-15 18:08:36.568443: step 42550, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.684 sec/batch; 55h:03m:54s remains)
INFO - root - 2017-12-15 18:08:43.203864: step 42560, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 53h:51m:07s remains)
INFO - root - 2017-12-15 18:08:49.836425: step 42570, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 53h:20m:21s remains)
INFO - root - 2017-12-15 18:08:56.484370: step 42580, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 52h:04m:05s remains)
INFO - root - 2017-12-15 18:09:03.102784: step 42590, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 53h:03m:15s remains)
INFO - root - 2017-12-15 18:09:09.757369: step 42600, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 53h:25m:35s remains)
2017-12-15 18:09:10.265431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.490256 -9.9260759 -8.9186115 -6.9882216 -6.7930012 -4.9796443 -3.0592041 -1.5886631 -0.70586872 -1.4941859 -2.2509153 -3.4680526 -5.1386709 -6.3044124 -5.8201489][-7.7289534 -8.2300034 -7.5455861 -6.174562 -6.5924239 -5.5861382 -4.4151597 -3.5714335 -3.3648114 -2.8181641 -2.8056791 -3.8656292 -5.4609957 -5.9233727 -5.6305461][-4.7185497 -6.5885472 -7.7865796 -5.6723328 -5.9668226 -5.7754922 -5.2290554 -4.2385011 -4.30519 -3.9384439 -4.1847343 -5.8247495 -7.9841909 -7.7817221 -7.5483837][-4.9383526 -6.496151 -7.3011675 -5.9812536 -6.0831804 -5.7355251 -5.2800827 -5.0519481 -5.0850992 -4.7245851 -4.6034269 -6.7476444 -9.4281 -9.8169117 -9.52953][-6.374023 -7.8142552 -7.7012076 -6.0460453 -6.0482631 -5.5847583 -5.5447779 -5.3595953 -4.6645789 -3.8685312 -3.4560657 -5.8893814 -9.1768341 -10.772522 -10.985904][-9.3725433 -8.7813416 -7.4894867 -5.5414376 -4.2723942 -2.8594697 -2.2242756 -1.6388059 -1.1687522 -1.8743489 -2.9472263 -4.3528991 -7.2926264 -9.8970213 -10.855228][-9.61889 -8.4806967 -6.3455009 -3.5272322 -1.3565664 0.16180992 0.87278461 1.9438138 2.1269526 0.98597622 0.20252657 -2.1598811 -5.890563 -8.2064066 -10.247549][-8.27186 -7.0642905 -5.0900331 -1.7624919 0.44659853 1.8133745 2.4551787 2.042273 2.6970172 2.768023 2.0554681 -1.9523408 -6.2988935 -8.0892429 -8.9366713][-5.2415147 -5.1188736 -3.9002094 -1.4313951 0.43834686 1.3352308 1.6529388 1.2238207 1.5714149 3.7450633 4.9041562 0.38707066 -4.4708443 -6.9232073 -7.6180987][-4.64491 -4.8194127 -4.6524405 -2.1504774 -0.88504457 -0.33470917 0.033451557 1.0890632 2.1660123 3.519568 4.7930264 1.5561285 -2.9685788 -5.4136806 -6.223362][-9.13452 -8.08546 -6.7829 -4.6624746 -3.6630785 -3.3444934 -2.763113 -1.4622741 -0.60413218 0.73577595 1.6798391 -0.040938377 -3.263778 -5.183135 -5.7228065][-13.313156 -12.587274 -10.932566 -7.8750763 -6.151989 -5.2584291 -4.650939 -4.3577871 -3.5722895 -2.260294 -1.0093021 -3.3759315 -5.9002914 -6.0438013 -5.3194165][-13.837517 -12.891836 -10.605175 -8.9388056 -7.9325409 -6.4432106 -5.1718659 -5.6776977 -5.6890759 -5.3621006 -4.3247766 -5.7248454 -7.155582 -5.8378778 -4.6957436][-12.189882 -10.408741 -8.3441887 -6.8741474 -6.6443462 -6.64888 -6.7291446 -6.4497938 -6.0149126 -6.9610839 -7.5166254 -8.87826 -9.83069 -9.0306778 -7.367425][-7.6531324 -7.6474037 -6.9946008 -5.7028017 -5.5541177 -5.4329338 -6.29729 -6.7033753 -6.9149933 -7.4028587 -8.2414446 -10.31848 -12.818256 -12.152039 -10.299356]]...]
INFO - root - 2017-12-15 18:09:16.855885: step 42610, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 52h:54m:51s remains)
INFO - root - 2017-12-15 18:09:23.480047: step 42620, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 52h:57m:49s remains)
INFO - root - 2017-12-15 18:09:30.096698: step 42630, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 53h:56m:26s remains)
INFO - root - 2017-12-15 18:09:36.643228: step 42640, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 52h:40m:40s remains)
INFO - root - 2017-12-15 18:09:43.269252: step 42650, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.691 sec/batch; 55h:37m:03s remains)
INFO - root - 2017-12-15 18:09:49.886351: step 42660, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.689 sec/batch; 55h:27m:05s remains)
INFO - root - 2017-12-15 18:09:56.552402: step 42670, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 52h:41m:53s remains)
INFO - root - 2017-12-15 18:10:03.202350: step 42680, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 53h:36m:10s remains)
INFO - root - 2017-12-15 18:10:09.859029: step 42690, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 51h:33m:35s remains)
INFO - root - 2017-12-15 18:10:16.584856: step 42700, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 52h:11m:17s remains)
2017-12-15 18:10:17.068188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5788445 -0.97090721 0.40426302 1.7259679 1.5244212 1.243495 0.90439606 0.84328842 0.63543081 0.024697781 -0.64440393 -4.0449781 -4.9854031 -4.9177647 -4.3854847][-2.4819114 -0.91372347 0.11883354 0.96920252 1.0375609 0.58608532 0.21503448 -0.048615932 -0.2919879 -1.0412898 -1.8467791 -4.8980317 -5.4714761 -5.5518894 -4.4394512][-2.1322703 -1.5321784 -0.92909241 0.45575523 0.35489321 0.23172808 -0.11293125 -0.69469976 -1.2067122 -1.6026506 -2.2304766 -5.24762 -5.4946651 -6.0782208 -5.3331332][-3.1219115 -2.1437058 -1.3065796 -0.46732473 -0.75696325 -0.65946531 -0.76986694 -1.3111949 -1.8023875 -2.5641649 -3.0759487 -5.4954786 -5.2812252 -5.2743979 -4.8616037][-3.8613415 -3.2775414 -2.4788508 -1.0350142 -0.90915108 -0.26436329 -0.27852821 -0.79850674 -1.44169 -2.2942321 -2.8205149 -4.5794611 -3.5813646 -3.7973533 -3.5330153][-5.5988703 -3.8810523 -2.278743 -0.62565088 -0.17952204 0.5957613 0.62866116 0.39734983 0.14848471 -0.45944881 -0.73590803 -2.9067698 -2.0676665 -1.7448902 -1.6147146][-6.8920259 -5.1927905 -3.2323248 -1.2428913 -0.26209259 0.55779123 0.61179543 0.7273674 1.0001154 0.56725168 0.32645559 -1.4449048 -0.70153 -0.75270128 -0.52891636][-7.7897949 -5.9463844 -3.7688346 -1.5146551 -0.53547573 0.30574942 0.38796806 0.42455816 0.81971216 1.3218579 1.7919459 -0.72125959 -0.4373064 -0.93719244 -1.5719042][-7.5976162 -6.18093 -4.1460705 -1.8291321 -0.67169571 -0.12857103 -0.20779848 -0.17377901 0.1603241 0.6910634 1.5109463 -0.64282703 -0.43646765 -1.2976227 -2.1233099][-6.9140182 -6.0146112 -4.4179945 -2.3140926 -1.1224394 -0.20437765 -0.34995651 -0.60600758 -0.30837774 0.2977066 0.90377426 -1.233705 -1.0591726 -2.8812523 -4.5157857][-10.193403 -8.9585972 -6.988203 -4.903101 -3.8001852 -2.5830667 -2.5356791 -2.6584196 -2.6283638 -2.173485 -1.3514204 -3.9584916 -4.4566956 -4.7058153 -4.8861089][-13.512488 -12.072941 -10.165483 -8.2311592 -7.0568013 -5.6049361 -5.6704807 -5.6054125 -5.2271433 -5.0003057 -4.6342373 -6.16403 -5.7704911 -6.1155825 -6.881423][-13.792318 -13.107846 -11.673187 -9.831995 -9.129302 -7.67571 -7.535491 -7.4316607 -7.3550949 -7.1170578 -6.5063195 -7.2414012 -6.8776207 -6.1099968 -5.3486271][-10.443232 -10.005018 -9.340169 -8.18983 -6.9609561 -6.5125184 -6.7964706 -6.8420777 -6.7266173 -6.9713812 -6.8112297 -5.9224167 -4.4252186 -4.3135538 -3.8940029][-5.4986796 -4.5571551 -4.3812356 -3.9683728 -2.6981444 -2.9420395 -2.9333751 -2.8832314 -3.4934204 -3.8478706 -3.6530108 -4.4107847 -4.630352 -4.516696 -4.7951751]]...]
INFO - root - 2017-12-15 18:10:23.633672: step 42710, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 52h:59m:13s remains)
INFO - root - 2017-12-15 18:10:30.263540: step 42720, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 54h:41m:13s remains)
INFO - root - 2017-12-15 18:10:36.852085: step 42730, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.691 sec/batch; 55h:37m:35s remains)
INFO - root - 2017-12-15 18:10:43.526584: step 42740, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 53h:42m:58s remains)
INFO - root - 2017-12-15 18:10:50.159500: step 42750, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 51h:31m:28s remains)
INFO - root - 2017-12-15 18:10:56.662734: step 42760, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 54h:06m:24s remains)
INFO - root - 2017-12-15 18:11:03.165969: step 42770, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.661 sec/batch; 53h:10m:41s remains)
INFO - root - 2017-12-15 18:11:09.871012: step 42780, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 53h:05m:26s remains)
INFO - root - 2017-12-15 18:11:16.463613: step 42790, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 53h:02m:41s remains)
INFO - root - 2017-12-15 18:11:23.053496: step 42800, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 54h:42m:38s remains)
2017-12-15 18:11:23.568141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8974574 -2.0805154 -2.348254 -1.7663445 -1.6612301 -1.801677 -2.2630413 -2.0684032 -1.4855914 -1.069653 -0.71967983 -2.8877983 -5.5302539 -8.0111675 -8.5907364][-2.2130063 -1.7049103 -1.7463174 -2.4281497 -3.1011896 -4.3597317 -4.4307084 -4.750895 -4.2852459 -3.5076036 -2.5059288 -3.1278627 -5.6662445 -7.2325621 -8.2969189][-2.1901267 -2.6196182 -3.2009535 -3.5909336 -4.8136716 -6.0051494 -5.9245262 -4.8244076 -4.4673409 -4.5604043 -4.3292861 -4.3279371 -6.4393654 -8.0730543 -8.4468756][-2.607789 -2.6018441 -2.6542211 -3.0135055 -3.7245507 -4.8139935 -4.7250514 -4.5289245 -4.5464087 -4.9109459 -6.4748406 -8.01943 -9.3330393 -9.8477287 -10.028498][-3.2241819 -3.2531238 -3.1323001 -3.1390629 -2.8972547 -2.3665359 -1.9381843 -2.527458 -3.4205048 -3.963891 -5.6103296 -8.2407532 -11.159209 -11.807749 -11.294311][-3.558202 -3.1139574 -1.9796753 -1.458539 -1.2237592 -0.36858177 1.4349289 1.3237042 0.23550129 -2.9336939 -5.7080731 -7.5477982 -10.313395 -11.794722 -10.739378][-4.1895847 -4.2415853 -3.2689571 -1.7643061 -0.5535183 1.2401667 3.6810918 4.100781 4.6770186 2.6396575 -0.90505981 -3.4485238 -6.8109264 -8.2970257 -8.7032566][-2.6591876 -2.7418396 -2.4744191 -1.9105675 -0.91196251 1.1904407 2.993814 4.4697118 4.8789849 3.5659165 1.4819479 -1.3852582 -4.6886568 -4.7299943 -4.6486378][-0.62401724 -1.6041131 -1.6829491 -1.6117716 -1.444737 0.12070227 2.0209026 3.4278274 3.8566709 2.5762239 1.8532267 -0.60152292 -4.6855669 -5.6029224 -5.0781155][0.14883089 -0.44701719 -0.38672829 -0.36999464 -0.67843676 -0.41261959 0.54304934 0.64384985 0.6791544 -0.081655025 -0.39801836 -2.4057281 -6.0076056 -6.5464172 -7.09336][-2.8932493 -2.5379789 -1.2001929 -0.75570726 -0.71165562 -0.44566059 -0.42590809 -1.5418186 -3.8350945 -6.915679 -8.2624083 -9.2108269 -9.6577454 -8.4550714 -7.1403337][-7.7064505 -6.0896273 -4.0345917 -2.2113898 -1.1576834 -1.00387 -2.2147894 -4.4633222 -7.0429773 -9.9381218 -11.998175 -13.463891 -12.898626 -11.837748 -9.5196877][-10.767642 -8.3858347 -5.4638867 -3.0697756 -2.2249651 -1.5651851 -2.84587 -4.9240513 -8.2697372 -10.967667 -12.054194 -12.359312 -11.862312 -10.90769 -8.9849234][-12.238263 -10.890448 -7.5456562 -4.8237128 -3.7191262 -2.8736718 -3.3390732 -3.5953999 -4.8308158 -6.9439869 -8.9657688 -9.7642336 -9.1627674 -9.0731068 -7.4030228][-8.5999966 -8.2097807 -7.4428387 -6.5356193 -5.71317 -4.7388668 -4.5238371 -3.8858662 -3.8586626 -3.6554208 -5.5285478 -6.3179746 -6.68548 -6.9776821 -6.2492433]]...]
INFO - root - 2017-12-15 18:11:30.161579: step 42810, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 53h:04m:19s remains)
INFO - root - 2017-12-15 18:11:36.753947: step 42820, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 53h:01m:14s remains)
INFO - root - 2017-12-15 18:11:43.301731: step 42830, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 53h:25m:16s remains)
INFO - root - 2017-12-15 18:11:49.918117: step 42840, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 52h:47m:26s remains)
INFO - root - 2017-12-15 18:11:56.469338: step 42850, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 53h:20m:54s remains)
INFO - root - 2017-12-15 18:12:03.056895: step 42860, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 52h:22m:13s remains)
INFO - root - 2017-12-15 18:12:09.724026: step 42870, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.671 sec/batch; 53h:57m:34s remains)
INFO - root - 2017-12-15 18:12:16.283401: step 42880, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 54h:39m:08s remains)
INFO - root - 2017-12-15 18:12:22.862117: step 42890, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 52h:45m:08s remains)
INFO - root - 2017-12-15 18:12:29.424553: step 42900, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 51h:59m:36s remains)
2017-12-15 18:12:29.885929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5224457 -3.73495 -2.0944221 -1.4301744 -1.0659909 -1.8700182 -2.3341868 -2.6936665 -2.3790298 -2.0885541 -1.2647343 -3.968518 -5.741004 -6.6656213 -7.02821][-3.9623907 -2.4956772 -0.7086215 -0.25612783 0.695106 -0.1398344 -2.056721 -3.4833879 -4.9837289 -5.4240746 -4.8273687 -6.1968832 -6.7114863 -7.1357408 -6.4567118][-2.3803563 -2.4720218 -1.2093015 0.020503998 0.72417021 0.35427904 -0.71359968 -1.9451816 -3.6330125 -5.1955256 -6.1393385 -8.9020472 -9.7127037 -9.4650593 -8.5421429][-1.4340858 -1.5886974 -1.4072828 -0.92774487 -0.17939806 -0.22452879 -0.20198727 -1.0932322 -2.6305559 -2.7987995 -3.0923748 -6.4748993 -8.862071 -10.264039 -9.3404789][-2.3597736 -2.793129 -2.7860711 -1.7715633 -0.29483366 1.0264931 1.2587972 0.14854717 -1.404511 -2.4497654 -2.959728 -6.3813138 -8.7230892 -9.9216 -10.263602][-3.4402764 -3.3047545 -2.8030198 -1.3177457 0.14272881 2.4253869 3.2509027 2.8337226 2.0094457 -0.16172218 -1.7083783 -4.9227886 -7.0088162 -8.5412407 -9.344594][-4.79799 -4.0558796 -2.7338841 -1.4291229 0.37137032 3.1103673 4.0808864 4.5476537 4.3486323 2.0504465 0.60031509 -3.1626809 -5.8658381 -7.580163 -8.8562174][-6.9385014 -5.7293634 -2.7614398 -1.0627499 0.019594193 2.4884276 3.6888518 3.9287772 3.9427 3.207829 1.3913894 -2.9587729 -6.5332804 -8.0391788 -8.0796309][-7.0078793 -5.6696348 -3.6809695 -2.1227982 0.33604479 1.6955161 2.8715234 3.3973804 4.3414235 4.3959422 3.4881482 -1.4342575 -5.4708104 -7.4707985 -8.3125181][-7.4280906 -6.5338435 -4.2173152 -2.0619819 -0.35853815 0.38790035 1.5365114 3.2533669 4.2733197 4.4975019 4.4177556 -0.62162638 -5.3925571 -7.287169 -7.8979139][-11.551601 -10.213638 -7.3849916 -4.5556884 -2.978348 -2.9966774 -2.0180159 -0.453269 1.1054797 1.6770673 1.2795658 -2.4886072 -6.0590711 -8.2191305 -8.3741531][-13.284369 -11.145115 -8.503706 -6.2539973 -4.5166249 -3.9894884 -3.9227338 -3.2224936 -2.3950143 -1.4147892 -0.99969053 -2.9829021 -6.3820338 -8.477396 -8.4535875][-11.42146 -9.2723818 -7.0345607 -5.0019855 -4.9175687 -4.5777521 -4.3409858 -4.3604126 -3.747972 -2.8965476 -2.9871433 -3.6709259 -5.6635604 -7.5278177 -7.8387556][-9.6470909 -8.6365566 -7.9700584 -5.8255839 -5.2003269 -5.6850429 -4.594059 -3.6819477 -3.0602391 -3.567565 -4.0337582 -4.6026154 -5.8383527 -6.4864254 -6.821949][-7.0306044 -6.6052141 -5.9399338 -5.5569963 -6.2244563 -5.5747232 -4.7475848 -4.8696642 -4.031518 -3.6433394 -4.0699263 -6.4036579 -9.1437635 -9.7412529 -10.023794]]...]
INFO - root - 2017-12-15 18:12:36.475182: step 42910, loss = 0.30, batch loss = 0.25 (12.4 examples/sec; 0.647 sec/batch; 52h:01m:36s remains)
INFO - root - 2017-12-15 18:12:43.088462: step 42920, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 52h:36m:17s remains)
INFO - root - 2017-12-15 18:12:49.632710: step 42930, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 54h:31m:08s remains)
INFO - root - 2017-12-15 18:12:56.243952: step 42940, loss = 0.27, batch loss = 0.23 (12.1 examples/sec; 0.662 sec/batch; 53h:15m:12s remains)
INFO - root - 2017-12-15 18:13:02.864734: step 42950, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 53h:43m:59s remains)
INFO - root - 2017-12-15 18:13:09.447833: step 42960, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 53h:58m:10s remains)
INFO - root - 2017-12-15 18:13:16.017594: step 42970, loss = 0.25, batch loss = 0.21 (11.6 examples/sec; 0.687 sec/batch; 55h:16m:09s remains)
INFO - root - 2017-12-15 18:13:22.611474: step 42980, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 53h:25m:34s remains)
INFO - root - 2017-12-15 18:13:29.290819: step 42990, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 52h:24m:14s remains)
INFO - root - 2017-12-15 18:13:35.796313: step 43000, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 52h:56m:27s remains)
2017-12-15 18:13:36.289661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.5045948 -9.1692381 -8.51195 -8.0369587 -8.4490852 -8.26981 -7.9997673 -7.6208673 -6.8583469 -5.93315 -5.3229136 -7.7879457 -9.1087837 -10.551094 -10.773741][-9.10193 -10.34317 -11.288913 -10.516127 -9.9526491 -9.5446043 -9.535285 -9.4548035 -9.1443453 -8.63036 -7.6706381 -9.3433 -10.4538 -11.376594 -10.885731][-7.5268736 -9.3470306 -10.355429 -9.3237066 -9.0733776 -8.5124454 -8.4441156 -8.9046955 -8.9063625 -8.7108679 -8.4721718 -10.22893 -10.830513 -11.34927 -11.01844][-5.191153 -6.6376271 -6.9262738 -6.536274 -7.0019164 -6.5245185 -5.9337773 -6.7480927 -7.6014886 -7.8213863 -7.9033551 -11.063072 -12.118211 -12.316572 -11.795944][-5.9785776 -5.7828822 -6.5633774 -5.6171222 -4.3031368 -3.2341294 -2.8313107 -3.0808783 -3.5711281 -4.3877411 -5.0965176 -8.71456 -10.612566 -11.849447 -11.70331][-6.4366536 -6.7977843 -5.6389003 -4.5756826 -3.9782672 -2.0115778 -0.52614021 0.35889673 -0.001768589 -1.6063251 -2.6935961 -6.3001657 -8.7570915 -10.62019 -9.973423][-7.2877069 -7.6439743 -6.76966 -4.6434712 -2.1989825 0.45163393 1.4175596 2.0664353 2.1872945 1.0033803 -0.0090045929 -4.230792 -6.8105278 -8.422677 -8.9682665][-6.7904048 -6.4144506 -4.8227177 -2.2309299 -0.33703375 2.5144067 3.9112811 4.0703435 3.774179 2.8181481 1.6413693 -2.5559716 -5.2639484 -7.1529264 -7.7248735][-5.0576372 -5.2013669 -3.5011063 -1.2761583 0.84158945 3.1652827 3.7902513 4.3357024 4.6975055 3.3773665 2.5797076 -0.60106516 -3.3140047 -6.3868279 -7.1036139][-5.7362614 -5.0919528 -4.1552434 -1.7798977 0.38603449 1.9734602 1.9288626 2.4562497 2.4038739 2.7964692 2.6217217 -1.4600139 -3.3816786 -4.7873607 -5.6787848][-8.3346453 -7.303978 -6.0579343 -4.7411485 -3.1767435 -1.2475681 -1.3809204 -1.8276305 -1.4127841 -0.97772312 -0.5450592 -3.5753222 -5.8122449 -6.6183524 -5.97004][-10.264492 -9.6492777 -8.3605022 -7.1044359 -6.3105245 -5.0952892 -5.4866233 -5.0667605 -4.8924522 -4.7818 -4.2298651 -6.2308159 -7.6317282 -9.5803795 -8.1253452][-11.620007 -10.177294 -8.9494381 -8.18596 -7.5781274 -5.5096917 -5.4080029 -6.5159225 -6.8484221 -6.4788694 -6.1575046 -7.0733809 -8.4495049 -9.8162231 -8.2335138][-9.6362438 -9.3805676 -7.9463086 -6.5536594 -6.575315 -6.6039343 -6.4905434 -5.811841 -5.7170506 -6.2441778 -6.7474356 -7.0058212 -6.9953804 -7.9761767 -7.6792417][-8.9810381 -8.6680727 -8.1396446 -7.077311 -6.2192273 -5.5397425 -5.8050647 -5.5731449 -5.756453 -5.800693 -5.7556734 -7.0643559 -8.7459488 -8.2458067 -7.3013778]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 18:13:42.897680: step 43010, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 52h:25m:23s remains)
INFO - root - 2017-12-15 18:13:49.511732: step 43020, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 53h:52m:53s remains)
INFO - root - 2017-12-15 18:13:56.129564: step 43030, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 53h:17m:56s remains)
INFO - root - 2017-12-15 18:14:02.735849: step 43040, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 52h:56m:51s remains)
INFO - root - 2017-12-15 18:14:09.382491: step 43050, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 53h:09m:44s remains)
INFO - root - 2017-12-15 18:14:16.019280: step 43060, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 55h:03m:34s remains)
INFO - root - 2017-12-15 18:14:22.669117: step 43070, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.692 sec/batch; 55h:39m:52s remains)
INFO - root - 2017-12-15 18:14:29.237646: step 43080, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.630 sec/batch; 50h:40m:19s remains)
INFO - root - 2017-12-15 18:14:35.787204: step 43090, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.630 sec/batch; 50h:39m:12s remains)
INFO - root - 2017-12-15 18:14:42.302815: step 43100, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 51h:17m:59s remains)
2017-12-15 18:14:42.783986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0442381 -7.8811612 -9.2423992 -9.8890057 -11.029839 -11.118553 -10.255272 -9.70369 -8.9960623 -8.93524 -8.93005 -8.8897448 -10.653563 -9.4316654 -8.3371754][-7.0088921 -9.6936045 -10.929693 -11.457594 -12.288877 -12.559058 -13.194358 -12.292042 -10.910852 -9.42585 -8.2643909 -9.1564331 -12.227647 -12.068039 -10.798145][-5.9077439 -8.5119886 -10.703984 -11.885666 -12.549924 -13.217089 -13.137384 -12.841267 -11.610485 -9.6454544 -8.0696859 -8.4204721 -10.550172 -11.648515 -11.837547][-7.5008845 -9.1077118 -10.566396 -11.284733 -11.379135 -10.733958 -10.850089 -11.648264 -11.03023 -9.5431757 -8.6194954 -8.9094353 -12.030001 -12.472461 -11.795113][-8.1171341 -11.080271 -12.982594 -12.404428 -10.316137 -4.8315969 -1.7182949 -4.8500748 -9.0455856 -8.5890417 -7.2348518 -7.5070019 -10.65796 -12.411463 -13.69081][-8.0154037 -10.873056 -11.510323 -10.834793 -8.476038 -1.0341773 7.0653806 6.2196908 1.534585 -3.4306946 -6.0717883 -4.8430581 -6.738348 -8.7767105 -11.512845][-7.8268304 -11.880535 -12.274406 -8.239502 -4.0800815 2.1387067 7.640059 9.8815422 8.5579681 1.1668239 -5.8565388 -5.1027379 -6.3470831 -7.8777714 -10.299099][-5.5595846 -8.9659185 -11.357606 -9.1322393 -3.470077 2.8803706 7.7699723 10.303127 9.7705841 3.3461785 -3.5954432 -5.9532127 -9.3434076 -8.9448624 -8.6738768][-3.7842016 -5.3700933 -7.1114831 -7.7610903 -5.1206155 -1.0091014 4.7131543 7.0527787 6.536036 1.5395823 -2.7862728 -6.8863726 -12.428156 -13.014367 -13.801767][-3.1606438 -4.7639318 -7.0060644 -7.7013626 -6.4081025 -3.0889769 0.61367512 2.291059 1.6527719 -1.0503726 -4.5574503 -7.8416185 -12.88591 -15.725519 -17.853905][-6.3379574 -7.669342 -9.4545441 -10.419054 -11.100863 -9.9039164 -8.3873882 -7.1356821 -6.6751671 -7.877327 -9.8048105 -11.073394 -12.165087 -14.088154 -15.913418][-10.42992 -10.499989 -10.854214 -10.353576 -11.214523 -11.247249 -11.405855 -11.859035 -12.306869 -12.770023 -12.937365 -13.208872 -14.050201 -13.36352 -12.928369][-8.2679 -9.3172741 -10.220609 -9.2333765 -9.4534721 -9.630475 -10.437944 -10.952826 -11.645894 -11.553977 -11.166107 -11.461069 -11.655277 -11.323399 -9.872612][-6.7549 -6.991334 -7.2948875 -7.8760223 -9.0867481 -9.22425 -9.3244333 -9.5793791 -9.6732883 -10.220347 -10.754257 -9.8810749 -9.3807793 -8.5167456 -8.51207][-4.8078642 -5.4535017 -4.4919858 -3.9250455 -4.7121811 -5.8402476 -7.2527108 -8.0588827 -7.1910939 -7.0832791 -7.4192338 -8.2883453 -9.0902147 -9.0097914 -7.5486083]]...]
INFO - root - 2017-12-15 18:14:49.272133: step 43110, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 51h:55m:13s remains)
INFO - root - 2017-12-15 18:14:55.831539: step 43120, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.629 sec/batch; 50h:31m:42s remains)
INFO - root - 2017-12-15 18:15:02.320278: step 43130, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.656 sec/batch; 52h:43m:10s remains)
INFO - root - 2017-12-15 18:15:08.922797: step 43140, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 53h:00m:56s remains)
INFO - root - 2017-12-15 18:15:15.600544: step 43150, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 54h:30m:38s remains)
INFO - root - 2017-12-15 18:15:22.123291: step 43160, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 52h:19m:21s remains)
INFO - root - 2017-12-15 18:15:28.674473: step 43170, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 53h:21m:30s remains)
INFO - root - 2017-12-15 18:15:35.288109: step 43180, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 52h:44m:36s remains)
INFO - root - 2017-12-15 18:15:41.839991: step 43190, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 52h:14m:47s remains)
INFO - root - 2017-12-15 18:15:48.553955: step 43200, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 53h:38m:35s remains)
2017-12-15 18:15:49.049009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4698691 -5.9029841 -4.6878605 -3.7482975 -4.1054974 -4.1556978 -4.649209 -4.4452987 -3.9228287 -3.6368802 -2.3500366 -2.1092019 -1.3903608 -3.2809043 -3.8333344][-5.6814423 -5.0294385 -3.8075657 -2.6619537 -2.8424177 -3.4990268 -3.3537636 -2.4754612 -2.0268228 -2.0503919 -0.26298809 -0.080690861 0.21029043 -2.21943 -2.9995925][-3.4056535 -3.6225934 -3.7088289 -2.8251116 -3.2062838 -3.7713006 -3.6108339 -2.1610739 -1.0673962 -1.4102626 -0.40984917 -0.23795271 0.85014582 -1.7116199 -3.4099588][-5.5666056 -5.9605064 -6.0778775 -5.7990742 -5.9731936 -4.5423827 -3.1869724 -2.5984285 -2.0131769 -1.3625765 -0.79158449 -1.2627358 -1.1177359 -3.2083416 -3.7701943][-5.349834 -6.8139715 -7.0923696 -5.399766 -4.9886608 -2.6613762 -0.67203856 -0.30366135 -0.30831432 -0.79842043 -1.1260266 -2.1594546 -2.2195139 -4.4488158 -5.1875577][-6.4534373 -6.6843724 -7.003119 -5.5080256 -3.2780795 1.0713067 4.443707 3.7432179 1.763351 -0.62579441 -2.1201286 -3.1465507 -3.1657794 -5.52442 -5.9592109][-8.5853529 -7.3447604 -4.9954624 -2.1623361 -0.29991961 4.0170293 8.3849487 7.3708863 4.3850818 0.6901741 -1.2170887 -2.9657869 -3.266618 -5.1534948 -5.4356937][-9.4591551 -8.9326429 -7.7876825 -2.7020588 1.3771849 5.26738 8.6969566 8.006628 5.1714606 0.68972731 -2.9207289 -4.4347491 -3.4730408 -5.5153379 -6.8386621][-10.733934 -10.250582 -8.5491486 -4.7465296 -1.6580429 2.4216266 5.7815394 4.4514661 1.2904935 -1.3172622 -3.5730374 -6.427115 -6.1374564 -6.7844658 -6.5296483][-9.5900679 -10.15495 -9.63636 -6.4752564 -4.8111258 -1.0817723 2.3005557 1.6808591 -0.95915794 -4.1991863 -6.8577852 -8.3147392 -7.2711992 -8.9626522 -8.7609673][-12.59606 -12.641918 -11.787483 -9.5231953 -8.1675377 -7.0054851 -4.8505964 -4.1689396 -5.2706542 -6.9306774 -8.4689217 -10.6814 -10.265636 -10.31254 -9.0944519][-14.25037 -14.531965 -13.851561 -11.87894 -10.59767 -9.8298512 -8.6481991 -7.9913349 -7.9865909 -9.0822468 -10.293552 -10.80372 -10.067141 -9.731144 -8.358551][-12.580076 -12.229914 -11.572317 -10.057285 -9.1097879 -8.1942673 -7.3539395 -7.462523 -8.0782042 -8.1951866 -7.7517877 -8.2652693 -8.3008509 -8.2414837 -7.5640445][-12.426929 -12.033201 -11.769178 -10.723091 -9.1729164 -8.3265362 -7.7330484 -6.5777025 -6.4331951 -7.0527105 -7.4698658 -6.8898287 -5.6462984 -5.2831593 -4.5826769][-8.7771053 -8.9022493 -8.6527338 -7.3631744 -6.3564911 -5.0488162 -3.9416819 -3.7446589 -4.2624345 -4.03681 -3.9758639 -4.8262277 -5.5182743 -6.5268078 -6.910634]]...]
INFO - root - 2017-12-15 18:15:55.677693: step 43210, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 52h:57m:46s remains)
INFO - root - 2017-12-15 18:16:02.289178: step 43220, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 51h:46m:59s remains)
INFO - root - 2017-12-15 18:16:08.877673: step 43230, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 52h:36m:43s remains)
INFO - root - 2017-12-15 18:16:15.489592: step 43240, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 52h:11m:41s remains)
INFO - root - 2017-12-15 18:16:22.036442: step 43250, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 52h:30m:56s remains)
INFO - root - 2017-12-15 18:16:28.644149: step 43260, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 52h:13m:19s remains)
INFO - root - 2017-12-15 18:16:35.298112: step 43270, loss = 0.18, batch loss = 0.14 (11.4 examples/sec; 0.699 sec/batch; 56h:08m:50s remains)
INFO - root - 2017-12-15 18:16:41.852130: step 43280, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 51h:50m:41s remains)
INFO - root - 2017-12-15 18:16:48.446723: step 43290, loss = 0.24, batch loss = 0.19 (12.0 examples/sec; 0.668 sec/batch; 53h:38m:31s remains)
INFO - root - 2017-12-15 18:16:55.035374: step 43300, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 51h:50m:46s remains)
2017-12-15 18:16:55.485603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1615009 -4.9341049 -4.8807764 -4.361836 -4.6473236 -5.612422 -6.3735166 -6.9117689 -7.0843177 -7.4058809 -7.389658 -6.6497641 -7.0593238 -9.8288307 -9.2962008][-7.966258 -7.0214868 -6.5469456 -6.7710586 -7.1863236 -7.9188623 -8.7775784 -8.54808 -8.4757328 -7.4120555 -7.4547844 -7.8454933 -9.6489086 -11.12196 -8.76194][-7.3142648 -7.547709 -8.5344687 -8.2073841 -9.1079941 -8.806366 -8.7427673 -8.6543293 -8.0256176 -7.9024086 -8.1551781 -6.4880505 -6.5372763 -7.4422617 -5.4559488][-4.97881 -4.9570971 -5.7165604 -7.6064692 -8.7958479 -9.069355 -9.0796881 -9.0020447 -7.853189 -6.394784 -6.0653195 -6.0671186 -7.254828 -7.5426178 -6.0485077][-6.3286371 -6.1875815 -6.5481987 -6.29555 -6.37435 -5.486928 -4.7774296 -4.8925128 -5.0696859 -4.2568913 -4.5871124 -3.0105619 -4.855341 -6.2729836 -5.82751][-7.2697191 -5.7517614 -4.9381614 -4.0790167 -3.7109404 -1.1670613 1.7100921 1.7840796 1.4044967 -0.52980661 -1.7133021 -1.8646128 -3.6042407 -6.2194905 -6.9240971][-6.804131 -7.4175878 -6.79128 -4.7407026 -2.4488685 0.3986702 4.5567203 6.476295 6.5344853 2.638185 -0.723259 -1.532424 -4.315259 -6.5121336 -6.536943][-7.0511184 -7.5787249 -7.3412824 -4.9317126 -2.3106499 2.6568046 6.1086364 8.6996841 9.3275528 5.2701335 0.81361771 -2.670531 -6.6612806 -8.0160618 -7.7463026][-6.9506769 -7.6339412 -7.211668 -5.2325439 -3.2409577 1.4910431 4.7727056 6.2254939 5.5256991 2.5977073 -1.1722488 -3.7609925 -7.1008396 -9.5817518 -8.4929857][-5.7961984 -6.2083864 -5.7453723 -4.7770267 -3.4483919 0.22123766 2.4337277 3.82655 2.8758063 -0.71390104 -3.739495 -5.1821604 -6.5023007 -9.1627531 -9.7486839][-7.2717013 -6.331233 -5.8469348 -5.0782375 -3.8855836 -2.6927078 -2.1278896 -1.4179821 -2.6721516 -5.8408694 -8.2468967 -8.09471 -8.8536358 -7.9283752 -5.5820684][-9.68316 -9.51374 -8.7201042 -7.3795462 -5.7745428 -5.9488935 -5.8452239 -5.9169083 -6.79004 -8.9637518 -10.485349 -10.15309 -9.6304045 -8.7736588 -6.1196232][-10.130064 -10.003725 -9.5077028 -8.3410645 -7.3435125 -6.8604274 -7.4208031 -8.4333477 -10.110285 -10.718955 -10.982746 -10.807292 -10.834515 -8.6701775 -6.7691579][-8.874177 -9.2752428 -8.22842 -7.3420134 -6.5274539 -6.6514215 -7.0618105 -7.6713228 -8.4902658 -8.7016869 -8.855238 -8.008111 -7.7075005 -7.6785741 -7.3220463][-6.1027131 -7.3076725 -8.0934944 -6.6413255 -5.2405229 -5.6275558 -5.4691944 -4.4547796 -4.8128467 -5.6212111 -6.070653 -6.5699468 -7.123733 -7.8284931 -8.4368753]]...]
INFO - root - 2017-12-15 18:17:02.133752: step 43310, loss = 0.17, batch loss = 0.13 (11.6 examples/sec; 0.691 sec/batch; 55h:31m:24s remains)
INFO - root - 2017-12-15 18:17:08.760037: step 43320, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 53h:27m:42s remains)
INFO - root - 2017-12-15 18:17:15.317150: step 43330, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 53h:02m:05s remains)
INFO - root - 2017-12-15 18:17:21.963775: step 43340, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 53h:22m:01s remains)
INFO - root - 2017-12-15 18:17:28.546199: step 43350, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 51h:20m:02s remains)
INFO - root - 2017-12-15 18:17:35.147522: step 43360, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 52h:35m:06s remains)
INFO - root - 2017-12-15 18:17:41.764995: step 43370, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 53h:06m:28s remains)
INFO - root - 2017-12-15 18:17:48.405036: step 43380, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 54h:11m:57s remains)
INFO - root - 2017-12-15 18:17:55.046046: step 43390, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 52h:40m:25s remains)
INFO - root - 2017-12-15 18:18:01.663678: step 43400, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 53h:44m:29s remains)
2017-12-15 18:18:02.221484: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7868824 -6.3973622 -5.2867684 -4.1695337 -3.8450122 -3.7298312 -3.8428316 -4.2324386 -4.6047721 -4.9137306 -4.9997454 -6.754046 -6.8214412 -7.7393403 -7.21633][-5.3690524 -5.1910877 -4.6987476 -4.1195555 -4.3061585 -4.0457878 -3.494364 -3.4657156 -3.5544906 -3.6883414 -3.4498739 -5.2197223 -6.2144456 -7.4788308 -7.215826][-3.3343022 -3.39958 -3.392612 -3.5328376 -3.6854856 -3.5746062 -3.4342134 -3.5403748 -3.8209472 -3.90827 -3.7062821 -5.1125965 -5.6097789 -6.1732445 -6.59541][-3.3502133 -3.3423696 -2.917737 -2.5549886 -2.9210119 -3.0331843 -2.7807825 -2.910717 -3.0613084 -3.1419325 -3.3889837 -5.4828334 -6.1391177 -6.9019012 -6.8772511][-3.5042341 -3.8868976 -3.8547781 -3.2190335 -3.0544286 -2.6238322 -2.4592214 -2.3374963 -2.3670297 -2.8316302 -3.2811706 -5.4721127 -6.4030261 -6.9333887 -7.2821593][-6.5318308 -5.7617106 -3.8659093 -2.3375866 -1.8904631 -0.51821041 0.45375824 0.52510357 -0.09875536 -0.88121748 -1.5250998 -4.2609839 -5.812356 -6.4755616 -7.35793][-6.6026006 -6.0691972 -4.4644842 -2.1930602 -0.872704 0.25598907 0.93639708 1.2085533 1.0359917 0.1702981 -0.75120306 -3.7784843 -5.559782 -6.5270386 -7.1483903][-6.2341404 -5.8598328 -4.3169355 -2.4951684 -1.3737202 0.11694288 0.73626995 0.79900265 1.2826877 0.69812727 -0.5012908 -3.6044023 -5.2661147 -6.4707789 -7.2551284][-5.2658486 -5.338 -3.3415916 -1.3708158 -0.29278517 -0.18463945 -0.48508835 -0.19133091 0.55491829 0.67919588 0.48899031 -2.5827904 -4.8265038 -5.9229031 -5.8199763][-4.0315361 -3.9132144 -2.5507562 -1.1553855 0.16105461 0.49232674 -0.10058546 -0.26143265 -0.11902142 -0.2524209 -0.44243813 -2.8595419 -4.3243723 -5.4139557 -5.9712982][-7.0692844 -5.5170527 -3.2958648 -2.1036539 -2.0685155 -2.3181381 -3.2019985 -3.6464171 -4.1378469 -4.0331392 -3.9788947 -6.1443148 -6.7451572 -6.5614276 -6.0898089][-9.3895264 -7.7438679 -5.8449416 -4.240839 -3.941164 -4.8321447 -6.3179793 -7.0192237 -7.1502309 -7.3984218 -7.7161427 -8.3081045 -8.2234707 -7.5320878 -6.3253736][-9.2407665 -7.5630059 -6.0862336 -5.4189348 -5.9717021 -5.9655123 -5.8896546 -6.7572069 -7.1265316 -7.1373491 -7.1334891 -7.8503838 -8.5089855 -7.7787228 -6.63308][-8.0480318 -6.7258863 -6.085515 -5.0505886 -4.2891965 -5.3692017 -5.8982306 -5.0941062 -5.1127205 -6.3908896 -7.2192631 -7.2187805 -6.7838631 -6.3426743 -6.0935316][-7.8332787 -7.4387689 -6.3944235 -4.7763891 -4.8696961 -4.4022684 -3.4279335 -3.5657141 -4.3037524 -3.9887285 -4.3525257 -5.8164811 -6.5228171 -7.22787 -7.2046366]]...]
INFO - root - 2017-12-15 18:18:08.883388: step 43410, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 53h:19m:03s remains)
INFO - root - 2017-12-15 18:18:15.386067: step 43420, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 52h:03m:47s remains)
INFO - root - 2017-12-15 18:18:21.987992: step 43430, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 53h:03m:51s remains)
INFO - root - 2017-12-15 18:18:28.657324: step 43440, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.688 sec/batch; 55h:13m:49s remains)
INFO - root - 2017-12-15 18:18:35.229725: step 43450, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 53h:00m:37s remains)
INFO - root - 2017-12-15 18:18:41.835222: step 43460, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.678 sec/batch; 54h:26m:31s remains)
INFO - root - 2017-12-15 18:18:48.485969: step 43470, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 53h:20m:56s remains)
INFO - root - 2017-12-15 18:18:55.147185: step 43480, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 53h:59m:47s remains)
INFO - root - 2017-12-15 18:19:01.664765: step 43490, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 51h:37m:37s remains)
INFO - root - 2017-12-15 18:19:08.332342: step 43500, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.675 sec/batch; 54h:11m:57s remains)
2017-12-15 18:19:08.950571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1133866 -7.487608 -7.6474805 -6.9486032 -7.1814871 -7.1626983 -8.5087929 -8.3973017 -7.728663 -6.7074656 -5.7545819 -5.4704909 -6.908618 -8.7781982 -9.88806][-5.9534664 -6.9615183 -6.4204283 -4.9329429 -5.4024134 -5.9778075 -6.5794644 -7.0815048 -7.0968828 -6.7742991 -5.501945 -4.87213 -6.6160116 -9.7242947 -9.7218056][-2.321018 -4.1815434 -5.3735437 -3.80865 -3.3714502 -4.5380282 -6.1137476 -6.4267941 -5.0788074 -4.5080266 -3.0393596 -3.1910319 -5.3766794 -9.7036715 -12.669838][-6.6599712 -8.25166 -8.50111 -7.6245852 -6.376421 -5.6014514 -5.80677 -6.4746056 -6.3641891 -4.4262676 -2.7278409 -2.5673215 -4.6957884 -8.8436565 -10.838987][-6.9326773 -8.9624491 -8.5728054 -7.1121993 -5.8086295 -3.4540641 -2.476336 -3.7858622 -5.079967 -4.54673 -3.0678582 -2.4648361 -3.5820045 -8.7099056 -11.504469][-6.6464057 -7.9931278 -7.72932 -5.7037506 -3.5040784 1.1499214 3.0187144 1.0299106 -0.67288828 -2.7633269 -3.987421 -3.1018937 -3.6740186 -7.4893446 -10.362136][-8.52704 -8.47895 -6.4684391 -3.5095947 -1.6463304 2.0329847 5.0612073 4.2677512 2.7731347 -1.5259848 -5.8869095 -6.3697047 -6.2019706 -8.4463367 -10.898157][-6.5308862 -7.2333517 -6.8864031 -2.88909 0.55946445 3.4247565 5.3201928 4.5153308 3.9040065 0.19064856 -4.3724895 -6.6608028 -9.21603 -10.759546 -11.476919][-6.3457179 -6.28103 -5.8347893 -3.2244496 -0.63936996 1.9561148 3.3069882 1.9699254 1.0740328 -1.724803 -5.2834997 -7.168931 -9.7418423 -12.683674 -13.834089][-5.4455171 -5.892693 -7.0776706 -6.0506916 -4.466279 -1.6015301 0.91188049 0.99620295 -0.86098337 -4.5988913 -7.88374 -9.284749 -10.278018 -12.248724 -14.814856][-9.787219 -8.6293621 -9.4902477 -9.109663 -9.6193237 -8.5230036 -6.8769026 -5.9976115 -5.6432962 -6.6268644 -9.702445 -12.067184 -13.6898 -14.113304 -13.526859][-15.829504 -15.44631 -14.360226 -12.890483 -13.703654 -13.432144 -12.791892 -11.76042 -11.559822 -12.533883 -13.262775 -12.232983 -11.407635 -12.224831 -12.393653][-14.436062 -12.822052 -13.305395 -12.673441 -12.468437 -12.312852 -11.968987 -11.922859 -11.117199 -11.33267 -11.694637 -10.710503 -10.390167 -9.7518272 -9.3180056][-12.778204 -11.753215 -10.905921 -9.6030178 -9.1157923 -10.016232 -10.924027 -9.7763405 -10.023635 -11.291712 -10.722114 -8.6034393 -7.8017368 -7.5399857 -7.3819523][-10.014684 -9.2979612 -6.7985797 -5.51551 -5.8008375 -6.3020415 -6.9837961 -8.0667858 -8.2661724 -8.1849194 -7.5638962 -6.9325376 -7.1688833 -7.5050383 -8.3853092]]...]
INFO - root - 2017-12-15 18:19:15.630187: step 43510, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 52h:31m:11s remains)
INFO - root - 2017-12-15 18:19:22.182924: step 43520, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 51h:29m:26s remains)
INFO - root - 2017-12-15 18:19:28.837137: step 43530, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 53h:32m:09s remains)
INFO - root - 2017-12-15 18:19:35.395518: step 43540, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 52h:43m:39s remains)
INFO - root - 2017-12-15 18:19:42.050058: step 43550, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 52h:36m:04s remains)
INFO - root - 2017-12-15 18:19:48.708069: step 43560, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 52h:52m:54s remains)
INFO - root - 2017-12-15 18:19:55.408736: step 43570, loss = 0.14, batch loss = 0.09 (11.5 examples/sec; 0.696 sec/batch; 55h:49m:47s remains)
INFO - root - 2017-12-15 18:20:02.140104: step 43580, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 53h:45m:33s remains)
INFO - root - 2017-12-15 18:20:08.709608: step 43590, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 52h:17m:53s remains)
INFO - root - 2017-12-15 18:20:15.281482: step 43600, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 51h:57m:11s remains)
2017-12-15 18:20:15.807145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1747551 -6.1021566 -5.7271047 -5.5334449 -6.8421726 -8.14856 -10.049772 -11.028646 -10.25144 -8.3414612 -6.44952 -5.926393 -8.06222 -8.9016685 -8.5382309][-7.1511021 -6.2923245 -5.9821677 -5.2994356 -6.3054748 -7.8440995 -8.849947 -9.79585 -9.44771 -6.7364559 -3.6634247 -3.5559461 -5.9165258 -7.8477616 -7.9483719][-5.7966552 -6.6942496 -6.7449903 -5.6319642 -5.3499966 -5.2948594 -5.9254246 -6.5127144 -5.5038137 -5.3099012 -3.887166 -3.818696 -5.6900496 -7.4938283 -7.6683607][-7.297555 -8.06517 -7.4055433 -5.6700935 -5.7855392 -5.1266508 -4.773973 -4.7822409 -3.9048865 -2.3150487 -1.8010614 -3.7625203 -6.9149342 -8.9303265 -9.1741028][-6.7296691 -7.5065 -7.3992562 -6.4576035 -5.1581168 -3.564853 -2.9353077 -2.3691711 -1.6703396 -0.77924824 -0.5758357 -2.8255756 -6.4290395 -10.004833 -10.017448][-7.56297 -7.820569 -6.702044 -4.1435213 -2.0393009 -0.003133297 1.5741038 1.5155354 1.5080471 1.2590151 0.41855526 -1.8866603 -5.9542589 -8.7919407 -8.8434219][-6.68846 -6.2593522 -5.7679715 -3.2687309 -0.83973789 1.4029703 3.78799 5.0466943 4.6462379 2.850636 0.34512424 -2.7932405 -7.298111 -9.0407228 -8.6521158][-5.5870748 -5.2047157 -5.1825747 -2.9855485 -0.34998798 2.9428239 5.5576444 5.9563823 6.24567 4.1810737 1.2292042 -2.8096023 -7.7881556 -9.9536209 -9.1445036][-5.0160532 -4.9661074 -4.9669752 -3.2672057 -1.8492064 0.71752357 3.1506095 4.433166 4.4466462 2.5702262 0.42405653 -4.0758982 -8.8870821 -11.065678 -9.7847614][-6.0640807 -6.1961017 -5.4252281 -4.3627114 -3.8834248 -1.4540076 0.3444562 1.4256115 1.2797899 -0.25530863 -2.8134172 -6.1145372 -9.6833477 -12.215199 -11.501797][-9.1551113 -8.0526218 -6.6475773 -5.8323913 -5.1996503 -4.6894493 -4.448349 -3.8984113 -4.1760325 -4.7178802 -6.0888271 -9.2350607 -12.72021 -13.337359 -10.750508][-13.333874 -10.173207 -8.1046371 -7.4707656 -6.8331313 -7.0858116 -7.7811437 -7.8131828 -7.8220806 -8.5229778 -9.7626724 -10.79241 -12.711069 -12.774548 -10.443966][-12.271175 -8.9491787 -5.3884282 -5.240438 -7.0066752 -7.0748053 -7.0438204 -7.9637089 -8.51112 -8.9157324 -9.6728926 -10.02785 -10.816184 -10.717639 -9.0444355][-8.6970711 -6.6892242 -3.9151704 -3.3049142 -5.1735997 -6.7034264 -7.8593092 -7.4927564 -7.3997436 -7.9867172 -8.3629894 -7.9989204 -7.3117743 -6.6691976 -5.8171959][-5.3090391 -4.068059 -2.3002486 -2.4229968 -4.3874159 -5.2836275 -6.0138493 -6.0339289 -5.9130087 -5.3301005 -5.3948159 -5.9824948 -6.253118 -5.9282513 -6.0382795]]...]
INFO - root - 2017-12-15 18:20:22.435659: step 43610, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 52h:33m:41s remains)
INFO - root - 2017-12-15 18:20:29.100432: step 43620, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.694 sec/batch; 55h:43m:25s remains)
INFO - root - 2017-12-15 18:20:35.783711: step 43630, loss = 0.14, batch loss = 0.09 (11.5 examples/sec; 0.694 sec/batch; 55h:41m:14s remains)
INFO - root - 2017-12-15 18:20:42.417023: step 43640, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 51h:46m:22s remains)
INFO - root - 2017-12-15 18:20:48.972035: step 43650, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 52h:50m:53s remains)
INFO - root - 2017-12-15 18:20:55.645669: step 43660, loss = 0.18, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 51h:04m:19s remains)
INFO - root - 2017-12-15 18:21:02.226826: step 43670, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 54h:07m:17s remains)
INFO - root - 2017-12-15 18:21:08.846374: step 43680, loss = 0.21, batch loss = 0.17 (12.0 examples/sec; 0.668 sec/batch; 53h:34m:57s remains)
INFO - root - 2017-12-15 18:21:15.471348: step 43690, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 54h:12m:45s remains)
INFO - root - 2017-12-15 18:21:22.107758: step 43700, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 53h:33m:01s remains)
2017-12-15 18:21:22.645230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4112716 -8.14395 -7.7365065 -7.9529276 -7.6813326 -6.2765303 -5.5922747 -3.9662251 -2.7214358 -2.6488655 -2.7885931 -4.6285992 -6.1043949 -5.3473487 -5.8537731][-6.1523409 -6.9660916 -6.3014774 -6.5724769 -6.32148 -6.1180382 -5.9353352 -4.6938119 -4.0499611 -3.7587333 -4.0253286 -5.2757158 -7.38366 -6.4783363 -7.247324][-4.639894 -5.4194837 -5.8337193 -5.8161311 -5.9317203 -5.9983892 -5.5898318 -4.9653292 -4.2038035 -3.96951 -4.3841829 -6.7593451 -9.9828091 -8.9118423 -9.2793856][-4.8165636 -5.3113585 -5.6528988 -4.9989934 -5.7603111 -6.5423212 -6.1434817 -6.000206 -6.0837779 -5.4631414 -5.4893141 -8.0558453 -11.384052 -10.71321 -11.64828][-4.6324005 -6.4809961 -6.0137448 -4.6472864 -4.1409454 -3.7398438 -3.53674 -5.2718277 -6.7769794 -6.0920353 -6.1980867 -8.462162 -11.759744 -12.101242 -14.076033][-6.2701468 -6.1605124 -4.5686474 -3.2510569 -2.2951605 -0.62084818 0.75615406 -1.4637027 -3.892612 -4.7860847 -6.7330194 -7.6835651 -10.127209 -11.670225 -13.897837][-7.0745044 -6.7019963 -4.3103957 -1.6369071 -0.028600216 1.7984524 3.1790071 2.3207679 1.7759418 -1.6565976 -5.6557555 -7.233314 -10.910699 -11.638258 -13.3641][-7.9642425 -6.6422038 -4.6669903 -2.0278931 1.0904403 3.126699 4.0229096 4.1681151 3.8129487 0.74633121 -2.4216394 -5.9513679 -10.381239 -10.615722 -11.833194][-6.0520611 -6.0463552 -4.0336652 -1.1698723 1.0771937 2.8862147 3.6047091 4.0898967 3.4308953 1.6159954 0.39413023 -3.6953127 -8.5819511 -9.3193913 -10.474197][-4.1320629 -4.4541111 -3.3542104 -0.80830336 1.1246357 0.651042 2.1615338 2.8364959 1.9569893 2.0748763 1.0342035 -2.3723476 -6.4965649 -7.23891 -8.69575][-7.7650123 -7.3647008 -6.1348314 -4.6114349 -3.7550535 -3.7988956 -2.9097497 -2.5793934 -2.7160795 -1.8739293 -1.8059368 -4.5656052 -6.5001621 -7.1060696 -7.3213339][-11.771914 -11.056547 -9.1146708 -7.945632 -7.5239177 -7.4790473 -7.4359722 -8.0052814 -7.7199426 -7.2948065 -7.04965 -7.5945206 -8.1284866 -7.3325553 -7.0764728][-13.012156 -11.642866 -9.57692 -8.297163 -7.72176 -7.8080425 -8.5312862 -9.2072411 -10.301483 -9.2069206 -8.0711327 -8.506053 -8.1560144 -6.5917559 -5.6949821][-10.376568 -10.423237 -8.8513432 -6.8645706 -5.8467593 -6.2395816 -7.3692827 -8.1873035 -9.421236 -9.4756289 -9.56872 -8.6961193 -7.6746669 -6.0442019 -6.0066805][-8.0435562 -8.0997171 -6.06995 -5.0552168 -3.3280551 -3.9433944 -5.2464142 -6.1281624 -7.3743224 -8.1526794 -8.2549419 -8.0701971 -8.2824354 -7.4669943 -7.08292]]...]
INFO - root - 2017-12-15 18:21:29.238266: step 43710, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 52h:41m:39s remains)
INFO - root - 2017-12-15 18:21:35.758826: step 43720, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 52h:38m:26s remains)
INFO - root - 2017-12-15 18:21:42.418908: step 43730, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 51h:06m:00s remains)
INFO - root - 2017-12-15 18:21:49.032900: step 43740, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 51h:44m:47s remains)
INFO - root - 2017-12-15 18:21:55.593392: step 43750, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 52h:17m:06s remains)
INFO - root - 2017-12-15 18:22:02.177038: step 43760, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 51h:29m:58s remains)
INFO - root - 2017-12-15 18:22:08.867842: step 43770, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.671 sec/batch; 53h:50m:10s remains)
INFO - root - 2017-12-15 18:22:15.431381: step 43780, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 54h:29m:32s remains)
INFO - root - 2017-12-15 18:22:22.135244: step 43790, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 51h:40m:10s remains)
INFO - root - 2017-12-15 18:22:28.848367: step 43800, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.684 sec/batch; 54h:51m:31s remains)
2017-12-15 18:22:29.405369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.5239596 -8.4609718 -7.7607045 -7.5654664 -8.0191212 -7.9046326 -7.9048376 -7.3008566 -6.9416819 -5.8070536 -4.50057 -5.9693966 -7.9873695 -8.2305965 -7.2553935][-8.1175547 -7.3809266 -6.876379 -6.3154106 -6.5436668 -7.3005843 -7.2547193 -7.1179147 -6.5577774 -5.8893542 -5.0649695 -6.423171 -8.872139 -9.984869 -9.4501066][-5.7647128 -7.0470538 -6.5609307 -5.7002506 -5.8674493 -5.6629887 -5.8177471 -5.6268129 -5.9656234 -6.0096855 -5.4583545 -7.7306852 -9.9969234 -10.168842 -9.47975][-6.6342263 -6.9780846 -5.9969807 -4.6600065 -4.4895468 -4.2832918 -4.457016 -5.3072882 -5.7194066 -6.0702219 -6.2740464 -7.8884315 -9.7439976 -10.75754 -10.301249][-7.0670018 -7.9542503 -7.1335249 -4.3606968 -2.4019537 -0.99195194 -1.5927029 -3.0348296 -4.6882477 -5.4356723 -5.2849445 -7.4540992 -9.4887571 -9.5624542 -9.1513691][-8.7815266 -9.2497644 -8.0096207 -3.6621656 0.28175831 2.2013707 3.2391772 1.0483656 -1.9406111 -3.2132175 -3.3253837 -4.8883953 -6.315556 -7.5212669 -7.6606803][-9.9947386 -9.1913986 -6.7635288 -2.6606066 0.31224251 4.3989797 7.3031011 5.7112756 2.5479417 -0.70502043 -3.2789028 -4.9546494 -6.2814946 -6.7400875 -6.0398774][-11.238092 -9.6385174 -6.6351805 -1.7610707 1.7382274 5.193892 7.435226 5.7399621 4.333529 0.94160795 -1.6965022 -4.8606172 -7.7680721 -8.1382122 -7.3573136][-8.5333986 -7.0700326 -5.0947142 -1.3992538 0.71532249 3.7679191 4.3915286 3.1763663 2.6041303 0.089432716 -2.2219706 -6.1997762 -9.553565 -9.884757 -9.2061882][-6.5719271 -5.7900743 -4.7072592 -2.3244143 -0.39052629 1.3021712 1.6230588 0.46595478 -0.790854 -2.9427738 -3.7221231 -7.1930709 -10.901823 -12.027708 -12.269722][-9.8055 -8.5970459 -7.2872963 -5.2202129 -4.5504837 -3.9377298 -3.7380624 -4.2542353 -4.4997759 -5.1364274 -5.5466022 -9.4509859 -11.922593 -13.076224 -12.558952][-12.836578 -12.412939 -10.872219 -9.7312746 -9.287221 -8.5303631 -7.9117346 -8.3411827 -8.5361252 -8.4275379 -8.0363674 -9.5857019 -10.926055 -12.735888 -12.478651][-14.45166 -13.791496 -12.901685 -11.256033 -10.413942 -9.8275356 -10.058025 -10.500772 -9.8939133 -9.4587011 -8.8391075 -9.3234529 -10.049716 -10.026419 -8.42605][-12.525647 -11.616716 -10.028522 -9.1491051 -8.3874807 -7.6663833 -7.80369 -7.88043 -8.1472025 -8.3010044 -8.0352335 -7.1325016 -6.8433409 -8.1624231 -7.4752264][-8.9393139 -7.2501068 -6.1052594 -4.793643 -3.7572854 -3.714247 -4.3742156 -5.0381975 -5.9057174 -5.766293 -6.1430063 -6.9138246 -7.2178917 -7.0702343 -7.7387886]]...]
INFO - root - 2017-12-15 18:22:36.102710: step 43810, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 53h:05m:24s remains)
INFO - root - 2017-12-15 18:22:42.776195: step 43820, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 53h:02m:12s remains)
INFO - root - 2017-12-15 18:22:49.414459: step 43830, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 54h:33m:37s remains)
INFO - root - 2017-12-15 18:22:56.050512: step 43840, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 52h:16m:43s remains)
INFO - root - 2017-12-15 18:23:02.673806: step 43850, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 52h:34m:28s remains)
INFO - root - 2017-12-15 18:23:09.210083: step 43860, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 53h:04m:37s remains)
INFO - root - 2017-12-15 18:23:15.827509: step 43870, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.637 sec/batch; 51h:02m:17s remains)
INFO - root - 2017-12-15 18:23:22.458577: step 43880, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.658 sec/batch; 52h:47m:34s remains)
INFO - root - 2017-12-15 18:23:28.965305: step 43890, loss = 0.31, batch loss = 0.27 (12.5 examples/sec; 0.642 sec/batch; 51h:27m:04s remains)
INFO - root - 2017-12-15 18:23:35.568064: step 43900, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 52h:21m:57s remains)
2017-12-15 18:23:36.077451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9231935 -5.90389 -7.4913149 -8.2765675 -8.8830671 -9.2520323 -10.177859 -10.013465 -9.119278 -8.4737406 -7.505621 -7.1795177 -7.3746476 -7.0761871 -5.7838035][-3.2444332 -3.7558362 -3.991251 -6.0166812 -7.7110186 -8.1771307 -8.6347618 -8.5856075 -8.21782 -7.504025 -6.430192 -5.8921814 -6.2298408 -6.7552853 -6.4719429][-1.3452897 -3.1379058 -5.478632 -7.3962984 -8.19482 -8.4346018 -8.7706585 -8.2566814 -7.0304418 -5.6921606 -4.2681804 -3.3841357 -3.8783944 -4.7328186 -5.6419182][-5.0988288 -5.6808181 -6.4446497 -8.8651962 -10.435744 -10.025712 -8.0017653 -7.6134548 -6.979063 -5.1920509 -3.58605 -3.8391318 -5.2819133 -5.8439317 -6.151444][-6.8483987 -9.1011009 -11.601078 -11.043789 -9.8117819 -7.6451697 -4.0433316 -4.2542849 -5.5024571 -4.8288207 -4.0153251 -4.7167673 -6.58491 -8.3041067 -9.6572828][-8.2567978 -10.203266 -11.249584 -9.9567757 -7.4398961 -3.279032 1.1756439 1.6070018 1.6062045 -1.2166171 -4.8272858 -5.2995806 -6.2649984 -8.8560467 -10.465066][-8.1212549 -10.239168 -10.281722 -7.331356 -4.4145813 0.84125566 6.5177741 6.9230781 5.218595 0.86616993 -3.4009013 -5.4881916 -7.8142424 -9.2325449 -9.2254629][-10.826439 -11.502061 -9.78345 -6.2408819 -3.2627647 1.6108294 6.9879794 6.9077439 6.0361381 2.0830736 -3.3591516 -6.1477585 -8.8093443 -10.397438 -10.502726][-7.6241331 -7.90685 -8.1897221 -7.4681673 -4.7001143 0.5116868 4.2727304 4.394505 4.1257377 -0.028740406 -4.4111886 -7.5236568 -11.626706 -12.349667 -11.035795][-5.2570577 -5.5362983 -5.5702553 -4.8764806 -3.9343481 -1.7043862 0.81976128 2.60389 2.9910369 -1.2228098 -5.8869743 -8.7112989 -12.243862 -13.758181 -14.190222][-9.9841385 -8.9262724 -7.9340439 -6.3772616 -5.4421453 -4.3879251 -2.5214865 -1.821569 -2.2563157 -4.4176049 -6.3736677 -9.77458 -13.277676 -13.222352 -12.123949][-15.367016 -13.45603 -10.919643 -9.4125843 -8.6167068 -8.0648365 -7.458087 -6.8481965 -7.0466161 -8.2732573 -8.9996548 -10.246119 -12.047476 -11.901855 -11.75303][-13.866716 -12.108706 -9.9850864 -9.49581 -8.59438 -7.994627 -7.5937438 -7.991786 -8.6996622 -9.19533 -9.4287529 -10.172477 -10.805746 -10.684771 -10.582626][-12.9126 -10.779482 -9.146944 -7.8127675 -6.3029852 -7.66864 -9.0232525 -8.4607086 -8.24042 -9.0555248 -9.998148 -10.06144 -10.167244 -9.3125153 -8.9259243][-10.047648 -8.2296562 -6.8945932 -5.8722162 -4.6477017 -4.8174958 -4.9624443 -6.2808409 -8.3051252 -8.456893 -7.5383234 -8.32362 -9.9069366 -10.077398 -9.5887794]]...]
INFO - root - 2017-12-15 18:23:42.636028: step 43910, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 53h:09m:19s remains)
INFO - root - 2017-12-15 18:23:49.295920: step 43920, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 54h:17m:49s remains)
INFO - root - 2017-12-15 18:23:55.951882: step 43930, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 51h:08m:20s remains)
INFO - root - 2017-12-15 18:24:02.481157: step 43940, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 51h:43m:38s remains)
INFO - root - 2017-12-15 18:24:09.160648: step 43950, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 53h:49m:42s remains)
INFO - root - 2017-12-15 18:24:15.763523: step 43960, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 52h:35m:56s remains)
INFO - root - 2017-12-15 18:24:22.383132: step 43970, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 51h:26m:40s remains)
INFO - root - 2017-12-15 18:24:28.971159: step 43980, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 53h:18m:53s remains)
INFO - root - 2017-12-15 18:24:35.624714: step 43990, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 53h:33m:24s remains)
INFO - root - 2017-12-15 18:24:42.281047: step 44000, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 54h:11m:27s remains)
2017-12-15 18:24:42.776189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8537412 -6.201982 -5.9205761 -4.8522425 -4.23339 -3.8168111 -3.8903818 -3.8234439 -3.7978833 -3.2224863 -2.6516175 -5.0553079 -6.3101325 -8.0487967 -8.8324261][-7.7679071 -6.6031857 -5.2527585 -4.9223795 -4.665554 -4.8625994 -4.6214542 -4.5234327 -4.6726193 -4.5748105 -4.6265926 -6.4361839 -7.6844869 -9.0749435 -8.9262972][-4.8328032 -5.3320675 -5.7925858 -4.7352486 -4.171699 -5.005477 -5.6255708 -5.141767 -4.5611334 -4.5173841 -4.7815576 -6.9472961 -8.3301392 -9.0382223 -9.2124376][-3.0512578 -3.2927449 -2.9840782 -2.7378242 -3.6012943 -3.6444039 -3.3042533 -3.9312644 -4.2293644 -4.0690756 -4.4645386 -6.3552408 -7.2509804 -8.9778328 -9.6659765][-2.5688605 -2.8370054 -2.5130324 -1.8312688 -2.3370168 -1.8363686 -1.5826435 -2.1532345 -2.7894862 -3.223424 -3.2207608 -5.2566037 -6.5554724 -8.25879 -8.7170591][-4.2496347 -4.1585836 -2.8760726 -1.4506326 -1.0258489 0.32011175 1.0318627 1.1618695 0.75044394 -0.22610807 -1.1924829 -3.2279143 -3.6239645 -5.8331413 -6.43935][-6.7284942 -5.4823656 -3.0854189 -0.8033371 0.60763693 1.5906343 2.3086848 3.5024629 4.0572667 2.7513261 1.5753927 -1.3384871 -2.7971809 -4.3285837 -4.450356][-6.1242003 -4.7882462 -2.6293147 -0.99361897 0.61260796 2.5177865 3.8008771 4.0289273 4.2053351 3.7639651 2.6501613 -0.79138041 -2.013742 -3.5101793 -3.8895993][-3.3822036 -3.2416928 -1.9394484 -0.45684242 0.7933321 1.6102881 2.5592628 3.0827317 3.12223 3.2048097 2.8774686 -1.2770128 -3.5773034 -4.6226258 -4.6452131][-2.8713579 -2.4361515 -1.2022796 0.2806983 1.0133653 0.39167166 0.49969482 1.8959069 2.7952008 2.5237927 1.5835233 -1.2195764 -2.9814987 -4.3786054 -4.792973][-5.4885993 -4.3396006 -3.2255769 -1.8295405 -0.88713646 -0.63580179 0.24752569 1.1240969 1.307127 1.3341451 0.35991716 -3.0744271 -4.8471375 -5.3306117 -5.7640519][-9.3872519 -9.3064957 -6.9943514 -4.5848951 -3.8107018 -3.3152635 -2.1983171 -1.4415817 -1.5251164 -1.3629913 -1.2506433 -3.5010173 -4.7931809 -4.9633856 -4.4633551][-12.315764 -11.495956 -9.9419079 -7.8057094 -6.6362476 -4.8469028 -4.1219444 -3.6533277 -3.5197425 -4.3985529 -4.8982377 -5.1210546 -4.151926 -3.2496581 -2.9108191][-11.590828 -11.177252 -9.9516478 -7.6717229 -6.6521535 -6.40792 -6.1371832 -5.0528145 -5.2270765 -5.6407123 -5.104816 -4.8334794 -3.4603143 -3.0050828 -2.6646318][-8.1357765 -7.603919 -7.8880472 -6.4941721 -5.4565811 -4.2175903 -4.3139782 -5.1869512 -5.7443075 -5.6076965 -5.0425763 -6.8285289 -7.3516035 -5.786458 -4.2315779]]...]
INFO - root - 2017-12-15 18:24:49.356817: step 44010, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 51h:01m:12s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 18:24:56.010245: step 44020, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 52h:53m:48s remains)
INFO - root - 2017-12-15 18:25:02.589384: step 44030, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 53h:46m:38s remains)
INFO - root - 2017-12-15 18:25:09.188735: step 44040, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 52h:34m:55s remains)
INFO - root - 2017-12-15 18:25:15.789727: step 44050, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 53h:06m:06s remains)
INFO - root - 2017-12-15 18:25:22.398587: step 44060, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 52h:02m:43s remains)
INFO - root - 2017-12-15 18:25:29.072617: step 44070, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 53h:47m:28s remains)
INFO - root - 2017-12-15 18:25:35.717678: step 44080, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 53h:47m:39s remains)
INFO - root - 2017-12-15 18:25:42.300179: step 44090, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 52h:38m:09s remains)
INFO - root - 2017-12-15 18:25:48.833530: step 44100, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 53h:50m:23s remains)
2017-12-15 18:25:49.306747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9730952 -3.1575296 -2.9534972 -2.8230777 -3.3961408 -3.5449715 -3.8873525 -3.3747447 -3.3183782 -3.681592 -4.5522633 -7.2091632 -8.8619747 -10.888437 -10.583277][-3.6644368 -3.632061 -4.1949654 -4.197032 -4.3960342 -4.2898631 -4.0853653 -3.7902355 -4.2763915 -4.2661476 -4.6310925 -7.2191658 -8.6816206 -10.874685 -10.815006][-4.5097795 -5.5729165 -5.2634058 -4.39578 -4.2729354 -3.5065465 -2.7797732 -2.9127469 -3.5803361 -3.8944921 -4.1916647 -6.3650355 -7.9126339 -9.9380989 -9.5084124][-5.9038014 -6.0399704 -5.4952893 -4.3057556 -3.8353782 -2.8491552 -2.1926916 -2.242101 -2.7868459 -2.9916234 -2.8822079 -5.0043325 -6.3223152 -8.56763 -8.267663][-6.5660052 -6.511375 -6.0637422 -3.7824192 -2.4068007 -0.6749711 -0.30978775 -0.57339144 -0.71619987 -1.3008132 -1.5640855 -3.1413341 -3.5293083 -6.5603623 -6.7713675][-6.7591076 -6.5717144 -4.767941 -1.3647513 1.2181344 2.6551023 2.8722892 2.3697772 1.9119349 0.80973577 0.08440733 -2.0337727 -3.0312064 -6.020401 -6.1959429][-7.5354195 -6.6738682 -4.6172938 -0.79529858 1.9256244 4.448853 5.3327546 4.9759212 4.3749843 2.8549056 1.383707 -1.7015438 -3.2394464 -5.6249022 -5.2692013][-6.35392 -6.458612 -4.8995008 -1.2777104 1.7005582 5.0077662 6.1643577 5.6499171 5.4708123 4.3016829 2.5939698 -0.64175987 -2.6263099 -5.3277822 -4.7781439][-5.8506441 -5.7400451 -4.7821636 -1.8104444 0.40421152 3.8193336 5.624722 5.9243045 5.9367986 4.404902 2.6272893 -0.97652197 -3.2255878 -5.160696 -4.7333622][-4.8854275 -4.8643579 -4.5071611 -1.8886566 -0.56725168 2.1293678 3.5994077 4.1555 3.8174348 2.4382911 1.7196531 -1.5717821 -4.1949692 -6.8818135 -7.3378558][-7.7259226 -7.7128067 -6.8491316 -4.6398993 -3.6877158 -1.5921974 -1.1987848 -1.2943778 -1.5055633 -2.4480081 -2.9374013 -5.9524994 -7.1207681 -9.2793179 -8.9307][-10.960821 -9.6704636 -8.7585964 -6.3647857 -5.3465152 -3.7621331 -4.4321942 -5.6081753 -5.9990959 -6.5712891 -6.8661046 -8.98365 -9.1002941 -10.991261 -10.773887][-10.681428 -9.7601633 -8.7440166 -7.5061321 -6.4160423 -5.45111 -6.0389462 -6.3149981 -7.9126196 -8.836031 -8.7994652 -9.490736 -9.1171732 -10.063448 -8.6765137][-11.035328 -10.986832 -9.8702736 -8.3567619 -6.9978857 -6.4926429 -6.7310276 -7.0706949 -7.989645 -8.1926184 -8.1090326 -7.4355288 -6.7587309 -7.1605721 -6.5616136][-8.8880882 -9.2476034 -8.8830452 -7.9719892 -6.9992185 -7.1661158 -7.3289614 -7.3036671 -7.4444957 -6.9939861 -7.3111033 -7.4168081 -6.6827445 -6.1354437 -5.5356293]]...]
INFO - root - 2017-12-15 18:25:55.823858: step 44110, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 52h:55m:22s remains)
INFO - root - 2017-12-15 18:26:02.355800: step 44120, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 52h:24m:01s remains)
INFO - root - 2017-12-15 18:26:09.033056: step 44130, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 53h:57m:27s remains)
INFO - root - 2017-12-15 18:26:15.620795: step 44140, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 52h:55m:43s remains)
INFO - root - 2017-12-15 18:26:22.148516: step 44150, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 52h:54m:58s remains)
INFO - root - 2017-12-15 18:26:28.773678: step 44160, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 52h:29m:55s remains)
INFO - root - 2017-12-15 18:26:35.370511: step 44170, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 51h:38m:02s remains)
INFO - root - 2017-12-15 18:26:42.061423: step 44180, loss = 0.19, batch loss = 0.14 (11.8 examples/sec; 0.678 sec/batch; 54h:18m:38s remains)
INFO - root - 2017-12-15 18:26:48.676926: step 44190, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 52h:35m:55s remains)
INFO - root - 2017-12-15 18:26:55.275587: step 44200, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 52h:31m:12s remains)
2017-12-15 18:26:55.786028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6048083 -7.2920003 -8.8354769 -9.1909895 -10.267607 -10.309473 -10.207638 -10.207558 -10.398949 -11.384396 -10.798608 -10.129082 -9.2780724 -12.307467 -12.238976][-7.460557 -8.8854589 -9.5435839 -9.8535042 -11.026529 -11.335985 -10.746731 -10.384471 -10.725561 -11.273925 -11.596307 -12.155902 -10.886075 -14.075121 -14.374657][-5.7787061 -7.8352013 -10.228676 -10.74877 -11.428392 -11.63274 -11.012966 -10.35641 -10.231131 -10.740608 -11.292714 -11.454414 -11.279026 -14.893148 -15.47287][-7.3165112 -8.7189655 -10.71559 -11.184954 -10.887151 -9.2409086 -8.3802967 -8.9501638 -9.10368 -9.4002943 -10.243259 -11.08698 -11.164743 -14.629885 -15.492298][-8.0176716 -10.708078 -12.907576 -11.698215 -9.6145239 -3.9940071 -0.14499569 -4.001863 -6.8213277 -7.0227551 -7.7038546 -8.5193481 -8.6718369 -12.879801 -13.625156][-8.5665712 -11.080025 -12.116757 -11.926937 -10.90458 -2.6080549 6.1629348 5.2341752 1.7170358 -2.8219061 -5.7616124 -5.443202 -5.3360929 -9.5066757 -11.239191][-9.1887865 -11.840637 -10.368466 -7.7320824 -7.5375452 -2.3359778 5.6778789 9.0648232 9.9930382 2.092061 -4.7981429 -5.1666536 -5.3139448 -8.6767788 -9.9328842][-8.0860577 -11.431934 -12.188387 -7.207799 -2.73288 1.9147601 5.6847863 7.0931916 10.534704 4.8403373 -1.4718251 -3.9970148 -5.8600793 -9.14647 -9.9620171][-5.6598964 -7.2475061 -10.500113 -9.8372755 -6.6664438 0.17702818 3.5953078 4.0629659 6.8205895 2.3852715 -1.5349579 -4.2494688 -5.3359904 -9.7931509 -12.57831][-3.9590926 -4.2797112 -8.1620369 -9.6414661 -9.5669537 -5.3057723 -1.635169 1.5542846 3.1507821 -0.5710063 -3.411906 -6.8461809 -6.6396413 -10.105187 -13.125222][-9.951148 -9.886219 -11.32722 -11.725934 -12.618351 -11.18161 -9.8521519 -7.0805907 -5.3536992 -6.0267277 -7.2574139 -9.0137177 -8.6708021 -12.343289 -13.119509][-12.848398 -13.993874 -14.462076 -14.380867 -14.496281 -13.332125 -13.501358 -12.451484 -11.331549 -10.866448 -11.094204 -11.055355 -8.6200638 -12.350037 -13.198954][-9.9209528 -11.535488 -11.974391 -13.078104 -13.407502 -12.589758 -12.539116 -12.96146 -13.135057 -11.150564 -9.4852734 -10.51314 -8.9837093 -10.465791 -9.56724][-9.11392 -8.5797272 -9.1441193 -9.38553 -9.2861786 -9.6543417 -9.55969 -10.370449 -12.182671 -11.843782 -10.185698 -8.7166481 -6.5086665 -7.6489944 -8.1809788][-6.6700397 -6.9777122 -6.0376368 -4.673419 -3.8297994 -4.654285 -4.9426379 -5.5440817 -7.6582689 -8.402564 -9.2007847 -8.5526314 -8.0723114 -8.0841064 -8.3697472]]...]
INFO - root - 2017-12-15 18:27:02.317619: step 44210, loss = 0.19, batch loss = 0.14 (11.7 examples/sec; 0.682 sec/batch; 54h:38m:00s remains)
INFO - root - 2017-12-15 18:27:09.054436: step 44220, loss = 0.13, batch loss = 0.09 (11.3 examples/sec; 0.705 sec/batch; 56h:28m:01s remains)
INFO - root - 2017-12-15 18:27:15.694882: step 44230, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 52h:42m:42s remains)
INFO - root - 2017-12-15 18:27:22.224037: step 44240, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 52h:49m:13s remains)
INFO - root - 2017-12-15 18:27:28.871993: step 44250, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 53h:06m:43s remains)
INFO - root - 2017-12-15 18:27:35.454743: step 44260, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 52h:44m:47s remains)
INFO - root - 2017-12-15 18:27:42.086997: step 44270, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 52h:21m:09s remains)
INFO - root - 2017-12-15 18:27:48.761554: step 44280, loss = 0.21, batch loss = 0.17 (12.2 examples/sec; 0.655 sec/batch; 52h:24m:59s remains)
INFO - root - 2017-12-15 18:27:55.457165: step 44290, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 54h:25m:56s remains)
INFO - root - 2017-12-15 18:28:02.044602: step 44300, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 53h:35m:26s remains)
2017-12-15 18:28:02.547988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1925421 -4.3558297 -4.3017282 -4.0866022 -4.297821 -4.22515 -4.1410275 -3.620543 -3.0659895 -3.00019 -2.402765 -2.8987823 -5.7811341 -7.0507421 -8.1425705][-2.919404 -2.5354702 -2.8041286 -3.1363544 -3.7695212 -3.7048392 -3.6604395 -2.8952825 -2.2572718 -2.1295512 -1.4521141 -2.2113924 -4.0225773 -5.1630259 -7.1248069][-1.0204148 -1.8921745 -3.3934412 -3.9175644 -4.5410781 -3.9967299 -3.3229904 -2.7864451 -1.96932 -1.4365692 -1.1802421 -2.1102808 -3.9983139 -5.2067552 -6.8114033][-2.153847 -3.1948183 -4.485342 -5.2744727 -6.19158 -4.6219158 -3.0060782 -1.8807464 -1.0897245 -0.83394766 -0.94566393 -2.8473957 -5.1540051 -6.82782 -7.8883781][-2.5160499 -4.3321872 -5.6773119 -6.1026015 -5.8027563 -3.0131078 -0.999949 -0.73904181 -1.1642537 -0.97444582 -1.8314118 -3.9265466 -6.741888 -7.7274733 -8.7734957][-4.3228288 -5.4890494 -5.8447394 -5.1346974 -3.4273462 0.564795 4.1302085 4.3321071 2.890882 0.57263803 -1.7937946 -3.4173186 -5.9785914 -7.6953645 -9.2879524][-5.6826577 -5.5174112 -4.380022 -3.3740208 -1.4710808 2.439796 6.9772172 8.1997452 6.6430469 2.6397195 -1.1264744 -4.3738022 -7.3093519 -8.0614538 -9.0529842][-8.33418 -8.1323757 -7.1259165 -4.0731821 -1.2888603 3.1598821 8.2859325 9.1599827 7.80473 4.2935805 0.2711072 -4.1524744 -8.1451378 -8.7138882 -9.10902][-7.9658165 -8.5522127 -8.0005217 -4.7512536 -2.7053394 1.0233202 5.2945151 8.1065483 7.1437926 3.9702106 0.54508734 -5.0539594 -10.074665 -10.529963 -10.406439][-8.4130707 -9.2267723 -8.6740084 -6.6843276 -5.1120667 -2.8408046 1.0950546 4.0834002 4.07297 2.1135612 -1.3094602 -5.8570719 -10.083282 -11.303234 -11.313256][-11.696508 -12.425983 -12.011308 -10.432701 -9.6257133 -8.3627214 -5.5801716 -3.1268694 -2.0476024 -2.434207 -4.4593906 -9.3412437 -12.783627 -12.952642 -11.572161][-14.271879 -14.464903 -13.749157 -13.206522 -12.906076 -11.735104 -9.7158937 -7.6834087 -6.8786278 -7.2015691 -8.8816347 -11.136808 -13.222476 -13.553709 -12.564466][-14.401588 -13.310937 -12.485947 -12.073916 -11.90164 -11.761281 -11.704189 -11.318037 -10.453851 -9.7036705 -9.8871346 -10.855294 -12.203291 -11.680595 -11.034824][-12.291656 -11.531323 -10.807504 -10.857923 -10.559276 -10.256693 -10.881563 -9.9717836 -9.8836813 -10.570701 -10.89772 -9.6726284 -8.9930878 -8.201643 -8.5909805][-9.87458 -9.9024754 -9.764658 -9.4266357 -8.3557253 -8.4461317 -9.2610683 -9.3380194 -9.615593 -8.8219481 -8.4745884 -8.7646475 -8.8669872 -8.819459 -9.4094515]]...]
INFO - root - 2017-12-15 18:28:09.137162: step 44310, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 53h:09m:50s remains)
INFO - root - 2017-12-15 18:28:15.724080: step 44320, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.687 sec/batch; 54h:58m:07s remains)
INFO - root - 2017-12-15 18:28:22.363500: step 44330, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 51h:58m:48s remains)
INFO - root - 2017-12-15 18:28:28.954444: step 44340, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 53h:11m:05s remains)
INFO - root - 2017-12-15 18:28:35.594706: step 44350, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 52h:19m:19s remains)
INFO - root - 2017-12-15 18:28:42.229803: step 44360, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.682 sec/batch; 54h:33m:31s remains)
INFO - root - 2017-12-15 18:28:48.820484: step 44370, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.631 sec/batch; 50h:27m:54s remains)
INFO - root - 2017-12-15 18:28:55.397030: step 44380, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 52h:32m:25s remains)
INFO - root - 2017-12-15 18:29:02.015249: step 44390, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.632 sec/batch; 50h:33m:39s remains)
INFO - root - 2017-12-15 18:29:08.719013: step 44400, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 53h:00m:29s remains)
2017-12-15 18:29:09.265338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1465855 -7.1515188 -6.6229734 -5.7876177 -6.1966376 -6.8402309 -7.3547597 -6.8275542 -6.5401611 -6.9501505 -7.2953143 -11.392101 -14.398655 -13.367698 -12.865858][-7.7776966 -8.1396713 -7.3938327 -7.0026665 -7.0171185 -7.5589795 -8.6645079 -9.3070345 -9.8608074 -9.7630148 -9.84114 -13.483616 -15.28796 -14.138453 -13.642376][-7.7492404 -9.0358419 -9.369669 -8.5073586 -8.2145948 -8.2649212 -8.5921669 -8.7927513 -9.5480394 -10.45735 -10.9758 -14.586077 -16.920238 -15.260695 -14.021032][-7.3242893 -8.1967392 -8.0330791 -7.1956644 -7.0578337 -6.8832178 -7.1014929 -7.5659008 -7.7318516 -7.9874392 -8.72308 -12.777954 -15.334824 -14.703972 -14.508207][-7.7201014 -9.4003077 -8.886487 -6.5992341 -5.0209284 -3.6056881 -3.4750984 -4.804862 -5.6947889 -5.2610874 -5.0058422 -8.9396324 -11.886023 -12.196989 -12.592799][-7.7564373 -8.79625 -7.4354939 -4.9323168 -2.5833592 0.98014259 2.5939717 0.87948656 -1.2183523 -2.4532919 -2.9789579 -6.5054274 -8.9300938 -8.7806549 -9.38691][-10.394942 -9.3606987 -7.0696254 -3.5533886 0.25071096 4.0096736 5.9290566 5.2439685 3.5296226 0.64842129 -1.5944724 -4.9301615 -7.0444365 -6.9301453 -7.6688719][-10.754456 -9.6985559 -6.9298458 -2.812712 1.4143744 5.834949 7.8015857 7.3590093 6.6542611 4.4459405 1.5941262 -4.3425622 -8.44976 -8.1412058 -8.53][-7.9548688 -7.9147577 -5.87469 -2.4351239 0.94160986 3.2715173 4.8690581 5.3823352 5.2676187 4.2581677 2.752089 -3.7708421 -9.3637877 -10.524615 -11.321192][-4.1281662 -5.3613596 -5.12812 -2.776556 -0.49025154 0.0843215 0.49094772 1.0725894 1.6544251 2.0291367 1.7049818 -4.361537 -9.7746487 -12.38086 -14.284189][-5.7417836 -6.4002962 -6.9719768 -5.6599412 -4.3502579 -3.9595137 -3.8472335 -4.2066174 -4.6860862 -4.4556689 -4.5065155 -7.4087214 -10.117535 -12.668264 -15.044258][-7.333179 -8.1223545 -8.8401442 -8.9898262 -8.9516287 -8.1965218 -8.0098648 -8.6166077 -8.6358681 -7.841507 -7.60462 -9.5228462 -11.246594 -12.528866 -13.625433][-13.776283 -12.825152 -12.260447 -11.291295 -11.317328 -11.985219 -12.491814 -12.446342 -12.191757 -11.687584 -11.26992 -12.10545 -12.694702 -11.280069 -10.949741][-13.894032 -14.188646 -13.354789 -11.860873 -11.408307 -10.991942 -11.460667 -11.832967 -11.81944 -11.315727 -10.61649 -10.635801 -10.668688 -10.254013 -9.8377][-11.829723 -12.624649 -11.970488 -10.310362 -8.3006878 -6.6451225 -7.2588453 -7.6581378 -8.1769667 -9.2672758 -9.8018541 -10.054607 -10.250074 -9.9330368 -9.217638]]...]
INFO - root - 2017-12-15 18:29:15.907866: step 44410, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.679 sec/batch; 54h:18m:39s remains)
INFO - root - 2017-12-15 18:29:22.593688: step 44420, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 52h:09m:39s remains)
INFO - root - 2017-12-15 18:29:29.165045: step 44430, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 52h:49m:53s remains)
INFO - root - 2017-12-15 18:29:35.755152: step 44440, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 52h:36m:51s remains)
INFO - root - 2017-12-15 18:29:42.292362: step 44450, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 51h:43m:31s remains)
INFO - root - 2017-12-15 18:29:48.874116: step 44460, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 53h:26m:28s remains)
INFO - root - 2017-12-15 18:29:55.512912: step 44470, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 53h:18m:25s remains)
INFO - root - 2017-12-15 18:30:02.058156: step 44480, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 52h:34m:10s remains)
INFO - root - 2017-12-15 18:30:08.755527: step 44490, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 53h:10m:20s remains)
INFO - root - 2017-12-15 18:30:15.417224: step 44500, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 54h:22m:56s remains)
2017-12-15 18:30:15.952614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3478508 -4.8179674 -3.9419074 -3.1981418 -2.7912152 -2.6510749 -2.7348218 -2.9652781 -3.1827717 -3.3537457 -2.7939944 -4.893321 -6.4563527 -7.4726357 -8.5635738][-4.1005735 -5.3731675 -5.8818669 -5.5654106 -4.9021049 -4.3604879 -3.8761406 -3.5067582 -4.17201 -4.524868 -4.0102048 -5.2833972 -5.9542289 -6.9401612 -7.5811434][-2.861058 -3.821075 -5.1947203 -5.5947933 -5.3432865 -4.8979497 -3.8669434 -3.6240847 -4.1631832 -4.1448951 -3.9132669 -4.7615728 -5.4074469 -5.8940887 -6.5185184][-2.2080178 -3.0473626 -4.1554365 -4.2248816 -4.3183289 -3.3803713 -2.295433 -2.285722 -2.501467 -2.591594 -2.8337202 -4.5462847 -5.8358769 -6.507412 -7.0038972][-3.2440476 -3.3974395 -3.0733714 -2.4076712 -2.727809 -1.454535 -0.48860693 -0.29506826 -0.42040205 -0.89489412 -1.7883234 -3.6258349 -4.522943 -5.6293731 -6.8099289][-4.6756411 -3.585511 -2.0999534 -1.3499475 -0.89994 0.27008724 1.1698737 1.3736081 1.2241445 1.1512213 1.1366749 -1.418889 -3.3385923 -4.9926019 -6.4189754][-6.262156 -5.0215864 -2.9923778 -0.69464207 0.74540043 1.5679402 1.7987142 1.6582441 1.6863527 1.6080217 1.4335589 -0.22895288 -1.6543374 -3.3080289 -5.4306612][-4.9743094 -4.4304719 -3.1971114 -0.82495975 0.79620934 2.1253324 2.3116755 1.6839232 1.5456333 1.7802362 2.6901994 0.8132515 -1.6353073 -4.0455723 -6.2734036][-4.1927991 -3.4925268 -2.2180629 0.079055786 0.92531204 1.4708209 1.2681985 1.1975861 1.1713343 1.5586338 2.0826259 0.25089216 -1.8626199 -4.1135149 -6.6107874][-2.8989828 -2.2717161 -1.9072938 -0.82269 -0.04101038 1.0581665 0.72239113 0.20105124 -0.040674686 1.0050683 2.552155 0.27016449 -2.1554978 -4.587184 -6.8966017][-7.094624 -5.6829095 -3.8457265 -3.0524616 -2.2838926 -1.8192644 -2.3547249 -1.8483591 -1.6712289 -1.2548552 -1.2762284 -3.9537034 -4.7892303 -6.0692773 -7.2363796][-10.813065 -9.3933945 -6.9128246 -5.307004 -4.4792023 -3.6792843 -3.534421 -4.0788546 -4.6136131 -4.080411 -3.3682058 -4.7357116 -5.801055 -6.4581108 -7.0476842][-12.54423 -10.629578 -8.3107834 -6.7636318 -5.9331479 -5.5513167 -5.4189148 -5.1761146 -5.4290771 -5.6719093 -5.5139475 -5.7297072 -5.9077082 -5.3200426 -5.3348789][-10.271073 -9.6565895 -8.4493542 -6.6787777 -5.9356761 -6.0751982 -6.0337873 -5.5471554 -5.1002703 -5.1226859 -4.8301992 -4.3008504 -4.6155624 -4.7710495 -5.5412297][-6.9453373 -7.6913247 -6.9888282 -5.7868571 -4.746491 -4.2780867 -4.4031677 -4.9697866 -5.2396274 -4.6302133 -4.24705 -4.9842749 -5.2052684 -5.9614439 -7.4757943]]...]
INFO - root - 2017-12-15 18:30:22.590793: step 44510, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 52h:07m:09s remains)
INFO - root - 2017-12-15 18:30:29.122772: step 44520, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 51h:25m:27s remains)
INFO - root - 2017-12-15 18:30:35.831105: step 44530, loss = 0.19, batch loss = 0.14 (10.9 examples/sec; 0.735 sec/batch; 58h:46m:50s remains)
INFO - root - 2017-12-15 18:30:42.424463: step 44540, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 52h:35m:26s remains)
INFO - root - 2017-12-15 18:30:49.041801: step 44550, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.653 sec/batch; 52h:15m:09s remains)
INFO - root - 2017-12-15 18:30:55.646127: step 44560, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 54h:03m:47s remains)
INFO - root - 2017-12-15 18:31:02.285587: step 44570, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 53h:54m:15s remains)
INFO - root - 2017-12-15 18:31:08.933580: step 44580, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.675 sec/batch; 54h:00m:12s remains)
INFO - root - 2017-12-15 18:31:15.565703: step 44590, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 51h:54m:10s remains)
INFO - root - 2017-12-15 18:31:22.209871: step 44600, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 51h:27m:24s remains)
2017-12-15 18:31:22.758620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5150089 -3.6588142 -3.662343 -3.1125793 -3.6045249 -3.9018526 -4.0724335 -4.0353808 -3.7295904 -3.8006082 -3.341259 -6.059711 -7.8140831 -9.5069141 -8.9065943][-5.0543761 -4.896554 -4.7541361 -4.1016994 -4.5280514 -4.9555092 -5.2399158 -5.5304942 -5.4622855 -4.8727355 -4.2267065 -6.5837412 -7.7005844 -9.8174458 -10.171692][-4.3574877 -5.0063553 -5.702106 -5.2026834 -5.7357864 -5.887928 -6.3225884 -6.2546358 -6.0793352 -5.9874425 -5.5502925 -7.693706 -8.947073 -10.571401 -10.732784][-5.0214386 -5.8289943 -5.8836627 -5.3115191 -5.8820128 -5.5286012 -5.3873515 -5.4940763 -5.4246058 -5.008194 -4.7646565 -7.3589392 -9.13608 -11.144862 -11.232571][-6.2535839 -7.375833 -7.7782412 -6.0314665 -5.4735885 -4.4746647 -4.39513 -4.4380083 -4.1890893 -3.7293406 -3.1943493 -6.1611171 -8.4745874 -11.057386 -11.790602][-7.4709806 -8.0222178 -7.4944677 -5.0993605 -2.8136823 -0.65391779 0.078513145 -0.28982973 -1.0384712 -1.5253992 -1.503396 -4.2169685 -6.4595585 -9.2786922 -10.497313][-8.6149216 -8.5307989 -7.0577259 -3.5318923 -0.76540804 1.2775826 2.7409711 3.4950204 3.3422771 1.320416 -0.19741964 -3.4686484 -5.7598143 -8.4973211 -9.4502439][-8.5371342 -8.210763 -6.469408 -2.4200625 0.41624975 3.3415427 5.0357985 4.4023175 4.2112355 2.9194999 0.69776297 -3.7371032 -6.4004145 -8.9187145 -9.5565138][-6.3163438 -6.3760476 -5.3455534 -2.705826 -1.1112633 2.4091163 4.09191 4.2476048 4.3906188 2.590404 0.78341675 -3.8740847 -7.1927404 -9.7351723 -9.853281][-5.4027905 -4.2727489 -3.5893295 -2.2212498 -1.4370999 -0.059771061 0.75662708 2.0917578 2.2016635 0.74682045 -0.56590128 -5.1206918 -8.2995644 -10.769217 -11.518307][-6.1773748 -5.4099975 -4.6311951 -3.3314457 -3.5855081 -3.2898104 -3.3199832 -2.9019945 -3.0691497 -3.3745389 -4.1264791 -8.246809 -11.050428 -12.709291 -12.406535][-9.0042229 -7.4830089 -6.1776314 -6.1661625 -6.1238084 -6.1122551 -6.9949703 -7.7624073 -8.1937838 -8.1546106 -8.4599533 -9.909255 -10.95689 -12.391119 -11.755581][-11.561682 -10.394719 -9.0112991 -8.4184675 -8.9929667 -8.6659946 -8.803793 -9.3489037 -10.334597 -10.742949 -10.607386 -10.916541 -11.254604 -11.306735 -9.7146969][-11.380672 -11.119471 -9.9550962 -9.2274809 -8.85117 -8.8233624 -8.9500952 -9.1502523 -9.4147129 -10.184205 -10.271141 -9.1188583 -8.3800726 -9.2350407 -8.9376926][-8.7425928 -9.1286316 -8.6676826 -7.2800574 -6.7879477 -6.5852633 -7.0492263 -7.5022759 -7.7544374 -7.79412 -7.7993212 -8.6552162 -9.4660587 -8.7236862 -8.3602371]]...]
INFO - root - 2017-12-15 18:31:29.358317: step 44610, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 52h:24m:51s remains)
INFO - root - 2017-12-15 18:31:35.977542: step 44620, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.675 sec/batch; 54h:00m:50s remains)
INFO - root - 2017-12-15 18:31:42.543350: step 44630, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 52h:19m:44s remains)
INFO - root - 2017-12-15 18:31:49.083782: step 44640, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 52h:55m:41s remains)
INFO - root - 2017-12-15 18:31:55.804536: step 44650, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 52h:55m:47s remains)
INFO - root - 2017-12-15 18:32:02.415372: step 44660, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 51h:30m:11s remains)
INFO - root - 2017-12-15 18:32:09.015526: step 44670, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 53h:31m:42s remains)
INFO - root - 2017-12-15 18:32:15.687101: step 44680, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 53h:10m:27s remains)
INFO - root - 2017-12-15 18:32:22.256342: step 44690, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 52h:44m:33s remains)
INFO - root - 2017-12-15 18:32:28.807300: step 44700, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 53h:07m:09s remains)
2017-12-15 18:32:29.301294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1951146 -6.4940166 -5.3737378 -3.6979375 -3.8821807 -3.1771228 -2.3026359 -0.98865271 -0.2644906 0.46578026 -0.90817308 -4.59775 -8.6499519 -10.068027 -8.6102829][-6.8409443 -8.2500916 -8.6707764 -6.9669628 -5.3853092 -4.3149586 -2.4517322 -0.73533916 -0.81837893 -1.4525523 -1.644032 -4.94939 -9.2441807 -10.156582 -10.173928][-5.1790333 -6.407341 -8.4204454 -7.9004922 -7.8657656 -5.7679682 -2.7576454 -1.1662669 -0.93495274 -1.9101937 -3.3528826 -7.0632796 -10.360767 -11.60544 -10.02495][-4.4178681 -5.67755 -6.7744465 -6.2355742 -6.8701315 -5.9079642 -4.5938954 -2.7871561 -1.74755 -2.5083849 -4.1337061 -8.9340248 -12.890333 -12.194796 -9.9986935][-5.8400459 -5.9033823 -5.7614493 -5.0351596 -5.3649244 -3.5727904 -1.7753899 -2.4898105 -3.0325921 -3.4007483 -4.4464765 -8.4480858 -12.572538 -12.891232 -10.610175][-8.2845612 -6.6208773 -6.5018578 -4.8328705 -4.3822193 -1.6958556 0.42387772 0.29031944 -0.68582392 -3.1860456 -5.3439078 -8.6685524 -11.958663 -13.078581 -10.934853][-8.5961046 -7.6625814 -7.4155636 -5.0183969 -3.9777324 -0.69557095 2.3426671 3.4812551 2.9850154 -0.68320704 -4.8847337 -8.0059233 -10.350221 -10.962108 -10.491632][-7.5349188 -6.959589 -6.7739434 -3.9124451 -1.9804144 1.0668311 3.98667 4.857758 4.4725175 1.6879978 -1.546618 -7.0487766 -11.323927 -11.229044 -9.8076477][-6.3767767 -5.2934623 -5.1518278 -2.6358891 -1.8484128 1.6653147 4.2323432 5.4260249 5.0390697 1.426631 -1.7274573 -6.7408147 -11.362149 -12.867357 -11.463629][-5.5042391 -4.8935242 -4.8378119 -2.8004038 -2.1953919 -0.20479679 0.89888334 2.8736053 3.4543023 1.5362172 -1.4051003 -7.0841179 -11.628377 -12.968583 -11.786591][-6.003006 -7.0778575 -7.323885 -6.4260736 -5.2439246 -3.6310565 -2.2337062 -1.1182342 -1.1022849 -1.704318 -2.9004579 -8.0368252 -11.788252 -13.692585 -12.699219][-9.8499222 -8.7587309 -8.0659266 -7.3817406 -7.5236654 -6.6867409 -6.6174397 -6.95394 -6.9823895 -6.4309182 -7.0570092 -9.3041468 -10.942513 -11.538334 -10.532612][-13.421703 -11.286299 -8.8071594 -8.3672934 -8.6237011 -8.7876081 -9.4001751 -8.6306448 -8.1712923 -7.9572582 -8.006731 -9.4567661 -11.739895 -11.225133 -8.9195471][-12.265514 -10.787008 -8.3062849 -8.1301451 -8.0961227 -7.1499667 -7.4596338 -8.1047421 -7.7453246 -8.2955847 -8.1119175 -6.6599813 -6.7823572 -8.9412975 -9.3935785][-9.2230272 -7.3796258 -6.5527225 -5.7876453 -5.8993835 -6.6554737 -6.6019378 -5.6751609 -5.3850183 -5.8729706 -6.53593 -8.8431454 -9.4228973 -8.3023987 -8.8355751]]...]
INFO - root - 2017-12-15 18:32:35.886475: step 44710, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 53h:09m:23s remains)
INFO - root - 2017-12-15 18:32:42.500300: step 44720, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 53h:19m:08s remains)
INFO - root - 2017-12-15 18:32:49.070981: step 44730, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 54h:18m:41s remains)
INFO - root - 2017-12-15 18:32:55.710305: step 44740, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 53h:46m:13s remains)
INFO - root - 2017-12-15 18:33:02.244944: step 44750, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 51h:49m:48s remains)
INFO - root - 2017-12-15 18:33:08.919761: step 44760, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 52h:29m:35s remains)
INFO - root - 2017-12-15 18:33:15.554497: step 44770, loss = 0.17, batch loss = 0.13 (11.5 examples/sec; 0.698 sec/batch; 55h:46m:15s remains)
INFO - root - 2017-12-15 18:33:22.191527: step 44780, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 51h:09m:45s remains)
INFO - root - 2017-12-15 18:33:28.852595: step 44790, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 53h:11m:44s remains)
INFO - root - 2017-12-15 18:33:35.603109: step 44800, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.689 sec/batch; 55h:05m:54s remains)
2017-12-15 18:33:36.123922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5588107 -6.0359249 -5.4292665 -5.7452803 -6.2066708 -7.1525879 -7.8021526 -8.3492355 -8.3985462 -7.5976434 -7.0614939 -8.1100311 -9.4175587 -10.461463 -9.0412846][-5.8782792 -6.7431259 -7.1069145 -7.6032529 -8.8376389 -9.9169769 -10.37314 -10.427527 -10.234092 -9.7664776 -9.7473755 -10.269897 -11.666084 -12.674857 -10.184617][-5.620337 -6.3637338 -6.7393289 -7.6511693 -8.6750536 -9.6234093 -9.9106369 -9.9811764 -9.9818192 -9.7008038 -9.5364876 -10.249329 -11.798996 -13.057965 -11.02018][-5.5333638 -5.9159527 -6.8408718 -7.31325 -7.9012 -7.1498508 -6.612711 -6.7958584 -7.0673203 -7.0893512 -7.146318 -8.8546066 -11.535458 -12.902361 -11.67209][-7.1578569 -7.8041105 -8.0769062 -7.4686751 -6.7370844 -4.6442318 -3.0485668 -3.0523741 -3.2730067 -3.7266076 -4.1304278 -6.2379489 -9.3262835 -12.315863 -11.575611][-7.8219843 -7.7906342 -7.6990128 -6.2563586 -3.964148 -1.2488394 1.1675582 2.0167298 1.9303622 0.83852768 -1.0593076 -3.8641849 -7.5310063 -10.239977 -9.3074646][-8.1758232 -8.2413626 -7.227087 -4.5666251 -1.5379076 2.1224885 5.2701468 6.7041316 6.8073144 3.6390347 0.60894012 -2.1822619 -5.5854373 -8.8096895 -7.9187946][-7.6959357 -6.8983827 -5.1359253 -2.6633773 -0.90549374 2.6692986 6.2603507 7.3718562 7.5910058 5.3969274 2.5590415 -1.1852283 -5.4882078 -7.8044767 -6.596344][-6.5993223 -5.9771814 -4.3345642 -1.3589501 0.16668129 2.8529782 5.2161593 6.321981 6.0978646 3.6893725 1.3822532 -1.7987516 -5.2772827 -8.2482433 -7.2459841][-3.6494668 -3.1849771 -2.3050063 -0.62480164 0.14799166 2.1670465 2.699409 2.9168019 2.25741 0.57323122 -0.613492 -3.3436618 -6.8589668 -9.7540617 -9.4090538][-6.5096207 -7.0395923 -6.642386 -4.685029 -3.9760809 -2.5445716 -2.2491806 -2.5857613 -4.0711284 -5.5753942 -6.3378882 -8.2745094 -9.8128929 -11.701329 -10.335268][-10.730401 -11.034891 -10.080957 -8.1441956 -7.7613182 -6.9976358 -7.1176825 -7.4459887 -8.2794466 -9.1380711 -9.6808844 -10.353844 -10.19762 -11.36038 -9.6765394][-11.917812 -12.007648 -10.302202 -7.872508 -6.3990116 -5.7301555 -5.7985387 -6.1500778 -7.1405315 -8.1249485 -8.419239 -9.1871672 -8.9667215 -9.1573315 -6.0151868][-12.604666 -10.892132 -9.3305569 -6.8112803 -5.179049 -4.4645958 -4.6170197 -5.035008 -5.6424437 -5.9856315 -6.3558793 -6.6772785 -6.5025053 -6.0819654 -4.5391688][-10.042515 -9.180274 -7.5555487 -6.33848 -4.4360533 -4.0001335 -4.0632477 -4.9195833 -5.5344586 -5.458497 -5.5260262 -6.0222855 -6.7188025 -6.5165372 -5.9602323]]...]
INFO - root - 2017-12-15 18:33:42.736522: step 44810, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 52h:16m:47s remains)
INFO - root - 2017-12-15 18:33:49.322490: step 44820, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 51h:59m:29s remains)
INFO - root - 2017-12-15 18:33:55.918811: step 44830, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 51h:47m:44s remains)
INFO - root - 2017-12-15 18:34:02.553429: step 44840, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 54h:03m:09s remains)
INFO - root - 2017-12-15 18:34:09.228630: step 44850, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.653 sec/batch; 52h:11m:25s remains)
INFO - root - 2017-12-15 18:34:15.889965: step 44860, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.675 sec/batch; 53h:54m:24s remains)
INFO - root - 2017-12-15 18:34:22.484080: step 44870, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 53h:17m:10s remains)
INFO - root - 2017-12-15 18:34:29.057956: step 44880, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 53h:42m:53s remains)
INFO - root - 2017-12-15 18:34:35.688577: step 44890, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 52h:49m:22s remains)
INFO - root - 2017-12-15 18:34:42.364132: step 44900, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.692 sec/batch; 55h:16m:05s remains)
2017-12-15 18:34:42.864236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3599157 -5.1581588 -5.2679944 -4.7837362 -4.6159506 -5.4621854 -6.6857758 -7.48999 -8.2826614 -7.577033 -6.6767564 -8.4868984 -9.4434242 -7.3235383 -5.1320724][-3.9320519 -5.0035319 -5.11509 -4.8603983 -5.0938063 -5.5771294 -6.1755104 -7.8025537 -8.29233 -7.3725209 -7.0903335 -8.6272125 -9.2701225 -8.3347464 -6.8654461][-3.0722342 -4.4492869 -5.222578 -5.6142707 -5.8316307 -5.9263077 -6.581192 -7.3799353 -7.4601116 -7.0113931 -6.3324308 -7.7751646 -9.3130226 -8.4583826 -7.5559883][-6.0468159 -7.3010345 -7.5006771 -6.5578184 -5.6429963 -5.8181853 -5.9414854 -6.4171691 -6.678926 -5.9491577 -4.8417549 -6.5621824 -8.33679 -8.1511631 -7.5852518][-7.6176724 -9.9183645 -10.190525 -7.8965626 -5.4503036 -3.6709161 -2.6789846 -4.1211543 -5.6673131 -5.1882305 -4.4641786 -6.4271226 -8.5884666 -8.3140965 -7.9571238][-8.3845 -9.974575 -10.033896 -7.0252609 -3.3854985 -0.075775146 2.4027028 1.2893791 -1.1130052 -2.693532 -3.6667843 -5.2852631 -6.861702 -6.82517 -7.2791443][-8.1345358 -9.2440577 -8.4189606 -4.4037108 -1.342032 2.108686 5.9048886 5.5270209 3.892529 1.0001822 -1.9806678 -5.0909133 -6.8360844 -6.332284 -7.0263577][-8.6367531 -8.7188282 -7.1628675 -3.1276922 0.37505484 3.8956695 6.1597848 6.0025773 5.6131186 3.5149283 0.53089285 -4.11238 -7.1708074 -6.9692841 -6.4451427][-7.5671616 -7.0281291 -5.4528255 -2.8474207 -0.095649242 1.8434935 2.8963389 2.8254561 2.9682822 2.0461693 0.84603834 -3.8997822 -8.0766735 -8.0259113 -8.2459984][-6.6740913 -6.2672682 -5.9805169 -3.3389959 -1.2390718 -0.61301422 0.78567934 -0.27094221 -1.2036724 -0.72085333 -0.41385126 -4.711237 -7.9681878 -8.0319386 -8.95561][-7.1049285 -8.2709866 -9.0125208 -6.818614 -5.8904047 -3.5242093 -1.5197067 -3.1011937 -3.9534254 -3.68426 -4.2278867 -8.1976728 -10.275971 -10.816542 -10.687527][-10.503288 -11.056654 -11.399362 -10.276275 -9.6308346 -7.8430996 -7.0280581 -6.9206724 -6.3200626 -6.9667025 -7.6547217 -10.013785 -11.543945 -11.668738 -11.639992][-11.05719 -10.118569 -9.4995365 -10.046633 -10.250517 -9.902935 -9.4475746 -7.8899727 -6.7700272 -7.4088416 -8.0074215 -10.651543 -11.484192 -10.27249 -9.0797768][-9.03138 -8.9046326 -8.4859343 -7.9327755 -8.2844915 -8.874403 -8.3703337 -7.4058504 -7.9422321 -8.031189 -8.1765251 -9.4350376 -9.1367273 -8.8592615 -8.2218523][-6.6504197 -6.7077341 -6.2620726 -5.380044 -4.7939138 -4.6848 -4.5823293 -5.5847344 -6.0800705 -6.5089445 -7.3216572 -9.0420656 -9.8488064 -8.709054 -7.906312]]...]
INFO - root - 2017-12-15 18:34:49.461188: step 44910, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.695 sec/batch; 55h:30m:59s remains)
INFO - root - 2017-12-15 18:34:56.070750: step 44920, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 53h:09m:59s remains)
INFO - root - 2017-12-15 18:35:02.642379: step 44930, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.656 sec/batch; 52h:24m:45s remains)
INFO - root - 2017-12-15 18:35:09.248599: step 44940, loss = 0.18, batch loss = 0.13 (11.6 examples/sec; 0.688 sec/batch; 54h:55m:11s remains)
INFO - root - 2017-12-15 18:35:15.828861: step 44950, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.635 sec/batch; 50h:42m:24s remains)
INFO - root - 2017-12-15 18:35:22.489516: step 44960, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 53h:53m:40s remains)
INFO - root - 2017-12-15 18:35:29.110940: step 44970, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 54h:20m:48s remains)
INFO - root - 2017-12-15 18:35:35.742732: step 44980, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 52h:31m:19s remains)
INFO - root - 2017-12-15 18:35:42.366120: step 44990, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 50h:44m:44s remains)
INFO - root - 2017-12-15 18:35:48.979639: step 45000, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 53h:06m:24s remains)
2017-12-15 18:35:49.586091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1299906 -7.0225115 -5.1298361 -4.0847378 -4.0332513 -4.9059639 -5.5407639 -5.3747339 -4.5642042 -4.4958177 -4.5286436 -5.4218335 -9.2822962 -10.509432 -11.050222][-6.4596925 -6.6451159 -5.6625056 -5.4999623 -5.2990546 -5.3158526 -5.2619333 -4.8969707 -4.2297187 -3.53724 -3.4442654 -4.9543338 -9.3615322 -11.28344 -12.523136][-4.07631 -5.4811664 -6.6193895 -6.9265771 -6.6650453 -6.0640116 -4.5722733 -3.5964363 -2.7212768 -2.1865227 -2.3869734 -3.4848495 -6.8663468 -9.0493221 -11.042154][-3.1811409 -5.5824885 -7.5571141 -8.4997673 -8.3392582 -6.548872 -4.8377333 -3.6512187 -2.5096452 -1.8117497 -1.5668864 -3.0436044 -7.2149935 -9.0734816 -10.433803][-3.6175616 -5.6829548 -7.8785458 -8.7308292 -7.7881804 -4.7913542 -2.2192786 -2.2822483 -2.4341326 -1.4684072 -0.52621174 -1.6080575 -5.7531505 -7.723176 -9.4766159][-5.6350384 -5.0721049 -5.235847 -5.9443316 -3.9812317 -0.13013744 3.0184588 3.0829816 0.5391264 -1.3928561 -2.304992 -2.223218 -4.9610066 -7.3537254 -9.4221821][-8.0440607 -7.0170665 -5.2019882 -3.9556072 -1.5304174 2.3353066 6.7321649 7.9512181 5.9624038 1.4347329 -3.0976448 -3.5402381 -6.755991 -8.49041 -9.9125652][-11.876116 -10.542072 -8.4557314 -4.803565 -0.73638058 3.4790721 7.9631076 8.5292435 7.4846406 4.3367009 -0.048163414 -3.5129802 -9.1591835 -10.17321 -10.904953][-13.707512 -13.539005 -11.668633 -8.0150709 -4.5328922 -0.025915146 4.5526404 5.2784076 5.3644996 2.6083922 -0.78216839 -3.7606711 -9.429162 -11.751276 -12.280279][-11.942427 -12.456955 -11.234611 -8.2728825 -5.7350469 -2.4799793 1.0570407 2.5234609 2.193471 -1.1331053 -4.2138414 -6.5873079 -11.377277 -13.166094 -13.180178][-13.033106 -13.231047 -12.836718 -10.704939 -8.7789888 -6.3791175 -3.9637418 -3.3212585 -3.5124774 -5.4406166 -7.9640923 -10.11087 -13.325972 -13.665163 -12.304547][-14.407593 -14.873295 -14.078386 -11.970244 -10.84024 -8.9431992 -7.3399162 -7.4018135 -8.31647 -9.7853756 -11.066193 -11.930424 -13.162958 -12.567784 -11.438507][-14.504545 -13.41769 -12.131577 -11.028666 -10.32618 -9.1135569 -8.7058954 -8.8924789 -9.4054651 -10.323028 -10.807718 -11.508083 -11.574671 -10.349133 -8.73138][-12.47924 -10.789588 -9.9603443 -9.1973591 -9.1720428 -8.3875523 -8.4750595 -8.9896336 -8.9666815 -9.3131447 -9.4145088 -8.5343761 -8.0268269 -6.6661797 -6.1327419][-8.9970274 -7.9374909 -7.7458553 -7.0096521 -6.3250432 -6.1395545 -6.7160659 -7.0317926 -7.8391132 -7.4640942 -6.9138317 -6.9682503 -7.0977049 -6.2632494 -5.8166647]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 18:35:57.280530: step 45010, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 51h:05m:11s remains)
INFO - root - 2017-12-15 18:36:03.841722: step 45020, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 52h:16m:08s remains)
INFO - root - 2017-12-15 18:36:10.433347: step 45030, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 51h:04m:19s remains)
INFO - root - 2017-12-15 18:36:16.959721: step 45040, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 51h:32m:03s remains)
INFO - root - 2017-12-15 18:36:23.494374: step 45050, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 51h:55m:14s remains)
INFO - root - 2017-12-15 18:36:30.180448: step 45060, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.680 sec/batch; 54h:18m:10s remains)
INFO - root - 2017-12-15 18:36:36.797645: step 45070, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 54h:08m:50s remains)
INFO - root - 2017-12-15 18:36:43.362772: step 45080, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.637 sec/batch; 50h:53m:36s remains)
INFO - root - 2017-12-15 18:36:49.961633: step 45090, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.673 sec/batch; 53h:45m:24s remains)
INFO - root - 2017-12-15 18:36:56.534781: step 45100, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 52h:01m:38s remains)
2017-12-15 18:36:57.082899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0454321 -5.7188969 -5.4850283 -5.3591108 -5.6247659 -6.351265 -6.9368424 -6.7475924 -5.4589405 -3.1564553 -1.1878486 -0.82206011 -3.084748 -4.2204738 -4.7713618][-7.0018291 -6.9298978 -6.0210738 -5.3604965 -6.0645618 -7.1525927 -7.8745666 -7.8189511 -6.8049679 -5.1267257 -3.2169251 -2.5606585 -4.6357808 -5.9978862 -5.8123417][-5.7687154 -6.2523246 -6.6106615 -6.0298839 -6.1882219 -6.7981834 -7.2524691 -7.1220713 -6.168479 -4.947083 -4.0282388 -4.5330229 -7.3067245 -8.6898623 -8.5873909][-7.0951319 -7.1772857 -5.619287 -5.4263868 -5.6702619 -5.6638861 -5.9173851 -5.5221834 -4.8191462 -4.3354897 -3.826602 -5.1555691 -8.7544546 -10.343876 -10.117712][-8.5407257 -8.221693 -7.1067958 -5.128334 -3.8383334 -2.7241778 -2.3049383 -2.756392 -3.1682029 -2.9270995 -3.1399775 -5.0639491 -8.8501282 -10.618782 -10.442092][-9.6689157 -8.4814091 -5.6768136 -3.2703109 -1.577651 0.64253139 1.2682261 1.160429 0.33699322 -0.88024282 -2.3751383 -4.4662123 -8.4043255 -9.9705791 -9.65114][-8.6332874 -7.1942992 -4.2326412 -1.2332048 0.58938551 2.0322576 3.1757264 4.0216966 4.1280503 2.2646666 -0.37688732 -3.0321927 -7.1963887 -8.9810925 -8.6129522][-7.775239 -6.0435586 -2.6890972 -0.053198814 1.5556345 3.2806106 4.3787 4.8569074 4.9373488 3.9458108 2.6627049 -0.50371027 -5.6233797 -7.3753452 -6.9593391][-5.2643394 -4.5068884 -2.8728294 -1.1169553 -0.26434946 1.6653714 3.3851495 4.1589961 4.3497634 3.630712 2.2557993 -0.086158752 -4.03109 -5.8262563 -5.63178][-4.0440464 -3.7750459 -2.9273148 -2.1047008 -2.0750942 -0.61150074 1.1717935 2.3813462 2.7123046 1.8809166 0.93605804 -0.0078911781 -2.723546 -3.8248534 -3.9122534][-7.7943869 -6.8407245 -5.8420253 -4.9045982 -4.5622978 -3.7893991 -3.0654485 -2.6640153 -2.9019754 -3.2972987 -3.5268133 -3.7202339 -4.5130191 -4.2245722 -3.3896551][-10.772806 -10.031441 -7.9616137 -7.0610147 -7.0781007 -6.8455005 -7.0123177 -7.2346535 -6.9030657 -7.0175314 -6.7828741 -7.0705609 -7.6754446 -7.0249319 -4.8388114][-11.661659 -9.5383987 -8.0516052 -7.3451433 -7.3592629 -7.4752307 -7.0659609 -7.3041992 -7.3059587 -6.7951927 -6.2270751 -6.7240043 -7.2344446 -6.3101559 -4.3374133][-9.9616575 -8.066946 -6.6620097 -5.4206676 -5.6257658 -5.7052665 -6.2081442 -6.2545338 -6.4494548 -6.4018273 -6.4288926 -6.0228057 -6.0758209 -5.3366008 -4.3107724][-6.7119546 -6.7859097 -6.1385489 -4.1118107 -3.5640018 -4.1931229 -4.335907 -4.173552 -4.4950018 -4.5593052 -4.74339 -5.3144908 -6.2869811 -6.7877574 -6.8553553]]...]
INFO - root - 2017-12-15 18:37:03.673992: step 45110, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.683 sec/batch; 54h:31m:04s remains)
INFO - root - 2017-12-15 18:37:10.256499: step 45120, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 54h:30m:53s remains)
INFO - root - 2017-12-15 18:37:16.832767: step 45130, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.687 sec/batch; 54h:51m:11s remains)
INFO - root - 2017-12-15 18:37:23.372066: step 45140, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 52h:32m:35s remains)
INFO - root - 2017-12-15 18:37:29.949306: step 45150, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 53h:04m:53s remains)
INFO - root - 2017-12-15 18:37:36.477514: step 45160, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.666 sec/batch; 53h:07m:35s remains)
INFO - root - 2017-12-15 18:37:43.051945: step 45170, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 52h:37m:27s remains)
INFO - root - 2017-12-15 18:37:49.656340: step 45180, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 51h:47m:15s remains)
INFO - root - 2017-12-15 18:37:56.190156: step 45190, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 52h:06m:22s remains)
INFO - root - 2017-12-15 18:38:02.738568: step 45200, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 53h:47m:40s remains)
2017-12-15 18:38:03.307029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0173147 -3.8722839 -4.0324731 -5.648881 -7.6501617 -8.3203316 -8.6042442 -7.4995213 -5.7914896 -4.3956676 -2.8732293 -3.9362788 -5.1851048 -6.3737917 -7.1555953][-3.1810906 -2.2979412 -2.3501372 -3.4936423 -5.2946262 -6.7643695 -7.4632463 -6.7101412 -4.271575 -2.6345139 -1.5779395 -2.5517943 -3.607065 -5.4108963 -6.5243578][0.16450071 -0.33953285 -2.0511262 -2.4061975 -3.1595521 -4.3353443 -5.0577908 -4.3577442 -2.9498045 -1.7822435 -0.34746361 -1.472157 -3.435766 -4.6304712 -5.1369419][-2.2869682 -1.6856046 -1.5141797 -1.9993374 -2.8774436 -3.1416273 -3.2603176 -3.6057341 -3.1881077 -2.6173558 -2.0391595 -3.8245859 -5.6677318 -6.7040968 -6.8307962][-3.8156912 -4.9344735 -4.68277 -2.9492776 -2.3919134 -1.45611 0.045693398 -1.3501639 -2.6649263 -2.39635 -2.5786273 -4.6764536 -6.2216296 -7.3064408 -8.009222][-8.2066975 -6.9294872 -4.7370806 -1.59131 0.68324852 2.9608178 4.5104642 2.8575206 1.0019846 -0.9948082 -3.4169066 -4.6931615 -5.5105567 -7.2492933 -8.1849241][-11.898367 -10.306482 -7.0456104 -1.8413808 1.4201713 4.3381209 7.3142562 7.5325589 6.2979083 2.0023398 -2.4449005 -5.2274966 -7.4876556 -8.6330357 -8.3425045][-13.772348 -10.997142 -6.7695112 -1.4626656 1.4135151 4.5199256 7.6483569 7.0863528 6.4982114 3.4699197 -1.0621724 -4.8756 -8.0446033 -9.2264814 -8.2017155][-10.838348 -10.104539 -7.29486 -3.2668715 -0.56167078 2.8084702 5.3736386 3.4013066 2.2554545 0.40780497 -2.6433189 -6.3696127 -9.7440186 -10.655251 -9.5927715][-9.24878 -7.8886566 -6.4346666 -4.42175 -2.5140898 -0.93417168 -0.069048405 -0.14476728 0.14898396 -2.4447415 -5.2564654 -7.1601925 -9.4694386 -10.654015 -9.9222431][-13.024363 -12.154547 -10.902735 -8.1955652 -7.5701928 -7.5007458 -6.7086596 -6.4459982 -6.2314663 -6.6056757 -7.0888157 -9.5916615 -11.58989 -11.047167 -9.4158621][-16.375727 -15.384205 -14.327986 -13.041317 -12.561001 -12.152708 -12.028111 -11.90287 -11.466747 -11.36578 -11.086472 -12.310943 -13.223715 -12.021607 -10.768345][-15.379734 -14.77825 -14.343548 -13.115877 -12.392951 -12.420055 -12.407708 -12.27841 -13.266887 -13.575329 -13.142179 -12.901522 -12.636316 -11.150633 -9.0314045][-12.667961 -12.458956 -12.786526 -11.205307 -9.6227551 -9.588932 -9.8226814 -9.6067419 -9.9131813 -11.079758 -11.971701 -11.796506 -12.142424 -10.02213 -7.6821179][-9.1072445 -9.8227825 -10.141953 -8.9257889 -7.585146 -6.4989305 -5.7024632 -5.8135505 -6.8744712 -7.2300863 -7.9193296 -8.8112612 -9.5903978 -9.4274378 -8.2324]]...]
INFO - root - 2017-12-15 18:38:09.922141: step 45210, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.691 sec/batch; 55h:10m:11s remains)
INFO - root - 2017-12-15 18:38:16.490120: step 45220, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 52h:43m:35s remains)
INFO - root - 2017-12-15 18:38:23.011074: step 45230, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 51h:39m:55s remains)
INFO - root - 2017-12-15 18:38:29.542687: step 45240, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 52h:46m:38s remains)
INFO - root - 2017-12-15 18:38:36.135924: step 45250, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 53h:26m:43s remains)
INFO - root - 2017-12-15 18:38:42.700165: step 45260, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 51h:45m:24s remains)
INFO - root - 2017-12-15 18:38:49.267137: step 45270, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 52h:23m:59s remains)
INFO - root - 2017-12-15 18:38:55.829015: step 45280, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 52h:44m:17s remains)
INFO - root - 2017-12-15 18:39:02.353530: step 45290, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 51h:54m:02s remains)
INFO - root - 2017-12-15 18:39:08.871551: step 45300, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 51h:35m:51s remains)
2017-12-15 18:39:09.398972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5830975 -8.6841679 -10.074004 -10.192635 -10.196477 -9.446311 -9.7338295 -10.138319 -10.515844 -10.935637 -10.394423 -9.9399281 -11.621306 -12.646111 -11.801994][-7.7686605 -10.175236 -11.040275 -12.045023 -11.41797 -10.276003 -10.20857 -10.777397 -11.631279 -11.510164 -10.998417 -12.022429 -13.93726 -14.844658 -14.609415][-6.46147 -9.6131744 -11.479766 -12.001524 -11.959986 -11.416624 -11.061659 -11.348392 -11.683123 -11.801502 -11.539828 -11.718876 -14.468513 -16.872143 -16.780365][-7.4742727 -9.5185528 -11.364029 -11.497609 -10.06513 -8.7554789 -8.8252258 -10.204905 -10.807337 -10.592072 -11.003143 -11.99472 -14.787849 -16.957792 -17.955332][-8.8487463 -12.137999 -14.503475 -13.015585 -8.7283077 -2.4906745 -0.28763247 -4.774065 -8.4664745 -8.4224253 -8.8026361 -9.7289333 -13.102505 -15.494377 -16.167673][-10.097437 -12.51664 -13.690174 -12.520029 -8.6026363 -0.14193583 7.0916677 5.0842872 -0.50835419 -4.5946479 -7.0242047 -6.9156919 -9.9015474 -12.24124 -13.08589][-10.212435 -12.598628 -11.505754 -8.9733286 -6.2972126 0.42877913 8.3425484 10.287464 8.1819077 0.13792801 -6.4785929 -6.3123589 -8.9889984 -11.828621 -12.267834][-9.3820171 -11.901545 -11.574144 -7.262773 -2.2192619 3.2315564 7.7080054 8.9192352 9.2870159 3.9464507 -2.1891487 -4.9139247 -9.5063753 -11.676203 -11.447536][-6.6543875 -8.5780239 -10.370365 -9.183094 -4.8996086 1.1482072 5.20332 5.5577559 5.7985673 2.0683966 -1.6942501 -5.0113316 -9.4930153 -12.305641 -14.283619][-5.1021829 -6.8016491 -9.4716463 -9.6767216 -8.0517731 -3.8238406 0.34472704 2.4286952 2.310411 -0.90100145 -3.2421579 -6.2689781 -9.9936 -12.330185 -14.703474][-10.366981 -11.736444 -12.980581 -13.175669 -13.233206 -10.947088 -8.2596188 -6.106689 -5.6588349 -6.4813595 -7.8640366 -9.1515789 -10.382241 -12.863407 -13.998697][-14.090864 -15.023342 -16.469065 -16.204147 -15.949705 -13.684999 -12.777486 -12.397338 -12.15435 -11.577398 -11.158133 -11.61989 -11.730547 -13.794821 -14.639917][-12.284735 -13.111767 -14.122274 -15.288212 -15.592491 -13.65949 -13.149856 -13.521614 -13.926243 -12.731559 -10.916119 -11.246585 -10.379156 -11.142404 -10.488027][-10.645874 -9.6017876 -10.37006 -10.653066 -11.203758 -10.910846 -10.278874 -10.890856 -12.486309 -12.963829 -12.136803 -10.205037 -8.1293869 -8.9507837 -9.46123][-8.3120184 -7.764492 -6.5367708 -5.5752082 -5.1833735 -5.4105535 -6.0663695 -6.9409423 -8.6085987 -9.4324818 -10.385536 -10.17104 -9.8414116 -9.7112112 -9.1135092]]...]
INFO - root - 2017-12-15 18:39:15.986446: step 45310, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 51h:56m:21s remains)
INFO - root - 2017-12-15 18:39:22.542241: step 45320, loss = 0.24, batch loss = 0.20 (12.2 examples/sec; 0.657 sec/batch; 52h:25m:40s remains)
INFO - root - 2017-12-15 18:39:29.194759: step 45330, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 54h:32m:10s remains)
INFO - root - 2017-12-15 18:39:35.860229: step 45340, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 52h:50m:42s remains)
INFO - root - 2017-12-15 18:39:42.458743: step 45350, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 53h:16m:29s remains)
INFO - root - 2017-12-15 18:39:49.059250: step 45360, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.655 sec/batch; 52h:14m:57s remains)
INFO - root - 2017-12-15 18:39:55.582281: step 45370, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 51h:46m:26s remains)
INFO - root - 2017-12-15 18:40:02.130319: step 45380, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 52h:37m:41s remains)
INFO - root - 2017-12-15 18:40:08.651859: step 45390, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 51h:43m:58s remains)
INFO - root - 2017-12-15 18:40:15.204904: step 45400, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 51h:05m:36s remains)
2017-12-15 18:40:15.704201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-11.586363 -12.679725 -12.794682 -13.211658 -13.553119 -13.15559 -12.158606 -9.8241577 -8.2733574 -7.9448318 -8.7188864 -10.718115 -13.907738 -14.292999 -11.786044][-10.622744 -10.469625 -10.482245 -11.512989 -12.526251 -12.846972 -12.297207 -11.272013 -10.612068 -9.3922272 -9.550024 -10.872799 -13.53351 -14.133171 -12.943869][-4.9912624 -6.5089016 -8.0898247 -9.859189 -10.780905 -10.617237 -9.8609495 -8.98928 -8.6821585 -7.77181 -7.0389156 -7.8815289 -11.145512 -12.943087 -13.085018][-3.099077 -2.7558749 -3.7145021 -5.6850495 -7.0811505 -6.6207762 -5.7413144 -5.8064966 -6.6224051 -6.7144971 -6.8744512 -7.4606767 -9.3505859 -10.283895 -10.56494][-4.0114288 -4.4268727 -5.0626106 -4.24165 -3.7491212 -2.2417054 -0.66081619 -1.7073517 -3.3497958 -3.8401794 -5.0091219 -6.6115465 -9.3212471 -10.72754 -9.6477928][-6.6289897 -6.8743448 -6.4744196 -4.8046589 -3.131573 0.046929836 3.08435 3.0770545 1.8797479 -0.26312733 -2.3849127 -3.9786341 -7.3461661 -8.71947 -8.3121185][-8.1027861 -8.1856823 -7.0540886 -4.4426084 -2.0747423 1.551559 4.763742 5.860395 6.8760781 4.5309587 1.1629462 -1.422894 -5.688313 -7.7428389 -6.9398427][-7.4599662 -6.8004508 -5.8727379 -3.4561167 -0.24565029 2.933661 5.2920413 5.7749257 6.1881967 4.2791781 1.6817064 -1.0379853 -4.9111395 -5.9583178 -4.6994724][-4.5149603 -3.388442 -3.2367864 -1.5351243 0.50708342 2.4687047 3.8016915 4.5326009 4.5465016 2.808435 0.92573261 -2.1721497 -5.7014289 -6.5195851 -4.8273506][-4.0198889 -2.7501483 -1.1075296 -0.45707703 0.11453915 1.017108 2.2983165 2.3665462 1.2634921 -0.22300291 -1.6112719 -3.5703602 -6.31627 -6.9384389 -5.6717048][-7.3953009 -5.04476 -4.2432413 -4.2488327 -3.9576635 -2.9197371 -0.89699793 -0.22000074 -1.8209426 -4.0016828 -4.8070865 -6.5003071 -8.3835526 -8.2289562 -6.0127425][-10.172234 -8.485714 -6.9141345 -6.6923795 -7.0497808 -5.946866 -4.7864342 -4.2898536 -5.2994876 -5.9721279 -5.629426 -5.769949 -6.6157408 -6.9563932 -6.5335851][-10.158062 -9.4070473 -8.578496 -8.2990284 -7.3887916 -5.7850032 -4.9897346 -4.8701482 -5.5811124 -6.7523727 -6.6846628 -6.7939653 -6.8839874 -6.275167 -5.4094129][-9.1059446 -8.6466579 -8.9065065 -8.0321941 -7.6131864 -6.0751748 -4.9501948 -4.8622565 -5.51382 -5.9038 -6.3586655 -5.5361576 -5.0464869 -5.4841094 -5.7459059][-7.0367441 -6.3292074 -5.7843065 -5.8915114 -5.0427284 -4.0541735 -3.5228212 -4.0268774 -4.9960203 -5.9201279 -6.6347566 -7.8383732 -8.39392 -8.541069 -7.6673975]]...]
INFO - root - 2017-12-15 18:40:22.231085: step 45410, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 51h:47m:11s remains)
INFO - root - 2017-12-15 18:40:28.870083: step 45420, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 52h:17m:59s remains)
INFO - root - 2017-12-15 18:40:35.483495: step 45430, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 52h:01m:37s remains)
INFO - root - 2017-12-15 18:40:42.049706: step 45440, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.687 sec/batch; 54h:46m:24s remains)
INFO - root - 2017-12-15 18:40:48.569248: step 45450, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 51h:49m:59s remains)
INFO - root - 2017-12-15 18:40:55.072288: step 45460, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 51h:16m:09s remains)
INFO - root - 2017-12-15 18:41:01.558383: step 45470, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 54h:21m:09s remains)
INFO - root - 2017-12-15 18:41:08.138183: step 45480, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 51h:17m:09s remains)
INFO - root - 2017-12-15 18:41:14.791879: step 45490, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 53h:26m:59s remains)
INFO - root - 2017-12-15 18:41:21.429006: step 45500, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 52h:08m:10s remains)
2017-12-15 18:41:21.894939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3569822 -8.0088367 -7.9562473 -8.7970228 -9.6623573 -9.546299 -8.6177559 -6.6986179 -4.8342657 -4.3550215 -3.8794506 -6.3120875 -7.8468237 -7.1101751 -5.6986432][-7.7837563 -7.0070934 -6.2267575 -7.1424522 -8.10222 -7.906436 -7.0708828 -5.8063388 -5.1556745 -4.1371279 -3.8144455 -6.3486319 -8.141922 -7.232688 -6.3102894][-6.5885086 -6.0892987 -5.0169773 -4.8654027 -6.4243841 -6.7629695 -5.7079883 -4.419755 -3.5943367 -3.5300863 -4.0930314 -6.4517307 -7.6830311 -7.4674845 -7.7807693][-8.1233673 -6.68341 -4.8396349 -4.7072191 -5.945662 -5.8231506 -4.9706988 -4.3016586 -3.7511835 -3.563132 -4.0498304 -7.0197492 -8.7897339 -8.45342 -9.1787968][-10.864315 -9.20651 -7.5696192 -6.0803533 -4.8782158 -3.9107106 -2.6664441 -2.0094678 -2.580569 -3.1813147 -3.6116488 -6.4500566 -8.338954 -8.4424505 -8.9528427][-11.946583 -8.8048315 -5.5101776 -2.5799851 -1.0510259 0.88362455 2.7354074 2.3513885 1.4161401 -0.26811886 -2.437197 -4.9084086 -6.8419242 -7.8827953 -7.8552141][-11.806729 -8.8046608 -4.0473557 0.52184248 2.5869231 4.6426215 6.9782796 7.2723708 5.7557864 2.1446195 -1.9351032 -5.0015297 -7.5239944 -8.7662764 -8.613121][-11.294522 -8.5993242 -4.2561312 0.12441921 1.811677 4.62923 7.7446837 7.3111844 6.2486434 2.9177003 -0.28554058 -5.2831044 -9.5138721 -10.417151 -10.553453][-9.0264339 -7.3661804 -4.5290685 -0.82161093 0.64890718 3.52556 5.9614244 5.8547139 5.314651 1.9701891 -1.0375085 -6.0481582 -10.422932 -11.173479 -11.523959][-8.80073 -7.130002 -4.8429685 -1.728864 0.47383928 2.7418046 4.2729859 4.1011434 3.8013139 0.91220331 -2.0149081 -7.0560408 -10.898327 -12.063705 -12.919724][-11.479152 -9.784708 -8.7453318 -6.5141683 -5.0617032 -3.3412118 -2.130676 -1.8632581 -2.471751 -4.1163836 -5.8616185 -11.175106 -13.860283 -13.753059 -12.75281][-16.021723 -14.595514 -12.251689 -11.013767 -10.932165 -9.7541294 -8.875864 -9.1941195 -9.1311312 -9.9661245 -10.951548 -13.859297 -14.983713 -15.133005 -13.96912][-17.41935 -16.329939 -15.213171 -13.29815 -12.071295 -11.409835 -10.856493 -11.207179 -11.591427 -11.660631 -11.513575 -12.672456 -13.041304 -12.288963 -10.09186][-15.711308 -14.760469 -12.964001 -12.28832 -10.746493 -9.7858944 -9.0630493 -9.0206642 -9.1383553 -9.5776939 -10.007452 -10.153744 -9.2110939 -9.5028906 -8.4004984][-12.372434 -11.794577 -10.771713 -8.5709438 -7.0175385 -6.9334373 -6.1204572 -6.0864043 -6.9058151 -7.0847988 -7.5428667 -8.7122555 -9.8507891 -8.7856865 -8.1125364]]...]
INFO - root - 2017-12-15 18:41:28.447617: step 45510, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.684 sec/batch; 54h:29m:58s remains)
INFO - root - 2017-12-15 18:41:34.955992: step 45520, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 51h:44m:12s remains)
INFO - root - 2017-12-15 18:41:41.549450: step 45530, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 53h:11m:32s remains)
INFO - root - 2017-12-15 18:41:48.123013: step 45540, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.648 sec/batch; 51h:37m:58s remains)
INFO - root - 2017-12-15 18:41:54.612941: step 45550, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 52h:32m:56s remains)
INFO - root - 2017-12-15 18:42:01.166225: step 45560, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 52h:20m:32s remains)
INFO - root - 2017-12-15 18:42:07.733338: step 45570, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.676 sec/batch; 53h:53m:24s remains)
INFO - root - 2017-12-15 18:42:14.276733: step 45580, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 51h:00m:45s remains)
INFO - root - 2017-12-15 18:42:20.883321: step 45590, loss = 0.11, batch loss = 0.06 (12.1 examples/sec; 0.660 sec/batch; 52h:33m:54s remains)
INFO - root - 2017-12-15 18:42:27.450333: step 45600, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 50h:42m:43s remains)
2017-12-15 18:42:28.039006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9735205 -2.321491 -2.3480175 -2.1123979 -2.3637679 -2.6576989 -3.0411108 -2.7102568 -3.5620892 -4.6275129 -5.0270996 -7.7420139 -8.9148445 -11.813625 -10.713915][-4.1947317 -2.3868215 -1.5128765 -1.3959951 -1.5430517 -1.8712318 -3.0690713 -3.050602 -3.6165223 -3.773057 -3.4685104 -7.13591 -8.7454014 -11.823198 -11.859711][-2.7932823 -2.3562307 -2.121593 -1.7887244 -2.1695075 -2.6242297 -2.8816135 -2.6999435 -2.6308632 -3.3551986 -4.4585524 -7.1945648 -8.1647615 -10.443544 -9.755827][-2.8938577 -2.9749215 -1.7052722 -1.2019515 -1.8214266 -2.132946 -2.5518386 -2.8315642 -3.448952 -4.1974254 -4.5728068 -7.4817772 -8.4506769 -10.626442 -9.8200264][-3.2531788 -3.4928534 -2.0742307 -0.56568336 -0.95482492 -0.24029922 0.3168478 -1.5385284 -3.2773378 -3.7269278 -4.2170196 -7.1912408 -7.6530104 -9.9009972 -10.014212][-3.9646735 -3.2686284 -1.9715395 0.028312683 0.53248453 1.6192336 2.0828457 1.3286486 -0.61375093 -2.3905387 -3.987206 -6.7997866 -7.6648827 -10.47526 -10.912926][-5.2068105 -3.7228723 -2.5153303 0.44304514 2.4597173 3.1780877 3.133678 3.735436 3.0316243 0.255383 -1.9380128 -5.2074828 -7.265595 -10.385952 -10.427626][-5.2206731 -4.5061178 -3.8234146 -1.0226316 1.9041867 4.2706656 5.2887044 4.8550467 3.8765302 2.0890274 0.86451864 -3.101238 -5.6130676 -9.176486 -10.443441][-4.4096222 -4.6234517 -3.3587327 -1.4091983 -0.59034634 1.4809346 3.4851069 3.5117354 3.0128531 2.4125943 1.4317789 -2.2986987 -4.8861666 -8.7324209 -9.4312773][-3.6003416 -4.9109106 -5.6273084 -3.4584968 -2.1630473 -1.1974635 -0.3578639 0.9279151 1.5852094 1.0649672 0.48690557 -2.8477294 -4.51837 -8.1405249 -9.3212671][-7.4218616 -7.4672956 -8.0406256 -6.482873 -5.8495941 -4.9731612 -4.8343115 -4.2995749 -2.7937944 -2.3182092 -2.4595547 -5.7604642 -7.5332384 -8.879775 -8.26359][-11.269741 -10.918706 -10.199838 -9.3065052 -9.1991129 -8.3442831 -8.0889826 -7.9647741 -7.1707664 -6.1530676 -5.3564048 -7.592557 -9.0590153 -10.034961 -8.9743652][-14.026501 -13.161367 -11.558982 -10.114223 -10.757893 -10.84992 -10.398487 -9.9701176 -9.7656326 -8.712389 -7.3473248 -8.8356628 -9.3997269 -9.852231 -8.7835][-11.172238 -10.997669 -10.374308 -10.604964 -10.285625 -9.9581509 -10.143638 -9.0155964 -7.994144 -8.246932 -8.1103172 -7.0806589 -6.8567767 -8.3246546 -7.8105068][-8.8116035 -8.1606007 -8.3537626 -8.6346989 -7.6743536 -7.7018943 -8.1234579 -7.6546173 -7.4831858 -7.5189514 -7.2633214 -8.2903252 -8.4293776 -7.8839235 -7.7476478]]...]
INFO - root - 2017-12-15 18:42:34.661484: step 45610, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 52h:28m:52s remains)
INFO - root - 2017-12-15 18:42:41.298978: step 45620, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 54h:24m:21s remains)
INFO - root - 2017-12-15 18:42:47.941924: step 45630, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 52h:54m:06s remains)
INFO - root - 2017-12-15 18:42:54.512275: step 45640, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 52h:20m:49s remains)
INFO - root - 2017-12-15 18:43:01.066867: step 45650, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 51h:53m:06s remains)
INFO - root - 2017-12-15 18:43:07.658544: step 45660, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 52h:33m:21s remains)
INFO - root - 2017-12-15 18:43:14.291702: step 45670, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 52h:44m:38s remains)
INFO - root - 2017-12-15 18:43:20.969971: step 45680, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 53h:06m:43s remains)
INFO - root - 2017-12-15 18:43:27.547072: step 45690, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 51h:33m:36s remains)
INFO - root - 2017-12-15 18:43:34.181185: step 45700, loss = 0.17, batch loss = 0.13 (11.6 examples/sec; 0.688 sec/batch; 54h:46m:52s remains)
2017-12-15 18:43:34.716645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1395845 -5.0453343 -4.7968392 -4.468339 -5.1457386 -5.8422327 -6.663331 -7.6529212 -7.6855416 -7.5461683 -7.9578562 -8.3152161 -10.345383 -8.4604254 -5.525866][-5.7144279 -4.9510779 -4.2327023 -4.4464617 -4.5291986 -4.14404 -2.7045994 -3.2067351 -4.4044342 -5.2827711 -5.789732 -6.141088 -8.2367954 -6.9799447 -5.3589373][-4.4849825 -5.1420035 -5.8193855 -3.3789532 -3.1798246 -3.99299 -3.8233638 -3.8100886 -3.0405526 -3.9863222 -4.4821754 -5.1946545 -6.7341652 -5.47473 -4.1970816][-6.43466 -6.4663377 -5.7330709 -4.7945552 -4.848125 -3.8672791 -3.4112885 -3.2013383 -3.7229538 -4.001267 -3.7337234 -4.4535389 -5.8442807 -5.0172229 -3.5180852][-6.7690978 -7.2656546 -6.8817768 -6.2853708 -5.8736286 -3.5368481 -2.2333572 -2.9069476 -3.4808724 -3.1115012 -2.6327381 -3.1812086 -6.1648498 -6.0042582 -5.0042329][-8.0644789 -6.7362709 -5.9987478 -5.1283679 -2.896775 -1.2854528 0.6352787 0.55613804 -1.504385 -2.3196938 -2.7349117 -2.5997889 -4.4105115 -5.6218529 -5.1658015][-7.3515515 -7.017169 -5.5202918 -2.448487 0.37707281 1.9702454 3.8129115 3.8558631 3.0406327 0.26145411 -2.0323668 -2.432981 -4.6207829 -5.2362022 -4.9475994][-6.0352116 -5.6771436 -4.6363149 -1.7051654 2.0953197 4.5438476 5.7724042 5.4309964 4.1804605 1.753684 -0.34526443 -1.4937463 -3.5855229 -3.9096098 -3.811825][-7.639286 -5.4127145 -2.9435613 -1.32021 0.51113796 3.3486476 5.1323256 4.6189647 2.4471393 0.60658169 -1.2126746 -2.9132323 -5.0732303 -4.6906481 -4.0524015][-8.1227226 -7.3910651 -5.5547071 -2.8015184 -1.5462298 0.51106882 3.2455363 2.526052 -0.18770695 -2.0723646 -3.289845 -4.1707196 -6.2913356 -6.199542 -5.2064786][-8.7656441 -8.2962227 -7.9384871 -6.2921362 -4.7247753 -3.4832885 -2.8847578 -2.5385389 -3.6349561 -5.2390404 -5.9440913 -6.6797838 -7.5306334 -7.6434755 -7.1107087][-11.47209 -10.052883 -8.4097567 -7.6974063 -7.2223258 -5.9884582 -5.8780289 -6.9259191 -7.4783816 -8.0615559 -8.7185946 -8.2547779 -7.3765144 -7.1835728 -6.246851][-12.44449 -11.124384 -9.032341 -7.7275486 -7.7041616 -6.8490548 -7.4679804 -7.9367614 -8.6257219 -8.9937344 -8.8476772 -7.817647 -7.7711177 -6.9726796 -5.3583617][-9.7720594 -9.0251245 -7.7322297 -6.0249224 -5.6873951 -5.8824306 -5.6117396 -5.3480916 -6.3971572 -6.63965 -6.782424 -4.9665504 -4.9686723 -5.0989704 -4.0652013][-6.1707921 -6.0284615 -5.7353282 -4.8295865 -4.2673988 -4.683351 -4.1281176 -3.2365375 -3.1538148 -3.4865627 -4.1585073 -2.7599888 -3.2540705 -3.4158714 -4.1761403]]...]
INFO - root - 2017-12-15 18:43:41.299481: step 45710, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 51h:31m:14s remains)
INFO - root - 2017-12-15 18:43:47.949528: step 45720, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 52h:43m:30s remains)
INFO - root - 2017-12-15 18:43:54.467415: step 45730, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 51h:33m:51s remains)
INFO - root - 2017-12-15 18:44:01.041887: step 45740, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 51h:39m:19s remains)
INFO - root - 2017-12-15 18:44:07.604953: step 45750, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 52h:58m:53s remains)
INFO - root - 2017-12-15 18:44:14.179826: step 45760, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 52h:24m:01s remains)
INFO - root - 2017-12-15 18:44:20.790998: step 45770, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.687 sec/batch; 54h:41m:50s remains)
INFO - root - 2017-12-15 18:44:27.418331: step 45780, loss = 0.20, batch loss = 0.16 (11.5 examples/sec; 0.693 sec/batch; 55h:10m:01s remains)
INFO - root - 2017-12-15 18:44:33.930424: step 45790, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 51h:01m:02s remains)
INFO - root - 2017-12-15 18:44:40.470379: step 45800, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 51h:16m:28s remains)
2017-12-15 18:44:40.974134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7508183 -6.6882496 -5.7373939 -4.0786681 -3.7912531 -3.4149408 -2.549108 -1.2961087 -0.96290779 -0.043745995 -0.59559822 -4.6117578 -8.7841339 -10.307049 -8.6674767][-6.660624 -8.0019054 -8.6703176 -7.6115322 -5.6449561 -3.9469883 -2.4231343 -0.85052538 -0.79980421 -1.160697 -1.6308413 -4.767333 -9.2866058 -9.9516048 -9.5247822][-5.1521111 -6.3972816 -8.32772 -7.792923 -7.5232267 -5.5568933 -2.5548308 -1.3298779 -1.2533684 -2.233459 -3.6652489 -7.2446346 -10.498345 -11.441851 -9.8751183][-4.0628004 -5.5561337 -6.5643988 -6.1566143 -6.6437368 -5.6414356 -4.4195142 -2.7609251 -1.8014352 -2.6555207 -4.3418794 -9.1066523 -13.085489 -12.093374 -9.7094116][-5.3584571 -5.29799 -5.453445 -4.6131845 -4.687274 -2.9857137 -1.3836799 -2.2279036 -2.7719753 -3.2789466 -4.3869047 -8.332283 -12.508756 -12.581146 -10.257627][-8.2923346 -6.4515576 -6.1132464 -4.2975979 -3.7168427 -0.94017982 0.98245573 0.52759409 -0.61613894 -3.0480745 -5.0547795 -8.3700161 -11.748487 -12.792413 -10.704124][-8.7355852 -7.6627893 -7.3454409 -4.9411144 -3.7217431 -0.2227149 2.8652196 3.7969956 2.9950252 -0.762825 -4.794116 -7.6574087 -10.027316 -10.605634 -10.200037][-7.2847695 -6.7160411 -6.7105532 -3.7928627 -1.7605765 1.3546057 4.2378097 5.1401591 4.86002 2.001884 -1.4834218 -6.9564738 -11.171211 -10.98016 -9.7182932][-6.4992943 -5.4065995 -4.8266768 -2.4930618 -1.6742015 1.665669 3.9871116 5.3044543 5.1534514 1.6433339 -1.5462027 -6.6943536 -11.394311 -12.675085 -11.220674][-5.7310376 -5.1619444 -4.5293193 -2.5348146 -1.5794597 0.018235683 0.92358828 2.7818236 3.1920676 1.617506 -0.90970755 -6.7528582 -11.454952 -12.886023 -11.558287][-6.5019975 -7.385118 -7.6163459 -6.4963369 -4.6855035 -3.3707128 -2.0388162 -1.0116901 -1.0155535 -1.33145 -2.3385599 -7.7354283 -11.749676 -13.822939 -12.818077][-10.430458 -8.73555 -7.9081068 -7.17961 -7.1901088 -6.5305424 -6.5630612 -6.9987383 -7.0229282 -6.3196774 -6.743216 -8.9244652 -10.73705 -11.389672 -10.264259][-13.480623 -11.570433 -8.5368919 -8.0930672 -8.3502293 -8.656146 -9.198801 -8.63582 -8.2223034 -7.7495718 -7.625874 -9.1736851 -11.571044 -10.985012 -8.8627291][-12.636268 -10.79153 -8.24967 -7.848423 -7.6582546 -6.7459631 -6.9565549 -7.6105251 -7.278532 -8.0539942 -7.9733019 -6.59349 -6.8642392 -8.7793665 -9.1288128][-8.8202543 -7.0868015 -6.2240653 -5.4044037 -5.5088587 -6.24351 -6.1981726 -5.2461224 -5.0620456 -5.79649 -6.5681047 -8.837451 -9.4483023 -8.4930277 -9.1159344]]...]
INFO - root - 2017-12-15 18:44:47.509509: step 45810, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 52h:26m:28s remains)
INFO - root - 2017-12-15 18:44:54.043893: step 45820, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 53h:58m:09s remains)
INFO - root - 2017-12-15 18:45:00.602685: step 45830, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 52h:55m:12s remains)
INFO - root - 2017-12-15 18:45:07.169952: step 45840, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 51h:19m:28s remains)
INFO - root - 2017-12-15 18:45:13.773407: step 45850, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 52h:08m:58s remains)
INFO - root - 2017-12-15 18:45:20.318847: step 45860, loss = 0.22, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 51h:35m:24s remains)
INFO - root - 2017-12-15 18:45:26.929540: step 45870, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 53h:10m:12s remains)
INFO - root - 2017-12-15 18:45:33.522216: step 45880, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 52h:02m:31s remains)
INFO - root - 2017-12-15 18:45:40.118143: step 45890, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 53h:10m:33s remains)
INFO - root - 2017-12-15 18:45:46.715510: step 45900, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 50h:58m:17s remains)
2017-12-15 18:45:47.257391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9034529 -4.2564764 -3.2527814 -1.8033135 -1.2186823 -1.955065 -2.6820793 -2.9691 -3.4838886 -3.599124 -3.8924823 -5.8270249 -7.6058311 -8.3199091 -8.37527][-2.935986 -2.6560385 -1.983763 -0.9749012 -0.65126038 -1.3555155 -1.8619189 -2.4294572 -2.6693509 -3.4608412 -4.3597722 -5.8601861 -7.5702381 -8.596612 -8.9611588][-1.3748093 -0.98066473 -0.59346676 0.10213327 -0.95059872 -2.0745759 -2.57472 -3.5563793 -3.9045978 -4.0438437 -4.2997279 -5.9255061 -7.9144344 -8.8735065 -9.2235842][-2.5394309 -2.4631269 -1.2909398 -1.2835956 -2.5983753 -3.337899 -3.8970914 -3.8406949 -4.1537962 -5.0548663 -5.675281 -6.838089 -8.5196228 -9.5706816 -9.9799633][-2.5009232 -2.3661427 -1.808296 -1.9436302 -2.616766 -2.3933372 -2.3250785 -2.5202279 -3.2027314 -3.6509166 -4.3837094 -6.4179897 -8.9449234 -10.600594 -11.273071][-3.7413557 -3.1347361 -2.3514433 -1.4199471 -0.87418938 -0.40317631 0.20750904 0.32269335 -0.10079956 -1.6233339 -3.847662 -5.665144 -7.7427168 -9.3164825 -10.113975][-4.3546281 -4.0061827 -2.7895794 -1.0957532 -0.13371181 0.92109108 2.136344 2.5603185 2.3275275 0.20660114 -2.1435623 -3.9308577 -5.5226188 -6.5926533 -7.7204781][-4.2478113 -4.1458063 -3.6280382 -2.080879 -0.88833475 0.78308439 2.1132898 2.9882722 3.3548102 2.177175 0.3976016 -1.7982254 -4.605299 -5.4227376 -5.9326162][-5.0430994 -4.1232138 -2.504612 -1.8888266 -1.1957669 0.662189 2.3441453 2.8394456 3.1640735 1.9999075 0.71912718 -1.0951476 -3.2311926 -4.7460537 -5.3023119][-5.8438129 -5.2874422 -4.0846157 -1.7009563 -0.089211464 0.058882236 0.536952 2.1007428 2.7178741 2.6329312 1.9682212 -0.78010035 -4.1151953 -4.4999886 -4.2288227][-10.532204 -8.7972126 -5.4282551 -3.0402255 -1.4358716 -0.59905148 -0.21899796 0.0032582283 -0.15708256 0.40098381 -0.077807426 -2.362041 -3.8837743 -4.2344 -3.8818824][-13.596231 -12.665642 -9.69256 -6.3416886 -3.3928702 -1.6564579 -1.6207576 -2.041152 -1.9825666 -2.0968215 -2.546118 -3.7736883 -4.60048 -5.1513371 -4.8126254][-12.286342 -11.223631 -8.983283 -6.4592867 -4.8852735 -3.4506989 -3.2506785 -3.7693634 -4.2751064 -4.6068707 -4.1463566 -3.8292022 -4.8210464 -4.4879384 -2.6781023][-9.2567978 -8.4921236 -6.9987564 -5.1791506 -3.6857295 -2.8972437 -3.6720862 -3.9492207 -5.0755062 -6.5862885 -7.1510811 -6.4932375 -5.7210107 -4.838026 -4.77388][-5.8396192 -5.6937447 -4.5140066 -4.1195135 -3.471535 -2.642992 -2.826957 -3.4289119 -4.7967191 -5.9162807 -6.8469596 -7.3558674 -8.2242413 -7.8013849 -7.0150671]]...]
INFO - root - 2017-12-15 18:45:53.922055: step 45910, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 53h:35m:53s remains)
INFO - root - 2017-12-15 18:46:00.530130: step 45920, loss = 0.19, batch loss = 0.15 (11.7 examples/sec; 0.685 sec/batch; 54h:32m:41s remains)
INFO - root - 2017-12-15 18:46:07.119611: step 45930, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 52h:12m:01s remains)
INFO - root - 2017-12-15 18:46:13.710728: step 45940, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 54h:23m:18s remains)
INFO - root - 2017-12-15 18:46:20.260575: step 45950, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 53h:01m:49s remains)
INFO - root - 2017-12-15 18:46:26.836615: step 45960, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 52h:03m:16s remains)
INFO - root - 2017-12-15 18:46:33.435397: step 45970, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 53h:38m:07s remains)
INFO - root - 2017-12-15 18:46:40.073865: step 45980, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.677 sec/batch; 53h:51m:22s remains)
INFO - root - 2017-12-15 18:46:46.668784: step 45990, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 53h:52m:39s remains)
INFO - root - 2017-12-15 18:46:53.223762: step 46000, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 53h:24m:12s remains)
2017-12-15 18:46:53.776644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4621081 -7.4911613 -7.6586738 -6.9423685 -6.9200659 -6.2276149 -5.3948979 -4.4134936 -3.3469441 -3.08609 -3.748214 -7.1294613 -8.8941288 -9.9942446 -7.5983763][-5.8917217 -5.733366 -4.9391651 -4.6625676 -5.8297071 -6.0871525 -5.7188563 -4.7196007 -3.9622049 -3.4706953 -3.6920362 -6.993885 -9.0256634 -10.909521 -8.3647041][-3.8804293 -4.5627942 -4.6598253 -4.432385 -4.6822033 -4.8202124 -4.7748332 -4.0600176 -3.33565 -3.0092366 -3.9701867 -7.6724663 -9.1368818 -10.475691 -8.6399584][-5.1801977 -4.7549534 -4.5837011 -4.417069 -5.1172228 -4.8550997 -5.0265503 -4.1341267 -3.8710523 -3.4548516 -3.9364777 -7.9040146 -10.540298 -12.066954 -8.8348436][-6.7112436 -7.5483866 -7.0298681 -5.1752024 -4.4343877 -3.3135307 -2.3653982 -2.6887844 -3.4532778 -2.6079736 -3.0071137 -6.6924586 -8.9182873 -11.663021 -9.6003742][-8.6302156 -8.0221682 -6.2658052 -3.0889075 -1.8479662 -0.037369728 1.2177858 1.5206685 0.97467661 -1.0955153 -3.6674829 -6.5682259 -7.6948137 -9.888504 -8.4266729][-10.085541 -8.4196148 -6.2131085 -2.7531312 -0.23891973 2.9007506 5.250339 4.6698413 3.4463038 0.52535582 -3.5077589 -6.958642 -8.5927258 -10.383867 -8.00057][-10.351023 -9.0123949 -6.5523357 -2.483012 0.12374973 3.2753749 5.9926581 5.9311795 5.4297624 2.1131887 -2.0998435 -7.4450464 -10.369515 -10.618207 -8.323144][-9.4438763 -7.0085444 -5.0424514 -2.5190701 -1.1833072 1.7068558 3.789578 4.1136422 3.9550939 0.34536171 -2.9444153 -7.3819847 -10.070906 -10.878662 -9.1518936][-9.0415421 -7.9565144 -6.5324163 -3.37965 -1.3251171 -0.07652235 1.3862834 1.9056039 0.81645679 -2.0956995 -5.1170707 -9.7039223 -12.044499 -13.251318 -10.916018][-10.877775 -8.6161242 -7.60563 -5.4571624 -4.2018929 -3.1740472 -1.8814623 -1.2980208 -2.2241061 -4.7085581 -7.2323341 -11.579166 -13.750448 -13.293931 -11.414986][-14.246534 -13.439829 -11.166996 -9.38821 -8.3748083 -7.8280907 -6.873107 -7.0308032 -7.1944733 -7.6220417 -9.0750446 -11.441772 -12.4728 -13.209757 -11.63344][-15.201262 -12.582209 -10.320884 -8.5567207 -8.27979 -7.9925938 -8.3135586 -8.1234207 -8.4459095 -9.0693312 -9.1173162 -10.033373 -10.197634 -8.9222393 -6.9185352][-12.101313 -10.67395 -8.9514017 -7.212532 -6.0320163 -6.0925074 -6.9876041 -6.798584 -6.8929024 -7.4390211 -7.6615105 -6.946732 -6.2435145 -5.7649708 -4.2200613][-8.0120907 -6.1380916 -3.8234062 -2.3580923 -2.297718 -2.5994847 -2.8954601 -4.0999613 -5.0831079 -4.7693152 -4.59074 -6.0580931 -7.0969448 -6.0127358 -4.6987081]]...]
INFO - root - 2017-12-15 18:47:00.388872: step 46010, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 52h:16m:00s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 18:47:06.955493: step 46020, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 53h:35m:31s remains)
INFO - root - 2017-12-15 18:47:13.545422: step 46030, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 53h:35m:37s remains)
INFO - root - 2017-12-15 18:47:20.137848: step 46040, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 53h:14m:07s remains)
INFO - root - 2017-12-15 18:47:26.717995: step 46050, loss = 0.11, batch loss = 0.07 (12.0 examples/sec; 0.665 sec/batch; 52h:54m:38s remains)
INFO - root - 2017-12-15 18:47:33.219130: step 46060, loss = 0.22, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 51h:34m:20s remains)
INFO - root - 2017-12-15 18:47:39.775229: step 46070, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 51h:10m:10s remains)
INFO - root - 2017-12-15 18:47:46.392648: step 46080, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 51h:28m:28s remains)
INFO - root - 2017-12-15 18:47:52.987130: step 46090, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 51h:50m:30s remains)
INFO - root - 2017-12-15 18:47:59.556967: step 46100, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.633 sec/batch; 50h:23m:44s remains)
2017-12-15 18:48:00.067485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8084507 -7.7231007 -6.7258096 -5.8815703 -6.2927704 -6.2840796 -6.9724388 -6.7007179 -5.7180634 -4.9206433 -3.6035326 -3.3340013 -4.3638921 -4.6812057 -4.6860056][-6.2931528 -6.2462983 -5.9907746 -5.028914 -4.9556246 -4.7370992 -4.9789867 -5.0228138 -4.8548789 -3.8439429 -2.7155042 -3.1959817 -4.2172065 -4.2220984 -4.0342293][-5.5316253 -5.2382021 -5.5048518 -5.1034389 -4.2817802 -4.4021263 -4.3688297 -3.9609716 -3.5582776 -2.5001912 -1.6433129 -2.5881903 -4.374939 -4.056427 -4.1263094][-4.3692746 -5.0594392 -5.8613563 -6.0809474 -5.9095964 -4.3225522 -2.9114833 -2.0091825 -1.8926282 -1.7542326 -1.5560279 -2.8189216 -4.5129118 -4.6983232 -5.6053162][-5.2546687 -6.3840771 -8.03331 -7.4261618 -6.0357113 -3.4462235 -0.61014223 0.57793188 0.0014181137 -1.14116 -1.4127321 -2.5600181 -3.5325935 -3.1547356 -4.4448614][-7.4566641 -7.5129948 -7.8529568 -5.0471563 -2.6662052 0.11942291 3.0227094 3.7352042 2.2853208 0.789711 -0.7376852 -2.2984073 -2.4571168 -2.3961854 -3.5174065][-6.7266722 -6.9230289 -5.855875 -3.1605232 -0.53639793 2.6720929 5.9664226 6.5190291 5.0134339 2.1462846 -1.0305252 -3.3579779 -4.6598492 -4.46659 -4.3216352][-6.5397773 -5.6296034 -3.9411731 -0.33984947 1.4968996 4.8382182 7.0852876 6.8353496 5.8780684 2.7578568 -1.0071588 -4.4754457 -6.252274 -5.3838153 -5.1993794][-6.1091022 -5.3843884 -4.0613384 -1.712513 0.7726841 2.8007665 3.6752772 4.3491416 3.3686576 1.1850109 -1.5579967 -4.8636441 -7.50889 -7.2277374 -6.5762968][-4.7163897 -4.6297774 -4.3859386 -1.9209363 -0.066758156 0.28029442 0.82260466 -0.0037093163 -1.1574335 -2.3111589 -3.3260276 -6.0149755 -8.5569553 -8.3159657 -8.2753849][-7.5913086 -7.3482237 -6.7570848 -3.999949 -3.7555671 -3.4143953 -2.8927727 -3.9320474 -5.6226511 -6.2554288 -7.0097523 -8.7160492 -9.0437346 -8.6069489 -8.0226545][-10.757704 -9.7081156 -7.9178104 -5.3502603 -4.4803467 -4.3610311 -5.1516428 -6.9141345 -7.8784947 -8.5735893 -9.3502636 -9.4150724 -8.7664881 -7.6645007 -6.847693][-12.082981 -10.563454 -7.7948289 -6.13845 -6.0195293 -6.0139012 -6.952877 -8.0931711 -9.0123205 -9.1249771 -8.8549919 -8.10027 -7.4098406 -6.5612617 -5.5756712][-10.958054 -10.041452 -7.1651745 -5.0111017 -4.5098085 -4.8128676 -5.9424543 -7.4134407 -8.1929474 -7.8229904 -7.8094616 -6.8375063 -6.0988092 -5.378933 -4.8759642][-7.62605 -7.5935588 -6.7437224 -4.5866289 -3.8252015 -4.1516414 -4.2908077 -4.8722744 -5.0781612 -5.2277274 -5.3038259 -5.0249143 -5.8241572 -5.9440866 -5.7465615]]...]
INFO - root - 2017-12-15 18:48:06.674287: step 46110, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 51h:57m:13s remains)
INFO - root - 2017-12-15 18:48:13.195717: step 46120, loss = 0.23, batch loss = 0.19 (12.5 examples/sec; 0.641 sec/batch; 50h:59m:21s remains)
INFO - root - 2017-12-15 18:48:19.801466: step 46130, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 53h:27m:16s remains)
INFO - root - 2017-12-15 18:48:26.434436: step 46140, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 52h:32m:09s remains)
INFO - root - 2017-12-15 18:48:32.994189: step 46150, loss = 0.31, batch loss = 0.26 (12.1 examples/sec; 0.661 sec/batch; 52h:35m:53s remains)
INFO - root - 2017-12-15 18:48:39.684028: step 46160, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.683 sec/batch; 54h:17m:21s remains)
INFO - root - 2017-12-15 18:48:46.301272: step 46170, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 51h:29m:58s remains)
INFO - root - 2017-12-15 18:48:52.880668: step 46180, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.658 sec/batch; 52h:22m:21s remains)
INFO - root - 2017-12-15 18:48:59.485767: step 46190, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 52h:54m:41s remains)
INFO - root - 2017-12-15 18:49:06.004184: step 46200, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 51h:47m:42s remains)
2017-12-15 18:49:06.553315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1697845 -5.3717933 -6.0817404 -6.1332574 -7.2912455 -7.4894748 -8.0542679 -7.7058544 -7.3665967 -7.6712584 -6.8622732 -5.7460127 -5.9202023 -7.6300135 -8.1012154][-4.5185776 -4.487031 -5.6961288 -7.7890806 -8.9703226 -10.278891 -10.400875 -9.77663 -9.6691017 -9.1017284 -8.3383017 -7.0562353 -7.7665782 -9.406106 -9.4689388][-3.9673946 -4.9225955 -7.1729674 -8.0736265 -9.6182833 -10.767633 -11.730037 -11.086334 -10.255278 -9.8404236 -9.2987394 -8.12034 -8.2864962 -9.77912 -9.7346191][-3.7375121 -4.3472409 -6.2936907 -7.7447672 -9.9524918 -10.640724 -9.8492451 -9.3017845 -8.62945 -8.417675 -8.9395237 -7.9651594 -9.0433121 -11.393885 -10.996069][-4.1517081 -5.3027363 -7.4470425 -8.0268011 -8.8446026 -6.8237786 -5.1612554 -5.3158922 -5.7416234 -5.719317 -6.2859721 -7.0404234 -9.0920525 -11.619223 -12.137239][-5.545073 -6.52856 -7.463119 -7.4184885 -7.5538268 -3.8301721 0.96393204 1.936892 1.2043262 -1.9426146 -4.7267447 -4.8464093 -7.0602074 -10.446371 -11.767994][-6.8569393 -6.7794819 -6.8038321 -7.008872 -6.2934079 -0.8185153 4.7067962 7.5759435 8.0658932 2.4452157 -3.1645896 -4.5347114 -6.0192766 -8.6551037 -9.4090462][-7.1027617 -6.4033175 -6.3877263 -6.2983279 -4.6232109 0.57479143 5.9033551 9.16317 10.88245 6.2214675 0.0797348 -2.8627682 -6.3151593 -8.2726622 -8.3726492][-5.4792957 -5.1905708 -4.974546 -5.6384716 -4.8960624 -1.3642344 3.2360034 7.4327197 8.97558 5.6379027 1.4970922 -2.094383 -6.2789807 -8.7837143 -9.3618307][-5.4887924 -4.8566256 -4.6051598 -4.4937892 -4.4888248 -3.1782129 -0.61672831 2.7444625 4.5214686 2.4105067 -1.2024894 -3.5238245 -7.3373227 -10.781998 -12.493097][-8.1161346 -7.7533855 -7.4292192 -6.3947086 -6.5256782 -6.8783417 -6.595479 -4.7805023 -3.0905168 -2.7840331 -4.3456683 -6.9816771 -9.8516121 -12.077174 -12.386736][-12.641584 -11.472935 -10.075323 -9.6127205 -9.2305489 -9.6155977 -10.576797 -9.94129 -9.44847 -9.02103 -9.7517805 -8.8765516 -9.48059 -11.107758 -10.992798][-13.814484 -11.759352 -9.6622162 -8.6782484 -9.0896444 -9.4764395 -9.9633541 -9.7341785 -9.6060648 -9.2705069 -9.4188538 -8.9799166 -10.537951 -10.424705 -9.120821][-10.041515 -9.77177 -9.2996235 -7.8118553 -7.1910253 -7.6935544 -8.11561 -8.05514 -7.6802497 -7.5484452 -8.0730038 -7.3598185 -6.911437 -6.7410536 -6.9955211][-5.8085685 -5.7861056 -5.2856255 -4.590724 -4.7472258 -5.089344 -5.1384435 -5.1363969 -4.8430581 -4.7147522 -4.756237 -5.2885818 -6.6230512 -6.8249884 -6.8695955]]...]
INFO - root - 2017-12-15 18:49:13.125860: step 46210, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 51h:27m:47s remains)
INFO - root - 2017-12-15 18:49:19.756260: step 46220, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 51h:29m:12s remains)
INFO - root - 2017-12-15 18:49:26.405922: step 46230, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 52h:11m:06s remains)
INFO - root - 2017-12-15 18:49:32.980087: step 46240, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 53h:14m:15s remains)
INFO - root - 2017-12-15 18:49:39.617981: step 46250, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 53h:00m:16s remains)
INFO - root - 2017-12-15 18:49:46.181145: step 46260, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 53h:17m:46s remains)
INFO - root - 2017-12-15 18:49:52.804889: step 46270, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 52h:13m:30s remains)
INFO - root - 2017-12-15 18:49:59.466313: step 46280, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 52h:30m:53s remains)
INFO - root - 2017-12-15 18:50:05.997978: step 46290, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 52h:29m:17s remains)
INFO - root - 2017-12-15 18:50:12.644683: step 46300, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 50h:58m:55s remains)
2017-12-15 18:50:13.161856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.31487 -8.57671 -7.3151979 -6.0920358 -5.5901971 -5.7938728 -6.3826776 -6.5251613 -6.7295389 -6.027904 -3.8149328 -3.6018281 -5.5492859 -6.8186097 -7.7358408][-7.3676682 -5.7055244 -3.1668055 -2.1960697 -2.9617021 -4.7609992 -6.7333307 -7.8683081 -7.4931173 -6.1731148 -4.3205209 -3.5261805 -4.8111749 -6.0365562 -6.4856529][-3.3135769 -2.829423 -1.8395212 -0.33063412 -0.84354639 -2.7757449 -4.5266056 -5.6728635 -5.42939 -4.3053055 -2.5025847 -2.279727 -3.878109 -5.373353 -5.9450121][-3.3787088 -3.4609172 -2.7931776 -1.4414468 -1.7990315 -2.8903642 -4.5787439 -5.5587597 -5.2310596 -3.7996387 -2.7035263 -3.5448601 -6.438117 -7.5755839 -7.2780647][-4.8992996 -6.0077095 -5.0699706 -2.7875309 -1.8607142 -1.12959 -0.95072412 -2.6430862 -3.4004593 -2.8159602 -2.8593602 -4.1933179 -6.7352633 -8.7944136 -9.3788719][-9.1281872 -7.9267416 -5.6799016 -3.1123865 -0.61200857 1.972611 3.5722346 2.0335064 0.85833645 -0.49237967 -2.7667103 -4.0554514 -6.6409307 -8.9156246 -9.5313835][-12.477736 -10.539784 -7.5459576 -3.1252918 -0.066152096 3.2984643 6.7153192 6.5498538 5.6381726 2.1969476 -1.6909614 -3.8852186 -7.0855665 -9.2363567 -9.7641411][-12.423157 -10.555775 -8.00185 -3.2758963 0.26021051 3.8914876 6.8496079 6.9023833 6.6245685 3.4880929 -0.14214802 -3.6562347 -8.7216854 -11.208418 -11.405608][-10.196346 -9.6837921 -7.959096 -3.4380803 -0.74796343 2.9766889 5.6407418 4.3915753 3.1046023 1.3699036 -1.3016405 -5.5643182 -10.526194 -12.716991 -12.674667][-10.227633 -9.7242718 -8.4651051 -5.6208439 -3.4345059 -0.68329191 1.5062704 1.4247317 0.52159691 -1.9033225 -4.5767784 -7.2409058 -11.131218 -13.567305 -14.181515][-13.225021 -12.734817 -11.656065 -9.6727066 -8.5384407 -6.7758512 -4.9254761 -4.7106242 -4.9226255 -6.0078959 -8.0419712 -10.872074 -13.202747 -13.844505 -13.210687][-17.422699 -17.207256 -16.532028 -14.298607 -12.950081 -12.437588 -12.13802 -11.075232 -10.665435 -11.606401 -12.335846 -12.968511 -14.452829 -14.211454 -13.51771][-15.463675 -15.336643 -14.863991 -14.779852 -14.346041 -12.759499 -12.081751 -12.162534 -13.151913 -12.753026 -12.40312 -13.649566 -14.436213 -13.913656 -12.648735][-12.65889 -13.320306 -14.003101 -13.122393 -12.041965 -11.201575 -10.775807 -9.3413124 -9.6955051 -10.425647 -10.865421 -11.407812 -12.41429 -11.798005 -11.721548][-8.3866014 -9.2716837 -10.010896 -9.2306089 -8.63464 -8.0986776 -7.1962996 -7.3469181 -6.828156 -6.4451685 -6.8545985 -8.6104364 -10.512428 -11.644913 -12.233427]]...]
INFO - root - 2017-12-15 18:50:19.696619: step 46310, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 51h:38m:42s remains)
INFO - root - 2017-12-15 18:50:26.302014: step 46320, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 52h:42m:35s remains)
INFO - root - 2017-12-15 18:50:32.911329: step 46330, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 51h:46m:35s remains)
INFO - root - 2017-12-15 18:50:39.539533: step 46340, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 50h:49m:35s remains)
INFO - root - 2017-12-15 18:50:46.121331: step 46350, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 53h:01m:43s remains)
INFO - root - 2017-12-15 18:50:52.680969: step 46360, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 52h:03m:08s remains)
INFO - root - 2017-12-15 18:50:59.282033: step 46370, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.635 sec/batch; 50h:29m:21s remains)
INFO - root - 2017-12-15 18:51:05.907590: step 46380, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 53h:04m:28s remains)
INFO - root - 2017-12-15 18:51:12.484329: step 46390, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 52h:20m:42s remains)
INFO - root - 2017-12-15 18:51:19.135870: step 46400, loss = 0.12, batch loss = 0.07 (11.6 examples/sec; 0.688 sec/batch; 54h:40m:55s remains)
2017-12-15 18:51:19.640621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.99253 -8.7444611 -8.9631634 -9.089653 -8.8865976 -9.0084209 -9.0142469 -9.0189486 -8.1919575 -6.2129154 -4.8829756 -5.3572445 -7.0773821 -6.83199 -7.06941][-5.2331028 -6.7599425 -7.1334915 -6.9652386 -7.0799069 -7.349803 -8.2809753 -8.80991 -8.657094 -7.1407728 -5.3082643 -4.6225481 -7.3228149 -8.7700539 -8.7875166][-2.6646547 -3.6628332 -4.2497315 -5.1135879 -5.373878 -5.6516461 -5.6784148 -6.3624721 -6.9309134 -5.9347496 -4.6018395 -5.3132129 -7.28823 -8.3627157 -9.1615028][-3.3371875 -3.9560513 -3.9526114 -4.1150026 -3.4547203 -3.2464807 -3.5625796 -3.5882676 -3.6415691 -4.4611683 -4.722 -4.903419 -7.5398393 -8.22806 -8.2905426][-3.9960496 -5.1877151 -4.9015379 -3.4237478 -2.6086903 -1.2240181 -0.1954813 -1.3933096 -2.894722 -2.6927922 -2.51435 -3.614027 -5.7939086 -8.1437464 -8.538002][-4.2826262 -4.7361469 -4.4102736 -3.0260789 -0.54353476 1.6366611 2.4051967 1.6559777 0.54825306 -0.43160009 -1.229291 -1.9438152 -4.066411 -5.2759438 -6.0662208][-4.5153666 -3.9462628 -2.5177047 -0.55230618 1.1697865 3.0681148 4.8234782 4.1531587 3.1172194 0.99270725 -1.3031316 -2.3132885 -4.3634391 -5.7463627 -6.0370331][-4.7207165 -3.4620297 -1.4124298 0.7162571 2.1883111 3.4960818 4.3158 4.4209075 3.8973222 2.7634959 0.67287159 -2.4475408 -6.9901714 -8.0777645 -7.0356531][-5.293221 -3.8011804 -1.5546131 0.57214642 1.6437769 2.649488 3.2771068 3.1723771 3.9539504 3.4614787 1.9806519 -1.1038742 -6.977766 -10.12797 -11.602963][-6.1353626 -5.1318712 -3.0639157 -0.36867666 0.77233076 0.75596476 1.0520735 1.4417148 1.279357 2.21484 2.7090507 -0.44216442 -6.2111864 -10.492932 -13.26386][-8.8585014 -7.9767222 -6.6696639 -4.4707322 -3.4690435 -3.3744462 -3.5273206 -2.5668054 -2.1473019 -1.2884312 -1.7083979 -3.6049569 -8.1362028 -11.207811 -12.729549][-10.979622 -11.190765 -10.890783 -9.4479332 -8.9162292 -8.2874651 -8.094429 -7.6877546 -7.2057395 -6.3339314 -5.4128141 -7.1466942 -10.192394 -11.598176 -11.659642][-12.260228 -11.650263 -11.143691 -11.005656 -11.648879 -11.619656 -11.026958 -8.9177008 -7.4972305 -6.7702961 -6.807158 -8.8824339 -11.256577 -12.787901 -11.031767][-13.28188 -12.783026 -11.549109 -10.499844 -11.359711 -11.904052 -11.270037 -10.168419 -9.455142 -8.199007 -7.9863377 -9.0637712 -9.4691124 -10.991917 -10.727676][-9.7281837 -10.837748 -10.359905 -8.4039211 -7.3649478 -7.6839089 -7.7345276 -7.7019491 -8.0823421 -8.89369 -9.858511 -10.045366 -9.3732662 -9.8144913 -10.18405]]...]
INFO - root - 2017-12-15 18:51:26.110778: step 46410, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 51h:59m:09s remains)
INFO - root - 2017-12-15 18:51:32.682762: step 46420, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 52h:26m:27s remains)
INFO - root - 2017-12-15 18:51:39.335253: step 46430, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 51h:48m:51s remains)
INFO - root - 2017-12-15 18:51:45.938499: step 46440, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.640 sec/batch; 50h:53m:32s remains)
INFO - root - 2017-12-15 18:51:52.511158: step 46450, loss = 0.29, batch loss = 0.25 (12.5 examples/sec; 0.642 sec/batch; 50h:58m:26s remains)
INFO - root - 2017-12-15 18:51:59.006647: step 46460, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 50h:41m:38s remains)
INFO - root - 2017-12-15 18:52:05.549068: step 46470, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 53h:24m:12s remains)
INFO - root - 2017-12-15 18:52:12.091767: step 46480, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 52h:21m:04s remains)
INFO - root - 2017-12-15 18:52:18.704771: step 46490, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 51h:00m:57s remains)
INFO - root - 2017-12-15 18:52:25.345343: step 46500, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 52h:34m:50s remains)
2017-12-15 18:52:25.866482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.292645 -9.9916544 -8.72635 -6.927279 -6.1989145 -6.5003071 -7.6112618 -7.4579978 -7.3059111 -6.5166354 -4.8938694 -2.6471467 -3.9452994 -5.6005654 -4.7073379][-10.902919 -11.700972 -10.837605 -8.3209553 -6.5824881 -6.174561 -6.21231 -6.6782293 -6.6841726 -5.8247414 -4.0873966 -2.1907616 -3.3425052 -5.0011711 -4.2047806][-8.22702 -9.7889681 -10.720974 -11.23811 -9.982645 -8.6577454 -6.722908 -6.7157531 -6.9757724 -6.5457845 -4.8555274 -2.6926639 -3.0306315 -4.8429718 -5.0812745][-9.1973286 -9.7962914 -10.836803 -12.460237 -12.050394 -9.4212751 -7.3386254 -7.6078677 -7.4833283 -7.2651606 -6.2073064 -4.3357754 -4.7229314 -5.9855227 -6.1715989][-10.352346 -10.844263 -12.225225 -12.972152 -10.25576 -6.1407962 -3.6226609 -5.6179523 -7.7377768 -6.9516144 -6.141089 -5.6597137 -6.563942 -8.2289562 -8.6179132][-10.067009 -9.8204155 -10.030479 -10.394827 -7.6729879 -1.5287056 3.4584241 -0.22223091 -5.5004029 -6.9077191 -5.80328 -4.5594735 -6.9285 -9.439 -9.7907944][-9.5760088 -9.7401142 -7.9982696 -6.2937703 -2.9632943 1.9216719 8.0933285 6.7504945 0.81596518 -4.3626556 -6.644968 -4.7429171 -6.1723933 -9.0038567 -9.8070545][-9.9444962 -9.0636024 -8.8129864 -6.0010676 -1.132854 4.0338607 8.4651451 7.7919936 5.6166129 -0.65366077 -6.2676835 -5.71536 -7.1384158 -8.8029261 -8.7906][-8.7574558 -7.21203 -6.1787925 -6.8861866 -5.4698148 -0.014635086 5.7611146 4.4967065 2.5184236 -2.1442533 -5.158524 -6.7038746 -9.95097 -10.542046 -9.4755754][-8.7807884 -7.5150957 -6.4415612 -7.396811 -7.941381 -4.3668327 0.641304 1.8027377 -0.11750174 -5.2909079 -8.5735931 -8.58526 -11.101521 -12.917818 -10.864081][-12.134382 -10.861708 -10.885286 -10.269156 -11.050095 -10.540041 -8.8890848 -5.4833589 -5.2061291 -9.0077915 -11.604452 -11.205976 -13.144935 -14.194244 -12.335927][-13.366055 -12.655703 -12.331731 -11.690447 -12.303841 -11.777117 -11.137945 -8.9879551 -8.8404894 -10.691593 -12.565763 -12.287298 -11.927534 -12.932972 -10.204435][-12.8508 -11.150452 -10.294235 -10.646281 -11.164403 -10.980441 -10.12399 -8.7852087 -8.4443016 -8.3875923 -9.3454685 -9.0828295 -10.072418 -10.965839 -7.549119][-11.421597 -9.3595161 -8.5965157 -8.7656755 -9.5917206 -9.400466 -9.2647085 -7.4793797 -6.4013429 -6.6367917 -7.1444631 -6.8361731 -6.940659 -7.5674982 -6.1492491][-8.98224 -8.2485647 -6.8906355 -5.973021 -6.6090603 -7.0635839 -6.4181528 -5.9838858 -5.73779 -5.4426336 -5.8742914 -6.1953073 -7.2415967 -7.4917631 -5.9299974]]...]
INFO - root - 2017-12-15 18:52:32.464849: step 46510, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 51h:08m:50s remains)
INFO - root - 2017-12-15 18:52:39.061291: step 46520, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 51h:20m:31s remains)
INFO - root - 2017-12-15 18:52:45.643885: step 46530, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 53h:09m:06s remains)
INFO - root - 2017-12-15 18:52:52.342325: step 46540, loss = 0.20, batch loss = 0.15 (11.8 examples/sec; 0.680 sec/batch; 54h:01m:03s remains)
INFO - root - 2017-12-15 18:52:58.886812: step 46550, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.686 sec/batch; 54h:29m:17s remains)
INFO - root - 2017-12-15 18:53:05.578259: step 46560, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 52h:05m:48s remains)
INFO - root - 2017-12-15 18:53:12.243731: step 46570, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 51h:28m:56s remains)
INFO - root - 2017-12-15 18:53:18.795512: step 46580, loss = 0.22, batch loss = 0.17 (11.6 examples/sec; 0.689 sec/batch; 54h:43m:57s remains)
INFO - root - 2017-12-15 18:53:25.367944: step 46590, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 51h:10m:43s remains)
INFO - root - 2017-12-15 18:53:31.922916: step 46600, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 51h:14m:31s remains)
2017-12-15 18:53:32.504501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8891449 -8.2048349 -9.2970009 -10.194685 -11.32296 -11.533401 -10.598766 -8.4525127 -7.6649036 -8.2007961 -8.2989254 -10.619085 -12.44076 -13.381531 -13.07395][-7.6609097 -8.0338268 -8.3491936 -9.4807014 -11.091068 -11.839672 -11.64996 -11.057036 -10.077734 -9.2167263 -9.1816549 -12.320952 -13.407815 -14.66045 -16.044966][-6.0497479 -6.9536614 -8.9648237 -9.0430136 -10.314753 -11.659351 -11.704493 -10.20193 -9.4526072 -9.8816433 -9.526515 -11.965418 -13.41795 -14.660192 -14.783058][-7.0072517 -7.7377653 -8.1481991 -7.0514803 -7.7987823 -8.3693466 -8.685 -9.2319031 -9.2782116 -8.5593 -7.9867945 -11.073828 -13.474203 -15.180477 -16.010445][-8.8786049 -9.894268 -9.769783 -7.1696811 -5.1817513 -3.8452454 -3.3888593 -5.1606717 -8.1869926 -7.2345943 -6.7359056 -10.066257 -12.388579 -15.253698 -16.907074][-11.004749 -10.243996 -9.2659626 -6.5848303 -2.6255929 1.2437658 4.0989718 2.0330386 -1.2218103 -3.927588 -7.2597437 -9.4530649 -10.761802 -13.69523 -15.764277][-11.254318 -11.36596 -8.4748268 -3.8219914 -0.24892187 2.5278182 6.8575587 8.20883 6.2840896 0.079936028 -5.720623 -8.5458317 -11.114937 -13.188042 -14.167145][-10.969478 -10.232091 -8.4900131 -3.2299006 1.0547071 4.8620934 8.4452095 7.6750169 7.2780347 3.0152812 -2.5037386 -7.8081217 -10.948421 -13.275417 -14.348469][-7.00139 -8.2206469 -7.5618916 -3.6727974 -1.0731258 3.8333383 7.7337365 6.4714942 6.2668805 2.35436 -1.984302 -7.8745575 -12.649515 -14.993322 -15.740116][-4.6235037 -5.4556508 -6.624135 -4.6094294 -3.0105374 -0.02076149 3.2555089 4.072144 4.0989957 0.93156672 -2.2952087 -8.8600311 -13.76897 -15.411469 -16.731421][-9.2399845 -9.7927 -9.9178762 -8.2477484 -7.5912523 -6.0656371 -3.9038651 -2.6861415 -1.8290625 -3.433053 -5.3593469 -11.908552 -14.58993 -16.244411 -16.487049][-14.733366 -14.430557 -14.234695 -13.182909 -12.695364 -10.875504 -10.438479 -10.393173 -8.785346 -8.849822 -10.080427 -13.103108 -14.342638 -16.315029 -15.676893][-17.099545 -16.633087 -15.566957 -15.670721 -15.021425 -13.40522 -12.857126 -13.05739 -12.92771 -12.10865 -11.398201 -13.435034 -13.366823 -13.883553 -13.285522][-15.550179 -14.212181 -13.839643 -13.803892 -13.395178 -12.883472 -12.775726 -12.06422 -11.860249 -12.529707 -12.515476 -12.478979 -11.665722 -12.72156 -11.829491][-10.997463 -11.522552 -9.954504 -9.2093315 -8.39406 -8.3871861 -9.0803165 -9.1938782 -10.010544 -9.8435774 -10.231649 -11.932066 -12.88141 -12.694506 -12.822556]]...]
INFO - root - 2017-12-15 18:53:39.150848: step 46610, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 52h:32m:10s remains)
INFO - root - 2017-12-15 18:53:45.710410: step 46620, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 52h:51m:17s remains)
INFO - root - 2017-12-15 18:53:52.242578: step 46630, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.639 sec/batch; 50h:45m:52s remains)
INFO - root - 2017-12-15 18:53:58.842285: step 46640, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 50h:33m:59s remains)
INFO - root - 2017-12-15 18:54:05.464252: step 46650, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.685 sec/batch; 54h:23m:21s remains)
INFO - root - 2017-12-15 18:54:12.164012: step 46660, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.675 sec/batch; 53h:37m:34s remains)
INFO - root - 2017-12-15 18:54:18.799780: step 46670, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 52h:58m:15s remains)
INFO - root - 2017-12-15 18:54:25.328475: step 46680, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 51h:36m:39s remains)
INFO - root - 2017-12-15 18:54:31.919191: step 46690, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 53h:10m:23s remains)
INFO - root - 2017-12-15 18:54:38.521378: step 46700, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 50h:32m:01s remains)
2017-12-15 18:54:39.070931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2505226 -4.7806454 -4.9776883 -4.257443 -3.6846464 -2.8628802 -2.1308591 -1.2679706 -0.81335878 -1.0983973 -1.4886727 -4.4238639 -6.4940562 -7.4146957 -7.7848425][-4.5795722 -4.0531292 -3.6665494 -3.1787958 -3.1209033 -2.4526942 -1.4116573 -0.23573637 0.53768206 0.77109337 0.10946894 -3.7275181 -6.1351156 -7.7611656 -8.5500116][-2.3032682 -2.9642518 -3.4895186 -2.1282682 -2.172241 -1.7533464 -1.2507625 -0.3166728 0.23606443 -0.15149307 -0.48973608 -3.459579 -5.6655059 -7.1818805 -8.1082172][-2.6241434 -3.4618721 -3.4015098 -1.9249322 -2.1345544 -1.3647494 -1.0000024 -0.8188076 -0.51178646 -0.20687771 -0.40955305 -3.8971672 -6.1232843 -7.1778059 -7.6173549][-3.8737965 -5.5477624 -5.5341349 -3.8031631 -2.756762 -0.8862319 -0.43600655 -1.0281372 -1.194869 -0.6520505 -0.65846443 -3.6590312 -5.7632165 -6.9802985 -7.307446][-4.70482 -5.9689274 -5.21811 -2.8387303 -0.89942551 1.3146567 2.3384681 1.4715443 0.85461092 -0.026340008 -0.82073021 -3.1508889 -5.0099096 -6.3053341 -7.0785155][-6.7672095 -6.355495 -4.6105375 -1.468935 0.85064793 2.8964677 4.3651261 3.9636035 3.1701941 1.0629272 -0.6958375 -3.4024138 -5.6946974 -6.9386992 -7.6127224][-7.6839814 -6.7691994 -5.0971508 -1.3425951 1.4589834 4.3675103 5.722218 4.7665772 4.4424291 2.6345382 0.62325811 -3.2461269 -6.4543309 -8.2569141 -9.031642][-6.0992107 -5.4996743 -4.8770223 -2.3803704 0.22970295 2.681644 3.7467122 3.1149621 2.4592347 0.88553429 -0.1440649 -4.0532246 -7.1893225 -8.8026094 -9.4029236][-6.5195775 -5.5214376 -4.5341892 -1.7442968 -0.97974396 0.73846912 1.6270132 1.3888068 0.54236555 -1.0608044 -2.2395418 -5.8415413 -8.2694721 -9.5118179 -10.097532][-9.5857468 -9.3164 -8.419795 -5.4510522 -4.8918114 -4.0412054 -3.6952338 -3.2411463 -3.6228502 -4.4067559 -5.4026 -9.5094433 -10.855054 -10.813835 -10.320429][-12.025162 -12.411572 -11.627248 -9.6109715 -8.653286 -7.8840547 -7.9423094 -8.1171532 -8.28587 -8.365715 -8.6836672 -10.75595 -11.504305 -11.55986 -10.80566][-13.780153 -13.605066 -12.525383 -11.572121 -10.908445 -9.6314869 -10.067596 -10.648159 -10.888741 -10.707953 -10.530265 -11.489354 -11.579041 -10.816141 -9.9146957][-11.973173 -12.00775 -11.065456 -9.7286472 -8.848568 -8.7691813 -9.3078756 -9.349556 -9.8866615 -10.362967 -10.438526 -9.68862 -9.6490364 -9.325985 -9.1160955][-8.5719471 -8.6590481 -7.9860716 -6.7927308 -5.9824057 -5.3194094 -5.3724246 -6.3066082 -7.1382794 -7.3002033 -7.625783 -8.2127619 -8.6171131 -8.5463324 -8.6870689]]...]
INFO - root - 2017-12-15 18:54:45.705651: step 46710, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.687 sec/batch; 54h:32m:47s remains)
INFO - root - 2017-12-15 18:54:52.378565: step 46720, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 53h:49m:23s remains)
INFO - root - 2017-12-15 18:54:59.001110: step 46730, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 52h:41m:29s remains)
INFO - root - 2017-12-15 18:55:05.596534: step 46740, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 52h:46m:04s remains)
INFO - root - 2017-12-15 18:55:12.196871: step 46750, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 52h:34m:46s remains)
INFO - root - 2017-12-15 18:55:18.711556: step 46760, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 52h:00m:21s remains)
INFO - root - 2017-12-15 18:55:25.302729: step 46770, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 52h:13m:43s remains)
INFO - root - 2017-12-15 18:55:31.905352: step 46780, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 52h:30m:36s remains)
INFO - root - 2017-12-15 18:55:38.457835: step 46790, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 51h:53m:48s remains)
INFO - root - 2017-12-15 18:55:45.065631: step 46800, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 53h:09m:39s remains)
2017-12-15 18:55:45.569279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8215532 -3.7460055 -2.907655 -3.0538855 -4.6797752 -5.26261 -6.1639442 -5.2471848 -3.6819422 -4.318449 -4.9288726 -6.1911044 -7.7620339 -8.1556644 -7.40413][-4.3587575 -3.6983223 -2.8099222 -2.5640407 -3.3297117 -3.6888762 -2.9600368 -2.040801 -1.8496873 -2.2869129 -2.8955185 -4.0927863 -5.3260651 -6.7935009 -6.7856646][-3.0772569 -3.2876668 -3.904243 -3.6742575 -4.0052309 -3.6450882 -2.8315582 -1.7153149 -0.96305323 -1.3823609 -1.9181712 -3.3119776 -4.435976 -5.0174127 -4.9976997][-4.9006243 -4.6786432 -5.1895022 -5.04586 -5.9204364 -4.4797111 -3.036222 -2.2364998 -1.1609998 -0.66732121 -0.59380293 -2.5165632 -4.7280626 -5.875814 -4.8493247][-3.6250007 -5.1062965 -4.7580237 -4.5505013 -6.236218 -4.8975658 -2.5128818 -1.3760138 -0.90071344 0.52160025 2.0267925 -0.12929296 -2.6789951 -5.2304859 -6.1305051][-4.2868023 -4.1947317 -3.5991664 -2.5771632 -1.6051464 -1.2321038 -0.059630871 1.8987551 1.5129318 0.9298811 0.67122984 -0.87000132 -2.9002228 -4.7468033 -4.9377751][-3.6400595 -2.7499394 -2.1399729 -0.51298666 1.1515279 3.0636497 4.2393374 4.2555947 4.0024467 2.6880584 1.0312386 -0.81492853 -2.5033596 -5.01471 -5.8973851][-3.1113632 -2.5533102 -2.823971 0.1907258 3.7281165 5.3586068 7.2457051 6.0850759 3.780725 2.5522256 1.8761096 -0.072220325 -1.9089017 -4.1553411 -4.732688][-4.2630558 -2.7171035 -1.958899 -0.60409069 0.95974445 4.4825549 6.093 5.1564765 2.8638682 0.77521133 0.24467802 -0.96906137 -2.7679005 -4.3865142 -4.422533][-4.4718719 -4.2152777 -2.839988 -0.725647 -0.46747637 1.3216639 3.3477683 3.2822995 1.4439344 -0.22800493 -0.96248341 -2.0599096 -2.9285746 -4.3780656 -4.9228415][-7.2549167 -6.3416052 -5.7003269 -4.4009967 -3.3323829 -2.6203494 -2.3354416 -1.6950808 -2.2630641 -3.2505779 -3.1302807 -3.9663205 -5.4272656 -6.1980586 -5.8115592][-10.410006 -8.6545744 -6.7377758 -5.7341852 -6.5909319 -6.2975445 -6.2078943 -6.3165627 -6.36222 -6.3328056 -6.0489926 -6.6654043 -7.5085325 -8.1300945 -6.2704844][-10.238018 -8.7440271 -7.67426 -6.2372518 -6.8491535 -8.1469288 -8.3105726 -7.9804468 -7.8338232 -6.9624619 -6.2123566 -6.7411585 -7.3470306 -6.3583755 -4.7921848][-8.7565432 -7.2225513 -6.0203295 -5.6039248 -5.920979 -6.826077 -6.9980197 -6.6278992 -5.9518838 -6.5111928 -6.376071 -4.5767937 -4.3945103 -4.2523537 -2.779825][-5.0170379 -5.412477 -5.5567026 -4.7830057 -4.1665683 -4.5289688 -4.1542935 -4.6180968 -5.0531778 -4.2213211 -3.9217758 -4.7008114 -4.8346214 -4.9288144 -5.2175188]]...]
INFO - root - 2017-12-15 18:55:52.123315: step 46810, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.632 sec/batch; 50h:11m:18s remains)
INFO - root - 2017-12-15 18:55:58.674104: step 46820, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 51h:28m:49s remains)
INFO - root - 2017-12-15 18:56:05.225780: step 46830, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 51h:35m:01s remains)
INFO - root - 2017-12-15 18:56:11.964805: step 46840, loss = 0.18, batch loss = 0.13 (11.6 examples/sec; 0.689 sec/batch; 54h:39m:15s remains)
INFO - root - 2017-12-15 18:56:18.610236: step 46850, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 52h:20m:34s remains)
INFO - root - 2017-12-15 18:56:25.236731: step 46860, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 52h:40m:36s remains)
INFO - root - 2017-12-15 18:56:31.774614: step 46870, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 52h:23m:00s remains)
INFO - root - 2017-12-15 18:56:38.367639: step 46880, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 52h:58m:47s remains)
INFO - root - 2017-12-15 18:56:44.988142: step 46890, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 53h:05m:24s remains)
INFO - root - 2017-12-15 18:56:51.568849: step 46900, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 52h:10m:34s remains)
2017-12-15 18:56:52.081442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.865778 -7.9693151 -8.0907383 -6.7643795 -6.5176544 -5.9823279 -5.8414173 -5.7513027 -5.2124662 -4.6772451 -4.3181543 -5.2556658 -7.8638248 -8.9109163 -8.1584225][-8.49705 -7.8287697 -6.5376492 -6.229795 -6.6471481 -5.5440531 -5.5105762 -5.1009817 -5.3493438 -5.2384262 -4.9211054 -6.2596412 -8.2392559 -9.2433319 -8.8300991][-3.2044265 -3.7783451 -5.0134163 -4.9402428 -5.126471 -5.2290382 -5.0473852 -4.7109509 -5.1395712 -4.9139705 -4.9420185 -5.7639451 -7.7533484 -8.1785393 -7.450388][-3.6662159 -3.3058379 -2.4357183 -2.9106545 -3.7209246 -3.9805493 -3.9886742 -4.2308917 -4.7167869 -4.6935487 -4.7763376 -4.8196568 -6.8569884 -7.8934717 -7.5509462][-2.1830392 -3.7772737 -5.1043887 -4.2463775 -2.714673 -1.6081824 -0.80797338 -1.8457236 -2.3052282 -2.1040735 -1.9252107 -2.8044343 -5.22396 -6.6405797 -6.7831173][-5.6301732 -5.1549158 -3.7803111 -2.9000461 -1.4096084 0.99248457 2.6357608 2.2478004 1.530232 0.47577477 -0.1225543 -0.60458755 -3.118458 -4.7395411 -5.2859268][-7.5219979 -7.1584692 -6.5288134 -3.5782373 -1.4497437 1.9322796 4.4440265 4.9672589 4.8829036 2.8145766 0.84380388 -0.85639954 -3.0015922 -3.7875533 -3.7939405][-6.16635 -5.8668342 -5.0414586 -2.1885285 0.867321 3.9905896 5.6632504 5.545939 5.4485126 3.0891109 0.96955347 -1.0162587 -3.6961267 -4.30293 -3.2822697][-4.3696795 -3.8503456 -2.5380166 -0.13430214 1.6699038 4.4427285 5.5531697 5.2275882 4.1975036 2.2402668 1.0930052 -1.3404779 -4.4163141 -5.4190865 -4.1165881][-3.6081231 -3.1362519 -1.7248542 1.3629322 3.3823094 4.5824103 5.1888919 4.878902 3.2354178 1.3385997 -1.0365391 -3.5859663 -5.9287066 -5.5845351 -3.5599864][-5.94293 -4.5595188 -1.7685385 0.093082428 1.4483848 1.6405735 1.7411656 1.7552629 0.71522141 -0.66682434 -3.2503836 -5.2663913 -7.3921943 -7.8594522 -6.3396153][-10.242292 -8.1581469 -4.717752 -1.4617271 0.57686472 0.86069536 -0.15963173 -1.2211404 -1.5936317 -2.5615082 -4.323667 -6.2063646 -7.5264921 -7.366147 -5.7992144][-12.024954 -10.223532 -8.0424 -5.1864734 -3.1544921 -3.0079319 -2.8076346 -4.0144386 -4.7770391 -5.1889114 -5.4778275 -6.9780416 -8.16387 -7.8098421 -6.1819267][-9.4102421 -8.5986223 -6.8873205 -4.9290886 -4.9871364 -5.6151209 -6.2063622 -6.8720613 -6.6643963 -6.7025614 -6.4149394 -6.2781029 -6.3597627 -5.473464 -4.434598][-6.9872923 -7.5610895 -6.6680264 -5.1168327 -4.1245871 -3.7341957 -4.6640644 -5.63159 -7.1494603 -8.44218 -9.0189228 -8.5736637 -7.9583244 -6.9844022 -6.1605697]]...]
INFO - root - 2017-12-15 18:56:58.637122: step 46910, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.641 sec/batch; 50h:51m:40s remains)
INFO - root - 2017-12-15 18:57:05.246459: step 46920, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 52h:36m:11s remains)
INFO - root - 2017-12-15 18:57:11.910842: step 46930, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 51h:37m:30s remains)
INFO - root - 2017-12-15 18:57:18.393209: step 46940, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 51h:48m:24s remains)
INFO - root - 2017-12-15 18:57:25.043305: step 46950, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 53h:29m:24s remains)
INFO - root - 2017-12-15 18:57:31.683948: step 46960, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 50h:40m:05s remains)
INFO - root - 2017-12-15 18:57:38.366095: step 46970, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.673 sec/batch; 53h:23m:47s remains)
INFO - root - 2017-12-15 18:57:44.951591: step 46980, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 50h:50m:33s remains)
INFO - root - 2017-12-15 18:57:51.496460: step 46990, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.633 sec/batch; 50h:12m:48s remains)
INFO - root - 2017-12-15 18:57:58.150188: step 47000, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 52h:14m:37s remains)
2017-12-15 18:57:58.683700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.45915 -7.1367178 -4.7778325 -3.4375372 -2.9309812 -3.6252728 -5.2716684 -6.451767 -6.2062626 -5.9485178 -5.3745327 -5.6079311 -6.1192727 -6.36682 -5.2510471][-6.3295393 -5.0712576 -2.7249053 -1.5721569 -2.272999 -2.9673891 -4.769434 -6.7395172 -7.1591158 -6.7754388 -6.34534 -7.1324511 -7.7988949 -8.0660505 -7.3239708][-3.1397712 -3.1255758 -2.10713 -0.60471773 -1.4401107 -3.0580385 -4.7814493 -6.1441522 -6.8405619 -7.3696017 -7.5800018 -8.6510344 -9.7933159 -9.7901592 -8.85651][-4.9017439 -4.9029074 -3.8838167 -2.9774351 -3.462996 -3.5641267 -4.2276955 -5.493865 -6.47966 -6.2344379 -6.837503 -8.7586136 -10.557204 -11.099903 -10.252512][-6.617156 -7.5193839 -5.6795015 -3.8225284 -3.2939949 -1.428822 -0.34320498 -1.8092737 -3.3625975 -4.3196235 -5.5390286 -7.669076 -9.9472179 -11.108926 -11.168172][-7.0927205 -7.2049713 -5.7514238 -3.6879094 -1.5770574 1.0659299 3.2842145 2.8480515 1.6270347 -1.0304713 -3.8580151 -6.1766868 -8.1522045 -9.9224424 -10.067247][-8.4889812 -8.18467 -6.74744 -3.5854306 -1.1091151 1.2683311 4.3154426 5.0262818 4.4896445 1.2766414 -1.5122108 -3.643718 -5.7026672 -7.1042275 -6.978241][-8.5505676 -7.444222 -5.7917914 -1.6710191 1.0659409 3.4603724 6.4127164 6.4584279 6.1537948 3.1348958 0.65887165 -1.3071957 -3.2375014 -4.3301444 -3.8067794][-7.143023 -6.5000978 -5.0290961 -1.0395775 1.3250952 4.1746879 6.1508 5.7285647 5.0199752 2.905519 1.1839781 -1.2742496 -3.4640746 -4.0775166 -3.2039325][-8.0948248 -7.2178292 -6.1406627 -2.9555695 -1.759398 0.1296339 2.3847976 3.4118943 3.4481568 2.1143336 0.93661118 -1.2033415 -2.9070265 -4.223074 -3.9498186][-12.052466 -11.103825 -9.529129 -7.4659262 -6.3759408 -5.3989086 -4.7714138 -4.0954661 -3.0450113 -3.01221 -3.40736 -5.250145 -5.9315872 -5.9620285 -5.007895][-15.555042 -14.802114 -13.603191 -11.028514 -9.8358555 -9.2812872 -9.3080673 -9.362792 -9.01697 -8.5444174 -8.1021261 -8.8066912 -8.5094776 -8.374135 -7.4457836][-13.832882 -13.463607 -12.553787 -10.936254 -10.147201 -9.9818907 -9.7223835 -9.4565859 -9.7089157 -8.74884 -8.0641069 -8.4855976 -7.7369294 -7.0959778 -6.1545982][-10.679651 -10.33754 -9.34564 -8.71705 -8.5545959 -8.7311964 -8.8555326 -7.9355249 -7.3465886 -7.226264 -7.3775129 -6.503593 -5.9807954 -5.7826796 -5.3218093][-7.3831677 -6.8826361 -6.5444856 -5.5017118 -4.4923086 -4.3846641 -4.6549258 -5.1489172 -5.5761976 -5.0324516 -4.62834 -4.8307815 -5.4352636 -5.9752107 -6.6360483]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 18:58:05.433256: step 47010, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 53h:16m:17s remains)
INFO - root - 2017-12-15 18:58:12.064146: step 47020, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 52h:34m:51s remains)
INFO - root - 2017-12-15 18:58:18.671063: step 47030, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.656 sec/batch; 52h:03m:00s remains)
INFO - root - 2017-12-15 18:58:25.187306: step 47040, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 53h:12m:32s remains)
INFO - root - 2017-12-15 18:58:31.737950: step 47050, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 51h:31m:55s remains)
INFO - root - 2017-12-15 18:58:38.351697: step 47060, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 52h:13m:54s remains)
INFO - root - 2017-12-15 18:58:44.986432: step 47070, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 52h:57m:11s remains)
INFO - root - 2017-12-15 18:58:51.736883: step 47080, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 51h:48m:48s remains)
INFO - root - 2017-12-15 18:58:58.356797: step 47090, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.655 sec/batch; 51h:56m:40s remains)
INFO - root - 2017-12-15 18:59:04.993938: step 47100, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 54h:05m:25s remains)
2017-12-15 18:59:05.532109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.84682751 -0.88759327 -0.78634071 -0.55747747 -0.90472317 -1.1594081 -1.4706407 -1.7973723 -2.2134798 -2.648891 -2.9592328 -5.231482 -7.1046753 -7.4795456 -6.8160458][-1.8225932 -1.4341955 -0.76016569 -0.80266 -1.7935615 -2.6235483 -2.8958673 -2.9486687 -2.8985486 -2.9092245 -3.2886479 -5.3939905 -6.9024024 -7.087935 -6.3527765][-0.95880175 -1.3137817 -1.6244187 -1.2107182 -1.578341 -2.7797308 -3.2386448 -3.5113802 -3.3397026 -3.088289 -3.3129289 -5.6817355 -7.8887205 -8.0288372 -7.6247168][-1.7382216 -1.3599086 -1.2908363 -1.1563663 -2.4698815 -3.6358714 -3.975949 -4.0586472 -3.3177831 -2.9221885 -2.3966129 -5.2214017 -8.1244831 -8.6445961 -8.4456453][-2.8411598 -2.2290397 -2.0784001 -1.440371 -2.0058255 -2.5681164 -2.6897821 -3.0941062 -2.7369134 -2.2202029 -1.5226841 -3.7584474 -5.9610124 -7.440259 -8.0305805][-3.9126015 -2.9476438 -1.8580818 -0.51682758 -0.42793036 -0.046882629 0.69273186 0.97123241 1.0664806 0.98337078 0.91737127 -1.8574128 -4.6505046 -5.6209588 -6.114892][-5.3819466 -4.4596839 -2.7310014 -0.46737766 0.401464 1.6233511 3.5741887 4.3327813 5.1102757 3.9711118 2.4523158 -0.89294004 -4.100163 -5.0765781 -5.5829678][-5.3540316 -4.9115434 -3.1789916 -0.55699873 0.58094931 2.3323703 4.4500155 5.1714854 6.478148 6.0290723 4.3856883 -0.57864428 -4.9648089 -6.4556074 -7.2264085][-4.3377376 -3.7296782 -2.5921228 -0.72786903 0.21146441 1.8732114 3.2706103 3.9697747 5.4405265 4.9888825 3.9073396 -0.98009062 -5.8461704 -7.652544 -8.5739861][-3.2470636 -2.8142805 -2.1062903 -0.4546771 -0.11539412 0.54468775 1.5488739 2.4183364 3.3726106 2.9876828 2.023973 -2.2995818 -6.1591816 -8.2692547 -9.6621857][-6.7746716 -6.2045417 -5.175025 -3.3093693 -3.4316943 -3.0881069 -3.3594577 -3.1023562 -2.3437712 -2.6287816 -2.9360862 -6.6506052 -8.7724152 -9.1735506 -8.9363089][-11.076088 -10.134708 -8.3646584 -6.904417 -7.0929513 -6.5161905 -7.167161 -7.6518755 -7.5064874 -7.346302 -7.26423 -9.0020018 -8.905056 -9.5023422 -9.2048569][-13.632303 -12.361115 -10.634604 -9.863987 -9.7742634 -8.55547 -9.2607212 -9.708683 -9.9920826 -9.7204037 -9.7023468 -10.156748 -9.6765089 -8.7979374 -6.5992389][-10.816393 -9.84679 -8.80197 -7.7650228 -7.4494104 -8.2425852 -8.698123 -8.3537025 -8.9284277 -8.980154 -9.1407433 -8.3980083 -7.6448612 -7.4517794 -6.7345686][-6.9235787 -6.1137586 -5.1552997 -4.4319348 -4.4771972 -4.4280243 -4.6200438 -5.5438952 -5.8497691 -6.0763431 -6.8338194 -7.7313113 -8.2395735 -8.1623583 -7.5959024]]...]
INFO - root - 2017-12-15 18:59:12.059361: step 47110, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 51h:00m:09s remains)
INFO - root - 2017-12-15 18:59:18.657473: step 47120, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 51h:59m:58s remains)
INFO - root - 2017-12-15 18:59:25.214347: step 47130, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 52h:10m:48s remains)
INFO - root - 2017-12-15 18:59:31.834855: step 47140, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 52h:17m:09s remains)
INFO - root - 2017-12-15 18:59:38.399182: step 47150, loss = 0.11, batch loss = 0.06 (11.7 examples/sec; 0.681 sec/batch; 53h:59m:57s remains)
INFO - root - 2017-12-15 18:59:44.990920: step 47160, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 50h:37m:43s remains)
INFO - root - 2017-12-15 18:59:51.623523: step 47170, loss = 0.26, batch loss = 0.22 (12.5 examples/sec; 0.641 sec/batch; 50h:46m:55s remains)
INFO - root - 2017-12-15 18:59:58.190908: step 47180, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 50h:46m:29s remains)
INFO - root - 2017-12-15 19:00:04.741151: step 47190, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 52h:52m:10s remains)
INFO - root - 2017-12-15 19:00:11.370345: step 47200, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 51h:48m:15s remains)
2017-12-15 19:00:11.859731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.1264067 -8.3142633 -7.62483 -6.3247294 -7.0552568 -7.8058929 -8.7272511 -9.2207184 -9.4562244 -8.8472252 -7.5614886 -7.794363 -9.2604523 -8.8683357 -5.8823557][-8.4664021 -8.430254 -6.7400351 -5.6703534 -6.6600523 -7.4194031 -7.978416 -8.7210979 -9.7257214 -9.7135448 -8.3740044 -7.9070406 -8.346137 -8.8244295 -6.733489][-7.6947527 -8.6223965 -8.3796911 -6.5420394 -6.4260392 -6.8847866 -7.60104 -7.9418969 -8.2582512 -8.2953262 -8.2273531 -8.8912506 -10.048964 -10.570606 -8.542552][-7.9635286 -8.7310019 -8.5919056 -7.1290321 -6.9484959 -6.4438667 -5.8808146 -6.6163616 -7.2618661 -7.0041189 -7.2827492 -8.5076895 -10.488781 -11.413613 -9.2338209][-9.7164955 -11.078409 -10.645281 -7.9767141 -6.0583239 -3.4754963 -1.8414476 -3.8019285 -5.9187484 -5.6919804 -6.5213971 -7.6502185 -9.7961025 -10.934791 -8.61186][-11.154571 -10.984741 -9.8229218 -7.0648689 -4.1369443 0.3083086 3.2615952 1.8182006 -0.20556116 -3.2067547 -6.3865981 -6.7651005 -8.4610262 -9.6303768 -7.8370419][-12.145176 -10.615488 -8.368145 -5.2294917 -2.4952776 2.3296723 7.2446561 6.9019666 4.0927005 -1.1452899 -6.1378269 -7.4437733 -10.267294 -10.681986 -8.1860332][-11.693621 -10.028636 -7.4593549 -3.0007906 0.55104256 3.2020192 6.4115682 7.0779376 6.30442 1.4209895 -4.8133736 -7.9288096 -11.366314 -12.359289 -9.9460049][-9.5461159 -8.6914778 -7.0958085 -3.3572474 -0.020932198 2.7564073 4.5161147 3.6642718 2.8315635 -0.23172712 -4.3808594 -7.6813016 -12.216446 -13.582424 -11.651409][-7.7767925 -6.4093289 -5.7292409 -3.3310041 -2.1944909 0.56506634 2.9482856 1.0263391 -1.8584325 -5.1711545 -7.5508804 -9.1125908 -12.331207 -13.840572 -12.363343][-11.046274 -10.551123 -9.7648544 -7.0361352 -6.7314982 -5.0643497 -3.7755542 -5.0443449 -6.8566837 -10.171394 -13.085018 -14.349312 -15.342388 -15.312563 -12.861884][-15.026573 -14.297338 -13.703213 -12.167415 -11.832754 -10.65058 -10.271603 -10.318396 -10.652106 -12.558958 -14.33478 -15.292925 -15.855663 -15.205202 -11.984596][-14.152954 -13.873062 -14.711535 -13.438401 -12.89303 -12.175647 -11.804559 -12.297749 -12.660462 -12.551935 -12.584332 -13.208261 -12.801697 -12.322306 -9.5976772][-11.639153 -10.852385 -11.548391 -10.012356 -9.1467981 -8.3153267 -9.1143293 -9.9448662 -10.683981 -10.804903 -10.678555 -9.611124 -9.0224667 -8.4734821 -6.5794725][-8.3273745 -7.6662383 -7.88973 -6.5918794 -6.005507 -4.335875 -4.085722 -5.7035103 -7.0462003 -7.2326856 -7.6247168 -7.154664 -6.9423838 -6.2355561 -4.1355367]]...]
INFO - root - 2017-12-15 19:00:18.400923: step 47210, loss = 0.20, batch loss = 0.16 (12.3 examples/sec; 0.650 sec/batch; 51h:32m:15s remains)
INFO - root - 2017-12-15 19:00:25.011052: step 47220, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 51h:45m:34s remains)
INFO - root - 2017-12-15 19:00:31.658327: step 47230, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.691 sec/batch; 54h:45m:55s remains)
INFO - root - 2017-12-15 19:00:38.172174: step 47240, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 52h:02m:43s remains)
INFO - root - 2017-12-15 19:00:44.843242: step 47250, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 50h:14m:13s remains)
INFO - root - 2017-12-15 19:00:51.405569: step 47260, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 53h:19m:06s remains)
INFO - root - 2017-12-15 19:00:58.057130: step 47270, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.693 sec/batch; 54h:56m:13s remains)
INFO - root - 2017-12-15 19:01:04.737091: step 47280, loss = 0.14, batch loss = 0.09 (11.5 examples/sec; 0.698 sec/batch; 55h:18m:56s remains)
INFO - root - 2017-12-15 19:01:11.305805: step 47290, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 52h:17m:08s remains)
INFO - root - 2017-12-15 19:01:17.856324: step 47300, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 52h:16m:15s remains)
2017-12-15 19:01:18.368410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7290988 -6.2433906 -6.0657344 -6.0596685 -6.6381645 -6.8410292 -7.3611212 -7.6967373 -7.848258 -7.4089031 -7.22303 -8.5034447 -10.770861 -10.936855 -9.2705564][-6.071641 -7.0187054 -7.6865788 -8.1386414 -8.9056339 -9.4671021 -9.7596912 -10.144064 -10.396387 -10.195211 -9.9823875 -10.423862 -12.942834 -14.062607 -12.266825][-4.3867407 -6.0888743 -7.4095883 -7.7766237 -7.9015207 -8.3096828 -9.0899353 -9.8181114 -10.366078 -9.8628082 -9.66792 -10.889343 -13.188845 -13.771938 -12.549928][-6.6184511 -7.9010382 -8.5513821 -7.7890434 -7.9831181 -7.4105678 -7.1091223 -8.0326824 -8.3678436 -8.6132908 -9.0859833 -10.08862 -12.508572 -13.662175 -12.04319][-8.8296041 -10.413546 -10.760533 -8.8165712 -6.7447281 -3.4105175 -1.9785998 -4.1123462 -5.9975071 -6.6124244 -7.3113637 -9.3405647 -12.374429 -13.202831 -12.024117][-11.338318 -11.255116 -10.559547 -7.0612574 -4.208952 0.15293884 3.4537711 3.1741071 1.6522713 -2.2238498 -5.6874137 -7.0586476 -9.8451138 -11.724541 -10.707137][-12.946442 -12.284412 -10.443628 -6.0937848 -2.3501971 2.6465402 6.9466166 7.2147651 6.5026317 1.8107381 -3.0160613 -5.3806911 -8.5115261 -9.019845 -7.486289][-12.564289 -11.724056 -9.4005079 -4.3194895 -0.054799557 4.1672578 7.4456668 7.8931661 7.5606694 3.1437955 -1.1593723 -3.7270963 -7.5777268 -8.0957079 -6.1042962][-10.687244 -10.611254 -8.8825455 -4.6188087 -1.055203 2.053689 4.7651343 5.271009 4.6517615 1.4019995 -1.2693081 -4.1813283 -7.8421 -7.4326606 -5.434556][-9.233078 -9.5761051 -9.0246344 -6.2727661 -3.9830184 -0.47684669 1.9121909 1.8374953 1.0026526 -0.93099928 -2.7154918 -5.0381012 -7.7763329 -8.1578751 -6.6421156][-11.434433 -12.341049 -12.665154 -10.509197 -9.03303 -6.2414083 -4.4779153 -3.6298008 -3.7630596 -4.627274 -5.9082808 -8.18155 -9.6960335 -9.6779938 -7.9126945][-14.632618 -15.119591 -14.794174 -12.160677 -10.816253 -9.6161251 -8.49657 -7.3551984 -6.8980222 -7.4527025 -8.0280561 -8.4743595 -9.0222387 -9.5096035 -8.384553][-12.30586 -11.441465 -11.718843 -11.436777 -10.425434 -8.5061293 -7.354414 -6.9481568 -6.8172846 -6.4860406 -6.4059796 -7.4090142 -8.0952778 -8.0260963 -6.2018008][-9.9058495 -9.3214855 -9.0004511 -8.5296478 -7.8183146 -7.9518609 -7.249135 -5.9364562 -5.3249173 -5.7750349 -6.5776081 -6.9337125 -6.9582357 -6.95233 -5.9837322][-7.6774893 -6.2148046 -5.4083438 -5.4917674 -4.4809303 -3.5612357 -3.0862334 -3.402616 -3.7750895 -3.8908992 -4.3962369 -5.9756079 -7.7496367 -8.6261854 -7.9276848]]...]
INFO - root - 2017-12-15 19:01:24.993171: step 47310, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 52h:32m:18s remains)
INFO - root - 2017-12-15 19:01:31.588850: step 47320, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 52h:47m:15s remains)
INFO - root - 2017-12-15 19:01:38.145796: step 47330, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 51h:37m:01s remains)
INFO - root - 2017-12-15 19:01:44.703383: step 47340, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.635 sec/batch; 50h:18m:54s remains)
INFO - root - 2017-12-15 19:01:51.240553: step 47350, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 52h:01m:52s remains)
INFO - root - 2017-12-15 19:01:57.784771: step 47360, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 51h:34m:42s remains)
INFO - root - 2017-12-15 19:02:04.397709: step 47370, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.687 sec/batch; 54h:22m:37s remains)
INFO - root - 2017-12-15 19:02:11.079575: step 47380, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 52h:14m:41s remains)
INFO - root - 2017-12-15 19:02:17.683819: step 47390, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 51h:22m:03s remains)
INFO - root - 2017-12-15 19:02:24.293764: step 47400, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 51h:56m:09s remains)
2017-12-15 19:02:24.786838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4142985 -6.9594975 -5.5325131 -4.5388236 -4.7090931 -5.0625448 -4.8397164 -4.2487206 -3.8280883 -3.1198537 -2.0841789 -2.303941 -3.5447721 -3.1678922 -2.3873107][-6.3160286 -5.3301358 -3.0758407 -1.41014 -1.6870222 -2.5280344 -2.5257387 -1.8543971 -1.5790076 -0.18528318 1.3478799 0.22942686 -1.4049463 -1.4603381 -2.03858][-3.9677963 -3.4928641 -3.1279547 -1.7424445 -1.2549634 -1.2881699 -0.78458071 -0.33848906 0.6220727 1.8419008 2.6048417 1.6044927 0.068727493 -0.085238934 -1.177599][-4.6234522 -4.5257812 -2.8887112 -2.19805 -2.4523132 -1.8131447 -1.3573914 -1.1496882 -0.54595089 0.49778557 1.2453294 -0.78058481 -3.3022063 -3.6967747 -4.5020156][-6.8089943 -6.0990734 -4.4146738 -2.5788851 -1.5812068 -0.18975925 0.71009016 0.21403217 -0.32141066 -0.25952196 -0.81381512 -2.6950877 -4.7772951 -4.6762209 -4.536386][-8.643693 -7.6940413 -5.42772 -2.3635724 -0.25835657 1.4962869 2.622129 2.7035365 2.6982245 1.0913615 -0.68625879 -2.7789915 -4.8427815 -5.3108993 -4.16348][-9.4480639 -7.9986305 -5.2018318 -1.8278425 0.32112551 2.6448359 4.3134694 3.526526 2.147388 0.83571291 -0.66199112 -3.3128469 -5.4679437 -4.8408246 -3.9353218][-9.7107782 -8.8426857 -6.2621379 -2.7221005 -0.36074781 2.0496273 3.3601851 2.0736513 0.95151091 -0.81566191 -2.6910872 -5.2674389 -7.2252641 -6.3661509 -5.0421543][-8.2904081 -7.5366049 -6.05979 -3.230864 -0.8194828 1.0532465 1.8240333 0.84169436 -0.013326645 -2.0900345 -4.0650167 -6.0999761 -7.8113613 -7.0396748 -5.0525823][-8.8560915 -7.4333 -5.7945685 -2.9055991 -0.44745588 0.43346834 0.78132439 0.4622426 -1.2089128 -3.0809491 -4.1180553 -6.1568413 -8.0701656 -7.3307 -6.0865426][-11.162411 -10.782001 -8.639473 -5.8600712 -4.006031 -2.0966218 -1.0737085 -2.094615 -3.5517666 -4.7737179 -5.5128036 -7.7628875 -8.5671024 -8.188283 -6.9559665][-15.806892 -14.844122 -13.440044 -10.174913 -8.2651806 -6.5726833 -6.5096836 -7.3748741 -7.6276059 -8.1410847 -8.6829767 -9.1729841 -8.77222 -8.7914162 -8.06749][-14.312571 -13.050047 -11.282505 -9.6105328 -8.6346931 -7.806119 -8.2454128 -8.6060114 -8.9432917 -8.9060049 -8.7811079 -8.6132021 -8.1150742 -7.1093292 -5.8119888][-11.447524 -10.310823 -9.2218819 -7.1357069 -6.0601549 -6.692008 -7.6118021 -7.477181 -7.5983334 -7.5779505 -7.856914 -7.3715 -6.8109217 -5.3693814 -4.3185806][-9.2433491 -8.2569714 -6.4753942 -5.3407664 -5.0574493 -5.6984706 -5.37011 -5.9458504 -5.1663661 -5.0951204 -5.0202785 -5.6984425 -5.4391327 -5.740519 -5.3122931]]...]
INFO - root - 2017-12-15 19:02:31.422574: step 47410, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 53h:43m:08s remains)
INFO - root - 2017-12-15 19:02:37.969488: step 47420, loss = 0.19, batch loss = 0.14 (11.5 examples/sec; 0.693 sec/batch; 54h:52m:05s remains)
INFO - root - 2017-12-15 19:02:44.530420: step 47430, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.654 sec/batch; 51h:47m:24s remains)
INFO - root - 2017-12-15 19:02:51.180002: step 47440, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 51h:53m:19s remains)
INFO - root - 2017-12-15 19:02:57.692370: step 47450, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 51h:17m:36s remains)
INFO - root - 2017-12-15 19:03:04.314465: step 47460, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 52h:40m:21s remains)
INFO - root - 2017-12-15 19:03:10.886491: step 47470, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 50h:53m:46s remains)
INFO - root - 2017-12-15 19:03:17.490512: step 47480, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.683 sec/batch; 54h:05m:34s remains)
INFO - root - 2017-12-15 19:03:24.173425: step 47490, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 52h:29m:17s remains)
INFO - root - 2017-12-15 19:03:30.757028: step 47500, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 51h:04m:04s remains)
2017-12-15 19:03:31.277319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8939204 -7.7418952 -7.9245071 -7.8336396 -7.5389576 -7.7270908 -8.4851561 -8.2627878 -7.782917 -6.8756924 -5.2910585 -6.5191512 -8.0494671 -8.2957983 -8.0823011][-4.3818192 -5.6019149 -5.5398555 -5.7183685 -6.7725115 -7.0223 -8.22238 -8.5858879 -8.5742073 -7.203495 -5.7525778 -6.8491631 -9.0877695 -10.056836 -9.6968832][-2.5877044 -3.5895989 -3.5409329 -4.7538614 -5.4190607 -5.7763391 -6.1127558 -6.5328217 -7.2239714 -7.1374059 -6.2930975 -7.2697082 -9.7610817 -10.789658 -11.616716][-2.6893044 -3.3420844 -3.7434616 -4.5046816 -4.5362864 -4.8415213 -5.2592297 -5.7296467 -5.1677132 -4.9218655 -4.9084115 -6.8841114 -9.25647 -9.5921745 -9.66347][-3.5038762 -4.7923355 -4.8209934 -4.7573371 -4.2747207 -3.2878048 -2.8061924 -3.9909687 -4.4796257 -3.9793952 -3.5545964 -4.8072014 -7.7016578 -8.8924417 -8.9072723][-3.6245019 -4.4680271 -4.4706411 -3.5833192 -1.970084 -0.18538332 1.1756926 0.14279842 -1.1041427 -1.448143 -1.9085193 -3.2970076 -6.1854944 -6.6486049 -6.897758][-4.10062 -4.0584579 -3.0741229 -1.426527 -0.39786196 1.2621861 3.8652654 3.9765897 3.2827611 0.9553647 -1.7026234 -3.7275043 -6.2520008 -6.9068675 -7.3236842][-5.1511183 -3.7480035 -2.627238 -1.4742489 -0.16764212 1.3291731 2.88629 3.0674882 3.4626317 2.2833552 0.64830685 -3.4360597 -7.8046589 -8.66659 -8.5592384][-6.3372707 -4.9715157 -3.2309115 -1.8338172 -1.129487 0.62287235 2.5196109 3.2168059 4.1974797 3.6897225 2.234951 -2.2882962 -8.1403685 -11.287992 -12.271481][-6.404798 -5.6327596 -4.06643 -1.9283295 -1.3919511 -0.20219707 0.78715515 1.0935435 1.9734349 2.9692492 2.6555037 -1.6597743 -6.8815455 -10.830692 -14.054514][-9.2557821 -8.82848 -7.538496 -5.4667988 -5.1719537 -4.0994196 -2.8544509 -2.1042039 -1.9375067 -2.2774618 -2.8300052 -6.3983116 -10.767416 -14.058016 -15.581667][-12.238571 -12.103531 -11.392478 -10.22303 -10.760937 -9.7713184 -8.8416338 -7.7753644 -7.1105318 -6.4851875 -6.8705149 -10.040117 -13.746687 -15.603876 -15.528355][-13.065678 -12.800413 -12.128735 -11.325146 -12.430666 -11.837299 -11.701757 -9.9044514 -8.7149916 -8.2185774 -8.9187965 -11.493145 -13.830658 -15.513454 -14.868151][-13.36509 -13.428841 -11.538983 -10.657772 -11.722862 -12.713152 -12.046646 -10.240633 -10.153435 -8.9329348 -8.7317181 -10.278172 -12.06452 -14.383139 -14.272612][-11.091222 -11.971589 -11.530359 -10.011237 -9.04849 -9.3607922 -9.98568 -9.5019493 -9.4062138 -9.4153891 -10.20849 -11.164819 -11.334412 -12.813721 -13.752253]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-47500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-47500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 19:03:39.018946: step 47510, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 51h:48m:18s remains)
INFO - root - 2017-12-15 19:03:45.615367: step 47520, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 51h:25m:29s remains)
INFO - root - 2017-12-15 19:03:52.343678: step 47530, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 52h:18m:53s remains)
INFO - root - 2017-12-15 19:03:59.022160: step 47540, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 51h:46m:57s remains)
INFO - root - 2017-12-15 19:04:05.536499: step 47550, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 51h:31m:11s remains)
INFO - root - 2017-12-15 19:04:12.074335: step 47560, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 51h:35m:12s remains)
INFO - root - 2017-12-15 19:04:18.644782: step 47570, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 51h:51m:23s remains)
INFO - root - 2017-12-15 19:04:25.206811: step 47580, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.634 sec/batch; 50h:12m:22s remains)
INFO - root - 2017-12-15 19:04:31.733814: step 47590, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 52h:55m:03s remains)
INFO - root - 2017-12-15 19:04:38.309852: step 47600, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 50h:54m:29s remains)
2017-12-15 19:04:38.854388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0646329 -4.2003293 -4.5124135 -5.4731603 -6.6326752 -6.6941915 -6.5456424 -7.1246996 -7.5525613 -8.2200279 -7.9508228 -8.8462238 -8.5923872 -10.005203 -8.503686][-3.7052679 -3.7339876 -4.9013453 -6.1001511 -7.1246157 -7.2036843 -6.9508243 -7.2436609 -7.9131007 -8.8558044 -8.577117 -9.7675247 -9.3425674 -11.336202 -9.0681925][-4.8790016 -4.4459238 -4.9934359 -6.9400287 -7.7681441 -7.2554283 -6.7944016 -6.7001505 -7.00602 -6.7860956 -6.5038176 -8.450964 -8.3655186 -10.821846 -9.53139][-5.3309679 -6.6476316 -8.8871641 -9.9556131 -10.412591 -9.7071095 -8.0012255 -7.315752 -7.6372519 -7.1971188 -6.6731858 -8.4316683 -8.8868361 -10.615345 -7.8739891][-7.4971261 -9.1741028 -11.493455 -10.921799 -9.154911 -5.3020682 -2.3625839 -3.7779093 -5.5573454 -4.9322 -5.4776855 -7.3441238 -8.4825611 -11.390617 -10.328356][-6.5648632 -8.7274971 -11.491623 -10.702795 -9.2130013 -2.8326449 3.2519841 2.4555335 -0.21145105 -2.7298152 -4.6908588 -6.7873673 -8.3538542 -11.371933 -10.391844][-6.346015 -8.2323294 -10.555159 -8.5556927 -5.1084332 1.7103214 8.78474 10.236387 8.1578007 1.0452757 -4.8835464 -6.352385 -7.1514583 -9.4894981 -9.1506138][-8.65711 -9.02103 -8.9738665 -7.1542072 -4.4816928 1.3922944 6.8643689 10.161726 10.848263 4.6250215 -1.7048159 -6.5321717 -9.39229 -10.8217 -8.6164885][-11.942215 -10.97336 -8.6453266 -5.5702968 -2.5837526 0.6819849 3.2663798 5.9055896 6.3740344 1.7591629 -3.2263284 -7.4995337 -9.9152374 -12.009317 -9.8786306][-15.42609 -17.497822 -15.855509 -9.9892426 -3.3840029 1.0522456 1.7873926 1.4290514 0.96283436 -1.5161266 -4.2943072 -9.3251266 -13.306377 -16.808308 -14.679535][-18.935963 -17.609512 -15.674112 -13.54949 -10.763695 -6.0145721 -3.1305258 -1.6443796 -2.746433 -5.656054 -7.8446441 -10.967012 -12.348464 -14.088064 -12.284773][-21.216492 -19.701517 -17.1276 -13.525261 -11.866166 -9.4078608 -7.33933 -6.0024452 -6.5554557 -6.68222 -7.3474922 -9.4896307 -9.9692307 -11.44248 -10.020852][-18.334421 -15.935139 -13.378609 -10.381542 -9.4400253 -8.66357 -7.157712 -5.8062987 -4.9484229 -5.7147255 -7.1683159 -9.64241 -9.4740515 -7.7990775 -4.4746184][-10.231358 -10.620764 -10.489868 -9.34577 -8.7577782 -8.1004944 -7.5670738 -6.6880107 -5.8050923 -4.9346704 -5.197649 -5.7169142 -5.8560696 -6.8543591 -5.7908692][-7.1019487 -5.7897019 -5.3004827 -6.0163779 -6.2952871 -5.6439342 -4.6495008 -4.8192453 -4.8117828 -5.0524974 -4.5843611 -6.440815 -8.125083 -7.6074681 -6.0874753]]...]
INFO - root - 2017-12-15 19:04:45.414013: step 47610, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 52h:20m:17s remains)
INFO - root - 2017-12-15 19:04:51.961372: step 47620, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 52h:18m:45s remains)
INFO - root - 2017-12-15 19:04:58.586388: step 47630, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 51h:21m:44s remains)
INFO - root - 2017-12-15 19:05:05.229252: step 47640, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 53h:18m:31s remains)
INFO - root - 2017-12-15 19:05:11.922794: step 47650, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 52h:34m:49s remains)
INFO - root - 2017-12-15 19:05:18.540054: step 47660, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 51h:40m:47s remains)
INFO - root - 2017-12-15 19:05:25.172685: step 47670, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 52h:30m:00s remains)
INFO - root - 2017-12-15 19:05:31.714969: step 47680, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.669 sec/batch; 52h:57m:54s remains)
INFO - root - 2017-12-15 19:05:38.327680: step 47690, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 52h:45m:39s remains)
INFO - root - 2017-12-15 19:05:44.943351: step 47700, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 53h:15m:56s remains)
2017-12-15 19:05:45.472282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.82177 -7.5936327 -8.5945969 -9.2520428 -10.056314 -10.576637 -10.329199 -8.8339663 -8.1723022 -7.9229 -7.3411074 -8.9136972 -10.453526 -10.280408 -9.8128719][-5.6867375 -6.0607038 -6.4315677 -7.3906016 -7.9415617 -8.4255409 -7.93764 -7.882966 -8.0184307 -7.30614 -6.6220336 -8.861001 -10.997667 -10.91815 -11.448711][-4.4767785 -6.1374173 -7.2621193 -6.7301145 -7.2558088 -7.77812 -7.6837983 -6.9693937 -6.0500321 -5.7963367 -5.6789346 -8.268816 -10.533657 -10.691755 -11.884955][-5.904284 -5.9676857 -6.1526055 -5.6188955 -5.6255221 -5.4536753 -4.9887567 -5.4061618 -5.6656847 -4.7493911 -4.454423 -7.9426756 -10.741512 -11.311096 -11.929523][-7.8833222 -8.4490967 -9.0501156 -7.3159943 -4.6370263 -2.4335511 -0.7738719 -2.8661771 -5.8844252 -5.4689412 -4.6594915 -7.8058438 -10.142063 -10.711049 -11.729806][-8.7539291 -9.0856781 -8.87112 -6.1020393 -2.201726 1.4107046 4.454062 3.0947156 -0.73919821 -3.5200016 -5.6424394 -7.6140285 -8.8151989 -9.3090286 -9.8749657][-10.546175 -10.007233 -7.5184007 -2.325635 1.2907977 3.4188256 6.5451188 6.5030866 4.0557446 -0.42017841 -5.0694418 -8.5926065 -10.684898 -10.094728 -9.8644066][-10.431385 -9.6772985 -7.4007177 -1.7850335 1.9288626 6.131537 8.9958382 6.3122363 4.5069137 1.8799 -2.3672998 -7.7669086 -11.130053 -11.01715 -11.402821][-7.0508237 -6.7969327 -6.0013008 -2.9817874 -0.025977135 4.4906774 6.8999629 5.2751021 5.1519513 1.3480115 -2.771127 -7.6171026 -11.84074 -12.586622 -12.406691][-5.8918414 -5.2189145 -5.4796114 -3.1188993 -1.2652326 -0.18098927 1.0068645 2.2363305 2.6811223 -0.43910217 -2.4302542 -7.3906336 -12.596792 -12.294869 -12.314739][-9.6674347 -9.87611 -10.893847 -8.63069 -6.9386964 -6.3365512 -5.2063355 -4.7533197 -4.1189804 -4.094717 -4.8959632 -10.616213 -13.194675 -12.486882 -12.337675][-13.746634 -12.897568 -13.045954 -12.850712 -12.67514 -10.289942 -8.6770573 -9.4788055 -9.14285 -8.6538744 -8.7429323 -11.739705 -12.718557 -12.979849 -12.43553][-14.402271 -13.943851 -13.454582 -14.40333 -14.384512 -12.739736 -11.831617 -11.298031 -11.127275 -10.828896 -10.163527 -11.906994 -12.037767 -11.498425 -10.193576][-13.885641 -13.034792 -11.452806 -9.4754915 -9.5947752 -11.040419 -11.630976 -10.050808 -9.6810074 -10.390102 -10.521607 -11.046695 -10.772491 -10.166353 -8.43853][-9.7468071 -9.0735989 -7.9427443 -6.2200069 -5.3080516 -5.2700734 -5.9577932 -7.316812 -8.722887 -7.9134336 -8.0715942 -10.462542 -11.321012 -10.995964 -10.078762]]...]
INFO - root - 2017-12-15 19:05:51.970286: step 47710, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 52h:18m:10s remains)
INFO - root - 2017-12-15 19:05:58.555359: step 47720, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 52h:07m:10s remains)
INFO - root - 2017-12-15 19:06:05.261948: step 47730, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 51h:55m:54s remains)
INFO - root - 2017-12-15 19:06:11.855256: step 47740, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 51h:36m:54s remains)
INFO - root - 2017-12-15 19:06:18.478058: step 47750, loss = 0.18, batch loss = 0.14 (11.5 examples/sec; 0.695 sec/batch; 55h:00m:01s remains)
INFO - root - 2017-12-15 19:06:25.161453: step 47760, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.653 sec/batch; 51h:40m:08s remains)
INFO - root - 2017-12-15 19:06:31.766202: step 47770, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 52h:36m:08s remains)
INFO - root - 2017-12-15 19:06:38.427481: step 47780, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.693 sec/batch; 54h:49m:33s remains)
INFO - root - 2017-12-15 19:06:45.017413: step 47790, loss = 0.22, batch loss = 0.18 (11.9 examples/sec; 0.672 sec/batch; 53h:09m:38s remains)
INFO - root - 2017-12-15 19:06:51.585446: step 47800, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 50h:35m:47s remains)
2017-12-15 19:06:52.077294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5295486 -4.8786144 -3.6260738 -2.0779805 -2.2718074 -3.0757396 -3.4315696 -4.2408075 -4.8991284 -5.2718062 -5.5641031 -8.21483 -10.839578 -11.067543 -10.779703][-4.1969004 -3.7928281 -3.0051525 -1.9607046 -2.7024808 -3.515074 -4.46232 -5.0118332 -5.5802264 -5.8706307 -5.2675333 -8.0177555 -11.197909 -12.229897 -12.451017][-3.445966 -4.1552477 -3.2696512 -1.8115633 -1.9092922 -2.9375269 -3.5747352 -3.9177847 -4.2792997 -4.1379981 -4.0754089 -7.1022253 -9.8780985 -10.894136 -10.606197][-3.5662537 -2.9983776 -1.8473642 -0.81913757 -1.5828562 -1.7048779 -1.9011326 -2.2136552 -2.084559 -1.7897937 -2.1024582 -5.4049048 -8.2415657 -8.6436758 -8.7978764][-4.5429864 -4.2630558 -3.9228215 -1.7828715 -1.0132952 -0.54290533 -0.67630768 -0.76113176 -0.56664515 -0.05292654 0.0021896362 -3.0294809 -6.881319 -7.84614 -8.3202553][-6.1483855 -5.5724845 -4.0779719 -1.9140024 -0.52543116 0.42467976 0.92813015 0.6269784 0.88360882 1.1463342 1.0396047 -2.1703405 -5.6223607 -6.9702849 -8.2660379][-6.3965263 -6.1045151 -4.8446221 -2.0278172 -0.31593513 1.5133371 2.7424331 3.1403413 3.5209565 2.949245 2.3268638 -1.078856 -4.311657 -5.606081 -6.6051869][-5.0581579 -4.6885786 -3.5165322 -0.82891655 0.49732637 2.515924 3.6559606 4.1261954 4.237308 3.8005795 3.1534648 -0.033479691 -3.2998588 -4.1587996 -4.50568][-5.3840351 -4.2724113 -3.154741 -0.8980093 0.38884974 2.0911527 3.1784921 3.5379882 3.203527 2.9652066 2.5749679 -0.47001314 -3.8922997 -3.9172144 -4.3993497][-4.415792 -3.5614944 -1.9614022 -0.87688637 -1.1980448 0.53168583 1.6815972 1.6125979 1.6664867 1.0684867 0.34793806 -2.0079386 -3.895287 -4.3155613 -4.8859358][-7.4529595 -6.7178025 -4.7996392 -3.4379208 -3.54168 -3.2118731 -2.613847 -3.0314789 -3.1991527 -3.1861286 -3.2111397 -6.0869083 -7.3376293 -6.7163277 -5.9887066][-9.5998611 -8.5699635 -7.4932137 -6.8983679 -6.4325757 -5.788486 -5.532866 -5.9653578 -6.0264544 -6.3370109 -6.9833679 -7.6732368 -8.1145954 -7.583838 -6.8720465][-10.661457 -9.4003859 -8.45656 -7.6669016 -7.7805033 -6.99103 -6.1531005 -6.0344768 -6.7671371 -7.4052992 -7.1425056 -7.1930647 -7.4276714 -6.3325372 -5.1527276][-9.655076 -9.1639833 -8.2965755 -7.0121117 -6.4933734 -6.3148623 -6.0944285 -5.6867208 -5.6774712 -6.0181818 -6.0604997 -4.595232 -4.3076324 -3.9335432 -4.024703][-7.2812109 -7.5674882 -7.1585546 -6.4482322 -5.5354671 -4.4458771 -3.8855476 -3.7146237 -3.9391785 -3.7924585 -3.4525166 -4.2464576 -4.45235 -4.2334633 -4.5674181]]...]
INFO - root - 2017-12-15 19:06:58.743263: step 47810, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 51h:55m:09s remains)
INFO - root - 2017-12-15 19:07:05.296525: step 47820, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 50h:44m:04s remains)
INFO - root - 2017-12-15 19:07:11.939190: step 47830, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 51h:17m:26s remains)
INFO - root - 2017-12-15 19:07:18.487076: step 47840, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 52h:36m:53s remains)
INFO - root - 2017-12-15 19:07:25.132654: step 47850, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 51h:20m:59s remains)
INFO - root - 2017-12-15 19:07:31.719784: step 47860, loss = 0.16, batch loss = 0.12 (12.7 examples/sec; 0.631 sec/batch; 49h:53m:59s remains)
INFO - root - 2017-12-15 19:07:38.322057: step 47870, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.674 sec/batch; 53h:15m:00s remains)
INFO - root - 2017-12-15 19:07:44.873301: step 47880, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 51h:52m:01s remains)
INFO - root - 2017-12-15 19:07:51.461958: step 47890, loss = 0.17, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 49h:48m:41s remains)
INFO - root - 2017-12-15 19:07:58.015512: step 47900, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 52h:02m:11s remains)
2017-12-15 19:07:58.492445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9543238 -6.1440639 -6.421855 -4.9268332 -4.0967693 -4.0291367 -4.1746712 -4.5883422 -4.980093 -5.1722422 -4.6747923 -5.7500043 -7.1676197 -6.67406 -5.2103796][-7.3411064 -6.4483 -4.6832294 -3.7206943 -3.8010459 -3.5832865 -3.1167502 -2.83771 -2.9046669 -3.2071567 -3.4686928 -4.4559803 -5.0522265 -5.9951854 -5.1051922][-6.0063715 -6.4172659 -6.1757574 -4.0146322 -3.3005261 -2.7524529 -2.7195768 -2.8721964 -2.8450756 -3.0649965 -3.3569219 -5.1844015 -6.1869507 -5.6965914 -5.0847292][-4.2056241 -4.5079718 -4.7113843 -4.2774582 -3.9613159 -3.2665815 -3.0356421 -2.8938265 -2.42455 -2.7834792 -3.1757922 -4.8833885 -6.0263171 -6.13853 -5.1744351][-3.7597146 -4.6558547 -4.3658543 -3.1097858 -2.2444599 -1.5688815 -1.4025378 -2.0840302 -2.225539 -2.2779486 -2.9535422 -4.6165876 -5.3298755 -5.0749383 -4.3850641][-6.3630328 -4.92626 -3.7219727 -2.7270427 -0.92411089 0.057283878 0.14035225 0.28432322 -0.0251503 -1.1816311 -1.8421049 -3.3457978 -4.1266379 -4.1130419 -2.2507737][-5.0962434 -4.9700713 -3.6815286 -1.7917042 0.47546339 2.3216319 3.1804433 2.7907748 1.6069031 0.38953543 0.28398514 -1.7225602 -3.4469602 -3.5899901 -2.1110647][-4.0285854 -3.0556903 -1.3213696 0.51055956 1.0217252 2.2117867 3.3460498 3.2794757 2.0850744 1.2178907 0.56017065 -1.0274529 -2.0798445 -2.299314 -0.72883654][-3.0772738 -1.2413211 0.013235092 1.1012435 1.128911 1.5730419 1.8722835 1.1727023 0.36158609 0.2323494 0.16376591 -1.3378043 -2.493273 -2.8624766 -1.6298761][-2.5508857 -1.4468546 -0.41702127 0.13256073 0.26685572 0.26859188 -0.099662781 -1.0501924 -2.0468533 -1.9547756 -1.1689391 -2.9308836 -4.2340918 -3.696521 -2.0600221][-5.1549077 -4.1965346 -2.6077669 -1.4767165 -1.8073547 -2.3543069 -2.2330995 -2.9333901 -3.6422234 -4.3626642 -4.0475249 -5.0729976 -5.7197552 -4.975606 -4.02967][-8.9300385 -8.8227062 -7.3578825 -5.2068763 -4.9060321 -5.1973958 -5.1958308 -5.0901213 -5.4054918 -5.6176734 -6.1455822 -7.0167475 -7.1653309 -6.78153 -4.8999367][-10.587842 -9.5638 -7.7930679 -6.9323359 -6.4043846 -6.2549963 -7.0080147 -6.9636278 -6.7143655 -6.7631817 -6.9404736 -7.8252888 -7.9484878 -6.4786286 -4.7915697][-7.2681179 -7.204587 -6.866292 -5.47445 -4.7629218 -4.4712272 -4.9456935 -5.6364007 -6.4267764 -6.3746433 -6.3611722 -6.5450339 -6.6771789 -5.9502397 -5.26611][-4.2128868 -3.4755173 -3.3657188 -3.4806111 -3.0630813 -3.3871205 -3.1682773 -3.577219 -4.3673215 -4.9731755 -5.48399 -5.2093406 -5.0210361 -5.6123047 -5.72166]]...]
INFO - root - 2017-12-15 19:08:05.174333: step 47910, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 53h:30m:33s remains)
INFO - root - 2017-12-15 19:08:11.815526: step 47920, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 52h:17m:54s remains)
INFO - root - 2017-12-15 19:08:18.444561: step 47930, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 53h:41m:20s remains)
INFO - root - 2017-12-15 19:08:25.098955: step 47940, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.681 sec/batch; 53h:48m:44s remains)
INFO - root - 2017-12-15 19:08:31.649137: step 47950, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 52h:32m:09s remains)
INFO - root - 2017-12-15 19:08:38.204013: step 47960, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 52h:27m:13s remains)
INFO - root - 2017-12-15 19:08:44.818183: step 47970, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 51h:27m:07s remains)
INFO - root - 2017-12-15 19:08:51.461914: step 47980, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 54h:04m:43s remains)
INFO - root - 2017-12-15 19:08:58.002037: step 47990, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 50h:51m:05s remains)
INFO - root - 2017-12-15 19:09:04.569227: step 48000, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 51h:02m:18s remains)
2017-12-15 19:09:05.122365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6229739 -3.5490272 -4.4692631 -5.3130965 -6.3600245 -7.1692591 -7.2943454 -6.777894 -6.2651634 -6.3135128 -6.5193911 -10.178584 -12.939877 -13.128912 -12.148741][-3.8455782 -4.6127839 -5.2788987 -7.0259533 -8.3146925 -9.8087568 -10.839728 -10.742822 -10.580751 -9.7472839 -9.0432224 -12.01465 -13.589265 -14.311489 -13.569553][-4.0408616 -5.0703564 -6.8294992 -7.6673007 -8.3010244 -8.8975182 -9.0225163 -9.6359291 -9.8294888 -10.49107 -10.614084 -13.78915 -15.456886 -15.264738 -13.721523][-5.7534385 -6.6750727 -7.4227324 -8.018364 -9.3481092 -9.2315083 -7.5779257 -7.0012369 -7.7065887 -8.8382339 -9.819581 -13.471989 -15.21365 -15.412594 -14.430664][-6.3590736 -8.0761766 -9.5544357 -9.5474157 -8.8384266 -7.2738347 -4.5155272 -3.7205405 -3.5158241 -4.9646206 -7.215044 -11.546621 -14.275377 -14.625277 -13.506176][-9.100955 -10.354495 -9.5821295 -7.8304429 -6.0797105 -3.0514164 0.79505682 1.0302324 0.72893095 -1.0970993 -3.6332202 -7.9604864 -11.08135 -11.682991 -10.819416][-9.7551193 -10.855906 -10.674606 -6.4019432 -1.8351765 2.5906539 7.4603515 6.07884 4.5717788 0.32653236 -2.8907094 -6.1312227 -8.5659132 -9.8887405 -9.1733551][-10.157612 -10.998076 -10.64033 -6.180635 -1.236434 5.7096438 11.25782 10.668558 9.5481186 3.6702027 -1.1477623 -6.2508569 -9.6731 -8.766943 -7.0520654][-8.4163132 -9.91463 -9.7534866 -6.5475731 -3.8493831 1.3029857 6.2226777 7.7922187 8.36515 3.7246718 -0.13920784 -5.7318721 -10.156155 -9.77142 -8.1365976][-8.1384621 -9.0221586 -8.85483 -7.1428123 -6.1023355 -2.1278164 1.2025003 1.2307596 1.4290686 -1.5271034 -3.5759578 -9.1501141 -12.330234 -12.317453 -10.104671][-11.10519 -12.088022 -11.388309 -10.843191 -10.032129 -8.0075445 -7.0201793 -6.2106156 -6.6609049 -8.4508734 -8.8581133 -12.995302 -15.162556 -13.853096 -11.251312][-16.1727 -16.453423 -15.608015 -14.781807 -12.616523 -10.238127 -9.43967 -9.8433952 -10.790485 -12.129251 -13.420916 -15.476658 -15.796715 -15.36014 -12.222126][-19.256783 -19.050461 -18.35318 -17.369303 -14.706732 -11.928427 -9.9594736 -10.447875 -11.88365 -12.800217 -13.697973 -15.296143 -15.115107 -13.524197 -11.542022][-18.736233 -17.608015 -15.892454 -14.689739 -12.586607 -11.254451 -9.8742151 -9.9366741 -11.406713 -12.350346 -13.353757 -12.829956 -12.286125 -10.918407 -10.047387][-10.631821 -10.754926 -9.8123245 -8.7840462 -7.4651594 -6.4587946 -5.6550527 -6.69097 -8.1073332 -9.9562559 -11.058107 -11.661104 -12.136494 -12.378478 -12.58321]]...]
INFO - root - 2017-12-15 19:09:11.767119: step 48010, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 52h:50m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 19:09:18.317896: step 48020, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 51h:17m:36s remains)
INFO - root - 2017-12-15 19:09:24.957694: step 48030, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 52h:51m:55s remains)
INFO - root - 2017-12-15 19:09:31.585333: step 48040, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 50h:32m:46s remains)
INFO - root - 2017-12-15 19:09:38.245947: step 48050, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 52h:14m:53s remains)
INFO - root - 2017-12-15 19:09:44.838319: step 48060, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 53h:27m:14s remains)
INFO - root - 2017-12-15 19:09:51.333060: step 48070, loss = 0.23, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 51h:40m:12s remains)
INFO - root - 2017-12-15 19:09:57.859537: step 48080, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.637 sec/batch; 50h:18m:03s remains)
INFO - root - 2017-12-15 19:10:04.514897: step 48090, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 53h:18m:21s remains)
INFO - root - 2017-12-15 19:10:11.085160: step 48100, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 53h:03m:50s remains)
2017-12-15 19:10:11.600911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3782759 -7.6680651 -6.4317522 -6.103251 -6.1859155 -5.5390821 -5.3128476 -4.8441257 -4.1320467 -3.0827782 -2.5266039 -4.1412129 -5.709496 -4.8040752 -5.3566508][-7.530467 -7.8711562 -6.6313119 -5.9741383 -6.0453053 -5.6803455 -5.6104569 -5.7063394 -5.6006131 -4.9480739 -4.2283764 -5.9488435 -7.4329834 -6.078557 -5.84713][-5.59159 -6.3032522 -6.3338737 -5.1075621 -4.6240435 -5.0917969 -5.183485 -4.6891785 -4.2583685 -4.6533065 -4.7519579 -6.6664391 -8.2527571 -7.1571941 -6.8723807][-5.9815621 -5.889514 -5.0157614 -4.144578 -3.9449615 -2.9505353 -2.5017157 -3.3635921 -4.0491648 -3.4424064 -2.6192455 -5.0545521 -7.0677605 -6.7038546 -6.9328613][-6.7712874 -7.6403604 -7.6609468 -4.9847355 -3.2178891 -1.5856347 -1.2568932 -2.807322 -4.2188339 -3.8506064 -3.236572 -4.883215 -6.4724693 -6.1485963 -6.5361595][-8.2500668 -8.1027069 -6.45043 -3.40972 -1.6619406 1.0147672 2.582377 1.5122075 -0.23859453 -2.0075746 -3.4566624 -4.8288512 -6.0381975 -5.3422494 -4.8724332][-10.238087 -10.103028 -7.000607 -2.485393 -0.019673347 3.4115977 6.080997 5.7096753 4.1332107 0.6378541 -2.4113553 -4.5541921 -6.29655 -5.5383487 -4.9999824][-12.528572 -11.150738 -7.1175265 -2.586549 0.49308062 3.7766891 6.0877404 6.1366735 5.1974587 1.9243627 -0.94779396 -5.1850462 -8.5591669 -7.3770204 -6.0045428][-10.446084 -8.4443378 -5.94256 -2.1859481 1.0526819 3.3500428 3.8276772 3.9803138 3.3596673 1.1954808 -0.61718559 -5.8536463 -9.7717085 -8.635047 -8.0237312][-8.068099 -6.8075848 -5.2532849 -1.7273145 1.0064111 1.8058968 2.2569032 2.1919723 0.86328554 -0.20296764 -1.3953815 -5.8438029 -9.19733 -9.2495956 -10.373789][-9.75754 -8.49702 -6.6427407 -4.1434174 -2.3859706 -1.0839791 -0.080469131 -1.0740457 -2.7758632 -3.9312143 -5.0372133 -8.34749 -9.7012186 -10.361177 -11.43505][-13.862116 -12.464746 -10.650288 -8.4649611 -7.7652717 -7.00256 -6.4736986 -6.7875538 -7.2924886 -7.7019281 -7.8679771 -10.019489 -11.085827 -11.028162 -10.770889][-15.291233 -13.657328 -12.146292 -10.572981 -10.075714 -9.610857 -9.5830927 -9.1762142 -9.2003546 -9.2630424 -9.1226635 -9.4365654 -9.97393 -8.89239 -8.389595][-12.539923 -11.311876 -9.9505157 -8.6072721 -8.1008873 -7.660789 -7.2510419 -7.4262915 -7.3161545 -7.0057268 -6.8174076 -6.6103616 -7.495337 -6.6867356 -6.5499206][-8.9522476 -9.1015062 -7.9202118 -7.0499258 -6.4770837 -4.6255522 -3.6719196 -3.3430355 -4.1122632 -4.5611334 -4.7435837 -5.9179134 -6.6333218 -6.4410558 -6.4255605]]...]
INFO - root - 2017-12-15 19:10:18.120044: step 48110, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 50h:23m:06s remains)
INFO - root - 2017-12-15 19:10:24.667896: step 48120, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 52h:34m:14s remains)
INFO - root - 2017-12-15 19:10:31.188803: step 48130, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.649 sec/batch; 51h:15m:31s remains)
INFO - root - 2017-12-15 19:10:37.797092: step 48140, loss = 0.28, batch loss = 0.24 (11.9 examples/sec; 0.670 sec/batch; 52h:53m:41s remains)
INFO - root - 2017-12-15 19:10:44.423780: step 48150, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 51h:39m:21s remains)
INFO - root - 2017-12-15 19:10:51.064308: step 48160, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 53h:11m:25s remains)
INFO - root - 2017-12-15 19:10:57.719317: step 48170, loss = 0.16, batch loss = 0.11 (11.3 examples/sec; 0.706 sec/batch; 55h:47m:10s remains)
INFO - root - 2017-12-15 19:11:04.367498: step 48180, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 51h:37m:55s remains)
INFO - root - 2017-12-15 19:11:10.981346: step 48190, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 53h:38m:31s remains)
INFO - root - 2017-12-15 19:11:17.514348: step 48200, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 52h:33m:38s remains)
2017-12-15 19:11:18.062030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6901 -3.4686666 -2.0823367 -2.5415304 -4.1790261 -5.3803353 -6.1542273 -7.528378 -8.6909609 -8.574954 -8.2282391 -8.2840605 -9.7508736 -11.097734 -10.724549][-3.8450077 -4.0345426 -3.8244944 -3.588325 -4.3098984 -5.6319818 -5.9998198 -6.9417353 -8.0362759 -8.1887836 -7.6558061 -7.7811613 -9.6574755 -11.236089 -10.774569][-2.8153026 -2.8795528 -3.3552837 -3.9293292 -5.1540303 -5.6905208 -5.9382582 -6.7821145 -7.1615076 -7.0058489 -6.5796366 -6.8161297 -8.4359827 -10.766886 -10.865][-2.8330052 -3.0159237 -2.6116745 -2.5853627 -4.1704082 -4.7532587 -4.3261313 -4.699192 -5.5257139 -5.3913612 -5.4432063 -7.1912613 -10.248646 -10.900766 -9.9323044][-5.8669405 -5.4087305 -4.5930877 -3.7053914 -3.9184251 -3.1126943 -2.3566225 -2.3040519 -2.9239061 -3.3961825 -4.8297787 -6.2463846 -9.219532 -11.674406 -11.302025][-7.4703932 -7.2549014 -5.5952053 -3.4768953 -3.1678739 -2.1860452 -0.16772127 1.1668196 1.1146245 -0.44682407 -2.8858354 -4.77536 -8.6873655 -10.636721 -10.099314][-7.8097076 -8.1007357 -6.6990967 -3.9059033 -1.9089103 -0.44917536 1.2143559 2.6638322 4.0535502 3.1568885 0.6143117 -1.8279424 -5.7393169 -8.2107029 -8.8471117][-6.5841575 -7.3660636 -6.226676 -3.358403 -1.3917904 0.63300419 2.910851 3.8085752 4.8991418 4.7836595 2.8450055 -0.33287811 -4.6246891 -7.0030403 -7.0208983][-5.6503115 -6.2635517 -4.3438511 -2.1045055 -0.34156513 1.8236656 3.1551118 4.0741668 5.0091252 4.3363128 2.887589 0.36054134 -3.5357094 -5.8385077 -5.72247][-6.395659 -6.511982 -4.6119814 -1.9016685 -0.6916275 0.0045461655 1.0788236 1.2973986 1.1968217 -0.41647625 -1.4101195 -3.0912869 -6.5120139 -8.8788071 -8.2964373][-10.739595 -9.8796463 -7.7733288 -5.4497519 -4.840898 -4.8343225 -5.0150909 -4.7271094 -4.1119242 -4.3940992 -5.8278461 -8.8691387 -11.258907 -11.261613 -9.0672884][-14.625875 -14.049892 -11.317791 -8.4491634 -7.6242294 -8.0870056 -8.720994 -8.9592705 -8.8511839 -8.8195629 -9.2499428 -10.336117 -12.293555 -12.946218 -10.533344][-14.137255 -14.464846 -12.627942 -11.123928 -10.29228 -9.2106209 -8.8403454 -8.8589859 -8.1154623 -7.5927587 -7.5979052 -8.9136686 -9.7561388 -9.8505363 -8.8109274][-12.232056 -13.088416 -12.455133 -10.710335 -9.3453808 -9.0617552 -8.3478794 -6.678751 -5.782618 -6.2833529 -6.9077387 -6.0948339 -6.8007708 -7.4094858 -7.2348137][-7.1867065 -8.0990963 -8.9529037 -8.251689 -7.4267273 -7.0274754 -6.0989447 -5.1144691 -3.9874487 -3.0298533 -3.0535355 -4.0516367 -5.0171576 -4.9891348 -5.9468017]]...]
INFO - root - 2017-12-15 19:11:24.638041: step 48210, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 51h:27m:33s remains)
INFO - root - 2017-12-15 19:11:31.206316: step 48220, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 50h:15m:55s remains)
INFO - root - 2017-12-15 19:11:37.878163: step 48230, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 52h:36m:55s remains)
INFO - root - 2017-12-15 19:11:44.445306: step 48240, loss = 0.18, batch loss = 0.13 (11.5 examples/sec; 0.696 sec/batch; 54h:58m:17s remains)
INFO - root - 2017-12-15 19:11:51.030186: step 48250, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 52h:25m:24s remains)
INFO - root - 2017-12-15 19:11:57.702988: step 48260, loss = 0.24, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 51h:54m:42s remains)
INFO - root - 2017-12-15 19:12:04.270707: step 48270, loss = 0.11, batch loss = 0.07 (12.4 examples/sec; 0.643 sec/batch; 50h:46m:49s remains)
INFO - root - 2017-12-15 19:12:10.886365: step 48280, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 51h:42m:33s remains)
INFO - root - 2017-12-15 19:12:17.458325: step 48290, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 51h:06m:17s remains)
INFO - root - 2017-12-15 19:12:24.071368: step 48300, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 53h:33m:22s remains)
2017-12-15 19:12:24.604848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-11.485043 -10.48715 -9.4163342 -8.162406 -8.0066319 -8.0366383 -7.5688396 -6.7830944 -5.4824581 -5.2868409 -5.0558848 -4.4079266 -5.9712172 -7.4894705 -8.4301929][-9.983181 -9.6698952 -9.3075914 -8.1805573 -7.7889528 -7.6711607 -7.2554865 -6.5811834 -5.3902593 -4.6846 -4.2335186 -4.5011773 -5.9223118 -7.3769455 -8.1986895][-6.4615321 -7.7876806 -8.51532 -7.4334083 -7.9026175 -7.7703056 -7.3994327 -6.779027 -6.1170025 -5.2359772 -4.0289488 -4.9812303 -5.9240427 -7.4206676 -8.2147121][-3.810374 -5.2991095 -6.2425957 -6.0581913 -7.14567 -6.6990566 -5.8374376 -5.6892381 -5.3416843 -4.2524848 -3.4193435 -4.6569481 -6.3770757 -8.4988909 -8.2615089][-4.2708626 -5.5422764 -6.4199815 -5.2185755 -5.4153 -5.0159159 -4.7059541 -4.0972776 -3.4480774 -3.0730708 -2.8408465 -3.8534312 -5.9412346 -8.7187691 -9.71607][-5.1312122 -5.0311675 -4.7761025 -3.7323947 -3.0071692 -2.3998158 -2.3698487 -2.0138857 -1.389286 -1.201735 -1.559072 -3.0045388 -4.8165708 -7.5136328 -8.8023739][-4.7230511 -3.6474128 -3.2643335 -1.8672693 -1.0143404 -0.27933216 0.15734863 0.38086462 0.72308826 0.29319382 -0.80130625 -2.3446434 -4.6393561 -6.6660795 -6.7220263][-4.5304852 -2.9515755 -2.1604266 -1.1485734 -0.91012192 -0.36935568 0.81873751 1.5213861 1.8681145 1.4546695 0.45661926 -2.0159905 -5.1575966 -7.2984581 -7.2819357][-4.5101314 -3.2054303 -1.9629216 -0.70203018 -0.64334106 -0.037361145 0.36226225 0.94252014 0.73698282 0.012872696 -0.47828054 -2.0056956 -4.7387972 -8.1268806 -9.1475391][-2.50791 -2.5376484 -1.7529461 -0.37184 -0.71330833 -1.7079244 -1.6256189 -0.81282663 -0.52297497 -0.56114054 -1.0586209 -2.8592522 -4.5796261 -7.832623 -9.7854328][-4.3460374 -3.294143 -3.5021675 -3.5387187 -4.0662713 -4.4302263 -4.4384322 -4.0616732 -3.4266651 -3.0093408 -3.4443944 -5.4230738 -7.1571054 -9.6007042 -10.938093][-8.4125528 -6.37523 -4.9046717 -5.6119757 -7.7563739 -8.9460783 -8.3660154 -7.8006296 -7.0363226 -6.8466825 -7.0241079 -7.5036454 -9.3683758 -10.714878 -10.335028][-10.448395 -8.9886446 -7.3949885 -7.6212935 -9.0844965 -9.3957043 -9.5662022 -8.8474751 -7.4347458 -7.8522692 -8.08125 -8.3670187 -8.5689049 -9.351409 -9.9953041][-10.55496 -9.2636509 -8.1226854 -6.6801767 -6.542933 -8.42096 -10.634361 -9.51812 -8.6333933 -7.846035 -7.2246485 -7.001801 -6.5718846 -7.1186509 -8.3209381][-7.1957388 -8.7129536 -8.7321854 -7.0571671 -5.6546183 -4.7270217 -6.0367279 -6.8404484 -7.3371997 -7.0539236 -6.6540151 -8.1302814 -9.0828285 -8.5223513 -8.3023386]]...]
INFO - root - 2017-12-15 19:12:31.168185: step 48310, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 51h:02m:14s remains)
INFO - root - 2017-12-15 19:12:37.717049: step 48320, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 50h:48m:22s remains)
INFO - root - 2017-12-15 19:12:44.270337: step 48330, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.653 sec/batch; 51h:35m:01s remains)
INFO - root - 2017-12-15 19:12:50.981919: step 48340, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 52h:14m:30s remains)
INFO - root - 2017-12-15 19:12:57.496541: step 48350, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 50h:00m:46s remains)
INFO - root - 2017-12-15 19:13:04.164248: step 48360, loss = 0.14, batch loss = 0.09 (11.5 examples/sec; 0.697 sec/batch; 54h:59m:39s remains)
INFO - root - 2017-12-15 19:13:10.819014: step 48370, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.692 sec/batch; 54h:38m:09s remains)
INFO - root - 2017-12-15 19:13:17.425508: step 48380, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 51h:21m:10s remains)
INFO - root - 2017-12-15 19:13:24.073957: step 48390, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.670 sec/batch; 52h:53m:08s remains)
INFO - root - 2017-12-15 19:13:30.619411: step 48400, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 53h:20m:08s remains)
2017-12-15 19:13:31.161148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1649847 -3.2906258 -2.1907971 -1.5377574 -2.5309725 -3.7473068 -5.1305323 -5.9758692 -5.023376 -3.9142451 -2.9920235 -3.8694468 -6.3611817 -7.6764817 -7.3508286][-3.6941347 -1.9297671 -0.6367712 0.17427683 -0.8708992 -2.570735 -5.1292634 -6.8706241 -6.4985442 -5.3816986 -4.3010912 -5.0627508 -7.8735609 -8.7476139 -8.9119034][-1.7981203 -1.664124 -2.2449787 -1.4114499 -3.1858709 -4.470417 -6.4026246 -7.5006952 -7.6530771 -7.2248487 -6.5461559 -7.938097 -11.043848 -11.386855 -10.854207][-5.5139995 -4.6025023 -4.4469652 -4.2576618 -5.6749072 -6.0991683 -6.6721821 -7.4238272 -7.3223791 -6.8142838 -6.9243197 -8.9756784 -13.022643 -13.532807 -12.95665][-8.0103121 -8.669939 -8.87352 -7.7811527 -6.6514039 -4.76093 -3.9482946 -5.027535 -5.970387 -5.5841889 -5.8311596 -8.9730864 -14.070766 -15.143398 -14.940859][-10.136282 -10.063267 -9.1006575 -6.9007411 -4.9938116 -1.7932303 1.0976791 0.83139467 -0.38651276 -2.1319852 -3.9590137 -6.7557693 -11.579827 -13.78672 -13.892105][-10.494427 -10.314423 -8.7891083 -4.7582097 -1.5748172 1.0070367 4.1395535 5.1122851 4.974566 1.8276596 -1.9198983 -5.142592 -9.5696115 -11.441924 -11.443863][-8.4687166 -8.0011873 -7.438693 -3.3599064 0.79393768 3.8845248 6.2477126 6.4882188 6.2400956 3.9678216 1.2589097 -2.3627014 -7.18188 -8.5808954 -8.142086][-5.4144387 -4.448132 -4.3084702 -2.1607022 0.23238325 3.113544 5.7606988 5.9111772 5.52058 3.332129 1.6457982 -1.8655772 -6.8044906 -7.490828 -6.6346726][-4.2816172 -2.910718 -1.5692062 0.046974182 0.73018837 1.8292427 4.0639777 4.5619407 4.1700206 2.5370116 1.049346 -2.291903 -6.6836152 -7.7100215 -7.1312103][-8.434145 -6.69177 -5.5592489 -4.0472307 -4.0978241 -3.6600413 -2.077805 -1.3325262 -1.3620348 -2.2785535 -3.9723773 -6.8711591 -9.8154774 -10.140268 -8.73825][-12.531857 -10.487828 -8.163372 -6.15548 -6.59748 -6.5011091 -6.8759575 -6.8197012 -6.2104225 -6.35933 -7.1607828 -9.8142052 -12.016739 -11.121414 -9.8127289][-12.551395 -11.414196 -10.280203 -9.3590908 -8.7527332 -8.3395405 -8.3524771 -8.4128923 -9.1473255 -8.3059025 -8.374218 -10.056997 -11.726068 -10.239329 -9.5000238][-10.096663 -9.5246964 -8.4112988 -8.4269495 -8.0515556 -7.9687176 -7.6963611 -7.4043469 -7.0368338 -7.1069422 -7.505435 -8.1963749 -9.3627405 -8.8180857 -8.1600847][-7.9214144 -8.0810423 -7.0595341 -6.3125434 -5.5946083 -6.4736767 -6.3030686 -6.0994043 -6.1132135 -6.2933583 -6.5066962 -7.6249442 -9.1624451 -9.4097977 -9.79413]]...]
INFO - root - 2017-12-15 19:13:37.786942: step 48410, loss = 0.11, batch loss = 0.06 (11.8 examples/sec; 0.678 sec/batch; 53h:29m:43s remains)
INFO - root - 2017-12-15 19:13:44.353941: step 48420, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 50h:39m:09s remains)
INFO - root - 2017-12-15 19:13:50.935571: step 48430, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 51h:10m:10s remains)
INFO - root - 2017-12-15 19:13:57.476620: step 48440, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 51h:18m:32s remains)
INFO - root - 2017-12-15 19:14:04.106432: step 48450, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.696 sec/batch; 54h:53m:07s remains)
INFO - root - 2017-12-15 19:14:10.678180: step 48460, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 52h:14m:56s remains)
INFO - root - 2017-12-15 19:14:17.277055: step 48470, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 52h:43m:46s remains)
INFO - root - 2017-12-15 19:14:23.852058: step 48480, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 51h:59m:19s remains)
INFO - root - 2017-12-15 19:14:30.487387: step 48490, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 51h:36m:51s remains)
INFO - root - 2017-12-15 19:14:37.033472: step 48500, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 52h:40m:49s remains)
2017-12-15 19:14:37.551065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6185818 -5.5144815 -4.4943724 -4.1484375 -4.3435211 -4.0495014 -3.697525 -3.308696 -3.0288432 -2.1593788 -2.5777283 -6.3026309 -8.1555119 -10.356058 -10.523254][-2.5467992 -2.7403803 -4.1817536 -3.8368077 -3.7397976 -3.3775318 -2.8799076 -2.4666574 -3.0772874 -3.013016 -2.6614203 -6.0647454 -7.7725363 -10.159763 -10.879549][-3.4274867 -3.44766 -3.251199 -3.0800471 -4.2488694 -3.7066646 -3.022635 -3.0265341 -3.397675 -2.9525979 -2.7029092 -5.9210911 -7.6730118 -10.079494 -11.24798][-2.9112341 -3.1086168 -4.150197 -3.3703091 -3.0138533 -3.3125634 -3.6283534 -3.0290804 -2.8719597 -2.0708416 -0.88510656 -4.1273913 -6.4816542 -9.0355511 -10.556329][-4.6462522 -3.9837861 -4.0565929 -4.1853108 -4.7864537 -4.0713935 -3.1273818 -2.7867405 -3.0914001 -2.0248237 -0.92046547 -3.9258876 -5.0531607 -8.0681906 -9.4147854][-3.584517 -3.9988725 -4.1968646 -3.4089777 -2.6102655 -1.9958079 -1.1622586 -0.68671274 -0.11302614 0.46235085 0.37422466 -2.5782568 -4.7837763 -8.0790043 -8.6885128][-3.9720516 -2.7898905 -2.7881486 -1.8371942 -0.93470716 -0.083921432 1.0073586 1.5288706 1.8888493 1.2948823 0.40794468 -2.7360418 -4.49922 -7.8677874 -9.11356][-3.6165471 -3.5697913 -2.7226419 -0.82447338 -0.44672632 0.37349558 1.9350171 2.5320115 2.4354882 1.7589355 0.11257553 -3.9127276 -6.026897 -8.763257 -9.7563658][-5.580287 -4.395575 -2.8099256 -1.6687112 -1.2972021 0.54300165 1.8362942 2.6747851 3.1388822 1.6671233 -0.35445976 -4.5867357 -7.5399275 -10.757515 -11.534691][-3.6092446 -3.9154463 -3.5017529 -1.8151314 -0.36559916 1.2428861 2.6077151 3.426476 2.9242806 1.7790847 0.67092419 -4.793045 -8.2755394 -11.898088 -13.570987][-6.4207125 -5.0855932 -4.7813406 -3.856045 -2.8216755 -1.6747432 -0.51763821 -0.49491262 -1.3070154 -2.5087762 -3.7872684 -9.2833195 -12.044175 -14.011021 -12.668146][-10.01828 -9.1677074 -8.365119 -6.9608173 -6.1707749 -5.389605 -5.3328676 -5.5827451 -5.9475517 -7.1673355 -8.4226274 -10.977695 -11.984472 -12.714901 -10.89702][-13.49044 -12.260029 -10.435772 -9.6708241 -9.4091034 -8.1021452 -6.7996354 -7.5708351 -8.7700253 -9.8878765 -10.295513 -11.38353 -11.041464 -11.025286 -9.5132732][-11.810503 -11.932343 -11.333639 -9.2300377 -7.3591089 -6.9770513 -7.8189292 -7.9088182 -7.1393776 -8.3386183 -9.54686 -9.5053711 -8.9343643 -9.5071583 -8.417902][-9.5245371 -8.6337557 -8.0361567 -7.7210064 -7.1219053 -5.8520641 -5.6635046 -6.3029246 -7.7980571 -7.8030453 -7.1275744 -8.6952362 -9.3170309 -8.98456 -8.8224773]]...]
INFO - root - 2017-12-15 19:14:44.058247: step 48510, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 53h:20m:25s remains)
INFO - root - 2017-12-15 19:14:50.628540: step 48520, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 52h:09m:00s remains)
INFO - root - 2017-12-15 19:14:57.194911: step 48530, loss = 0.12, batch loss = 0.08 (12.7 examples/sec; 0.630 sec/batch; 49h:42m:43s remains)
INFO - root - 2017-12-15 19:15:03.835675: step 48540, loss = 0.23, batch loss = 0.18 (12.1 examples/sec; 0.663 sec/batch; 52h:16m:30s remains)
INFO - root - 2017-12-15 19:15:10.398123: step 48550, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 52h:04m:09s remains)
INFO - root - 2017-12-15 19:15:16.915446: step 48560, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 50h:32m:42s remains)
INFO - root - 2017-12-15 19:15:23.449942: step 48570, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 52h:57m:54s remains)
INFO - root - 2017-12-15 19:15:30.125387: step 48580, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.686 sec/batch; 54h:07m:45s remains)
INFO - root - 2017-12-15 19:15:36.728088: step 48590, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 51h:55m:03s remains)
INFO - root - 2017-12-15 19:15:43.277361: step 48600, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 51h:39m:20s remains)
2017-12-15 19:15:43.823762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2181454 -6.2878623 -8.00404 -9.9693041 -12.271338 -11.713511 -9.5701313 -8.490818 -8.2412624 -9.0151548 -10.276672 -12.527656 -15.058428 -18.284496 -16.897097][-4.7841983 -5.3542356 -6.2850785 -7.3997011 -9.7319565 -11.021526 -10.91477 -10.010508 -9.3302574 -8.2993469 -8.4080658 -9.517889 -12.854486 -17.47892 -17.541573][-3.0370462 -4.3242464 -7.2111888 -8.1417637 -10.05258 -10.723424 -10.331017 -9.2661076 -7.5670552 -7.2540264 -8.4355412 -9.758049 -11.406031 -14.222357 -14.664871][-4.2165289 -5.213851 -5.5089159 -5.5919261 -7.32949 -6.6527267 -6.3274608 -7.5344625 -6.9404125 -6.1162658 -5.7888203 -7.1739993 -9.8025265 -13.726921 -12.54555][-4.2926497 -4.6179523 -5.8752732 -5.3266468 -5.3089933 -3.6812766 -1.346045 -1.6216183 -2.3200412 -2.7151487 -3.6291747 -4.4204845 -7.2138147 -13.688624 -15.36124][-4.8049326 -4.3967261 -4.9853635 -3.2536366 -1.5338278 1.3403215 3.9902558 4.4117808 3.4063563 1.5811043 -0.40636587 -1.5868249 -4.5202885 -9.2618732 -13.241371][-4.9876394 -4.0842962 -3.1925397 -2.238795 -0.76384115 2.7147908 6.6278691 8.3189163 8.1080494 4.8797984 1.8666697 1.2822485 -0.72272253 -6.276895 -8.8022594][-5.6857557 -4.0365667 -3.5454679 -1.43087 0.033019066 4.3681464 7.9385352 8.0300522 7.7225976 6.6537623 4.4294772 2.1506381 -2.7182074 -6.9516306 -7.8507357][-4.4182491 -3.4725924 -2.9674892 -1.3014722 -2.0659046 1.0974398 5.0950055 6.8647437 7.7623477 4.9578242 1.8058248 -0.67271376 -5.5496693 -10.341154 -11.822068][-3.6447198 -3.9851394 -4.3414359 -2.8634691 -3.1106086 -0.93877029 1.0897055 2.4824529 1.7036457 1.3591576 0.34580421 -4.0574665 -8.6523628 -14.374435 -17.660748][-8.765728 -8.1053514 -8.6668606 -7.3572636 -6.8501949 -5.8798957 -5.2120905 -4.149797 -3.7497778 -5.1486034 -6.1823478 -9.6460009 -15.127308 -18.878908 -19.342989][-10.264849 -9.9206371 -9.6116781 -9.7560482 -9.5019836 -7.4740844 -7.5115738 -8.64411 -10.098013 -9.8689976 -10.557098 -11.534933 -13.40738 -19.05353 -19.451006][-12.42788 -11.088274 -10.36875 -9.22819 -10.441402 -9.46752 -9.1451817 -9.9219913 -9.5927067 -9.74655 -11.111277 -11.192724 -14.228474 -16.181847 -15.113588][-11.050369 -11.20677 -10.089754 -9.1742573 -9.5529137 -9.7772427 -9.7066755 -7.8439226 -6.6557336 -7.4230576 -8.4852753 -8.44854 -8.1644526 -9.3316326 -9.9867687][-9.2875423 -7.9543214 -8.4759874 -7.940052 -6.2951159 -6.9639225 -8.0285892 -9.4104586 -9.8222666 -9.2477045 -7.9339514 -7.5246468 -8.4056711 -7.3575349 -6.609973]]...]
INFO - root - 2017-12-15 19:15:50.419423: step 48610, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 52h:04m:09s remains)
INFO - root - 2017-12-15 19:15:57.009330: step 48620, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 52h:16m:53s remains)
INFO - root - 2017-12-15 19:16:03.638921: step 48630, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 52h:49m:44s remains)
INFO - root - 2017-12-15 19:16:10.146452: step 48640, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 51h:36m:35s remains)
INFO - root - 2017-12-15 19:16:16.742597: step 48650, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 52h:23m:22s remains)
INFO - root - 2017-12-15 19:16:23.369272: step 48660, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 51h:09m:17s remains)
INFO - root - 2017-12-15 19:16:29.973699: step 48670, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 51h:22m:23s remains)
INFO - root - 2017-12-15 19:16:36.534065: step 48680, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 51h:56m:40s remains)
INFO - root - 2017-12-15 19:16:43.174680: step 48690, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 51h:52m:50s remains)
INFO - root - 2017-12-15 19:16:49.806086: step 48700, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 52h:03m:31s remains)
2017-12-15 19:16:50.338824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.2158461 -9.26286 -9.3614893 -9.3465433 -9.3444633 -9.1595459 -9.3405428 -9.1265965 -8.5196819 -7.5464396 -6.3993187 -4.712163 -5.0574889 -7.0481653 -8.2075062][-8.7047729 -8.55691 -8.0211229 -7.5371084 -8.2467117 -8.4521742 -8.7575426 -8.7118931 -8.6647673 -7.921937 -6.4094214 -4.8448634 -5.5762286 -7.3057494 -8.8325729][-6.6745396 -7.2668433 -7.7491865 -7.3395925 -8.0396624 -8.2455387 -8.39471 -8.1215019 -7.564127 -7.2116776 -6.755394 -5.1803579 -5.097218 -7.2435222 -9.9346142][-7.3029008 -7.1837783 -6.8708096 -6.4334259 -7.2797346 -6.9233313 -6.8693013 -7.0659423 -7.1912212 -5.9634089 -4.7825761 -3.5493333 -4.4846592 -7.4927483 -9.8209572][-8.0766735 -9.1745739 -8.8218241 -7.067729 -6.2142348 -3.4787757 -1.5489678 -3.3583744 -5.9609504 -5.4225507 -4.3855495 -3.1073098 -2.8988869 -5.3471465 -7.4982285][-9.6996832 -9.133256 -7.7418823 -5.4987216 -3.7002504 -0.30146313 3.0505776 2.6932578 1.0023832 -1.6859021 -4.1149583 -3.0672112 -3.0686872 -5.389956 -6.9509826][-11.294481 -10.857867 -8.3934183 -4.2604227 -1.9558671 0.86433315 5.2692847 6.2123637 5.2565503 0.12157106 -4.645453 -4.0546041 -4.4762173 -6.9774547 -8.1134825][-12.53725 -11.004814 -8.0065756 -2.5458217 0.1567359 3.3295255 5.9644456 5.4633145 5.3183017 2.2921605 -1.5395846 -3.9174337 -7.1184559 -9.9026375 -10.510488][-9.7926521 -8.5894012 -7.111383 -3.3695168 -0.38251162 2.7950835 3.9675317 4.0044284 3.9662881 1.2067142 -1.7455862 -4.490304 -7.9663935 -10.713539 -12.834118][-7.0248013 -6.4908004 -5.6522946 -3.4667494 -1.4399228 0.75505114 1.6975865 1.2844563 0.5709095 -0.73593712 -2.3527417 -4.9794168 -8.3575554 -12.151833 -15.705502][-11.015115 -9.5882473 -9.2477188 -7.2457285 -5.9070191 -5.212676 -4.4886689 -4.4689369 -5.3948517 -6.4813261 -7.5706716 -8.927887 -10.557949 -13.009796 -14.865093][-15.378227 -13.500145 -12.501535 -11.559967 -11.310593 -10.198348 -9.75856 -10.600731 -10.986031 -10.535057 -10.74021 -11.545959 -12.471755 -13.891796 -14.00588][-15.806364 -15.337889 -13.221174 -11.750487 -11.757441 -11.674719 -11.418644 -11.431211 -11.819456 -11.703247 -11.21636 -10.36141 -10.790871 -11.512976 -11.434999][-13.679581 -13.670234 -12.125988 -10.543285 -10.714369 -10.568867 -10.123615 -9.4105415 -9.4507456 -9.4329023 -9.4612827 -7.9353251 -8.702879 -9.0841007 -8.4788141][-10.622007 -10.580958 -9.8090391 -8.0893908 -7.2247343 -6.600142 -6.7646871 -6.9188423 -7.8764796 -7.5506854 -7.7710929 -7.7827044 -8.7148533 -8.6567564 -8.1175375]]...]
INFO - root - 2017-12-15 19:16:56.959097: step 48710, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 51h:42m:25s remains)
INFO - root - 2017-12-15 19:17:03.603087: step 48720, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 51h:22m:14s remains)
INFO - root - 2017-12-15 19:17:10.211671: step 48730, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.698 sec/batch; 55h:03m:21s remains)
INFO - root - 2017-12-15 19:17:16.806907: step 48740, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 52h:46m:43s remains)
INFO - root - 2017-12-15 19:17:23.373407: step 48750, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 51h:05m:45s remains)
INFO - root - 2017-12-15 19:17:29.939738: step 48760, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 51h:11m:03s remains)
INFO - root - 2017-12-15 19:17:36.486521: step 48770, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 51h:18m:46s remains)
INFO - root - 2017-12-15 19:17:43.141694: step 48780, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 50h:20m:18s remains)
INFO - root - 2017-12-15 19:17:49.742931: step 48790, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 51h:42m:38s remains)
INFO - root - 2017-12-15 19:17:56.307222: step 48800, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 49h:51m:52s remains)
2017-12-15 19:17:56.872859: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9059706 -8.128418 -8.234971 -7.4054236 -7.7210293 -7.1298862 -6.5565763 -6.8788714 -7.6028767 -7.894454 -7.9820442 -8.231246 -7.38759 -5.4524446 -2.280735][-7.8785238 -5.757515 -5.261374 -4.3822732 -4.4910536 -5.0288506 -5.5844803 -5.9426317 -6.389245 -6.8529377 -6.950119 -6.7503233 -6.2713594 -4.9007425 -1.9313366][-4.613121 -5.208159 -5.5619 -3.4007747 -3.3772638 -3.1142631 -2.824564 -2.7495511 -3.6330764 -5.3791022 -6.2208076 -6.31321 -5.4987512 -5.0307484 -2.6657362][-4.6263027 -5.3390846 -6.7193952 -5.614603 -5.7334118 -4.781209 -4.6740837 -4.749711 -4.6673222 -5.1047649 -6.0707803 -6.7786684 -6.050189 -5.4554067 -2.9610362][-6.7541032 -6.8157687 -6.5586643 -5.5647731 -5.6048093 -2.9141488 -1.3192391 -2.4478014 -4.5525656 -5.6061597 -6.4075031 -7.068202 -6.8021069 -6.1560354 -3.8844233][-8.18652 -6.6348248 -5.5757384 -4.0645018 -2.0946839 1.5693569 4.56422 3.2185836 1.2142668 -1.0745478 -3.8010027 -4.5219703 -5.3882332 -6.3894534 -4.5655756][-11.302561 -8.2463045 -6.1649413 -3.032872 -1.3498716 2.2669182 6.9497209 6.9966903 4.9657178 -0.29065371 -4.5692635 -5.7479286 -6.5841265 -5.79868 -4.8411965][-10.823875 -9.6933193 -9.3024082 -5.0247321 -2.0317059 1.3934507 5.7835097 7.59005 6.3522325 0.97729254 -2.936671 -5.3182225 -7.9431362 -7.8755918 -5.2905779][-9.1768789 -7.0867252 -7.4871988 -4.848155 -2.5945051 0.7317071 4.7563357 5.3733172 4.1004395 -0.2404604 -4.5985637 -7.6342711 -8.7321587 -7.9753866 -6.18163][-6.8692937 -6.06685 -6.3678493 -4.5800591 -4.3635912 -0.80395365 2.2054038 1.751317 0.88262224 -1.9350147 -4.9450092 -7.3228126 -8.262681 -8.59736 -7.0229025][-8.1491194 -7.1670332 -6.8835387 -5.4059381 -5.2200379 -4.1495132 -3.1836174 -2.9602666 -3.1374447 -4.7731586 -6.1892805 -8.2757807 -8.7823668 -9.4796505 -8.2196569][-12.850357 -12.637156 -12.330431 -11.169645 -11.353279 -10.862026 -10.319883 -9.512 -9.4903049 -10.189739 -10.674191 -10.945507 -9.7779474 -9.2065487 -8.4206038][-13.00075 -12.319412 -12.436287 -11.911398 -11.962881 -11.692921 -11.522631 -11.578217 -11.527424 -10.426105 -9.5476246 -10.242387 -9.5801449 -8.7704449 -7.190567][-9.7741327 -9.753396 -10.456478 -10.368902 -9.6919193 -8.9349422 -9.5238247 -9.8719406 -9.1649218 -7.9466591 -7.3818736 -6.4761195 -5.98469 -5.7259603 -5.1202669][-8.7872887 -7.7789884 -6.4608827 -7.0043373 -7.5293536 -6.8373518 -6.8648624 -6.5081649 -6.6848059 -6.3601103 -5.72929 -5.3546362 -6.0428548 -6.8191414 -6.0548687]]...]
INFO - root - 2017-12-15 19:18:03.422966: step 48810, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 51h:43m:30s remains)
INFO - root - 2017-12-15 19:18:09.950569: step 48820, loss = 0.23, batch loss = 0.18 (12.4 examples/sec; 0.645 sec/batch; 50h:48m:24s remains)
INFO - root - 2017-12-15 19:18:16.539848: step 48830, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 50h:35m:25s remains)
INFO - root - 2017-12-15 19:18:23.209135: step 48840, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 53h:26m:22s remains)
INFO - root - 2017-12-15 19:18:29.776632: step 48850, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 50h:39m:23s remains)
INFO - root - 2017-12-15 19:18:36.361897: step 48860, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 52h:28m:53s remains)
INFO - root - 2017-12-15 19:18:43.035305: step 48870, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 51h:15m:54s remains)
INFO - root - 2017-12-15 19:18:49.557419: step 48880, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 50h:42m:35s remains)
INFO - root - 2017-12-15 19:18:56.155005: step 48890, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 51h:55m:47s remains)
INFO - root - 2017-12-15 19:19:02.825575: step 48900, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 51h:55m:13s remains)
2017-12-15 19:19:03.388497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2216477 -2.8098314 -2.4117019 -2.5994744 -3.6562459 -3.7504652 -4.3691964 -5.4984393 -5.79951 -6.0420103 -5.8200541 -7.8973145 -9.0468969 -8.7428493 -8.0016594][-2.390409 -1.3100948 -1.405736 -2.1067936 -3.6799817 -4.2106333 -5.0522346 -5.8308144 -6.3424177 -6.4032631 -6.5865951 -8.2759523 -8.6928492 -9.2168274 -9.1879425][-0.96741772 -0.12913418 -1.1408391 -1.2804537 -2.7849953 -3.2217178 -3.6007473 -4.5847716 -5.17088 -5.9925385 -6.2993279 -8.1157188 -8.4920206 -8.388689 -7.6163912][-2.7871296 -1.7763929 -1.7469211 -1.3178945 -2.3359168 -2.6969402 -2.8511415 -3.4483783 -4.2118988 -4.4183111 -4.3345771 -6.4967489 -7.5543747 -7.1849194 -6.4489384][-4.0179424 -4.3724637 -4.3031511 -3.1165373 -3.0257018 -1.7927372 -1.5311332 -2.0687888 -2.7398884 -3.3524563 -3.1221972 -5.0002203 -5.862123 -6.5082421 -6.556282][-6.1185145 -6.2037826 -5.770155 -3.1588426 -1.5700173 0.4557457 1.2343001 0.57143831 -0.33486032 -1.13977 -1.5166197 -3.3386054 -3.9338534 -5.0336552 -5.913166][-6.839344 -6.5635386 -5.377995 -2.3463702 -0.3252573 1.9203663 3.5380731 3.8942924 3.2875524 1.7544899 0.048416138 -2.3256042 -3.3859706 -4.1528263 -4.6393948][-7.7605715 -6.66982 -5.7252231 -1.9518416 0.2040081 2.3999829 4.484139 4.51776 4.1947064 3.5005221 2.105763 -0.86647558 -2.1757162 -3.3436506 -4.1346741][-7.8858604 -6.911356 -5.2485852 -2.2365608 -0.8938961 1.2036557 3.2032552 3.2338691 2.8806987 2.6774611 2.3296103 -0.20743513 -2.0047152 -3.6069491 -3.866415][-7.9699173 -7.817359 -6.8401337 -4.0017114 -2.1055388 0.18605518 2.338572 2.171422 1.7895508 1.7207642 1.2756243 -1.0074692 -1.8990498 -3.1293583 -4.6363215][-11.116579 -10.576997 -9.5657368 -6.7116838 -5.3926983 -2.9044592 -0.75740385 0.076925278 0.15963316 0.13132954 -0.41442871 -2.9735491 -4.4176369 -4.8416071 -4.8464971][-14.376759 -14.021404 -12.516738 -10.106356 -8.0855064 -5.4353175 -4.4638042 -3.7336159 -2.7260268 -2.5606396 -3.0354168 -4.9638939 -5.5116 -5.65682 -5.8879833][-14.572474 -14.617041 -13.566752 -11.213687 -9.5395031 -7.1887851 -6.6336851 -6.1606221 -5.5323458 -5.0239677 -5.4491677 -6.62496 -7.1393228 -6.1748528 -5.8211889][-10.90738 -11.624464 -11.090018 -9.3698473 -7.2243867 -6.1871738 -5.7703915 -4.8675537 -5.1150122 -5.7496309 -6.0657969 -6.3112469 -6.38779 -6.4777689 -6.44901][-7.5200224 -7.7950954 -7.5444388 -6.0152783 -4.8102384 -3.9256673 -3.0919232 -3.6752195 -3.9599562 -4.2943974 -4.1869764 -5.3356667 -6.2293129 -6.8006859 -7.2401471]]...]
INFO - root - 2017-12-15 19:19:09.937632: step 48910, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 51h:54m:55s remains)
INFO - root - 2017-12-15 19:19:16.514462: step 48920, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 51h:54m:36s remains)
INFO - root - 2017-12-15 19:19:23.189426: step 48930, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 50h:53m:36s remains)
INFO - root - 2017-12-15 19:19:29.786083: step 48940, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.677 sec/batch; 53h:17m:51s remains)
INFO - root - 2017-12-15 19:19:36.374580: step 48950, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 52h:17m:56s remains)
INFO - root - 2017-12-15 19:19:43.036712: step 48960, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 52h:16m:51s remains)
INFO - root - 2017-12-15 19:19:49.660551: step 48970, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 52h:46m:06s remains)
INFO - root - 2017-12-15 19:19:56.286345: step 48980, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 52h:26m:15s remains)
INFO - root - 2017-12-15 19:20:02.840678: step 48990, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 52h:37m:44s remains)
INFO - root - 2017-12-15 19:20:09.403056: step 49000, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.692 sec/batch; 54h:28m:53s remains)
2017-12-15 19:20:09.922742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4183464 -5.1174965 -6.0816913 -5.5281167 -6.2959995 -6.5660286 -7.0728693 -7.9512296 -8.69765 -9.4584293 -8.4583359 -8.5364876 -11.394588 -11.959457 -12.019535][-3.7586565 -5.3102679 -6.731164 -6.7144246 -7.1065273 -7.8852587 -8.9344559 -9.856081 -10.49463 -10.591177 -10.017241 -9.5599737 -11.287485 -11.785574 -11.474043][-1.926791 -4.4537191 -7.7089033 -8.9800873 -9.76285 -9.1768951 -9.1863956 -9.5706539 -9.643446 -10.299848 -9.1620321 -8.96565 -10.999092 -11.446728 -11.144112][-5.4868259 -6.5876937 -8.62478 -10.859718 -12.010982 -10.677523 -9.4629908 -10.305754 -10.016008 -8.7012644 -8.2984447 -8.8743057 -10.910628 -11.50566 -11.399973][-8.2159576 -11.749949 -13.347277 -12.854692 -11.803261 -8.3630848 -4.6685743 -6.3270459 -8.7819881 -8.5053835 -7.4406457 -8.505621 -12.190777 -13.40588 -13.147352][-9.9479332 -12.158606 -13.815012 -12.016939 -8.8355665 -4.5747285 1.4275422 1.3918824 -0.1816597 -4.2698264 -8.7401457 -8.1531391 -11.141491 -13.467066 -13.444008][-13.076084 -12.497182 -10.878389 -7.7646475 -2.9747541 2.0506611 7.6797814 9.5779915 9.9494247 1.558949 -7.0966425 -8.5057993 -13.007538 -14.55777 -14.643625][-12.851814 -12.964745 -11.984589 -7.5057907 -0.70127726 6.3199992 13.008736 13.053833 11.414618 5.3528657 -1.2307267 -7.416121 -14.816547 -15.842535 -15.173613][-11.214457 -12.563053 -12.787793 -8.9475117 -3.7427025 3.9207397 12.173679 12.781216 9.4091148 2.6770625 -3.54061 -9.1337328 -16.364424 -17.210798 -15.781155][-9.7555637 -11.187552 -11.946801 -10.385059 -8.5769081 -3.1705008 3.9818606 7.6983829 6.3088965 -0.68217611 -7.1435642 -10.912473 -16.983253 -19.276207 -18.215376][-13.351835 -14.594103 -14.997013 -13.909977 -12.157763 -9.99143 -7.5880795 -4.9386687 -3.9262419 -5.7923274 -9.4337559 -13.738289 -18.347971 -19.016291 -17.764778][-18.679396 -19.141146 -19.100866 -17.674751 -16.089336 -15.357882 -14.649002 -13.50231 -12.382672 -12.203547 -13.850573 -16.118086 -18.623232 -18.287994 -15.789221][-18.829082 -19.801981 -21.07469 -18.923328 -17.17659 -15.485039 -14.845997 -15.381102 -14.92074 -13.936298 -13.690172 -14.047808 -16.244593 -15.654593 -13.63393][-17.105371 -18.594723 -18.723173 -16.240767 -14.892809 -15.225994 -14.659807 -13.908386 -13.406633 -13.236904 -12.856665 -11.863904 -12.702339 -13.028744 -12.028923][-11.65481 -12.528888 -13.17469 -11.482262 -10.111693 -9.7793484 -9.7454538 -9.6981611 -10.165403 -10.278631 -10.384272 -11.197665 -12.131395 -12.163775 -12.602634]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 19:20:16.454614: step 49010, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 50h:06m:20s remains)
INFO - root - 2017-12-15 19:20:23.073480: step 49020, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 52h:32m:13s remains)
INFO - root - 2017-12-15 19:20:29.646101: step 49030, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 51h:52m:31s remains)
INFO - root - 2017-12-15 19:20:36.120126: step 49040, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.633 sec/batch; 49h:52m:51s remains)
INFO - root - 2017-12-15 19:20:42.734150: step 49050, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 51h:57m:31s remains)
INFO - root - 2017-12-15 19:20:49.332061: step 49060, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 50h:56m:00s remains)
INFO - root - 2017-12-15 19:20:55.885660: step 49070, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 50h:24m:03s remains)
INFO - root - 2017-12-15 19:21:02.517626: step 49080, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.696 sec/batch; 54h:49m:36s remains)
INFO - root - 2017-12-15 19:21:09.076104: step 49090, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 50h:33m:25s remains)
INFO - root - 2017-12-15 19:21:15.602589: step 49100, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 49h:59m:01s remains)
2017-12-15 19:21:16.135252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.5861197 -8.6579361 -8.5635986 -7.9710488 -8.4666128 -9.4440641 -9.4761171 -9.13459 -8.857317 -7.9709644 -6.633523 -6.769805 -8.8086338 -10.1262 -9.3965883][-6.0470352 -6.8259621 -7.2229805 -7.0733032 -8.7962837 -10.479095 -11.580465 -11.432354 -11.008289 -10.27262 -9.2580833 -9.0278463 -10.315162 -11.599887 -10.79883][-3.6619077 -4.5290027 -5.0501623 -4.8362594 -6.9668159 -8.7010126 -10.347677 -10.883747 -10.518534 -10.007225 -9.9376431 -9.6874847 -10.474752 -11.682159 -10.952954][-3.7342107 -4.337502 -3.9845672 -3.3223846 -5.0936766 -5.1554832 -5.46773 -7.1251421 -8.4259424 -8.0612984 -7.4580717 -7.7484608 -9.9771023 -11.270132 -10.417923][-4.6153474 -5.2781296 -5.0614309 -3.5860889 -3.5293953 -3.1996796 -3.4929247 -3.7077365 -3.5313938 -4.1748924 -5.131279 -5.7142625 -7.8329611 -10.296904 -10.298036][-8.5650616 -7.986846 -6.9580956 -5.2574 -3.5967734 -1.3165302 -0.34106112 -0.46817446 -0.38285923 -1.0186515 -1.8232872 -3.5141253 -6.503756 -8.9320965 -9.6842318][-9.8874922 -8.6579037 -6.8879824 -4.8168435 -3.5157025 -0.22665071 2.5055814 3.2059894 3.2057023 1.9093347 0.29106855 -1.6082907 -4.7923493 -7.0103979 -7.3973722][-8.9025564 -8.8369865 -7.9851656 -4.0437088 -0.83012867 1.4127936 3.1655173 4.321156 5.0324206 4.1047769 2.2835488 -0.0322752 -3.1621008 -5.1995897 -4.8498325][-7.9097762 -7.7664175 -6.7572761 -3.4963074 -1.066958 1.3928285 3.0241218 3.3732219 3.4179511 3.9922805 3.793426 1.5044875 -2.2341568 -4.0345173 -3.6702647][-8.7963877 -7.5806465 -5.81166 -4.1397839 -3.00728 -1.2451334 0.38201237 1.5928969 2.1691637 1.9278421 1.4822202 0.75021315 -1.3289237 -2.6272609 -2.5136814][-11.343495 -10.72896 -9.00899 -6.2489352 -4.6348581 -4.0105214 -4.1300073 -3.5408666 -2.4357059 -1.509562 -1.7461531 -2.9602165 -4.648181 -4.5894961 -2.1901722][-14.230974 -13.336615 -10.730284 -7.1268883 -6.1250677 -5.41022 -5.0330625 -5.5241418 -6.2669005 -6.1038508 -5.7877073 -5.6230021 -6.4497333 -6.0089412 -4.0248179][-13.82456 -12.874191 -10.560383 -7.3793659 -6.3445072 -5.0066748 -4.2028475 -4.2987108 -4.7589221 -5.2585011 -5.700119 -6.7651749 -6.8996897 -6.2378936 -4.30832][-13.049015 -12.11945 -9.6015882 -7.5059128 -6.3108945 -5.6998587 -5.049345 -3.31864 -1.6932502 -1.4211483 -2.6419802 -3.6413226 -4.7301083 -4.9762363 -3.9416506][-8.4751949 -9.0660181 -7.6074419 -6.3225908 -5.0298462 -5.0392351 -4.7503619 -3.8373504 -1.915024 -0.33446789 -0.043869495 -0.42345381 -1.8019578 -3.8175569 -5.1287317]]...]
INFO - root - 2017-12-15 19:21:22.615412: step 49110, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 51h:46m:00s remains)
INFO - root - 2017-12-15 19:21:29.298154: step 49120, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 53h:20m:09s remains)
INFO - root - 2017-12-15 19:21:35.854510: step 49130, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 51h:54m:23s remains)
INFO - root - 2017-12-15 19:21:42.407635: step 49140, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 50h:52m:35s remains)
INFO - root - 2017-12-15 19:21:49.013701: step 49150, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 53h:48m:33s remains)
INFO - root - 2017-12-15 19:21:55.696882: step 49160, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 52h:02m:08s remains)
INFO - root - 2017-12-15 19:22:02.292493: step 49170, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 52h:31m:12s remains)
INFO - root - 2017-12-15 19:22:08.980041: step 49180, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 50h:45m:57s remains)
INFO - root - 2017-12-15 19:22:15.562934: step 49190, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 50h:11m:22s remains)
INFO - root - 2017-12-15 19:22:22.097284: step 49200, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 50h:35m:38s remains)
2017-12-15 19:22:22.635417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.1030684 -8.0909863 -7.8119283 -6.9300346 -6.6902018 -6.0744915 -5.160316 -4.5188708 -3.4682548 -2.5032234 -1.3977389 -1.9387488 -2.311553 -2.5479369 -2.62184][-7.1231518 -7.7183194 -8.26218 -8.4205055 -8.383625 -8.3374958 -8.119153 -7.3425713 -6.2443638 -4.7517505 -3.45776 -3.6263616 -4.4823928 -4.8734126 -4.1502767][-5.5960817 -6.6949673 -8.1096926 -7.5202618 -7.7610483 -7.9357548 -7.6608438 -7.118753 -6.3720584 -5.6355882 -5.2713532 -5.8495288 -7.0391889 -6.8969688 -5.3845677][-5.2659688 -6.1444273 -5.9849849 -5.3784676 -5.6803689 -5.1912632 -4.7935233 -4.9316335 -5.5763702 -5.479424 -4.9024034 -6.8373604 -8.6565466 -8.173604 -6.3492622][-5.5843472 -6.0220518 -6.5681605 -4.9556994 -3.5121629 -2.4999657 -1.8116336 -2.2613509 -2.9045668 -3.5937126 -4.1402884 -6.3568087 -8.7457533 -9.1904726 -7.512074][-6.7088156 -6.0011535 -5.2123685 -3.2778397 -1.0158982 1.4575591 3.1351514 2.2810016 1.0553255 -0.21160889 -1.4520855 -4.3083286 -6.6849523 -7.8522377 -7.3544559][-8.3274479 -7.0559149 -5.67954 -2.7148683 0.028220177 3.1986938 5.7068353 6.4783769 5.9942317 3.0557733 -0.2107935 -3.1650774 -5.7949934 -7.1929054 -6.3331633][-7.6545939 -6.4582415 -5.7548218 -2.4567103 0.84238195 3.3851495 5.6664796 6.551517 6.1293435 4.1654429 1.2907534 -2.5952547 -5.4283576 -6.0122094 -5.5261955][-7.5112219 -6.1166453 -4.1805196 -1.1806588 0.063138962 2.8459191 4.8789077 5.9310737 5.8321652 3.5601869 1.0742621 -2.8496068 -5.78834 -6.94944 -6.8892117][-5.8979025 -5.1565094 -3.2126615 -1.1726413 -0.22159052 1.4022474 2.6990771 3.0263038 2.7982669 1.7897835 0.24007463 -3.3284643 -6.2190437 -7.6942739 -7.6895161][-6.84928 -5.3400607 -3.8449547 -2.1654525 -1.4481149 -1.1079545 -0.95658779 -0.515821 -0.49791002 -1.6353431 -2.42118 -5.3431473 -7.6683059 -9.0687828 -8.8194408][-9.1142864 -8.5998774 -6.7984428 -4.932126 -4.8369074 -5.0577908 -5.0704579 -5.7311378 -6.1006708 -5.6474333 -5.5111642 -7.6370053 -8.1443329 -8.6517277 -8.2828531][-11.921005 -10.555103 -8.31567 -5.8630524 -5.4963932 -6.0878224 -6.6261983 -7.255167 -7.2301488 -8.0079327 -8.0924559 -8.628788 -8.4230518 -7.8258333 -6.6031113][-10.089666 -8.6675491 -7.4825153 -6.3422494 -6.1390767 -5.51602 -6.0145664 -5.9008932 -5.7847838 -5.9797792 -6.0432382 -6.4992352 -5.6580882 -5.7188354 -4.3143082][-8.0660152 -7.0539374 -5.8602262 -4.8641453 -4.4346623 -4.6809072 -4.8285551 -4.3682413 -4.640204 -4.8583488 -4.9017997 -4.863832 -5.5435977 -5.3265362 -4.9145851]]...]
INFO - root - 2017-12-15 19:22:29.324167: step 49210, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 52h:22m:09s remains)
INFO - root - 2017-12-15 19:22:35.946610: step 49220, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 53h:05m:19s remains)
INFO - root - 2017-12-15 19:22:42.523645: step 49230, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 51h:53m:08s remains)
INFO - root - 2017-12-15 19:22:49.099524: step 49240, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 50h:21m:05s remains)
INFO - root - 2017-12-15 19:22:55.651461: step 49250, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 51h:52m:58s remains)
INFO - root - 2017-12-15 19:23:02.229080: step 49260, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 51h:22m:47s remains)
INFO - root - 2017-12-15 19:23:08.880788: step 49270, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.648 sec/batch; 50h:56m:56s remains)
INFO - root - 2017-12-15 19:23:15.452783: step 49280, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 52h:08m:02s remains)
INFO - root - 2017-12-15 19:23:21.981992: step 49290, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 51h:05m:20s remains)
INFO - root - 2017-12-15 19:23:28.546308: step 49300, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 51h:36m:41s remains)
2017-12-15 19:23:29.081263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5878415 -3.7284656 -3.8958335 -3.3134863 -3.700495 -4.1299391 -4.7313366 -5.1264763 -5.2506142 -5.784699 -5.6413708 -6.1810279 -7.7658811 -6.7469292 -4.3987403][-3.7460556 -3.9459004 -3.2826569 -2.85604 -2.5818439 -3.1937103 -4.1211762 -4.1305189 -3.74732 -3.9130535 -4.2797985 -4.9385066 -6.0055671 -6.3357954 -5.4614015][-3.2622812 -3.917335 -4.2561502 -2.8856156 -3.0144908 -2.7028298 -2.7234011 -2.7718127 -2.8261735 -2.8013856 -3.6121292 -4.2194715 -5.312129 -6.0330739 -5.6408796][-4.357573 -4.5413122 -4.7113848 -3.4058108 -2.7577884 -1.9802346 -1.5572224 -1.6866298 -2.0006795 -1.7201867 -1.8247111 -3.2863276 -5.0679078 -6.074008 -4.5070224][-5.1633964 -6.2498741 -6.1842985 -3.9786582 -1.8927767 0.02968359 1.1663389 0.69444752 -0.36740685 -1.0741138 -2.2697175 -2.423615 -3.6159818 -4.7536488 -3.5057044][-6.5289116 -6.4648576 -5.2587795 -2.2350843 0.89889574 3.5648904 5.55405 5.3570876 4.5028138 2.5071225 0.073215008 -0.62982273 -2.3161027 -3.2709846 -2.6105082][-7.4657941 -6.9103966 -5.3393044 -2.0740163 0.676733 4.033072 6.9975781 6.8120923 6.1027865 3.238956 0.66644192 -1.0613723 -4.2893066 -4.3106031 -3.3782704][-7.5624375 -6.2706461 -5.2995996 -2.9791207 -0.049509048 3.5980792 5.6067166 5.7499185 5.449101 3.6200995 1.4545317 -1.6462917 -5.1059608 -6.179709 -6.2655969][-6.4554815 -5.8417282 -5.1168628 -4.3133974 -1.6032352 2.0435581 4.050324 4.4972491 4.0283332 2.1187153 0.52823925 -2.5960598 -5.8727856 -7.1288338 -7.1462269][-5.1867676 -5.255414 -5.9962716 -4.3897858 -2.8186634 -0.32230234 1.7444258 2.9813905 2.1331887 0.29056787 -0.95650291 -3.1296365 -5.99502 -8.59161 -8.8379965][-8.518528 -8.5324364 -8.426322 -7.3280168 -6.4158649 -4.5746779 -2.5424635 -1.7760282 -2.4867058 -3.271251 -4.5669155 -6.6914577 -8.2427464 -8.6958809 -7.8513727][-12.156029 -11.203373 -10.386537 -8.8885765 -8.3403721 -7.2425323 -6.370779 -5.9589291 -5.6320724 -5.9444785 -6.5476894 -6.8841729 -7.4239674 -8.6973228 -8.2108383][-13.074273 -12.741644 -11.352301 -10.09338 -8.7985363 -7.1531687 -6.7669768 -6.6212997 -6.714859 -6.9755678 -7.285058 -6.0332994 -6.1362143 -6.12861 -4.6400571][-10.418428 -10.193245 -9.2537689 -7.7770848 -6.7651095 -6.2289743 -5.488502 -4.5200768 -4.1879435 -4.4800749 -4.5825834 -4.03613 -4.17582 -4.6586514 -4.4699054][-8.5463753 -8.0080318 -6.8754821 -6.6576924 -5.6010318 -3.8294187 -3.0845892 -2.7404976 -2.6008973 -2.6748505 -2.9519055 -3.8224344 -4.6793013 -4.6376276 -4.4328003]]...]
INFO - root - 2017-12-15 19:23:35.551171: step 49310, loss = 0.13, batch loss = 0.08 (12.7 examples/sec; 0.631 sec/batch; 49h:38m:54s remains)
INFO - root - 2017-12-15 19:23:42.026345: step 49320, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 50h:02m:43s remains)
INFO - root - 2017-12-15 19:23:48.656896: step 49330, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 51h:29m:20s remains)
INFO - root - 2017-12-15 19:23:55.289301: step 49340, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 51h:48m:15s remains)
INFO - root - 2017-12-15 19:24:01.859806: step 49350, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 51h:00m:34s remains)
INFO - root - 2017-12-15 19:24:08.391011: step 49360, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.648 sec/batch; 50h:55m:38s remains)
INFO - root - 2017-12-15 19:24:14.978732: step 49370, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 51h:45m:17s remains)
INFO - root - 2017-12-15 19:24:21.517867: step 49380, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 50h:34m:27s remains)
INFO - root - 2017-12-15 19:24:28.048740: step 49390, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 52h:11m:25s remains)
INFO - root - 2017-12-15 19:24:34.589990: step 49400, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 51h:38m:41s remains)
2017-12-15 19:24:35.085423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6107235 -6.2986674 -7.1410489 -8.636878 -10.131716 -10.663707 -10.626686 -8.9355373 -8.0467319 -7.9150357 -7.7487388 -8.26815 -11.916346 -12.305845 -12.009607][-6.52393 -6.44503 -6.524744 -7.4595594 -8.862093 -10.564194 -11.431446 -11.291493 -11.391699 -10.17161 -9.4113636 -9.6514359 -11.943438 -12.469713 -13.257292][-3.3341625 -4.6790304 -6.2672534 -6.0765409 -7.2732797 -9.34457 -10.749293 -11.768828 -11.530486 -10.811756 -10.818976 -10.511622 -13.197637 -12.90456 -13.008968][-2.0195072 -3.4287896 -4.4158926 -4.9183354 -6.1646733 -6.1553369 -6.6883473 -8.9373226 -10.624149 -10.103921 -9.7827358 -10.494152 -12.978505 -12.745705 -13.008726][-4.2416086 -4.7063904 -5.1441994 -5.2086282 -4.6324844 -3.8036833 -2.4483402 -4.3359652 -6.7848182 -8.0021486 -8.154851 -8.196063 -11.571489 -12.004786 -11.623758][-7.620729 -7.2519646 -6.53785 -5.8359866 -3.8690853 -0.46371841 3.6833882 2.4806657 0.803565 -2.9398553 -6.9852958 -6.1731949 -8.7472572 -9.9062347 -10.533909][-10.583338 -10.123522 -7.746171 -5.8457584 -3.5499659 1.6786928 6.9504991 8.4177284 7.8873096 1.376225 -4.7342448 -5.4946728 -9.3268185 -9.0287 -8.5668154][-9.8286953 -9.4740124 -8.7670517 -6.0583992 -3.7221708 1.3307347 6.605906 9.5846348 10.006508 4.8658957 -1.3399158 -5.8829765 -11.695703 -10.090219 -8.6487179][-6.1019597 -6.1433282 -5.99432 -4.9003849 -4.1066837 -0.90177536 3.062202 6.4828143 8.0752869 3.8002667 -1.5007095 -7.3516 -13.438053 -13.644961 -12.93182][-3.7601361 -4.0667987 -3.5789988 -3.2764516 -3.6791213 -2.6544938 -0.33819294 1.6262341 3.4168983 1.5376263 -3.9754245 -9.9290228 -16.334173 -18.065298 -17.432938][-7.0662127 -6.2370663 -5.6371775 -5.6724305 -7.04748 -6.8922911 -5.8412013 -4.9816151 -4.0416546 -5.4653196 -8.2000265 -11.136337 -16.329607 -19.015114 -18.89361][-13.225761 -11.976534 -11.276257 -10.104431 -11.022396 -10.916771 -11.693672 -12.151505 -11.576338 -11.968167 -13.212509 -14.130505 -15.509367 -16.994658 -16.027369][-15.176662 -15.407457 -14.333521 -13.294523 -13.272198 -12.907543 -12.790052 -13.572511 -13.862461 -13.645966 -13.677599 -14.365679 -14.698498 -13.190288 -11.907677][-13.292953 -13.345858 -12.831573 -11.891778 -11.581272 -10.918448 -11.941122 -11.934114 -12.044806 -12.472965 -12.631596 -11.205136 -9.9234581 -8.9928513 -7.4161][-7.6673069 -7.0096121 -5.7095351 -5.7842288 -5.510181 -5.3801179 -6.1179223 -6.6789618 -7.2647285 -7.5185566 -8.58939 -9.1621323 -9.2600431 -7.9713488 -5.9573259]]...]
INFO - root - 2017-12-15 19:24:41.638186: step 49410, loss = 0.21, batch loss = 0.17 (12.1 examples/sec; 0.660 sec/batch; 51h:53m:25s remains)
INFO - root - 2017-12-15 19:24:48.222968: step 49420, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 52h:05m:39s remains)
INFO - root - 2017-12-15 19:24:54.785044: step 49430, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 52h:36m:21s remains)
INFO - root - 2017-12-15 19:25:01.368006: step 49440, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 51h:47m:06s remains)
INFO - root - 2017-12-15 19:25:07.923958: step 49450, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 52h:13m:14s remains)
INFO - root - 2017-12-15 19:25:14.399391: step 49460, loss = 0.20, batch loss = 0.15 (12.7 examples/sec; 0.629 sec/batch; 49h:26m:06s remains)
INFO - root - 2017-12-15 19:25:20.967997: step 49470, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 51h:00m:45s remains)
INFO - root - 2017-12-15 19:25:27.606596: step 49480, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.680 sec/batch; 53h:29m:41s remains)
INFO - root - 2017-12-15 19:25:34.216035: step 49490, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 52h:18m:51s remains)
INFO - root - 2017-12-15 19:25:40.819635: step 49500, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 52h:02m:22s remains)
2017-12-15 19:25:41.326554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5039816 -8.3277693 -9.4303808 -9.7266359 -9.5093651 -9.9628773 -9.4213991 -8.3870487 -7.6214962 -6.7605848 -4.9479294 -3.4734042 -4.3255997 -6.2200909 -6.09295][-5.3531833 -6.6719828 -9.0012426 -9.1764259 -9.1150951 -8.7716064 -6.8898606 -7.0285053 -6.9879947 -6.2849522 -4.4853873 -3.0750284 -4.4308591 -6.5785165 -5.9382949][-5.3280115 -4.9908161 -6.3534555 -7.3343759 -8.9293318 -8.8775015 -7.7446713 -6.15368 -5.0241079 -4.8893242 -4.3920779 -3.2757812 -3.4908278 -6.1963415 -6.5593147][-5.5317221 -7.3442888 -7.8462949 -7.2099628 -8.3896208 -7.8674459 -5.8229146 -5.9466805 -5.9702554 -5.4528785 -4.1084385 -3.3087134 -4.2731118 -6.1767969 -5.9051118][-7.8356509 -8.5499268 -9.8160686 -9.3400536 -8.382185 -5.3526869 -4.0805912 -4.8302603 -4.9543896 -4.8825283 -4.8698463 -4.6366758 -4.7920685 -7.5803571 -7.5381112][-8.4799347 -8.4496813 -7.5538993 -7.2035537 -6.7267261 -2.0893478 0.814476 -0.1357193 -0.62759447 -1.7706418 -3.1751249 -3.1558485 -4.090765 -6.9508619 -6.7619686][-8.9871054 -6.9790592 -6.2339582 -4.5954738 -2.6196873 0.96565676 3.3469548 4.1401381 4.2217765 1.5461702 -0.83452892 -1.0404401 -2.9443696 -5.0878005 -5.402688][-7.5147033 -6.1127162 -4.0050907 -1.2109423 0.029698372 3.4334931 5.5348325 6.0056853 6.5919518 3.6549602 1.3372288 -0.030066967 -1.9569335 -4.1359625 -5.289989][-6.0163875 -5.2069306 -3.8694091 -0.38737297 1.2620101 3.6365342 5.3244414 5.3680463 4.8566794 2.8018861 1.366477 -0.37764597 -3.1908326 -5.71549 -5.5128937][-4.7814164 -4.3066549 -3.6766853 -1.5797029 -0.9904418 1.9838948 3.86132 3.6608367 2.3172774 0.34009504 -1.0239849 -1.7428992 -3.5312245 -6.8727207 -7.7401776][-4.265049 -4.220562 -3.4547079 -2.6684444 -2.7469037 -1.439198 0.16133356 0.49481487 -0.52800465 -1.6761155 -3.2126629 -4.26072 -6.0598488 -8.9483833 -8.8735437][-9.4027338 -8.8775663 -8.1679077 -6.7071371 -7.1546955 -7.1185846 -7.0808849 -5.72924 -5.0279393 -6.1368165 -7.9463186 -7.3212614 -7.8645506 -9.9729033 -10.051946][-12.479126 -11.542583 -10.15555 -8.9160013 -8.9486322 -7.9503469 -8.1372309 -7.5426154 -7.772974 -8.1277618 -8.4314346 -8.1845264 -9.6769753 -10.418222 -9.3370457][-11.221558 -11.694413 -11.052971 -9.2795687 -7.9044924 -7.2601528 -6.6539292 -5.5784497 -5.7048841 -5.7841568 -5.90559 -4.838212 -4.8705115 -6.0686398 -5.8052864][-8.7017784 -8.78593 -9.7104645 -7.9589329 -6.3897681 -4.9423375 -3.711669 -3.30866 -3.9287724 -4.3188391 -4.4807758 -3.858902 -4.5174346 -4.343626 -3.7792654]]...]
INFO - root - 2017-12-15 19:25:47.863605: step 49510, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 52h:24m:19s remains)
INFO - root - 2017-12-15 19:25:54.537582: step 49520, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 53h:13m:28s remains)
INFO - root - 2017-12-15 19:26:01.162640: step 49530, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 51h:50m:37s remains)
INFO - root - 2017-12-15 19:26:07.745581: step 49540, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 51h:34m:01s remains)
INFO - root - 2017-12-15 19:26:14.382258: step 49550, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 52h:54m:12s remains)
INFO - root - 2017-12-15 19:26:21.015558: step 49560, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 52h:55m:28s remains)
INFO - root - 2017-12-15 19:26:27.646235: step 49570, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 52h:39m:31s remains)
INFO - root - 2017-12-15 19:26:34.198663: step 49580, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 52h:28m:26s remains)
INFO - root - 2017-12-15 19:26:40.785647: step 49590, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 52h:08m:27s remains)
INFO - root - 2017-12-15 19:26:47.381042: step 49600, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 51h:46m:44s remains)
2017-12-15 19:26:47.921880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0579453 -5.7034192 -7.2351084 -8.0698757 -9.25354 -10.724045 -11.771147 -10.687168 -10.26412 -10.382883 -9.2386284 -10.101572 -12.010912 -11.386458 -13.030752][-7.7946596 -8.55084 -9.058341 -9.7689152 -10.117331 -10.63792 -11.787165 -12.802578 -13.708912 -12.414447 -11.110177 -11.823846 -13.256111 -12.394386 -13.026723][-4.7607903 -7.0318727 -9.8674164 -9.5653419 -9.0050011 -10.999504 -12.361402 -12.229652 -12.136625 -12.519513 -12.469275 -11.751598 -12.864378 -13.113815 -13.84606][-7.8315668 -9.7417812 -10.874803 -9.4950323 -9.16772 -9.4201412 -9.0985594 -11.761864 -12.923218 -11.165155 -9.6574421 -11.921709 -14.737442 -13.079707 -12.588928][-9.0311451 -12.39676 -13.889154 -11.589888 -9.1045151 -4.7360506 -2.5518646 -6.9313178 -11.865736 -11.878731 -11.435981 -11.133867 -12.30481 -12.910973 -14.160852][-12.403849 -14.244364 -13.776045 -11.287945 -7.9930124 -2.2114451 3.468884 1.3305659 -2.058279 -7.1724863 -12.386423 -11.398759 -11.989229 -12.027107 -13.732389][-15.468025 -15.691437 -13.475109 -9.2760973 -3.8908892 1.6535907 6.0391574 6.0443196 5.6023765 -1.2794352 -9.0366573 -10.418043 -12.869817 -11.919899 -12.853977][-14.826017 -14.266748 -12.9813 -8.4507923 -1.2348905 5.9996009 10.102119 6.6426892 4.8735585 0.038142681 -5.4753318 -8.7660236 -12.563988 -12.500508 -14.274502][-11.45668 -10.721497 -12.142822 -8.7769051 -3.1320326 2.7387633 8.3168259 9.2011967 5.1445374 -2.873117 -7.4820156 -9.7199774 -14.100998 -13.974592 -14.775297][-9.8411341 -9.9885054 -10.456719 -7.0236921 -4.8489752 -2.5558515 2.4015498 4.2065072 2.27606 -2.67469 -8.4417944 -11.99262 -14.904165 -14.441841 -16.411375][-10.157431 -11.758886 -12.493761 -10.79214 -9.0490656 -6.7269616 -3.4880126 -3.9526153 -4.366282 -5.2510943 -8.7850637 -13.282383 -15.54701 -15.646519 -15.758589][-16.795052 -16.459917 -16.51223 -15.938883 -14.465696 -11.897125 -11.280243 -12.169888 -11.72768 -11.938673 -13.495922 -13.978477 -14.651255 -14.78158 -14.720377][-16.53792 -14.542299 -14.47842 -16.604153 -15.948465 -14.87215 -12.788139 -12.750925 -14.078842 -13.43124 -12.992908 -14.964533 -14.873348 -12.986118 -12.562043][-13.361165 -12.545544 -13.209706 -11.135185 -10.877235 -12.07465 -12.520978 -12.039668 -11.421942 -11.36787 -12.24806 -12.237597 -11.916725 -10.292093 -10.63888][-9.6528931 -8.1236229 -6.8905554 -6.0288057 -6.1053762 -6.182919 -7.036572 -8.9923325 -9.3997021 -8.5715351 -8.7928419 -10.79513 -10.960119 -10.855169 -12.019766]]...]
INFO - root - 2017-12-15 19:26:54.584809: step 49610, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 52h:29m:48s remains)
INFO - root - 2017-12-15 19:27:01.195229: step 49620, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 50h:30m:08s remains)
INFO - root - 2017-12-15 19:27:07.810649: step 49630, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 53h:51m:11s remains)
INFO - root - 2017-12-15 19:27:14.396552: step 49640, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 51h:24m:49s remains)
INFO - root - 2017-12-15 19:27:20.998385: step 49650, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 51h:27m:45s remains)
INFO - root - 2017-12-15 19:27:27.556390: step 49660, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 50h:08m:51s remains)
INFO - root - 2017-12-15 19:27:34.099010: step 49670, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 51h:17m:11s remains)
INFO - root - 2017-12-15 19:27:40.718026: step 49680, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 50h:10m:22s remains)
INFO - root - 2017-12-15 19:27:47.303290: step 49690, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 50h:27m:01s remains)
INFO - root - 2017-12-15 19:27:53.895096: step 49700, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 51h:28m:37s remains)
2017-12-15 19:27:54.381203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4802027 -7.2236266 -8.3692284 -9.1305695 -11.205478 -11.078743 -10.117777 -8.5810928 -7.414547 -7.8113575 -9.0678883 -10.395266 -13.678021 -18.100101 -15.614189][-4.0870714 -4.8873696 -7.0044775 -8.2520332 -9.8340263 -10.7542 -10.703364 -9.3257179 -8.9234362 -7.8843641 -7.2355494 -8.067771 -11.673163 -17.698116 -17.012627][-3.88557 -5.0691752 -7.7769814 -8.5810823 -10.14403 -10.744719 -10.301001 -9.32641 -7.8540459 -7.1826391 -7.6822042 -7.870656 -9.8315086 -14.09613 -14.005442][-3.763963 -5.9718046 -6.2639337 -5.5945191 -7.0024652 -6.4989824 -6.34891 -7.4149771 -7.3187752 -6.5895987 -6.4578848 -6.1844025 -8.7727051 -13.245165 -12.647353][-4.2187424 -4.9018135 -6.6840649 -6.487318 -5.4925513 -2.7298696 -1.0054297 -1.5817995 -2.790766 -3.29582 -4.0332994 -4.1941032 -6.4164238 -12.405207 -14.202942][-5.8591943 -5.5622058 -5.542737 -4.5989628 -2.8941267 0.60702372 3.7897773 4.3971419 2.3512216 -0.07916069 -1.6523623 -1.526237 -3.4063601 -8.9017553 -10.963529][-5.5894837 -5.1199865 -4.8164864 -3.416708 -1.4021893 1.9431167 5.5110526 7.5404468 6.98922 3.6859469 0.47396421 0.85580063 -1.6300335 -7.4736528 -8.1241636][-5.918633 -4.088861 -3.7861814 -2.5351429 -1.1883478 4.1985927 7.5660481 7.4328628 7.2676854 5.2968354 3.0965323 1.7897711 -2.5281575 -6.8774562 -6.6090622][-4.4792514 -3.6559958 -3.4317126 -2.2567258 -2.6110165 0.59533119 4.5346503 6.8510938 7.1696153 4.6810269 2.1242256 -0.2133913 -3.9823806 -9.6161385 -11.492321][-4.3117762 -3.213449 -2.8413382 -1.7441375 -2.7216043 -0.36515093 1.7814107 3.7249722 3.572679 2.2024975 0.79386044 -1.8910968 -6.309772 -12.743444 -14.048046][-9.4471006 -8.58708 -8.7472353 -6.3984442 -5.9363108 -5.3176184 -4.8975668 -2.430274 -1.2923651 -2.8897171 -4.6956639 -6.3924775 -11.216061 -16.821541 -16.718559][-9.4544706 -8.8008089 -8.8258677 -8.5457821 -8.8193274 -5.9050641 -5.2532845 -5.5606074 -6.9252915 -7.4253597 -7.8458214 -8.2366819 -10.0119 -16.172598 -17.783957][-11.04674 -9.0980644 -8.3692675 -8.1797285 -9.3265629 -8.1163425 -6.2612944 -6.0631938 -7.5308523 -8.9894886 -10.144286 -10.02726 -12.023241 -15.523056 -14.855267][-9.7064991 -9.524106 -8.7801533 -8.67959 -7.8986444 -6.874156 -6.972991 -5.5557342 -4.1353312 -6.3963466 -7.8063345 -7.8434682 -8.0738764 -10.69682 -11.580267][-7.3615837 -6.3152533 -6.555697 -5.7919707 -4.1454 -4.4866648 -4.8708258 -5.5058374 -6.9177241 -6.3127623 -5.6777077 -6.9211154 -9.0475368 -8.4214287 -7.0051661]]...]
INFO - root - 2017-12-15 19:28:01.053685: step 49710, loss = 0.24, batch loss = 0.20 (12.0 examples/sec; 0.668 sec/batch; 52h:27m:57s remains)
INFO - root - 2017-12-15 19:28:07.684239: step 49720, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 52h:39m:15s remains)
INFO - root - 2017-12-15 19:28:14.328783: step 49730, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.684 sec/batch; 53h:44m:40s remains)
INFO - root - 2017-12-15 19:28:20.960987: step 49740, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 51h:45m:12s remains)
INFO - root - 2017-12-15 19:28:27.595230: step 49750, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 53h:11m:58s remains)
INFO - root - 2017-12-15 19:28:34.206548: step 49760, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 51h:21m:14s remains)
INFO - root - 2017-12-15 19:28:40.881525: step 49770, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 51h:33m:52s remains)
INFO - root - 2017-12-15 19:28:47.428141: step 49780, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 51h:23m:04s remains)
INFO - root - 2017-12-15 19:28:54.044531: step 49790, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 51h:37m:19s remains)
INFO - root - 2017-12-15 19:29:00.717967: step 49800, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.694 sec/batch; 54h:29m:57s remains)
2017-12-15 19:29:01.248575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4093122 -5.7030597 -6.8337173 -6.8365216 -7.6359057 -8.5864859 -9.1099205 -8.5006523 -7.9953923 -7.76163 -7.0549436 -9.3105736 -10.525834 -11.04714 -9.7476435][-6.2822018 -8.2358952 -9.0356216 -9.5455914 -10.231521 -10.449742 -10.06425 -9.9942856 -10.424238 -10.333051 -9.59934 -11.144452 -11.29873 -12.159313 -10.629345][-6.6919265 -8.8247461 -11.087953 -11.228359 -11.247658 -11.301437 -10.477126 -9.9187164 -10.266146 -10.600763 -10.309872 -11.760593 -11.371279 -11.891703 -11.002256][-8.0213518 -8.9372425 -9.7312984 -10.377422 -10.729692 -8.8195934 -6.9247804 -7.4682117 -8.4446459 -7.8076558 -7.5789537 -10.350603 -10.88773 -12.026736 -11.302114][-8.5902061 -10.629715 -11.657091 -10.647739 -9.7526445 -5.7783384 -2.1993496 -3.5012846 -4.8819132 -4.7115297 -4.8632326 -6.0281048 -5.897788 -8.5591841 -9.5106163][-9.7431641 -11.231279 -11.802828 -8.835557 -5.942142 -2.4030073 1.3062668 2.9193587 2.9260011 -0.23803568 -2.6745298 -4.1147633 -4.417717 -5.8172584 -6.7365494][-9.6275043 -10.844614 -10.266056 -6.0078797 -1.9188261 2.5286241 5.243793 6.8205638 7.5682693 3.2091889 0.057179451 -1.6761808 -2.0947444 -4.0776172 -5.225688][-6.9299121 -6.845643 -7.2943974 -4.1309762 0.74665451 6.5054355 10.393169 9.330862 7.6500182 4.6605754 2.4954896 -1.1349077 -2.8102014 -5.2840562 -6.8013043][-4.4679642 -4.0619378 -3.8948996 -2.78862 -1.2238531 2.34102 5.9323525 6.3680291 5.2837844 2.0392551 -0.016317844 -4.3015394 -6.2334018 -7.97044 -8.4290848][-3.8067191 -3.7640553 -5.0698471 -3.8046885 -3.8498704 -2.7028685 0.31100035 1.2044883 -0.014983177 -1.7664368 -3.0067954 -6.7291431 -8.1753035 -10.727194 -12.819143][-6.9546337 -8.0566111 -8.50438 -8.1931753 -8.0895758 -7.1832781 -6.2378411 -4.7187881 -4.3841014 -5.305593 -6.0026584 -9.1597118 -11.016016 -12.916988 -13.144777][-10.394751 -10.399247 -10.393264 -10.460676 -11.006057 -11.148218 -10.695096 -9.8931684 -9.3967762 -8.42511 -8.7714748 -10.57747 -10.739999 -12.343765 -12.670042][-12.933331 -12.636501 -11.754838 -11.416059 -12.895197 -12.954599 -12.960995 -13.080128 -12.346928 -11.739145 -11.848383 -11.408474 -11.496141 -11.474698 -11.066629][-9.508831 -8.9052172 -10.052427 -9.9598618 -9.9182625 -10.778886 -11.742588 -11.486594 -11.348255 -10.939421 -10.531987 -9.5905285 -8.2749271 -8.3711767 -9.1061039][-5.5855641 -6.011055 -5.9878073 -5.5793223 -5.3384895 -5.8025937 -6.3105664 -7.8063941 -8.7366734 -8.351306 -7.7383289 -8.4302406 -9.0813189 -8.8161364 -8.2921638]]...]
INFO - root - 2017-12-15 19:29:07.929831: step 49810, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 52h:28m:06s remains)
INFO - root - 2017-12-15 19:29:14.545719: step 49820, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 52h:46m:50s remains)
INFO - root - 2017-12-15 19:29:21.246310: step 49830, loss = 0.12, batch loss = 0.08 (11.3 examples/sec; 0.706 sec/batch; 55h:26m:48s remains)
INFO - root - 2017-12-15 19:29:27.859609: step 49840, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 51h:47m:21s remains)
INFO - root - 2017-12-15 19:29:34.398576: step 49850, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 51h:29m:58s remains)
INFO - root - 2017-12-15 19:29:41.029630: step 49860, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 51h:49m:45s remains)
INFO - root - 2017-12-15 19:29:47.709327: step 49870, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 51h:55m:01s remains)
INFO - root - 2017-12-15 19:29:54.330189: step 49880, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 51h:22m:06s remains)
INFO - root - 2017-12-15 19:30:00.967972: step 49890, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 52h:40m:21s remains)
INFO - root - 2017-12-15 19:30:07.549990: step 49900, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 50h:58m:07s remains)
2017-12-15 19:30:08.088598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.5143862 -8.0545874 -8.470541 -6.8706565 -7.0414162 -7.0966382 -7.5294557 -8.0958776 -7.8827138 -8.0344982 -8.1113577 -7.6211772 -8.2190685 -5.4416571 -2.9350915][-8.1719456 -7.3994141 -6.2768421 -4.4228334 -4.9591942 -4.4020133 -4.7632422 -4.6532688 -5.0029454 -5.72423 -5.52014 -4.999176 -5.83819 -3.811691 -1.8691361][-4.4491858 -6.2325883 -6.313096 -3.9886971 -4.082653 -4.0039883 -3.4440696 -3.1618783 -3.4667766 -4.3417697 -4.5847607 -4.6093745 -5.35117 -4.4887953 -2.2518263][-4.2534075 -5.1330528 -5.447351 -3.9373939 -4.5429258 -4.2375259 -3.7647414 -3.6947658 -3.4708133 -4.1860609 -4.96632 -4.8231134 -5.4729009 -4.4084587 -3.0647547][-5.5955143 -6.4000278 -6.4530153 -5.6772408 -5.7363276 -3.4840353 -1.7382121 -2.4123304 -3.9994035 -4.9079323 -5.2608247 -5.6031337 -6.8153777 -5.8413935 -4.4379492][-8.6998463 -7.6928282 -6.27364 -4.6276512 -3.5254247 -0.54576588 2.4525933 2.8159127 1.7457123 -1.1965504 -4.2803507 -3.9440103 -5.2598596 -5.7197766 -5.2584934][-10.563297 -8.4524841 -6.3308392 -3.674825 -2.5507655 0.76255512 5.2861114 6.5533681 5.1708636 1.0234895 -3.2758226 -4.1294708 -6.1073728 -5.7155337 -5.0434651][-9.5868073 -8.3935928 -8.0594292 -5.3295131 -2.6520803 1.2056537 5.5139966 6.7039895 5.7351975 2.0600348 -1.5444293 -3.5757356 -6.6478209 -5.920754 -5.0753131][-7.9784751 -6.5287657 -5.9804192 -4.0823312 -2.9547555 1.0308394 5.0178266 5.7949843 4.9392295 0.40276289 -3.895649 -5.5049872 -7.5485258 -6.8550267 -5.74305][-5.9086442 -5.4919086 -5.2772627 -3.9938102 -3.3948822 -0.35868502 1.9208608 2.8536172 1.8092818 -2.3460319 -5.1538811 -6.3286438 -7.9666419 -7.9115424 -6.5284963][-8.1992168 -7.5172806 -6.8151665 -5.9175291 -5.1881142 -4.1484909 -3.1340418 -2.5267234 -3.7459917 -5.2172341 -6.7104411 -7.5691223 -9.2155418 -8.8662519 -7.966733][-11.775038 -11.852911 -11.694918 -11.289263 -11.300797 -10.21615 -9.1962061 -9.056694 -9.8275967 -10.01069 -10.283417 -9.89393 -10.712866 -9.9395828 -8.989027][-12.52343 -12.45806 -12.300997 -12.2635 -12.280472 -11.566 -11.595208 -11.562652 -11.562424 -11.116681 -10.396731 -9.7130079 -9.4060841 -9.3063507 -8.4912548][-10.220263 -10.262594 -10.063431 -9.9273014 -9.9948263 -9.2217417 -9.7274456 -9.736021 -9.556921 -9.0874863 -8.4291964 -6.8624158 -6.5310926 -6.3712664 -5.5353637][-8.1314383 -7.3563027 -6.351151 -5.77829 -5.1487851 -5.216876 -5.9424763 -5.726531 -6.0218978 -5.761384 -5.5131335 -4.892704 -5.5915222 -5.8816514 -6.7556696]]...]
INFO - root - 2017-12-15 19:30:14.710838: step 49910, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 52h:52m:32s remains)
INFO - root - 2017-12-15 19:30:21.323217: step 49920, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 51h:53m:43s remains)
INFO - root - 2017-12-15 19:30:27.932343: step 49930, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 51h:31m:29s remains)
INFO - root - 2017-12-15 19:30:34.575433: step 49940, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.675 sec/batch; 52h:59m:36s remains)
INFO - root - 2017-12-15 19:30:41.139080: step 49950, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 52h:07m:16s remains)
INFO - root - 2017-12-15 19:30:47.701781: step 49960, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 50h:42m:24s remains)
INFO - root - 2017-12-15 19:30:54.346374: step 49970, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 52h:37m:26s remains)
INFO - root - 2017-12-15 19:31:00.974342: step 49980, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 50h:42m:08s remains)
INFO - root - 2017-12-15 19:31:07.567930: step 49990, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 51h:34m:19s remains)
INFO - root - 2017-12-15 19:31:14.107983: step 50000, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 51h:20m:13s remains)
2017-12-15 19:31:14.623225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8849564 -8.6937666 -7.1653838 -5.5022469 -5.2078042 -5.6386662 -5.8104115 -5.2383223 -4.7954645 -4.7280478 -4.4119415 -6.4705315 -9.1780167 -9.4480963 -9.6799135][-9.1279945 -9.6194086 -9.85165 -8.83449 -7.9960737 -7.5757513 -7.1972208 -6.7433891 -6.02135 -5.0884151 -4.051156 -6.4070449 -9.1165714 -10.168714 -10.91075][-6.118402 -8.7823009 -9.9177914 -8.3485146 -8.4659548 -8.1316767 -7.4830904 -7.1277943 -6.3574133 -6.0470667 -5.3538647 -6.8416224 -9.0635376 -9.8526173 -10.426904][-5.3263664 -7.1261249 -7.8788815 -6.8924255 -7.5229354 -6.121058 -4.5515432 -4.9928064 -5.8493586 -5.4059834 -4.9723387 -7.4084091 -9.8631058 -10.332966 -10.457092][-6.2654724 -8.0859051 -8.2840576 -6.8855119 -5.7419953 -2.6234999 -0.77553988 -1.8792033 -3.444303 -4.3417439 -4.8562179 -7.5937657 -10.631253 -11.0312 -10.407692][-7.5751266 -8.2354937 -7.2187638 -5.4980164 -3.1774054 0.11945152 2.8279462 2.2283068 0.86664534 -1.0962167 -3.1273024 -6.002748 -8.77171 -9.8171749 -10.134705][-6.713594 -7.1607342 -7.1834126 -4.613822 -2.1515989 1.481391 4.9827247 5.7969079 4.8200097 1.6630688 -1.3024621 -4.4245353 -7.39462 -7.9451551 -7.9020748][-4.9898248 -4.9739475 -5.3164263 -2.5924714 -0.2041626 2.5287318 5.0006585 6.0324006 5.7197433 3.4413276 0.94179583 -3.098675 -6.9725132 -7.7916837 -7.8661242][-3.4371531 -3.4844339 -3.9667695 -2.4712865 0.15309095 2.6521897 4.624455 5.8830171 5.6284719 3.199861 0.33889103 -4.28155 -8.1706648 -8.9524574 -8.604167][-2.3800566 -3.610306 -4.7273607 -3.4380546 -1.2779479 1.093101 3.3978972 4.4074636 3.6502223 2.218823 -0.25855875 -5.0618119 -9.2511644 -9.935091 -9.9090986][-5.7375183 -6.2480817 -7.3276892 -6.5123677 -5.4206419 -4.3310776 -1.2337508 0.41962624 0.26828957 -1.2148662 -3.3237267 -8.1363144 -10.901515 -10.62672 -9.7549925][-10.289902 -9.8948708 -10.420696 -9.8028011 -9.0523787 -7.460104 -6.0066495 -5.2536521 -4.7362342 -5.4457273 -6.9646058 -9.6720886 -11.555948 -10.868305 -9.5282965][-12.239516 -11.784779 -11.917087 -12.020638 -12.105907 -10.718187 -9.6016178 -8.8846035 -8.4037228 -8.5780964 -9.9182625 -11.303291 -12.167916 -10.725077 -9.2639][-10.83222 -11.24453 -10.95121 -10.081369 -10.042755 -9.5873919 -8.96457 -8.8387432 -9.5591335 -10.135714 -10.16297 -10.922314 -11.343376 -10.010323 -9.170064][-8.5288887 -8.9480524 -8.7304888 -8.07241 -7.0891018 -6.4931483 -6.22528 -6.3168664 -6.7262506 -7.4264388 -7.883585 -9.4038353 -10.898769 -10.768233 -11.394175]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 19:31:22.156498: step 50010, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 50h:09m:12s remains)
INFO - root - 2017-12-15 19:31:28.748502: step 50020, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 50h:44m:27s remains)
INFO - root - 2017-12-15 19:31:35.485775: step 50030, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.681 sec/batch; 53h:26m:53s remains)
INFO - root - 2017-12-15 19:31:42.043990: step 50040, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 50h:47m:49s remains)
INFO - root - 2017-12-15 19:31:48.605272: step 50050, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 52h:16m:52s remains)
INFO - root - 2017-12-15 19:31:55.226158: step 50060, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 53h:20m:42s remains)
INFO - root - 2017-12-15 19:32:01.785002: step 50070, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 51h:00m:32s remains)
INFO - root - 2017-12-15 19:32:08.414795: step 50080, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 51h:17m:00s remains)
INFO - root - 2017-12-15 19:32:15.104551: step 50090, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 50h:10m:24s remains)
INFO - root - 2017-12-15 19:32:21.673237: step 50100, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 51h:06m:14s remains)
2017-12-15 19:32:22.165857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.60844 -5.8729081 -5.7770066 -5.070528 -5.7816377 -6.1582456 -6.2075539 -6.1364875 -6.4617667 -6.7264428 -6.5267792 -7.8360872 -8.5692892 -6.3074741 -5.5351672][-5.2109618 -5.1576138 -4.348752 -3.8765736 -4.5331807 -4.9938869 -5.0704403 -5.4009929 -6.1311741 -6.28571 -6.4571109 -8.015564 -8.5672207 -6.4041152 -6.2506819][-2.6491985 -2.5803633 -3.2449751 -2.7096226 -2.8757665 -3.5108888 -3.6677616 -3.6633568 -4.3950076 -4.8792315 -5.0605063 -7.3666782 -8.4946747 -6.7257648 -6.3466563][-3.5807302 -3.3685534 -2.9208958 -2.4420311 -2.8833122 -3.3097398 -3.1969054 -3.084698 -3.901413 -4.1525559 -4.119873 -6.4218946 -8.0878716 -7.1703181 -6.8551855][-4.334692 -4.9831471 -5.004806 -3.4134617 -2.4069998 -2.1456614 -2.0697353 -2.3355815 -3.2720385 -3.4784594 -4.0892687 -6.280107 -8.0254164 -7.3142614 -6.8038139][-7.4486456 -6.8232536 -4.6961541 -1.5640879 0.67083549 2.2142096 3.0556054 2.0389085 0.78497648 -0.37447453 -1.928863 -4.4129734 -6.6352682 -6.1840358 -6.0192313][-9.8266773 -8.4176474 -5.3222685 -1.6476688 0.94129419 3.8435616 5.9211679 4.8260055 3.6589055 2.2360587 0.37553549 -2.8210182 -5.590064 -5.0578957 -5.1041985][-10.225969 -9.1416225 -7.2166996 -3.1525638 0.43266773 3.9550242 5.7251058 5.5391793 5.3278527 3.7110267 1.5055075 -2.1383054 -5.4981346 -5.4705725 -5.008749][-8.7809162 -8.3858089 -6.7399917 -3.3167593 -0.33739567 2.1355367 3.157 3.6676583 3.5114608 1.4132342 -0.56167364 -3.9763958 -7.5225773 -7.2621222 -6.7166591][-8.6613331 -7.9409351 -6.1326551 -3.5262983 -1.6442595 -0.27590084 1.3189034 1.5457191 0.6846323 -0.42002964 -1.6024432 -4.9752288 -8.1081333 -7.4800587 -6.9752607][-9.9333 -9.4848385 -7.7198029 -6.0056939 -5.43193 -4.0111532 -2.2929916 -2.5298474 -2.7903035 -2.9465482 -3.8610387 -7.4830432 -8.8545809 -8.4900236 -7.6462049][-13.629864 -13.17868 -11.702036 -9.943903 -10.040295 -8.8077717 -7.8037605 -7.4693575 -6.8895683 -6.6424971 -6.9686246 -8.6469936 -8.9107456 -7.8057394 -6.5141597][-12.680773 -11.701265 -10.337341 -9.6605949 -9.5959244 -9.6559124 -9.6086121 -9.1233482 -8.8527088 -8.6219473 -8.2443619 -8.7632484 -8.7074833 -7.3274965 -6.4075565][-8.8246727 -8.1100273 -7.0368414 -5.9930058 -5.7702179 -6.398242 -7.4251671 -7.2910194 -7.3298621 -7.120656 -6.9259176 -6.872499 -6.4879 -5.6714005 -5.6219769][-6.5271025 -6.195735 -4.8008614 -3.4050939 -2.8030157 -3.0695515 -3.1109364 -3.4215472 -3.869873 -3.6468704 -3.957406 -4.7135057 -5.1742029 -5.6317816 -6.112287]]...]
INFO - root - 2017-12-15 19:32:28.721219: step 50110, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 50h:52m:05s remains)
INFO - root - 2017-12-15 19:32:35.257888: step 50120, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 50h:00m:43s remains)
INFO - root - 2017-12-15 19:32:41.822324: step 50130, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 51h:25m:37s remains)
INFO - root - 2017-12-15 19:32:48.428063: step 50140, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.686 sec/batch; 53h:47m:17s remains)
INFO - root - 2017-12-15 19:32:55.034337: step 50150, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.688 sec/batch; 53h:56m:12s remains)
INFO - root - 2017-12-15 19:33:01.710255: step 50160, loss = 0.16, batch loss = 0.11 (12.7 examples/sec; 0.630 sec/batch; 49h:22m:36s remains)
INFO - root - 2017-12-15 19:33:08.393605: step 50170, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 52h:24m:15s remains)
INFO - root - 2017-12-15 19:33:15.050411: step 50180, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.657 sec/batch; 51h:31m:43s remains)
INFO - root - 2017-12-15 19:33:21.688436: step 50190, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 52h:24m:46s remains)
INFO - root - 2017-12-15 19:33:28.244362: step 50200, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 50h:59m:24s remains)
2017-12-15 19:33:28.727927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.46477 -9.6750927 -9.8625984 -9.8627319 -10.019769 -10.330312 -10.664249 -10.193549 -8.9492245 -8.2965746 -7.6031623 -7.5096154 -9.8744488 -8.761611 -6.6233277][-10.065624 -11.078333 -10.851912 -9.3193893 -9.0562 -9.633297 -10.132467 -10.537474 -10.532301 -9.6012745 -8.6438522 -8.1670094 -9.6089516 -9.1855192 -8.0834188][-6.3188577 -8.34062 -9.6076374 -8.7179737 -7.6724067 -7.657021 -8.0517921 -9.0351992 -9.5176477 -9.6716061 -9.7849541 -9.3220615 -10.382821 -9.423254 -8.1100168][-4.8620968 -5.9793758 -6.19769 -5.3866091 -5.2581792 -5.5306454 -6.0508533 -6.4437518 -7.2363219 -7.448976 -8.0661707 -8.822504 -11.008994 -9.7816372 -8.4210253][-6.0995359 -7.1867156 -6.4088197 -4.1868348 -2.669992 -1.3600135 -1.553658 -3.7347441 -5.6196966 -6.3429017 -6.709136 -7.5837016 -10.364029 -9.4379673 -8.1528988][-8.3437662 -7.6962347 -6.0536957 -3.7126007 -0.99508715 1.9608355 2.884501 1.017386 -1.9143469 -4.2114515 -5.3575473 -5.7783976 -8.2143974 -7.5186405 -5.9562654][-9.7037153 -8.37402 -5.2538471 -2.3381429 -0.31885242 2.6861691 5.0266089 4.8641763 2.6029344 -1.5300388 -4.27919 -4.86046 -6.9188004 -6.203505 -5.0924864][-9.2859144 -7.8118215 -5.2735848 -0.64500666 2.087019 4.003675 4.9667296 5.278379 4.4780469 0.88740778 -2.196697 -4.2716541 -7.7048941 -6.7882252 -4.981534][-6.9464617 -5.8697076 -3.1831088 -0.29429674 1.9382467 4.1941438 4.3828273 4.2263923 3.3086019 0.89557171 -1.4525619 -4.1610317 -7.87362 -8.2980433 -7.1645465][-4.1579084 -3.3109925 -1.9708252 0.45170307 1.7969613 2.2726331 2.284698 1.7786722 0.27692175 -1.2024193 -2.7161682 -4.3131113 -7.8630352 -8.455368 -8.4630928][-6.4424667 -5.1695814 -3.7960811 -1.9817498 -0.86677647 -1.0015311 -1.0124764 -1.6811643 -3.2412004 -4.9294443 -6.701467 -7.7647266 -9.51434 -9.2313089 -8.1166792][-9.6230354 -8.6107912 -6.6385436 -5.1428027 -5.1047006 -5.4516068 -5.6755023 -5.9070129 -6.2718887 -8.0216494 -9.7220583 -11.344603 -12.176161 -11.416107 -9.2363892][-11.136781 -10.501341 -9.4124079 -7.5562353 -6.528295 -6.8594117 -7.9254818 -9.1260481 -10.036528 -10.407482 -10.495751 -10.587332 -10.653732 -9.3005924 -7.0993681][-8.9913988 -8.4475269 -7.9023781 -7.2492714 -6.3561192 -6.751318 -7.8126397 -8.1580944 -9.1048832 -9.6622753 -9.8529568 -8.7687454 -7.9718566 -6.0568581 -5.0096455][-6.4062195 -6.9384871 -7.1056242 -6.1662259 -5.4128442 -5.3575497 -5.4935369 -6.077558 -6.6267505 -6.7055645 -7.1635504 -7.3116164 -7.5268388 -5.7851768 -4.7213]]...]
INFO - root - 2017-12-15 19:33:35.339777: step 50210, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 52h:31m:06s remains)
INFO - root - 2017-12-15 19:33:41.945593: step 50220, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 52h:33m:36s remains)
INFO - root - 2017-12-15 19:33:48.504286: step 50230, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 49h:57m:03s remains)
INFO - root - 2017-12-15 19:33:55.086094: step 50240, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 52h:05m:09s remains)
INFO - root - 2017-12-15 19:34:01.673646: step 50250, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.635 sec/batch; 49h:48m:07s remains)
INFO - root - 2017-12-15 19:34:08.310165: step 50260, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.685 sec/batch; 53h:43m:23s remains)
INFO - root - 2017-12-15 19:34:14.936592: step 50270, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 51h:19m:04s remains)
INFO - root - 2017-12-15 19:34:21.581693: step 50280, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.685 sec/batch; 53h:40m:48s remains)
INFO - root - 2017-12-15 19:34:28.095106: step 50290, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 50h:40m:51s remains)
INFO - root - 2017-12-15 19:34:34.639049: step 50300, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 50h:11m:19s remains)
2017-12-15 19:34:35.235085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9387965 -3.7965891 -2.9933813 -2.2360747 -2.3312552 -2.0950868 -2.1281283 -1.8380291 -2.0487523 -2.3735888 -3.5816858 -7.5532026 -9.7660046 -10.343791 -9.1968431][-5.4370222 -4.9260249 -4.6256237 -3.5881214 -2.8168733 -2.4781854 -2.285758 -2.4561126 -2.9565949 -3.5276318 -4.9321537 -9.1167316 -11.010855 -11.740838 -10.54442][-3.9462156 -4.3608255 -5.1161394 -3.9106665 -3.560225 -3.7799473 -3.5973191 -3.0736597 -2.8078923 -4.2372565 -5.8553476 -10.255054 -12.044071 -12.471279 -11.372996][-6.1226969 -5.7378349 -5.5434647 -4.7766376 -5.335947 -4.7507725 -4.2957134 -4.7349582 -5.3426561 -5.1453562 -5.8378444 -10.451843 -12.157059 -12.738127 -10.528481][-7.6426835 -8.1770344 -7.8135853 -6.2803926 -5.0586615 -2.6746356 -1.9473855 -3.2079253 -5.5024953 -5.78986 -6.0638895 -9.3973541 -11.005694 -12.702682 -10.977297][-8.4885674 -8.5389957 -6.8697443 -4.2060351 -1.9702885 0.32051849 2.3218904 2.4578891 -0.0545516 -3.3641844 -6.2184153 -9.0555763 -9.885416 -11.646263 -10.924723][-9.023035 -8.1664581 -6.6781125 -2.4885855 1.2344494 3.9049726 6.3161407 5.4432378 3.5558896 0.070424557 -4.2585516 -8.5703 -10.152252 -10.751468 -9.98247][-9.04385 -7.3376446 -5.2856731 -1.4200563 2.2422128 6.127605 8.9310169 7.233377 5.3746362 1.9114723 -1.7002358 -7.2640009 -9.6080093 -10.218462 -8.9321489][-7.3301868 -6.2854724 -5.4055934 -1.6807828 0.64272833 3.6940379 6.4981542 6.2799859 5.1672263 0.80350208 -2.0369005 -6.8722925 -9.2108583 -10.501479 -8.9417372][-6.5690403 -5.402802 -5.0305548 -2.9578748 -2.4712009 -0.72414875 1.0115409 2.0967364 2.1167369 -0.85138321 -3.2751365 -7.9075165 -9.5419188 -10.351851 -9.6063623][-8.3327856 -7.8299685 -7.0891142 -6.1965032 -6.0656438 -4.2478685 -3.4568989 -3.5320673 -3.7262301 -4.3652153 -5.322855 -10.035479 -10.647738 -9.8863888 -8.7930079][-12.912334 -11.753468 -9.3321953 -8.6924458 -8.3830652 -7.6476383 -8.3718224 -8.8574429 -8.895999 -9.1072111 -9.5310869 -10.973647 -9.7359982 -9.1370621 -8.2092638][-12.60154 -11.308017 -9.6761122 -8.6926613 -8.0162735 -8.2808571 -9.0871429 -9.6278944 -10.826603 -10.947983 -10.800562 -10.517532 -9.9370518 -8.1948156 -6.7664909][-9.3743916 -8.0948172 -7.5198417 -6.79963 -5.7691627 -6.9385347 -7.5222692 -8.505949 -8.87463 -9.02539 -9.585825 -8.122633 -7.4571929 -7.0943241 -6.6752591][-5.42998 -4.8727093 -4.4170885 -4.5850677 -5.8230338 -6.3232865 -6.1424065 -7.2223368 -7.0329657 -6.7777648 -6.522644 -7.3599768 -7.6098943 -7.9879436 -9.0246363]]...]
INFO - root - 2017-12-15 19:34:41.851893: step 50310, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.690 sec/batch; 54h:03m:41s remains)
INFO - root - 2017-12-15 19:34:48.389965: step 50320, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 50h:41m:34s remains)
INFO - root - 2017-12-15 19:34:55.098897: step 50330, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 50h:56m:07s remains)
INFO - root - 2017-12-15 19:35:01.648054: step 50340, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 51h:42m:21s remains)
INFO - root - 2017-12-15 19:35:08.239396: step 50350, loss = 0.19, batch loss = 0.14 (11.7 examples/sec; 0.684 sec/batch; 53h:34m:18s remains)
INFO - root - 2017-12-15 19:35:14.837766: step 50360, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 52h:18m:41s remains)
INFO - root - 2017-12-15 19:35:21.385153: step 50370, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 51h:50m:17s remains)
INFO - root - 2017-12-15 19:35:27.903921: step 50380, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 51h:38m:06s remains)
INFO - root - 2017-12-15 19:35:34.416272: step 50390, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.634 sec/batch; 49h:41m:21s remains)
INFO - root - 2017-12-15 19:35:41.069125: step 50400, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 52h:23m:35s remains)
2017-12-15 19:35:41.588545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4608893 -6.2629671 -6.1133609 -5.7695684 -6.5755835 -8.1165257 -9.5585327 -9.4945555 -9.0055275 -8.2961025 -7.5091553 -10.800343 -14.451172 -15.282009 -12.537531][-9.3697882 -9.81687 -9.4444351 -8.05531 -8.1468582 -9.8802776 -11.537922 -13.066374 -12.757088 -11.024757 -10.306156 -13.784678 -16.823915 -17.473999 -14.87635][-8.60309 -10.369411 -10.615358 -9.3688726 -9.1866693 -10.194847 -11.572634 -12.914739 -13.363752 -12.644424 -11.87426 -14.815292 -17.567181 -18.205019 -15.828993][-9.161911 -10.646656 -10.330141 -8.82568 -9.2030344 -9.5070219 -10.208841 -11.806145 -12.040258 -10.963318 -10.545323 -14.074993 -17.2548 -18.052454 -16.169712][-11.063096 -12.383006 -12.524078 -9.2752581 -6.9978614 -5.4074683 -5.5660534 -7.7878809 -9.3750687 -8.6078176 -9.0784216 -12.35368 -15.010231 -16.69869 -15.29808][-14.111702 -14.205824 -12.803078 -9.0590591 -5.0786691 -1.4961119 0.75714254 0.24508238 -1.0207605 -3.5687919 -6.7789249 -9.6744156 -12.933066 -14.693674 -12.676428][-15.379395 -15.582365 -13.273131 -7.7374487 -3.2351196 0.71686506 3.9475207 5.2647557 5.3126597 1.6328721 -2.7167299 -7.408 -12.524826 -13.657843 -11.608829][-14.305773 -15.043201 -13.030661 -7.2190857 -2.2092645 3.132483 6.985189 5.9012628 6.016232 4.3198609 0.40687752 -6.5539021 -13.416098 -15.17338 -13.03442][-11.726994 -12.307503 -11.570134 -7.9134769 -3.7747657 2.2183304 5.8867297 6.1237645 5.7626424 2.2583275 -1.3529882 -8.0415869 -14.81468 -16.983322 -15.373863][-10.516386 -10.406275 -10.805119 -8.4496679 -4.8504472 -0.35583639 2.6026969 3.5390325 2.5046754 -0.67023373 -3.5745225 -10.012121 -15.942158 -18.41412 -17.413181][-10.411881 -11.580634 -11.392626 -9.4464788 -7.6516552 -4.8990746 -1.8774683 -1.0311556 -2.0623367 -4.514636 -7.5641618 -13.417103 -16.169912 -18.138859 -17.868414][-14.543396 -14.431002 -13.595646 -12.337478 -11.494373 -9.4692707 -8.8572617 -9.3303785 -9.5246449 -11.004456 -13.435443 -16.541258 -17.967567 -19.002665 -16.766144][-15.02734 -14.858114 -14.190815 -13.85741 -13.205248 -12.641556 -12.0762 -12.18704 -12.764561 -13.303461 -14.126348 -16.388655 -16.875582 -17.08968 -14.986856][-13.41917 -12.636997 -12.332294 -11.991465 -11.344974 -11.270243 -11.309741 -11.25038 -11.665152 -11.872976 -12.831669 -12.487787 -12.563879 -13.252155 -11.715538][-10.440603 -10.19327 -9.1779413 -7.9244328 -6.8434339 -6.8772235 -7.3342986 -8.36365 -8.8585443 -8.7816038 -9.2470284 -11.198513 -12.313292 -10.924042 -10.218441]]...]
INFO - root - 2017-12-15 19:35:48.059607: step 50410, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 50h:35m:44s remains)
INFO - root - 2017-12-15 19:35:54.638339: step 50420, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 52h:14m:06s remains)
INFO - root - 2017-12-15 19:36:01.231242: step 50430, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 51h:48m:06s remains)
INFO - root - 2017-12-15 19:36:07.763848: step 50440, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 51h:16m:01s remains)
INFO - root - 2017-12-15 19:36:14.333990: step 50450, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 51h:37m:49s remains)
INFO - root - 2017-12-15 19:36:20.909697: step 50460, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 52h:43m:39s remains)
INFO - root - 2017-12-15 19:36:27.498474: step 50470, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 52h:21m:22s remains)
INFO - root - 2017-12-15 19:36:34.087494: step 50480, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 53h:38m:37s remains)
INFO - root - 2017-12-15 19:36:40.645855: step 50490, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 49h:56m:27s remains)
INFO - root - 2017-12-15 19:36:47.176412: step 50500, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 50h:59m:21s remains)
2017-12-15 19:36:47.761365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1754608 -5.0540018 -5.0305367 -5.4100795 -6.9124002 -8.4482441 -8.26058 -7.6502419 -6.863574 -6.9695435 -6.8867083 -6.2486043 -6.5366459 -7.219348 -7.26532][-4.4813209 -4.9760065 -5.9544172 -6.2250328 -6.5633073 -6.7055531 -6.3767824 -5.1598883 -5.2078385 -4.9067378 -4.32883 -5.1702385 -6.4774976 -7.403882 -7.9536023][-4.8281651 -5.034759 -5.8048329 -5.4841127 -6.871398 -7.4862 -6.6702547 -5.4072533 -3.8090572 -2.9263005 -2.20854 -3.4488266 -5.5961461 -7.8317423 -8.099472][-4.541533 -5.9666171 -6.5462909 -7.2268791 -8.430191 -8.0927858 -7.0273347 -4.8107457 -2.3920505 -1.6759758 -2.2953007 -3.1885345 -4.6181812 -5.8366704 -6.4523764][-5.960227 -6.0151343 -7.0415106 -7.7044153 -7.3388863 -5.853601 -4.0917616 -2.4395854 -1.5525546 -0.62244368 -0.94225168 -2.2866662 -3.9328828 -5.4126539 -6.059526][-6.1968665 -5.5764923 -6.0636444 -5.9872775 -4.301652 -0.075689316 2.8940654 2.9862056 1.5050468 -0.39900255 -1.8495522 -1.9604318 -4.0224104 -5.4044766 -6.1926246][-5.1495647 -4.9394855 -4.7314143 -3.2634327 -0.22320652 2.5591483 6.1690059 7.5890622 5.5239511 1.2475123 -2.1910324 -2.9832895 -4.327733 -5.8773661 -6.35277][-6.1074572 -4.867013 -4.2704306 -1.9264398 -0.6916151 2.4527974 6.6342273 8.3007336 7.515697 3.5787368 -0.97114706 -3.7876697 -6.0655951 -5.984519 -5.8208179][-5.6200142 -3.7865939 -2.7425921 -1.8752911 -0.97570419 1.3967676 4.0695014 5.3247018 4.8836303 2.001265 -1.2894235 -4.3387079 -7.1576133 -7.9993658 -7.6276073][-5.3117433 -4.731235 -4.42505 -2.7645538 -1.7874422 0.14432669 1.6874352 2.8330779 2.3034425 -1.2857838 -4.5113282 -7.09523 -10.296726 -11.189083 -10.290832][-6.56423 -6.3990455 -6.2115855 -5.0305328 -5.0071549 -3.7949669 -2.4416082 -2.3335578 -3.310899 -4.6633444 -7.0519104 -9.2661781 -11.480804 -12.833792 -12.353958][-11.814393 -10.926376 -10.164348 -9.35339 -9.4708157 -8.6335077 -8.1075048 -8.2870893 -8.6664562 -9.8162117 -10.606461 -10.075875 -10.600991 -11.565924 -11.470225][-13.664408 -12.722083 -11.655254 -10.400408 -10.702774 -10.903246 -11.406813 -11.17478 -10.585999 -10.369703 -10.295546 -10.344306 -10.351589 -10.124347 -9.4123592][-11.918243 -11.971346 -11.308479 -10.017033 -9.7531672 -9.7242985 -10.279802 -10.495926 -10.375095 -9.8890648 -9.1041718 -8.01559 -7.6594763 -7.7688513 -7.4337049][-8.26213 -8.3098555 -8.1232567 -7.815979 -7.6104941 -7.49776 -7.3694706 -7.7973614 -8.525 -8.7631311 -8.1391506 -7.8057947 -7.5335708 -7.4131145 -7.6760941]]...]
INFO - root - 2017-12-15 19:36:54.375052: step 50510, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.657 sec/batch; 51h:26m:53s remains)
INFO - root - 2017-12-15 19:37:01.023204: step 50520, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 51h:20m:46s remains)
INFO - root - 2017-12-15 19:37:07.600765: step 50530, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 52h:28m:49s remains)
INFO - root - 2017-12-15 19:37:14.217550: step 50540, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 50h:03m:31s remains)
INFO - root - 2017-12-15 19:37:20.850022: step 50550, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 51h:06m:41s remains)
INFO - root - 2017-12-15 19:37:27.392367: step 50560, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 50h:18m:18s remains)
INFO - root - 2017-12-15 19:37:34.003947: step 50570, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 50h:53m:46s remains)
INFO - root - 2017-12-15 19:37:40.676340: step 50580, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 52h:47m:37s remains)
INFO - root - 2017-12-15 19:37:47.295135: step 50590, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 52h:27m:31s remains)
INFO - root - 2017-12-15 19:37:53.917845: step 50600, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 52h:36m:19s remains)
2017-12-15 19:37:54.432210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4242535 -5.2570305 -5.1079359 -4.930975 -6.5853586 -7.2482414 -7.8333254 -8.05397 -7.9254332 -8.2851524 -7.2594032 -7.03532 -6.9786239 -8.0163279 -6.8791685][-6.7805681 -6.5492635 -6.8672795 -7.2595983 -8.49473 -9.8915176 -10.610105 -10.664362 -10.69561 -10.539652 -9.4543362 -9.1499758 -9.1015806 -9.4610662 -7.1173344][-5.9255028 -6.5472765 -8.3057566 -9.4829693 -10.458169 -11.557337 -11.774612 -11.074697 -10.461948 -10.525756 -9.6856928 -9.477005 -9.2100353 -10.449148 -8.7086306][-6.3396068 -6.7231236 -8.2722292 -9.8219891 -11.270141 -11.270267 -9.88714 -9.1813173 -9.1051817 -9.0749207 -8.7963867 -9.5567636 -10.306803 -11.96229 -9.8374825][-7.7086787 -8.9326754 -10.858274 -10.502084 -9.2623215 -6.7728224 -4.6986561 -4.8316221 -5.7982774 -6.5824437 -7.2539592 -8.6117849 -10.014446 -13.006281 -11.446251][-9.49592 -9.6631594 -10.396181 -9.1385059 -7.8755188 -2.13155 4.1612582 3.5973134 0.66763973 -2.1850975 -4.72608 -6.5584474 -8.9472218 -12.75145 -11.913113][-10.440007 -10.138023 -10.014855 -7.6062412 -4.1284018 2.4003968 8.5121822 9.609436 8.2338676 1.0158367 -4.9121094 -6.5905786 -8.4822855 -11.401657 -10.180367][-11.089664 -10.303944 -9.5922995 -7.5790558 -3.2664213 4.5599523 10.526033 11.431059 11.319729 4.8977675 -1.5173349 -5.95058 -9.826519 -11.694773 -9.57334][-9.4675808 -8.8452148 -8.4302073 -7.1019244 -3.4484167 3.1243653 8.324194 9.5510426 8.1086292 2.5808253 -1.3307047 -5.4687567 -9.8720455 -12.089431 -10.230196][-9.2335882 -8.6308241 -8.3818531 -6.9896154 -5.1203156 -0.29523659 3.8860888 6.32496 5.4646964 -0.17495584 -4.4997253 -7.2811437 -10.372976 -13.702004 -13.551475][-10.882906 -10.60532 -10.626623 -8.5636454 -7.4624805 -5.6755457 -4.53838 -2.0701082 -1.9254446 -4.0901289 -6.6974978 -10.464918 -12.306969 -12.78742 -10.993174][-15.527592 -13.978325 -12.575983 -11.163326 -9.9019184 -9.3862267 -9.5213079 -8.2406921 -8.7296991 -9.7949982 -10.975412 -11.288117 -11.679249 -11.997868 -8.6740866][-15.483896 -12.522575 -10.140857 -9.4965534 -8.7294273 -8.1395855 -7.6128531 -7.6818981 -8.8907576 -8.841774 -8.3406868 -9.1133852 -10.776945 -9.656394 -6.3845081][-12.233828 -11.015808 -9.1595945 -7.9877396 -7.3349018 -7.6905279 -7.1113672 -6.191083 -6.1748385 -6.5810461 -6.7371459 -5.0938396 -5.0930042 -5.0859032 -4.0933342][-7.046988 -6.1477623 -4.76907 -3.47523 -3.7722542 -4.0962315 -4.0646334 -3.7921491 -3.5681136 -3.5959921 -3.6665735 -4.1723633 -5.53143 -5.34015 -5.1496539]]...]
INFO - root - 2017-12-15 19:38:00.978141: step 50610, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 50h:45m:42s remains)
INFO - root - 2017-12-15 19:38:07.580539: step 50620, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 50h:28m:11s remains)
INFO - root - 2017-12-15 19:38:14.134555: step 50630, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.663 sec/batch; 51h:55m:14s remains)
INFO - root - 2017-12-15 19:38:20.764222: step 50640, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 52h:07m:48s remains)
INFO - root - 2017-12-15 19:38:27.310900: step 50650, loss = 0.21, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 50h:43m:37s remains)
INFO - root - 2017-12-15 19:38:33.895889: step 50660, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 52h:27m:12s remains)
INFO - root - 2017-12-15 19:38:40.437684: step 50670, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 49h:36m:45s remains)
INFO - root - 2017-12-15 19:38:47.056219: step 50680, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 51h:50m:17s remains)
INFO - root - 2017-12-15 19:38:53.667112: step 50690, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.631 sec/batch; 49h:21m:37s remains)
INFO - root - 2017-12-15 19:39:00.240276: step 50700, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 51h:46m:56s remains)
2017-12-15 19:39:00.757056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2698159 -6.7298608 -7.6150627 -8.2928743 -9.3108206 -10.682493 -11.55686 -10.353544 -9.6778822 -9.8180523 -9.032052 -9.627821 -11.500971 -11.117384 -12.278187][-8.2634153 -9.0928154 -9.374855 -10.09763 -10.369909 -11.006325 -11.669282 -12.617465 -13.294506 -12.327728 -11.066493 -11.763033 -12.974473 -12.323075 -12.882282][-5.639359 -7.6757369 -10.005605 -9.60275 -8.8682146 -10.802691 -12.118201 -11.775584 -11.557753 -12.243298 -12.270939 -11.825098 -12.658592 -12.832521 -13.280998][-8.5001888 -10.299229 -10.853762 -9.4284706 -9.1742573 -9.39377 -8.7411213 -11.290724 -12.878891 -11.183073 -9.8611431 -12.085817 -14.183712 -12.747789 -12.032124][-9.2387943 -12.380761 -13.543379 -11.363842 -8.1844425 -4.2759728 -2.3858714 -6.73049 -11.424675 -11.718983 -11.633209 -11.507004 -12.12764 -12.722622 -13.470472][-12.369162 -14.021881 -13.340067 -10.916918 -7.3156395 -1.4885368 3.8733716 1.4494224 -2.3115363 -7.5143609 -12.484016 -11.626763 -11.939393 -11.554974 -12.524619][-15.615498 -15.100769 -13.063359 -8.8429708 -3.4304426 2.0017452 6.08789 6.2599969 5.0308433 -2.0623488 -9.276659 -11.035838 -12.970026 -11.552363 -12.106785][-15.284964 -14.374571 -12.551979 -8.20101 -1.3738427 5.7927728 9.8384037 6.855701 4.6442924 -0.18365574 -6.3265848 -9.9800434 -12.588159 -12.384937 -13.727388][-12.503624 -11.888804 -12.564974 -8.763937 -2.6784506 3.1299787 7.7418027 8.3702831 4.6714826 -2.733376 -7.6603408 -10.549642 -14.626215 -14.153147 -14.059013][-10.8559 -10.604404 -10.383726 -7.0313282 -4.0566907 -1.5434494 2.8358378 3.9440093 1.5084839 -3.0635884 -8.3050823 -12.315956 -15.206558 -14.417278 -15.918428][-11.118156 -12.571033 -12.591413 -10.53941 -8.6372194 -5.9609823 -2.4821041 -3.125056 -4.1584282 -5.4808207 -8.8432455 -13.131491 -15.399996 -15.440174 -14.986752][-16.712364 -16.121946 -15.851543 -15.368118 -13.97559 -11.721811 -10.650284 -10.878398 -10.590645 -11.329095 -12.990509 -13.823578 -14.379414 -14.386496 -14.100615][-15.871441 -13.980282 -14.111914 -16.122095 -15.483488 -13.882586 -12.45858 -12.280031 -13.249663 -12.555402 -12.205122 -13.930498 -14.220602 -12.336092 -11.424815][-13.594463 -12.685837 -11.909403 -10.482389 -10.688046 -11.910524 -12.311301 -11.610207 -11.127254 -11.097973 -11.848402 -11.784756 -11.181847 -9.5870752 -9.807972][-9.8784161 -8.640831 -6.8869476 -6.2380033 -6.2223845 -6.1766949 -6.5719109 -8.1014032 -9.0923662 -8.5124435 -8.6588106 -10.533647 -10.617229 -10.377471 -11.213154]]...]
INFO - root - 2017-12-15 19:39:07.365705: step 50710, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 50h:50m:58s remains)
INFO - root - 2017-12-15 19:39:13.925828: step 50720, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 51h:34m:03s remains)
INFO - root - 2017-12-15 19:39:20.500087: step 50730, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 51h:35m:45s remains)
INFO - root - 2017-12-15 19:39:27.081348: step 50740, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 51h:44m:59s remains)
INFO - root - 2017-12-15 19:39:33.666439: step 50750, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 51h:09m:32s remains)
INFO - root - 2017-12-15 19:39:40.256231: step 50760, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 52h:19m:11s remains)
INFO - root - 2017-12-15 19:39:46.932888: step 50770, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 51h:47m:24s remains)
INFO - root - 2017-12-15 19:39:53.564900: step 50780, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 51h:26m:28s remains)
INFO - root - 2017-12-15 19:40:00.127283: step 50790, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 50h:25m:16s remains)
INFO - root - 2017-12-15 19:40:06.735779: step 50800, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 51h:22m:05s remains)
2017-12-15 19:40:07.268643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8938193 -5.6749358 -5.3411808 -6.0433474 -7.6207027 -9.1575451 -10.461023 -9.4852257 -7.1171913 -5.6533008 -5.0844092 -7.7970037 -11.007574 -11.696018 -9.4709911][-6.4585571 -5.606132 -4.6936693 -5.791347 -7.6603642 -9.7232361 -10.993551 -11.0068 -9.79964 -7.3334351 -5.9019046 -9.1483994 -11.928469 -13.127241 -11.414167][-4.9370408 -5.041183 -4.8082552 -5.0341706 -5.8844848 -8.030899 -9.2264633 -9.5136642 -9.4254541 -8.3782959 -7.32512 -9.2732487 -10.90493 -12.303936 -11.473265][-5.8050666 -5.5535975 -5.1992245 -4.9198875 -4.95323 -5.6013708 -5.920526 -7.2384348 -8.1926088 -7.33754 -6.2302585 -8.949791 -11.433659 -12.668463 -11.76739][-7.2838392 -8.2168159 -8.0754585 -5.7449884 -3.7293992 -2.449074 -1.6795707 -4.52637 -7.1826797 -7.2268853 -6.6881385 -8.7571974 -9.889637 -10.520897 -9.5616589][-8.5110359 -8.1003866 -6.6885548 -3.8251944 -1.1943445 1.6764331 3.8130383 1.425879 -0.62008 -3.4798694 -6.3802824 -7.8104105 -9.1389351 -9.5214834 -8.0932083][-10.435743 -8.8161392 -6.4395795 -2.5129364 0.86492205 4.2213092 7.3694081 6.6514955 5.1005731 0.61720896 -4.0985718 -7.5032034 -9.42584 -9.3126259 -8.08652][-10.744985 -9.520998 -7.1589842 -2.9926205 0.55019665 5.0026126 8.561533 7.9441819 6.9273238 3.62083 -0.77684259 -6.2542315 -10.518814 -10.916402 -9.5280094][-9.3303814 -8.5683842 -7.1876221 -4.1493864 -0.979187 2.6294932 4.918386 5.1797814 5.5706019 2.3801866 -1.5679264 -6.9733381 -11.461687 -12.463733 -11.21264][-8.7714977 -7.7482071 -7.1508346 -3.8324318 -1.413115 -1.0802379 -0.10259962 1.368928 2.1511526 -0.48213196 -3.1003582 -8.2256136 -12.541735 -13.584988 -12.885984][-11.306673 -10.585934 -9.7804508 -7.6581535 -6.7110014 -5.8359308 -4.9183307 -4.9426923 -4.9025364 -6.114574 -7.1125641 -11.992842 -14.729689 -14.506737 -12.7251][-16.081408 -14.720877 -14.03796 -12.671952 -12.116086 -11.533711 -11.264154 -11.656265 -11.428265 -11.332659 -11.114934 -13.72823 -14.519077 -14.463331 -12.950903][-16.796497 -15.590382 -14.988663 -14.578386 -13.855755 -12.885101 -13.058909 -13.435944 -13.958656 -13.219118 -12.157874 -13.219267 -13.314884 -12.64916 -9.9436531][-13.86071 -12.316959 -11.751923 -11.750286 -11.693335 -12.589008 -13.202885 -12.068559 -11.775936 -12.001507 -11.865879 -11.450645 -10.733139 -10.193142 -8.2426109][-9.4136934 -7.5657997 -6.4861917 -5.8970747 -6.1431761 -7.000515 -7.2324548 -7.8801041 -8.7571068 -8.076273 -7.7230139 -9.16267 -9.6467152 -8.9101849 -7.2105503]]...]
INFO - root - 2017-12-15 19:40:13.933160: step 50810, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.677 sec/batch; 52h:59m:36s remains)
INFO - root - 2017-12-15 19:40:20.481667: step 50820, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 51h:17m:18s remains)
INFO - root - 2017-12-15 19:40:27.049508: step 50830, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 50h:18m:17s remains)
INFO - root - 2017-12-15 19:40:33.685160: step 50840, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 50h:28m:14s remains)
INFO - root - 2017-12-15 19:40:40.350852: step 50850, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 51h:42m:47s remains)
INFO - root - 2017-12-15 19:40:46.971072: step 50860, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 52h:04m:57s remains)
INFO - root - 2017-12-15 19:40:53.582013: step 50870, loss = 0.19, batch loss = 0.15 (11.8 examples/sec; 0.679 sec/batch; 53h:07m:28s remains)
INFO - root - 2017-12-15 19:41:00.189639: step 50880, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 53h:37m:11s remains)
INFO - root - 2017-12-15 19:41:06.788111: step 50890, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 50h:12m:50s remains)
INFO - root - 2017-12-15 19:41:13.380683: step 50900, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 50h:45m:09s remains)
2017-12-15 19:41:13.967813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3089209 -4.662106 -6.0035448 -6.7072425 -7.6007361 -8.2577763 -8.3143291 -7.5602336 -7.2491369 -7.2560821 -6.796381 -8.4759312 -11.550688 -11.627194 -10.279202][-4.4987659 -6.8038607 -8.7028008 -9.4136829 -9.8480339 -9.7748852 -9.1006327 -9.1224689 -9.7166529 -9.6423683 -8.7427578 -10.079675 -12.286209 -12.349752 -11.143288][-5.24896 -7.9525633 -10.062248 -10.422268 -11.156162 -10.578876 -9.4356508 -8.892458 -9.189537 -9.5127268 -9.3924694 -10.623291 -12.379356 -12.334129 -11.417961][-6.7794981 -8.3781433 -9.7772579 -10.500997 -10.596903 -8.33189 -6.2238755 -7.0311003 -8.1432095 -7.24845 -6.8808513 -9.3911171 -11.644018 -11.718464 -11.501766][-7.8928776 -9.9052143 -11.743108 -11.419956 -9.984025 -5.5129986 -1.6412973 -3.1717341 -5.5070424 -5.3809385 -4.8593187 -5.6119509 -7.5465622 -9.2542439 -10.092262][-8.4871922 -10.310068 -11.204171 -10.249354 -6.977591 -2.4120429 1.8056359 3.4448237 2.165648 -1.3024187 -3.5639126 -4.3381286 -5.8012753 -6.2552757 -6.8501167][-8.2342958 -9.794241 -9.5129395 -6.9037309 -3.0455868 1.3125467 5.1977496 7.7427182 7.4638038 2.5043931 -1.5951591 -2.717171 -3.7212424 -4.3929 -5.8853745][-5.9415784 -6.8942719 -6.8846684 -4.2566118 0.28544903 5.8693509 9.602005 9.8968506 8.5648518 5.1711717 2.1908274 -1.9170828 -4.9636488 -5.864274 -6.7492328][-3.6605265 -4.0225306 -4.6918535 -3.4549997 -1.0354242 2.399653 6.0010018 7.6534314 6.7621837 3.4912686 0.62068081 -3.7779555 -7.690362 -8.6091366 -8.8227329][-2.0118365 -2.7140696 -5.0645547 -4.1704955 -3.1965668 -1.3327594 1.4240851 2.3441091 1.5827169 0.4326272 -0.58965445 -4.8747034 -9.571269 -11.700712 -13.385127][-5.2150693 -6.4192829 -7.931325 -8.048996 -7.4381022 -6.1727929 -5.0647039 -3.8660634 -4.1116533 -4.809309 -5.3545489 -8.2769661 -11.197512 -13.274641 -13.941547][-10.162334 -10.17062 -10.974495 -11.384518 -12.121836 -10.980133 -9.6855688 -9.8885832 -9.5543442 -8.85463 -8.85173 -10.109758 -11.652838 -12.648777 -13.269142][-11.592833 -11.88759 -11.353251 -11.863329 -13.521392 -13.572907 -13.366928 -12.719844 -11.946466 -11.537201 -11.407692 -11.16259 -11.13232 -10.865181 -10.558167][-10.426587 -9.3757706 -10.131907 -10.608205 -10.58502 -12.076941 -12.851431 -12.504578 -11.973568 -11.324675 -11.130353 -9.9781036 -9.4090786 -9.3432713 -9.7139444][-6.5354652 -6.9848967 -7.0349865 -6.5511422 -7.0214081 -8.17377 -9.2262669 -10.005064 -9.9227066 -9.4281092 -8.6441689 -9.38166 -9.798975 -9.3455458 -9.1437283]]...]
INFO - root - 2017-12-15 19:41:20.564094: step 50910, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 51h:04m:12s remains)
INFO - root - 2017-12-15 19:41:27.174870: step 50920, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 50h:28m:54s remains)
INFO - root - 2017-12-15 19:41:33.786078: step 50930, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 51h:07m:33s remains)
INFO - root - 2017-12-15 19:41:40.357960: step 50940, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 50h:23m:00s remains)
INFO - root - 2017-12-15 19:41:46.930148: step 50950, loss = 0.21, batch loss = 0.16 (11.4 examples/sec; 0.700 sec/batch; 54h:46m:40s remains)
INFO - root - 2017-12-15 19:41:53.601837: step 50960, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.686 sec/batch; 53h:36m:35s remains)
INFO - root - 2017-12-15 19:42:00.172444: step 50970, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 50h:08m:47s remains)
INFO - root - 2017-12-15 19:42:06.707386: step 50980, loss = 0.18, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 49h:33m:10s remains)
INFO - root - 2017-12-15 19:42:13.217638: step 50990, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 51h:24m:02s remains)
INFO - root - 2017-12-15 19:42:19.825502: step 51000, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 52h:10m:35s remains)
2017-12-15 19:42:20.336496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5868874 -6.3597865 -6.9097834 -6.9704566 -7.7148647 -8.065773 -8.4974756 -8.6419277 -8.5980206 -8.9753695 -8.5613308 -7.2416244 -5.7881775 -4.145195 -1.6927662][-6.6402349 -5.0484056 -5.7034979 -6.080987 -6.297596 -6.1321359 -6.5383325 -6.9474525 -7.2229285 -7.5957165 -6.4298115 -5.0477715 -4.4929466 -4.5225377 -1.651783][-5.39569 -5.9463024 -6.4840374 -4.8633895 -5.7719164 -5.7265167 -4.9327478 -4.7310743 -4.4428492 -3.979269 -3.708164 -3.3017612 -1.8938615 -2.4549623 -1.229701][-4.9081068 -4.9215479 -6.2236052 -5.5107875 -6.1274581 -4.8786941 -4.2006092 -4.4905982 -4.1252761 -4.089128 -3.7568526 -2.9729803 -1.6482406 -2.2595205 0.029913425][-7.558672 -7.9752178 -8.3837156 -6.991806 -6.5609756 -3.5532832 -2.2136574 -3.4418488 -4.0907316 -4.0495043 -4.4609985 -4.0809445 -2.6823442 -3.1368876 -0.9562645][-6.3790936 -6.1984711 -6.7061009 -4.5391493 -2.2398231 1.9262643 4.7998939 2.903338 0.92960405 -0.25896692 -1.3667302 -0.47930288 -1.0848293 -3.5154915 -1.7274079][-7.2503815 -6.0637689 -5.925561 -3.6324036 -1.4423814 2.9240947 6.8938403 7.2353597 5.5839086 1.6671867 -0.45935488 0.12342072 -0.12935829 -0.99125051 -0.30006886][-6.9539728 -6.4035435 -5.7213426 -2.66312 -0.0079050064 3.34123 6.1801438 6.5688691 4.314755 1.0442715 -0.99253416 -1.7712789 -1.871974 -2.7956831 -1.8366127][-5.8367562 -4.5171962 -4.2188911 -1.7967219 0.054430485 1.7457733 4.2149129 4.6091018 3.0606065 0.55987358 -1.2543464 -2.5026717 -2.8105993 -4.6467676 -3.5916145][-6.0651674 -4.6026392 -4.3028474 -2.1232982 -1.0187712 1.7020988 2.8299565 1.9302611 0.42694855 -2.3735714 -3.4786456 -3.8021636 -3.2591429 -5.2260771 -4.9765682][-5.9076986 -5.0158439 -5.2591681 -4.3931341 -4.9504757 -4.4193888 -2.866775 -2.3106492 -2.9266429 -3.8315558 -5.0122452 -6.0527968 -5.5600452 -7.3453722 -6.415915][-11.175084 -9.5661983 -10.208352 -9.3570042 -9.29102 -8.7485447 -8.5659313 -8.7294369 -8.3826637 -8.5281582 -8.4428482 -8.3694963 -7.9101725 -7.9031253 -6.7951074][-14.129969 -13.222976 -12.91254 -12.000975 -12.54869 -11.352243 -10.952204 -10.744694 -10.097792 -9.64966 -8.7715607 -7.9211445 -6.8968515 -7.3465624 -6.7073421][-8.7989721 -8.0830564 -8.1102648 -8.6740961 -8.2122278 -7.6201715 -7.8338585 -7.5891547 -7.6138973 -7.7003446 -7.70686 -6.2738862 -5.9013066 -6.4557452 -6.1672416][-8.7263031 -6.6564069 -5.3335967 -4.98185 -4.2824707 -3.7106376 -3.5558569 -4.1185751 -4.8520441 -5.8701863 -6.0140114 -5.2024808 -6.2906561 -6.4059577 -5.6785774]]...]
INFO - root - 2017-12-15 19:42:26.998790: step 51010, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 52h:26m:24s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 19:42:33.534504: step 51020, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 50h:21m:54s remains)
INFO - root - 2017-12-15 19:42:40.122030: step 51030, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.649 sec/batch; 50h:46m:51s remains)
INFO - root - 2017-12-15 19:42:46.718957: step 51040, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.671 sec/batch; 52h:26m:41s remains)
INFO - root - 2017-12-15 19:42:53.301178: step 51050, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.684 sec/batch; 53h:28m:32s remains)
INFO - root - 2017-12-15 19:42:59.864176: step 51060, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 52h:58m:49s remains)
INFO - root - 2017-12-15 19:43:06.421293: step 51070, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 50h:28m:34s remains)
INFO - root - 2017-12-15 19:43:13.039272: step 51080, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 52h:06m:07s remains)
INFO - root - 2017-12-15 19:43:19.683471: step 51090, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 53h:22m:19s remains)
INFO - root - 2017-12-15 19:43:26.346633: step 51100, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 51h:35m:09s remains)
2017-12-15 19:43:26.860278: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4683125 -4.2291493 -4.4911866 -3.7037945 -3.8820345 -4.7254462 -5.4410939 -6.1496191 -5.9542637 -6.2670932 -6.2247486 -8.1386623 -9.575861 -9.6192741 -8.6005135][-3.4069037 -2.6146936 -2.7859347 -2.3429158 -2.4821482 -2.2793376 -2.7455246 -3.5233924 -4.5983424 -5.2197266 -5.0539126 -6.9338045 -8.1874933 -9.4784641 -9.827322][-0.23245859 -0.7497673 -1.3563643 -0.96557474 -1.7074981 -1.8239663 -2.4705827 -2.7968097 -3.0594504 -4.1747084 -4.8715711 -6.7563162 -8.1106691 -8.77328 -9.5490065][-0.268826 -1.0320377 -1.3360505 -0.43543768 -0.97683382 -1.322711 -2.0443954 -2.5926778 -3.5996919 -3.8854351 -3.1786497 -4.7599893 -6.4139519 -8.2604647 -9.56098][-0.042196751 -1.5168905 -2.6493769 -1.4798417 -1.6131973 -1.4124794 -1.0472708 -1.9406261 -3.5262144 -3.5891962 -3.1672723 -4.688539 -5.8660073 -7.7949834 -9.4674654][-2.7034342 -3.1300008 -3.176383 -2.1413176 -0.75071716 1.0590458 2.3164916 1.8959966 0.59649467 -0.94039154 -1.7863362 -3.46215 -4.9980078 -7.11493 -8.8034058][-4.8225884 -4.8886175 -3.9968874 -1.6775465 0.35120153 2.4589915 4.9996009 5.8407712 4.4435306 2.0166135 0.21022749 -2.3094459 -5.5135407 -7.2276268 -8.4757919][-5.6649575 -5.4791784 -4.7296491 -1.6775551 1.0377612 4.5013938 6.8705878 7.3226295 6.4753728 4.1939664 2.5153565 -0.982152 -4.5309529 -6.6396513 -7.8695412][-5.0702152 -5.3108678 -4.5514588 -1.7699876 0.53156376 3.0714269 5.2590384 6.1129689 5.862844 3.9703727 1.4540172 -2.3063419 -5.4696741 -7.3238058 -8.2383432][-4.8680491 -4.6735525 -4.2493215 -1.6234498 -0.059336662 1.3906403 2.5694098 2.7072597 2.2842059 1.3214064 -0.19519281 -3.9566536 -7.0387082 -8.3181858 -9.6783667][-8.9007025 -7.1930189 -5.3365178 -3.3549349 -2.9930055 -1.9617004 -0.670526 -0.2922864 -0.23691893 -1.4207416 -3.5648327 -8.2518492 -11.024484 -11.206354 -10.843619][-12.769925 -10.338712 -7.89165 -5.3039179 -4.4758711 -4.319303 -4.8005238 -4.6361485 -4.5951543 -5.8408084 -7.5417781 -9.6026964 -11.005663 -11.220944 -10.882408][-13.256544 -11.530642 -9.466629 -7.9950247 -7.7440481 -6.9702182 -6.5290542 -6.4718504 -7.0312095 -7.5291839 -8.7006359 -10.602458 -11.497404 -9.560626 -7.8866663][-13.094935 -12.406047 -10.646057 -8.2639236 -6.9927511 -6.6518884 -7.1817861 -6.7103868 -6.4134197 -7.0020342 -8.5733852 -8.7796955 -8.3089437 -7.4359446 -5.7227111][-9.95796 -10.786144 -10.0383 -8.5571547 -7.0209475 -6.218925 -6.0818372 -5.93216 -5.5015707 -5.4100509 -5.7946181 -7.2885551 -7.9919252 -7.3008547 -7.0212841]]...]
INFO - root - 2017-12-15 19:43:33.476086: step 51110, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 50h:05m:12s remains)
INFO - root - 2017-12-15 19:43:40.051688: step 51120, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 51h:14m:42s remains)
INFO - root - 2017-12-15 19:43:46.707988: step 51130, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.664 sec/batch; 51h:55m:44s remains)
INFO - root - 2017-12-15 19:43:53.323361: step 51140, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 51h:20m:05s remains)
INFO - root - 2017-12-15 19:43:59.974784: step 51150, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.675 sec/batch; 52h:44m:02s remains)
INFO - root - 2017-12-15 19:44:06.599541: step 51160, loss = 0.19, batch loss = 0.14 (11.7 examples/sec; 0.684 sec/batch; 53h:28m:49s remains)
INFO - root - 2017-12-15 19:44:13.164001: step 51170, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 50h:39m:21s remains)
INFO - root - 2017-12-15 19:44:19.719160: step 51180, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 50h:04m:14s remains)
INFO - root - 2017-12-15 19:44:26.393797: step 51190, loss = 0.18, batch loss = 0.14 (11.4 examples/sec; 0.700 sec/batch; 54h:39m:51s remains)
INFO - root - 2017-12-15 19:44:32.990600: step 51200, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 52h:33m:58s remains)
2017-12-15 19:44:33.543373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.365078 -0.32785463 -0.38286734 0.1308136 -0.71768522 -2.2462401 -3.5794241 -4.9921665 -4.9108577 -4.40362 -4.3567615 -5.7437196 -6.7081461 -8.0075283 -7.7768831][-2.5357382 0.61044788 3.4263291 3.6297069 1.9750648 0.024058342 -1.8351355 -3.044863 -4.7531271 -5.6953735 -6.1569128 -9.2612171 -10.385778 -11.232479 -11.47915][-1.4911079 -0.16083813 0.6893158 2.7342143 2.345264 1.0474267 -0.32363129 -2.0810711 -3.4925981 -5.4475689 -7.0965304 -9.5947771 -11.659916 -13.353565 -11.877508][-2.5497975 -1.3471847 0.104177 1.8304849 1.373446 0.41307211 -0.44245243 -1.0813756 -2.2653334 -4.2935224 -5.8755255 -9.326601 -11.412049 -12.798523 -12.909569][-3.15661 -3.2251875 -3.0263577 -0.91636658 0.17702007 1.2219825 1.2067494 -0.99775887 -2.8768795 -4.2277689 -5.4245682 -7.933651 -9.741106 -11.46036 -11.369091][-4.1405597 -3.8960338 -3.5882845 -1.3146677 0.041641712 1.1499829 1.3970022 0.89904356 -0.12759018 -1.8258073 -3.4091005 -5.5900683 -6.2756982 -7.8486142 -8.8456469][-5.2432194 -4.3597126 -2.8927207 -0.8842082 0.35394478 2.5265622 3.406548 2.7800908 2.0521588 0.53844547 -0.95559454 -3.5995584 -5.1884093 -6.5898433 -6.3254986][-6.5122709 -5.298995 -3.2507727 -0.55659103 0.39024448 2.44249 4.2120461 3.8288579 3.1082444 2.0784426 1.1458998 -1.8476479 -3.7584147 -5.79316 -6.7627363][-7.1060781 -4.9640961 -3.3551707 -1.10182 -0.1847887 0.66036034 1.4590979 1.753448 2.2168827 1.4637785 0.85885763 -1.3049269 -3.1087062 -4.9558563 -5.9657679][-6.7552004 -5.310308 -4.1961493 -1.971494 -1.1691532 -0.22183466 0.57645082 0.86477709 1.8079929 1.7390656 1.4850345 -1.0382452 -3.1763399 -5.87059 -7.6581125][-10.694853 -9.2370768 -7.4809675 -4.7817149 -3.3451469 -2.7401853 -2.2992704 -2.2083821 -1.8261554 -1.0655799 -0.90752316 -4.0543327 -6.2822766 -7.5230865 -7.7227087][-14.127968 -13.420604 -11.400846 -8.6592073 -7.4417973 -6.0303774 -5.6152878 -5.687191 -5.4547229 -5.0588288 -5.1400027 -6.653841 -7.4729509 -9.1697788 -10.083591][-13.752243 -13.502943 -12.881699 -10.921604 -9.5789833 -8.5996952 -8.3146343 -8.1063318 -8.4672546 -8.4839859 -8.28068 -8.7408867 -8.2644453 -7.8480253 -8.0467491][-12.014765 -11.122292 -10.50328 -9.8868647 -8.8663311 -8.6181231 -8.2728271 -7.8654976 -8.0496063 -8.1806278 -8.0430136 -7.8619294 -7.3258858 -6.9922056 -7.2350292][-7.1832471 -7.6945519 -7.5459895 -7.3995771 -6.3008614 -5.3419542 -4.8439603 -5.0155044 -5.6038961 -5.8146305 -5.912014 -7.7579269 -8.8498516 -8.56479 -8.40478]]...]
INFO - root - 2017-12-15 19:44:40.120110: step 51210, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 51h:13m:19s remains)
INFO - root - 2017-12-15 19:44:46.712127: step 51220, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 51h:33m:38s remains)
INFO - root - 2017-12-15 19:44:53.326379: step 51230, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.686 sec/batch; 53h:33m:56s remains)
INFO - root - 2017-12-15 19:44:59.974401: step 51240, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.652 sec/batch; 50h:55m:02s remains)
INFO - root - 2017-12-15 19:45:06.610283: step 51250, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 49h:52m:46s remains)
INFO - root - 2017-12-15 19:45:13.290171: step 51260, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 52h:00m:40s remains)
INFO - root - 2017-12-15 19:45:19.829757: step 51270, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 49h:56m:53s remains)
INFO - root - 2017-12-15 19:45:26.402429: step 51280, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 51h:55m:42s remains)
INFO - root - 2017-12-15 19:45:32.937089: step 51290, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 51h:39m:00s remains)
INFO - root - 2017-12-15 19:45:39.594187: step 51300, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 51h:10m:39s remains)
2017-12-15 19:45:40.116535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9610662 -8.0412769 -8.5380211 -8.997674 -10.062288 -11.133318 -10.941355 -10.157766 -9.2189608 -7.7066841 -5.8977566 -5.9743605 -8.3196087 -9.7848978 -9.24614][-5.9247088 -5.443789 -5.498332 -6.3602057 -8.1468534 -9.9877 -10.614307 -10.465552 -10.402516 -9.4544287 -7.660409 -7.0301952 -9.0736141 -10.525106 -9.6709785][-2.7600729 -2.8855374 -3.1927762 -3.542526 -5.8154984 -8.1347437 -8.7989149 -8.5629435 -8.4560776 -8.599103 -8.1282959 -8.1042595 -10.168139 -11.439179 -11.119614][-2.9179776 -2.5445039 -1.4621186 -0.97998238 -2.4934313 -3.3833454 -3.8073061 -5.0271454 -6.0542407 -6.1249948 -5.6910052 -6.3114614 -8.5515451 -9.5590782 -9.045785][-3.3805318 -4.0818949 -3.224561 -1.5815601 -0.66278887 -0.40948677 -0.65633631 -1.1911645 -2.5818458 -3.5056164 -3.5526159 -4.5705147 -6.6092629 -7.679872 -7.9382911][-6.505475 -6.0510392 -4.9013309 -2.8198025 -1.4387107 1.3340073 2.9242702 2.0394235 0.74168253 -0.60502768 -1.8515973 -3.4446161 -5.9970808 -7.15669 -7.1026726][-7.2649012 -6.6715355 -5.3293557 -2.6353986 -1.0447626 2.2564406 4.349299 4.967648 4.9489446 3.3069558 1.580368 -0.97347069 -4.6205044 -5.529418 -5.1292295][-7.1081314 -6.2889433 -5.0217385 -2.6092327 -0.0071635246 3.4968066 5.382875 5.7163062 5.6889777 4.0515504 2.5283771 -0.25947046 -4.6686945 -5.6259561 -4.4269462][-4.483922 -3.9350672 -3.4804425 -1.7822721 -1.2386003 0.92584705 2.9011693 3.3719182 3.2153249 2.9096093 2.4510369 -0.65987253 -4.3376536 -4.6294689 -3.1816342][-5.7552543 -3.7642586 -2.8028212 -2.2497411 -2.9207821 -1.5823417 -0.40398741 -0.10038757 0.20406961 -0.32586002 -0.42282343 -1.3927798 -4.395483 -4.5240946 -2.2874334][-9.233984 -7.3556948 -6.4449148 -6.28056 -6.9155235 -6.5124259 -5.9419627 -5.5873728 -5.2626591 -4.7250624 -4.3495307 -5.4738183 -6.5220919 -5.2929549 -2.2206526][-12.291151 -10.683697 -9.2485695 -8.99091 -9.4068232 -8.8918295 -7.778882 -7.3707309 -7.8932729 -7.8061886 -7.59061 -8.4515505 -8.47165 -6.3982582 -3.0779333][-11.852585 -10.236757 -9.8341217 -10.113303 -10.61272 -9.4349327 -7.7999139 -6.9763532 -6.2988396 -6.3241282 -6.6431179 -7.6236038 -8.412364 -6.3874316 -2.9928193][-11.512156 -11.334455 -9.6822224 -9.5084476 -9.55131 -8.4133024 -7.2463717 -5.5291853 -4.1184216 -4.2547927 -5.1391354 -5.4440107 -5.4117489 -4.2551484 -2.7171571][-8.9091053 -8.977787 -9.33886 -9.0232391 -8.0729256 -7.0140905 -5.0801072 -3.5094202 -2.5417161 -1.1508093 -0.96979618 -2.6298513 -3.6617684 -4.082634 -4.2785134]]...]
INFO - root - 2017-12-15 19:45:46.682859: step 51310, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 51h:57m:02s remains)
INFO - root - 2017-12-15 19:45:53.276738: step 51320, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 51h:11m:12s remains)
INFO - root - 2017-12-15 19:45:59.924474: step 51330, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 51h:13m:52s remains)
INFO - root - 2017-12-15 19:46:06.517891: step 51340, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 50h:16m:40s remains)
INFO - root - 2017-12-15 19:46:13.184276: step 51350, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 52h:33m:19s remains)
INFO - root - 2017-12-15 19:46:19.750521: step 51360, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 52h:32m:04s remains)
INFO - root - 2017-12-15 19:46:26.482518: step 51370, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.687 sec/batch; 53h:39m:49s remains)
INFO - root - 2017-12-15 19:46:33.077407: step 51380, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 52h:01m:52s remains)
INFO - root - 2017-12-15 19:46:39.631923: step 51390, loss = 0.19, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 49h:58m:39s remains)
INFO - root - 2017-12-15 19:46:46.201154: step 51400, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 52h:09m:14s remains)
2017-12-15 19:46:46.697207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1566067 -5.0501509 -3.3940654 -1.6887846 -1.6294894 -0.60270739 -0.43752337 -0.8171277 -0.87623787 -1.1061754 -0.95022011 -2.6914167 -4.8486118 -5.1576238 -4.8319931][-3.0645297 -2.3588254 -1.2690835 -0.90956211 -0.72300482 -0.19364595 0.007668972 -0.39735794 -1.073276 -0.99891186 -0.84134054 -2.4817166 -4.6931677 -4.6132731 -4.1630516][-2.6463923 -2.8000867 -2.6350429 -2.2605813 -1.4740477 -1.4396873 -1.2784634 -1.5927753 -1.9953675 -2.2504797 -1.5856662 -3.3983254 -5.6896114 -5.9811244 -6.5666866][-4.8408852 -4.7555451 -4.1392155 -3.4050248 -2.9001322 -2.7871068 -3.3054838 -3.0828655 -2.531553 -2.7585158 -2.453361 -4.7111311 -7.7032733 -8.1900921 -8.7277937][-5.9800453 -6.6376367 -6.5908303 -5.1774406 -3.7646508 -2.6905248 -2.8893881 -3.5865958 -3.9461226 -3.828783 -3.8617535 -5.8021369 -8.4916382 -9.1170273 -10.100941][-5.7176628 -5.3359256 -3.8166609 -2.7235622 -2.1331463 -1.2936091 -2.0090106 -2.5306287 -2.8638973 -3.219636 -3.2228143 -5.594203 -8.3195915 -8.5810995 -8.5210009][-5.0997033 -3.159498 -1.1674213 -0.63780832 -0.45413876 0.14725828 0.083957195 -0.91240025 -1.8301477 -2.5506306 -2.6687987 -5.1074586 -7.68504 -7.8688478 -8.012557][-4.6615434 -3.032954 -0.68112183 0.7812252 0.90846682 1.3678441 0.94935465 0.88707495 0.98184347 0.016174793 -0.62482452 -3.5913279 -6.5030532 -7.1465459 -7.0885377][-4.3159161 -4.2128811 -2.500833 -0.51430941 -0.065361977 0.11548948 -0.020719528 0.76045036 1.1722641 1.2331409 1.1756129 -2.4902816 -6.0135779 -6.5414824 -6.8396826][-6.4299841 -6.3956556 -4.8659186 -2.8703589 -2.5281384 -1.6489124 -0.50359058 0.54839945 1.2317066 2.0580006 2.5990682 -0.33442736 -3.8144937 -5.815135 -7.424767][-11.046673 -10.74649 -8.8440266 -7.2186117 -5.8028 -3.833075 -2.4338183 -1.3180494 -0.64895535 -0.1885767 -0.021585941 -2.547955 -5.1658697 -5.633647 -6.387651][-12.81143 -11.895849 -10.527922 -8.4659653 -6.9132576 -5.774991 -4.8517489 -3.870434 -3.1863449 -2.9421184 -3.2039285 -5.0497456 -6.1778064 -6.7395239 -7.365716][-12.704639 -11.226046 -9.7394924 -7.8450165 -6.5151753 -5.5142932 -5.2890077 -4.9871778 -4.7660913 -5.6371408 -5.8343182 -6.8711376 -7.373023 -7.1354895 -6.6939425][-10.576698 -9.8875675 -8.0180416 -5.9205451 -4.7907276 -4.7324343 -4.8999157 -4.9337759 -5.56213 -5.9366703 -6.6660662 -6.9574127 -6.8876953 -6.5279312 -5.8549237][-7.2885108 -7.6372356 -7.2379174 -6.1733394 -5.0366893 -4.4584522 -4.542007 -4.5582657 -4.7759428 -5.9877357 -7.1769137 -6.9580441 -7.8293104 -8.37522 -8.3563662]]...]
INFO - root - 2017-12-15 19:46:53.321428: step 51410, loss = 0.13, batch loss = 0.08 (11.5 examples/sec; 0.693 sec/batch; 54h:07m:41s remains)
INFO - root - 2017-12-15 19:46:59.906870: step 51420, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 50h:28m:29s remains)
INFO - root - 2017-12-15 19:47:06.478208: step 51430, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 50h:13m:25s remains)
INFO - root - 2017-12-15 19:47:13.118497: step 51440, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 51h:15m:49s remains)
INFO - root - 2017-12-15 19:47:19.706235: step 51450, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 51h:13m:48s remains)
INFO - root - 2017-12-15 19:47:26.280585: step 51460, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 51h:02m:05s remains)
INFO - root - 2017-12-15 19:47:32.802654: step 51470, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 50h:24m:37s remains)
INFO - root - 2017-12-15 19:47:39.368089: step 51480, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 52h:46m:46s remains)
INFO - root - 2017-12-15 19:47:45.971405: step 51490, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 51h:59m:03s remains)
INFO - root - 2017-12-15 19:47:52.564010: step 51500, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 53h:02m:10s remains)
2017-12-15 19:47:53.192418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2422328 -2.74078 -1.8514006 -1.2354069 -1.6368432 -1.9104187 -2.1827075 -2.3683724 -2.7126775 -3.1397455 -3.5797904 -5.2734394 -7.2322869 -7.9896641 -6.6917224][-2.9048347 -1.9494967 0.024971962 -0.15239334 -1.2813258 -1.4355292 -1.1740518 -1.2192969 -1.1488485 -1.5855823 -1.9467733 -4.1014943 -7.0054674 -7.5218139 -6.3858628][-1.4496045 -1.4334803 -1.2952251 -0.57749557 -0.45869112 -0.75948668 -0.89328146 -0.51575851 0.037815571 -0.31619358 -0.81600285 -2.9339087 -5.0715642 -6.9204741 -6.7425][-3.6152055 -2.6039493 -1.5214834 -0.64315128 -0.50393438 -0.7038784 -1.1055212 -0.86110115 -0.1011796 0.758389 0.52813721 -1.7551947 -4.1649675 -5.8150821 -5.8212485][-3.8817739 -4.4709797 -4.6121416 -2.6462166 -1.9038167 -1.4154611 -1.0598316 -1.4655609 -1.3941021 -0.55603886 -0.24633026 -1.6048632 -3.6064739 -5.8175082 -5.7680039][-5.0121174 -4.4670997 -3.4412105 -1.2119799 -0.04203558 1.0403342 1.930027 1.1594114 0.36035585 -0.22594976 -0.49802017 -1.2253432 -2.6744509 -4.5688491 -4.1406116][-4.9913726 -3.8328905 -2.0124035 0.44405127 1.3422222 2.7474198 3.6688743 2.9358077 2.2208428 1.3042064 0.71441555 -0.30890131 -2.6644194 -3.3773494 -2.7010853][-5.2855959 -3.9936993 -2.0046794 0.20067215 1.7058392 3.4319329 4.1939054 3.6567407 2.9217868 1.9571776 1.3423319 -0.24826241 -2.3832853 -2.8655279 -1.9559872][-4.7169404 -3.4729109 -2.0686705 -0.087918282 1.3744211 2.4685721 3.0232377 2.7528443 2.7565827 1.5363274 0.81905127 -0.66312981 -2.7320476 -3.709686 -2.7556288][-4.8222327 -4.3068805 -3.7398109 -0.72684622 0.64654636 1.1256275 1.6415224 1.0195541 0.69984579 -0.71945095 -1.6645851 -3.0940115 -4.6292667 -5.1572323 -3.7987466][-7.2253342 -6.668582 -6.2978811 -3.9190023 -3.7761459 -3.220654 -2.7940643 -2.606972 -2.1063454 -2.962343 -3.9922404 -6.9490881 -8.08993 -6.8696203 -4.1586304][-10.001184 -10.057812 -8.8456545 -6.8605852 -6.8345318 -5.9361944 -6.0659704 -5.9386621 -5.3619637 -5.3834319 -5.4929476 -6.8160815 -6.8326006 -6.42428 -3.7684774][-12.346422 -11.49679 -10.166227 -8.6296482 -8.1776867 -6.9689078 -6.2988091 -6.2098737 -6.9130678 -7.2714176 -7.348763 -7.1808305 -7.4848266 -6.5530453 -4.4462614][-7.7653933 -6.8707004 -5.9589171 -4.4582024 -4.4326134 -4.1968184 -4.003962 -3.4452925 -3.8603368 -5.226171 -6.9870949 -6.9324284 -6.8022723 -6.0664315 -5.1330318][-3.7977591 -4.4356251 -2.8479137 -1.2110581 -0.25198936 -0.45244694 -1.2188134 -1.5211515 -1.4139585 -2.5911353 -3.8869691 -5.1435814 -5.5508103 -5.5467563 -5.4105983]]...]
INFO - root - 2017-12-15 19:47:59.707300: step 51510, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 52h:29m:54s remains)
INFO - root - 2017-12-15 19:48:06.280986: step 51520, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 51h:44m:04s remains)
INFO - root - 2017-12-15 19:48:12.826511: step 51530, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 51h:41m:40s remains)
INFO - root - 2017-12-15 19:48:19.376784: step 51540, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 50h:27m:02s remains)
INFO - root - 2017-12-15 19:48:25.929268: step 51550, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 51h:03m:12s remains)
INFO - root - 2017-12-15 19:48:32.567193: step 51560, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 51h:55m:28s remains)
INFO - root - 2017-12-15 19:48:39.209194: step 51570, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 52h:56m:41s remains)
INFO - root - 2017-12-15 19:48:45.710192: step 51580, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 49h:35m:54s remains)
INFO - root - 2017-12-15 19:48:52.206043: step 51590, loss = 0.20, batch loss = 0.16 (12.7 examples/sec; 0.629 sec/batch; 49h:07m:06s remains)
INFO - root - 2017-12-15 19:48:58.777325: step 51600, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 52h:38m:51s remains)
2017-12-15 19:48:59.349487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3824363 -8.0205059 -8.59001 -8.82627 -9.9582787 -10.57671 -10.681374 -10.951239 -10.556025 -9.525528 -8.1957436 -9.947051 -9.8542271 -11.262053 -11.014746][-7.7126865 -9.5849638 -10.407094 -11.08012 -11.917298 -12.743418 -13.037138 -12.652357 -11.136939 -9.8077927 -8.68139 -10.090919 -10.464972 -11.808428 -10.961522][-7.2206254 -10.117123 -12.102833 -13.241364 -13.865498 -13.916821 -13.741795 -12.951165 -11.242594 -10.207798 -9.3227224 -11.125963 -11.874097 -13.457012 -12.385708][-7.6685615 -9.4870815 -11.200949 -12.185596 -13.540173 -12.258827 -10.635697 -9.6731825 -7.8254528 -7.9962044 -8.5656376 -11.054227 -11.969611 -13.693649 -13.979139][-8.4444485 -10.45928 -12.597008 -11.325704 -10.667871 -7.2213097 -4.7406816 -5.2649317 -5.3774652 -4.9520025 -5.2624979 -8.0788918 -10.075939 -13.258903 -13.675359][-9.6260424 -11.582493 -12.984558 -9.605876 -6.2349906 -1.1965661 3.3918624 3.3262458 1.4836888 -1.5864401 -3.7449322 -6.3506861 -8.5344505 -12.095453 -12.959464][-8.08433 -9.9731007 -11.340331 -7.6151118 -3.7360771 1.8437347 7.2112107 9.0808868 8.3941116 4.2142205 0.17638397 -5.297688 -8.11092 -11.177758 -12.077809][-5.852541 -8.0910969 -8.7441359 -6.5700512 -2.4699707 5.7929263 11.757288 10.98741 9.4781036 4.6733623 -0.066566944 -6.0137515 -8.6891966 -11.409783 -11.715546][-3.8129952 -6.0896859 -7.0660977 -6.0775704 -4.4923682 1.4176526 7.7207971 7.4885392 6.3117309 1.0228043 -3.12461 -9.0903883 -12.239728 -14.716852 -14.341316][-3.01693 -4.8170595 -7.3503013 -6.0685186 -5.3371506 -3.1337538 -0.84263372 1.1019721 0.92280006 -3.899436 -7.4706941 -13.382416 -15.859648 -18.626816 -18.642715][-6.177094 -7.4361277 -9.2937927 -9.2799816 -9.0923958 -7.8204894 -6.7049012 -5.2884459 -5.2337775 -8.0945435 -10.117541 -15.844416 -17.075733 -19.607754 -19.27619][-9.2817812 -10.037558 -11.327863 -10.568253 -10.71847 -10.041162 -10.541302 -10.696249 -11.030234 -11.745399 -12.07983 -14.500305 -14.376284 -16.926386 -15.391958][-11.657515 -10.724301 -10.676341 -10.692742 -11.873412 -10.778299 -11.752351 -11.992445 -12.311569 -13.375955 -13.33882 -13.279594 -12.803648 -14.495152 -12.008032][-8.8942871 -8.7956362 -9.4183712 -8.7191448 -8.6002483 -8.7542791 -9.3602161 -9.9380989 -10.259332 -10.155186 -10.426218 -10.111925 -9.2925987 -10.245859 -10.297884][-7.2459917 -7.5888762 -6.507442 -5.0892086 -5.2725153 -5.3230929 -5.3732176 -5.9260807 -6.7436929 -7.4646339 -7.814858 -8.0417595 -8.1132317 -8.2965212 -8.6183138]]...]
INFO - root - 2017-12-15 19:49:05.973029: step 51610, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 53h:07m:50s remains)
INFO - root - 2017-12-15 19:49:12.659254: step 51620, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 51h:03m:29s remains)
INFO - root - 2017-12-15 19:49:19.200062: step 51630, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 51h:11m:30s remains)
INFO - root - 2017-12-15 19:49:25.803097: step 51640, loss = 0.24, batch loss = 0.19 (12.0 examples/sec; 0.666 sec/batch; 51h:58m:37s remains)
INFO - root - 2017-12-15 19:49:32.289885: step 51650, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 51h:54m:01s remains)
INFO - root - 2017-12-15 19:49:38.940796: step 51660, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 52h:32m:31s remains)
INFO - root - 2017-12-15 19:49:45.491790: step 51670, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 51h:12m:10s remains)
INFO - root - 2017-12-15 19:49:52.128366: step 51680, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 52h:07m:48s remains)
INFO - root - 2017-12-15 19:49:58.684185: step 51690, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.642 sec/batch; 50h:06m:10s remains)
INFO - root - 2017-12-15 19:50:05.303676: step 51700, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 53h:02m:33s remains)
2017-12-15 19:50:05.799226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.325532 -5.2669935 -6.0985379 -6.2497606 -5.6309371 -4.0791569 -3.2489674 -4.5029011 -5.4440241 -5.8267064 -5.8934569 -6.0374012 -8.4718513 -8.777935 -8.4508705][-4.8866196 -6.200788 -5.1515026 -4.9357667 -4.3735161 -3.5112164 -4.2593093 -3.8741322 -5.0994768 -6.6679053 -6.3118472 -6.8807468 -9.7702961 -10.760466 -10.692949][-4.9762549 -5.7935219 -5.636601 -5.4955993 -5.7948713 -5.7138834 -5.1243582 -4.7598124 -5.2102447 -6.7987537 -8.1822338 -8.4601469 -10.621626 -12.046511 -11.987453][-7.5134554 -7.2576232 -5.7686605 -5.7252784 -6.6436353 -6.2961364 -5.5211492 -5.0040388 -5.6775379 -6.7982597 -7.651 -9.7711039 -13.35008 -13.297161 -12.338686][-8.2106037 -8.4476795 -8.4742308 -5.8179588 -5.248981 -5.1152382 -4.0307293 -3.8065491 -4.8092422 -5.5692225 -6.6402469 -9.0576077 -11.911106 -13.772327 -14.298954][-8.2755041 -7.8916407 -7.0087929 -5.3056383 -4.057023 -0.907084 1.7371769 0.85430908 -0.71334219 -2.9107547 -5.6228714 -6.5997095 -9.61662 -12.162291 -12.388865][-8.5775023 -7.9664841 -6.3401914 -3.9984045 -1.6559806 1.6877251 4.9750562 5.624517 5.5958924 0.61907864 -3.9829631 -4.3533731 -7.7132974 -10.138786 -11.294008][-8.9796295 -7.5723753 -6.0932074 -4.1011109 -1.5175986 3.0504842 7.2061582 7.8237052 7.0581365 3.0400987 -1.2905164 -4.3742723 -8.4581528 -9.2281637 -9.7045565][-7.8844004 -7.3217554 -5.5238748 -4.1505165 -2.7190168 1.4185843 6.054749 7.2512012 6.2067657 2.4231534 -2.1263425 -5.5679426 -10.500278 -11.840493 -11.770695][-7.918767 -7.4163418 -6.1809568 -4.3670568 -2.8064334 -0.80306816 1.4719586 3.6076808 3.4657435 -0.39147329 -4.2929363 -7.3296185 -12.407016 -14.579208 -15.104191][-10.345456 -10.705654 -10.183403 -7.5379095 -5.7334695 -5.312119 -3.9763873 -2.6927948 -3.3988926 -5.0638027 -7.7536087 -11.203395 -15.856924 -16.514492 -15.847803][-14.207804 -13.632952 -12.345587 -9.70172 -8.7682648 -8.0775242 -7.1622496 -8.1009827 -8.7887287 -9.2599363 -11.103445 -13.048241 -16.008373 -17.707535 -16.399378][-16.120436 -14.971325 -13.315032 -12.096495 -11.988157 -10.28466 -9.7828865 -9.7364264 -10.031438 -11.025257 -12.223722 -14.093536 -15.264921 -15.033552 -12.164908][-13.106191 -12.878896 -11.38571 -10.209914 -10.205685 -9.7880917 -10.511142 -10.071957 -10.093905 -10.550626 -10.551163 -9.7856655 -9.5788193 -11.364347 -10.35821][-9.7992926 -9.21472 -8.1087322 -7.2064033 -7.2133665 -8.1738167 -9.0173092 -8.7292023 -8.8352013 -8.8841419 -9.259388 -10.08161 -10.356667 -8.2850542 -7.8891745]]...]
INFO - root - 2017-12-15 19:50:12.394884: step 51710, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 51h:20m:24s remains)
INFO - root - 2017-12-15 19:50:19.015450: step 51720, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.688 sec/batch; 53h:39m:40s remains)
INFO - root - 2017-12-15 19:50:25.613193: step 51730, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 51h:37m:39s remains)
INFO - root - 2017-12-15 19:50:32.165962: step 51740, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 51h:33m:06s remains)
INFO - root - 2017-12-15 19:50:38.794274: step 51750, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 52h:09m:01s remains)
INFO - root - 2017-12-15 19:50:45.398764: step 51760, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 51h:23m:15s remains)
INFO - root - 2017-12-15 19:50:51.996226: step 51770, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 50h:08m:11s remains)
INFO - root - 2017-12-15 19:50:58.579152: step 51780, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 52h:20m:21s remains)
INFO - root - 2017-12-15 19:51:05.072875: step 51790, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 49h:57m:35s remains)
INFO - root - 2017-12-15 19:51:11.749738: step 51800, loss = 0.21, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 50h:17m:05s remains)
2017-12-15 19:51:12.307536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2088037 -4.6087203 -4.9563956 -5.5500722 -7.0711846 -7.6159563 -8.1440773 -8.3874512 -8.6662655 -9.3092842 -9.2866516 -9.60998 -10.158259 -9.35348 -6.5129113][-6.8689818 -6.4825449 -6.1002297 -6.6590285 -7.889421 -9.1362324 -9.8222027 -9.7972851 -9.5911894 -9.5073729 -9.5117483 -9.8792133 -10.552352 -10.674488 -8.1174736][-5.1563253 -5.7882423 -6.8588066 -7.1240277 -7.5121336 -8.4343777 -9.2284107 -9.0609369 -8.6770172 -8.74381 -8.8492785 -8.6067762 -9.2793083 -10.236819 -9.0922031][-6.3578348 -6.5571508 -6.9078794 -7.0108585 -7.9409742 -8.0887451 -7.97091 -8.6585712 -8.6126652 -8.1384869 -8.1840382 -8.1758862 -8.4877872 -8.7307854 -8.3986511][-6.6864533 -8.0193424 -8.6100855 -7.633872 -7.0150414 -6.0071492 -5.3999009 -6.3246369 -6.7785168 -7.0219746 -7.564395 -7.7411218 -8.8054533 -9.210104 -7.8870831][-7.039135 -7.1717505 -6.0536623 -5.3946137 -4.2822914 -0.77883291 1.8154001 0.90716171 -0.14110327 -2.3248987 -5.0424356 -5.7375979 -6.8463941 -7.8179326 -7.9988718][-8.0467148 -7.6525455 -6.7495389 -4.982461 -2.415638 0.96776485 5.3773274 7.0367141 6.6908441 2.0435066 -2.416038 -3.3800161 -5.78017 -7.7945509 -8.3195972][-7.9684834 -7.8535786 -6.5468993 -4.0052714 -0.93956757 2.9812655 6.2984195 8.0274487 9.5296955 4.040154 -2.4791107 -4.6170669 -6.3310518 -7.546174 -8.4516354][-4.8691459 -5.8593464 -6.2906528 -3.9701362 -1.8479998 1.0103974 4.6168828 6.4970775 6.7784276 3.0541615 -1.041162 -4.5343103 -8.7119322 -10.058389 -9.3365755][-3.3746283 -3.1506715 -3.9855452 -3.2519863 -2.4408007 -0.57255125 1.5051379 2.6313977 2.7590585 -0.5325551 -3.6355207 -6.54166 -10.027979 -12.231196 -12.491186][-5.468575 -4.9183245 -4.71102 -4.7526488 -4.7191973 -3.9823213 -3.6347322 -4.3182216 -4.7168703 -6.176116 -8.6350613 -10.889196 -12.762502 -13.45352 -12.935523][-10.266262 -10.21538 -10.19832 -9.1355085 -8.3493176 -8.09086 -8.9321327 -9.4809084 -9.4609232 -10.931538 -12.908762 -13.573264 -13.991972 -13.47485 -11.873266][-10.513697 -10.028339 -9.1614323 -8.9261417 -9.472682 -9.01144 -8.4218721 -9.4589977 -10.470975 -11.334972 -11.65741 -12.408957 -12.168612 -10.716155 -8.5952072][-9.9570026 -8.897687 -8.225997 -8.2947636 -8.7742195 -8.4570637 -8.5730925 -8.5089817 -8.3139448 -8.9893236 -9.6247931 -9.4721956 -8.7048931 -8.7492685 -7.6219473][-7.78656 -7.0941315 -5.6451812 -4.5248604 -4.8519092 -4.6807823 -4.9860392 -5.2928038 -5.5977178 -6.2740145 -6.5518203 -7.5599823 -7.609129 -6.9605393 -6.6982512]]...]
INFO - root - 2017-12-15 19:51:19.001289: step 51810, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.654 sec/batch; 50h:58m:25s remains)
INFO - root - 2017-12-15 19:51:25.656547: step 51820, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 51h:28m:17s remains)
INFO - root - 2017-12-15 19:51:32.276215: step 51830, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 51h:49m:20s remains)
INFO - root - 2017-12-15 19:51:38.988717: step 51840, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 52h:30m:44s remains)
INFO - root - 2017-12-15 19:51:45.614000: step 51850, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 51h:07m:01s remains)
INFO - root - 2017-12-15 19:51:52.284623: step 51860, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 52h:12m:08s remains)
INFO - root - 2017-12-15 19:51:58.880263: step 51870, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 51h:10m:06s remains)
INFO - root - 2017-12-15 19:52:05.496718: step 51880, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.634 sec/batch; 49h:25m:18s remains)
INFO - root - 2017-12-15 19:52:12.102665: step 51890, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 50h:09m:56s remains)
INFO - root - 2017-12-15 19:52:18.690478: step 51900, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 51h:00m:15s remains)
2017-12-15 19:52:19.238526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.7211947 -7.5749631 -7.0622177 -7.2718587 -8.134841 -9.3672161 -10.706287 -10.418259 -9.139185 -7.7493105 -6.2936482 -9.1820793 -11.857154 -10.65567 -9.0404739][-7.8984613 -6.4720821 -5.3962522 -5.7065334 -6.7734418 -8.7210369 -10.113455 -10.476482 -9.6107769 -7.6959939 -6.4137249 -9.9718838 -12.188761 -11.779421 -10.859919][-6.5252647 -5.9774561 -5.7415528 -4.9959126 -5.6257238 -7.5955849 -8.7033491 -9.0183172 -9.0406847 -8.5845222 -7.9137306 -11.095833 -12.903591 -12.488726 -11.884338][-6.841959 -6.3641462 -6.5995383 -5.712975 -5.8905716 -6.1354766 -6.4480882 -7.317049 -8.3412132 -7.9604239 -7.6374049 -11.070148 -13.198153 -12.957712 -11.709584][-9.2664928 -9.6938553 -9.5094395 -6.5538716 -5.0775981 -3.6952984 -2.3605461 -4.5622163 -6.9773436 -7.3507271 -7.5675344 -10.936491 -12.705076 -11.33769 -10.140218][-9.4349833 -9.6684322 -9.1212635 -5.0980554 -2.2310257 1.1722889 4.0659604 1.4051576 -0.77197742 -2.9944313 -6.3314242 -9.5248442 -11.175909 -9.9619694 -8.1094847][-10.463248 -9.3127909 -7.4827409 -3.1871305 0.26253176 4.4634137 8.2745457 7.194386 5.2997165 0.49059916 -3.7263625 -7.8668079 -11.541533 -10.038992 -7.8622794][-10.246486 -8.1836624 -6.8228354 -2.2425866 0.95314455 5.1709266 9.4069958 8.87602 6.9509616 2.6085267 -1.5013962 -7.1616335 -11.644575 -11.187994 -9.9670734][-9.0138769 -7.9744864 -6.4998565 -3.2917607 -0.67192507 2.9537053 5.6574588 5.3155026 4.653789 1.4777503 -2.723448 -8.7506018 -13.071717 -12.914146 -12.439709][-8.62614 -7.2098093 -6.8252549 -4.0027223 -1.3385272 -0.52447319 -0.090957642 0.22842121 0.33571482 -1.9523489 -4.4993887 -10.296692 -14.640957 -14.593061 -13.252204][-12.203186 -11.495552 -10.641171 -8.28799 -7.279768 -6.3695765 -4.9691916 -5.3112173 -6.3905935 -7.6840377 -8.8126326 -13.818981 -16.59112 -15.863625 -14.071083][-17.743546 -17.232014 -16.384226 -14.295748 -13.33724 -12.445724 -11.917224 -11.881645 -11.99144 -12.490274 -12.971556 -16.071478 -17.226875 -15.74585 -13.488731][-17.837446 -16.611208 -16.0166 -15.478233 -14.756611 -13.754141 -13.822369 -13.855083 -14.020363 -13.48501 -13.582584 -15.110151 -15.020741 -12.898079 -10.303251][-14.499201 -12.654683 -11.796732 -10.697929 -10.837505 -11.687563 -12.905653 -12.315548 -12.283823 -12.366718 -12.456976 -12.504839 -12.183561 -10.226384 -8.2393064][-9.8675251 -8.0857592 -6.6813536 -4.7361455 -4.7862234 -5.3997025 -6.4513388 -7.3873529 -8.4973736 -8.1701975 -8.5124035 -10.39687 -11.06262 -9.211113 -7.80373]]...]
INFO - root - 2017-12-15 19:52:25.902560: step 51910, loss = 0.16, batch loss = 0.11 (11.4 examples/sec; 0.703 sec/batch; 54h:45m:21s remains)
INFO - root - 2017-12-15 19:52:32.480616: step 51920, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 50h:49m:15s remains)
INFO - root - 2017-12-15 19:52:39.119083: step 51930, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 51h:16m:59s remains)
INFO - root - 2017-12-15 19:52:45.798673: step 51940, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 50h:55m:05s remains)
INFO - root - 2017-12-15 19:52:52.533162: step 51950, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 52h:05m:22s remains)
INFO - root - 2017-12-15 19:52:59.169374: step 51960, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 52h:14m:29s remains)
INFO - root - 2017-12-15 19:53:05.831408: step 51970, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 52h:07m:21s remains)
INFO - root - 2017-12-15 19:53:12.462287: step 51980, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 52h:59m:08s remains)
INFO - root - 2017-12-15 19:53:19.050317: step 51990, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.658 sec/batch; 51h:18m:25s remains)
INFO - root - 2017-12-15 19:53:25.636083: step 52000, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 50h:58m:46s remains)
2017-12-15 19:53:26.168935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2021456 -5.1389747 -5.1356621 -4.2060394 -4.8206387 -4.8266568 -5.0582342 -5.8269191 -6.3402576 -6.8672404 -6.8704815 -7.2938385 -9.3093719 -10.801317 -9.72752][-7.6301632 -6.1477156 -5.280705 -4.7459655 -5.0406642 -5.8109303 -6.2691813 -6.838306 -7.5177693 -7.7160721 -7.706315 -8.7259722 -10.16267 -12.10969 -11.40018][-7.4750147 -6.4348307 -5.8231459 -5.2993522 -6.5616169 -6.8144846 -5.7280316 -4.6319571 -4.7813206 -6.4755721 -7.6660914 -8.1364412 -9.306221 -9.8193016 -9.5615711][-7.9002538 -7.0240169 -6.3799076 -5.9123173 -6.5527072 -5.7717285 -4.348506 -4.1266356 -4.8465285 -4.3641562 -4.1821213 -5.4892335 -7.6610665 -9.9083824 -10.189531][-6.0603471 -7.94573 -9.4354267 -7.741344 -6.6825628 -4.5925121 -2.6833138 -2.6666172 -3.690485 -4.657299 -4.4583883 -4.9793396 -7.0456252 -8.8836565 -10.082993][-5.56689 -5.5335188 -7.0109892 -7.5131216 -5.0119224 -0.73012686 2.2958026 2.5720038 1.1087627 -1.8503382 -4.7148256 -5.2507839 -5.8308659 -8.6721134 -9.758276][-5.9164124 -6.02593 -5.4470091 -4.1827507 -1.8535018 2.8650308 6.416369 7.3907866 6.0767245 1.5241241 -2.508358 -4.8378191 -7.2227173 -9.0248966 -8.1309681][-7.8619823 -8.2312708 -7.7544832 -4.8172679 -1.2526298 3.1108766 7.1017919 8.023222 6.5237279 3.2374463 -0.23053885 -4.0621715 -8.0919447 -9.8435955 -8.7865572][-8.2156553 -9.1059217 -8.0591583 -6.0410218 -3.9178555 0.26917124 4.0060039 5.4777884 3.8818498 0.796083 -0.90522623 -3.7224607 -8.2426291 -10.543839 -10.161332][-8.4411182 -8.2439613 -7.4502845 -5.4386086 -3.4085374 -1.2727499 1.7882113 2.7514014 1.1984301 -1.2235546 -4.0063524 -6.43259 -8.9628639 -11.948227 -13.206002][-10.228693 -11.214067 -11.43842 -9.3242912 -6.2696857 -4.3458424 -4.1027179 -4.0549297 -4.5281048 -5.8396907 -7.880847 -10.821754 -13.436189 -13.59392 -12.716655][-13.519 -14.256062 -14.885509 -12.925959 -11.291453 -10.393286 -10.159069 -11.026703 -11.656569 -11.688448 -12.20893 -12.046114 -13.582426 -14.848555 -14.708494][-15.402004 -15.754654 -14.471092 -12.638706 -13.190145 -12.106936 -11.920057 -13.596817 -14.491518 -14.302952 -13.663704 -13.286983 -13.609405 -12.737204 -12.780405][-12.370255 -12.414515 -12.051006 -10.695639 -10.087893 -10.704687 -11.942014 -12.700404 -13.075787 -13.019138 -12.925137 -11.311838 -10.091045 -10.926351 -10.946957][-8.6414261 -8.5313005 -8.4624023 -7.5043039 -4.9141316 -3.841496 -5.8764281 -7.8561172 -9.2009087 -10.149133 -10.652301 -10.215155 -10.266247 -9.8941059 -10.2707]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 19:53:32.731541: step 52010, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 51h:23m:29s remains)
INFO - root - 2017-12-15 19:53:39.290721: step 52020, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 51h:59m:51s remains)
INFO - root - 2017-12-15 19:53:45.938376: step 52030, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.647 sec/batch; 50h:25m:31s remains)
INFO - root - 2017-12-15 19:53:52.544705: step 52040, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 51h:02m:21s remains)
INFO - root - 2017-12-15 19:53:59.194133: step 52050, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 52h:08m:40s remains)
INFO - root - 2017-12-15 19:54:05.801290: step 52060, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.658 sec/batch; 51h:17m:45s remains)
INFO - root - 2017-12-15 19:54:12.461286: step 52070, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 49h:55m:55s remains)
INFO - root - 2017-12-15 19:54:19.097768: step 52080, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 50h:33m:36s remains)
INFO - root - 2017-12-15 19:54:25.759302: step 52090, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 51h:26m:10s remains)
INFO - root - 2017-12-15 19:54:32.282467: step 52100, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 50h:33m:35s remains)
2017-12-15 19:54:32.796068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.74876 -10.922145 -11.620115 -12.055304 -12.819221 -13.092859 -13.414276 -12.679972 -12.520416 -12.362265 -11.024186 -10.782034 -13.747923 -14.244215 -14.448639][-10.736341 -12.836842 -14.027292 -13.273048 -13.06989 -13.537055 -14.06665 -14.905087 -15.505417 -14.158791 -12.586782 -12.872146 -15.089901 -15.341614 -15.036539][-9.2680025 -11.662333 -14.072834 -14.01683 -13.481426 -13.065344 -12.558693 -12.934286 -14.277969 -14.448492 -13.11188 -12.065866 -13.660629 -14.439312 -14.574747][-9.6844864 -10.924252 -11.936667 -11.486961 -11.412447 -10.41972 -9.1102343 -10.266857 -11.481918 -11.209726 -10.943752 -11.654086 -14.048372 -14.11389 -14.517927][-11.534819 -12.729824 -12.881899 -9.7221279 -6.9878731 -3.8815117 -1.2646523 -4.8076382 -9.190753 -9.7225018 -9.5501366 -9.5258265 -12.404078 -14.252378 -15.367714][-13.172598 -14.38097 -13.660144 -9.9972973 -4.8185325 2.0562291 7.0180564 3.7636733 -0.71045637 -4.8481126 -8.6284523 -8.6751652 -10.901554 -12.535545 -14.191437][-14.504545 -14.157593 -12.971465 -8.3514109 -3.4252322 2.5571361 9.0930653 9.6905766 7.1039395 -0.7053895 -7.4506574 -7.8619432 -11.346128 -12.86303 -13.304161][-14.228817 -13.826128 -12.189739 -6.3935509 -1.2366304 3.5006108 8.106205 7.9857345 6.4948497 1.8907232 -3.9938712 -8.0190535 -13.125435 -13.973225 -14.557621][-11.235266 -12.014137 -11.275002 -6.82443 -2.9860451 1.8417649 5.9125485 5.5039935 3.4628215 -1.1724248 -5.4718189 -9.2303715 -15.543171 -17.963108 -17.978632][-7.7542114 -8.6508064 -9.4582405 -7.5831079 -5.1715422 -2.785558 0.59902477 1.4149837 -0.19230127 -2.9957695 -6.5242953 -10.561325 -15.898895 -18.715015 -20.174118][-11.608637 -11.419836 -11.31188 -10.752068 -10.440781 -10.106691 -8.0685091 -7.4370852 -7.8636088 -8.4318457 -10.798109 -13.76993 -16.868156 -18.368847 -19.284891][-16.001694 -15.009045 -13.967386 -13.685818 -13.885281 -14.574442 -14.731052 -14.531664 -13.733393 -13.033175 -13.888634 -15.072708 -16.601154 -17.158369 -16.778738][-17.122042 -16.47036 -15.267487 -15.226122 -15.988539 -15.789124 -15.715532 -16.096411 -15.583168 -14.647074 -14.038733 -14.482975 -14.330219 -13.339287 -13.125993][-14.652012 -12.870515 -12.228306 -11.525024 -11.638488 -12.944019 -13.913645 -13.088671 -12.749563 -12.98522 -12.932188 -12.036404 -11.512091 -11.166876 -11.104798][-10.191952 -8.4362707 -7.2502103 -5.80316 -5.1698451 -6.1220069 -7.4979997 -8.2023335 -8.5709286 -8.8077068 -9.4286661 -9.9523134 -11.071537 -10.800902 -10.313833]]...]
INFO - root - 2017-12-15 19:54:39.408374: step 52110, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 51h:32m:04s remains)
INFO - root - 2017-12-15 19:54:45.967110: step 52120, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 51h:25m:33s remains)
INFO - root - 2017-12-15 19:54:52.671216: step 52130, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.669 sec/batch; 52h:07m:04s remains)
INFO - root - 2017-12-15 19:54:59.292977: step 52140, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 51h:46m:47s remains)
INFO - root - 2017-12-15 19:55:05.955240: step 52150, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 51h:35m:21s remains)
INFO - root - 2017-12-15 19:55:12.502404: step 52160, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 51h:31m:32s remains)
INFO - root - 2017-12-15 19:55:19.049011: step 52170, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 50h:32m:33s remains)
INFO - root - 2017-12-15 19:55:25.631151: step 52180, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 50h:48m:09s remains)
INFO - root - 2017-12-15 19:55:32.168276: step 52190, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 52h:07m:05s remains)
INFO - root - 2017-12-15 19:55:38.796665: step 52200, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.689 sec/batch; 53h:40m:14s remains)
2017-12-15 19:55:39.324643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6170764 -2.9993093 -1.8671987 -0.70202017 -0.6760087 -1.7605481 -2.2286019 -2.6920071 -2.8500972 -2.070817 -0.87434149 -2.7084382 -3.9082184 -5.8656945 -6.949543][-1.4142208 -1.661829 -1.156064 -0.06342268 -0.1699214 -0.77247429 -1.8441763 -3.082057 -3.7002883 -3.7525387 -2.9898658 -3.2633948 -4.4851637 -6.1839614 -6.60928][-1.22261 -2.1300657 -2.5007088 -2.5282843 -2.9830351 -3.7736015 -3.7116263 -3.6697347 -3.3846688 -2.7861302 -2.7983835 -5.5103459 -7.7373619 -8.5825605 -9.6846495][-2.7510149 -2.7179377 -2.9865448 -3.1927722 -3.573422 -3.7334194 -4.1431379 -3.8207526 -2.6958354 -2.7260725 -3.011452 -4.8910661 -7.6820583 -9.7324238 -11.243299][-3.2621746 -4.33634 -4.7043409 -3.5479062 -3.1232612 -2.6874373 -2.2359984 -2.23949 -1.8606966 -1.5961299 -1.9736423 -5.4746094 -8.8109493 -10.337042 -11.63188][-2.9401689 -2.6989791 -2.7060564 -2.4159977 -1.3336625 1.0437608 1.6924076 1.5200815 1.4592524 -0.048796654 -1.6152186 -4.0733805 -6.9370055 -8.7326145 -10.269412][-3.717968 -3.2050071 -2.6802166 -1.2288871 -0.038665771 1.5685177 3.223453 4.3884826 4.6203303 2.4708295 0.35995817 -3.20058 -6.486165 -8.0676727 -8.8000822][-4.1924706 -3.1067512 -1.9211414 -0.010248184 0.78260756 2.4874682 4.2229486 4.1822858 3.9922957 2.2915845 0.24289751 -4.0238218 -7.5698433 -9.6419487 -9.9844475][-4.5661569 -4.0720019 -2.8440433 -1.1291523 0.16445446 1.7205915 2.3852754 1.7394509 1.7560606 0.59248114 -0.70893717 -4.4414382 -7.7389708 -9.2968435 -9.2847433][-5.6848688 -5.10621 -3.8211126 -2.1299469 -0.94000006 -0.38828897 -0.66530657 -1.2862892 -1.726681 -2.0904567 -1.6532178 -4.08095 -6.2327509 -7.1751633 -7.6298103][-9.0826721 -8.4157734 -6.6735411 -4.6994762 -3.6374471 -3.8799057 -4.0712819 -4.909318 -5.5153151 -4.949717 -3.7439671 -5.2756491 -5.0138206 -4.711359 -4.5957861][-12.256751 -11.338955 -9.1713972 -7.5906634 -7.476759 -7.5719018 -7.7295103 -7.9880924 -8.2117043 -7.7878742 -6.8637977 -6.6161308 -5.3837833 -4.5939097 -3.7016923][-10.597878 -10.000351 -8.9363861 -8.692173 -8.9059658 -9.0289526 -9.1993637 -9.284174 -9.2201023 -9.0610189 -8.1189079 -7.8397608 -6.7435679 -5.0492992 -3.4907465][-7.8069363 -8.0934029 -8.0453873 -7.1576281 -8.22913 -9.10139 -9.0076218 -8.0896273 -7.9676404 -8.3391562 -8.7293968 -8.1936455 -7.4084706 -6.7278266 -6.298059][-5.8895435 -6.3428564 -6.4636788 -6.5679107 -6.410408 -6.1050725 -6.2901182 -6.2503567 -6.662981 -6.8872805 -7.5111237 -9.0895739 -9.7592964 -9.7222652 -9.7067423]]...]
INFO - root - 2017-12-15 19:55:45.851662: step 52210, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 50h:26m:16s remains)
INFO - root - 2017-12-15 19:55:52.468171: step 52220, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 51h:45m:44s remains)
INFO - root - 2017-12-15 19:55:59.073131: step 52230, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 51h:02m:01s remains)
INFO - root - 2017-12-15 19:56:05.724881: step 52240, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 53h:11m:35s remains)
INFO - root - 2017-12-15 19:56:12.343698: step 52250, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 51h:12m:01s remains)
INFO - root - 2017-12-15 19:56:19.030610: step 52260, loss = 0.11, batch loss = 0.07 (11.7 examples/sec; 0.681 sec/batch; 53h:01m:38s remains)
INFO - root - 2017-12-15 19:56:25.645730: step 52270, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 53h:08m:02s remains)
INFO - root - 2017-12-15 19:56:32.279033: step 52280, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 49h:38m:35s remains)
INFO - root - 2017-12-15 19:56:38.868884: step 52290, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 50h:34m:35s remains)
INFO - root - 2017-12-15 19:56:45.455319: step 52300, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.658 sec/batch; 51h:14m:57s remains)
2017-12-15 19:56:45.908259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4441354 -2.146461 -2.1253841 -1.4566369 -2.3280983 -3.4100547 -4.7386336 -3.831672 -2.6792371 -2.954659 -3.4161077 -6.5527077 -8.4334383 -9.9258938 -9.7814951][-1.4167233 -0.11536074 -0.20995188 -0.4677763 -2.3224831 -3.1274734 -3.7188408 -4.8733225 -5.8936973 -5.5518141 -5.1763468 -8.3229342 -10.104208 -12.091577 -11.97843][-0.8604331 -0.035795212 0.34049177 1.8555436 1.096889 -0.24864769 -2.1659608 -3.8556333 -4.9138393 -5.9566422 -6.8065858 -9.3098221 -10.451251 -11.958985 -11.399509][-2.11732 -0.17228889 0.89212465 1.4620852 0.229815 -0.15461874 -0.44093895 -1.6049957 -2.8082495 -3.89679 -4.6624146 -7.6570673 -9.4783411 -10.610792 -10.418129][-2.8325508 -2.9779639 -2.5698848 -0.45532942 -0.35446596 -0.071703434 -0.0051059723 -0.82956219 -1.9151886 -2.9606726 -3.771445 -6.3427005 -7.5945468 -9.4456425 -9.8504448][-4.0254521 -3.4815412 -2.7567294 -0.78279543 0.062875748 1.5522499 2.1307349 1.7576909 1.0554132 -0.56339407 -2.1386118 -4.6908317 -6.2875748 -7.6962395 -7.898459][-5.6849656 -4.8082132 -3.2762113 -0.69041443 1.0059371 2.8450894 3.6965327 3.7520642 3.2833018 1.5221343 -0.57470465 -3.8369672 -5.8861728 -8.2971258 -8.5909481][-6.9277081 -6.0885296 -4.2238488 -1.359304 0.03148365 2.3232961 4.3056836 4.39718 4.2823043 3.5630622 1.9436097 -2.3547854 -5.5008812 -7.8014021 -8.9514484][-7.3212314 -6.6168127 -5.522934 -2.7896097 -1.2209229 0.65899134 2.3617702 3.0962319 3.5733218 2.6538749 1.2079597 -2.6970837 -5.6511073 -7.8500071 -8.0909214][-8.95025 -7.5889187 -5.8143458 -3.1056008 -1.674067 -0.48151064 0.63548088 1.020721 1.5154047 0.97222281 -0.023077488 -3.0525613 -5.361588 -6.7874603 -6.768024][-11.582922 -9.7437935 -7.9276218 -5.0676341 -3.7251143 -3.1811254 -2.1989455 -2.0010345 -1.9805088 -2.3004813 -3.1886616 -6.0636578 -7.809824 -8.3886089 -7.5027275][-14.418257 -12.762218 -11.19453 -9.1202526 -8.05048 -6.7148242 -5.6208596 -5.6588168 -5.9477229 -6.2215633 -6.7615585 -8.3836966 -9.4373226 -9.7143984 -8.5689325][-13.984018 -12.698952 -11.200768 -10.111467 -10.106665 -9.5016079 -8.6698666 -8.1571407 -7.9409194 -8.0948219 -8.5101862 -9.1631117 -9.18098 -8.2311716 -7.7194929][-11.532761 -10.793619 -10.129744 -8.4656458 -7.6755161 -7.5718565 -8.2422276 -8.1103773 -7.8280592 -7.9826522 -7.8946857 -8.0456858 -8.0095425 -7.3576651 -6.8687358][-8.527914 -7.2722964 -6.0716095 -4.8687725 -4.4758725 -4.135603 -3.7919471 -4.6586242 -5.7882719 -6.2761045 -6.8267097 -8.2606649 -8.78524 -8.4378033 -8.5564709]]...]
INFO - root - 2017-12-15 19:56:52.557095: step 52310, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 51h:43m:51s remains)
INFO - root - 2017-12-15 19:56:59.161657: step 52320, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 49h:35m:28s remains)
INFO - root - 2017-12-15 19:57:05.712890: step 52330, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 51h:12m:44s remains)
INFO - root - 2017-12-15 19:57:12.273741: step 52340, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 51h:42m:53s remains)
INFO - root - 2017-12-15 19:57:18.887950: step 52350, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.690 sec/batch; 53h:43m:13s remains)
INFO - root - 2017-12-15 19:57:25.545481: step 52360, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 50h:43m:13s remains)
INFO - root - 2017-12-15 19:57:32.136183: step 52370, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 51h:43m:46s remains)
INFO - root - 2017-12-15 19:57:38.742296: step 52380, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.681 sec/batch; 52h:57m:14s remains)
INFO - root - 2017-12-15 19:57:45.354168: step 52390, loss = 0.21, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 50h:38m:25s remains)
INFO - root - 2017-12-15 19:57:51.889886: step 52400, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 50h:05m:04s remains)
2017-12-15 19:57:52.524184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9351463 -8.0471191 -9.3751507 -10.930217 -12.323151 -12.54067 -11.712055 -10.206118 -9.144701 -7.6266985 -5.75686 -5.7090497 -8.2360182 -8.7107544 -8.2579012][-6.5842619 -7.406333 -8.2693062 -8.5625992 -10.47296 -12.210636 -12.834754 -11.737126 -10.389803 -8.5267992 -7.0552907 -7.2394643 -9.7269859 -11.398712 -11.036636][-4.2128382 -5.5868917 -6.8079462 -7.1570344 -8.0308342 -8.47224 -8.6759014 -9.1313963 -9.7396564 -8.8989944 -7.7389917 -7.5320091 -9.8760262 -11.361835 -11.908813][-4.619082 -4.8209157 -4.6374111 -4.3998008 -5.2157 -5.7185869 -5.9451241 -6.050014 -5.8377547 -5.7904377 -5.89619 -6.1910648 -8.45443 -9.7302856 -10.686727][-5.9538689 -6.0022264 -4.8801637 -2.0433557 -0.35404444 -0.93279171 -2.6345539 -4.6944146 -5.4730048 -5.5079517 -4.8514938 -5.2462335 -7.6987114 -8.9746122 -9.2403831][-7.853817 -7.3906984 -5.1716537 -2.0441618 0.55774212 2.3941846 2.6548333 -0.028944016 -2.3916974 -3.5901892 -4.0343351 -3.7747188 -5.6775055 -7.5123491 -8.6137905][-7.0701551 -6.128684 -3.3445232 0.54983759 2.4173741 4.3219619 4.8568635 4.0100837 2.6041455 -0.27029181 -2.2146757 -3.3690467 -5.4881129 -6.20063 -7.3767781][-7.1687441 -5.8175168 -2.4588096 1.239172 3.3558316 5.0430455 4.9914451 3.9470878 3.3569818 1.994523 0.032600403 -2.4038787 -6.0151057 -6.3412695 -5.1875973][-6.608933 -6.0509791 -4.186944 -0.616457 1.2087154 2.3982463 2.4577384 2.2474875 1.466424 0.2369175 -0.47762251 -1.8099544 -4.8945069 -6.366993 -5.4307318][-7.3315768 -6.5114241 -4.7755542 -3.1465311 -2.9527957 -2.4970965 -2.0066867 -1.046854 -0.51609325 -1.2465682 -2.0120993 -3.1081829 -5.3255906 -5.64185 -4.4359756][-11.148134 -9.8607254 -9.3809586 -7.7775841 -7.1576447 -7.2419758 -6.9684362 -5.6993394 -4.637 -4.0188713 -4.3608031 -5.2359786 -6.7372589 -6.1166687 -3.7158701][-12.795248 -11.627293 -11.097876 -10.242748 -9.1489248 -7.2915912 -5.8973074 -5.2788682 -5.1305289 -6.0022316 -7.0220594 -7.6604605 -7.7870646 -6.5645833 -4.5644073][-13.003458 -12.237156 -11.615917 -11.298816 -10.182077 -8.1732168 -5.3950071 -4.2119374 -4.6031303 -5.9598241 -7.3907971 -8.2601223 -8.4543467 -6.5049248 -3.6517458][-11.948706 -10.729954 -9.5991182 -9.1854706 -7.2897058 -5.9649811 -4.6098843 -2.3939843 -1.6194954 -3.0958216 -4.8388724 -5.5948024 -5.8708119 -5.106926 -4.0802097][-8.6429453 -8.350975 -7.762342 -7.3476872 -6.8843131 -5.298048 -2.7287021 -1.139956 -0.6437974 -0.71719837 -1.9647918 -3.3729224 -4.3716011 -4.1396151 -3.6654794]]...]
INFO - root - 2017-12-15 19:57:59.095807: step 52410, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 52h:54m:37s remains)
INFO - root - 2017-12-15 19:58:05.621623: step 52420, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 49h:39m:18s remains)
INFO - root - 2017-12-15 19:58:12.253900: step 52430, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 49h:33m:38s remains)
INFO - root - 2017-12-15 19:58:18.830771: step 52440, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 50h:13m:32s remains)
INFO - root - 2017-12-15 19:58:25.423212: step 52450, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 50h:05m:15s remains)
INFO - root - 2017-12-15 19:58:32.023947: step 52460, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.683 sec/batch; 53h:07m:46s remains)
INFO - root - 2017-12-15 19:58:38.522335: step 52470, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 50h:59m:05s remains)
INFO - root - 2017-12-15 19:58:45.063616: step 52480, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 50h:09m:50s remains)
INFO - root - 2017-12-15 19:58:51.692822: step 52490, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 50h:46m:18s remains)
INFO - root - 2017-12-15 19:58:58.240377: step 52500, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 50h:22m:33s remains)
2017-12-15 19:58:58.758105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9687924 -5.1837034 -4.5232439 -2.7697346 -1.7985735 -1.349473 -1.5319304 -1.2692108 -1.0605707 -0.79721975 -0.819571 -2.4127409 -2.5849223 -2.8256774 -2.9257586][-2.7852859 -2.1702156 -1.4896483 -0.98056269 -1.8859563 -1.4758902 -1.3019967 -1.4838552 -1.7038651 -1.1425405 -0.23985863 -1.7879641 -2.3082304 -2.4819753 -2.2483532][-1.9315491 -2.1160319 -2.0520163 -1.3126678 -1.0971494 -1.6021619 -2.5721636 -2.743058 -2.0687137 -1.4591265 -0.72578955 -2.5956657 -3.5541859 -4.9530854 -5.5124965][-4.6036468 -4.3699265 -3.9035192 -3.0480905 -2.5256262 -2.7092512 -3.7212813 -3.6320267 -3.4087589 -3.2251561 -2.7075808 -5.1130252 -6.6239247 -7.3712406 -8.01408][-6.0630679 -6.3332763 -5.9398336 -4.6040721 -3.4268641 -2.1708317 -2.348238 -2.8565304 -3.8409762 -4.3359561 -4.23267 -7.2383513 -8.8474236 -9.7049065 -9.8852205][-7.2217107 -5.4447546 -3.4307809 -2.094964 -1.2016201 -0.11906195 0.084269047 0.20456266 -0.35330915 -2.1260183 -4.4446683 -7.03048 -8.2357092 -9.7829323 -10.365285][-6.304924 -4.4347157 -2.0777297 -0.87556171 -0.51332808 -0.16505527 0.500185 1.1852846 1.1964583 -0.19318724 -2.0756574 -6.4893985 -9.4943981 -11.051533 -10.876764][-7.7047472 -5.7342415 -3.1162927 -0.79996586 -0.57035494 -0.7280426 -1.1326776 -1.1543112 0.2692523 -0.52235794 -2.1803925 -6.0382729 -8.5015554 -10.985266 -11.384735][-7.6758013 -6.5622396 -3.8260169 -1.6419811 -1.1330838 -0.93036413 -1.5614004 -2.1529348 -1.894757 -1.6521649 -1.9936934 -6.1671305 -9.1601191 -10.559082 -9.3924065][-8.293128 -7.2956557 -5.7370539 -3.3458803 -1.7819211 -1.4456873 -1.7582395 -0.90916967 -0.49460793 -0.65472794 -1.6179485 -4.881381 -6.7298861 -8.85407 -9.0298738][-13.033551 -11.523598 -8.8138924 -6.6900053 -5.2325096 -4.0614958 -3.1198478 -3.0065651 -3.4103856 -2.7862439 -2.7144012 -5.8499475 -7.554635 -8.5350428 -7.469202][-14.696007 -14.350897 -12.012426 -9.0164175 -7.3219748 -6.7946177 -7.1948462 -7.1452446 -6.7302155 -6.4813223 -6.8046122 -9.0026789 -8.8422356 -8.5511656 -7.880291][-13.57347 -11.724229 -9.8521957 -8.9809 -8.8291779 -7.7106123 -7.6918292 -8.2132168 -8.9659557 -9.2246876 -8.4751015 -9.015974 -8.4304428 -7.7942772 -6.2041769][-11.511728 -10.259026 -8.523118 -6.2146072 -4.9458294 -6.2544923 -8.0977459 -7.9275527 -7.6072335 -8.5375576 -9.0918512 -9.0136137 -8.0332642 -6.78543 -5.8676357][-7.7846689 -8.0420113 -7.176115 -6.414145 -6.8077812 -6.2758975 -5.7492523 -6.2672973 -7.5384173 -7.5476036 -7.3981314 -8.80029 -9.7784882 -9.2512684 -8.9118786]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-52500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-52500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 19:59:06.412287: step 52510, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 51h:40m:35s remains)
INFO - root - 2017-12-15 19:59:12.963665: step 52520, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 49h:38m:42s remains)
INFO - root - 2017-12-15 19:59:19.522849: step 52530, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 50h:31m:09s remains)
INFO - root - 2017-12-15 19:59:26.065456: step 52540, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 51h:33m:18s remains)
INFO - root - 2017-12-15 19:59:32.664918: step 52550, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.675 sec/batch; 52h:29m:57s remains)
INFO - root - 2017-12-15 19:59:39.247531: step 52560, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 50h:09m:54s remains)
INFO - root - 2017-12-15 19:59:45.737751: step 52570, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 49h:58m:30s remains)
INFO - root - 2017-12-15 19:59:52.232345: step 52580, loss = 0.27, batch loss = 0.22 (12.2 examples/sec; 0.657 sec/batch; 51h:04m:51s remains)
INFO - root - 2017-12-15 19:59:58.813247: step 52590, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 50h:29m:28s remains)
INFO - root - 2017-12-15 20:00:05.366750: step 52600, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 50h:32m:42s remains)
2017-12-15 20:00:05.907495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0380144 -3.9393258 -5.3879094 -5.8796153 -5.9841862 -6.2529945 -6.2511683 -5.5270038 -4.8791041 -4.9466429 -5.1575627 -8.1050282 -11.60877 -12.404629 -10.372266][-4.9405184 -5.13999 -5.6474962 -5.5156503 -6.3603163 -6.699985 -5.6601124 -4.7714128 -3.9351025 -3.9112022 -4.7605486 -7.926198 -11.026542 -12.926819 -11.101468][-4.9917955 -5.6880975 -6.4282074 -6.6134 -6.8848052 -6.4661665 -5.6629267 -4.8351245 -4.1975727 -4.2308607 -4.3529577 -7.2634187 -10.262777 -11.727215 -11.092388][-3.2158148 -4.0482664 -5.517066 -5.6821861 -6.6654124 -5.3835897 -3.7768533 -3.7880774 -3.6166067 -3.994607 -4.84306 -7.7765503 -10.672646 -11.871454 -10.094316][-3.6505249 -3.3827674 -3.7588902 -3.8875675 -4.409337 -2.8026452 -2.4634364 -2.1006486 -1.9917824 -3.1873088 -4.2760763 -7.4558311 -10.543762 -11.546492 -9.546113][-3.6876254 -3.2697425 -2.9934895 -2.0566304 -1.6333494 0.1195941 0.56729746 0.63329077 0.023620605 -1.6997561 -3.6640034 -7.3376846 -10.674717 -11.181446 -9.5720968][-4.2771997 -4.2921619 -3.6573544 -1.8740005 -0.53439903 1.2664151 2.2495046 2.7886672 2.6460576 0.63823557 -1.8573234 -6.2481971 -9.6027069 -10.742124 -9.7892466][-3.447257 -3.2467272 -2.6829607 -0.42791557 0.91753912 2.0981536 2.2112904 2.5520597 3.1208491 1.648067 -0.31802607 -4.4341593 -8.160471 -9.9052792 -9.0362129][-2.3741374 -2.0099313 -1.5218868 -0.002779007 1.1050062 2.3027577 1.6715813 1.2295551 1.3722935 0.99361086 0.083707333 -4.0656905 -7.7480645 -9.5173693 -8.4115276][-2.4306796 -1.5675611 -1.4517412 0.1435008 1.5723772 2.2099504 1.7538538 1.9195585 1.5793691 0.062408924 -0.14364767 -3.6751816 -7.447453 -9.2225285 -8.482255][-3.8242 -2.9764369 -1.8535295 -1.3819666 -0.55761003 0.87712383 1.5977073 1.6277704 1.0335255 -0.078724384 -2.1414104 -5.2321882 -8.0014439 -9.5825634 -9.0642939][-6.0714884 -5.1187282 -4.6838427 -3.2389295 -2.5062668 -2.3457448 -1.9561546 -1.7022514 -2.0738547 -2.7846005 -3.369076 -6.2721157 -8.0278931 -9.6929684 -9.2621][-8.4048805 -6.4333534 -5.2631874 -5.02333 -4.7611609 -4.2633014 -4.2071104 -4.0968947 -4.5259957 -5.8743072 -6.4426174 -7.59315 -9.380187 -9.6589985 -8.3356323][-7.8510284 -6.7689133 -5.7017469 -4.799407 -4.9640746 -4.9113779 -4.8846388 -5.4513874 -6.1669521 -6.7736197 -7.2417412 -8.2137337 -8.7583475 -9.8232422 -9.5653563][-5.7842679 -5.0968857 -5.0134277 -5.201458 -5.7883873 -5.4704857 -4.8943744 -4.968195 -6.3289862 -7.2921591 -8.2737207 -9.7075806 -10.379274 -10.220472 -10.378225]]...]
INFO - root - 2017-12-15 20:00:12.559465: step 52610, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 50h:07m:01s remains)
INFO - root - 2017-12-15 20:00:19.147120: step 52620, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 50h:57m:48s remains)
INFO - root - 2017-12-15 20:00:25.738938: step 52630, loss = 0.30, batch loss = 0.25 (11.6 examples/sec; 0.687 sec/batch; 53h:23m:31s remains)
INFO - root - 2017-12-15 20:00:32.347372: step 52640, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 51h:45m:17s remains)
INFO - root - 2017-12-15 20:00:38.937248: step 52650, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 51h:34m:51s remains)
INFO - root - 2017-12-15 20:00:45.456336: step 52660, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 50h:09m:11s remains)
INFO - root - 2017-12-15 20:00:52.045228: step 52670, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.679 sec/batch; 52h:46m:52s remains)
INFO - root - 2017-12-15 20:00:58.732241: step 52680, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.697 sec/batch; 54h:08m:25s remains)
INFO - root - 2017-12-15 20:01:05.431817: step 52690, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 49h:59m:55s remains)
INFO - root - 2017-12-15 20:01:12.037190: step 52700, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 50h:51m:25s remains)
2017-12-15 20:01:12.596243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3058934 -4.5972214 -5.0890942 -5.2834754 -5.2529097 -6.2668185 -7.3119659 -7.2042046 -7.3416824 -7.704257 -7.9428959 -9.2331018 -10.645893 -11.180537 -11.027821][-3.7916083 -3.249593 -3.3202748 -5.0655479 -5.86796 -6.6528492 -7.9019585 -8.8868551 -9.7672434 -9.4311228 -9.3436632 -10.965229 -11.835189 -12.278232 -12.252826][-2.0519254 -2.0312383 -4.1610589 -4.592967 -5.191977 -6.705821 -7.462203 -8.3472815 -9.522007 -10.42944 -10.69232 -11.14206 -11.978084 -11.980486 -11.78524][-1.8494997 -2.5543518 -3.7193851 -4.4433703 -4.9812555 -4.6130428 -4.19594 -4.7783427 -6.0470638 -7.4165187 -8.2528038 -10.115344 -10.969368 -11.395693 -11.094896][-3.5981786 -4.6070223 -5.0412068 -4.965055 -4.7851028 -2.836107 -1.4936256 -1.7016559 -2.4815168 -4.1582584 -5.688725 -7.2231431 -8.4417686 -9.113451 -9.5585089][-7.3964291 -7.4135761 -6.2944036 -5.4766197 -3.74965 -1.5979986 1.0658269 1.774375 2.0587473 0.02082777 -1.9710658 -4.285553 -6.6691203 -7.4924335 -7.9063153][-9.5458069 -8.8969593 -6.931839 -4.6118164 -2.351074 0.31796026 3.0636563 4.4790111 6.4779277 3.6406341 0.2756114 -2.5275116 -6.3827658 -8.211133 -8.8371677][-8.1741657 -7.8399992 -7.0646286 -4.6603727 -2.216141 0.75509977 3.400125 5.0249743 6.065413 3.8670993 1.3651576 -3.0812862 -7.7736335 -9.3254223 -10.812353][-4.759109 -4.602088 -3.2746713 -2.7381341 -2.1797748 -0.25907803 0.73029232 2.1149983 3.8436952 3.26782 2.162353 -3.5562487 -9.1906338 -12.231825 -15.015516][-1.7393389 -1.1037216 -1.4598732 -0.32701159 -0.038793564 -0.58068132 -0.98339033 -1.325768 -1.4938798 -0.42811108 -0.032360077 -4.0828729 -7.7851152 -12.209026 -16.146135][-6.1839252 -5.1194859 -4.1721058 -3.5037405 -3.6971748 -3.5554571 -3.9101453 -5.9485669 -6.9072394 -5.8753619 -5.1512852 -7.1238832 -9.9661913 -11.938458 -13.685595][-9.9755554 -9.6379719 -9.2857857 -8.5127525 -8.9205065 -8.2536306 -7.8264523 -9.8768253 -11.155533 -11.740824 -11.422837 -11.128977 -10.729229 -11.454606 -11.993196][-15.785114 -15.362234 -14.193995 -13.808201 -13.987272 -13.324871 -12.846952 -13.086517 -12.703789 -13.036233 -12.823296 -13.62905 -13.95447 -12.724599 -10.578849][-13.791376 -13.343639 -13.0632 -11.580956 -10.979986 -11.415558 -11.51761 -11.428469 -11.307625 -11.43568 -11.455132 -10.957153 -10.260737 -10.010317 -9.4724016][-9.3260059 -9.2634859 -9.148325 -8.6418037 -7.800703 -7.2815766 -7.4502125 -8.1202793 -8.7128773 -8.8019791 -9.4858894 -9.7541456 -10.216084 -8.6623163 -7.9558206]]...]
INFO - root - 2017-12-15 20:01:19.277553: step 52710, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 50h:59m:29s remains)
INFO - root - 2017-12-15 20:01:25.916846: step 52720, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 51h:12m:13s remains)
INFO - root - 2017-12-15 20:01:32.481528: step 52730, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 50h:12m:57s remains)
INFO - root - 2017-12-15 20:01:39.124052: step 52740, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 50h:40m:22s remains)
INFO - root - 2017-12-15 20:01:45.694817: step 52750, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.680 sec/batch; 52h:51m:08s remains)
INFO - root - 2017-12-15 20:01:52.193987: step 52760, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 49h:47m:58s remains)
INFO - root - 2017-12-15 20:01:58.786447: step 52770, loss = 0.21, batch loss = 0.16 (12.4 examples/sec; 0.644 sec/batch; 50h:02m:31s remains)
INFO - root - 2017-12-15 20:02:05.418374: step 52780, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 51h:00m:11s remains)
INFO - root - 2017-12-15 20:02:12.012968: step 52790, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 50h:27m:54s remains)
INFO - root - 2017-12-15 20:02:18.633542: step 52800, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 51h:20m:52s remains)
2017-12-15 20:02:19.120571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7227087 -6.347548 -6.1621304 -5.9732828 -6.5798111 -7.3787704 -7.8077188 -7.9750414 -8.0711994 -7.8388619 -6.4964719 -5.1069422 -5.2934051 -7.1655121 -6.7598786][-6.712286 -7.3088474 -8.0383511 -8.5210361 -9.1114111 -9.5828762 -9.6279964 -9.7603474 -10.035073 -9.0378637 -7.5936069 -6.2407913 -6.6467032 -7.9365959 -7.1735477][-6.3418508 -7.4850192 -8.6595478 -8.7880449 -10.093261 -11.169445 -11.487217 -10.483341 -10.170713 -9.9554958 -8.4539595 -7.2654228 -7.690794 -8.6546373 -7.8883257][-6.1020894 -7.0566034 -8.0158205 -9.0726881 -11.272184 -10.364443 -9.3037834 -9.5752878 -9.2027178 -8.6243076 -8.4388475 -8.1867695 -9.0057106 -10.930828 -9.0022326][-6.5458112 -8.4483633 -9.7226019 -9.1679058 -9.2270832 -6.3310165 -5.0896139 -6.57638 -7.5125751 -6.7777319 -6.9298706 -7.32034 -9.859314 -12.118301 -11.052536][-8.635994 -8.8904743 -9.7547131 -9.2951965 -7.6823077 -2.7197137 2.35049 1.4160104 -1.1144137 -4.4970713 -6.5679946 -5.8805308 -8.8069429 -11.573746 -11.249569][-9.42404 -9.0378494 -9.3447514 -8.4024258 -5.1271477 1.2860136 7.5390124 7.1727929 4.87108 -0.94393349 -4.85632 -5.8675919 -8.2970219 -10.123871 -9.7038469][-9.9640007 -9.0365963 -8.7354746 -7.5155735 -3.1009657 4.4309621 10.191402 10.780519 8.8053589 2.8268723 -2.4412928 -4.8209009 -8.5742025 -10.132986 -9.23354][-8.2733841 -7.688581 -7.5107484 -7.6395283 -4.77618 2.2684155 7.9430108 9.3197021 6.8655829 1.3055348 -1.825088 -4.9024472 -8.879612 -10.65861 -9.6067333][-8.2036858 -7.7796745 -8.1465769 -6.8294392 -5.624939 -1.9740233 2.1554308 5.1741433 3.4733386 -1.8931551 -5.5480933 -8.1790867 -10.500619 -12.893086 -12.614731][-11.125658 -10.573909 -10.828878 -9.3408279 -9.0731869 -7.4172082 -6.0665226 -3.7162356 -3.6983435 -6.1064529 -9.1434546 -11.58573 -13.443825 -14.383032 -12.861753][-14.655701 -12.932213 -12.228878 -11.38938 -10.778746 -10.663955 -10.857967 -9.637064 -10.033783 -10.793302 -11.956783 -12.44861 -13.061403 -14.221067 -10.901277][-14.997122 -12.915634 -11.616219 -10.926764 -10.776163 -10.879347 -10.909551 -10.680225 -10.982591 -11.223932 -10.904385 -10.367015 -12.633814 -12.318097 -8.3508205][-11.945639 -10.525852 -9.9723015 -8.8370476 -8.3431969 -9.4657068 -9.4499989 -8.68802 -8.5972319 -8.8973427 -8.90102 -7.8921652 -8.2481642 -8.6404343 -6.911026][-7.1382003 -6.9624405 -5.5000358 -4.7945628 -5.15943 -6.08828 -6.6519766 -6.6115746 -5.9356937 -5.5966597 -5.8432941 -6.8378925 -7.8170471 -7.4790039 -6.4031997]]...]
INFO - root - 2017-12-15 20:02:25.816963: step 52810, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 50h:31m:26s remains)
INFO - root - 2017-12-15 20:02:32.360901: step 52820, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 51h:04m:16s remains)
INFO - root - 2017-12-15 20:02:38.945292: step 52830, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 52h:38m:29s remains)
INFO - root - 2017-12-15 20:02:45.489245: step 52840, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 50h:35m:55s remains)
INFO - root - 2017-12-15 20:02:52.066081: step 52850, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.691 sec/batch; 53h:40m:28s remains)
INFO - root - 2017-12-15 20:02:58.607577: step 52860, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 52h:17m:40s remains)
INFO - root - 2017-12-15 20:03:05.178442: step 52870, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 51h:02m:13s remains)
INFO - root - 2017-12-15 20:03:11.777795: step 52880, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 50h:41m:52s remains)
INFO - root - 2017-12-15 20:03:18.381119: step 52890, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.648 sec/batch; 50h:20m:31s remains)
INFO - root - 2017-12-15 20:03:24.937869: step 52900, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.691 sec/batch; 53h:39m:34s remains)
2017-12-15 20:03:25.442075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2657671 -4.3233738 -4.0987091 -3.40149 -3.8108311 -4.7448454 -5.6282511 -6.3552117 -6.8948612 -7.3232307 -6.8322091 -5.6225619 -5.7590613 -7.2674785 -7.95][-6.278017 -5.23909 -4.6661158 -5.022737 -5.6085868 -6.6770272 -7.5350552 -7.9834604 -8.0653391 -7.3431654 -7.5252 -7.5482717 -8.522337 -7.8233232 -6.2263842][-6.2039609 -6.5717072 -6.4873581 -5.7811265 -6.1254125 -5.990778 -6.2794447 -7.147069 -7.4758959 -7.260603 -7.1182394 -5.9904423 -5.9049907 -6.3814006 -5.6263156][-4.2141075 -4.6733785 -4.7253671 -5.5121846 -6.642838 -7.3242188 -7.669939 -8.3636131 -7.6047306 -6.3956447 -5.9204931 -5.6284442 -6.6558523 -6.7618027 -5.9258833][-5.6670737 -5.8503528 -6.5131774 -6.5919428 -6.1705332 -4.8131061 -4.1973448 -4.1997514 -4.8979454 -4.2572746 -4.3477697 -3.6040375 -4.9199677 -6.3040972 -6.514966][-6.7536163 -5.812593 -4.4732714 -3.107553 -2.8681872 -1.1166763 1.3327541 1.3942356 0.73241282 -0.67661524 -1.7125587 -1.9818115 -4.0125313 -5.4595423 -6.7522058][-6.9209557 -7.3035212 -5.9893141 -4.3533783 -2.4916618 -0.27123165 3.5536504 5.9474235 5.9883275 3.1384816 -0.50090218 -2.4164741 -4.6870861 -6.2330384 -6.7675309][-7.1883144 -7.936264 -6.6255126 -3.6359854 -1.5272732 2.3312287 5.3175178 6.8605971 6.970058 4.1895986 0.381557 -2.72543 -5.9674959 -6.3758039 -7.2775946][-7.3741817 -8.0753231 -6.3837729 -3.8888073 -2.2165711 1.3560009 3.8930621 5.1860862 5.7406898 3.5421224 0.37362671 -2.4424183 -5.3203835 -6.9199958 -6.4840417][-5.945147 -6.5304008 -5.8385019 -3.9137132 -2.7826042 0.25004864 2.6100593 4.1815295 3.72759 0.68819094 -2.1530666 -3.1712735 -4.358839 -6.4390583 -8.4038229][-8.2976227 -7.6275673 -6.3348975 -4.9159136 -3.9028394 -3.0102649 -2.8498852 -1.5175977 -1.379065 -3.2413449 -5.5032439 -5.9622235 -7.2495389 -6.1545839 -4.8124967][-9.4856606 -9.6916208 -8.9081907 -7.5738955 -5.9546475 -6.1204205 -6.0664015 -5.7449174 -6.2972078 -7.2775126 -8.6027527 -8.8123379 -7.87481 -7.9422803 -6.4712009][-10.503339 -10.194832 -9.4898548 -7.8749294 -7.5895452 -6.9060392 -7.4508572 -8.2678185 -9.0720463 -9.6239395 -9.6727962 -10.143136 -10.548695 -8.9602833 -6.8535275][-7.4996386 -8.1752911 -8.1939859 -7.9510994 -7.7024484 -7.8173943 -8.0507727 -7.8613505 -7.834136 -8.3376932 -9.2539358 -8.1518564 -7.0927229 -7.5014334 -7.65996][-5.8839946 -8.0326414 -9.1111336 -7.6403608 -6.7105088 -6.5118113 -6.2159281 -5.6689386 -5.14201 -6.0052791 -6.4732914 -7.0612311 -7.8479872 -7.8337212 -8.57349]]...]
INFO - root - 2017-12-15 20:03:31.950623: step 52910, loss = 0.13, batch loss = 0.08 (12.7 examples/sec; 0.629 sec/batch; 48h:50m:00s remains)
INFO - root - 2017-12-15 20:03:38.556438: step 52920, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 51h:11m:49s remains)
INFO - root - 2017-12-15 20:03:45.221165: step 52930, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.693 sec/batch; 53h:49m:07s remains)
INFO - root - 2017-12-15 20:03:51.773374: step 52940, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 52h:13m:07s remains)
INFO - root - 2017-12-15 20:03:58.458345: step 52950, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 50h:52m:02s remains)
INFO - root - 2017-12-15 20:04:05.072410: step 52960, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 51h:09m:18s remains)
INFO - root - 2017-12-15 20:04:11.738964: step 52970, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 51h:06m:52s remains)
INFO - root - 2017-12-15 20:04:18.329338: step 52980, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 50h:08m:19s remains)
INFO - root - 2017-12-15 20:04:24.955884: step 52990, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 52h:15m:05s remains)
INFO - root - 2017-12-15 20:04:31.444655: step 53000, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 50h:34m:51s remains)
2017-12-15 20:04:31.952043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1842732 -7.5471268 -8.7922678 -9.3434973 -10.613255 -11.609753 -12.252306 -12.385201 -12.350962 -12.212672 -11.689778 -12.514971 -10.666674 -11.589453 -9.138835][-10.518492 -9.0498486 -9.4992867 -9.3895988 -10.071987 -11.338627 -12.409044 -13.348553 -13.587767 -13.2759 -12.82659 -12.650257 -10.301277 -12.113655 -9.0309944][-9.1609583 -9.7585144 -10.96488 -10.197526 -10.66017 -10.931091 -11.450493 -12.052784 -12.904344 -13.585034 -13.550466 -13.929337 -11.656298 -13.161793 -9.8273859][-9.6819868 -9.41132 -11.111838 -10.750416 -10.816444 -9.2351284 -8.7314739 -10.457886 -11.072745 -11.230248 -12.061503 -13.004986 -10.978012 -12.319433 -9.3446436][-10.922819 -10.912706 -12.156123 -11.367995 -10.086814 -6.3258986 -4.2435656 -6.7805529 -9.7844 -10.105595 -10.722532 -12.075217 -10.997204 -12.905136 -9.2998838][-12.025394 -11.283443 -11.88131 -10.3828 -8.5043068 -2.3879647 2.6813807 1.3076301 -2.2082386 -6.7191267 -10.274693 -9.9079838 -8.2243109 -11.483387 -10.30929][-14.719995 -12.708261 -11.030591 -8.3254814 -6.0838537 -0.91960335 5.1085019 7.4735579 6.6139722 -1.2475123 -8.517065 -9.5151854 -8.2091408 -10.186433 -8.4527073][-14.990686 -14.351181 -13.122641 -6.57015 -1.419116 2.6485524 6.5158315 8.0491943 8.9000244 3.028501 -5.0248961 -9.4669733 -9.4905987 -10.500923 -8.470252][-11.173727 -10.906807 -12.420316 -8.5871878 -4.0966139 2.8846908 6.8620191 5.7550483 5.4762187 1.4568553 -4.7935376 -10.320917 -11.534138 -13.466732 -10.867165][-8.2793255 -7.4539428 -9.4500647 -8.8232317 -8.3691444 -2.6084738 2.0470452 2.1227674 1.7172542 -3.438997 -8.1031866 -11.644341 -12.916049 -15.397963 -13.544979][-11.457914 -10.139435 -10.590202 -10.237249 -11.017178 -9.2874756 -7.3735085 -6.1006441 -5.92719 -8.9507046 -12.086514 -15.214521 -14.761963 -16.56356 -14.357981][-15.185514 -14.119375 -14.288189 -12.605856 -12.467613 -12.692005 -13.786776 -13.607496 -12.851059 -13.845146 -14.171268 -16.168535 -15.052818 -15.489201 -12.644659][-14.808094 -14.675844 -14.717384 -14.011599 -14.172155 -12.69759 -13.028313 -14.353394 -14.376242 -13.543383 -13.095377 -14.582006 -13.262888 -12.975186 -9.6649418][-11.662157 -11.378696 -11.637236 -11.573822 -11.780449 -10.838863 -10.622246 -10.29605 -10.523157 -10.685412 -10.335609 -9.8648663 -8.9747505 -9.2499676 -7.1387129][-8.87292 -7.3730669 -6.3161683 -5.9692578 -5.6502552 -6.296864 -6.7188911 -5.6014361 -5.41713 -5.6904831 -5.9629774 -7.0840764 -7.0641065 -7.4369183 -6.7735829]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 20:04:38.553454: step 53010, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 50h:58m:42s remains)
INFO - root - 2017-12-15 20:04:45.147163: step 53020, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 51h:55m:14s remains)
INFO - root - 2017-12-15 20:04:51.788147: step 53030, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 50h:45m:18s remains)
INFO - root - 2017-12-15 20:04:58.483572: step 53040, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 51h:27m:59s remains)
INFO - root - 2017-12-15 20:05:05.053770: step 53050, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 51h:34m:12s remains)
INFO - root - 2017-12-15 20:05:11.656641: step 53060, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 50h:24m:56s remains)
INFO - root - 2017-12-15 20:05:18.226886: step 53070, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 50h:46m:03s remains)
INFO - root - 2017-12-15 20:05:24.828792: step 53080, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 50h:57m:41s remains)
INFO - root - 2017-12-15 20:05:31.440161: step 53090, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 50h:45m:37s remains)
INFO - root - 2017-12-15 20:05:38.066760: step 53100, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 51h:45m:24s remains)
2017-12-15 20:05:38.616543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4143538 -4.7236328 -4.6451097 -3.3159852 -2.5046313 -2.4437416 -1.1959481 0.48903847 1.1652727 -0.60208368 -2.9317169 -6.9289713 -10.400908 -14.390305 -14.645395][-3.0421653 -3.5661488 -3.9566422 -3.9559436 -3.7552028 -2.7541099 -0.71005726 -0.084838867 0.51615572 -0.42034149 -2.275928 -7.0369835 -10.7904 -15.137972 -17.592941][-2.817369 -3.6196575 -3.5978656 -3.1442177 -3.7519355 -3.4051492 -1.5052457 0.40230417 -0.043792248 -0.99086142 -2.0060511 -7.1215124 -11.023961 -14.509083 -16.126684][-1.6173625 -2.1733165 -2.335053 -1.5613103 -1.6586785 -0.9573307 -0.94711018 0.049641132 0.8382473 -0.15641451 -2.2671056 -6.6559124 -10.162254 -14.859571 -15.791204][-1.7206511 -1.5726128 -1.4463134 -1.9422505 -1.727442 -0.12698126 0.47574139 0.9154315 0.67828226 0.5539732 -1.9689403 -7.293211 -10.571415 -14.740978 -16.536903][-1.8478968 -3.1781771 -2.3364518 -1.4184341 -0.3019371 0.38862467 1.8274403 2.5249734 1.5944653 0.32635546 -1.8501925 -6.5955324 -11.472766 -14.734306 -15.663174][-2.8803549 -4.3228292 -4.1358042 -2.0535793 0.1939373 2.1389318 3.6940227 4.1055751 4.4476113 1.9046159 -1.0451975 -5.4789133 -9.1690321 -13.530921 -15.234362][-4.2684226 -4.5053945 -4.0933867 -2.3740995 -0.3788209 2.4749103 4.5454392 4.9983878 5.0482459 3.7767797 0.94819975 -4.2647142 -7.8764 -12.688179 -14.511246][-4.6288357 -6.4582882 -5.4454951 -2.9009275 -1.6972909 0.72675848 3.423902 4.6746573 4.3179231 2.9733224 0.69958115 -4.1358213 -8.5420856 -12.423927 -13.842758][-4.1002417 -5.3534327 -6.0502033 -4.6079721 -2.9360514 -1.2513824 0.39942932 2.1029849 2.3698792 0.56782293 -1.1059475 -4.8113809 -8.4658785 -12.234829 -13.604061][-5.8181696 -7.0619 -7.347261 -6.643239 -6.32605 -4.948102 -3.5886478 -2.8788805 -2.2307181 -2.9516137 -4.4196672 -8.8294382 -10.240677 -12.658745 -12.506277][-7.6108761 -8.1795139 -8.2431707 -7.5891623 -7.6046534 -7.5228672 -6.9045625 -6.7090654 -6.4565797 -6.4580011 -7.9685078 -10.191746 -11.11987 -12.479106 -12.924033][-9.0608664 -9.2165432 -8.1039448 -7.9378138 -8.2857132 -8.3166695 -8.3449221 -8.133296 -7.6817975 -8.2099695 -8.6966648 -9.9315968 -11.338161 -11.353717 -11.497126][-8.7440176 -8.4185734 -8.0066986 -7.3703032 -7.3221521 -8.1662922 -8.540678 -8.2828808 -7.8975573 -8.199872 -7.69034 -7.9037066 -8.1374187 -9.9888773 -10.432575][-9.6912642 -8.98856 -8.4428062 -8.0274715 -8.1786165 -8.0871553 -8.9407177 -9.0749741 -8.6714287 -8.2931147 -8.3486805 -9.0465326 -9.9651794 -11.012406 -11.250569]]...]
INFO - root - 2017-12-15 20:05:45.155811: step 53110, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 51h:17m:58s remains)
INFO - root - 2017-12-15 20:05:51.727339: step 53120, loss = 0.20, batch loss = 0.16 (12.3 examples/sec; 0.648 sec/batch; 50h:17m:39s remains)
INFO - root - 2017-12-15 20:05:58.275515: step 53130, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 51h:02m:21s remains)
INFO - root - 2017-12-15 20:06:04.810937: step 53140, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 49h:12m:31s remains)
INFO - root - 2017-12-15 20:06:11.458081: step 53150, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 50h:54m:46s remains)
INFO - root - 2017-12-15 20:06:18.083764: step 53160, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 49h:37m:58s remains)
INFO - root - 2017-12-15 20:06:24.631490: step 53170, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 52h:26m:03s remains)
INFO - root - 2017-12-15 20:06:31.136261: step 53180, loss = 0.21, batch loss = 0.17 (12.3 examples/sec; 0.652 sec/batch; 50h:36m:19s remains)
INFO - root - 2017-12-15 20:06:37.724209: step 53190, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 51h:09m:33s remains)
INFO - root - 2017-12-15 20:06:44.364420: step 53200, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.698 sec/batch; 54h:07m:50s remains)
2017-12-15 20:06:44.853591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8080807 -8.55876 -8.15038 -7.7752171 -8.7394218 -10.234889 -11.603566 -12.032238 -11.705998 -9.686142 -7.927 -8.1279278 -9.5785332 -10.278826 -10.16704][-9.6761084 -9.0048342 -8.65342 -8.1089 -8.2866182 -9.395359 -10.304525 -10.908791 -10.528664 -7.9842873 -5.3190293 -5.2839222 -6.6610613 -8.7837782 -9.1987648][-8.0922775 -9.127492 -9.0749416 -7.1965566 -6.5911407 -6.4504476 -6.9078841 -7.4076047 -6.8480363 -6.1330285 -5.1122222 -5.9737225 -7.1217041 -8.3062754 -8.79981][-8.1432037 -9.217226 -8.8472576 -6.7895546 -5.5547891 -4.8754244 -4.3362021 -4.61059 -4.4830728 -3.1003845 -2.1310492 -4.6090207 -7.5876026 -9.5961275 -9.8201857][-7.8991385 -9.1442 -8.6165428 -7.1572561 -5.0637884 -3.00325 -1.8385968 -2.0430553 -1.906765 -1.5868649 -1.6053524 -4.3481903 -7.6714888 -10.553385 -10.951902][-8.5449781 -9.2455578 -7.9748878 -5.2295618 -2.4844456 0.28928757 2.7432961 2.4200168 1.8562827 1.3142715 0.10339403 -3.1815889 -6.4374313 -8.6233692 -9.1266518][-8.3718414 -8.1360874 -7.16232 -4.2778473 -0.30653095 2.800818 5.7339883 6.3601136 6.0698028 3.5796809 0.57284307 -3.2925406 -7.2606797 -8.3483019 -8.57454][-6.9682374 -7.3293772 -6.68554 -3.2510087 0.52344942 4.625216 8.1542225 8.3160477 7.9029717 5.374949 1.8517356 -3.0103588 -7.7695675 -9.3836355 -9.1209211][-6.752512 -6.490108 -5.88263 -3.2326975 -0.72099781 2.252717 5.6933122 6.3115191 5.6594758 3.3668065 0.77024746 -4.1726589 -9.056366 -10.779301 -10.160069][-6.9174833 -7.3423014 -6.3791666 -4.7567434 -3.5684485 -1.180685 1.3789005 2.2997985 1.8234811 0.21815443 -2.3866239 -7.149488 -11.040016 -12.913274 -12.613827][-9.1757107 -8.6050415 -7.4601345 -5.9995422 -4.9073524 -4.746531 -4.2930384 -3.5636883 -3.4597108 -3.9335265 -5.2041545 -9.763401 -13.67197 -14.218826 -12.314231][-12.95598 -10.281496 -8.2098036 -7.2756891 -6.9086952 -6.6046085 -7.5063457 -7.7048664 -7.7687745 -8.1690865 -9.5556355 -12.057487 -13.806108 -13.888826 -12.515649][-11.790309 -9.3454218 -6.1751127 -5.8900089 -6.2943435 -6.7695961 -7.1316628 -7.8187742 -8.55685 -8.709465 -9.2802029 -10.013601 -10.820385 -10.596741 -9.4540787][-8.5301723 -7.02957 -4.0507221 -3.317 -4.5199256 -5.9387441 -7.4589992 -7.6685224 -7.3190889 -7.7106194 -7.8739605 -7.0890651 -6.92002 -7.4161053 -6.576314][-5.5365663 -4.5576181 -2.9656491 -1.9986408 -3.2721543 -4.4942436 -5.5799546 -6.4083405 -6.4280596 -5.4364672 -5.1089239 -5.6263709 -6.3483887 -6.9544454 -7.13775]]...]
INFO - root - 2017-12-15 20:06:51.450208: step 53210, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 50h:43m:42s remains)
INFO - root - 2017-12-15 20:06:58.047220: step 53220, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 52h:20m:15s remains)
INFO - root - 2017-12-15 20:07:04.658414: step 53230, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 50h:03m:02s remains)
INFO - root - 2017-12-15 20:07:11.218457: step 53240, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 51h:33m:56s remains)
INFO - root - 2017-12-15 20:07:17.802730: step 53250, loss = 0.24, batch loss = 0.20 (11.5 examples/sec; 0.697 sec/batch; 54h:03m:03s remains)
INFO - root - 2017-12-15 20:07:24.324663: step 53260, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 51h:35m:41s remains)
INFO - root - 2017-12-15 20:07:30.795729: step 53270, loss = 0.21, batch loss = 0.16 (12.3 examples/sec; 0.651 sec/batch; 50h:30m:33s remains)
INFO - root - 2017-12-15 20:07:37.434753: step 53280, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.664 sec/batch; 51h:28m:53s remains)
INFO - root - 2017-12-15 20:07:44.066898: step 53290, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.685 sec/batch; 53h:08m:05s remains)
INFO - root - 2017-12-15 20:07:50.680074: step 53300, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 51h:37m:15s remains)
2017-12-15 20:07:51.224860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.3721628 -7.8736334 -6.6919622 -5.612215 -5.2603335 -4.4411354 -4.0520744 -2.9544868 -1.3857546 0.047039032 0.93062353 -1.7052426 -4.0536571 -6.8362608 -7.7519846][-6.89129 -6.9456177 -6.92108 -6.4223328 -6.3913121 -6.2313085 -5.6463814 -3.9512711 -2.1722343 -0.62517452 0.625257 -2.1597981 -4.8974652 -7.9795151 -9.08458][-3.6212542 -5.4720974 -7.0173559 -6.511847 -7.1346235 -7.1957979 -6.7967119 -5.0678105 -3.0236228 -1.9201584 -1.8839221 -4.8775425 -7.4776173 -9.70208 -10.146034][-3.1096454 -4.6376033 -5.4827127 -5.5723414 -6.4134398 -6.25342 -5.6504431 -5.5715442 -4.6782446 -3.4087791 -2.7031066 -6.3217087 -9.49495 -12.392088 -13.296806][-2.8899643 -4.9005022 -6.7134466 -5.6231446 -5.0070682 -4.2433476 -3.3939033 -4.3082714 -5.0587325 -4.3238773 -4.1252093 -8.2939034 -10.785437 -13.963934 -15.028423][-5.5008888 -6.3832459 -6.135602 -4.0547967 -2.5985129 -0.61536646 0.91530037 -0.03639698 -0.93852139 -1.9169977 -4.1975222 -7.404285 -9.9353561 -14.004473 -14.903845][-6.9117312 -7.3817086 -5.7181859 -2.0450449 -0.081146717 1.5061965 3.2699199 4.4881253 4.4821391 0.61227751 -3.1384768 -6.5589604 -10.058496 -14.230757 -14.569651][-6.4371824 -6.552062 -4.3474474 -0.96871185 0.46640253 3.2842278 5.2882638 5.6062884 5.1293235 3.2946544 0.48526 -5.1073513 -10.039152 -13.875071 -14.888039][-4.0945258 -4.0855284 -2.0284967 -0.0230155 1.2327704 3.2412543 3.6376319 3.6765485 4.1844878 3.5330834 2.1054358 -3.9790621 -8.6047659 -13.299578 -15.151493][-4.095974 -3.5744369 -1.9551361 0.30312157 2.1718717 2.3787704 1.8664622 2.6050754 2.8163886 2.4865279 1.2439818 -3.5357916 -8.6083174 -12.826633 -13.986521][-8.2173891 -7.3578787 -4.8002434 -2.0374975 0.01394558 -0.31473684 -0.37104273 0.031356335 -0.45249033 -0.89652729 -0.73581886 -4.666482 -7.67142 -11.091252 -12.624828][-12.709772 -11.826666 -9.1200485 -6.1754904 -4.0555515 -4.5549917 -4.348608 -4.7553349 -5.6326289 -4.7355518 -3.9294972 -6.2075276 -7.3662486 -9.9448757 -10.776667][-14.318041 -12.71463 -11.459557 -9.1987209 -8.0286217 -7.5693436 -7.4313583 -8.9515324 -9.85803 -8.86288 -8.2303181 -7.67154 -6.8334336 -7.3234653 -7.6913319][-12.107377 -11.418079 -9.9139776 -7.99916 -7.3124337 -8.4001856 -8.6753712 -9.16138 -9.9178381 -10.040154 -9.7607574 -8.6617851 -7.208952 -7.7463837 -7.74963][-9.4422836 -8.9664545 -8.3703384 -6.9653187 -6.0249381 -6.0033932 -5.8686 -7.1707282 -8.723197 -9.763876 -10.691574 -11.109949 -11.282978 -10.58481 -9.6682234]]...]
INFO - root - 2017-12-15 20:07:57.902235: step 53310, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 51h:49m:47s remains)
INFO - root - 2017-12-15 20:08:04.468207: step 53320, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 49h:25m:36s remains)
INFO - root - 2017-12-15 20:08:11.154530: step 53330, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 51h:21m:59s remains)
INFO - root - 2017-12-15 20:08:17.777666: step 53340, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 51h:09m:32s remains)
INFO - root - 2017-12-15 20:08:24.373843: step 53350, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 50h:38m:35s remains)
INFO - root - 2017-12-15 20:08:30.968325: step 53360, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 50h:06m:54s remains)
INFO - root - 2017-12-15 20:08:37.513851: step 53370, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.675 sec/batch; 52h:21m:42s remains)
INFO - root - 2017-12-15 20:08:44.141182: step 53380, loss = 0.23, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 50h:42m:11s remains)
INFO - root - 2017-12-15 20:08:50.628690: step 53390, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 50h:08m:24s remains)
INFO - root - 2017-12-15 20:08:57.244403: step 53400, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 52h:10m:08s remains)
2017-12-15 20:08:57.799542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9256668 -7.6345577 -8.3354759 -7.2812891 -7.7843924 -7.266561 -7.437242 -6.8401766 -5.828094 -4.3950567 -3.1168358 -4.2428427 -4.4459152 -3.6808248 -2.4039054][-5.8854127 -5.8079672 -5.1560111 -4.4534268 -4.5446057 -4.4374728 -4.87047 -4.7246604 -3.461123 -1.6858759 -0.71082497 -2.4786825 -3.7586379 -3.8660114 -3.0493238][-4.3924956 -4.6697278 -4.5812368 -3.4055638 -3.4876065 -3.2470922 -3.1525815 -2.7407372 -1.7049699 -0.22722149 0.44700289 -1.8946145 -3.3291092 -3.7573247 -3.4043725][-5.4326448 -5.8042107 -5.2698379 -3.3506377 -2.8586915 -2.1509323 -2.1643491 -1.9844441 -1.7042289 -0.41707468 0.57234526 -2.3699789 -4.4264216 -4.461257 -3.6999218][-5.1864419 -5.3248119 -4.5239286 -2.0980983 -0.87555885 0.40746212 0.46877193 -0.90248537 -1.7706175 -0.89858294 -0.49751616 -3.0426702 -4.5509658 -5.0390406 -4.7223434][-7.0855961 -6.4719386 -4.4325776 -1.2480831 0.89813423 2.523376 2.6988807 1.5920482 0.13348055 -1.3880482 -2.5212986 -4.0678544 -4.3551569 -4.3454418 -3.5593865][-9.047574 -7.0848913 -4.2539835 -0.65761185 1.7919488 4.2689757 5.246098 4.5512767 3.3647132 0.72403669 -1.1707096 -3.1914115 -3.7920618 -3.6637361 -2.9916444][-8.1978273 -7.0973668 -4.516047 -0.0095300674 3.1050496 4.4153 4.318212 3.71423 3.3256345 2.0628781 0.58332253 -2.3706374 -3.8922138 -4.0850377 -3.6839576][-6.7881889 -5.0403152 -3.7282965 -0.35212708 1.6015706 3.380012 3.7930045 2.5829921 1.3544068 0.39091396 -0.51251745 -3.4453168 -5.4970474 -6.0409493 -5.3196168][-6.3592157 -5.5944514 -4.8065715 -2.6000729 -0.82790852 1.3396897 2.2974372 0.90299749 -0.674078 -2.2634587 -3.4766016 -5.9681854 -7.3162279 -7.468327 -6.966527][-11.384637 -10.486415 -8.7973232 -6.1435018 -5.1136227 -4.0741391 -3.4620619 -3.6752579 -3.7528124 -5.055378 -6.3864021 -8.9377651 -9.1411705 -7.613194 -5.5297213][-15.412766 -13.95331 -11.958908 -10.03009 -9.590971 -9.0170355 -9.1168232 -8.8526363 -8.9987459 -9.2301254 -9.1710033 -10.539111 -10.14662 -8.4074545 -6.1235142][-14.954102 -14.127758 -13.222035 -11.378286 -10.481158 -10.115191 -9.8540325 -10.336391 -10.538101 -10.231567 -9.9824448 -10.609098 -9.1429462 -6.4063568 -4.0989723][-11.688961 -11.139513 -10.034941 -8.0303183 -7.749208 -7.2865543 -7.4920917 -7.6865234 -7.9897776 -8.5898628 -8.5296841 -7.4918647 -5.1636081 -4.0748096 -2.8450778][-5.839963 -5.0846643 -4.3094239 -3.0331519 -1.9566891 -1.7517121 -2.4350848 -3.4325283 -3.4179277 -4.0734272 -3.4507127 -3.6406486 -4.3403668 -3.8392782 -3.3281145]]...]
INFO - root - 2017-12-15 20:09:04.326374: step 53410, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.628 sec/batch; 48h:42m:00s remains)
INFO - root - 2017-12-15 20:09:10.961193: step 53420, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 51h:39m:16s remains)
INFO - root - 2017-12-15 20:09:17.598887: step 53430, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.635 sec/batch; 49h:13m:28s remains)
INFO - root - 2017-12-15 20:09:24.180474: step 53440, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 50h:14m:39s remains)
INFO - root - 2017-12-15 20:09:30.795872: step 53450, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 51h:27m:48s remains)
INFO - root - 2017-12-15 20:09:37.415553: step 53460, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 52h:17m:14s remains)
INFO - root - 2017-12-15 20:09:43.962044: step 53470, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 50h:57m:52s remains)
INFO - root - 2017-12-15 20:09:50.537501: step 53480, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 50h:45m:44s remains)
INFO - root - 2017-12-15 20:09:57.207863: step 53490, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 50h:21m:42s remains)
INFO - root - 2017-12-15 20:10:03.813352: step 53500, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 51h:28m:59s remains)
2017-12-15 20:10:04.387757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.850255 -2.2256677 -2.1268828 -0.81504107 -0.069051266 -0.16216135 -0.52082682 -0.95956993 -0.95094395 -1.1797681 -2.146661 -5.2113419 -6.97427 -9.281786 -10.910608][-3.1515634 -2.86382 -2.7395656 -2.5460169 -2.9237673 -3.0737157 -2.9064031 -2.2872477 -2.3252947 -1.8031173 -2.3165634 -6.5785046 -8.0866108 -9.2245436 -10.503851][-0.98953104 -3.4215984 -5.9547925 -5.1930728 -5.320262 -4.9130092 -3.7746725 -3.2788954 -3.1935647 -3.5062532 -3.7884851 -5.8275146 -7.3841505 -8.38534 -9.3537483][-4.0349312 -4.77687 -6.1269221 -6.9259734 -7.09342 -6.030715 -4.9936543 -3.7849309 -3.349932 -4.1894941 -5.3401136 -8.1090412 -9.2919855 -9.4881144 -9.868721][-2.9550469 -5.33094 -6.5888214 -5.6448126 -5.6351008 -3.7023864 -2.2525115 -3.2430286 -4.2921391 -4.7396326 -5.227766 -8.3726845 -9.9324169 -9.9499121 -9.8349876][-4.3946834 -4.8532915 -4.3206635 -3.2185628 -2.0495906 -0.26743269 0.80756807 1.0313883 -0.384645 -3.6675057 -5.3042531 -8.0866213 -9.8291855 -9.5611792 -8.68404][-3.2695012 -4.6992207 -4.055829 -2.1879585 -0.88679934 1.454061 2.438767 2.4902177 1.7858267 0.069344044 -1.8339911 -5.6775956 -7.0816436 -7.3379831 -8.0906439][-4.0710621 -3.3396611 -2.483319 -1.0165749 0.48788357 3.3916421 3.9626918 3.4377732 2.3252006 0.48555803 -0.35087872 -4.6529865 -7.4722381 -6.5468163 -5.3366385][-3.553941 -4.0582418 -2.8713925 -0.93249321 -0.41389608 1.5771918 2.6968112 3.1815257 3.0445437 1.8696461 0.92412519 -3.8835111 -6.81483 -6.8561239 -6.7661238][-3.1772683 -3.9292917 -3.4820046 -0.68890953 1.0218325 1.8875952 1.1023049 1.3989635 1.8694162 1.3119459 1.0279589 -3.574734 -6.6992211 -7.2309337 -7.4140778][-4.5969119 -4.4621058 -3.2914605 -1.9700828 -1.126399 -0.18510389 -1.6936955 -3.2114925 -3.9884367 -4.3605967 -4.5719171 -6.3601112 -6.2491674 -5.7703314 -4.8824687][-8.2382565 -7.1380558 -5.1375818 -3.6123543 -3.3339481 -2.8626356 -4.6803422 -6.734313 -8.2400036 -8.4838591 -8.0845366 -9.4122553 -9.1326866 -7.2131538 -5.316957][-11.744336 -10.417988 -7.8201818 -4.9849844 -4.60645 -4.4546676 -6.0220976 -8.6558123 -10.060823 -10.566046 -10.936641 -10.892916 -10.486238 -8.0402975 -5.0371614][-10.408949 -10.231777 -7.8523107 -5.6470623 -5.2117648 -4.3897619 -5.3016109 -6.7010059 -8.0590019 -8.4060574 -7.4827647 -7.1844664 -7.9279833 -8.0388393 -7.0710545][-7.2707438 -7.1185484 -6.4752555 -6.0147719 -5.33877 -4.5628123 -4.9687843 -5.2035742 -4.99973 -5.2467647 -4.9715691 -4.7785 -4.3969154 -5.3592219 -6.529006]]...]
INFO - root - 2017-12-15 20:10:10.951061: step 53510, loss = 0.28, batch loss = 0.23 (12.4 examples/sec; 0.646 sec/batch; 50h:01m:57s remains)
INFO - root - 2017-12-15 20:10:17.513062: step 53520, loss = 0.18, batch loss = 0.13 (12.6 examples/sec; 0.637 sec/batch; 49h:22m:07s remains)
INFO - root - 2017-12-15 20:10:24.028862: step 53530, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 49h:52m:23s remains)
INFO - root - 2017-12-15 20:10:30.600521: step 53540, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 51h:02m:16s remains)
INFO - root - 2017-12-15 20:10:37.167623: step 53550, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 51h:00m:21s remains)
INFO - root - 2017-12-15 20:10:43.826707: step 53560, loss = 0.26, batch loss = 0.21 (12.1 examples/sec; 0.660 sec/batch; 51h:07m:03s remains)
INFO - root - 2017-12-15 20:10:50.413559: step 53570, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.676 sec/batch; 52h:22m:11s remains)
INFO - root - 2017-12-15 20:10:56.992015: step 53580, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 51h:25m:06s remains)
INFO - root - 2017-12-15 20:11:03.651099: step 53590, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 51h:33m:25s remains)
INFO - root - 2017-12-15 20:11:10.338322: step 53600, loss = 0.20, batch loss = 0.16 (11.9 examples/sec; 0.671 sec/batch; 51h:58m:48s remains)
2017-12-15 20:11:10.841618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5773082 -5.4824862 -5.5466528 -4.4825439 -4.0296841 -3.4833746 -3.3587358 -3.7456274 -3.8051219 -3.4745243 -3.4918876 -5.9960337 -8.1389418 -9.3784142 -9.6327486][-3.6397126 -3.2873838 -2.5395086 -1.3694196 -1.8438523 -2.2116151 -2.73563 -4.0940938 -4.9122896 -5.328383 -5.2602024 -7.2310839 -9.5791035 -11.592564 -11.906603][-2.4192367 -2.5746758 -1.4587097 -0.18046427 -0.29795456 -1.1941581 -2.5251503 -4.2887907 -5.6261005 -6.2872171 -6.7888985 -8.9345026 -11.682102 -13.384443 -13.563597][-3.4649532 -2.8300173 -1.613646 0.32679319 0.37363052 -0.10766363 -1.5033221 -3.2358694 -4.9722724 -5.7138481 -6.3726649 -9.1716194 -12.143547 -13.85046 -14.356615][-2.3791175 -2.5624592 -1.5401387 0.15800142 0.63667965 0.77572012 0.37638521 -0.54549837 -2.4274037 -3.7306828 -5.0406494 -8.3811054 -12.156571 -13.883148 -14.353487][-3.2535994 -2.69823 -0.6656189 0.69960976 1.3964162 1.8173947 1.807837 1.1015148 -0.10725737 -1.910136 -3.7431967 -6.8068414 -10.6042 -12.620132 -13.422922][-2.4495096 -1.394824 -0.21369934 1.3186431 2.1207595 2.0821352 2.1833191 1.8634486 1.2260675 0.18459129 -1.4852023 -5.4469237 -9.3621769 -10.817802 -11.017305][-2.9381025 -1.5959568 -0.4629631 1.3878899 2.2150717 2.2210445 2.2901664 1.7034349 0.7668581 -0.11406612 -0.84189367 -3.5930684 -6.1530852 -7.5191836 -7.8241882][-2.827158 -1.3082242 0.0022850037 1.4344296 2.2119627 1.9727759 1.9004459 0.79037046 -0.089244843 0.080177307 0.23642921 -1.7243087 -3.8272657 -5.0176005 -4.9529033][-4.1606207 -2.9667063 -1.9707105 -0.977612 -0.48739195 -0.22302008 0.691895 0.83032084 0.27078629 -0.11360455 0.16828251 -0.745029 -2.2626512 -2.6815341 -3.0867562][-10.47172 -8.433116 -7.1482182 -7.4676795 -7.3799448 -5.585125 -3.9596851 -2.9027441 -2.9244094 -2.3349948 -1.3468094 -1.7122102 -1.8204648 -1.4075313 -1.3253093][-13.822456 -12.35885 -11.333241 -10.883426 -10.945728 -10.699818 -9.2000923 -7.0102005 -5.493062 -5.3594213 -4.4022303 -4.2571659 -3.957551 -2.9990182 -1.9692833][-12.455173 -11.247059 -10.284969 -10.176681 -10.538691 -9.5415287 -9.1463842 -8.2269459 -7.900568 -7.3365955 -5.80106 -5.1998587 -4.0853443 -3.2268524 -2.570282][-10.118232 -9.1363716 -7.5933366 -7.523612 -7.2284794 -7.0013943 -7.8410435 -6.9733071 -6.3105731 -6.6172051 -6.4540763 -5.9137816 -5.4993858 -5.1943588 -5.7595267][-8.27142 -8.2818756 -7.2366748 -6.7215891 -7.1992774 -6.9780974 -6.4911151 -5.7502508 -5.8579254 -5.8275661 -5.572731 -6.4010954 -6.7875195 -7.780252 -8.3684177]]...]
INFO - root - 2017-12-15 20:11:17.357036: step 53610, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 49h:56m:36s remains)
INFO - root - 2017-12-15 20:11:23.950348: step 53620, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 49h:29m:54s remains)
INFO - root - 2017-12-15 20:11:30.520085: step 53630, loss = 0.21, batch loss = 0.17 (12.2 examples/sec; 0.653 sec/batch; 50h:36m:40s remains)
INFO - root - 2017-12-15 20:11:37.126901: step 53640, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 49h:48m:21s remains)
INFO - root - 2017-12-15 20:11:43.704275: step 53650, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 50h:21m:27s remains)
INFO - root - 2017-12-15 20:11:50.291660: step 53660, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.671 sec/batch; 51h:58m:18s remains)
INFO - root - 2017-12-15 20:11:56.921694: step 53670, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 51h:28m:46s remains)
INFO - root - 2017-12-15 20:12:03.502354: step 53680, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 50h:20m:55s remains)
INFO - root - 2017-12-15 20:12:10.086480: step 53690, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 50h:40m:30s remains)
INFO - root - 2017-12-15 20:12:16.685641: step 53700, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 50h:54m:16s remains)
2017-12-15 20:12:17.167171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6630774 -6.1864271 -5.8272858 -6.3973055 -6.8675919 -7.5248423 -8.3183365 -8.6903639 -8.5806561 -7.5831943 -5.907599 -6.2586021 -6.5224075 -6.2557454 -5.8036666][-5.4755788 -5.4779739 -5.1267943 -4.2291985 -4.7708788 -5.8119245 -6.9252968 -7.1577945 -6.9640613 -6.5528116 -5.7571707 -6.6402025 -7.3038177 -6.68699 -5.1647158][-3.5936399 -3.3159068 -4.1709833 -2.9558918 -3.5616176 -4.5283461 -5.1930685 -5.1854515 -5.2349658 -4.6754265 -4.7083087 -6.4095979 -7.7001324 -6.70203 -6.0439634][-5.76468 -5.3382044 -4.9164991 -3.5988183 -3.4150569 -3.112494 -3.9476962 -4.4122348 -4.8012233 -4.8088455 -4.3272061 -4.967072 -6.9206376 -7.5914879 -6.8288379][-5.5601363 -6.4973559 -6.8841176 -4.2771292 -2.7972713 -1.5987148 -2.2082274 -2.8519437 -2.8061218 -3.4554412 -4.6218066 -5.8947039 -7.3684931 -6.7591543 -6.9819379][-6.8363457 -6.2984109 -5.6605759 -2.9971097 -0.74177408 0.83582067 1.6500373 0.66754055 -0.21483755 -1.6039648 -3.1833005 -5.3358364 -7.9362745 -7.3731012 -5.7105961][-6.1246982 -5.1222734 -3.6555462 -1.7233441 -0.58625221 1.6928091 3.9246449 3.6363215 2.1188827 -0.11735201 -1.286447 -3.4644601 -7.131166 -6.3465557 -5.580759][-6.8480167 -5.5725436 -3.5327365 -0.74601889 0.67309332 2.9935699 4.84061 5.1340709 4.4422612 1.467135 -0.99895 -3.4920378 -5.2769923 -5.0036392 -4.2389469][-5.5236506 -3.1709926 -2.2121582 0.19524336 1.2801023 1.3356113 1.7802358 2.9290195 3.1958041 1.2035208 -0.679811 -3.8290973 -6.7673445 -5.3501673 -3.6476228][-6.2978525 -3.9135413 -3.0062578 0.136343 1.0442438 0.016238689 -0.40558577 -0.31094408 -0.62469673 -0.4575448 -0.65030718 -3.9492242 -6.4540672 -6.3478928 -5.9397879][-8.1978674 -7.0643358 -6.3766623 -4.1592245 -4.0639696 -3.3742833 -2.5582952 -3.5748181 -4.5886531 -4.3826303 -3.4699304 -5.6810293 -6.8356285 -6.291357 -5.72623][-11.600428 -10.676458 -9.8188095 -8.58017 -9.2802868 -7.9148288 -6.7289886 -7.1564865 -7.1740365 -6.9406686 -7.4606862 -7.9691458 -7.0380859 -6.6398621 -5.7341008][-12.538815 -12.293242 -10.771873 -10.458946 -9.997179 -9.5745354 -9.5458908 -9.3625164 -8.8089237 -8.844409 -8.75186 -9.5459652 -9.2685633 -6.641902 -5.3611689][-11.264115 -9.818222 -7.8737469 -7.152195 -6.4216738 -6.1683254 -7.2520256 -7.7323279 -7.1411376 -7.0666065 -7.4801512 -7.7156777 -7.390223 -7.0851765 -6.1484838][-6.2824092 -5.8368025 -4.98101 -4.0978203 -3.9091523 -3.2587714 -2.6264639 -2.4209437 -3.4066823 -3.6174843 -3.7902532 -5.0755219 -6.3204808 -7.0706525 -7.0636611]]...]
INFO - root - 2017-12-15 20:12:23.718526: step 53710, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 52h:35m:51s remains)
INFO - root - 2017-12-15 20:12:30.346466: step 53720, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 51h:42m:56s remains)
INFO - root - 2017-12-15 20:12:37.019968: step 53730, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 51h:16m:37s remains)
INFO - root - 2017-12-15 20:12:43.625725: step 53740, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 50h:04m:33s remains)
INFO - root - 2017-12-15 20:12:50.346134: step 53750, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 51h:19m:00s remains)
INFO - root - 2017-12-15 20:12:56.942497: step 53760, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 49h:34m:09s remains)
INFO - root - 2017-12-15 20:13:03.561048: step 53770, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.676 sec/batch; 52h:22m:04s remains)
INFO - root - 2017-12-15 20:13:10.180404: step 53780, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 50h:11m:39s remains)
INFO - root - 2017-12-15 20:13:16.826781: step 53790, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 50h:16m:53s remains)
INFO - root - 2017-12-15 20:13:23.490953: step 53800, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 50h:54m:36s remains)
2017-12-15 20:13:24.071743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.2486973 -7.0246763 -6.3746858 -4.9777155 -4.8298335 -5.1080303 -5.9348836 -6.918695 -7.5643215 -8.2677412 -7.9117212 -10.364178 -11.952881 -10.89122 -9.0190287][-8.64756 -7.9207525 -6.9179449 -6.3100972 -6.3533044 -6.6742 -7.1241779 -7.2903452 -7.8185344 -8.0268812 -7.621182 -9.7818241 -10.826457 -10.22211 -8.3294935][-6.12596 -7.4362903 -7.880487 -7.7247524 -8.4358711 -8.2872267 -8.8620062 -8.652319 -8.5570107 -9.0309124 -8.3471069 -9.58781 -10.297184 -9.1381979 -7.26534][-6.9636822 -7.77663 -7.32618 -7.1219921 -8.4334307 -7.6306725 -7.028585 -8.1611538 -7.7829022 -7.3241029 -7.5831685 -9.9185677 -10.717817 -9.9566784 -8.5727234][-7.2856646 -8.9013367 -9.7868652 -7.7827787 -6.7440467 -4.8573165 -4.4648962 -4.1436987 -4.9312515 -5.4190273 -5.5729833 -8.41825 -9.3876505 -9.6525249 -8.8342609][-9.289794 -9.4824972 -10.177885 -6.9002075 -3.5818911 -0.81662273 0.034236431 0.57707834 -0.31898737 -3.0627484 -4.4768653 -6.4251256 -9.393012 -10.261974 -9.02345][-8.942421 -8.8682232 -8.0498676 -4.6767836 0.2197094 3.4391208 5.7062631 4.7594886 3.1213803 0.73906565 -1.6519375 -5.902348 -8.3314791 -9.3296509 -9.0297318][-8.7560005 -7.3821526 -5.7071857 -2.5425744 0.1002717 5.0682731 8.2278061 7.1991124 5.0091424 2.5845637 -0.0026674271 -4.7089715 -7.2171092 -8.5817719 -8.9483461][-8.4382153 -7.4958916 -4.8499575 -1.3956313 -0.37268448 2.1123996 4.9395022 4.6691194 2.3700566 0.19056082 -0.52608776 -4.1285028 -7.4560227 -8.50537 -7.6408067][-9.0265713 -8.8001938 -7.2389488 -3.6905975 -2.053762 -0.81868505 -0.9060092 0.20480871 -0.398561 -3.4573512 -4.8280144 -6.4712424 -8.4392424 -8.6662836 -7.7199135][-11.201435 -9.6497631 -9.30036 -7.0062885 -5.6442103 -4.5889626 -5.5250621 -6.0263577 -5.6384683 -6.9706473 -7.2683849 -10.220622 -11.691332 -9.9328842 -7.1733041][-14.937632 -12.846207 -11.19681 -9.0774765 -6.7727518 -5.1162977 -6.6398292 -7.7724829 -8.5779724 -8.6197567 -9.1755533 -11.742075 -12.078674 -10.324215 -6.3613558][-13.877234 -12.264168 -9.4699268 -7.8715239 -7.1878252 -4.770431 -3.9905167 -6.10016 -8.1172237 -8.484767 -8.2764721 -8.4110308 -9.1652269 -8.690691 -5.1964655][-12.721956 -10.571873 -7.4315276 -4.9629269 -3.4443264 -4.1815238 -5.0831294 -4.9381561 -6.2617521 -7.5984287 -7.2383819 -6.608356 -5.7073884 -4.8932595 -3.4665613][-11.419471 -9.96052 -6.9484177 -4.5612707 -4.1019297 -3.8713472 -4.2867069 -4.9817915 -5.5449305 -6.2315531 -5.57902 -5.5883412 -5.5023732 -5.4171653 -4.3244958]]...]
INFO - root - 2017-12-15 20:13:30.684726: step 53810, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 50h:58m:54s remains)
INFO - root - 2017-12-15 20:13:37.305802: step 53820, loss = 0.36, batch loss = 0.32 (12.2 examples/sec; 0.654 sec/batch; 50h:36m:52s remains)
INFO - root - 2017-12-15 20:13:43.887363: step 53830, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 50h:20m:33s remains)
INFO - root - 2017-12-15 20:13:50.513597: step 53840, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 50h:11m:35s remains)
INFO - root - 2017-12-15 20:13:57.130939: step 53850, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 50h:16m:22s remains)
INFO - root - 2017-12-15 20:14:03.769062: step 53860, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 52h:23m:21s remains)
INFO - root - 2017-12-15 20:14:10.325591: step 53870, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 50h:22m:35s remains)
INFO - root - 2017-12-15 20:14:16.951703: step 53880, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 50h:53m:00s remains)
INFO - root - 2017-12-15 20:14:23.466633: step 53890, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 49h:56m:27s remains)
INFO - root - 2017-12-15 20:14:30.060243: step 53900, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 51h:31m:16s remains)
2017-12-15 20:14:30.593090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4971595 -5.8889532 -5.3146434 -4.4456348 -5.0917211 -5.4013405 -5.5004988 -5.6604538 -5.092988 -4.49842 -3.5005774 -2.2205012 -2.9143786 -3.83146 -4.0278444][-6.1003 -5.1272149 -3.7976081 -3.3950913 -3.9728754 -4.6940517 -4.6558294 -4.2187648 -3.8156722 -3.2267146 -1.9491172 -0.72276497 -1.3216853 -2.9598939 -3.5770369][-4.0567703 -4.1016889 -4.4578252 -3.9482818 -4.3282762 -4.6314535 -4.4616718 -4.0274043 -2.9337058 -2.8182087 -2.1785154 -0.53162241 -0.71023989 -1.7926903 -2.8490973][-5.0404973 -5.389195 -5.950417 -5.7411714 -5.9715667 -4.6365366 -3.2823596 -3.1159759 -3.2599154 -3.027868 -2.0843606 -1.2872868 -2.1900496 -3.12474 -3.0923445][-4.6993566 -6.0868216 -6.8849821 -5.9857054 -5.8320513 -3.6890256 -1.111414 -0.81974173 -2.1605129 -2.8375127 -2.6093407 -2.1889985 -3.1218832 -4.2358 -4.572145][-6.2889695 -6.5100341 -6.5220766 -6.0643153 -4.703948 -0.38499689 3.0574527 3.0784144 0.84922218 -1.9856923 -3.7274423 -3.0165267 -3.558506 -4.9507108 -5.7072654][-9.12961 -8.1161652 -5.649684 -4.1464 -2.0783322 2.4387975 6.6645465 6.469769 4.4396358 0.97252178 -2.4141796 -3.2662544 -4.420064 -5.47299 -5.6855478][-10.854768 -9.4723721 -8.6140079 -5.5631971 -0.96083975 4.26905 8.4707642 8.6803074 6.2435622 1.1927581 -2.6718552 -4.0440607 -5.674149 -6.676486 -6.7703509][-11.085767 -10.541065 -9.4973831 -7.2665186 -3.9219871 1.360136 6.5057492 6.902514 3.9655309 -0.036162853 -3.2242684 -5.619678 -7.8017187 -7.9222627 -6.9460187][-10.827074 -11.110809 -10.789238 -8.6815052 -6.1473565 -2.2597013 2.6368871 3.9354186 1.8436255 -2.5118113 -6.3455787 -7.805366 -9.6943579 -10.345198 -9.4262114][-13.601342 -13.787964 -13.427079 -11.762024 -9.5663662 -7.4105635 -4.6883845 -3.191931 -3.5860126 -5.4419584 -8.556386 -11.281134 -12.952484 -12.155576 -10.651925][-15.128332 -15.369884 -14.403297 -12.656565 -11.693314 -9.8602257 -8.5127964 -7.9694276 -7.3745003 -8.651577 -10.361527 -11.450256 -12.994471 -12.787168 -10.778103][-13.671826 -12.92252 -12.617001 -11.872938 -10.717619 -9.3665333 -8.1958694 -7.8687119 -8.590764 -8.4652557 -8.4199543 -9.4603605 -10.471485 -10.149878 -9.2480307][-12.591009 -12.615532 -12.26178 -11.072235 -10.023006 -9.4781933 -8.5826092 -7.1090889 -6.8576441 -7.5187435 -7.6324677 -7.0072274 -7.2189312 -6.9470816 -6.474627][-9.2350807 -9.7703552 -9.7645512 -8.6137466 -8.0069246 -7.1363153 -6.2091322 -5.9457822 -5.5212946 -4.4328966 -4.4070063 -5.3825164 -6.5666785 -7.3624511 -8.2729864]]...]
INFO - root - 2017-12-15 20:14:37.130552: step 53910, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 49h:39m:00s remains)
INFO - root - 2017-12-15 20:14:43.803060: step 53920, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 50h:27m:46s remains)
INFO - root - 2017-12-15 20:14:50.392838: step 53930, loss = 0.15, batch loss = 0.11 (12.7 examples/sec; 0.631 sec/batch; 48h:50m:35s remains)
INFO - root - 2017-12-15 20:14:56.994220: step 53940, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 51h:19m:22s remains)
INFO - root - 2017-12-15 20:15:03.628268: step 53950, loss = 0.13, batch loss = 0.09 (11.4 examples/sec; 0.700 sec/batch; 54h:07m:52s remains)
INFO - root - 2017-12-15 20:15:10.179182: step 53960, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.681 sec/batch; 52h:41m:46s remains)
INFO - root - 2017-12-15 20:15:16.735120: step 53970, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 51h:27m:56s remains)
INFO - root - 2017-12-15 20:15:23.335025: step 53980, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 51h:28m:00s remains)
INFO - root - 2017-12-15 20:15:29.939268: step 53990, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 50h:50m:59s remains)
INFO - root - 2017-12-15 20:15:36.508461: step 54000, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 52h:08m:25s remains)
2017-12-15 20:15:37.015558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.992372 -2.7271655 -2.6016586 -3.5840795 -4.5791473 -5.3265176 -5.9987731 -5.4790215 -4.5425477 -4.5585728 -4.6966405 -5.2711396 -6.6872993 -7.8894548 -5.5241804][-6.5483093 -6.21274 -6.3231683 -6.1639814 -5.8758912 -7.1079111 -7.3429317 -6.3452659 -5.6370621 -4.7190342 -4.0855713 -6.1986732 -7.5289941 -8.0518618 -7.3956842][-5.0239477 -5.2554531 -6.5717349 -7.1611247 -8.073822 -7.7626858 -6.9808025 -6.9416246 -5.4869714 -4.6057639 -5.0965261 -5.8862939 -6.7109823 -7.9632149 -6.1876321][-4.90459 -4.8985472 -5.1589055 -5.8226833 -7.2441416 -7.6637821 -7.0919075 -6.3359656 -5.6870384 -5.0291181 -4.1973133 -5.3676252 -6.1902523 -6.3592544 -4.9067526][-8.1857882 -7.7930312 -7.1075339 -6.3274322 -5.9683809 -4.297575 -3.4266462 -4.1533 -3.899148 -4.0498724 -4.2142138 -4.5344434 -5.2374988 -5.9295821 -3.7663856][-7.6267929 -7.5551243 -6.7517385 -4.6875644 -2.9476962 -0.34930611 2.076643 1.5779996 0.68854523 -1.9121654 -3.4242156 -3.7853661 -3.7777214 -3.5846996 -2.224088][-8.6640358 -8.41598 -6.61619 -3.8337169 -1.6785417 1.8494968 4.3946385 4.9662948 5.3606696 1.8964252 -0.576426 -2.1456935 -3.2843943 -3.5420878 -1.6315193][-8.9746885 -7.9217224 -6.014246 -3.0787225 -0.16819811 2.272481 3.3913932 4.1450171 4.4591203 2.7123981 1.3287382 -1.6832604 -4.4777656 -4.5557623 -3.0256126][-6.4452462 -5.9140105 -4.7856159 -2.8216155 -0.62341118 2.1732788 3.15558 3.5971322 4.1984839 2.5301557 1.1715131 -1.6613021 -4.4834356 -6.3696437 -5.8143721][-6.4294057 -5.2819948 -4.2825756 -2.004091 -0.21329927 0.97791052 1.782825 2.6103902 1.578506 -0.41050673 -1.1249051 -3.5458488 -5.4753861 -7.5138454 -7.2105427][-5.9892282 -5.3567748 -4.689178 -3.880074 -3.2195537 -2.5624242 -2.4170065 -1.7106152 -1.8057635 -2.8869774 -3.6768148 -6.2430434 -8.1432419 -9.4860535 -8.5378971][-9.2024813 -8.1057386 -7.8903656 -7.4680524 -7.8930659 -8.2642326 -7.8831329 -8.1940622 -8.5739155 -7.7231159 -7.3991623 -8.8343325 -9.5003328 -10.676508 -9.8444366][-11.554149 -10.707603 -10.220548 -10.530107 -11.234107 -10.904964 -11.283379 -11.551988 -11.154378 -11.026108 -10.69502 -10.520107 -10.275909 -10.297167 -9.0124645][-9.6073437 -9.3432751 -8.5905018 -8.5101337 -8.9744339 -9.630393 -10.545954 -10.202156 -10.341595 -10.264784 -9.5731392 -9.311902 -8.9992914 -9.1104279 -8.3679514][-8.122139 -7.2443595 -6.766891 -5.9941168 -6.041131 -5.9424529 -6.2795887 -6.7186365 -7.1212726 -7.4673915 -7.552897 -8.2644129 -8.8038187 -8.2037621 -7.35181]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 20:15:43.627986: step 54010, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 50h:35m:33s remains)
INFO - root - 2017-12-15 20:15:50.229056: step 54020, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 52h:06m:18s remains)
INFO - root - 2017-12-15 20:15:56.781272: step 54030, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 50h:15m:23s remains)
INFO - root - 2017-12-15 20:16:03.392627: step 54040, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 50h:13m:43s remains)
INFO - root - 2017-12-15 20:16:10.046937: step 54050, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.678 sec/batch; 52h:27m:34s remains)
INFO - root - 2017-12-15 20:16:16.701256: step 54060, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 51h:32m:46s remains)
INFO - root - 2017-12-15 20:16:23.312261: step 54070, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 50h:13m:47s remains)
INFO - root - 2017-12-15 20:16:30.016974: step 54080, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 50h:31m:14s remains)
INFO - root - 2017-12-15 20:16:36.571920: step 54090, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 51h:28m:10s remains)
INFO - root - 2017-12-15 20:16:43.208209: step 54100, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 51h:44m:01s remains)
2017-12-15 20:16:43.753806: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7128053 -5.6060395 -5.5028586 -4.9946365 -5.3071179 -5.35794 -5.6154852 -4.6050835 -4.827764 -5.0955276 -4.7645617 -7.432888 -9.3931141 -8.8654547 -8.9887753][-3.8704193 -3.4855907 -2.99363 -2.529531 -3.3361921 -4.1127005 -4.4513993 -4.4711637 -4.7237945 -3.9874597 -3.7649548 -6.2850037 -7.4005589 -8.44543 -9.5782137][-2.5721927 -2.6403689 -2.5096598 -1.9330535 -2.3448291 -2.4506376 -2.8594964 -2.9452031 -2.6307366 -2.9178104 -2.9657168 -5.4102578 -7.2484431 -7.9982586 -8.3385849][-3.4703767 -2.6315973 -1.9624512 -1.3559589 -1.6741738 -1.8718472 -1.7358186 -1.7651434 -2.307888 -2.5412264 -2.0534391 -4.6603336 -6.8019357 -7.2364755 -7.1292911][-2.5067513 -3.0691888 -2.4582527 -1.2667961 -0.9707098 -0.72923994 -0.61320543 -0.5755 -0.17702866 -0.37465096 -0.52375174 -3.3636339 -5.0383811 -5.6340852 -6.734447][-3.8979142 -3.2394681 -1.8499744 -0.77303982 -0.28571463 0.43360424 0.50092411 0.34711456 0.57620049 0.53733063 0.10245228 -2.6352391 -4.2055321 -4.859611 -5.1651096][-5.6860023 -4.7454181 -2.8180077 -1.4799938 -0.1462965 1.2279377 1.4457898 1.4184575 1.4955502 1.5024447 1.5907016 -1.4751439 -3.4417312 -3.588027 -4.5298362][-6.0840693 -5.2047634 -3.5275292 -0.95102978 0.27189016 1.2393265 1.8582144 1.8239703 1.8679438 1.5089998 1.0816894 -1.4409156 -2.9457874 -3.7742023 -4.903337][-4.7584558 -4.1372137 -2.842026 -0.4315815 0.57091475 1.7364693 1.8064079 1.6793079 2.0532699 1.4304671 0.94748163 -2.289757 -3.9539733 -4.6910124 -5.2413125][-5.1556206 -4.0627837 -1.9630544 0.016120434 0.56734562 1.3798699 1.2877102 0.693521 0.13198376 0.43278742 0.58286047 -2.2443552 -3.7724371 -4.5994525 -5.5623188][-8.089 -6.5816031 -3.7462103 -0.78161716 -0.24998999 -0.16318178 -0.2984705 -1.1758981 -1.9489377 -1.812222 -1.7100387 -4.656981 -6.1464262 -6.0048141 -6.5576215][-9.7518473 -8.5722866 -6.42417 -3.1855109 -2.1895485 -2.7188904 -3.7345581 -4.8127422 -5.0940595 -4.9168835 -5.201561 -6.8574705 -7.1186543 -6.3244634 -6.6515479][-9.9899712 -8.644474 -6.050684 -4.4708486 -3.9456365 -3.9926653 -4.6460428 -5.5616226 -6.1840096 -6.4311037 -6.0575681 -6.6184244 -6.7500167 -5.6258016 -5.0054884][-9.3778839 -8.0218544 -6.548892 -5.5911646 -4.7894464 -5.1684842 -6.0248008 -5.911365 -5.5339041 -5.4865017 -5.5341644 -4.8812127 -4.6830125 -3.6743319 -3.6313519][-6.3688636 -6.6370664 -6.0384083 -5.4053626 -4.7165961 -4.3029971 -4.4282856 -4.1049156 -4.2596326 -3.1898861 -2.6541653 -3.3186052 -4.2426891 -4.8066235 -5.5225167]]...]
INFO - root - 2017-12-15 20:16:50.311185: step 54110, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 50h:47m:33s remains)
INFO - root - 2017-12-15 20:16:56.910502: step 54120, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 50h:26m:51s remains)
INFO - root - 2017-12-15 20:17:03.537420: step 54130, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 50h:39m:26s remains)
INFO - root - 2017-12-15 20:17:10.104786: step 54140, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 52h:16m:56s remains)
INFO - root - 2017-12-15 20:17:16.727157: step 54150, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.675 sec/batch; 52h:11m:43s remains)
INFO - root - 2017-12-15 20:17:23.336116: step 54160, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 52h:17m:02s remains)
INFO - root - 2017-12-15 20:17:29.922367: step 54170, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 50h:13m:52s remains)
INFO - root - 2017-12-15 20:17:36.486919: step 54180, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 51h:49m:02s remains)
INFO - root - 2017-12-15 20:17:43.090483: step 54190, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 51h:41m:19s remains)
INFO - root - 2017-12-15 20:17:49.643093: step 54200, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 50h:45m:57s remains)
2017-12-15 20:17:50.141193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0029066 -4.2780228 -4.6136627 -3.2008128 -2.398833 -2.2359746 -0.90929842 0.98655319 1.9148436 0.15520144 -2.2762942 -6.6478519 -9.63941 -14.168543 -15.323812][-2.8517048 -3.3418906 -3.9086843 -3.8462415 -3.6491075 -2.5122635 -0.50059938 0.16435575 1.1023583 0.48618078 -1.4739404 -6.5914793 -10.314177 -15.241463 -17.180611][-3.0319533 -3.856765 -3.821794 -3.3427854 -3.9795151 -3.2152743 -1.3773932 0.67614317 0.38670683 -0.7836771 -1.7038598 -7.0377035 -10.4332 -14.21736 -16.404268][-1.5732164 -2.2892761 -2.66558 -1.7470624 -1.5580602 -0.92288971 -1.1456304 -0.18117952 0.66920853 -0.371387 -2.3372521 -7.0641518 -10.164659 -15.05764 -16.294371][-1.4116173 -1.4520607 -1.3705683 -1.7772338 -1.6713676 0.045488834 0.54174137 0.56663752 0.47531939 0.40970755 -2.3275259 -8.0639086 -10.869047 -15.0278 -16.851091][-1.5517735 -3.0843177 -2.2730911 -1.3805513 -0.35363865 0.72630692 2.0280695 2.8036675 2.0157094 0.53236771 -1.6404042 -7.0108104 -11.738839 -14.958744 -16.01189][-2.6733527 -3.9784076 -4.1811647 -2.3294842 -0.22893572 1.9718785 3.4848104 4.0858388 4.555943 1.9597497 -1.1418319 -6.0481267 -9.2472811 -13.63443 -15.489157][-4.5191622 -4.5796943 -4.373477 -3.0782337 -1.0517039 1.8885846 4.16275 4.6148696 4.8080754 3.4167628 0.63812828 -5.0669904 -8.2783937 -12.813309 -14.795053][-4.4764171 -6.273777 -5.7412119 -3.2007904 -1.9331071 0.092111111 3.0331731 4.7900605 4.0242839 2.4409375 0.23643017 -4.7770982 -8.6282425 -12.721227 -13.895876][-4.1833997 -5.6566849 -6.6794362 -4.9024067 -3.1034441 -1.237638 0.50544167 2.0283875 2.1841908 0.26533842 -1.5713277 -5.5588846 -8.7176619 -12.598194 -14.227377][-6.317596 -7.3723078 -7.5512581 -6.6518478 -6.028667 -4.6675291 -2.9376864 -2.5788879 -2.11689 -2.9847734 -4.4059076 -9.372921 -10.62706 -13.276505 -12.679382][-8.2189083 -9.1810112 -9.0054359 -7.7469006 -7.1896191 -7.4414678 -6.7634225 -6.45968 -6.0736432 -6.5435262 -8.2147083 -10.689348 -11.159531 -12.99592 -13.466982][-9.707283 -9.7799873 -8.5874634 -8.1921787 -8.2465591 -8.20286 -8.2614574 -8.2455673 -7.8702421 -8.3446274 -9.0728378 -10.700587 -11.984837 -11.924957 -11.550493][-8.7648945 -8.9595022 -8.4242659 -7.2605238 -6.9782362 -8.1167269 -8.9905052 -8.6822634 -8.2103767 -8.6856375 -8.3454857 -8.5447664 -8.4744282 -10.135011 -10.923376][-9.1044378 -8.7286024 -8.7077217 -8.3026705 -8.32515 -8.3784113 -8.948925 -9.57061 -9.3793678 -8.4845963 -8.403161 -9.6473446 -10.328895 -11.320364 -11.596006]]...]
INFO - root - 2017-12-15 20:17:56.732999: step 54210, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 52h:55m:38s remains)
INFO - root - 2017-12-15 20:18:03.290956: step 54220, loss = 0.24, batch loss = 0.19 (12.2 examples/sec; 0.653 sec/batch; 50h:29m:36s remains)
INFO - root - 2017-12-15 20:18:09.934754: step 54230, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 50h:42m:22s remains)
INFO - root - 2017-12-15 20:18:16.467857: step 54240, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 49h:28m:00s remains)
INFO - root - 2017-12-15 20:18:23.028382: step 54250, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 51h:05m:19s remains)
INFO - root - 2017-12-15 20:18:29.581177: step 54260, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 51h:24m:54s remains)
INFO - root - 2017-12-15 20:18:36.123779: step 54270, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 50h:22m:57s remains)
INFO - root - 2017-12-15 20:18:42.762740: step 54280, loss = 0.20, batch loss = 0.16 (12.4 examples/sec; 0.648 sec/batch; 50h:03m:07s remains)
INFO - root - 2017-12-15 20:18:49.320816: step 54290, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 52h:16m:07s remains)
INFO - root - 2017-12-15 20:18:55.935603: step 54300, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 49h:16m:12s remains)
2017-12-15 20:18:56.432795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5782471 -7.2528877 -7.1464348 -6.8089633 -7.6029706 -9.0219221 -10.232418 -10.262806 -10.90901 -11.319357 -10.203967 -10.997576 -10.413924 -8.8514805 -7.6747422][-6.8795571 -7.4193563 -6.9888663 -6.5752625 -7.55051 -8.8557014 -9.9005756 -10.619722 -10.779549 -9.5460224 -8.2174835 -9.9999409 -8.7176247 -8.349309 -8.1821747][-5.8119888 -7.6571631 -8.3394709 -7.890625 -8.3717747 -8.3870068 -8.9493666 -9.0878086 -8.8380985 -8.5380611 -8.3171921 -8.9720955 -8.5258961 -8.8589945 -8.5250559][-7.7382936 -8.7517748 -9.2892885 -8.0396461 -7.0731959 -6.0993032 -5.541491 -6.3326945 -6.6867404 -6.6839385 -7.1729736 -8.7680969 -9.20337 -9.4215975 -8.4722414][-9.3022184 -11.184302 -11.770271 -9.1611519 -6.6388783 -3.0706568 -0.75506926 -1.9195971 -3.4237523 -4.3810019 -6.467382 -8.9848509 -9.1394968 -8.9722481 -8.1856518][-10.795866 -11.174467 -9.9023571 -7.8235908 -4.5005164 0.89418554 4.1715503 3.8430858 3.0389705 -0.177032 -3.7214568 -6.2480669 -7.1236982 -7.1300316 -5.7263103][-11.72047 -12.540821 -10.46716 -7.022747 -3.0853577 2.0802479 6.3726773 6.7835059 5.6275954 1.2877092 -2.5283992 -5.4497151 -6.951581 -7.0057449 -6.0545425][-12.039795 -12.803528 -10.92263 -5.0714808 0.28040504 4.3958468 7.6827407 6.9680114 5.5193744 2.3695045 -1.151196 -5.2024231 -7.2189713 -7.637435 -7.5855141][-10.240154 -10.948551 -10.785795 -5.9076529 -0.40307808 3.7396579 6.7153764 5.5335584 3.0871739 0.4859066 -1.8684986 -6.0335717 -8.0905209 -9.2272806 -10.081908][-8.7742815 -8.5571394 -8.0701885 -5.5271997 -2.4180975 1.1265426 5.117826 4.1449294 1.1310482 -1.6284246 -4.1019092 -8.32796 -9.6108847 -10.471457 -11.45364][-10.9221 -11.082382 -10.586618 -7.3520775 -6.0705843 -3.8557205 -1.1646447 -2.0447628 -3.3060982 -4.5383668 -6.698379 -10.955129 -12.175641 -12.761797 -13.098778][-15.609375 -15.167574 -13.544144 -11.619247 -10.69274 -9.0946455 -8.2453518 -9.3172464 -9.511816 -9.3690548 -10.306442 -12.850172 -13.920986 -14.243601 -14.241812][-15.205339 -14.810974 -13.403471 -12.45603 -11.178194 -10.403323 -10.215107 -10.229015 -10.464749 -10.418425 -10.219249 -11.580572 -11.745977 -11.039043 -10.943419][-13.480743 -12.42305 -11.499581 -10.5798 -9.7419872 -8.8498325 -8.38623 -8.303195 -9.0244942 -9.0762348 -8.6045513 -8.9209986 -8.1837063 -8.1677589 -7.8398976][-10.543041 -9.757102 -8.5099144 -6.5719976 -5.7754793 -5.6253276 -5.0499806 -5.088285 -5.1122174 -5.3810925 -5.8534441 -6.8986826 -7.1076746 -6.6044 -7.2231407]]...]
INFO - root - 2017-12-15 20:19:03.035115: step 54310, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 49h:16m:56s remains)
INFO - root - 2017-12-15 20:19:09.672011: step 54320, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 50h:22m:31s remains)
INFO - root - 2017-12-15 20:19:16.228758: step 54330, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 49h:16m:55s remains)
INFO - root - 2017-12-15 20:19:22.910504: step 54340, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 49h:42m:17s remains)
INFO - root - 2017-12-15 20:19:29.502293: step 54350, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 50h:39m:42s remains)
INFO - root - 2017-12-15 20:19:36.093088: step 54360, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.663 sec/batch; 51h:11m:51s remains)
INFO - root - 2017-12-15 20:19:42.727268: step 54370, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.675 sec/batch; 52h:07m:43s remains)
INFO - root - 2017-12-15 20:19:49.369751: step 54380, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 51h:40m:11s remains)
INFO - root - 2017-12-15 20:19:55.978312: step 54390, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 51h:58m:57s remains)
INFO - root - 2017-12-15 20:20:02.595426: step 54400, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.681 sec/batch; 52h:37m:17s remains)
2017-12-15 20:20:03.144787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8375964 -4.6855688 -5.1550379 -4.7050562 -4.2995243 -3.838706 -3.3728344 -3.12728 -2.912848 -2.663651 -2.292383 -3.1651495 -5.1383977 -7.3600221 -6.515183][-4.1049137 -4.4029303 -3.2802794 -2.8955748 -3.267859 -3.0546651 -2.8888764 -2.8462992 -2.9127502 -2.8652012 -2.8980775 -3.2547247 -4.5394473 -7.0988531 -6.8289137][-2.6075261 -3.0549192 -3.3087902 -2.8229809 -2.4060278 -2.60557 -3.2241716 -3.0141959 -2.470356 -2.7326746 -2.9239042 -4.1273556 -6.163023 -8.563446 -8.4134159][-2.8459029 -3.237994 -3.0887637 -2.9886312 -2.7172396 -2.8382673 -2.6527576 -2.9013693 -2.9434934 -2.2642176 -2.2806559 -4.0027809 -6.3160267 -8.7681007 -8.7695465][-3.148747 -3.3706908 -3.2785072 -2.3123436 -1.5662479 -1.3869929 -2.0119267 -2.4783425 -2.1390657 -2.2327535 -2.3864987 -3.4344251 -6.0693784 -9.244978 -9.53804][-3.6457088 -3.2278996 -2.5551584 -0.576777 0.71082735 0.9726615 0.78677082 0.105721 -0.28344011 -0.58067417 -1.0399432 -2.7563055 -5.65181 -9.0566235 -9.68802][-4.0098639 -3.8335967 -2.9368379 -0.35353374 0.720695 1.9435506 2.9700503 2.4848762 2.2607269 1.3095746 0.40463781 -1.5024881 -4.1257138 -7.0610256 -7.4657555][-4.37876 -3.3731327 -1.6691136 0.1505003 0.6257844 2.4399362 3.923224 3.8432927 3.7464938 1.8793983 0.14175081 -1.14464 -3.3893061 -6.4245353 -6.7984][-3.6664207 -3.2073948 -2.1016595 -0.83108664 -0.093351364 1.9350719 2.8012004 2.8971324 3.3118167 2.1848588 1.6579781 -0.52014971 -2.9157076 -5.5250735 -5.52075][-2.5853586 -3.071847 -2.7964325 -1.6868439 -1.2484384 -0.2785058 0.834641 2.0572481 2.9005857 2.4192643 1.6068983 -0.38842726 -2.8952649 -5.9272771 -5.77279][-5.8614645 -6.3481846 -5.9055638 -4.9316378 -3.9311743 -3.1950743 -2.7586505 -1.7864704 -1.1271415 -1.4051623 -1.3305955 -2.9304748 -4.2417679 -5.7872567 -4.9789138][-7.2696838 -7.4156818 -6.693965 -5.5744762 -5.3033772 -5.2879047 -4.7463861 -4.1507158 -3.514432 -3.5633743 -3.7398045 -3.8612962 -3.9133368 -6.0280867 -5.9295535][-10.563787 -8.7250919 -7.3098755 -6.8011379 -6.0949965 -5.950861 -6.7097878 -7.1765962 -7.284184 -6.8644443 -6.6292348 -5.67391 -4.9514971 -5.294003 -4.27597][-7.93016 -7.3728909 -7.0542927 -4.731216 -4.1709185 -4.296741 -4.994719 -6.0569587 -6.9972463 -7.2608304 -6.880497 -6.1061687 -5.6048074 -5.999022 -5.6128969][-6.4475026 -5.4299231 -4.8323007 -4.3335948 -3.9298418 -4.0149651 -4.5999117 -5.5164204 -6.2831459 -6.3203287 -6.3197036 -6.171773 -6.0816078 -6.5573993 -6.8620749]]...]
INFO - root - 2017-12-15 20:20:09.726766: step 54410, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 50h:23m:09s remains)
INFO - root - 2017-12-15 20:20:16.375459: step 54420, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.671 sec/batch; 51h:51m:35s remains)
INFO - root - 2017-12-15 20:20:22.859086: step 54430, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 50h:12m:37s remains)
INFO - root - 2017-12-15 20:20:29.381004: step 54440, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 49h:55m:19s remains)
INFO - root - 2017-12-15 20:20:36.021166: step 54450, loss = 0.23, batch loss = 0.18 (12.1 examples/sec; 0.660 sec/batch; 50h:58m:15s remains)
INFO - root - 2017-12-15 20:20:42.609987: step 54460, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 49h:24m:33s remains)
INFO - root - 2017-12-15 20:20:49.248154: step 54470, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 50h:53m:08s remains)
INFO - root - 2017-12-15 20:20:55.847568: step 54480, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.681 sec/batch; 52h:33m:28s remains)
INFO - root - 2017-12-15 20:21:02.454576: step 54490, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 49h:47m:51s remains)
INFO - root - 2017-12-15 20:21:09.058129: step 54500, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 52h:39m:19s remains)
2017-12-15 20:21:09.574404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.82589006 -0.84770393 0.028347015 1.3861651 1.2656035 -0.19707012 -2.9346907 -4.8936028 -5.4810672 -5.8102427 -5.919579 -8.1982193 -11.300924 -12.448146 -10.71756][-0.67425871 1.3990459 3.7832217 4.38786 3.4395204 1.1889391 -1.4038053 -3.3016608 -5.4604559 -6.8427005 -8.0180607 -11.063375 -14.257748 -15.132717 -14.53731][-0.045631886 0.57999945 1.1354418 3.5665603 4.032414 2.5050206 0.49965096 -1.7886508 -4.2223434 -6.2840829 -8.0612907 -10.501352 -13.340587 -15.610014 -15.548294][-1.342957 -0.53422689 1.3795381 3.5579019 2.5422826 1.3770437 0.24552584 -1.2925959 -3.2145875 -4.6992779 -6.0033765 -8.9082031 -12.433465 -13.502589 -13.047691][-0.80343819 -1.074327 -0.40588617 1.511436 1.7615304 2.6801181 2.4828143 0.52573872 -1.035017 -2.3215413 -3.8569336 -7.2922611 -10.819229 -12.861977 -12.717717][-2.8740013 -2.2694221 -1.7103472 1.3160357 2.9997115 4.6976209 4.9790721 4.1506305 3.1776032 1.5329475 -0.61298037 -3.2172275 -6.5110283 -8.9900637 -9.0683594][-3.1983671 -3.0147669 -1.5697703 0.86112404 2.5816455 5.1066966 6.5957417 6.8770528 6.4078269 3.9136739 1.3824592 -1.6213298 -5.7984438 -7.9201069 -8.2259474][-6.4382362 -4.9473505 -2.8535516 0.57051754 2.9482007 4.7726712 6.3209329 6.5398784 6.3102775 4.9951634 2.8405366 -1.1717277 -6.1068864 -8.9245424 -9.442153][-7.1380405 -5.5683889 -3.3477263 -0.36741781 1.5746732 4.0733466 5.2456737 4.6598115 4.5489717 3.7501969 2.3041043 -1.2281342 -6.0451536 -8.575346 -8.9085836][-7.9229217 -6.608511 -4.4280372 -1.3624039 0.91542721 2.611052 3.0432725 3.1795659 3.4408898 2.0997362 0.35188675 -2.7956924 -6.7679386 -9.0755348 -9.6394377][-12.650849 -11.460198 -9.6485376 -5.8288121 -3.9519129 -2.4372644 -1.3010745 -0.91163397 -0.92986488 -1.4422808 -1.7954676 -4.8657928 -7.8024187 -9.3095169 -8.3455315][-15.302425 -14.91354 -13.362747 -10.239092 -8.8648 -7.2631707 -7.2004433 -6.7742972 -6.4430494 -6.2237892 -5.9728951 -6.9660168 -7.7841377 -8.6770134 -7.7252626][-14.837212 -14.932095 -13.839275 -11.099775 -9.7643366 -8.2983646 -8.4619141 -9.4705887 -9.9307079 -9.1247663 -8.1912165 -8.00063 -8.22888 -8.3643169 -6.7364593][-11.145926 -11.00636 -10.923607 -8.88185 -8.2662144 -7.5522313 -8.2416363 -7.9282751 -7.2323828 -7.7107978 -7.641726 -6.3560948 -5.7815804 -6.2214103 -6.1951895][-8.4776983 -7.9144526 -6.7579751 -5.8873868 -6.1035957 -4.9682274 -4.4589748 -4.5762939 -5.6402988 -5.541533 -5.2493153 -5.8200021 -6.5304861 -6.3213148 -6.6137342]]...]
INFO - root - 2017-12-15 20:21:16.236040: step 54510, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 50h:07m:26s remains)
INFO - root - 2017-12-15 20:21:22.772322: step 54520, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 52h:18m:57s remains)
INFO - root - 2017-12-15 20:21:29.429444: step 54530, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 51h:04m:48s remains)
INFO - root - 2017-12-15 20:21:35.958920: step 54540, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 49h:32m:42s remains)
INFO - root - 2017-12-15 20:21:42.529214: step 54550, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 51h:05m:31s remains)
INFO - root - 2017-12-15 20:21:49.130451: step 54560, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 50h:17m:59s remains)
INFO - root - 2017-12-15 20:21:55.777972: step 54570, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 51h:12m:40s remains)
INFO - root - 2017-12-15 20:22:02.369322: step 54580, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 50h:22m:13s remains)
INFO - root - 2017-12-15 20:22:08.898062: step 54590, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 49h:57m:21s remains)
INFO - root - 2017-12-15 20:22:15.514448: step 54600, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 50h:32m:02s remains)
2017-12-15 20:22:16.043715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.5762911 -8.3791943 -8.3897352 -8.2019854 -9.47477 -9.3733273 -9.1519928 -9.85063 -9.132761 -7.5286212 -5.0660095 -3.1110656 -3.1601439 -4.0787349 -3.1556284][-7.8846784 -8.15267 -8.7616062 -8.3411264 -9.02536 -8.8446274 -7.6380973 -7.3284373 -6.6966076 -4.817564 -2.4929373 -0.92272139 -1.0570269 -3.145309 -2.9454236][-4.6570334 -5.8560538 -7.6164837 -7.9427414 -8.9401169 -8.4556112 -7.8767109 -7.4783211 -6.3551092 -4.7912593 -2.4542542 -0.91265106 -0.01578331 -1.4301448 -2.3040466][-7.2719421 -7.249938 -9.090045 -8.80739 -9.7359028 -7.9808731 -5.7961631 -5.7684216 -5.5048351 -4.1395826 -2.8389437 -1.8461211 -1.8722804 -3.929419 -3.761476][-7.9965935 -9.6580286 -11.251706 -11.398779 -10.690191 -6.440372 -3.3079422 -3.9745798 -4.679143 -3.0235095 -2.0504143 -1.6686063 -1.622344 -3.3899481 -4.7714128][-8.8520184 -9.9559526 -11.008583 -9.7021132 -8.1946754 -2.7213023 2.3228755 1.3993602 -0.54015589 -2.4494178 -4.1602335 -2.9519277 -2.4982941 -4.8728476 -5.3783531][-10.208483 -11.175017 -10.459212 -6.6013136 -2.997412 2.1502447 6.7602029 6.6446452 4.5438313 -0.62826729 -5.132021 -4.7927113 -5.1080279 -6.1833124 -6.2642021][-9.3332253 -10.528749 -9.4774666 -4.4381585 0.64560747 5.6369681 8.73867 7.0065389 5.2916417 1.0600085 -3.8852749 -5.4796143 -6.5962033 -6.9153557 -6.173903][-7.5188179 -7.653842 -6.8187938 -3.290684 -0.74834919 3.8948092 6.9800715 4.6398063 2.0651078 -2.2674544 -5.5975575 -6.78416 -7.7370691 -8.1660633 -7.8729267][-7.33418 -8.1094131 -7.0835481 -3.9933219 -3.0171311 -0.71102619 2.5778804 2.8388057 0.87383318 -3.8420177 -7.221808 -8.6343088 -9.4657278 -9.1489105 -8.3328161][-9.8870621 -10.459925 -10.665351 -8.3876991 -8.0254717 -6.4351773 -4.8003969 -4.3185577 -4.6475668 -6.4582062 -8.9286633 -10.483401 -10.256715 -10.04772 -8.49028][-15.617123 -14.135391 -12.055542 -9.7338352 -9.2859964 -8.2349663 -7.6618056 -7.4944773 -7.9249649 -9.6196518 -10.344011 -10.8385 -10.823467 -9.7786551 -8.5238218][-15.094454 -14.068695 -12.070573 -10.711883 -9.8109035 -8.06414 -7.3910036 -8.25964 -9.4203758 -10.116055 -10.797078 -10.99526 -10.669277 -9.7090569 -7.9315472][-11.203686 -10.955427 -10.646725 -9.4347458 -8.6886253 -7.9685774 -7.5339293 -7.4547963 -8.4687462 -9.2508345 -9.40665 -8.8813381 -8.6444445 -7.3894506 -6.7287111][-8.7832479 -7.9196987 -6.9806242 -5.910151 -5.1260595 -5.540834 -5.8359838 -6.2520795 -6.6031728 -6.5232611 -6.6143684 -6.7241197 -6.71521 -6.160656 -6.1522431]]...]
INFO - root - 2017-12-15 20:22:22.561716: step 54610, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 50h:18m:46s remains)
INFO - root - 2017-12-15 20:22:29.109598: step 54620, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 51h:23m:00s remains)
INFO - root - 2017-12-15 20:22:35.563886: step 54630, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 50h:07m:15s remains)
INFO - root - 2017-12-15 20:22:42.073169: step 54640, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 50h:33m:43s remains)
INFO - root - 2017-12-15 20:22:48.747953: step 54650, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 52h:54m:09s remains)
INFO - root - 2017-12-15 20:22:55.403815: step 54660, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 49h:14m:27s remains)
INFO - root - 2017-12-15 20:23:02.000221: step 54670, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.688 sec/batch; 53h:07m:33s remains)
INFO - root - 2017-12-15 20:23:08.620869: step 54680, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 50h:20m:07s remains)
INFO - root - 2017-12-15 20:23:15.273790: step 54690, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 50h:57m:39s remains)
INFO - root - 2017-12-15 20:23:21.868471: step 54700, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 50h:47m:25s remains)
2017-12-15 20:23:22.385109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4557295 -5.8079705 -5.2252645 -4.8182073 -5.5413156 -5.7394152 -5.51489 -5.1970692 -4.5345478 -3.5529065 -2.5967307 -3.6875196 -4.0982356 -4.457056 -4.1791987][-6.8283458 -5.6316724 -5.46327 -6.0076361 -7.0302482 -7.7930393 -7.8186808 -6.8099785 -5.9693446 -5.127604 -4.3495264 -4.86966 -6.5622988 -6.3218465 -4.1807566][-4.6714568 -5.3233848 -6.485477 -6.4857564 -7.9760547 -8.5507641 -8.8502254 -8.7256317 -8.1215973 -7.0389795 -6.0251803 -6.7476611 -8.1218176 -7.6283379 -6.4409566][-4.2646613 -4.6868839 -5.4386396 -5.8387346 -7.6034474 -8.0356617 -8.09902 -7.6340027 -7.3284659 -7.4279394 -6.8893452 -7.8405619 -8.9127045 -8.5610619 -7.6006217][-4.5732117 -5.0911455 -5.2515607 -5.0265851 -5.3312559 -4.8036189 -4.9006181 -5.598454 -6.2077284 -6.4435825 -6.2796946 -7.5226488 -8.309453 -8.1226168 -7.1729493][-6.97324 -6.4449382 -5.2287135 -3.7927241 -3.3777664 -1.9549754 -1.2504745 -0.88909531 -1.0615721 -2.6530786 -4.0550809 -5.3452425 -6.6686764 -7.1254306 -6.2833815][-7.9467373 -6.6304803 -4.2685757 -2.3043807 -1.0762339 1.0002761 2.0320415 2.2751508 2.3640666 0.93057775 -0.74192619 -2.6395886 -4.8761244 -5.5901546 -5.0172448][-7.5113955 -6.2757816 -4.2029071 -1.756176 0.62921429 2.1283317 3.136375 4.1174951 4.5697026 2.9200597 1.5965052 -0.87421274 -3.4379451 -3.5734656 -2.826869][-5.7387567 -4.2915163 -2.7271976 -0.59051609 0.77973747 2.5977435 3.5429978 3.8719687 4.0228086 2.6592278 1.7286272 -0.24022341 -2.5026927 -3.6696932 -2.5384061][-5.6504979 -3.4492142 -1.6078682 0.17124605 0.64178276 1.3369265 1.8719234 2.5895381 2.5775886 1.4132729 1.0298276 -1.3872027 -4.1329989 -4.0752873 -2.6077189][-9.0803738 -6.8652053 -3.9357429 -1.5743589 -1.2749801 -1.1338329 -0.87926722 -1.0016618 -1.6331763 -2.4043651 -2.8320305 -4.9356337 -5.9674067 -5.725297 -3.5353274][-14.25559 -11.817474 -7.9088993 -4.6373291 -3.8272662 -3.4475904 -4.2644925 -5.1008267 -5.6079021 -6.3500819 -7.018084 -8.5986166 -9.2572718 -7.8500023 -5.3132911][-14.577044 -12.425967 -9.1745653 -6.5407829 -4.9454112 -4.4910793 -5.1817961 -6.7970638 -8.557353 -8.9410744 -9.5407257 -9.290659 -9.0271816 -8.5873508 -6.7256041][-12.256392 -10.873486 -8.1579351 -5.6697669 -4.4861231 -3.4212439 -4.6019688 -5.4155397 -6.4022751 -7.5418377 -8.5979137 -7.756196 -7.641077 -7.2519484 -6.7571092][-7.6442246 -7.0809712 -5.4956117 -4.3618197 -3.3334413 -2.8386657 -2.8177404 -2.7924685 -3.9222496 -5.2648134 -6.3232875 -6.923666 -7.9383469 -8.5819912 -8.2858791]]...]
INFO - root - 2017-12-15 20:23:29.041558: step 54710, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 52h:28m:54s remains)
INFO - root - 2017-12-15 20:23:35.575333: step 54720, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 49h:23m:29s remains)
INFO - root - 2017-12-15 20:23:42.157300: step 54730, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 49h:12m:09s remains)
INFO - root - 2017-12-15 20:23:48.686538: step 54740, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 49h:36m:16s remains)
INFO - root - 2017-12-15 20:23:55.289386: step 54750, loss = 0.29, batch loss = 0.25 (12.5 examples/sec; 0.639 sec/batch; 49h:17m:05s remains)
INFO - root - 2017-12-15 20:24:01.898614: step 54760, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 49h:53m:30s remains)
INFO - root - 2017-12-15 20:24:08.401481: step 54770, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 51h:07m:10s remains)
INFO - root - 2017-12-15 20:24:15.018557: step 54780, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 51h:15m:39s remains)
INFO - root - 2017-12-15 20:24:21.694413: step 54790, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 51h:27m:44s remains)
INFO - root - 2017-12-15 20:24:28.319052: step 54800, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 50h:28m:48s remains)
2017-12-15 20:24:28.848061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.581821 -2.6885362 -2.7797005 -2.8614233 -3.4509349 -3.5866394 -4.0557661 -3.5933807 -3.6231284 -4.0768938 -4.9284697 -6.9153891 -8.3103561 -9.8621044 -9.4538841][-3.5899241 -3.4113052 -3.9000604 -4.0819035 -4.5555425 -4.5500164 -4.2950168 -3.936902 -4.2673807 -4.1799135 -4.6501942 -6.6307697 -8.196106 -9.876461 -10.063183][-4.7185931 -5.6965027 -5.53112 -4.797883 -4.6919003 -3.9858875 -3.2236562 -3.0713909 -3.7001016 -4.0499587 -4.1667304 -5.9660821 -7.1875591 -9.4265156 -9.1609354][-6.1976743 -6.3487039 -5.7705112 -4.6341896 -4.1593533 -3.3213441 -2.70642 -2.578114 -2.9768729 -3.0643237 -2.9717529 -4.8861933 -6.2579618 -8.5130272 -8.0568981][-7.1139779 -6.9408751 -6.3221192 -4.0260477 -2.6748519 -0.88367748 -0.51125383 -0.78670263 -0.79466867 -1.3319082 -1.6126804 -2.8886244 -3.3990211 -6.3440051 -6.3527474][-7.1961222 -6.8950162 -5.018631 -1.6387911 0.88647223 2.3783088 2.6388631 2.2771344 1.8553576 0.7678442 -0.033516884 -1.8640831 -2.8113103 -5.8341737 -5.7914615][-7.936729 -7.0656686 -4.87254 -1.0897427 1.6859651 4.2065272 5.1844954 4.9259133 4.4920249 2.8892751 1.262311 -1.4712257 -3.0488966 -5.522162 -5.0227785][-6.6957421 -6.8233824 -5.1863284 -1.671371 1.3635721 4.816011 6.0299067 5.4724956 5.4982762 4.2351918 2.4750209 -0.42365408 -2.5189693 -5.1678047 -4.3821917][-5.9396958 -5.744164 -4.8416972 -2.1163368 0.20377445 3.5478263 5.3860564 5.6941743 5.8850741 4.3368239 2.5360618 -0.74555683 -3.1170826 -5.1164746 -4.6222844][-4.657052 -4.6971779 -4.3445721 -1.8814089 -0.56070566 1.8164511 3.3445697 3.845572 3.7502723 2.3765969 1.6180701 -1.3325834 -4.0817208 -6.8536253 -7.3331842][-7.8080072 -7.5497646 -6.638382 -4.6011457 -3.5662422 -1.7310584 -1.3688307 -1.5808053 -1.7810645 -2.7206359 -3.0556283 -5.7062793 -6.9755907 -9.1357174 -8.9746027][-11.131818 -9.4704924 -8.4466534 -6.2074418 -5.2392054 -4.0449486 -4.6424556 -5.7769723 -6.4377642 -7.2321491 -7.3497381 -8.8310957 -9.0923071 -10.752168 -10.790991][-10.840614 -10.048916 -8.9505091 -7.6510749 -6.6407561 -5.758585 -6.2326837 -6.8289003 -8.4018288 -9.35809 -9.3810987 -9.8939486 -9.4484835 -10.468296 -9.0493832][-11.019454 -11.150648 -10.305707 -8.619257 -7.1109667 -6.4758997 -6.852437 -7.6690907 -8.2564468 -8.5479031 -8.5530052 -7.759491 -6.8038492 -7.3823748 -6.4698586][-8.7472286 -9.3834562 -8.8431292 -8.0474119 -7.0965409 -7.5441132 -7.4154396 -7.552062 -7.78753 -7.4787197 -7.4659905 -7.1865511 -6.6168313 -6.1913867 -5.2944341]]...]
INFO - root - 2017-12-15 20:24:35.371933: step 54810, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 50h:10m:07s remains)
INFO - root - 2017-12-15 20:24:41.965720: step 54820, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 50h:08m:30s remains)
INFO - root - 2017-12-15 20:24:48.621162: step 54830, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 50h:47m:45s remains)
INFO - root - 2017-12-15 20:24:55.232546: step 54840, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 51h:13m:20s remains)
INFO - root - 2017-12-15 20:25:01.846068: step 54850, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 49h:47m:08s remains)
INFO - root - 2017-12-15 20:25:08.540879: step 54860, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 50h:31m:03s remains)
INFO - root - 2017-12-15 20:25:15.206723: step 54870, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 51h:20m:54s remains)
INFO - root - 2017-12-15 20:25:21.789066: step 54880, loss = 0.16, batch loss = 0.12 (12.7 examples/sec; 0.632 sec/batch; 48h:43m:23s remains)
INFO - root - 2017-12-15 20:25:28.439007: step 54890, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 51h:59m:10s remains)
INFO - root - 2017-12-15 20:25:35.089243: step 54900, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 50h:58m:08s remains)
2017-12-15 20:25:35.588227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6163535 -8.2678356 -8.7657509 -9.02631 -10.141172 -10.737679 -10.829058 -11.078848 -10.648561 -9.6246519 -8.0848846 -10.136561 -10.055313 -11.571752 -11.452465][-7.8458972 -9.79281 -10.612755 -11.276337 -12.092955 -12.844286 -13.060474 -12.663489 -11.203115 -9.9530983 -8.9370728 -10.352047 -10.624038 -12.295484 -11.499907][-7.5091329 -10.425685 -12.375942 -13.451375 -13.987612 -14.001583 -13.819668 -13.000337 -11.31209 -10.319613 -9.3733931 -11.329802 -12.252912 -13.854624 -13.24044][-8.0486622 -9.9418564 -11.644251 -12.520836 -13.80547 -12.393726 -10.621415 -9.6738014 -8.0216455 -8.2299862 -8.8124657 -11.353269 -12.392859 -14.148373 -14.765018][-8.7427959 -10.823839 -12.977398 -11.587599 -10.901831 -7.4043317 -4.6956129 -5.2131896 -5.5448318 -5.1461787 -5.4453063 -8.3377218 -10.451653 -13.705882 -14.252167][-9.8302631 -11.890094 -13.365751 -9.8390083 -6.336051 -1.3028111 3.2805963 3.2607064 1.5276489 -1.5532007 -3.9407525 -6.6261811 -8.850564 -12.493175 -13.417559][-8.2053261 -10.203901 -11.667702 -7.9378471 -3.987927 1.650393 7.05675 8.9566822 8.4238815 4.2967381 0.020680904 -5.6040568 -8.5039825 -11.610376 -12.562289][-5.9026608 -8.2824717 -9.0378361 -6.9443541 -2.8386016 5.5283647 11.640223 10.766001 9.1505909 4.4026484 -0.3712163 -6.512538 -9.3198128 -12.010002 -12.245923][-3.9655776 -6.2986584 -7.3063612 -6.3890786 -4.8590374 1.1846528 7.5579839 7.2004342 5.8976445 0.64363575 -3.4572191 -9.51778 -12.774914 -15.327681 -14.945055][-3.1560438 -4.9958944 -7.5499535 -6.2853723 -5.6197815 -3.4062221 -1.1066041 0.8717885 0.65440226 -4.2409124 -7.8540554 -13.758577 -16.198816 -18.953394 -19.039501][-6.3094139 -7.6147723 -9.4783707 -9.4977264 -9.3479929 -8.1071968 -7.0003271 -5.5324841 -5.4033394 -8.2437029 -10.272401 -16.038481 -17.245487 -19.842064 -19.421684][-9.4865551 -10.244965 -11.504965 -10.791996 -10.980991 -10.284998 -10.685164 -10.833491 -11.1185 -11.789244 -12.168633 -14.705368 -14.667879 -17.117058 -15.481693][-11.855636 -10.886878 -11.029264 -11.107817 -12.271181 -11.260833 -11.919247 -12.132186 -12.420822 -13.424093 -13.358891 -13.364039 -12.850574 -14.630726 -12.156559][-9.4212942 -9.3111048 -9.9719791 -9.2637577 -8.7554827 -8.8813906 -9.5293818 -10.037998 -10.333221 -10.242401 -10.530567 -10.20636 -9.3186684 -10.285816 -10.405277][-7.7317915 -7.9755616 -6.9064054 -5.6097107 -5.6443639 -5.6107883 -5.4132628 -5.9385829 -6.8196583 -7.545002 -7.8799796 -8.1046715 -8.2942476 -8.40607 -8.7399645]]...]
INFO - root - 2017-12-15 20:25:42.224610: step 54910, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 51h:11m:36s remains)
INFO - root - 2017-12-15 20:25:48.806036: step 54920, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 50h:11m:02s remains)
INFO - root - 2017-12-15 20:25:55.422915: step 54930, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.680 sec/batch; 52h:23m:41s remains)
INFO - root - 2017-12-15 20:26:02.034260: step 54940, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 49h:42m:10s remains)
INFO - root - 2017-12-15 20:26:08.701255: step 54950, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 51h:23m:42s remains)
INFO - root - 2017-12-15 20:26:15.353263: step 54960, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 50h:59m:18s remains)
INFO - root - 2017-12-15 20:26:21.956633: step 54970, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 50h:54m:29s remains)
INFO - root - 2017-12-15 20:26:28.593206: step 54980, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 52h:41m:13s remains)
INFO - root - 2017-12-15 20:26:35.239068: step 54990, loss = 0.24, batch loss = 0.19 (11.8 examples/sec; 0.679 sec/batch; 52h:19m:44s remains)
INFO - root - 2017-12-15 20:26:41.834786: step 55000, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 51h:42m:01s remains)
2017-12-15 20:26:42.365249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8640552 -3.8632717 -4.0700026 -4.092308 -4.3917089 -4.3348002 -4.3085527 -4.1582856 -3.4478836 -2.7909241 -1.8643117 -3.1505029 -3.5760922 -4.82891 -4.8206835][-2.5415509 -2.2602541 -3.2418289 -3.7380743 -4.98516 -4.8539314 -3.9976876 -3.0433428 -2.5741248 -1.8404777 -0.89273643 -1.9014938 -2.0856001 -3.3888533 -3.5360053][-1.9502013 -2.5459266 -3.6441643 -3.67452 -4.6379037 -4.5056181 -3.6135077 -2.8056262 -2.0528522 -1.5583973 -0.68433666 -2.0204136 -2.0273194 -3.0798476 -2.5696411][-3.582891 -4.0124927 -4.5993557 -4.5200324 -5.2082806 -4.004612 -2.2648745 -1.2954149 -0.92327595 -1.0047755 -1.0410876 -2.9121606 -3.2468987 -3.9496069 -3.2903731][-4.9770803 -6.2378621 -7.2392855 -6.3975816 -6.2248287 -3.7200894 -1.2457528 -0.17938614 0.29134226 -0.14786577 -0.73443222 -3.0130205 -3.7558866 -4.4994006 -3.6825109][-6.0482965 -6.5811319 -7.4705019 -6.3770423 -4.3460159 -0.27347326 2.312079 3.2018294 3.1442924 1.2482939 -0.65919781 -3.3787789 -4.1345263 -5.3779392 -4.4553909][-7.5733252 -7.632628 -7.0304914 -4.98413 -2.8403258 1.7936935 5.4795775 7.026937 6.6276793 3.1341386 0.20775414 -3.0907495 -5.110291 -6.4969668 -5.4090533][-8.0720024 -8.0311594 -7.5419559 -4.5053453 -1.6261482 3.5466228 7.64364 8.2969017 7.0015473 3.767014 0.599308 -3.6438062 -5.8162966 -7.2507677 -6.6168427][-8.790802 -8.6583309 -7.6492352 -4.5491447 -2.4350533 1.9144697 5.5171218 7.1091619 6.480125 3.4204926 0.62466621 -4.3244505 -7.355197 -8.4152708 -7.0660648][-7.9898939 -8.2139273 -8.2963934 -5.0510464 -2.5231853 0.2931304 1.7125778 3.2601542 3.697845 0.84073734 -2.7068019 -6.4890318 -8.4988613 -9.5229836 -8.2883978][-11.591032 -11.034718 -10.090555 -7.7266006 -6.250936 -4.3224611 -2.889698 -2.539242 -2.5803719 -3.4755473 -5.0808783 -9.0932446 -11.571875 -11.488306 -9.477025][-13.453049 -13.316162 -11.96533 -9.413763 -8.1833973 -7.0969381 -6.2748957 -6.5546517 -7.3849964 -7.6818781 -8.4896774 -10.409473 -11.161777 -11.568913 -10.335768][-13.436238 -12.577719 -11.305807 -9.1793079 -9.064703 -8.163002 -7.5610256 -7.8969784 -8.3300724 -8.1757278 -7.8518362 -9.0980148 -9.9209309 -9.5021563 -7.8311405][-12.62257 -11.823687 -10.662247 -9.1971731 -8.4028969 -8.0795527 -8.441453 -7.9765511 -7.1447067 -7.5809937 -7.6067843 -7.4075594 -6.6041265 -6.7227192 -5.7692709][-9.6407833 -9.641798 -9.4976158 -8.7573471 -7.4662337 -6.518785 -6.4102225 -6.5643024 -7.0281076 -6.3834429 -5.5727072 -6.314671 -6.9672995 -7.5675335 -7.7057605]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-55000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-55000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 20:26:49.904663: step 55010, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.676 sec/batch; 52h:08m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 20:26:56.539231: step 55020, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 51h:18m:11s remains)
INFO - root - 2017-12-15 20:27:03.199898: step 55030, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 51h:33m:47s remains)
INFO - root - 2017-12-15 20:27:09.800164: step 55040, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 49h:26m:26s remains)
INFO - root - 2017-12-15 20:27:16.390723: step 55050, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 51h:38m:23s remains)
INFO - root - 2017-12-15 20:27:23.072018: step 55060, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.690 sec/batch; 53h:09m:30s remains)
INFO - root - 2017-12-15 20:27:29.670510: step 55070, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 51h:59m:34s remains)
INFO - root - 2017-12-15 20:27:36.220855: step 55080, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 50h:06m:45s remains)
INFO - root - 2017-12-15 20:27:42.823970: step 55090, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 50h:20m:48s remains)
INFO - root - 2017-12-15 20:27:49.448040: step 55100, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 50h:37m:22s remains)
2017-12-15 20:27:49.929240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8126783 -7.8314638 -7.8307452 -7.0776138 -6.9122548 -6.3661885 -6.2620549 -6.0143361 -5.8282652 -6.2546477 -6.0839033 -8.1087952 -9.2986345 -6.9706411 -5.436625][-5.8721089 -5.6532254 -6.0444012 -5.9842424 -6.7529459 -6.3752708 -5.9473028 -6.1833925 -6.3768225 -6.551136 -6.702611 -8.7901077 -9.3613157 -8.0874062 -7.6324387][-3.448559 -4.8063254 -5.5302544 -5.3744192 -5.7115088 -5.2745576 -5.2030954 -5.1578412 -5.3918791 -5.891017 -6.3184013 -8.4211178 -9.5769 -8.5006409 -8.4413366][-3.7697015 -5.3188114 -5.4026475 -5.5458269 -5.5742459 -5.0615516 -4.706975 -4.9614587 -4.9948168 -5.2845721 -5.4355488 -7.8868885 -9.3901157 -9.3967352 -9.1766109][-6.2914534 -7.5882506 -7.3820791 -6.3270073 -4.4926605 -1.6729846 -0.93743229 -2.7874589 -4.0756407 -3.8169806 -4.4078379 -6.4831505 -8.2245169 -8.9517365 -10.10523][-7.1150508 -8.443573 -6.9726419 -5.3205028 -2.1285422 0.99929047 3.7143998 2.7194781 1.0582013 -0.90405655 -3.17085 -4.7736773 -6.7149711 -7.9684668 -9.1496935][-8.8460293 -6.9379716 -5.6253061 -2.8368542 -0.88439083 3.9815812 8.165123 7.7788568 6.1502557 2.3463264 -1.0685058 -4.1995087 -6.989604 -7.4283752 -8.0662575][-8.6836433 -8.0831642 -5.2336531 -2.5612226 0.0021729469 4.0369067 8.2936382 9.2115936 8.568222 4.4096894 0.089026928 -5.0188675 -8.5678482 -8.9195576 -8.8296251][-6.9192743 -6.5638552 -5.5163922 -1.8371708 0.20843792 3.6323514 6.9020219 8.2528667 7.8686795 4.2807307 0.82776642 -4.8664441 -8.66916 -9.281559 -10.687401][-6.9075861 -5.9619775 -4.5048418 -1.0166936 0.5294776 2.951982 5.8893638 5.7270036 4.3873315 2.1867986 -1.4824848 -6.634871 -10.06632 -11.291985 -12.833416][-9.2017069 -8.4162521 -7.3615389 -4.4982691 -2.8927078 -1.5993271 -0.12273598 -0.7819767 -1.3157225 -3.1044264 -5.4415932 -10.073563 -12.713219 -13.688276 -13.944294][-14.343187 -13.586864 -11.914146 -9.1117506 -7.7104688 -6.7608547 -6.418786 -7.431633 -8.1474819 -9.437149 -10.577679 -12.227652 -13.181852 -13.439274 -14.625975][-15.709698 -13.842009 -12.113831 -10.73595 -9.44488 -8.7259312 -8.6769447 -9.5299463 -9.9739113 -10.707914 -10.450352 -11.572201 -12.436424 -11.647282 -11.901323][-13.13685 -12.241827 -11.632185 -9.6596775 -7.58875 -7.3938017 -7.655324 -7.5788584 -8.0372124 -8.6651745 -8.907937 -8.7702579 -9.0019941 -8.9069757 -8.8918819][-10.391277 -9.4067583 -9.1229916 -7.8286428 -7.0311742 -5.730381 -5.3027153 -5.833024 -6.5082994 -6.37051 -6.8681126 -7.9221864 -8.0428867 -6.845242 -6.6962786]]...]
INFO - root - 2017-12-15 20:27:56.549970: step 55110, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 50h:47m:19s remains)
INFO - root - 2017-12-15 20:28:03.129060: step 55120, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.648 sec/batch; 49h:54m:01s remains)
INFO - root - 2017-12-15 20:28:09.742595: step 55130, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.686 sec/batch; 52h:49m:06s remains)
INFO - root - 2017-12-15 20:28:16.367098: step 55140, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.652 sec/batch; 50h:14m:58s remains)
INFO - root - 2017-12-15 20:28:22.981833: step 55150, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 51h:17m:39s remains)
INFO - root - 2017-12-15 20:28:29.629508: step 55160, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 50h:43m:09s remains)
INFO - root - 2017-12-15 20:28:36.252799: step 55170, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 50h:50m:34s remains)
INFO - root - 2017-12-15 20:28:42.905715: step 55180, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.675 sec/batch; 52h:00m:39s remains)
INFO - root - 2017-12-15 20:28:49.501698: step 55190, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 49h:35m:44s remains)
INFO - root - 2017-12-15 20:28:56.078918: step 55200, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.653 sec/batch; 50h:15m:57s remains)
2017-12-15 20:28:56.557933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2955151 -3.7437322 -3.2628078 -2.2919998 -2.5990613 -2.4872458 -2.6337352 -2.93693 -2.9172277 -2.3541956 -1.661057 -3.1880867 -3.9928749 -5.1756592 -6.8177948][-2.6438766 -3.9844022 -5.3845606 -5.408565 -5.01612 -5.0023284 -4.8508244 -4.2515783 -4.1982961 -4.0118189 -3.829917 -5.0635867 -4.5386305 -4.9452491 -6.0062652][-2.9330213 -4.170362 -5.1192183 -5.9863572 -6.2754793 -5.76552 -5.3167267 -5.2673531 -4.9707208 -4.7202158 -4.1439447 -5.1521673 -4.7435474 -4.4708233 -4.7204428][-2.7232347 -3.8339887 -4.8170118 -4.8570862 -5.1626215 -4.2873106 -3.7867353 -4.0621767 -4.0801339 -3.889004 -3.8482821 -5.5039611 -5.4803824 -5.6782613 -6.0500488][-3.5400879 -4.1272573 -4.1886387 -3.2296996 -3.5118659 -2.8373995 -1.9099836 -1.5970349 -1.5611238 -2.4666166 -3.1097069 -4.8648562 -5.0559797 -5.5151243 -6.3576508][-5.3405638 -3.7429466 -2.4033031 -1.7281668 -1.2826014 -0.37461329 -0.00411129 -0.027482033 -0.2579565 -0.62908173 -0.73903656 -3.3336625 -3.9685898 -4.2194896 -5.40469][-6.0392871 -5.191884 -3.0649977 -1.0082936 0.049005985 1.1179585 1.2349548 1.0160489 0.4530015 0.11532545 0.41104031 -1.3631787 -1.760514 -2.6594458 -4.5407333][-5.8682084 -5.2083111 -3.0160851 -0.44512653 0.82058859 2.3748488 2.6099315 1.8598499 1.6153378 1.7383666 2.4061017 0.28689289 -0.765985 -2.1237836 -4.4156346][-5.0675988 -4.1451683 -2.297092 -0.064466476 1.289854 2.0277181 1.5002489 1.649972 1.4619622 1.4597664 2.0186725 0.54320717 -0.051671505 -1.5440984 -3.4898803][-3.5435038 -2.9133203 -1.8046947 -0.40788412 0.10076284 1.358551 1.4950366 0.8885355 0.50018978 1.1159067 2.4206662 0.88086224 -0.19424248 -1.5352573 -3.55699][-5.9309497 -5.2287531 -3.56959 -2.2858274 -1.9833851 -1.755975 -2.1117663 -1.2857594 -0.86266184 -0.72816086 -0.14861059 -2.5146594 -3.6550322 -3.7455049 -4.9815507][-9.8856411 -8.7647934 -6.43961 -4.758543 -4.0215931 -3.4268947 -3.8192983 -4.0510774 -4.2450886 -3.3523748 -2.8662629 -4.3503466 -5.0809274 -5.7087984 -6.2439036][-11.598009 -9.8872776 -7.7415257 -5.8744974 -5.0033622 -5.0444512 -4.8435073 -4.9809508 -5.2749557 -5.1708164 -5.3733149 -6.0198832 -5.7613807 -5.0651836 -4.9780779][-10.314754 -8.2168846 -6.7509284 -5.3556743 -4.9041243 -5.3872795 -5.4585915 -4.6859322 -4.4090767 -4.9392614 -4.4974174 -4.2271566 -4.751009 -4.0866861 -4.3133426][-6.3750343 -5.9541521 -5.6314826 -4.4523554 -3.6237786 -3.4547622 -3.5009315 -3.5021904 -3.8924005 -3.320698 -2.6300156 -3.9738812 -4.9954152 -5.4731641 -6.625041]]...]
INFO - root - 2017-12-15 20:29:03.136607: step 55210, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 51h:09m:56s remains)
INFO - root - 2017-12-15 20:29:09.737341: step 55220, loss = 0.24, batch loss = 0.20 (12.0 examples/sec; 0.667 sec/batch; 51h:22m:24s remains)
INFO - root - 2017-12-15 20:29:16.352517: step 55230, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 51h:45m:14s remains)
INFO - root - 2017-12-15 20:29:22.941453: step 55240, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 49h:08m:37s remains)
INFO - root - 2017-12-15 20:29:29.489221: step 55250, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.685 sec/batch; 52h:44m:34s remains)
INFO - root - 2017-12-15 20:29:36.093635: step 55260, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 50h:09m:08s remains)
INFO - root - 2017-12-15 20:29:42.694735: step 55270, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 50h:58m:52s remains)
INFO - root - 2017-12-15 20:29:49.296664: step 55280, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 51h:52m:54s remains)
INFO - root - 2017-12-15 20:29:55.822459: step 55290, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.643 sec/batch; 49h:28m:44s remains)
INFO - root - 2017-12-15 20:30:02.457749: step 55300, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 50h:40m:46s remains)
2017-12-15 20:30:02.978465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4251795 -3.8388362 -3.8836985 -3.8881075 -4.363018 -4.7527547 -4.934093 -4.6374879 -4.2772655 -4.064755 -3.2793939 -5.8785563 -8.2346191 -9.68709 -9.985569][-5.0030007 -4.9265928 -5.1304631 -4.9998431 -5.3025441 -5.7723775 -6.0741739 -6.2436981 -6.059628 -5.2885542 -4.3582368 -6.6868138 -8.252902 -10.303231 -11.105179][-4.4801211 -5.4744763 -6.321876 -6.0841756 -6.6784053 -7.0517073 -7.6844249 -7.4476228 -6.7144504 -6.4505424 -6.0152922 -8.021369 -9.886879 -11.472471 -11.885302][-5.3247662 -6.385571 -6.76363 -5.9756064 -6.4723549 -6.41557 -6.3303175 -6.5793829 -6.6524477 -6.01585 -5.1321859 -7.9919372 -10.387443 -12.424875 -13.056339][-6.7916117 -7.971333 -8.663559 -6.6439848 -6.4630933 -5.4899716 -4.4433427 -4.6890974 -5.1351008 -4.6662674 -4.0281029 -6.8246169 -9.3974476 -12.009521 -13.541563][-8.2653961 -8.5020437 -7.6597943 -5.5161514 -3.3270633 -1.4226303 -0.56519985 -0.95063591 -1.3917489 -1.9045951 -2.2509766 -4.902782 -7.6139603 -10.286833 -11.844481][-9.180769 -8.7031317 -7.1583557 -4.00766 -0.73884821 1.1735353 2.2603431 2.902328 2.6754537 0.77912617 -0.91800547 -4.24115 -6.9274635 -9.3077478 -10.716139][-8.2695608 -8.2689457 -6.2124863 -2.6074567 0.23252964 3.2408223 5.1963277 4.3037829 3.6240916 2.3695827 0.1054616 -4.2953386 -7.3443117 -9.6932507 -10.802357][-5.9080667 -5.6923881 -4.6559691 -2.5049868 -0.90968227 2.265656 3.7968202 3.9873376 4.1070838 2.10667 0.16525936 -4.5181217 -8.29331 -10.556632 -10.839529][-4.087019 -3.0271916 -2.7624686 -1.801101 -1.0391121 -0.223104 0.36593819 1.6731558 1.6733255 0.37209606 -0.70626259 -5.5378675 -9.3394165 -11.704592 -12.687883][-5.0770311 -4.2885885 -3.8035588 -3.1107409 -3.4322951 -3.3224728 -3.5834174 -3.3308234 -3.5624044 -3.7654672 -4.4586525 -8.94392 -12.193209 -13.605572 -13.315689][-9.1682005 -7.6241894 -6.2933545 -6.4746971 -6.7057729 -6.7504139 -7.8182707 -8.5146227 -8.8899794 -8.7093468 -8.8706818 -10.668936 -12.338203 -13.654767 -13.140045][-11.75412 -11.077774 -9.3913717 -8.5539484 -9.8919945 -9.6575089 -9.65671 -10.12638 -11.331867 -11.440369 -11.34699 -11.704807 -12.009262 -11.950832 -10.728605][-12.072175 -11.354223 -10.150051 -9.49068 -9.3795443 -9.8724957 -10.211058 -10.390388 -10.381298 -10.757362 -11.067926 -9.9495468 -9.2191706 -10.196981 -9.7262173][-9.7118092 -9.67097 -9.3811388 -8.0715885 -7.35786 -7.2859426 -7.9420366 -8.7469482 -9.2119007 -9.1394711 -8.6903734 -9.46465 -10.483624 -9.7524242 -9.6179981]]...]
INFO - root - 2017-12-15 20:30:09.564174: step 55310, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 52h:06m:48s remains)
INFO - root - 2017-12-15 20:30:16.070793: step 55320, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 50h:46m:54s remains)
INFO - root - 2017-12-15 20:30:22.650122: step 55330, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 50h:02m:09s remains)
INFO - root - 2017-12-15 20:30:29.270527: step 55340, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 49h:31m:20s remains)
INFO - root - 2017-12-15 20:30:35.804727: step 55350, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 49h:49m:01s remains)
INFO - root - 2017-12-15 20:30:42.383585: step 55360, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 49h:32m:39s remains)
INFO - root - 2017-12-15 20:30:48.928151: step 55370, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 51h:30m:28s remains)
INFO - root - 2017-12-15 20:30:55.623195: step 55380, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 52h:19m:41s remains)
INFO - root - 2017-12-15 20:31:02.217471: step 55390, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 52h:09m:34s remains)
INFO - root - 2017-12-15 20:31:08.858646: step 55400, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 49h:34m:24s remains)
2017-12-15 20:31:09.361290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.585536 -5.9734926 -4.9341431 -3.6528654 -3.6779253 -3.0213397 -2.1332326 -0.86016083 -0.10034037 0.60694838 -0.77936077 -4.4282203 -8.5843019 -9.8237858 -8.353569][-6.424613 -7.6864538 -8.0653582 -6.814455 -5.1698384 -4.0610762 -2.2217283 -0.55244064 -0.59861803 -1.2539225 -1.465786 -4.8121982 -9.1701546 -10.252057 -10.189414][-5.254138 -6.3749242 -8.1492176 -7.7265048 -7.6378288 -5.5852437 -2.576967 -0.99461937 -0.73132467 -1.7233968 -3.1783793 -6.9142637 -10.252655 -11.453564 -9.79263][-3.9163475 -5.5370922 -6.6505818 -6.1715083 -6.667151 -5.6845188 -4.399868 -2.592041 -1.4691219 -2.2474339 -3.9354439 -8.7964611 -12.813402 -12.12973 -9.8950081][-5.640739 -5.5866585 -5.6810184 -4.9647665 -5.1383214 -3.3833714 -1.6364355 -2.3191378 -2.8375363 -3.2201555 -4.2553258 -8.2867346 -12.511095 -12.856337 -10.504608][-8.3494511 -6.4693642 -6.2511754 -4.602355 -4.2076945 -1.5851417 0.50300121 0.35403824 -0.62060404 -3.1067064 -5.2264128 -8.5402222 -11.85153 -13.004679 -10.827765][-8.45932 -7.5273085 -7.2803607 -4.8712597 -3.8358774 -0.63066292 2.3936229 3.549314 3.015986 -0.66977882 -4.8377209 -7.9398417 -10.274748 -10.855127 -10.309782][-7.3676581 -6.9058819 -6.7163844 -3.8496132 -1.9901266 1.004395 3.9225011 4.9003978 4.5732636 1.7368212 -1.5314674 -6.9559374 -11.218227 -11.102587 -9.6356516][-6.6600895 -5.6035709 -5.0949845 -2.662112 -1.8424456 1.639708 4.1433539 5.3762088 5.0259471 1.4881516 -1.5820384 -6.5830264 -11.234306 -12.650331 -11.215901][-5.9725642 -5.4417648 -4.7417946 -2.8036804 -1.9371336 -0.17214775 0.913383 2.8657775 3.426373 1.5446806 -1.3755016 -6.9216805 -11.432871 -12.791443 -11.624436][-6.2018404 -7.4184613 -7.7665119 -6.6411624 -4.9154749 -3.5550795 -2.2038672 -1.0870223 -1.1225591 -1.7493978 -2.9344716 -7.9770012 -11.700098 -13.519844 -12.497469][-10.233088 -8.6810331 -8.0208921 -7.3548765 -7.4530416 -6.6935134 -6.6171732 -7.0130167 -7.0704441 -6.4762688 -7.0517683 -9.27492 -10.870335 -11.370923 -10.323944][-13.457775 -11.683622 -8.708149 -8.24906 -8.4559193 -8.824749 -9.4643764 -8.73228 -8.2963409 -8.0540142 -8.06116 -9.4171858 -11.619023 -11.078812 -8.715271][-12.511059 -10.991226 -8.6009369 -7.7712822 -7.7457337 -7.162992 -7.4958115 -8.1325645 -7.7873316 -8.2437191 -8.0377884 -6.6390343 -6.8062086 -8.8675232 -9.2578764][-9.4152031 -7.6753316 -6.8714471 -5.6945238 -5.7455983 -6.581171 -6.5118241 -5.6639709 -5.367496 -5.8094211 -6.4547248 -8.7705688 -9.4231424 -8.3233528 -8.7647791]]...]
INFO - root - 2017-12-15 20:31:15.917497: step 55410, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 49h:21m:45s remains)
INFO - root - 2017-12-15 20:31:22.502949: step 55420, loss = 0.28, batch loss = 0.24 (12.1 examples/sec; 0.659 sec/batch; 50h:42m:17s remains)
INFO - root - 2017-12-15 20:31:29.104072: step 55430, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 50h:11m:01s remains)
INFO - root - 2017-12-15 20:31:35.735352: step 55440, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.686 sec/batch; 52h:47m:07s remains)
INFO - root - 2017-12-15 20:31:42.353970: step 55450, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 51h:20m:01s remains)
INFO - root - 2017-12-15 20:31:48.977437: step 55460, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 50h:26m:26s remains)
INFO - root - 2017-12-15 20:31:55.596088: step 55470, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 51h:14m:45s remains)
INFO - root - 2017-12-15 20:32:02.197510: step 55480, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 50h:52m:59s remains)
INFO - root - 2017-12-15 20:32:08.837418: step 55490, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 50h:42m:09s remains)
INFO - root - 2017-12-15 20:32:15.414259: step 55500, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 49h:40m:28s remains)
2017-12-15 20:32:15.925036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0903535 -4.154829 -3.5816219 -2.5719049 -2.8282659 -2.8240564 -3.0803413 -4.5140505 -5.9397345 -7.01789 -7.7695541 -8.7442856 -10.253059 -10.140192 -8.9989967][-4.5823612 -3.6663263 -2.6975155 -1.9134631 -1.6090207 -2.0678577 -2.9812503 -4.6407251 -6.138165 -7.7187872 -8.6366472 -10.908568 -12.443191 -11.880527 -10.176071][-3.4775414 -3.8708797 -4.2850189 -3.3546355 -3.363879 -3.269664 -3.8227885 -5.2524123 -6.7584152 -7.9733982 -8.8085194 -9.9673777 -11.829695 -11.79914 -10.284782][-6.4471884 -6.1802659 -5.5351019 -4.1465225 -3.8870065 -3.446285 -3.5323088 -4.223114 -5.382473 -6.5091114 -6.9868379 -8.7835112 -10.677721 -10.994778 -10.630941][-6.5311437 -6.8976007 -6.4633093 -4.3519011 -3.3493454 -2.7612767 -2.6568787 -3.6655452 -4.4356594 -3.9019737 -4.4042206 -5.7927828 -7.8477039 -9.4248466 -9.6390667][-6.4258657 -5.869328 -4.6884346 -2.6471989 -1.0236883 0.92507219 2.044991 0.92156267 -0.69692421 -2.0924377 -2.999655 -4.1515441 -5.4816809 -6.8462615 -8.0151482][-4.8061829 -4.2989755 -2.9234641 -0.61885118 1.8749785 4.23023 5.3567262 4.6874804 2.8773732 -0.28286314 -2.8611271 -3.3887269 -4.2270679 -5.1369472 -6.2081671][-5.2339745 -3.9583471 -2.8021479 0.24787331 3.3421235 5.6826148 6.769299 6.1395431 3.94729 0.68897867 -1.1730189 -2.7043743 -4.331749 -4.707057 -4.8624482][-5.5780754 -4.5361819 -2.5901411 -0.18556452 1.563138 4.2084155 5.5063872 3.8236337 1.6110334 0.10874557 -0.60372114 -1.7914226 -3.4979522 -4.2976065 -4.7558928][-5.8324342 -5.6755633 -4.16557 -2.0202382 -1.1644168 0.61460066 1.7410889 0.63730288 -0.80770779 -1.0606241 -0.14843893 -1.3885074 -3.0094056 -3.6642156 -5.4469829][-9.1629143 -8.5882263 -7.5395856 -6.5084133 -6.0698066 -5.0928221 -4.7102661 -4.5000362 -4.1294422 -3.3756771 -2.8563275 -3.7445576 -5.2888813 -5.6221833 -5.98152][-12.133303 -11.962509 -10.731614 -8.6982565 -7.7893314 -7.7837429 -9.1128769 -8.6944189 -7.2811894 -6.7323027 -5.887188 -6.5422053 -7.1680679 -6.7336979 -6.5946279][-13.061049 -12.477892 -12.159328 -10.262537 -9.8577318 -9.75063 -10.68213 -10.286507 -9.0934525 -8.3047295 -7.3989754 -6.882761 -7.2840605 -6.1912394 -5.3526073][-10.870333 -11.544866 -11.145514 -9.9732056 -9.2300758 -9.0593233 -9.7324753 -9.5143442 -8.8208046 -7.5286617 -5.7128749 -5.0729275 -4.5504923 -4.6916666 -5.5767441][-7.6107836 -8.3827505 -9.5926847 -9.0370579 -7.8597879 -6.8716354 -6.468698 -5.6502566 -6.1878386 -6.14525 -5.389492 -5.09958 -6.185781 -6.2120996 -6.021296]]...]
INFO - root - 2017-12-15 20:32:22.503404: step 55510, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 50h:33m:42s remains)
INFO - root - 2017-12-15 20:32:29.039862: step 55520, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 49h:33m:12s remains)
INFO - root - 2017-12-15 20:32:35.634904: step 55530, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 51h:18m:19s remains)
INFO - root - 2017-12-15 20:32:42.198530: step 55540, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 49h:17m:39s remains)
INFO - root - 2017-12-15 20:32:48.784703: step 55550, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 49h:35m:31s remains)
INFO - root - 2017-12-15 20:32:55.431041: step 55560, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 51h:51m:12s remains)
INFO - root - 2017-12-15 20:33:02.105648: step 55570, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 51h:01m:11s remains)
INFO - root - 2017-12-15 20:33:08.699829: step 55580, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 50h:48m:30s remains)
INFO - root - 2017-12-15 20:33:15.315687: step 55590, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 52h:10m:17s remains)
INFO - root - 2017-12-15 20:33:21.975413: step 55600, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 51h:14m:16s remains)
2017-12-15 20:33:22.533153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0236535 -7.7951608 -7.7080097 -7.6666384 -8.1263447 -8.2003689 -8.2845211 -8.3649044 -7.9906197 -7.4364605 -6.1113276 -5.7046 -6.2012339 -5.4393306 -4.467073][-7.5139523 -7.6712503 -7.513689 -6.8424959 -7.7616615 -7.6136103 -7.4028111 -7.7823982 -7.63336 -6.2106051 -4.2172132 -4.9412274 -5.9595156 -5.4395523 -5.0310616][-7.0126405 -6.7622027 -6.5413046 -5.4432979 -6.23514 -6.80371 -7.058671 -6.4250154 -5.703361 -4.9731259 -4.5193925 -5.0224648 -6.4522076 -6.4037714 -6.9668632][-7.0213161 -6.296277 -5.2694163 -4.3306923 -4.9931684 -4.6207728 -4.6015558 -5.2565608 -4.7275133 -3.6994312 -3.0463846 -4.0072346 -6.1235757 -7.0503345 -8.3691044][-7.1568136 -6.9031811 -6.4186811 -3.5215337 -3.1161056 -2.5844462 -1.6993289 -2.6425827 -3.9035945 -2.9228487 -2.2457662 -3.4989996 -5.6353159 -7.0166478 -7.4511566][-7.7003055 -6.8619847 -4.9042315 -1.5032291 0.25164604 2.2763734 3.3167272 2.4155316 1.5565705 -0.15596151 -2.5639317 -2.6798165 -4.5997615 -6.576005 -7.3652229][-8.0493021 -6.0822058 -4.101141 -0.41612911 1.7450156 3.7254186 5.3364291 5.4544492 5.412128 2.7507386 0.01031065 -2.8908432 -7.1922741 -7.9054527 -8.289854][-8.8466463 -6.2373114 -3.7011707 0.50199842 1.716526 3.83184 5.1798425 4.719502 4.3985267 2.4205213 0.013742447 -3.7933493 -7.3178968 -8.0728817 -8.789835][-6.4808111 -4.5563278 -3.1987255 -0.33398676 0.61036682 2.4482493 4.0849233 3.4445128 2.070745 0.69920635 -1.3559713 -4.7704768 -7.7264872 -8.671011 -9.1436777][-6.1872168 -4.6741037 -4.663321 -1.6841254 -0.29943609 0.01655817 1.0625348 0.32932377 -0.7068429 -1.9120533 -3.4925716 -6.5142903 -10.001732 -10.27954 -9.9220915][-9.2237415 -8.7392178 -8.0805445 -5.87477 -5.8635936 -5.1469035 -3.9660683 -4.6405687 -4.8763609 -6.1376228 -8.1720181 -11.457312 -13.307011 -12.5425 -11.431673][-13.628407 -12.716819 -11.401072 -10.179789 -10.527665 -9.48057 -9.1907444 -9.6130123 -10.276217 -10.834341 -12.364607 -14.611824 -16.10734 -14.92127 -13.092639][-14.982494 -13.165147 -11.810945 -10.262726 -10.227034 -10.00491 -10.08008 -10.380644 -11.172579 -12.172031 -13.053604 -13.699253 -13.628304 -11.905405 -10.343282][-11.316479 -10.053816 -9.2820959 -7.3663206 -6.5120783 -7.0086222 -7.4812212 -7.4112225 -7.7601576 -8.7152719 -9.1833925 -9.1072025 -8.3269024 -8.13811 -7.5443087][-9.6724443 -10.323259 -9.5506859 -7.2592134 -5.3891349 -5.1983147 -5.5419016 -5.5556989 -5.5923157 -5.059001 -4.7944627 -5.9822822 -6.766325 -6.6397948 -5.7253628]]...]
INFO - root - 2017-12-15 20:33:29.119857: step 55610, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 49h:26m:19s remains)
INFO - root - 2017-12-15 20:33:35.825122: step 55620, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.688 sec/batch; 52h:55m:21s remains)
INFO - root - 2017-12-15 20:33:42.379654: step 55630, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 51h:50m:10s remains)
INFO - root - 2017-12-15 20:33:49.033757: step 55640, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.647 sec/batch; 49h:43m:18s remains)
INFO - root - 2017-12-15 20:33:55.693363: step 55650, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 50h:07m:09s remains)
INFO - root - 2017-12-15 20:34:02.299478: step 55660, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 50h:44m:50s remains)
INFO - root - 2017-12-15 20:34:08.894855: step 55670, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 50h:31m:33s remains)
INFO - root - 2017-12-15 20:34:15.545693: step 55680, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 50h:34m:31s remains)
INFO - root - 2017-12-15 20:34:22.101789: step 55690, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 50h:41m:14s remains)
INFO - root - 2017-12-15 20:34:28.667394: step 55700, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 49h:33m:10s remains)
2017-12-15 20:34:29.141505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7095509 -7.8582 -7.7424822 -7.6882062 -7.4361639 -8.1652536 -9.57344 -10.088959 -9.6436567 -9.1246367 -7.4598703 -6.734601 -5.7395797 -4.1952672 -2.5756326][-5.8832741 -5.2000394 -4.1080122 -5.3364081 -6.3421159 -7.14969 -8.5523415 -9.4120121 -9.301939 -8.5015507 -6.634881 -5.2471304 -3.9713511 -3.4083397 -2.8149872][-3.9409285 -4.1253195 -5.790658 -6.3037896 -6.5754662 -7.3045015 -8.2045088 -8.6133356 -7.7378216 -6.0317335 -4.3073893 -2.0107305 -0.64649582 -0.94295168 -1.912137][-5.6329184 -5.6177516 -6.0621214 -7.0573339 -8.1650066 -7.9940729 -7.3029919 -7.5076413 -7.0740242 -5.5074143 -4.0822582 -3.1111419 -2.722877 -2.2133744 -2.2606807][-6.856431 -8.585351 -10.011612 -9.0227757 -9.0791607 -6.917479 -3.7673471 -4.1326766 -5.5098925 -4.6188612 -3.90723 -4.0869632 -4.7206092 -5.2537355 -6.1612215][-9.2345161 -10.686757 -11.89505 -10.018098 -7.3566055 -3.2038994 0.7156 1.3987193 1.2521062 -0.98841095 -4.5840797 -4.0011053 -3.9046488 -5.1528 -5.9238858][-10.13827 -11.76857 -11.014478 -7.9903717 -5.0142488 0.564198 5.3664136 5.5245032 4.5942321 0.80727768 -4.1921387 -5.4208841 -6.167243 -6.2042561 -6.1050282][-13.015579 -12.207106 -10.280442 -6.0179582 -1.8173361 3.031096 7.105125 7.250587 6.1223941 2.0711646 -2.8070009 -4.9425673 -6.8751535 -7.1038575 -6.0762663][-10.110601 -10.079791 -8.8350353 -5.6365929 -2.5013475 1.971539 5.6122479 4.8005853 3.173183 -0.53386688 -4.2863789 -6.7405939 -9.203167 -8.95919 -7.36328][-8.8448019 -7.71399 -6.334219 -4.402267 -3.2243662 -0.47568226 2.7857261 2.7214198 0.83985519 -3.1351078 -6.9718595 -8.77951 -10.365224 -10.809454 -10.603722][-12.26372 -10.350266 -8.5631065 -6.1257987 -5.4958649 -4.5907512 -2.450407 -2.0558586 -2.6743543 -5.3712258 -8.32045 -10.891123 -11.448095 -11.061408 -9.6356373][-15.963358 -12.902286 -10.3531 -8.1731625 -7.3909717 -7.1574564 -6.9595761 -6.4390211 -6.5479097 -8.3275089 -9.8006783 -11.244967 -11.916312 -11.447347 -9.5883636][-13.902918 -12.13171 -9.5013752 -7.7427473 -7.0321369 -6.7042322 -6.1446218 -6.65541 -7.5280237 -8.4600124 -9.3189917 -10.161226 -10.669405 -10.093056 -8.660037][-12.418196 -10.20936 -8.8719015 -6.9085054 -5.1266866 -5.9283466 -6.487092 -6.4688716 -7.1571312 -8.2508183 -9.1370478 -9.2769089 -9.133913 -7.5442376 -6.2059312][-9.7413769 -7.4708128 -5.2373772 -4.741437 -3.8500838 -4.0506296 -4.9458723 -5.3885903 -6.46407 -6.7792549 -6.5336256 -6.8861 -7.4042845 -7.6948423 -7.6102142]]...]
INFO - root - 2017-12-15 20:34:35.709309: step 55710, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.673 sec/batch; 51h:42m:41s remains)
INFO - root - 2017-12-15 20:34:42.352494: step 55720, loss = 0.22, batch loss = 0.17 (11.7 examples/sec; 0.683 sec/batch; 52h:29m:14s remains)
INFO - root - 2017-12-15 20:34:49.024130: step 55730, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 51h:09m:43s remains)
INFO - root - 2017-12-15 20:34:55.727224: step 55740, loss = 0.12, batch loss = 0.08 (11.0 examples/sec; 0.724 sec/batch; 55h:40m:30s remains)
INFO - root - 2017-12-15 20:35:02.339031: step 55750, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 50h:55m:30s remains)
INFO - root - 2017-12-15 20:35:08.995370: step 55760, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 51h:35m:51s remains)
INFO - root - 2017-12-15 20:35:15.591806: step 55770, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 49h:46m:50s remains)
INFO - root - 2017-12-15 20:35:22.176979: step 55780, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 50h:46m:28s remains)
INFO - root - 2017-12-15 20:35:28.838743: step 55790, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.675 sec/batch; 51h:54m:55s remains)
INFO - root - 2017-12-15 20:35:35.505693: step 55800, loss = 0.16, batch loss = 0.11 (11.5 examples/sec; 0.698 sec/batch; 53h:39m:05s remains)
2017-12-15 20:35:36.034648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.945734 -6.4067841 -5.1400833 -4.1535754 -3.8599854 -4.3176975 -3.9050126 -4.4929562 -4.0173149 -4.1076908 -3.6146507 -5.5506268 -7.6581597 -7.7985945 -8.6763792][-6.7366972 -6.673595 -5.1580238 -3.915056 -4.9456086 -4.3300843 -3.9573915 -3.5147371 -2.8145144 -2.2993207 -1.6578927 -4.0508337 -6.1256618 -6.480166 -8.0632067][-4.7649088 -4.6631732 -4.2189331 -4.5804954 -5.1443648 -5.1296654 -4.5379224 -3.1221838 -2.5216129 -2.1736355 -1.7029095 -3.6848898 -5.8345027 -5.4987221 -7.1320076][-4.8680749 -4.4693675 -3.3648028 -3.6246817 -4.7846456 -4.6610909 -4.53079 -3.6450331 -2.6350188 -1.964407 -2.0220079 -3.864572 -5.8036513 -6.0089636 -7.6388168][-5.28402 -5.9141111 -5.8322229 -4.594049 -4.1045055 -4.1884851 -3.0391052 -2.4310467 -1.9805322 -1.7063255 -1.7037697 -2.9312177 -4.7273135 -5.5152588 -7.2112226][-5.9008102 -5.9914527 -5.1036978 -3.7765217 -3.5096605 -2.39368 -1.1724277 -0.52286911 -0.25282574 -0.045719147 -0.56399012 -2.7386093 -5.084394 -5.3973379 -6.9396267][-6.7807717 -6.3294458 -4.7798 -2.4279289 -1.6093268 -0.13284636 0.78799725 1.440701 1.8269634 0.78869057 -0.19124794 -2.0505075 -4.3174057 -4.5323815 -6.57287][-5.9896398 -4.7699323 -3.2333846 -0.35661411 0.17784071 1.5055461 2.0992017 2.8548512 2.6036086 1.6276608 0.7556 -2.511245 -4.5903354 -4.0518131 -5.5589495][-4.5313883 -3.0459812 -1.0906138 0.76023865 0.1699338 0.67967892 1.3562427 1.932817 1.4654269 0.65460396 -0.41390467 -3.3509588 -5.8148355 -5.4493661 -4.9451671][-3.2373083 -2.8968732 -1.2036018 0.28500795 0.17166185 -1.0826974 -0.53290176 0.45194292 0.50334167 0.41797209 -0.56230736 -3.5634766 -6.3318944 -5.954319 -5.9741564][-6.1359048 -6.1225843 -5.0438256 -4.1381817 -3.9620252 -3.0762579 -1.5149655 -2.0306454 -1.5807419 -1.6723294 -2.549932 -5.6350422 -7.8842406 -6.8880367 -6.7833214][-9.9385529 -9.2997036 -8.8831711 -8.4670086 -8.2117329 -6.5356345 -4.863719 -4.6418643 -4.3298378 -4.4307418 -4.6126385 -6.5776911 -7.445343 -6.6574249 -6.530664][-11.735433 -11.094212 -10.078196 -8.8449793 -7.9917517 -6.4832115 -5.7549906 -5.6473331 -5.8314657 -6.2992826 -6.09274 -6.9751987 -6.5945196 -5.8720016 -5.2816553][-10.843716 -9.2894545 -8.4309177 -7.2736359 -5.8858328 -5.2031288 -5.1560965 -4.7027817 -5.300703 -5.938664 -6.55493 -6.3531642 -4.9823256 -3.7952833 -3.0589457][-8.61483 -7.33368 -6.20445 -5.3182206 -4.7679577 -3.2328222 -2.8035083 -3.4786682 -4.5896244 -4.4045334 -5.2584295 -6.4699554 -7.1407528 -6.9000015 -6.4135728]]...]
INFO - root - 2017-12-15 20:35:42.697217: step 55810, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 50h:49m:49s remains)
INFO - root - 2017-12-15 20:35:49.353421: step 55820, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 50h:51m:48s remains)
INFO - root - 2017-12-15 20:35:56.018555: step 55830, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 49h:36m:03s remains)
INFO - root - 2017-12-15 20:36:02.556593: step 55840, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 51h:32m:01s remains)
INFO - root - 2017-12-15 20:36:09.293965: step 55850, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 50h:17m:00s remains)
INFO - root - 2017-12-15 20:36:15.865477: step 55860, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 50h:14m:47s remains)
INFO - root - 2017-12-15 20:36:22.466012: step 55870, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 50h:26m:22s remains)
INFO - root - 2017-12-15 20:36:29.133258: step 55880, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.686 sec/batch; 52h:42m:12s remains)
INFO - root - 2017-12-15 20:36:35.656506: step 55890, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 51h:45m:30s remains)
INFO - root - 2017-12-15 20:36:42.273657: step 55900, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 50h:13m:57s remains)
2017-12-15 20:36:42.824650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3496547 -3.4402618 -3.4552088 -4.0576029 -5.7910175 -7.5932121 -8.9829035 -9.4717035 -8.3794012 -6.7949233 -5.9993753 -4.9364381 -6.1414981 -8.0783272 -6.876008][-3.2968671 -2.3622057 -2.1184862 -2.5706604 -4.0226889 -5.7248082 -7.8354349 -8.3646069 -8.5583248 -8.0336514 -6.5266495 -5.694922 -7.1181879 -8.8630972 -7.2736521][-4.642271 -2.4619906 -1.9901466 -1.1777887 -2.2180345 -3.4633169 -4.7538805 -5.2573438 -5.2402797 -5.9428525 -5.8949966 -4.4647288 -6.219213 -8.38705 -6.527863][-5.4560122 -4.9007959 -3.9143069 -1.5512753 -1.3077936 -1.4182177 -2.3361838 -2.7640028 -3.5451477 -4.7684879 -5.5305824 -6.0392108 -7.9984293 -8.54887 -6.8596697][-5.7806821 -5.7344623 -4.4199624 -2.7423446 -1.0563278 0.36539698 0.28998661 -1.132524 -3.2086291 -4.3937449 -5.6725922 -7.2785883 -9.5269661 -11.025143 -9.100317][-7.1493311 -7.2086244 -5.9992914 -3.6341417 -1.7816329 1.1742454 2.6773543 1.1930709 -1.460834 -3.9660263 -6.8499632 -7.71177 -9.0761366 -10.939087 -9.1911879][-7.5222459 -8.5957909 -7.3784666 -3.7742124 -1.5375137 1.1862893 3.348762 3.4182887 2.7135968 -1.8836184 -5.4862719 -5.6883869 -7.4290886 -9.6970739 -8.014492][-8.1447229 -7.1109562 -4.9952178 -1.3538942 1.3141327 3.2755151 4.465363 4.8475728 5.247457 1.8218293 -2.1069963 -3.4059193 -5.1048136 -7.3818207 -6.3926687][-7.7235823 -7.0537663 -4.5929928 -1.2016625 0.77575445 3.6127181 4.85064 4.6144719 3.9224687 2.528439 1.0019064 -0.62577009 -3.2833121 -6.390852 -5.609808][-8.4698257 -6.9177442 -4.3348441 -1.0372524 0.54669285 2.7921367 3.3289952 2.3576713 1.7416067 1.3554854 1.2716055 0.44277239 -2.4985702 -6.3149562 -6.4212751][-12.593494 -10.04261 -6.6629052 -4.7609649 -2.7132349 -1.6692462 -0.65850449 -0.44312 -1.0224791 -2.1900008 -2.7982669 -2.3632293 -3.6655943 -5.795455 -5.4663525][-15.317301 -12.991493 -10.704964 -6.970747 -5.0757942 -6.2530165 -7.6237984 -7.0917058 -5.5275621 -4.1356306 -3.8503532 -4.3609753 -5.3407249 -7.275063 -7.8104439][-13.571621 -12.834723 -11.575562 -9.0194769 -8.0428934 -7.0421963 -7.3892989 -8.4596605 -8.2797623 -7.6329179 -6.8838425 -6.3072162 -6.2639604 -6.7061596 -6.1405807][-10.382233 -9.9884109 -9.3228664 -7.6468811 -8.41763 -9.216958 -10.429718 -10.278299 -9.730238 -8.5003 -7.7460904 -7.0016532 -6.1003671 -7.2522221 -6.7252216][-5.5658593 -5.5725718 -4.8669252 -6.1706328 -7.7295709 -7.5792851 -7.39627 -6.7674575 -6.7733173 -7.8409042 -7.071455 -6.3791208 -7.0776796 -7.7592778 -7.6411443]]...]
INFO - root - 2017-12-15 20:36:49.399254: step 55910, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 49h:10m:59s remains)
INFO - root - 2017-12-15 20:36:55.977282: step 55920, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 50h:55m:13s remains)
INFO - root - 2017-12-15 20:37:02.568131: step 55930, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 49h:53m:15s remains)
INFO - root - 2017-12-15 20:37:09.169568: step 55940, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 50h:18m:22s remains)
INFO - root - 2017-12-15 20:37:15.701059: step 55950, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 50h:19m:36s remains)
INFO - root - 2017-12-15 20:37:22.194194: step 55960, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 49h:52m:15s remains)
INFO - root - 2017-12-15 20:37:28.680825: step 55970, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 50h:56m:09s remains)
INFO - root - 2017-12-15 20:37:35.276617: step 55980, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.654 sec/batch; 50h:15m:56s remains)
INFO - root - 2017-12-15 20:37:41.919950: step 55990, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 51h:03m:42s remains)
INFO - root - 2017-12-15 20:37:48.440831: step 56000, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 50h:03m:08s remains)
2017-12-15 20:37:48.954910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.491715 -5.902585 -6.2107778 -6.4532061 -8.5106039 -9.5017662 -9.3357782 -9.1797676 -9.1355648 -9.3761578 -9.2426729 -8.9736624 -8.5531139 -10.262169 -9.4192162][-7.0500579 -7.4039969 -8.2906408 -7.9849339 -8.4604588 -9.4171705 -9.728014 -10.210796 -10.766847 -11.089475 -11.306499 -11.398573 -10.283377 -12.473351 -11.714485][-5.7621193 -6.945159 -8.1506348 -8.6307573 -9.071104 -9.0591717 -8.13024 -8.7922039 -10.134472 -11.387439 -12.222158 -11.661827 -10.652472 -12.710112 -12.454735][-5.5554795 -5.8237119 -6.7205935 -7.7345033 -8.635911 -8.0052967 -5.6073666 -6.1817751 -8.3048592 -10.123285 -10.826296 -10.49373 -10.052319 -11.819067 -11.954543][-6.5305371 -6.4248571 -6.6915736 -6.52888 -5.9229922 -3.9280968 -1.0686159 -3.0370624 -6.71924 -8.4188042 -9.4017029 -8.8648291 -8.060956 -10.831227 -10.953772][-8.468832 -7.1247549 -6.2235103 -6.1491947 -5.3426495 -0.25118065 5.5424075 4.4731679 0.25148869 -4.5066752 -8.6103153 -7.3500147 -5.8209233 -9.0788383 -9.72664][-9.2363625 -8.1003485 -8.1753778 -6.715929 -5.4468174 -0.7616148 6.4921775 9.2540321 8.5639172 0.39835548 -7.6113772 -7.0716653 -6.2224903 -8.8158054 -8.430027][-10.10415 -9.0273266 -8.8553362 -6.1130886 -3.6551147 -0.70383072 3.6697087 6.3955169 9.3794689 5.0196633 -1.8794599 -5.8271646 -8.2207308 -10.510241 -9.8912163][-8.3590193 -8.18998 -8.8023338 -7.1334152 -4.7597809 0.22765017 3.1344876 3.3248019 6.8389258 3.5316367 -0.04504776 -3.4792731 -7.9424992 -13.50514 -13.976963][-5.3936949 -5.5074162 -7.6783433 -7.6968346 -7.5552831 -3.1612387 0.63092709 1.3966327 3.0927844 -0.013246536 -3.0124733 -4.7337866 -7.2059693 -12.842682 -15.733595][-9.4670229 -7.614974 -7.7551022 -8.1601772 -8.9974079 -8.7063255 -7.8488646 -6.4048419 -4.9895759 -6.113163 -7.6311326 -8.8456421 -8.9177208 -12.218734 -14.193291][-14.938585 -12.472212 -11.095063 -10.61236 -10.822575 -11.244423 -12.414532 -11.786636 -10.11099 -10.195349 -10.565327 -10.55679 -9.8321648 -11.437652 -11.614777][-12.647821 -13.330669 -12.445911 -12.299791 -12.341022 -11.628799 -11.630643 -12.337234 -11.814356 -10.50222 -9.5278082 -10.999578 -10.624904 -10.48908 -8.40114][-10.822187 -9.1893044 -9.148715 -10.010834 -9.2994375 -9.1012955 -9.7066278 -9.8466969 -10.15938 -10.597479 -10.092427 -9.5648174 -8.97416 -8.6818514 -7.2698812][-7.6333532 -6.6865878 -5.45132 -4.1971989 -3.6732433 -4.2606831 -5.186667 -5.3193283 -5.7862291 -6.168282 -6.6539726 -7.4159255 -8.7360306 -7.92537 -7.8267827]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 20:37:55.570075: step 56010, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 49h:34m:47s remains)
INFO - root - 2017-12-15 20:38:02.191953: step 56020, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 51h:48m:23s remains)
INFO - root - 2017-12-15 20:38:08.803368: step 56030, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 50h:29m:33s remains)
INFO - root - 2017-12-15 20:38:15.381340: step 56040, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 51h:11m:04s remains)
INFO - root - 2017-12-15 20:38:21.809778: step 56050, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 49h:49m:11s remains)
INFO - root - 2017-12-15 20:38:28.436148: step 56060, loss = 0.13, batch loss = 0.08 (11.4 examples/sec; 0.702 sec/batch; 53h:55m:58s remains)
INFO - root - 2017-12-15 20:38:35.059514: step 56070, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 49h:32m:52s remains)
INFO - root - 2017-12-15 20:38:41.592341: step 56080, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 50h:42m:28s remains)
INFO - root - 2017-12-15 20:38:48.088257: step 56090, loss = 0.19, batch loss = 0.15 (12.5 examples/sec; 0.640 sec/batch; 49h:08m:29s remains)
INFO - root - 2017-12-15 20:38:54.660890: step 56100, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 50h:11m:51s remains)
2017-12-15 20:38:55.142899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.16152 -4.8484488 -4.65448 -3.828948 -4.485805 -4.6327572 -4.274148 -3.4261677 -2.5067234 -2.0099828 -1.5964665 -3.217504 -4.4484606 -5.657937 -4.5295897][-3.2036431 -3.2244089 -3.7700241 -3.5720193 -3.7091227 -3.4481161 -3.1323009 -2.4299369 -1.76649 -1.0145707 -0.035686016 -0.93783951 -1.6597896 -3.5248005 -3.6401467][-2.9939461 -3.6582289 -4.3346152 -3.7048335 -4.2957172 -4.3800726 -3.3139637 -1.9014809 -1.3530993 -0.96656418 -0.18331099 -1.2841253 -1.8857467 -2.9252353 -2.3619664][-4.9636974 -5.1682124 -5.2300005 -4.4752703 -4.7772059 -4.263196 -3.1183908 -1.8299217 -1.3082643 -1.2264447 -0.58025932 -1.4639349 -1.7683432 -2.8504355 -1.9073684][-6.4225106 -6.5558133 -6.2129416 -4.7899675 -4.3104362 -2.8823957 -1.5458951 -1.0239277 -1.1830544 -1.6996584 -1.0309992 -2.2458129 -2.5565262 -3.4339244 -2.9501112][-6.9450274 -6.99006 -6.6425772 -3.8945737 -1.970166 0.35243177 1.4002037 1.2901583 0.63827085 -0.5963335 -0.84122133 -2.0191522 -2.5932484 -3.8449411 -3.5686653][-7.994554 -7.2479091 -5.9521461 -2.4396975 -0.14464617 2.8214421 4.1087508 3.929234 3.2832084 1.3454757 -0.041920185 -2.6710427 -4.20307 -5.8835654 -5.5703182][-8.2369413 -6.7174268 -4.7707729 -1.7621541 0.54131556 4.218986 6.1915412 5.0670323 3.4897513 1.5372777 0.10950613 -2.9339886 -4.9364223 -6.5366364 -6.0793195][-8.0519238 -6.9062409 -5.0552621 -2.1210039 -0.11038065 2.3077312 3.5938287 3.7185988 3.3411145 2.0244422 0.6486659 -3.2671857 -5.382401 -6.7656865 -6.2568159][-7.1748276 -6.256835 -5.5121212 -2.8662786 -1.573956 -0.32466316 0.16362715 0.97613144 1.1767287 -0.59605932 -2.0531125 -4.4677715 -6.0705013 -7.4665523 -7.0359964][-11.537256 -11.178038 -9.4603672 -6.5247254 -5.1784511 -4.3531485 -4.4389429 -4.4864244 -4.3472428 -4.7701893 -5.6719089 -8.5875759 -10.125465 -10.068954 -7.9936113][-14.4505 -13.468512 -12.199681 -9.7639475 -8.0530205 -6.7718353 -6.42939 -7.4763451 -8.6882448 -9.1628571 -9.2131157 -10.271632 -10.496328 -10.849248 -9.3722734][-14.057043 -12.58139 -11.058192 -9.8626328 -9.8500338 -8.9712181 -8.6528168 -8.6608858 -9.1179228 -9.657835 -10.03529 -10.535742 -10.250822 -9.4986858 -7.6840129][-13.041529 -11.463592 -10.291765 -9.062252 -8.1666784 -7.7428565 -8.3006992 -7.741992 -7.3801928 -7.6767516 -8.0048733 -8.4721832 -8.3462629 -8.2769651 -6.8495131][-9.0141106 -8.3550968 -7.8474078 -7.5071363 -7.5768709 -7.1875625 -6.7301912 -6.0852008 -6.56353 -6.5937471 -6.2474737 -7.4397526 -8.2769356 -8.3796329 -7.7666855]]...]
INFO - root - 2017-12-15 20:39:01.732581: step 56110, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 49h:48m:17s remains)
INFO - root - 2017-12-15 20:39:08.434359: step 56120, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.689 sec/batch; 52h:54m:57s remains)
INFO - root - 2017-12-15 20:39:14.992728: step 56130, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 50h:39m:51s remains)
INFO - root - 2017-12-15 20:39:21.631749: step 56140, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 51h:48m:59s remains)
INFO - root - 2017-12-15 20:39:28.311464: step 56150, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 50h:35m:27s remains)
INFO - root - 2017-12-15 20:39:34.969019: step 56160, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 51h:40m:05s remains)
INFO - root - 2017-12-15 20:39:41.626223: step 56170, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.670 sec/batch; 51h:23m:44s remains)
INFO - root - 2017-12-15 20:39:48.351193: step 56180, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 50h:52m:22s remains)
INFO - root - 2017-12-15 20:39:54.942250: step 56190, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 50h:49m:54s remains)
INFO - root - 2017-12-15 20:40:01.542514: step 56200, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 50h:07m:25s remains)
2017-12-15 20:40:02.152384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2707725 -7.6226006 -7.4735847 -7.1367369 -7.4496841 -7.4976964 -7.6435275 -6.5743 -5.7250743 -5.1955361 -4.4372544 -5.706183 -5.7450061 -6.99341 -6.060699][-5.8255653 -5.6384721 -5.5153728 -5.7290964 -5.9182768 -5.700984 -5.889431 -5.5443325 -5.2565317 -4.7124 -4.26755 -6.2846589 -6.0866714 -7.354691 -7.225203][-3.8573456 -3.245537 -3.5337267 -3.5591786 -4.754355 -4.6800289 -4.4634647 -4.0932012 -4.0928144 -3.8443928 -3.8562074 -5.9910612 -6.1039176 -8.0805626 -8.4479][-4.540976 -3.7413716 -3.3305657 -3.3445671 -4.706862 -4.8042908 -4.3633041 -4.0774097 -3.7525587 -4.1963849 -4.4685655 -6.6378584 -6.8208523 -8.8319855 -9.5030422][-6.0461421 -5.5360036 -5.7117949 -4.8886919 -4.7255468 -3.7443175 -1.7546856 -1.4395075 -2.9147263 -3.1927829 -3.507555 -5.923975 -6.1634946 -8.5092239 -8.9947462][-6.3184838 -4.42315 -3.6940906 -3.2599938 -2.0843453 0.054373264 1.9196887 2.4600644 1.5207086 -0.36582565 -2.4659216 -4.9736323 -4.4050722 -6.7428489 -7.6671953][-6.9780951 -5.2942858 -3.5231223 -0.20364666 0.8155756 2.3804383 5.1030993 5.0950208 4.3851 1.7921772 -1.9056742 -4.3906922 -5.1795049 -7.6390815 -7.9281831][-7.5273542 -4.7712054 -2.9003632 -0.82789469 -0.0956521 2.214613 5.3065639 5.3480487 4.697474 2.6778769 -0.074981213 -4.3680692 -6.7605219 -9.01818 -9.1512508][-6.713706 -3.7493868 -2.0625477 -0.59894705 -0.10463715 2.5452704 4.3256879 3.4717555 3.7025828 1.3176708 -1.1853671 -4.8231754 -6.5474052 -8.666997 -9.7333136][-6.3639712 -3.9417038 -2.6467729 -0.910285 0.49000216 1.6960349 2.8786769 3.6082768 3.0510678 -0.16440678 -2.4608181 -5.80494 -7.2636156 -9.84193 -11.019632][-9.1604328 -7.5602312 -7.2627029 -5.1015525 -4.155776 -3.8157349 -3.1314614 -2.1275077 -1.5692024 -3.1707237 -5.0687985 -8.9300566 -10.440578 -11.928347 -12.056978][-12.815966 -11.181171 -10.179922 -8.7550564 -8.315135 -7.8829756 -7.9393477 -8.406065 -8.5278854 -8.7603245 -8.5855389 -10.695879 -11.305333 -13.175293 -13.030252][-14.587589 -13.305387 -12.847905 -11.355721 -11.090673 -9.6404486 -8.8112717 -9.3564291 -10.355919 -10.731218 -10.164482 -10.446327 -9.3908558 -9.407 -8.2606592][-13.132372 -12.044004 -11.756064 -10.611547 -8.652216 -8.13217 -8.6089582 -7.8807316 -7.4342966 -7.7349977 -7.7302551 -7.213088 -6.7796745 -7.4295111 -6.7533693][-9.6949263 -9.6089411 -8.4059315 -6.9029403 -6.7701979 -6.4793057 -5.4163556 -5.6317153 -6.1536579 -5.7531123 -6.0488715 -6.5315132 -7.1250958 -6.0581589 -5.8259668]]...]
INFO - root - 2017-12-15 20:40:08.708292: step 56210, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 50h:12m:06s remains)
INFO - root - 2017-12-15 20:40:15.320054: step 56220, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 51h:44m:00s remains)
INFO - root - 2017-12-15 20:40:21.919236: step 56230, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 51h:52m:42s remains)
INFO - root - 2017-12-15 20:40:28.612115: step 56240, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 50h:22m:17s remains)
INFO - root - 2017-12-15 20:40:35.232492: step 56250, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 52h:27m:15s remains)
INFO - root - 2017-12-15 20:40:41.827003: step 56260, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.691 sec/batch; 53h:01m:45s remains)
INFO - root - 2017-12-15 20:40:48.478962: step 56270, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 51h:12m:40s remains)
INFO - root - 2017-12-15 20:40:55.015886: step 56280, loss = 0.20, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 48h:56m:44s remains)
INFO - root - 2017-12-15 20:41:01.590409: step 56290, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 48h:56m:42s remains)
INFO - root - 2017-12-15 20:41:08.195530: step 56300, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 52h:34m:08s remains)
2017-12-15 20:41:08.726878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.87403 -5.4940577 -6.2047682 -5.9057765 -6.2049646 -5.9595885 -5.10473 -3.6366448 -2.4345241 -1.8467767 -1.174933 -4.396883 -6.2206779 -7.3262949 -7.3206944][-4.5968194 -4.829813 -5.1057634 -5.3309727 -5.4879255 -5.2519355 -4.4893131 -3.14616 -1.7292595 -0.39720726 0.13466597 -3.9929154 -6.0674353 -7.7541475 -8.2414732][-2.6742263 -3.6706765 -4.8482752 -4.0859275 -4.4997029 -4.4000092 -3.5857806 -2.6948583 -1.560288 -1.068512 -0.98672819 -4.3976612 -5.8444386 -7.4198914 -8.433691][-3.3227098 -4.5032711 -5.3199539 -4.2176805 -4.5881786 -3.8854756 -2.7766371 -2.3927522 -1.9817994 -1.117826 -0.8331027 -4.7407675 -6.3872185 -7.6898546 -7.9398241][-3.5515776 -6.0178361 -7.3617363 -5.44137 -5.0429726 -3.1599002 -1.3669868 -1.940299 -2.3375814 -1.6293125 -1.3644409 -4.44167 -5.8593779 -7.1414886 -7.6110134][-5.1278229 -6.7707319 -6.7487063 -4.1531959 -2.4984753 0.44064188 2.7572951 1.63026 0.46323109 -0.5152936 -1.9102571 -4.5088968 -5.4958696 -6.6135488 -7.2958689][-7.0287266 -6.8982763 -5.3287191 -2.171644 -0.30879736 2.7077537 5.4712205 4.9386477 3.7154918 0.90219688 -1.5515776 -4.8511763 -6.2814474 -7.2495856 -7.9057608][-9.0232744 -8.602725 -6.8808274 -2.4189997 0.89471436 4.5705323 7.1657281 6.1906133 4.9409137 2.7089734 0.26425171 -4.558959 -6.7245383 -8.1513643 -9.2507849][-8.1178989 -7.2335591 -6.0005908 -2.918644 -0.48541069 2.4867015 5.0096755 4.6172214 3.7190089 1.7353716 -0.062896252 -5.1171126 -7.0986385 -8.4792395 -9.33106][-7.8592558 -7.3542585 -6.2583079 -3.2298498 -1.5540562 0.47635889 2.4003444 2.4453568 2.0252347 -0.097246647 -2.3338017 -6.5921268 -7.9885397 -9.30755 -10.246151][-10.839834 -10.370795 -9.2044983 -6.1465 -5.2361913 -3.9862485 -2.638211 -2.1879814 -1.9279118 -2.7289119 -4.1467342 -8.7978029 -10.680692 -11.004303 -10.659706][-13.206705 -13.503208 -12.683807 -10.620935 -9.9926357 -8.545084 -7.5693521 -7.42009 -6.9221563 -7.1536803 -7.5138512 -9.96386 -10.879145 -11.545595 -11.607331][-14.435808 -14.020174 -12.787884 -11.609011 -12.137395 -10.671412 -9.9037342 -10.242489 -10.348185 -9.8120861 -9.7788935 -10.82103 -11.091694 -10.857321 -10.373093][-12.210136 -11.720871 -11.250975 -9.8442822 -9.4958611 -9.3211384 -9.0238132 -9.0925493 -9.1572895 -9.6156635 -10.003787 -9.8209257 -9.2332087 -9.24038 -9.2557669][-8.3292007 -8.35272 -8.2110138 -5.9609818 -5.2442908 -5.359344 -5.1320176 -5.6589031 -6.2685184 -6.7075529 -7.2527328 -8.1242981 -8.538929 -8.6683884 -9.0534344]]...]
INFO - root - 2017-12-15 20:41:15.385472: step 56310, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 50h:31m:34s remains)
INFO - root - 2017-12-15 20:41:22.028666: step 56320, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 49h:51m:32s remains)
INFO - root - 2017-12-15 20:41:28.611424: step 56330, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 50h:41m:09s remains)
INFO - root - 2017-12-15 20:41:35.208900: step 56340, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 49h:24m:25s remains)
INFO - root - 2017-12-15 20:41:41.790255: step 56350, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 49h:50m:44s remains)
INFO - root - 2017-12-15 20:41:48.359104: step 56360, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.663 sec/batch; 50h:49m:23s remains)
INFO - root - 2017-12-15 20:41:54.944342: step 56370, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 50h:27m:08s remains)
INFO - root - 2017-12-15 20:42:01.518487: step 56380, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 50h:38m:47s remains)
INFO - root - 2017-12-15 20:42:08.083612: step 56390, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 49h:35m:51s remains)
INFO - root - 2017-12-15 20:42:14.659314: step 56400, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 50h:28m:01s remains)
2017-12-15 20:42:15.210612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3295331 -2.5837374 -2.2341354 -2.7986498 -4.0717068 -5.2864175 -6.17056 -4.8641195 -3.3488669 -3.64764 -4.3539939 -6.3983655 -8.6292267 -8.7854328 -7.5477886][-3.4824049 -2.8313479 -2.332818 -2.1193342 -2.4281838 -3.4380736 -2.7628429 -2.0681891 -1.249651 -1.7170386 -2.2321167 -4.3242822 -7.2178397 -7.8925076 -7.7139812][-2.741035 -3.4966671 -3.885242 -3.3294702 -4.1466107 -3.8246889 -3.1450365 -1.8909631 -0.766377 -1.5268073 -1.5151248 -3.3859096 -5.7620397 -6.4424272 -5.9186311][-4.6227255 -4.9732985 -5.2729893 -4.9980631 -5.5004096 -4.78553 -3.458395 -2.0129547 -0.39109278 -0.063593864 -0.353786 -2.7039807 -5.6471128 -6.5688324 -5.4536667][-2.8479657 -4.2132339 -4.7715611 -4.9687405 -4.9040766 -3.2974136 -1.8094258 -0.8087225 0.44087744 1.3397632 2.0994673 -0.52705717 -3.9676385 -5.7314425 -6.458499][-3.7198744 -3.443702 -2.8912196 -2.1234963 -1.2729621 0.14357996 1.6362457 2.6652408 2.2333269 2.1254463 1.7271266 -0.7814126 -3.8708563 -5.1510377 -5.2553515][-3.2057812 -2.4634032 -1.6884179 -0.18039989 1.8105845 3.7847123 5.4308782 5.2080836 5.23914 3.8223624 1.8945365 -0.64900351 -2.9544215 -5.1190696 -5.9242859][-2.7344582 -2.4408557 -2.2257607 1.2118559 3.5179076 5.1050811 7.4617553 6.2697024 4.7669797 3.6017766 2.4403138 -0.41722822 -2.898303 -4.27549 -4.6181946][-3.9427028 -2.5146563 -1.5162778 0.29241467 1.1461205 3.7857909 5.380033 4.8546357 2.795382 1.4724302 0.69990444 -1.3918748 -3.5774267 -4.6831546 -4.7312403][-4.0643158 -3.4369929 -2.0278151 0.098437786 0.41747808 1.307601 3.346808 2.7201562 1.4389882 0.015127659 -0.47944307 -2.4148066 -4.5422297 -5.4998579 -5.5089827][-7.4432721 -6.2281833 -5.338532 -4.08632 -3.6470931 -3.4185805 -3.4567616 -2.742758 -2.9811664 -3.4543676 -3.104996 -5.1856494 -6.5671453 -7.2807312 -6.0563679][-11.455259 -8.9239445 -7.5925903 -6.5914297 -7.3763933 -7.3510323 -7.4057555 -7.4955463 -7.6721411 -7.1691718 -6.8539805 -7.2903371 -8.3486824 -8.3263912 -6.3989081][-10.52282 -8.9684906 -8.0399742 -6.9948077 -7.6207232 -8.4385214 -8.8706245 -8.6127443 -8.0930328 -7.699831 -6.9714613 -7.1700459 -7.3229017 -6.6015983 -5.4117742][-8.9371414 -7.3925667 -6.2490292 -5.8004756 -5.9523916 -6.9598589 -6.9763293 -6.5996675 -6.4147439 -6.7519493 -6.4133854 -5.2574644 -5.2431407 -4.7840905 -3.26335][-5.9597244 -6.2552705 -6.139317 -5.5165882 -5.4700446 -4.7853727 -4.5172019 -5.2207561 -4.9081335 -4.0809727 -5.1509256 -5.6356797 -5.50717 -5.3839269 -6.1958728]]...]
INFO - root - 2017-12-15 20:42:21.803591: step 56410, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 50h:23m:18s remains)
INFO - root - 2017-12-15 20:42:28.340012: step 56420, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 49h:26m:00s remains)
INFO - root - 2017-12-15 20:42:34.972401: step 56430, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 49h:08m:41s remains)
INFO - root - 2017-12-15 20:42:41.548511: step 56440, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 49h:55m:22s remains)
INFO - root - 2017-12-15 20:42:48.094518: step 56450, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 50h:43m:45s remains)
INFO - root - 2017-12-15 20:42:54.699149: step 56460, loss = 0.32, batch loss = 0.27 (12.4 examples/sec; 0.648 sec/batch; 49h:39m:59s remains)
INFO - root - 2017-12-15 20:43:01.315874: step 56470, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 52h:21m:59s remains)
INFO - root - 2017-12-15 20:43:07.859004: step 56480, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 51h:27m:42s remains)
INFO - root - 2017-12-15 20:43:14.385309: step 56490, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 49h:02m:16s remains)
INFO - root - 2017-12-15 20:43:20.930412: step 56500, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.646 sec/batch; 49h:32m:36s remains)
2017-12-15 20:43:21.457384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2781758 -1.4880996 -0.68506193 -0.529644 -1.0626268 -0.794384 -0.63037491 -0.24447536 -0.87615347 -2.12495 -2.8439775 -4.7591867 -6.9130173 -7.1051331 -5.5266814][-1.9028528 -0.54548836 0.0092573166 0.21684027 -0.095627308 -0.10549593 -0.26842737 -0.63064337 -1.43819 -2.4949157 -3.4835258 -5.5723886 -8.0262394 -9.2892694 -7.6567335][-0.80197763 -0.12780952 -0.24393368 0.32688951 -0.12146235 -0.57311678 -0.92622805 -1.2238874 -1.7854879 -2.6476932 -2.8213549 -4.1805477 -6.3634844 -8.0787077 -7.5318093][-1.6920128 -1.65486 -1.9248669 -1.1275191 -1.1466403 -1.5765495 -1.7786193 -1.9438791 -2.7997439 -3.2693114 -3.0324798 -4.1947861 -6.115304 -7.1913466 -6.2114806][-2.9574685 -3.0895052 -3.7588005 -3.3954577 -3.5352645 -2.2585835 -0.72315693 -0.37917662 -0.91138887 -1.4312 -1.5767255 -2.6818874 -4.9008708 -6.2690415 -6.5721011][-4.1089849 -2.9679687 -2.9101238 -3.1896918 -3.0697863 -1.7085032 0.35654402 1.549027 1.4145617 0.42719173 -0.23453379 -1.2669463 -3.0400348 -5.0990462 -5.1180763][-4.3382549 -2.9171565 -2.6746104 -0.95888233 -1.3710308 -0.46412754 1.6620483 3.2782788 3.4018512 1.7293572 -0.43412256 -1.9209814 -3.498261 -4.4375539 -3.4622254][-4.5671287 -3.1088314 -1.4666295 -0.25115871 -0.50415516 0.78785896 2.2172589 3.7895598 4.5094647 2.5481915 0.817348 -2.1171675 -4.7185411 -5.4177508 -4.7020793][-4.6812296 -3.3114917 -1.8910198 -1.2277756 -1.3400774 -1.0040545 0.052423 1.9164791 3.1748023 2.8121934 1.4277763 -2.1743765 -5.3922462 -7.1554136 -5.9463205][-6.1386619 -4.8678331 -4.7082605 -3.1044221 -2.4540908 -1.4759693 -0.25697708 0.78692436 1.9154701 1.8108921 0.58810091 -2.830024 -6.1773167 -8.1345673 -8.1815338][-8.5907078 -8.5140152 -7.9264727 -5.7558289 -5.0082788 -3.8426747 -2.3436222 -0.29832792 0.20565081 -0.97247648 -1.6228418 -5.0275917 -8.0286932 -9.4273357 -7.7430954][-10.65062 -9.5681038 -9.130024 -8.6235647 -7.4879045 -5.9076509 -4.5093589 -2.7772741 -2.4350505 -2.8740673 -4.2608528 -6.049942 -6.998035 -7.8141613 -7.5489588][-10.360961 -8.5143013 -7.4401593 -7.2513657 -6.6451993 -5.140255 -3.3943617 -3.0116522 -2.882972 -3.4782472 -4.677372 -5.8817453 -7.2696695 -7.6717415 -5.7742639][-6.6902037 -6.2642231 -5.362 -4.4059763 -4.5936003 -4.8286996 -3.7265682 -2.122987 -1.6893501 -2.7497225 -4.0624084 -4.8758163 -4.6115475 -5.8793173 -5.0611811][-6.4902344 -4.7771988 -2.964299 -2.4697897 -2.3105805 -3.0510337 -3.5635743 -2.796258 -2.0540898 -2.765516 -3.2487385 -4.9218674 -6.1474891 -6.2754364 -6.4258909]]...]
INFO - root - 2017-12-15 20:43:28.013207: step 56510, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 50h:46m:15s remains)
INFO - root - 2017-12-15 20:43:34.559277: step 56520, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 50h:26m:14s remains)
INFO - root - 2017-12-15 20:43:41.124986: step 56530, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.633 sec/batch; 48h:31m:14s remains)
INFO - root - 2017-12-15 20:43:47.723104: step 56540, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 49h:55m:56s remains)
INFO - root - 2017-12-15 20:43:54.335849: step 56550, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 50h:29m:51s remains)
INFO - root - 2017-12-15 20:44:00.919983: step 56560, loss = 0.16, batch loss = 0.11 (12.8 examples/sec; 0.627 sec/batch; 48h:03m:22s remains)
INFO - root - 2017-12-15 20:44:07.527304: step 56570, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 50h:02m:10s remains)
INFO - root - 2017-12-15 20:44:14.107995: step 56580, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 49h:47m:33s remains)
INFO - root - 2017-12-15 20:44:20.686972: step 56590, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 52h:30m:04s remains)
INFO - root - 2017-12-15 20:44:27.258350: step 56600, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 50h:10m:11s remains)
2017-12-15 20:44:27.800110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.462863 -3.297703 -2.539989 -1.4775476 -1.8283095 -2.7032397 -3.9041748 -5.0342402 -5.1374269 -3.2304864 -1.0522285 -2.3476744 -3.1973629 -4.409708 -6.3889][-2.1412704 -1.7560399 -1.285347 -0.67957115 -0.5061121 -2.1475661 -4.3573127 -4.6629705 -4.770956 -4.9424639 -3.601552 -4.5115371 -5.5701585 -5.6218119 -6.6578641][-1.338151 -1.9225881 -2.1991446 -2.4091065 -2.9273398 -2.9081836 -3.5302317 -4.0105696 -3.9741373 -3.8184223 -3.2467546 -5.6741738 -7.37097 -7.3968844 -7.8469853][-3.5360432 -2.8574927 -3.0522997 -3.5384519 -3.7513204 -3.9152956 -4.2066135 -3.3701084 -2.9765315 -3.4056864 -3.8453023 -6.0946183 -7.5051312 -8.2142582 -8.8048153][-3.632365 -3.9890988 -3.7801895 -3.3253305 -3.6671958 -3.3821781 -2.9361076 -2.8265045 -2.7791257 -2.4884369 -2.2762783 -5.50127 -8.0443611 -9.2797976 -9.8429165][-2.9156868 -3.6000087 -3.230618 -2.7859685 -1.3644915 0.16139412 0.73327255 0.73060131 0.10538912 -1.214817 -2.6088696 -5.1865635 -6.83765 -7.5848885 -8.6848068][-4.0100994 -3.4249768 -3.1672463 -1.2384601 0.21261168 1.8656254 3.421433 3.6034093 3.2962832 0.84872675 -1.3252454 -3.9839635 -6.0545421 -6.9590011 -7.1421366][-4.9289508 -3.826519 -2.6209571 -1.1133718 0.22467995 2.6583219 4.4961848 4.3687892 3.2873883 1.2545805 -0.93616247 -4.7026157 -6.7105083 -6.7482996 -7.095716][-4.7561641 -3.6339536 -2.0348907 -1.3579359 -1.0582995 0.6725111 1.6456242 1.4377732 1.0253057 -0.54483366 -1.7520826 -4.4048 -6.7446246 -7.0881796 -6.3135748][-4.521801 -4.0009036 -3.2686734 -2.4509504 -1.7571781 -1.8525403 -2.2574568 -1.8064692 -1.2451639 -1.7047276 -1.7094092 -4.2504454 -6.1842294 -6.0399723 -6.1087146][-8.5700388 -7.8961353 -6.7778668 -5.501102 -4.7239065 -5.1008267 -5.4831266 -6.4097676 -6.5077977 -5.1584454 -3.8545418 -4.8414636 -5.8039165 -4.4720654 -3.3575671][-11.986461 -11.005742 -10.047163 -9.1404552 -8.9003124 -8.4966354 -8.2698021 -8.6657019 -8.3123131 -7.7010942 -6.8407917 -6.3911762 -5.9820361 -4.2477193 -2.7595379][-12.006371 -11.057434 -10.277781 -10.009733 -10.20874 -9.909503 -9.663887 -10.116447 -9.9603062 -9.5482244 -8.7594128 -8.715766 -7.9948969 -5.0434537 -2.2909255][-8.970808 -9.0524473 -8.5310307 -8.3949232 -8.4580994 -8.3565331 -8.5848513 -8.4765234 -7.801342 -8.5071268 -8.2638121 -8.2270641 -7.1780272 -6.1043663 -5.7807703][-6.3550811 -6.505775 -6.8230639 -5.5860415 -5.0762186 -5.1105433 -4.7179027 -5.3496275 -6.0381126 -6.4586611 -7.0013156 -8.0723782 -8.6986046 -8.46958 -8.0097322]]...]
INFO - root - 2017-12-15 20:44:34.404541: step 56610, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 49h:19m:26s remains)
INFO - root - 2017-12-15 20:44:40.930742: step 56620, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 50h:25m:46s remains)
INFO - root - 2017-12-15 20:44:47.543652: step 56630, loss = 0.13, batch loss = 0.08 (11.4 examples/sec; 0.699 sec/batch; 53h:35m:04s remains)
INFO - root - 2017-12-15 20:44:54.262082: step 56640, loss = 0.29, batch loss = 0.24 (11.8 examples/sec; 0.678 sec/batch; 51h:55m:27s remains)
INFO - root - 2017-12-15 20:45:00.877377: step 56650, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 50h:41m:29s remains)
INFO - root - 2017-12-15 20:45:07.475779: step 56660, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 50h:30m:55s remains)
INFO - root - 2017-12-15 20:45:14.044343: step 56670, loss = 0.20, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 49h:35m:24s remains)
INFO - root - 2017-12-15 20:45:20.668623: step 56680, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 51h:34m:07s remains)
INFO - root - 2017-12-15 20:45:27.233110: step 56690, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 49h:48m:18s remains)
INFO - root - 2017-12-15 20:45:33.802535: step 56700, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 50h:46m:34s remains)
2017-12-15 20:45:34.329680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9831071 -5.3442163 -5.1985807 -4.0820589 -4.0981989 -4.7880621 -5.6686759 -6.1486936 -6.3197274 -6.6758537 -7.02507 -7.709197 -8.68017 -7.1763463 -5.0066743][-4.7636194 -5.4279017 -4.9018345 -4.1555257 -4.1593857 -4.5097227 -5.5938582 -6.4006748 -7.03085 -7.0038338 -7.2467608 -8.4936686 -10.038698 -8.6653242 -7.3135452][-2.5499451 -4.1508017 -4.8954949 -3.5289752 -3.7199211 -4.09124 -4.6935768 -5.2235379 -5.6374221 -6.0761309 -6.5292778 -7.7599263 -9.5386944 -8.6440887 -7.6798954][-4.3033371 -5.5639591 -5.4608421 -4.4711342 -3.8340564 -3.5814533 -3.4298589 -3.7916532 -4.2218523 -4.5448432 -5.0209012 -6.4451213 -8.5782528 -8.4696646 -7.74619][-5.8552217 -6.4889183 -5.85082 -3.8013091 -2.2654278 -0.82651424 -0.45407915 -1.7515364 -3.5943108 -3.7696564 -3.697736 -5.6725368 -8.2711372 -8.4531794 -7.90425][-6.252399 -6.3881207 -5.7639227 -3.5379446 -1.1090503 1.4249444 3.3112464 2.4678836 0.24405241 -1.6208329 -3.5050762 -4.352416 -6.5134897 -6.6724734 -6.4733319][-6.3014383 -6.2476659 -5.3268533 -2.8253288 0.31913853 3.2737479 5.200027 4.9187903 3.9105792 1.6076069 -0.94433832 -2.9644878 -5.8411641 -5.1204405 -4.9084058][-8.2391529 -7.1220412 -5.0620728 -2.3666594 -0.25102758 3.3200603 5.7935424 5.4217563 4.143352 2.8009543 1.7229028 -1.1363449 -4.7567291 -4.6814308 -5.0565519][-7.8327627 -6.9554834 -4.8635373 -3.0273433 -1.0084391 1.1153584 2.8965783 3.1468806 2.7046132 1.8436961 0.77413988 -1.7946248 -5.0571909 -5.1245775 -6.6557922][-6.8457346 -6.4423618 -5.1442389 -2.4929821 -0.90916014 -0.0056786537 0.65290451 1.9947777 2.5804687 1.3620453 -0.085740089 -2.5409083 -5.3593893 -5.5630875 -6.4881988][-6.6247463 -8.3097506 -7.9194269 -5.5192542 -3.6717155 -2.347702 -1.4436831 -0.58702421 -0.21696472 -0.75376558 -1.8947639 -4.4859943 -6.552629 -6.8773732 -6.8096733][-9.9724159 -9.9251947 -9.84132 -8.6411972 -7.2383785 -5.9034228 -4.1631594 -3.9151521 -4.5606222 -5.0041437 -5.5525117 -6.2472568 -6.79019 -7.77143 -8.5255785][-11.355844 -10.39303 -9.4990883 -9.4479866 -9.5432587 -7.6283388 -6.5305948 -5.7420893 -5.6197824 -6.0529523 -6.8809452 -6.9917293 -7.0792122 -6.1750259 -5.2952][-8.791955 -8.361043 -7.783123 -6.6273875 -5.5512862 -5.6831903 -5.6281738 -5.2313509 -5.7668939 -5.7620215 -5.9978805 -5.7913575 -5.7026644 -5.3251185 -4.8694677][-5.0004344 -4.6475387 -4.5606518 -3.696074 -3.0280159 -2.55923 -1.9160931 -2.83523 -4.0472865 -4.042634 -4.7761793 -6.19665 -7.1595845 -6.7301655 -6.4065952]]...]
INFO - root - 2017-12-15 20:45:41.077291: step 56710, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.655 sec/batch; 50h:10m:40s remains)
INFO - root - 2017-12-15 20:45:47.611603: step 56720, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 51h:02m:04s remains)
INFO - root - 2017-12-15 20:45:54.170537: step 56730, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 51h:27m:37s remains)
INFO - root - 2017-12-15 20:46:00.760442: step 56740, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 50h:35m:41s remains)
INFO - root - 2017-12-15 20:46:07.394980: step 56750, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 50h:00m:45s remains)
INFO - root - 2017-12-15 20:46:13.986991: step 56760, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 51h:05m:47s remains)
INFO - root - 2017-12-15 20:46:20.647224: step 56770, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.686 sec/batch; 52h:32m:03s remains)
INFO - root - 2017-12-15 20:46:27.184356: step 56780, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 50h:28m:06s remains)
INFO - root - 2017-12-15 20:46:33.689035: step 56790, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 49h:52m:17s remains)
INFO - root - 2017-12-15 20:46:40.298059: step 56800, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 50h:18m:57s remains)
2017-12-15 20:46:40.841174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4557476 -5.0430083 -6.4683404 -6.8579173 -6.6503897 -6.2070704 -6.0516462 -5.8759179 -5.362196 -3.9130239 -2.7456584 -3.0858164 -4.6668243 -5.8154931 -6.033123][-4.185142 -4.8848076 -5.7452555 -6.8756123 -8.47063 -8.1453876 -6.4160132 -5.4799528 -4.4324327 -2.3728006 -0.75341368 -1.2630653 -3.5972652 -4.8596783 -4.7413588][-1.2822952 -2.8282216 -4.2348547 -5.6389565 -7.9420333 -7.6104221 -5.7443361 -3.8893924 -2.4570334 -1.6645694 -1.1071463 -1.584043 -3.2809451 -4.1800933 -4.861516][-0.9831171 -3.1927335 -4.6761227 -5.5779247 -6.7875366 -5.813848 -4.1589751 -3.3183978 -2.5329556 -1.735683 -1.6847153 -2.7631958 -4.8970079 -6.0958829 -6.1225762][-2.2733567 -3.3855584 -5.4195304 -7.0360608 -7.4162731 -5.0066175 -2.3749616 -2.7259531 -2.9902587 -2.1946087 -2.0533438 -3.7748413 -5.4687204 -7.0956841 -7.5052376][-5.0818653 -5.2924142 -5.483758 -5.80343 -5.2098675 -2.303705 0.64089441 1.5979681 0.55563736 -1.8680584 -3.2877805 -3.0191717 -4.4462376 -6.734354 -8.1469059][-7.3361449 -7.8086271 -7.3919754 -4.9598169 -3.0221729 0.53042412 4.2020364 6.0605149 5.5367818 1.6278324 -2.3516974 -3.7483239 -5.4695883 -7.0729628 -8.1292934][-7.6172423 -8.0824356 -8.0120068 -5.0716157 -1.7873552 2.8183141 6.82575 7.7491918 6.7827191 3.929687 0.20930338 -3.6572294 -6.6022406 -8.5129023 -9.7351494][-7.7194252 -7.8075733 -7.1598387 -4.82481 -2.5290892 1.4906831 4.8175235 5.6186204 4.8062987 1.9360251 -0.7702508 -3.7340088 -8.2185869 -10.69129 -11.474154][-7.2454381 -6.3701925 -5.7983494 -5.1097603 -3.8489304 -0.86949492 0.69252157 1.7252951 2.1758871 -0.29496813 -3.3106136 -6.4081111 -10.258994 -13.350513 -14.599072][-9.4891872 -7.7010212 -6.9871054 -7.45145 -6.9253941 -4.7664027 -3.0888925 -2.3459177 -3.0454741 -4.2863693 -6.4127669 -9.9585037 -13.10543 -14.70326 -14.653843][-11.198341 -9.5930481 -8.7089109 -8.6584511 -8.8990231 -8.2885551 -6.4002461 -4.9188643 -5.6495352 -6.5267396 -7.8490396 -10.54504 -13.016094 -13.497014 -12.837559][-10.393514 -9.1212492 -8.0331821 -9.13931 -10.797173 -9.8050356 -8.2170229 -7.1956158 -6.1333151 -6.4381666 -7.3359423 -8.4628077 -10.087647 -10.479361 -9.6338215][-9.4401712 -9.0075817 -7.7451668 -8.7409382 -9.1293821 -8.6570034 -7.9252706 -6.9471569 -5.5941563 -5.8051252 -6.3198462 -6.8304486 -7.5994792 -7.9425125 -7.4024439][-6.8104424 -7.0333886 -7.3840437 -7.0542021 -6.6379876 -6.5423937 -5.2847524 -4.4350028 -4.3013678 -4.1480694 -4.8331289 -6.0330243 -7.1649556 -7.1553893 -7.6317415]]...]
INFO - root - 2017-12-15 20:46:47.468729: step 56810, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 51h:17m:01s remains)
INFO - root - 2017-12-15 20:46:54.005870: step 56820, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 49h:34m:33s remains)
INFO - root - 2017-12-15 20:47:00.589842: step 56830, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 49h:26m:09s remains)
INFO - root - 2017-12-15 20:47:07.268275: step 56840, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 52h:22m:46s remains)
INFO - root - 2017-12-15 20:47:13.885904: step 56850, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 49h:39m:53s remains)
INFO - root - 2017-12-15 20:47:20.468316: step 56860, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 49h:51m:36s remains)
INFO - root - 2017-12-15 20:47:27.078753: step 56870, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 49h:25m:00s remains)
INFO - root - 2017-12-15 20:47:33.628554: step 56880, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 49h:25m:14s remains)
INFO - root - 2017-12-15 20:47:40.191331: step 56890, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 49h:18m:43s remains)
INFO - root - 2017-12-15 20:47:46.878483: step 56900, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 51h:09m:39s remains)
2017-12-15 20:47:47.417374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3596475 -2.6735673 -3.5977287 -4.1227937 -4.0628314 -4.1799707 -4.4105644 -4.1188455 -3.786036 -3.2274606 -2.6459954 -4.5543747 -6.1130571 -6.8543363 -6.9406958][-3.3732138 -4.2173767 -3.9246588 -3.7071631 -3.9553561 -4.1824217 -4.4719481 -4.6044197 -4.367548 -4.0835366 -3.8987846 -5.3447518 -6.6104512 -7.5581555 -7.8178139][-2.4369266 -2.8126369 -3.9845355 -4.2706 -4.1401715 -4.5182309 -4.7143779 -4.6222143 -4.4169354 -4.5235715 -4.5438433 -6.046032 -7.6321087 -8.6091213 -9.2491331][-1.7009315 -2.5809994 -3.1705673 -3.202975 -3.8317091 -4.4427137 -4.8120131 -4.7890139 -4.821074 -4.8206654 -5.2114892 -6.9906693 -8.6020374 -9.3449783 -9.7222176][-2.5724368 -2.2575531 -3.1621745 -3.1411059 -2.8718953 -2.429812 -2.7068093 -3.8906851 -5.0560331 -4.6934805 -5.1857114 -6.7808275 -8.5000839 -9.7723675 -10.833275][-3.673322 -3.6325216 -3.0683267 -1.9420464 -1.4692693 -0.67329073 0.16437054 -0.42304659 -1.682374 -3.0497253 -4.5573597 -5.5384917 -7.6956868 -9.0340948 -9.8099756][-5.0521092 -4.527812 -3.2314179 -1.3403311 -0.7872138 0.24776363 1.6452613 1.8917375 1.4868169 -0.43865538 -3.3095691 -5.1244712 -7.2040048 -8.4513721 -9.2188969][-4.7996039 -3.7845354 -3.0631161 -0.71638727 0.77830029 2.0181246 2.369679 2.2845902 2.5187001 0.10996199 -2.6232283 -4.8540392 -6.9237375 -8.02679 -8.7239466][-3.2531495 -2.6241314 -1.5656471 -0.6556263 0.047658443 2.0374808 2.3882613 1.1051402 0.9592948 0.058810711 -1.0851398 -3.996371 -6.2012868 -7.16078 -7.6993837][-3.2629592 -2.7432044 -1.6867442 -0.73317432 -0.30381203 0.081035137 0.58783245 1.2003932 1.1770964 -0.69729567 -2.0910652 -4.3021803 -5.7928405 -6.9938788 -6.911757][-6.4494591 -5.9765768 -4.3772974 -2.9039907 -1.9847367 -2.1816945 -2.0657806 -1.7150979 -1.6542182 -1.8545194 -2.4696271 -5.2083197 -5.7052541 -6.6690855 -6.1391845][-9.9313679 -9.0779295 -8.0045023 -5.9262152 -4.2366128 -4.4469557 -4.7799463 -4.3933182 -3.4608266 -3.8893604 -4.4350653 -4.9509058 -5.2670145 -6.6191788 -6.7163367][-11.079332 -9.9630489 -8.588501 -7.8954949 -7.0509429 -6.0388637 -5.7028189 -6.3221679 -6.6674595 -6.4969339 -7.2517543 -7.9014421 -6.9830146 -6.7145162 -5.1136446][-10.158157 -9.3490829 -9.1958656 -7.9503627 -6.989923 -7.5229645 -6.868053 -5.8094025 -6.2877779 -7.1184926 -7.6292963 -7.6147265 -7.6747589 -6.9293494 -6.4076629][-6.8403096 -7.0781074 -6.4314489 -6.6627712 -7.6058908 -7.1229868 -6.9729333 -6.3494015 -6.0727334 -6.0748019 -7.3195853 -7.8224626 -8.2512522 -8.3074751 -8.3948631]]...]
INFO - root - 2017-12-15 20:47:53.935364: step 56910, loss = 0.20, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 49h:29m:51s remains)
INFO - root - 2017-12-15 20:48:00.591881: step 56920, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 50h:47m:26s remains)
INFO - root - 2017-12-15 20:48:07.172774: step 56930, loss = 0.11, batch loss = 0.07 (12.5 examples/sec; 0.638 sec/batch; 48h:49m:09s remains)
INFO - root - 2017-12-15 20:48:13.800614: step 56940, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 51h:03m:13s remains)
INFO - root - 2017-12-15 20:48:20.310470: step 56950, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.633 sec/batch; 48h:26m:26s remains)
INFO - root - 2017-12-15 20:48:26.969290: step 56960, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 51h:16m:47s remains)
INFO - root - 2017-12-15 20:48:33.524629: step 56970, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 51h:28m:08s remains)
INFO - root - 2017-12-15 20:48:40.122110: step 56980, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 51h:35m:13s remains)
INFO - root - 2017-12-15 20:48:46.689001: step 56990, loss = 0.11, batch loss = 0.07 (12.7 examples/sec; 0.631 sec/batch; 48h:15m:39s remains)
INFO - root - 2017-12-15 20:48:53.215848: step 57000, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 49h:51m:46s remains)
2017-12-15 20:48:53.801215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8356647 -6.5140295 -6.7699986 -5.9003782 -5.6999021 -5.3287683 -5.5431566 -5.0628414 -3.9945359 -3.7169445 -2.7589085 -3.2069325 -4.7536421 -6.36526 -6.9218192][-4.8232102 -4.464406 -2.7082717 -1.5150337 -2.0536287 -1.7968874 -2.3873641 -3.1756384 -2.6626716 -1.4619355 -1.004992 -2.0253997 -3.4117413 -5.8071232 -7.2722712][-0.92568779 -1.7207553 -1.4367495 -0.62225914 -0.28425074 0.59803343 0.55511093 -0.49412584 -1.402113 -1.0660529 -0.13334513 -1.6542907 -3.4679427 -5.0162969 -7.0096774][-1.145638 -1.4583254 -1.5047164 -0.99494314 -1.0288115 -0.16205072 -0.14795971 -0.99877596 -1.6506982 -1.3264108 -1.2398415 -2.4178247 -4.2235193 -6.1715426 -7.9029236][-3.5241792 -5.2441945 -4.8033104 -3.7981415 -3.2103374 -1.4467769 -1.0056105 -2.6675789 -3.6778426 -3.1898944 -2.8197937 -4.7679396 -6.7059093 -8.9853582 -10.312576][-8.4820709 -8.6429977 -7.4576311 -5.1735687 -3.5471969 -0.88831425 1.0135288 0.22882652 -1.3751354 -2.3400548 -3.3144729 -4.3404169 -5.9206443 -7.6875429 -8.8503122][-10.985851 -10.863462 -8.144083 -4.1723166 -1.7282391 0.40144348 3.9458995 4.3366446 3.1276126 0.26891327 -3.0117292 -4.44119 -5.9092035 -7.3440561 -7.9071627][-11.414033 -11.516094 -9.0805073 -3.6958485 -0.35589695 2.9273963 6.3065257 6.1043124 5.220861 3.3011231 0.18106174 -2.8187401 -5.9146771 -6.8416638 -7.5243177][-8.4385443 -8.9020557 -7.9013939 -5.1469793 -2.6888387 1.7130494 6.0173259 5.1107535 4.3172317 2.2804108 -1.1622996 -4.3299809 -8.5869713 -10.254542 -10.995796][-7.0987439 -7.3772326 -7.0961618 -5.3790865 -4.8907633 -3.0656414 0.35961056 1.7344575 2.3778329 -0.12011337 -3.5297635 -6.7651167 -10.213169 -12.758549 -13.891815][-10.667737 -11.004089 -10.212763 -8.7951355 -8.5463676 -7.4699097 -5.6872735 -4.7786131 -4.2732196 -5.523448 -7.3279028 -10.208649 -14.283951 -15.192127 -15.147017][-13.910259 -14.053982 -13.442064 -12.716387 -12.312044 -11.419905 -10.184602 -10.441706 -9.88744 -10.085581 -10.926117 -13.424949 -15.453489 -14.922089 -13.659248][-15.152826 -14.514261 -13.285456 -12.836842 -12.336511 -11.235635 -10.488036 -10.390917 -10.819338 -11.150276 -10.992029 -11.349026 -12.078132 -10.833572 -10.511115][-11.979624 -10.589743 -9.0829239 -8.9130783 -8.0695887 -8.3193169 -8.3058338 -7.9223628 -8.04049 -8.7270985 -9.7479 -9.0643921 -7.8983326 -8.0175962 -7.5545235][-8.8567142 -7.5078797 -5.8734279 -4.3156576 -3.3022058 -3.8843725 -4.7974644 -5.3207541 -6.0430293 -6.1948195 -6.3786497 -7.1459446 -7.7560139 -8.0201607 -8.2011271]]...]
INFO - root - 2017-12-15 20:49:00.356393: step 57010, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 49h:14m:42s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 20:49:06.993433: step 57020, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 50h:31m:47s remains)
INFO - root - 2017-12-15 20:49:13.678024: step 57030, loss = 0.22, batch loss = 0.18 (12.0 examples/sec; 0.667 sec/batch; 51h:00m:43s remains)
INFO - root - 2017-12-15 20:49:20.271314: step 57040, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 50h:02m:06s remains)
INFO - root - 2017-12-15 20:49:26.875622: step 57050, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 49h:17m:30s remains)
INFO - root - 2017-12-15 20:49:33.475356: step 57060, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 51h:04m:59s remains)
INFO - root - 2017-12-15 20:49:40.099287: step 57070, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 49h:05m:43s remains)
INFO - root - 2017-12-15 20:49:46.714564: step 57080, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 50h:31m:01s remains)
INFO - root - 2017-12-15 20:49:53.267361: step 57090, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 49h:56m:35s remains)
INFO - root - 2017-12-15 20:49:59.827141: step 57100, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 51h:17m:12s remains)
2017-12-15 20:50:00.372096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.509469 -7.1588864 -7.5783315 -7.9641595 -7.8826389 -8.32123 -8.260169 -8.3635054 -7.6590276 -6.9243479 -6.0545692 -6.8128462 -6.9513516 -7.8201647 -8.2542524][-4.3293262 -4.8312855 -5.7853932 -5.3659792 -6.2148986 -6.907526 -7.5635581 -7.82734 -7.6374903 -7.1122732 -5.9686708 -6.3029661 -7.7111969 -9.0455475 -9.517725][-2.848495 -3.5125604 -3.9980588 -4.321002 -6.0400791 -6.4511766 -6.9983559 -7.3024549 -7.0631151 -7.2404089 -7.5764446 -7.5119987 -8.2247715 -10.229477 -12.071253][-3.2219331 -3.1467395 -3.3479402 -4.0735059 -5.1095018 -5.601861 -5.40955 -6.1763864 -6.1539979 -6.1701412 -6.3456736 -6.5828304 -7.808249 -9.64177 -11.077105][-2.8913021 -3.7455425 -4.3415632 -4.3016095 -4.423501 -3.8034222 -3.0406537 -4.4765258 -5.2423406 -6.1149917 -5.5217967 -5.4018884 -6.2501678 -8.2150106 -10.058537][-2.7697546 -3.3365045 -4.0236244 -3.623013 -2.7671516 -0.94288826 0.62455273 -0.29903412 -1.8386474 -3.0919397 -3.8290219 -4.792357 -5.3504844 -6.9008255 -7.7626505][-3.8313589 -4.0155687 -3.1659911 -1.7978084 -1.0227857 1.01368 3.3142924 3.9313254 3.4668469 -0.20492172 -3.8568945 -5.0942206 -6.4958324 -7.1063042 -7.3619089][-4.9186335 -3.8445415 -2.9871595 -0.71720743 0.54248667 1.8803511 3.6352048 3.8871989 4.155767 2.2801881 0.026844978 -2.8405805 -6.9789486 -8.9556074 -10.109275][-6.68731 -5.2161894 -4.057261 -1.7911375 -0.77938986 0.57415581 2.3859906 3.1784425 4.3618217 3.1699252 2.3697095 -1.2146587 -7.0934892 -12.024151 -13.950771][-5.8968115 -5.7616143 -5.2318125 -2.6718004 -1.7544451 -0.38329458 1.5991497 1.4556456 2.0539303 2.6810222 2.3701243 -1.0291805 -4.8590589 -11.058757 -16.1098][-7.8213968 -8.39086 -8.9215031 -7.3066974 -5.841094 -4.3716269 -2.5847824 -2.2868469 -2.2961047 -2.8748991 -3.5856493 -5.9155183 -8.8561678 -13.325254 -15.842978][-10.634531 -10.691252 -10.722824 -9.5209389 -9.7823887 -9.353672 -8.0338259 -7.0684528 -5.8757005 -5.3236032 -6.9182997 -9.7510824 -12.800818 -15.09552 -15.817055][-13.931717 -13.507551 -13.620058 -12.660986 -12.659222 -11.819692 -11.749565 -11.402555 -10.513737 -9.2161617 -8.2732191 -10.757274 -13.484797 -15.542719 -16.891577][-13.619835 -14.426851 -13.694143 -12.265315 -12.235291 -12.426581 -12.685438 -11.569124 -11.085415 -9.7127333 -9.4748039 -10.843637 -10.97575 -13.339741 -15.125914][-10.946459 -11.991457 -11.323404 -10.61981 -9.2949867 -9.4048815 -9.3216095 -9.243391 -10.25816 -10.219269 -10.042192 -10.237705 -10.075717 -11.230495 -12.704321]]...]
INFO - root - 2017-12-15 20:50:07.025985: step 57110, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 49h:44m:45s remains)
INFO - root - 2017-12-15 20:50:13.661926: step 57120, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 50h:50m:22s remains)
INFO - root - 2017-12-15 20:50:20.197915: step 57130, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 50h:14m:43s remains)
INFO - root - 2017-12-15 20:50:26.840415: step 57140, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 50h:04m:52s remains)
INFO - root - 2017-12-15 20:50:33.353629: step 57150, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 49h:13m:35s remains)
INFO - root - 2017-12-15 20:50:39.986020: step 57160, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 50h:59m:00s remains)
INFO - root - 2017-12-15 20:50:46.664777: step 57170, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 49h:10m:44s remains)
INFO - root - 2017-12-15 20:50:53.282528: step 57180, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 48h:52m:47s remains)
INFO - root - 2017-12-15 20:51:00.009230: step 57190, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 51h:45m:31s remains)
INFO - root - 2017-12-15 20:51:06.620403: step 57200, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 50h:44m:49s remains)
2017-12-15 20:51:07.102779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8891225 -6.9727058 -6.3544874 -4.7701464 -5.4079604 -6.5579796 -6.6257696 -6.4035373 -6.382822 -7.8401079 -8.6511889 -10.82557 -11.744211 -12.285714 -11.492414][-6.3362327 -6.1420035 -6.2741818 -5.6330285 -6.4019432 -6.925632 -6.9037628 -6.7587175 -6.531918 -6.5263271 -6.3171921 -8.8629627 -9.9036407 -10.623591 -10.605476][-5.0729303 -5.7004647 -6.6946115 -6.9377093 -7.3883548 -7.5157833 -7.2839217 -7.0360727 -6.2998009 -6.5944195 -6.9822025 -8.9879475 -9.840765 -10.135042 -9.8221731][-3.3112805 -4.7616577 -5.9882455 -6.8686171 -8.1796265 -6.7462091 -4.8784761 -5.4968328 -5.5045 -5.2651782 -5.0272546 -6.9057374 -8.3098431 -9.7574148 -10.224289][-4.0237012 -4.6894412 -5.8401427 -5.6720815 -5.8050303 -5.4970093 -3.7698352 -3.1566994 -2.3418036 -3.6136904 -5.0911951 -7.2882271 -8.4281254 -10.206215 -10.50139][-3.4732649 -4.8781853 -6.0559325 -5.6610718 -4.94409 -2.72588 0.09467411 1.0375328 1.372951 -0.77290106 -2.5261312 -5.2128787 -7.4372787 -10.064043 -10.278408][-5.0295463 -4.9909763 -5.5518942 -4.0021186 -2.3616982 0.059905052 2.925447 4.4719625 4.9622922 2.1255856 -0.94821692 -5.08825 -6.8671346 -8.895154 -9.8925056][-4.3351507 -5.4451065 -5.4845853 -3.9585195 -1.827734 2.1051917 5.4657378 5.0842166 4.5791907 2.83524 0.60879469 -3.9975615 -6.0891414 -8.2784615 -9.3943548][-3.2218122 -3.5036204 -3.7868042 -2.5845785 -0.50754261 2.1440129 4.3879132 4.5018725 3.4447246 0.57381058 -1.5961785 -5.3363948 -7.92039 -10.79372 -11.031245][-2.4550111 -3.5872941 -4.4391136 -4.1812811 -2.6977053 -0.097839355 2.4630141 2.8707414 1.8537388 -0.39658642 -3.3226936 -7.2777934 -9.378294 -12.175408 -13.137352][-4.5074911 -4.6577311 -5.1782126 -5.5279765 -5.488637 -3.3618312 -1.9333434 -1.9079158 -2.6596239 -4.7978334 -7.0614419 -11.237984 -13.640802 -14.69153 -14.345831][-7.4313593 -8.7346087 -8.7349892 -8.4460821 -8.9965363 -7.4492621 -5.5270424 -5.3775291 -6.453126 -8.2202454 -10.079451 -12.246668 -13.554802 -14.672342 -14.138353][-9.9820185 -10.512548 -10.407417 -10.861591 -11.21314 -9.7057133 -9.6335783 -9.3574295 -9.5688848 -11.49552 -12.008808 -12.392105 -13.576762 -13.34705 -12.059307][-10.100552 -11.363433 -10.66127 -9.6608124 -10.254311 -9.4781675 -8.41016 -8.48704 -8.8924446 -9.8550625 -10.790783 -10.398548 -9.9182434 -9.5955553 -9.0985718][-8.65249 -9.277586 -9.3138733 -8.0872707 -7.419508 -6.7929363 -7.2227974 -7.5604215 -7.2738495 -8.0226154 -7.7991323 -8.1553764 -8.5155487 -7.8126779 -7.9821525]]...]
INFO - root - 2017-12-15 20:51:13.625815: step 57210, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 49h:57m:08s remains)
INFO - root - 2017-12-15 20:51:20.230470: step 57220, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 51h:33m:01s remains)
INFO - root - 2017-12-15 20:51:26.900818: step 57230, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 50h:52m:52s remains)
INFO - root - 2017-12-15 20:51:33.524592: step 57240, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 51h:47m:14s remains)
INFO - root - 2017-12-15 20:51:40.084826: step 57250, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 48h:56m:31s remains)
INFO - root - 2017-12-15 20:51:46.669584: step 57260, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 48h:31m:13s remains)
INFO - root - 2017-12-15 20:51:53.347166: step 57270, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.638 sec/batch; 48h:44m:54s remains)
INFO - root - 2017-12-15 20:51:59.919737: step 57280, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 51h:26m:00s remains)
INFO - root - 2017-12-15 20:52:06.625038: step 57290, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 49h:40m:36s remains)
INFO - root - 2017-12-15 20:52:13.237068: step 57300, loss = 0.14, batch loss = 0.09 (11.5 examples/sec; 0.698 sec/batch; 53h:21m:42s remains)
2017-12-15 20:52:13.749305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.245913 -7.6693287 -8.6186028 -8.869216 -9.1989174 -9.4948969 -9.5482149 -7.5331693 -6.3405423 -5.8666592 -5.2463212 -8.3062353 -10.751834 -11.812078 -11.803831][-5.7553568 -6.2576346 -7.3604126 -8.5454063 -10.133095 -10.720436 -11.568236 -11.172682 -10.178699 -8.14786 -6.7366629 -9.8439417 -11.837523 -13.339406 -13.593443][-1.2777567 -4.4893432 -7.6121979 -7.9892225 -9.2498407 -10.92725 -12.202551 -11.450955 -10.290478 -9.4895191 -9.0385666 -12.178246 -13.66818 -13.891396 -13.663057][-0.84821224 -3.1195977 -5.6683426 -6.3108959 -7.9889131 -8.4920464 -8.463232 -9.7189636 -11.342869 -10.187925 -8.5174627 -12.376571 -15.149054 -15.70055 -14.980078][-2.5873568 -4.2454534 -6.9117684 -6.6767921 -6.6763306 -4.985085 -3.2935441 -5.9699178 -9.4700441 -9.3876085 -9.3071957 -12.575391 -14.365841 -15.887474 -15.569128][-6.6002765 -6.8333912 -7.6846261 -5.5600429 -3.9322248 -0.27734423 2.7567363 1.0498452 -1.6253123 -5.1629343 -9.385437 -11.517673 -12.235769 -13.753874 -14.585379][-9.5275784 -8.9524364 -6.8635955 -3.3051033 -1.8806767 1.9672508 6.7829766 7.3474536 5.0797429 -0.8286972 -6.4104156 -9.5137386 -11.430335 -12.842109 -13.557224][-10.887877 -9.7103977 -7.8355002 -3.9680228 -1.4080625 4.1642766 9.1093063 8.2503986 6.9613013 2.4809184 -3.0737939 -8.1640587 -10.608244 -11.587755 -12.95118][-7.4275894 -7.8343468 -7.4999471 -3.5656955 -0.067884922 4.4005246 7.26411 7.1880078 6.8969398 1.4189506 -3.7523654 -8.5137835 -12.563408 -14.170483 -13.5352][-6.4683142 -6.0664568 -6.1677074 -3.1300793 -0.39271355 2.1016665 4.875648 5.714622 4.5706029 -0.035355568 -3.7405884 -9.9509068 -14.317467 -15.159548 -15.560528][-10.53965 -8.9683418 -9.2628727 -6.6868525 -4.9172735 -3.3413963 -0.78392172 -0.85240078 -2.2086434 -3.8867216 -6.4033256 -12.886763 -16.06592 -17.033869 -16.078867][-13.717011 -13.120861 -12.571779 -11.216606 -10.542814 -8.3474236 -7.3352289 -8.5116291 -8.6889915 -9.01023 -10.241331 -13.43868 -15.449272 -15.649912 -15.133482][-16.590113 -15.913235 -14.716976 -13.707708 -13.286726 -11.953305 -11.21669 -11.478758 -12.169912 -12.346644 -12.400922 -14.26465 -13.809425 -13.25684 -12.803923][-13.851044 -13.007776 -13.308579 -12.742851 -12.470627 -12.59929 -12.055991 -11.315975 -11.211685 -11.790936 -12.240303 -11.87208 -11.739689 -11.404613 -9.7262764][-9.64168 -9.2906322 -8.8217211 -8.2531776 -8.2589054 -8.472436 -8.9520988 -10.006111 -10.472921 -9.8631907 -9.1821718 -11.57938 -11.919456 -10.087073 -9.8101883]]...]
INFO - root - 2017-12-15 20:52:20.220344: step 57310, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 49h:05m:01s remains)
INFO - root - 2017-12-15 20:52:26.762959: step 57320, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 49h:22m:26s remains)
INFO - root - 2017-12-15 20:52:33.276668: step 57330, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 50h:30m:04s remains)
INFO - root - 2017-12-15 20:52:39.877439: step 57340, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 51h:22m:06s remains)
INFO - root - 2017-12-15 20:52:46.396470: step 57350, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 51h:10m:13s remains)
INFO - root - 2017-12-15 20:52:52.899512: step 57360, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 49h:16m:21s remains)
INFO - root - 2017-12-15 20:52:59.452029: step 57370, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 50h:27m:37s remains)
INFO - root - 2017-12-15 20:53:06.016988: step 57380, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 48h:54m:38s remains)
INFO - root - 2017-12-15 20:53:12.567590: step 57390, loss = 0.23, batch loss = 0.18 (11.8 examples/sec; 0.679 sec/batch; 51h:54m:55s remains)
INFO - root - 2017-12-15 20:53:19.201794: step 57400, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 50h:33m:07s remains)
2017-12-15 20:53:19.694985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1241524 -2.5456386 -2.874409 -2.6509447 -3.4361787 -3.3848069 -3.3989179 -3.1869709 -3.4817576 -4.6864471 -5.5690813 -8.4661007 -10.102412 -12.05501 -11.21644][-4.0558405 -2.9476802 -3.0502708 -3.0573685 -3.8608208 -4.485137 -4.4001918 -3.4821756 -3.9330926 -4.3981953 -4.6673737 -8.6041746 -10.120688 -12.228891 -12.431311][-2.5042083 -2.9573774 -3.6426473 -3.8537145 -4.5649672 -4.4939089 -4.4529428 -3.814693 -3.1981559 -4.0661435 -5.1685572 -7.6328268 -9.3209352 -10.603102 -10.141872][-2.0667427 -2.2464843 -2.5118 -3.2758453 -4.2721148 -4.5830727 -4.453105 -3.9102135 -4.1214805 -4.5665026 -5.2331467 -8.4935389 -9.9174767 -11.506609 -10.779531][-2.6298943 -3.4632325 -3.2885876 -2.3649089 -3.1478646 -2.2749016 -1.735254 -3.205132 -4.1490116 -4.3027496 -4.81462 -8.0701656 -8.8252573 -10.646192 -10.885714][-4.1149931 -3.725884 -2.7108335 -1.1512027 -0.863122 0.36274242 1.2802777 1.32897 -0.43602228 -2.4345107 -4.2076764 -7.369307 -9.2916555 -11.709124 -11.538655][-5.4247642 -4.5997987 -3.291887 -0.4026823 1.5108666 2.6575961 3.9128861 4.9436374 3.4994359 0.51786709 -1.6591868 -5.676343 -8.412756 -11.188947 -11.352154][-4.8219466 -4.4108453 -4.1567745 -1.6072907 1.1533289 3.8948865 5.4342284 5.8627734 5.252892 3.0618005 1.2421985 -3.2170305 -6.5566468 -10.027138 -11.401415][-3.5829904 -3.7795973 -4.0491662 -2.3238337 -0.6256423 2.2991405 4.0349021 5.0605187 4.9362264 3.2453237 1.3209615 -2.8911552 -6.1328406 -9.96068 -10.937517][-3.7526979 -4.9727254 -5.4806523 -3.606817 -2.9946527 -1.4882979 0.29651356 2.012804 2.4167094 1.4610033 0.25561047 -4.5496178 -7.000061 -9.959094 -10.835299][-8.3583183 -8.3224554 -8.50938 -7.3286176 -7.0085926 -5.9114113 -5.0692782 -3.5568478 -2.0154817 -2.0516031 -2.6365306 -6.8063226 -9.0452251 -10.286712 -10.09943][-12.269878 -11.425985 -11.066645 -10.955044 -10.976002 -9.6937542 -9.2634239 -8.396162 -6.6413636 -5.7342815 -5.7346411 -8.4565039 -10.124731 -10.849849 -9.6337147][-14.553423 -14.135206 -12.784311 -11.4615 -11.839136 -11.733495 -11.592976 -11.073479 -9.8083582 -7.9077086 -7.5367522 -9.268652 -10.040471 -10.311489 -8.5496559][-12.088499 -13.097418 -12.461345 -11.325979 -11.058777 -10.789768 -10.573475 -9.1134586 -8.3153715 -8.2928619 -7.5888085 -7.3940439 -7.6909223 -8.1775875 -7.6471109][-9.67018 -10.415634 -10.864085 -10.200482 -9.479641 -9.4508839 -9.37102 -8.68189 -8.7084122 -8.6913195 -7.6886406 -8.9798031 -9.4984493 -9.6426945 -9.8298292]]...]
INFO - root - 2017-12-15 20:53:26.307488: step 57410, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 51h:07m:19s remains)
INFO - root - 2017-12-15 20:53:32.852330: step 57420, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.648 sec/batch; 49h:29m:00s remains)
INFO - root - 2017-12-15 20:53:39.469278: step 57430, loss = 0.19, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 48h:40m:14s remains)
INFO - root - 2017-12-15 20:53:46.189894: step 57440, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 50h:05m:45s remains)
INFO - root - 2017-12-15 20:53:52.747829: step 57450, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 50h:12m:40s remains)
INFO - root - 2017-12-15 20:53:59.406020: step 57460, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 49h:15m:18s remains)
INFO - root - 2017-12-15 20:54:06.053780: step 57470, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 50h:09m:25s remains)
INFO - root - 2017-12-15 20:54:12.673789: step 57480, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 51h:15m:18s remains)
INFO - root - 2017-12-15 20:54:19.310280: step 57490, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 51h:20m:47s remains)
INFO - root - 2017-12-15 20:54:26.015768: step 57500, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 49h:07m:00s remains)
2017-12-15 20:54:26.558020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.434284 -3.3256576 -2.9354849 -3.1006546 -4.2945395 -5.74765 -7.2112274 -7.5386262 -7.5556645 -8.5745163 -8.9052143 -10.80127 -12.377893 -12.979938 -10.933279][-5.3387942 -4.9226451 -3.9639478 -4.0613337 -4.75745 -6.1155577 -7.2884178 -8.0632372 -8.4105692 -8.036232 -8.24394 -10.773296 -12.667144 -14.022379 -11.795989][-3.8025258 -4.20232 -4.7302151 -3.8393297 -3.8781271 -5.3945932 -6.6045403 -6.9180708 -6.7702732 -7.366652 -8.2123346 -10.459648 -12.734623 -15.03792 -14.706846][-6.563211 -6.1186743 -5.476521 -4.194077 -4.4015059 -4.5671482 -5.0653534 -6.2633533 -6.868084 -6.6073828 -6.7267075 -10.051613 -12.908329 -15.023546 -13.989584][-8.71969 -9.2454033 -8.0288486 -5.2827053 -3.3615115 -1.3432565 -0.78013134 -2.1966555 -4.36509 -5.9743032 -7.5998659 -9.8190861 -11.454403 -14.406836 -13.910917][-10.238477 -9.741785 -7.8563004 -3.7890503 -0.37824678 3.0390229 5.6920238 4.6401582 2.4589343 -1.6215243 -6.3194079 -9.07448 -10.876869 -12.5402 -11.682898][-12.169904 -10.808132 -7.9213123 -3.1221888 0.33360577 5.312695 9.29818 9.0237656 7.8036695 2.380444 -3.5026069 -8.0511112 -11.44248 -12.198147 -10.999368][-12.170782 -10.136392 -7.6037297 -3.3281775 0.26861668 6.07931 9.9827824 9.8009987 9.1290417 3.90276 -1.8581231 -6.9619789 -11.168417 -12.717316 -11.752165][-9.8112869 -8.5992174 -7.4271817 -3.7246718 -0.53283072 3.8509974 7.007791 7.9791541 7.7744451 2.3054585 -2.8137527 -8.1858034 -12.172467 -13.066769 -11.998601][-8.06824 -7.1474743 -6.5518 -4.3562589 -2.9122293 -0.60886908 2.0277104 3.8572745 4.0107141 0.26156855 -4.0600739 -8.6579857 -12.097794 -13.916059 -13.131104][-11.072145 -10.899342 -10.019091 -8.3000584 -7.6357718 -5.9856305 -4.2880192 -3.5496297 -3.4447222 -4.9315944 -7.020617 -11.580963 -13.72879 -14.381197 -12.337774][-16.890381 -15.198118 -13.869234 -12.728775 -12.246874 -11.022129 -10.749619 -10.218906 -9.5713453 -10.128165 -11.024764 -12.213812 -11.947861 -13.368379 -11.937192][-14.97224 -13.213221 -12.771933 -13.470854 -13.395454 -12.524761 -12.279358 -11.919813 -12.255267 -11.423479 -10.926143 -11.244755 -10.899191 -9.9641819 -7.7206917][-10.387224 -9.8409863 -9.2860117 -7.9677377 -7.7019243 -9.7224312 -11.056671 -10.669212 -10.61152 -9.7866421 -9.5667839 -8.335516 -7.4802179 -7.2466335 -6.8101096][-7.8733044 -5.3559127 -3.6591601 -3.4752026 -3.4735751 -3.9984081 -4.6943007 -6.0636415 -7.201148 -6.3280931 -6.17058 -6.8513975 -7.4886689 -7.792326 -8.1364517]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-57500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-57500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 20:54:34.377417: step 57510, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 49h:50m:30s remains)
INFO - root - 2017-12-15 20:54:41.005914: step 57520, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 49h:33m:54s remains)
INFO - root - 2017-12-15 20:54:47.745654: step 57530, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 49h:53m:57s remains)
INFO - root - 2017-12-15 20:54:54.288616: step 57540, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 50h:45m:02s remains)
INFO - root - 2017-12-15 20:55:00.879928: step 57550, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 50h:47m:07s remains)
INFO - root - 2017-12-15 20:55:07.374010: step 57560, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 50h:01m:05s remains)
INFO - root - 2017-12-15 20:55:13.988638: step 57570, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.691 sec/batch; 52h:47m:54s remains)
INFO - root - 2017-12-15 20:55:20.596582: step 57580, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 50h:39m:29s remains)
INFO - root - 2017-12-15 20:55:27.207407: step 57590, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 50h:47m:42s remains)
INFO - root - 2017-12-15 20:55:33.907409: step 57600, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 50h:58m:09s remains)
2017-12-15 20:55:34.455626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1943674 -7.614233 -7.4596696 -7.3440881 -7.4874072 -6.9185739 -7.0377893 -6.8740926 -7.4684005 -7.7275534 -8.1641855 -11.174152 -12.551807 -12.774528 -11.233046][-7.3902373 -8.2340155 -9.0650806 -8.7894716 -8.3835211 -7.6899557 -7.3047104 -7.1513457 -7.4320412 -7.9043884 -8.65692 -11.704063 -13.643032 -13.026056 -11.4991][-4.424458 -6.2932086 -7.739892 -6.6942534 -6.2149 -6.2030492 -6.0739594 -6.1961622 -6.7454119 -7.3059955 -8.3304205 -10.12652 -10.550274 -10.393017 -8.6732063][-3.4638803 -3.9581938 -4.9729156 -4.9746976 -4.6743193 -3.9674127 -3.9944069 -4.0961823 -3.9078104 -4.6541939 -4.99043 -7.5803165 -9.39553 -9.0441675 -7.9340973][-2.4439449 -2.8850055 -3.4105642 -2.7677207 -2.3550315 -1.2996092 -0.97855759 -1.4223666 -2.1748531 -2.1552582 -2.0845845 -3.5604424 -5.0435786 -6.6188397 -6.7864151][-3.5679789 -2.5509665 -1.4272838 -0.43984604 0.23524904 1.6150861 2.6109233 2.1374788 1.4011688 0.58722878 0.33794355 -1.3638725 -3.6049011 -5.334621 -5.6882739][-6.618299 -5.2575064 -3.3892756 0.10290575 1.7136245 3.1706815 3.9544797 4.1745133 3.6972785 1.6959152 0.79970932 -1.4447708 -4.1944566 -5.589612 -5.7050567][-6.98358 -6.2267776 -4.5666447 -1.2542233 0.31787443 2.583406 3.6244102 3.336082 2.6533494 1.0101457 -0.060470104 -2.7749345 -6.0412927 -7.7803249 -7.5644317][-6.8815813 -6.9986916 -6.2197671 -3.7291687 -1.2532263 0.62691164 1.9688096 1.7756205 1.6367459 1.0284514 0.93743324 -2.7277234 -5.6130209 -6.24725 -5.7703538][-8.2896671 -8.4189539 -7.6307168 -6.2991538 -5.3648319 -2.5878057 -1.1990824 -0.69522285 -0.48662949 -1.2782302 -1.2109704 -3.1669607 -4.9218364 -6.2499642 -6.4212832][-12.038691 -11.542096 -11.385786 -9.7097607 -7.4504857 -5.8429642 -5.6667233 -4.966578 -5.0601892 -5.0018153 -4.6133537 -5.5631838 -6.5513916 -5.9513474 -4.9764938][-15.866579 -15.529749 -13.862476 -11.930832 -10.690195 -9.92893 -10.37999 -9.9447212 -9.0356388 -9.1456375 -8.7754507 -8.4313774 -7.0920639 -6.3734522 -5.2364206][-14.789703 -14.520571 -13.317942 -11.587801 -11.432999 -10.397234 -9.96397 -9.9400311 -10.525633 -9.5324116 -8.2375965 -7.715013 -6.8221049 -5.4308 -4.2052131][-13.364836 -12.036052 -10.39146 -8.2842855 -6.7370615 -6.2139597 -6.65542 -6.2506833 -6.5957241 -6.6484642 -6.4820609 -6.0113478 -5.543016 -5.1693912 -4.6186433][-8.8025322 -8.3448906 -7.8326511 -5.9545951 -3.5340037 -2.8074727 -2.9472425 -3.0254915 -3.3780687 -3.5614865 -3.8433666 -4.3185821 -4.3493977 -4.7152734 -4.6052828]]...]
INFO - root - 2017-12-15 20:55:40.925765: step 57610, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 51h:35m:53s remains)
INFO - root - 2017-12-15 20:55:47.536769: step 57620, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 51h:21m:19s remains)
INFO - root - 2017-12-15 20:55:54.121978: step 57630, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.639 sec/batch; 48h:45m:25s remains)
INFO - root - 2017-12-15 20:56:00.712153: step 57640, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.675 sec/batch; 51h:33m:22s remains)
INFO - root - 2017-12-15 20:56:07.274537: step 57650, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.630 sec/batch; 48h:05m:23s remains)
INFO - root - 2017-12-15 20:56:13.795051: step 57660, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 51h:02m:45s remains)
INFO - root - 2017-12-15 20:56:20.336269: step 57670, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 48h:58m:29s remains)
INFO - root - 2017-12-15 20:56:26.984885: step 57680, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 49h:41m:20s remains)
INFO - root - 2017-12-15 20:56:33.543523: step 57690, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 49h:28m:42s remains)
INFO - root - 2017-12-15 20:56:40.302994: step 57700, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 50h:55m:53s remains)
2017-12-15 20:56:40.896803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4808931 -6.7899876 -4.6644058 -2.6017866 -1.8677468 -2.3032632 -3.9738693 -5.5892515 -5.5454826 -4.5820742 -3.6217592 -5.1343651 -6.963316 -6.4925246 -6.12506][-6.621614 -5.3291936 -3.2584648 -1.5021658 -2.0532877 -2.8741157 -4.1316466 -5.8845525 -7.0264421 -6.93377 -6.207437 -7.7291126 -9.1975088 -8.3392487 -7.7940693][-2.9110353 -3.2423038 -2.9143307 -0.78030396 -1.3833795 -2.9948039 -5.04261 -6.5567837 -7.2299786 -7.5418711 -7.6048875 -9.65691 -11.824353 -10.991592 -9.7827559][-5.1708207 -4.8858643 -3.9461374 -2.2980864 -3.4285784 -4.0437384 -4.7438965 -6.0401025 -7.1643319 -7.18378 -7.1620827 -9.9507256 -13.039028 -12.649281 -12.066761][-5.1988506 -6.28893 -5.7751422 -3.8947868 -3.9716711 -2.48971 -1.5129638 -2.4054341 -4.03361 -4.9940982 -5.9798894 -9.114152 -12.229103 -13.020001 -13.59297][-6.432065 -6.7211418 -6.168251 -4.1688156 -3.0500548 -0.96153736 1.3912482 1.8245592 1.5582943 -0.92308664 -4.4288092 -8.1271629 -11.35162 -12.459393 -12.630387][-7.5049086 -7.3067341 -6.298871 -3.2847841 -0.78268385 1.1323171 3.4766564 4.7366452 4.8482833 1.6521344 -1.8427961 -5.31222 -8.8603821 -9.5225811 -9.6179142][-8.8121414 -7.9023476 -6.7503805 -2.986259 0.1645813 2.767199 5.4179473 6.0517945 6.0217652 3.4278741 0.39283037 -3.3503985 -7.0526409 -7.3718724 -7.3345666][-8.2036276 -7.0632315 -6.2052279 -2.3222015 1.6946726 4.602201 6.0619445 5.8075938 5.3934722 3.8736911 1.8949771 -2.5614305 -6.7251978 -6.5287604 -5.6852851][-8.2627134 -7.1986055 -6.471446 -3.2898073 0.17431021 2.109024 3.9324384 4.2860932 3.3960366 2.3443866 1.3562651 -2.1628275 -5.6262503 -6.3367224 -6.7401323][-12.955541 -11.256849 -10.353026 -7.5937452 -5.2711229 -3.5107718 -1.9217596 -2.1083007 -2.6784823 -2.6395779 -2.5919392 -5.7415595 -7.7272024 -7.0944567 -6.1986389][-16.783567 -15.349874 -13.787399 -11.153433 -9.9569778 -8.6881208 -7.6904721 -7.4211073 -7.8180609 -7.8411894 -7.5102305 -8.8137293 -9.2871943 -8.7703915 -8.09647][-16.158487 -14.931414 -13.828833 -11.863963 -10.491121 -10.070648 -10.035357 -10.06645 -10.378033 -9.5514688 -8.9361954 -9.3669624 -9.54451 -8.2837715 -6.9915857][-12.543591 -12.360115 -11.404406 -9.7780142 -9.3283644 -9.6709528 -9.8415117 -9.0617466 -8.1588182 -8.01231 -8.3215828 -7.837513 -7.880857 -7.2655778 -6.5195675][-9.8520222 -9.7093306 -9.2256756 -7.7472506 -6.6781693 -5.6374226 -5.7652664 -6.5950947 -6.9153433 -6.375608 -5.5143266 -6.4045644 -7.6684666 -7.8845544 -8.0002213]]...]
INFO - root - 2017-12-15 20:56:47.500956: step 57710, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 52h:01m:40s remains)
INFO - root - 2017-12-15 20:56:54.182231: step 57720, loss = 0.17, batch loss = 0.13 (11.5 examples/sec; 0.697 sec/batch; 53h:11m:59s remains)
INFO - root - 2017-12-15 20:57:00.862908: step 57730, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 50h:37m:22s remains)
INFO - root - 2017-12-15 20:57:07.425549: step 57740, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.640 sec/batch; 48h:51m:19s remains)
INFO - root - 2017-12-15 20:57:14.013096: step 57750, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 48h:46m:12s remains)
INFO - root - 2017-12-15 20:57:20.598357: step 57760, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 49h:15m:49s remains)
INFO - root - 2017-12-15 20:57:27.304825: step 57770, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.689 sec/batch; 52h:33m:26s remains)
INFO - root - 2017-12-15 20:57:33.952881: step 57780, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.693 sec/batch; 52h:50m:53s remains)
INFO - root - 2017-12-15 20:57:40.596383: step 57790, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 49h:22m:25s remains)
INFO - root - 2017-12-15 20:57:47.236821: step 57800, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 49h:49m:30s remains)
2017-12-15 20:57:47.788090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3776145 -1.6294136 -1.056263 -0.34860706 -0.83524847 -1.3053269 -1.3199539 -1.5126734 -1.9565618 -2.5484083 -2.7237651 -4.6162686 -5.5821142 -5.6546974 -4.2345133][-2.3030837 -1.902153 -1.4324431 -1.0835953 -1.6041961 -2.1487668 -2.1700048 -2.2766078 -2.511054 -2.5462966 -2.8725848 -4.5850921 -5.2040982 -5.6731653 -4.1317239][-1.7120285 -1.9442379 -1.9058328 -0.8848033 -1.4876051 -1.9683888 -1.9818864 -2.2128372 -2.015554 -1.996321 -2.3616838 -4.4815226 -5.6396236 -5.5750461 -4.7844472][-2.8230219 -2.2580085 -2.3011844 -1.5887604 -2.5724344 -2.9926722 -3.1489704 -3.1604588 -2.607384 -2.0713825 -1.818069 -4.4660797 -5.967576 -6.8109245 -6.0615034][-3.4578872 -2.965585 -2.9212549 -1.9537389 -1.9879248 -1.8622603 -1.9298799 -2.0930905 -1.3317137 -0.8499589 -0.70292854 -3.0778925 -4.6319914 -6.5166678 -6.2945523][-4.7438507 -3.7103472 -2.8592849 -0.95892143 -0.16537857 0.38186502 1.1874433 1.5455384 1.9698019 1.8399796 1.4851956 -1.4791102 -3.4628179 -5.4616561 -5.4649653][-6.5042267 -5.2898064 -3.3014166 -0.73021173 0.66103125 1.9779067 3.5894685 4.5103679 4.950458 3.7416453 2.3130355 -0.81126356 -3.9272766 -5.5054517 -5.6149445][-6.430697 -5.5596991 -3.8014131 -0.37282896 1.1668634 3.4321628 5.0415463 5.493711 6.3258691 5.3123279 3.2581334 -1.4315581 -4.7432303 -6.8231454 -7.0340834][-5.6537714 -4.6217508 -3.3788159 -0.72151041 0.7064147 2.6826787 3.9113097 4.5667987 4.9998794 3.9112916 2.3997188 -2.0045578 -5.5406609 -7.8589506 -8.0650444][-4.9344368 -4.010263 -3.5293827 -1.117662 -0.30566835 1.192831 2.4946809 3.0910172 3.3897128 1.7338552 0.51549244 -3.6325357 -6.8138041 -8.9286957 -9.23193][-7.4728036 -7.1224022 -5.8170381 -4.0404978 -3.8289549 -3.3355932 -3.1596611 -2.8275456 -2.7663879 -3.4393792 -3.8969111 -7.7938776 -9.3131256 -10.210322 -9.1354647][-10.88303 -9.89674 -8.589613 -7.2621174 -7.123291 -6.3994055 -7.0236568 -7.7724733 -7.5538568 -7.6622791 -7.7149734 -9.558444 -9.1680489 -10.009356 -8.6383486][-11.851414 -10.94136 -9.7164755 -8.75341 -8.8339653 -7.8021226 -8.6932755 -9.1119862 -9.352663 -9.2412786 -9.1644964 -9.8007383 -9.0215712 -8.8320961 -6.1092296][-10.713917 -9.8073311 -8.9735737 -7.9916663 -7.459898 -7.8840652 -8.7201424 -8.4860153 -8.7474575 -8.8123617 -8.8331909 -7.8884077 -6.7608895 -6.1627145 -5.1560869][-7.3618059 -6.9460125 -6.0777178 -4.7256451 -4.3071985 -4.1557627 -4.6865439 -5.0078182 -5.9180059 -5.6826959 -5.8608947 -6.436573 -6.652504 -6.329504 -6.0236206]]...]
INFO - root - 2017-12-15 20:57:54.398222: step 57810, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 49h:53m:27s remains)
INFO - root - 2017-12-15 20:58:01.012032: step 57820, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.671 sec/batch; 51h:10m:45s remains)
INFO - root - 2017-12-15 20:58:07.632863: step 57830, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 50h:13m:06s remains)
INFO - root - 2017-12-15 20:58:14.245968: step 57840, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 50h:24m:05s remains)
INFO - root - 2017-12-15 20:58:20.835039: step 57850, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 50h:08m:33s remains)
INFO - root - 2017-12-15 20:58:27.393796: step 57860, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.691 sec/batch; 52h:44m:12s remains)
INFO - root - 2017-12-15 20:58:34.032644: step 57870, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 50h:49m:54s remains)
INFO - root - 2017-12-15 20:58:40.630416: step 57880, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 50h:35m:48s remains)
INFO - root - 2017-12-15 20:58:47.123226: step 57890, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.671 sec/batch; 51h:09m:52s remains)
INFO - root - 2017-12-15 20:58:53.803062: step 57900, loss = 0.14, batch loss = 0.10 (11.4 examples/sec; 0.700 sec/batch; 53h:24m:23s remains)
2017-12-15 20:58:54.327200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.134645 -10.448015 -10.670948 -9.276329 -8.7327194 -8.593174 -8.471693 -8.7681446 -8.934618 -8.3330116 -7.2508516 -7.5598083 -7.972764 -7.0929942 -4.7566929][-8.5411911 -7.82429 -5.6424375 -5.4977703 -7.3258605 -7.386025 -7.0340843 -6.9297256 -7.4127274 -7.8469915 -7.817956 -8.1695442 -7.8609519 -8.1267252 -5.9494324][-5.3004441 -5.8221097 -5.724215 -4.5654755 -4.5640788 -5.12508 -5.3351007 -5.2789011 -6.0898004 -6.4836669 -6.9325838 -8.0668287 -8.6124392 -9.14716 -8.9844761][-5.74821 -6.0726991 -5.7923474 -4.9599323 -5.2095366 -4.7025504 -4.3382454 -4.2079935 -5.1820855 -6.4251561 -7.4720688 -9.120348 -9.763833 -10.675467 -9.4874973][-6.0860023 -6.9164724 -5.8535547 -4.061162 -3.9692109 -2.6641243 -1.2044373 -1.5090022 -2.6321502 -4.0488987 -6.3651338 -9.0168381 -9.9901562 -11.445615 -11.005735][-7.6871142 -7.4902282 -5.1774325 -3.2732427 -1.9554408 1.1415672 3.5719151 3.2682509 1.8468413 -1.5758777 -5.6031008 -7.6418209 -8.8163729 -10.296457 -9.3138371][-10.597828 -9.343914 -5.2043686 -2.1167142 -1.0812626 2.2767749 6.3384891 7.1409183 5.8047633 0.98555374 -4.145586 -6.8615074 -7.9239769 -9.0208483 -7.39011][-12.307516 -10.413636 -6.9298105 -2.5665407 0.87646818 3.7367864 6.1224704 7.5014348 8.4304562 4.0779862 -1.4814892 -5.2875905 -7.5935473 -8.5856066 -7.3767233][-10.457499 -9.3103714 -6.7996564 -3.3354869 -0.7481041 1.9443836 4.0792804 4.8425794 5.4040151 2.6606774 -0.67817831 -4.848732 -7.9732637 -9.5574722 -9.2255354][-10.518164 -8.6079054 -6.196579 -2.9576402 -1.5777497 -0.58570337 0.69610739 0.75104332 0.66062784 -0.81680155 -3.0082078 -6.5731297 -9.1291323 -11.447021 -11.073545][-12.892655 -11.669363 -9.3146563 -6.449193 -5.9014049 -5.5791516 -4.9175482 -4.4666042 -4.9064479 -5.9943142 -8.1064138 -10.74107 -12.265292 -14.057812 -13.572355][-14.506203 -14.489895 -13.667183 -11.880035 -11.342148 -10.581078 -10.527113 -10.653042 -10.393359 -11.167713 -12.433153 -13.435974 -13.210613 -13.411141 -12.391407][-14.670818 -13.321886 -11.573887 -10.32227 -9.962183 -9.9940033 -10.203253 -10.33868 -10.503036 -10.919797 -11.49398 -12.826898 -13.396103 -12.059399 -9.1997528][-11.166964 -11.329131 -10.803946 -8.4920034 -7.0066738 -6.9733934 -7.6614614 -7.6806135 -7.9083543 -8.4344511 -9.2675438 -9.5507174 -9.9974356 -8.5667019 -7.1064997][-8.2096777 -8.34172 -9.3212852 -7.95516 -5.8396573 -4.4599471 -4.3664875 -4.7065573 -5.135354 -5.28119 -4.9036975 -5.505928 -5.6217451 -5.3720675 -5.7192836]]...]
INFO - root - 2017-12-15 20:59:01.042424: step 57910, loss = 0.11, batch loss = 0.07 (11.5 examples/sec; 0.694 sec/batch; 52h:55m:25s remains)
INFO - root - 2017-12-15 20:59:07.634937: step 57920, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 50h:40m:11s remains)
INFO - root - 2017-12-15 20:59:14.290300: step 57930, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 49h:55m:32s remains)
INFO - root - 2017-12-15 20:59:20.851940: step 57940, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 50h:31m:12s remains)
INFO - root - 2017-12-15 20:59:27.470980: step 57950, loss = 0.23, batch loss = 0.18 (12.0 examples/sec; 0.668 sec/batch; 50h:57m:57s remains)
INFO - root - 2017-12-15 20:59:34.083700: step 57960, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 49h:55m:49s remains)
INFO - root - 2017-12-15 20:59:40.753569: step 57970, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 51h:16m:54s remains)
INFO - root - 2017-12-15 20:59:47.439346: step 57980, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 49h:15m:37s remains)
INFO - root - 2017-12-15 20:59:54.145319: step 57990, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 51h:08m:59s remains)
INFO - root - 2017-12-15 21:00:00.759447: step 58000, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 49h:37m:06s remains)
2017-12-15 21:00:01.279535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9207745 -7.8807678 -9.4999466 -8.330431 -7.0043511 -5.8263283 -5.1382828 -5.1094623 -5.0828724 -5.6536293 -5.1453805 -5.1747408 -7.4661527 -10.343199 -10.815138][-3.4251766 -5.6736608 -7.3616004 -7.2122021 -5.5776987 -4.1772289 -4.4251294 -4.5144248 -4.6833839 -4.6108694 -3.8662446 -5.3879285 -8.1274157 -11.23225 -12.038096][-3.0172908 -4.2278256 -6.5329022 -6.6808481 -5.9259257 -5.1688638 -4.50775 -3.560823 -4.1110754 -4.2332749 -3.090306 -3.4817114 -5.9966488 -9.7838821 -11.800406][-4.0327578 -6.0352855 -7.3541536 -7.9852285 -7.736495 -6.3885751 -5.0886245 -4.0017843 -2.8527071 -2.7491369 -2.1331499 -3.1454208 -5.0264506 -8.0348072 -9.3948593][-4.0447679 -7.4727211 -9.48075 -9.1361961 -6.6507707 -3.8385944 -2.7526636 -3.2312629 -3.2347839 -2.2476745 -1.9597816 -2.7355564 -5.2152853 -8.8865719 -10.125608][-4.9070778 -5.6251392 -8.2295408 -8.100132 -6.59836 -1.8125217 1.2033978 1.0330744 -0.20093012 -0.34883833 -0.69229364 -1.2617292 -3.7651839 -8.1684971 -10.159282][-4.3222671 -5.2706866 -6.1781211 -4.6897907 -2.996134 -0.017751694 2.6786132 4.3111444 4.60803 2.17701 0.8027854 -0.048527241 -3.2091157 -6.903244 -8.3546505][-2.2216816 -3.0805628 -3.1109529 -0.80041075 -0.1680336 2.0798483 3.3842511 4.3925776 4.5229058 3.6123919 2.1820755 -0.70684052 -3.8264856 -6.7089887 -8.389061][-1.1293025 -2.1314969 -2.0751331 0.44306564 1.0771995 2.0367951 3.0137105 3.4233575 2.0115371 1.5247083 1.4845262 0.033102036 -3.2079356 -7.5060992 -8.7841587][-3.287575 -2.7643275 -3.3210943 -2.2634878 -1.3866501 0.60720873 1.6232252 1.210144 0.10531664 -0.18253374 -0.15359068 -1.5302658 -3.7011175 -7.0525937 -8.3557529][-4.4631186 -5.7049274 -5.9286604 -6.2903681 -6.2414031 -4.7366581 -3.5753036 -2.3427305 -2.6802349 -2.2899294 -2.034513 -3.6294353 -5.613193 -7.7811928 -9.0537224][-8.675909 -8.81936 -8.705677 -7.8900681 -7.4128437 -8.4187689 -7.1076651 -5.9303379 -6.4474835 -5.9811211 -5.6008983 -6.4047947 -7.4157743 -9.4416637 -9.966177][-12.185619 -10.93862 -10.209263 -9.7497368 -9.902833 -9.4096193 -9.852478 -9.7641773 -8.4279079 -8.2971153 -8.0919809 -8.8968353 -9.1263075 -9.6755581 -10.192754][-10.043022 -9.50075 -8.2751751 -9.4496794 -8.8100719 -7.9789715 -7.9757357 -8.0769253 -8.345665 -7.9452338 -7.7738256 -7.8163886 -7.7000208 -8.0389862 -7.7051721][-6.2982392 -6.3897138 -6.1836042 -5.94871 -5.7663646 -5.9630179 -6.5010448 -6.525908 -6.6078687 -6.9194093 -6.8279629 -6.3753886 -7.3377337 -8.2620544 -9.0912771]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 21:00:07.945770: step 58010, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.665 sec/batch; 50h:44m:21s remains)
INFO - root - 2017-12-15 21:00:14.593065: step 58020, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 50h:55m:58s remains)
INFO - root - 2017-12-15 21:00:21.223713: step 58030, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 51h:35m:52s remains)
INFO - root - 2017-12-15 21:00:27.878544: step 58040, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.687 sec/batch; 52h:23m:23s remains)
INFO - root - 2017-12-15 21:00:34.506329: step 58050, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 50h:27m:54s remains)
INFO - root - 2017-12-15 21:00:41.091853: step 58060, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 51h:44m:43s remains)
INFO - root - 2017-12-15 21:00:47.641462: step 58070, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 50h:20m:53s remains)
INFO - root - 2017-12-15 21:00:54.262564: step 58080, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 50h:34m:52s remains)
INFO - root - 2017-12-15 21:01:00.839682: step 58090, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 50h:16m:30s remains)
INFO - root - 2017-12-15 21:01:07.433299: step 58100, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 50h:26m:24s remains)
2017-12-15 21:01:08.082193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0576129 -4.9793358 -3.3680725 -2.3260834 -2.7102087 -3.8249285 -5.0305247 -5.3379235 -5.0685024 -4.8875146 -3.754477 -3.8016629 -5.0845122 -4.5337138 -4.1308889][-4.3683658 -3.121402 -1.7976477 -0.80315781 -0.99117756 -2.0843368 -2.9989588 -3.2450097 -2.5987992 -1.218214 0.073321819 -0.94716358 -2.7856677 -2.972199 -3.3625937][-1.5639877 -1.5485325 -0.96050787 0.064546108 -0.37552118 -1.1071668 -1.8203883 -2.0787854 -1.5200472 -0.43647051 0.91853237 0.37639284 -1.1661716 -2.1996574 -3.0581048][-2.855341 -2.3776419 -1.3422356 -0.90159512 -1.3693376 -1.2228746 -1.2177448 -1.4049406 -1.1723495 -0.48782587 0.05632782 -1.1781549 -3.1476359 -3.7097044 -3.8631115][-3.4319241 -3.725152 -3.3248589 -1.5467505 -0.590199 1.1004152 1.8329124 1.0905991 0.52124643 0.40211296 0.24664497 -1.3187294 -3.6920652 -4.7604151 -5.2820554][-5.5389137 -4.9673147 -3.0322385 -0.17820787 2.0925241 4.1015792 5.2190795 4.9822164 4.3751283 2.2949624 0.030063152 -1.4340591 -3.7763615 -4.8343406 -5.3259196][-8.7726307 -7.1612515 -3.8761411 -0.58386326 1.6451635 5.1853709 8.8589172 8.4869022 7.1306863 4.5911756 1.5865941 -1.5167232 -4.8029237 -5.4768591 -5.6112504][-9.674428 -8.250824 -6.3991952 -2.9110379 0.27896357 4.3893294 7.67489 7.3033881 6.9916253 4.1976581 0.75042009 -2.4060266 -5.5593758 -5.9202576 -6.2417326][-9.4979677 -8.5534191 -7.1224394 -4.0389218 -1.2248516 1.2797823 3.7899079 5.2302594 5.6244187 2.3799787 -0.64777088 -3.8233776 -7.8468466 -7.9943666 -6.6526542][-9.2307129 -8.3273888 -7.1551208 -5.1403 -3.0781102 -1.4901543 0.77519035 2.3808608 1.8641982 -0.34139919 -2.1647472 -4.8625569 -7.8485022 -8.57187 -8.6303473][-13.616031 -13.277296 -11.080143 -8.9451323 -7.8834443 -6.419363 -4.733613 -3.9763458 -3.6438031 -3.5856605 -4.7631154 -8.0433187 -10.017746 -9.4615526 -8.34576][-16.516428 -15.713493 -13.965548 -11.814847 -10.915938 -9.53642 -8.0933924 -7.5762248 -7.1966238 -7.2692204 -8.0606079 -8.65826 -8.8468838 -8.3820248 -7.6494846][-13.853031 -12.03109 -10.510442 -10.339384 -10.006329 -9.4766216 -9.2409611 -8.4769354 -8.1590719 -7.6505084 -7.5050383 -7.9373579 -7.957448 -6.1145725 -4.4269342][-10.774673 -10.074781 -8.7531719 -7.6270404 -7.1097403 -7.9224825 -8.1337738 -7.2335806 -7.0969391 -7.6254625 -8.0291481 -7.1164389 -6.3870449 -5.8231359 -5.1938887][-7.1948094 -6.6233087 -6.2281094 -5.3931847 -4.5202742 -3.6969936 -3.0633342 -4.2283258 -4.725698 -4.3402476 -4.6751409 -5.8539257 -6.6107597 -6.4627972 -6.7096429]]...]
INFO - root - 2017-12-15 21:01:14.643198: step 58110, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.675 sec/batch; 51h:28m:30s remains)
INFO - root - 2017-12-15 21:01:21.279943: step 58120, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 51h:00m:35s remains)
INFO - root - 2017-12-15 21:01:27.829479: step 58130, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 48h:53m:22s remains)
INFO - root - 2017-12-15 21:01:34.536398: step 58140, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 51h:20m:08s remains)
INFO - root - 2017-12-15 21:01:41.197814: step 58150, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 50h:08m:22s remains)
INFO - root - 2017-12-15 21:01:47.895799: step 58160, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.688 sec/batch; 52h:24m:52s remains)
INFO - root - 2017-12-15 21:01:54.487197: step 58170, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.679 sec/batch; 51h:44m:48s remains)
INFO - root - 2017-12-15 21:02:01.057129: step 58180, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 50h:40m:43s remains)
INFO - root - 2017-12-15 21:02:07.559794: step 58190, loss = 0.13, batch loss = 0.08 (12.7 examples/sec; 0.632 sec/batch; 48h:11m:12s remains)
INFO - root - 2017-12-15 21:02:14.188906: step 58200, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.683 sec/batch; 52h:02m:07s remains)
2017-12-15 21:02:14.714322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.416595 -5.7673435 -6.7900028 -6.47858 -6.2966771 -6.718698 -5.9894543 -5.1205764 -4.06738 -3.3858521 -2.7759202 -5.5753055 -7.9064693 -9.6343842 -11.211544][-2.5852842 -3.3383586 -3.3315189 -3.2899809 -3.8451374 -4.9828138 -5.4274955 -4.7711239 -3.6007836 -2.4423182 -1.4584169 -3.8801496 -5.5858636 -9.4467993 -12.634411][-0.97147322 -1.9613192 -2.7160468 -3.4237523 -3.9855981 -4.5335364 -4.13048 -3.4321702 -2.213264 -1.3069634 -1.0895305 -4.1334581 -6.652174 -9.6690607 -12.953087][-1.522409 -2.6365032 -3.0213411 -3.7456355 -4.4220095 -4.0622935 -3.4383552 -3.1748486 -2.2856796 -1.3714542 -1.4390211 -4.43849 -6.767241 -9.8822689 -13.190575][-2.3029501 -3.6488819 -4.6918125 -5.0835972 -4.6570764 -4.0365772 -1.6210418 -1.5177379 -1.9482796 -1.7474754 -1.4889646 -4.6127954 -6.002943 -8.4933729 -12.227341][-2.9063377 -4.1134181 -5.0295172 -4.398303 -3.5520706 -1.9776008 1.2522621 1.6012759 0.4072094 -0.59062386 -1.3906336 -4.4459248 -5.9083509 -7.8157053 -10.88377][-4.3275928 -4.5196576 -5.0799575 -3.8166196 -1.8549585 0.32010603 3.4114661 4.3577733 4.9570355 3.138207 0.23222494 -3.3667769 -5.8358207 -7.7211943 -9.8982525][-3.8414946 -4.5402894 -3.9129477 -2.6624172 -0.30346632 3.3997226 6.6840177 6.8469796 7.651567 5.8053069 2.6408782 -2.5279696 -6.9857306 -8.0986786 -9.9451637][-4.2084765 -3.8928971 -3.8927684 -2.9021409 -0.91720772 2.3598371 5.4492145 6.072145 5.85531 4.2875648 3.3739486 -2.2926543 -6.4062481 -9.0452251 -12.68327][-4.0448804 -4.8282084 -3.6135201 -3.2800577 -3.194577 0.14093256 2.7666736 3.3321452 2.66956 1.5236225 -0.51843309 -5.5367236 -9.275301 -12.136379 -15.842051][-7.6784334 -8.4453907 -8.8841381 -7.7892251 -6.068954 -4.5172491 -2.9374878 -2.1104794 -3.0642538 -4.022944 -4.6662269 -9.0944462 -13.991716 -15.082567 -16.700691][-11.581675 -12.224262 -11.231709 -10.819094 -9.9568539 -8.2084761 -7.4446683 -7.2335095 -7.6310387 -9.0552349 -9.6647167 -11.993742 -13.841413 -15.48702 -16.959763][-15.827881 -15.943209 -15.27107 -14.900099 -14.063389 -12.848996 -12.013704 -11.412542 -12.276838 -13.12547 -12.918931 -13.827089 -13.434519 -13.350449 -12.853018][-14.860199 -15.41762 -14.42926 -15.154068 -14.258169 -13.130385 -12.928167 -12.171952 -11.804251 -10.76152 -11.503626 -12.164919 -10.40699 -11.189586 -11.588467][-12.242706 -12.846479 -11.83206 -10.843175 -10.31637 -11.779995 -12.502821 -12.916475 -12.781466 -11.094952 -9.4553528 -10.147186 -10.896612 -11.102827 -10.915819]]...]
INFO - root - 2017-12-15 21:02:21.337552: step 58210, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 51h:49m:01s remains)
INFO - root - 2017-12-15 21:02:27.850031: step 58220, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 50h:39m:08s remains)
INFO - root - 2017-12-15 21:02:34.391827: step 58230, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 49h:24m:39s remains)
INFO - root - 2017-12-15 21:02:40.982257: step 58240, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 51h:01m:56s remains)
INFO - root - 2017-12-15 21:02:47.578149: step 58250, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 51h:16m:25s remains)
INFO - root - 2017-12-15 21:02:54.212142: step 58260, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 50h:06m:17s remains)
INFO - root - 2017-12-15 21:03:00.842183: step 58270, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 51h:09m:17s remains)
INFO - root - 2017-12-15 21:03:07.417946: step 58280, loss = 0.20, batch loss = 0.16 (12.3 examples/sec; 0.653 sec/batch; 49h:44m:18s remains)
INFO - root - 2017-12-15 21:03:13.996348: step 58290, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 49h:38m:49s remains)
INFO - root - 2017-12-15 21:03:20.593311: step 58300, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 50h:48m:13s remains)
2017-12-15 21:03:21.121614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.1989336 -9.3010511 -9.7782288 -9.1841888 -9.5826635 -9.6689644 -9.1775856 -8.9767294 -8.8151207 -7.5000205 -5.9158154 -5.1663465 -6.0350986 -6.5986352 -6.3099022][-8.2549286 -8.0045023 -7.8396935 -6.8490863 -7.5888791 -7.7002277 -7.6870542 -8.0575771 -8.19424 -6.6504107 -5.0421395 -5.0026407 -5.6311488 -6.473474 -6.9173312][-6.122695 -6.6907244 -6.8275347 -5.4884605 -6.4855838 -6.8812447 -6.731442 -6.5382881 -6.5567083 -6.6609154 -6.3891163 -6.379951 -7.2741547 -8.0039492 -9.0631666][-6.3023458 -6.0511856 -5.8776245 -5.4617867 -6.1064663 -5.4155655 -5.41387 -5.6035333 -5.7169466 -5.4673543 -5.1712961 -6.286428 -7.9108663 -9.237998 -10.27113][-7.8110394 -8.79751 -8.470562 -7.1462288 -6.2046332 -3.2265031 -1.833638 -2.8967488 -4.6302967 -5.0939317 -4.5651369 -5.08379 -6.764286 -8.3558559 -9.1264563][-10.113035 -9.1931486 -8.5182991 -6.0819731 -3.222343 0.43000937 3.197084 2.585382 0.38726425 -2.0106528 -3.2912679 -4.2278633 -5.601233 -6.6874585 -7.4110017][-11.429688 -10.583452 -8.2776985 -3.35862 -1.0574236 2.4339948 6.5768523 6.9046397 4.1958261 -0.4314847 -3.1687584 -4.6636267 -5.9700985 -6.7411318 -6.6630511][-11.986732 -10.22369 -7.0190954 -1.4699059 0.96922636 4.0239253 6.6907325 7.03023 5.7506356 2.2181606 -1.1855378 -5.1637506 -8.5395536 -9.4863758 -9.51184][-9.674222 -8.2128 -5.703681 -2.1340883 -0.37639713 2.5051131 4.2702689 3.8417306 3.0649257 0.88140535 -2.0531511 -6.4419146 -10.121437 -12.000538 -12.472549][-8.1020861 -6.3274484 -4.4604797 -2.1806917 -1.3673301 0.26445055 1.5985999 0.95222187 -0.20765305 -1.0210681 -2.3399603 -6.1377649 -9.8956823 -12.548588 -14.246237][-8.8074951 -8.0859375 -7.4545288 -5.2890425 -4.8265033 -4.5153189 -3.7457108 -3.829576 -4.299448 -4.7288227 -5.3978057 -7.848135 -10.169312 -13.101381 -14.646805][-13.159937 -12.25773 -11.522512 -10.292495 -10.109802 -9.6762629 -9.19987 -9.8539095 -9.869648 -9.6772585 -9.5664272 -10.372387 -11.114162 -13.003746 -14.092667][-15.662683 -15.376997 -14.159601 -12.360736 -11.981554 -11.497749 -11.609003 -11.700515 -12.103594 -12.185966 -10.96266 -10.200455 -10.434097 -10.421572 -10.706888][-14.257944 -13.878639 -12.971693 -10.693474 -9.7097015 -8.9879208 -9.5546675 -9.39159 -9.7410564 -10.047325 -9.83778 -7.9977813 -7.4507556 -7.0040298 -6.4462314][-10.48661 -10.100416 -9.1340008 -7.1168756 -6.1775169 -4.6313868 -5.0352368 -5.3621044 -6.7453661 -6.7733245 -6.8636661 -6.4698682 -6.6768856 -6.27084 -5.8663855]]...]
INFO - root - 2017-12-15 21:03:27.663244: step 58310, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 49h:38m:31s remains)
INFO - root - 2017-12-15 21:03:34.325564: step 58320, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 50h:19m:44s remains)
INFO - root - 2017-12-15 21:03:40.909638: step 58330, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 49h:45m:20s remains)
INFO - root - 2017-12-15 21:03:47.550639: step 58340, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.665 sec/batch; 50h:36m:49s remains)
INFO - root - 2017-12-15 21:03:54.208916: step 58350, loss = 0.24, batch loss = 0.20 (12.0 examples/sec; 0.666 sec/batch; 50h:44m:35s remains)
INFO - root - 2017-12-15 21:04:00.776358: step 58360, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 50h:39m:10s remains)
INFO - root - 2017-12-15 21:04:07.373788: step 58370, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 51h:21m:34s remains)
INFO - root - 2017-12-15 21:04:14.013973: step 58380, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 50h:48m:17s remains)
INFO - root - 2017-12-15 21:04:20.571472: step 58390, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 50h:46m:59s remains)
INFO - root - 2017-12-15 21:04:27.137950: step 58400, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 49h:02m:53s remains)
2017-12-15 21:04:27.686323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4277496 -7.3200617 -8.0030613 -8.5099077 -9.5975761 -10.539665 -11.299292 -11.786421 -11.583674 -11.164871 -10.456688 -10.656147 -9.6119156 -10.427031 -8.5659876][-8.3274422 -8.9738884 -8.8430367 -9.1766338 -10.423035 -11.702096 -12.738785 -13.386311 -13.202388 -12.38959 -11.704545 -12.746099 -12.218651 -13.474419 -11.208042][-7.1267281 -8.6099167 -9.9598017 -10.815083 -12.126965 -13.014516 -14.03661 -13.978785 -13.25292 -12.71165 -12.109915 -13.177842 -12.892557 -14.379671 -12.961952][-8.9319687 -10.041393 -11.861751 -12.40029 -12.492544 -11.426045 -10.877319 -11.29458 -11.150833 -9.893405 -9.3167791 -11.464197 -11.911411 -13.219565 -11.717203][-10.11166 -12.290674 -13.779802 -13.248583 -11.861986 -7.82351 -4.8806887 -6.1705632 -7.6934223 -8.1057434 -8.357585 -9.7050295 -10.601149 -13.031448 -11.770302][-10.013678 -11.671076 -13.195477 -12.40287 -9.558773 -3.1380441 2.8689523 3.1764102 0.87469053 -3.6207805 -6.8937297 -8.0127106 -8.9296 -11.4515 -10.957115][-10.104002 -11.235119 -10.517078 -9.0795059 -7.1643453 -1.2018046 6.0137239 8.7083244 8.3884315 1.2077103 -6.0126009 -8.5649118 -9.2099514 -11.55196 -10.516647][-10.140733 -10.642814 -11.245069 -8.6197348 -4.4164295 1.9220481 7.4061379 9.4970016 11.230904 4.8798938 -3.2716699 -8.4665766 -10.892895 -12.738474 -10.818271][-9.16019 -9.43619 -9.9303331 -8.5734749 -5.7075734 0.0021319389 4.9081588 6.8524461 8.1910057 2.9128413 -3.04968 -7.7169981 -11.466797 -14.91741 -13.62351][-8.0311642 -8.5528765 -10.142964 -9.0983315 -7.7399473 -3.523221 0.56451511 3.392673 3.6625276 -1.0056181 -5.2648125 -9.6107073 -12.170616 -15.8248 -15.897274][-10.622616 -10.674706 -11.105282 -9.2921238 -9.3388443 -7.222168 -5.2550759 -4.44647 -4.3937206 -6.3905516 -8.8795967 -13.01824 -14.566198 -16.617748 -15.234974][-16.298342 -16.040339 -15.961739 -14.181644 -13.039478 -11.202097 -11.108969 -11.190279 -11.815079 -13.21967 -13.890991 -15.087143 -14.140617 -15.228251 -13.649021][-15.721758 -15.844549 -15.370592 -13.457289 -12.838127 -11.680635 -11.941504 -12.569157 -12.978779 -12.440008 -12.058107 -13.423628 -13.526005 -13.06728 -10.446463][-13.025476 -12.72082 -11.991926 -10.303617 -9.7311287 -10.334072 -10.94668 -10.99366 -11.047194 -11.068733 -10.806938 -9.6169033 -8.3280354 -8.61055 -8.1846714][-8.0761395 -8.8445377 -7.2381277 -5.6846333 -4.8523431 -4.8443375 -5.9008408 -7.0155325 -7.0314989 -6.717226 -6.9088936 -8.0209522 -8.4407835 -8.4240952 -7.020524]]...]
INFO - root - 2017-12-15 21:04:34.236474: step 58410, loss = 0.15, batch loss = 0.11 (12.7 examples/sec; 0.631 sec/batch; 48h:01m:41s remains)
INFO - root - 2017-12-15 21:04:40.740248: step 58420, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 48h:49m:03s remains)
INFO - root - 2017-12-15 21:04:47.399386: step 58430, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 52h:02m:05s remains)
INFO - root - 2017-12-15 21:04:54.023475: step 58440, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 49h:53m:20s remains)
INFO - root - 2017-12-15 21:05:00.761876: step 58450, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 49h:11m:26s remains)
INFO - root - 2017-12-15 21:05:07.354501: step 58460, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.682 sec/batch; 51h:56m:40s remains)
INFO - root - 2017-12-15 21:05:13.932497: step 58470, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.633 sec/batch; 48h:09m:30s remains)
INFO - root - 2017-12-15 21:05:20.542353: step 58480, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 51h:32m:55s remains)
INFO - root - 2017-12-15 21:05:27.113340: step 58490, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 50h:57m:32s remains)
INFO - root - 2017-12-15 21:05:33.644793: step 58500, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 50h:45m:52s remains)
2017-12-15 21:05:34.139997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2655687 -6.2557578 -4.6825457 -2.5945344 -2.6113036 -2.840858 -3.6271038 -4.1584167 -4.0820713 -3.5913084 -2.98627 -3.7919755 -3.8778727 -4.5541415 -4.9905958][-5.6231847 -5.0124092 -4.5793972 -3.5896022 -3.4442024 -3.8918095 -4.8771296 -5.57969 -5.4876742 -5.427968 -5.1880145 -6.2579207 -6.5638294 -6.9698353 -6.8108387][-4.444283 -4.4862356 -3.3626375 -2.9675772 -3.6201947 -3.9745808 -4.4729123 -5.3106642 -5.2705956 -5.5618081 -5.4682341 -6.3377008 -6.5368643 -6.9472227 -6.9675217][-3.3089712 -3.4431841 -3.2596147 -3.1086352 -3.1638563 -2.8810539 -2.7171476 -3.3168154 -3.4447513 -3.43098 -3.5483744 -5.4153752 -6.28156 -6.6164937 -6.5913715][-4.0389404 -4.3476257 -3.8151069 -2.4721863 -1.268908 -0.53265905 -0.2521019 -0.47819996 -1.0488973 -0.96913862 -1.2029953 -3.1067843 -4.0566149 -5.2962232 -6.3020849][-5.2783456 -4.8343616 -3.3470979 -1.528522 -0.42466593 1.5406508 2.3418183 1.220552 0.2103219 0.33069277 0.25048351 -1.3169112 -2.2404032 -3.8381038 -4.94839][-6.8162241 -6.3383126 -4.3143177 -1.7297156 -0.20478582 1.8387146 2.5506911 2.3890872 2.6756377 1.9737573 0.83043003 -0.67914915 -1.1561084 -2.6057675 -3.7624993][-4.9985366 -4.6079698 -3.0411367 -0.66841507 0.91621637 2.1862583 2.931335 2.8258414 3.4600015 2.9523921 2.6301055 0.42719841 -0.91352654 -1.9155955 -2.5514431][-4.456471 -3.54493 -1.9901292 0.35128069 1.0888977 1.5468097 2.5685773 2.7239413 2.6254096 2.9640908 3.65947 1.9821334 0.31340742 -1.1129465 -2.2900085][-4.0797224 -3.3535657 -1.9649637 -0.58957481 -0.34433937 0.59771919 1.3557329 1.5839381 2.8283229 3.2080789 2.5937324 0.83755207 0.20000982 -0.50047445 -1.919095][-7.77744 -6.9616771 -5.5823164 -3.8661346 -3.0268462 -2.4485173 -1.9947834 -0.89005518 -0.15797949 -0.029970169 -0.027398109 -1.2360134 -2.1707976 -1.7492075 -1.0614004][-9.8708305 -9.7851763 -9.1258154 -7.5696397 -6.3741274 -5.0515742 -4.4932017 -3.8489113 -3.4625947 -2.9442718 -2.9040451 -3.5920556 -2.9254384 -1.2929168 -0.94333744][-9.9755592 -9.992425 -8.7986832 -7.33078 -6.5749755 -5.2552538 -3.4697855 -2.8552423 -2.8823464 -2.970434 -3.8093808 -4.8210006 -4.3883896 -2.0880587 -0.45302439][-7.6760068 -7.197916 -6.8044591 -5.6316566 -4.485908 -3.6495116 -3.5274076 -3.2312405 -2.9787166 -3.3261583 -3.4820442 -3.3512466 -2.4579184 -1.3402829 -1.0541158][-3.5384779 -3.8912232 -3.8954988 -3.0261536 -2.2350137 -1.3049459 -0.66352129 -0.48726511 -1.6377788 -1.9987206 -1.9738414 -2.4698157 -2.2300947 -2.4565971 -2.9185266]]...]
INFO - root - 2017-12-15 21:05:40.579065: step 58510, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.629 sec/batch; 47h:51m:40s remains)
INFO - root - 2017-12-15 21:05:47.069067: step 58520, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 49h:57m:36s remains)
INFO - root - 2017-12-15 21:05:53.658269: step 58530, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 49h:41m:06s remains)
INFO - root - 2017-12-15 21:06:00.229474: step 58540, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.646 sec/batch; 49h:08m:23s remains)
INFO - root - 2017-12-15 21:06:06.870830: step 58550, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.646 sec/batch; 49h:09m:56s remains)
INFO - root - 2017-12-15 21:06:13.468728: step 58560, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.660 sec/batch; 50h:15m:14s remains)
INFO - root - 2017-12-15 21:06:20.013094: step 58570, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 49h:27m:47s remains)
INFO - root - 2017-12-15 21:06:26.590282: step 58580, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 48h:50m:35s remains)
INFO - root - 2017-12-15 21:06:33.232591: step 58590, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 50h:57m:21s remains)
INFO - root - 2017-12-15 21:06:39.724377: step 58600, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 50h:18m:38s remains)
2017-12-15 21:06:40.254043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9824457 -4.5613971 -3.5727575 -3.1078968 -3.5539033 -3.8824058 -4.1244187 -4.1038857 -3.9815764 -4.1550369 -3.9600954 -5.6188669 -7.0365868 -8.0144005 -7.1095896][-6.3391237 -6.6899171 -6.4774203 -5.8623433 -6.232892 -6.6954465 -6.8868375 -7.1143346 -7.2474837 -7.1567097 -6.9556608 -8.7493229 -9.8663559 -10.367924 -9.3704987][-6.2545996 -6.2683315 -6.0336018 -5.4811773 -6.0313616 -6.7491989 -7.2926679 -7.0443139 -6.8457808 -6.9351087 -6.8491936 -8.3045053 -9.4471092 -10.623556 -10.272248][-6.2106605 -5.8171964 -5.0649467 -4.23657 -4.6571207 -4.9482245 -5.019341 -5.3006606 -5.5101547 -4.8662686 -4.3628273 -6.209146 -7.4716539 -9.2588711 -9.0713043][-8.2482986 -7.9354844 -6.7653017 -4.8606672 -4.0759983 -3.6491835 -3.4363298 -3.8625598 -4.0440216 -3.6238191 -3.4974461 -4.7350373 -5.8553739 -7.8811913 -7.7166939][-9.4534788 -8.4382 -6.1998658 -3.2142391 -1.7366006 -0.39489698 0.80453968 0.85239792 1.0660248 0.31362534 -1.0602446 -2.8856282 -4.3599257 -5.4284582 -5.2063708][-10.606114 -10.269844 -7.8210297 -3.2478833 -0.24838638 2.7999349 4.9094691 5.0323606 5.02529 2.8569207 0.60486174 -1.8332019 -3.89533 -5.5200229 -5.5688477][-9.2396221 -9.2266788 -7.3171239 -4.3086696 -1.581037 2.5298724 4.6253905 4.3166814 5.2131753 3.7521186 1.8038912 -2.2088265 -6.1247044 -7.988698 -7.8557415][-6.3471117 -6.86926 -6.589407 -4.183363 -2.0997503 0.62484646 1.867147 3.565227 4.7203431 2.2804627 1.0928988 -2.487186 -5.8763905 -8.2448063 -9.18639][-4.0975637 -3.4995053 -3.2245557 -2.2300611 -1.1487098 1.108315 1.945817 2.9030833 2.5986342 1.6086664 0.84106779 -2.8286793 -5.3988309 -8.2062511 -10.072426][-4.5694 -4.23383 -3.8119974 -4.0930758 -3.791085 -3.1484234 -2.6889679 -2.0129769 -2.6879427 -2.9452312 -3.4072533 -6.3838453 -7.409163 -9.210125 -8.7519016][-10.339651 -9.4928741 -9.0837994 -8.526577 -8.3838558 -8.6348648 -8.6987514 -9.0944586 -9.48019 -9.1629877 -9.0889559 -9.6628227 -9.1583509 -9.7757549 -8.9281311][-13.165483 -12.308238 -12.370575 -11.65064 -12.106287 -11.73565 -11.260035 -11.050919 -11.19141 -11.090183 -10.528332 -9.0784073 -8.158905 -7.3613725 -6.4051456][-10.944328 -10.123991 -9.74403 -9.2914867 -9.1326113 -9.0526524 -9.4428368 -9.2162895 -8.714571 -8.5027285 -8.50872 -7.1333156 -6.0457392 -4.9735622 -4.8181748][-8.89206 -8.34347 -7.337028 -6.6268764 -5.3595357 -4.35122 -4.1683235 -4.9114046 -5.7696471 -6.0286112 -6.3562517 -7.250638 -7.5212073 -6.8463211 -6.0178409]]...]
INFO - root - 2017-12-15 21:06:46.845414: step 58610, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 51h:00m:17s remains)
INFO - root - 2017-12-15 21:06:53.466623: step 58620, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 50h:26m:42s remains)
INFO - root - 2017-12-15 21:07:00.080619: step 58630, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 50h:52m:08s remains)
INFO - root - 2017-12-15 21:07:06.700778: step 58640, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 51h:12m:52s remains)
INFO - root - 2017-12-15 21:07:13.221351: step 58650, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 49h:12m:31s remains)
INFO - root - 2017-12-15 21:07:19.805208: step 58660, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 50h:14m:00s remains)
INFO - root - 2017-12-15 21:07:26.438923: step 58670, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 49h:57m:51s remains)
INFO - root - 2017-12-15 21:07:33.094854: step 58680, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.687 sec/batch; 52h:13m:06s remains)
INFO - root - 2017-12-15 21:07:39.681526: step 58690, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 50h:05m:35s remains)
INFO - root - 2017-12-15 21:07:46.270439: step 58700, loss = 0.24, batch loss = 0.19 (12.3 examples/sec; 0.653 sec/batch; 49h:38m:36s remains)
2017-12-15 21:07:46.750321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4393573 -6.8119326 -4.723412 -1.3572478 0.3500514 -1.0204697 -3.0164242 -3.985971 -5.4442911 -6.2502432 -6.2574835 -8.4611015 -11.25886 -11.953217 -9.5319767][-6.1328011 -6.1870594 -6.8559632 -4.5340157 -3.1373367 -1.8883739 -1.0938659 -2.2217278 -4.0042634 -5.7762427 -7.8835936 -10.15543 -11.10277 -13.033415 -11.01597][-4.5008907 -6.43769 -7.3910708 -5.1123557 -4.8316231 -3.177037 -1.8983417 -2.3397346 -3.5070975 -4.1994228 -4.8851924 -7.99323 -10.181181 -11.294567 -10.410952][-4.5907216 -6.5461597 -7.7888913 -7.5131555 -7.4957075 -4.6303706 -1.6523829 -0.29225063 -1.5251336 -4.820312 -8.118535 -11.542572 -11.902576 -12.731134 -10.792504][-5.9493589 -7.3260584 -7.9709272 -7.2071838 -5.2894583 -1.8353391 -0.37827396 -1.0130849 -2.1156135 -3.1504464 -5.83191 -10.808711 -13.097809 -14.89603 -13.134065][-7.2679296 -6.1942124 -6.1124849 -5.8447714 -3.25322 2.4775109 7.7258677 7.2155366 2.7592902 -2.5539763 -6.3697352 -9.8053665 -12.261713 -14.687998 -13.667041][-8.375493 -7.9897566 -7.8486328 -4.853126 -1.5971694 4.1643634 8.9149876 10.201612 8.8423252 1.2822347 -5.0440788 -7.3470778 -9.4246788 -12.486586 -12.972447][-8.4772062 -9.9259377 -10.050549 -5.1572504 -0.39703894 6.3263516 11.408562 10.441055 9.0646725 2.7569938 -3.4744432 -8.4827213 -10.468519 -10.828136 -10.087351][-7.743 -9.7871666 -8.8169832 -5.6867809 -3.0006895 3.4145055 7.9678025 8.5520573 6.8926597 -0.30781555 -4.5631137 -9.4246864 -12.499296 -13.557872 -11.684963][-7.0661087 -8.0745678 -7.9496136 -5.7280841 -3.4845862 0.43669748 2.606751 3.6520295 2.0715418 -3.7493148 -9.4435263 -14.555664 -16.275061 -16.048397 -13.881927][-10.027174 -11.442751 -10.06441 -7.8753405 -6.6079717 -4.4485731 -4.1182294 -3.7984619 -5.706017 -8.899435 -11.869024 -15.312767 -18.165375 -18.291752 -15.509199][-14.255054 -14.653738 -14.320484 -11.460518 -7.9012456 -6.5785522 -7.0972848 -8.1638927 -9.9822693 -12.280404 -13.86421 -15.870085 -15.969435 -14.99387 -13.685498][-15.601912 -15.203163 -13.804228 -12.494221 -11.511044 -10.446563 -9.610383 -10.281237 -10.994742 -11.187143 -11.255772 -12.142277 -12.773264 -11.611926 -9.4275694][-11.939282 -11.001365 -11.295491 -11.114239 -10.76451 -10.016641 -9.6297932 -9.7339573 -9.9932489 -10.312714 -10.339981 -8.2957191 -8.00467 -7.3809366 -7.4449887][-6.1885347 -6.4568906 -5.1892982 -4.6246324 -6.3199649 -7.7779245 -8.07243 -6.4163465 -6.2754421 -6.8310804 -6.6620731 -7.059413 -8.429678 -7.4743624 -7.132154]]...]
INFO - root - 2017-12-15 21:07:53.256078: step 58710, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 50h:43m:14s remains)
INFO - root - 2017-12-15 21:07:59.907756: step 58720, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.686 sec/batch; 52h:09m:11s remains)
INFO - root - 2017-12-15 21:08:06.477217: step 58730, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 50h:35m:59s remains)
INFO - root - 2017-12-15 21:08:13.119647: step 58740, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 51h:59m:28s remains)
INFO - root - 2017-12-15 21:08:19.613569: step 58750, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 50h:06m:17s remains)
INFO - root - 2017-12-15 21:08:26.221426: step 58760, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 49h:22m:05s remains)
INFO - root - 2017-12-15 21:08:32.838295: step 58770, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 49h:59m:06s remains)
INFO - root - 2017-12-15 21:08:39.531490: step 58780, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 50h:37m:50s remains)
INFO - root - 2017-12-15 21:08:46.103349: step 58790, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.661 sec/batch; 50h:14m:13s remains)
INFO - root - 2017-12-15 21:08:52.775903: step 58800, loss = 0.19, batch loss = 0.15 (11.8 examples/sec; 0.677 sec/batch; 51h:27m:11s remains)
2017-12-15 21:08:53.351234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7090368 -6.416522 -6.0533381 -5.0216546 -4.513216 -4.3704925 -3.3073738 -3.4271696 -2.3824446 0.92527008 2.731277 1.2708583 -1.2998109 -4.5134335 -6.2288885][-5.2951565 -4.5467787 -4.3441138 -2.4511235 -1.1295276 -0.80071735 -0.76551723 -0.56720877 -0.67830849 -0.68331575 -0.1310339 -1.1434846 -3.8471603 -6.2231407 -6.7613239][-2.6735165 -2.841059 -3.3230584 -2.3472764 -2.2446473 -1.324492 -0.14443731 0.66100407 1.1092219 1.3866353 -0.23263836 -3.3262224 -6.1289477 -7.6251369 -7.857666][-4.0407 -3.9352226 -2.7153671 -2.4421878 -2.040139 -1.5888667 -0.9624548 0.37905025 1.1224413 0.86748266 0.55938625 -2.3854847 -5.7627611 -8.1119595 -8.9194183][-2.473592 -2.3437839 -2.431905 -1.0339689 -1.0434327 -0.91624212 -0.34193134 0.60932541 1.6611624 1.3683686 0.50015259 -1.9615922 -4.79967 -7.3035502 -8.6777277][-3.260555 -2.9245589 -1.7437401 -0.23908663 0.62862349 1.620223 1.5104055 1.0041118 0.93514681 0.51111364 0.44722319 -1.7871389 -4.2256389 -6.8510218 -7.8766618][-4.5156198 -3.0622132 -1.4170084 1.1469078 2.3852162 3.1908412 3.3594508 3.4213319 2.6187224 1.0487251 0.50388622 -0.74453449 -2.5304761 -5.3134208 -6.4704695][-4.9533386 -3.4043169 -0.86496449 0.98881721 2.23202 3.9663138 4.8048835 4.781425 3.7914062 2.5129189 1.3310061 -1.5090632 -3.9565914 -5.1270709 -4.4448185][-4.9695458 -3.7364962 -2.19674 0.1063695 1.5194273 3.0589604 3.7341285 4.085485 3.168798 1.6487818 0.58260679 -1.3202381 -3.6650376 -5.0866022 -4.91764][-4.7718477 -4.9111881 -3.6843402 -2.2010484 -0.9587841 0.068410873 1.1272035 1.2941589 0.67952108 -0.28842068 -1.5585055 -3.7023067 -5.5780096 -6.2571192 -5.8238106][-9.6744394 -9.2246208 -8.40741 -6.1717 -4.9606667 -4.7027655 -4.439405 -5.14599 -5.8202043 -6.0179381 -6.037044 -6.7700834 -7.5235691 -7.9138103 -7.6937332][-12.26792 -11.637942 -10.034832 -8.1552591 -8.0970736 -7.6476216 -7.4836164 -8.6056242 -9.9404335 -10.201118 -9.6011028 -9.065485 -8.7659559 -8.8908768 -8.2045383][-12.707983 -11.830574 -10.273806 -8.5987682 -8.1140575 -8.04194 -8.4946156 -9.6886873 -11.703176 -12.779505 -12.821962 -10.749693 -9.042881 -8.5278111 -8.1208506][-10.517762 -9.8006592 -8.5539064 -7.4109726 -7.3674512 -8.11452 -8.0327415 -8.81996 -9.2259531 -9.64646 -10.02964 -8.1064854 -6.5795135 -6.4178877 -6.4930272][-8.1376629 -8.2391758 -7.9031134 -6.3512087 -5.2893281 -4.929904 -4.6076822 -5.6879549 -6.2757869 -6.9718356 -7.4075594 -7.9409132 -7.7323947 -7.04285 -6.8094292]]...]
INFO - root - 2017-12-15 21:08:59.889361: step 58810, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 50h:39m:29s remains)
INFO - root - 2017-12-15 21:09:06.660568: step 58820, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 49h:59m:59s remains)
INFO - root - 2017-12-15 21:09:13.229834: step 58830, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 49h:20m:30s remains)
INFO - root - 2017-12-15 21:09:19.891706: step 58840, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.681 sec/batch; 51h:48m:14s remains)
INFO - root - 2017-12-15 21:09:26.479385: step 58850, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 51h:12m:41s remains)
INFO - root - 2017-12-15 21:09:33.070053: step 58860, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.671 sec/batch; 50h:58m:35s remains)
INFO - root - 2017-12-15 21:09:39.580397: step 58870, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.640 sec/batch; 48h:37m:20s remains)
INFO - root - 2017-12-15 21:09:46.180140: step 58880, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 50h:51m:31s remains)
INFO - root - 2017-12-15 21:09:52.742159: step 58890, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 49h:45m:09s remains)
INFO - root - 2017-12-15 21:09:59.353080: step 58900, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 49h:00m:35s remains)
2017-12-15 21:09:59.867251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9154143 -6.3593459 -6.8448443 -7.8782711 -8.7890072 -8.6881275 -8.22049 -7.57609 -6.8192172 -6.7860804 -6.2157135 -7.8018093 -8.8422508 -9.5297585 -8.9434109][-7.286088 -7.1811566 -7.2797055 -7.7619677 -8.768095 -9.3222265 -9.59945 -9.1249771 -8.0357437 -8.3800488 -8.0801306 -9.1478968 -10.185936 -11.625208 -10.87002][-6.0077496 -7.2814016 -8.3011971 -8.776329 -9.9417381 -9.8195076 -8.8660011 -8.0199528 -7.3258457 -8.0601349 -9.3216238 -9.3985 -9.84144 -10.795843 -10.14744][-6.5394611 -6.8887324 -6.719501 -6.856195 -7.4614587 -6.5976448 -6.0731673 -6.6378436 -6.9743509 -7.5693469 -7.9255323 -9.2319145 -10.832636 -11.049931 -10.268551][-6.2837553 -7.2809038 -6.6438789 -5.2125878 -4.2547393 -2.3601878 -1.1937017 -2.5940828 -4.4899015 -5.5581651 -6.6740122 -8.2337246 -9.3667316 -10.755836 -10.405548][-6.7202806 -7.3599453 -6.0052052 -3.4889572 -1.9780354 1.2395558 3.4427142 1.8356056 -0.22111177 -3.2670281 -5.2425923 -6.2887788 -7.6338797 -9.3042145 -9.535162][-7.452497 -8.2618265 -6.9612246 -3.8716168 -1.0074205 2.3915238 4.7356324 5.4911828 4.909493 0.99104977 -2.5416896 -5.2108788 -7.6167517 -9.3087444 -8.6062145][-7.3161359 -7.5464239 -6.6348391 -2.9952292 0.66465235 4.3524861 7.2317634 7.1283584 5.4550958 2.9428411 0.95682812 -3.0026307 -6.0837464 -7.4328089 -7.200057][-5.0650358 -5.3303528 -4.04606 -2.3258588 -0.50106907 2.9765468 5.636807 6.6229911 5.7419572 3.6512294 1.9598007 -1.1597056 -4.0583429 -7.3343983 -7.1508865][-2.6499496 -3.0721414 -2.5301297 -0.68431282 0.3270998 1.6684594 2.1974888 2.5209708 1.7859092 0.70977449 -0.75214338 -2.4538989 -3.8851471 -5.8240814 -6.8814793][-6.6677046 -6.2377591 -4.9658875 -3.5070546 -2.3155789 -1.8321764 -2.5426974 -2.5323563 -2.789283 -3.0435731 -3.9460473 -6.4400291 -9.0398121 -8.703351 -7.4400616][-10.098598 -10.383966 -9.7507267 -8.1594772 -6.7157927 -6.2489762 -7.0256596 -8.1809273 -9.1402178 -9.1449451 -8.6188412 -10.11304 -10.523174 -9.4274759 -8.8807011][-14.1373 -13.716734 -11.433588 -11.288404 -10.293919 -8.9241333 -9.4995222 -11.304212 -12.224032 -11.969233 -11.342828 -11.116312 -11.167608 -9.995451 -8.0312767][-12.988111 -13.437313 -13.465107 -11.664374 -9.3879986 -9.0540447 -10.896006 -11.828194 -10.91403 -10.950186 -10.006439 -9.1224117 -8.187995 -7.5031462 -7.4164991][-10.853042 -12.052616 -11.414124 -9.1725521 -7.3222294 -7.6792035 -7.8196588 -7.1445475 -7.6303749 -6.9079266 -6.0807242 -6.3827186 -7.0499434 -6.1529374 -5.5076542]]...]
INFO - root - 2017-12-15 21:10:06.455524: step 58910, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 49h:50m:34s remains)
INFO - root - 2017-12-15 21:10:13.059546: step 58920, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 49h:30m:40s remains)
INFO - root - 2017-12-15 21:10:19.649432: step 58930, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.653 sec/batch; 49h:39m:13s remains)
INFO - root - 2017-12-15 21:10:26.193358: step 58940, loss = 0.12, batch loss = 0.07 (12.7 examples/sec; 0.630 sec/batch; 47h:54m:37s remains)
INFO - root - 2017-12-15 21:10:32.774122: step 58950, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 49h:57m:03s remains)
INFO - root - 2017-12-15 21:10:39.379198: step 58960, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 48h:29m:23s remains)
INFO - root - 2017-12-15 21:10:46.009148: step 58970, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 50h:49m:45s remains)
INFO - root - 2017-12-15 21:10:52.570890: step 58980, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.695 sec/batch; 52h:46m:30s remains)
INFO - root - 2017-12-15 21:10:59.147464: step 58990, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 50h:49m:36s remains)
INFO - root - 2017-12-15 21:11:05.794859: step 59000, loss = 0.17, batch loss = 0.13 (11.6 examples/sec; 0.692 sec/batch; 52h:36m:38s remains)
2017-12-15 21:11:06.352530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.443305 -9.7219057 -9.74306 -8.42742 -8.37008 -8.2406263 -7.5839872 -7.0785623 -5.2191658 -5.0627928 -5.0598865 -3.7297564 -4.6079941 -7.0515032 -8.5066614][-9.0760994 -9.3628941 -8.7184944 -7.4468465 -7.8090639 -7.6824436 -7.4028378 -6.3334632 -5.1398525 -4.1373181 -3.7121139 -3.8889413 -4.0567307 -6.6380134 -8.5953665][-7.4157743 -8.3696995 -8.7770329 -7.370244 -7.4193726 -7.22301 -7.0304861 -6.26648 -5.1675558 -4.3721037 -3.7196434 -3.696054 -4.5688438 -7.048223 -9.0156918][-6.6944294 -6.4617262 -7.07996 -6.9447947 -6.9622364 -5.952971 -5.8108759 -5.0968676 -4.5842385 -3.4858632 -3.2203693 -3.7944498 -4.5706425 -7.5912285 -9.0454607][-6.7406073 -7.9128294 -7.0628943 -5.2225451 -5.774909 -5.3178091 -4.4458075 -4.1320591 -3.409682 -3.2417662 -3.1038725 -3.7424836 -4.1992741 -7.669467 -10.184442][-8.47249 -7.989068 -6.1801662 -4.6723356 -4.0570664 -3.5439279 -3.7562542 -3.133918 -2.4890046 -2.0991895 -2.5215256 -3.0350366 -4.430666 -7.345037 -9.3275909][-6.8896456 -6.7551804 -5.3651209 -2.9101319 -2.14824 -1.348774 -1.7161174 -1.6834207 -1.0063934 -1.3879051 -2.1059549 -2.7508459 -3.8601663 -6.3172231 -7.80984][-6.7959852 -5.8635693 -3.7136869 -1.5720434 -1.5572391 -1.0016127 -0.91830158 -0.6483202 -0.5263629 0.031400681 -0.5578146 -1.882216 -3.4783461 -5.907443 -7.6694098][-5.8346848 -5.3324308 -3.1296554 -1.0405445 -0.69776344 -0.41424513 -0.70576334 -1.1341572 -0.58538532 -0.65744066 -0.11162472 -1.0113611 -2.9029911 -5.902287 -8.119462][-3.5792909 -3.719636 -2.8338032 0.11830997 -0.2611804 -1.0314159 -1.5491595 -1.6450491 -1.9617977 -1.6207976 -2.1486115 -2.0330741 -2.080246 -5.1513662 -8.0738869][-5.283361 -3.7187581 -3.8377161 -2.6197579 -2.7747381 -3.5608222 -4.2119331 -4.7939286 -4.3281212 -4.2988548 -4.2659 -4.9298105 -4.9031954 -6.1432152 -7.0062971][-8.2670164 -5.9785194 -4.0663533 -3.6426125 -4.9996276 -6.0308123 -6.1437683 -6.1348391 -6.4203892 -6.6943564 -7.3719835 -7.0814815 -7.6778049 -8.7876148 -8.288578][-9.2475252 -8.1637487 -6.4125381 -5.5346465 -6.4889917 -6.9720821 -5.8757763 -5.8269482 -6.1543927 -7.0631394 -8.0916748 -8.399601 -8.1356506 -8.9150314 -9.4041147][-9.735652 -8.5883179 -7.7108021 -6.9213181 -5.71371 -6.6449924 -6.9525366 -6.6733756 -4.9040375 -6.0992432 -6.8819027 -7.35409 -7.0201035 -7.2597685 -9.1700573][-7.1945448 -8.1437035 -8.1260118 -5.9500737 -5.3367763 -5.1553407 -5.3084641 -5.4066191 -4.8167133 -5.0212283 -5.9678826 -6.7059493 -7.4435005 -7.4885588 -8.3944988]]...]
INFO - root - 2017-12-15 21:11:13.088789: step 59010, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 50h:53m:03s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 21:11:19.761633: step 59020, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 49h:45m:48s remains)
INFO - root - 2017-12-15 21:11:26.283181: step 59030, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 48h:42m:27s remains)
INFO - root - 2017-12-15 21:11:32.771438: step 59040, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 50h:03m:02s remains)
INFO - root - 2017-12-15 21:11:39.371873: step 59050, loss = 0.16, batch loss = 0.12 (11.5 examples/sec; 0.695 sec/batch; 52h:46m:40s remains)
INFO - root - 2017-12-15 21:11:45.903522: step 59060, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.647 sec/batch; 49h:10m:01s remains)
INFO - root - 2017-12-15 21:11:52.374622: step 59070, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 50h:38m:31s remains)
INFO - root - 2017-12-15 21:11:58.987702: step 59080, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 50h:31m:04s remains)
INFO - root - 2017-12-15 21:12:05.581466: step 59090, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 49h:52m:10s remains)
INFO - root - 2017-12-15 21:12:12.208226: step 59100, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 50h:05m:11s remains)
2017-12-15 21:12:12.704659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3368058 -5.8778749 -5.021822 -4.2124376 -4.668303 -5.2236414 -5.5770812 -6.2451067 -6.4808826 -6.8523064 -6.7070246 -6.3943939 -8.2845364 -9.3766479 -8.4524851][-6.0156231 -5.4541774 -4.237905 -3.6300781 -4.422626 -5.3856506 -6.3173261 -7.1127253 -7.3405514 -7.7687836 -7.9280024 -7.5954213 -8.4164829 -9.6812325 -9.39719][-4.0919757 -4.1532927 -4.1055636 -3.7599812 -4.4636707 -5.0355539 -5.2762175 -5.7772121 -6.078229 -6.4616385 -6.7841244 -6.5784535 -8.0768871 -9.1570024 -8.3223991][-4.4306016 -4.6549392 -4.1465735 -4.0331354 -4.7759924 -4.1275167 -3.6062646 -4.1763644 -4.90137 -5.1674752 -5.184484 -4.8824811 -6.6868243 -8.1910954 -8.2232361][-4.8042483 -5.5973716 -5.383389 -4.6416225 -4.5161438 -2.2392724 -0.59944487 -1.5740728 -3.0721741 -3.5055473 -3.8054752 -3.8819895 -5.5282955 -7.3128433 -7.7594252][-6.8033051 -6.2325506 -4.80792 -3.8457921 -3.1407566 -0.09791851 2.3375 2.4523044 1.1444187 -0.9666667 -2.4972553 -2.4201794 -4.6054916 -6.8908606 -7.3958068][-8.0504818 -7.4299374 -5.8808823 -4.0990667 -2.393136 1.4271407 5.2051721 5.5903945 4.2753692 1.388114 -2.0031292 -2.4877019 -4.4626851 -6.0109653 -5.75787][-7.6886683 -7.0282207 -5.5971336 -2.7553792 -0.42148113 2.1955276 5.2585368 6.389771 5.6960263 2.1910305 -1.7945516 -2.3179939 -4.336391 -5.7239809 -5.2944655][-6.1881585 -6.1085014 -5.3068047 -2.7210872 -0.28463745 1.7793598 3.5513749 4.4998584 4.5830646 1.5848684 -1.9258065 -2.9365377 -5.6515279 -6.3992419 -4.8922839][-4.7696748 -5.2535996 -4.7028456 -3.3669872 -2.4283504 -0.72044611 1.8444643 2.9686952 2.0954065 -0.391232 -2.8873041 -3.2444186 -4.843399 -6.1107979 -5.8603][-7.8536348 -7.6585312 -7.2574596 -6.3544011 -5.4630527 -4.7665467 -3.8556488 -2.6340148 -2.2704954 -3.2633874 -5.277144 -5.6878414 -6.9741516 -7.5320649 -6.455873][-9.91523 -9.7723007 -9.5774612 -8.6299515 -7.4649849 -6.5260129 -6.2257919 -5.8555174 -5.1638126 -5.3869982 -6.468133 -6.0463018 -6.8346243 -7.7128625 -7.2686744][-9.2941093 -8.3133163 -7.7173219 -7.8036537 -7.8971782 -7.4087977 -6.7081671 -6.1634159 -5.871366 -5.7056637 -5.7380013 -5.609015 -6.1534982 -6.4316883 -6.3191977][-7.8029842 -7.4519243 -7.1968136 -6.9413977 -6.7450542 -6.5101495 -6.7493591 -6.1285515 -5.2563839 -5.3450341 -5.5195084 -4.8420315 -5.108108 -5.5290575 -5.72095][-6.4596744 -5.1988611 -5.347671 -5.9576054 -5.3059268 -4.7456651 -4.4506412 -4.3066654 -4.63865 -4.3040323 -4.3495636 -4.6886134 -5.4875259 -6.5422125 -7.3707242]]...]
INFO - root - 2017-12-15 21:12:19.370858: step 59110, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 50h:39m:09s remains)
INFO - root - 2017-12-15 21:12:26.013926: step 59120, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 51h:21m:40s remains)
INFO - root - 2017-12-15 21:12:32.686159: step 59130, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 49h:46m:50s remains)
INFO - root - 2017-12-15 21:12:39.309337: step 59140, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 51h:29m:17s remains)
INFO - root - 2017-12-15 21:12:45.888698: step 59150, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 48h:48m:43s remains)
INFO - root - 2017-12-15 21:12:52.469077: step 59160, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 50h:07m:54s remains)
INFO - root - 2017-12-15 21:12:59.119921: step 59170, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 50h:14m:37s remains)
INFO - root - 2017-12-15 21:13:05.657282: step 59180, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 50h:35m:46s remains)
INFO - root - 2017-12-15 21:13:12.231367: step 59190, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 50h:27m:02s remains)
INFO - root - 2017-12-15 21:13:18.737270: step 59200, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 49h:53m:25s remains)
2017-12-15 21:13:19.248977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8634958 -7.5480156 -6.5363064 -5.5476937 -4.9859948 -4.8625288 -5.4190631 -5.5846543 -5.1537833 -3.9697528 -3.6273739 -6.1894531 -7.2324796 -8.5843725 -8.1836519][-5.3737192 -4.1195869 -2.8859711 -2.4051604 -1.9404771 -1.9424808 -1.7584932 -1.7677329 -2.3810008 -2.9579835 -3.0044768 -4.7878466 -6.2272792 -8.0612888 -7.3981433][-2.3395011 -2.8276896 -2.1133142 -0.58470392 -0.58973026 -0.18245792 0.2456708 -0.50652122 -0.85608339 -1.0745006 -1.968255 -4.3619852 -4.8577404 -6.7888203 -7.0217676][-1.4833155 -1.3520017 -0.76310158 -0.2231431 -0.05612421 0.4271965 0.60156012 0.41083527 -0.20650768 -1.0121713 -1.1394391 -3.7797146 -4.949214 -6.052927 -5.6257839][-1.7637711 -2.3462007 -2.5306785 -1.1195097 -0.600265 0.10697412 0.84128571 0.19863367 -1.2545195 -1.5854487 -1.4659505 -4.4961772 -5.9535713 -7.6753907 -7.787097][-3.7316613 -3.88932 -3.1702642 -1.9151649 -0.77207184 -0.072473049 0.33571148 0.14441061 0.058971882 -0.60528088 -1.2877703 -4.1217513 -5.4401979 -7.3041196 -6.9125714][-6.037395 -5.7180886 -4.0725908 -1.2578459 0.39718628 0.97284317 0.96350622 1.2499304 1.3635111 0.21399021 -1.00036 -3.8492532 -5.29631 -7.801301 -7.9432282][-6.1620216 -5.4653764 -4.0127735 -1.1488352 0.18890476 1.1218281 2.1019778 1.6848655 0.73680019 0.065555096 -0.20950174 -3.8540339 -6.3601351 -9.1333752 -9.1539211][-6.0146246 -5.4625897 -4.4454775 -2.1935098 -1.0547605 0.55642509 1.2185779 0.90634823 1.1935039 0.58610106 -0.31969595 -4.302453 -6.5553851 -9.6592808 -10.161457][-5.2864528 -4.3348494 -3.2435172 -2.074883 -1.3385077 -0.098049641 0.45481968 0.64561605 0.077614784 -0.20761681 0.027655602 -4.0096397 -6.8710604 -10.307286 -11.101645][-7.1511846 -6.032351 -4.7227173 -3.4143279 -3.0211639 -2.3754416 -1.9805391 -1.8331239 -2.5866778 -2.8615119 -2.3673503 -6.3907008 -9.0150576 -11.513805 -11.457516][-10.401844 -8.4883528 -6.6377363 -5.533133 -6.5833387 -6.6585679 -6.3148069 -6.2517152 -6.5169449 -5.9049306 -5.8745484 -8.6584482 -9.7688475 -11.390778 -11.076974][-12.22032 -11.534855 -10.469481 -9.3918943 -9.8002415 -10.367132 -10.098675 -9.6833754 -9.3864594 -9.6426334 -9.8633575 -10.39533 -10.163713 -11.184963 -10.737985][-11.424717 -11.841093 -11.585969 -9.9035978 -8.8332939 -9.7138405 -10.428427 -9.765995 -9.2768841 -9.1780138 -9.0115566 -9.6955528 -10.110481 -10.380079 -9.2826977][-10.762512 -10.660154 -10.34882 -10.646261 -10.754772 -9.5133228 -8.2114029 -8.0367222 -9.1498508 -9.8509521 -9.1262951 -9.1046467 -9.832015 -10.4501 -10.975024]]...]
INFO - root - 2017-12-15 21:13:25.832617: step 59210, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 49h:02m:43s remains)
INFO - root - 2017-12-15 21:13:32.395338: step 59220, loss = 0.16, batch loss = 0.11 (12.7 examples/sec; 0.629 sec/batch; 47h:43m:19s remains)
INFO - root - 2017-12-15 21:13:38.992187: step 59230, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 49h:40m:14s remains)
INFO - root - 2017-12-15 21:13:45.551009: step 59240, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 49h:44m:40s remains)
INFO - root - 2017-12-15 21:13:52.193273: step 59250, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 50h:31m:36s remains)
INFO - root - 2017-12-15 21:13:58.737719: step 59260, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 50h:13m:14s remains)
INFO - root - 2017-12-15 21:14:05.311503: step 59270, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 50h:23m:40s remains)
INFO - root - 2017-12-15 21:14:11.923623: step 59280, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 50h:14m:02s remains)
INFO - root - 2017-12-15 21:14:18.474280: step 59290, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 51h:27m:06s remains)
INFO - root - 2017-12-15 21:14:25.057144: step 59300, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 49h:57m:54s remains)
2017-12-15 21:14:25.672680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.8323956 -10.355548 -10.800809 -10.335829 -10.759827 -10.811343 -10.408813 -10.181005 -9.5141821 -10.061346 -10.98227 -13.072054 -14.799149 -13.419966 -10.543285][-7.0194526 -8.45887 -9.4243774 -9.175087 -9.4330673 -9.4566 -9.5726337 -9.49681 -9.7172747 -9.6857033 -10.041405 -12.212036 -13.609202 -12.711864 -10.348669][-6.9082828 -7.2724309 -7.7944608 -8.1246605 -8.67948 -8.2669878 -7.6770668 -7.5976734 -8.121911 -8.3996811 -9.3576736 -11.144377 -11.485618 -10.511036 -8.5974][-5.833343 -6.3371406 -6.5024033 -5.5155292 -4.9953413 -4.1324921 -4.0932193 -4.7810278 -5.5063848 -5.3517551 -5.884254 -7.9322047 -9.5150862 -8.4129095 -6.6793427][-5.6593928 -5.8924274 -5.8116422 -4.2894988 -2.8242128 -1.2222857 -0.6593442 -1.6895337 -2.7017558 -2.8952038 -4.1166306 -6.012444 -7.3055978 -7.2227831 -6.7368617][-6.4145136 -5.6392603 -4.8405294 -2.755995 -0.53127193 1.7826037 3.3064342 2.2961764 0.43954897 -0.54136038 -2.2383454 -4.2939734 -6.0546236 -5.8382683 -5.653399][-6.2533784 -4.8436227 -3.6318426 -1.3918738 1.3083725 3.5420728 5.7521853 5.5487437 3.8961892 1.9266467 -0.34058189 -2.5277176 -4.3029485 -4.3753052 -4.676918][-5.7826729 -3.8189349 -3.1833665 0.046065807 2.3667445 4.8442674 6.5871167 5.8688617 4.8572211 2.3594489 0.084064007 -2.5663157 -4.7617207 -4.17875 -3.9965162][-5.7655749 -3.7539959 -2.6487806 -0.59146738 1.292315 3.6880717 5.068234 4.4545503 3.7565312 1.8389125 -0.19926834 -3.2696579 -6.5150337 -7.2299538 -6.8352146][-6.2414823 -4.7308779 -4.2433791 -1.6682549 -0.71750069 1.7290058 3.1697297 2.4713149 1.3823304 0.31215096 -0.3007946 -4.0659389 -8.3715429 -10.195853 -10.314821][-9.7569618 -8.8593121 -6.8082147 -4.1514721 -2.9423914 -2.2315335 -1.955451 -1.8756335 -2.1697907 -3.18162 -4.277854 -6.9137163 -10.3895 -13.577215 -13.634439][-11.057087 -10.633746 -9.38594 -6.7461491 -5.3974934 -4.5628419 -5.3353877 -6.2787733 -6.3710346 -5.6235108 -6.3903351 -8.6206064 -11.053766 -13.302428 -12.027403][-10.693207 -9.493124 -7.9302464 -6.67802 -6.0727434 -5.5740972 -5.3847256 -5.6337819 -6.6735778 -7.0271239 -7.807025 -9.0459642 -9.8720379 -10.776694 -7.8412][-9.8313 -8.5691729 -7.143611 -6.9459124 -5.8901725 -5.8083968 -4.4382982 -4.672821 -4.3077555 -4.53771 -6.301003 -6.6942544 -7.3152494 -7.2139664 -5.5343819][-7.039259 -5.7361231 -4.1723056 -2.9223607 -1.9597292 -2.5772662 -3.4764497 -4.7041521 -5.0290446 -4.3647256 -5.1603904 -5.046504 -6.0347533 -6.5579438 -5.6020184]]...]
INFO - root - 2017-12-15 21:14:32.194723: step 59310, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 50h:47m:27s remains)
INFO - root - 2017-12-15 21:14:38.770260: step 59320, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 49h:38m:47s remains)
INFO - root - 2017-12-15 21:14:45.416638: step 59330, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 49h:55m:59s remains)
INFO - root - 2017-12-15 21:14:52.012864: step 59340, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 48h:49m:43s remains)
INFO - root - 2017-12-15 21:14:58.689829: step 59350, loss = 0.22, batch loss = 0.17 (11.7 examples/sec; 0.686 sec/batch; 52h:04m:56s remains)
INFO - root - 2017-12-15 21:15:05.357239: step 59360, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 49h:31m:51s remains)
INFO - root - 2017-12-15 21:15:12.019816: step 59370, loss = 0.23, batch loss = 0.19 (12.1 examples/sec; 0.664 sec/batch; 50h:20m:52s remains)
INFO - root - 2017-12-15 21:15:18.677225: step 59380, loss = 0.22, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 49h:18m:44s remains)
INFO - root - 2017-12-15 21:15:25.278996: step 59390, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 49h:27m:49s remains)
INFO - root - 2017-12-15 21:15:31.893282: step 59400, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 49h:33m:04s remains)
2017-12-15 21:15:32.405980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9468355 -8.3660078 -8.3305225 -8.7213068 -9.4249935 -9.661334 -10.201751 -10.054922 -9.0811749 -7.87441 -6.04193 -4.3923216 -5.8333254 -8.0370369 -9.5769558][-7.6482949 -7.3107724 -6.4563823 -6.6463652 -7.79847 -8.7484474 -9.3331547 -8.9040346 -8.9262123 -8.1633921 -5.7098632 -5.0063705 -6.16894 -7.9494514 -10.961342][-5.49037 -6.2667232 -7.2523808 -6.0347514 -6.967926 -7.7475934 -8.6349916 -8.1060619 -7.9894466 -8.0895233 -6.6906762 -5.8127365 -6.4225245 -8.2726212 -10.534607][-7.1948662 -7.0478516 -6.5348954 -5.912282 -7.0859666 -6.3515916 -5.5525432 -6.2373705 -7.221849 -6.5688081 -4.8703232 -4.8747587 -5.5854197 -7.7188654 -9.2131519][-7.3840828 -8.9517422 -8.37275 -7.2393317 -7.3395672 -4.0043612 -0.58337784 -2.5886381 -4.7072124 -4.6212449 -4.7753496 -3.97377 -3.9660664 -5.4635677 -7.2751589][-9.4741688 -9.6793146 -8.3697615 -6.2105494 -4.5768566 0.18468189 4.1840854 3.3990083 2.2767525 -0.39110136 -2.8580375 -1.4115338 -2.560611 -5.0217333 -6.5929031][-11.077976 -10.399475 -7.8331838 -4.0727654 -1.4011073 3.1211228 7.4983478 8.4680576 7.3150582 1.5983939 -2.3596709 -2.1651185 -3.8648913 -4.8340669 -7.4467573][-12.257366 -10.317129 -7.8424 -2.5644054 1.2632151 5.5533633 8.7221756 8.7971516 7.2323041 2.5714221 -1.1762581 -2.9053836 -5.3202372 -7.1766558 -9.8683014][-9.6470175 -8.3519573 -6.373085 -2.6449115 0.25160074 3.7027459 6.2324309 6.094625 4.26421 0.798841 -1.9352982 -5.1401267 -8.0528126 -10.039464 -11.808329][-7.5195436 -6.1878462 -4.9466872 -2.5011137 -1.7442787 -0.20408344 2.8779664 2.8249965 -0.069326878 -1.710104 -3.1526103 -6.1627107 -8.6275215 -11.697683 -14.260384][-10.036293 -9.097868 -7.9510436 -6.1558414 -5.4188328 -4.6743455 -2.7586522 -3.0229089 -4.0916986 -4.9375081 -7.4028177 -9.3753633 -11.442617 -13.354029 -14.162165][-14.612637 -13.779985 -12.196017 -10.488951 -9.9812269 -9.28772 -8.0445023 -8.62707 -8.7104177 -9.2441807 -10.589149 -10.717173 -11.486731 -13.170786 -14.15262][-16.395607 -15.670229 -13.874756 -11.761 -11.432743 -10.903057 -10.971337 -11.120279 -11.167931 -10.266695 -9.8231812 -9.1680832 -10.051329 -10.210761 -10.060329][-14.372854 -13.581072 -11.429655 -9.2541485 -8.6448154 -8.1919765 -9.0746565 -8.7564945 -8.151969 -8.3634186 -8.6082354 -6.6643782 -6.2918549 -6.7005415 -6.9367237][-9.6453209 -9.3175869 -7.8161 -5.451292 -4.1697493 -3.860105 -3.7614546 -4.1622491 -5.4200435 -5.2301655 -5.2826934 -4.7322636 -5.9084277 -5.2829347 -5.2001634]]...]
INFO - root - 2017-12-15 21:15:39.005642: step 59410, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 49h:59m:19s remains)
INFO - root - 2017-12-15 21:15:45.606124: step 59420, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 51h:24m:16s remains)
INFO - root - 2017-12-15 21:15:52.257248: step 59430, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 50h:12m:12s remains)
INFO - root - 2017-12-15 21:15:58.817823: step 59440, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 49h:11m:08s remains)
INFO - root - 2017-12-15 21:16:05.432736: step 59450, loss = 0.20, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 50h:56m:18s remains)
INFO - root - 2017-12-15 21:16:12.023217: step 59460, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 49h:24m:33s remains)
INFO - root - 2017-12-15 21:16:18.566864: step 59470, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 50h:10m:30s remains)
INFO - root - 2017-12-15 21:16:25.207363: step 59480, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 49h:01m:49s remains)
INFO - root - 2017-12-15 21:16:31.765639: step 59490, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.675 sec/batch; 51h:09m:14s remains)
INFO - root - 2017-12-15 21:16:38.277960: step 59500, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 49h:02m:38s remains)
2017-12-15 21:16:38.879610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.6822157 -8.4706907 -9.41773 -9.5312166 -10.105014 -9.51626 -8.5251961 -7.6518192 -6.427392 -6.666934 -6.80495 -9.3241673 -12.249168 -11.956631 -11.654655][-6.750679 -6.4589696 -6.1826439 -6.0302129 -6.7279453 -6.8628488 -6.7540817 -6.5760746 -6.1219659 -6.2956381 -6.7668195 -9.7332945 -11.789338 -13.153328 -14.059053][-4.9141474 -6.3174133 -7.780879 -6.8516407 -7.4709883 -7.828249 -7.6454153 -7.2774391 -6.6318569 -5.9189959 -6.085186 -9.9472942 -12.935682 -13.960976 -14.310959][-5.8503294 -6.8440666 -7.733964 -7.7649069 -7.9567075 -7.2497535 -6.97445 -7.2364278 -6.621171 -5.2108035 -5.0285707 -8.9102936 -12.386803 -14.8937 -15.550503][-6.7959309 -8.827673 -9.9396744 -7.5033193 -6.008759 -3.679508 -2.2471642 -5.495378 -7.300396 -4.973464 -4.3956704 -7.6624174 -11.087154 -14.376806 -15.469028][-7.4882364 -8.04314 -8.6782484 -5.68321 -2.5250378 1.4203796 4.4979453 1.4898844 -1.0770507 -2.4039934 -4.714283 -6.4980841 -8.9783945 -11.751455 -13.032074][-8.399826 -7.5521564 -6.3431888 -2.863775 -0.65819216 3.9082618 8.183918 6.1709771 4.4835544 0.55249929 -3.861269 -6.5367184 -10.172989 -12.291552 -13.426743][-8.0507183 -6.7997065 -6.5098429 -2.8629806 -0.96739054 2.6943936 6.3701243 4.9654231 4.5933566 1.6157379 -1.8791363 -6.0304341 -11.40778 -13.599129 -14.390694][-6.5810008 -6.1196709 -5.9106975 -3.7022924 -1.6351895 1.774775 4.2529798 3.3829474 2.5944905 -0.044980049 -2.0968835 -7.0452867 -12.231696 -13.948442 -15.35251][-3.8844888 -3.5656092 -4.2119427 -2.9880257 -2.2484932 -0.20841169 2.6392999 2.1580911 0.2453723 -2.2547805 -4.56222 -8.9441185 -12.578383 -14.115643 -16.10788][-8.8263359 -7.3185596 -6.844892 -6.88729 -6.8688774 -6.5442162 -4.4239368 -3.6818087 -4.06466 -4.8036056 -7.002182 -11.422893 -14.145088 -15.291536 -15.358583][-14.665663 -14.568277 -14.018187 -12.488674 -12.263718 -11.99198 -12.01211 -11.583178 -10.290436 -9.7824335 -9.9897289 -11.845858 -13.646629 -15.144367 -15.739132][-16.196169 -16.780565 -17.189217 -16.191235 -15.846363 -15.089479 -14.829523 -15.546652 -15.233633 -14.144753 -13.160696 -12.702361 -12.338879 -12.53544 -12.740669][-14.188 -13.96357 -14.49271 -13.94614 -13.484295 -13.395882 -13.559353 -13.242526 -12.708105 -12.642334 -12.509813 -11.318537 -11.214163 -11.099178 -10.418041][-11.150482 -10.547497 -9.875205 -9.39 -9.7815609 -9.5187321 -9.3810558 -10.110874 -9.4121523 -9.154192 -9.4294834 -10.166596 -11.160646 -11.075419 -11.113592]]...]
INFO - root - 2017-12-15 21:16:45.577927: step 59510, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 50h:27m:36s remains)
INFO - root - 2017-12-15 21:16:52.124343: step 59520, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 50h:32m:03s remains)
INFO - root - 2017-12-15 21:16:58.799511: step 59530, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.680 sec/batch; 51h:33m:32s remains)
INFO - root - 2017-12-15 21:17:05.420277: step 59540, loss = 0.19, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 48h:36m:53s remains)
INFO - root - 2017-12-15 21:17:12.020489: step 59550, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 49h:17m:15s remains)
INFO - root - 2017-12-15 21:17:18.641718: step 59560, loss = 0.24, batch loss = 0.19 (12.2 examples/sec; 0.655 sec/batch; 49h:40m:39s remains)
INFO - root - 2017-12-15 21:17:25.195133: step 59570, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 48h:43m:30s remains)
INFO - root - 2017-12-15 21:17:31.766670: step 59580, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 50h:13m:54s remains)
INFO - root - 2017-12-15 21:17:38.344764: step 59590, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 50h:57m:13s remains)
INFO - root - 2017-12-15 21:17:44.867089: step 59600, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 49h:09m:11s remains)
2017-12-15 21:17:45.365234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8339043 -1.9032426 -1.6023388 -1.3172116 -2.6232464 -3.2821565 -3.3752778 -3.4665058 -3.1679149 -2.5963094 -2.3136189 -4.7592859 -6.8786273 -7.0044217 -7.3424749][-1.91956 -1.9410305 -2.4777544 -2.6169515 -3.1661365 -3.8340931 -4.8571119 -5.6095977 -5.7482467 -5.0583477 -4.2409053 -5.9391289 -7.6095095 -7.7429295 -7.9299455][-1.3554263 -1.55161 -1.7813251 -1.9184241 -2.9863534 -4.0086784 -5.0456882 -5.5235753 -5.5086 -5.5913415 -5.0712166 -6.2535334 -7.5652723 -7.0619154 -6.5154796][-2.4337742 -2.1594908 -1.8398623 -2.0852897 -2.93508 -3.4037249 -3.6622229 -4.0909228 -4.7693634 -5.0918574 -5.0525575 -6.3466473 -7.5672688 -6.7554483 -6.4020815][-2.6957521 -2.9659786 -3.0907888 -1.9591312 -1.2937136 -1.1765018 -1.1847968 -1.6044002 -2.26995 -2.9003861 -3.605288 -5.4390974 -6.9663792 -6.959774 -6.973094][-5.1274166 -4.73143 -3.7151318 -1.3683643 0.10051394 1.2186518 2.3718886 2.2235374 1.0546918 -0.56786251 -1.6444616 -3.6347635 -5.1776338 -4.5385284 -4.5917048][-7.0458288 -6.5662541 -5.0757375 -2.16297 0.33203363 3.0857263 5.3244729 5.33023 4.1882644 1.8238707 0.20945311 -1.9958756 -3.553627 -2.8863494 -3.500596][-7.3620796 -7.2290778 -6.1283054 -2.8094704 0.025998592 3.2054667 5.0626407 5.3202996 4.8575034 3.3305488 2.2199926 -0.71844816 -2.751719 -2.074687 -2.5922177][-6.7535868 -6.9203663 -6.2879734 -3.4365628 -0.97260857 1.6988873 3.1852946 3.5685191 2.9294715 2.2792125 2.5578475 -0.024388313 -1.8939631 -2.0365081 -2.9341879][-6.8191319 -7.0066276 -6.2354484 -3.89637 -2.4357138 0.28155851 1.7257361 2.0442405 1.2353091 1.0951791 1.7739544 -0.19941854 -1.7924495 -2.2105548 -4.1624203][-10.239365 -9.8145885 -8.5311794 -6.9087534 -5.56638 -3.3016713 -2.2189534 -1.8936872 -2.1664481 -2.0187316 -1.5146494 -3.262949 -3.7385488 -3.6459835 -5.0348396][-12.758123 -12.392896 -11.75748 -9.9132156 -8.574789 -7.0261593 -6.3370171 -5.6097302 -5.3465509 -5.4215775 -5.1524706 -5.6408529 -5.6339078 -5.273983 -5.9326658][-13.456018 -12.893373 -11.958649 -11.357956 -10.941167 -10.064448 -9.8530035 -8.7537851 -7.8361015 -7.2702723 -6.8175926 -6.9505334 -6.7809267 -6.1862874 -6.1837406][-10.539577 -10.302414 -9.7976952 -9.73871 -9.4117527 -9.566556 -9.87309 -9.187191 -8.691144 -8.3896751 -8.01306 -7.3921108 -6.9772506 -6.7609496 -6.7978611][-7.2958732 -7.1225781 -6.5608945 -5.7429309 -5.0559444 -5.5953112 -5.7639246 -6.327127 -6.9661837 -6.9860849 -7.2991872 -8.0381985 -8.6633854 -8.5433846 -8.6498928]]...]
INFO - root - 2017-12-15 21:17:51.961968: step 59610, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.688 sec/batch; 52h:06m:52s remains)
INFO - root - 2017-12-15 21:17:58.661054: step 59620, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 50h:43m:58s remains)
INFO - root - 2017-12-15 21:18:05.269359: step 59630, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 50h:10m:31s remains)
INFO - root - 2017-12-15 21:18:11.893834: step 59640, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 49h:20m:13s remains)
INFO - root - 2017-12-15 21:18:18.613289: step 59650, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.681 sec/batch; 51h:34m:57s remains)
INFO - root - 2017-12-15 21:18:25.189597: step 59660, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 50h:37m:50s remains)
INFO - root - 2017-12-15 21:18:31.855530: step 59670, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 50h:31m:07s remains)
INFO - root - 2017-12-15 21:18:38.564292: step 59680, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 51h:16m:55s remains)
INFO - root - 2017-12-15 21:18:45.167156: step 59690, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 49h:29m:58s remains)
INFO - root - 2017-12-15 21:18:51.803141: step 59700, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 51h:21m:53s remains)
2017-12-15 21:18:52.324073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0359387 -6.2622957 -8.4009075 -8.8752842 -9.5111685 -9.2845755 -8.9729767 -9.2925568 -9.5168114 -9.3756208 -8.6635571 -7.5322647 -9.075263 -9.8825626 -10.160208][-4.0740757 -5.5026217 -7.2707105 -8.5522118 -9.2704124 -8.5046654 -8.4550915 -8.8938046 -9.29761 -8.93987 -8.5378838 -8.1480522 -9.637681 -10.306196 -9.30142][-3.7815461 -6.2247143 -8.768733 -8.796814 -9.2314558 -8.8998995 -8.1141644 -8.1361628 -9.8847809 -10.186241 -8.6409855 -7.8875732 -10.62588 -11.009735 -10.302538][-6.4533863 -7.24109 -10.340879 -11.071807 -11.06196 -7.9151039 -6.4737225 -7.6380386 -9.0733757 -9.9074812 -10.083973 -9.1146 -10.397425 -11.929782 -11.923227][-9.2415657 -12.111782 -14.435472 -14.465639 -12.056063 -5.2828183 -1.410171 -5.070128 -9.9958134 -10.117861 -9.815197 -9.9576206 -11.887026 -13.467421 -14.036074][-10.305721 -13.615332 -15.670424 -13.593319 -9.3078747 -1.9177506 4.1147304 2.95225 -1.3753543 -6.8497481 -11.085207 -8.2335663 -10.208796 -12.930256 -13.810881][-10.438662 -11.543989 -12.240646 -9.6219254 -4.3138323 2.5764756 9.558672 9.9844189 7.3069024 -1.0739312 -7.7746592 -6.9153905 -10.014938 -12.590768 -13.453777][-10.52998 -11.844021 -11.445214 -8.3657331 -2.3565629 6.4960351 13.154814 11.929808 10.031704 3.2253413 -2.9744339 -5.2107677 -9.690649 -10.940041 -11.348862][-9.1720715 -10.120758 -11.198265 -9.2911081 -2.847641 4.9390006 10.427639 10.472137 6.944325 -0.16877127 -4.8869448 -7.0257626 -10.742224 -11.757137 -11.99312][-7.4505157 -9.1966705 -10.529936 -10.705822 -7.9664631 -1.3113441 4.3587375 6.032248 2.6148648 -4.4182029 -8.5055008 -9.2612448 -12.507866 -13.687415 -13.537952][-9.4732561 -11.401428 -13.671509 -13.380143 -11.693239 -10.042587 -5.8201632 -4.3713951 -6.1559238 -8.9339523 -11.801735 -13.249639 -14.131916 -14.519888 -13.928019][-15.297302 -15.38147 -15.972404 -15.978992 -14.799353 -14.416845 -14.778236 -14.059507 -12.558099 -14.052431 -15.99127 -15.178692 -14.489496 -13.503023 -12.619318][-17.825464 -17.955347 -18.442467 -17.699913 -16.69799 -16.019575 -15.393648 -15.826654 -16.304203 -15.332705 -14.287188 -14.318218 -14.181465 -12.773427 -10.293718][-14.957865 -15.431982 -16.274384 -14.454683 -12.783419 -12.673766 -13.128469 -12.705581 -12.117403 -12.025494 -12.006254 -11.474457 -11.459597 -10.439217 -9.1152792][-9.3295994 -9.8108 -10.162701 -9.2730408 -8.9165821 -8.3547392 -7.6898346 -7.5371819 -7.5996404 -7.5244093 -8.2275925 -9.5098228 -9.7647028 -10.34054 -11.489179]]...]
INFO - root - 2017-12-15 21:18:58.858296: step 59710, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 50h:55m:56s remains)
INFO - root - 2017-12-15 21:19:05.383923: step 59720, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 50h:35m:31s remains)
INFO - root - 2017-12-15 21:19:12.020334: step 59730, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 49h:40m:11s remains)
INFO - root - 2017-12-15 21:19:18.644610: step 59740, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 49h:55m:47s remains)
INFO - root - 2017-12-15 21:19:25.228683: step 59750, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 50h:07m:28s remains)
INFO - root - 2017-12-15 21:19:31.821645: step 59760, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 51h:01m:02s remains)
INFO - root - 2017-12-15 21:19:38.357375: step 59770, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 49h:05m:19s remains)
INFO - root - 2017-12-15 21:19:44.869957: step 59780, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 50h:48m:07s remains)
INFO - root - 2017-12-15 21:19:51.418945: step 59790, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 49h:05m:14s remains)
INFO - root - 2017-12-15 21:19:58.097208: step 59800, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 49h:43m:47s remains)
2017-12-15 21:19:58.593981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2922025 -5.572063 -4.6410151 -2.3056791 0.14793873 -0.49104691 -2.3369596 -4.0106497 -4.899384 -5.2531509 -5.255352 -7.4266772 -10.119209 -11.761389 -10.344244][-4.5983891 -4.2317505 -5.5702515 -5.0343719 -3.9993939 -2.3528371 -0.29946613 -1.4889464 -3.4168773 -5.6335068 -8.1848183 -10.834816 -11.087551 -12.456689 -11.280731][-3.1583791 -4.539423 -6.1969824 -4.6162381 -4.8937759 -4.4765749 -3.8814721 -3.7478485 -3.6591752 -4.0116358 -5.2350059 -8.8070879 -10.731759 -11.564981 -10.462022][-2.9684064 -4.3345857 -6.4978919 -7.3934474 -8.0748348 -5.8811588 -3.1528862 -1.6479697 -1.943675 -4.4848642 -8.1267519 -11.551746 -11.744396 -12.274702 -10.240361][-4.9942222 -5.8723783 -7.4008856 -7.5934997 -6.6805444 -3.866446 -2.5429616 -2.7333217 -2.8661845 -2.9452953 -4.8569894 -10.017036 -12.783213 -14.23366 -12.176281][-7.1973343 -5.4123049 -5.4412794 -5.7582088 -4.0915809 0.49479532 5.4254785 5.2506404 1.7655911 -2.82007 -6.1166582 -8.770895 -10.778217 -13.416069 -12.833048][-7.8461514 -7.7039223 -8.5603218 -6.1231971 -2.7160745 2.7432609 7.2305932 8.1718731 6.8292384 0.720706 -4.5909719 -6.9593782 -9.0455379 -11.723093 -12.082003][-8.2815228 -9.1359262 -9.3773613 -5.7699289 -2.0895164 3.8623443 9.0702 9.2959251 8.1886044 2.7451253 -3.0960882 -7.6344533 -9.5875072 -9.928606 -9.5227966][-6.754375 -9.8801594 -9.7959442 -6.9062214 -4.0344682 1.9019985 6.3849053 7.2208686 6.8154588 0.82556629 -3.4603548 -8.3284378 -11.867645 -13.282012 -11.551377][-6.66562 -7.5842881 -7.7432442 -6.4449573 -4.6916938 -0.90992737 1.9807987 3.2117105 2.7906241 -2.5277638 -7.7040834 -12.849041 -15.753794 -15.95587 -14.097189][-9.2971087 -11.540501 -10.703812 -7.7698 -6.5621605 -4.7859583 -4.2427144 -3.6918569 -5.1271076 -8.7132463 -11.165083 -14.590143 -17.715689 -18.318117 -15.676035][-13.086924 -13.671775 -13.581561 -11.312229 -8.0853968 -6.2584591 -6.703845 -7.3778234 -8.7943335 -11.675829 -14.01779 -16.134115 -16.173388 -15.070221 -13.851828][-14.67325 -14.491261 -13.185353 -12.589714 -12.081033 -10.563442 -9.2984352 -9.60529 -10.509604 -10.748017 -11.394575 -12.293365 -13.017147 -11.676458 -9.7673759][-10.711376 -9.9489079 -10.418301 -10.947563 -11.073695 -10.363775 -9.8250933 -9.6556072 -9.5752439 -10.487983 -10.319082 -8.677496 -8.4620285 -7.7112145 -7.4938607][-5.7679043 -6.0632906 -5.1953444 -4.8429174 -6.1429472 -7.9352822 -8.53444 -7.6355877 -6.9291844 -7.0451035 -7.0305138 -7.5975904 -8.2285194 -7.3963394 -6.9471192]]...]
INFO - root - 2017-12-15 21:20:05.155886: step 59810, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 49h:26m:33s remains)
INFO - root - 2017-12-15 21:20:11.849026: step 59820, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.632 sec/batch; 47h:50m:42s remains)
INFO - root - 2017-12-15 21:20:18.434398: step 59830, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 49h:33m:39s remains)
INFO - root - 2017-12-15 21:20:25.021419: step 59840, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 49h:55m:25s remains)
INFO - root - 2017-12-15 21:20:31.522603: step 59850, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 48h:26m:31s remains)
INFO - root - 2017-12-15 21:20:38.081303: step 59860, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 50h:18m:39s remains)
INFO - root - 2017-12-15 21:20:44.621683: step 59870, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 49h:09m:58s remains)
INFO - root - 2017-12-15 21:20:51.379588: step 59880, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 50h:40m:58s remains)
INFO - root - 2017-12-15 21:20:57.999606: step 59890, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 50h:59m:13s remains)
INFO - root - 2017-12-15 21:21:04.569616: step 59900, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 49h:35m:43s remains)
2017-12-15 21:21:05.106839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9583278 -4.7174044 -4.8963571 -3.0162268 -2.4820223 -1.731271 -2.5000319 -2.9323831 -2.6660798 -1.4904675 0.62718964 -1.0381246 -1.42138 -2.3015881 -2.9629297][-2.7329402 -1.2027197 -1.377346 -1.2860966 -1.6120958 -2.2062151 -1.9138069 -1.9420121 -1.5129824 -0.80471706 0.16684961 -1.3901854 -2.462465 -3.5375059 -3.7700114][-1.9939556 -1.2040634 -0.76697636 -0.74596357 -2.4158714 -2.4793355 -2.5134149 -2.5199232 -2.086628 -1.2043843 0.12910891 -1.9974356 -3.5015416 -4.1802697 -4.151916][-2.5879228 -1.2488976 0.058461189 -0.65020466 -1.9406652 -2.3650165 -2.3521605 -1.6630359 -1.8138802 -1.4118023 -0.93870687 -3.59108 -5.3363853 -5.69873 -5.6253138][-3.1676672 -2.4960315 -2.1427383 -0.65803671 -1.4009337 -1.3825579 -1.1467199 -0.77317238 -0.17115784 -0.12660265 0.055246353 -3.4037976 -5.8783212 -6.67837 -7.0284233][-4.1931238 -2.7407961 -1.8616843 -1.2871556 -0.52603245 0.56078148 0.73822689 1.2544479 0.96788025 0.66316128 -0.086942196 -3.602752 -5.3221416 -6.1231737 -6.956893][-4.2232409 -3.7943795 -2.5921586 -1.137794 0.034959316 1.7861919 2.6318135 2.9195542 3.0699296 1.7953501 -0.4063921 -3.919446 -5.3249054 -6.1251249 -6.5254455][-5.2974253 -4.5187259 -2.6185274 -0.62767982 0.60577965 2.0026598 2.84123 3.7061372 3.6169829 2.6779857 1.9598107 -2.5957117 -5.73536 -7.2614079 -7.5418682][-4.830533 -4.4018159 -3.81202 -1.1345563 0.72176313 2.1829605 2.8963742 3.7441382 4.3252549 3.4400048 2.9651694 -0.946208 -3.8381612 -5.8655057 -6.682251][-6.0639167 -4.8748236 -3.0619426 -1.6638975 -0.67685413 0.94462872 2.0121617 3.4061809 3.26784 3.1546922 3.5753198 0.0587945 -2.4384661 -3.3624787 -4.3778014][-7.02323 -6.620882 -5.0466552 -2.7948053 -1.2434716 -0.31411505 0.1794486 0.66477966 0.82959986 1.1790943 1.9250054 -0.49024296 -2.1686282 -3.7171245 -3.9814091][-10.546446 -9.117116 -7.2907648 -4.9215751 -3.7893581 -2.6686442 -2.706989 -2.5804555 -2.8555005 -2.1395569 -0.93793869 -2.4720144 -3.1717889 -3.8235998 -4.9646525][-10.877442 -9.5001411 -7.1120319 -5.4714274 -4.3727684 -3.7817698 -3.6832709 -4.2777348 -4.3193274 -3.5782645 -2.0234871 -2.2884953 -2.5951293 -3.4162176 -3.599474][-8.2071724 -6.6501713 -5.2344365 -3.791503 -3.4499393 -3.2716367 -3.6196542 -4.1935487 -4.3264275 -4.4476137 -3.8248155 -3.4894116 -2.5974336 -3.3090365 -3.3938673][-5.71888 -4.732862 -3.6917202 -2.5560968 -2.2619743 -1.3697095 -1.247973 -1.8987672 -3.1399744 -4.3617568 -4.3349118 -4.5430284 -4.6533294 -5.8592324 -6.32261]]...]
INFO - root - 2017-12-15 21:21:11.678871: step 59910, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 48h:55m:15s remains)
INFO - root - 2017-12-15 21:21:18.261201: step 59920, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 48h:26m:21s remains)
INFO - root - 2017-12-15 21:21:24.934785: step 59930, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.690 sec/batch; 52h:14m:16s remains)
INFO - root - 2017-12-15 21:21:31.560390: step 59940, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 49h:07m:18s remains)
INFO - root - 2017-12-15 21:21:38.144094: step 59950, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 49h:11m:16s remains)
INFO - root - 2017-12-15 21:21:44.651938: step 59960, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 49h:51m:59s remains)
INFO - root - 2017-12-15 21:21:51.258844: step 59970, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 49h:13m:30s remains)
INFO - root - 2017-12-15 21:21:57.812951: step 59980, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 48h:29m:09s remains)
INFO - root - 2017-12-15 21:22:04.431339: step 59990, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 49h:18m:37s remains)
INFO - root - 2017-12-15 21:22:11.079440: step 60000, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 49h:29m:22s remains)
2017-12-15 21:22:11.679664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5651259 -3.440695 -2.7011557 -2.2779686 -3.3014588 -4.7979093 -6.6697273 -7.807714 -7.4594021 -6.7186618 -5.7506914 -5.3890018 -6.0154858 -5.4633336 -5.0541577][-2.1275589 -2.0418386 -1.8259332 -1.2551441 -2.5211258 -3.6852298 -4.8766184 -5.8715854 -5.9672375 -5.213129 -3.8417916 -3.9074168 -4.9189038 -4.6316195 -4.8063307][-0.59935236 -1.8602381 -3.0236881 -1.9890914 -2.1185369 -3.3046517 -4.9827147 -5.6005974 -5.0419359 -4.0998316 -3.1500883 -3.9467573 -5.4891362 -6.4578433 -7.0102806][-2.5283909 -3.3937678 -4.0537739 -3.7022293 -3.7879481 -3.1832318 -3.088357 -3.3915269 -3.2374377 -2.9078796 -2.3307176 -3.43963 -5.411406 -6.651576 -6.7255192][-3.1401858 -4.6061764 -6.0735197 -4.7806482 -3.5038445 -1.3142095 0.19327593 -0.25523806 -1.2539392 -1.4007478 -1.6769276 -3.0403678 -5.0931363 -6.5928249 -6.5469503][-4.9671664 -6.0916719 -6.1373448 -3.4986153 -0.70523405 2.2857585 4.5839067 4.6952319 4.5624433 2.4211802 -0.43160152 -1.8693535 -4.1811662 -4.9803085 -4.3695059][-6.717555 -5.835362 -3.8857422 -1.7789385 0.17118216 4.4519935 8.3532028 8.4858551 7.9012523 5.0072455 1.4906087 -1.3985276 -5.1938558 -5.7868724 -5.3810911][-6.3937306 -5.6226158 -5.1136055 -2.5286508 -0.78703213 3.3332753 7.28608 7.503695 6.5507674 4.0804133 1.2459583 -2.388309 -6.5465584 -7.1282248 -6.7998972][-4.4702539 -4.5786018 -4.4681282 -2.7989445 -1.0950308 1.0995269 3.109097 4.41907 4.3482261 1.6498342 -1.086751 -4.1479425 -7.409945 -7.8914976 -7.5995684][-4.5999956 -4.8086548 -5.1605935 -4.2988138 -3.0225525 -1.8497584 0.079982758 1.8708205 0.86752176 -1.8356376 -4.06414 -6.88833 -9.7390728 -10.111753 -9.3928547][-11.242439 -11.06813 -10.928752 -9.3092146 -8.5188236 -7.7870035 -5.6820006 -5.3466663 -6.2253785 -6.5037584 -8.0766792 -10.794985 -12.318871 -12.077448 -10.468094][-15.29446 -15.468428 -15.106558 -14.266891 -13.735023 -12.329124 -11.169001 -11.04493 -10.572081 -10.900456 -11.839264 -12.64551 -13.474243 -13.286555 -11.821049][-13.608189 -13.39799 -13.564816 -13.252197 -13.497728 -12.415997 -11.763597 -11.329975 -11.271315 -11.434488 -11.268916 -10.903602 -11.300142 -9.8961353 -7.506062][-10.080957 -10.19249 -10.178818 -9.0928459 -8.41497 -8.1366138 -8.5155621 -7.685533 -7.2762103 -7.5253129 -8.0250359 -8.0121212 -7.6100359 -6.2260652 -5.7634811][-9.0936537 -9.1586323 -7.991354 -6.3176041 -5.4538212 -4.210393 -3.8684402 -4.9567137 -5.4464335 -4.706389 -4.5761356 -5.0216246 -5.9805431 -5.91633 -5.2787662]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 21:22:19.426513: step 60010, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 50h:20m:56s remains)
INFO - root - 2017-12-15 21:22:26.119097: step 60020, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 48h:38m:40s remains)
INFO - root - 2017-12-15 21:22:32.679269: step 60030, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 49h:46m:59s remains)
INFO - root - 2017-12-15 21:22:39.241305: step 60040, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.664 sec/batch; 50h:13m:33s remains)
INFO - root - 2017-12-15 21:22:45.817625: step 60050, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 50h:02m:47s remains)
INFO - root - 2017-12-15 21:22:52.417967: step 60060, loss = 0.18, batch loss = 0.13 (12.6 examples/sec; 0.635 sec/batch; 48h:02m:47s remains)
INFO - root - 2017-12-15 21:22:58.964395: step 60070, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 49h:11m:09s remains)
INFO - root - 2017-12-15 21:23:05.579335: step 60080, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.687 sec/batch; 52h:00m:51s remains)
INFO - root - 2017-12-15 21:23:12.103186: step 60090, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 48h:00m:00s remains)
INFO - root - 2017-12-15 21:23:18.602689: step 60100, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 48h:19m:50s remains)
2017-12-15 21:23:19.133940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9747124 -5.871882 -5.9457135 -7.3018241 -8.9506044 -10.313987 -10.424468 -9.1112642 -7.6421804 -7.8528481 -7.66891 -9.371769 -11.362373 -12.874826 -12.458213][-6.3901157 -7.0305157 -7.0973768 -7.6304712 -8.4921722 -9.9483223 -11.118101 -11.8559 -11.522841 -10.315182 -9.2404461 -11.161258 -12.518385 -13.760071 -14.446293][-3.1202562 -4.7852974 -7.0406904 -7.40457 -8.1021223 -9.742691 -10.662186 -11.251369 -10.746531 -10.566961 -9.975666 -11.106173 -12.075848 -14.489471 -14.579922][-2.3964021 -3.7914648 -4.8005137 -5.6382093 -6.968421 -6.8835182 -7.2487364 -9.2590666 -10.662392 -9.92412 -8.9151659 -10.905184 -12.683208 -13.589584 -13.836128][-5.0505486 -5.3506775 -5.2555919 -5.0445881 -4.0348959 -3.4115047 -2.2295384 -3.8961287 -7.2145429 -8.8798008 -8.5994291 -9.4655781 -10.803244 -13.227967 -13.505474][-8.0643921 -7.8784838 -6.6720057 -5.4069791 -3.5801053 -0.33359957 4.3237395 2.8273158 0.30152082 -3.118139 -7.2058191 -7.8453169 -8.5599432 -11.229692 -12.548611][-10.361751 -10.405487 -8.7763214 -6.0685177 -5.1125269 -0.038780212 6.508039 8.4556847 7.7304158 0.8276763 -5.6632819 -7.6955595 -8.8585644 -9.8012848 -9.7913895][-9.0488148 -9.5317411 -9.2486048 -6.2954817 -5.3706393 -0.86534548 4.4250503 7.4663882 9.2730694 4.2186828 -2.0412223 -7.4330473 -12.075015 -11.793898 -9.477313][-5.5325236 -6.2453489 -6.4909439 -5.6611128 -5.5704765 -1.6364398 1.4059477 4.1105695 7.2172484 3.7497487 -1.8201351 -8.5909376 -13.496914 -14.879929 -14.416967][-3.0572577 -3.3677192 -3.9611082 -3.9629302 -4.76397 -3.1480079 -1.4486547 0.56292725 3.8284059 1.7328801 -3.6021712 -10.43261 -16.221245 -19.456142 -19.108181][-6.920742 -6.7931843 -6.1173773 -5.8343382 -6.7774019 -7.142055 -6.9195442 -5.5346231 -3.6775324 -5.267076 -7.0103855 -11.437203 -15.498007 -19.060589 -19.658123][-13.539256 -11.773817 -11.209978 -10.637345 -11.097515 -11.690842 -12.403656 -12.688505 -11.655253 -11.865265 -12.389458 -13.425404 -13.783184 -16.731295 -16.89921][-14.457502 -14.429646 -14.065132 -13.269837 -13.037006 -12.631889 -12.917772 -13.33389 -13.676383 -12.951258 -12.800338 -14.457851 -15.033215 -14.531609 -12.683838][-12.232342 -12.341228 -11.995953 -10.749945 -11.622505 -12.541904 -12.841619 -12.301986 -12.099133 -12.280592 -12.528101 -11.388718 -10.30383 -10.958487 -10.091171][-7.5265322 -7.9941816 -6.5896039 -6.4278879 -6.3718824 -6.3707294 -7.3860626 -8.8023624 -9.2208662 -9.50115 -9.6551323 -10.617605 -11.101334 -9.66899 -8.1311979]]...]
INFO - root - 2017-12-15 21:23:25.694083: step 60110, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 50h:17m:28s remains)
INFO - root - 2017-12-15 21:23:32.272841: step 60120, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 49h:56m:18s remains)
INFO - root - 2017-12-15 21:23:38.853154: step 60130, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.672 sec/batch; 50h:48m:55s remains)
INFO - root - 2017-12-15 21:23:45.377104: step 60140, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 48h:43m:32s remains)
INFO - root - 2017-12-15 21:23:51.972452: step 60150, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 48h:23m:05s remains)
INFO - root - 2017-12-15 21:23:58.525409: step 60160, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 50h:49m:21s remains)
INFO - root - 2017-12-15 21:24:05.056719: step 60170, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 48h:51m:37s remains)
INFO - root - 2017-12-15 21:24:11.660947: step 60180, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 48h:44m:34s remains)
INFO - root - 2017-12-15 21:24:18.135335: step 60190, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 49h:46m:24s remains)
INFO - root - 2017-12-15 21:24:24.678001: step 60200, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 49h:41m:23s remains)
2017-12-15 21:24:25.256042: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.93362 -12.985234 -13.550879 -11.795768 -10.575937 -9.1035023 -8.4183664 -8.6876488 -8.2192154 -8.5893822 -6.620965 -6.8727703 -9.7381649 -9.8237362 -10.213865][-7.769412 -10.578043 -10.705053 -8.9979486 -7.4854975 -6.7000346 -7.3395195 -8.071682 -8.1956415 -7.57728 -5.83236 -7.3566365 -9.9081516 -9.8025284 -10.500177][-5.5758915 -8.4966908 -9.5332041 -8.6159973 -6.9932313 -5.7724838 -6.5063844 -6.7937222 -6.9219236 -6.8136992 -5.254385 -6.3668857 -8.887167 -10.011285 -11.299954][-8.5828705 -9.6046581 -9.4572439 -9.1503277 -7.898643 -6.1344857 -5.5073676 -5.5684495 -6.0867438 -5.7402735 -4.6338477 -5.7688408 -8.3123865 -8.6211 -9.7734566][-10.211335 -11.986622 -11.452414 -8.8621445 -6.3473272 -4.3410807 -3.0601475 -3.325613 -4.4170918 -3.9931874 -3.8225262 -5.2300816 -7.7629194 -8.637043 -9.4706974][-9.7496777 -10.565141 -10.280293 -7.8023763 -3.3996177 1.2756987 3.4062152 1.0358615 -0.9612689 -1.2401366 -2.9073582 -4.2650638 -7.2966275 -8.6327534 -10.269241][-7.9416924 -8.8100529 -7.7557306 -4.3855352 -0.68959284 3.788054 6.9081483 5.8217082 3.9823375 0.5559516 -1.4789124 -2.8817255 -6.7052822 -7.5451612 -8.5277309][-6.1847858 -6.1626015 -4.0806203 -0.82876348 2.235189 5.1649175 6.0843425 5.8428521 4.9629436 1.8064699 -1.298708 -4.0429306 -7.1381922 -7.3131723 -8.4452343][-6.5519409 -6.3030968 -4.2820005 -0.18002796 1.5130072 3.5234628 4.8300481 3.6550221 1.0280089 -0.28773451 -1.0478582 -3.6262083 -7.7070942 -8.7142935 -8.7985992][-7.5373678 -6.3175535 -5.9491744 -3.9768882 -2.2881916 -0.40623903 0.73416948 0.17685413 -1.3129349 -1.4540024 -1.9118011 -3.4517579 -6.5271583 -7.6690655 -9.3132668][-11.418756 -11.835884 -11.411582 -9.8558445 -8.6203947 -7.380743 -5.5345626 -5.9710865 -6.1607161 -5.2423248 -4.9515142 -5.6858697 -7.6544237 -8.4276638 -8.7369576][-14.485147 -14.251226 -12.941334 -11.620289 -11.961296 -10.565128 -9.2336578 -9.8758574 -9.0104771 -8.8692722 -7.9443865 -8.0614986 -9.3643894 -8.9200439 -9.188055][-15.505461 -14.370596 -13.498229 -13.408207 -12.813972 -11.010573 -11.238806 -9.8035116 -8.4932785 -9.3816395 -9.0258312 -9.8208723 -10.200005 -9.6477222 -9.0944824][-12.226404 -10.283861 -9.9624329 -9.8505955 -10.260069 -10.032726 -9.9510155 -9.1247749 -9.1368132 -9.27551 -8.3715649 -8.2393532 -8.6088314 -8.0017128 -7.9026995][-8.84725 -8.10934 -7.7841749 -7.613451 -6.474268 -5.8305593 -7.8591442 -8.3029346 -8.00475 -7.8012686 -7.4810615 -7.5852923 -7.7822485 -8.1670656 -8.3178043]]...]
INFO - root - 2017-12-15 21:24:31.790584: step 60210, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 50h:25m:04s remains)
INFO - root - 2017-12-15 21:24:38.446638: step 60220, loss = 0.16, batch loss = 0.11 (11.5 examples/sec; 0.695 sec/batch; 52h:32m:14s remains)
INFO - root - 2017-12-15 21:24:45.047360: step 60230, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 48h:55m:21s remains)
INFO - root - 2017-12-15 21:24:51.650045: step 60240, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 49h:33m:47s remains)
INFO - root - 2017-12-15 21:24:58.245597: step 60250, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 49h:44m:32s remains)
INFO - root - 2017-12-15 21:25:04.740089: step 60260, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 50h:16m:46s remains)
INFO - root - 2017-12-15 21:25:11.343721: step 60270, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 49h:28m:25s remains)
INFO - root - 2017-12-15 21:25:17.941506: step 60280, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 49h:49m:03s remains)
INFO - root - 2017-12-15 21:25:24.573942: step 60290, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 49h:41m:01s remains)
INFO - root - 2017-12-15 21:25:31.162884: step 60300, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 50h:19m:36s remains)
2017-12-15 21:25:31.644756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5152059 -5.9868269 -7.2135649 -8.0904751 -9.0454578 -10.220474 -11.316454 -10.679638 -10.353191 -9.9955425 -9.06591 -9.9939213 -11.867915 -11.258198 -12.33516][-7.8316016 -8.8149748 -9.1956921 -9.8854294 -10.187552 -10.945242 -12.037181 -13.198404 -13.784822 -12.247791 -11.023919 -11.875272 -13.155539 -12.340078 -12.668432][-4.7243004 -7.2135768 -10.023411 -9.7211723 -9.061224 -10.858779 -12.5112 -12.637657 -12.404305 -12.546726 -12.408697 -11.981293 -13.084229 -12.814196 -13.778709][-8.2131786 -10.178904 -11.285448 -10.003878 -9.5960464 -9.8751936 -9.58033 -12.077079 -13.557133 -11.575405 -9.8053646 -12.239656 -14.574984 -13.009437 -12.634024][-8.913784 -12.186319 -14.263596 -12.25263 -9.3470621 -5.1096368 -2.9916086 -7.3082438 -11.961516 -11.935012 -11.655217 -11.4174 -12.524405 -13.145826 -14.174826][-12.146431 -13.909592 -13.543932 -11.23724 -8.0889282 -2.742662 2.768681 0.9937439 -2.5078664 -7.6142511 -12.326063 -11.515616 -12.199116 -12.25571 -13.475814][-15.165171 -14.642508 -12.962904 -9.1971149 -4.0937967 1.4206448 5.8647857 5.718101 4.5447869 -2.0587516 -9.1768408 -10.707279 -13.102846 -12.096447 -12.909234][-14.981256 -14.347494 -12.84771 -8.3024616 -1.4942465 5.4072051 9.7029476 6.8203092 4.5683303 -0.25232983 -6.1631217 -9.4915771 -12.248999 -12.382244 -14.389236][-12.494833 -11.737046 -12.506466 -8.77688 -3.3122509 2.5290055 7.7831912 8.5608406 4.8037238 -2.6606715 -7.8793955 -10.56465 -14.333141 -13.90671 -14.476326][-10.293299 -10.22757 -10.519721 -7.167428 -4.8980293 -2.2721071 2.6901507 4.0931907 1.9844394 -2.9774041 -8.5856524 -12.219674 -14.966341 -14.504349 -16.338196][-10.46693 -11.997581 -12.44788 -10.91235 -8.9760189 -6.4555221 -3.1388552 -3.4580331 -4.2531767 -5.6385832 -9.1033916 -13.613629 -15.740822 -15.762777 -15.647392][-16.72551 -16.022188 -16.121174 -15.59944 -14.443773 -11.478442 -10.763234 -11.491554 -11.183457 -11.81797 -13.651715 -14.467621 -14.920031 -14.887171 -14.661356][-15.615898 -14.009108 -13.911543 -15.685263 -15.673391 -14.211 -12.25404 -12.18159 -13.537837 -13.11989 -12.927446 -14.795996 -14.983227 -13.334426 -12.870236][-12.689124 -11.856312 -12.323452 -11.065726 -10.996916 -12.257186 -12.302664 -11.787539 -11.306009 -11.239903 -12.062117 -12.416649 -12.316317 -10.732393 -10.903027][-9.3468676 -8.030962 -6.92489 -6.4296312 -6.89954 -6.4624414 -7.1316457 -8.9155111 -9.363162 -8.6619749 -8.7733526 -10.736237 -10.946127 -11.23489 -12.448502]]...]
INFO - root - 2017-12-15 21:25:38.193962: step 60310, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 49h:38m:51s remains)
INFO - root - 2017-12-15 21:25:44.810259: step 60320, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 49h:22m:59s remains)
INFO - root - 2017-12-15 21:25:51.458974: step 60330, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.685 sec/batch; 51h:46m:52s remains)
INFO - root - 2017-12-15 21:25:58.090350: step 60340, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 51h:35m:24s remains)
INFO - root - 2017-12-15 21:26:04.702755: step 60350, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 50h:27m:05s remains)
INFO - root - 2017-12-15 21:26:11.303152: step 60360, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.658 sec/batch; 49h:44m:55s remains)
INFO - root - 2017-12-15 21:26:17.917966: step 60370, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 48h:44m:28s remains)
INFO - root - 2017-12-15 21:26:24.494940: step 60380, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 48h:44m:48s remains)
INFO - root - 2017-12-15 21:26:31.166137: step 60390, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 48h:50m:30s remains)
INFO - root - 2017-12-15 21:26:37.687364: step 60400, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 50h:49m:26s remains)
2017-12-15 21:26:38.246006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3054404 -6.7269855 -7.29685 -7.3234987 -7.3423228 -6.2199278 -4.5741062 -3.7305381 -3.0879779 -2.8965213 -3.3255916 -3.024678 -4.9887428 -6.3721609 -6.6431904][-4.7697616 -5.2611508 -6.1008129 -6.0055337 -5.7186952 -4.7450671 -2.8086565 -1.5939765 -1.065865 -2.0248835 -2.4339089 -2.1601205 -4.4406962 -6.8977919 -7.4828997][-4.318151 -5.1115842 -5.9018922 -5.2571635 -4.5270753 -3.8193974 -2.8126194 -1.1935034 -1.10284 -1.792872 -1.8672414 -2.3668382 -4.9905658 -6.3597412 -7.7756763][-4.5217004 -6.2643809 -7.4367447 -6.8217306 -6.0053854 -4.1861753 -2.8241227 -2.337429 -1.9624166 -2.4703119 -2.0770423 -2.0937679 -4.1628923 -6.8127871 -8.7470341][-4.9932246 -7.2552 -9.02231 -8.5027695 -6.7746611 -3.5686898 -2.2002156 -1.5075197 -2.3217182 -2.2868371 -1.6008244 -1.7493863 -3.0810237 -5.3321414 -7.7599649][-6.3073249 -7.1304917 -7.554852 -6.8021841 -4.1705713 -0.31252241 1.8658371 1.4465818 -0.059895039 -0.7563324 -1.2812753 -1.0697079 -2.4235876 -4.709507 -6.0323319][-6.3507032 -6.246819 -6.0870924 -3.7865267 -0.334476 3.2703338 5.8341985 5.6622653 3.6825166 0.791564 -1.2149048 -0.9436512 -2.7468314 -4.9420791 -6.2121983][-5.4749546 -5.197845 -4.8691096 -2.2093592 0.52196121 4.2420306 6.7833457 6.6546235 5.143415 2.6459584 0.38171434 -0.62133408 -2.9237516 -4.8624005 -6.5821052][-3.3068683 -3.3146253 -2.9113946 -1.7232354 -0.58512449 1.6229897 3.7958436 3.4356179 2.0101342 0.51876259 -0.41700268 -1.1977224 -3.8836055 -6.1296964 -7.5670719][-3.3560965 -3.878526 -3.3939412 -2.2415073 -2.2006063 -0.76836967 0.58100557 0.51071835 -1.0505424 -2.7834895 -3.591867 -3.3527246 -5.3984981 -7.8029919 -9.2059517][-6.316865 -7.2037144 -6.996809 -6.3800974 -5.8413262 -4.9213223 -3.9153309 -3.4852004 -4.2796845 -5.6810884 -6.8273711 -7.0666695 -8.8730507 -10.461982 -10.906105][-9.9487715 -9.8450308 -9.7982874 -9.0026474 -8.7513828 -8.6245012 -7.3221087 -6.9077406 -7.2696896 -7.3364773 -7.6958466 -8.2798605 -9.5654564 -10.704716 -10.714198][-11.6408 -11.063174 -9.9455395 -8.4584866 -8.739254 -8.8812618 -7.9541607 -7.2289114 -6.8820124 -6.8650103 -7.3744392 -8.0394173 -9.1032248 -9.2907429 -9.0454865][-9.424964 -9.5298824 -8.9891205 -8.1006584 -7.7757463 -8.4903612 -8.3229628 -7.6268077 -6.3226066 -5.9665794 -5.82267 -5.4286671 -5.8030353 -6.4481835 -6.2492442][-7.0696893 -7.6522036 -7.6436176 -7.4343424 -6.90775 -7.04041 -8.3459921 -7.2945175 -6.4415679 -6.4414978 -6.0236993 -5.4724684 -6.1726894 -6.2530532 -6.8062]]...]
INFO - root - 2017-12-15 21:26:44.799153: step 60410, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 49h:15m:01s remains)
INFO - root - 2017-12-15 21:26:51.325305: step 60420, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 50h:52m:41s remains)
INFO - root - 2017-12-15 21:26:58.013156: step 60430, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 51h:34m:42s remains)
INFO - root - 2017-12-15 21:27:04.566375: step 60440, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 49h:12m:34s remains)
INFO - root - 2017-12-15 21:27:11.004347: step 60450, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 49h:13m:49s remains)
INFO - root - 2017-12-15 21:27:17.566910: step 60460, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 48h:45m:11s remains)
INFO - root - 2017-12-15 21:27:24.217575: step 60470, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 50h:46m:06s remains)
INFO - root - 2017-12-15 21:27:30.799097: step 60480, loss = 0.21, batch loss = 0.17 (12.3 examples/sec; 0.648 sec/batch; 48h:59m:43s remains)
INFO - root - 2017-12-15 21:27:37.352437: step 60490, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 48h:46m:19s remains)
INFO - root - 2017-12-15 21:27:43.978963: step 60500, loss = 0.23, batch loss = 0.19 (12.0 examples/sec; 0.669 sec/batch; 50h:31m:14s remains)
2017-12-15 21:27:44.460702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.1675835 -9.6375771 -10.31911 -10.19821 -10.225468 -9.8372507 -10.061232 -10.856687 -10.082054 -8.502758 -7.0085859 -5.7934856 -4.3946533 -5.440505 -5.9027438][-7.5210958 -7.665132 -8.1631966 -7.9699554 -8.8446722 -8.5582495 -8.2302446 -9.5071373 -10.389932 -8.509716 -6.34208 -5.236433 -4.32687 -5.2341766 -6.3338819][-5.4546719 -6.0544767 -7.3825297 -6.6233177 -7.0803428 -7.671082 -8.0945787 -8.3976593 -8.374939 -7.6374536 -6.8457847 -5.418376 -4.3860264 -6.160471 -7.4811125][-6.0472078 -5.4590688 -6.1887069 -5.9269252 -7.6990175 -7.4132242 -6.3449588 -7.3072634 -7.5053606 -6.7563939 -5.2174578 -4.06391 -3.6168141 -6.2754216 -8.0336895][-6.2480578 -6.3634706 -7.1748047 -7.1477323 -7.4799447 -4.5745153 -1.578804 -4.166183 -6.6671128 -5.30207 -4.0600972 -3.1816795 -2.2677608 -5.0445662 -6.6928887][-8.755373 -7.9616108 -7.7433844 -6.4657545 -5.2930632 -1.8710558 2.2104936 1.4222932 -0.099638462 -1.9682717 -4.1915355 -3.5065327 -2.5898254 -5.1788464 -6.1281466][-9.8933258 -8.3201962 -6.75809 -3.9795563 -2.5071819 0.35719252 4.5671039 5.7895579 5.4945874 0.73071432 -4.3221788 -4.028511 -3.8108604 -6.537 -7.8317885][-10.047294 -8.5270262 -6.420927 -1.4080734 0.38716459 2.5689731 5.0317349 5.2620273 5.2545829 1.9448857 -1.8874159 -4.4539518 -6.3304663 -9.0824413 -10.267425][-7.0220056 -5.232347 -4.8160167 -2.4324408 -0.43901157 2.732615 4.8728471 4.8048244 3.8169894 1.3766108 -2.1321697 -5.5011659 -7.5769806 -10.328299 -12.303728][-6.1087222 -4.459353 -4.0390387 -3.129909 -2.3365777 0.73156691 3.3059268 2.39356 0.94301128 -0.752357 -2.4647849 -5.4847097 -7.8053303 -11.488577 -14.936516][-9.7457418 -8.5420666 -7.6473637 -6.2321639 -5.646678 -5.0470834 -4.1584468 -3.6215677 -4.2435923 -5.24279 -6.7536235 -8.8578081 -11.112249 -12.813742 -13.181763][-14.77804 -13.55854 -12.662628 -11.021562 -10.820863 -10.447628 -9.9509163 -9.7318859 -9.8279362 -9.648303 -9.4531069 -9.89212 -10.980547 -12.674744 -13.556208][-15.3976 -14.759325 -13.148108 -11.449873 -11.150772 -10.325516 -10.521968 -10.952551 -10.723099 -9.6608315 -8.74613 -8.5814667 -9.55418 -10.446903 -9.8664284][-14.191631 -14.657101 -13.568356 -11.15147 -9.5904264 -9.4527617 -9.5950108 -9.3071041 -9.0153856 -9.1062069 -8.7711668 -6.9637132 -6.4502807 -7.2727919 -7.2555218][-10.637345 -10.493879 -9.651 -7.4241066 -6.4444022 -5.1953125 -5.2922978 -5.9133687 -6.8908696 -6.4466329 -6.6481647 -6.3628087 -6.9761972 -7.436789 -7.0213547]]...]
INFO - root - 2017-12-15 21:27:51.032958: step 60510, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 49h:06m:34s remains)
INFO - root - 2017-12-15 21:27:57.565216: step 60520, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.648 sec/batch; 48h:55m:55s remains)
INFO - root - 2017-12-15 21:28:04.208606: step 60530, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 50h:50m:14s remains)
INFO - root - 2017-12-15 21:28:10.827038: step 60540, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 49h:39m:29s remains)
INFO - root - 2017-12-15 21:28:17.436306: step 60550, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 49h:58m:21s remains)
INFO - root - 2017-12-15 21:28:23.993242: step 60560, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 49h:41m:42s remains)
INFO - root - 2017-12-15 21:28:30.486173: step 60570, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 49h:31m:00s remains)
INFO - root - 2017-12-15 21:28:37.076882: step 60580, loss = 0.18, batch loss = 0.14 (12.7 examples/sec; 0.630 sec/batch; 47h:37m:01s remains)
INFO - root - 2017-12-15 21:28:43.604614: step 60590, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 49h:35m:53s remains)
INFO - root - 2017-12-15 21:28:50.218758: step 60600, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 49h:18m:28s remains)
2017-12-15 21:28:50.733775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8364615 -8.3593664 -8.5463791 -7.8267403 -7.6102037 -7.1263838 -7.4488378 -7.7175183 -8.2247915 -8.5857611 -8.4274273 -8.5631027 -7.7151818 -5.619009 -2.5214784][-8.0125408 -5.9060073 -5.3536172 -4.5934997 -5.3265438 -5.7492266 -5.9857292 -6.2300591 -6.3718672 -7.0165548 -7.2464285 -6.8377209 -6.3386436 -5.0579023 -2.1844847][-4.422009 -5.2268581 -5.6362782 -3.5812175 -3.7574377 -3.5762513 -3.333411 -3.4505286 -4.360014 -5.847003 -6.7566867 -6.801806 -5.7483883 -5.2134562 -2.9183621][-4.4778705 -5.2065578 -6.6422739 -5.5603414 -5.7593007 -4.9544849 -4.9615655 -5.0723052 -4.8609247 -5.288168 -6.3077345 -6.9214592 -6.2058363 -5.6418729 -3.2087829][-6.6207476 -6.7379017 -6.4341183 -5.5039964 -5.518589 -2.8717794 -1.3702836 -2.5748019 -4.6456337 -5.6196346 -6.4835348 -7.1408324 -6.8819647 -6.2240167 -4.079546][-7.9083591 -6.4346924 -5.5108857 -3.9702878 -1.9269705 1.6723313 4.5977454 3.1558433 1.227282 -1.0727196 -3.7325568 -4.3974581 -5.4704018 -6.3960137 -4.7870755][-10.899614 -7.9525752 -5.9802909 -3.0985863 -1.3784556 2.1564527 6.7759995 6.8687892 5.0041423 -0.20148087 -4.5194159 -5.7408509 -6.7360125 -6.3544726 -5.5215807][-10.638993 -9.5025158 -9.32405 -5.18666 -2.1979055 1.1048312 5.5628772 7.3891644 6.2461429 0.89953279 -3.0955176 -5.4284487 -8.201664 -7.8638363 -5.6043782][-9.1133738 -6.9960771 -7.4759588 -4.9636493 -2.7008605 0.6577239 4.6810079 5.2924218 4.078898 -0.28432703 -4.6924758 -7.7578182 -9.0114231 -8.4097481 -6.2922859][-6.7738752 -6.0259833 -6.3419294 -4.6214495 -4.3428383 -0.80971146 2.2645378 1.8157182 0.98024988 -1.8232195 -4.9341326 -7.5100665 -8.72434 -9.0679541 -7.422998][-8.367115 -7.3799672 -7.0855904 -5.6204166 -5.3595185 -4.2727242 -3.177176 -2.9533167 -3.1995113 -4.9331942 -6.4809613 -8.6436424 -9.1860189 -9.934166 -8.8555584][-12.837997 -12.625834 -12.356538 -11.195636 -11.369331 -10.744637 -10.096025 -9.36412 -9.5022945 -10.334473 -10.912568 -11.174358 -10.066582 -9.459754 -8.6706038][-13.076791 -12.022196 -12.282285 -11.791002 -11.842505 -11.628613 -11.461495 -11.535933 -11.512098 -10.601414 -9.8610687 -10.553226 -9.776823 -8.8813362 -7.4674444][-10.015032 -9.6301842 -10.352411 -10.252993 -9.6334133 -8.9006176 -9.5075808 -9.8503571 -9.2872982 -8.1619787 -7.6528268 -6.5149641 -6.061358 -5.8984351 -5.32537][-9.1298189 -8.0889206 -7.0105128 -7.1719584 -7.6916575 -7.0568976 -7.0573535 -6.69291 -6.9632645 -6.5793934 -6.02262 -5.7596722 -6.30589 -6.69328 -6.3355141]]...]
INFO - root - 2017-12-15 21:28:57.356826: step 60610, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.680 sec/batch; 51h:22m:53s remains)
INFO - root - 2017-12-15 21:29:03.923887: step 60620, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 51h:03m:54s remains)
INFO - root - 2017-12-15 21:29:10.494734: step 60630, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 49h:35m:58s remains)
INFO - root - 2017-12-15 21:29:17.052995: step 60640, loss = 0.15, batch loss = 0.11 (12.7 examples/sec; 0.632 sec/batch; 47h:41m:32s remains)
INFO - root - 2017-12-15 21:29:23.673505: step 60650, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 49h:52m:44s remains)
INFO - root - 2017-12-15 21:29:30.272867: step 60660, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 49h:04m:44s remains)
INFO - root - 2017-12-15 21:29:36.888751: step 60670, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 49h:39m:04s remains)
INFO - root - 2017-12-15 21:29:43.438103: step 60680, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 48h:17m:31s remains)
INFO - root - 2017-12-15 21:29:49.989129: step 60690, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 50h:17m:36s remains)
INFO - root - 2017-12-15 21:29:56.549670: step 60700, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 49h:15m:29s remains)
2017-12-15 21:29:57.093722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.571738 -2.2307787 -2.204407 -2.9533439 -4.5892768 -5.0589619 -5.2213759 -5.1108871 -5.0202942 -4.3542919 -3.8085523 -6.005619 -8.2601347 -8.9828072 -8.8699551][-2.8532684 -2.8126707 -2.8223341 -3.5938263 -4.9389467 -6.290143 -7.3275371 -7.4391966 -7.2276769 -6.384481 -5.1287737 -6.5289311 -8.5625715 -9.379097 -8.9488068][-1.923198 -2.6084015 -2.5904455 -3.4399853 -4.6676817 -5.2843895 -6.3272486 -6.8433132 -6.9911375 -6.6165514 -5.8682694 -6.9732666 -8.3510637 -8.5487413 -7.6729007][-2.5038455 -2.344986 -2.4868176 -3.3267558 -3.5492792 -3.5454078 -3.65566 -4.5640616 -5.7623539 -6.0090833 -5.9163003 -7.1482544 -8.1794853 -8.0980692 -7.8392878][-3.1748505 -3.4440119 -3.4200485 -2.8316591 -1.6676035 -1.0230603 -0.76957273 -2.0637541 -3.7847304 -4.7517757 -5.1646147 -6.5653996 -7.4485474 -7.4438314 -7.3562479][-5.8429818 -5.2915125 -4.51156 -2.8285422 -0.38938618 1.8776946 3.3826919 2.0998244 -0.24139166 -1.7555153 -2.6701171 -4.6893029 -5.9364824 -5.634429 -5.5727272][-7.8654852 -7.3643055 -6.2875733 -4.514163 -0.92699051 3.3520074 6.8138976 6.1326013 3.9965968 1.1025982 -1.2691894 -2.7621903 -3.6432068 -3.9106004 -4.8915386][-8.8543119 -8.7614517 -7.6223221 -5.6194606 -2.1638665 2.0083418 5.2940612 5.2742448 4.2552791 1.9756722 -0.14539623 -1.700758 -3.065038 -3.1972919 -3.7600942][-7.4640474 -8.0391483 -7.5738306 -5.6186504 -2.5688729 0.35331106 2.7481532 3.238421 2.9536033 1.7181993 1.225853 -0.56694984 -2.8664217 -3.7819438 -5.390595][-7.3449178 -7.5574365 -7.1394367 -5.2585564 -3.1909306 -0.63011408 1.2743783 1.4223614 1.2035532 0.93820238 1.250927 -0.30055809 -2.0915174 -3.6474919 -7.3801007][-10.863845 -10.418451 -9.9366159 -8.7169943 -6.8037405 -4.2988977 -2.7591238 -3.09229 -3.4089277 -3.3626184 -3.0274086 -3.2353663 -3.7484941 -4.4817839 -6.7916956][-14.263071 -13.656547 -12.197545 -11.087605 -10.025696 -8.1498623 -7.2186046 -6.8347111 -6.99805 -7.273344 -6.837841 -5.7152667 -5.7474113 -5.5036979 -6.7000709][-12.47868 -12.224096 -11.323622 -10.911622 -11.498363 -10.688004 -10.28018 -9.6510649 -9.1928635 -8.5040312 -7.5244131 -7.5221066 -7.1858611 -6.4977021 -6.6745768][-9.136797 -9.6088343 -9.0797377 -9.2497759 -9.5504637 -9.5913153 -9.7857933 -9.7924776 -9.5436764 -9.2000923 -8.68071 -8.6211052 -7.982255 -8.146431 -8.4230328][-6.1770897 -5.924098 -5.6746264 -5.309917 -5.5829253 -5.300478 -5.122591 -5.6953363 -6.8158507 -7.5801973 -7.8339987 -8.1669979 -8.644824 -9.2023878 -9.7927933]]...]
INFO - root - 2017-12-15 21:30:03.731977: step 60710, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 50h:07m:10s remains)
INFO - root - 2017-12-15 21:30:10.299429: step 60720, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 49h:13m:46s remains)
INFO - root - 2017-12-15 21:30:16.888774: step 60730, loss = 0.32, batch loss = 0.28 (12.5 examples/sec; 0.642 sec/batch; 48h:28m:15s remains)
INFO - root - 2017-12-15 21:30:23.537047: step 60740, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 51h:27m:12s remains)
INFO - root - 2017-12-15 21:30:30.103551: step 60750, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 50h:52m:41s remains)
INFO - root - 2017-12-15 21:30:36.666735: step 60760, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 48h:33m:40s remains)
INFO - root - 2017-12-15 21:30:43.266900: step 60770, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.672 sec/batch; 50h:42m:40s remains)
INFO - root - 2017-12-15 21:30:49.801134: step 60780, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 48h:24m:16s remains)
INFO - root - 2017-12-15 21:30:56.458168: step 60790, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.677 sec/batch; 51h:06m:47s remains)
INFO - root - 2017-12-15 21:31:03.103634: step 60800, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.664 sec/batch; 50h:04m:37s remains)
2017-12-15 21:31:03.629031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0334835 -5.8326178 -6.134706 -5.2809033 -5.9828548 -6.4936385 -6.2965479 -6.5686173 -6.610074 -7.2136407 -7.8819351 -8.9831905 -9.6944427 -8.073947 -4.8689766][-5.3161283 -4.7952509 -4.7156086 -4.3817263 -4.8439083 -5.9824963 -6.8298626 -7.5414624 -8.0497208 -7.7392092 -7.9067292 -9.3046656 -9.9482946 -8.8938227 -6.829289][-2.9896383 -4.3177733 -4.7991314 -3.6490984 -4.599617 -4.8656044 -5.1329865 -5.8497114 -6.1963997 -6.3722639 -7.0013313 -8.22915 -9.3922644 -8.8532257 -6.7379775][-5.0163178 -5.5010562 -5.3668318 -4.4458151 -4.5449557 -4.9064283 -5.1620259 -5.5062995 -5.7852592 -5.9133205 -5.9830451 -6.7987995 -8.1661568 -7.8097534 -6.3426623][-6.2148352 -6.7283449 -6.2050743 -4.0996771 -2.8678672 -1.6061578 -1.5239305 -2.6825571 -4.0029783 -4.4221563 -5.0497861 -6.5351243 -8.2581663 -8.0081558 -6.5255527][-6.7692723 -6.951622 -6.1364255 -3.7646918 -1.2999969 0.8275218 1.6680031 1.3900728 0.1660552 -2.2364082 -4.1297646 -4.6688838 -6.3977113 -6.9972348 -6.1076727][-7.5203443 -6.7068968 -5.3467731 -2.0365984 0.46920013 2.4952731 4.1245608 4.0680661 3.2620482 0.81267262 -1.3360515 -3.2697177 -6.2911844 -5.834621 -4.8817682][-8.760458 -7.296391 -4.8624229 -1.3140125 0.82484913 2.9080758 4.6888614 3.9287934 2.9623437 1.8748393 0.82126093 -2.0855477 -4.8154674 -5.2146988 -4.822185][-7.62531 -6.8616252 -5.3698764 -2.3574064 -0.2834754 1.9052796 2.92413 1.5617895 1.5992017 0.80004311 0.10985613 -2.3130188 -5.606988 -6.20707 -5.8643055][-6.7886066 -5.5572844 -4.88333 -2.6036522 -0.56423426 1.0005803 1.1847844 1.1930542 1.157835 -0.64305925 -1.3534703 -3.2206869 -5.5094476 -6.13719 -5.8884311][-8.0719051 -8.6500759 -8.2712555 -5.9413919 -4.3055105 -2.7390795 -1.6449418 -1.5534091 -1.9961083 -2.6311285 -3.6473577 -6.2813039 -7.6917067 -6.8181872 -6.0807776][-12.062939 -11.349207 -10.123116 -8.2028246 -7.4934511 -6.1646986 -4.7821679 -5.069458 -5.72237 -5.93227 -6.4007649 -7.5935721 -7.9507275 -8.5268135 -8.9800062][-10.598391 -11.113053 -10.362986 -9.7521439 -9.2564993 -7.0770912 -6.6674662 -6.6013184 -6.3568587 -6.843708 -7.7407594 -8.5955791 -8.5119629 -7.21434 -6.14578][-8.23959 -8.3180895 -8.354661 -7.426971 -6.1329231 -5.8046241 -5.5243869 -5.2623377 -6.1559024 -6.88194 -7.1849461 -7.3131161 -6.8220339 -6.8189397 -6.3960791][-5.2704382 -4.735249 -4.8225451 -5.1661749 -4.520474 -3.8813457 -3.4278417 -3.8096759 -4.2937517 -4.7324758 -5.732975 -7.4448647 -8.02996 -7.378057 -6.809958]]...]
INFO - root - 2017-12-15 21:31:10.248410: step 60810, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 49h:10m:16s remains)
INFO - root - 2017-12-15 21:31:16.825205: step 60820, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 49h:19m:41s remains)
INFO - root - 2017-12-15 21:31:23.447278: step 60830, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 50h:59m:33s remains)
INFO - root - 2017-12-15 21:31:30.033683: step 60840, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 51h:10m:41s remains)
INFO - root - 2017-12-15 21:31:36.600906: step 60850, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.648 sec/batch; 48h:52m:31s remains)
INFO - root - 2017-12-15 21:31:43.303649: step 60860, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 50h:06m:43s remains)
INFO - root - 2017-12-15 21:31:49.862724: step 60870, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 52h:05m:29s remains)
INFO - root - 2017-12-15 21:31:56.422414: step 60880, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 48h:41m:42s remains)
INFO - root - 2017-12-15 21:32:03.011887: step 60890, loss = 0.21, batch loss = 0.17 (11.9 examples/sec; 0.671 sec/batch; 50h:36m:28s remains)
INFO - root - 2017-12-15 21:32:09.659844: step 60900, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 49h:48m:45s remains)
2017-12-15 21:32:10.207586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5213175 -5.1304827 -5.927218 -6.5950532 -7.8184462 -9.2026539 -9.3788433 -8.2984934 -8.2895794 -7.7685895 -6.5651922 -8.4208241 -10.996731 -11.122227 -10.575513][-3.5044973 -4.6648355 -5.2577906 -5.7017903 -7.2929754 -8.95883 -10.679358 -11.04861 -10.76314 -9.6796865 -8.6805859 -9.99184 -12.576595 -13.009251 -12.061999][-1.0319777 -1.9843912 -2.616802 -3.0615544 -5.4301319 -6.9640265 -8.0272026 -8.8957434 -9.9498158 -9.341608 -8.064909 -9.1096077 -11.641949 -12.505565 -11.899044][-2.2928166 -2.3978708 -2.2176344 -1.4891443 -2.0919025 -2.5862772 -3.6794987 -4.9478779 -5.5054221 -5.5513396 -5.5391459 -7.1205587 -10.010916 -11.118568 -11.319735][-4.2122664 -4.5550928 -4.3525071 -2.2977605 -0.91349077 -0.46908569 -0.64120579 -0.95338106 -1.0738554 -1.7337081 -2.3616672 -4.6237097 -8.6177082 -9.7498741 -9.3475389][-7.635169 -7.2104416 -5.9764018 -4.3099608 -2.763248 0.30361509 2.6333556 2.551002 2.2437587 1.5472965 0.71832895 -2.4735119 -6.5281944 -8.5573788 -9.6153975][-8.7699642 -7.7943277 -6.3086205 -4.1471658 -1.9889851 0.90245819 2.945333 4.620502 6.1828885 4.880218 2.529407 -1.2071791 -5.4944057 -6.7100863 -7.6541729][-6.7491074 -6.7850132 -6.5411224 -3.8938236 -0.74644852 2.4520135 4.7121978 5.5902772 5.9631619 5.3624949 3.7841334 -0.39751911 -4.9218826 -6.1025629 -5.7113504][-6.5744829 -5.6521893 -4.9315443 -2.2815583 -0.57123995 1.2415471 2.9509072 4.4637828 4.7653413 4.3459487 2.8820796 -0.60756969 -4.5970674 -4.7615995 -3.789475][-6.9357934 -5.8680196 -4.5665121 -3.1236851 -1.9347858 -0.71234179 0.28469133 1.0118175 1.6338377 1.5687051 1.4232554 -1.8558245 -5.5184097 -4.8005738 -3.4108677][-8.7257366 -8.8664169 -7.4594107 -5.8041959 -5.4111128 -5.2475371 -4.6749225 -4.2665796 -3.6805813 -2.5877969 -2.7613645 -5.7828884 -7.5054855 -6.7334557 -4.7334766][-11.980692 -10.606897 -8.8518314 -7.0854387 -6.2436504 -5.9951468 -6.0925908 -6.6756272 -7.3840427 -7.4795308 -7.6008878 -8.7656717 -9.7463036 -8.636137 -6.9048276][-13.410971 -11.532553 -9.1125078 -7.9568119 -7.8199768 -7.7394528 -7.5868158 -7.5343094 -8.115551 -7.8698492 -7.7575827 -8.8631039 -9.9481173 -8.2701864 -5.9137669][-13.670182 -12.356152 -10.084297 -8.6270618 -7.647562 -7.5643039 -7.102335 -5.8316369 -4.6970272 -5.3422742 -6.5770578 -5.7956905 -6.0016212 -5.5729547 -4.6281691][-8.6579237 -9.0250263 -8.1822472 -7.7841334 -7.9848022 -7.5761471 -6.4911613 -5.5739646 -4.5338321 -3.1689036 -1.9085832 -3.3847306 -4.7153397 -5.1896377 -5.5869694]]...]
INFO - root - 2017-12-15 21:32:16.756282: step 60910, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 48h:56m:03s remains)
INFO - root - 2017-12-15 21:32:23.298694: step 60920, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 49h:04m:30s remains)
INFO - root - 2017-12-15 21:32:30.020037: step 60930, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 50h:15m:59s remains)
INFO - root - 2017-12-15 21:32:36.683085: step 60940, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 49h:12m:53s remains)
INFO - root - 2017-12-15 21:32:43.314901: step 60950, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 49h:19m:37s remains)
INFO - root - 2017-12-15 21:32:49.888811: step 60960, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 49h:44m:05s remains)
INFO - root - 2017-12-15 21:32:56.506234: step 60970, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 49h:29m:58s remains)
INFO - root - 2017-12-15 21:33:03.140528: step 60980, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.681 sec/batch; 51h:22m:54s remains)
INFO - root - 2017-12-15 21:33:09.737487: step 60990, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 48h:56m:26s remains)
INFO - root - 2017-12-15 21:33:16.405279: step 61000, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 50h:58m:56s remains)
2017-12-15 21:33:16.957514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0836935 -7.53421 -6.2417932 -6.0356145 -6.5017743 -6.2248392 -5.9037995 -5.3240142 -5.1716337 -4.1029739 -3.1016395 -4.6043425 -6.8053722 -6.4755287 -8.1517467][-6.8551297 -7.462626 -6.7318468 -6.5446887 -6.464047 -6.0113068 -5.859489 -5.9127254 -5.5466905 -4.8927383 -3.7748537 -5.211832 -6.9509859 -6.5859995 -8.4406023][-5.0708351 -6.331944 -6.6708736 -6.1330805 -6.2447543 -6.230258 -6.0182977 -5.3981905 -4.5923786 -4.5576248 -3.8358154 -5.56278 -7.2422523 -6.9665427 -8.82407][-7.0183706 -7.5359192 -6.7334929 -6.6467562 -6.8118353 -5.7822952 -4.8977647 -4.9101686 -5.0435505 -4.2296019 -2.9096057 -4.8321271 -7.1469226 -7.5724354 -9.5589523][-8.0383224 -9.7651205 -9.9045658 -7.7956367 -5.9624944 -4.1833005 -3.2420974 -3.4776671 -4.223516 -3.5778542 -2.6014647 -4.3624134 -6.1311288 -6.7114067 -9.2628021][-9.4185133 -10.239208 -8.7759457 -5.3529677 -3.0638309 -0.59107685 1.2514987 1.3926878 0.33662319 -0.82154083 -2.2060304 -3.3210361 -4.6753941 -5.6117187 -7.556036][-10.049686 -10.817695 -8.6646519 -3.920892 -0.75042868 2.7387881 4.8224645 4.9486508 4.7613864 1.6894746 -1.5870357 -4.1734967 -5.8983126 -5.9507532 -7.8118382][-10.9565 -10.620675 -7.5464621 -2.7788928 1.0147805 4.79602 6.3256412 5.6915555 5.631907 2.8630195 -0.14191771 -4.9132152 -8.8477182 -9.5262089 -10.490724][-8.6608582 -7.6487408 -5.72991 -2.8507466 -0.034900188 2.5729146 3.5376973 3.5549941 3.4608054 1.4969301 -0.44083834 -5.706553 -10.70219 -11.291136 -13.070706][-6.705966 -5.7039161 -4.168437 -2.5097318 -0.59337568 0.013637066 0.4605279 0.75776243 0.19797277 -0.32541895 -1.2870321 -5.5350432 -10.381475 -12.102051 -15.864313][-8.9932213 -7.9129725 -6.4257951 -4.4723377 -3.8682833 -3.8412166 -3.6694911 -3.9614034 -4.6833 -4.870255 -5.6169028 -8.8675985 -11.08162 -13.012859 -15.305092][-13.302717 -12.233937 -10.595011 -9.1509619 -9.4105406 -8.8179569 -8.23037 -9.0747814 -9.5934429 -8.9329834 -8.609354 -10.057261 -11.280775 -12.547028 -13.25419][-15.894617 -14.519022 -12.438211 -11.547371 -11.281584 -11.223585 -11.689609 -11.261857 -11.281618 -11.140369 -10.570036 -9.8431206 -9.9474163 -9.0156584 -8.7171574][-13.725763 -13.337242 -12.027989 -10.387502 -9.5929365 -8.6902294 -9.1350079 -9.2050114 -8.9801617 -8.8824253 -9.3283663 -8.046195 -7.8047161 -6.9107656 -7.0198131][-9.7032948 -9.4631319 -8.332778 -7.3998842 -6.6553235 -5.2188063 -4.2452655 -4.9718366 -6.1602459 -6.2399788 -6.6342449 -7.6595945 -8.5054188 -7.7267551 -7.6831961]]...]
INFO - root - 2017-12-15 21:33:23.590353: step 61010, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 49h:01m:54s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 21:33:30.231060: step 61020, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 49h:00m:59s remains)
INFO - root - 2017-12-15 21:33:36.843026: step 61030, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 49h:09m:04s remains)
INFO - root - 2017-12-15 21:33:43.508386: step 61040, loss = 0.16, batch loss = 0.11 (11.5 examples/sec; 0.698 sec/batch; 52h:37m:58s remains)
INFO - root - 2017-12-15 21:33:50.092646: step 61050, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 50h:34m:10s remains)
INFO - root - 2017-12-15 21:33:56.752256: step 61060, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 49h:56m:14s remains)
INFO - root - 2017-12-15 21:34:03.255486: step 61070, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 48h:53m:22s remains)
INFO - root - 2017-12-15 21:34:09.860285: step 61080, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 49h:03m:29s remains)
INFO - root - 2017-12-15 21:34:16.506620: step 61090, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 51h:08m:07s remains)
INFO - root - 2017-12-15 21:34:23.113387: step 61100, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 49h:48m:39s remains)
2017-12-15 21:34:23.596994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.1258936 -8.8680372 -7.6942353 -7.2759752 -8.081336 -8.3518066 -8.258152 -7.8664045 -7.032362 -6.1726246 -5.0340476 -6.6798677 -7.193799 -9.3359623 -11.036963][-7.3409786 -7.8195295 -6.55464 -6.5190058 -6.6940084 -7.8886485 -9.15451 -9.1741762 -8.1935053 -6.9586754 -5.7359738 -7.5059028 -8.2706537 -10.212562 -11.961851][-4.7022929 -6.1499262 -7.1323848 -6.57441 -6.6002884 -7.6177154 -8.4614544 -9.372364 -8.990016 -7.6889191 -5.9085431 -8.2633018 -9.32366 -10.355378 -11.877863][-3.6178393 -4.7650995 -4.9572277 -5.6848111 -6.8990855 -7.3160024 -7.26223 -7.0836573 -6.592217 -6.0008116 -5.8569536 -8.630868 -9.993845 -11.777271 -13.189501][-5.86454 -5.9228587 -6.2517128 -5.6396012 -6.0522127 -6.0745273 -5.0172424 -5.126471 -5.4521422 -5.1298795 -4.5731812 -7.6467524 -10.082983 -12.129025 -13.835566][-7.011436 -6.9276543 -6.2660131 -5.1635194 -4.2806511 -1.9498756 0.83239603 0.61075211 -0.44445658 -2.1270456 -3.795836 -7.4421053 -9.3736105 -10.820877 -12.203522][-8.5210962 -7.8156824 -6.9214554 -4.4701734 -2.4346914 0.2390151 2.9399838 3.7718606 4.2495189 0.90576696 -3.266026 -7.2982082 -9.4855776 -11.053032 -11.346556][-8.2716141 -7.7086811 -6.6684823 -3.7768974 -1.6484313 1.8872848 5.1956534 6.3567128 6.2396379 1.6843648 -2.491461 -7.6242924 -10.42523 -10.68689 -10.867383][-6.4511747 -5.1993055 -4.5886736 -3.2459202 -2.2204578 0.81913471 4.0307956 4.6287608 4.5588889 2.2718134 -1.117826 -7.1047735 -10.718442 -12.394003 -12.922667][-4.3295908 -3.872715 -3.3208866 -2.4781985 -2.2324584 -0.95129204 0.49515486 1.3490167 1.6594572 0.15844965 -1.7747562 -7.0891032 -10.042953 -12.588454 -14.178909][-6.3372731 -5.7626004 -5.8528047 -5.2354393 -5.4296527 -5.3804312 -4.5319443 -4.2375689 -3.5409505 -3.7527449 -5.167594 -9.6802845 -12.589619 -14.740776 -15.557842][-11.895777 -11.187088 -10.268146 -9.1411285 -9.6986446 -9.7179775 -10.392148 -10.447009 -9.87149 -9.2023087 -9.7403946 -12.03808 -12.580986 -14.459656 -14.584274][-15.778332 -15.186928 -13.293087 -11.747921 -11.790476 -12.272846 -12.483308 -12.412159 -12.403384 -12.291658 -12.804523 -13.549747 -13.165966 -13.437473 -13.308315][-13.41979 -13.075716 -11.059502 -9.6979218 -8.57981 -8.6762743 -8.9320707 -9.743145 -10.650927 -10.922453 -12.08213 -10.647319 -9.77876 -10.291341 -9.7258377][-9.1113014 -8.3791771 -7.644237 -5.5006528 -4.3215103 -4.7128639 -5.0900192 -5.3041859 -6.2769036 -7.5726275 -8.5596867 -9.9654331 -10.854589 -10.100021 -9.3409967]]...]
INFO - root - 2017-12-15 21:34:30.127984: step 61110, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.634 sec/batch; 47h:47m:52s remains)
INFO - root - 2017-12-15 21:34:36.727103: step 61120, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 49h:27m:42s remains)
INFO - root - 2017-12-15 21:34:43.331866: step 61130, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 49h:04m:02s remains)
INFO - root - 2017-12-15 21:34:49.924859: step 61140, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 49h:59m:43s remains)
INFO - root - 2017-12-15 21:34:56.434601: step 61150, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 50h:00m:42s remains)
INFO - root - 2017-12-15 21:35:03.062422: step 61160, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 50h:13m:33s remains)
INFO - root - 2017-12-15 21:35:09.608958: step 61170, loss = 0.20, batch loss = 0.15 (12.6 examples/sec; 0.637 sec/batch; 48h:02m:33s remains)
INFO - root - 2017-12-15 21:35:16.224609: step 61180, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 49h:16m:54s remains)
INFO - root - 2017-12-15 21:35:22.876177: step 61190, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 50h:07m:08s remains)
INFO - root - 2017-12-15 21:35:29.611081: step 61200, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 50h:30m:39s remains)
2017-12-15 21:35:30.124868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5010757 -6.1708765 -5.6900578 -6.0614624 -6.6623869 -7.932848 -7.4036326 -6.6664782 -6.49291 -6.5335808 -7.0157166 -6.6711755 -8.3149014 -9.6001244 -12.019934][-5.1725688 -6.2731314 -6.5224919 -6.9018135 -6.9819365 -6.6418958 -6.0160322 -5.0549626 -5.1846375 -5.9127312 -5.5368676 -6.17763 -8.9969492 -9.2852612 -11.875334][-4.4871087 -5.4603343 -6.9444976 -6.8647494 -6.9278703 -8.2513714 -7.1769276 -6.0763741 -4.2705383 -3.1555755 -3.8088269 -5.0310187 -7.7300472 -10.076159 -12.589018][-4.4063292 -6.4050207 -7.4526825 -7.4856791 -8.8147182 -8.4722347 -7.6131921 -5.3749528 -3.1494393 -2.5600481 -2.7499046 -4.2846947 -6.7558789 -7.4907045 -9.7210932][-4.954217 -6.0957961 -7.107584 -8.7783022 -8.320858 -7.3806868 -4.4822512 -2.0988195 -2.5438 -1.2733984 -1.6530428 -3.7711017 -5.8065758 -7.2981462 -9.9768677][-6.9585953 -6.4348974 -6.8224874 -6.9848833 -5.8928523 -1.4202108 3.5430369 2.8460736 1.4749961 -0.0066041946 -3.5797348 -3.8584013 -4.9964929 -6.5702848 -9.2071762][-6.6113439 -6.7132158 -6.0182962 -5.68505 -2.4204943 1.5833707 5.5006442 7.951077 6.9775014 2.0051923 -2.9768403 -3.7286654 -5.7201638 -7.6788073 -9.7469711][-7.7466087 -6.5827384 -5.6590843 -4.2688036 -2.0484579 1.3802814 6.4992833 8.2746773 9.134182 4.8631806 -0.95414162 -4.08389 -7.1167641 -7.4764204 -9.6232262][-6.9164057 -5.7044892 -4.8388634 -3.8650246 -3.2022846 0.39439344 3.6655192 5.8250747 6.4899182 3.3423848 -0.37868595 -4.4878807 -9.17402 -10.638758 -11.67844][-5.46989 -6.3232584 -5.8593822 -5.5195513 -4.1504593 -1.9849715 1.7395911 4.0215344 3.1637921 0.39502954 -4.351449 -7.7068906 -11.871052 -13.54206 -15.491865][-7.2892427 -7.9494753 -9.02978 -7.7434492 -7.2696409 -6.3068638 -4.0142426 -2.531009 -1.9267261 -3.2106874 -6.6859417 -10.674078 -14.626633 -16.371578 -16.654064][-12.180073 -11.45586 -10.800701 -10.573914 -10.35739 -10.117399 -9.615078 -8.8240643 -8.4935169 -8.7078094 -9.5946627 -10.283907 -12.804806 -14.827087 -15.231497][-15.270954 -14.404112 -13.52573 -12.129209 -11.996576 -12.918303 -12.63755 -12.322325 -11.176012 -11.48277 -11.502848 -11.885349 -12.284849 -12.12101 -12.098703][-12.503551 -12.633041 -12.511379 -11.642094 -10.933469 -11.578983 -11.885891 -12.256645 -12.060573 -11.040617 -10.400328 -9.5886726 -9.2066269 -9.8391609 -9.6966839][-8.5517168 -9.4217625 -8.9268246 -8.9104195 -8.4074593 -8.4367771 -8.7494965 -9.8695679 -10.786369 -10.991909 -10.404168 -9.3813677 -9.3403177 -9.2532692 -9.068882]]...]
INFO - root - 2017-12-15 21:35:36.736595: step 61210, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 50h:42m:37s remains)
INFO - root - 2017-12-15 21:35:43.382140: step 61220, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 49h:22m:05s remains)
INFO - root - 2017-12-15 21:35:50.016289: step 61230, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 49h:29m:42s remains)
INFO - root - 2017-12-15 21:35:56.660241: step 61240, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 50h:43m:55s remains)
INFO - root - 2017-12-15 21:36:03.194692: step 61250, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 50h:33m:23s remains)
INFO - root - 2017-12-15 21:36:09.811941: step 61260, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.671 sec/batch; 50h:33m:49s remains)
INFO - root - 2017-12-15 21:36:16.455309: step 61270, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 50h:57m:13s remains)
INFO - root - 2017-12-15 21:36:23.052159: step 61280, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 48h:20m:46s remains)
INFO - root - 2017-12-15 21:36:29.638481: step 61290, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 49h:12m:23s remains)
INFO - root - 2017-12-15 21:36:36.206361: step 61300, loss = 0.22, batch loss = 0.17 (12.2 examples/sec; 0.656 sec/batch; 49h:23m:04s remains)
2017-12-15 21:36:36.773086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.498383 -4.3807425 -4.1387539 -4.8023472 -6.0721803 -6.6790085 -6.7647567 -7.0288038 -8.06427 -8.2989616 -7.514349 -7.1948409 -7.7845025 -7.1601725 -6.1130242][-5.493886 -5.8072696 -5.2791076 -4.7906461 -5.417274 -6.808259 -7.4629364 -7.7423973 -8.504097 -8.964839 -8.8139 -8.0762033 -7.7826185 -7.6289434 -7.0855341][-3.5557578 -5.03438 -6.0617 -5.6294379 -5.8056335 -6.2107353 -5.8094749 -6.8096352 -7.6119366 -7.8671541 -8.4194422 -8.0792494 -8.1245832 -8.544467 -8.9596653][-3.795248 -5.2670336 -5.3446031 -5.17375 -5.6795406 -5.4910455 -4.8340168 -5.5826306 -6.7994552 -7.5712934 -7.6789331 -7.9249163 -9.3161058 -10.039732 -10.504647][-5.4426279 -6.1931276 -6.4608068 -5.4234304 -4.2497563 -1.947927 -0.0078978539 -1.8253102 -3.170161 -4.5036564 -6.2794695 -6.0561781 -7.1107154 -8.7166433 -9.4546223][-7.3104267 -6.3041954 -5.2871308 -4.298058 -1.9667211 1.2072845 3.9923377 4.3977571 3.9201369 0.00077819824 -4.3666649 -5.3165827 -6.433897 -6.7638397 -7.158946][-7.26069 -6.846487 -5.4656591 -2.5011642 -0.30128288 2.4140801 5.8247485 6.7334924 7.174623 1.8946409 -3.6135039 -4.7295637 -7.9165573 -8.595253 -7.0770674][-8.1525755 -7.0333624 -6.2086344 -3.4675527 -0.32913446 3.2204051 6.5455842 7.4806428 7.6092134 3.1457124 -0.83978939 -4.3622665 -8.759182 -9.5954695 -9.240489][-6.2690129 -6.0573621 -6.0802913 -5.1690135 -3.6749582 0.1447525 3.5370698 4.7091031 4.9609017 1.7982545 -1.096467 -5.0337539 -10.058022 -11.446333 -10.972976][-5.3715467 -5.214262 -5.1356192 -5.0185313 -5.5730348 -3.6550331 -1.4311166 0.28898335 1.3711138 -0.82054996 -4.0807009 -7.219306 -9.5586023 -11.207048 -11.781266][-9.2571774 -9.7087727 -9.2585793 -9.184679 -9.23382 -8.6874075 -8.1579885 -7.1210241 -6.4070134 -6.679122 -7.7853107 -10.888922 -13.385455 -12.391399 -10.934165][-13.682914 -13.963438 -13.547994 -12.708429 -12.269772 -12.799362 -13.398075 -13.184217 -12.05207 -11.540897 -11.55648 -12.88357 -12.863209 -13.488904 -13.296166][-13.275415 -13.363138 -12.912538 -12.959908 -13.086103 -11.950418 -11.568573 -12.323792 -12.470913 -11.934328 -11.62274 -11.783248 -10.374259 -9.25873 -8.8943672][-11.827008 -11.139551 -10.081205 -9.90181 -9.8307076 -9.5531387 -9.6389189 -9.2320356 -9.5362759 -10.415489 -10.545652 -9.1913652 -8.1851568 -7.3753953 -5.8132615][-8.7611294 -7.9330015 -6.4227915 -5.2841663 -4.3212271 -3.9198298 -4.7932687 -5.5664911 -5.8642631 -5.7324176 -6.37352 -7.1153326 -7.0369797 -6.2919879 -6.473074]]...]
INFO - root - 2017-12-15 21:36:43.449846: step 61310, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 49h:39m:56s remains)
INFO - root - 2017-12-15 21:36:50.021197: step 61320, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 48h:41m:14s remains)
INFO - root - 2017-12-15 21:36:56.624190: step 61330, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 48h:11m:52s remains)
INFO - root - 2017-12-15 21:37:03.280519: step 61340, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 49h:32m:28s remains)
INFO - root - 2017-12-15 21:37:09.908919: step 61350, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 49h:34m:21s remains)
INFO - root - 2017-12-15 21:37:16.483472: step 61360, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 49h:56m:34s remains)
INFO - root - 2017-12-15 21:37:23.027229: step 61370, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 49h:30m:06s remains)
INFO - root - 2017-12-15 21:37:29.598812: step 61380, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 48h:56m:56s remains)
INFO - root - 2017-12-15 21:37:36.239965: step 61390, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 49h:33m:07s remains)
INFO - root - 2017-12-15 21:37:42.933114: step 61400, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 50h:15m:12s remains)
2017-12-15 21:37:43.454017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7149315 -4.8963413 -5.535079 -6.0642834 -7.2772074 -7.9024124 -8.61861 -8.9492779 -9.2928467 -9.378334 -9.0724716 -9.1696205 -10.482664 -10.210142 -6.2104163][-7.2480078 -5.8715043 -5.936358 -6.5580573 -7.5029931 -8.9345312 -9.7042246 -9.655571 -9.5404577 -9.273675 -8.95952 -9.2316408 -10.580667 -10.704082 -7.3221955][-5.5365868 -6.238225 -7.40346 -6.9174438 -7.5408182 -8.3780394 -9.23456 -9.35134 -9.0318031 -8.7323732 -8.768507 -8.4125824 -9.8366413 -10.840289 -8.3549213][-5.9467111 -5.8031626 -6.4761 -6.8660631 -7.9644108 -7.8104644 -7.80124 -8.6356907 -8.5187759 -8.0842628 -8.093976 -7.7562356 -8.7020321 -9.1866655 -7.1841545][-6.3704863 -7.4790487 -8.208477 -7.6207581 -6.8969755 -5.9095321 -5.2477584 -5.9843907 -6.5883641 -6.6766429 -6.9839473 -6.9034081 -8.6112 -9.0193386 -6.225966][-6.7034106 -6.3233938 -5.8848333 -5.212431 -4.3206658 -0.72904539 1.8038702 1.1165304 0.2486105 -2.0963473 -4.2615633 -4.4845705 -6.4156809 -7.829051 -6.1117563][-8.0167084 -7.4546356 -6.4608364 -4.8266072 -2.5854416 0.9525423 5.0364623 7.0170445 6.7787118 2.361342 -1.4175477 -2.4515486 -5.3231444 -7.3539381 -6.5290365][-7.987154 -8.0871487 -7.3376927 -3.8297181 -0.651855 3.06529 5.8960061 8.3400936 9.470356 4.2116151 -1.5116754 -3.8168182 -6.4014964 -7.4910164 -6.6729436][-5.4146423 -5.8345542 -6.4045053 -4.3143425 -2.1706524 1.8339214 5.0650668 6.7939134 7.1837087 4.1677537 -0.25829506 -4.0116763 -8.0401287 -9.4106617 -7.4698281][-3.4300754 -3.0822539 -3.827004 -3.468766 -2.5003211 -0.19824886 1.3881917 2.8188996 3.1066632 -0.386477 -3.4843435 -6.2029133 -9.8682022 -11.884512 -10.931094][-5.60528 -4.6584396 -4.4353466 -4.4291272 -4.5762634 -3.9965835 -3.9728332 -3.9359751 -4.5537729 -6.4915304 -8.6556253 -10.297854 -12.855989 -13.751724 -11.103676][-10.146942 -9.4782887 -9.1917238 -7.9899712 -7.6853542 -8.5656233 -9.549017 -9.69982 -10.217639 -11.536371 -12.787125 -13.006348 -13.501892 -12.859915 -9.9652691][-11.601573 -10.363021 -9.1128473 -8.0823135 -9.0880756 -9.3475008 -9.4614439 -10.62374 -11.37532 -11.772568 -11.856637 -12.096048 -12.166373 -11.060566 -7.3276482][-11.204559 -10.138485 -9.0742483 -8.4549208 -8.5989313 -8.5011892 -9.4781685 -9.5181522 -9.16957 -9.6039267 -9.7727938 -9.1913395 -8.7672749 -8.859602 -6.3247852][-9.520937 -8.44003 -6.8604412 -5.5993109 -5.55776 -5.6750937 -6.1002626 -5.9402986 -6.0351038 -6.4534883 -6.6298156 -7.0940633 -7.2719879 -6.6401596 -5.4538932]]...]
INFO - root - 2017-12-15 21:37:50.042125: step 61410, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.684 sec/batch; 51h:30m:12s remains)
INFO - root - 2017-12-15 21:37:56.635031: step 61420, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 48h:48m:32s remains)
INFO - root - 2017-12-15 21:38:03.212613: step 61430, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 49h:51m:52s remains)
INFO - root - 2017-12-15 21:38:09.883664: step 61440, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 50h:36m:19s remains)
INFO - root - 2017-12-15 21:38:16.488379: step 61450, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 49h:25m:19s remains)
INFO - root - 2017-12-15 21:38:23.014248: step 61460, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 51h:04m:49s remains)
INFO - root - 2017-12-15 21:38:29.607345: step 61470, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 49h:41m:47s remains)
INFO - root - 2017-12-15 21:38:36.273588: step 61480, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 49h:39m:14s remains)
INFO - root - 2017-12-15 21:38:42.951614: step 61490, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 48h:20m:22s remains)
INFO - root - 2017-12-15 21:38:49.609418: step 61500, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.644 sec/batch; 48h:30m:31s remains)
2017-12-15 21:38:50.163767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7213354 -4.3651671 -4.3396311 -4.2180452 -3.4035194 -3.6185296 -3.1942048 -4.5649815 -4.3808637 -3.5012567 -3.2019775 -4.4921875 -5.4251637 -5.9541745 -6.3603897][-2.4346461 -2.6651773 -2.5890858 -1.647038 -0.4000349 -0.37316465 -1.4562507 -1.8251235 -1.8938611 -2.2535067 -2.1480021 -4.2467031 -5.6966848 -5.571857 -6.7987356][-2.6729131 -3.8786356 -3.99827 -1.9652727 -1.0978532 -0.88053274 -1.027905 -2.111237 -3.0158679 -3.2367196 -2.7105241 -4.5800924 -6.9870219 -7.1320572 -7.7826118][-3.8966312 -4.9581652 -5.2931328 -4.8470769 -3.6913905 -2.0687342 -1.3159862 -2.0373297 -2.635674 -2.786612 -2.6602631 -5.7272329 -8.6502132 -9.2129478 -9.9840508][-5.5187912 -7.0767026 -7.2371016 -6.9577689 -5.54794 -2.2261066 -0.079205036 -2.0704432 -3.7787037 -2.4169192 -1.7141628 -5.0454607 -8.634922 -11.144357 -13.036335][-7.30552 -6.8296404 -6.4078207 -4.0498128 -1.4908628 0.74616241 3.41215 2.0229187 -0.053175449 -0.89943409 -1.8431411 -4.6611323 -8.2486439 -11.123846 -12.98621][-6.5935049 -6.7886062 -5.039464 -1.8671575 1.0331087 3.9827657 6.2299705 6.2337689 4.6508527 0.79307842 -2.1185248 -5.0612173 -8.619463 -11.129225 -12.951738][-5.950017 -5.7910056 -5.0876317 -1.1244292 2.0230856 4.7868524 7.3838038 7.1750445 5.2433133 2.5619254 -0.14295626 -4.723803 -8.8452473 -10.748661 -11.825153][-4.8149147 -4.5304422 -4.1699028 -1.5517597 0.0794878 2.7198777 5.4457889 4.806211 2.7082753 0.25775433 -1.6559315 -6.6926384 -10.237566 -11.391611 -12.141407][-3.679889 -3.9694672 -3.4584453 -2.5444329 -2.1933191 -0.11183167 1.5653925 1.6723924 -0.0073561668 -2.2040367 -3.745575 -8.1606522 -11.805487 -13.000435 -13.967142][-7.5906043 -6.9684534 -6.9096003 -6.8786922 -5.3975644 -6.215416 -5.9425535 -4.7533855 -4.9815326 -5.7220225 -6.7277722 -10.896942 -13.112194 -13.418435 -13.44771][-12.427744 -12.422016 -12.081216 -11.409988 -11.095976 -11.369881 -11.416155 -11.028168 -10.173347 -9.31734 -9.9093342 -11.864161 -12.839081 -13.275947 -13.393995][-16.010889 -15.697311 -15.557411 -14.369772 -14.761677 -14.288618 -13.693108 -13.898609 -13.561659 -12.034477 -11.21933 -11.154329 -11.737463 -10.992079 -10.15361][-15.553911 -15.24287 -14.434845 -14.237415 -14.125237 -14.048849 -14.589842 -13.681681 -12.163844 -11.789622 -10.979177 -9.3393488 -9.130785 -8.5999193 -8.3008766][-10.92787 -11.26313 -10.988136 -10.604679 -9.8787718 -9.91263 -10.158313 -9.7974892 -9.92872 -9.4226055 -9.1362476 -9.3788137 -9.2117214 -7.9845343 -8.3282948]]...]
INFO - root - 2017-12-15 21:38:56.819302: step 61510, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 48h:53m:24s remains)
INFO - root - 2017-12-15 21:39:03.370387: step 61520, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 48h:54m:01s remains)
INFO - root - 2017-12-15 21:39:09.985568: step 61530, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 49h:40m:10s remains)
INFO - root - 2017-12-15 21:39:16.621211: step 61540, loss = 0.22, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 50h:05m:40s remains)
INFO - root - 2017-12-15 21:39:23.219192: step 61550, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 49h:42m:50s remains)
INFO - root - 2017-12-15 21:39:29.824685: step 61560, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.650 sec/batch; 48h:56m:46s remains)
INFO - root - 2017-12-15 21:39:36.436160: step 61570, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 47h:52m:45s remains)
INFO - root - 2017-12-15 21:39:43.020043: step 61580, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 48h:48m:14s remains)
INFO - root - 2017-12-15 21:39:49.664732: step 61590, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.684 sec/batch; 51h:28m:12s remains)
INFO - root - 2017-12-15 21:39:56.309012: step 61600, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.682 sec/batch; 51h:17m:23s remains)
2017-12-15 21:39:56.802385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9460015 -3.5490282 -3.931047 -3.5553362 -4.5100069 -5.1233115 -4.99362 -4.4299273 -3.1848998 -2.3603168 -2.2381418 -4.1815429 -6.5240602 -7.2906981 -7.3864183][-3.7580042 -3.6892469 -3.4282908 -3.4494491 -4.0880461 -5.0650368 -5.0570579 -3.9057992 -3.1338263 -2.4387853 -1.4377546 -3.3624341 -6.0426722 -7.1977315 -6.9398985][-2.677618 -3.325768 -4.4008532 -4.3100991 -5.1093535 -5.5213103 -5.1494617 -4.5351577 -3.6652215 -3.1265163 -3.1702728 -5.0769386 -7.0594306 -7.4921865 -7.5722694][-2.8987405 -3.582747 -4.2095242 -4.4234505 -4.862144 -4.7728648 -4.6489391 -4.7740784 -4.2757773 -2.9628198 -2.0196893 -3.3681836 -5.1450086 -6.3097825 -7.2275591][-3.6708002 -4.6047759 -5.3482838 -4.196085 -3.276396 -2.4513667 -2.050106 -2.7882917 -3.0106189 -2.5717566 -2.2113822 -2.8589325 -4.0586758 -4.8259072 -5.2217364][-4.2690306 -4.4064541 -4.0940385 -3.3993618 -0.9313345 1.7201838 2.7140498 1.7549381 0.7862134 -0.17541552 -0.54285908 -1.5354328 -3.03804 -3.9580567 -4.8517036][-4.1378508 -3.969276 -3.6003473 -2.2145011 -0.78901148 2.2982783 4.943994 4.9565988 4.1309447 1.2453909 -0.45635366 -1.6643467 -3.3738842 -3.5653763 -3.8155365][-4.075036 -3.445303 -2.528641 -1.8703482 -0.59055614 2.4687829 5.120544 5.6111693 4.9055934 2.4227929 0.078767776 -1.9446242 -4.0254555 -4.1568356 -4.2481585][-3.4084938 -3.1351404 -2.4755595 -1.240407 -0.44398975 1.3340611 3.0950017 4.3350053 3.9939742 1.9709525 0.0785532 -3.6414847 -7.2029476 -6.5241828 -5.5485005][-2.9299822 -2.579915 -2.0275979 -1.2079 -1.0070972 0.2904439 1.6423626 2.3525805 1.8635464 -0.40786457 -2.2795873 -5.9303861 -8.8749237 -9.09811 -8.4918861][-6.5414181 -6.4314804 -5.6290531 -4.3137603 -4.01007 -3.291327 -3.2183418 -3.4108782 -3.8456798 -4.5698652 -5.5643482 -9.05424 -11.223551 -10.738846 -9.6748018][-10.087173 -9.8719711 -9.156517 -7.88757 -7.5636683 -6.5138173 -6.380455 -7.0777884 -7.8811865 -8.7155666 -9.6124134 -10.565733 -10.410173 -11.164644 -10.762701][-13.267832 -12.558126 -10.978216 -9.4685144 -8.82611 -8.2465668 -8.5177984 -9.2570763 -10.597495 -10.766253 -10.578642 -11.074032 -11.12684 -10.485804 -9.5409269][-10.395475 -10.288153 -9.1690617 -8.5874386 -7.7940121 -7.5798264 -8.1739063 -8.516325 -8.5746326 -8.7263145 -8.86396 -8.0437536 -7.5987597 -8.087184 -8.4270763][-6.9315324 -7.0521879 -6.4019933 -5.7709913 -4.64335 -4.2369957 -4.3898563 -4.420938 -5.5071969 -5.7815328 -5.5541725 -5.496542 -5.7324328 -6.6688414 -8.257081]]...]
INFO - root - 2017-12-15 21:40:03.436992: step 61610, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 50h:46m:04s remains)
INFO - root - 2017-12-15 21:40:10.049251: step 61620, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 49h:16m:22s remains)
INFO - root - 2017-12-15 21:40:16.637834: step 61630, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 49h:14m:29s remains)
INFO - root - 2017-12-15 21:40:23.263725: step 61640, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 51h:24m:23s remains)
INFO - root - 2017-12-15 21:40:29.960110: step 61650, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.681 sec/batch; 51h:13m:23s remains)
INFO - root - 2017-12-15 21:40:36.582307: step 61660, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 49h:55m:44s remains)
INFO - root - 2017-12-15 21:40:43.211819: step 61670, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 50h:38m:57s remains)
INFO - root - 2017-12-15 21:40:49.738575: step 61680, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 49h:37m:20s remains)
INFO - root - 2017-12-15 21:40:56.238939: step 61690, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 49h:52m:19s remains)
INFO - root - 2017-12-15 21:41:02.814117: step 61700, loss = 0.24, batch loss = 0.19 (12.2 examples/sec; 0.657 sec/batch; 49h:26m:36s remains)
2017-12-15 21:41:03.380792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.034825 -10.818031 -10.75408 -10.880286 -11.808664 -12.379676 -13.30833 -12.872974 -12.41115 -11.916216 -10.044106 -9.6573391 -12.485494 -11.451258 -11.572843][-10.742369 -12.032417 -11.972775 -11.463492 -11.311655 -11.623762 -13.178703 -14.522718 -15.188446 -14.032122 -11.818225 -11.652197 -14.041952 -13.404502 -13.56954][-8.1952343 -10.040939 -12.486755 -12.454676 -11.884003 -11.970118 -12.659876 -13.425276 -14.369905 -14.134979 -12.434783 -11.394064 -13.66726 -13.477154 -13.956322][-8.5325117 -8.8587694 -9.8942413 -9.6618862 -9.59349 -9.2342157 -9.1600742 -10.647327 -12.285154 -11.768352 -10.623009 -10.896066 -13.636878 -13.194075 -13.617512][-10.527035 -11.58599 -11.877297 -8.8975821 -6.06206 -3.2142425 -1.4650302 -5.0819807 -10.2718 -10.189811 -9.03332 -9.4081993 -12.771139 -13.530144 -14.879761][-13.961315 -13.879288 -13.258377 -9.7575483 -4.4738054 2.2093487 7.2524056 4.0514483 -1.6995678 -5.7442789 -9.4481287 -8.6774406 -10.723429 -12.145285 -13.959387][-14.372313 -13.853394 -12.849947 -8.1530428 -3.0465963 2.789845 9.4651814 9.9589081 6.9514956 -0.81127787 -8.4570389 -8.8145485 -11.754398 -12.127854 -13.0495][-13.310232 -13.040666 -11.793439 -6.7763834 -1.074574 3.7534719 8.58054 8.2041817 6.0266414 1.8079867 -4.045939 -8.2573624 -14.007885 -14.010635 -14.971584][-9.6320267 -9.9680872 -10.640293 -7.2326269 -2.9909167 1.9016232 6.5257773 6.3910079 3.2669015 -0.88934469 -5.26943 -9.1138535 -15.753599 -17.666016 -18.604067][-8.5433722 -8.4447041 -9.1824532 -7.7553144 -6.0610971 -4.0896225 0.954474 2.6180568 0.004070282 -2.7019989 -6.1685882 -10.226929 -16.14217 -18.053835 -20.320482][-13.008065 -12.741085 -12.110216 -12.051504 -11.615915 -11.319904 -9.0722475 -7.8646455 -8.3036823 -7.9784431 -10.086097 -13.469063 -17.601097 -18.822414 -20.240292][-17.856165 -17.37604 -17.156727 -16.774168 -17.201088 -17.090107 -15.742455 -15.060625 -13.921595 -13.189304 -13.638552 -14.459286 -16.538933 -16.406376 -16.760036][-19.167513 -18.948746 -17.927031 -18.93557 -18.842163 -17.543447 -16.960833 -16.591307 -16.123285 -15.08149 -13.897097 -14.322556 -14.764082 -13.102895 -13.049099][-16.478163 -15.261772 -14.689837 -14.134123 -13.273055 -13.941586 -14.510092 -13.791256 -13.259811 -13.56472 -13.235264 -11.998589 -11.549049 -10.538887 -10.503778][-10.792135 -9.4549217 -8.3872623 -7.3836417 -5.9334278 -6.4597445 -7.379775 -8.2402573 -8.810071 -8.8697624 -9.2890625 -9.998476 -11.124464 -10.742252 -11.025206]]...]
INFO - root - 2017-12-15 21:41:10.025713: step 61710, loss = 0.17, batch loss = 0.13 (11.6 examples/sec; 0.687 sec/batch; 51h:41m:57s remains)
INFO - root - 2017-12-15 21:41:16.683809: step 61720, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 48h:36m:33s remains)
INFO - root - 2017-12-15 21:41:23.324394: step 61730, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 50h:23m:04s remains)
INFO - root - 2017-12-15 21:41:29.921251: step 61740, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 49h:07m:30s remains)
INFO - root - 2017-12-15 21:41:36.575009: step 61750, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 49h:54m:36s remains)
INFO - root - 2017-12-15 21:41:43.220299: step 61760, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 49h:26m:49s remains)
INFO - root - 2017-12-15 21:41:49.869708: step 61770, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 48h:07m:39s remains)
INFO - root - 2017-12-15 21:41:56.486150: step 61780, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 49h:16m:18s remains)
INFO - root - 2017-12-15 21:42:03.080279: step 61790, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 50h:51m:24s remains)
INFO - root - 2017-12-15 21:42:09.737128: step 61800, loss = 0.24, batch loss = 0.20 (12.0 examples/sec; 0.667 sec/batch; 50h:09m:04s remains)
2017-12-15 21:42:10.304268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8872132 -5.9944859 -6.66497 -5.8434911 -5.8711905 -6.9394064 -7.775238 -8.4863834 -8.6860838 -7.7613888 -6.116684 -5.9138041 -6.3010421 -5.746985 -5.0936446][-3.9934766 -3.7051456 -4.1764631 -5.0603466 -6.3260126 -6.1114893 -5.7251258 -5.8511658 -6.3312559 -6.3243513 -5.4388204 -5.4379368 -5.7373433 -6.6383872 -6.4268513][-2.7034128 -2.8811946 -3.323617 -3.0237093 -4.154192 -5.2098436 -5.8203321 -5.5638041 -5.6919031 -6.1791453 -5.1079903 -4.978507 -5.972909 -7.070653 -7.3132496][-4.4009781 -4.7152834 -4.3911743 -3.9038181 -4.556469 -3.9259467 -4.0267944 -5.5590086 -6.4766536 -6.3011813 -5.2761688 -5.2583475 -6.0764108 -7.5282712 -7.6349607][-4.4493966 -5.85844 -6.2569604 -4.2275848 -3.6323476 -2.6606388 -1.7917483 -3.2654378 -4.7597303 -4.373385 -3.6521029 -4.0682673 -4.8623834 -6.9143486 -7.5493259][-5.8991966 -6.4814782 -5.3128228 -2.8201468 -1.1625347 1.8236628 3.1767163 0.77364588 -1.1211982 -1.9145463 -2.6667738 -3.2767167 -4.6993442 -7.2869987 -7.9225059][-6.7208767 -6.3756776 -4.8911672 -2.3194559 0.14299059 4.3372512 6.9692674 4.9323297 2.2765632 0.11563492 -1.4082255 -3.1566968 -4.7005067 -7.1205249 -7.9339838][-9.0308838 -6.8286481 -4.4705715 -0.85655928 1.3477941 5.1052537 7.6582913 6.3418651 4.0983415 0.71421242 -2.016444 -3.7982824 -5.1148367 -7.1266761 -7.532908][-8.9247589 -7.1454964 -6.0997763 -1.6947389 0.91395426 3.2428155 4.6618752 3.4893546 0.88643551 -1.016099 -2.3866529 -4.6645741 -5.7633471 -6.4525032 -6.0099359][-8.7152967 -7.629519 -6.4513426 -2.8903031 -0.38289309 1.9087553 3.0994687 1.5845413 -0.70719576 -3.0959778 -4.4847045 -6.24463 -6.559391 -7.3008022 -6.2475824][-12.127619 -10.617386 -8.6440449 -4.6764297 -2.899076 -1.3381314 -0.26265478 -1.0670519 -3.7423515 -5.2386847 -5.7933311 -7.0508289 -7.8134604 -7.2195826 -4.9958725][-16.534971 -14.636831 -11.601643 -7.3724957 -4.33093 -3.1473978 -3.9181461 -4.0972285 -5.1056714 -7.02138 -8.2069368 -7.5211821 -6.9912481 -6.2371969 -4.8771057][-15.290039 -14.121841 -11.898003 -8.5889091 -6.4637394 -4.8681426 -4.7241263 -5.126689 -6.6048946 -7.0510726 -6.69625 -7.2323112 -6.63436 -4.7726464 -3.0868373][-13.13446 -12.648714 -10.989037 -8.44623 -6.2333016 -5.4727931 -6.2034926 -5.2138534 -4.7330065 -5.6482635 -6.3306785 -5.8988919 -5.5209107 -4.0784569 -2.6040776][-8.0790243 -9.0827713 -8.3811827 -7.2120738 -5.8489923 -3.6895704 -3.1771433 -4.2273865 -4.7052431 -4.3667603 -4.3237462 -5.8945017 -6.2134705 -5.4595652 -5.2432408]]...]
INFO - root - 2017-12-15 21:42:16.890401: step 61810, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 48h:40m:11s remains)
INFO - root - 2017-12-15 21:42:23.425812: step 61820, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 49h:41m:28s remains)
INFO - root - 2017-12-15 21:42:29.987562: step 61830, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 49h:53m:23s remains)
INFO - root - 2017-12-15 21:42:36.625477: step 61840, loss = 0.18, batch loss = 0.14 (11.6 examples/sec; 0.690 sec/batch; 51h:52m:04s remains)
INFO - root - 2017-12-15 21:42:43.232037: step 61850, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 49h:53m:23s remains)
INFO - root - 2017-12-15 21:42:49.766341: step 61860, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 47h:54m:31s remains)
INFO - root - 2017-12-15 21:42:56.368439: step 61870, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 48h:51m:34s remains)
INFO - root - 2017-12-15 21:43:03.019972: step 61880, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 49h:00m:36s remains)
INFO - root - 2017-12-15 21:43:09.512419: step 61890, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 49h:05m:08s remains)
INFO - root - 2017-12-15 21:43:16.040619: step 61900, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 48h:01m:08s remains)
2017-12-15 21:43:16.569366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0468583 -5.7116427 -5.8793497 -6.3699818 -7.3646417 -7.9246349 -8.503993 -8.2346 -8.398778 -7.9982615 -8.4757729 -9.9776974 -12.181778 -10.883076 -8.0223923][-6.4836779 -6.8578529 -7.2653337 -8.0266638 -9.1549416 -9.8021555 -9.9582062 -9.2133207 -8.7812395 -8.8146915 -9.0411949 -8.6587257 -9.6977186 -10.179719 -9.0631161][-3.4895082 -4.8366957 -7.0465889 -7.7680731 -8.3695869 -8.8128519 -9.0097837 -9.0336084 -8.7773914 -8.3107662 -8.6240387 -9.3667755 -10.151909 -9.3471127 -7.9975338][-4.6962118 -4.7821846 -5.2067509 -6.2119131 -6.8674049 -6.5984082 -5.7015681 -5.9159603 -7.0903487 -7.9404354 -8.3940582 -8.8977594 -11.012548 -11.341671 -9.77487][-4.9082856 -5.6063137 -6.50626 -5.858233 -4.9629817 -3.4361327 -1.753927 -2.338953 -3.7289739 -4.9828691 -6.7494287 -8.3484364 -10.574839 -10.735567 -10.245362][-6.4557681 -5.8489618 -4.9810066 -3.5063381 -1.9435651 -0.39283466 1.3550472 2.0273185 1.5709691 -1.7694588 -5.7027435 -6.7882223 -9.5454674 -10.348676 -9.9320335][-7.6887536 -6.3833117 -4.7292047 -2.5703869 -0.0086216927 2.7207608 5.461103 5.2603421 4.3138251 1.2228003 -2.3949826 -5.2618613 -9.2883625 -9.4427671 -9.023222][-7.1610365 -5.8856435 -4.111887 -1.1602545 1.1859035 3.8592267 6.314384 6.1336656 5.9782825 3.2388215 0.17076015 -3.4983706 -8.0632591 -9.996233 -10.623311][-4.8986897 -3.7480228 -2.6871209 -0.62880611 0.98736668 3.1243901 4.9592481 5.0896869 5.3082557 2.448235 -0.9017477 -4.2720137 -8.4413633 -10.710304 -11.433586][-2.5315096 -2.3865225 -3.0028226 -1.8494713 -1.1442509 0.30336523 1.4422197 2.2830253 2.5818276 0.72731876 -1.2989678 -4.5106249 -8.8268919 -11.683786 -12.225386][-6.8495107 -6.7015076 -6.7335734 -6.593246 -7.2150421 -5.8319812 -4.1144252 -3.4410229 -4.0620227 -5.0598397 -6.5487285 -9.21955 -11.19195 -12.344041 -11.929794][-11.369631 -12.033252 -11.179825 -9.6366386 -9.3101025 -9.11256 -9.5959578 -8.92763 -8.1505489 -8.2772408 -10.155376 -12.455112 -13.677994 -14.049892 -13.16613][-11.999349 -11.828108 -10.286173 -9.7189331 -9.9519882 -8.5037174 -7.9084406 -8.8116884 -9.7204742 -10.059772 -10.365927 -11.735148 -13.160454 -12.652667 -10.857335][-9.8674431 -9.6657085 -8.8338518 -6.8865337 -5.1963773 -5.6398292 -6.7152033 -6.5411892 -6.9520578 -8.0497513 -8.9751778 -9.4919691 -10.055506 -9.7164507 -9.5051966][-7.7075319 -7.2606897 -6.979948 -4.8633947 -2.9767711 -3.7925324 -4.5279007 -5.1255531 -5.5880218 -4.8881111 -5.2214565 -6.528584 -7.9902906 -7.8698115 -7.4945374]]...]
INFO - root - 2017-12-15 21:43:23.103402: step 61910, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 48h:59m:17s remains)
INFO - root - 2017-12-15 21:43:29.780737: step 61920, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 48h:28m:47s remains)
INFO - root - 2017-12-15 21:43:36.433820: step 61930, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 50h:28m:09s remains)
INFO - root - 2017-12-15 21:43:43.053247: step 61940, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 49h:49m:56s remains)
INFO - root - 2017-12-15 21:43:49.573448: step 61950, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 49h:10m:55s remains)
INFO - root - 2017-12-15 21:43:56.168534: step 61960, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 49h:40m:39s remains)
INFO - root - 2017-12-15 21:44:02.679855: step 61970, loss = 0.21, batch loss = 0.17 (12.5 examples/sec; 0.640 sec/batch; 48h:06m:19s remains)
INFO - root - 2017-12-15 21:44:09.287622: step 61980, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 48h:48m:28s remains)
INFO - root - 2017-12-15 21:44:15.771512: step 61990, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 49h:26m:04s remains)
INFO - root - 2017-12-15 21:44:22.380353: step 62000, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 50h:49m:29s remains)
2017-12-15 21:44:22.892240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0171146 -5.3516893 -5.0030065 -4.3589435 -4.0018477 -4.256403 -4.6067677 -4.8027649 -4.6133547 -4.3007488 -4.1869226 -4.4853115 -5.727231 -6.2952809 -5.8489194][-4.4581914 -4.2306013 -3.1719775 -2.4959004 -2.5373914 -3.3229423 -3.9112003 -3.2628083 -2.3829129 -2.343858 -2.9047668 -3.0224535 -4.2384706 -5.2066803 -5.5308867][-2.2511411 -1.9191244 -1.1552925 -1.3831754 -2.1010509 -2.61201 -3.5841165 -3.5420055 -2.3347631 -1.4824109 -1.7863936 -2.3120492 -3.519273 -5.1071558 -5.666925][-3.1179063 -2.9417977 -1.9752712 -2.3844352 -2.736707 -2.7059734 -2.829452 -3.0152493 -2.9847019 -2.1494923 -1.459743 -2.2176452 -3.9410148 -4.085032 -3.4980204][-3.55066 -3.8120878 -3.4508166 -3.3502223 -2.9979973 -1.5384965 -0.61108685 -0.53023291 -0.61418438 -0.69968224 -1.1238837 -2.014323 -1.7882409 -2.5531738 -2.8573477][-6.16435 -5.4541473 -4.2137575 -3.8741996 -2.8343191 -0.10868263 1.2300701 1.1970754 1.1924233 1.0814338 0.693326 -0.31577492 -0.99309635 -1.8655207 -2.2486305][-7.7656288 -6.5340419 -5.31683 -3.5879357 -1.7465553 1.0430222 2.6998305 2.7374969 2.0088186 0.94302559 0.13765192 -0.80097151 -2.1674373 -3.1999285 -3.1087358][-8.6632462 -7.8469992 -6.5051155 -3.565047 -0.74883938 1.7536211 3.3021283 3.0357165 1.822814 0.41131449 -0.35199881 -2.0762424 -3.3665526 -4.8542895 -5.0641546][-9.9965076 -7.80441 -6.016757 -3.3372641 -1.6953859 0.044642925 0.79750538 1.205349 0.29693413 -0.86992836 -2.0711792 -3.4787862 -4.6214108 -5.28994 -5.626636][-11.34532 -9.1934452 -7.1525197 -4.2789483 -3.0398304 -1.3304081 -0.16139793 -0.1423316 -1.2817383 -2.7821453 -3.8938956 -4.5459805 -5.5923467 -5.7608323 -5.1614385][-15.00773 -13.534646 -10.541584 -6.8104258 -5.1778331 -3.8317013 -1.9610078 -2.2400005 -3.5075974 -4.9351268 -5.9366565 -6.8430319 -6.6148252 -6.4084325 -5.4656529][-15.102663 -13.875463 -11.602152 -8.8235283 -7.3801441 -5.8194003 -4.3244791 -4.1374021 -4.8628688 -6.1260738 -6.8018045 -7.4657235 -7.2108183 -6.854002 -6.0487905][-12.280285 -10.590181 -8.6475773 -7.1694489 -7.0077858 -5.8289852 -5.4925981 -5.6034427 -5.9263244 -6.4359179 -6.3773341 -6.4686112 -6.123724 -5.7039738 -4.7674885][-9.8742533 -8.0400219 -6.1541119 -5.4805126 -4.9268985 -5.0700121 -5.4407253 -4.9580293 -5.2494817 -5.9881144 -5.9996567 -5.6261172 -4.9470716 -4.4835348 -3.997035][-6.5001287 -5.650157 -4.6475267 -3.8025236 -4.0575705 -4.3416142 -4.3121367 -4.8129725 -5.0312877 -4.0790725 -3.997519 -3.8781557 -4.4253321 -4.0130672 -4.278266]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 21:44:29.520226: step 62010, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 51h:27m:43s remains)
INFO - root - 2017-12-15 21:44:36.064914: step 62020, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 51h:16m:07s remains)
INFO - root - 2017-12-15 21:44:42.638758: step 62030, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 50h:24m:08s remains)
INFO - root - 2017-12-15 21:44:49.168214: step 62040, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 48h:56m:05s remains)
INFO - root - 2017-12-15 21:44:55.805424: step 62050, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 50h:16m:37s remains)
INFO - root - 2017-12-15 21:45:02.390242: step 62060, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 51h:22m:29s remains)
INFO - root - 2017-12-15 21:45:08.980147: step 62070, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 49h:57m:51s remains)
INFO - root - 2017-12-15 21:45:15.523302: step 62080, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 49h:26m:17s remains)
INFO - root - 2017-12-15 21:45:22.079483: step 62090, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 49h:03m:20s remains)
INFO - root - 2017-12-15 21:45:28.650663: step 62100, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 50h:27m:39s remains)
2017-12-15 21:45:29.179772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.536252 -6.5743051 -6.6822162 -7.47698 -7.6091619 -8.2642908 -8.2839537 -6.7087121 -5.4550791 -4.2269106 -3.2249007 -4.5848446 -6.8463926 -6.6978703 -7.2535119][-6.8690848 -7.9622726 -7.1539769 -7.27466 -7.3173065 -7.1401858 -6.9675217 -7.1016068 -6.8889847 -5.7859511 -5.0008254 -5.5568957 -6.9431462 -7.3343186 -8.1939411][-4.293066 -5.2659135 -6.4425888 -6.2271557 -6.3900852 -7.3282185 -7.2134504 -6.7179613 -6.46324 -6.4455657 -6.1247554 -7.3713903 -9.3395681 -9.2215424 -10.144743][-3.6557474 -4.5036321 -4.2673883 -4.285017 -4.8480053 -4.78074 -4.5444255 -5.6231484 -6.7882032 -6.6780543 -7.035717 -8.4485579 -11.023346 -10.76578 -11.73992][-2.3123631 -3.0866151 -3.512445 -2.2065597 -2.425658 -2.3242185 -1.9815857 -3.0753796 -4.9777021 -5.6706877 -5.9912691 -8.3270674 -11.689615 -12.03628 -13.461094][-5.1337891 -3.9649239 -2.7401791 -2.389406 -2.2185793 -0.8840456 0.13903761 0.027821064 -1.1356578 -3.2987518 -5.2543163 -7.0005875 -10.257425 -10.97585 -12.197104][-6.0170979 -5.6946864 -3.1390922 -0.66745377 0.79184246 1.9529362 3.4052262 3.2064872 2.8143878 0.59590054 -2.1945438 -5.4612322 -8.5987177 -9.3080463 -10.435098][-6.4554176 -4.7972832 -3.0260637 -0.37906122 1.5956521 3.2175927 4.4103923 4.81086 4.77045 2.4383082 -0.2302537 -4.1341686 -7.8332186 -7.5417705 -8.8413877][-4.6485524 -3.2683787 -1.2002749 -0.055182457 0.70870209 2.9055638 4.287951 3.8494515 3.0093665 2.7775655 1.695437 -2.2966194 -5.3652105 -6.4150724 -7.0953112][-3.8226161 -3.1994953 -1.8977644 -0.32288408 0.36730146 -0.1077261 0.8900795 2.5071034 2.9876533 2.2885814 1.6031804 -1.4055967 -4.7874842 -5.1770148 -6.7130241][-7.6122212 -6.7339773 -5.9073195 -4.7736268 -3.925631 -3.4438689 -2.9329054 -1.830147 -0.50125933 -0.20215082 -1.1173358 -3.5934443 -4.4942317 -4.8239994 -4.4505572][-10.216654 -10.796801 -10.160061 -9.1538725 -8.5905123 -8.0652542 -7.8180971 -6.4390073 -4.5322151 -3.6659641 -3.2244906 -3.404494 -4.4198384 -4.82501 -6.1173196][-12.990079 -12.068403 -11.154474 -11.917971 -11.670786 -10.367842 -9.7195377 -8.72816 -7.8592911 -7.08794 -6.7879329 -6.204155 -5.7543125 -5.363658 -5.276473][-10.179653 -10.252261 -9.3743591 -9.7306976 -9.6280231 -10.287405 -9.9353056 -8.9135132 -8.1988144 -8.3362589 -7.8374367 -7.6152339 -6.9889994 -6.400598 -6.7772713][-5.9826579 -5.8623018 -5.851387 -6.7660866 -7.0868983 -6.3750911 -6.9827371 -7.5486755 -7.7601557 -8.2253237 -8.8922787 -9.2066364 -9.3165493 -8.6807728 -8.5081139]]...]
INFO - root - 2017-12-15 21:45:35.769398: step 62110, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 48h:59m:25s remains)
INFO - root - 2017-12-15 21:45:42.290023: step 62120, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 49h:12m:48s remains)
INFO - root - 2017-12-15 21:45:48.833623: step 62130, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 49h:18m:48s remains)
INFO - root - 2017-12-15 21:45:55.432592: step 62140, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 50h:45m:07s remains)
INFO - root - 2017-12-15 21:46:02.078780: step 62150, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 49h:20m:17s remains)
INFO - root - 2017-12-15 21:46:08.671098: step 62160, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 48h:07m:20s remains)
INFO - root - 2017-12-15 21:46:15.212703: step 62170, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 49h:35m:58s remains)
INFO - root - 2017-12-15 21:46:21.788663: step 62180, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 49h:50m:09s remains)
INFO - root - 2017-12-15 21:46:28.473423: step 62190, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 50h:19m:21s remains)
INFO - root - 2017-12-15 21:46:35.149733: step 62200, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 48h:56m:26s remains)
2017-12-15 21:46:35.680439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.322813 -1.5722294 -1.2144246 -0.68085337 -1.2365789 -1.6614418 -2.1752498 -2.5894759 -3.4906142 -3.9612942 -4.4749608 -6.4647069 -8.79144 -9.7843542 -8.41252][-1.0062709 -0.34816408 0.50513077 0.43303347 -0.22261715 -0.83639145 -1.2026196 -1.5079179 -1.8741183 -2.818661 -3.6422698 -5.7227392 -7.97367 -8.8202429 -8.1258106][-0.70019054 -0.52261448 -0.74672842 -0.39787817 -0.85046721 -1.2293687 -1.5201197 -1.5812163 -1.3202467 -1.7472095 -1.9872365 -4.0795722 -6.0364809 -8.0134621 -7.62091][-2.6494792 -1.6701717 -1.7478735 -1.3146009 -1.5085611 -1.4283962 -1.4884667 -1.0961547 -0.75916624 -0.73419285 -0.85333967 -2.7070224 -4.9117646 -6.9091978 -7.0496569][-3.5227392 -4.027627 -4.39449 -2.547159 -2.1557274 -1.2513475 -0.65957594 -0.59270191 -0.3365674 0.13918304 0.49733257 -1.7109041 -4.3267856 -6.8606877 -6.5098257][-4.8901415 -4.5684023 -3.7122521 -1.6737914 -0.19799185 1.0657873 2.2265515 1.9549227 1.3111424 0.84552765 0.96595 -0.63104773 -3.018517 -4.8737068 -4.7133722][-4.8315644 -4.264091 -2.7376063 -0.26686287 1.147398 2.9259229 3.9147668 3.7588916 3.5119615 2.480783 1.8482661 -0.059525967 -2.8383353 -3.8348675 -3.3557518][-5.1194248 -3.6779692 -2.0024998 0.59086752 2.3700633 4.3154569 5.2464585 4.802814 4.3596015 3.2839541 2.3059306 0.38290167 -2.1187887 -3.2454307 -2.6294849][-4.7961669 -3.4415343 -1.5806894 0.70373344 1.9098644 2.7250705 3.5313964 3.8350644 3.5710502 2.2088375 1.362432 -1.0872312 -3.8900099 -4.7097316 -3.1874592][-5.0342903 -4.55355 -3.6507344 -0.54075 0.30396986 0.85306168 1.3853011 1.0096145 0.37471437 -0.96855259 -2.0046065 -4.177228 -6.2777824 -6.6058717 -4.4474454][-9.4552679 -9.3308268 -8.3960686 -5.9677334 -5.5271482 -4.7825952 -4.3006239 -4.25107 -3.8345089 -4.3622432 -5.3777595 -8.9236679 -10.175375 -8.8872557 -5.9671359][-12.625666 -12.729498 -12.03334 -9.7277031 -8.9879341 -7.983954 -7.4118242 -7.103744 -7.0770779 -7.2127037 -7.8904991 -9.3835573 -9.55475 -8.9375629 -5.7235637][-12.845289 -11.5194 -10.613718 -9.731041 -9.4200964 -7.9530983 -7.3146119 -6.92851 -7.2125325 -7.8158965 -8.5867758 -8.8681107 -9.9149942 -8.6270885 -5.5675688][-8.6882639 -7.9603224 -6.4585562 -5.5857649 -4.9111009 -4.7587051 -4.6249847 -4.7098761 -5.1410723 -6.1613274 -8.1831551 -8.8393555 -8.6292162 -7.3809915 -6.0798407][-5.6923375 -4.8343105 -3.3165345 -1.2118797 -0.9441433 -1.4296885 -1.2896557 -1.6763964 -2.039469 -3.5245752 -4.960526 -6.6213713 -7.6215191 -6.8468175 -6.265255]]...]
INFO - root - 2017-12-15 21:46:42.296535: step 62210, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 50h:40m:20s remains)
INFO - root - 2017-12-15 21:46:48.871307: step 62220, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 50h:25m:04s remains)
INFO - root - 2017-12-15 21:46:55.499342: step 62230, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 49h:36m:02s remains)
INFO - root - 2017-12-15 21:47:02.129072: step 62240, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 50h:33m:02s remains)
INFO - root - 2017-12-15 21:47:08.758283: step 62250, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 49h:21m:26s remains)
INFO - root - 2017-12-15 21:47:15.305063: step 62260, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 48h:46m:08s remains)
INFO - root - 2017-12-15 21:47:21.772207: step 62270, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 49h:07m:03s remains)
INFO - root - 2017-12-15 21:47:28.449595: step 62280, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 49h:21m:32s remains)
INFO - root - 2017-12-15 21:47:35.069445: step 62290, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 50h:32m:08s remains)
INFO - root - 2017-12-15 21:47:41.600813: step 62300, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 49h:09m:02s remains)
2017-12-15 21:47:42.089261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0463443 -7.3870935 -7.2409258 -6.6546373 -7.0636711 -8.4735508 -8.5728579 -7.2254539 -7.2387037 -7.3735743 -7.0846462 -7.5018697 -8.2165966 -8.6562691 -9.527668][-5.9803982 -7.0048676 -7.5948973 -7.0559473 -7.3311419 -7.1157174 -6.5354891 -6.1157064 -6.1873484 -5.8620844 -5.4906921 -6.8079906 -8.1792536 -8.3209858 -8.6841469][-6.7209067 -7.1708336 -7.0366 -6.1935868 -6.7524595 -7.8129897 -7.4341421 -5.8471861 -4.7569585 -4.2976193 -4.2373023 -5.6640029 -7.5209484 -8.7939186 -9.1053362][-6.1193991 -7.5435743 -7.894588 -6.87345 -7.5681195 -7.4880581 -6.8589296 -5.4850674 -3.5161643 -2.2898121 -3.3201957 -5.33312 -6.2218566 -6.813622 -6.9288969][-6.1057062 -6.6080484 -7.2354975 -7.5698776 -6.7819762 -5.5663986 -4.5772781 -3.0335753 -2.2171688 -1.3161325 -1.4591804 -3.4978759 -5.1477919 -5.9117522 -6.2459607][-7.4467115 -7.5358973 -6.9153452 -5.6604805 -4.0962896 -1.0275955 2.5063262 2.4532638 0.40793991 -0.77356768 -2.9876087 -3.7981982 -4.9664092 -5.9623318 -6.1437888][-6.8671989 -6.8695192 -6.2488852 -4.1908092 -1.0553002 2.3098435 5.4050632 6.0919452 4.7038312 1.0997562 -3.0618753 -4.4108405 -5.4174118 -6.5880771 -7.2824755][-8.14953 -6.8295031 -5.5549135 -2.8250096 -1.2192216 2.5255876 6.2344756 6.4616246 6.7399917 3.4602571 -1.0805311 -4.4587631 -6.7448387 -6.9060106 -6.8004417][-8.1940908 -6.638031 -4.6822953 -3.133491 -2.5010567 1.2131915 3.5258279 4.625587 5.459455 2.4022012 -0.77692842 -4.0238748 -7.7637219 -9.2479191 -8.6390362][-7.4504972 -7.537806 -6.4225426 -4.3485413 -2.435849 -0.50160837 0.94955015 2.5750403 2.2950292 -0.13338137 -3.52279 -7.132411 -10.298586 -11.224138 -11.294216][-8.1303177 -8.7773314 -9.19263 -6.9151635 -5.9032784 -4.0564995 -2.2584383 -2.0451112 -2.2416034 -3.1286926 -5.6494746 -9.4268112 -12.74591 -13.966825 -13.686712][-10.987376 -10.501406 -9.59935 -8.99243 -8.6156874 -8.0121489 -7.2033086 -6.5821133 -6.448925 -7.3691926 -8.3616905 -9.0100193 -10.383378 -11.473583 -11.765852][-13.802729 -12.896297 -11.354115 -9.9359245 -9.8508759 -10.456215 -11.022541 -10.501289 -8.997303 -9.03598 -9.344346 -9.9711866 -10.500645 -10.195599 -9.368845][-12.12632 -11.624278 -10.972804 -9.4420242 -8.6071777 -9.125412 -9.6479864 -9.7623615 -10.315235 -9.4734259 -8.3610916 -7.6893744 -7.7100139 -7.9412718 -7.3353224][-8.4658775 -8.6358061 -8.0192127 -8.1143656 -7.4300766 -7.2393141 -7.3221197 -7.205493 -7.9114304 -8.6706953 -8.8214054 -7.9791985 -7.3730106 -7.3638511 -7.4028845]]...]
INFO - root - 2017-12-15 21:47:48.648736: step 62310, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 48h:38m:13s remains)
INFO - root - 2017-12-15 21:47:55.242016: step 62320, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 50h:12m:08s remains)
INFO - root - 2017-12-15 21:48:01.821590: step 62330, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 48h:06m:13s remains)
INFO - root - 2017-12-15 21:48:08.423210: step 62340, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.665 sec/batch; 49h:55m:07s remains)
INFO - root - 2017-12-15 21:48:15.022702: step 62350, loss = 0.21, batch loss = 0.17 (12.2 examples/sec; 0.654 sec/batch; 49h:05m:45s remains)
INFO - root - 2017-12-15 21:48:21.614543: step 62360, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 49h:21m:42s remains)
INFO - root - 2017-12-15 21:48:28.181023: step 62370, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 49h:17m:34s remains)
INFO - root - 2017-12-15 21:48:34.826756: step 62380, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 48h:35m:08s remains)
INFO - root - 2017-12-15 21:48:41.435483: step 62390, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 49h:38m:30s remains)
INFO - root - 2017-12-15 21:48:48.002036: step 62400, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 50h:00m:47s remains)
2017-12-15 21:48:48.519538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3443937 -6.9381061 -7.612843 -8.1278172 -8.9788847 -7.8986335 -6.9517546 -6.3645897 -6.0317755 -6.0777431 -6.3151531 -7.6048427 -7.9179416 -8.6615658 -8.7062235][-2.8861299 -4.134644 -5.390275 -6.3272786 -7.1601896 -7.3992529 -7.3311486 -6.6165681 -5.917738 -6.2968864 -7.12764 -8.5509167 -8.8843517 -11.06951 -10.756531][-2.7765615 -3.4603965 -4.3600178 -5.8948903 -7.0140715 -7.8388977 -7.87405 -7.2364511 -6.7186952 -7.087132 -8.5161572 -10.48344 -10.315969 -11.334963 -11.608065][-4.5972185 -5.5606275 -5.5404587 -5.7926345 -6.5936351 -7.6755533 -7.2934465 -6.6253963 -7.0639234 -7.2487259 -7.7010808 -10.639582 -12.007754 -12.599224 -11.819678][-6.1727304 -7.76915 -8.54395 -7.4098792 -6.2468295 -5.3015828 -4.0770764 -4.6396475 -5.5596366 -5.7115307 -6.6065869 -8.8890266 -10.265642 -12.818584 -13.332424][-6.9091821 -8.5553827 -9.0638 -7.015646 -4.2308869 -0.32246923 2.359674 0.93549824 -1.9238777 -3.3776782 -4.8049273 -6.9371729 -7.8275375 -10.730482 -12.501184][-7.8173075 -8.4713392 -7.9242077 -5.7638974 -2.8889701 2.5660777 7.9121518 8.04418 5.9291482 0.67749166 -4.8117952 -5.9100037 -5.8902793 -8.9041929 -10.559045][-6.82325 -7.6866846 -7.6725969 -4.9237084 -1.7326257 4.1275773 9.8791447 10.75832 9.8343143 4.4193625 -2.2727582 -6.2603621 -7.1341887 -7.8016133 -8.9042339][-5.9026566 -5.6675792 -5.1106777 -4.0105467 -3.5827384 0.14511776 5.0922141 6.838306 6.8659854 1.6944036 -2.8019304 -6.632535 -10.17695 -10.780365 -10.047276][-5.1756353 -5.7879086 -5.1539617 -2.6895452 -2.2156222 -1.5432849 -0.018618584 0.51164818 1.257391 -2.9210763 -7.40629 -11.514294 -13.484938 -14.770277 -15.020878][-7.9875455 -8.0627489 -7.7777472 -6.011652 -5.6603861 -5.0010157 -5.1004171 -6.32469 -6.2475362 -8.1508474 -11.080988 -14.810991 -16.991798 -17.495924 -16.609562][-12.472515 -12.447884 -11.491587 -10.531089 -10.239818 -8.99656 -9.559473 -11.078014 -11.422173 -13.154146 -14.705492 -15.962711 -15.796537 -15.641043 -15.321707][-14.954554 -13.212769 -11.613739 -11.030895 -11.748137 -11.523393 -11.428116 -11.614326 -11.779655 -12.07432 -12.642057 -14.132572 -13.677608 -11.983397 -9.9544554][-12.994289 -11.843112 -11.363401 -10.711809 -10.406017 -10.585636 -11.362989 -11.701622 -10.965297 -10.776703 -10.825874 -9.6266212 -7.7521048 -7.8415356 -7.4551792][-6.8406782 -6.3490534 -5.5580225 -5.4983163 -5.6553712 -5.8872538 -6.6717544 -7.3971577 -7.8792086 -7.6875105 -7.2911053 -8.2042732 -7.9376669 -6.8991461 -6.3400688]]...]
INFO - root - 2017-12-15 21:48:55.117182: step 62410, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 48h:54m:01s remains)
INFO - root - 2017-12-15 21:49:01.638329: step 62420, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 49h:32m:14s remains)
INFO - root - 2017-12-15 21:49:08.240625: step 62430, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 49h:26m:47s remains)
INFO - root - 2017-12-15 21:49:14.870597: step 62440, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 48h:54m:51s remains)
INFO - root - 2017-12-15 21:49:21.415127: step 62450, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 49h:55m:27s remains)
INFO - root - 2017-12-15 21:49:28.048480: step 62460, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 49h:36m:35s remains)
INFO - root - 2017-12-15 21:49:34.644182: step 62470, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 49h:48m:25s remains)
INFO - root - 2017-12-15 21:49:41.160698: step 62480, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 49h:36m:08s remains)
INFO - root - 2017-12-15 21:49:47.712658: step 62490, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 49h:19m:42s remains)
INFO - root - 2017-12-15 21:49:54.286239: step 62500, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 50h:46m:47s remains)
2017-12-15 21:49:54.836331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0612564 -4.2786312 -4.2699146 -3.7864404 -3.9102178 -4.8420267 -5.6184893 -6.4249697 -7.1521158 -6.9639254 -6.6281509 -5.4088807 -5.4321833 -7.9005494 -8.4452477][-6.32089 -5.5964956 -5.4848619 -5.7192564 -6.0570383 -6.9179006 -7.505939 -7.8820467 -8.1204538 -7.5463638 -7.2369061 -6.8727436 -8.37012 -8.8094568 -6.804594][-6.1463642 -6.6271958 -7.0403638 -6.9732752 -7.578537 -7.19526 -7.1586146 -7.652585 -7.8195 -7.5799484 -7.4381242 -5.8131123 -6.0231385 -7.0010695 -5.5644054][-4.0435734 -4.5144958 -5.2001762 -6.5725684 -7.6933842 -8.0242538 -8.2779493 -8.757576 -7.748312 -6.0783215 -5.8408594 -5.3160124 -6.487093 -7.2747412 -6.0711484][-4.8517785 -5.108552 -5.810133 -6.5903478 -6.5806117 -5.0123835 -4.6067796 -4.9110317 -4.8486848 -4.1151333 -4.1781216 -3.1368856 -4.9288344 -6.7442317 -6.639431][-6.3448486 -4.8841863 -4.0651412 -3.5142024 -3.1697414 -0.94929457 1.555191 1.4062872 0.861434 -0.667891 -1.5240312 -1.418736 -3.5710223 -5.888864 -6.9429088][-6.0712414 -6.4800625 -5.4489751 -4.1290131 -2.5128319 -0.24377298 3.2504706 5.6605306 6.0377192 2.9498878 -0.196527 -1.7821515 -4.3225131 -6.2092509 -6.5385747][-6.4772739 -7.4164457 -6.4476924 -3.9794681 -1.6477275 2.3991132 4.9245696 6.8808351 7.2856441 4.3927236 0.53854704 -2.3788812 -5.800456 -7.0184126 -7.073185][-7.2173786 -7.8149023 -6.2597618 -4.1294518 -2.4406502 1.3165131 3.0975432 4.5800996 5.0182767 3.1916738 0.67210054 -2.2261555 -5.7133083 -8.0633869 -7.1782494][-6.7929344 -6.9384465 -5.8152428 -4.5049658 -3.1704237 -0.044073105 2.188992 3.9549375 3.2591729 0.2860465 -2.1185269 -2.9642198 -4.4912786 -7.6854239 -9.52947][-8.2261314 -7.7879462 -6.149744 -4.9789443 -3.7830791 -3.1658862 -3.0499554 -1.5223579 -1.6394329 -3.6568749 -5.7968459 -6.0744262 -7.4583492 -7.0663495 -5.6890454][-9.5282 -9.1735725 -8.8697281 -7.6985531 -5.9107518 -6.1122475 -5.8988047 -5.58068 -6.2011733 -7.2072625 -8.74354 -8.7362461 -8.05823 -7.5468445 -6.4200983][-10.077835 -9.79362 -9.19768 -8.0332584 -7.401114 -6.7280326 -7.3763404 -8.4103136 -9.57008 -9.9208126 -9.9268456 -9.9430323 -10.634256 -8.8532753 -6.3569756][-7.9384055 -8.0163574 -8.1003084 -7.8768086 -6.6176114 -6.6915569 -7.3402553 -7.7379093 -8.407383 -9.4887056 -9.9880657 -8.1702976 -7.6138391 -7.8416367 -7.7819614][-5.8577766 -7.8682756 -8.8688192 -7.2790861 -6.0081606 -5.8611012 -6.2855415 -5.7638378 -5.7786183 -6.1862803 -7.0878153 -7.2465949 -8.0396175 -8.6199236 -9.5516644]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-62500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-62500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 21:50:02.417600: step 62510, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 50h:07m:48s remains)
INFO - root - 2017-12-15 21:50:09.080573: step 62520, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 49h:01m:12s remains)
INFO - root - 2017-12-15 21:50:15.693888: step 62530, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 49h:29m:49s remains)
INFO - root - 2017-12-15 21:50:22.330094: step 62540, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 49h:28m:43s remains)
INFO - root - 2017-12-15 21:50:28.934648: step 62550, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 50h:50m:49s remains)
INFO - root - 2017-12-15 21:50:35.460482: step 62560, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 49h:26m:28s remains)
INFO - root - 2017-12-15 21:50:42.050121: step 62570, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.682 sec/batch; 51h:09m:36s remains)
INFO - root - 2017-12-15 21:50:48.688089: step 62580, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 50h:33m:38s remains)
INFO - root - 2017-12-15 21:50:55.199711: step 62590, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 48h:32m:37s remains)
INFO - root - 2017-12-15 21:51:01.774362: step 62600, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 48h:15m:10s remains)
2017-12-15 21:51:02.306418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.8497143 -6.8166838 -6.5242133 -5.4151545 -5.3365312 -5.2148018 -4.8733006 -4.4460816 -4.5446796 -4.8725424 -5.3626142 -8.347456 -9.351553 -9.4758425 -9.7522516][-5.4528704 -4.52923 -4.0517921 -3.7882032 -4.8025932 -5.227283 -5.6981106 -5.6519647 -5.0857697 -4.2549229 -4.640234 -7.0246692 -7.9896526 -9.1965723 -8.6256943][-3.9063997 -3.5255327 -3.1251812 -2.9958181 -3.8946605 -3.542315 -3.7781367 -3.3810458 -3.0478494 -3.6964769 -3.9925666 -6.6191754 -7.82662 -7.8321791 -7.72294][-3.0314062 -2.9688716 -3.2905886 -2.9585958 -3.4691272 -2.8210936 -2.7842314 -2.5399451 -2.3053739 -2.2730198 -2.942205 -5.4676123 -6.6255603 -7.291646 -7.3415017][-3.0089271 -3.6220262 -3.6530061 -2.3834271 -1.9194636 -1.1934848 -1.0973625 -0.42633247 -0.403656 -0.80784559 -1.2710857 -4.0764656 -5.8721972 -6.7248936 -6.9005709][-4.9248419 -4.2801909 -3.5402389 -2.1359627 -1.2383375 0.080204964 1.2172809 1.4984937 1.1749353 0.80419064 0.16904116 -2.9237216 -4.84385 -5.4858007 -5.8589764][-6.464169 -5.6895008 -4.094903 -2.45839 -1.0126553 0.77650404 1.8892412 3.1281362 3.047543 2.17656 1.2302194 -2.2341924 -4.6946373 -5.195085 -5.4436803][-6.1097884 -6.4623642 -5.0759521 -2.5116925 -0.814065 1.2934875 2.6249747 3.0794969 2.4304671 2.0806117 1.0096726 -2.6061673 -4.7263389 -5.4427462 -6.063077][-5.7741995 -5.874074 -4.4728079 -1.5736938 0.549788 1.8813987 2.1033559 1.8339133 1.3362885 1.0702105 0.21153402 -2.8558137 -4.3956537 -5.1880636 -5.0782151][-5.4634585 -5.8676925 -4.3493533 -1.7659581 -0.57896519 1.2550492 2.0043402 1.3038845 0.31029129 0.293231 -0.24340105 -3.6141229 -4.7228289 -5.5040965 -5.9279475][-9.3500328 -8.5214853 -6.037127 -3.5764802 -1.755846 -0.54226303 -0.73066807 -1.7920465 -2.5727723 -2.7415681 -3.2700665 -6.1959891 -7.2144718 -6.3818469 -6.2162108][-10.919417 -9.4731655 -7.1399269 -4.8353596 -3.0851862 -2.7830288 -4.0083847 -5.0798063 -5.2150946 -5.4058013 -6.1318855 -7.2014675 -6.87901 -5.8415461 -5.4284019][-10.660604 -8.9405708 -6.3169012 -4.7083144 -4.2706347 -4.2734981 -4.4022937 -4.9969273 -5.4699178 -5.4534721 -5.8041511 -7.1559219 -6.9723406 -5.5024967 -4.8571243][-10.979437 -9.1349421 -7.5361304 -5.6718526 -3.8973978 -3.5532796 -4.8331447 -5.1824288 -4.87756 -5.8737712 -5.9931192 -5.5087204 -4.9758368 -4.4819646 -4.2130594][-7.3028564 -7.289649 -6.8363457 -4.7152972 -3.6923468 -3.6954467 -2.8876009 -2.5959549 -3.1557987 -2.6658416 -2.6333194 -3.9320452 -4.1077223 -5.07755 -5.8221555]]...]
INFO - root - 2017-12-15 21:51:08.928446: step 62610, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 49h:53m:18s remains)
INFO - root - 2017-12-15 21:51:15.482057: step 62620, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.689 sec/batch; 51h:38m:55s remains)
INFO - root - 2017-12-15 21:51:22.041771: step 62630, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 50h:10m:03s remains)
INFO - root - 2017-12-15 21:51:28.606667: step 62640, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 49h:41m:01s remains)
INFO - root - 2017-12-15 21:51:35.200759: step 62650, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 48h:48m:41s remains)
INFO - root - 2017-12-15 21:51:41.834348: step 62660, loss = 0.24, batch loss = 0.19 (12.1 examples/sec; 0.661 sec/batch; 49h:30m:54s remains)
INFO - root - 2017-12-15 21:51:48.427917: step 62670, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 49h:08m:54s remains)
INFO - root - 2017-12-15 21:51:54.970854: step 62680, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 49h:51m:46s remains)
INFO - root - 2017-12-15 21:52:01.584679: step 62690, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 48h:17m:33s remains)
INFO - root - 2017-12-15 21:52:08.277404: step 62700, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 49h:13m:17s remains)
2017-12-15 21:52:08.832432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8442314 -3.2505336 -2.6475072 -1.3215904 -2.0016577 -2.2813466 -2.6490955 -3.0750523 -3.4071443 -3.8271203 -4.9218059 -6.7447963 -7.1814346 -8.4268341 -9.9032173][-0.88136005 -1.7952371 -2.7580183 -2.2722366 -1.8854771 -0.85953379 -0.47774458 -0.8692565 -2.6748831 -3.9993925 -4.3714209 -6.5339136 -7.2275057 -8.088562 -9.5395288][-1.7998624 -2.2077653 -2.7548044 -1.6410899 -2.5098138 -2.7195539 -1.9428368 -0.89210463 -0.74828482 -1.7564616 -2.4534702 -4.1856728 -4.6983848 -5.7563138 -7.0527358][-3.201745 -3.6845684 -2.7744148 -1.1252403 -1.4362321 -1.5388327 -0.92717218 -0.7980113 -0.97544718 -0.5492382 -0.045313358 -1.8783224 -2.523175 -3.6915009 -5.1052632][-4.4215579 -5.3490443 -4.5015788 -2.233645 -1.7598512 -0.80929661 -0.57965422 -0.58355808 -0.59331894 -0.30735922 0.24290466 -0.93614054 -1.7472584 -3.7814085 -6.0676727][-2.66599 -3.5515072 -3.0703681 -2.0010817 -1.3169136 0.60370255 0.90626955 0.37943935 0.22694349 0.64535 0.54465675 -1.0610037 -1.7235734 -3.3675065 -6.0489674][-0.86770725 -1.6481352 -1.8397565 -1.1282697 -0.92280149 0.09927845 0.63214827 1.5001283 1.9197788 1.6312714 1.4231801 -0.81813431 -2.1806428 -3.6295125 -5.8537426][0.51655817 -0.063259125 -0.40845966 -0.41259909 -0.4537816 0.55701685 1.0303383 0.9831419 1.017077 1.7246895 2.5369334 0.77581167 -0.65334034 -3.0445101 -5.1855693][-2.6612067 -2.7629769 -1.4101996 0.22002363 0.74679661 1.7852807 2.1150141 1.7205443 1.5101724 0.93667126 0.69803476 -0.56114292 -1.2238708 -3.4053185 -5.8815889][-3.7956676 -4.1510963 -3.8255472 -1.6966643 -0.55739832 0.8198061 0.5887599 0.74986458 0.88875532 0.18068218 -1.2309847 -2.9761667 -3.1295977 -5.0008531 -6.8347626][-6.1602979 -5.8147273 -4.818923 -3.5909762 -2.7863889 -1.844588 -2.4017472 -2.6338251 -2.3275378 -1.6408138 -1.3362312 -4.1135812 -5.8868241 -6.308485 -7.0372896][-9.7864933 -7.8412609 -5.7953577 -3.8618131 -2.7877698 -2.1193514 -2.4675486 -4.5313091 -5.9831367 -5.4298387 -4.7656822 -6.0831776 -7.0439682 -7.3128777 -6.4540248][-10.072128 -8.6727858 -6.2446966 -4.3481646 -3.6804891 -3.1298165 -4.0035253 -5.1544027 -6.3810558 -7.1800842 -7.1342716 -6.9662542 -6.917963 -5.5098791 -4.1641831][-8.548419 -8.9032669 -6.6973486 -3.9513409 -2.1977816 -2.3699651 -3.8303576 -5.0395322 -5.3611994 -6.5083866 -7.3151689 -5.7275629 -3.8875825 -4.1057167 -4.0238285][-4.6461687 -6.1078119 -6.3630595 -5.0727196 -3.8538179 -3.1885915 -3.4913993 -3.2994888 -3.0884094 -3.1247008 -2.7475276 -3.9453344 -4.707768 -5.2527666 -5.891593]]...]
INFO - root - 2017-12-15 21:52:15.465068: step 62710, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 51h:09m:11s remains)
INFO - root - 2017-12-15 21:52:22.024183: step 62720, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 48h:12m:17s remains)
INFO - root - 2017-12-15 21:52:28.635307: step 62730, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 49h:27m:38s remains)
INFO - root - 2017-12-15 21:52:35.176895: step 62740, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 50h:19m:52s remains)
INFO - root - 2017-12-15 21:52:41.870738: step 62750, loss = 0.19, batch loss = 0.14 (11.5 examples/sec; 0.693 sec/batch; 51h:54m:04s remains)
INFO - root - 2017-12-15 21:52:48.407865: step 62760, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 48h:19m:26s remains)
INFO - root - 2017-12-15 21:52:55.086279: step 62770, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 50h:07m:47s remains)
INFO - root - 2017-12-15 21:53:01.603422: step 62780, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 48h:45m:16s remains)
INFO - root - 2017-12-15 21:53:08.170368: step 62790, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 49h:00m:00s remains)
INFO - root - 2017-12-15 21:53:14.790805: step 62800, loss = 0.20, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 47h:35m:16s remains)
2017-12-15 21:53:15.412730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9911561 -6.551095 -6.1464868 -5.0320635 -4.5795422 -4.8670187 -5.339323 -4.8581743 -5.3452773 -5.7275743 -5.4783125 -6.5333929 -8.4608879 -9.7186661 -8.6390572][-3.9199796 -4.7073383 -5.041841 -4.1557827 -3.4717212 -3.843776 -4.1193352 -3.5724661 -4.3246531 -4.6114635 -4.0063882 -5.1603384 -7.4517717 -8.6871939 -8.5530272][-2.9184661 -4.3611655 -5.148056 -3.861249 -2.8282206 -2.4513566 -1.8236067 -2.1727602 -2.6323011 -2.7887049 -2.466074 -4.0200377 -6.614686 -8.3451576 -7.661057][-0.29617691 -1.5510545 -2.3039291 -2.65967 -2.7293978 -1.0402565 0.2663908 -1.0538564 -2.9833074 -2.9927325 -1.8910375 -3.8284802 -6.7849054 -8.7353935 -8.4211845][-1.7633042 -2.5310442 -3.233537 -2.9242527 -2.12264 -0.48310137 -0.26380444 -1.165359 -1.5201373 -1.7225091 -1.5023274 -3.2533722 -5.8949676 -7.9370956 -8.4919815][-2.7117548 -2.8399954 -3.1195569 -2.1384556 -0.48003864 1.7466526 2.5157304 1.2304506 0.20445251 -0.75220203 -1.2809157 -2.8453293 -5.2644072 -7.0837383 -7.7728052][-3.7284184 -3.6517563 -3.9445672 -2.2293069 0.77009058 3.3885913 5.3515506 5.6109939 4.3055854 2.1860838 0.96163321 -1.7890449 -4.5665636 -6.1622353 -7.4621863][-3.1743166 -3.5294406 -3.5166123 -1.0964789 1.3503485 4.0760303 6.1591926 6.7360177 6.525991 3.968761 1.7638922 -1.6848493 -4.7477632 -6.899272 -7.6771307][-4.1566076 -4.1838574 -3.5300076 -1.2871189 0.49722004 3.1197915 5.1403842 5.7505727 5.489902 4.0672164 2.830276 -0.93327618 -5.0788393 -7.9571524 -8.2917385][-3.2698457 -3.9356017 -3.4439721 -1.5403533 -0.28750563 1.7609706 3.2073455 3.984323 3.3099904 2.0364161 0.9058404 -2.8214605 -6.5075235 -9.671917 -11.011501][-5.2748804 -5.4793 -4.8033528 -3.8992109 -3.5622363 -3.0238357 -2.1777034 -1.1576605 -1.2692037 -1.8783023 -2.5654318 -5.863627 -8.250391 -10.211815 -11.553722][-7.7238188 -8.0536957 -7.2331986 -6.3427687 -6.5237303 -6.3185744 -6.1621675 -6.6610041 -6.8089013 -6.3271494 -6.4798045 -7.5093889 -8.7733135 -10.621008 -11.626607][-9.8093014 -9.3071556 -7.8039913 -7.4537706 -8.0640354 -8.1138353 -8.2640457 -8.6446772 -8.97831 -8.6973429 -8.1158237 -8.0547628 -8.6675014 -8.9817505 -8.7278376][-9.395153 -8.8422546 -7.5669489 -6.9264164 -7.0017128 -7.7843552 -7.9311476 -7.6167722 -7.9700966 -7.9846325 -7.0618563 -5.8571553 -6.1107159 -7.1893215 -7.3759918][-11.021226 -9.7475052 -7.9241686 -7.6804886 -7.4737058 -7.4883218 -7.343308 -7.9492807 -8.477354 -7.4026313 -6.4817266 -6.7349768 -6.088901 -5.8520141 -6.6262817]]...]
INFO - root - 2017-12-15 21:53:22.058671: step 62810, loss = 0.17, batch loss = 0.12 (11.3 examples/sec; 0.709 sec/batch; 53h:08m:14s remains)
INFO - root - 2017-12-15 21:53:28.673403: step 62820, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.683 sec/batch; 51h:10m:02s remains)
INFO - root - 2017-12-15 21:53:35.246622: step 62830, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 49h:43m:39s remains)
INFO - root - 2017-12-15 21:53:41.799154: step 62840, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 49h:06m:35s remains)
INFO - root - 2017-12-15 21:53:48.406291: step 62850, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 50h:04m:05s remains)
INFO - root - 2017-12-15 21:53:55.024396: step 62860, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 48h:03m:37s remains)
INFO - root - 2017-12-15 21:54:01.566854: step 62870, loss = 0.23, batch loss = 0.18 (12.3 examples/sec; 0.652 sec/batch; 48h:50m:22s remains)
INFO - root - 2017-12-15 21:54:08.085763: step 62880, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 49h:11m:00s remains)
INFO - root - 2017-12-15 21:54:14.755070: step 62890, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 48h:47m:45s remains)
INFO - root - 2017-12-15 21:54:21.330068: step 62900, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.687 sec/batch; 51h:27m:06s remains)
2017-12-15 21:54:21.859997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.64094 -5.8159294 -6.1530766 -6.8478918 -7.6186171 -7.9978437 -8.1638384 -7.6876516 -6.9600372 -6.7916212 -6.1070213 -7.0774536 -8.1640377 -7.1112132 -6.4787121][-5.4593883 -6.2424765 -6.9649267 -7.2627134 -8.41326 -9.1101484 -9.2128248 -9.1651344 -8.7937193 -8.4651337 -8.2181244 -9.3443575 -11.016508 -10.722617 -9.3471165][-4.4915686 -5.7774796 -7.1321955 -7.3298244 -8.2294378 -8.4232225 -8.291605 -8.311965 -7.9172277 -7.9830971 -8.2765884 -9.8740263 -11.844614 -11.51195 -11.050803][-5.8102555 -6.4986525 -6.9293118 -6.7909155 -7.2394671 -6.2939034 -5.3581929 -6.1263909 -6.2912912 -5.9824672 -6.4003296 -8.6562481 -11.580915 -11.214767 -10.814833][-7.1337762 -9.1534033 -9.68203 -7.9122171 -6.2902431 -4.6143141 -2.6899827 -3.0686858 -3.648639 -4.0838051 -4.576272 -6.5830641 -9.4773884 -10.384861 -10.71694][-7.8742828 -8.7537689 -8.65134 -6.2439895 -4.0237217 -0.84664154 1.7131019 1.369771 1.0470667 -0.50354481 -2.1195023 -4.8977618 -8.0469751 -8.5240459 -8.7040138][-7.8704348 -8.5412827 -8.2011194 -4.6728396 -0.89399767 2.7603612 5.2536292 5.8676543 5.7247519 2.5780864 0.15056849 -2.4361379 -6.1868286 -7.9464912 -8.1336308][-6.8883018 -7.1558466 -6.5971155 -3.6386943 0.17072487 4.037787 6.2043452 6.2450452 6.1975322 4.5108809 2.7623429 -1.1423492 -6.1378393 -7.3333588 -7.5221109][-4.6928215 -5.23611 -5.6190653 -3.7679749 -1.9890554 1.157927 3.7541776 4.8567271 4.8962007 3.3147683 2.3261161 -0.84006691 -5.2433691 -8.0110016 -9.0942726][-2.7993512 -3.5279951 -4.2092681 -3.291724 -2.002646 0.020658493 1.6897268 2.4732833 2.3128419 2.1278844 1.0529366 -2.9117403 -6.5117555 -8.5771446 -10.405323][-6.2088771 -6.4415793 -6.2758808 -5.1646533 -3.562762 -2.9218426 -2.4315343 -2.0836341 -2.5726156 -3.3395061 -3.741374 -6.8375134 -9.7099781 -10.800052 -10.626632][-10.773067 -10.056192 -8.9293165 -7.5268545 -6.06153 -5.3682785 -5.7277923 -6.4235115 -7.4640784 -7.57963 -8.1220074 -9.3587456 -10.858753 -11.682038 -11.800787][-11.682127 -10.753818 -8.0612869 -7.0955329 -6.9433656 -6.3057871 -6.632905 -7.3166523 -8.335453 -8.9645576 -9.0147514 -9.5290451 -10.10539 -9.55705 -8.6302977][-11.378165 -11.054331 -8.6001549 -6.0136852 -4.7477303 -4.4851923 -4.7700777 -5.4739366 -6.5240297 -7.4680448 -8.1145344 -7.4921527 -7.0450196 -6.8433285 -6.5170636][-9.6106291 -9.630621 -8.9957457 -7.2083788 -4.5210285 -3.3954651 -3.7259626 -4.5780878 -4.867382 -5.2268939 -6.3584113 -8.3643236 -8.4640055 -6.9839497 -6.0437641]]...]
INFO - root - 2017-12-15 21:54:28.435031: step 62910, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 49h:02m:36s remains)
INFO - root - 2017-12-15 21:54:35.060188: step 62920, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 49h:43m:53s remains)
INFO - root - 2017-12-15 21:54:41.717317: step 62930, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 49h:30m:30s remains)
INFO - root - 2017-12-15 21:54:48.310691: step 62940, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 48h:23m:02s remains)
INFO - root - 2017-12-15 21:54:54.903144: step 62950, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 50h:42m:01s remains)
INFO - root - 2017-12-15 21:55:01.477185: step 62960, loss = 0.13, batch loss = 0.08 (12.8 examples/sec; 0.626 sec/batch; 46h:53m:50s remains)
INFO - root - 2017-12-15 21:55:08.062110: step 62970, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 48h:45m:31s remains)
INFO - root - 2017-12-15 21:55:14.717785: step 62980, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 50h:02m:20s remains)
INFO - root - 2017-12-15 21:55:21.302860: step 62990, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 49h:23m:01s remains)
INFO - root - 2017-12-15 21:55:27.921571: step 63000, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 49h:08m:18s remains)
2017-12-15 21:55:28.440009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6337619 -1.6054993 -1.9433351 -2.5650711 -3.8445685 -4.2432284 -4.3374457 -3.8984179 -3.4134221 -1.6649532 -0.40319204 -2.9752574 -5.5762548 -10.250824 -12.122873][-1.9120998 -1.8483398 -2.095309 -2.4622498 -3.605943 -4.5998535 -4.4248862 -4.0479193 -5.0233731 -5.37538 -4.592813 -7.3494282 -10.241179 -12.895109 -13.445395][-1.361825 -1.1419706 -3.084614 -3.9692497 -5.1367598 -5.691443 -6.3146443 -4.9933567 -4.3360071 -4.7680216 -5.6935072 -9.0886517 -11.307313 -13.142496 -13.566011][-3.9438791 -3.2641721 -4.386281 -5.2617168 -6.2239418 -5.7486043 -5.3372951 -4.8328633 -4.7647567 -4.605711 -5.7234116 -9.506856 -11.748951 -13.211975 -12.945837][-4.6737385 -5.0774736 -5.9089708 -5.8123956 -6.4517832 -4.7860928 -2.5749402 -3.0530682 -4.9170403 -4.8524418 -5.0338564 -7.2799673 -10.433573 -13.591496 -14.320351][-7.1487751 -7.1307878 -7.59714 -5.0537539 -2.8882415 0.92296124 5.1036735 4.5602603 1.7085981 -1.3429189 -4.3434057 -6.5616865 -7.9898386 -10.755715 -12.933205][-7.754447 -7.8688903 -8.0064135 -4.9420652 -2.2478459 2.5532594 7.19093 8.7034454 8.509697 2.7974505 -2.6966879 -5.9221187 -8.106966 -9.5421467 -10.628435][-8.369525 -7.9500284 -6.6080613 -3.2752924 -0.82140064 4.5481381 8.8756733 8.8244686 8.4803276 3.9150834 -1.0590348 -5.972312 -8.9220734 -10.354254 -10.979032][-7.9945688 -7.8389616 -6.0441065 -3.2371972 -2.82832 2.0807328 6.3674254 6.7542357 6.06947 1.8171973 -1.687829 -6.3298683 -9.5869112 -10.944933 -11.01404][-7.71483 -7.7894416 -6.6817431 -4.2926235 -4.2691011 -1.8143191 0.79223728 2.047102 2.6837296 -0.84865093 -3.8494964 -8.3619127 -10.519529 -11.434065 -11.598991][-10.667633 -11.264727 -10.520225 -7.6437254 -7.4309425 -7.1161227 -6.7805624 -5.4970131 -4.5691009 -6.2953582 -8.03974 -12.294373 -12.240746 -11.463986 -10.052035][-15.158073 -15.083504 -13.875788 -12.403202 -11.840259 -10.506409 -11.527515 -11.767103 -10.620571 -11.161755 -11.724133 -13.093188 -12.607363 -12.339808 -10.65538][-15.603657 -14.772011 -13.599024 -13.95153 -14.030499 -12.394744 -12.841776 -13.450346 -13.106995 -12.395454 -11.935904 -12.361704 -11.723621 -11.146637 -9.8518009][-11.52438 -11.17897 -10.737795 -10.413357 -10.180823 -10.963631 -11.424639 -11.770904 -12.217873 -12.742079 -12.582549 -10.756176 -9.1532936 -9.3468723 -9.2254686][-6.8957238 -6.8247924 -7.1129527 -5.5303802 -4.7467589 -5.6966166 -6.5788145 -7.1573734 -7.0758004 -7.6922011 -9.0053968 -9.64765 -9.1475811 -8.6034107 -9.113493]]...]
INFO - root - 2017-12-15 21:55:35.030308: step 63010, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 48h:25m:57s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 21:55:41.632600: step 63020, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 48h:18m:48s remains)
INFO - root - 2017-12-15 21:55:48.232819: step 63030, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.686 sec/batch; 51h:22m:36s remains)
INFO - root - 2017-12-15 21:55:54.812503: step 63040, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 48h:31m:06s remains)
INFO - root - 2017-12-15 21:56:01.401865: step 63050, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 49h:23m:52s remains)
INFO - root - 2017-12-15 21:56:07.948830: step 63060, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 49h:20m:41s remains)
INFO - root - 2017-12-15 21:56:14.587988: step 63070, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 50h:49m:03s remains)
INFO - root - 2017-12-15 21:56:21.217893: step 63080, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 49h:44m:25s remains)
INFO - root - 2017-12-15 21:56:27.817369: step 63090, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 49h:02m:57s remains)
INFO - root - 2017-12-15 21:56:34.367832: step 63100, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 48h:24m:54s remains)
2017-12-15 21:56:34.875449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9417076 -5.2436175 -5.41116 -4.5322285 -4.6670136 -5.4847741 -6.1634531 -5.6379189 -5.5754609 -5.7284536 -4.9718771 -6.8618813 -9.4232645 -9.0912437 -8.5747128][-4.9066796 -5.4360723 -5.8188195 -5.6558495 -5.9240389 -6.2692595 -7.5752268 -8.2929125 -8.0514708 -7.6486559 -7.1603127 -8.6927681 -11.203671 -10.832912 -9.8311214][-1.9522231 -3.1751494 -3.5560563 -3.5049818 -4.85418 -5.8413854 -6.2191682 -6.7839413 -7.697854 -6.8313293 -6.2107453 -7.9136248 -10.477188 -10.867048 -10.213314][-3.0667789 -3.9973874 -3.7748346 -2.3055692 -2.323195 -2.0393138 -2.8361943 -3.682498 -4.1877289 -4.5767546 -4.3056993 -5.2728672 -7.9953308 -8.5178337 -8.6423454][-4.2107372 -4.4032879 -4.8626552 -3.2435067 -1.3133392 -0.7193594 -0.98855829 -1.0383382 -0.76218319 -1.0295591 -1.3456125 -4.1213841 -7.1605892 -7.2731256 -6.7079306][-7.8654032 -7.6073093 -5.9308491 -3.1952767 -1.861474 0.68923044 2.4086537 2.4640565 2.8616595 2.5900226 2.565661 -0.95003033 -5.3828087 -6.7298694 -7.4761772][-8.9704294 -7.97742 -6.2512879 -3.4576323 -1.2370791 1.1312299 2.7201123 4.33734 5.7614703 5.0462213 3.6211658 -0.44443655 -4.9920259 -5.9809203 -6.53776][-7.2218332 -6.6177607 -5.9821343 -2.5767283 0.11966753 2.249773 3.7376657 4.4212375 5.5896916 5.1527019 3.8097558 -0.043421745 -4.2969494 -5.5841184 -5.2244368][-6.1758337 -4.9436517 -4.0162625 -1.2984595 0.27169561 1.2955699 2.0800505 3.5035424 3.9080291 3.3765054 3.0040965 -0.85188246 -5.0244708 -5.0315075 -4.294807][-5.5362034 -4.5817823 -3.0416677 -1.4241867 -0.51750183 0.13217402 0.68778849 0.744668 0.95451164 1.1049476 1.2671566 -1.3009949 -4.7876287 -4.4419947 -3.2120602][-7.18223 -6.0513258 -5.343998 -3.9756112 -3.6711116 -4.1149344 -4.2242236 -3.7942119 -3.2119961 -2.4521322 -2.4546156 -5.0101438 -7.337863 -6.1972103 -3.6711614][-10.588137 -8.278223 -5.9759989 -4.6102428 -4.3683128 -4.5232038 -5.0180111 -6.1513767 -6.5222969 -6.2650189 -5.9919739 -6.8885684 -8.6882677 -7.2236137 -5.6138539][-10.89866 -8.4459934 -5.9148412 -5.0888805 -5.548769 -5.5543904 -6.100318 -6.6082621 -6.8256993 -6.4399924 -6.3539743 -6.8767123 -8.0161171 -6.8796477 -4.7759781][-10.852535 -9.4251842 -6.8707738 -5.518364 -5.7454243 -5.648643 -5.7226167 -4.9640541 -3.7468615 -4.179801 -5.2562418 -5.3354988 -4.7203264 -3.7824481 -3.5687814][-6.49331 -5.8263726 -4.9514031 -4.6578979 -4.748991 -4.4064651 -4.1669416 -3.717654 -3.0703392 -1.9335821 -0.672235 -3.1676707 -4.3229036 -3.9400966 -3.8528547]]...]
INFO - root - 2017-12-15 21:56:41.514091: step 63110, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 49h:01m:03s remains)
INFO - root - 2017-12-15 21:56:48.075030: step 63120, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 47h:53m:09s remains)
INFO - root - 2017-12-15 21:56:54.695967: step 63130, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 49h:48m:26s remains)
INFO - root - 2017-12-15 21:57:01.305745: step 63140, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 48h:59m:06s remains)
INFO - root - 2017-12-15 21:57:07.952323: step 63150, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 49h:30m:04s remains)
INFO - root - 2017-12-15 21:57:14.511999: step 63160, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 50h:29m:54s remains)
INFO - root - 2017-12-15 21:57:21.076380: step 63170, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 49h:31m:50s remains)
INFO - root - 2017-12-15 21:57:27.711262: step 63180, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 48h:36m:26s remains)
INFO - root - 2017-12-15 21:57:34.202429: step 63190, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 50h:14m:08s remains)
INFO - root - 2017-12-15 21:57:40.850910: step 63200, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 50h:08m:20s remains)
2017-12-15 21:57:41.370299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8247042 -4.03133 -3.3296046 -3.9589911 -3.9542139 -3.6570182 -3.4606116 -3.1981382 -2.0717995 -1.39148 -0.68996048 -3.4472673 -5.7641873 -6.3879395 -7.1680923][-2.1432662 -1.8074682 -0.8268652 -1.9248412 -2.6181204 -2.7345433 -2.3085744 -2.0134215 -1.1008954 0.29401255 1.1487141 -2.234833 -5.2139487 -6.4981256 -7.5867963][-1.9430645 -0.92800331 0.449512 0.19624853 -0.5745163 -1.5479898 -1.8271725 -1.4056892 -0.77209949 -0.25848341 0.035262585 -2.3380451 -4.4629908 -5.6248522 -7.4879675][-1.8348253 -2.1049078 -1.5678782 -1.230865 -1.405859 -1.6850529 -1.1476216 -1.0437918 -1.4780221 -0.72015285 0.19800091 -3.2443891 -6.0841746 -7.4369235 -8.2029209][-3.0238092 -3.7565703 -3.5236962 -2.4975116 -1.7864523 -1.4151959 -0.49880362 -0.52350283 -0.89320946 -0.65318251 -1.1266375 -3.9221306 -5.3248787 -6.7229972 -8.0583763][-6.703783 -5.5687714 -3.1482861 -1.7501097 -0.39661646 0.99881935 2.3004031 1.8732347 1.4186487 0.78123093 -1.1824784 -4.1636295 -6.1590767 -6.6512251 -7.6066647][-8.35764 -6.9438553 -4.2069168 -1.5591555 0.44869089 1.5972848 3.0032668 3.5036302 3.5032592 1.4415317 -1.508903 -5.2739687 -8.3323135 -7.8492661 -8.1573772][-10.447889 -8.3114023 -5.0582032 -1.3611374 1.291338 3.0629697 4.239675 3.1817851 2.5465436 1.1462512 -0.66897345 -5.2613611 -8.4469852 -8.4838982 -9.4365292][-7.94876 -6.5095091 -5.1017685 -1.5125933 1.518436 3.2160611 4.6306109 3.4331088 2.1271906 0.22984648 -1.6551661 -5.6066136 -8.6351309 -8.6794434 -9.2560778][-7.0264397 -6.8311186 -5.7453618 -2.6437366 -0.58306026 1.2404923 2.8476634 1.786356 0.67634344 -0.83729267 -2.4723959 -6.3959126 -8.6364031 -8.3367424 -8.562252][-9.7170639 -8.1692858 -5.592361 -4.037919 -3.347075 -2.6392956 -1.6406784 -2.3189282 -2.4506192 -3.3591566 -4.2873182 -8.58349 -10.189816 -10.372889 -8.1237564][-12.865213 -10.685582 -8.0626993 -6.5322795 -5.4299269 -5.2123451 -5.4664989 -6.3956065 -6.6065803 -7.1065321 -7.1769552 -9.3973732 -9.27054 -9.0213108 -7.1594892][-12.216753 -10.446005 -8.2680855 -7.1102633 -6.9069591 -7.1288147 -7.3098817 -8.3118706 -8.9839325 -8.8437014 -8.8033009 -9.5234928 -9.2477684 -8.9722652 -6.2693539][-8.489522 -7.8495283 -6.8914495 -5.9169345 -5.2346172 -5.6664071 -6.5059323 -7.3649268 -7.8584156 -8.5870113 -8.8831577 -8.1702881 -7.282752 -7.5564289 -6.6325512][-7.3131766 -6.3849549 -5.8517423 -5.2780447 -5.094224 -5.333674 -5.678884 -6.0006042 -6.2116747 -6.2815642 -6.3374448 -7.0130625 -6.9696155 -7.2784839 -7.1980929]]...]
INFO - root - 2017-12-15 21:57:48.008164: step 63210, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 48h:21m:44s remains)
INFO - root - 2017-12-15 21:57:54.733409: step 63220, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 48h:52m:07s remains)
INFO - root - 2017-12-15 21:58:01.404252: step 63230, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 48h:37m:07s remains)
INFO - root - 2017-12-15 21:58:08.008765: step 63240, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 50h:40m:04s remains)
INFO - root - 2017-12-15 21:58:14.524711: step 63250, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 50h:26m:10s remains)
INFO - root - 2017-12-15 21:58:21.189128: step 63260, loss = 0.11, batch loss = 0.07 (11.7 examples/sec; 0.685 sec/batch; 51h:12m:19s remains)
INFO - root - 2017-12-15 21:58:27.744166: step 63270, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 49h:31m:49s remains)
INFO - root - 2017-12-15 21:58:34.239930: step 63280, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 48h:17m:18s remains)
INFO - root - 2017-12-15 21:58:40.902371: step 63290, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 48h:50m:29s remains)
INFO - root - 2017-12-15 21:58:47.441653: step 63300, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 50h:18m:03s remains)
2017-12-15 21:58:47.954180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.8853588 -9.8358631 -9.1192789 -8.3858147 -8.58002 -8.8684025 -8.8023682 -8.4137774 -7.8254213 -7.2645903 -6.7051177 -8.7009706 -10.152332 -11.191766 -11.241478][-10.510056 -11.51399 -11.724202 -10.934505 -9.9674 -9.8069525 -10.146354 -10.605257 -10.28229 -9.393939 -8.4405413 -10.218129 -11.076785 -10.347046 -11.070932][-7.63328 -9.6559553 -11.45044 -10.424902 -9.9184732 -10.29737 -10.294874 -10.406363 -10.694633 -10.140448 -9.6500473 -11.743519 -11.933445 -11.333584 -11.215155][-6.0158997 -7.0683403 -6.9932566 -7.1692929 -8.041502 -7.9672546 -8.0051384 -8.7553883 -9.4071093 -9.78449 -9.1077 -11.635193 -13.316422 -12.648199 -11.308132][-5.5157766 -5.7326169 -6.5182691 -5.2817016 -4.2199879 -3.3404474 -3.0890808 -4.1306162 -5.6149545 -6.4366713 -7.1279182 -10.483088 -11.574825 -11.766376 -12.257773][-5.56046 -6.1646667 -4.806674 -3.5865915 -2.8624275 -0.957026 0.11665678 0.5961318 -0.33972168 -3.1332176 -4.8222852 -8.1478605 -9.5613852 -10.694038 -11.235852][-6.9448872 -6.6058955 -5.3254147 -3.2156909 -1.1167603 1.0742164 2.3021188 2.7202964 2.1483765 0.49791574 -0.80165911 -4.9873161 -7.7425208 -8.789257 -9.90296][-7.0777035 -6.7149458 -4.7066483 -2.2586269 -0.10329056 2.6616645 3.682013 3.7085538 3.3982854 2.6826892 1.4099908 -2.7336931 -5.3521581 -7.0067606 -8.7807856][-5.4381223 -5.2174635 -3.3280346 -1.0498042 0.5680666 2.1821551 3.1882768 4.89031 5.3261085 3.9427476 2.9842591 -0.61469555 -3.7570045 -5.7427778 -7.677567][-4.7257719 -4.7348208 -3.295347 -0.83755732 0.99521208 1.7088356 1.4510493 2.2820573 3.348125 4.0792594 3.3193507 -1.1582212 -3.3880136 -4.63617 -6.1153584][-8.4747314 -7.7799511 -6.2371168 -3.9457541 -2.3561375 -1.2716002 -1.3836937 -1.9714332 -1.2094598 -0.85129833 -1.2500954 -4.6122136 -6.7888026 -6.5457973 -6.4656873][-11.263716 -10.860955 -9.8084316 -8.161087 -6.3906918 -5.7484426 -6.0755081 -6.3510275 -6.4633155 -6.5662789 -6.9156666 -8.7133522 -9.9319191 -10.264806 -8.6931019][-12.220455 -11.762384 -10.702003 -9.069334 -7.8358088 -6.9484835 -7.2773838 -8.3182373 -8.733881 -8.7266006 -8.66622 -10.536519 -11.721888 -10.630177 -9.1668434][-10.456394 -10.197123 -9.5967541 -8.4056759 -7.3296657 -6.9120884 -7.2039409 -6.9287357 -6.9879231 -8.2364979 -9.000391 -9.2601652 -9.1684961 -8.5964727 -8.7868347][-8.4287415 -8.4445248 -8.0512009 -7.3115106 -6.8525362 -6.4371266 -6.2056022 -5.9921322 -6.4747672 -6.3917003 -6.3861561 -8.0541573 -9.0437145 -8.6848192 -8.7639494]]...]
INFO - root - 2017-12-15 21:58:54.473650: step 63310, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 48h:04m:07s remains)
INFO - root - 2017-12-15 21:59:01.037197: step 63320, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 50h:59m:34s remains)
INFO - root - 2017-12-15 21:59:07.636707: step 63330, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 50h:02m:26s remains)
INFO - root - 2017-12-15 21:59:14.137257: step 63340, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 47h:35m:07s remains)
INFO - root - 2017-12-15 21:59:20.736548: step 63350, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 49h:42m:34s remains)
INFO - root - 2017-12-15 21:59:27.337622: step 63360, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 50h:15m:21s remains)
INFO - root - 2017-12-15 21:59:33.989857: step 63370, loss = 0.20, batch loss = 0.15 (11.7 examples/sec; 0.682 sec/batch; 50h:59m:35s remains)
INFO - root - 2017-12-15 21:59:40.545195: step 63380, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 48h:47m:27s remains)
INFO - root - 2017-12-15 21:59:47.115553: step 63390, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 48h:57m:09s remains)
INFO - root - 2017-12-15 21:59:53.738530: step 63400, loss = 0.17, batch loss = 0.13 (11.6 examples/sec; 0.690 sec/batch; 51h:32m:33s remains)
2017-12-15 21:59:54.312938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5817938 -5.2422905 -3.9191847 -2.93075 -3.7860684 -4.2595267 -4.3772173 -4.0762024 -3.7850332 -3.8854923 -4.30072 -7.4039888 -9.0624962 -8.9369469 -7.8838453][-9.3823185 -8.7047529 -5.9937963 -4.272449 -4.1743708 -5.2895975 -6.3586087 -6.09872 -5.6102414 -5.708293 -5.9324365 -9.7415266 -11.709625 -11.104023 -10.26956][-5.474997 -8.2558346 -8.5846176 -6.3640695 -5.270484 -5.1163826 -5.2398605 -6.3028831 -7.4960928 -6.8825397 -6.5699058 -11.107815 -12.851914 -11.919477 -11.724991][-3.6399219 -4.7562714 -4.561058 -3.7087548 -4.2283368 -4.4011602 -4.08626 -4.1474538 -4.9303832 -5.3830204 -5.772541 -9.2515354 -10.495897 -11.412251 -11.48683][-6.0712581 -6.5377388 -4.4897294 -1.4312339 -0.23812485 -1.065043 -1.3967237 -2.54968 -4.073226 -3.0250888 -2.1562326 -6.5450463 -8.908864 -9.6239548 -9.7558689][-6.2933092 -7.1321416 -5.9539151 -3.4157517 -0.36486816 1.9583278 3.8612113 0.82903862 -1.8781936 -0.54266691 -0.31791162 -4.2426543 -5.5030694 -7.0328226 -8.99979][-7.3221259 -7.6442204 -6.1920409 -3.752862 -1.6832662 1.2679071 4.7698226 3.7806125 2.8195891 0.64384508 -1.8362854 -3.374274 -3.6410604 -4.7864838 -4.5439615][-7.7332587 -6.4923272 -4.34617 -1.8751471 -0.40328693 1.9311881 4.9399171 5.05073 5.3755794 3.1960502 0.51677847 -5.0946836 -8.8308907 -7.9927974 -6.3433166][-6.8612437 -5.5433168 -4.1628256 -1.4502177 -0.40666008 2.0016432 2.9434428 1.9350228 1.8198791 1.8099451 1.1741738 -4.9697828 -10.092339 -11.984655 -11.52342][-5.4287109 -4.6421843 -3.5304954 -1.7950587 -2.0090501 -0.38163137 0.49375534 0.1399231 0.035207272 -1.4417648 -2.6396639 -6.8937931 -10.176626 -11.745041 -12.394406][-3.420928 -3.2236526 -3.2471325 -2.2072732 -1.8322277 -1.3151188 -1.8482835 -0.93643713 -0.14557791 -2.2611697 -3.1696498 -6.8403969 -8.9001675 -11.404467 -12.749997][-6.1681638 -4.861248 -3.4870543 -3.1702664 -3.4645798 -3.4909728 -3.579107 -2.6481512 -2.6967304 -3.8187056 -4.3335505 -7.2546444 -8.455142 -10.714291 -9.6015711][-12.173214 -11.047108 -8.6561232 -7.3254557 -7.3216634 -7.3302822 -7.1844277 -6.3883224 -6.0755038 -5.5276513 -5.2020307 -7.5291348 -9.4606657 -10.756601 -9.6308327][-9.8977985 -9.8233709 -8.7110481 -8.3819647 -7.7869081 -7.8006296 -7.8681908 -7.9862 -7.9999104 -6.7703037 -5.8936434 -5.8574452 -5.7721062 -8.1136608 -7.973948][-6.0754352 -5.7236609 -5.6063571 -3.2948091 -2.6159623 -4.3503046 -5.8224387 -5.879776 -5.2611341 -6.1262345 -6.9116449 -6.4124618 -6.5535531 -6.5464897 -5.507937]]...]
INFO - root - 2017-12-15 22:00:00.820767: step 63410, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 48h:08m:10s remains)
INFO - root - 2017-12-15 22:00:07.428435: step 63420, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 48h:41m:39s remains)
INFO - root - 2017-12-15 22:00:13.988908: step 63430, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 49h:49m:47s remains)
INFO - root - 2017-12-15 22:00:20.592767: step 63440, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 48h:03m:37s remains)
INFO - root - 2017-12-15 22:00:27.219883: step 63450, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 49h:52m:11s remains)
INFO - root - 2017-12-15 22:00:33.854874: step 63460, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 49h:54m:26s remains)
INFO - root - 2017-12-15 22:00:40.484901: step 63470, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 49h:27m:22s remains)
INFO - root - 2017-12-15 22:00:47.150421: step 63480, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.669 sec/batch; 50h:01m:14s remains)
INFO - root - 2017-12-15 22:00:53.715196: step 63490, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 48h:40m:31s remains)
INFO - root - 2017-12-15 22:01:00.339071: step 63500, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 49h:12m:34s remains)
2017-12-15 22:01:00.830186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0350919 -5.6280117 -5.48036 -5.4915953 -6.0569506 -6.4666052 -6.8807573 -7.4441509 -7.7276907 -6.965672 -5.5478096 -3.5104773 -5.6389642 -7.3704538 -7.1475592][-6.6734366 -6.6338582 -6.614881 -6.6256561 -7.0523267 -7.6792912 -7.9366198 -8.1506338 -8.3825 -7.5771127 -6.2929912 -4.4745646 -6.1597509 -8.2214432 -7.6579113][-4.402092 -5.7487521 -7.4755797 -8.2711573 -8.5596113 -8.6251011 -8.6749 -8.1523991 -8.1739626 -8.1055527 -7.1997995 -5.7292404 -7.7702379 -9.3656864 -8.4855633][-6.3182416 -7.0264726 -7.6717596 -8.358017 -8.7547894 -8.1199646 -7.2776828 -7.8156061 -7.9784813 -7.5517058 -7.2393613 -6.3049397 -8.9900723 -10.793894 -9.7829323][-6.9573312 -8.66902 -9.9119644 -8.4934483 -7.5599871 -4.8754482 -2.5550373 -5.3486366 -7.4383111 -6.88088 -6.1829443 -5.8280787 -9.0906067 -11.356115 -10.803686][-8.2539587 -8.7807722 -8.8683481 -7.9310265 -5.7240062 0.25474215 4.2440715 1.0367846 -2.6429932 -5.1609144 -6.2713327 -4.47556 -7.5059228 -10.86648 -10.670948][-10.16818 -10.094259 -9.1007233 -6.4867692 -1.8068912 4.2874446 7.9658923 6.3981633 3.0523572 -2.5424018 -5.8507643 -4.5506907 -7.4681387 -9.8419619 -9.2572651][-9.8187637 -9.8112268 -9.6560993 -6.771174 -1.2090526 5.0534415 9.1999454 8.7733517 6.706634 0.77700663 -4.0088582 -4.6501608 -8.3203125 -9.7825794 -8.77468][-8.7859707 -8.6252718 -8.0917692 -7.0016751 -4.008914 2.6099038 7.6359181 5.8841138 2.4511895 -0.92002296 -3.304451 -4.6092706 -8.999527 -10.959972 -9.4955788][-7.4733377 -7.8423576 -7.874989 -7.4083729 -5.7685733 -1.3648443 1.6306438 1.4921217 -0.78638077 -4.2219381 -5.8541102 -5.79257 -9.682169 -12.287022 -11.469908][-9.8006115 -9.6231718 -10.098876 -9.45381 -8.1151829 -6.7950859 -6.2511153 -5.5014591 -5.6398911 -7.8136725 -9.072896 -8.104147 -10.901397 -12.783556 -11.661757][-13.090372 -12.139172 -12.316974 -11.331388 -10.2436 -9.8577156 -9.4438763 -8.9414349 -9.2531137 -10.128223 -10.755061 -9.9627886 -11.655669 -11.471865 -9.4699974][-13.701326 -12.947083 -11.581423 -11.021017 -10.795009 -10.000137 -9.5436239 -9.5585375 -9.6763849 -9.35236 -8.8307247 -7.87745 -9.1677914 -9.4704227 -7.3362112][-9.9501038 -9.2633257 -8.1803865 -8.6607857 -8.9862909 -8.5735054 -8.2143154 -7.3225136 -6.7915959 -6.9314413 -6.8268929 -6.0318556 -6.0892611 -6.8699179 -6.3446951][-5.3225789 -5.195365 -4.780405 -4.0230546 -3.9046412 -5.2111993 -5.0407467 -4.3926396 -4.3763461 -4.3788772 -4.6289158 -5.0147328 -5.9930496 -6.4364519 -6.4545064]]...]
INFO - root - 2017-12-15 22:01:07.434169: step 63510, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 48h:16m:54s remains)
INFO - root - 2017-12-15 22:01:14.001183: step 63520, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 48h:04m:52s remains)
INFO - root - 2017-12-15 22:01:20.602380: step 63530, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 49h:26m:47s remains)
INFO - root - 2017-12-15 22:01:27.104714: step 63540, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 48h:02m:41s remains)
INFO - root - 2017-12-15 22:01:33.721132: step 63550, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 49h:46m:02s remains)
INFO - root - 2017-12-15 22:01:40.351078: step 63560, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 49h:22m:22s remains)
INFO - root - 2017-12-15 22:01:46.895387: step 63570, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 48h:54m:30s remains)
INFO - root - 2017-12-15 22:01:53.567846: step 63580, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 48h:49m:30s remains)
INFO - root - 2017-12-15 22:02:00.191440: step 63590, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 48h:03m:43s remains)
INFO - root - 2017-12-15 22:02:06.761637: step 63600, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 51h:05m:09s remains)
2017-12-15 22:02:07.325850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9865756 -7.271389 -8.576314 -9.2276249 -10.171747 -10.809088 -10.71212 -9.0150862 -8.2513084 -8.7628994 -8.99095 -10.779801 -12.717134 -13.606024 -11.421721][-6.4511747 -7.3205361 -7.806859 -8.8117876 -9.8787308 -10.045095 -9.3759 -9.9691715 -10.48064 -9.4534388 -8.3920994 -11.151505 -13.429731 -13.69655 -13.109205][-5.1079984 -7.7736807 -9.20587 -8.9486046 -9.358943 -9.6683674 -10.000284 -9.136054 -8.0972872 -8.54331 -8.5391273 -11.006954 -12.632244 -13.208941 -13.618124][-6.9840336 -7.8712049 -8.1824169 -7.2158933 -7.1792054 -7.1043096 -6.7487049 -7.7405567 -8.2765408 -7.5026069 -6.9070821 -10.247085 -13.201138 -14.270176 -13.615723][-9.212532 -10.325998 -11.461687 -9.0527 -5.5027046 -2.7932634 -1.2474408 -3.6582243 -7.1521478 -7.4368877 -7.2986917 -10.239191 -12.108926 -13.099035 -13.172692][-9.5370255 -10.258924 -10.97158 -7.5382752 -3.1728349 0.99068737 4.5927625 3.2254491 -0.58096838 -4.1193671 -7.4830651 -9.8773117 -10.78055 -11.500133 -11.577844][-10.177917 -11.164625 -9.3218136 -4.2176075 -0.052732468 2.6817908 6.4459586 7.430326 5.072535 -0.13513136 -5.7962337 -9.5081568 -11.879104 -12.477994 -11.355983][-10.621054 -10.231113 -8.5529728 -2.7529662 1.3071094 5.8671069 9.6272469 6.839478 4.9071212 2.4840493 -2.2804406 -7.9758096 -11.714634 -12.602427 -12.873583][-7.2397175 -8.4976768 -7.5556059 -4.0871296 -1.0702596 5.0482183 7.8033376 5.9163117 6.0637279 1.7546306 -3.0987334 -8.3515511 -12.337344 -13.73435 -13.336814][-6.5331783 -6.1148 -6.4414206 -3.9122102 -1.3682566 0.88050318 2.03261 3.2261386 3.740026 0.12806702 -2.7692506 -8.059761 -13.42099 -13.809393 -13.291796][-9.7829838 -11.026013 -12.028736 -9.15694 -7.195889 -6.0541706 -4.1732616 -3.59537 -3.0150704 -3.6136575 -4.8780589 -11.361538 -14.724089 -14.952492 -14.319616][-13.73979 -13.025184 -13.73333 -12.939963 -12.393978 -10.126093 -8.6870518 -9.4814291 -8.6071949 -8.5194569 -9.2683029 -12.104998 -13.68492 -15.734299 -14.976734][-13.459095 -13.198893 -13.417648 -14.633434 -14.768478 -12.749209 -12.192509 -12.201296 -11.821711 -10.836683 -10.434294 -12.716584 -13.68687 -14.297508 -12.385305][-13.101356 -11.752329 -10.402132 -9.9040356 -10.357782 -11.905727 -12.662102 -11.446226 -11.095369 -11.678242 -11.150302 -11.794914 -11.854671 -12.257874 -10.3541][-9.4949827 -9.0833158 -8.0883055 -7.2005386 -6.3322978 -6.80764 -7.9941349 -9.2221489 -10.391081 -9.6980886 -9.4670124 -11.75482 -12.835454 -12.295784 -11.316669]]...]
INFO - root - 2017-12-15 22:02:13.949924: step 63610, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 50h:13m:20s remains)
INFO - root - 2017-12-15 22:02:20.717017: step 63620, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 51h:04m:03s remains)
INFO - root - 2017-12-15 22:02:27.301117: step 63630, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 49h:07m:05s remains)
INFO - root - 2017-12-15 22:02:33.887754: step 63640, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 48h:38m:10s remains)
INFO - root - 2017-12-15 22:02:40.560684: step 63650, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 49h:17m:15s remains)
INFO - root - 2017-12-15 22:02:47.200499: step 63660, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 49h:24m:50s remains)
INFO - root - 2017-12-15 22:02:53.764177: step 63670, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 50h:04m:06s remains)
INFO - root - 2017-12-15 22:03:00.409877: step 63680, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 48h:55m:30s remains)
INFO - root - 2017-12-15 22:03:06.996847: step 63690, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 49h:28m:53s remains)
INFO - root - 2017-12-15 22:03:13.646048: step 63700, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 50h:12m:40s remains)
2017-12-15 22:03:14.235915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3317657 -6.4048862 -5.9370456 -4.8624964 -4.8807187 -4.9232931 -4.9753995 -4.591651 -4.1496973 -4.4184489 -4.302979 -6.3850079 -8.7435007 -8.4352512 -9.0510273][-4.230094 -3.7518468 -3.1795807 -2.4005091 -3.1593208 -3.5976057 -3.5955799 -3.2845545 -3.5505466 -3.3339598 -3.1269343 -5.0764027 -6.5960245 -7.5245657 -9.4279423][-2.4506586 -2.4522147 -2.59888 -1.7188852 -2.3077414 -2.4317379 -2.7944355 -2.870451 -2.5741169 -2.4935608 -2.6068606 -4.7485838 -6.8234692 -7.4044619 -8.1420231][-2.4849365 -2.0254436 -1.3749948 -1.274075 -2.4064724 -2.1498358 -1.974076 -1.9493349 -2.0497859 -2.3061113 -1.9745567 -3.932538 -6.4828238 -7.2035785 -7.6371584][-1.7670841 -2.5795319 -1.8697116 -0.97242069 -1.5495028 -0.89555645 -0.67044592 -0.41845655 -0.075835705 -0.40343809 -0.9244051 -3.1634827 -5.1131544 -5.8608751 -6.9027185][-2.9989123 -2.9386532 -2.0783989 -1.2039247 -1.0544634 -0.028748512 0.59258509 0.88887787 1.2198753 1.1001925 0.23356676 -2.3125072 -4.1873088 -4.8904114 -5.6077542][-5.088903 -4.7376785 -3.4442821 -2.0536594 -0.51800537 1.0793705 1.5785728 1.4715114 1.5489154 1.1400695 0.91627264 -1.603107 -3.60631 -3.7015719 -4.8637676][-5.4205608 -5.143796 -3.8142877 -1.7626123 -0.2819643 0.64843178 1.4019585 1.7810764 1.700933 1.2682142 0.86757946 -1.0961261 -2.9770188 -3.7516623 -5.1983995][-4.5686622 -4.5943594 -3.3888276 -1.0565872 0.076207161 0.92304134 1.2687159 1.2899528 1.197897 0.69309092 0.056872368 -2.4194968 -4.3289633 -5.0453939 -6.0038819][-4.501492 -3.5719769 -1.4818153 0.24886942 0.96464872 1.1216297 0.63223267 0.64836264 0.097554207 0.37588692 0.14110279 -2.6139386 -4.7162356 -5.6075282 -6.8493481][-7.2829156 -6.1908226 -3.8574531 -1.1046414 -0.53561211 -0.74089622 -1.0932698 -1.7176213 -2.55681 -2.3610907 -2.2958992 -4.9409218 -6.8551135 -6.738482 -7.679894][-8.7538357 -8.145339 -6.1627769 -3.2759702 -2.5880175 -3.252439 -4.0587406 -5.2151532 -5.7666874 -5.1985245 -5.2600737 -6.46636 -7.5418606 -6.928174 -6.97896][-9.072916 -8.2624454 -6.364984 -4.9789662 -4.6343441 -4.2615891 -4.776319 -5.4402113 -5.9890852 -6.0867133 -5.3898764 -5.3548717 -6.3052125 -5.6384635 -5.4620271][-8.5460243 -7.3967967 -6.1997604 -5.7770348 -5.3793139 -5.3641176 -5.8198304 -5.4719439 -4.6709738 -4.9234338 -4.8825035 -4.5096021 -4.4224095 -3.723001 -3.603833][-6.6923938 -6.8901353 -6.6675673 -5.4845948 -4.9151525 -4.3441792 -4.1264973 -4.3302979 -4.3632193 -4.0448031 -3.5214329 -3.6497283 -4.6043787 -5.1406984 -5.4442482]]...]
INFO - root - 2017-12-15 22:03:20.834399: step 63710, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 48h:59m:09s remains)
INFO - root - 2017-12-15 22:03:27.502905: step 63720, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 50h:28m:46s remains)
INFO - root - 2017-12-15 22:03:34.133099: step 63730, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 48h:51m:54s remains)
INFO - root - 2017-12-15 22:03:40.769123: step 63740, loss = 0.19, batch loss = 0.15 (11.3 examples/sec; 0.708 sec/batch; 52h:53m:17s remains)
INFO - root - 2017-12-15 22:03:47.363390: step 63750, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 48h:38m:45s remains)
INFO - root - 2017-12-15 22:03:53.939969: step 63760, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.655 sec/batch; 48h:53m:07s remains)
INFO - root - 2017-12-15 22:04:00.526122: step 63770, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 49h:39m:18s remains)
INFO - root - 2017-12-15 22:04:07.087492: step 63780, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 48h:58m:15s remains)
INFO - root - 2017-12-15 22:04:13.656410: step 63790, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 48h:00m:12s remains)
INFO - root - 2017-12-15 22:04:20.287454: step 63800, loss = 0.18, batch loss = 0.14 (11.4 examples/sec; 0.700 sec/batch; 52h:14m:49s remains)
2017-12-15 22:04:20.851906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.3714161 -10.289018 -10.870741 -10.136036 -10.052025 -10.140936 -9.9366169 -9.0212812 -8.3404837 -7.8109169 -6.8614388 -5.9234824 -7.2576594 -9.7230415 -9.7650948][-7.9078684 -9.5834026 -9.2948227 -8.083128 -8.9004383 -9.4625521 -9.0889378 -8.3900452 -8.6246119 -8.5485439 -7.420589 -6.354929 -7.098114 -10.869747 -11.181627][-4.9211354 -7.42107 -8.9394579 -8.0057087 -8.1622162 -8.3126068 -8.8868895 -8.5035191 -7.8118792 -7.7188606 -7.5247078 -7.2017684 -8.1314411 -11.898463 -13.309093][-6.7100859 -8.7498255 -9.744523 -9.1165543 -7.8332663 -6.0095682 -6.2948828 -7.3507333 -7.8075724 -7.605123 -6.4029341 -5.9384961 -7.9121289 -12.441258 -13.172209][-9.2927837 -12.053496 -12.836809 -10.980989 -7.9873486 -3.06855 -0.41426706 -3.6012752 -6.5648632 -6.8784537 -6.67391 -5.2168927 -6.3425479 -11.959423 -14.2941][-10.311052 -11.303606 -12.466297 -10.952042 -6.7663307 1.3397875 5.9616485 3.8604236 0.090305805 -3.6389282 -6.6231246 -4.8558025 -4.5039482 -9.6373148 -12.477359][-10.743347 -10.589254 -8.93685 -7.0600562 -3.9377875 1.7713237 8.2241859 9.9448967 6.5568757 -0.82818842 -6.9929385 -7.4212952 -7.6641483 -9.5795259 -9.8064127][-9.2989016 -9.3002062 -9.180872 -4.0353665 -0.7155261 3.2159066 8.1718235 9.482193 7.8158984 2.3312602 -5.5712218 -8.882761 -10.765521 -12.182645 -11.503643][-7.9652147 -8.1907673 -8.45353 -5.8100491 -2.9173536 1.5587006 5.0222592 5.318665 4.7065358 0.30859852 -5.406682 -9.4071112 -12.985895 -16.099531 -14.81712][-6.2078304 -6.7895246 -7.7368975 -7.5625105 -7.0231237 -4.4141846 -0.40020514 1.1273642 0.28903246 -3.1458654 -6.9586658 -10.396727 -13.496836 -17.879013 -18.028307][-9.7409325 -9.07908 -11.330781 -10.088385 -10.755549 -10.92166 -8.448246 -6.6960163 -6.0574541 -6.7915096 -10.305869 -13.526011 -16.206362 -18.348013 -18.028774][-15.314219 -15.413469 -14.908463 -14.003315 -14.42206 -14.765543 -14.695932 -14.135338 -11.938725 -12.087417 -13.594484 -14.220909 -13.624756 -15.206413 -14.507448][-15.772106 -15.439362 -16.125931 -15.052038 -14.548519 -14.236191 -14.685085 -13.873581 -12.353189 -12.449223 -12.650263 -13.098763 -12.014892 -11.914263 -9.9306593][-12.950509 -12.5103 -12.518101 -10.583256 -9.3528214 -10.402864 -11.64666 -10.406591 -9.4864569 -9.7758007 -9.908227 -8.8478994 -7.1753912 -7.2763572 -7.1725864][-7.7847414 -7.790626 -7.5700769 -5.6577783 -4.7236881 -4.6340532 -5.2818365 -5.66086 -6.2819929 -6.4381676 -7.0229688 -7.4251523 -6.1307516 -6.5556178 -6.2005215]]...]
INFO - root - 2017-12-15 22:04:27.481814: step 63810, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 48h:38m:03s remains)
INFO - root - 2017-12-15 22:04:34.092126: step 63820, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 49h:08m:28s remains)
INFO - root - 2017-12-15 22:04:40.680318: step 63830, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 49h:15m:59s remains)
INFO - root - 2017-12-15 22:04:47.298877: step 63840, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.636 sec/batch; 47h:29m:35s remains)
INFO - root - 2017-12-15 22:04:53.926884: step 63850, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 48h:20m:45s remains)
INFO - root - 2017-12-15 22:05:00.518006: step 63860, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 49h:04m:49s remains)
INFO - root - 2017-12-15 22:05:07.164157: step 63870, loss = 0.16, batch loss = 0.11 (11.5 examples/sec; 0.696 sec/batch; 51h:57m:01s remains)
INFO - root - 2017-12-15 22:05:13.805966: step 63880, loss = 0.19, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 48h:16m:23s remains)
INFO - root - 2017-12-15 22:05:20.448177: step 63890, loss = 0.16, batch loss = 0.12 (11.5 examples/sec; 0.696 sec/batch; 51h:55m:10s remains)
INFO - root - 2017-12-15 22:05:27.031400: step 63900, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 49h:08m:30s remains)
2017-12-15 22:05:27.586915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.1096439 -6.9630938 -5.8716793 -5.2737226 -5.8648129 -6.7242966 -7.1568213 -7.6858711 -7.3666058 -7.4934063 -7.1385479 -7.1100311 -7.7149267 -7.1911697 -6.2811475][-7.4227562 -6.3997746 -5.769362 -5.2172437 -5.6954556 -6.8919225 -7.0751562 -7.0559134 -6.8275347 -7.1629248 -6.8192797 -6.886446 -7.615036 -7.4233713 -6.1761508][-5.0292625 -4.8195481 -5.0503235 -4.3420806 -5.203126 -5.4974165 -5.4493532 -6.2015862 -6.611958 -6.3221421 -6.4334521 -6.613018 -7.6506424 -8.0288162 -6.8652811][-4.0744519 -3.7940149 -3.9006321 -3.9935532 -4.2438741 -4.0291586 -3.6791523 -3.9713964 -5.00572 -5.7524371 -6.3073473 -7.5639782 -8.4979534 -8.2661171 -6.6801605][-4.8268189 -4.2163248 -4.6630182 -4.1053782 -3.7958736 -3.067142 -2.4027736 -2.166532 -2.8023703 -3.958786 -5.1180043 -6.3214979 -8.2031021 -8.8099451 -6.90391][-7.1952176 -5.4732814 -4.3275752 -3.6532347 -2.4995403 -1.1706924 -0.1054287 0.639884 0.17957354 -1.7421203 -4.1379457 -5.6059318 -7.2484245 -7.8071647 -5.9946251][-7.3462558 -6.1769996 -4.7805767 -2.6349919 -1.0308318 0.44550562 1.9308023 2.521914 1.9878082 -0.087369442 -2.0349915 -4.1670547 -6.6489959 -6.87985 -4.7110491][-7.6169419 -6.1375022 -4.9449148 -2.5686653 -0.76878262 1.3666682 2.958818 3.66781 3.35949 1.4126959 -0.87273312 -3.8111734 -6.3994603 -7.3869853 -5.506855][-7.499136 -6.1108527 -4.927527 -2.2807281 -0.18503761 1.0303531 2.259891 3.2621045 2.8691602 0.7748003 -1.559145 -4.5047174 -7.6874 -9.0936842 -7.0927076][-6.86342 -5.5677423 -3.2845111 -1.8924084 -0.34596491 0.8498354 1.277174 1.5845075 1.4200678 -0.029298306 -2.5454817 -5.4088125 -8.7666 -11.288298 -10.013969][-7.8283272 -6.5863 -4.9197893 -2.8536429 -2.3211672 -1.5599685 -1.7972195 -2.8242288 -3.98029 -5.0339074 -6.8895183 -9.7248306 -12.349434 -12.4958 -10.639959][-11.021565 -9.9247713 -8.0928917 -6.3151813 -5.5064583 -5.9848485 -6.723516 -7.3199639 -8.4115334 -10.258842 -11.414305 -11.287177 -12.504189 -12.612249 -11.059509][-9.9609966 -8.8835487 -7.8411493 -7.4833555 -7.3838258 -6.4208078 -6.4655042 -8.8413858 -10.416159 -10.985453 -11.647693 -12.26689 -13.015959 -11.097441 -9.4603252][-10.880835 -9.5847569 -8.4057407 -7.0513 -6.1333861 -6.2047973 -6.6688643 -6.8984737 -7.3860779 -8.3400707 -8.6674347 -7.9717 -7.7441859 -7.7431374 -7.4755068][-9.0691385 -9.1676655 -8.2711143 -6.9395432 -5.9191132 -4.5349321 -4.6500835 -4.9853749 -5.8789139 -5.1796336 -4.5910072 -5.7120419 -6.8533978 -6.9914842 -6.9963989]]...]
INFO - root - 2017-12-15 22:05:34.232587: step 63910, loss = 0.12, batch loss = 0.07 (11.6 examples/sec; 0.687 sec/batch; 51h:15m:16s remains)
INFO - root - 2017-12-15 22:05:40.824771: step 63920, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 48h:44m:19s remains)
INFO - root - 2017-12-15 22:05:47.372708: step 63930, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.639 sec/batch; 47h:39m:58s remains)
INFO - root - 2017-12-15 22:05:54.038483: step 63940, loss = 0.18, batch loss = 0.14 (11.6 examples/sec; 0.689 sec/batch; 51h:25m:58s remains)
INFO - root - 2017-12-15 22:06:00.696930: step 63950, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 49h:23m:24s remains)
INFO - root - 2017-12-15 22:06:07.251911: step 63960, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 48h:42m:38s remains)
INFO - root - 2017-12-15 22:06:13.918958: step 63970, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.692 sec/batch; 51h:37m:00s remains)
INFO - root - 2017-12-15 22:06:20.482544: step 63980, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 48h:53m:12s remains)
INFO - root - 2017-12-15 22:06:27.033083: step 63990, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 49h:49m:11s remains)
INFO - root - 2017-12-15 22:06:33.499191: step 64000, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 47h:27m:51s remains)
2017-12-15 22:06:33.996200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.1743021 -9.2618513 -8.6358776 -6.6660304 -6.755538 -5.4066377 -3.3732748 -1.4047966 -0.53795576 -0.23072052 -0.0875535 -2.1110973 -4.0303345 -5.526226 -6.350203][-6.975317 -8.0465641 -7.3590336 -6.7770581 -7.5952358 -6.4880152 -5.299345 -4.2414522 -3.778002 -3.0038443 -1.9443719 -2.6381946 -4.0079403 -4.9965234 -4.8101115][-3.2004437 -5.4769831 -6.6426067 -6.2630835 -6.4285512 -7.2262411 -6.67665 -5.4839067 -5.1489325 -4.456696 -4.3173752 -6.2694454 -7.9237366 -7.6588111 -7.4946575][-4.0898991 -5.6380687 -6.4852595 -6.1222229 -6.2453279 -6.0645313 -5.317575 -5.5961847 -5.4339991 -4.4505005 -4.1539049 -6.4876227 -8.8402109 -10.143242 -10.53228][-5.5556183 -7.2945457 -7.432291 -6.4231563 -6.2129478 -5.4706483 -4.6022172 -4.90112 -4.5054245 -2.9707034 -2.4950001 -5.3894629 -8.3528719 -10.42058 -11.512711][-8.7507038 -8.6660442 -7.5474443 -5.8424921 -3.919291 -1.9241469 -0.66569805 -0.68508053 -0.73417234 -1.1975493 -2.3216825 -3.850059 -6.3812695 -9.12343 -10.78656][-8.1445065 -7.2271843 -5.6994457 -2.979341 -0.6380949 0.87999105 2.189631 2.4906249 2.8483558 1.5137777 0.55567122 -1.7976687 -5.38022 -7.9473352 -10.152952][-6.826674 -6.255271 -4.5232506 -1.0966949 0.58662605 2.6388745 4.2871919 4.73881 5.1131721 3.5029101 2.5664496 -1.8546624 -6.3452215 -8.2197485 -9.5270834][-3.5674314 -3.7272253 -2.8802483 -0.77177334 0.60186768 2.076479 2.4285598 3.014339 3.2944894 4.0829377 5.1263413 -0.3404603 -5.216085 -7.4047561 -8.1438856][-2.6255403 -3.2680347 -2.8197656 -1.3039713 -0.44894505 -0.32453775 -0.14109755 1.69209 3.0781713 3.7990775 4.1048427 0.25409985 -3.3407459 -6.6476707 -8.5697012][-6.6646061 -5.6504693 -4.7118216 -3.1478708 -3.0021369 -2.8127046 -2.4780555 -1.3335381 -0.17811251 1.1044407 1.9887085 -0.595984 -2.5072348 -4.599153 -5.4712849][-11.367908 -10.931532 -9.18874 -7.23125 -6.2126203 -5.7400274 -5.2354317 -4.8821259 -4.0734515 -2.9223344 -2.0564833 -4.000762 -5.4084563 -5.4252563 -5.2553568][-13.410107 -12.210192 -10.909397 -9.5440426 -8.5899687 -7.5171771 -6.454617 -6.9394603 -7.1047835 -6.6619253 -6.1928906 -7.0863285 -7.0314903 -5.96195 -4.2012634][-11.691271 -11.008217 -10.307034 -9.1414547 -8.1762495 -8.4232073 -8.28295 -7.7385697 -7.3069525 -7.9898624 -9.081481 -9.4319077 -9.2783985 -8.1905842 -6.4419346][-9.0404987 -8.4278908 -7.5786009 -6.8340225 -7.1551185 -6.4704528 -6.8673148 -7.433217 -7.2233448 -7.6938362 -8.8483944 -10.826562 -12.303988 -12.390361 -11.146696]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 22:06:40.621125: step 64010, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 50h:10m:37s remains)
INFO - root - 2017-12-15 22:06:47.225938: step 64020, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 50h:07m:09s remains)
INFO - root - 2017-12-15 22:06:53.876338: step 64030, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 50h:06m:12s remains)
INFO - root - 2017-12-15 22:07:00.546549: step 64040, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 47h:26m:12s remains)
INFO - root - 2017-12-15 22:07:07.176702: step 64050, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 49h:08m:19s remains)
INFO - root - 2017-12-15 22:07:13.769430: step 64060, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 48h:45m:24s remains)
INFO - root - 2017-12-15 22:07:20.387017: step 64070, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 49h:11m:14s remains)
INFO - root - 2017-12-15 22:07:26.960019: step 64080, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 47h:43m:56s remains)
INFO - root - 2017-12-15 22:07:33.534506: step 64090, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 48h:58m:12s remains)
INFO - root - 2017-12-15 22:07:40.037642: step 64100, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 47h:40m:41s remains)
2017-12-15 22:07:40.592180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.67773867 -0.59727669 0.37879515 1.7186184 1.6450891 0.17156124 -2.7671506 -4.9296007 -5.3611584 -5.693296 -6.1847453 -8.5970335 -11.825257 -13.06398 -11.157566][-0.80217743 1.5795689 4.1106086 4.8288369 4.0381074 1.6391678 -1.1771498 -3.1307464 -5.6417031 -7.0753675 -7.8764582 -11.271491 -14.574884 -15.611147 -15.019745][-0.32050514 0.43780804 1.1598783 3.7952323 4.3963828 2.8975968 0.93418121 -1.4505072 -3.991045 -6.1197324 -8.1411133 -10.654997 -13.694614 -16.340544 -16.172684][-1.7397654 -0.88158417 1.1367121 3.4735913 2.6461482 1.5356302 0.50073671 -1.009994 -3.0748279 -4.6319695 -5.9789352 -8.9073858 -12.452919 -13.766212 -13.265707][-0.81654119 -1.2399497 -0.52542019 1.5466342 1.8964953 2.95373 2.8215127 0.76782084 -0.76769876 -2.1189103 -3.700156 -7.2353716 -10.888594 -13.20875 -12.878202][-2.9471331 -2.5587006 -2.0818734 1.1797152 3.1021962 4.9721656 5.4042277 4.6078687 3.6084685 1.8616371 -0.36155844 -2.9462042 -6.3415308 -8.7848492 -8.7571659][-3.2960668 -3.3031607 -1.8638816 0.66823626 2.3383508 5.0489488 6.9648118 7.3434567 6.8729119 4.2590756 1.6574683 -1.4185252 -5.7096982 -8.0019207 -8.4762783][-7.0628867 -5.7053256 -3.524586 0.64331436 2.8174071 4.6834836 6.3471055 6.6051517 6.6870103 5.3063979 2.9854922 -1.0953345 -6.1360412 -8.9442883 -9.6536055][-7.7949581 -6.2077365 -4.0137715 -0.84634924 0.985384 3.9707646 5.1591678 4.5008416 4.68391 3.9332786 2.4115758 -1.1265483 -5.9831519 -8.9500113 -9.00901][-8.1522369 -7.332963 -5.0962043 -1.8603044 0.5511446 2.7812791 3.1173806 3.1606774 3.5688128 2.1573291 0.23385286 -2.8849471 -6.8059659 -9.3462362 -9.8894453][-13.041164 -12.013083 -10.237537 -6.2842822 -4.4139128 -2.5743945 -1.3509569 -0.88660049 -0.89646244 -1.4608097 -1.7429817 -4.8428326 -7.8722296 -9.7000809 -8.5354443][-15.287722 -15.407734 -13.914795 -10.791069 -9.405901 -7.5487905 -7.5225229 -7.1013227 -6.6547675 -6.4534044 -6.1247344 -7.060564 -7.8029766 -8.9354057 -8.0596924][-15.014311 -15.060724 -14.043762 -11.466644 -10.336018 -8.4848146 -8.667017 -9.7019768 -10.25828 -9.4825821 -8.43309 -8.1567316 -8.3762455 -8.50526 -6.698216][-11.06558 -11.180962 -11.354741 -9.4315872 -8.7838078 -7.8948784 -8.5267754 -8.1060066 -7.3488116 -7.86001 -7.7883844 -6.3964033 -5.7148771 -6.1261339 -6.0466824][-8.7195082 -7.99586 -6.9348211 -6.0670061 -6.5171404 -5.1274786 -4.5540652 -4.6839037 -5.75884 -5.7082238 -5.3005075 -5.7983017 -6.4045715 -6.0958257 -6.2625341]]...]
INFO - root - 2017-12-15 22:07:47.140347: step 64110, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 48h:50m:16s remains)
INFO - root - 2017-12-15 22:07:53.815622: step 64120, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 48h:59m:01s remains)
INFO - root - 2017-12-15 22:08:00.370603: step 64130, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 50h:08m:02s remains)
INFO - root - 2017-12-15 22:08:06.961425: step 64140, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.633 sec/batch; 47h:11m:36s remains)
INFO - root - 2017-12-15 22:08:13.575350: step 64150, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 48h:40m:27s remains)
INFO - root - 2017-12-15 22:08:20.252292: step 64160, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 48h:20m:47s remains)
INFO - root - 2017-12-15 22:08:26.771280: step 64170, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 47h:48m:41s remains)
INFO - root - 2017-12-15 22:08:33.406274: step 64180, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 49h:13m:56s remains)
INFO - root - 2017-12-15 22:08:39.985019: step 64190, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 48h:40m:47s remains)
INFO - root - 2017-12-15 22:08:46.586785: step 64200, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 49h:07m:40s remains)
2017-12-15 22:08:47.085809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3502159 -7.7639003 -8.6584291 -10.071779 -11.192401 -10.871165 -11.146578 -11.065018 -10.239149 -9.9842749 -9.3063784 -7.6501732 -7.3116851 -6.1252451 -3.9774117][-6.139184 -7.3936386 -8.1058626 -9.6066914 -11.353941 -11.346033 -11.080966 -11.142368 -11.35794 -10.587709 -9.3038845 -8.2270546 -7.1580868 -6.1660852 -5.0346336][-4.093255 -6.7395959 -8.9695816 -10.128326 -10.710968 -10.144352 -9.66242 -9.6264334 -9.6381607 -9.5378494 -9.0725231 -8.2373047 -7.4127426 -6.5068607 -5.9694014][-5.6835952 -7.32852 -8.7620707 -9.9902115 -10.482271 -9.1963081 -7.1784649 -8.1604233 -9.631897 -9.36043 -9.1021214 -10.074568 -11.224294 -11.01951 -9.1244326][-6.18428 -9.1468525 -11.16008 -10.542894 -9.1016521 -5.8916779 -2.4368148 -4.1285543 -6.5914407 -8.2176533 -9.8541012 -10.399206 -10.946667 -11.740837 -10.331803][-7.1845665 -8.68057 -9.1056957 -7.2601118 -5.3018594 -1.4086847 2.6840072 2.3826241 0.67376375 -4.1667042 -8.5644093 -9.0487213 -9.3693027 -10.283747 -9.556921][-8.7670345 -7.6557961 -6.981926 -4.0917854 -1.8776021 2.9990621 8.1737823 7.482614 5.6883683 0.27313375 -5.1958022 -7.5412054 -9.6770744 -10.085217 -8.11474][-10.562339 -9.8207512 -7.0643015 -2.7837746 -0.83139896 4.1096931 9.3237419 8.1185265 6.355783 1.4515719 -4.0895538 -6.9632568 -9.4642229 -10.296541 -9.9696388][-10.146288 -10.188348 -8.5606689 -5.0634117 -3.0302525 0.60399151 4.3020892 5.1416621 4.8395991 -0.46190166 -5.7100754 -8.8562365 -12.071251 -12.214842 -11.214817][-10.865021 -10.736832 -10.352827 -7.4837127 -5.1212883 -3.3093324 -0.97674513 0.041750431 0.28070068 -2.7589178 -6.9133534 -10.683113 -14.106079 -15.720726 -15.167486][-15.224133 -14.451193 -11.728115 -9.6658955 -8.1083469 -5.7814918 -3.2346609 -3.6706555 -4.5046558 -6.5484595 -8.9850464 -12.488294 -15.323004 -15.79174 -14.9764][-20.28019 -19.104267 -15.321844 -11.536816 -9.021512 -7.2568922 -6.449542 -6.7937098 -6.8931165 -9.0089579 -11.624837 -13.402986 -14.338547 -14.561251 -13.770231][-18.888895 -17.554482 -15.422773 -12.478043 -10.152456 -8.0337925 -6.37512 -7.0305123 -8.4190464 -9.3194857 -10.577383 -12.424181 -12.825298 -12.285064 -11.007386][-16.907909 -15.587008 -13.245679 -10.783499 -9.25193 -8.5417824 -8.7256947 -8.1332865 -7.7135105 -8.5272179 -10.063774 -10.387245 -10.487892 -9.9964342 -9.0261784][-11.597857 -11.132755 -10.371277 -8.5477085 -7.1825924 -5.8177371 -5.5596952 -6.6743011 -7.90611 -7.4543877 -7.5352283 -9.0497055 -9.9788237 -9.8846359 -9.65766]]...]
INFO - root - 2017-12-15 22:08:53.698558: step 64210, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 49h:36m:19s remains)
INFO - root - 2017-12-15 22:09:00.286910: step 64220, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 48h:28m:53s remains)
INFO - root - 2017-12-15 22:09:06.939005: step 64230, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 50h:50m:51s remains)
INFO - root - 2017-12-15 22:09:13.590546: step 64240, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 49h:57m:45s remains)
INFO - root - 2017-12-15 22:09:20.260094: step 64250, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 49h:30m:55s remains)
INFO - root - 2017-12-15 22:09:26.870831: step 64260, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 49h:16m:05s remains)
INFO - root - 2017-12-15 22:09:33.607978: step 64270, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 48h:43m:28s remains)
INFO - root - 2017-12-15 22:09:40.204582: step 64280, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 48h:37m:02s remains)
INFO - root - 2017-12-15 22:09:46.827915: step 64290, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 48h:23m:43s remains)
INFO - root - 2017-12-15 22:09:53.418180: step 64300, loss = 0.16, batch loss = 0.11 (11.4 examples/sec; 0.702 sec/batch; 52h:17m:41s remains)
2017-12-15 22:09:54.013694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6961212 -5.1513348 -4.0316486 -1.9729075 -1.2883244 -1.3948483 -2.2645593 -2.705982 -2.5386541 -3.0914621 -3.3740077 -5.0988932 -5.955924 -7.5203485 -7.3079758][-4.6363077 -3.7159779 -2.109333 -0.37018251 -0.10919189 -0.80951452 -2.3085232 -3.2832992 -3.4767427 -4.2633181 -3.8123128 -5.5882974 -7.198338 -7.6881218 -7.0457826][-4.0448422 -3.4892747 -1.7670829 -0.093362808 -0.30920362 -1.1194034 -1.8716085 -3.1360123 -3.3268597 -3.3201542 -3.3155391 -5.8590188 -7.6145372 -8.0863 -7.4923005][-4.5810833 -4.1897449 -2.9390326 -0.8370204 -0.049275875 -0.855391 -2.1203427 -2.620621 -2.357404 -2.7101669 -2.7038186 -5.2361426 -7.544065 -8.8008986 -8.164753][-5.1171231 -4.9738131 -4.2053037 -2.2722566 -0.9510994 -0.82410812 -1.3240399 -1.6669707 -1.2285762 -1.0356622 -1.6667938 -4.8410139 -7.4837933 -8.757391 -8.5798664][-5.678175 -5.62418 -4.0034904 -1.9020271 -0.9517417 0.29025936 0.61978769 0.29526806 0.89065933 0.51239586 -1.2441626 -3.9490316 -6.2998486 -8.7631941 -8.4762306][-6.5430274 -5.7331066 -4.2654314 -2.3282537 -1.4602318 0.49059391 1.3214855 2.3709855 3.1482329 2.2252316 0.68894386 -3.3880053 -5.4922314 -7.2624879 -7.7605247][-7.5961161 -6.3959842 -4.033792 -1.5892239 0.058768749 1.3061786 1.601316 2.2366242 3.3743768 2.4728494 0.88897324 -3.0275335 -6.3010931 -7.9182186 -6.6936684][-7.5657845 -6.8872042 -4.449399 -2.1422489 -0.064072609 1.0384879 1.0880799 1.7097373 2.3843741 2.1 1.672534 -2.1774921 -5.8680983 -7.5277767 -6.5502877][-7.4318175 -6.9592357 -4.7637668 -1.9574983 -0.69180346 -0.61647844 -0.44407701 0.53793192 1.2899103 2.0730076 1.911221 -2.1110547 -4.5787745 -6.5400691 -6.9095488][-9.8130722 -8.6690111 -6.2546439 -2.8373084 -2.2448156 -2.8766184 -2.1929581 -0.90700245 -0.14152193 0.21560001 0.18974447 -3.3926039 -5.583046 -6.7083549 -6.9518261][-11.614779 -10.73401 -8.2772827 -5.6004615 -4.0838985 -3.8532524 -4.3569069 -3.7000897 -1.9980042 -1.7137866 -1.7142797 -4.2425938 -5.9872961 -6.2817588 -6.7063603][-10.507544 -9.6989841 -6.8997173 -4.5877738 -3.5657361 -3.0742872 -3.810632 -4.4280558 -3.6719398 -3.1775186 -2.9360805 -4.1982608 -5.4560871 -5.4756556 -5.7719512][-8.4698772 -6.9978361 -6.072505 -4.6264939 -4.203846 -4.0080085 -3.3960419 -3.3251681 -4.0934544 -3.9481049 -4.1049428 -4.9729085 -5.4159737 -6.709178 -6.6960764][-5.5522485 -5.2535858 -4.7009354 -4.0577965 -3.959229 -4.4949579 -4.2535706 -3.9424834 -3.9768007 -4.3010488 -5.0931368 -6.4140911 -8.2522507 -9.1425829 -9.2882023]]...]
INFO - root - 2017-12-15 22:10:00.527413: step 64310, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 49h:42m:27s remains)
INFO - root - 2017-12-15 22:10:07.063036: step 64320, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 48h:08m:23s remains)
INFO - root - 2017-12-15 22:10:13.694305: step 64330, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 49h:20m:41s remains)
INFO - root - 2017-12-15 22:10:20.313025: step 64340, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.687 sec/batch; 51h:10m:47s remains)
INFO - root - 2017-12-15 22:10:26.925336: step 64350, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 49h:46m:29s remains)
INFO - root - 2017-12-15 22:10:33.500877: step 64360, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 47h:45m:54s remains)
INFO - root - 2017-12-15 22:10:40.083210: step 64370, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 49h:21m:53s remains)
INFO - root - 2017-12-15 22:10:46.648243: step 64380, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 48h:45m:16s remains)
INFO - root - 2017-12-15 22:10:53.166864: step 64390, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 48h:47m:20s remains)
INFO - root - 2017-12-15 22:10:59.738023: step 64400, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 50h:59m:41s remains)
2017-12-15 22:11:00.280620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2592053 -7.6983709 -8.4350643 -8.445816 -8.5976162 -8.4538517 -7.9939461 -8.1350269 -7.6503572 -7.4187574 -7.521256 -9.8269234 -11.363567 -11.771389 -11.007438][-8.436 -8.76885 -8.5570478 -7.9022942 -8.3147192 -8.5377121 -7.9044614 -6.7348437 -5.9771624 -5.6409879 -6.1068864 -9.12965 -11.61063 -12.615581 -11.48201][-8.085433 -8.29636 -8.1571417 -7.5790544 -7.3850479 -6.8795328 -6.9651341 -6.4729834 -5.582943 -4.9831042 -4.9569855 -7.6439242 -10.026309 -10.812377 -10.833934][-4.5837331 -5.5849767 -5.8443871 -5.8111157 -5.5227842 -4.2464213 -3.4212537 -3.3863943 -3.843163 -4.2129145 -4.5776615 -7.513555 -9.6309166 -10.239905 -10.00687][-3.3547068 -3.8846757 -3.6899652 -2.6735399 -1.8758495 -0.80341864 -0.34325361 -0.388134 -0.9440279 -2.3435793 -4.3456383 -7.0307789 -9.0281525 -10.037875 -9.2682343][-4.9004593 -3.34736 -1.7524137 -0.87436628 0.1199131 1.6870532 2.2097621 1.9861479 1.3723617 -0.014027119 -1.941195 -5.9001389 -9.0336981 -9.3084726 -8.8014584][-6.3100491 -6.0675392 -4.3127341 -1.3733025 1.258678 3.3085685 4.4772582 4.0650983 2.9772649 1.8338451 -0.19035625 -4.4953184 -7.5498171 -8.6699371 -8.7744045][-4.6606321 -3.6875448 -2.354528 -0.6490922 1.1042738 2.6479564 3.582109 4.4483724 4.4288983 3.0781417 1.086441 -3.0947869 -6.179307 -7.9684134 -9.223875][-2.9317453 -2.3535178 -0.78085566 0.51564932 1.3015575 2.5071206 2.5803685 2.8172593 3.0700212 2.4377065 0.78646231 -3.4155765 -5.9403014 -7.8829713 -8.08761][-2.2092459 -1.7600765 -1.4130359 -0.39896154 1.2048454 1.8267674 1.2040629 1.4642186 1.3975906 0.43177748 0.060026646 -3.4963892 -6.1579919 -7.7098608 -7.6950717][-5.1216764 -4.5850463 -3.74399 -3.539135 -2.9193504 -1.5768065 -0.78672743 -0.75869894 -1.9231453 -2.7885334 -4.2329969 -7.2588282 -8.3040562 -8.8772964 -7.266911][-8.46473 -8.0242186 -7.531991 -6.5830593 -5.2155404 -4.7936535 -4.9126582 -4.5993919 -5.4096403 -5.9230056 -6.511508 -9.0929241 -9.5624084 -9.3690958 -8.50367][-8.7598448 -7.5741324 -6.4788871 -6.754704 -6.3897953 -6.0257063 -5.695066 -5.8529673 -6.7481308 -7.9936795 -9.330266 -10.846699 -10.812666 -10.144961 -8.1247368][-7.1556726 -7.0766788 -6.7242284 -5.7636571 -4.9124494 -5.389132 -5.9037023 -5.5968285 -6.5348845 -7.6424184 -8.0085592 -8.9961853 -9.6935081 -9.56111 -9.3219252][-5.8228517 -5.9298978 -5.7746878 -5.2605195 -5.0539765 -4.7811852 -4.4748316 -5.3483992 -6.299809 -6.3120832 -7.0043192 -8.3762646 -8.94979 -9.3804159 -9.62181]]...]
INFO - root - 2017-12-15 22:11:06.805666: step 64410, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 47h:57m:42s remains)
INFO - root - 2017-12-15 22:11:13.392230: step 64420, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 48h:06m:52s remains)
INFO - root - 2017-12-15 22:11:20.002384: step 64430, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 49h:38m:02s remains)
INFO - root - 2017-12-15 22:11:26.640254: step 64440, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 49h:11m:59s remains)
INFO - root - 2017-12-15 22:11:33.133625: step 64450, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 49h:14m:43s remains)
INFO - root - 2017-12-15 22:11:39.680779: step 64460, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 48h:22m:48s remains)
INFO - root - 2017-12-15 22:11:46.210589: step 64470, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 48h:49m:16s remains)
INFO - root - 2017-12-15 22:11:52.920875: step 64480, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 47h:27m:19s remains)
INFO - root - 2017-12-15 22:11:59.555275: step 64490, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.683 sec/batch; 50h:52m:52s remains)
INFO - root - 2017-12-15 22:12:06.213968: step 64500, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.691 sec/batch; 51h:25m:57s remains)
2017-12-15 22:12:06.763380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3905139 -5.8615007 -5.9928041 -5.811233 -5.793581 -5.8318343 -4.9287171 -5.6588731 -6.7925248 -8.5571928 -8.7592754 -8.1951084 -9.4552956 -9.4205751 -8.430089][-5.2507243 -5.9439483 -6.6382861 -6.0024185 -5.7182937 -5.9212151 -6.0884595 -5.9588518 -6.8592806 -7.255353 -7.6506195 -6.9497223 -8.1959362 -8.4006252 -7.937408][-4.019691 -5.3986974 -6.5051794 -6.1702042 -6.4398408 -6.244556 -6.0502872 -6.4187965 -7.312768 -8.0796223 -7.5112858 -6.4359393 -7.7666521 -7.16391 -6.556252][-4.8943319 -5.9902654 -5.8580346 -6.6568451 -7.3380222 -5.6073203 -5.4178085 -6.2702627 -6.7488108 -7.304575 -7.0955687 -5.6986723 -7.556128 -8.2190857 -7.908534][-5.0544133 -7.0693121 -7.1278262 -6.3389468 -6.6891489 -4.05323 -2.7513254 -3.2527306 -5.3873572 -6.0396743 -6.6254783 -5.6927452 -6.7370377 -7.8187265 -8.1378117][-6.7663097 -6.8098187 -7.5831909 -5.8872557 -4.2907991 -3.1147189 -0.97383595 -0.29212427 -1.6759906 -4.4890523 -6.1475935 -5.6062951 -6.8371768 -7.8815393 -8.1651325][-6.9861369 -6.0931268 -5.8667397 -4.2178335 -1.0988851 1.5690413 3.1459479 3.6492667 2.4554181 -0.19099188 -3.2439468 -4.6884961 -6.5118728 -7.3411345 -8.3321877][-7.5456066 -6.622149 -5.1065326 -3.8272736 -1.7957385 2.5706697 5.2736268 5.1712584 3.4591966 2.3358474 -0.19550276 -2.6311922 -5.49042 -6.6387391 -7.6520872][-7.1764388 -6.9054952 -5.9328728 -3.2228272 -2.2953093 0.28729677 3.4529881 3.6777205 1.1689067 -0.35483932 -0.702785 -1.4826598 -5.1131988 -7.4502487 -7.0823402][-8.1585236 -7.5529909 -6.5914927 -4.85183 -2.5156682 -0.5325489 0.46077108 0.381392 -0.87635994 -2.7490916 -3.4053934 -2.964222 -5.1316128 -6.3252687 -6.4763403][-12.649839 -11.131161 -10.13309 -8.6238279 -6.3257036 -4.61038 -4.1154404 -5.2569809 -5.7428637 -6.186698 -5.5554762 -5.6575131 -7.5954385 -7.0057197 -6.3051214][-13.815926 -12.058254 -10.811527 -8.5325336 -6.2804608 -4.3121471 -4.3430085 -5.9452777 -7.3990641 -6.6219587 -5.6676025 -5.3727212 -7.5293055 -7.5250783 -5.3879595][-13.042099 -11.82897 -8.9048452 -6.8669558 -5.3688717 -3.5611584 -3.4932137 -5.7669725 -7.2122078 -7.180512 -5.6598673 -4.1930361 -5.1011047 -5.8546324 -5.5499496][-10.821071 -9.263092 -6.6807365 -4.239439 -2.962033 -2.7145224 -3.8972826 -4.881639 -5.5040402 -5.869729 -4.3708153 -3.7695153 -3.9072981 -3.3597167 -3.158016][-9.6731758 -9.4726191 -6.6613193 -3.3221071 -2.7023299 -4.3046122 -5.3905215 -6.162075 -6.3305531 -5.7250714 -4.2862415 -4.0916853 -4.8463712 -3.8605864 -3.6119864]]...]
INFO - root - 2017-12-15 22:12:13.313595: step 64510, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 48h:17m:18s remains)
INFO - root - 2017-12-15 22:12:19.871839: step 64520, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 49h:39m:38s remains)
INFO - root - 2017-12-15 22:12:26.482080: step 64530, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 48h:02m:05s remains)
INFO - root - 2017-12-15 22:12:33.100211: step 64540, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 48h:39m:21s remains)
INFO - root - 2017-12-15 22:12:39.739840: step 64550, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 51h:21m:11s remains)
INFO - root - 2017-12-15 22:12:46.289139: step 64560, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.648 sec/batch; 48h:11m:49s remains)
INFO - root - 2017-12-15 22:12:52.848599: step 64570, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 48h:18m:09s remains)
INFO - root - 2017-12-15 22:12:59.437073: step 64580, loss = 0.16, batch loss = 0.11 (11.5 examples/sec; 0.693 sec/batch; 51h:34m:14s remains)
INFO - root - 2017-12-15 22:13:06.051330: step 64590, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 48h:45m:10s remains)
INFO - root - 2017-12-15 22:13:12.709696: step 64600, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 49h:15m:40s remains)
2017-12-15 22:13:13.244881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2614098 -5.155076 -5.0609193 -5.0796132 -5.1183333 -4.923316 -4.6446304 -5.0389719 -5.5968351 -5.4129124 -3.9814854 -3.6981881 -5.189827 -6.999465 -6.6466608][-4.2298241 -4.4757438 -4.7846651 -4.2801075 -4.7542281 -4.5209064 -4.0362659 -3.592241 -4.0741887 -4.2026339 -3.2062492 -2.6093123 -2.7633004 -5.1338663 -6.0097265][-3.8087897 -4.3892512 -4.9561529 -4.8505416 -5.080966 -4.9149604 -4.5078144 -4.089643 -3.9536395 -3.9354198 -3.1189158 -3.2805405 -3.8139799 -4.7833428 -5.1929078][-4.8127804 -5.9838419 -7.1291347 -6.6805329 -6.4976287 -4.3978477 -2.702142 -2.0336361 -2.0266654 -3.0507045 -3.107311 -3.2368941 -4.6266546 -6.1674867 -5.9168983][-6.1142616 -7.7488022 -8.5087395 -7.0853615 -5.7867393 -2.6166179 -0.41823196 -0.22484827 -1.4139791 -2.5652049 -2.702033 -3.9328475 -5.2995429 -7.0219307 -7.1579132][-7.6604633 -7.8690138 -8.1506176 -6.720314 -4.1970396 0.78601122 4.3401828 3.75629 1.1594868 -1.866642 -3.6792564 -4.4894295 -5.7421913 -7.8114471 -7.686862][-9.628027 -8.958271 -6.9890561 -3.976505 -0.73331213 3.6076989 7.0584531 7.501164 5.4112153 0.48551989 -3.0242629 -4.4104118 -5.7780209 -7.4723082 -7.9838238][-10.776445 -9.9113636 -8.5748425 -4.9678507 -0.27379274 4.9726796 8.5644436 8.2257328 6.3394084 1.9066324 -2.5054431 -5.2950521 -7.473299 -8.6429634 -7.9455919][-11.723579 -11.338022 -10.169725 -6.2541132 -3.247829 2.0434995 6.6895423 6.523088 4.1848407 1.0225291 -1.4880915 -5.2811923 -8.7375774 -10.625872 -10.337132][-11.832455 -12.027617 -11.066948 -8.1804867 -6.5597534 -2.798954 1.2667289 2.6241517 2.065722 -1.4121599 -4.8049655 -7.5062208 -10.358488 -11.766642 -10.733478][-15.279192 -14.846926 -14.267593 -12.014124 -10.038837 -8.0833654 -6.1792817 -4.3410759 -4.1253614 -5.5705237 -7.7506876 -11.120537 -13.935631 -14.175117 -12.209048][-15.073133 -15.853481 -15.134737 -12.393805 -11.515104 -9.7869644 -8.4368248 -8.1721516 -7.8530631 -8.6418266 -9.6280251 -11.669186 -13.644213 -14.459234 -12.602738][-15.343349 -14.43116 -13.613602 -12.15708 -12.060166 -10.713002 -9.60854 -9.286849 -9.0020885 -8.7581692 -8.6944752 -10.158298 -11.09376 -11.28557 -10.620512][-13.075808 -13.072077 -12.468918 -11.05745 -10.291101 -9.0983181 -8.8531923 -8.0407505 -7.8294754 -7.9855762 -7.6561294 -7.5588927 -7.9097929 -8.370821 -7.9223018][-9.5075321 -10.147654 -9.7332315 -8.7245369 -7.8336139 -7.2567434 -7.5734539 -6.7279954 -5.9749331 -5.77489 -5.9880476 -6.3906732 -7.3586354 -8.385087 -8.8350735]]...]
INFO - root - 2017-12-15 22:13:19.923876: step 64610, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 50h:09m:41s remains)
INFO - root - 2017-12-15 22:13:26.582736: step 64620, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 48h:07m:19s remains)
INFO - root - 2017-12-15 22:13:33.179455: step 64630, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 48h:58m:16s remains)
INFO - root - 2017-12-15 22:13:39.778336: step 64640, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 48h:36m:21s remains)
INFO - root - 2017-12-15 22:13:46.365112: step 64650, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 48h:42m:06s remains)
INFO - root - 2017-12-15 22:13:52.884573: step 64660, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 49h:04m:09s remains)
INFO - root - 2017-12-15 22:13:59.517000: step 64670, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 50h:28m:56s remains)
INFO - root - 2017-12-15 22:14:06.095426: step 64680, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.673 sec/batch; 50h:02m:12s remains)
INFO - root - 2017-12-15 22:14:12.734125: step 64690, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 50h:29m:26s remains)
INFO - root - 2017-12-15 22:14:19.357880: step 64700, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 49h:29m:16s remains)
2017-12-15 22:14:19.953782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.344202 -3.2254691 -2.8860102 -3.5037687 -4.2729235 -4.7607317 -5.1298666 -4.9066176 -4.0812306 -3.9044256 -4.5365586 -5.28356 -6.607729 -7.506669 -5.10678][-6.2499719 -6.0377784 -5.71794 -5.0608029 -5.0592256 -6.191041 -6.4408193 -5.4482522 -5.0531392 -4.3194609 -3.275152 -5.4362111 -6.6901603 -6.4207754 -5.496191][-5.9735079 -5.6472158 -6.3403249 -7.154017 -7.26268 -6.5847945 -5.9435415 -6.2867317 -5.522944 -4.3720984 -4.4955359 -5.6890993 -6.3926859 -7.0318093 -4.8977618][-4.9636865 -4.7110343 -4.6758351 -4.9527149 -6.1142173 -6.3202329 -5.8046055 -5.6405711 -5.3164725 -4.7219415 -4.0359907 -5.661293 -6.2816215 -5.8026757 -4.0207567][-7.9486122 -6.7449727 -5.5708346 -4.4662004 -3.8804626 -2.899596 -2.3782556 -3.2418885 -2.906719 -3.0764871 -3.4757473 -3.9604185 -4.7136259 -5.1964226 -2.859282][-7.0336204 -6.1827664 -4.8326621 -2.741353 -0.875854 1.4388084 2.9356618 2.337688 0.7856226 -1.4769735 -2.5353281 -3.5273359 -3.5294428 -3.0986931 -1.3555183][-8.1662846 -7.2927427 -5.5545831 -2.7212758 -0.31221771 3.1665463 5.4860215 6.0426154 5.7930322 2.2116117 -0.47334719 -1.9954698 -2.8092332 -2.5789044 -0.64245558][-8.6131334 -7.560935 -5.4614415 -2.2138236 0.77984858 3.2706647 4.4728265 5.2119489 5.0488009 3.0372043 1.4140558 -1.7802908 -4.235496 -4.0216284 -1.8843682][-6.5138531 -5.9480529 -4.6106486 -2.5703919 -0.54972744 2.311379 3.268908 3.6744437 3.7453933 2.4888406 1.991384 -1.3981218 -4.7647595 -6.1077695 -4.770359][-6.4658508 -5.0019083 -4.2890334 -1.8012102 -0.22753572 1.1173763 1.8019686 1.9402118 0.65009785 -0.7085228 -1.1011095 -3.6661024 -5.296154 -7.1081829 -6.9780931][-6.3795304 -6.1177659 -5.0602422 -4.2440505 -3.2390211 -2.3040695 -2.1351554 -1.8020065 -2.1111085 -2.8418479 -3.9775555 -6.2238054 -7.7834635 -8.9758577 -7.925674][-9.3296013 -8.865448 -7.9602838 -6.9703069 -7.1385078 -7.3823519 -7.4592409 -7.6116409 -8.07213 -7.5691414 -7.4402423 -8.5478048 -8.8694553 -10.001766 -9.2013941][-11.745853 -10.642047 -9.3535795 -9.8139858 -10.654045 -10.293731 -10.65008 -10.896894 -10.384127 -10.155202 -10.091919 -10.008037 -9.869957 -9.78792 -8.4422951][-10.843115 -10.27342 -9.7499523 -9.5316315 -9.5101442 -10.222965 -10.579537 -10.355356 -10.528568 -9.9797211 -9.1628418 -8.6540966 -8.2026 -8.3745575 -7.6625261][-8.4482746 -7.6803179 -7.3368378 -6.3735189 -5.8525071 -6.4120173 -6.8312759 -7.1148148 -6.9413815 -7.012969 -7.2423134 -7.6708369 -7.9130154 -7.1342454 -6.4090252]]...]
INFO - root - 2017-12-15 22:14:26.570121: step 64710, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 50h:01m:36s remains)
INFO - root - 2017-12-15 22:14:33.296509: step 64720, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 49h:50m:05s remains)
INFO - root - 2017-12-15 22:14:39.922734: step 64730, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 50h:38m:35s remains)
INFO - root - 2017-12-15 22:14:46.448862: step 64740, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 48h:09m:38s remains)
INFO - root - 2017-12-15 22:14:53.025844: step 64750, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.675 sec/batch; 50h:12m:43s remains)
INFO - root - 2017-12-15 22:14:59.611428: step 64760, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 47h:07m:36s remains)
INFO - root - 2017-12-15 22:15:06.160850: step 64770, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 48h:45m:21s remains)
INFO - root - 2017-12-15 22:15:12.746317: step 64780, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 47h:31m:54s remains)
INFO - root - 2017-12-15 22:15:19.317478: step 64790, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 48h:24m:26s remains)
INFO - root - 2017-12-15 22:15:26.025580: step 64800, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 48h:59m:58s remains)
2017-12-15 22:15:26.620780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8465905 -5.8126631 -4.4047422 -3.5497689 -3.8097696 -3.9261026 -3.7830496 -3.8078392 -3.50245 -2.8085697 -2.4133344 -4.4869947 -5.506464 -6.8866582 -8.135149][-5.331573 -5.9217544 -4.8341732 -4.0606766 -4.6904197 -4.9394078 -4.5139313 -4.4484959 -3.985194 -2.8466287 -2.0338929 -3.1446242 -3.7359495 -5.0969033 -5.9611535][-3.1231453 -3.348666 -3.6360562 -3.8390427 -4.109169 -4.5938106 -4.4294481 -4.1819887 -3.8904428 -3.3238778 -2.9569554 -3.8723464 -4.6004333 -5.4246712 -5.8549838][-1.382688 -2.0373733 -2.5108161 -2.2508664 -2.704016 -2.9212151 -2.9027686 -3.3440988 -3.6829047 -2.7071881 -2.1760564 -4.3764191 -5.3509064 -5.895844 -6.7178941][-0.89030027 -1.7619481 -1.9240246 -1.0631723 -1.6222653 -1.4599185 -0.97594404 -1.2108707 -1.4023776 -1.8886995 -2.5894325 -4.2778826 -5.250412 -6.5340738 -6.9973907][-2.7450075 -2.2445085 -1.31078 -0.94539404 -0.69676733 0.10308743 0.61895466 1.1164942 0.8313098 -0.07013464 -1.0390992 -3.6289866 -4.77244 -5.498209 -5.5729675][-5.4226465 -4.0507545 -2.1470373 -0.58917665 0.72303867 2.046288 2.8861337 3.160996 2.738523 1.6958365 0.66069269 -2.0109773 -3.6194468 -4.7035379 -5.3541217][-5.680758 -4.8639607 -3.3422086 -1.244061 0.98461866 2.5520444 3.3133674 3.7342029 3.1489406 2.1797829 1.9334888 -0.67818546 -2.5948396 -4.1535454 -4.8565269][-5.3600006 -5.5459971 -4.9389067 -2.0490499 -0.32227612 1.4135051 2.4163299 3.1881328 3.4004769 2.2825732 1.4398909 -1.2728014 -3.0087473 -4.3553748 -5.6454415][-4.9080858 -4.7554617 -3.9545622 -2.3565788 -1.1078544 0.52853346 1.2470284 1.5057836 1.203342 1.0137701 0.86951542 -2.4259713 -4.6703634 -6.2188067 -6.9588513][-6.82465 -6.4314604 -5.0464492 -2.814945 -1.9801328 -1.1502018 -0.77965689 -1.2021956 -2.2250452 -2.8347826 -3.129415 -6.0473084 -7.6264596 -7.9443092 -8.1410255][-9.6319332 -7.8781071 -5.7267408 -3.7847013 -2.400933 -3.0685523 -4.0335379 -4.6701365 -5.734899 -6.4956551 -6.8516755 -8.1199551 -8.6534061 -8.6556559 -8.24013][-10.939296 -9.1069336 -6.6403871 -5.2781258 -5.3217154 -4.8757806 -4.9486685 -5.9963593 -6.870121 -7.6184154 -8.4020424 -8.9294043 -8.4280214 -6.6948743 -5.2294197][-11.197325 -9.5663986 -7.8288684 -5.518724 -4.4044127 -5.381536 -5.6518641 -5.3130188 -5.3146105 -6.0723472 -6.2654943 -5.879456 -5.958725 -5.4328489 -4.7902031][-7.8431273 -8.0744524 -7.5400352 -6.0670271 -5.0675559 -4.3012104 -4.2415776 -4.9979548 -5.0887523 -4.7644892 -4.295249 -5.0974054 -5.3427043 -5.4806957 -6.0091581]]...]
INFO - root - 2017-12-15 22:15:33.268141: step 64810, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 48h:28m:23s remains)
INFO - root - 2017-12-15 22:15:39.903380: step 64820, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 48h:57m:29s remains)
INFO - root - 2017-12-15 22:15:46.480248: step 64830, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 48h:16m:51s remains)
INFO - root - 2017-12-15 22:15:53.034155: step 64840, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 47h:37m:48s remains)
INFO - root - 2017-12-15 22:15:59.620025: step 64850, loss = 0.27, batch loss = 0.23 (12.3 examples/sec; 0.651 sec/batch; 48h:24m:04s remains)
INFO - root - 2017-12-15 22:16:06.217775: step 64860, loss = 0.20, batch loss = 0.15 (11.8 examples/sec; 0.678 sec/batch; 50h:24m:45s remains)
INFO - root - 2017-12-15 22:16:12.736158: step 64870, loss = 0.22, batch loss = 0.17 (12.6 examples/sec; 0.636 sec/batch; 47h:16m:06s remains)
INFO - root - 2017-12-15 22:16:19.279018: step 64880, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 47h:42m:46s remains)
INFO - root - 2017-12-15 22:16:25.823061: step 64890, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 48h:12m:41s remains)
INFO - root - 2017-12-15 22:16:32.351880: step 64900, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 48h:27m:16s remains)
2017-12-15 22:16:32.896969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0370505 -4.4785728 -4.9173083 -3.75849 -2.9282386 -3.1816776 -1.9398773 0.20210791 1.4981694 -0.36450911 -2.9175339 -7.0872059 -10.043423 -15.005613 -15.908314][-3.919466 -4.0306506 -4.2274761 -4.3184352 -3.8512979 -3.1287205 -1.0916281 0.041797161 0.96795845 0.086560726 -1.815362 -6.5349317 -10.352121 -15.750156 -17.721338][-2.6216474 -3.6930084 -3.7820308 -3.1996629 -3.4444256 -3.7959583 -1.9193065 0.39787102 0.11261082 -1.5347967 -2.4658153 -7.1863303 -10.740619 -15.114559 -16.509985][-2.3133464 -2.6617982 -3.3917086 -2.3757331 -1.4457302 -0.91804886 -1.0276089 -0.21553373 0.65202093 -0.66342068 -3.0085466 -7.2156277 -9.9676723 -15.287035 -16.003601][-0.88172579 -0.76550579 -0.79530287 -1.3734207 -1.5245833 0.078551769 0.63060188 0.60383606 0.42882347 0.42562914 -2.2653639 -8.1036 -10.769295 -15.170931 -16.312578][-1.4899068 -2.8794007 -1.7943172 -1.1288972 -0.25883913 1.1509352 2.6650157 3.4166379 2.680162 1.0127831 -1.6662569 -6.7925706 -11.310526 -15.046083 -15.624912][-2.9819331 -4.1751833 -4.2385969 -2.6776593 -0.41338253 2.2283316 3.4823394 4.1795373 4.9388881 2.3379455 -1.1305838 -5.6557221 -8.6622114 -13.498987 -14.853277][-4.9278121 -4.9673719 -4.3202786 -2.9077578 -0.89631987 2.1471543 4.5699954 4.7533984 4.8994784 3.6370053 0.87198591 -4.5933619 -8.0974655 -12.546543 -13.854326][-4.5728011 -6.2463369 -5.23664 -3.1280286 -1.7568181 0.231668 3.2281117 4.43903 3.6704726 2.0859065 -0.067154408 -4.7480578 -8.4238672 -12.6667 -13.622744][-4.336144 -5.3452749 -5.9214849 -4.606389 -3.2080212 -1.4961085 0.31634808 1.6230597 1.5621538 -1.002944 -2.4852414 -5.8358755 -8.6312122 -12.490793 -13.348854][-7.2849565 -8.3292856 -8.2420454 -6.785 -6.5650406 -4.8150897 -3.4202952 -3.4501891 -3.4197044 -4.6528273 -6.3830819 -10.98111 -11.533893 -14.277702 -13.313288][-9.2242012 -9.7216167 -8.9816084 -7.3609343 -7.4042196 -7.5198393 -6.8500004 -6.9087119 -7.1047354 -7.6397715 -9.1817226 -11.395818 -12.379963 -13.884567 -13.600878][-11.176437 -11.104365 -9.5732212 -8.4945612 -8.5892544 -8.9636021 -9.3432655 -9.1407137 -8.8424711 -9.4303017 -10.349463 -11.822157 -12.841188 -12.828213 -12.454705][-9.6073914 -9.7365818 -9.3592272 -7.7715111 -7.3560209 -8.7604141 -9.5069275 -9.33678 -9.3916073 -9.9200811 -9.4523859 -9.2570934 -8.6357994 -10.354486 -10.353067][-9.6499872 -9.4248142 -9.7114077 -9.4265766 -8.8217115 -8.9916468 -9.9602814 -10.897427 -10.212371 -9.1583548 -9.3945036 -10.383097 -10.508992 -11.363479 -11.511405]]...]
INFO - root - 2017-12-15 22:16:39.414264: step 64910, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 48h:51m:51s remains)
INFO - root - 2017-12-15 22:16:46.037891: step 64920, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 49h:45m:24s remains)
INFO - root - 2017-12-15 22:16:52.634622: step 64930, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 47h:44m:46s remains)
INFO - root - 2017-12-15 22:16:59.211161: step 64940, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 50h:23m:52s remains)
INFO - root - 2017-12-15 22:17:05.780829: step 64950, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 48h:10m:59s remains)
INFO - root - 2017-12-15 22:17:12.380847: step 64960, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 48h:42m:59s remains)
INFO - root - 2017-12-15 22:17:18.928214: step 64970, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 49h:18m:19s remains)
INFO - root - 2017-12-15 22:17:25.572266: step 64980, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 49h:10m:38s remains)
INFO - root - 2017-12-15 22:17:32.278892: step 64990, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 49h:00m:17s remains)
INFO - root - 2017-12-15 22:17:38.818948: step 65000, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.661 sec/batch; 49h:06m:57s remains)
2017-12-15 22:17:39.327786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3067684 -4.002748 -3.2281551 -3.137085 -3.6295309 -3.4839377 -3.2622488 -3.3221698 -3.6133144 -3.8946309 -3.7223687 -5.6878376 -5.7669015 -6.5345993 -6.8456945][-3.3815742 -2.8916872 -2.9286449 -3.0242128 -3.3895853 -3.2805364 -3.0391428 -2.7705276 -2.8002889 -3.3577211 -3.596698 -6.0193477 -5.7045069 -5.6159778 -6.007916][-2.3651586 -2.2788036 -2.4886971 -2.6330998 -2.5946896 -2.3242311 -2.3581228 -2.8908336 -2.4323254 -2.1444368 -2.745369 -4.7903996 -5.1574707 -5.444169 -5.4785352][-2.1674192 -2.1141057 -1.8897014 -1.4939928 -1.6828089 -1.45362 -1.290041 -1.3615742 -1.9156163 -2.5649564 -3.0400147 -5.5591393 -5.9575095 -5.6292772 -5.2407522][-3.3808599 -2.6178684 -2.3084433 -1.5319118 -0.96078491 -0.2197814 -0.15196943 -0.54135275 -1.4526663 -1.8472049 -1.8936102 -4.1260896 -4.6814752 -5.0217838 -5.2509837][-5.3661757 -3.8916068 -2.4113767 -1.2013097 -0.51149178 0.14482737 0.75998259 1.0674582 0.36656523 -0.76025534 -1.8286238 -4.3811555 -4.4447823 -4.2359262 -3.9827266][-5.6540704 -4.7773809 -2.957099 -0.8433466 0.637743 1.6341419 2.5708356 2.6781592 1.9060707 0.22795296 -0.38922787 -3.1753509 -4.2271442 -3.789649 -3.1538978][-5.450294 -4.4610224 -2.5387404 -0.30393696 0.5192461 1.8317323 3.3361363 3.3778653 2.9661574 1.3755469 -0.59368038 -4.2052445 -4.5136933 -4.5556755 -3.8521986][-5.3389111 -4.4294734 -2.5403473 0.40785503 1.3532619 2.2133746 2.8378558 2.8649879 2.8221087 1.252471 -0.40320063 -4.5457745 -5.5220923 -5.3573132 -4.4303932][-5.7814221 -4.2425718 -1.9226291 0.57671165 1.3105674 1.7692413 2.0706024 1.925128 1.9742122 0.68048763 -0.95923424 -4.8918128 -6.3970513 -7.2220607 -6.2703018][-6.9270549 -5.8053446 -4.2809653 -1.3476295 0.037766933 -0.14511442 -0.95436668 -1.873872 -2.2443523 -2.6666687 -3.644263 -7.4906015 -8.7647247 -8.4858456 -7.2131867][-9.4388962 -8.6969728 -6.856535 -4.3063555 -3.7239103 -3.6887872 -4.3532119 -4.8874454 -5.46093 -6.2201748 -6.9427853 -8.3354836 -8.4609385 -8.3294649 -7.6639132][-9.4382086 -8.848978 -7.4849682 -5.7427874 -5.8112721 -5.6721926 -5.9264016 -6.6763797 -7.3185058 -7.6774588 -7.3563986 -8.4192762 -8.5750694 -7.6845646 -6.3890233][-10.489571 -9.0452938 -7.9863396 -6.1068921 -4.7831559 -5.3956165 -6.2843924 -6.496191 -5.6378174 -5.6938739 -5.9306035 -5.7661743 -5.1344538 -5.4920735 -5.8414359][-6.5494556 -6.4659381 -6.0048475 -5.0949411 -4.434937 -4.2376966 -4.470829 -4.4256034 -4.0837445 -3.553309 -2.9378254 -4.8087487 -6.0402737 -6.3285828 -5.9022708]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-65000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-65000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 22:17:47.067957: step 65010, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 47h:28m:50s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 22:17:53.726833: step 65020, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.681 sec/batch; 50h:37m:43s remains)
INFO - root - 2017-12-15 22:18:00.297469: step 65030, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 49h:10m:25s remains)
INFO - root - 2017-12-15 22:18:06.908872: step 65040, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 50h:09m:13s remains)
INFO - root - 2017-12-15 22:18:13.539237: step 65050, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 48h:37m:38s remains)
INFO - root - 2017-12-15 22:18:20.034942: step 65060, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 47h:29m:35s remains)
INFO - root - 2017-12-15 22:18:26.671746: step 65070, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 49h:44m:19s remains)
INFO - root - 2017-12-15 22:18:33.300536: step 65080, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 48h:03m:45s remains)
INFO - root - 2017-12-15 22:18:39.898718: step 65090, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 49h:23m:54s remains)
INFO - root - 2017-12-15 22:18:46.450037: step 65100, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 47h:34m:57s remains)
2017-12-15 22:18:46.982196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.62543 -7.7499485 -6.8039093 -6.5992002 -5.8468461 -4.9109731 -4.285533 -3.1564231 -2.7996833 -2.8932343 -2.833951 -5.1412745 -6.7669668 -5.1880584 -4.1848717][-5.7334442 -6.0183473 -5.9946814 -5.8151708 -5.607163 -4.8823967 -4.1635251 -3.6521106 -3.8664474 -4.18637 -3.8535519 -5.642128 -7.374752 -5.9652972 -5.8334703][-3.9698653 -5.2689323 -5.152791 -4.7344155 -5.0074358 -4.3663111 -4.0303841 -3.7020321 -3.8355107 -3.9513326 -4.364994 -6.7636361 -8.92903 -7.911664 -7.7054434][-3.6683896 -4.0786824 -4.4032841 -4.20677 -5.038332 -4.973331 -4.742877 -5.3343086 -5.6093879 -5.446311 -5.4255261 -8.0810165 -10.821432 -9.8220177 -10.026954][-3.9935455 -5.3801131 -5.5855141 -4.4764233 -3.8528895 -2.6503055 -2.6972458 -4.4854589 -6.3872786 -6.131444 -5.9971957 -8.577035 -11.683247 -11.564583 -12.183036][-5.8236909 -5.5978765 -4.6284132 -2.7910697 -1.3427444 -0.0056567192 0.9088583 -0.33842421 -1.8766818 -3.8803785 -5.9723983 -7.6098704 -9.9679279 -11.040743 -12.235977][-7.089026 -6.5807166 -5.1025448 -0.85849667 1.7134414 2.2727771 2.9354663 3.3226371 3.3481898 -0.054308414 -3.8187456 -6.8535595 -10.452366 -10.129339 -10.437062][-7.1720042 -5.714848 -4.4884872 -0.84253645 2.5844436 4.3418021 5.6760116 5.846911 4.9165864 2.1684728 -0.29124689 -5.038413 -9.3356247 -8.98451 -9.756918][-5.269875 -4.6171875 -3.5819888 -1.1065536 0.965147 3.9459271 6.3699422 5.5200686 4.0915103 3.0002627 1.5952406 -3.2605886 -7.4271803 -7.5625572 -7.4734921][-3.618639 -3.5856893 -3.0818954 -1.3798552 0.22738743 1.9910865 3.9717107 4.4568238 4.0705333 3.3390164 2.1499491 -1.9060295 -5.324132 -5.3860416 -6.4579258][-6.7485318 -6.3373327 -5.4874721 -3.7768598 -2.9898932 -2.8694484 -1.8676155 -0.773468 0.0067629814 0.011148453 -0.42688084 -4.2488823 -6.21772 -5.4861345 -4.52033][-11.275597 -10.787073 -9.0593824 -7.2138515 -6.4978743 -7.2167625 -6.7953658 -6.2657657 -5.8194685 -5.5099335 -5.1819534 -6.4993439 -6.8926263 -6.2316947 -4.8980737][-12.84454 -11.347448 -9.5875177 -8.4310484 -8.3188686 -7.8404846 -7.6023221 -8.5846548 -9.5615969 -8.3310575 -7.0025263 -7.2306719 -6.8800144 -5.803031 -4.72909][-10.607396 -10.075161 -8.5970726 -6.9140196 -6.0903087 -6.2940226 -7.5927925 -8.0761395 -8.6205359 -8.5475769 -8.5642958 -7.4807038 -6.1003747 -5.1122131 -5.1048717][-7.595046 -6.9732428 -5.8927693 -4.5543122 -3.6123822 -4.12936 -5.2817197 -6.2661448 -6.9013343 -7.3691812 -6.8343182 -7.0373211 -7.5644159 -6.709414 -6.1388917]]...]
INFO - root - 2017-12-15 22:18:53.575456: step 65110, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 49h:40m:23s remains)
INFO - root - 2017-12-15 22:19:00.172301: step 65120, loss = 0.20, batch loss = 0.16 (11.7 examples/sec; 0.682 sec/batch; 50h:37m:44s remains)
INFO - root - 2017-12-15 22:19:06.699679: step 65130, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 48h:01m:41s remains)
INFO - root - 2017-12-15 22:19:13.279804: step 65140, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 48h:41m:11s remains)
INFO - root - 2017-12-15 22:19:19.917869: step 65150, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 50h:07m:30s remains)
INFO - root - 2017-12-15 22:19:26.539763: step 65160, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 48h:35m:04s remains)
INFO - root - 2017-12-15 22:19:33.084526: step 65170, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 47h:53m:56s remains)
INFO - root - 2017-12-15 22:19:39.708694: step 65180, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 47h:19m:23s remains)
INFO - root - 2017-12-15 22:19:46.311796: step 65190, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.669 sec/batch; 49h:42m:34s remains)
INFO - root - 2017-12-15 22:19:52.880918: step 65200, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 49h:35m:37s remains)
2017-12-15 22:19:53.413237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.202713 -9.89191 -9.071682 -7.8727226 -8.2854233 -8.0588865 -7.4852071 -6.7156496 -6.0526643 -5.3974795 -4.6549234 -6.5585346 -6.8949738 -5.4333935 -5.0097003][-8.1322441 -8.11977 -7.8143539 -6.9074726 -6.8007951 -7.0346518 -6.7574716 -6.298697 -5.8247981 -5.2189269 -4.8811293 -6.8211346 -6.860981 -7.15705 -6.4946074][-6.2628813 -7.3617816 -7.10107 -5.9260907 -6.2511859 -6.0386019 -5.8297291 -5.5013585 -5.3733268 -5.2499747 -5.3149095 -7.8420396 -8.293108 -7.3949404 -7.1065726][-7.7411108 -8.101284 -7.1628232 -5.0229478 -4.3573613 -4.031888 -4.2005529 -4.3426876 -4.4979525 -5.0257444 -5.3043051 -7.4986448 -8.0892181 -7.8985891 -6.8954453][-8.7928705 -9.5531206 -8.1358986 -4.62345 -2.0229053 -0.0037584305 0.24255753 -0.99635458 -2.6387179 -3.5966425 -4.0021043 -6.0510135 -6.3500032 -6.2993994 -5.7465243][-10.746204 -10.378838 -8.1729908 -3.5511284 -0.41405869 2.6117167 4.14716 2.6826015 0.76632643 -0.81320381 -1.588769 -3.3401067 -3.6822913 -3.7466068 -3.9961424][-12.216208 -10.307383 -6.8696275 -2.5529258 0.49503994 4.27972 6.2193303 5.9843335 4.6360173 1.812356 -0.151752 -2.5456009 -3.3641429 -3.4646561 -3.5511763][-12.488993 -10.063178 -6.2184997 -0.61323977 3.2163024 5.5743155 6.1500039 5.2861466 4.8952556 3.2469935 1.4785981 -2.8138175 -4.8267131 -4.8152428 -4.6864843][-10.495012 -9.2122974 -6.1931577 -1.2070141 2.2496762 4.6645427 5.051723 3.1466641 1.8187079 1.2201662 0.15063334 -3.9455571 -6.1886048 -6.5072007 -6.7615438][-9.3044682 -8.0453119 -6.0971956 -2.7320514 -0.036857128 2.8712153 3.9322963 1.5158162 -0.6074996 -2.0393052 -2.7561314 -5.6535344 -7.5927973 -8.3880463 -8.8775892][-12.653604 -11.260141 -9.3873577 -7.0866523 -5.4984593 -3.2753568 -2.5370138 -3.2423005 -3.7169049 -4.7854719 -5.387578 -8.3174639 -8.86367 -8.33969 -8.2643356][-15.24242 -12.955738 -11.037537 -9.1704693 -8.9550238 -7.9666433 -7.5225105 -6.8932295 -6.1958103 -6.5402 -6.8935633 -8.8519745 -8.6944895 -8.4184542 -7.8683767][-15.02416 -13.895567 -11.810242 -10.009231 -9.0838661 -8.2844753 -8.4491844 -8.0745258 -8.0373812 -7.8025613 -7.6949039 -8.4622107 -7.4717879 -6.13011 -4.6074939][-12.123396 -11.066147 -9.3104286 -6.8775444 -5.6938663 -5.568841 -5.6823797 -5.6894207 -5.685811 -5.5967278 -5.8376074 -5.1378617 -4.4407334 -4.6666245 -4.0602827][-7.7314649 -6.3381691 -4.6687713 -2.9146266 -2.1878147 -1.5451493 -1.4192467 -1.7360573 -2.5082259 -2.8830616 -3.5622578 -3.8829703 -3.5260665 -3.570183 -3.8317809]]...]
INFO - root - 2017-12-15 22:20:00.044316: step 65210, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.685 sec/batch; 50h:51m:38s remains)
INFO - root - 2017-12-15 22:20:06.695573: step 65220, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 49h:13m:11s remains)
INFO - root - 2017-12-15 22:20:13.306894: step 65230, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 50h:15m:47s remains)
INFO - root - 2017-12-15 22:20:19.867587: step 65240, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 48h:55m:34s remains)
INFO - root - 2017-12-15 22:20:26.439118: step 65250, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 47h:27m:47s remains)
INFO - root - 2017-12-15 22:20:33.092304: step 65260, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 49h:06m:15s remains)
INFO - root - 2017-12-15 22:20:39.627717: step 65270, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 47h:08m:28s remains)
INFO - root - 2017-12-15 22:20:46.244214: step 65280, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 47h:52m:17s remains)
INFO - root - 2017-12-15 22:20:52.836362: step 65290, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 48h:03m:22s remains)
INFO - root - 2017-12-15 22:20:59.445189: step 65300, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 47h:50m:29s remains)
2017-12-15 22:20:59.976955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.0851994 -7.0626273 -6.0065274 -5.7059674 -6.5275145 -7.1145334 -7.4436169 -8.6692638 -8.7524948 -8.2438679 -6.6560082 -5.4156914 -5.3881903 -6.0730762 -5.0784054][-7.5326624 -5.2606373 -4.5783358 -3.9112105 -5.381567 -5.701478 -5.52679 -6.0555129 -6.7225785 -6.049655 -4.1152573 -2.9749599 -3.4837205 -5.9156241 -5.81411][-5.2952628 -4.3945632 -4.665041 -3.7942562 -4.5096893 -4.4384975 -4.5829558 -4.7600646 -4.5852475 -3.8477979 -2.0275042 -1.3024664 -1.8125451 -3.9951308 -3.8428664][-7.0829353 -6.0649977 -6.7362356 -6.3265619 -6.5644932 -4.8984823 -3.3492191 -4.0986233 -4.3071728 -2.927402 -1.7852767 -1.6134715 -2.9416242 -4.6807418 -4.3583937][-9.2085247 -8.967926 -8.9403782 -9.0816422 -8.5896635 -4.589222 -1.7818675 -2.8755803 -3.9097419 -2.9308505 -2.5896893 -2.5618985 -3.5099583 -5.1903739 -5.3133125][-10.4175 -10.30664 -9.5282974 -7.3779573 -5.5142717 -0.78345966 3.2837176 2.9387479 1.6435227 -1.2175026 -3.7216625 -2.5278442 -3.3215215 -5.5260835 -5.0329714][-12.347282 -10.817152 -9.5267 -6.991786 -3.5704067 1.2161322 6.1591268 6.967977 5.830791 0.2875247 -4.7275128 -4.6507974 -5.6598959 -6.5439177 -5.0109463][-11.956514 -11.948751 -10.295969 -5.6622281 -2.1165564 2.2054086 7.2007489 7.265223 6.2223868 1.9248481 -3.1881161 -4.8052621 -6.8865628 -7.36343 -5.5664535][-12.552206 -12.037083 -10.953457 -6.8813305 -3.2447302 0.65334225 3.9842057 4.1649566 3.1470542 -0.46999073 -4.416141 -6.4438214 -8.70378 -9.4730816 -7.7483149][-12.33367 -12.513039 -10.798174 -8.0013866 -6.1675258 -1.8657436 1.079483 1.4483733 0.72602177 -3.0595405 -6.5273709 -7.53773 -9.3666782 -11.025595 -10.337801][-15.04847 -14.601234 -13.010563 -10.405399 -9.0066366 -6.7439251 -4.1978312 -3.6426923 -4.3530087 -6.2216344 -8.4922886 -9.6345053 -9.7308054 -10.525183 -9.4290218][-17.824537 -15.90103 -12.890514 -10.605625 -9.6656647 -8.3017 -8.44487 -7.9762716 -7.4825063 -8.8770638 -10.653508 -11.215351 -11.270391 -11.173506 -10.289228][-14.004734 -12.411355 -9.8244858 -8.0166035 -8.1779919 -7.6591935 -7.6728344 -8.2337494 -8.75376 -9.5829725 -10.445163 -10.705607 -10.399778 -9.647069 -8.8680229][-11.095168 -9.05636 -7.8195333 -6.7962933 -6.251873 -7.25313 -7.643579 -7.2100687 -7.8348293 -8.90192 -9.6595268 -9.0068188 -8.68622 -7.7709832 -7.04542][-7.9520025 -7.1974111 -5.389575 -4.6871367 -4.3152256 -4.6806765 -4.9797583 -5.6887784 -6.010509 -5.9248838 -6.7436934 -6.7432895 -6.6666512 -7.3471003 -8.1956882]]...]
INFO - root - 2017-12-15 22:21:06.634633: step 65310, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 49h:35m:42s remains)
INFO - root - 2017-12-15 22:21:13.276063: step 65320, loss = 0.18, batch loss = 0.14 (11.6 examples/sec; 0.687 sec/batch; 50h:59m:06s remains)
INFO - root - 2017-12-15 22:21:19.874344: step 65330, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 49h:37m:14s remains)
INFO - root - 2017-12-15 22:21:26.541346: step 65340, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 48h:23m:59s remains)
INFO - root - 2017-12-15 22:21:33.170207: step 65350, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 49h:08m:56s remains)
INFO - root - 2017-12-15 22:21:39.825085: step 65360, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 49h:40m:12s remains)
INFO - root - 2017-12-15 22:21:46.464463: step 65370, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 50h:08m:08s remains)
INFO - root - 2017-12-15 22:21:53.047843: step 65380, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 48h:42m:10s remains)
INFO - root - 2017-12-15 22:21:59.601630: step 65390, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 49h:36m:11s remains)
INFO - root - 2017-12-15 22:22:06.136114: step 65400, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 48h:33m:25s remains)
2017-12-15 22:22:06.652139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.312674 -5.1256118 -2.8128352 -1.4178324 -1.5192261 -2.0729079 -2.6892142 -3.1802173 -2.4865408 -2.0076277 -0.97806168 -2.5387986 -4.8996844 -5.567184 -5.8444886][-3.9075112 -3.596796 -2.1440609 -0.22782612 0.34084845 -0.69582605 -2.5945833 -4.1251292 -4.2068057 -4.210371 -3.2271664 -4.480123 -5.8448219 -5.6811442 -5.799345][-2.9666865 -2.7167525 -2.0007966 0.12644911 -0.025326252 -1.2189345 -2.2874987 -3.6291556 -3.8609471 -4.7360325 -4.2840085 -6.1318936 -8.1824894 -7.2516437 -6.2144365][-2.7887108 -2.7559593 -1.9295154 -0.4638896 -0.048481941 -1.0635924 -2.1346726 -3.3944135 -3.7707024 -3.0899148 -2.8569946 -5.0967016 -7.4287567 -8.440999 -8.260582][-2.4939005 -2.9093349 -2.8637657 -1.5825963 -0.19640017 -0.2190218 -0.60406303 -1.8067181 -2.0828362 -2.2957139 -2.4738045 -5.1778092 -7.7799597 -8.2823887 -8.478919][-3.3042142 -3.4863436 -2.7645824 -0.99055529 -0.51503086 1.0895443 1.8146887 0.89565182 1.1765289 0.19772291 -1.0796132 -3.7294888 -5.9662685 -7.8773041 -8.7165089][-4.8414135 -4.0204577 -2.9645574 -1.9487915 -0.8571434 1.5095611 2.6576209 3.4902215 4.15465 2.3121591 0.63540936 -2.9490976 -5.2245884 -6.3914666 -7.7018266][-6.0244393 -5.4080372 -2.4148922 -0.58189917 0.27439976 1.9673023 2.6728644 3.4890609 4.0504622 3.3165793 1.3378692 -3.6725922 -6.6863003 -7.39659 -7.1668596][-5.9978666 -5.2134342 -3.5673838 -1.0853291 0.71565294 1.3138618 1.4191828 2.7504058 4.0384755 4.0964007 3.1420484 -2.0760226 -5.6910067 -7.1801682 -6.7966604][-6.7830825 -6.2270079 -4.4435539 -1.3878231 0.32013798 0.61627865 0.91643572 2.5173984 3.908112 3.74958 3.1580071 -1.3928461 -5.1534324 -6.3551993 -7.2072978][-11.142651 -10.211472 -7.8528862 -4.3403912 -3.0101254 -2.336163 -1.1968818 -0.43405533 0.45162535 0.63440418 0.5720582 -2.7564325 -6.1124725 -7.187284 -7.6557713][-14.05851 -13.495821 -10.344314 -7.0248318 -4.9231615 -3.4305155 -3.3356609 -3.1625485 -2.5178392 -1.978792 -1.800833 -3.8200541 -5.4374137 -5.9045992 -6.4870944][-13.067791 -11.986738 -9.3006258 -5.6957073 -5.0646052 -3.9739056 -3.7002285 -4.4561577 -4.2863922 -3.8936634 -3.7937522 -4.2397366 -5.3055148 -4.8522329 -5.6037159][-10.819872 -9.2238684 -8.0291023 -6.2713752 -6.0225163 -5.67098 -5.4859257 -4.7089906 -4.4278135 -4.8538351 -5.84343 -6.01076 -6.6993122 -6.6745625 -7.3472795][-6.849865 -7.0313449 -6.8940945 -6.3155131 -6.4131393 -5.8974452 -5.824409 -5.24381 -5.0670981 -5.0706544 -5.858254 -7.9176273 -10.931553 -10.795348 -10.71486]]...]
INFO - root - 2017-12-15 22:22:13.289010: step 65410, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.689 sec/batch; 51h:05m:58s remains)
INFO - root - 2017-12-15 22:22:19.938455: step 65420, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 47h:57m:22s remains)
INFO - root - 2017-12-15 22:22:26.568270: step 65430, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 48h:47m:06s remains)
INFO - root - 2017-12-15 22:22:33.115303: step 65440, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.633 sec/batch; 46h:59m:36s remains)
INFO - root - 2017-12-15 22:22:39.676593: step 65450, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 48h:56m:10s remains)
INFO - root - 2017-12-15 22:22:46.304069: step 65460, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 49h:04m:19s remains)
INFO - root - 2017-12-15 22:22:52.930788: step 65470, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 49h:32m:57s remains)
INFO - root - 2017-12-15 22:22:59.495512: step 65480, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 48h:33m:04s remains)
INFO - root - 2017-12-15 22:23:06.067834: step 65490, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 49h:29m:28s remains)
INFO - root - 2017-12-15 22:23:12.717229: step 65500, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 49h:24m:04s remains)
2017-12-15 22:23:13.262083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7857413 -5.987709 -6.5311933 -6.12634 -6.1823859 -6.2975893 -6.3473172 -6.3602109 -6.0817518 -5.453258 -4.4398866 -4.6211786 -6.4278097 -7.3289013 -7.358984][-3.7002306 -3.8187423 -4.1270757 -4.1033535 -4.9160805 -5.41817 -5.5084991 -5.46284 -5.2119503 -3.722569 -2.0781038 -2.6364908 -4.5559258 -6.2611465 -7.0143805][-0.18088198 -2.499732 -3.9834852 -3.5116298 -3.2561977 -3.5726106 -3.9232969 -4.0039892 -3.711792 -2.9553449 -2.0720661 -2.1984563 -4.1390829 -6.5953131 -7.3353977][-0.77381516 -3.2345667 -4.9543524 -4.1851339 -4.2694817 -3.8634982 -3.4999988 -4.0504069 -3.9204226 -3.2359552 -2.88076 -3.9379559 -6.7896538 -8.5664148 -9.2465363][-3.4232588 -5.531177 -6.5568309 -5.9597325 -5.02106 -2.1925373 -0.2327137 -1.7641351 -3.3829637 -3.223038 -3.2191951 -4.7035403 -7.504704 -9.741972 -10.179029][-6.5905242 -7.3356285 -5.9237919 -4.352664 -1.4382358 2.3922505 4.983871 3.9985318 2.001761 -0.403656 -2.7978704 -3.9408505 -6.409596 -9.1255636 -9.9813251][-10.225798 -9.0982275 -7.0613728 -3.5690024 -0.22040987 4.3208327 8.6246719 9.082655 7.7496505 3.0605178 -1.3576069 -3.6902373 -7.047863 -9.0996265 -9.5203276][-11.833006 -10.886808 -8.6602383 -4.2162256 -0.78512 3.3056579 7.1032929 7.9881225 7.3204103 3.4378581 -0.650527 -3.4609482 -8.0961142 -10.480743 -10.829057][-11.303438 -11.040209 -9.3863068 -5.6758089 -2.9553547 0.52097225 3.845902 4.459744 3.6851735 1.0064754 -1.9318225 -5.8833084 -10.460596 -11.642171 -11.209352][-10.158325 -10.669351 -10.21559 -7.1563244 -4.9525557 -2.3552625 0.12926722 0.9795208 0.47921133 -2.0270319 -4.3083334 -7.1139164 -10.56468 -12.22005 -12.484891][-14.248047 -14.160482 -13.299479 -10.867075 -9.2946615 -7.6274633 -5.6778474 -5.2318296 -5.0467024 -6.2490625 -7.7377372 -10.353924 -12.505704 -13.010036 -12.536377][-18.959915 -18.751078 -16.996174 -15.363867 -14.580387 -13.659281 -13.28558 -12.379044 -11.411179 -11.6364 -11.683603 -11.962415 -13.248342 -13.549526 -12.337303][-17.266254 -16.296673 -14.262562 -14.057709 -13.922033 -14.157825 -14.233776 -13.272652 -12.456306 -12.040455 -11.549479 -11.502504 -12.054155 -11.593784 -10.987455][-13.000231 -12.540209 -11.490921 -10.588018 -9.7363739 -11.024309 -12.210505 -11.962687 -11.304386 -10.644397 -9.7282391 -8.3486071 -8.8028345 -8.5412159 -8.2805157][-9.0237808 -9.4211483 -9.1553659 -7.7094665 -6.8934031 -7.2070708 -7.9706678 -8.4743519 -8.2851143 -7.89738 -6.61235 -6.1399693 -7.0523324 -7.37387 -8.0477161]]...]
INFO - root - 2017-12-15 22:23:19.881262: step 65510, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 47h:56m:14s remains)
INFO - root - 2017-12-15 22:23:26.463848: step 65520, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.646 sec/batch; 47h:56m:04s remains)
INFO - root - 2017-12-15 22:23:33.150332: step 65530, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 48h:52m:37s remains)
INFO - root - 2017-12-15 22:23:39.727816: step 65540, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 48h:20m:30s remains)
INFO - root - 2017-12-15 22:23:46.305148: step 65550, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 49h:52m:20s remains)
INFO - root - 2017-12-15 22:23:52.990028: step 65560, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 49h:25m:07s remains)
INFO - root - 2017-12-15 22:23:59.615482: step 65570, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 48h:39m:02s remains)
INFO - root - 2017-12-15 22:24:06.223456: step 65580, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 50h:32m:54s remains)
INFO - root - 2017-12-15 22:24:12.771937: step 65590, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 47h:06m:26s remains)
INFO - root - 2017-12-15 22:24:19.359240: step 65600, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 48h:47m:19s remains)
2017-12-15 22:24:19.866463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1333928 -7.1381989 -8.6321926 -9.3347645 -9.9665279 -9.9013538 -9.8583651 -9.809288 -10.080227 -11.18261 -10.626762 -9.6774769 -9.0490646 -12.190845 -10.991947][-7.3622704 -8.2995634 -8.38113 -9.05341 -9.8569088 -10.061228 -9.6784067 -9.7127151 -9.8868294 -10.257004 -10.659197 -11.102777 -10.214922 -13.941082 -14.205236][-5.0853171 -6.9937029 -9.3280163 -9.728075 -9.92033 -10.319211 -10.013876 -9.6869469 -9.5090618 -9.8065166 -10.423012 -10.768463 -10.905134 -14.638098 -14.296545][-7.176384 -8.09744 -9.74947 -10.107656 -9.2733679 -7.5616932 -7.6249876 -8.317482 -8.0911942 -8.5419807 -9.5155249 -10.482718 -10.894053 -14.552519 -14.517241][-8.2063789 -10.269167 -12.628858 -11.247355 -8.3463 -2.4995296 0.7359252 -3.8107171 -6.4772625 -5.9880276 -6.8246632 -8.0329685 -8.9566412 -12.89485 -12.692891][-7.946991 -9.8630066 -11.033823 -11.738946 -10.327094 -0.95601511 7.4497695 5.1187358 0.86600161 -3.2557509 -5.9384007 -5.2192354 -5.69183 -10.125967 -10.424926][-9.013835 -10.798946 -8.76816 -6.7621708 -7.232461 -1.6706104 6.8048406 9.8982048 10.286505 1.7664108 -5.3234563 -4.7216125 -5.2629533 -8.9463568 -9.1306686][-8.8363008 -11.332241 -11.25304 -5.5696087 -1.6148043 2.1735306 5.8491654 7.2310739 10.829794 5.2029366 -1.7493548 -3.4282792 -5.2751923 -8.6770763 -8.5960922][-6.2695227 -7.6938853 -10.655652 -9.1841955 -5.7948637 1.6616421 4.5401635 3.7557874 6.5385747 2.3445582 -1.7687688 -3.6161497 -4.1858673 -8.5899315 -10.84449][-4.7742248 -5.1365447 -8.8112373 -9.4715261 -9.2954578 -4.3318653 -0.37134933 1.8771996 3.5793939 -0.45844221 -3.6434753 -6.3335781 -5.8014631 -8.6936626 -10.47696][-10.186544 -10.590042 -11.978054 -12.056056 -12.920382 -11.024973 -9.21993 -6.7188935 -4.7314782 -5.4496865 -7.2115188 -8.7525539 -8.1561022 -11.674646 -11.881715][-13.188437 -14.287924 -14.924126 -14.452717 -14.424665 -13.057463 -12.859725 -12.357432 -10.960812 -10.322309 -10.542399 -10.67417 -8.3818216 -12.023605 -11.940356][-9.9396524 -10.988831 -11.275545 -12.528223 -13.301796 -12.193647 -11.563282 -12.079948 -12.003197 -10.188702 -8.5378456 -9.6466408 -7.9284883 -9.9783745 -8.8363228][-9.3079052 -8.3412828 -8.9626007 -8.5775528 -8.3950367 -8.8425932 -8.5847778 -9.1248512 -10.670897 -10.929249 -9.788887 -8.4229984 -5.9650712 -7.0801854 -7.437252][-6.4211235 -6.8836317 -6.4029393 -4.1228704 -3.1325219 -3.6161323 -3.9061985 -4.3059835 -6.0764675 -6.7518177 -8.1514988 -8.0143032 -7.4862671 -7.9706936 -8.4884443]]...]
INFO - root - 2017-12-15 22:24:26.450470: step 65610, loss = 0.25, batch loss = 0.20 (12.4 examples/sec; 0.646 sec/batch; 47h:54m:13s remains)
INFO - root - 2017-12-15 22:24:33.051340: step 65620, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 47h:34m:44s remains)
INFO - root - 2017-12-15 22:24:39.671433: step 65630, loss = 0.15, batch loss = 0.10 (11.4 examples/sec; 0.700 sec/batch; 51h:53m:46s remains)
INFO - root - 2017-12-15 22:24:46.274763: step 65640, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.688 sec/batch; 51h:01m:46s remains)
INFO - root - 2017-12-15 22:24:52.805781: step 65650, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.648 sec/batch; 48h:00m:43s remains)
INFO - root - 2017-12-15 22:24:59.416357: step 65660, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 50h:15m:40s remains)
INFO - root - 2017-12-15 22:25:05.961680: step 65670, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 48h:21m:35s remains)
INFO - root - 2017-12-15 22:25:12.582599: step 65680, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 49h:29m:00s remains)
INFO - root - 2017-12-15 22:25:19.124473: step 65690, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 48h:56m:37s remains)
INFO - root - 2017-12-15 22:25:25.683317: step 65700, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 47h:41m:05s remains)
2017-12-15 22:25:26.201776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.692553 -5.340713 -5.315856 -6.2176847 -7.4203119 -7.8855505 -8.301095 -8.6208267 -8.38209 -7.8218861 -6.7409482 -5.1052675 -6.7752781 -9.2024574 -7.2738609][-6.7667665 -7.1368127 -7.5118642 -7.848073 -8.7240114 -9.6489935 -9.9899635 -9.515749 -9.29932 -9.232934 -7.9326882 -6.0406623 -7.5580592 -9.5253019 -6.6558633][-5.9620118 -5.8482742 -6.6597805 -7.3469515 -9.457427 -10.826748 -10.872973 -9.10543 -8.2752457 -8.561657 -8.1169033 -6.6101437 -7.2498212 -9.2496 -6.7598176][-7.7608891 -7.3936062 -8.0915337 -7.9484062 -8.9904509 -9.0555754 -8.3798246 -7.8905554 -8.5597572 -8.6669254 -7.8026161 -6.0399437 -6.7909503 -8.4516306 -6.2153645][-8.472681 -8.1109276 -8.366107 -7.7809768 -7.3588691 -5.1111298 -2.9463406 -3.1235785 -6.0337677 -8.0144005 -8.32652 -6.1272154 -6.3324676 -8.0458412 -6.1997442][-8.47236 -8.8937426 -9.4848919 -8.4852448 -6.80878 -1.6199532 3.9286475 2.9505086 -0.37585306 -4.1222639 -6.6569262 -5.0568609 -5.9091415 -8.28622 -7.0285382][-9.5292511 -9.3583364 -8.6451588 -7.1165714 -4.4002891 0.93497324 6.2984233 7.9108844 6.2873654 -0.33798361 -5.3271232 -4.1876507 -5.1431437 -7.7751923 -6.8597174][-11.892084 -10.710953 -10.341162 -7.1826806 -2.4128621 3.3580441 7.5530353 9.4141922 9.5455933 2.360538 -4.1161704 -4.4588423 -6.4783039 -8.6775589 -6.7265177][-11.492617 -10.52354 -9.4217682 -7.064487 -3.605722 1.9992876 5.3323922 5.6071734 4.3364739 -0.070186138 -4.3032 -6.1142282 -8.7863932 -10.46502 -7.7960892][-11.172674 -10.338677 -9.21299 -6.7115803 -4.856503 -0.27675533 2.5198579 2.6206794 0.57538128 -4.8142195 -8.9208508 -9.45907 -11.277111 -13.025609 -10.688109][-12.605383 -11.910407 -10.70947 -8.46426 -6.9337649 -4.7475662 -3.5482991 -2.4522126 -3.48649 -7.0493875 -10.336378 -11.215513 -12.945267 -13.508209 -10.034391][-15.497692 -14.403751 -13.677366 -11.21514 -8.3371077 -7.3346891 -6.8961315 -6.6256337 -7.827302 -10.122642 -11.99194 -11.753495 -11.876026 -11.188251 -8.1595869][-14.777418 -13.889713 -11.880587 -10.00772 -7.9274049 -6.1942496 -5.3809443 -6.4667029 -7.9497662 -8.69616 -8.67693 -8.358736 -9.64507 -9.3113232 -5.7346225][-11.40176 -10.831607 -10.078301 -8.6524763 -6.7063513 -5.32356 -5.1068459 -4.9576087 -5.1892309 -6.2447443 -7.0051007 -6.3466554 -5.6459451 -5.7636046 -5.0686169][-5.6684203 -5.722899 -5.0320807 -4.5861959 -4.9843836 -4.6468768 -3.8923914 -3.5124621 -4.1391091 -4.0973439 -3.5483034 -3.8098378 -4.77635 -5.0649228 -4.8388934]]...]
INFO - root - 2017-12-15 22:25:32.762927: step 65710, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 47h:38m:14s remains)
INFO - root - 2017-12-15 22:25:39.352401: step 65720, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 47h:03m:19s remains)
INFO - root - 2017-12-15 22:25:45.971114: step 65730, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 49h:04m:08s remains)
INFO - root - 2017-12-15 22:25:52.513608: step 65740, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 48h:41m:21s remains)
INFO - root - 2017-12-15 22:25:59.184147: step 65750, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 48h:59m:48s remains)
INFO - root - 2017-12-15 22:26:05.770963: step 65760, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 50h:19m:21s remains)
INFO - root - 2017-12-15 22:26:12.346006: step 65770, loss = 0.18, batch loss = 0.13 (11.6 examples/sec; 0.692 sec/batch; 51h:17m:13s remains)
INFO - root - 2017-12-15 22:26:18.958705: step 65780, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.686 sec/batch; 50h:50m:23s remains)
INFO - root - 2017-12-15 22:26:25.556939: step 65790, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 48h:03m:29s remains)
INFO - root - 2017-12-15 22:26:32.090592: step 65800, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 48h:53m:19s remains)
2017-12-15 22:26:32.615496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5025163 -7.5719471 -6.7263312 -6.6105146 -7.2484903 -7.5423188 -8.033947 -7.3100505 -7.5495663 -7.8652549 -7.9122529 -11.462229 -15.154213 -14.675224 -15.351973][-7.2218237 -7.966527 -8.0413513 -8.61978 -8.0435381 -8.6908493 -9.788681 -10.260202 -10.962841 -10.741439 -11.239592 -14.290434 -16.434027 -15.892324 -15.947338][-5.6339126 -8.5725269 -10.578758 -9.8870926 -9.1116028 -9.6341887 -9.9198675 -9.9551582 -11.030037 -11.990672 -12.341626 -15.200558 -18.10944 -16.682997 -16.497732][-5.8659067 -7.9554768 -7.9004784 -7.3891897 -7.6592779 -7.7432013 -7.7632356 -8.0451546 -8.3922243 -8.9112282 -9.7604923 -13.612309 -16.639978 -15.637196 -16.018789][-7.1497717 -8.8332424 -8.1666355 -6.4754391 -4.92574 -2.7613947 -2.4438863 -4.4901905 -5.9367137 -5.2689295 -5.07954 -9.2458391 -13.103553 -12.739424 -14.33382][-7.5562954 -8.9885817 -6.9193935 -4.612092 -1.4059873 2.0626354 3.9152741 2.52842 0.78472996 -1.1491814 -2.6205604 -6.0300207 -9.2448816 -9.4999189 -11.53975][-9.76461 -8.4816427 -6.1472735 -2.7322514 1.2097144 4.6216731 7.19817 7.7306523 5.8536582 1.9271235 -0.33476543 -3.0228965 -6.7221317 -7.7859616 -10.180742][-9.8083858 -8.9429693 -6.3916392 -2.48407 2.662807 7.1746345 8.4386578 7.6650081 6.8399243 5.5259681 2.9196477 -2.8794112 -8.2303677 -9.3161535 -11.887147][-6.7824578 -7.5239267 -5.7615056 -1.9040449 1.2343245 3.9731622 5.9301019 5.7089744 4.9985557 4.5994897 3.1946874 -2.9489195 -9.9548035 -11.90139 -14.43552][-2.8548226 -4.674963 -4.8429823 -3.7389336 -1.567553 -0.87137938 0.60848141 1.8439026 2.17175 2.2372808 1.245471 -4.2828493 -10.281662 -14.313242 -17.748404][-5.6777806 -7.1828542 -7.8836718 -6.6331992 -5.8271713 -5.8143482 -5.4382787 -5.6350622 -5.2510467 -5.0225682 -5.6000328 -9.216114 -12.541317 -15.29451 -17.789024][-8.9975948 -10.454018 -11.280716 -11.819978 -11.222274 -10.590497 -10.873547 -11.37316 -10.159397 -8.9674082 -9.6866531 -12.300726 -13.692677 -14.372726 -16.436041][-14.459528 -13.816898 -14.466372 -12.980726 -13.265623 -14.22752 -14.29351 -14.620377 -14.149103 -13.392267 -12.191348 -13.365052 -13.915194 -12.163422 -11.946182][-13.570162 -13.210039 -12.824562 -11.120272 -11.573221 -11.708479 -12.730542 -12.820057 -12.507778 -12.367183 -11.757074 -11.425961 -9.6256285 -9.639266 -11.303843][-11.107182 -10.546737 -10.282246 -9.27081 -6.833725 -6.7665305 -8.0466862 -8.0430069 -9.1372271 -9.8526917 -9.572525 -10.167409 -10.009652 -9.6085043 -9.464263]]...]
INFO - root - 2017-12-15 22:26:39.262276: step 65810, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 48h:25m:49s remains)
INFO - root - 2017-12-15 22:26:45.868608: step 65820, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 48h:41m:47s remains)
INFO - root - 2017-12-15 22:26:52.428921: step 65830, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 49h:14m:28s remains)
INFO - root - 2017-12-15 22:26:59.025522: step 65840, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 48h:56m:00s remains)
INFO - root - 2017-12-15 22:27:05.630315: step 65850, loss = 0.22, batch loss = 0.17 (11.8 examples/sec; 0.676 sec/batch; 50h:02m:39s remains)
INFO - root - 2017-12-15 22:27:12.241105: step 65860, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 49h:49m:35s remains)
INFO - root - 2017-12-15 22:27:18.854322: step 65870, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 49h:24m:32s remains)
INFO - root - 2017-12-15 22:27:25.465881: step 65880, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 50h:05m:27s remains)
INFO - root - 2017-12-15 22:27:32.004848: step 65890, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 48h:08m:43s remains)
INFO - root - 2017-12-15 22:27:38.696261: step 65900, loss = 0.14, batch loss = 0.09 (11.5 examples/sec; 0.695 sec/batch; 51h:29m:48s remains)
2017-12-15 22:27:39.264852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5309167 -5.5620503 -4.8029361 -3.8308518 -4.10323 -4.4048939 -4.350029 -3.8487058 -3.0701463 -2.0622656 -1.5382428 -3.6332884 -6.1476502 -8.4219265 -9.4268351][-4.8161011 -5.3160682 -4.9805403 -4.5606623 -4.6434636 -4.7951822 -4.7075663 -4.4789686 -3.6231141 -2.1317477 -1.0822706 -2.4038551 -4.7357988 -7.2772427 -8.7328434][-2.9126823 -3.4683547 -3.6956708 -4.019361 -4.5076513 -4.62541 -4.6581626 -4.13547 -3.6661429 -3.0375643 -2.4578805 -3.6978924 -5.8169274 -7.7612534 -9.1656237][-0.60415459 -1.6571708 -2.087774 -2.3041618 -3.4499331 -3.3912659 -3.0913117 -3.347574 -3.4378588 -2.6661463 -2.4775016 -4.4432487 -6.3715258 -8.0195818 -8.8259563][-0.10890388 -0.89605188 -1.3816609 -1.5097508 -2.3042626 -2.2173259 -1.7027464 -1.2910428 -0.98936987 -1.3413434 -2.0772893 -4.5608582 -6.9298487 -8.67831 -9.1255875][-1.9475539 -1.5006175 -0.85884857 -0.91916895 -1.0971131 -0.26926231 0.6007514 1.2523756 1.2504005 0.80143356 -0.60004663 -3.4039023 -5.8113055 -7.0885911 -7.844162][-4.7697735 -3.9476862 -2.126883 -0.50075579 -0.073215485 1.4617381 2.6304841 3.4096494 3.5592885 2.7114348 1.28438 -1.6813846 -4.7159181 -6.2620525 -7.287549][-5.8458619 -5.2462811 -4.2303739 -1.976831 -0.22855043 1.5144291 3.2204394 4.4880328 4.7204528 3.6118493 2.2536669 -0.80237389 -3.9304042 -6.1129131 -6.8765521][-5.1685424 -6.4121881 -5.8401666 -3.8259406 -1.7986467 0.57140684 2.6264586 4.0235267 4.7115779 3.223907 1.8564487 -1.5529032 -5.1073742 -7.0573273 -7.8843555][-3.8295968 -4.7543154 -4.516923 -3.2091405 -2.1074984 -0.2884841 1.0020475 2.0109105 2.0814281 1.6197901 0.9080286 -2.4728684 -6.0344729 -8.4709816 -9.580596][-6.4787412 -6.3606672 -5.99657 -4.4180226 -3.8710847 -2.7437329 -2.4520223 -2.203407 -2.4683895 -3.0414696 -3.6992626 -6.6920662 -8.4493017 -9.6941032 -10.137856][-9.4173059 -7.3141727 -5.5888271 -4.3667264 -3.9035678 -4.1056948 -4.8347359 -5.1100612 -5.9813766 -6.8383451 -7.6350646 -9.0226517 -9.6832571 -10.140655 -10.161318][-11.725546 -9.632205 -7.1082664 -6.2281981 -6.3579803 -5.6628718 -5.5350533 -6.586843 -7.4520321 -8.8514061 -9.5971584 -9.6699858 -9.4397717 -8.5506277 -7.6487846][-11.936932 -11.323184 -9.0608015 -6.4353857 -5.2902546 -5.6393051 -5.93636 -6.0920324 -6.8157248 -7.4763188 -8.0003061 -8.2730637 -7.7701969 -7.1166658 -6.1325846][-9.0603886 -9.0872526 -8.68275 -7.3940105 -6.0133624 -5.6444888 -6.0728412 -6.416873 -6.3275194 -6.1120114 -6.4965529 -6.7738771 -6.9343009 -7.2668953 -6.8391662]]...]
INFO - root - 2017-12-15 22:27:45.780159: step 65910, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 48h:45m:00s remains)
INFO - root - 2017-12-15 22:27:52.337233: step 65920, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 48h:29m:47s remains)
INFO - root - 2017-12-15 22:27:58.983242: step 65930, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 48h:30m:38s remains)
INFO - root - 2017-12-15 22:28:05.615108: step 65940, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 48h:16m:53s remains)
INFO - root - 2017-12-15 22:28:12.164787: step 65950, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 48h:09m:39s remains)
INFO - root - 2017-12-15 22:28:18.694971: step 65960, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 47h:10m:14s remains)
INFO - root - 2017-12-15 22:28:25.302163: step 65970, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 48h:52m:34s remains)
INFO - root - 2017-12-15 22:28:31.980480: step 65980, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 48h:30m:26s remains)
INFO - root - 2017-12-15 22:28:38.566218: step 65990, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 48h:27m:04s remains)
INFO - root - 2017-12-15 22:28:45.157359: step 66000, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 49h:01m:36s remains)
2017-12-15 22:28:45.781281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0303569 -4.1865449 -4.2055426 -4.0677733 -4.54071 -4.758038 -4.8318024 -4.66451 -4.5048027 -4.4218993 -3.8552427 -6.7412691 -9.71895 -10.039329 -10.476402][-5.1940618 -5.3481388 -5.810432 -5.6694493 -6.0509415 -6.490406 -6.6325765 -6.7360525 -6.5341158 -5.8734555 -5.1632667 -7.5901289 -10.080904 -10.463934 -11.519997][-4.6136918 -5.9981146 -7.0086422 -6.6073451 -7.3280067 -7.7785549 -7.9218593 -7.5545025 -7.0854335 -6.6960955 -6.2310071 -8.7554436 -11.636978 -11.684332 -12.101778][-5.917376 -7.1394281 -7.4650664 -6.6696825 -7.2542944 -6.7905369 -6.7775326 -6.8258734 -6.63392 -6.2085714 -5.7248373 -8.4655743 -11.846346 -12.567606 -13.540417][-7.3489842 -8.9527292 -9.3107719 -7.2454276 -6.6053796 -5.2464042 -4.5829897 -4.6714067 -4.9604321 -4.3972487 -3.9649618 -7.29182 -10.915194 -12.13024 -13.696234][-8.7788363 -9.1425552 -8.207509 -5.6229014 -3.2868588 -0.98332977 0.27425718 -0.23800135 -0.78109407 -1.3776622 -2.0155694 -4.8665485 -8.6729078 -10.225123 -12.168926][-9.5311127 -9.28377 -7.5526886 -3.9248629 -0.30531216 1.9809661 3.3534436 3.4582686 3.1115985 1.0458493 -0.63152504 -4.0391712 -8.2326984 -9.28776 -10.952376][-8.8179569 -8.3603668 -7.1527653 -3.1118877 0.295897 3.7344956 6.1097341 5.21643 4.4044795 2.2390904 0.00809288 -4.2588568 -8.6346617 -9.55903 -10.865614][-6.4700155 -6.7243547 -5.7493324 -3.2234313 -1.5472255 1.6798501 3.9077296 4.5830207 4.7402081 2.5655236 0.12910271 -5.086318 -9.8643284 -10.424152 -11.133604][-4.7319293 -4.3260927 -4.2352905 -2.600414 -1.8943524 -0.87325478 0.14058304 1.5489202 1.6436687 0.49917698 -0.800488 -6.0402923 -11.124633 -12.118031 -13.379696][-5.9011173 -6.1131244 -5.3729067 -4.6313629 -5.189744 -5.0040402 -4.8628149 -4.5428033 -4.5683088 -4.1956344 -5.1199355 -10.035652 -13.265823 -13.624532 -14.291403][-10.067118 -8.7020321 -7.5210867 -7.11653 -7.8445849 -8.4524641 -9.40163 -9.8742113 -10.045691 -9.550395 -9.8756056 -11.655441 -12.958646 -13.255803 -13.321886][-13.057298 -12.204665 -10.221913 -9.6671963 -10.365381 -10.177478 -10.75355 -11.545154 -12.587183 -12.555876 -12.277045 -12.456414 -12.544933 -11.323181 -10.7554][-12.891389 -12.698744 -11.148918 -9.920352 -10.083975 -10.081937 -10.285399 -10.763262 -11.181547 -11.714624 -11.976674 -10.804535 -10.321878 -9.8719559 -9.7498169][-9.9026661 -10.109547 -9.5389261 -7.8834133 -7.1944056 -7.0420866 -7.8520794 -8.6423178 -8.9758224 -9.2117472 -9.3156509 -9.9865589 -10.941698 -10.397705 -10.461815]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 22:28:52.372429: step 66010, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 48h:35m:22s remains)
INFO - root - 2017-12-15 22:28:59.025097: step 66020, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 50h:19m:49s remains)
INFO - root - 2017-12-15 22:29:05.791414: step 66030, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 49h:41m:42s remains)
INFO - root - 2017-12-15 22:29:12.325688: step 66040, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.648 sec/batch; 47h:59m:04s remains)
INFO - root - 2017-12-15 22:29:18.910879: step 66050, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 47h:42m:38s remains)
INFO - root - 2017-12-15 22:29:25.571000: step 66060, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 49h:37m:10s remains)
INFO - root - 2017-12-15 22:29:32.114055: step 66070, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 47h:07m:05s remains)
INFO - root - 2017-12-15 22:29:38.666631: step 66080, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 48h:11m:25s remains)
INFO - root - 2017-12-15 22:29:45.294032: step 66090, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.631 sec/batch; 46h:41m:29s remains)
INFO - root - 2017-12-15 22:29:51.935797: step 66100, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 49h:17m:21s remains)
2017-12-15 22:29:52.496333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3032444 -1.9734697 -1.9487593 -1.8227372 -2.6467922 -3.4938312 -3.6025932 -3.2290666 -3.5103855 -3.3168681 -2.8014119 -4.284534 -4.9210749 -3.5960071 -3.865737][-3.6428998 -2.4977703 -1.3777771 -1.4073653 -2.0444376 -2.6514525 -3.0997262 -2.648984 -2.6440775 -2.9834023 -2.6409895 -3.7069139 -5.2092919 -3.8680353 -2.0452881][-2.0366788 -2.2412529 -2.0029094 -1.1686411 -2.3688991 -3.6439536 -3.1934552 -2.7928386 -2.6411383 -2.0797462 -1.6761165 -3.4450037 -4.6628475 -3.8533459 -4.698801][-1.9892929 -1.9110682 -1.8810964 -1.5388303 -2.2063553 -2.7498395 -2.663836 -2.0896316 -1.3596096 -1.3238091 -1.582612 -3.2898943 -4.5710897 -3.0492473 -2.637675][-2.8195469 -4.01704 -3.5102615 -2.166698 -2.1258323 -2.2527733 -2.3690391 -1.7294984 -1.1728163 -1.5989618 -1.6403904 -3.0111361 -4.6844177 -4.804523 -5.4457512][-3.529371 -3.7895873 -3.3999743 -1.8440769 -0.8056035 0.058064938 0.50663424 0.4639864 0.9691987 0.79013443 -0.25468779 -2.8652725 -4.7152023 -3.6537449 -2.6733775][-4.7928047 -4.2672229 -2.9919369 -1.9400418 -0.94548368 1.392344 2.9219375 2.7036891 2.4046121 1.2270093 0.5638361 -2.0187237 -4.9225626 -4.8996792 -3.5271411][-4.9295764 -4.7307138 -3.8585167 -2.1026406 -0.61848307 1.5783963 3.56107 3.9947095 3.3569837 1.7821388 0.830071 -2.291965 -5.2030916 -4.7252564 -3.477308][-5.08797 -4.5326123 -3.4849341 -1.9491065 -1.1291227 0.26073456 2.0613012 2.7087846 2.3262868 1.4004626 0.089509964 -3.6907732 -6.3045716 -5.4965138 -4.6140738][-5.6047626 -4.67513 -3.539325 -2.3830273 -1.5001159 0.29088688 0.96417904 0.51561928 0.42234421 -0.42589045 -1.5150242 -4.3728895 -6.8064413 -6.7884054 -6.2530775][-7.0941887 -6.9329433 -5.4862232 -4.0279436 -3.5921202 -2.2018464 -1.5234733 -2.1483073 -2.8625221 -3.0601733 -3.6838865 -6.3820047 -7.9446468 -7.9057641 -8.8974848][-10.51667 -9.8646593 -8.711545 -7.4821124 -6.977253 -5.9979572 -6.2620077 -6.3141084 -6.9351945 -7.9992819 -8.2692747 -8.5815439 -9.583662 -9.110095 -8.6275425][-11.514597 -11.749186 -10.773912 -9.6242676 -9.5692472 -9.0815086 -8.717205 -8.9040661 -9.5251522 -9.5670013 -9.5264492 -9.2624035 -9.4178276 -8.8061666 -8.8823957][-8.7753353 -8.557704 -7.8957949 -7.5999985 -7.8160124 -7.4556055 -7.1894751 -7.3946853 -7.7029524 -7.8553143 -8.0915337 -7.16014 -6.8271866 -6.0772705 -5.5394197][-6.7817278 -6.3804026 -5.6912208 -6.4090977 -6.1482534 -5.8445721 -6.1254082 -5.7323642 -5.3359208 -5.8554468 -6.3038096 -5.8903823 -6.0106568 -6.2150569 -6.8179388]]...]
INFO - root - 2017-12-15 22:29:59.124282: step 66110, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.629 sec/batch; 46h:31m:31s remains)
INFO - root - 2017-12-15 22:30:05.763158: step 66120, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 48h:28m:11s remains)
INFO - root - 2017-12-15 22:30:12.403700: step 66130, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.658 sec/batch; 48h:42m:27s remains)
INFO - root - 2017-12-15 22:30:19.008500: step 66140, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 50h:02m:06s remains)
INFO - root - 2017-12-15 22:30:25.642475: step 66150, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 50h:10m:06s remains)
INFO - root - 2017-12-15 22:30:32.267181: step 66160, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 48h:57m:38s remains)
INFO - root - 2017-12-15 22:30:38.799456: step 66170, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 48h:58m:23s remains)
INFO - root - 2017-12-15 22:30:45.409683: step 66180, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 49h:27m:34s remains)
INFO - root - 2017-12-15 22:30:52.007971: step 66190, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 49h:52m:28s remains)
INFO - root - 2017-12-15 22:30:58.683830: step 66200, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 48h:03m:06s remains)
2017-12-15 22:30:59.173190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9010251 -2.7109125 -1.4668193 -1.3280282 -1.507688 -1.848994 -2.7556198 -3.0749569 -2.3678265 -0.6619277 -0.87845182 -3.3749886 -5.8941913 -8.3416443 -9.7607288][-1.4364667 -0.87893581 -0.82476854 -0.94691324 -1.2770729 -2.1645052 -2.1789706 -3.3620384 -3.6968794 -3.5309227 -3.4592054 -5.03818 -7.0193453 -9.5526094 -10.133621][-2.3503649 -3.0024226 -2.454443 -2.4445746 -3.2792873 -3.5616009 -3.2382307 -3.2344437 -3.5270658 -4.1188936 -4.5123873 -7.227479 -9.4155331 -11.246357 -12.010637][-4.5456409 -4.4913807 -4.3306928 -4.2999048 -4.4987464 -4.3393421 -3.701879 -3.7480423 -4.3765593 -4.51178 -4.426178 -6.8604441 -8.3778372 -10.361854 -10.991791][-3.6783879 -4.3613353 -4.3729267 -5.1620007 -5.5258541 -4.610383 -3.6162508 -3.3059945 -2.5378959 -2.7970388 -4.0193172 -6.7672734 -8.140687 -9.7276649 -10.305895][-2.0380838 -3.0823071 -3.112134 -2.8812683 -2.8061187 -1.6259537 0.056404591 0.15174627 0.21984577 -1.0757537 -2.426759 -5.3323078 -6.2492619 -7.2840862 -8.6203346][-1.1261835 -1.3224406 -2.3984063 -1.9897137 -1.0527234 1.0331893 3.0669608 3.7247052 3.7310662 0.89116144 -1.724406 -4.7641973 -5.1255674 -6.2819824 -6.6443706][-1.8473761 -1.5962138 -1.5892243 -0.84990644 0.68028069 3.2221656 4.7516494 4.20849 3.592113 1.0101089 -1.8184106 -5.6453261 -6.295454 -6.7488322 -6.001749][-2.4989872 -2.5017083 -2.2137134 -1.8230374 -0.89971733 1.1448798 2.8289332 2.6687341 1.2676258 -0.52970982 -1.7088161 -5.6277318 -7.0299158 -6.8583121 -6.7966075][-3.2204173 -3.8056035 -3.8622277 -3.194767 -2.1249895 -1.1524868 -0.472013 -0.7690239 -1.6299868 -1.6300683 -1.9784627 -4.6715908 -5.2582936 -6.3315086 -6.5940208][-7.3071513 -7.1645641 -6.3974257 -5.5760484 -5.1381984 -5.2476311 -5.308949 -5.3692088 -5.2898016 -4.2015953 -3.6118417 -5.4515028 -5.38827 -6.224165 -6.0348716][-11.004627 -11.495 -11.317612 -10.289118 -9.5893908 -9.2143564 -9.943367 -9.2279015 -8.1706848 -8.213191 -7.0721164 -7.2153821 -6.7841172 -6.290544 -5.9947534][-12.54938 -12.394552 -12.460209 -12.527184 -12.095724 -11.411743 -11.511761 -11.403206 -10.758261 -10.044044 -9.2156162 -9.201169 -8.1058979 -6.4133797 -5.3843918][-9.947196 -10.908601 -11.667263 -11.495974 -10.674442 -9.9070873 -10.400366 -10.435152 -10.475521 -10.350107 -9.9530354 -9.2734947 -8.1685266 -6.8954678 -6.833221][-7.13801 -7.7927642 -7.8474674 -7.3075461 -7.4882832 -6.8989491 -6.3692718 -7.1640644 -8.2540112 -8.874342 -8.9508018 -9.5218277 -9.9686985 -9.8353729 -9.0350056]]...]
INFO - root - 2017-12-15 22:31:05.772033: step 66210, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 49h:55m:21s remains)
INFO - root - 2017-12-15 22:31:12.340148: step 66220, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 48h:35m:49s remains)
INFO - root - 2017-12-15 22:31:18.849310: step 66230, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 47h:34m:10s remains)
INFO - root - 2017-12-15 22:31:25.487783: step 66240, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 48h:59m:38s remains)
INFO - root - 2017-12-15 22:31:32.108674: step 66250, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 49h:20m:29s remains)
INFO - root - 2017-12-15 22:31:38.781946: step 66260, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 48h:31m:32s remains)
INFO - root - 2017-12-15 22:31:45.457431: step 66270, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 49h:03m:45s remains)
INFO - root - 2017-12-15 22:31:52.028998: step 66280, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 48h:08m:08s remains)
INFO - root - 2017-12-15 22:31:58.544598: step 66290, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.628 sec/batch; 46h:27m:23s remains)
INFO - root - 2017-12-15 22:32:05.165433: step 66300, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.687 sec/batch; 50h:45m:53s remains)
2017-12-15 22:32:05.705731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5930314 -4.936306 -3.8956232 -2.4265656 -1.2767601 -0.42125082 -0.19684124 0.8183589 0.42263126 0.02223587 -0.40125418 -2.917856 -5.0165892 -7.2058287 -7.6124063][-4.8625789 -3.0392358 0.23040581 1.0545578 1.0946879 1.4825468 2.1306114 1.7212777 0.71596241 -0.1592803 -1.2133813 -4.1778231 -7.43645 -10.409897 -10.983555][-1.5442185 -1.136353 -0.902174 1.0509815 2.295815 2.1742716 1.934813 1.6316891 0.65298367 -0.55386686 -2.5005846 -6.2412515 -9.62475 -12.908843 -14.06638][-1.7454104 -0.40861702 1.1438637 1.5495472 1.009129 1.0910625 1.0582538 0.21876192 -0.93853188 -2.3124204 -4.1855354 -7.4033303 -11.19488 -14.05806 -14.488928][0.20199156 0.31191444 0.56869936 1.3432202 1.6854224 1.9242978 1.1508894 -0.2377615 -1.6753612 -2.1497767 -3.0629961 -6.4909477 -11.046434 -14.565155 -15.231508][-2.991678 -1.9465396 -0.95973921 1.4040489 3.262054 4.0279536 4.1486363 3.1004434 1.7190709 -0.10826588 -1.8100061 -3.8866367 -6.7827349 -10.455662 -11.219101][-3.7036221 -2.9058247 -0.59152555 2.259799 3.447721 4.1276488 5.2099156 5.1051154 3.9675622 1.0399675 -0.98135042 -3.8668087 -6.3448668 -7.7217126 -8.1708746][-6.7247791 -4.8198237 -2.0870888 1.4315882 3.1393256 4.6809363 5.8398414 4.8218684 4.1730294 2.1976714 0.032113075 -3.450201 -6.1651926 -8.0402145 -7.2555313][-5.5465055 -4.4458694 -2.6162069 -0.21464205 2.2352195 4.4465623 4.5168939 3.0072818 2.7017455 1.0111341 -0.5281415 -4.2037644 -6.556365 -7.8952761 -7.7180691][-6.3164454 -5.408432 -3.6683743 -2.0515735 -0.58486032 0.97119427 2.2202983 2.6051202 1.8549571 -0.29149389 -1.2681684 -4.1185102 -7.3930559 -9.5660677 -8.8961973][-11.034449 -10.197992 -8.9319448 -6.7426243 -5.85974 -4.7868996 -3.4853261 -2.2839367 -1.9210711 -2.3245442 -2.4539471 -6.0199084 -8.1704235 -9.3412027 -8.66109][-12.993235 -13.46014 -12.400063 -10.929492 -10.700788 -9.55024 -8.5909548 -7.786171 -7.1010103 -6.594327 -6.1816211 -7.7150283 -8.2171946 -9.5501156 -8.5346146][-13.145432 -13.522591 -12.951468 -12.562599 -12.447956 -10.867116 -10.577971 -10.737932 -10.978977 -10.628143 -10.128756 -9.8247795 -10.04042 -9.8534088 -7.973999][-8.1548271 -8.794096 -9.0557537 -9.0801983 -9.215416 -9.6478252 -10.332143 -9.4446964 -9.2758541 -9.7046843 -10.089645 -9.2981958 -9.2371874 -9.5766869 -8.900218][-5.049283 -3.8647075 -2.4063997 -3.2097261 -4.3532062 -5.2048941 -5.8218875 -6.5948772 -7.6836209 -7.7105021 -7.7542076 -8.7527447 -10.12088 -9.7637415 -9.9941559]]...]
INFO - root - 2017-12-15 22:32:12.321793: step 66310, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 47h:47m:50s remains)
INFO - root - 2017-12-15 22:32:18.932135: step 66320, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.679 sec/batch; 50h:11m:10s remains)
INFO - root - 2017-12-15 22:32:25.450694: step 66330, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 47h:30m:37s remains)
INFO - root - 2017-12-15 22:32:32.016494: step 66340, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 48h:59m:33s remains)
INFO - root - 2017-12-15 22:32:38.683589: step 66350, loss = 0.35, batch loss = 0.30 (11.8 examples/sec; 0.675 sec/batch; 49h:56m:20s remains)
INFO - root - 2017-12-15 22:32:45.258756: step 66360, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 48h:27m:45s remains)
INFO - root - 2017-12-15 22:32:51.878505: step 66370, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.634 sec/batch; 46h:50m:33s remains)
INFO - root - 2017-12-15 22:32:58.483041: step 66380, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 48h:00m:11s remains)
INFO - root - 2017-12-15 22:33:05.076036: step 66390, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.648 sec/batch; 47h:52m:08s remains)
INFO - root - 2017-12-15 22:33:11.672666: step 66400, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 49h:52m:29s remains)
2017-12-15 22:33:12.231258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.7813797 -2.5394325 -5.1637621 -6.68259 -6.739953 -6.61933 -7.1914248 -8.52973 -8.3224392 -8.1864061 -8.384408 -10.885254 -12.958542 -14.299046 -13.846926][-1.9791524 -2.9869261 -5.1435962 -5.3702927 -5.4239774 -5.4199023 -4.7805142 -4.4610825 -5.2205944 -6.0443439 -6.85908 -9.435112 -11.020609 -13.619361 -13.90358][-1.7891152 -2.5963342 -4.8960848 -6.2041392 -6.6903553 -6.8278213 -6.1081309 -4.3155112 -3.8901005 -4.5694265 -5.403688 -7.8240647 -9.9852295 -11.96645 -12.117008][-5.0793104 -5.4514828 -6.70317 -7.69865 -7.705616 -5.7528992 -4.6447191 -3.27595 -1.9014995 -2.4905565 -3.7687328 -6.1670203 -8.3668976 -9.8450117 -10.203478][-6.7563043 -8.02718 -9.9205933 -10.270375 -8.5458412 -4.9262276 -2.2732518 -0.04713726 -0.75476217 -1.7614 -3.1406786 -6.0472851 -7.0657759 -7.4409266 -8.0345421][-8.2001266 -8.3155527 -7.6212745 -7.5444722 -6.2072234 -3.4182425 0.28571558 2.2722721 1.3100061 -1.0560546 -3.7224381 -5.0004916 -5.0751472 -6.2768555 -6.363956][-7.4063215 -7.9972229 -7.8415966 -5.7042575 -2.1554492 1.0531616 4.0809312 5.0028033 4.7174954 2.4140048 -1.4876842 -4.4380503 -5.4150009 -4.9683914 -4.0486794][-7.4367 -7.2814827 -7.0653715 -6.421937 -3.9682913 1.3242946 6.1215739 7.4029317 7.5714507 5.3107543 2.4463334 -1.308332 -4.1403828 -4.34159 -4.437233][-9.4196157 -8.3994751 -7.0545216 -5.6464319 -3.7371683 -1.1854811 2.3761802 4.8095269 5.5881772 4.103498 2.6957707 -0.961534 -4.5944481 -7.427392 -7.4797983][-8.8023224 -9.5838041 -8.5919008 -7.3566008 -4.9056468 -1.8461058 0.084519386 1.435514 2.5111775 1.9606986 1.6427565 -2.2198718 -6.337049 -10.000078 -11.097372][-11.946751 -10.250483 -9.8018818 -9.6124592 -8.7948589 -7.0887256 -4.6165953 -2.5502176 -2.1103065 -2.7250996 -2.559154 -6.2033186 -10.303019 -12.130165 -12.57929][-12.640553 -10.727814 -9.6127768 -9.4939423 -9.3181543 -8.8938694 -8.17299 -6.7905889 -6.6096621 -6.4301596 -5.6409178 -8.5551081 -10.815332 -12.965345 -13.57658][-11.729249 -9.8174562 -7.3770962 -6.9662013 -8.1663036 -7.8334122 -7.4340038 -7.227128 -7.1129537 -7.298593 -6.9492741 -8.656682 -10.227976 -10.973204 -11.601938][-11.368729 -9.6427937 -7.79646 -7.0376034 -6.2070432 -6.4768677 -7.0972505 -5.7166491 -5.4213581 -5.7557206 -5.3261948 -5.2836471 -7.0519228 -8.02539 -7.9324713][-9.3132992 -9.29037 -8.4535017 -6.7210345 -5.0212674 -5.4888167 -5.8019938 -5.0481539 -5.3940172 -5.4441552 -5.177001 -6.0219707 -6.80663 -7.6103954 -8.1008816]]...]
INFO - root - 2017-12-15 22:33:18.801507: step 66410, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 49h:04m:13s remains)
INFO - root - 2017-12-15 22:33:25.358512: step 66420, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 48h:15m:31s remains)
INFO - root - 2017-12-15 22:33:31.922143: step 66430, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 48h:53m:49s remains)
INFO - root - 2017-12-15 22:33:38.554562: step 66440, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 50h:14m:21s remains)
INFO - root - 2017-12-15 22:33:45.186725: step 66450, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 49h:59m:35s remains)
INFO - root - 2017-12-15 22:33:51.759784: step 66460, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 48h:12m:40s remains)
INFO - root - 2017-12-15 22:33:58.372667: step 66470, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 47h:53m:49s remains)
INFO - root - 2017-12-15 22:34:05.007849: step 66480, loss = 0.12, batch loss = 0.07 (11.4 examples/sec; 0.701 sec/batch; 51h:46m:45s remains)
INFO - root - 2017-12-15 22:34:11.602648: step 66490, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 49h:33m:19s remains)
INFO - root - 2017-12-15 22:34:18.220456: step 66500, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 49h:21m:09s remains)
2017-12-15 22:34:18.751270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9335165 -7.9590249 -6.4065814 -5.1040154 -5.7207417 -6.0763884 -6.2625141 -6.6631327 -7.0256119 -6.4577246 -5.4615707 -5.4361944 -8.1201105 -8.2610588 -7.4508691][-6.3241549 -7.1888719 -5.8052874 -3.87876 -3.9426823 -4.4405103 -5.2115393 -5.4888086 -6.4317341 -6.6374073 -6.0720177 -6.5303984 -9.13346 -9.2622871 -8.257061][-5.2366333 -5.9985814 -5.9489441 -4.5904074 -3.7082422 -3.9181037 -4.5197935 -4.3130813 -4.2093053 -4.8901672 -5.2188969 -6.3418913 -9.2237825 -9.8952045 -9.7867155][-7.5014143 -7.5812149 -5.6999211 -3.8534045 -3.4054341 -2.733659 -2.6741867 -2.7390716 -3.1038656 -3.6280863 -3.568831 -4.4272485 -7.3623924 -9.2274027 -9.9479313][-8.78671 -8.6679525 -6.5919447 -3.4080486 -2.2107751 -1.0122895 -0.66780949 -0.98471546 -1.9487433 -2.1953464 -2.1707892 -2.9290278 -5.6576009 -6.9405303 -7.2551918][-9.5224228 -8.6534767 -5.7607303 -2.3514249 -0.24661684 1.4223471 1.7783198 0.85398006 -0.3015132 -1.1481552 -1.6974754 -2.3624265 -4.8506351 -5.8827634 -6.0825553][-8.08913 -7.5471153 -5.0474186 -0.93499994 2.3625045 4.2030683 4.2622762 3.897922 3.0735831 0.927392 0.10955286 -0.6106267 -3.1707268 -4.1637964 -4.3860517][-7.4719524 -6.011694 -3.7280059 0.73831654 4.2838922 6.459599 6.9923739 5.79149 4.127707 2.7757421 2.0454149 0.22967625 -2.4764729 -2.6163824 -2.3042121][-6.3620458 -5.1756558 -2.9932616 0.87756538 4.7218957 6.1736093 6.0616574 4.6046815 2.1969771 1.1951537 1.9055586 1.0293741 -1.3896618 -2.2070389 -1.9721496][-5.3873568 -4.7020388 -3.4484389 -0.96036768 1.2702317 2.3649087 2.8687806 2.3070722 1.4461861 1.1219735 1.2070551 0.50749445 -1.4124289 -1.7026691 -1.9203155][-10.661541 -9.0457706 -7.1119461 -4.8978777 -2.9660857 -2.3524506 -2.0381098 -2.1127088 -2.1615939 -1.4172196 -1.0097213 -1.7989821 -3.7068634 -4.0718789 -3.2027056][-14.375486 -12.618223 -10.024 -7.7389212 -5.9994621 -5.146204 -4.9172616 -5.6148248 -5.8589683 -4.5133328 -3.5577328 -4.54961 -5.7644148 -4.8670435 -4.0013008][-13.598736 -11.006437 -9.1584425 -8.0789146 -5.9693775 -4.8512936 -5.38705 -5.7656431 -5.6803842 -5.6009908 -4.5744224 -3.8701873 -4.3383965 -3.6522386 -2.8240483][-9.66642 -8.1236439 -6.4224014 -5.9067564 -5.3553195 -4.6095276 -5.1972241 -6.0826488 -6.3113074 -5.3592086 -4.2701716 -2.8099675 -1.9789355 -2.1831751 -3.3275363][-6.1658878 -6.069459 -5.9715624 -4.1421628 -2.4655066 -2.86091 -2.9691789 -3.5395644 -5.3496118 -5.6007833 -5.1874571 -5.0472441 -5.0633039 -4.6300716 -4.262435]]...]
INFO - root - 2017-12-15 22:34:25.339370: step 66510, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.661 sec/batch; 48h:49m:38s remains)
INFO - root - 2017-12-15 22:34:31.979728: step 66520, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 49h:01m:24s remains)
INFO - root - 2017-12-15 22:34:38.586206: step 66530, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 49h:42m:25s remains)
INFO - root - 2017-12-15 22:34:45.199970: step 66540, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 48h:18m:39s remains)
INFO - root - 2017-12-15 22:34:51.773043: step 66550, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.681 sec/batch; 50h:16m:22s remains)
INFO - root - 2017-12-15 22:34:58.274676: step 66560, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 47h:31m:54s remains)
INFO - root - 2017-12-15 22:35:04.926308: step 66570, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 48h:27m:27s remains)
INFO - root - 2017-12-15 22:35:11.529883: step 66580, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 49h:36m:55s remains)
INFO - root - 2017-12-15 22:35:18.175747: step 66590, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 47h:19m:36s remains)
INFO - root - 2017-12-15 22:35:24.845894: step 66600, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.684 sec/batch; 50h:31m:37s remains)
2017-12-15 22:35:25.394261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1833591 -2.9217033 -2.7754545 -2.1633391 -2.7058344 -3.4706585 -3.7588449 -3.7851148 -2.4914765 0.02995348 1.9167404 1.1552095 -1.5779033 -4.231225 -6.196569][-3.3839762 -3.5667019 -2.9139442 -3.0481052 -3.6139433 -4.8915491 -5.6430836 -4.6693192 -3.146656 -1.8802266 0.11483765 0.63220024 -1.3194027 -3.4113474 -4.7175736][-2.0765951 -3.0262411 -3.0066237 -3.0738139 -4.2114377 -4.9164109 -4.7043266 -4.691772 -3.4777045 -2.3218732 -1.7024622 -1.9384446 -3.6974511 -4.5870781 -4.8845181][-2.7494874 -3.1970305 -3.1318343 -3.8978438 -4.5470009 -4.5246239 -4.0547523 -3.6338217 -2.9219787 -2.2988088 -1.6119423 -3.428786 -5.9989061 -6.3256731 -5.9223771][-4.3325644 -4.316999 -4.0466375 -4.0842876 -3.2556839 -2.415509 -1.9844091 -2.6876671 -3.1516931 -3.2326703 -2.8812714 -4.1688609 -6.713892 -8.0278835 -8.0421963][-5.796247 -5.3240228 -4.3565655 -2.5370941 -0.48991013 1.2807612 2.6747251 1.5925169 0.41356277 -1.5498991 -2.9558654 -4.0112839 -6.2001014 -7.8933678 -8.34338][-6.7847366 -6.7674356 -4.7078381 -1.5990591 0.9657836 3.6124701 5.6828179 5.1009116 4.0397048 0.73387671 -1.7876179 -2.9948006 -5.6924081 -7.3912058 -7.52695][-7.8528795 -6.7775669 -4.54284 -1.3803577 1.4884615 4.8537288 7.313766 6.6538234 5.7855487 1.9093285 -1.5054154 -3.7031074 -6.7031536 -7.7185354 -7.0289187][-6.382875 -6.6916828 -5.2147727 -2.8631577 -0.8889451 1.8012133 3.8761144 5.00868 5.412621 1.8764176 -0.9745822 -4.2593393 -8.1409 -8.9737473 -8.1074066][-5.3924565 -5.9272509 -5.1462603 -3.7628787 -3.2193861 -1.8200667 -0.057787895 1.2613139 1.3979158 -1.0659246 -2.8249397 -5.8306375 -9.3028193 -10.51316 -10.717938][-8.541338 -9.0869 -7.8974657 -6.5827565 -6.5144043 -6.0613332 -6.0868979 -5.6982131 -5.2895536 -5.3444734 -5.9962678 -9.36735 -11.939407 -12.074868 -11.715405][-13.076849 -13.047577 -11.85659 -10.395109 -9.9446793 -9.5773087 -10.002016 -10.081467 -9.4735918 -9.1233368 -9.472806 -10.843774 -11.213148 -11.725636 -11.563465][-14.672205 -14.924629 -13.650291 -12.570314 -12.204461 -11.703684 -11.606838 -11.817923 -11.991039 -11.176893 -10.528624 -10.875757 -10.147968 -8.8077717 -8.2794914][-12.702446 -13.041096 -13.619631 -12.771402 -11.765631 -11.566143 -11.40031 -10.566221 -10.109428 -9.9824123 -10.212383 -9.2798233 -8.4933624 -7.6134262 -7.2325754][-9.0419664 -9.1261654 -9.5243082 -9.4907045 -8.9017763 -7.95763 -7.3005219 -8.1691236 -8.5219316 -7.7368555 -7.9034953 -8.5455561 -9.0668154 -8.8682251 -8.9614]]...]
INFO - root - 2017-12-15 22:35:31.905733: step 66610, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 49h:37m:13s remains)
INFO - root - 2017-12-15 22:35:38.545960: step 66620, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 48h:44m:16s remains)
INFO - root - 2017-12-15 22:35:45.132562: step 66630, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 49h:15m:27s remains)
INFO - root - 2017-12-15 22:35:51.824760: step 66640, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.637 sec/batch; 47h:01m:24s remains)
INFO - root - 2017-12-15 22:35:58.413996: step 66650, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 47h:39m:07s remains)
INFO - root - 2017-12-15 22:36:05.085502: step 66660, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 48h:58m:52s remains)
INFO - root - 2017-12-15 22:36:11.669658: step 66670, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.636 sec/batch; 46h:57m:53s remains)
INFO - root - 2017-12-15 22:36:18.324175: step 66680, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 47h:39m:04s remains)
INFO - root - 2017-12-15 22:36:24.992341: step 66690, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 49h:44m:49s remains)
INFO - root - 2017-12-15 22:36:31.606876: step 66700, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 47h:56m:46s remains)
2017-12-15 22:36:32.175759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6731653 -6.42515 -6.5531778 -6.6425815 -7.5006495 -8.2031326 -8.4975214 -8.0680447 -7.6515465 -8.3491144 -8.1372671 -8.72096 -9.0935078 -9.8695431 -10.042288][-6.0682731 -5.8644581 -6.2682896 -6.9187851 -7.8820724 -8.6287088 -8.429018 -7.6449947 -7.390522 -7.8099709 -8.1446981 -9.7056046 -10.700567 -11.332558 -10.365307][-4.3232269 -4.8181591 -6.230854 -6.3739905 -6.7448964 -7.1867785 -8.5194016 -8.5525389 -7.7867756 -8.5034084 -8.40336 -9.3676138 -10.679065 -11.929525 -10.285368][-5.2630739 -5.394824 -5.1310854 -5.3980732 -7.2825613 -7.0324326 -5.734283 -6.0789566 -6.0981584 -5.6876593 -6.7292275 -9.0205679 -10.316036 -12.857733 -12.460157][-4.3428645 -5.8192635 -6.387393 -4.8777046 -5.1682334 -3.764941 -2.1359875 -2.5328782 -3.0929024 -3.1357667 -4.4111114 -6.820281 -8.6046505 -11.347341 -12.703281][-6.1483192 -5.7568035 -5.5565624 -4.4274783 -3.2244143 -0.36188984 2.0097613 1.7471027 0.56803274 -1.7296267 -2.2298377 -3.7586908 -6.844635 -10.374199 -11.880787][-5.6642518 -4.9971061 -5.2780347 -3.2056987 0.23942184 3.72656 5.9112592 5.0374293 4.1206861 0.9225688 -0.43646812 -1.6579609 -4.8376961 -7.9272127 -9.4681768][-6.1568542 -4.6308289 -3.6748908 -1.500164 1.0124145 4.1479592 7.4265056 7.6973596 6.7918468 4.8184438 1.9965873 -0.84205484 -3.1472182 -6.752923 -8.5334044][-5.4668374 -5.1337962 -4.7648339 -2.6431212 -0.38650894 2.0429029 6.0571342 7.6901145 6.1871247 4.0253415 2.4016442 -0.2486639 -3.5576987 -7.1033287 -7.7287846][-6.8799095 -6.3963585 -5.279428 -3.3890781 -2.5124614 0.43402767 3.866858 5.3430858 4.508904 1.805923 0.12283802 -2.6167233 -4.391264 -8.4050446 -10.664338][-11.229225 -10.115686 -8.8363686 -6.8981843 -5.207902 -3.0295901 -1.7892146 -1.1556792 -1.0669913 -2.2032781 -4.0035172 -6.7743449 -9.3899679 -10.462112 -9.7484779][-16.109037 -12.742357 -9.6926222 -8.5787029 -6.9461679 -5.9273343 -5.7186747 -4.8461018 -4.5600796 -5.1247578 -5.8712206 -7.1114273 -9.6304188 -11.114348 -10.07528][-14.185726 -12.572619 -8.6822557 -6.5181837 -5.875802 -4.9498844 -5.1519203 -4.8160472 -4.8255739 -5.0801129 -4.8022656 -5.8756666 -7.948338 -9.03125 -7.5438924][-8.98152 -7.8258209 -5.8287029 -4.0350771 -2.7028856 -2.8702512 -3.6410723 -3.0313234 -1.7395031 -1.8619337 -2.9016292 -3.3845105 -4.2103143 -6.229712 -7.0146751][-4.97055 -5.060936 -3.815336 -2.1950748 -2.3172367 -1.3775449 -0.63192844 -1.0915556 -1.4551859 -1.0541062 -0.68842888 -2.5491261 -5.2264214 -5.7040944 -6.0414515]]...]
INFO - root - 2017-12-15 22:36:38.760597: step 66710, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 48h:23m:37s remains)
INFO - root - 2017-12-15 22:36:45.288154: step 66720, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 50h:25m:25s remains)
INFO - root - 2017-12-15 22:36:51.874577: step 66730, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 47h:56m:56s remains)
INFO - root - 2017-12-15 22:36:58.423281: step 66740, loss = 0.17, batch loss = 0.13 (12.7 examples/sec; 0.630 sec/batch; 46h:32m:07s remains)
INFO - root - 2017-12-15 22:37:05.118096: step 66750, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 47h:56m:07s remains)
INFO - root - 2017-12-15 22:37:11.755999: step 66760, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 49h:10m:03s remains)
INFO - root - 2017-12-15 22:37:18.318205: step 66770, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 48h:20m:24s remains)
INFO - root - 2017-12-15 22:37:24.901791: step 66780, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 48h:51m:50s remains)
INFO - root - 2017-12-15 22:37:31.493067: step 66790, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.675 sec/batch; 49h:49m:16s remains)
INFO - root - 2017-12-15 22:37:38.024295: step 66800, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 47h:28m:33s remains)
2017-12-15 22:37:38.516121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-12.194613 -13.06827 -13.665934 -14.716902 -14.948442 -14.133404 -12.858664 -10.383593 -9.0328531 -8.854229 -9.5373316 -10.839548 -12.687335 -12.742081 -10.287942][-11.017665 -11.299693 -11.057354 -12.044835 -12.83054 -13.281506 -12.40311 -11.320827 -10.666438 -9.7649317 -9.9217882 -10.93807 -12.713873 -13.35849 -12.257259][-5.2688179 -6.4784012 -8.0476036 -9.9362345 -10.798328 -10.727011 -9.9484186 -9.2087812 -8.4515018 -7.7341814 -7.16455 -8.1533165 -10.777547 -12.309258 -12.421835][-3.3768215 -2.7017307 -3.2991436 -4.6809573 -6.3339634 -6.4324946 -5.6623154 -5.9378853 -6.4179721 -6.5967603 -6.7464008 -7.1526241 -8.5513 -10.035478 -10.305651][-3.6097152 -4.0322943 -4.7110858 -3.5719295 -3.17702 -2.0272033 -0.68710566 -1.8491955 -3.231925 -3.8343511 -5.0439258 -6.5047789 -8.4614944 -9.7862988 -8.7343521][-6.5932097 -6.5469947 -5.954525 -4.3871779 -2.7628744 0.98847723 3.6369567 3.1003423 1.8350329 -0.41655731 -2.4314089 -4.0474882 -6.12554 -7.2450089 -6.8848772][-8.4191093 -8.4884892 -7.3955283 -4.2885036 -1.6619458 2.0386043 5.2310452 6.2651544 6.9997211 4.6410069 1.3668742 -1.0635557 -4.4766417 -6.2500958 -5.2435904][-7.8195863 -7.0825453 -6.0700316 -3.5396907 -0.19875145 3.1136317 5.6270547 6.3889337 6.5708022 4.4322476 1.5678158 -0.76711893 -3.2993152 -4.1163268 -3.0642605][-4.9946995 -3.86458 -3.3141 -1.7603123 0.0319376 2.1591783 3.7194018 4.9205155 5.2996707 3.1610246 0.78574944 -1.9803979 -4.4844813 -5.2565475 -3.4548147][-4.2487607 -3.0618815 -1.5119281 -0.7563653 0.083611488 0.77303934 1.8424788 2.1846051 1.3549271 -0.23676062 -1.2459841 -3.118844 -5.1605334 -5.4090052 -4.3016229][-7.0597019 -4.8766041 -3.9693353 -3.990978 -3.3021235 -2.0242715 -0.29150629 0.66619873 -0.62875319 -2.5748913 -3.5974402 -5.3747959 -7.7766151 -7.6243238 -5.668952][-9.802331 -7.83906 -6.2636137 -6.0367951 -6.0558147 -4.8774891 -3.4632089 -2.8628221 -3.8410406 -4.43311 -3.9893303 -4.3924208 -5.4537888 -6.3164825 -6.159163][-9.6498327 -8.7833872 -7.7559376 -7.5647063 -6.8482075 -4.927702 -3.9055991 -3.5031326 -4.2418685 -5.3368239 -5.1330509 -5.1936531 -5.6336813 -5.7504015 -4.9382029][-7.9654503 -7.4735394 -7.8519125 -7.2013822 -6.5911646 -5.207026 -4.1575766 -3.9477544 -4.4694681 -4.7903924 -4.8898778 -3.9943337 -3.3067589 -4.0403147 -4.6794963][-5.7583404 -4.9202337 -4.6104174 -4.8322062 -4.20229 -3.5131731 -3.1420779 -3.544853 -4.5889339 -5.295619 -5.6702375 -6.2435632 -7.1976151 -7.199265 -6.2044597]]...]
INFO - root - 2017-12-15 22:37:45.141081: step 66810, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 48h:27m:48s remains)
INFO - root - 2017-12-15 22:37:51.771867: step 66820, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 49h:09m:10s remains)
INFO - root - 2017-12-15 22:37:58.400395: step 66830, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 47h:43m:31s remains)
INFO - root - 2017-12-15 22:38:05.054757: step 66840, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 48h:38m:37s remains)
INFO - root - 2017-12-15 22:38:11.631273: step 66850, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.633 sec/batch; 46h:41m:36s remains)
INFO - root - 2017-12-15 22:38:18.279159: step 66860, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 50h:09m:21s remains)
INFO - root - 2017-12-15 22:38:24.814128: step 66870, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 49h:07m:24s remains)
INFO - root - 2017-12-15 22:38:31.465117: step 66880, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 49h:41m:18s remains)
INFO - root - 2017-12-15 22:38:37.955795: step 66890, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 46h:46m:59s remains)
INFO - root - 2017-12-15 22:38:44.512361: step 66900, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 48h:52m:24s remains)
2017-12-15 22:38:45.018035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8296089 -10.43408 -11.616394 -11.664781 -12.659561 -12.213449 -11.487267 -10.238401 -8.9946861 -9.0111217 -8.5737152 -9.56269 -12.141119 -11.542194 -11.695828][-8.7535763 -8.98134 -8.6785374 -8.88761 -10.38734 -10.650703 -10.658497 -9.8417559 -8.8782291 -8.835268 -8.8457584 -10.799974 -13.577972 -13.128784 -13.283489][-7.3747311 -8.3140564 -9.4923277 -9.3945045 -10.643647 -11.029232 -10.771376 -9.6133509 -8.2677536 -8.0557861 -7.5923481 -9.3148527 -12.666842 -12.533755 -13.200146][-8.1354532 -9.3480968 -10.191624 -9.3003025 -10.034418 -9.4272022 -9.1469936 -8.94129 -9.0678539 -8.3716412 -6.4537597 -7.6225405 -10.797049 -11.744814 -13.656348][-10.413679 -12.011717 -12.208227 -9.6760721 -7.7174463 -5.2330303 -3.3018866 -5.726594 -9.5844135 -8.6307211 -6.8845758 -8.2554951 -11.26808 -11.552029 -13.099495][-12.561374 -13.25765 -12.950207 -8.8779249 -4.6547384 1.1079946 5.4872727 1.4481764 -3.2615497 -5.8975887 -7.9653378 -8.2089405 -9.4384165 -10.159323 -12.787427][-13.193087 -13.518458 -12.011755 -6.1902037 -2.4885452 3.5189519 8.6536083 7.8532815 5.0045896 -1.3517594 -7.008399 -8.1484413 -10.496021 -11.428228 -12.872934][-12.387341 -11.963146 -10.622116 -5.7562833 -1.3186522 4.6203828 8.8260174 8.03133 5.883306 1.0618277 -3.3251607 -6.991693 -11.424757 -11.495251 -12.392021][-10.103543 -8.5853491 -8.09091 -4.5955954 -1.5360718 1.9770207 5.1652093 4.5890527 1.901381 -1.2205329 -4.1287842 -8.33093 -12.443287 -12.308899 -13.143891][-7.7569952 -7.2634435 -6.6006989 -4.0021696 -3.5430081 -2.279495 1.0994973 0.63999462 -2.4235139 -5.3864794 -8.0106373 -10.451002 -13.471737 -13.60593 -14.819048][-11.05159 -11.699886 -11.341431 -8.9392319 -8.30316 -7.5383363 -6.1855483 -7.005621 -7.6999588 -8.6089325 -10.804512 -12.763151 -13.70668 -14.764626 -14.72797][-15.882784 -16.325914 -16.085526 -14.201576 -13.93173 -13.301674 -13.169802 -13.647858 -12.661024 -12.933277 -13.583753 -13.447802 -13.770357 -13.707548 -13.713078][-17.762989 -18.164989 -18.942764 -18.151112 -16.772404 -15.432016 -15.023432 -14.736197 -14.883093 -14.307364 -13.20999 -13.579699 -13.347729 -12.572254 -12.380283][-14.676315 -15.088003 -15.301563 -15.397472 -14.160496 -13.590987 -13.112334 -12.448563 -11.890522 -11.493935 -11.777807 -11.586858 -11.384678 -10.657548 -10.262684][-9.8567238 -9.7914162 -9.3106251 -9.0183716 -8.4753857 -8.1298141 -7.9614077 -7.9080205 -7.9798932 -8.2773695 -8.4596052 -9.4384651 -10.344368 -10.89567 -11.016499]]...]
INFO - root - 2017-12-15 22:38:51.571772: step 66910, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 50h:21m:46s remains)
INFO - root - 2017-12-15 22:38:58.208484: step 66920, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 49h:11m:42s remains)
INFO - root - 2017-12-15 22:39:04.896822: step 66930, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 48h:06m:05s remains)
INFO - root - 2017-12-15 22:39:11.592797: step 66940, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 49h:03m:04s remains)
INFO - root - 2017-12-15 22:39:18.184846: step 66950, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 49h:38m:29s remains)
INFO - root - 2017-12-15 22:39:24.827215: step 66960, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 48h:00m:45s remains)
INFO - root - 2017-12-15 22:39:31.464780: step 66970, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 48h:37m:57s remains)
INFO - root - 2017-12-15 22:39:38.135096: step 66980, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.679 sec/batch; 50h:04m:49s remains)
INFO - root - 2017-12-15 22:39:44.753944: step 66990, loss = 0.17, batch loss = 0.12 (11.5 examples/sec; 0.696 sec/batch; 51h:18m:15s remains)
INFO - root - 2017-12-15 22:39:51.271993: step 67000, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 49h:21m:20s remains)
2017-12-15 22:39:51.858511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.08846 -4.84803 -3.82586 -3.5703948 -3.9753873 -3.4238272 -3.0645692 -3.1690197 -3.1900949 -3.0930579 -3.0818477 -5.1504893 -6.4036341 -7.2784505 -7.3129315][-6.2296758 -6.1652851 -5.208889 -4.2470827 -4.2982969 -4.4650841 -3.7600803 -2.8935199 -2.6309979 -2.4409904 -2.2543912 -4.2163768 -5.7900014 -6.9110928 -6.8074121][-5.5548339 -5.10066 -4.7284636 -4.2613959 -4.0358419 -3.7704077 -3.4834776 -3.2260525 -2.8755963 -2.786634 -2.836596 -4.426569 -5.8455987 -6.6099448 -6.5709052][-4.1315851 -4.4080825 -3.5104525 -2.66038 -2.3325632 -2.3126569 -2.4336419 -3.0354879 -3.2741511 -2.8955922 -2.7538331 -4.8122663 -6.3980336 -7.5553613 -7.7239275][-2.777353 -2.6473973 -2.5975425 -1.917284 -2.0379431 -1.5556021 -1.358325 -1.6235719 -1.6772242 -2.0845602 -2.6951442 -4.3638225 -5.9652095 -7.5195751 -7.8031664][-3.9161651 -3.2723155 -1.8237572 -0.70277977 -0.57795954 0.037453651 0.5801878 0.95972729 0.66755819 -0.37376785 -1.3763118 -3.6382589 -5.27536 -6.584806 -7.2612329][-5.9564323 -4.4942608 -2.7211344 -1.2517085 -0.0010170937 1.6673346 2.7148938 3.1024137 2.8059888 1.5674701 0.096112728 -2.648284 -5.0261364 -6.7117114 -7.2569308][-5.1676044 -4.4835672 -3.046072 -1.0746775 0.69704294 2.4478221 3.3888249 3.400044 3.1997066 2.4460435 1.4940944 -1.7742445 -4.604352 -6.5722866 -7.2637324][-4.0308619 -4.4548869 -3.9724412 -1.8106689 -0.1226058 1.6855483 2.2211595 2.8597026 3.3495135 2.3891072 1.2726479 -1.3931723 -4.1922884 -6.2143111 -6.9049611][-4.5670414 -4.9663186 -4.3423414 -2.2965589 -0.55726528 1.0449309 1.3533463 1.3683901 1.4006195 1.2556233 0.63822603 -2.5086136 -5.3247318 -6.9289885 -7.5079265][-6.5506668 -5.9050541 -4.7295661 -3.0174949 -2.1919241 -1.0696182 -0.84857225 -1.50424 -2.4422932 -2.6039369 -2.810112 -5.3605123 -7.5154562 -8.2989864 -8.8986111][-9.3426056 -8.28286 -6.2973228 -4.2447233 -3.1688948 -3.6108932 -4.2425289 -4.7389383 -5.518075 -6.0839171 -6.3782487 -7.1020727 -7.7676172 -8.39389 -8.9673176][-9.8336582 -7.6664867 -5.76483 -5.4338942 -5.4159865 -5.153193 -5.1699777 -5.9567528 -6.1754189 -6.519311 -7.1014442 -7.0318069 -7.0916014 -6.3822517 -6.389596][-10.598635 -9.0458984 -7.108326 -5.549799 -4.9894118 -5.9278874 -6.4396086 -5.8366361 -5.3821549 -5.2077937 -5.3715014 -5.2213674 -5.7367315 -5.7019544 -4.9897017][-8.4457369 -8.151186 -7.0084286 -5.5690169 -4.8697338 -5.224782 -5.2822642 -5.4105186 -4.9201927 -4.0018888 -3.683286 -4.2886481 -5.0745 -6.2570596 -6.8326864]]...]
INFO - root - 2017-12-15 22:39:58.534116: step 67010, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 48h:40m:46s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 22:40:05.141182: step 67020, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 47h:23m:24s remains)
INFO - root - 2017-12-15 22:40:11.823758: step 67030, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.654 sec/batch; 48h:11m:50s remains)
INFO - root - 2017-12-15 22:40:18.408545: step 67040, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 48h:07m:38s remains)
INFO - root - 2017-12-15 22:40:24.970578: step 67050, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 49h:29m:42s remains)
INFO - root - 2017-12-15 22:40:31.614368: step 67060, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 49h:04m:55s remains)
INFO - root - 2017-12-15 22:40:38.227953: step 67070, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 48h:24m:41s remains)
INFO - root - 2017-12-15 22:40:44.842236: step 67080, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 48h:13m:24s remains)
INFO - root - 2017-12-15 22:40:51.566144: step 67090, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 49h:51m:10s remains)
INFO - root - 2017-12-15 22:40:58.204142: step 67100, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 47h:51m:18s remains)
2017-12-15 22:40:58.775380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2826104 -4.8008986 -4.0984015 -3.7958016 -4.3845716 -5.2588062 -6.1579776 -6.6740742 -6.8706059 -7.7910166 -8.1515656 -9.9732771 -12.161901 -11.936002 -10.069452][-6.9034386 -6.6187496 -5.8997688 -5.496573 -5.8829808 -6.9881268 -8.0020943 -9.0735989 -9.8845844 -9.7618389 -10.054617 -12.343315 -13.941065 -14.08172 -11.730197][-4.7845831 -5.378068 -5.5714011 -4.9965763 -5.7840881 -7.2172561 -8.0509968 -8.1763382 -8.2153788 -8.8273277 -9.6052275 -11.599905 -13.972036 -14.938705 -13.339619][-6.7993526 -6.3930869 -5.7008476 -4.551302 -4.9792771 -5.5602612 -6.0572152 -7.0767708 -7.46893 -6.9401822 -7.3342404 -10.449578 -13.557565 -14.800243 -13.403848][-10.237821 -10.509535 -8.8690491 -5.7219315 -4.1229453 -2.3761861 -1.9089124 -3.3672161 -4.448184 -5.1930022 -6.5022888 -8.8869457 -11.360575 -13.605185 -13.249386][-12.164377 -11.106918 -8.7306213 -5.7552414 -2.854387 0.7676301 3.7387967 4.2117839 3.2795348 -0.12487745 -4.334178 -7.8069572 -10.754261 -11.634062 -11.063196][-13.06789 -11.779285 -8.5666857 -4.2559042 -1.1104755 2.1981645 5.9530625 7.7126365 8.2195263 3.6363797 -1.9839842 -7.0244651 -11.444621 -12.051745 -11.327154][-12.631641 -11.137255 -8.6906719 -4.2406845 -0.612762 4.0276361 7.6318955 7.89376 8.3425159 4.9884477 -0.1442461 -6.1617551 -11.754131 -13.492657 -12.652246][-10.813961 -9.2024078 -7.8669767 -4.5039854 -1.2169056 3.1417184 6.34164 8.3700619 9.2188644 4.2641006 -1.0651355 -7.0242009 -12.215668 -13.389872 -12.851107][-8.6333313 -6.9914479 -5.6398311 -3.5590212 -1.8929915 0.53023005 2.9097686 5.5471482 6.0125012 3.3093066 -0.72236681 -7.1135411 -12.456923 -14.338001 -13.734081][-11.048617 -10.188417 -8.4908047 -6.7966719 -5.9225574 -4.6067748 -3.3335667 -2.5792031 -2.565496 -3.4444036 -5.950027 -10.990816 -14.128006 -14.588762 -13.187149][-16.651045 -14.597757 -12.250368 -11.783033 -11.352146 -10.037111 -9.281744 -9.32776 -9.5603313 -9.9628029 -11.43777 -13.191326 -13.871878 -14.012611 -12.318356][-14.692707 -13.545502 -13.225458 -12.896326 -12.36615 -11.624216 -11.041079 -10.838818 -11.814807 -12.102381 -11.925835 -12.379114 -12.173815 -10.73862 -9.1627693][-11.205106 -9.9248705 -8.8622837 -7.8877058 -7.7325335 -8.8640289 -9.2453556 -8.9106321 -9.2847958 -9.71176 -10.340433 -9.8417759 -8.9658966 -8.10306 -7.6827183][-8.234354 -6.575736 -4.8017378 -3.6176755 -2.9416149 -3.1300395 -3.590369 -4.8473873 -6.0830259 -5.970377 -6.1852303 -7.397872 -8.2225513 -7.9134231 -8.1193275]]...]
INFO - root - 2017-12-15 22:41:05.401946: step 67110, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 48h:45m:50s remains)
INFO - root - 2017-12-15 22:41:12.047693: step 67120, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.666 sec/batch; 49h:06m:58s remains)
INFO - root - 2017-12-15 22:41:18.679122: step 67130, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 48h:43m:27s remains)
INFO - root - 2017-12-15 22:41:25.304060: step 67140, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 49h:23m:32s remains)
INFO - root - 2017-12-15 22:41:31.791030: step 67150, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 49h:23m:01s remains)
INFO - root - 2017-12-15 22:41:38.423632: step 67160, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.670 sec/batch; 49h:22m:49s remains)
INFO - root - 2017-12-15 22:41:44.958078: step 67170, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 47h:50m:28s remains)
INFO - root - 2017-12-15 22:41:51.524298: step 67180, loss = 0.24, batch loss = 0.19 (12.1 examples/sec; 0.662 sec/batch; 48h:46m:17s remains)
INFO - root - 2017-12-15 22:41:58.111844: step 67190, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 48h:33m:55s remains)
INFO - root - 2017-12-15 22:42:04.698384: step 67200, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 48h:40m:53s remains)
2017-12-15 22:42:05.241415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.9956732 -8.498888 -6.8045011 -5.6409268 -5.4939513 -5.2952738 -4.6560054 -3.6275988 -2.5285327 -2.1639514 -2.0273271 -3.9630494 -7.8130059 -8.6127491 -8.2142506][-7.1751056 -6.5388575 -5.3256431 -3.9740872 -3.6356087 -3.6118319 -3.2729359 -2.0609264 -1.3670468 -2.0260496 -2.0387025 -3.3620734 -6.8307538 -9.1173506 -9.3715544][-4.60275 -4.7448745 -4.9656634 -4.1840243 -3.9683061 -3.5941942 -3.0835297 -1.4409709 -0.93101025 -1.5686398 -1.7076445 -3.0461652 -5.1485772 -7.1322541 -8.7462091][-5.7181931 -5.8325653 -6.141417 -4.8417778 -5.5042839 -5.2557349 -4.4230967 -3.5178072 -3.1493976 -2.59737 -2.0510561 -3.4396908 -5.7908759 -6.8163509 -6.6073785][-4.9291873 -5.7139969 -6.1096497 -4.8427663 -4.6573052 -3.9064584 -3.209796 -2.707613 -2.751256 -2.7867863 -2.37097 -3.3662074 -5.5634804 -7.4381781 -7.738976][-7.6050253 -7.7967396 -6.7598023 -2.8781703 -1.1972451 0.27394056 1.389267 0.8946209 -0.55932 -1.3142242 -2.6713996 -3.0708966 -5.0762391 -6.4031277 -6.579371][-7.77573 -7.4677296 -5.5836177 -1.8761716 0.060256958 2.8206182 4.2101912 4.8829722 4.5241523 1.8614531 -1.3819532 -3.0292919 -6.5998068 -7.5521088 -7.4743981][-7.8486624 -6.6729016 -4.7357979 -0.53839493 2.116189 3.7594609 4.6914296 5.5322127 5.782125 3.1795621 0.38060379 -2.8960023 -6.00479 -7.1838717 -7.5683327][-9.222517 -6.826 -4.0753851 0.11974764 1.1101284 1.5954447 2.7649026 3.5592351 4.4809785 2.5315995 -0.20770693 -3.25073 -6.6529427 -8.301096 -8.0083332][-9.5106306 -8.241683 -6.3812628 -2.4207921 -1.7068176 -0.4990778 -0.0028233528 0.56364107 1.4104362 -0.0031647682 -1.9797933 -3.9452739 -7.1408648 -8.6495285 -9.059762][-11.738688 -11.207218 -10.556904 -7.8557029 -7.3444042 -6.1348648 -4.7749009 -3.4576151 -3.4652367 -4.1354914 -5.6097913 -7.4170246 -8.6455717 -8.930665 -8.4356852][-13.545696 -13.173683 -12.182137 -10.334364 -10.089903 -8.9305449 -7.8656015 -7.0244331 -6.1728692 -6.8212414 -7.9056873 -9.1895237 -9.2047119 -9.7660866 -9.0839663][-14.065018 -12.860901 -11.393753 -11.022155 -10.662236 -9.6011829 -8.9727707 -8.34136 -7.870307 -8.1316423 -8.4773035 -9.3025713 -8.8251038 -7.9676819 -6.641][-10.678852 -10.497038 -9.3135138 -7.9361248 -6.7738967 -7.0192442 -7.4010339 -7.2167864 -6.9916353 -6.7276697 -7.0227056 -7.1869659 -7.0580349 -6.8863249 -6.4428725][-7.3910112 -7.162179 -6.0403938 -4.5517769 -3.0663846 -3.0432034 -3.3088994 -3.6064496 -4.2383747 -4.7766786 -4.7432728 -5.725584 -6.4926186 -6.6201925 -7.031168]]...]
INFO - root - 2017-12-15 22:42:11.843098: step 67210, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.661 sec/batch; 48h:41m:56s remains)
INFO - root - 2017-12-15 22:42:18.419521: step 67220, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 49h:01m:58s remains)
INFO - root - 2017-12-15 22:42:25.028661: step 67230, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 47h:39m:41s remains)
INFO - root - 2017-12-15 22:42:31.682873: step 67240, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 50h:11m:07s remains)
INFO - root - 2017-12-15 22:42:38.303914: step 67250, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 48h:56m:07s remains)
INFO - root - 2017-12-15 22:42:44.936375: step 67260, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.673 sec/batch; 49h:33m:45s remains)
INFO - root - 2017-12-15 22:42:51.564564: step 67270, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 47h:54m:53s remains)
INFO - root - 2017-12-15 22:42:58.167380: step 67280, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 49h:06m:47s remains)
INFO - root - 2017-12-15 22:43:04.876321: step 67290, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 48h:53m:18s remains)
INFO - root - 2017-12-15 22:43:11.491075: step 67300, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 47h:40m:37s remains)
2017-12-15 22:43:12.007922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4014263 -4.28967 -3.5279121 -3.0424697 -2.86917 -3.6424515 -3.8436718 -3.760078 -4.3376989 -2.7522781 -1.2878857 -2.3324227 -4.429553 -8.3806219 -8.8893881][-3.1587358 -2.1108773 -1.909472 -0.47447109 -0.97791862 -2.4077103 -4.0489826 -4.7186956 -4.8955421 -4.699111 -4.0733552 -4.6819296 -8.4592075 -10.555529 -9.7838249][-3.3678486 -2.1824834 -1.3519258 -0.67318869 -0.70572758 -1.6779943 -2.9322815 -3.4826488 -3.9659913 -4.22472 -4.4196963 -5.7284164 -8.9896393 -11.791647 -11.136286][-4.0565281 -3.8429132 -2.6137705 -1.6197529 -1.7113109 -2.3295422 -2.912065 -2.414149 -3.0349231 -2.8071823 -3.1186082 -4.8712616 -7.9459605 -10.582037 -10.697739][-3.313802 -2.9318969 -2.6972039 -1.2518291 -0.56332684 -1.3431015 -2.3911266 -3.1737492 -3.1755974 -1.9865189 -1.6574135 -3.15746 -6.9988832 -9.3332615 -9.1442471][-3.8724594 -3.7341022 -2.2159395 -0.98926353 -0.076732635 1.2544789 1.066184 0.39687777 -0.38574266 -0.33098269 -0.24135971 -1.0283084 -4.085134 -7.3524446 -8.2535944][-4.9922709 -4.0480003 -2.7923605 -0.38825464 0.79195738 1.6507592 2.4808564 2.5838656 2.7892995 1.775064 1.1107054 0.25691652 -2.7131658 -4.6997595 -4.8842258][-5.1641064 -4.6175566 -2.53728 -0.67580557 1.4868793 3.0412354 3.0492682 2.8570561 2.8888535 2.2010412 0.76170158 -0.4899025 -3.5769742 -4.0479493 -1.8790772][-4.3717241 -4.0380778 -3.3260062 -0.57482672 1.1958942 2.6644721 3.4900584 2.73739 1.6013069 0.2527895 -0.36173677 -1.2491455 -4.0065794 -5.0468993 -2.4845221][-6.39773 -6.3397183 -5.7465787 -3.1216843 -0.92986107 0.23812819 0.90462446 0.85082722 -0.16689539 -2.2028406 -2.8959157 -3.3483162 -5.5037208 -5.3531351 -2.7450502][-11.985373 -11.662436 -11.084268 -9.0335121 -6.4797177 -5.0394125 -4.0896463 -3.5367086 -4.346312 -4.7892919 -5.6405177 -6.1150894 -7.2478213 -7.4229784 -4.6516871][-15.596813 -15.764698 -13.881208 -12.014885 -10.357978 -8.5043077 -7.2205329 -6.7439628 -6.8244267 -6.634 -5.9292674 -7.0500631 -7.1388621 -6.7261419 -5.1771407][-14.205839 -13.960741 -13.288713 -11.634634 -10.153529 -7.7624712 -6.4397902 -6.3730912 -5.8901744 -5.4030147 -5.1301231 -5.006722 -5.503325 -5.35352 -3.8159339][-9.8164043 -9.3968582 -9.2476521 -9.0510674 -8.16968 -6.5535021 -5.2125359 -4.5852141 -4.7278843 -4.6893187 -4.4740539 -3.6531434 -3.3479695 -3.7621951 -2.3760419][-5.3673978 -5.3730121 -5.5427728 -4.8618975 -4.6783714 -4.3065577 -2.7087109 -1.9330647 -2.8576543 -3.3486059 -3.9457521 -4.4047794 -4.8276277 -4.6389389 -4.13607]]...]
INFO - root - 2017-12-15 22:43:18.595335: step 67310, loss = 0.18, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 46h:53m:58s remains)
INFO - root - 2017-12-15 22:43:25.145024: step 67320, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 47h:54m:29s remains)
INFO - root - 2017-12-15 22:43:31.653702: step 67330, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 48h:00m:40s remains)
INFO - root - 2017-12-15 22:43:38.226877: step 67340, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 48h:41m:46s remains)
INFO - root - 2017-12-15 22:43:44.880802: step 67350, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 48h:24m:20s remains)
INFO - root - 2017-12-15 22:43:51.539084: step 67360, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.681 sec/batch; 50h:07m:58s remains)
INFO - root - 2017-12-15 22:43:58.071281: step 67370, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 47h:29m:06s remains)
INFO - root - 2017-12-15 22:44:04.669812: step 67380, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 47h:28m:57s remains)
INFO - root - 2017-12-15 22:44:11.271574: step 67390, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 48h:07m:45s remains)
INFO - root - 2017-12-15 22:44:17.932796: step 67400, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 48h:26m:00s remains)
2017-12-15 22:44:18.541483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.87019873 -0.94822645 -0.38150883 0.70815468 0.63876677 -0.73981905 -3.3801434 -5.4690266 -5.8575459 -6.1768041 -6.2995863 -8.540863 -11.443575 -12.496789 -10.614988][-0.80634022 1.3224931 3.795939 4.3318543 2.9762731 0.5542922 -1.9925947 -3.7526474 -5.8339667 -7.2144752 -8.3479242 -11.391665 -14.565802 -15.291048 -14.471611][-0.052760124 0.57616806 0.81694031 3.2023368 3.8319945 2.288775 0.28592634 -2.1403162 -4.6059279 -6.7878857 -8.63409 -11.034125 -13.673887 -15.901268 -15.721432][-1.1718912 -0.40051031 1.5039582 3.5634789 2.1987009 1.031426 -0.1448288 -1.673306 -3.6094673 -5.1378779 -6.4501209 -9.37262 -12.886631 -14.014545 -13.472852][-0.76779747 -1.0137324 -0.36033964 1.3603144 1.7128491 2.611547 2.5154967 0.36881638 -1.4721513 -2.7773118 -4.2920609 -7.7759047 -11.288804 -13.281729 -13.001678][-2.7739882 -2.18117 -1.6914563 1.2327948 2.8477979 4.577045 4.9492679 4.057838 3.0930543 1.2325382 -1.1867146 -3.7124627 -6.8291521 -9.2953625 -9.3308887][-3.1446247 -2.9994457 -1.5769696 0.88601494 2.6426969 5.0819497 6.51543 6.8523574 6.5027385 3.811111 1.0163312 -2.0627396 -6.16962 -8.1425962 -8.31243][-6.5986757 -5.1179581 -3.0152586 0.50317144 2.8839278 4.7577281 6.4129062 6.528564 6.2167964 4.7897067 2.5501132 -1.531949 -6.5292597 -9.3670731 -9.6008177][-7.4309783 -5.8085728 -3.5842855 -0.615232 1.2850633 4.0066991 5.3271251 4.6333346 4.4242187 3.509378 1.9036813 -1.7980685 -6.7102838 -9.1967421 -8.972518][-8.3321285 -6.9987092 -4.8680043 -1.7475746 0.66941786 2.5085988 2.9135156 3.1131673 3.501246 1.9512153 -0.11794424 -3.2845995 -7.2181787 -9.5242386 -9.8983784][-12.98126 -11.7092 -9.8711567 -6.0745873 -4.1508179 -2.614908 -1.3781834 -0.94846821 -0.9715786 -1.5205121 -1.9387653 -5.1529355 -8.186 -9.6511641 -8.1003466][-15.402973 -15.148396 -13.583463 -10.413258 -9.0784473 -7.5012741 -7.376389 -6.9509621 -6.6652832 -6.414114 -6.05832 -7.0076785 -7.8554654 -8.7566395 -7.9626279][-15.041601 -15.119556 -13.956375 -11.301126 -10.13919 -8.3255625 -8.3157015 -9.3088474 -10.222495 -9.3601112 -8.3761539 -8.1002293 -8.2153339 -8.3468094 -6.4689007][-11.036082 -11.135846 -11.298979 -9.3342953 -8.4800787 -7.5837421 -8.2820168 -7.8819971 -7.1337023 -7.7597475 -8.1276073 -6.8519168 -5.8179579 -6.0871487 -6.0682964][-8.9031563 -8.1199942 -7.073863 -6.1503453 -6.4222236 -5.2392654 -4.4469452 -4.510427 -5.8453493 -5.9346533 -5.6623621 -6.2285852 -6.8444157 -6.5798798 -6.5729241]]...]
INFO - root - 2017-12-15 22:44:25.115969: step 67410, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 47h:59m:12s remains)
INFO - root - 2017-12-15 22:44:31.707977: step 67420, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 47h:54m:09s remains)
INFO - root - 2017-12-15 22:44:38.292280: step 67430, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 47h:50m:00s remains)
INFO - root - 2017-12-15 22:44:44.902769: step 67440, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.632 sec/batch; 46h:32m:22s remains)
INFO - root - 2017-12-15 22:44:51.478673: step 67450, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 48h:31m:11s remains)
INFO - root - 2017-12-15 22:44:57.999291: step 67460, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 47h:20m:54s remains)
INFO - root - 2017-12-15 22:45:04.615323: step 67470, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.692 sec/batch; 50h:57m:17s remains)
INFO - root - 2017-12-15 22:45:11.268830: step 67480, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 47h:13m:17s remains)
INFO - root - 2017-12-15 22:45:17.899882: step 67490, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.658 sec/batch; 48h:28m:15s remains)
INFO - root - 2017-12-15 22:45:24.562050: step 67500, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 48h:12m:44s remains)
2017-12-15 22:45:25.105979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3811588 -6.2425 -6.291276 -5.376893 -5.8356094 -6.6553059 -6.5648632 -6.862956 -6.881814 -6.6985674 -5.7514372 -6.3252439 -7.5332041 -7.5242949 -6.9419475][-5.6544223 -5.3117986 -5.2102575 -4.5513148 -4.9642015 -5.3218012 -5.3479972 -4.8480134 -5.0308228 -4.7994905 -4.0043883 -5.3342862 -6.27567 -5.8658862 -5.3557005][-3.4380198 -2.9311287 -3.4040143 -2.5364857 -2.9791448 -3.3287921 -2.7221341 -3.1389854 -3.798614 -3.561147 -3.6866529 -5.1828489 -6.3914018 -6.381155 -6.0666509][-3.6156187 -4.60087 -4.1645956 -3.940011 -4.2106767 -2.8933618 -2.0892122 -1.5670357 -2.299511 -3.2778897 -3.836714 -4.9739742 -6.1710939 -6.3946242 -6.3269453][-2.8351378 -3.541965 -4.5051379 -4.6380181 -4.445312 -2.7965803 -0.71429348 -0.55976439 -1.5801539 -2.4007235 -3.8097005 -5.3581023 -6.5755138 -6.744935 -7.5873132][-4.7175546 -4.6688986 -3.2789295 -2.7152646 -1.4845076 0.14247417 1.7510238 2.2612286 2.2288909 0.64957619 -1.0719781 -3.64443 -5.8578076 -6.2266045 -6.3115468][-4.7954912 -4.3572636 -4.2330389 -2.8577046 -0.8803215 1.4890151 3.8268619 4.6251073 4.0033717 1.8512983 -0.063473225 -3.2119734 -5.6484456 -6.2097645 -6.3140321][-5.6814065 -5.6183228 -3.9825025 -1.5272665 1.1957688 3.656816 5.3793817 5.2771325 4.3206277 1.7257099 -0.57193089 -3.4779282 -5.8323679 -6.5374308 -6.5305958][-5.4012485 -4.7513795 -3.8543947 -1.2515488 0.8405571 1.811976 3.1688294 2.8132873 1.934608 0.48984432 -0.8653183 -4.3167543 -6.9984212 -6.9783516 -6.5533195][-6.576499 -4.6176281 -2.6519358 0.20924282 1.9562368 2.6192546 2.955852 1.5202656 0.018696785 -1.6672659 -3.447865 -6.2735648 -8.505518 -8.8975554 -8.7154522][-9.66669 -8.2524166 -5.165812 -1.4376249 -0.61484241 -0.25501013 0.45342684 -0.65199661 -1.8512614 -3.5484462 -5.4466419 -8.47811 -9.4769344 -8.5782413 -8.0033531][-12.32548 -10.52465 -7.2361135 -3.6022568 -2.1192229 -1.3053088 -1.1302986 -1.5833473 -2.9852054 -5.3553772 -6.8769803 -8.1109571 -8.7598038 -8.6958923 -7.7786465][-11.492524 -9.6339655 -6.0902677 -3.5815506 -3.2995703 -3.3447659 -3.5073612 -3.9042635 -4.4592943 -5.612164 -7.4570446 -8.9361591 -8.2553844 -6.916914 -6.4128041][-8.7036161 -7.4855213 -4.8559289 -2.3504303 -0.97334337 -1.2671509 -1.8510048 -2.5156579 -3.9452205 -5.0877953 -5.8812003 -6.4022307 -6.8520217 -6.1009707 -4.4776535][-5.5056095 -4.5930114 -3.1777906 -1.8228991 -0.58434296 0.27433825 0.25040579 -1.1323366 -2.7463934 -4.1689525 -5.467063 -6.2931204 -6.7772961 -7.1778345 -7.4096684]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-67500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-67500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 22:45:32.625554: step 67510, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 48h:09m:55s remains)
INFO - root - 2017-12-15 22:45:39.273743: step 67520, loss = 0.19, batch loss = 0.14 (11.3 examples/sec; 0.710 sec/batch; 52h:16m:26s remains)
INFO - root - 2017-12-15 22:45:45.911002: step 67530, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 48h:29m:56s remains)
INFO - root - 2017-12-15 22:45:52.528117: step 67540, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 47h:59m:02s remains)
INFO - root - 2017-12-15 22:45:59.126858: step 67550, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 48h:15m:51s remains)
INFO - root - 2017-12-15 22:46:05.673097: step 67560, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.631 sec/batch; 46h:26m:21s remains)
INFO - root - 2017-12-15 22:46:12.294808: step 67570, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 49h:38m:51s remains)
INFO - root - 2017-12-15 22:46:18.831222: step 67580, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 48h:20m:06s remains)
INFO - root - 2017-12-15 22:46:25.516577: step 67590, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 50h:14m:26s remains)
INFO - root - 2017-12-15 22:46:32.116627: step 67600, loss = 0.11, batch loss = 0.07 (12.5 examples/sec; 0.640 sec/batch; 47h:04m:40s remains)
2017-12-15 22:46:32.620372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1359358 -3.2816126 -3.0853202 -3.2677355 -3.7943621 -4.5765519 -5.394845 -5.8409815 -5.7788777 -5.0825224 -4.0479994 -4.6642051 -4.33098 -4.4754505 -4.5252295][-4.9983573 -4.5609779 -4.7309613 -3.6837778 -3.3948357 -4.0434442 -5.092453 -5.9415646 -6.2146192 -5.6419578 -4.8190718 -6.0252743 -6.07607 -5.4183774 -4.5763574][-3.5226252 -3.4036713 -3.8177714 -3.8686769 -4.6170049 -5.1305203 -5.4664965 -5.6633306 -5.6149106 -5.4024072 -4.8476906 -6.0691104 -6.6132326 -6.6423478 -5.9081473][-4.40002 -4.5444279 -4.2279716 -3.6648436 -3.8317709 -3.4860344 -3.4782116 -3.7351151 -3.9293447 -4.1383791 -4.1441412 -6.2651372 -7.2567859 -7.8548131 -7.8718657][-5.7041488 -6.0346832 -6.0741124 -4.5982571 -3.5758355 -2.5778513 -2.0281711 -1.7088628 -1.5415583 -2.0188158 -2.2672198 -4.747365 -6.72676 -8.2736645 -8.8408546][-6.46038 -6.6713433 -6.1004782 -5.1250477 -3.7301817 -0.88038254 0.25636959 -0.12280655 -0.51024723 -1.4016814 -2.0395408 -5.0670671 -7.2351589 -8.1624374 -7.906918][-6.2636609 -5.8341112 -5.4498196 -3.9376926 -2.6160769 -0.10914326 2.1255636 2.0642247 1.4548869 0.35717106 -0.95927334 -3.9296026 -5.8442636 -6.9562955 -7.1932468][-4.8526044 -4.9960728 -3.8024178 -2.2983227 -0.56981707 2.5551915 3.8409581 3.922894 3.496222 1.4980836 0.32249355 -2.2671936 -4.0695786 -5.0803294 -5.1897469][-5.0088959 -4.67438 -3.8313217 -1.4631314 -0.37643003 1.726675 3.6326108 3.7610917 3.9142203 2.3539281 0.57446671 -2.7300682 -4.1510243 -4.5711761 -4.5010791][-4.4917974 -4.5896616 -2.9517391 -1.0496078 -0.69980717 0.81352139 1.748651 2.1904302 1.9254565 0.48118162 -0.89428043 -4.0810227 -5.2708645 -5.2373853 -4.4975643][-6.7502971 -5.8293996 -4.4517803 -1.4918175 -0.26876926 -0.92064333 -2.1476371 -2.6131337 -2.2828577 -2.6827767 -3.5969117 -6.759346 -8.0200443 -7.7325039 -5.6637306][-8.7543011 -7.4882989 -5.6307039 -2.9480128 -1.2827582 -1.8909903 -4.000639 -6.2805834 -7.3092432 -7.3684492 -7.1946754 -8.2605371 -8.640379 -7.7267895 -6.2418675][-10.694859 -8.2888775 -5.8996649 -4.0356283 -3.8886182 -4.2476254 -5.4048495 -7.4162531 -7.91705 -7.5946093 -7.4888334 -8.4172707 -9.0336533 -8.2002792 -6.0895281][-8.592041 -8.1609783 -7.0257583 -5.3464379 -4.1444297 -3.8569782 -4.7280731 -5.3494544 -5.4382753 -6.0515542 -6.2493677 -6.3834834 -5.9918656 -6.5142078 -6.3826923][-6.5810003 -6.9021387 -6.555088 -4.886343 -3.6759927 -3.6855271 -3.4738395 -3.4636526 -3.6901634 -3.5360541 -3.1906066 -4.6352086 -5.8579297 -6.7032447 -7.3549347]]...]
INFO - root - 2017-12-15 22:46:39.090915: step 67610, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 48h:02m:13s remains)
INFO - root - 2017-12-15 22:46:45.655319: step 67620, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 47h:34m:14s remains)
INFO - root - 2017-12-15 22:46:52.257687: step 67630, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 49h:14m:08s remains)
INFO - root - 2017-12-15 22:46:58.836775: step 67640, loss = 0.24, batch loss = 0.19 (12.1 examples/sec; 0.662 sec/batch; 48h:43m:47s remains)
INFO - root - 2017-12-15 22:47:05.479273: step 67650, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 47h:48m:22s remains)
INFO - root - 2017-12-15 22:47:12.006347: step 67660, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 47h:45m:28s remains)
INFO - root - 2017-12-15 22:47:18.499761: step 67670, loss = 0.12, batch loss = 0.08 (12.7 examples/sec; 0.632 sec/batch; 46h:28m:03s remains)
INFO - root - 2017-12-15 22:47:25.090784: step 67680, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 48h:12m:35s remains)
INFO - root - 2017-12-15 22:47:31.686362: step 67690, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 50h:24m:36s remains)
INFO - root - 2017-12-15 22:47:38.274514: step 67700, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 48h:18m:43s remains)
2017-12-15 22:47:38.849378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3812 -6.1658096 -6.8776307 -6.9221172 -7.8645678 -8.250432 -8.1329441 -7.8513517 -7.0477576 -5.6773777 -4.0842862 -4.28551 -5.5051203 -7.1560631 -7.4174571][-5.4255967 -5.5104418 -5.7947278 -6.2029061 -7.6772313 -8.7253923 -9.4048157 -9.3679466 -9.0826235 -7.6272006 -6.2359848 -6.9669418 -8.6381845 -9.4489059 -8.1459875][-4.18294 -5.6177568 -6.312119 -6.646019 -7.2828918 -8.6738729 -9.85885 -9.4144306 -8.6742382 -8.2607288 -7.8996286 -8.2927065 -9.4562359 -10.551342 -9.5933428][-5.7968025 -6.3482428 -7.2661796 -7.7605677 -8.4436474 -8.4271765 -8.0007086 -8.5578613 -9.5275412 -8.4059114 -7.7683773 -9.5318375 -11.347321 -11.583365 -10.72601][-6.4885592 -8.1185284 -9.1890831 -8.0274715 -7.0090761 -4.8835216 -3.3260174 -4.5052681 -6.4985657 -7.1172829 -7.6904564 -8.4858818 -10.617207 -11.878914 -11.144747][-7.9473872 -8.9151268 -8.3061075 -6.1064453 -4.3166084 -0.9855547 2.0254674 1.2713666 0.62197781 -2.0606058 -5.4625206 -6.5793786 -7.7798433 -9.2205667 -9.4666843][-8.1075106 -8.7937527 -7.840663 -4.99991 -2.4112024 1.4543839 5.9303203 7.1840711 6.9493003 2.0009546 -2.495254 -3.7724891 -6.6328697 -7.7199812 -7.4418488][-8.2190685 -7.342618 -6.4031086 -3.4985974 -0.49185038 3.81671 8.479866 9.3536148 9.6743813 5.657063 0.76684618 -2.8934982 -6.4946113 -7.676158 -7.7705631][-5.9788756 -5.4937224 -4.7090712 -2.3917077 -0.50124407 2.721683 5.6823297 6.5235972 7.2165408 4.3497672 1.233047 -3.2416825 -7.93196 -9.291441 -8.9401884][-5.7001233 -5.0144153 -4.4960651 -3.0357602 -2.0524652 -0.36553288 2.1647768 3.2862725 3.2172894 1.1986346 -1.3810482 -5.0589256 -8.7352381 -11.040694 -11.589504][-10.932766 -11.159292 -10.105247 -7.5941081 -6.6793251 -5.6732121 -4.3637118 -3.6027708 -3.7844963 -4.6944437 -6.3869438 -9.7433119 -12.448534 -12.898988 -12.575287][-16.298313 -16.608576 -15.621408 -12.996216 -11.030809 -10.095432 -10.666316 -11.430059 -11.534483 -11.862378 -12.655277 -13.229734 -13.826324 -13.962774 -13.538794][-15.305834 -15.175348 -13.47508 -11.976841 -10.480326 -9.987463 -10.666428 -12.344927 -13.888334 -13.996655 -12.786614 -13.136944 -13.185724 -12.383722 -11.434471][-12.919641 -12.880629 -12.790123 -10.272646 -8.4553366 -9.164299 -10.282357 -10.007214 -10.382029 -10.936657 -10.716551 -10.008178 -9.6993818 -9.0130138 -8.7183752][-9.9426 -9.231041 -9.5905132 -9.5241261 -9.0228987 -7.7285085 -6.6053371 -6.819911 -7.8039088 -7.1949759 -6.6556978 -7.2690134 -7.6615491 -8.0803137 -8.6837454]]...]
INFO - root - 2017-12-15 22:47:45.400040: step 67710, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 49h:02m:23s remains)
INFO - root - 2017-12-15 22:47:51.974698: step 67720, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 48h:35m:56s remains)
INFO - root - 2017-12-15 22:47:58.544485: step 67730, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 48h:34m:02s remains)
INFO - root - 2017-12-15 22:48:05.188549: step 67740, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 47h:27m:15s remains)
INFO - root - 2017-12-15 22:48:11.702060: step 67750, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 48h:52m:00s remains)
INFO - root - 2017-12-15 22:48:18.296533: step 67760, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 49h:18m:14s remains)
INFO - root - 2017-12-15 22:48:24.868579: step 67770, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 48h:50m:28s remains)
INFO - root - 2017-12-15 22:48:31.541861: step 67780, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 47h:32m:38s remains)
INFO - root - 2017-12-15 22:48:38.101354: step 67790, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 48h:11m:10s remains)
INFO - root - 2017-12-15 22:48:44.630816: step 67800, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 48h:30m:38s remains)
2017-12-15 22:48:45.178802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6934166 -7.1230659 -7.4018621 -7.8080058 -7.7569327 -8.0344772 -8.19489 -8.6388893 -8.6244392 -8.1486111 -6.8658223 -6.9987097 -8.1183376 -7.854497 -7.5720148][-4.5839448 -5.2694569 -5.3464928 -5.5339069 -6.5259242 -7.1432843 -8.05356 -8.70526 -8.7880821 -8.3927574 -7.1929364 -7.3075786 -9.2251587 -10.087838 -9.8635941][-3.4834154 -3.5835829 -3.477632 -4.5463362 -5.8633623 -6.6913538 -7.1993847 -7.55997 -7.7603073 -7.8601112 -7.9379454 -7.9314137 -10.09124 -11.08417 -11.486749][-3.6012521 -3.5654154 -3.2892804 -4.2720971 -5.35574 -5.8421769 -5.9669414 -6.6435862 -6.5570683 -6.6952653 -6.577035 -7.0311127 -9.6718693 -10.750345 -11.068325][-3.953793 -4.5877295 -4.9447656 -4.82548 -4.4793386 -3.6069388 -3.0623362 -4.3637962 -5.6382813 -6.0600657 -5.6533456 -5.7629404 -8.1170321 -9.5619125 -9.9795866][-3.7908216 -3.8426719 -4.1628428 -4.1557684 -2.6666298 -0.63826895 0.68953276 0.33107519 -1.0010409 -2.6265271 -3.5480618 -4.4232178 -7.009079 -7.3675618 -7.1954145][-4.91282 -4.2562017 -2.7305279 -1.5687327 -0.5055809 1.4731073 3.9634738 4.178926 3.5035663 0.76414776 -2.8950729 -4.4219494 -7.0425482 -7.5250607 -7.3466091][-5.9244418 -4.3774533 -3.0715694 -0.85436106 0.83619022 2.60916 4.5865226 4.129416 4.1741853 2.6890473 0.8943491 -2.2924898 -8.424366 -9.7428789 -9.9385414][-7.3078814 -5.4703007 -3.72893 -1.8347154 -0.83035994 1.0637808 2.9764667 3.5190034 4.3891711 3.4276443 2.5236478 -0.62355518 -7.709342 -11.748915 -13.800241][-6.8154464 -6.308856 -5.24959 -3.1227818 -2.0468006 -0.4582572 1.2087674 1.5034642 1.9351182 2.7432628 2.721787 -0.86684465 -7.1864519 -12.035302 -15.433699][-8.6126747 -8.9610748 -8.56901 -7.0455036 -5.9568477 -4.2513881 -2.8007367 -2.5652254 -2.6102314 -3.0118775 -3.582289 -5.2551355 -9.9832306 -13.855877 -16.078701][-12.601877 -12.537932 -11.218661 -10.234676 -10.815982 -9.8236475 -8.4807606 -7.8155875 -7.3725371 -7.3177414 -7.839901 -10.509833 -14.177364 -15.767584 -15.599234][-14.221235 -14.048109 -13.320746 -12.419199 -12.367383 -11.678523 -11.628428 -10.917988 -9.6969948 -8.8119354 -8.489994 -11.435379 -14.193077 -16.453415 -16.068316][-14.160221 -15.32233 -14.192701 -12.748485 -12.345123 -12.134151 -12.220853 -11.298103 -10.932301 -9.8678322 -9.50957 -10.80834 -11.627802 -13.677641 -14.430458][-11.242754 -12.128487 -12.125475 -11.769392 -10.196463 -9.560442 -9.011364 -9.1246262 -9.8295774 -9.7992191 -10.341085 -10.728676 -10.668991 -11.89497 -12.456877]]...]
INFO - root - 2017-12-15 22:48:51.723355: step 67810, loss = 0.22, batch loss = 0.17 (11.8 examples/sec; 0.679 sec/batch; 49h:55m:30s remains)
INFO - root - 2017-12-15 22:48:58.356472: step 67820, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 48h:30m:35s remains)
INFO - root - 2017-12-15 22:49:05.017191: step 67830, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 48h:06m:56s remains)
INFO - root - 2017-12-15 22:49:11.654959: step 67840, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 48h:29m:44s remains)
INFO - root - 2017-12-15 22:49:18.292852: step 67850, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 47h:42m:56s remains)
INFO - root - 2017-12-15 22:49:24.883541: step 67860, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 48h:40m:58s remains)
INFO - root - 2017-12-15 22:49:31.451842: step 67870, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 49h:16m:32s remains)
INFO - root - 2017-12-15 22:49:37.976056: step 67880, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 48h:37m:25s remains)
INFO - root - 2017-12-15 22:49:44.495370: step 67890, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 48h:39m:45s remains)
INFO - root - 2017-12-15 22:49:51.070817: step 67900, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.673 sec/batch; 49h:28m:38s remains)
2017-12-15 22:49:51.597564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9283152 -3.8446095 -3.6647563 -3.4281151 -3.8741918 -4.3151979 -4.3916492 -3.963752 -3.726207 -3.7263093 -3.5221694 -5.6194019 -6.8847704 -8.8460712 -8.6996117][-5.0326037 -4.034358 -3.0358255 -3.001061 -4.197011 -4.8728762 -5.1375618 -4.5508032 -3.4044693 -2.8329608 -2.9774809 -5.2248845 -6.5034351 -7.7643228 -7.7780571][-2.4644763 -3.4837067 -4.3987379 -4.6409597 -5.3758464 -5.5508156 -5.39081 -5.0699077 -4.1256776 -3.3608246 -2.7171979 -5.1714363 -6.9462237 -8.0793648 -7.9348764][-2.3646939 -2.8379412 -3.7009947 -4.4150352 -5.46902 -4.9600687 -3.2795637 -3.3890421 -2.9888587 -3.4698207 -3.5709636 -5.2439141 -6.6926994 -8.2778692 -8.3292284][-2.2251182 -3.2198553 -4.6956511 -4.7281547 -4.6928177 -4.2683825 -2.9109735 -2.3727293 -1.6707845 -2.315886 -3.2005951 -5.6835413 -7.3501778 -9.238018 -9.0689964][-3.1066837 -3.4371037 -4.3796015 -4.2697234 -3.6273961 -1.6213942 0.5024271 1.5930634 1.5456328 0.12973261 -1.225451 -3.9069791 -5.7930789 -7.9829178 -8.6653271][-4.7005091 -4.4641361 -4.4558163 -2.6580439 -1.1312017 1.2393122 3.4414878 4.1230588 4.0609231 1.390089 -0.62435246 -3.7398152 -5.345499 -7.1984158 -7.0966139][-4.2520018 -4.3517718 -3.733603 -1.6539984 0.4829731 2.7174859 4.4154878 3.8185134 3.2704234 2.549561 1.1263113 -2.5578783 -3.9275951 -5.8930869 -6.2798624][-2.8798988 -2.5054741 -2.2562864 -0.69980907 0.47920561 2.5204015 3.4945149 3.040482 2.2909055 0.76305485 -0.20801926 -3.3893085 -5.7616405 -7.8956375 -8.0233288][-2.4281795 -2.3765783 -2.2884023 -1.6654377 -0.80593157 -0.10898733 1.3391838 1.910038 1.4212632 0.46443319 -1.3283396 -5.0681095 -6.3773479 -8.9744768 -9.6766653][-3.3213272 -3.2993317 -3.2617464 -2.8680625 -2.0332181 -0.41056919 -0.15592432 -0.51898909 -1.2793489 -3.0656564 -4.1806908 -7.7068229 -10.525953 -12.127049 -11.103291][-5.1794028 -5.0604486 -4.9632616 -4.9569235 -4.6573067 -4.4723449 -3.0123119 -2.8926878 -3.9259229 -4.84056 -6.1526227 -9.0018673 -9.6144972 -11.366978 -11.320873][-8.0447073 -7.6088972 -6.5411677 -6.5557876 -6.909934 -6.135716 -5.6727576 -5.5901074 -5.853538 -7.9033279 -8.6396637 -9.5241432 -10.787201 -10.684553 -9.4582024][-8.2463665 -7.1524534 -6.2565165 -5.3485589 -5.1366615 -5.95514 -5.4442673 -5.5770621 -6.2572069 -7.2879543 -8.2672577 -7.8799396 -7.5724053 -8.25307 -7.6362076][-6.6806273 -6.9821081 -6.9092684 -6.3092146 -5.6994834 -5.8811488 -6.4268403 -6.4856243 -5.6011806 -6.1818242 -6.2759724 -6.7921124 -7.0083942 -6.356885 -5.8519511]]...]
INFO - root - 2017-12-15 22:49:58.090383: step 67910, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 48h:36m:52s remains)
INFO - root - 2017-12-15 22:50:04.685251: step 67920, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.684 sec/batch; 50h:16m:37s remains)
INFO - root - 2017-12-15 22:50:11.299825: step 67930, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 49h:24m:43s remains)
INFO - root - 2017-12-15 22:50:17.881506: step 67940, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.642 sec/batch; 47h:11m:09s remains)
INFO - root - 2017-12-15 22:50:24.515903: step 67950, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 48h:34m:46s remains)
INFO - root - 2017-12-15 22:50:31.099942: step 67960, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 50h:09m:32s remains)
INFO - root - 2017-12-15 22:50:37.621655: step 67970, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 46h:44m:12s remains)
INFO - root - 2017-12-15 22:50:44.245044: step 67980, loss = 0.12, batch loss = 0.07 (11.6 examples/sec; 0.687 sec/batch; 50h:28m:06s remains)
INFO - root - 2017-12-15 22:50:50.812272: step 67990, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 48h:20m:00s remains)
INFO - root - 2017-12-15 22:50:57.439744: step 68000, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 48h:07m:22s remains)
2017-12-15 22:50:57.993378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6865053 -6.88467 -8.4006529 -9.2707376 -10.063014 -10.639669 -10.292022 -8.8710976 -8.2508907 -8.6096411 -8.1063042 -10.389852 -11.134798 -13.212816 -13.133717][-6.4367414 -7.3361521 -8.1548891 -9.2342844 -10.38628 -11.199534 -11.198233 -11.299137 -11.042088 -9.95727 -8.6925144 -11.770107 -12.160189 -14.241858 -15.172451][-4.8727388 -7.159853 -9.1886044 -8.7640419 -9.2255182 -10.70437 -11.159098 -10.095562 -9.8865213 -10.531561 -10.202478 -12.396206 -12.320974 -15.069643 -16.053272][-7.4602437 -8.9953594 -9.3568411 -7.8845153 -7.4442329 -7.6933165 -7.84168 -9.3717642 -10.56654 -9.8717937 -8.3911982 -11.999437 -13.491467 -15.842266 -16.247252][-8.595089 -10.312307 -11.595873 -9.322484 -6.894618 -4.348156 -2.3943815 -5.5076637 -9.1994171 -8.7800913 -8.3342714 -11.310816 -12.073919 -15.822916 -17.278929][-10.014193 -10.947393 -11.267573 -8.3252792 -4.896039 -0.33657217 3.1621594 0.51034975 -2.0995576 -5.0868678 -8.6285782 -10.812553 -10.910049 -14.331675 -15.445024][-9.95324 -11.364421 -10.198288 -5.0074687 -0.46350241 3.5491128 6.685451 6.4853797 5.009726 -1.0114732 -6.562099 -9.5367241 -11.585165 -14.628473 -14.63463][-10.062417 -9.83853 -8.8139687 -3.3581707 1.1046538 6.1904492 10.377605 7.9251132 6.3338103 2.1282611 -3.0258782 -8.107254 -10.390152 -13.626135 -15.494764][-8.008111 -8.4320965 -8.5047922 -4.0681477 -0.70527029 5.2316241 9.1687965 7.1914544 6.8033423 1.8166294 -2.9659495 -8.4240551 -12.204449 -14.839783 -15.413746][-5.6910415 -5.899951 -7.4136505 -4.8110328 -2.0801179 2.2988687 4.8154626 4.484179 4.3398356 0.45464325 -2.7702951 -8.7640142 -12.95492 -15.994541 -16.563292][-8.3892889 -9.7128935 -11.166121 -8.8251743 -7.4126697 -4.5311751 -1.819196 -1.1213455 -1.0117121 -3.4964559 -5.2407846 -11.784397 -14.168776 -16.107616 -15.696949][-14.31682 -14.180426 -14.56052 -13.75795 -13.448912 -10.397968 -8.2747231 -8.4538431 -7.6783104 -9.0352936 -10.265062 -13.131255 -14.184418 -16.117416 -15.305338][-15.904417 -15.583866 -15.863211 -16.31226 -16.002846 -13.121019 -12.077535 -12.374878 -12.45058 -11.597656 -11.604094 -13.820379 -13.693995 -13.760616 -12.696898][-13.690425 -13.259466 -13.382375 -12.836489 -13.60212 -14.134871 -13.185032 -11.264252 -10.923159 -12.197891 -12.803489 -12.981415 -12.423691 -13.116943 -11.617167][-9.353776 -9.5477028 -9.2336626 -8.4914818 -8.5920372 -8.7849312 -9.1424 -9.3851566 -10.30958 -10.291456 -10.317663 -12.306928 -12.758496 -12.145435 -12.714981]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 22:51:04.636487: step 68010, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 49h:22m:05s remains)
INFO - root - 2017-12-15 22:51:11.261747: step 68020, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 47h:53m:24s remains)
INFO - root - 2017-12-15 22:51:17.884378: step 68030, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 48h:14m:28s remains)
INFO - root - 2017-12-15 22:51:24.508554: step 68040, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 48h:23m:14s remains)
INFO - root - 2017-12-15 22:51:31.100162: step 68050, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 49h:28m:30s remains)
INFO - root - 2017-12-15 22:51:37.754618: step 68060, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 48h:41m:52s remains)
INFO - root - 2017-12-15 22:51:44.348022: step 68070, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 49h:41m:11s remains)
INFO - root - 2017-12-15 22:51:51.014473: step 68080, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 48h:57m:27s remains)
INFO - root - 2017-12-15 22:51:57.572810: step 68090, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 48h:02m:59s remains)
INFO - root - 2017-12-15 22:52:04.156758: step 68100, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 48h:43m:31s remains)
2017-12-15 22:52:04.662556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9562292 -5.293663 -5.6562724 -6.3978662 -7.4830656 -6.8765583 -7.117341 -6.7854013 -5.9815545 -5.0435357 -4.0465503 -5.9913449 -8.611454 -9.8440685 -10.357445][-4.3634586 -5.0255232 -4.9515228 -6.0928259 -6.8932276 -7.0363545 -7.2312036 -7.6110983 -7.9419785 -6.9050555 -6.2892132 -8.23585 -10.59424 -11.914375 -11.972795][-2.9456403 -4.3949728 -5.2123575 -5.2853413 -6.0971479 -6.62041 -6.4742403 -6.4838867 -6.4846516 -6.8889618 -7.4414 -9.8702126 -12.83415 -13.390034 -13.716812][-3.0255194 -3.872067 -3.8302286 -3.5605915 -3.6490512 -2.7582014 -1.9379656 -3.9279866 -5.2698112 -5.4277263 -5.2505441 -7.5337868 -11.371332 -13.099911 -13.451374][-4.4632459 -5.4044 -5.028625 -3.2260296 -2.1246502 -0.86324835 0.16869164 -0.86402225 -1.923027 -2.7271409 -3.0284202 -5.1474357 -8.1978731 -10.174849 -11.651606][-5.7640562 -5.7253923 -3.903194 -1.8862176 -0.22018385 1.2572017 2.1679583 2.7875466 2.5865798 0.88780832 -0.95830917 -3.8334801 -7.0093136 -8.2667866 -9.25501][-5.8625674 -5.2658367 -3.9462385 -1.6345339 0.94472027 3.06113 4.987658 5.56704 5.4233289 3.2327514 0.9187746 -1.8465128 -4.9106665 -6.6180596 -8.8113689][-4.6623888 -3.9899554 -2.9453635 -0.74556875 1.3951135 4.0826344 6.3108964 6.2444777 5.1740289 3.6754642 3.0297627 -0.67623043 -5.2106843 -6.9494791 -8.0336914][-2.571214 -2.8665676 -2.0893538 0.043999195 1.5513191 3.470921 4.35809 3.8483043 2.5559 2.3525596 2.0261402 -1.1607494 -4.0924888 -6.9756708 -9.1286688][-0.2550211 -1.5753822 -1.5459566 -0.16156721 0.34058523 0.6622839 0.73549366 0.72097683 0.52453375 1.1831703 0.79921627 -2.5404489 -5.9901567 -8.6647577 -11.10784][-3.9035859 -4.124999 -3.6498508 -3.3627162 -3.2210183 -2.9578001 -2.5705521 -2.6599085 -3.295615 -3.7205787 -4.0343218 -6.02992 -8.0485573 -10.089954 -11.667404][-8.7350674 -7.7766171 -6.930902 -7.0755439 -7.090127 -7.05565 -7.449995 -7.6506119 -7.5151148 -7.0812483 -6.9606571 -8.2833138 -9.4493866 -9.8523846 -10.73339][-11.568657 -10.476145 -9.0672855 -8.7257137 -9.2153368 -9.3206434 -9.40116 -8.650507 -8.3805752 -8.587492 -8.0386467 -8.2288265 -8.27272 -7.9842644 -8.5886364][-9.6795645 -9.142642 -8.4300461 -7.8346033 -7.3240457 -7.6987677 -7.967309 -6.7529759 -5.5940671 -4.7868567 -4.6673989 -4.9329305 -5.310339 -5.9673796 -6.3823881][-7.6226797 -7.5512619 -7.3260813 -6.6643782 -5.9694748 -5.5964565 -5.0695744 -4.8621559 -4.9344158 -4.0341349 -3.8604171 -5.0305691 -6.2664289 -6.3521934 -6.141932]]...]
INFO - root - 2017-12-15 22:52:11.295802: step 68110, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 48h:04m:03s remains)
INFO - root - 2017-12-15 22:52:17.915172: step 68120, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 48h:24m:23s remains)
INFO - root - 2017-12-15 22:52:24.549565: step 68130, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 48h:17m:52s remains)
INFO - root - 2017-12-15 22:52:31.077217: step 68140, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 48h:34m:18s remains)
INFO - root - 2017-12-15 22:52:37.690749: step 68150, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 49h:16m:43s remains)
INFO - root - 2017-12-15 22:52:44.340551: step 68160, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 48h:02m:50s remains)
INFO - root - 2017-12-15 22:52:50.885833: step 68170, loss = 0.16, batch loss = 0.11 (12.7 examples/sec; 0.631 sec/batch; 46h:18m:51s remains)
INFO - root - 2017-12-15 22:52:57.424280: step 68180, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.637 sec/batch; 46h:46m:48s remains)
INFO - root - 2017-12-15 22:53:04.018579: step 68190, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 49h:05m:54s remains)
INFO - root - 2017-12-15 22:53:10.687034: step 68200, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 50h:18m:19s remains)
2017-12-15 22:53:11.228136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.785358 -4.88656 -3.5724404 -2.5661831 -2.4844379 -2.8489873 -3.1481271 -3.0892093 -2.9465473 -3.0707366 -2.9518306 -4.7402639 -6.8507466 -7.8658509 -7.0360241][-6.3078351 -6.1498146 -5.8272629 -5.177681 -4.9325538 -5.1924224 -5.4738312 -5.798203 -5.9683852 -5.8119187 -5.6496787 -7.4031277 -9.1685581 -9.7756653 -9.30559][-5.3983145 -5.3592143 -4.877491 -4.4956679 -4.8288121 -5.3377094 -5.9200082 -5.6367774 -5.563941 -5.78378 -5.7707357 -7.1764135 -8.9268 -9.5102606 -8.9890633][-5.6571474 -5.2905021 -4.3439875 -3.3239048 -3.062582 -3.2270813 -3.5201352 -3.8751025 -4.2602634 -4.0525737 -3.7871327 -5.3823514 -7.1370945 -7.9812374 -7.6593666][-7.2824388 -6.8544745 -5.5905852 -3.7729297 -2.8474612 -2.1811967 -1.9927869 -2.1577327 -2.3982716 -2.538758 -2.7195277 -3.9476395 -5.4573731 -6.7267418 -6.58293][-8.0269089 -7.2230482 -5.2488022 -2.9635944 -1.732599 -0.57125759 -0.021366119 0.40788794 0.86027765 0.59141254 0.040193081 -1.8147354 -4.12099 -4.6867981 -4.2021179][-8.6170244 -8.1352663 -5.7174087 -1.7672784 0.86528349 2.8726621 3.5681787 3.6925607 4.1164451 3.1200604 1.9872885 -0.21148872 -2.9543245 -4.3816919 -4.6222229][-7.5383983 -7.0883913 -5.5208945 -3.1212077 -0.61126423 2.7140183 4.1784635 4.6039739 5.4374804 5.0107608 3.9814801 0.28609037 -3.647912 -5.2465196 -5.6927376][-5.4040232 -5.4301147 -5.06807 -3.4498858 -1.6703362 0.46165228 1.3729649 2.9902186 4.2497182 3.3009763 2.855927 0.05513525 -3.2406504 -5.6542149 -7.1432247][-3.8433652 -2.9487705 -2.2869503 -1.519196 -0.85999441 0.6267705 1.4720855 2.4435945 2.2994113 2.3611379 2.1762495 -1.2442927 -4.4557891 -6.6595111 -8.06962][-4.8165336 -3.6127923 -2.8142507 -3.1913998 -3.3471737 -2.5835662 -2.2851276 -1.8177464 -1.8788714 -1.9962296 -2.4371667 -4.8652878 -6.5090103 -7.8748274 -8.0443316][-10.162718 -8.4097509 -7.0185528 -6.7257152 -7.2321749 -7.9868956 -7.9186563 -7.9388185 -8.0703478 -7.6222115 -7.6473866 -8.4680109 -8.515234 -8.509201 -7.7800207][-11.608624 -10.044056 -9.4699669 -9.8744612 -10.796294 -10.984694 -10.659658 -10.001383 -9.3223715 -9.133276 -8.6146984 -7.6843052 -7.8159046 -6.4107447 -5.0871596][-8.9782848 -7.4306946 -6.6242533 -6.388742 -6.9548454 -7.7895017 -8.169611 -7.746953 -6.890985 -6.327683 -5.609374 -4.8502641 -4.4414539 -3.2408893 -3.1059971][-7.4115047 -6.4569368 -5.6684752 -4.3791356 -2.9020364 -2.7898436 -3.4699774 -3.43286 -3.2575052 -3.2157855 -2.8855188 -4.2879152 -5.4013581 -4.3483753 -3.3753171]]...]
INFO - root - 2017-12-15 22:53:17.777060: step 68210, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 47h:00m:18s remains)
INFO - root - 2017-12-15 22:53:24.407274: step 68220, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 47h:38m:09s remains)
INFO - root - 2017-12-15 22:53:31.028617: step 68230, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 48h:07m:00s remains)
INFO - root - 2017-12-15 22:53:37.768456: step 68240, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 50h:39m:15s remains)
INFO - root - 2017-12-15 22:53:44.368798: step 68250, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 50h:10m:13s remains)
INFO - root - 2017-12-15 22:53:50.913085: step 68260, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 47h:17m:28s remains)
INFO - root - 2017-12-15 22:53:57.481462: step 68270, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.677 sec/batch; 49h:42m:27s remains)
INFO - root - 2017-12-15 22:54:04.085929: step 68280, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 49h:24m:03s remains)
INFO - root - 2017-12-15 22:54:10.642327: step 68290, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 48h:20m:19s remains)
INFO - root - 2017-12-15 22:54:17.181320: step 68300, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 48h:41m:35s remains)
2017-12-15 22:54:17.697272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5547006 -1.9329553 -1.4523296 -1.4248548 -2.3339112 -2.0386553 -1.1939263 -0.94376945 -1.8906784 -2.56806 -2.91023 -3.2401822 -5.7628517 -6.5558538 -4.3290343][-1.8207865 -0.41703272 -0.679646 -0.45764828 -0.97765493 -1.1216025 -1.8443222 -2.4793279 -2.7541478 -3.5763254 -4.6408043 -5.6604414 -7.2347593 -7.8243394 -6.9233665][-0.96421146 0.12285566 -0.0966773 0.20286512 -1.8780119 -2.0820432 -2.1326246 -2.114949 -3.1888614 -4.401041 -4.51375 -4.1061144 -6.3177371 -8.1959343 -7.0316277][-2.0615833 -1.283433 -1.6675229 -1.8148065 -2.1883795 -2.1889982 -2.9949687 -2.9386165 -3.3173118 -3.7579303 -4.2744694 -4.6156626 -6.0413251 -6.7465472 -6.2639632][-3.4500499 -2.7640188 -3.9563532 -3.3674889 -3.3568065 -1.8733463 -0.72077322 -0.73423815 -1.7774205 -2.3480124 -2.7277625 -3.7447643 -5.5522909 -6.17845 -5.7116513][-3.6726081 -2.8520775 -3.548861 -2.8801811 -2.1768022 -0.73693895 0.77517223 1.5689912 1.0062375 -0.73072672 -1.1946325 -1.6226444 -3.3992507 -5.3772373 -4.8353982][-4.3517661 -3.0991504 -2.8130453 -1.171195 -1.3412609 0.11398363 2.1892943 3.61099 3.478621 1.0994596 -1.1924343 -2.2893572 -3.686969 -4.356019 -3.1498263][-5.3009825 -4.1840672 -2.0542705 -0.25403309 -0.59548092 0.46734 2.2219329 3.4138217 3.4180112 1.5252814 0.22314215 -2.082232 -4.6960993 -5.7991323 -4.8397746][-5.8713574 -3.6403 -2.1794305 -1.4590354 -1.4496026 -0.76193142 -0.36168528 1.0094204 2.580503 1.9871755 0.5885849 -1.9330392 -5.1687779 -7.14021 -5.641264][-7.30584 -6.1413407 -4.8959317 -2.3467395 -1.8555498 -0.40648365 0.48734379 0.97536278 1.6496539 0.77495193 -0.48082209 -2.9325569 -6.0115318 -7.6892104 -7.3529196][-8.8501835 -8.92497 -7.5787425 -5.5934734 -4.7087755 -3.7971296 -2.2817359 0.024477959 0.42970085 -0.96151733 -2.1083992 -4.9982228 -8.1956215 -9.2795315 -7.0603309][-9.72603 -8.8345585 -9.3239641 -8.6464891 -7.08965 -5.6924906 -4.2411828 -2.6737847 -1.6388993 -2.3413949 -3.5173004 -6.215775 -7.4684033 -8.7155037 -7.9743495][-9.7985764 -9.0485582 -8.4308186 -7.3319683 -6.7975192 -5.2664461 -3.82992 -3.0415938 -2.734251 -3.3743055 -4.6836944 -6.1342793 -7.7363768 -7.8407545 -5.8379712][-6.640285 -6.4504666 -5.7105942 -5.2282887 -4.8007655 -4.4520903 -3.4635251 -1.983938 -1.3622122 -2.6649346 -4.1990948 -4.7626076 -5.1836915 -5.8878765 -5.2036405][-6.1692586 -4.6109953 -3.1980362 -2.927372 -2.1812577 -3.3473718 -3.5140247 -2.2505937 -1.7439837 -2.2409768 -3.0754309 -4.3834748 -5.7247043 -5.6333575 -5.4962444]]...]
INFO - root - 2017-12-15 22:54:24.199344: step 68310, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 49h:16m:06s remains)
INFO - root - 2017-12-15 22:54:30.816027: step 68320, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.648 sec/batch; 47h:31m:54s remains)
INFO - root - 2017-12-15 22:54:37.384701: step 68330, loss = 0.15, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 46h:39m:27s remains)
INFO - root - 2017-12-15 22:54:43.901565: step 68340, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 47h:16m:36s remains)
INFO - root - 2017-12-15 22:54:50.463301: step 68350, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 47h:22m:15s remains)
INFO - root - 2017-12-15 22:54:57.020844: step 68360, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 49h:48m:00s remains)
INFO - root - 2017-12-15 22:55:03.679740: step 68370, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 49h:23m:24s remains)
INFO - root - 2017-12-15 22:55:10.257554: step 68380, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 47h:54m:16s remains)
INFO - root - 2017-12-15 22:55:16.856556: step 68390, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 49h:04m:02s remains)
INFO - root - 2017-12-15 22:55:23.512687: step 68400, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 49h:48m:41s remains)
2017-12-15 22:55:24.055188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7698176 -2.043786 -2.1337619 -1.137238 -2.346159 -3.7281966 -3.6645312 -3.4206018 -2.7366955 -1.4841146 -0.30489063 -1.7238958 -3.4618154 -5.0607519 -4.2502327][-2.449507 -2.2015836 -1.6033988 -1.0873775 -1.9980881 -2.8900833 -2.6391072 -2.7176757 -2.6047013 -2.1024835 -0.969749 -1.0555921 -2.2059138 -3.6810219 -3.6164412][-2.2229106 -1.7479391 -2.5063686 -2.6891868 -4.280592 -5.36125 -4.1172447 -2.6249478 -1.7084455 -1.967453 -2.283489 -3.6516356 -5.7664042 -6.0108676 -5.5222363][-5.213429 -4.4035149 -3.9376636 -4.2952161 -6.0607171 -6.4117641 -5.9243126 -4.7334485 -3.4571736 -3.5004926 -3.2273934 -5.2083197 -7.81891 -8.2522392 -8.3011093][-4.6623211 -5.392282 -5.8116841 -5.1901054 -5.6201611 -4.7886257 -3.6270344 -3.5038397 -3.5341592 -3.7238536 -3.739994 -5.7568512 -8.3648319 -9.8825436 -9.7058573][-5.2110562 -4.3550243 -4.0726261 -4.1055627 -3.9511938 -1.9286656 0.25403738 0.679719 0.23076105 -1.9401309 -3.3106425 -5.5750723 -7.6004252 -9.284833 -9.9236012][-4.8371983 -3.722605 -3.069973 -2.0118632 -2.0887086 -0.34153461 2.2142591 3.7125096 3.9874444 0.885767 -1.9406033 -4.298418 -6.9467192 -9.1869049 -9.0051832][-4.9439273 -3.2642522 -2.2338629 -1.0164247 -0.53994656 1.8299193 3.8043027 5.0775771 5.7647576 2.2210231 -1.0010486 -4.2808456 -7.3137503 -8.4305382 -7.1017513][-4.2686968 -2.4418752 -1.3388133 0.15232229 0.39523029 2.0137439 3.4828935 4.3695159 4.3940072 1.2131448 -1.8377166 -5.496541 -8.45525 -8.7207851 -6.9902072][-6.1206355 -4.0744414 -2.6706939 -1.1981244 -0.025379181 1.0004258 1.7325706 2.6692204 2.7606025 0.34182072 -2.5332756 -6.6608744 -10.473207 -11.267045 -8.8489819][-11.259409 -8.79463 -6.3758903 -4.3585467 -3.7618814 -2.7256105 -2.2458587 -2.2856286 -2.4956086 -3.5697782 -5.4086161 -8.68854 -11.705549 -12.235865 -10.333608][-16.716751 -14.775311 -11.406012 -8.6094494 -7.6518731 -6.9575353 -7.5607119 -7.6186242 -7.1288347 -6.8907952 -7.3253374 -9.99384 -11.617881 -12.032375 -10.434638][-15.76033 -14.129637 -12.03944 -10.647739 -8.9590912 -7.8493223 -7.9385266 -8.8315849 -9.41374 -9.1131725 -8.7840624 -8.7111273 -8.6595688 -8.2330647 -6.9951286][-12.588282 -11.796953 -10.95247 -9.6866417 -9.1218319 -8.8828316 -7.9936695 -7.64612 -8.4609766 -8.4653645 -8.153285 -6.9811573 -6.0091467 -5.5054359 -5.4085875][-10.240446 -10.264975 -9.1776762 -8.09573 -7.252809 -6.8420382 -6.0678496 -5.2730002 -5.5176888 -6.2662926 -6.86802 -7.2953138 -6.9194455 -6.607233 -6.7944093]]...]
INFO - root - 2017-12-15 22:55:30.610757: step 68410, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.666 sec/batch; 48h:49m:20s remains)
INFO - root - 2017-12-15 22:55:37.262082: step 68420, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.695 sec/batch; 50h:57m:44s remains)
INFO - root - 2017-12-15 22:55:43.850764: step 68430, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 48h:28m:14s remains)
INFO - root - 2017-12-15 22:55:50.483577: step 68440, loss = 0.13, batch loss = 0.08 (11.5 examples/sec; 0.696 sec/batch; 51h:04m:42s remains)
INFO - root - 2017-12-15 22:55:57.141151: step 68450, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 48h:27m:19s remains)
INFO - root - 2017-12-15 22:56:03.734748: step 68460, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 47h:22m:36s remains)
INFO - root - 2017-12-15 22:56:10.301150: step 68470, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 47h:10m:04s remains)
INFO - root - 2017-12-15 22:56:16.922599: step 68480, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 47h:09m:08s remains)
INFO - root - 2017-12-15 22:56:23.527291: step 68490, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 49h:40m:33s remains)
INFO - root - 2017-12-15 22:56:30.094349: step 68500, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 49h:26m:26s remains)
2017-12-15 22:56:30.612079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1096363 -5.1116071 -4.5823669 -3.9903307 -4.5649238 -4.3623462 -4.0156016 -3.2753747 -2.8310175 -3.7363851 -5.1673203 -8.3159466 -10.651438 -11.536521 -10.055861][-5.0633988 -4.377367 -3.7419033 -3.6353958 -4.0001931 -4.1386909 -3.6025085 -2.6197302 -2.6477776 -3.1038694 -3.7177856 -7.3793812 -10.634998 -12.10758 -11.371659][-4.0871849 -5.0140562 -5.328289 -4.0054688 -4.1725521 -4.4385605 -4.322906 -3.3987615 -3.0362611 -3.3361921 -4.4710355 -8.5926781 -10.312293 -11.443977 -10.922098][-5.3859258 -6.0784903 -5.9892478 -5.1122885 -5.0502582 -4.0543156 -3.4140878 -3.3567312 -3.8077922 -3.9461255 -4.4027877 -8.0967846 -10.908052 -12.084925 -11.096152][-5.4235535 -7.3485775 -8.1086731 -6.7492628 -5.8823442 -3.9603324 -2.7735417 -2.8226037 -2.9105792 -3.2274828 -4.2734761 -7.6150265 -9.8891659 -12.217085 -11.929705][-6.9465461 -7.3738618 -7.3962593 -6.0893903 -4.6224422 -1.830426 0.13635874 0.43952847 0.13475609 -0.96899462 -3.2879889 -6.6708059 -8.6270313 -11.351089 -12.07375][-8.5641422 -8.9329329 -7.8758121 -5.2419095 -2.6981223 0.89028835 4.435966 4.5210528 3.7780442 1.2229481 -2.2276289 -5.1946545 -8.0776634 -10.324286 -11.361282][-8.8242121 -8.3062038 -7.8631973 -4.440114 -0.95261431 2.5151105 6.0107274 7.5429311 7.1892848 4.3792958 0.7328763 -4.6963329 -8.3723316 -9.51656 -10.139489][-7.7181692 -7.1191435 -5.9694352 -3.1031332 -0.90036345 2.4597745 6.0862193 6.5333581 6.2022958 2.8180032 -0.77495813 -5.0881076 -9.1134949 -10.838581 -10.245173][-5.1018939 -5.11764 -5.4171629 -2.7292297 -1.2384372 1.0922432 3.0735755 3.5196242 2.8510051 0.18663406 -2.4869416 -7.2115459 -10.092588 -11.553272 -11.658482][-8.49507 -8.07349 -7.710669 -6.5996528 -6.2132454 -3.8907206 -1.9523277 -2.0078607 -2.8694828 -4.9138489 -7.4030743 -11.462282 -13.647309 -13.81081 -12.146124][-11.700873 -11.038788 -11.037207 -9.699667 -9.8036575 -9.1209822 -8.1085825 -8.2771873 -9.54871 -9.9139376 -10.841494 -12.78293 -13.939577 -13.649565 -12.407515][-13.455115 -12.945708 -11.423056 -11.216549 -10.907804 -10.149903 -10.559139 -10.416435 -10.980622 -11.432973 -12.255116 -13.104422 -13.297349 -11.595146 -9.2471542][-10.310074 -10.451941 -9.2485352 -8.874382 -8.8558807 -9.4456253 -10.184618 -9.9036617 -9.9832048 -10.316999 -10.328646 -9.0801525 -8.736064 -8.46844 -8.02692][-7.0245647 -7.40808 -6.872478 -6.6565886 -6.5951128 -6.8003364 -7.4575834 -7.3929567 -8.0033159 -7.4364781 -7.3740826 -8.1476974 -7.8935909 -7.915072 -8.2583885]]...]
INFO - root - 2017-12-15 22:56:37.180875: step 68510, loss = 0.22, batch loss = 0.18 (11.5 examples/sec; 0.693 sec/batch; 50h:49m:50s remains)
INFO - root - 2017-12-15 22:56:43.811684: step 68520, loss = 0.16, batch loss = 0.11 (12.7 examples/sec; 0.630 sec/batch; 46h:10m:07s remains)
INFO - root - 2017-12-15 22:56:50.333838: step 68530, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.653 sec/batch; 47h:51m:45s remains)
INFO - root - 2017-12-15 22:56:56.913753: step 68540, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 47h:05m:23s remains)
INFO - root - 2017-12-15 22:57:03.444861: step 68550, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 48h:51m:41s remains)
INFO - root - 2017-12-15 22:57:10.067045: step 68560, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 47h:56m:35s remains)
INFO - root - 2017-12-15 22:57:16.667979: step 68570, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.643 sec/batch; 47h:06m:31s remains)
INFO - root - 2017-12-15 22:57:23.202770: step 68580, loss = 0.18, batch loss = 0.14 (11.6 examples/sec; 0.689 sec/batch; 50h:29m:48s remains)
INFO - root - 2017-12-15 22:57:29.741795: step 68590, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 48h:30m:00s remains)
INFO - root - 2017-12-15 22:57:36.435813: step 68600, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 48h:52m:45s remains)
2017-12-15 22:57:36.964241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.956624 -6.9248619 -6.7218966 -6.3455844 -7.1252093 -7.8210506 -7.371273 -6.5662785 -5.4188647 -4.76368 -4.3296685 -4.6888328 -5.6809669 -7.51674 -7.511641][-4.7471519 -6.1994953 -7.3773146 -7.3751874 -6.8654084 -7.2406774 -6.7337284 -5.3708358 -4.7778063 -4.0705376 -3.4750478 -3.7035253 -5.181531 -6.2287951 -6.4945211][-2.6371307 -3.2007103 -4.2598042 -5.23039 -5.5958524 -5.5463567 -4.5198832 -4.0053077 -3.4079597 -2.5031335 -2.6849139 -3.0301173 -4.0494423 -5.8946958 -6.2959118][-3.2943921 -3.4621224 -4.0701046 -4.7588077 -5.6687908 -4.626689 -2.2445714 -1.7647088 -2.235285 -2.6143374 -3.1283247 -3.2858737 -4.2650089 -6.1032147 -6.9470825][-4.4635153 -5.6134658 -6.2830119 -5.8401365 -6.4712162 -4.3184237 -2.0458157 -1.2189336 -0.65387726 -0.86013508 -2.4509175 -2.82696 -4.7135992 -7.5763865 -8.8245926][-7.5868335 -5.6233015 -4.8363914 -4.9771366 -4.1388259 -0.44904327 3.1383786 3.5227695 3.1090331 1.2716761 -1.010788 -1.9027274 -4.3957019 -7.0491853 -8.3997593][-6.8178082 -6.4025335 -5.23824 -3.8369126 -2.3857079 0.97238493 5.2890935 7.3719115 7.0516725 3.1427903 -0.037225723 -1.0683289 -3.3804605 -6.0053897 -7.151145][-6.3340559 -6.4056158 -6.6193776 -4.3088121 -2.2599027 1.0189157 4.3628392 6.3695769 6.9503255 3.8569779 0.51166439 -1.9214196 -4.5349579 -6.7671108 -7.72999][-6.4413881 -6.8640485 -6.6317225 -4.6366367 -3.7743187 -1.3239527 0.89229584 2.2642813 2.7867513 1.2131138 -0.17664242 -2.6707313 -6.0420523 -8.10682 -9.02509][-7.1848383 -7.5036149 -7.0134239 -4.9188709 -4.2354374 -2.9518192 -1.8063221 -0.12631893 0.40684843 -1.0024533 -3.0189579 -5.8079314 -8.3183966 -10.431786 -10.159933][-10.79339 -10.074774 -8.56672 -7.487494 -6.2090406 -4.91529 -5.0066953 -4.5016894 -4.0255527 -3.685014 -4.7644119 -7.2008085 -10.083326 -11.290951 -9.9645386][-12.227551 -11.416584 -10.156752 -8.2277107 -7.0582151 -5.2374249 -4.3649678 -5.1769147 -5.8015132 -6.5447025 -7.1785259 -8.2891283 -9.00853 -9.4820976 -7.9952841][-10.783035 -10.319689 -8.5577154 -7.3853312 -6.5853491 -5.1317463 -3.9992089 -3.8753815 -4.8256779 -5.8169928 -6.14476 -7.0001483 -7.0822716 -7.6758618 -5.9850049][-9.0922594 -8.459589 -6.858201 -6.7726626 -6.00132 -4.9607968 -4.7755933 -4.1014853 -3.8192096 -4.7150545 -5.7499928 -5.9151721 -5.2188678 -5.3950033 -4.7975612][-6.2160635 -5.7316961 -4.6952453 -3.3518469 -2.6689835 -3.2264128 -2.6641662 -2.4160583 -3.2955565 -3.9309549 -4.19191 -5.2688918 -5.2972093 -5.2788625 -5.7338877]]...]
INFO - root - 2017-12-15 22:57:43.551673: step 68610, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 47h:52m:59s remains)
INFO - root - 2017-12-15 22:57:50.157228: step 68620, loss = 0.19, batch loss = 0.14 (11.7 examples/sec; 0.684 sec/batch; 50h:07m:22s remains)
INFO - root - 2017-12-15 22:57:56.737265: step 68630, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 49h:21m:20s remains)
INFO - root - 2017-12-15 22:58:03.261049: step 68640, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 48h:16m:51s remains)
INFO - root - 2017-12-15 22:58:09.883817: step 68650, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 48h:59m:19s remains)
INFO - root - 2017-12-15 22:58:16.463874: step 68660, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 47h:59m:19s remains)
INFO - root - 2017-12-15 22:58:23.068349: step 68670, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.685 sec/batch; 50h:11m:03s remains)
INFO - root - 2017-12-15 22:58:29.735320: step 68680, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 48h:42m:28s remains)
INFO - root - 2017-12-15 22:58:36.338853: step 68690, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 46h:51m:05s remains)
INFO - root - 2017-12-15 22:58:42.917718: step 68700, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 46h:57m:03s remains)
2017-12-15 22:58:43.452233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1281838 -5.3910961 -4.7486386 -3.8290949 -3.7887874 -4.2773275 -4.8523917 -4.6512909 -4.2330332 -3.8700094 -3.6257212 -6.3905783 -9.1831894 -9.4116631 -8.7812529][-4.5981393 -5.3228011 -5.6333189 -5.7188845 -6.0914989 -6.6802754 -7.8125162 -8.5023508 -8.3839588 -7.2626262 -6.4306688 -8.684165 -10.557899 -10.140366 -9.3355722][-3.4399784 -5.4278827 -7.1264315 -7.1052918 -7.0834637 -7.4749408 -7.6749859 -8.1953678 -8.4541187 -7.9876633 -7.3050466 -9.3632145 -11.781601 -11.747583 -10.914579][-4.9283552 -5.8553624 -6.4405222 -6.3900995 -6.0373845 -5.7162056 -6.1434221 -7.0156441 -7.506505 -7.2956791 -6.9851165 -9.527771 -12.013646 -11.804365 -11.250776][-6.7671223 -7.9174495 -7.7877107 -6.2601843 -4.4530606 -2.6059513 -2.2481532 -3.6886582 -5.1277232 -5.5382848 -5.731133 -7.789238 -10.139219 -11.227964 -11.096985][-8.0665836 -8.6108627 -7.6077304 -4.8208027 -1.9238513 0.61135578 2.3355966 1.8175054 -0.022041798 -1.8082535 -3.348722 -5.9573059 -8.3385525 -8.98337 -8.8274527][-9.3664427 -9.66456 -7.7390652 -3.8309581 -0.46551514 2.6995769 5.6057343 5.4176412 3.7349677 0.98802614 -1.8741648 -4.4904847 -6.8155384 -6.7315044 -7.107214][-10.059052 -9.8866119 -7.5222597 -3.0212677 0.37526798 4.0564237 6.9108348 5.9287648 4.3791633 1.9062638 -0.9589057 -4.5845175 -7.0352163 -6.6527743 -6.801651][-8.805747 -7.7828436 -6.1265359 -3.0088396 -0.59341 2.2900653 4.5050778 4.7529292 4.4999452 1.8756123 -0.86147738 -4.4158497 -7.46196 -7.3559089 -7.6539459][-7.3446136 -6.7245378 -5.7707095 -3.3939483 -2.5421574 -0.71417236 0.8250351 1.5408363 2.3805547 1.3922043 -0.07779932 -4.0643611 -7.6004925 -8.3204556 -9.13744][-9.8456287 -8.97402 -7.5229263 -5.6424055 -5.107965 -4.6936703 -4.4712257 -4.0623732 -3.2555032 -3.1319191 -3.135448 -7.0890665 -9.5309734 -9.8621082 -10.44964][-12.244025 -11.659759 -10.173943 -8.563365 -7.963213 -7.2288623 -7.8496885 -8.3890038 -7.5066867 -6.6053982 -6.2509594 -8.1427107 -9.563921 -10.868868 -12.228262][-13.604972 -12.186403 -10.922197 -9.6117373 -9.3892059 -9.1536016 -9.0849085 -9.02657 -8.8987017 -8.3922577 -7.6437407 -8.1324005 -8.23986 -8.6018353 -8.9364862][-12.515429 -11.591524 -11.05614 -9.2630749 -8.5332031 -9.1503515 -9.38567 -8.8742342 -8.187273 -7.4011111 -7.3283272 -7.02405 -6.3149519 -6.2728882 -5.8566341][-9.7824774 -9.59349 -8.6080532 -7.5429392 -6.3936028 -6.292767 -5.9766574 -6.2903843 -6.6564212 -6.5320282 -6.5128646 -7.1679797 -7.5364246 -7.3116751 -6.6066146]]...]
INFO - root - 2017-12-15 22:58:50.033982: step 68710, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 48h:31m:09s remains)
INFO - root - 2017-12-15 22:58:56.627487: step 68720, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 48h:26m:46s remains)
INFO - root - 2017-12-15 22:59:03.217364: step 68730, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 47h:38m:29s remains)
INFO - root - 2017-12-15 22:59:09.843447: step 68740, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 48h:13m:34s remains)
INFO - root - 2017-12-15 22:59:16.468604: step 68750, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 46h:56m:50s remains)
INFO - root - 2017-12-15 22:59:23.068484: step 68760, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 48h:38m:37s remains)
INFO - root - 2017-12-15 22:59:29.664251: step 68770, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 49h:26m:42s remains)
INFO - root - 2017-12-15 22:59:36.291875: step 68780, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 47h:20m:17s remains)
INFO - root - 2017-12-15 22:59:42.934126: step 68790, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 49h:17m:06s remains)
INFO - root - 2017-12-15 22:59:49.423243: step 68800, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.639 sec/batch; 46h:48m:47s remains)
2017-12-15 22:59:49.964487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8501363 -5.6808038 -4.8769975 -4.4236016 -5.4189682 -6.27717 -6.9977574 -6.3312235 -5.8848858 -5.2579126 -4.8982363 -5.1961255 -5.9269052 -4.350719 -2.7390516][-5.7401514 -4.1858997 -3.367141 -2.9592974 -3.57919 -5.0177665 -5.4907508 -5.0487833 -4.1505823 -3.038131 -2.4458592 -3.3176591 -4.5849833 -3.6256123 -2.7578831][-3.2241488 -2.7411788 -3.0468681 -2.1350133 -2.8611405 -3.4991546 -3.8355317 -2.8779438 -1.7046123 -0.97501707 0.19206667 -0.91011286 -2.238364 -2.1239939 -2.1420848][-4.6684713 -3.5377922 -2.3796434 -3.216291 -4.6875448 -4.2546892 -3.9604559 -3.2083368 -2.4542542 -1.6243172 -1.3102136 -3.0027668 -4.4107132 -4.1997256 -3.2658172][-6.0513253 -5.8489666 -5.1770697 -3.7528429 -2.4625487 -1.2750387 -0.91478777 -1.086947 -1.0299945 -0.562942 -1.3517594 -3.3474627 -5.2171845 -5.1510539 -3.5852771][-7.5877309 -6.3642936 -5.1023936 -2.3246117 0.65733385 2.1039824 3.2323022 2.5383182 1.9332132 0.017325878 -1.5014429 -3.0976825 -5.3052373 -5.083437 -3.3175292][-9.7994785 -7.9209948 -5.1447158 -1.4237671 1.038115 4.4620795 7.5501008 5.9646287 3.9462686 1.0211215 -0.69254446 -3.1529877 -5.8972259 -5.0218687 -4.0191097][-9.4978342 -8.3208036 -6.738493 -2.2658672 0.80114746 4.3879237 7.2876887 6.0409532 4.4929233 1.6436992 -0.6263113 -3.8546324 -6.4709988 -5.9903398 -4.93901][-9.8417444 -8.2567091 -6.6353779 -3.4938674 -1.3585758 2.2297621 4.36033 3.9398913 2.9385333 -0.30421162 -2.4315562 -5.6579943 -7.931921 -6.9994192 -5.5543103][-10.092299 -9.095602 -8.0921488 -5.0166473 -2.960701 -0.15103769 1.6923318 1.6690416 0.16248989 -2.291414 -4.1278839 -6.7346687 -7.9779243 -7.5475712 -6.6836205][-12.414732 -12.066283 -10.381252 -7.8853936 -6.0239463 -4.3955183 -2.9431682 -2.6192923 -3.3570063 -4.39964 -5.3515344 -8.1503057 -8.5633106 -8.1980247 -7.1286988][-16.613155 -16.157787 -13.785269 -11.847113 -11.544832 -9.1037769 -7.0927644 -6.9921813 -7.2293291 -7.52991 -7.8645411 -8.5940742 -7.9577084 -7.6183963 -6.3922033][-13.603569 -12.703062 -11.474874 -10.610909 -9.9832211 -8.6070862 -8.0548229 -7.5264077 -7.4148474 -7.1022491 -7.1072483 -7.1211619 -6.2207694 -5.1326628 -3.796771][-10.357827 -9.628005 -8.3132467 -7.2834 -6.9829111 -7.4547434 -7.852 -7.0448456 -6.8360829 -6.8263435 -6.8358545 -6.0106158 -5.0857782 -3.8847938 -2.4278097][-7.59441 -6.4030895 -4.905426 -3.5638556 -3.378597 -3.1727874 -3.2250679 -3.59957 -4.1744804 -4.0501113 -4.393856 -4.7981267 -4.8148642 -4.8803225 -4.7417808]]...]
INFO - root - 2017-12-15 22:59:56.595850: step 68810, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 48h:43m:14s remains)
INFO - root - 2017-12-15 23:00:03.259637: step 68820, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 49h:35m:30s remains)
INFO - root - 2017-12-15 23:00:09.907377: step 68830, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 48h:02m:29s remains)
INFO - root - 2017-12-15 23:00:16.575576: step 68840, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 49h:36m:47s remains)
INFO - root - 2017-12-15 23:00:23.111197: step 68850, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.647 sec/batch; 47h:21m:45s remains)
INFO - root - 2017-12-15 23:00:29.578917: step 68860, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 48h:33m:18s remains)
INFO - root - 2017-12-15 23:00:36.164177: step 68870, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 47h:14m:09s remains)
INFO - root - 2017-12-15 23:00:42.839294: step 68880, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 47h:47m:13s remains)
INFO - root - 2017-12-15 23:00:49.474420: step 68890, loss = 0.15, batch loss = 0.11 (11.4 examples/sec; 0.701 sec/batch; 51h:19m:40s remains)
INFO - root - 2017-12-15 23:00:56.099731: step 68900, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 47h:10m:26s remains)
2017-12-15 23:00:56.627994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8187056 -6.6205821 -7.8198791 -6.8368316 -7.3610048 -7.4104357 -6.4357233 -5.9144511 -4.62758 -3.7928557 -3.1488707 -5.5649176 -8.7200985 -10.130329 -9.5832272][-3.635859 -3.1480758 -3.9908152 -4.2787619 -3.7401195 -4.959846 -5.0035982 -4.0756855 -3.315345 -3.0782266 -1.9413283 -4.1494169 -6.7330732 -9.3160973 -10.927719][-1.1748509 -2.40528 -3.8777676 -4.0184617 -5.1636744 -5.6302252 -4.6164627 -2.8209834 -1.9560945 -1.3315926 -0.84504271 -3.844399 -6.03548 -9.0122957 -11.172245][-3.4967589 -3.5453372 -3.4665871 -3.9889216 -4.4699469 -4.0291853 -3.2474933 -2.329613 -0.89967108 -0.44241667 -0.80320358 -4.470118 -6.9161205 -10.090611 -10.922581][-4.7688746 -5.4350867 -5.7551641 -4.2202148 -3.5023592 -2.5105784 -1.3495345 -0.84564638 -0.46789742 -0.096968651 -0.57585955 -3.6568224 -6.3840222 -9.05459 -11.446448][-5.1367245 -5.0906663 -5.1846833 -2.6305335 -1.9710081 0.70505714 2.3402343 1.5784779 1.1734009 0.24943113 0.089313507 -2.6647058 -5.3485565 -8.0660906 -10.509865][-6.233089 -5.9202113 -5.2605743 -2.7645695 -0.88378906 1.3793702 3.9063687 4.0657125 4.4774861 3.3011594 1.4686441 -1.5449095 -4.248106 -6.4871345 -9.1459866][-6.7363267 -6.0944023 -4.349432 -2.4374287 -0.49364042 4.3328156 5.7963357 6.3777661 6.7798781 4.7901015 2.1645036 -1.9631703 -5.965013 -8.3135891 -8.8481178][-6.3823357 -6.0962291 -5.5587668 -3.8126593 -1.5675445 1.0128403 3.4881177 5.5104165 4.6562848 2.9813085 2.3786445 -2.6254463 -6.7987547 -9.19319 -11.106571][-7.1808181 -6.2223163 -3.8388405 -3.171385 -3.1618464 -1.0795884 0.95141172 2.4583278 1.4608402 0.906034 -1.231308 -5.8348756 -8.6874962 -11.68148 -13.9419][-10.055328 -10.330164 -9.7174788 -7.2985744 -6.8721609 -6.7001796 -4.4958286 -3.31072 -3.84813 -4.03833 -5.6627569 -9.9379587 -13.369483 -14.728878 -14.683495][-12.782011 -12.744888 -11.744284 -9.8898973 -9.1472807 -9.1407852 -9.0827885 -9.1361313 -8.6832466 -9.0364323 -10.024225 -11.428095 -13.996334 -15.955456 -15.01943][-15.03273 -14.774471 -14.159025 -11.728703 -11.54739 -11.075098 -10.846393 -11.551094 -11.993151 -12.313872 -11.815823 -11.746487 -11.66935 -12.063559 -11.343979][-13.556405 -12.589087 -12.477123 -11.279581 -10.793733 -9.7237644 -10.405291 -9.9308386 -9.4521742 -10.142045 -10.615513 -10.981482 -9.4938726 -10.285187 -8.3902254][-11.688168 -11.777109 -10.44943 -8.2914906 -8.1017637 -8.8856277 -9.9845676 -10.057168 -11.065193 -9.7050476 -9.53219 -9.6585808 -10.087548 -10.207058 -9.85762]]...]
INFO - root - 2017-12-15 23:01:03.210515: step 68910, loss = 0.24, batch loss = 0.19 (12.4 examples/sec; 0.646 sec/batch; 47h:17m:22s remains)
INFO - root - 2017-12-15 23:01:09.820933: step 68920, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.681 sec/batch; 49h:53m:16s remains)
INFO - root - 2017-12-15 23:01:16.389338: step 68930, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 48h:07m:04s remains)
INFO - root - 2017-12-15 23:01:23.026758: step 68940, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 49h:22m:27s remains)
INFO - root - 2017-12-15 23:01:29.663244: step 68950, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.682 sec/batch; 49h:55m:15s remains)
INFO - root - 2017-12-15 23:01:36.189440: step 68960, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 47h:13m:56s remains)
INFO - root - 2017-12-15 23:01:42.706806: step 68970, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 47h:35m:31s remains)
INFO - root - 2017-12-15 23:01:49.246972: step 68980, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 48h:44m:52s remains)
INFO - root - 2017-12-15 23:01:55.722212: step 68990, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 47h:40m:40s remains)
INFO - root - 2017-12-15 23:02:02.284441: step 69000, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 47h:36m:05s remains)
2017-12-15 23:02:02.764400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.751719 -5.4269438 -4.9844551 -5.1880374 -5.5656924 -6.1993866 -6.4127617 -5.9195395 -6.1588817 -6.4945765 -6.3014064 -8.0398808 -9.1156015 -9.095376 -7.1945734][-5.7424493 -5.5686159 -4.6304421 -5.0105162 -5.864923 -6.3276224 -6.2700734 -6.4386606 -6.6217489 -6.1991835 -6.301878 -8.26508 -8.1427822 -8.3720217 -7.3545504][-4.2907519 -4.9681621 -5.7589045 -5.837337 -6.293385 -6.7694783 -6.4110126 -6.0242834 -6.1305513 -6.4876518 -6.2739873 -8.09556 -9.518816 -9.5347233 -7.5209603][-3.678462 -3.5615153 -3.7402921 -4.6659708 -6.3519363 -5.5556097 -4.3704414 -5.0733042 -4.7047024 -5.1854596 -5.9476724 -7.6314206 -8.9361143 -10.342295 -10.5664][-4.7142487 -4.5756836 -4.9613447 -5.0695963 -5.7110281 -4.37574 -2.1927016 -2.2765651 -2.807198 -3.6607342 -4.8348665 -7.9486132 -10.323338 -11.223816 -11.041853][-5.4647536 -4.867424 -4.6067019 -4.009069 -2.9288416 0.23140383 3.0249581 3.15583 2.5175281 -0.24192429 -2.1221278 -5.1116452 -7.8189983 -9.6795778 -9.5877][-6.7544975 -5.9480124 -5.7517428 -2.9674778 -0.57663012 1.9590278 5.3216834 7.3568139 7.2974515 3.0535111 -0.38334084 -4.4262066 -7.0547276 -8.7284584 -9.07002][-7.379478 -6.7748942 -6.1033926 -3.5869439 -0.29837322 4.5661845 8.1163273 8.2376785 7.5746427 4.456099 1.2609162 -3.479126 -6.0683718 -8.2911654 -8.9291868][-6.8870573 -6.4764967 -5.6794009 -3.3269649 -1.8594012 2.1866922 6.3721166 6.9520888 5.9295735 3.0859504 0.58225536 -4.50846 -8.1420946 -9.6459417 -10.630703][-4.5921183 -4.6144996 -4.8028836 -3.3882539 -2.107758 0.29192924 1.9304566 2.6485734 1.5434108 -0.37346363 -2.5559464 -6.8520355 -9.0150795 -11.584332 -12.119003][-6.8527169 -7.0371532 -7.0411854 -5.6481752 -6.1028004 -4.968998 -4.2376986 -3.9348531 -4.1184411 -5.4899855 -7.325686 -11.540819 -13.630318 -14.765589 -14.951027][-10.84778 -10.343397 -10.208346 -9.6007051 -8.8046293 -8.7245064 -9.1278782 -10.117199 -10.24366 -10.610708 -11.104286 -12.87739 -13.818388 -14.893845 -14.6371][-14.595802 -14.223196 -13.398914 -13.353128 -13.457697 -12.803858 -12.565398 -12.668695 -12.425273 -12.19719 -11.887663 -12.954069 -13.943821 -14.040592 -12.50264][-13.327451 -12.711821 -12.567395 -12.230974 -11.648653 -12.208324 -11.466488 -10.642914 -10.87385 -10.691135 -10.671341 -10.173618 -9.8757057 -10.912659 -10.123005][-11.141666 -11.156408 -10.628283 -10.078962 -8.9971676 -9.1705208 -9.2819853 -9.1526127 -8.4529409 -8.0491829 -8.4501333 -9.3609085 -9.8540325 -9.624649 -9.0247526]]...]
INFO - root - 2017-12-15 23:02:09.459653: step 69010, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 49h:38m:49s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 23:02:15.993139: step 69020, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 47h:06m:21s remains)
INFO - root - 2017-12-15 23:02:22.506932: step 69030, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 47h:19m:28s remains)
INFO - root - 2017-12-15 23:02:29.060981: step 69040, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.632 sec/batch; 46h:15m:13s remains)
INFO - root - 2017-12-15 23:02:35.723156: step 69050, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 49h:27m:23s remains)
INFO - root - 2017-12-15 23:02:42.367278: step 69060, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 46h:36m:44s remains)
INFO - root - 2017-12-15 23:02:48.975789: step 69070, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 47h:39m:21s remains)
INFO - root - 2017-12-15 23:02:55.536358: step 69080, loss = 0.17, batch loss = 0.12 (12.7 examples/sec; 0.631 sec/batch; 46h:08m:23s remains)
INFO - root - 2017-12-15 23:03:02.136294: step 69090, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 49h:58m:27s remains)
INFO - root - 2017-12-15 23:03:08.810435: step 69100, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.648 sec/batch; 47h:23m:27s remains)
2017-12-15 23:03:09.329857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.2602215 -9.6515179 -9.37136 -9.157033 -10.060799 -10.331764 -11.426022 -11.911394 -11.900852 -11.147207 -9.724432 -8.7758026 -9.9279671 -10.430809 -8.3801708][-11.005997 -11.418956 -10.676249 -9.82907 -10.619708 -12.091146 -13.638884 -13.466314 -13.203056 -12.362471 -11.395264 -10.760677 -12.022443 -13.40565 -10.84072][-9.2220373 -10.719126 -11.796113 -10.967333 -11.082062 -11.776993 -13.28851 -13.308764 -12.201758 -11.187449 -11.540984 -11.807376 -13.09252 -14.313786 -13.230839][-10.02542 -10.863873 -12.470337 -10.943497 -10.865971 -10.176313 -10.054358 -10.280843 -10.226604 -9.7762375 -10.300083 -11.328447 -13.788897 -14.104048 -11.547724][-11.007205 -13.390461 -14.456608 -11.497272 -9.281805 -5.2402034 -3.3835075 -5.2190833 -6.3222761 -6.8261657 -8.9376183 -9.9513464 -11.690296 -14.034256 -12.489439][-11.908853 -13.200668 -13.775269 -11.402519 -8.16341 -0.88677692 4.1920743 2.8574996 0.20063257 -3.2380717 -7.0380397 -7.4447217 -10.225487 -12.985187 -11.889503][-14.288721 -14.228392 -12.202227 -9.4768887 -6.4039431 0.50472784 7.3936267 9.1767426 7.7158923 -0.3411994 -7.6549683 -7.5463638 -9.8446941 -12.137529 -11.151478][-14.676956 -14.846964 -12.982637 -6.9046073 -1.5880804 2.9396167 7.1824708 9.5538807 10.447042 3.0049405 -5.6245008 -7.49543 -11.631929 -13.199949 -11.332767][-12.738738 -12.7579 -13.406939 -9.7270651 -4.2937651 2.9838061 7.0806975 7.0906053 6.0180182 0.070366383 -5.717732 -7.8842816 -12.132767 -15.208178 -13.152648][-10.483923 -10.496332 -11.320225 -10.646509 -8.0644073 -2.1846526 3.5476146 5.2299237 2.9004254 -3.2946439 -8.4009 -10.096917 -12.164265 -14.985525 -15.360004][-12.368828 -12.824917 -13.806936 -12.229882 -11.235756 -9.575304 -6.7781582 -3.3368866 -3.8586025 -7.8936634 -11.504512 -12.653605 -15.084881 -15.296576 -13.644876][-15.986847 -16.400408 -17.003706 -14.880526 -13.768978 -13.341666 -13.055385 -11.610491 -11.251705 -12.680044 -13.547523 -13.496119 -14.800482 -15.866972 -13.754063][-14.711863 -14.474073 -15.326336 -14.220913 -14.113327 -12.957181 -12.020248 -12.344774 -13.305737 -12.931433 -12.182076 -12.389511 -13.15052 -13.221237 -10.052565][-12.98723 -11.513445 -11.861786 -10.372002 -10.209502 -10.641317 -10.606659 -10.089594 -10.137743 -10.67123 -10.669252 -9.6557255 -9.3278017 -9.1169586 -8.546937][-8.6179914 -7.0839887 -5.6011329 -3.9227548 -3.5028696 -4.1582613 -5.3705006 -5.5450125 -5.8773031 -6.0949054 -6.2682738 -6.9162741 -9.0965309 -9.8607635 -8.7741108]]...]
INFO - root - 2017-12-15 23:03:15.902092: step 69110, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 48h:04m:16s remains)
INFO - root - 2017-12-15 23:03:22.477328: step 69120, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 48h:18m:46s remains)
INFO - root - 2017-12-15 23:03:29.018262: step 69130, loss = 0.19, batch loss = 0.15 (12.4 examples/sec; 0.647 sec/batch; 47h:22m:10s remains)
INFO - root - 2017-12-15 23:03:35.641397: step 69140, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 47h:18m:14s remains)
INFO - root - 2017-12-15 23:03:42.199357: step 69150, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 47h:12m:30s remains)
INFO - root - 2017-12-15 23:03:48.823848: step 69160, loss = 0.20, batch loss = 0.15 (11.8 examples/sec; 0.675 sec/batch; 49h:23m:32s remains)
INFO - root - 2017-12-15 23:03:55.439999: step 69170, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 48h:27m:14s remains)
INFO - root - 2017-12-15 23:04:02.134115: step 69180, loss = 0.12, batch loss = 0.07 (11.3 examples/sec; 0.705 sec/batch; 51h:34m:55s remains)
INFO - root - 2017-12-15 23:04:08.713860: step 69190, loss = 0.22, batch loss = 0.17 (12.0 examples/sec; 0.665 sec/batch; 48h:37m:35s remains)
INFO - root - 2017-12-15 23:04:15.369882: step 69200, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 46h:58m:29s remains)
2017-12-15 23:04:15.872762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7602348 -6.319808 -4.6644435 -2.8213084 -2.7314734 -2.7519343 -2.791203 -3.3573287 -3.193866 -3.148845 -3.3601735 -5.486886 -5.3956923 -6.1753125 -6.1823235][-5.1999578 -4.6514392 -4.2003593 -3.3404484 -3.4630098 -3.6818058 -3.4816661 -4.2199831 -4.5071669 -4.6408873 -5.5088234 -8.495306 -8.6857815 -8.2751255 -7.4273839][-4.3695788 -4.3485169 -3.4712327 -3.2257445 -3.9940033 -3.8267112 -4.0283031 -4.7972269 -4.9306936 -5.035017 -5.7236266 -8.69855 -8.86853 -8.6220274 -7.8188405][-3.4718769 -4.1702566 -3.8773255 -3.4971888 -3.6188774 -4.0457854 -3.6804178 -3.9294395 -4.1111937 -4.1707916 -4.219718 -6.9642377 -7.5715914 -7.9614506 -7.3541341][-4.6364908 -5.0646634 -5.1163597 -4.0564857 -2.988308 -2.2691369 -1.4246449 -1.670259 -1.7153769 -1.7510002 -2.3439562 -5.0687037 -5.441699 -5.8609037 -6.4924455][-6.0543733 -6.3444343 -5.1007423 -3.2969971 -2.0413733 -0.27093363 1.3211174 0.967062 0.46072483 0.12946892 -0.25241756 -2.7831352 -3.5236454 -5.0007067 -5.4012136][-6.8397007 -6.8263636 -5.6105886 -2.5773356 -1.3400426 1.0498619 2.9594092 2.6453156 2.8185115 1.8912525 0.7615037 -2.0159752 -2.2072177 -3.5608251 -4.6025639][-5.6293907 -5.1379929 -4.0231066 -1.2844496 0.16345644 1.4496593 2.8532729 2.9901948 3.9294095 2.8896089 1.5361071 -1.9542012 -3.0802948 -3.1938586 -3.4501438][-4.4138727 -4.0280294 -2.4565833 -0.19935846 1.2236686 1.45964 2.2869782 2.5729012 2.8729014 2.5052419 2.2139859 -1.0693913 -2.0624685 -2.3428555 -2.4470611][-4.4875274 -4.1712055 -2.9357922 -1.3536835 -0.52827358 0.031896114 0.8482461 1.0885992 1.9623289 2.809175 2.3889022 -1.6736956 -2.4637463 -2.1289017 -2.3213005][-7.8052931 -7.8143983 -6.4304094 -4.4363551 -3.5109861 -2.8468211 -1.9629755 -1.1072245 -0.582366 -0.68595982 -0.92181396 -4.0406075 -4.5160933 -3.16105 -1.8215477][-10.567314 -10.481905 -9.5747356 -7.6084509 -5.8606639 -5.1303153 -4.1752167 -3.9632864 -3.6124871 -2.8447738 -2.8693364 -5.0154905 -4.8693571 -3.3420324 -2.2891881][-10.615782 -10.342202 -8.9600134 -7.7793388 -7.0226631 -6.0528922 -4.7109928 -3.8265848 -4.2429867 -4.3337622 -4.3589449 -5.1676884 -4.7322297 -2.335079 -0.48906422][-7.7534089 -7.5974112 -7.4604688 -5.7510004 -4.8138967 -5.04253 -5.5039477 -5.2174873 -4.3471146 -3.9786012 -3.9476881 -4.2119312 -3.413347 -1.73328 -0.56950378][-3.7795558 -3.696039 -4.3832359 -3.5232904 -2.8498583 -2.6723211 -2.0182028 -2.2104285 -3.2578142 -3.6307294 -3.45956 -3.8224733 -3.468312 -2.8244135 -2.6203094]]...]
INFO - root - 2017-12-15 23:04:22.470455: step 69210, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 47h:13m:35s remains)
INFO - root - 2017-12-15 23:04:29.025944: step 69220, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 47h:45m:13s remains)
INFO - root - 2017-12-15 23:04:35.648562: step 69230, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 49h:14m:22s remains)
INFO - root - 2017-12-15 23:04:42.217043: step 69240, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 47h:00m:23s remains)
INFO - root - 2017-12-15 23:04:48.809092: step 69250, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 47h:31m:50s remains)
INFO - root - 2017-12-15 23:04:55.394070: step 69260, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 46h:44m:42s remains)
INFO - root - 2017-12-15 23:05:02.034813: step 69270, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.697 sec/batch; 50h:59m:13s remains)
INFO - root - 2017-12-15 23:05:08.675679: step 69280, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 47h:57m:38s remains)
INFO - root - 2017-12-15 23:05:15.264132: step 69290, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 49h:17m:58s remains)
INFO - root - 2017-12-15 23:05:21.905417: step 69300, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 48h:00m:17s remains)
2017-12-15 23:05:22.443719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9048223 -5.3784108 -5.2864084 -4.6614575 -4.3282242 -4.8505754 -4.6871576 -4.5260124 -4.3241816 -4.5313811 -4.6285849 -6.0446248 -7.5097375 -7.1660423 -6.5859814][-3.7732015 -4.1016026 -3.9690652 -3.6109049 -3.8724403 -3.1407549 -2.6295354 -3.2050633 -3.0067873 -2.9569914 -2.9526708 -4.7584877 -6.5232859 -7.481956 -7.266511][-3.7168508 -4.2271361 -3.8445468 -2.9731121 -2.7986798 -2.8559048 -3.1168928 -3.1025629 -3.1861587 -3.3076389 -2.739017 -4.4601765 -5.9722824 -7.2215643 -8.1213055][-2.6980124 -4.0652423 -5.2384377 -4.5824218 -3.7010522 -3.3233881 -3.2467854 -3.2425082 -3.6564085 -3.6063509 -3.3969514 -5.0751653 -6.1520109 -7.1321487 -7.4286847][-3.4061313 -4.5624189 -4.5323143 -3.8832402 -3.0649242 -1.0161357 -0.77340031 -2.2320051 -3.4746108 -3.6099105 -4.1480017 -5.4037466 -5.79661 -6.0246558 -5.6757956][-4.5624051 -4.6795878 -4.035821 -2.1495967 -0.038258553 1.7284703 1.7299623 1.2000804 0.30326366 -1.605967 -2.8759744 -3.4218853 -4.3693252 -4.5736561 -3.7397664][-3.809309 -4.0929713 -3.1127567 -0.42570496 1.7440376 3.5195441 4.4056935 4.1044335 2.8548465 0.97413111 0.32018471 -1.5430174 -3.5958664 -3.8683777 -3.2860327][-2.7972693 -2.0106301 0.21407223 2.124083 3.104486 4.5838704 4.6039138 3.6581378 3.0502162 2.1446266 1.5956292 -0.41369009 -1.8406453 -2.1012876 -2.2421799][-2.0229635 -0.784029 0.37054491 2.356132 3.597362 3.6825643 2.5422692 1.4767776 0.67923307 0.48202562 0.88279057 -0.64523268 -2.2989559 -2.9117243 -2.4783189][-1.9013782 -1.5356159 -0.31663132 1.2531447 2.464395 1.9159026 1.1711197 -0.38572407 -1.6169081 -1.0166178 -0.37961483 -2.3641124 -4.11926 -4.08749 -3.6319749][-6.4036417 -5.4507523 -3.7436786 -2.6884677 -2.2003903 -1.9125407 -2.0059533 -3.2544725 -3.6873107 -3.5059769 -3.688694 -5.4866505 -5.92067 -6.1175003 -5.9378128][-9.9271374 -9.0520859 -8.0665751 -6.4801707 -5.758532 -5.3520288 -5.4075704 -5.5867133 -5.4886284 -5.4816566 -5.3043585 -5.9019375 -6.2640982 -6.531106 -5.9736686][-10.467697 -9.7089157 -8.717885 -7.6624846 -6.8744707 -6.6234121 -6.9187031 -7.2503371 -7.4392438 -6.8210063 -6.9191589 -7.5759463 -7.3490343 -6.5650511 -6.0501857][-8.0811958 -8.2308969 -7.2861643 -5.4510169 -4.347331 -5.21649 -6.0924773 -6.7877092 -7.5346441 -7.8061008 -8.0001221 -7.66338 -6.9524188 -6.3307714 -6.5779428][-5.6085196 -5.3081536 -4.8159189 -4.4477825 -4.2455573 -3.8333504 -3.9115343 -5.2623463 -6.2364521 -6.4628673 -6.7607622 -6.8181 -6.9416604 -7.7102008 -8.0721035]]...]
INFO - root - 2017-12-15 23:05:29.067518: step 69310, loss = 0.11, batch loss = 0.06 (12.0 examples/sec; 0.664 sec/batch; 48h:34m:09s remains)
INFO - root - 2017-12-15 23:05:35.612377: step 69320, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.631 sec/batch; 46h:05m:58s remains)
INFO - root - 2017-12-15 23:05:42.259977: step 69330, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 47h:55m:45s remains)
INFO - root - 2017-12-15 23:05:48.830109: step 69340, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 46h:46m:37s remains)
INFO - root - 2017-12-15 23:05:55.421618: step 69350, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 47h:02m:07s remains)
INFO - root - 2017-12-15 23:06:02.030336: step 69360, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 46h:45m:53s remains)
INFO - root - 2017-12-15 23:06:08.641435: step 69370, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 48h:07m:15s remains)
INFO - root - 2017-12-15 23:06:15.258212: step 69380, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.698 sec/batch; 51h:00m:25s remains)
INFO - root - 2017-12-15 23:06:21.868708: step 69390, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 47h:25m:43s remains)
INFO - root - 2017-12-15 23:06:28.397046: step 69400, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 47h:01m:26s remains)
2017-12-15 23:06:28.910892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4038677 -6.8338757 -4.6491652 -1.5423903 0.27023649 -1.1632156 -2.9727104 -4.2485857 -5.4346924 -6.5630789 -6.4929447 -8.48949 -11.243334 -11.865582 -9.5599785][-6.3304524 -6.2324967 -6.9428892 -4.4539962 -2.9814529 -2.0402458 -1.1739063 -2.5578406 -4.1988711 -5.8129673 -7.5754786 -9.8919964 -10.866497 -12.937054 -10.991924][-4.5622697 -6.53159 -7.4175434 -4.9699821 -4.5367589 -2.8410912 -1.9338117 -2.2265358 -3.4175584 -4.1330833 -4.6742153 -7.8118496 -9.8488483 -11.112568 -10.33241][-4.3500981 -6.3438826 -7.6857948 -7.3737226 -7.3052444 -4.6395884 -1.7278862 -0.47181034 -1.6688123 -4.4291987 -7.7636156 -11.00034 -11.46707 -12.482847 -10.549643][-5.6490602 -7.0739589 -7.825604 -7.1000395 -5.2645316 -1.7218914 -0.22380352 -1.0434704 -2.1926854 -3.0402815 -5.5256295 -10.379732 -12.729784 -14.607649 -12.863478][-7.0515151 -5.9406314 -5.8982234 -5.6545467 -3.0157189 2.7015576 7.8741508 7.1359468 2.7276864 -2.4416106 -6.2770476 -9.5823088 -11.931022 -14.427237 -13.444783][-8.1948175 -7.8116384 -7.7339516 -4.6384125 -1.3656206 4.488709 9.4086437 10.548855 9.0719528 1.3658452 -5.0862713 -7.1835117 -9.128973 -12.324106 -12.771282][-8.4050589 -9.8359241 -9.8616066 -4.7931991 -0.052136421 6.7109838 11.835922 10.917719 9.4490509 3.019711 -3.3858221 -8.24483 -10.133214 -10.52859 -9.820488][-7.5767441 -9.4518919 -8.4878063 -5.4598274 -2.79437 3.882895 8.6059532 9.1757832 7.50549 0.24326801 -4.2033191 -9.0158663 -12.076233 -13.129128 -11.23118][-6.9130468 -7.7990007 -7.6368809 -5.4076014 -3.2078962 0.72469568 3.0320077 4.1137948 2.7143493 -3.1007621 -8.8632917 -13.949305 -15.708845 -15.537331 -13.366333][-9.6825247 -11.022694 -9.718298 -7.5255108 -6.3336086 -4.2439523 -3.8961246 -3.3465745 -4.9532003 -8.2860994 -11.327431 -14.837606 -17.725706 -17.881289 -15.116434][-13.914835 -14.352644 -14.062399 -11.262217 -7.7703528 -6.4366851 -7.0210314 -7.999732 -9.6417179 -11.853673 -13.492576 -15.777557 -15.855713 -14.832603 -13.638803][-15.373177 -14.922617 -13.538396 -12.326389 -11.430664 -10.414985 -9.5962458 -10.200833 -10.871744 -11.100662 -11.2925 -12.343345 -12.997772 -11.645291 -9.55235][-11.816942 -10.961405 -11.430668 -11.203463 -10.815114 -9.9860449 -9.6112318 -9.7033138 -9.8738937 -10.214762 -10.098485 -8.30001 -8.03145 -7.5315294 -7.5649986][-6.2104363 -6.5113363 -5.3048763 -4.8468547 -6.483592 -7.9010448 -8.1103611 -6.4242177 -6.3273191 -6.9919262 -6.7098975 -7.156044 -8.4300117 -7.4626203 -7.0921535]]...]
INFO - root - 2017-12-15 23:06:35.403096: step 69410, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 47h:54m:38s remains)
INFO - root - 2017-12-15 23:06:41.948452: step 69420, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 46h:51m:16s remains)
INFO - root - 2017-12-15 23:06:48.544762: step 69430, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 48h:45m:19s remains)
INFO - root - 2017-12-15 23:06:55.059829: step 69440, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.662 sec/batch; 48h:24m:18s remains)
INFO - root - 2017-12-15 23:07:01.739235: step 69450, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 46h:54m:14s remains)
INFO - root - 2017-12-15 23:07:08.442191: step 69460, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.696 sec/batch; 50h:49m:25s remains)
INFO - root - 2017-12-15 23:07:15.054638: step 69470, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 46h:39m:54s remains)
INFO - root - 2017-12-15 23:07:21.584370: step 69480, loss = 0.21, batch loss = 0.17 (11.8 examples/sec; 0.679 sec/batch; 49h:36m:50s remains)
INFO - root - 2017-12-15 23:07:28.221415: step 69490, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 48h:48m:45s remains)
INFO - root - 2017-12-15 23:07:34.818648: step 69500, loss = 0.27, batch loss = 0.22 (12.4 examples/sec; 0.647 sec/batch; 47h:17m:31s remains)
2017-12-15 23:07:35.320397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0276341 -3.4677498 -3.1955574 -1.8501077 -1.640368 -2.4781659 -3.7547197 -4.6459446 -4.6079426 -3.7494767 -3.1577792 -4.3787508 -5.4021049 -6.5802579 -4.1356][-1.7823749 -0.86848783 -0.14086008 -0.019525051 -1.6240749 -2.721904 -4.090322 -5.2085481 -5.3405581 -5.2801852 -4.66103 -6.0086689 -6.5848503 -7.7004166 -5.3462954][-0.44808674 0.160604 0.21517563 1.3151188 0.65863323 -1.0042849 -2.9025548 -4.5407248 -5.3100538 -5.8920107 -5.9007177 -6.9027085 -7.7626133 -9.4654512 -7.8419414][-2.05748 -1.0558128 -0.23793554 0.58834457 0.30331755 -0.92418194 -2.2080741 -3.7842436 -4.8142748 -4.6659741 -4.3456416 -6.5691643 -8.3915215 -10.151913 -8.3844719][-2.5431311 -1.9395061 -1.3887925 0.020908356 0.49346209 1.1515784 0.15167236 -2.9979615 -4.1202059 -4.1982131 -4.4043016 -6.0886331 -7.7478666 -10.377846 -9.4761734][-2.4601011 -1.2153845 -0.68910694 0.9735117 1.6824536 3.1452651 3.4400897 1.7918305 0.22798872 -1.8056087 -3.0525377 -5.1055832 -7.5587869 -9.8868828 -8.4411278][-3.2020106 -2.0543339 -1.0466685 1.099534 2.0899 2.3975015 3.493402 3.4136252 2.9527469 0.23768377 -1.4513698 -3.9784532 -6.3636756 -8.7674456 -7.3781557][-4.6050754 -1.4416943 0.55771732 1.7666221 1.8823504 3.4471831 4.4774642 3.8473763 3.3749413 1.7245173 0.23235369 -2.5475738 -5.1267104 -7.5696144 -5.8632584][-3.3322513 -0.2204504 1.1156311 2.3193307 3.9238582 4.9121156 4.3355565 2.4860711 1.7785096 1.2157817 0.61532927 -2.2063339 -4.9454222 -7.1019731 -5.0818315][-4.3642755 -2.0330896 -0.11942577 1.839294 2.7340636 3.7710261 4.5744023 4.1557832 3.5353236 1.7980051 1.105391 -1.574645 -3.7592878 -5.6674891 -4.630558][-10.51227 -7.296555 -4.2736192 -1.9199133 -0.46850681 1.2336345 2.87332 3.2294049 2.1280575 1.6450176 0.94038057 -2.3267629 -4.3572369 -5.64434 -4.7723131][-12.558262 -11.131018 -9.11549 -5.750452 -4.3390636 -4.1240597 -3.5469065 -2.528805 -2.2946839 -2.9595218 -3.2416718 -4.7405777 -5.6153479 -7.2616148 -6.6979442][-12.687564 -9.7405205 -7.7416983 -6.9824276 -5.8565197 -4.6549983 -5.1041093 -5.8440566 -6.4728069 -5.5760841 -5.0972834 -5.6532121 -6.3876848 -7.3597484 -6.9861693][-11.419657 -10.186455 -8.8839769 -6.8036709 -5.8388519 -6.0230446 -5.7897816 -5.3068342 -5.4051948 -6.0709519 -6.5029421 -6.3767991 -6.6371956 -7.7230949 -6.7676048][-10.42935 -8.8740826 -8.2967777 -7.1249833 -6.96043 -7.2498941 -6.3554173 -5.8267078 -5.9958129 -5.7816195 -5.8924308 -7.2044516 -8.198019 -8.34405 -8.6756382]]...]
INFO - root - 2017-12-15 23:07:41.984921: step 69510, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 48h:26m:16s remains)
INFO - root - 2017-12-15 23:07:48.587427: step 69520, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.675 sec/batch; 49h:20m:19s remains)
INFO - root - 2017-12-15 23:07:55.189233: step 69530, loss = 0.21, batch loss = 0.17 (11.9 examples/sec; 0.673 sec/batch; 49h:10m:25s remains)
INFO - root - 2017-12-15 23:08:01.834204: step 69540, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 49h:32m:37s remains)
INFO - root - 2017-12-15 23:08:08.418910: step 69550, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.674 sec/batch; 49h:12m:16s remains)
INFO - root - 2017-12-15 23:08:14.996547: step 69560, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 48h:50m:03s remains)
INFO - root - 2017-12-15 23:08:21.602463: step 69570, loss = 0.22, batch loss = 0.17 (11.8 examples/sec; 0.677 sec/batch; 49h:24m:56s remains)
INFO - root - 2017-12-15 23:08:28.243940: step 69580, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.678 sec/batch; 49h:32m:44s remains)
INFO - root - 2017-12-15 23:08:34.832403: step 69590, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 46h:26m:45s remains)
INFO - root - 2017-12-15 23:08:41.441566: step 69600, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 47h:28m:41s remains)
2017-12-15 23:08:41.961488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.89504766 -1.2289419 -0.76756334 -0.16773129 -0.76533937 -1.1519308 -1.3883176 -1.4989853 -1.7873311 -2.1857293 -2.3843958 -4.487802 -5.5200496 -6.1404767 -4.8352346][-1.7864404 -1.6406612 -1.3882408 -1.1342349 -1.8000414 -2.4751761 -2.7385821 -2.7216122 -2.6988919 -2.6995013 -2.9406555 -4.6607156 -5.5218077 -5.93887 -4.5894241][-1.5578718 -1.8868048 -2.008601 -1.0581841 -1.7234576 -2.492017 -2.7369022 -3.0662708 -2.9637535 -2.8400953 -2.9623296 -4.8732367 -6.1549244 -6.3762956 -5.544117][-3.1305995 -2.3429518 -2.027828 -1.5793352 -2.6403103 -3.3177564 -3.6852965 -3.7464926 -3.2406938 -2.8343275 -2.2665451 -4.8232327 -6.6419358 -7.4150724 -6.5299478][-3.9489145 -3.348707 -3.0251384 -1.9183161 -1.9966486 -2.0767252 -2.1325121 -2.5031729 -1.9765553 -1.5943651 -1.0012712 -3.1815486 -4.7991323 -6.4750853 -6.2832632][-5.2040677 -4.1843605 -2.848865 -1.2275286 -0.22740316 0.37602663 1.227056 1.5841732 1.8374505 1.4793587 1.360528 -1.4097204 -3.6288605 -5.2026753 -5.4781804][-7.20757 -6.07235 -3.8504238 -1.0807567 0.56199455 1.9945259 3.875814 4.921865 5.3726792 4.1870494 2.7522511 -0.50027037 -3.4128 -5.3169155 -5.3742008][-7.1981139 -6.3183193 -4.4826875 -1.0696044 0.70431376 3.2204881 5.0805774 5.5802941 6.7625833 5.9383492 4.1066794 -0.73408747 -4.2189727 -6.4198442 -6.5447159][-6.3080945 -5.4194503 -4.2234116 -1.4149442 0.36730337 2.78272 4.1216931 4.84164 5.6034646 4.6676164 3.3491988 -1.2824321 -5.0882215 -7.8245764 -8.0184917][-5.3203235 -4.4012542 -3.713666 -1.2002702 -0.32560921 1.1599388 2.3197455 3.3515849 3.9524217 2.5278516 1.4698148 -2.6289184 -6.0502839 -8.3490944 -8.9868507][-8.0579987 -7.3490677 -6.0617027 -3.9440031 -3.4768572 -2.8332732 -2.7034357 -2.4475594 -2.2820849 -2.8275256 -3.17807 -7.0845814 -8.7837791 -9.9978752 -8.9363337][-11.446848 -10.19556 -8.7417374 -7.1304555 -6.8880353 -5.8712945 -6.5465646 -7.2042632 -7.0083532 -7.0235004 -7.2087183 -9.2640467 -8.925271 -9.8335228 -8.5100145][-12.127366 -11.249081 -9.820364 -8.88831 -8.4776955 -7.4362888 -8.288147 -8.4700451 -8.6443148 -8.598155 -8.7802572 -9.3707809 -8.7436256 -8.4441662 -5.8320518][-10.581018 -9.7029533 -8.3366833 -7.2291522 -6.9777546 -7.727756 -8.2301321 -7.8520117 -8.1935043 -8.1206989 -8.30689 -7.4908991 -6.7848492 -6.2452269 -5.0453806][-6.8516483 -5.9628067 -4.9433889 -3.6949031 -3.6954935 -4.0610142 -4.1541963 -4.575428 -4.9310603 -4.9948874 -5.3335352 -6.3029137 -6.7755103 -6.1975965 -5.8794227]]...]
INFO - root - 2017-12-15 23:08:48.587703: step 69610, loss = 0.14, batch loss = 0.10 (11.3 examples/sec; 0.710 sec/batch; 51h:51m:26s remains)
INFO - root - 2017-12-15 23:08:55.238141: step 69620, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 47h:33m:21s remains)
INFO - root - 2017-12-15 23:09:01.826028: step 69630, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.675 sec/batch; 49h:18m:03s remains)
INFO - root - 2017-12-15 23:09:08.439436: step 69640, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 49h:52m:43s remains)
INFO - root - 2017-12-15 23:09:14.985605: step 69650, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 48h:15m:45s remains)
INFO - root - 2017-12-15 23:09:21.564238: step 69660, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 47h:16m:23s remains)
INFO - root - 2017-12-15 23:09:28.212133: step 69670, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 49h:01m:49s remains)
INFO - root - 2017-12-15 23:09:34.923741: step 69680, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 47h:54m:30s remains)
INFO - root - 2017-12-15 23:09:41.526010: step 69690, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 47h:00m:44s remains)
INFO - root - 2017-12-15 23:09:48.102186: step 69700, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 48h:20m:12s remains)
2017-12-15 23:09:48.667574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.6498909 -9.1375084 -8.7293234 -7.3365579 -7.4253564 -7.5791297 -7.4371114 -7.16737 -6.7578058 -6.6652184 -6.8095417 -8.3219643 -8.8662024 -9.833065 -8.63592][-8.03243 -7.9233365 -7.3109455 -6.6957607 -7.1331348 -7.5953512 -7.4196539 -6.6469913 -6.1444006 -6.0680466 -5.2924051 -7.1852636 -8.4143887 -9.2198486 -9.0179529][-5.5080514 -6.581 -7.6912193 -7.3733506 -7.8735151 -8.124897 -7.9107141 -6.8213363 -5.5746369 -5.2357669 -6.1435986 -8.714653 -9.1500731 -9.6921711 -9.4826889][-5.3280206 -6.5194197 -6.89118 -8.0788364 -9.1447849 -7.6367488 -5.9827609 -4.7228861 -4.3785067 -4.2982597 -4.6706972 -7.3757668 -9.0488176 -9.4170427 -8.4695377][-5.4446015 -6.87854 -8.6899347 -8.8739891 -8.9374733 -6.5587778 -3.5890193 -2.0106614 -2.2357161 -3.3396971 -4.5202088 -7.4307528 -8.3051777 -8.4428024 -7.1285019][-6.8520689 -6.9960103 -7.3942652 -6.2148075 -5.8390613 -2.8371854 0.63605881 1.7374525 0.75487041 -1.4283757 -3.6951296 -6.2480578 -6.219348 -6.738235 -6.5065036][-6.1328673 -6.7962289 -6.9456134 -5.0340905 -3.0288363 -0.42197418 3.6270175 5.2344966 3.9271865 0.89496517 -2.4778607 -5.3699379 -5.4594383 -5.0681915 -4.4950809][-5.3150439 -6.1519976 -6.6566219 -4.9570312 -3.7552366 0.83255816 5.0100427 5.5714364 4.6246133 1.8718338 -0.88320494 -4.8024511 -6.5479217 -5.9614978 -4.3469157][-4.5981374 -6.3580027 -5.706676 -3.4616635 -2.9186082 -0.11104822 3.9743104 4.9195209 3.83923 1.1925464 -1.7284274 -5.6090035 -7.3143244 -7.8163557 -6.5063024][-3.9791732 -6.1921487 -6.3162785 -4.2100639 -3.5665233 -0.87948751 2.1045356 2.6211772 2.6663623 -0.32446432 -3.0792885 -6.6204529 -7.7750807 -8.1668472 -7.0284452][-6.814806 -7.8376932 -8.2235165 -6.5314789 -6.456161 -5.7077432 -3.5556567 -2.079572 -1.175869 -2.7842991 -5.5740347 -9.4682312 -10.150005 -10.007059 -8.4356346][-11.376484 -11.202137 -11.931719 -10.786707 -9.4333515 -7.7102175 -6.69043 -6.506784 -6.1237049 -6.9056253 -8.2975531 -10.057331 -9.697588 -8.4256306 -6.7157469][-14.479652 -13.851246 -12.284605 -11.236944 -10.911039 -9.2412367 -8.0071793 -8.2270317 -8.3699284 -8.1875448 -8.7291155 -9.3324165 -8.6890182 -7.9207354 -6.1171474][-11.941793 -11.303289 -9.9139738 -7.9010277 -7.5557666 -7.3003035 -6.7017946 -6.2818217 -6.7419696 -6.9631758 -7.1498885 -7.3312039 -7.423461 -7.2132721 -6.8195486][-6.540246 -7.6801252 -6.3654747 -4.164907 -2.9437132 -3.4312634 -4.4007807 -3.8891811 -4.2882891 -4.8994722 -5.1241331 -6.2846403 -8.3001585 -8.7750416 -7.7560859]]...]
INFO - root - 2017-12-15 23:09:55.282889: step 69710, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 46h:57m:48s remains)
INFO - root - 2017-12-15 23:10:01.925791: step 69720, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 47h:10m:12s remains)
INFO - root - 2017-12-15 23:10:08.460960: step 69730, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 48h:32m:54s remains)
INFO - root - 2017-12-15 23:10:14.984114: step 69740, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.669 sec/batch; 48h:51m:57s remains)
INFO - root - 2017-12-15 23:10:21.673284: step 69750, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 48h:24m:42s remains)
INFO - root - 2017-12-15 23:10:28.287846: step 69760, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.641 sec/batch; 46h:44m:52s remains)
INFO - root - 2017-12-15 23:10:35.001655: step 69770, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.672 sec/batch; 49h:01m:08s remains)
INFO - root - 2017-12-15 23:10:41.574875: step 69780, loss = 0.33, batch loss = 0.29 (12.3 examples/sec; 0.649 sec/batch; 47h:20m:00s remains)
INFO - root - 2017-12-15 23:10:48.176544: step 69790, loss = 0.19, batch loss = 0.15 (11.6 examples/sec; 0.689 sec/batch; 50h:16m:48s remains)
INFO - root - 2017-12-15 23:10:54.835232: step 69800, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 48h:30m:43s remains)
2017-12-15 23:10:55.449882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3657107 -4.749351 -4.3912287 -4.0214372 -4.0832806 -4.261476 -4.7038918 -4.66606 -4.28599 -3.9518778 -3.1516726 -5.13985 -6.684238 -6.7417507 -4.2961287][-4.440237 -4.0241594 -3.7671633 -2.9717865 -3.3295867 -4.0276895 -4.5635581 -5.2171774 -5.5992064 -4.7245383 -3.9171176 -5.5239 -6.673327 -6.9103465 -5.2892218][-2.3142831 -3.3713846 -4.1624312 -2.756073 -3.0132728 -3.4126995 -3.9313705 -4.2698874 -3.9433661 -3.7396431 -3.7519197 -5.5283513 -7.101017 -7.8791757 -6.783113][-4.3523312 -4.4334955 -3.5431883 -3.4972231 -4.7462344 -3.9080729 -3.6631229 -5.111434 -5.4348531 -4.4590082 -3.9319096 -6.3511972 -8.6185036 -9.07797 -7.7106924][-4.6651287 -5.4744573 -5.62514 -4.7072191 -3.7685902 -1.9588196 -1.7484069 -3.8377585 -4.9944367 -4.8895044 -4.6627226 -6.824173 -9.3226128 -10.892214 -10.126682][-6.2839823 -6.4019313 -5.3233666 -3.471683 -1.5976043 0.44251919 1.1309896 0.83697081 0.38973665 -2.2795208 -4.0770879 -5.9160161 -8.0544567 -10.248934 -9.808033][-7.0075712 -6.6724977 -5.860074 -2.3267996 0.31693792 2.2578959 3.8268704 4.3095136 4.2729974 0.90835571 -1.3394089 -5.1845565 -8.1694765 -9.2696886 -7.7928782][-5.9718194 -5.6462369 -4.9279957 -1.0853481 1.6701236 4.8584132 6.3475547 5.3289323 4.4797845 2.3986602 1.162065 -2.8973901 -6.3521633 -8.0319042 -7.188282][-4.9299741 -3.6448081 -2.5315812 -0.85903978 0.27035952 3.9669423 5.7572284 4.0640664 3.2606463 2.3707013 1.3261652 -2.5042949 -4.9905667 -5.9486837 -4.5975385][-3.6928415 -3.3073661 -1.8991768 0.073781013 1.0820699 2.8531375 4.2311673 4.2232709 3.5860009 1.0973697 0.056333065 -1.6889935 -3.0263054 -4.119976 -3.3652332][-7.5033455 -5.2953582 -2.8606682 -1.3756909 -0.63086271 -0.55132484 -0.25105047 0.26515102 0.4215951 -0.64614248 -1.0871272 -3.8582387 -4.7746205 -3.9862006 -2.6625619][-9.8422756 -8.8037567 -6.9614882 -4.0207806 -3.7437344 -4.9848185 -5.8028216 -5.2215171 -4.4874926 -4.4530449 -3.78225 -4.86657 -4.9953475 -3.9715273 -2.9752829][-10.094467 -7.7084985 -6.1997252 -6.4791684 -6.8846216 -6.2550159 -6.7198067 -7.7502513 -8.2287731 -6.7079997 -5.4860845 -5.3596139 -4.9300046 -4.1859541 -3.1127758][-7.7008772 -7.2114449 -6.1013451 -5.6671791 -5.1942291 -6.2465339 -7.5498757 -7.590734 -7.5610104 -7.2045026 -6.7185917 -5.5653629 -5.0975456 -4.827085 -4.6719704][-7.0692873 -5.8754177 -4.170434 -3.3049114 -3.2452888 -4.0630627 -4.7068982 -6.3091192 -7.2447796 -6.5170074 -6.0253181 -6.6511545 -7.0628586 -7.1995568 -7.7218533]]...]
INFO - root - 2017-12-15 23:11:02.004851: step 69810, loss = 0.12, batch loss = 0.08 (11.5 examples/sec; 0.695 sec/batch; 50h:40m:38s remains)
INFO - root - 2017-12-15 23:11:08.595447: step 69820, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 48h:14m:39s remains)
INFO - root - 2017-12-15 23:11:15.222990: step 69830, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 48h:38m:08s remains)
INFO - root - 2017-12-15 23:11:21.889722: step 69840, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 48h:51m:48s remains)
INFO - root - 2017-12-15 23:11:28.529050: step 69850, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 49h:01m:58s remains)
INFO - root - 2017-12-15 23:11:35.140565: step 69860, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.675 sec/batch; 49h:13m:31s remains)
INFO - root - 2017-12-15 23:11:41.782172: step 69870, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.690 sec/batch; 50h:18m:22s remains)
INFO - root - 2017-12-15 23:11:48.487836: step 69880, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 47h:07m:21s remains)
INFO - root - 2017-12-15 23:11:55.200371: step 69890, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 48h:36m:57s remains)
INFO - root - 2017-12-15 23:12:01.851098: step 69900, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.696 sec/batch; 50h:48m:07s remains)
2017-12-15 23:12:02.408531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4573936 -6.9963684 -6.0868645 -5.5957971 -5.4705205 -5.55721 -5.6007862 -5.5880275 -5.4366589 -5.17912 -4.5775652 -5.7544537 -7.1086612 -7.5981555 -8.07555][-5.4532194 -5.681571 -5.1918879 -4.3323908 -4.1816716 -4.1496449 -3.9697952 -4.5255518 -5.0549908 -5.026557 -4.6614723 -5.9850764 -7.4420424 -7.6330504 -7.318202][-3.8461742 -3.474848 -3.0141654 -3.3128626 -3.5111852 -2.9070327 -2.6613214 -2.837878 -2.8696673 -3.1883156 -3.3684464 -5.3744226 -7.2115846 -7.3183556 -7.7175484][-3.9858932 -3.9415498 -3.6425183 -3.0601003 -2.6369486 -2.0185978 -1.7583573 -1.6568894 -1.771363 -2.1056781 -2.4079669 -4.3631024 -6.1209431 -6.5880318 -6.8734226][-3.6247509 -3.7943172 -3.6892488 -2.9650111 -2.5458148 -1.2926903 -0.44522429 -0.2374177 -0.62495375 -1.2401004 -1.7154288 -4.2537179 -6.3387671 -6.8524203 -7.06404][-4.588748 -4.0676279 -3.0415456 -2.0483866 -1.2635698 0.22859907 2.07125 2.4404283 2.0757318 1.1850796 -0.12984228 -3.2450502 -6.0440779 -6.8404889 -7.716074][-6.0981817 -5.1310911 -4.0062523 -2.1974416 -0.72433758 0.92857313 2.8789907 3.5642562 3.4501481 2.0605612 0.10148716 -3.2210033 -6.5233068 -7.5346508 -7.8752093][-6.9161992 -6.1354203 -4.2260823 -1.3808064 -0.30596924 1.6006126 3.1599135 3.3239255 3.4621358 1.6430984 -0.49576616 -4.1569557 -7.230072 -7.7593689 -7.1626244][-6.9255481 -5.404449 -3.2085576 -0.77453136 0.0056443214 1.8503642 3.064302 3.0297341 3.0939288 1.8718791 0.32453442 -3.803319 -7.3047638 -7.7592158 -8.362977][-4.6298118 -3.8300772 -2.2763996 -0.16249323 0.45525074 1.4040484 2.1401238 1.9447656 1.6879563 1.2374973 0.4264884 -3.6964717 -7.2803383 -8.2414551 -9.4566669][-7.5659561 -7.0773878 -5.3975563 -3.2694116 -2.725076 -2.0198128 -1.5157342 -1.9370866 -2.3380451 -3.0911438 -3.4395394 -7.1258311 -9.3961658 -9.5842638 -9.4418545][-11.923371 -10.856575 -9.15759 -7.3118138 -7.0905476 -6.7254734 -6.5897193 -6.7280321 -6.869102 -7.2637763 -7.4622393 -9.1246576 -9.640749 -9.6522951 -9.340024][-11.919315 -11.041655 -10.169024 -9.42215 -8.7546682 -8.3795986 -8.5953875 -8.8742113 -9.2339907 -9.3576031 -8.775197 -8.900032 -8.4776516 -8.6411047 -7.7055216][-10.796349 -10.952629 -9.76215 -8.3934946 -7.835638 -7.9308338 -7.8991117 -7.4550328 -7.5379343 -7.2456112 -7.2550974 -7.1925817 -7.1832218 -7.0261312 -6.8851309][-8.3780985 -7.9451694 -7.4433093 -6.4701166 -5.2462926 -5.2246685 -5.37558 -5.7754507 -6.43018 -5.3917103 -5.5496054 -6.1317759 -6.6086912 -6.9838324 -7.3438816]]...]
INFO - root - 2017-12-15 23:12:09.022275: step 69910, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 47h:28m:06s remains)
INFO - root - 2017-12-15 23:12:15.657076: step 69920, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 47h:18m:58s remains)
INFO - root - 2017-12-15 23:12:22.351843: step 69930, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 48h:46m:05s remains)
INFO - root - 2017-12-15 23:12:28.951763: step 69940, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.628 sec/batch; 45h:48m:25s remains)
INFO - root - 2017-12-15 23:12:35.542477: step 69950, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 48h:16m:25s remains)
INFO - root - 2017-12-15 23:12:42.147249: step 69960, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 48h:43m:52s remains)
INFO - root - 2017-12-15 23:12:48.694257: step 69970, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 47h:46m:58s remains)
INFO - root - 2017-12-15 23:12:55.363602: step 69980, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.693 sec/batch; 50h:30m:53s remains)
INFO - root - 2017-12-15 23:13:01.884519: step 69990, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.657 sec/batch; 47h:54m:16s remains)
INFO - root - 2017-12-15 23:13:08.499382: step 70000, loss = 0.19, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 46h:14m:24s remains)
2017-12-15 23:13:09.082764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2075438 -5.7555413 -6.8393636 -6.6479979 -6.53524 -7.17274 -6.2826276 -6.012991 -4.7341065 -3.6019886 -3.3039443 -4.7304673 -6.8315611 -8.0217552 -9.6615362][-3.5439889 -3.1464796 -2.9123378 -3.4561002 -4.0578918 -4.829536 -5.5802317 -5.149086 -4.0478086 -2.9728382 -2.3187425 -3.7765496 -5.4828081 -7.6923294 -10.001543][-1.1833639 -2.0081871 -2.6839015 -3.3708122 -4.1183786 -5.1118526 -4.9301052 -3.7854929 -2.571564 -2.2251234 -1.5594578 -3.8589203 -6.1212668 -8.2611675 -10.45472][-1.7415569 -2.6027913 -2.9460039 -3.7501855 -4.4164176 -4.519073 -4.0037794 -3.54402 -2.2412341 -1.3218427 -1.2298427 -4.1429219 -6.593174 -8.95034 -11.563862][-2.3462956 -3.8384271 -5.445425 -5.781785 -5.3707657 -4.7958674 -2.7885547 -2.3141832 -2.0551457 -1.5132723 -1.5347819 -3.685724 -5.5319734 -7.7439871 -10.479234][-3.3186004 -4.2949624 -5.1655006 -4.5168633 -3.4648206 -1.4396534 1.2888823 1.6273365 0.26844835 -1.0428052 -1.528482 -3.4937015 -5.3353705 -6.9168434 -9.2723618][-5.0562282 -4.9514112 -4.8164487 -2.607573 -1.4256949 0.31186771 3.3771291 4.0897269 4.0036759 2.4478488 0.24859333 -2.1753485 -4.5814514 -6.0444484 -8.3981075][-4.804739 -5.4671335 -3.5821259 -1.555119 -0.010802269 3.5081344 6.1745486 5.6373563 6.8297944 5.4173808 2.0209317 -2.0021157 -5.7739944 -6.9269233 -8.66506][-5.0831766 -5.25088 -4.3210387 -2.3870585 -0.42831945 2.2040486 4.401382 4.700211 4.9403472 3.2174764 2.6858077 -1.3282671 -6.181458 -8.2868214 -10.682829][-5.5034151 -5.3559918 -3.6259041 -3.3874259 -2.2665184 0.025531769 1.3199096 2.0093756 1.3547435 -0.1523838 -1.2263794 -5.34992 -8.7447643 -11.085424 -14.372116][-8.4770937 -8.277689 -8.042141 -7.1280661 -5.8035378 -4.597002 -3.3612397 -3.1764035 -4.650157 -4.6824875 -5.3713245 -9.1786089 -12.808414 -13.824303 -15.515886][-11.827179 -11.423105 -10.342188 -10.534647 -8.8681355 -8.3540373 -8.1069088 -8.0431824 -8.624321 -9.40893 -10.279501 -11.348985 -12.914402 -14.012547 -14.966352][-15.944677 -16.431259 -15.405464 -13.78027 -13.495411 -12.926466 -11.855518 -11.972265 -13.053806 -13.651195 -13.160577 -13.23221 -13.120975 -12.349107 -10.817108][-14.734421 -14.89196 -14.601475 -14.179913 -12.79723 -12.317147 -12.238375 -11.471185 -11.222645 -10.880853 -11.369928 -11.708551 -9.750761 -9.1679373 -7.79609][-11.396206 -12.193177 -11.2761 -10.411423 -9.7278214 -9.9817276 -10.109371 -10.007421 -10.538907 -9.140254 -8.4759207 -9.0499306 -9.7008591 -10.41649 -9.8724365]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 23:13:16.828265: step 70010, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 47h:46m:45s remains)
INFO - root - 2017-12-15 23:13:23.362518: step 70020, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 48h:40m:53s remains)
INFO - root - 2017-12-15 23:13:29.951143: step 70030, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 47h:50m:57s remains)
INFO - root - 2017-12-15 23:13:36.592643: step 70040, loss = 0.25, batch loss = 0.20 (12.0 examples/sec; 0.667 sec/batch; 48h:35m:42s remains)
INFO - root - 2017-12-15 23:13:43.141789: step 70050, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 47h:51m:26s remains)
INFO - root - 2017-12-15 23:13:49.718924: step 70060, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 48h:30m:33s remains)
INFO - root - 2017-12-15 23:13:56.351746: step 70070, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 47h:55m:38s remains)
INFO - root - 2017-12-15 23:14:02.978490: step 70080, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 48h:31m:25s remains)
INFO - root - 2017-12-15 23:14:09.591231: step 70090, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 47h:09m:19s remains)
INFO - root - 2017-12-15 23:14:16.194289: step 70100, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 47h:24m:21s remains)
2017-12-15 23:14:16.731896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4310136 -7.6019797 -7.8666706 -7.6299248 -7.9134183 -7.7074733 -7.7589369 -7.7517881 -7.5849762 -7.367074 -6.3023181 -4.6786771 -4.97824 -6.6369505 -7.8794394][-6.5901794 -6.2030883 -5.5157208 -5.5017362 -5.9227295 -6.3137445 -6.6095076 -6.6332512 -7.2322912 -6.7363257 -5.1927557 -4.6061387 -5.3911247 -5.8478732 -7.9187808][-5.7024922 -5.5296655 -5.4132705 -5.028007 -5.8294115 -5.9199424 -6.36484 -6.3982706 -6.1333575 -5.87213 -5.7358 -5.0810456 -5.5404963 -7.416997 -9.3183022][-5.79965 -4.9987864 -4.4378848 -4.1200304 -5.047256 -4.9375777 -4.7010288 -5.1818485 -5.6758571 -4.7705045 -3.9497554 -4.1186867 -4.9386406 -7.0877028 -9.3487282][-6.1520705 -7.3441391 -6.9286442 -5.196106 -5.0528078 -3.0642567 -1.3367867 -3.2468719 -5.07909 -4.3651886 -4.2176051 -3.3354564 -3.6014743 -5.8706775 -7.595541][-7.78349 -7.9868169 -6.7035027 -4.3051786 -2.8198922 -0.1672821 2.6902413 2.4203153 1.5877786 -1.0382123 -3.0550077 -2.653141 -3.1407099 -5.2436285 -6.8244495][-10.596573 -10.036135 -7.6958714 -3.7060778 -2.1848001 1.3396153 5.043714 6.0242219 6.2687984 1.9830914 -2.0851109 -2.0624373 -3.6195376 -5.4606867 -6.7668014][-11.064209 -9.8496113 -6.6543107 -2.6480076 0.11194563 4.1744056 6.3954568 6.5043235 6.85092 3.8318295 0.64253473 -2.5866396 -6.3008318 -7.9512234 -9.5080738][-8.1879 -7.524991 -6.1481366 -3.0447323 -0.18269253 2.9455266 4.3333592 5.4640555 5.0607305 2.133461 -0.27021313 -3.9942906 -7.4621582 -9.3150654 -11.719997][-6.15737 -5.0000448 -4.19669 -2.6969266 -0.98993015 0.34821796 1.9104772 2.6623979 1.8804836 0.8716507 -0.92500257 -4.4997005 -7.7176156 -10.582169 -13.883192][-8.90805 -7.4434872 -6.9160137 -5.2727494 -4.4333386 -4.31889 -3.7037416 -3.3670893 -3.7099106 -3.5380116 -4.1304 -5.976953 -9.1749287 -11.781324 -13.633418][-11.546173 -10.433586 -9.2461109 -8.8108559 -8.72184 -8.6255264 -8.0683422 -8.6395407 -8.8207855 -8.1247873 -8.04454 -8.7843132 -11.114643 -12.056747 -13.436557][-15.405508 -14.977032 -13.665583 -12.178757 -12.013836 -11.491378 -11.530014 -11.588266 -11.876602 -11.51228 -11.052765 -9.6938572 -10.703482 -11.365986 -11.178041][-13.649183 -13.934883 -12.78467 -10.97019 -10.7155 -10.020348 -9.956358 -9.18399 -9.2419624 -9.9361849 -10.026428 -8.3041115 -7.9757214 -7.69582 -7.6260815][-11.005901 -11.057586 -9.9266033 -8.3121986 -7.2555337 -6.4132776 -6.016078 -6.5868254 -7.6067362 -7.3108697 -7.7800856 -7.2334414 -7.5860324 -7.1731305 -6.6636682]]...]
INFO - root - 2017-12-15 23:14:23.435050: step 70110, loss = 0.15, batch loss = 0.10 (11.4 examples/sec; 0.703 sec/batch; 51h:13m:01s remains)
INFO - root - 2017-12-15 23:14:30.015372: step 70120, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 48h:05m:54s remains)
INFO - root - 2017-12-15 23:14:36.524599: step 70130, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 46h:31m:07s remains)
INFO - root - 2017-12-15 23:14:43.081212: step 70140, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.632 sec/batch; 46h:03m:33s remains)
INFO - root - 2017-12-15 23:14:49.655871: step 70150, loss = 0.11, batch loss = 0.07 (11.7 examples/sec; 0.682 sec/batch; 49h:41m:05s remains)
INFO - root - 2017-12-15 23:14:56.295809: step 70160, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.690 sec/batch; 50h:16m:27s remains)
INFO - root - 2017-12-15 23:15:02.860544: step 70170, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 47h:59m:15s remains)
INFO - root - 2017-12-15 23:15:09.424122: step 70180, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 49h:34m:12s remains)
INFO - root - 2017-12-15 23:15:16.036815: step 70190, loss = 0.24, batch loss = 0.20 (11.7 examples/sec; 0.686 sec/batch; 50h:01m:00s remains)
INFO - root - 2017-12-15 23:15:22.637798: step 70200, loss = 0.23, batch loss = 0.18 (12.1 examples/sec; 0.663 sec/batch; 48h:20m:03s remains)
2017-12-15 23:15:23.215170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6796446 -6.9364352 -7.5542183 -8.3360891 -9.0319624 -9.3663769 -10.183144 -10.446959 -9.9542465 -9.5482845 -8.5095654 -7.7584057 -7.8246379 -8.1753588 -7.4483194][-7.737299 -8.8161173 -8.8461275 -9.2316246 -10.376974 -11.437302 -12.525031 -12.982187 -12.964499 -12.115596 -10.849589 -10.616232 -10.643038 -11.727921 -9.7767973][-6.613904 -8.3331432 -9.8951588 -10.92296 -11.72893 -12.811295 -13.999134 -14.194157 -13.384674 -12.533854 -12.169161 -12.063166 -12.312311 -13.280266 -11.82234][-8.307519 -9.7750549 -11.268322 -11.827903 -12.167953 -11.613529 -11.028006 -11.63518 -12.212988 -11.227184 -9.8543835 -10.416073 -11.815475 -12.80633 -11.314803][-9.2865868 -11.366308 -13.204381 -12.470123 -11.626074 -8.1468019 -5.0896091 -6.92625 -9.624095 -9.7058992 -9.57052 -9.875103 -10.2724 -11.899801 -11.09803][-9.8732443 -11.310987 -12.962471 -12.580288 -10.05491 -3.4649661 1.8288479 0.37879705 -1.8331509 -5.3228068 -9.5792332 -9.1057777 -8.6036472 -10.72506 -10.598872][-10.111015 -11.491157 -11.195324 -9.6562843 -6.72683 -0.13298607 5.4202018 7.2139964 6.5920072 -1.1524692 -8.9514952 -9.2231684 -9.4008484 -11.073525 -10.128461][-10.486442 -11.406893 -11.48156 -8.8291636 -4.3520751 2.1136723 7.5491328 10.270449 9.9541245 2.9167066 -4.7528715 -8.026844 -10.905456 -11.520348 -10.352709][-9.2798386 -10.264685 -10.984985 -9.75268 -5.9285254 0.52479744 5.8076024 7.9392285 7.884068 2.1032381 -3.7571197 -7.0652652 -11.435273 -14.100597 -12.961523][-8.0258751 -9.3060064 -11.457564 -10.908398 -8.3851833 -3.588762 0.99725151 4.0621963 4.0599847 -0.78060722 -5.073822 -8.5503063 -11.699492 -14.720676 -15.028109][-10.031926 -10.109694 -11.768993 -10.155067 -9.1229115 -7.2212033 -4.9632592 -2.8309109 -3.2279894 -5.701643 -8.5256119 -11.24334 -14.032495 -15.909792 -14.899149][-14.859699 -15.205694 -15.136547 -13.217026 -12.37128 -10.496201 -9.337369 -9.0108385 -10.084383 -11.442089 -12.785007 -14.151226 -14.99354 -15.121908 -13.355367][-14.84243 -15.194454 -14.541178 -12.898652 -12.683933 -11.246559 -10.540528 -11.692058 -12.166716 -11.438218 -11.567888 -12.868623 -13.370544 -12.781462 -10.447431][-13.389341 -12.671227 -12.366953 -11.267954 -10.280931 -10.287017 -11.450294 -11.503468 -10.683426 -11.096069 -11.274258 -9.8629856 -8.6906061 -9.3204861 -8.51857][-8.4593334 -8.579421 -7.4394331 -6.0681896 -5.0578442 -5.9306879 -7.2383285 -7.7093539 -7.791687 -8.1764574 -7.5833626 -7.687582 -8.9268742 -8.5898628 -7.0460863]]...]
INFO - root - 2017-12-15 23:15:29.739582: step 70210, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 47h:18m:50s remains)
INFO - root - 2017-12-15 23:15:36.326181: step 70220, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.644 sec/batch; 46h:57m:17s remains)
INFO - root - 2017-12-15 23:15:42.880398: step 70230, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 47h:55m:24s remains)
INFO - root - 2017-12-15 23:15:49.498929: step 70240, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 48h:57m:58s remains)
INFO - root - 2017-12-15 23:15:56.105266: step 70250, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 48h:49m:10s remains)
INFO - root - 2017-12-15 23:16:02.627275: step 70260, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 48h:39m:45s remains)
INFO - root - 2017-12-15 23:16:09.262879: step 70270, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 48h:19m:21s remains)
INFO - root - 2017-12-15 23:16:15.818436: step 70280, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 47h:12m:18s remains)
INFO - root - 2017-12-15 23:16:22.424153: step 70290, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 47h:22m:16s remains)
INFO - root - 2017-12-15 23:16:28.964187: step 70300, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 49h:28m:49s remains)
2017-12-15 23:16:29.489132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.28482676 -0.13313055 0.070903778 -0.2294631 -1.6097059 -2.6878295 -3.1900854 -3.7015905 -4.0228786 -2.9604938 -2.3992016 -4.6946821 -7.1671915 -7.9863005 -6.394464][-1.1965642 0.24423075 -0.67083931 -1.5107737 -2.5176404 -4.1745787 -5.3683062 -6.2042627 -6.2342525 -5.4080782 -4.8351974 -6.1492815 -8.1343155 -8.9822426 -7.2258205][-0.47077084 -0.29966688 -0.83411217 -1.2690659 -2.7929099 -4.1859689 -5.1688342 -6.2659497 -6.6310463 -6.1515617 -5.6559114 -6.7097616 -8.5157738 -8.3756313 -5.8775854][-1.2415047 -0.7925086 -1.6300273 -2.1674218 -3.1885524 -3.4935179 -4.0296063 -5.4910617 -6.4078221 -6.0686913 -6.5941229 -7.6370764 -9.037159 -8.2194843 -6.2617435][-2.2640719 -2.4314017 -2.874893 -2.2626708 -2.0018737 -1.4879098 -1.2071266 -3.0405204 -4.1174917 -4.8539348 -5.8107653 -6.8391581 -8.3016043 -8.0075884 -6.3931184][-5.0055361 -4.1022224 -4.215476 -1.9873672 -0.55237675 1.1517973 2.6559386 1.7868929 0.81384373 -1.3122249 -3.2361996 -4.1549988 -5.8522582 -4.6346059 -2.5951498][-7.3276482 -6.50685 -5.5107365 -2.7484651 -0.49837685 2.407907 5.4908309 5.8780723 5.2350249 2.078228 -0.24445581 -1.4926572 -3.2355487 -2.4549007 -1.6184382][-8.2153931 -7.5784698 -6.999167 -3.8455184 -0.90216875 3.157949 5.8221974 5.9206052 6.2044978 3.7876916 2.2044544 0.02264595 -2.220155 -1.6704779 -0.84029627][-7.5338736 -7.6350603 -7.6453195 -4.6207676 -1.91173 1.5181088 3.6992612 4.8275218 4.9827933 3.5063767 3.062326 1.0714731 -0.85403013 -1.4637284 -1.325067][-8.0338812 -7.9702091 -7.2448759 -4.2688026 -2.9115257 0.45165539 2.3428874 3.2161021 2.740006 2.0114641 2.4580836 0.44980526 -1.8266053 -2.2688134 -3.4189832][-10.501014 -10.635441 -9.4850063 -7.8395658 -6.415318 -3.5648971 -1.9512668 -1.3324537 -1.9316766 -1.5516911 -0.8537817 -2.9751916 -2.9962175 -3.4554 -4.2117696][-12.97694 -12.806438 -10.947662 -10.405117 -9.3212013 -7.6075306 -6.5692563 -6.2684712 -6.4675074 -6.3515396 -5.9872704 -5.99974 -5.8464003 -5.2943945 -4.7761316][-11.387836 -11.042744 -9.8349676 -10.302357 -10.415292 -9.8566666 -9.8294239 -9.3238993 -8.7730417 -8.1004343 -7.6969552 -7.8293543 -7.273705 -6.497601 -5.5453491][-7.4405756 -7.4521918 -7.1873155 -8.0506916 -8.3938465 -9.2572422 -9.737318 -9.752739 -9.7749491 -9.240593 -8.29291 -8.200243 -8.0609074 -7.4452868 -6.9265757][-6.1821408 -4.7017078 -4.4669762 -4.5170889 -4.2192469 -4.8663359 -5.6634049 -6.6953888 -6.9521494 -6.6875181 -6.7312403 -8.1070585 -9.1969738 -9.6121454 -10.14708]]...]
INFO - root - 2017-12-15 23:16:36.090787: step 70310, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 48h:38m:29s remains)
INFO - root - 2017-12-15 23:16:42.636447: step 70320, loss = 0.11, batch loss = 0.06 (12.1 examples/sec; 0.663 sec/batch; 48h:17m:44s remains)
INFO - root - 2017-12-15 23:16:49.223490: step 70330, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.686 sec/batch; 49h:57m:02s remains)
INFO - root - 2017-12-15 23:16:55.746170: step 70340, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 47h:00m:47s remains)
INFO - root - 2017-12-15 23:17:02.315259: step 70350, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 48h:49m:09s remains)
INFO - root - 2017-12-15 23:17:08.949873: step 70360, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 47h:40m:50s remains)
INFO - root - 2017-12-15 23:17:15.517652: step 70370, loss = 0.11, batch loss = 0.07 (12.5 examples/sec; 0.640 sec/batch; 46h:36m:16s remains)
INFO - root - 2017-12-15 23:17:22.084941: step 70380, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 48h:43m:34s remains)
INFO - root - 2017-12-15 23:17:28.721003: step 70390, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 48h:35m:15s remains)
INFO - root - 2017-12-15 23:17:35.387885: step 70400, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 49h:16m:41s remains)
2017-12-15 23:17:35.915721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.817749 -6.7024937 -5.944519 -4.9264364 -5.6638408 -6.5060029 -7.7224321 -7.6356378 -7.8065958 -7.8600836 -7.3611407 -8.3110867 -8.1580572 -6.0392394 -4.6247873][-6.7375031 -6.1344643 -4.1599236 -3.7093668 -4.6528563 -5.3096013 -6.2669616 -7.3809109 -8.59306 -8.2061481 -7.6655397 -9.3263369 -9.4692173 -7.0617023 -5.9243045][-3.1974721 -3.8929758 -3.9672499 -2.3942111 -1.9949079 -3.102762 -3.8892207 -4.4717236 -5.4531851 -6.1430421 -6.565146 -8.2857809 -8.925868 -7.0418749 -6.1212006][-3.6731133 -3.2052066 -2.5448 -2.2840772 -2.8462791 -3.0070338 -3.3650637 -4.2787585 -5.611711 -5.7269197 -5.9108572 -8.2244005 -9.2959938 -7.8133135 -7.2941389][-6.1232247 -6.3598042 -5.7453742 -3.6064436 -2.7301004 -1.7349894 -2.0778227 -4.1215792 -6.506897 -6.5261016 -6.4778209 -8.9014454 -9.8658085 -8.9192934 -7.9007773][-8.9200764 -7.8761559 -5.0386505 -1.3330684 0.47120619 2.3354602 3.5334544 1.7393909 -0.68933392 -3.4964089 -6.2121596 -8.0129013 -9.1070938 -8.4309607 -7.7713008][-11.677237 -9.3173008 -5.7660823 -1.3021383 1.8989358 5.1337514 7.3171535 5.9394841 4.5642171 1.051126 -2.5434299 -5.5543308 -7.8988962 -6.8959522 -6.2206964][-12.672835 -11.308098 -8.3397446 -2.707206 1.1608548 5.80829 8.0913773 6.2340665 5.1536126 2.7119098 -0.14808369 -4.70759 -8.1522141 -7.5730047 -6.2823296][-11.36728 -10.965377 -9.1523323 -4.683291 -0.88386106 2.6347976 4.318048 4.5966964 4.4329667 1.0270405 -1.5003338 -5.1095309 -8.5646849 -8.4511642 -8.3368158][-10.322605 -9.9610586 -8.4166374 -5.1330738 -2.4477034 -0.596405 1.2133713 1.5742855 0.34791327 -1.1996188 -3.1922369 -6.9027052 -9.6019754 -9.2828655 -8.780407][-12.086637 -11.194789 -9.6610012 -7.8965263 -6.3077254 -4.296628 -1.953232 -2.4242015 -3.39231 -3.9592862 -5.3075452 -8.8461246 -9.9369259 -9.42132 -8.4048786][-16.318739 -16.059834 -14.676798 -13.084008 -11.878671 -10.010211 -8.2576284 -8.1089106 -7.8580513 -7.9967003 -8.69187 -10.298967 -10.456055 -9.56544 -7.9560575][-15.259903 -13.857985 -12.494955 -11.723062 -11.968229 -11.321241 -11.263809 -11.166463 -10.779051 -10.033033 -9.38987 -10.284027 -10.856432 -8.940239 -7.7994981][-11.813419 -10.612621 -9.2070808 -8.4244337 -7.7221017 -7.8051462 -9.7131786 -9.4946547 -9.571743 -9.49259 -9.2219076 -9.1020775 -8.577199 -7.5687256 -6.722403][-8.6710663 -8.5301247 -6.716814 -4.7989216 -3.7094228 -3.6165693 -4.3374577 -5.3244815 -6.9344416 -6.22656 -6.0677691 -7.0538139 -7.1615019 -6.9237304 -7.2037888]]...]
INFO - root - 2017-12-15 23:17:42.621034: step 70410, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 47h:44m:55s remains)
INFO - root - 2017-12-15 23:17:49.324327: step 70420, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 47h:53m:03s remains)
INFO - root - 2017-12-15 23:17:56.098717: step 70430, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.689 sec/batch; 50h:07m:37s remains)
INFO - root - 2017-12-15 23:18:02.690227: step 70440, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 49h:28m:18s remains)
INFO - root - 2017-12-15 23:18:09.283048: step 70450, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 47h:20m:22s remains)
INFO - root - 2017-12-15 23:18:15.839457: step 70460, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 48h:28m:03s remains)
INFO - root - 2017-12-15 23:18:22.430605: step 70470, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 47h:52m:11s remains)
INFO - root - 2017-12-15 23:18:29.041067: step 70480, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.691 sec/batch; 50h:16m:58s remains)
INFO - root - 2017-12-15 23:18:35.574543: step 70490, loss = 0.11, batch loss = 0.07 (12.3 examples/sec; 0.649 sec/batch; 47h:13m:17s remains)
INFO - root - 2017-12-15 23:18:42.122108: step 70500, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 47h:07m:38s remains)
2017-12-15 23:18:42.654677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8702216 -6.9501839 -6.897191 -7.6525307 -8.1172361 -8.0750217 -7.9762549 -7.7265482 -7.246377 -6.5770707 -5.3083696 -6.3348079 -8.0872183 -9.5922089 -10.160087][-7.446373 -8.9304142 -8.67227 -8.5275478 -9.5964861 -10.40374 -10.752895 -10.893578 -10.895004 -10.168628 -8.7417545 -8.9050121 -9.4706535 -9.7318592 -9.7436275][-6.6680169 -6.9411774 -7.5679045 -7.807086 -8.7439461 -9.4797926 -9.65344 -10.221292 -10.892933 -10.695709 -9.9589272 -11.768721 -13.58078 -12.701021 -12.016103][-9.4467335 -9.3938894 -8.6566725 -8.3069563 -8.7716465 -8.9292488 -9.1019478 -9.2305336 -9.3693829 -10.16114 -11.255703 -13.240076 -14.864754 -15.414888 -14.361727][-10.112453 -11.261288 -11.067879 -8.9767179 -7.6623979 -6.3213444 -5.9760194 -6.8077459 -7.9981346 -8.6433611 -9.0541916 -12.364183 -15.524845 -15.819105 -15.110132][-12.050928 -12.194303 -10.166722 -8.2144394 -5.6350818 -0.55267048 2.522984 0.33467436 -2.5112195 -4.7392187 -6.5995979 -9.8840675 -12.796988 -13.349092 -12.870305][-11.504765 -11.76862 -9.6588163 -6.2639675 -2.1223719 3.2656417 7.9012771 7.7616181 4.8273206 -1.0581551 -5.3932743 -7.579761 -9.7891426 -10.705118 -10.269168][-12.654199 -11.82176 -8.8969574 -4.82305 0.30950356 5.3403058 9.7422657 10.463209 7.7119956 1.4244151 -4.1115718 -8.0279837 -10.006421 -8.9897585 -6.6186237][-11.863354 -12.151683 -9.5070877 -6.99759 -3.8957632 1.4903898 5.8729692 6.9742503 5.4884782 0.21172571 -4.6710596 -9.37602 -12.813129 -10.777572 -7.9571743][-10.68771 -12.147745 -11.973845 -9.3510466 -7.1639967 -3.7758226 0.01654911 1.7052851 0.0038561821 -3.9972072 -6.4748359 -10.549606 -14.057056 -13.413363 -11.596704][-13.066496 -14.284044 -13.867805 -12.981583 -11.726511 -8.80848 -6.6516352 -5.7836804 -5.6251006 -7.33885 -9.3054419 -12.385544 -14.356014 -14.099672 -12.730274][-16.776497 -17.731556 -17.020584 -14.716244 -12.43675 -11.014947 -9.9338894 -9.0811806 -9.4842253 -11.098954 -12.168251 -12.976126 -13.790209 -13.189093 -11.265591][-19.819786 -19.807873 -17.968578 -16.325592 -13.631678 -11.186414 -9.7084284 -9.96512 -10.847214 -11.22287 -11.557951 -12.395151 -12.557492 -12.158978 -11.292714][-16.18008 -15.650997 -14.919846 -13.334429 -11.659781 -10.376215 -9.9243279 -9.2190285 -9.0765276 -9.8739729 -10.749493 -10.505112 -11.230274 -11.253984 -11.367328][-9.4643126 -10.097242 -8.7249775 -7.1720829 -6.2295256 -5.8857121 -6.2660103 -6.7250237 -7.0624628 -7.687479 -8.4274826 -10.453023 -11.987879 -12.659649 -13.285614]]...]
INFO - root - 2017-12-15 23:18:49.259035: step 70510, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 46h:42m:11s remains)
INFO - root - 2017-12-15 23:18:55.871724: step 70520, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.685 sec/batch; 49h:50m:24s remains)
INFO - root - 2017-12-15 23:19:02.505672: step 70530, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 47h:36m:32s remains)
INFO - root - 2017-12-15 23:19:09.117126: step 70540, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 47h:12m:56s remains)
INFO - root - 2017-12-15 23:19:15.728788: step 70550, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 49h:22m:16s remains)
INFO - root - 2017-12-15 23:19:22.391858: step 70560, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.664 sec/batch; 48h:17m:20s remains)
INFO - root - 2017-12-15 23:19:29.029439: step 70570, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 46h:52m:35s remains)
INFO - root - 2017-12-15 23:19:35.600829: step 70580, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 47h:39m:20s remains)
INFO - root - 2017-12-15 23:19:42.140443: step 70590, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.638 sec/batch; 46h:23m:02s remains)
INFO - root - 2017-12-15 23:19:48.680216: step 70600, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 46h:43m:34s remains)
2017-12-15 23:19:49.199230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1727562 -5.6914253 -5.8257504 -5.8220525 -6.7574425 -8.8275518 -10.510284 -10.983162 -9.8822231 -7.8491154 -5.5235643 -5.6658077 -4.6423888 -4.0319681 -5.0120535][-2.9433653 -3.7721562 -3.2326992 -4.2625132 -6.5299096 -8.4872389 -10.514486 -12.000607 -11.710253 -9.8028011 -7.0467415 -6.78137 -6.039319 -5.0168476 -4.1047869][-1.4924812 -1.2969198 -1.357944 -1.6544237 -2.805141 -5.3100619 -7.5355024 -9.2063627 -9.7300549 -8.9141893 -7.2729592 -8.1200857 -7.7082548 -6.4406195 -6.224165][-1.5803671 -1.8638947 -1.732712 -2.559797 -3.6121771 -4.58237 -5.2731771 -6.622808 -7.3325396 -6.4713664 -5.4023428 -7.0379438 -7.4183254 -7.3537836 -7.3446994][-2.593405 -3.5779543 -4.1552758 -3.1435466 -2.7094896 -2.0820761 -0.94913626 -2.8168471 -4.9757924 -5.5681038 -5.696949 -7.500556 -8.1670246 -8.2240572 -8.160429][-5.1373534 -4.6077776 -4.2007971 -2.5507989 -0.96617079 1.0096912 3.4384522 1.8888922 0.010277748 -2.5014713 -5.6603904 -8.0200472 -8.8272686 -9.0357723 -9.1099434][-6.7396259 -7.4571052 -5.8641405 -2.0015447 -0.62975454 2.5835147 5.8648887 4.742763 4.0388 0.89927292 -3.3293059 -6.532073 -8.2378654 -8.0195827 -8.3062181][-9.1474791 -8.3045616 -6.14493 -2.6633539 -0.68315315 3.1981034 6.788033 5.870007 5.4698491 2.2535114 -1.2921691 -5.4110208 -7.6775932 -7.5068989 -7.1195612][-7.7669182 -7.9808006 -6.5333958 -3.1728196 -0.66000462 2.5301566 4.3670259 3.8326554 4.2964196 1.3580832 -1.8609757 -5.177103 -7.7443552 -7.2387848 -6.6028409][-7.6739902 -6.5976734 -5.9236031 -3.5660892 -1.551827 0.67422009 3.2008691 2.0505481 1.280972 -0.42275953 -2.6035922 -5.2749252 -7.1837063 -7.0964022 -7.4102664][-9.9432507 -8.7086744 -6.0008593 -4.0999942 -2.9367077 -1.538866 0.086508274 -1.1397176 -2.0530341 -3.94504 -5.7214985 -8.6137772 -9.1692753 -9.0660706 -7.8401246][-14.018444 -12.597916 -9.4130163 -6.9346995 -6.5539174 -5.3793588 -5.1416225 -5.6953492 -6.0542231 -7.0968966 -8.2004356 -9.70759 -9.3325872 -9.0960693 -7.6451521][-13.834164 -11.408262 -9.4893961 -7.5269995 -6.9083014 -6.8561029 -6.3973689 -6.9903584 -7.5135469 -7.6871476 -8.3474045 -8.8447762 -7.7902822 -7.005085 -4.6629372][-13.029793 -10.385102 -7.3977175 -6.2867923 -5.1935787 -5.8420897 -6.1250362 -6.2674375 -6.5262151 -6.4891729 -7.3072734 -7.0270023 -6.4716177 -4.9951854 -3.2061658][-8.6064262 -7.8285561 -5.64371 -4.4993734 -3.8848209 -3.6103654 -2.7387381 -3.8291888 -5.0510426 -4.3825989 -4.5104055 -4.7609386 -5.5182748 -5.9110951 -5.1132565]]...]
INFO - root - 2017-12-15 23:19:55.740037: step 70610, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.638 sec/batch; 46h:25m:02s remains)
INFO - root - 2017-12-15 23:20:02.309854: step 70620, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 48h:34m:43s remains)
INFO - root - 2017-12-15 23:20:09.029839: step 70630, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.672 sec/batch; 48h:54m:45s remains)
INFO - root - 2017-12-15 23:20:15.516478: step 70640, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 47h:06m:55s remains)
INFO - root - 2017-12-15 23:20:22.096246: step 70650, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 47h:42m:21s remains)
INFO - root - 2017-12-15 23:20:28.754688: step 70660, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 47h:10m:15s remains)
INFO - root - 2017-12-15 23:20:35.334644: step 70670, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 47h:24m:59s remains)
INFO - root - 2017-12-15 23:20:41.858991: step 70680, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 48h:03m:23s remains)
INFO - root - 2017-12-15 23:20:48.405922: step 70690, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 47h:33m:46s remains)
INFO - root - 2017-12-15 23:20:55.045183: step 70700, loss = 0.19, batch loss = 0.14 (11.5 examples/sec; 0.693 sec/batch; 50h:24m:37s remains)
2017-12-15 23:20:55.619228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0105596 -6.2071867 -5.6384234 -5.04574 -5.223597 -6.1526222 -7.0179806 -5.8768253 -3.8923109 -2.0557351 -0.6700902 -0.85323811 -1.3755312 -2.4115157 -1.918062][-5.6073508 -4.6397619 -3.1648054 -3.1718729 -3.8584003 -4.3101254 -4.1304436 -3.8027391 -2.9170771 -0.46920681 2.0118074 1.067595 -0.66664457 -1.3925972 -1.8358138][-2.2311842 -3.0560086 -4.1211529 -3.2041175 -2.2854431 -2.8716583 -3.1352324 -1.9689534 -0.28873968 1.0208807 1.8328958 0.29832172 -1.4747958 -4.2669597 -6.022162][-4.8874893 -4.693079 -4.2616119 -3.5280762 -3.5180969 -3.0842621 -2.817759 -2.3867662 -1.6916265 -0.44744492 0.92993212 -1.2550325 -3.9968116 -6.2081952 -6.8707438][-6.9126244 -6.1709146 -5.4618316 -4.1835403 -3.1016295 -1.0981741 0.27495527 -0.58208656 -1.5263619 -0.77496862 -0.14792061 -2.1606371 -3.794188 -6.1287827 -6.8007965][-8.7965813 -7.4809108 -4.9586878 -2.2017965 0.17692327 2.3834109 3.0500855 2.4055686 2.2010407 1.0539341 -0.42732143 -2.177501 -3.5538108 -5.5010428 -5.5092006][-10.278852 -7.4082727 -3.1160457 0.72355175 2.8796954 4.4888549 5.7581964 5.2208171 3.8189797 1.8729396 0.1471467 -2.3662186 -4.69248 -6.6864243 -6.6666856][-9.1296463 -7.6747341 -4.9858956 0.35830736 3.2553039 5.1683373 6.0511422 4.2747045 2.6646657 0.85767365 -1.4302478 -4.1306143 -6.279099 -8.1439056 -8.24506][-9.33332 -7.6484241 -5.4535723 -1.1546745 1.2643743 3.4074054 3.9650578 2.7580085 1.9149075 -0.39558983 -2.8245893 -5.7827582 -8.5924273 -10.626642 -9.6692533][-8.3721437 -8.4627991 -7.4526129 -3.5082054 -1.3881626 0.49572277 2.063674 1.5025582 0.38041592 -2.243324 -5.1657333 -8.2646246 -10.607592 -12.331108 -11.647572][-13.888758 -12.991089 -11.330368 -7.7539873 -6.4374738 -5.4340472 -4.6452012 -4.8024693 -4.767004 -6.3588209 -8.7655106 -12.142408 -13.445118 -13.231554 -11.259179][-17.340139 -16.13324 -14.304337 -11.371207 -10.337217 -9.683239 -9.2343292 -9.4377069 -10.212366 -11.076217 -12.26083 -13.772856 -13.904867 -12.772821 -10.935265][-14.268314 -12.796813 -11.590055 -9.773365 -9.2439184 -8.9667425 -8.8280411 -10.083197 -11.20445 -11.14406 -12.120054 -13.068692 -12.858265 -11.019802 -7.959516][-11.341188 -9.5762091 -8.1049881 -6.3063579 -5.4337325 -6.3978462 -7.1825309 -6.8624625 -7.1877222 -8.7825823 -10.785814 -10.629065 -9.7670612 -8.4607878 -6.6218362][-8.1646919 -7.3676486 -5.4147582 -2.9876311 -1.742039 -2.2851989 -2.1684856 -3.3267951 -4.907351 -5.2583728 -5.3117003 -6.7305541 -7.8933392 -7.8812304 -6.8939567]]...]
INFO - root - 2017-12-15 23:21:02.235369: step 70710, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 47h:47m:22s remains)
INFO - root - 2017-12-15 23:21:08.806657: step 70720, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 47h:55m:50s remains)
INFO - root - 2017-12-15 23:21:15.406535: step 70730, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 49h:09m:26s remains)
INFO - root - 2017-12-15 23:21:21.977357: step 70740, loss = 0.22, batch loss = 0.17 (11.9 examples/sec; 0.675 sec/batch; 49h:03m:31s remains)
INFO - root - 2017-12-15 23:21:28.535451: step 70750, loss = 0.18, batch loss = 0.14 (12.7 examples/sec; 0.629 sec/batch; 45h:42m:58s remains)
INFO - root - 2017-12-15 23:21:35.122980: step 70760, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.664 sec/batch; 48h:17m:31s remains)
INFO - root - 2017-12-15 23:21:41.689314: step 70770, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 48h:45m:51s remains)
INFO - root - 2017-12-15 23:21:48.266757: step 70780, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 48h:31m:41s remains)
INFO - root - 2017-12-15 23:21:54.884871: step 70790, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 49h:22m:02s remains)
INFO - root - 2017-12-15 23:22:01.528362: step 70800, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 47h:50m:46s remains)
2017-12-15 23:22:02.091961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.15952 -7.5364237 -6.216002 -5.26629 -5.7769809 -5.8380022 -5.53117 -4.8648109 -4.1872668 -3.3899212 -2.78657 -4.2651277 -6.5292678 -8.6637363 -7.9793224][-7.30936 -6.9804087 -5.615694 -4.97131 -4.6107006 -4.8861542 -5.1043258 -4.1797724 -3.4646866 -2.8853693 -1.4960837 -3.4590662 -4.7988791 -7.9931841 -7.9698009][-5.5363226 -6.483264 -6.257277 -5.1720018 -5.0214086 -5.117887 -4.9575262 -4.4590831 -3.5441887 -2.8903017 -1.5332842 -3.5367565 -5.2323117 -7.9960995 -8.2426567][-5.525177 -6.7188768 -6.4873872 -5.0860262 -4.9548044 -5.1826725 -4.91792 -4.0056968 -3.6610618 -2.9982431 -1.7455018 -4.2748384 -6.3076549 -9.1193161 -8.7043409][-5.8074379 -6.9293971 -6.7208757 -5.0309386 -4.4806032 -3.7991087 -3.8853815 -3.8830154 -3.0703707 -2.3988323 -1.6114702 -4.0010366 -6.7885804 -10.475012 -10.332199][-6.926528 -7.05254 -6.2758417 -4.4466915 -3.0980096 -1.9667439 -1.7365189 -1.5063348 -1.6918044 -1.3312984 -0.67845774 -3.7459641 -6.8834615 -9.7383327 -9.5346088][-5.2273097 -4.8974 -4.5562096 -2.605407 -1.5097632 -0.15310001 0.31378174 -0.00046348572 0.018322468 -0.048518181 0.18162441 -2.3384473 -5.4322276 -8.4529591 -8.2087831][-4.0147972 -3.2126453 -2.6392853 -1.29631 -0.24534559 0.46948004 0.77739 1.1734495 0.76694489 1.0404711 1.5716715 -0.9659543 -3.8234632 -7.5470271 -7.8006487][-4.2520018 -3.2056508 -1.5428834 -0.41783237 -0.92589617 -0.30527925 0.0898695 -0.17531061 -0.389894 0.009542942 1.0929818 -0.7135191 -3.44745 -7.1738849 -7.445549][-3.7059431 -2.9642303 -1.8401682 -0.95792484 -1.785742 -2.356672 -1.9864132 -1.3902283 -1.5159745 -0.30202293 0.40365458 -1.8036103 -3.4038913 -7.4984617 -7.7609859][-5.6863422 -3.95829 -2.7151566 -2.3448586 -3.3977582 -4.5729094 -4.8852262 -4.0225954 -2.5625381 -1.5668864 -0.99204063 -3.2883792 -5.6078119 -7.9192233 -7.2977743][-8.6185274 -5.9690042 -3.2751846 -3.0944984 -4.8909712 -6.4511161 -6.2360268 -5.6086535 -4.8502855 -3.3444862 -2.3804395 -3.8806758 -5.6826329 -7.7485294 -7.6843185][-9.6615582 -8.52168 -7.599021 -6.2748971 -6.5332856 -7.8292608 -8.0136776 -6.9754257 -5.5658627 -5.3965068 -5.21542 -4.9290533 -5.4403377 -7.3474388 -7.3574824][-8.3184958 -8.0876913 -7.483108 -6.4127245 -5.3359156 -6.2113638 -7.2496138 -6.8803525 -5.7609611 -5.2014918 -5.0663033 -4.7137566 -4.1672983 -6.1016755 -7.1292629][-5.8813944 -6.2479849 -6.0678172 -4.6555767 -4.3841019 -5.1330862 -4.9126639 -5.0533457 -5.1348495 -5.5494514 -5.5928822 -5.7945585 -6.7090735 -7.7689028 -8.8715115]]...]
INFO - root - 2017-12-15 23:22:08.668144: step 70810, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 48h:21m:30s remains)
INFO - root - 2017-12-15 23:22:15.229073: step 70820, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 47h:19m:28s remains)
INFO - root - 2017-12-15 23:22:21.885664: step 70830, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 49h:35m:56s remains)
INFO - root - 2017-12-15 23:22:28.408568: step 70840, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 47h:00m:17s remains)
INFO - root - 2017-12-15 23:22:34.978318: step 70850, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 48h:17m:18s remains)
INFO - root - 2017-12-15 23:22:41.599291: step 70860, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 47h:40m:52s remains)
INFO - root - 2017-12-15 23:22:48.163904: step 70870, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.673 sec/batch; 48h:54m:27s remains)
INFO - root - 2017-12-15 23:22:54.729976: step 70880, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 48h:35m:38s remains)
INFO - root - 2017-12-15 23:23:01.340560: step 70890, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 47h:33m:53s remains)
INFO - root - 2017-12-15 23:23:07.945025: step 70900, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 48h:04m:43s remains)
2017-12-15 23:23:08.452972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.3016348 -9.0916309 -8.085783 -7.8048816 -8.799428 -8.1748047 -7.4449863 -6.9638004 -6.8317132 -7.354074 -7.225687 -8.81893 -11.316135 -10.506651 -9.0029116][-9.1611052 -8.6105518 -7.7894182 -6.6332588 -6.9302549 -7.4641824 -7.2902164 -6.6845675 -6.2317414 -6.4176264 -6.6796722 -8.6461248 -10.882915 -11.222037 -11.446142][-7.3525791 -6.6983929 -5.4823756 -4.6749935 -6.0102715 -5.916719 -6.108 -5.3387403 -4.7245579 -4.9598913 -5.677557 -8.03411 -10.828815 -11.461321 -11.50692][-7.5729308 -6.0769773 -4.7365541 -4.1334109 -5.0831885 -5.5243926 -5.8179841 -5.2153988 -5.3185186 -4.9189825 -5.2208271 -7.1314993 -9.3428049 -10.458971 -10.794973][-8.8503094 -8.4283915 -7.2325592 -5.5602045 -4.8286247 -4.0257063 -3.3306594 -3.7893806 -6.2954288 -5.9413309 -5.2593293 -6.3910551 -8.22161 -9.43686 -10.380143][-11.628798 -9.826807 -8.01459 -5.3302126 -2.887609 -0.094472408 1.7178168 0.93203449 -1.6390085 -3.5065536 -5.0792542 -5.625874 -7.2224183 -8.3909492 -9.5950451][-11.235545 -9.2606153 -6.3967223 -2.8247955 -1.179162 0.93648911 3.214582 4.1768689 3.6470485 0.39897776 -3.2088602 -4.8716946 -7.33224 -8.54779 -8.5704479][-10.462725 -7.5294318 -3.9207716 -1.1209064 0.5841713 2.4774165 4.1690249 5.1611342 5.1786675 2.6478467 -0.050604343 -3.3347704 -7.4303522 -8.6989346 -8.9530354][-7.6519871 -6.1282363 -3.3625331 -0.60854816 0.2815733 1.3175111 2.5645232 2.65898 0.78302383 -0.69303894 -1.3198504 -4.7335505 -8.65745 -9.7012167 -10.10605][-6.6781731 -5.3521738 -3.7414708 -2.1339169 -0.93008471 -0.31027603 1.4177818 1.0981498 -1.5246434 -2.9293475 -4.4653106 -6.9185381 -9.14614 -10.140957 -10.530784][-6.8266859 -6.3687115 -5.2531481 -3.6323607 -3.8104825 -3.7010632 -2.7969353 -3.2459886 -4.4537845 -5.5820923 -7.4499969 -9.8131618 -10.302576 -10.836906 -10.747345][-10.155544 -9.0805712 -7.9034009 -7.284235 -7.88159 -8.1633711 -8.0056 -8.7583561 -8.8639851 -9.0008793 -9.7219925 -11.265083 -11.010332 -11.158102 -10.742949][-13.621574 -12.944615 -11.981705 -10.709594 -10.61992 -10.79743 -11.284569 -11.463337 -11.287443 -11.067466 -10.589844 -10.495323 -10.206559 -9.7489643 -9.5005884][-11.727694 -11.179757 -10.787781 -10.884159 -10.59148 -10.474648 -10.363544 -10.058825 -10.135419 -9.7121658 -9.8134689 -8.818 -7.7968907 -7.63494 -7.4250536][-8.50946 -7.9456663 -6.7271652 -6.0055194 -5.7007179 -6.1469517 -6.4716973 -6.792778 -6.6458635 -6.7777267 -7.0774388 -6.9433479 -7.1224761 -7.77168 -7.4094496]]...]
INFO - root - 2017-12-15 23:23:15.087341: step 70910, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 47h:57m:19s remains)
INFO - root - 2017-12-15 23:23:21.740869: step 70920, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 48h:39m:44s remains)
INFO - root - 2017-12-15 23:23:28.298250: step 70930, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 47h:56m:02s remains)
INFO - root - 2017-12-15 23:23:34.892419: step 70940, loss = 0.11, batch loss = 0.06 (12.1 examples/sec; 0.663 sec/batch; 48h:12m:20s remains)
INFO - root - 2017-12-15 23:23:41.441628: step 70950, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 47h:29m:10s remains)
INFO - root - 2017-12-15 23:23:48.035889: step 70960, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 48h:15m:24s remains)
INFO - root - 2017-12-15 23:23:54.674744: step 70970, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 47h:41m:53s remains)
INFO - root - 2017-12-15 23:24:01.307533: step 70980, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.682 sec/batch; 49h:31m:14s remains)
INFO - root - 2017-12-15 23:24:07.958534: step 70990, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 49h:23m:55s remains)
INFO - root - 2017-12-15 23:24:14.637915: step 71000, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 49h:05m:12s remains)
2017-12-15 23:24:15.158490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8753748 -1.5588951 -2.100034 -2.8626854 -3.7109325 -3.9707608 -3.8654239 -3.9019084 -3.9698024 -4.3692036 -4.6572733 -4.0458832 -5.9580808 -7.2122259 -6.5343451][-2.8988378 -1.677413 -2.1011083 -3.42121 -4.3604975 -5.0439663 -5.3523378 -5.4831448 -5.2687936 -5.2798595 -5.4211216 -5.7025218 -8.0180655 -9.6672583 -8.6734219][-2.2153094 -2.1437972 -3.3451865 -3.0590529 -3.7970488 -5.3932233 -6.3327026 -5.9165359 -5.7743049 -5.5374937 -5.6538992 -5.23868 -6.8875322 -9.2478647 -8.9821644][-3.6928186 -4.6916356 -5.7047248 -5.6163163 -6.3066249 -6.2656937 -5.8283572 -5.4197888 -4.8890533 -4.9547696 -5.38761 -4.9407663 -6.65644 -7.4427247 -6.6557951][-5.2148037 -7.3283958 -8.9671106 -8.9005127 -8.6137619 -6.567081 -4.2338743 -3.3212593 -3.0958371 -3.0318651 -3.22905 -3.54194 -5.3909187 -6.114655 -5.4553494][-5.2029939 -7.1185942 -7.8442793 -8.0610523 -6.5993552 -2.3182833 0.9669714 2.2660422 2.6729684 1.4050331 0.14434814 -0.016502857 -2.2168734 -4.7067366 -5.1648993][-5.0376973 -4.5413275 -5.6880088 -5.91881 -4.1183114 0.15598631 3.8146367 6.4113736 6.5763297 3.4944739 0.62583351 0.83914852 -1.1655955 -3.5258856 -4.7461228][-3.5942774 -2.9314032 -3.4775608 -4.2088013 -3.0281231 -0.046083927 4.4319062 7.4542613 7.4481406 4.6412177 1.7025461 0.18190241 -3.1208549 -5.0996122 -6.2415323][-1.7650261 -0.99384642 -1.5076509 -1.9321022 -2.6637146 -1.5454707 0.64612532 4.1137652 5.9375587 4.4019513 2.4160132 0.37067223 -3.3238964 -6.1747293 -7.4743886][-2.9527628 -2.6891351 -3.0242429 -2.8374603 -3.3049681 -2.8011894 -1.0829105 -0.416739 -0.059961796 0.72689581 0.90489864 0.39843082 -2.9515202 -6.900414 -8.7188921][-6.5835228 -5.8705931 -6.5374012 -6.004343 -6.848227 -6.3539476 -5.8451533 -5.129715 -4.3592787 -4.4363337 -4.1255989 -3.801307 -6.7246761 -8.336422 -8.922739][-10.805552 -9.91515 -9.0418425 -9.2037258 -8.9856815 -9.4035673 -9.2027836 -8.4320984 -8.4335966 -8.3562145 -8.7663584 -8.0588551 -8.6752472 -9.1536636 -9.37615][-12.512806 -11.632362 -11.187656 -9.4321527 -9.1066132 -7.8238592 -7.5564914 -8.3047218 -8.912385 -8.7423534 -8.8577909 -9.3342571 -10.008201 -9.1150522 -7.5343962][-9.5212936 -9.0573959 -8.1706524 -7.8762069 -7.7021227 -7.2647481 -7.1117435 -7.156992 -6.7535105 -6.9788103 -6.9453287 -6.5489817 -6.7968965 -7.5520267 -7.3421335][-6.1539536 -6.8428278 -5.7448521 -4.2922626 -2.533582 -2.6558661 -3.2325046 -3.2883248 -3.6462865 -4.4850845 -4.6723027 -5.2046905 -7.06336 -7.1916714 -6.5028486]]...]
INFO - root - 2017-12-15 23:24:21.793024: step 71010, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 46h:45m:33s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 23:24:28.432429: step 71020, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 47h:21m:39s remains)
INFO - root - 2017-12-15 23:24:35.121349: step 71030, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 48h:57m:33s remains)
INFO - root - 2017-12-15 23:24:41.809130: step 71040, loss = 0.18, batch loss = 0.14 (11.5 examples/sec; 0.693 sec/batch; 50h:18m:51s remains)
INFO - root - 2017-12-15 23:24:48.406398: step 71050, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 47h:24m:12s remains)
INFO - root - 2017-12-15 23:24:55.011162: step 71060, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 47h:31m:22s remains)
INFO - root - 2017-12-15 23:25:01.582616: step 71070, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 47h:58m:32s remains)
INFO - root - 2017-12-15 23:25:08.176510: step 71080, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 47h:42m:33s remains)
INFO - root - 2017-12-15 23:25:14.793384: step 71090, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 48h:05m:11s remains)
INFO - root - 2017-12-15 23:25:21.423838: step 71100, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 48h:14m:30s remains)
2017-12-15 23:25:21.963774: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3899341 -4.3472385 -3.9216757 -4.1667366 -4.1883011 -4.2368641 -3.809289 -4.0805216 -3.7254739 -2.8521724 -2.6184227 -2.6464057 -3.8904123 -3.5643854 -4.079299][-3.3193905 -3.8354652 -3.4447789 -2.7099345 -1.9290016 -1.8368301 -2.2167251 -1.6861601 -0.76133871 -0.81674194 -0.96812773 -1.25769 -2.0457907 -2.3990207 -3.4416933][-3.1204772 -4.5987349 -5.4592514 -4.5043826 -3.2913265 -2.1162107 -1.883939 -1.7523661 -1.8088901 -1.8219104 -1.7816808 -2.6425931 -4.3429365 -4.0967221 -4.9821334][-4.9021044 -6.272294 -6.6609421 -6.31683 -4.9501009 -2.9886405 -1.7176266 -1.7658689 -2.1988807 -2.2288177 -2.4172878 -3.9721069 -6.2845716 -6.7830305 -7.55658][-5.3928318 -7.7647352 -8.6328878 -7.8739719 -5.769012 -2.5757647 -0.258317 -1.5012331 -2.7090755 -2.1234398 -2.2837017 -4.4517841 -7.8715181 -10.062712 -11.373491][-7.3874054 -7.4014368 -7.697113 -5.3942175 -2.3811955 0.29991674 3.6775346 2.9824758 0.45355225 -1.2689137 -2.8687766 -3.8561912 -7.5222116 -10.148453 -11.814695][-6.7884159 -6.7220564 -6.0673709 -3.980104 -0.16626549 4.090528 7.0709805 6.92506 5.2107377 0.90191746 -2.5631278 -4.241334 -7.5086036 -10.196676 -12.142471][-6.4266911 -6.0011635 -5.6810832 -3.3511975 0.061556816 3.7419744 7.6044316 8.3959618 6.0216317 2.5815177 -0.69811869 -4.371067 -8.1497746 -9.6896849 -10.902185][-5.7515264 -4.4477978 -4.0734043 -2.4425309 -1.1084733 2.2614636 5.8766189 5.8051476 3.3022752 0.29374838 -2.3891501 -6.3688221 -9.897541 -10.585587 -11.166836][-4.3700247 -4.4303675 -4.3100533 -2.7293875 -2.4840484 -0.86717272 1.7539573 2.5574841 0.74611521 -2.2144146 -4.9825811 -8.0800457 -11.358357 -12.911999 -13.833605][-8.4486942 -7.7277603 -7.7025957 -6.7100434 -5.5092106 -6.0621166 -5.582819 -4.7639709 -5.1365838 -5.9156365 -7.9324503 -11.073026 -12.800477 -12.865911 -13.069178][-12.749384 -12.407192 -11.935268 -11.334272 -11.60269 -11.832707 -11.520617 -11.297441 -10.630167 -9.9179039 -10.595897 -11.492212 -12.551077 -12.826063 -13.011845][-15.162294 -14.600853 -14.103224 -13.552737 -14.806225 -14.152796 -13.633133 -13.957678 -13.495933 -11.939873 -11.689697 -11.525206 -11.952995 -11.132316 -10.735725][-13.481447 -13.618518 -12.703844 -12.036802 -12.939792 -13.318609 -13.643588 -13.393042 -11.988879 -11.380585 -11.003086 -9.6907272 -9.3288994 -8.9631319 -8.3743877][-9.3943758 -9.41198 -9.1071692 -8.8832626 -7.9531088 -8.017334 -8.8054962 -8.968586 -9.1651773 -8.78699 -8.4995556 -9.1695576 -9.5507631 -8.3669605 -8.2013721]]...]
INFO - root - 2017-12-15 23:25:28.555935: step 71110, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 47h:11m:25s remains)
INFO - root - 2017-12-15 23:25:35.099117: step 71120, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 48h:37m:22s remains)
INFO - root - 2017-12-15 23:25:41.752898: step 71130, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 48h:02m:29s remains)
INFO - root - 2017-12-15 23:25:48.394941: step 71140, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 48h:03m:49s remains)
INFO - root - 2017-12-15 23:25:54.988054: step 71150, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 47h:08m:31s remains)
INFO - root - 2017-12-15 23:26:01.634927: step 71160, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 48h:59m:26s remains)
INFO - root - 2017-12-15 23:26:08.223263: step 71170, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 48h:25m:29s remains)
INFO - root - 2017-12-15 23:26:14.805102: step 71180, loss = 0.19, batch loss = 0.14 (11.8 examples/sec; 0.678 sec/batch; 49h:11m:53s remains)
INFO - root - 2017-12-15 23:26:21.505978: step 71190, loss = 0.19, batch loss = 0.15 (11.8 examples/sec; 0.677 sec/batch; 49h:08m:47s remains)
INFO - root - 2017-12-15 23:26:28.070589: step 71200, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 47h:33m:27s remains)
2017-12-15 23:26:28.575406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1859479 -3.8149633 -3.7482009 -3.4896302 -4.2073812 -4.6913328 -4.86543 -4.5679917 -3.7551436 -2.5736897 -1.9495461 -4.0064793 -6.6740074 -7.8341341 -7.7538013][-3.0882714 -3.33056 -3.3206086 -3.5758412 -4.0090628 -4.9393854 -4.8551664 -3.514117 -2.8526204 -2.28444 -1.5282106 -3.2494912 -6.129684 -7.5076566 -7.47373][-2.8673294 -3.0631928 -3.5290864 -3.7364101 -4.5096269 -4.9152083 -4.6638465 -4.0614071 -3.1980624 -2.7701452 -2.36181 -4.0792317 -6.3171086 -7.1077785 -7.5503654][-2.7729957 -3.702039 -4.0206127 -3.9808264 -4.6249123 -4.5043769 -4.1180015 -3.8684759 -3.6920388 -2.9206583 -1.8587556 -3.2414217 -5.2552848 -6.3667459 -6.7370996][-3.8489015 -4.8563461 -5.6349454 -4.5789862 -3.8034227 -2.3397772 -1.3653278 -1.4266481 -1.7478011 -1.8098278 -1.8811409 -2.7695227 -3.9091325 -4.7095876 -5.0908327][-4.343678 -5.2371545 -4.632844 -3.5380626 -1.4352069 1.3391938 2.8687282 1.9314342 0.853179 -0.024201393 -0.60627079 -1.6903071 -3.4157281 -3.9097505 -4.0121746][-4.1626692 -4.4494138 -3.9878769 -2.5135491 -0.65775347 2.3483205 5.064527 5.1742063 4.2182574 1.4865522 -0.39550829 -2.1675935 -4.37497 -4.2968054 -4.0431366][-4.1121993 -3.4020565 -2.2729475 -1.3347316 -0.30979156 2.5628018 4.9346442 5.4458432 5.2306113 2.3427434 -0.33505249 -2.5692677 -4.87866 -5.1365633 -4.6767964][-3.3565722 -2.8016312 -1.9942946 -0.35108471 0.30315781 1.3851466 3.0258317 4.4483314 4.4843373 2.2083907 0.029884338 -3.827858 -7.4659538 -7.337986 -6.1514091][-3.9583886 -2.937916 -1.9562619 -0.46210241 0.003279686 0.86648273 1.9300442 2.6483827 2.2016411 0.00093889236 -2.0967927 -5.907732 -9.3060389 -9.7952414 -9.3105593][-6.495872 -6.0902958 -4.6999178 -3.2333496 -3.1119304 -2.3893037 -2.0327868 -1.9656417 -2.5505168 -3.7838748 -5.2293186 -9.5310125 -12.192286 -11.88528 -10.432215][-9.7383509 -9.5938272 -8.5548286 -7.2270083 -6.2791157 -4.7119169 -4.8193045 -5.50411 -6.471199 -7.551898 -9.0770226 -10.913125 -11.590019 -12.348707 -11.626326][-11.652293 -10.867964 -9.6691236 -8.5762959 -7.6419916 -6.9577837 -7.1969252 -7.6158724 -9.0184956 -9.7351656 -10.56187 -11.549674 -11.748066 -10.779219 -9.7370129][-9.1115646 -9.4817724 -8.6747541 -7.493763 -6.3460655 -6.3812785 -6.8555045 -7.5444174 -8.2557859 -9.02769 -9.7145119 -9.096241 -8.4677944 -8.274497 -8.4098768][-6.2311325 -6.2269206 -5.7133179 -5.4287434 -5.2882166 -5.2107716 -4.6587753 -4.962 -5.7608352 -6.1384549 -6.5047069 -7.0036473 -6.9477086 -7.3623481 -9.1528988]]...]
INFO - root - 2017-12-15 23:26:35.212698: step 71210, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 47h:03m:59s remains)
INFO - root - 2017-12-15 23:26:41.797724: step 71220, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 47h:53m:51s remains)
INFO - root - 2017-12-15 23:26:48.382346: step 71230, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 48h:58m:24s remains)
INFO - root - 2017-12-15 23:26:54.942282: step 71240, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 48h:23m:34s remains)
INFO - root - 2017-12-15 23:27:01.548043: step 71250, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.635 sec/batch; 46h:03m:07s remains)
INFO - root - 2017-12-15 23:27:08.183287: step 71260, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 47h:34m:21s remains)
INFO - root - 2017-12-15 23:27:14.800853: step 71270, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 47h:58m:59s remains)
INFO - root - 2017-12-15 23:27:21.404289: step 71280, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 48h:24m:37s remains)
INFO - root - 2017-12-15 23:27:28.034925: step 71290, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 47h:22m:18s remains)
INFO - root - 2017-12-15 23:27:34.646065: step 71300, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 47h:28m:08s remains)
2017-12-15 23:27:35.207015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.2932978 -9.0966969 -8.2328224 -6.4303865 -6.4087892 -6.6482468 -7.1042042 -6.983706 -6.7204537 -7.1253438 -7.5153327 -7.4461288 -6.8465977 -5.2586164 -2.1985927][-7.9915066 -7.3247886 -5.8160729 -3.9133391 -4.1856861 -4.0883069 -4.2650127 -4.2807727 -4.7095346 -4.9983754 -4.4078112 -4.4021349 -4.4493656 -3.46539 -1.1592302][-6.0148869 -7.0411911 -5.961412 -3.1894817 -2.929033 -2.8996255 -2.4608896 -2.0642483 -2.8250532 -3.7482381 -4.2315645 -4.0588617 -3.8771312 -4.1052957 -2.0529208][-5.4069943 -6.4311028 -6.3292875 -4.3906631 -4.9414258 -4.1398 -3.1466415 -3.6515281 -4.1268454 -4.529201 -5.1860981 -4.9873071 -4.556078 -4.6645842 -2.6877599][-7.7681322 -7.8079438 -7.4430642 -6.4542365 -5.9293923 -3.0118427 -1.1694341 -2.3887479 -4.1147833 -4.8670349 -5.5601845 -5.3834119 -5.1054373 -5.495873 -3.7871015][-9.3644037 -7.430872 -5.509798 -3.4457669 -1.7158904 1.2501392 4.2293105 4.3244681 2.7903476 -0.87277937 -3.9335632 -3.6659632 -4.2767153 -5.5904121 -4.43289][-10.878218 -8.3899727 -5.944829 -3.0960715 -1.4657626 2.9238343 7.4531827 7.2454314 5.4854016 0.98273182 -3.3571048 -3.8446932 -4.8720579 -5.6022291 -4.6890335][-11.059813 -10.13243 -9.3127041 -5.8351941 -2.3232265 2.156291 6.3109012 7.2462506 6.4342685 1.7585707 -2.3089383 -3.854691 -5.70805 -6.5172009 -5.1092248][-8.600316 -7.4256668 -6.5884838 -4.5959854 -2.7489221 1.2206163 4.9952073 5.5732064 3.339005 -1.2445493 -4.6454487 -5.7633581 -7.234169 -7.3339272 -5.3715177][-6.2213564 -5.8012319 -5.8911524 -4.2238135 -2.6885197 -0.017035484 1.9306908 2.2267265 0.37781763 -3.2727437 -5.6345758 -6.2624636 -6.9053764 -8.0026369 -6.43892][-8.3680878 -7.4368854 -6.4419236 -5.6737065 -4.9249048 -3.2227149 -2.0571918 -2.2883523 -3.5289147 -5.0891633 -6.2137003 -6.923789 -7.8754029 -8.9552975 -7.8576088][-13.489262 -12.685869 -12.437721 -11.329103 -10.469932 -9.3608608 -8.5652761 -8.7515993 -9.6610775 -10.521724 -10.555128 -9.3986492 -8.733263 -9.2498951 -8.3601017][-12.720778 -12.212029 -12.022102 -11.349981 -11.181723 -10.288498 -10.038498 -10.458448 -10.782125 -10.446879 -9.451313 -8.4406395 -7.5825319 -7.9933743 -6.8060536][-8.9003725 -9.3328152 -9.3830376 -8.2730875 -7.99216 -7.289875 -8.1301212 -7.920383 -7.4448047 -6.8805861 -6.2249246 -4.6105108 -4.2894888 -4.7509408 -3.9095488][-7.64966 -6.5916495 -5.517169 -4.9855719 -4.7514277 -4.2634144 -4.7868032 -5.021944 -4.9029837 -4.5899181 -4.0656896 -3.8640592 -4.5237656 -5.0926375 -4.7998161]]...]
INFO - root - 2017-12-15 23:27:41.798243: step 71310, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 46h:30m:38s remains)
INFO - root - 2017-12-15 23:27:48.365488: step 71320, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.669 sec/batch; 48h:31m:35s remains)
INFO - root - 2017-12-15 23:27:54.996127: step 71330, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 47h:16m:57s remains)
INFO - root - 2017-12-15 23:28:01.542072: step 71340, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 46h:40m:09s remains)
INFO - root - 2017-12-15 23:28:08.088006: step 71350, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 48h:51m:09s remains)
INFO - root - 2017-12-15 23:28:14.775837: step 71360, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 47h:43m:51s remains)
INFO - root - 2017-12-15 23:28:21.342291: step 71370, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 47h:54m:47s remains)
INFO - root - 2017-12-15 23:28:27.925113: step 71380, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 48h:11m:56s remains)
INFO - root - 2017-12-15 23:28:34.511177: step 71390, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 47h:57m:22s remains)
INFO - root - 2017-12-15 23:28:41.142228: step 71400, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 47h:22m:56s remains)
2017-12-15 23:28:41.636232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7531905 -4.9927154 -3.5958538 -2.7964759 -3.169744 -3.3957376 -3.4886014 -3.1500442 -3.1845472 -2.8784788 -2.494884 -5.1924067 -8.8034267 -9.6370068 -9.6372242][-8.0298977 -7.1401396 -6.1869073 -5.4081569 -5.339294 -5.1002984 -4.9438648 -4.7829666 -4.3349981 -3.3713176 -3.2373831 -5.9271684 -9.4210911 -11.308641 -12.173931][-6.113719 -7.0776916 -7.6160436 -6.4846058 -6.7323122 -6.4637723 -6.299396 -5.6581321 -4.817349 -4.6014438 -4.6999884 -6.7358408 -10.318691 -12.593165 -12.32031][-5.5347357 -6.2841682 -6.1232162 -5.7371812 -6.6725712 -5.9286857 -4.7915034 -4.7712851 -5.1715164 -4.9570227 -4.8555512 -7.7186594 -10.968018 -12.299204 -12.568048][-7.5515008 -7.8964515 -7.5835829 -5.9847064 -5.174407 -3.63888 -2.4082026 -2.6320515 -3.5271125 -4.2544317 -5.265058 -8.5286131 -11.707752 -12.69131 -11.999552][-9.56925 -9.0421848 -7.6147146 -5.4653673 -3.3235359 -0.5122838 1.1770344 1.3781166 0.93379736 -1.3823051 -4.0337181 -7.0396624 -9.9443092 -10.841865 -9.7577][-9.0814772 -9.1788206 -8.7959595 -5.6606517 -2.7984109 0.581676 3.4467187 4.62032 3.9865813 1.0652494 -1.7823203 -5.3629608 -8.7060146 -8.9871187 -7.8223057][-6.5051231 -7.0460777 -7.3731651 -4.3413925 -1.7114129 1.2814837 4.1682477 5.1296077 4.7023721 2.3560395 -0.54095173 -4.631639 -7.9534025 -7.75303 -6.9897408][-5.6777768 -6.061645 -6.0141997 -3.5933328 -0.65111494 2.2470455 3.9300418 4.7961011 4.1933007 1.4318452 -1.0115066 -5.025919 -8.2822285 -8.6700687 -7.1295505][-5.8992596 -5.9594793 -5.6015916 -3.6054964 -0.76782084 1.7028732 3.4509759 3.4373956 2.5375314 0.66725349 -1.0768237 -4.9417634 -8.96946 -9.96929 -9.3671722][-8.3573351 -8.008708 -7.37915 -4.7731247 -3.0125468 -1.3468943 0.08137846 -0.26864815 -1.8446567 -3.2663651 -4.0777192 -8.0374317 -10.514555 -10.387636 -9.3134212][-10.991054 -9.5920372 -7.9313831 -5.8125815 -5.2601705 -4.5305691 -4.3050222 -4.83422 -5.95412 -6.7735777 -7.3138485 -9.94958 -11.702215 -10.861567 -9.7061768][-11.666758 -10.14212 -8.4478111 -7.1663346 -7.0388827 -6.7429304 -7.0870256 -7.9616909 -8.8603754 -9.7247677 -10.018454 -11.142427 -11.986321 -10.744553 -9.2394][-9.6576977 -8.6421709 -6.8431015 -5.4664216 -5.2653351 -5.9378648 -7.3295026 -8.22193 -9.1702938 -9.8146191 -10.410778 -10.707216 -11.161502 -10.852221 -10.061163][-7.2713346 -7.568872 -5.8402734 -4.5957394 -4.0647783 -4.2296486 -4.8213649 -5.8550253 -6.9024043 -7.359129 -7.8058567 -10.016026 -11.676277 -12.238846 -12.698346]]...]
INFO - root - 2017-12-15 23:28:48.194702: step 71410, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.632 sec/batch; 45h:52m:12s remains)
INFO - root - 2017-12-15 23:28:54.877964: step 71420, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 46h:37m:59s remains)
INFO - root - 2017-12-15 23:29:01.488064: step 71430, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 47h:32m:43s remains)
INFO - root - 2017-12-15 23:29:08.103012: step 71440, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 49h:00m:11s remains)
INFO - root - 2017-12-15 23:29:14.762856: step 71450, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.692 sec/batch; 50h:12m:55s remains)
INFO - root - 2017-12-15 23:29:21.354521: step 71460, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 46h:53m:46s remains)
INFO - root - 2017-12-15 23:29:27.988203: step 71470, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 48h:55m:22s remains)
INFO - root - 2017-12-15 23:29:34.474296: step 71480, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 46h:32m:33s remains)
INFO - root - 2017-12-15 23:29:41.078663: step 71490, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 48h:24m:28s remains)
INFO - root - 2017-12-15 23:29:47.624387: step 71500, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 48h:03m:30s remains)
2017-12-15 23:29:48.166789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0227637 -5.9071236 -6.892889 -7.1174855 -8.4750919 -8.9169464 -8.5564508 -9.2181559 -10.29908 -10.6469 -10.250076 -9.5796022 -8.5109711 -8.21133 -6.5313778][-6.6334519 -6.0245485 -6.4385748 -5.711412 -6.054111 -7.8342371 -8.9434891 -9.5369873 -9.3077841 -9.2851591 -9.4004145 -9.1983929 -9.5294514 -8.8543053 -5.2892036][-4.347147 -6.47436 -7.9806566 -6.5696492 -7.0059609 -6.9514589 -7.4542189 -8.5191126 -9.0969124 -9.7959938 -8.895277 -8.7515736 -8.6083755 -9.9315176 -7.7238088][-6.6836553 -7.0569925 -8.6519527 -8.4094877 -9.7832489 -8.5287895 -7.2647133 -8.0345917 -8.0548687 -7.1484318 -6.8440809 -7.9328961 -8.9103584 -10.182194 -9.2272778][-7.8182087 -9.3950768 -10.544844 -11.416595 -11.291295 -7.4974756 -4.5850787 -6.9160862 -8.563467 -6.9918284 -6.8071404 -7.2033467 -9.5306616 -11.913139 -10.641021][-9.0663719 -9.7432594 -10.318811 -7.96786 -5.7594395 -2.705044 1.8175287 0.77262545 -1.0273371 -4.6335869 -7.7267084 -6.3093081 -7.8522234 -11.663094 -11.02319][-12.062182 -9.1475649 -8.4115162 -5.0488462 -1.6622162 2.9226718 8.76474 9.0608463 8.2103653 0.40528488 -7.7423968 -8.694952 -10.502311 -12.563303 -11.399755][-13.308607 -11.929083 -10.417669 -5.367692 -1.5678935 5.1552863 12.86031 11.653727 10.137203 3.7584176 -3.6003563 -7.6987081 -11.894301 -13.734581 -12.20755][-10.729157 -10.419957 -11.005834 -6.4744277 -2.96042 3.2867732 8.7546234 8.0074062 7.2368436 1.16855 -5.1768308 -8.984684 -13.737917 -15.287642 -12.710266][-9.4632845 -8.6395578 -9.8970947 -9.0467987 -7.8401895 -2.441515 1.8363047 3.2165809 2.9477572 -3.3841205 -7.9856272 -10.424026 -14.099081 -16.645361 -14.891895][-13.195892 -12.609344 -13.394094 -12.37524 -12.447228 -10.000179 -7.3251104 -5.6849113 -5.9563794 -8.9758558 -11.430672 -14.033249 -15.658579 -17.122278 -15.71846][-19.098557 -17.758278 -17.460716 -17.256512 -16.505959 -14.78512 -14.266157 -13.278124 -12.816106 -12.822948 -13.194565 -14.954576 -15.760567 -15.853363 -13.272953][-20.074293 -19.144783 -18.381247 -19.047953 -17.606047 -15.095005 -14.911631 -14.715607 -14.298958 -13.939299 -13.483293 -13.762381 -13.534567 -13.358294 -9.89776][-15.003439 -15.214251 -14.066725 -12.844093 -11.3097 -11.482664 -11.109245 -10.104285 -9.8383074 -9.63994 -9.5035381 -10.338989 -10.7307 -10.465532 -8.080061][-10.603992 -9.5017223 -9.8634377 -8.9529352 -6.9631453 -6.2839494 -5.8360591 -6.141634 -7.062983 -6.9841223 -7.0597 -8.9463968 -10.126225 -10.37281 -9.2995405]]...]
INFO - root - 2017-12-15 23:29:54.711953: step 71510, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 46h:30m:51s remains)
INFO - root - 2017-12-15 23:30:01.206276: step 71520, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.641 sec/batch; 46h:28m:14s remains)
INFO - root - 2017-12-15 23:30:07.845354: step 71530, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 46h:31m:04s remains)
INFO - root - 2017-12-15 23:30:14.443126: step 71540, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 48h:18m:22s remains)
INFO - root - 2017-12-15 23:30:21.015004: step 71550, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 48h:07m:32s remains)
INFO - root - 2017-12-15 23:30:27.637084: step 71560, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 47h:53m:03s remains)
INFO - root - 2017-12-15 23:30:34.304567: step 71570, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 49h:03m:07s remains)
INFO - root - 2017-12-15 23:30:40.882030: step 71580, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 46h:56m:50s remains)
INFO - root - 2017-12-15 23:30:47.461793: step 71590, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 47h:33m:10s remains)
INFO - root - 2017-12-15 23:30:54.111573: step 71600, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 48h:36m:40s remains)
2017-12-15 23:30:54.664077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8223038 -5.4044747 -5.1919222 -5.0699162 -5.7504487 -7.181602 -8.1121922 -7.4021091 -7.2216725 -7.0341926 -5.9499674 -8.2452307 -11.736292 -11.687149 -10.687975][-4.2704988 -5.1032743 -5.6710649 -5.8722792 -6.6188264 -8.023387 -9.7912235 -10.341496 -9.8092756 -8.7739286 -7.9286041 -9.6349955 -12.967157 -13.260874 -11.976288][-2.2163138 -2.8249013 -3.4409287 -3.6567576 -4.9528055 -6.4029188 -7.4976215 -8.1160641 -8.9348888 -8.3321133 -7.3138666 -9.1723051 -12.271536 -13.345402 -12.435317][-3.5668297 -4.3068714 -3.9267936 -2.4259043 -2.0977557 -2.1879735 -3.2193866 -4.2449083 -4.5789375 -4.9296427 -5.012629 -6.5011806 -9.7313662 -10.82682 -10.748299][-5.5698895 -5.8206887 -5.8861361 -3.5850525 -1.1432157 -0.39811373 -0.68824482 -0.8924861 -0.701396 -1.1091962 -1.6141081 -4.7707906 -8.7435923 -9.4536562 -8.9414463][-8.6056566 -8.3720284 -6.2454386 -3.9441681 -2.411087 0.46751928 2.683146 2.7187619 3.0645576 2.4227958 2.1338205 -1.6934576 -6.5182614 -8.2105522 -9.354517][-9.1643438 -8.6473722 -6.8152561 -4.076406 -1.736589 1.100503 3.0542026 4.5055423 6.2730756 5.3599238 3.6625419 -0.93995094 -5.9003882 -6.8927703 -7.6041303][-6.9715691 -6.9939861 -6.773262 -3.3873394 -0.17396736 2.4429555 4.1190104 4.51798 5.3176694 5.0644174 3.7229056 -0.73091078 -5.4071302 -6.5912962 -6.1767244][-7.1833749 -6.3344507 -4.9249158 -2.3709934 -0.39097118 0.8711071 1.9788709 3.3597083 3.767179 3.1242089 2.2516732 -1.8320515 -6.3504858 -6.5096827 -5.4914961][-6.529264 -5.6851544 -3.8251696 -2.4410071 -1.3170605 -0.42955256 -0.026610851 0.025621891 0.577847 0.85197687 1.0508814 -2.2054381 -6.3478985 -6.210361 -4.6026416][-7.3789611 -7.0345726 -6.1849141 -5.0747404 -4.5225911 -4.4121871 -4.6772561 -4.4086218 -3.6954467 -3.076031 -2.6674383 -5.5062208 -8.4386415 -7.8010616 -5.1387343][-10.820457 -9.1991215 -6.7352328 -5.6655507 -5.2523165 -4.8185954 -5.6576385 -7.05595 -7.5402193 -6.9181833 -6.6811743 -8.061204 -10.069836 -9.31092 -7.52141][-11.005347 -8.1984024 -6.1531053 -5.5780926 -5.8508306 -5.8729687 -6.5909986 -6.8860779 -7.1190343 -6.8222027 -6.6602721 -7.4386892 -9.5085011 -8.2024651 -6.1056714][-11.600346 -9.7841644 -7.5391574 -6.5013933 -6.4699993 -6.7440524 -6.5853615 -5.4717703 -3.7545629 -4.0354137 -5.4233241 -5.2733855 -5.1358914 -4.2931404 -3.8384433][-7.3754525 -7.1798697 -6.3488336 -5.4806843 -5.6269765 -5.2030854 -4.7630072 -4.916749 -4.1382475 -2.007345 -0.70165873 -3.0764701 -4.5274153 -4.1444941 -4.2523527]]...]
INFO - root - 2017-12-15 23:31:01.270317: step 71610, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 48h:35m:39s remains)
INFO - root - 2017-12-15 23:31:07.841927: step 71620, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 47h:10m:48s remains)
INFO - root - 2017-12-15 23:31:14.475480: step 71630, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 47h:52m:00s remains)
INFO - root - 2017-12-15 23:31:21.040128: step 71640, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 47h:06m:51s remains)
INFO - root - 2017-12-15 23:31:27.631182: step 71650, loss = 0.20, batch loss = 0.16 (12.5 examples/sec; 0.641 sec/batch; 46h:24m:38s remains)
INFO - root - 2017-12-15 23:31:34.196611: step 71660, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 47h:53m:27s remains)
INFO - root - 2017-12-15 23:31:40.787164: step 71670, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 47h:48m:16s remains)
INFO - root - 2017-12-15 23:31:47.380153: step 71680, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.685 sec/batch; 49h:35m:49s remains)
INFO - root - 2017-12-15 23:31:53.961856: step 71690, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 47h:44m:36s remains)
INFO - root - 2017-12-15 23:32:00.461175: step 71700, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 47h:11m:17s remains)
2017-12-15 23:32:00.932203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9585285 -2.8470941 -2.0409544 -1.4982185 -1.5860171 -1.4599113 -1.6822085 -1.3905025 -1.6168261 -2.3407595 -3.5957353 -7.5885906 -10.65626 -9.00877 -8.272665][-4.59639 -4.7431173 -4.2294769 -3.1391859 -2.775842 -2.5140448 -2.4704406 -2.7813051 -3.0504396 -3.8505583 -5.0887508 -9.4999571 -12.275774 -11.083042 -9.8059006][-3.8985817 -4.4741917 -4.8362088 -3.505506 -3.1284261 -3.4751258 -3.5105228 -2.79888 -2.4699588 -3.8450193 -5.1846781 -10.279316 -13.364338 -11.752606 -10.74482][-6.5438943 -6.0284829 -4.9941125 -3.9247012 -4.5282931 -4.15354 -3.6092117 -4.1122379 -5.0110497 -4.994463 -5.364717 -10.041161 -13.301369 -12.323997 -10.206861][-7.7977133 -8.1749725 -7.7287979 -5.696506 -4.0726271 -2.3567598 -1.7174172 -3.1203837 -5.5289383 -5.7771349 -5.9154825 -9.7162724 -12.578785 -12.208905 -10.590853][-8.9394007 -8.6001558 -6.6391721 -3.9628043 -1.612206 0.83777666 2.4888806 2.1196566 -0.53607368 -3.6431878 -6.5806003 -9.8812771 -11.811157 -10.959541 -10.624051][-10.021505 -9.0830908 -7.3969722 -3.3055422 0.34071636 3.4827733 6.131969 5.3465695 2.659802 -1.4170599 -5.3272557 -9.7731237 -12.452515 -10.871853 -10.295565][-10.592263 -9.2028065 -6.5985932 -2.6981986 0.83016729 5.0854735 8.2391148 7.1478305 5.1329818 1.2733736 -2.8932164 -8.7973232 -11.549082 -9.9394283 -9.2463293][-9.1692581 -7.9839525 -6.6248007 -2.6882517 0.53865004 4.0575662 6.0724692 6.0578475 5.1056457 0.87468004 -2.7152376 -8.3186617 -11.873665 -10.907242 -8.9789114][-8.1093178 -6.9096694 -5.5317082 -3.3912904 -2.1948671 -0.22084713 1.8759651 2.7188449 1.5403628 -1.0668311 -3.1498942 -8.5651407 -11.845726 -10.766996 -9.808569][-8.1017275 -7.6265249 -6.4955926 -5.1454015 -4.7999229 -3.3465271 -1.6265988 -1.6295543 -2.7965519 -3.5450077 -4.5067711 -9.5617189 -11.468092 -10.10088 -9.0462723][-13.124399 -11.847179 -9.4583092 -8.5808592 -7.8193235 -6.7393565 -7.4380083 -7.7217379 -7.7516708 -8.2186842 -9.1360579 -10.447793 -10.10864 -9.7955027 -9.3316231][-12.021152 -10.848283 -9.3920412 -8.5074759 -7.5467143 -7.5502009 -8.0132008 -8.7726326 -10.203772 -10.03046 -9.8494282 -10.059202 -10.241379 -8.53342 -7.3354383][-9.7739925 -8.3268137 -7.90071 -6.9125395 -5.8992004 -6.7943406 -7.1447287 -7.9233704 -8.43218 -8.7615137 -9.3285151 -8.1718283 -8.1333389 -6.8832006 -6.2606831][-5.6087146 -5.2403436 -4.499639 -4.5060863 -5.8023849 -6.022109 -5.4656687 -6.6179695 -6.9274025 -6.8699584 -6.3566375 -7.3290238 -7.7237825 -7.5081034 -8.3829832]]...]
INFO - root - 2017-12-15 23:32:07.473279: step 71710, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.646 sec/batch; 46h:48m:47s remains)
INFO - root - 2017-12-15 23:32:14.055951: step 71720, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 47h:25m:30s remains)
INFO - root - 2017-12-15 23:32:20.538110: step 71730, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 46h:29m:13s remains)
INFO - root - 2017-12-15 23:32:27.159983: step 71740, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 48h:14m:27s remains)
INFO - root - 2017-12-15 23:32:33.796592: step 71750, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 47h:12m:43s remains)
INFO - root - 2017-12-15 23:32:40.409188: step 71760, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 47h:01m:04s remains)
INFO - root - 2017-12-15 23:32:46.978111: step 71770, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 47h:31m:02s remains)
INFO - root - 2017-12-15 23:32:53.610806: step 71780, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 48h:17m:43s remains)
INFO - root - 2017-12-15 23:33:00.239174: step 71790, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 46h:22m:48s remains)
INFO - root - 2017-12-15 23:33:06.764693: step 71800, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 48h:52m:04s remains)
2017-12-15 23:33:07.398854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6198833 -4.1337786 -5.5146322 -6.40736 -7.5848789 -8.1780605 -8.9429893 -9.1476851 -9.5598907 -9.6412811 -8.7027712 -9.6766644 -12.487181 -12.292746 -12.554476][-4.0345421 -6.0866828 -6.9697504 -7.62707 -7.9394941 -7.5698171 -8.0483 -9.4927473 -10.722655 -10.398407 -9.332756 -10.583717 -13.374933 -14.011797 -14.273624][-5.1153488 -7.6138844 -8.9090729 -9.0245724 -8.4852428 -7.7593784 -7.7862663 -8.3551168 -9.3431263 -10.537942 -10.36289 -10.829693 -12.826437 -13.599401 -14.243589][-7.0763187 -9.0531425 -10.175275 -9.7312183 -8.5359354 -6.7318864 -5.3140464 -6.5675182 -8.3433914 -8.5480213 -7.7046781 -9.1203547 -11.884765 -12.798342 -13.604919][-7.978137 -10.648394 -12.006863 -10.307645 -6.9470749 -2.8823035 -0.89827013 -3.3699799 -6.4113545 -7.1271753 -6.9512644 -7.5571289 -9.4708691 -11.021519 -12.291903][-8.2885389 -10.082547 -9.9335251 -8.0646534 -3.8751953 1.3429055 4.4309735 3.1435199 0.068712234 -3.3702452 -5.5492535 -5.7243228 -7.3175173 -8.67186 -10.099718][-7.650301 -9.56919 -8.3418131 -5.2331038 -0.71518612 4.2620606 7.8935313 7.6132712 5.3847766 0.21618509 -4.1870613 -5.5931354 -8.5080414 -8.6612043 -8.7275982][-7.6439571 -8.278985 -7.13208 -4.2362728 0.19115019 5.3200583 8.6714687 7.873857 6.3304448 2.8550305 -0.93210983 -4.76939 -9.09518 -10.338951 -11.003071][-6.7038064 -7.6359134 -7.4760413 -6.1693168 -2.2874582 2.1404877 4.5325208 5.1708722 4.223453 1.0954695 -0.71387625 -4.8389158 -10.541157 -12.287485 -13.529895][-4.2499485 -5.8903823 -6.8983545 -6.9172058 -4.7058396 -1.7426479 0.87061262 2.8534741 2.080514 -0.038452625 -1.5388465 -5.7314148 -10.844517 -13.431949 -16.019281][-7.9757056 -9.1438913 -10.281633 -9.7100487 -8.3552132 -6.7549353 -4.4028664 -3.2989423 -3.6347504 -4.231 -6.0579128 -10.134774 -13.625786 -14.663509 -15.323666][-12.996893 -13.297565 -12.883685 -12.609746 -12.21658 -10.529654 -9.2916451 -9.1656075 -9.316576 -9.1825457 -9.9570618 -11.481435 -13.245345 -14.693251 -14.999557][-13.788784 -13.186522 -12.746389 -13.260407 -13.440151 -12.264332 -12.158968 -11.592295 -11.106565 -10.990759 -10.951231 -11.17235 -11.875682 -11.1019 -10.495538][-11.742479 -11.475672 -10.724752 -9.2067442 -8.6685181 -9.37686 -10.176497 -9.8509054 -9.5672245 -9.8413429 -10.228447 -9.208477 -9.2403374 -9.3872013 -9.1886845][-8.7335081 -8.2802057 -6.9837608 -5.1535788 -4.6432943 -4.01554 -4.7219152 -5.8949223 -6.9043841 -7.0132084 -7.5254464 -8.7320309 -9.3232574 -8.12746 -7.24893]]...]
INFO - root - 2017-12-15 23:33:14.022256: step 71810, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 46h:25m:54s remains)
INFO - root - 2017-12-15 23:33:20.646875: step 71820, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 48h:47m:00s remains)
INFO - root - 2017-12-15 23:33:27.277528: step 71830, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 47h:30m:31s remains)
INFO - root - 2017-12-15 23:33:33.834299: step 71840, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 47h:14m:29s remains)
INFO - root - 2017-12-15 23:33:40.391543: step 71850, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.676 sec/batch; 48h:57m:01s remains)
INFO - root - 2017-12-15 23:33:47.012408: step 71860, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 48h:50m:55s remains)
INFO - root - 2017-12-15 23:33:53.740050: step 71870, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 47h:34m:21s remains)
INFO - root - 2017-12-15 23:34:00.286210: step 71880, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 46h:33m:13s remains)
INFO - root - 2017-12-15 23:34:06.890083: step 71890, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 48h:17m:49s remains)
INFO - root - 2017-12-15 23:34:13.555067: step 71900, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 49h:08m:08s remains)
2017-12-15 23:34:14.114855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0106091 -6.80577 -7.1420612 -6.9143877 -7.7369156 -8.5903187 -8.1435852 -7.2863183 -7.1054735 -7.4335952 -7.1451569 -6.6662164 -7.7398281 -8.5125713 -8.5250826][-5.8582411 -6.5339222 -7.7691021 -7.5298638 -7.8029928 -7.3237629 -6.5658255 -6.0374827 -6.3017869 -6.0315056 -4.7749615 -6.4990492 -8.2662449 -8.17655 -8.1829681][-5.6773024 -6.3829284 -7.0309525 -6.240171 -7.24503 -8.083848 -7.5444965 -6.2466078 -4.9312196 -4.4055653 -4.1118126 -5.045 -7.3607159 -9.184288 -8.4501057][-4.7123594 -6.8084311 -7.4864798 -6.8718195 -8.16297 -8.0711107 -7.4017839 -5.7948737 -4.2280765 -2.9130006 -3.2850776 -5.0982242 -6.164556 -6.6364427 -6.0441771][-5.3028359 -6.1151605 -7.3454647 -8.1841269 -7.4719391 -5.2897825 -3.6078827 -2.938669 -2.5021572 -1.804343 -2.2696202 -3.6462736 -5.2732887 -6.2444406 -5.5077744][-6.8143878 -6.8232789 -7.1489005 -5.9738345 -4.6642246 -0.0777154 3.4218783 2.9674745 1.6152959 -1.2497449 -3.3292463 -3.2340906 -4.6932125 -5.8629775 -5.1179252][-6.4364619 -6.2579184 -5.6485715 -4.0617218 -1.2692261 3.0642161 6.4953847 7.9994082 6.1903548 1.0414429 -2.9733958 -3.9642673 -5.3351603 -6.628974 -6.1588922][-7.7436051 -6.3667345 -5.3846011 -2.8461137 -0.76701689 3.2281556 6.9965539 8.305687 7.9591479 3.6979308 -0.95175076 -4.2817488 -6.6893482 -6.7137766 -6.1265359][-7.5342278 -6.1813579 -5.0376277 -3.1141951 -2.3997796 1.3996587 4.3753285 5.8045878 5.9555306 2.3843284 -1.2697048 -4.434463 -8.3147974 -9.7888641 -8.6056662][-6.3853097 -6.3837996 -5.785975 -4.8960371 -3.2624378 -1.0214329 1.26683 3.0764356 2.8794236 -0.22678089 -4.3105745 -7.6907096 -11.370035 -12.775833 -11.852463][-7.5530787 -7.8076744 -8.4496584 -6.8780527 -6.8065681 -5.0303617 -2.7018101 -2.0853019 -1.846324 -3.12106 -6.1208086 -10.019875 -13.477616 -14.777275 -13.573414][-12.6673 -11.667725 -10.870223 -10.514465 -10.77776 -10.081161 -8.9749994 -8.1954956 -8.013979 -8.5812941 -9.8507919 -10.446902 -12.094837 -13.304434 -12.9373][-14.671215 -14.069763 -12.754354 -11.622023 -11.973991 -11.964662 -12.130793 -11.753025 -10.329721 -10.229349 -10.715522 -11.170944 -11.597383 -11.428724 -10.343213][-12.318413 -12.225431 -12.224967 -10.928073 -10.522359 -11.032944 -11.314994 -11.224697 -11.477358 -10.392403 -9.2538128 -8.5142441 -8.6398439 -8.6296005 -7.9484138][-8.4170952 -8.4837027 -7.6572723 -7.8121567 -7.6746516 -7.9811039 -8.300601 -8.7688351 -9.1707878 -9.3372526 -9.0322018 -8.5620079 -8.0768356 -7.6830678 -7.7366567]]...]
INFO - root - 2017-12-15 23:34:20.729971: step 71910, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 48h:49m:29s remains)
INFO - root - 2017-12-15 23:34:27.292176: step 71920, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 47h:21m:33s remains)
INFO - root - 2017-12-15 23:34:33.921824: step 71930, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 48h:11m:01s remains)
INFO - root - 2017-12-15 23:34:40.530571: step 71940, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 47h:10m:48s remains)
INFO - root - 2017-12-15 23:34:47.047450: step 71950, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 47h:58m:21s remains)
INFO - root - 2017-12-15 23:34:53.678989: step 71960, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 48h:04m:44s remains)
INFO - root - 2017-12-15 23:35:00.310716: step 71970, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 47h:35m:40s remains)
INFO - root - 2017-12-15 23:35:06.897127: step 71980, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 48h:21m:04s remains)
INFO - root - 2017-12-15 23:35:13.582128: step 71990, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.683 sec/batch; 49h:24m:32s remains)
INFO - root - 2017-12-15 23:35:20.188608: step 72000, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 47h:24m:01s remains)
2017-12-15 23:35:20.751853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1584167 -5.3907976 -5.252275 -5.843267 -7.2187624 -7.7748451 -7.882947 -8.23515 -9.0263033 -9.4201174 -9.0950146 -7.5724225 -7.4293022 -7.3298125 -6.3697853][-6.2679071 -6.6405163 -6.4308457 -5.9305944 -6.3526335 -7.5070424 -8.1177731 -8.396122 -8.8218727 -9.5491314 -9.7300987 -8.2824974 -7.1618338 -7.26174 -7.3355074][-4.43923 -6.0880089 -7.3282237 -7.1040139 -7.1708322 -7.3692145 -7.1820555 -8.0984011 -8.9171238 -9.1210556 -9.599432 -8.271821 -8.2867327 -8.7349854 -9.1690416][-4.0712681 -5.8402572 -6.3265076 -6.2376966 -6.1476612 -6.065033 -5.6782846 -6.3666739 -7.8893414 -8.431963 -8.6244678 -8.0451326 -8.3850708 -9.3802853 -10.501287][-5.66716 -6.3061309 -7.0749311 -6.2190189 -4.2744303 -1.600636 0.32220364 -1.961844 -3.9900093 -5.308156 -7.0557117 -5.8145442 -6.0530353 -7.7562246 -8.95047][-7.5987659 -6.9035177 -5.8623629 -5.71081 -2.7514286 0.82356882 4.2660384 4.7507319 2.835465 -1.4539819 -5.925703 -5.3344326 -5.20914 -5.8253183 -6.3259306][-8.4064894 -7.2609382 -5.6699438 -3.4499249 -1.1792684 1.3132505 5.3193984 6.8356032 6.8529372 1.2182345 -5.1409426 -5.3511567 -7.1419449 -7.6541705 -6.4462605][-9.7782784 -8.5488472 -7.6508369 -5.0317917 -1.3763776 1.8175941 5.13825 6.3050761 6.4398246 2.7534356 -2.3868241 -5.1567054 -9.0187511 -10.075247 -9.4338932][-7.4346046 -7.0081558 -7.8730974 -7.1425152 -5.0181975 -1.3836703 2.3728166 3.7096629 4.3764224 0.90254354 -2.9481823 -4.8156424 -9.1907082 -11.565661 -12.11547][-6.5430746 -5.7020493 -6.2409921 -6.6985955 -7.2823863 -5.4340606 -2.6038177 -0.8396492 0.76713133 -1.8321404 -5.347578 -7.1367083 -9.1297235 -10.681618 -12.08918][-10.244194 -10.533846 -10.24733 -10.535485 -10.882988 -10.343428 -9.4919367 -8.1946135 -6.8622828 -7.2007589 -8.87089 -10.605385 -12.866913 -12.761258 -11.673366][-13.617241 -13.894304 -13.710949 -13.251949 -12.795898 -13.370972 -14.119114 -13.756887 -12.223325 -11.429136 -11.675913 -12.2554 -12.616621 -13.044938 -13.409929][-12.083879 -12.357883 -12.503164 -12.527761 -12.649275 -11.74882 -11.313195 -12.272744 -12.561111 -12.167019 -11.821998 -11.55865 -10.376987 -9.3008862 -9.1760244][-11.208273 -10.61772 -9.6291218 -9.485815 -9.6170425 -9.3908777 -9.284071 -9.0305834 -9.3616 -10.40711 -10.963374 -9.6343651 -8.2700453 -7.1896267 -6.0426369][-8.4810486 -8.0664415 -6.4992352 -5.1142712 -4.2061834 -3.7621629 -4.7962623 -5.2607808 -5.3036809 -5.3636818 -6.1715722 -7.2567844 -7.1440649 -6.551765 -7.0886021]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 23:35:27.397076: step 72010, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 48h:03m:22s remains)
INFO - root - 2017-12-15 23:35:34.041586: step 72020, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 46h:04m:45s remains)
INFO - root - 2017-12-15 23:35:40.662878: step 72030, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 48h:49m:50s remains)
INFO - root - 2017-12-15 23:35:47.322241: step 72040, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 48h:58m:41s remains)
INFO - root - 2017-12-15 23:35:54.017778: step 72050, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 46h:47m:54s remains)
INFO - root - 2017-12-15 23:36:00.630635: step 72060, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 48h:15m:48s remains)
INFO - root - 2017-12-15 23:36:07.240863: step 72070, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.689 sec/batch; 49h:49m:45s remains)
INFO - root - 2017-12-15 23:36:13.985038: step 72080, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 47h:29m:59s remains)
INFO - root - 2017-12-15 23:36:20.506086: step 72090, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.656 sec/batch; 47h:27m:07s remains)
INFO - root - 2017-12-15 23:36:27.084333: step 72100, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.658 sec/batch; 47h:37m:42s remains)
2017-12-15 23:36:27.672387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7148533 -7.1962738 -8.6864519 -9.3848114 -10.189627 -11.409858 -12.079622 -11.308969 -10.836987 -10.556303 -9.7294273 -11.527363 -13.269254 -13.491093 -13.631834][-8.1304684 -9.1552982 -9.9778614 -10.728521 -11.302677 -12.06089 -12.608257 -13.725958 -14.232029 -12.958559 -11.766634 -13.569563 -14.852501 -14.972887 -14.213278][-5.9634733 -8.8426266 -11.244544 -11.070124 -10.832651 -12.287188 -13.493668 -13.591114 -13.010851 -13.502155 -13.380087 -13.846151 -14.607113 -14.919161 -14.171585][-9.266283 -11.651939 -12.5762 -11.333965 -10.717357 -10.495375 -10.092806 -12.470203 -13.894039 -12.535917 -10.934549 -14.345636 -16.380053 -15.360199 -13.89934][-10.704 -13.464681 -15.087851 -13.384408 -10.073126 -5.1842103 -2.992579 -7.5352731 -11.928562 -12.590694 -12.258751 -13.177938 -13.79969 -15.438395 -15.735785][-14.206062 -15.373068 -15.354511 -12.619946 -8.8152733 -2.6820345 2.5500169 1.5204663 -1.6458607 -7.7984324 -12.884447 -13.413588 -13.681459 -14.171787 -14.253735][-16.820343 -16.6461 -14.823448 -9.1662865 -2.8002627 2.2069983 6.3216147 6.40669 4.8152747 -2.1177552 -9.0413456 -12.765357 -15.177425 -14.35265 -13.875168][-15.840097 -15.594137 -14.395626 -8.8748188 -1.8879285 5.7537169 11.255426 8.4875488 4.9996314 -0.42548323 -6.2197819 -10.758793 -13.655147 -14.875071 -15.761995][-13.189475 -13.07252 -13.080011 -8.9557905 -3.6768315 3.18883 9.0676079 8.9707336 5.6521173 -1.9656854 -8.2455845 -12.633258 -16.348585 -16.290707 -15.597994][-11.116009 -11.086599 -11.245859 -7.9069262 -4.3725157 -0.94312859 2.9796519 3.5535359 2.3003869 -2.7915545 -8.4252186 -13.716372 -17.48682 -17.675947 -17.691681][-12.827824 -13.923597 -15.076134 -12.844458 -10.460543 -6.9497447 -3.5192142 -3.9154186 -4.6759534 -6.816721 -10.140537 -15.634647 -18.53179 -17.804228 -16.78487][-17.346342 -16.869987 -16.99638 -16.067612 -15.385418 -13.127321 -11.182079 -11.296259 -11.312737 -12.514708 -13.789663 -16.190783 -17.271223 -17.14575 -16.3447][-17.712898 -15.806124 -16.441614 -18.048605 -17.769764 -15.663774 -14.29714 -14.225159 -14.515892 -14.428743 -14.315128 -15.494871 -15.512728 -14.428492 -13.36315][-14.850391 -13.938522 -13.498207 -11.966646 -11.895931 -13.498875 -14.232111 -13.263308 -12.604808 -12.908937 -13.342551 -13.537348 -12.948359 -11.806862 -11.0309][-10.675686 -9.5814152 -8.37044 -7.5119066 -7.0869064 -7.0816708 -7.6770587 -9.2728729 -10.449536 -10.008994 -10.058889 -11.958708 -12.310296 -12.499896 -12.677441]]...]
INFO - root - 2017-12-15 23:36:34.172784: step 72110, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 47h:04m:56s remains)
INFO - root - 2017-12-15 23:36:40.722972: step 72120, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 46h:55m:14s remains)
INFO - root - 2017-12-15 23:36:47.394035: step 72130, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.680 sec/batch; 49h:12m:47s remains)
INFO - root - 2017-12-15 23:36:53.924496: step 72140, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.686 sec/batch; 49h:35m:54s remains)
INFO - root - 2017-12-15 23:37:00.551387: step 72150, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 47h:59m:38s remains)
INFO - root - 2017-12-15 23:37:07.081199: step 72160, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 46h:43m:27s remains)
INFO - root - 2017-12-15 23:37:13.703808: step 72170, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 48h:13m:32s remains)
INFO - root - 2017-12-15 23:37:20.332970: step 72180, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 47h:23m:49s remains)
INFO - root - 2017-12-15 23:37:26.922606: step 72190, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 46h:53m:05s remains)
INFO - root - 2017-12-15 23:37:33.526764: step 72200, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 47h:48m:09s remains)
2017-12-15 23:37:34.046819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9980779 -7.5235748 -8.8426752 -9.3443947 -10.328449 -11.257086 -11.97921 -12.39443 -12.723501 -12.63953 -11.990265 -12.053555 -11.662935 -12.644116 -10.155031][-10.305796 -9.3555279 -9.7927456 -9.6320457 -10.16782 -11.392482 -12.455517 -13.52734 -13.855612 -13.511244 -13.194159 -12.417135 -11.181194 -12.864788 -9.9678555][-8.4587517 -9.9495506 -11.417326 -10.596012 -11.015219 -11.07346 -11.810546 -12.49342 -13.191771 -14.129402 -13.881069 -13.518501 -12.533876 -14.008184 -11.118647][-9.1595421 -9.3184671 -11.098164 -11.138711 -11.12624 -9.2761 -8.6462212 -10.794827 -11.63052 -11.220829 -11.822842 -12.551493 -11.822926 -13.067758 -9.8125706][-10.269009 -10.996058 -12.473176 -12.095936 -11.041334 -6.595695 -3.8970618 -6.9716206 -10.163033 -10.239666 -10.627008 -11.16106 -11.344021 -13.730106 -10.147701][-11.928316 -11.041319 -12.113453 -11.021614 -9.0242147 -2.7381442 2.6768079 1.8353715 -1.1549134 -6.3473239 -10.181476 -8.4596519 -8.1840477 -12.064612 -11.105748][-14.915195 -12.76908 -11.186609 -8.4558792 -6.1932063 -0.80249596 5.6267285 8.0251732 7.2598166 -0.26030731 -7.63129 -8.1845646 -8.54372 -10.244303 -8.5515709][-14.899584 -14.392059 -13.728668 -6.8160224 -1.2066765 3.496964 8.0390911 9.2627811 9.7697315 3.9537702 -4.228591 -8.1908484 -9.6200523 -10.930012 -8.8437939][-11.396933 -10.700113 -12.224607 -9.0658131 -4.5609016 3.3802581 8.0846043 7.3462567 7.2801423 2.3465905 -4.7438574 -9.6695175 -12.003195 -14.107534 -11.733768][-8.4192677 -7.5312247 -9.6230459 -8.99859 -8.380209 -2.8217225 1.9893141 3.2974229 3.0880284 -2.637495 -8.0087528 -11.18288 -13.895683 -16.526123 -14.429319][-11.774479 -10.86182 -11.25893 -10.537994 -11.273281 -10.176598 -8.2442818 -6.2800012 -5.939106 -8.6475983 -12.253545 -15.62064 -15.567696 -17.324648 -15.321039][-15.35252 -14.479416 -14.643429 -13.127483 -13.177715 -13.353523 -14.300526 -13.996588 -13.330242 -13.933205 -14.454639 -16.315672 -15.488382 -15.844147 -12.908216][-15.476562 -15.386972 -15.398825 -15.093361 -14.98444 -13.084466 -13.263665 -14.245981 -14.524258 -13.780624 -13.538656 -14.591215 -13.5137 -13.29974 -9.913249][-11.651623 -11.888833 -11.858606 -11.877199 -11.878845 -11.403666 -10.814743 -9.9760437 -10.303497 -10.523163 -10.221777 -9.6140251 -9.40065 -9.4575491 -7.2168384][-8.4157295 -7.7022924 -7.0982609 -6.1834507 -5.6482644 -6.6855993 -6.8838005 -5.8907967 -5.9678588 -5.910367 -6.3477015 -7.1864352 -7.1905003 -7.5048947 -7.252912]]...]
INFO - root - 2017-12-15 23:37:40.633304: step 72210, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 48h:50m:45s remains)
INFO - root - 2017-12-15 23:37:47.200664: step 72220, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 46h:35m:56s remains)
INFO - root - 2017-12-15 23:37:53.810481: step 72230, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.631 sec/batch; 45h:36m:56s remains)
INFO - root - 2017-12-15 23:38:00.513316: step 72240, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 48h:32m:43s remains)
INFO - root - 2017-12-15 23:38:07.198034: step 72250, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 48h:12m:06s remains)
INFO - root - 2017-12-15 23:38:13.831027: step 72260, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 47h:38m:09s remains)
INFO - root - 2017-12-15 23:38:20.431917: step 72270, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 47h:56m:18s remains)
INFO - root - 2017-12-15 23:38:27.045931: step 72280, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.676 sec/batch; 48h:52m:47s remains)
INFO - root - 2017-12-15 23:38:33.623784: step 72290, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 47h:19m:26s remains)
INFO - root - 2017-12-15 23:38:40.292507: step 72300, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 46h:38m:10s remains)
2017-12-15 23:38:40.800894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.58087873 -0.98325634 -0.88818741 -0.6777792 -1.6403074 -1.8946543 -2.2192841 -2.6186554 -2.86197 -3.1597469 -3.0320175 -4.751894 -5.7375512 -8.6494541 -9.55361][0.1943655 0.49107409 -0.10085106 -0.1708293 -1.5320339 -2.1851022 -2.6511269 -3.1057215 -3.6516969 -4.0162687 -3.6982927 -5.0909271 -4.4900622 -7.8099432 -9.277195][1.3375244 0.9339695 -0.051892281 -0.35630035 -1.4106836 -1.5784221 -1.944051 -2.0897388 -2.6929886 -3.3899767 -3.3598909 -4.7905393 -5.0128655 -7.4875412 -7.6745172][0.11284351 -0.01417017 -0.82726049 -0.29007912 -0.83082581 -1.0954528 -1.20609 -1.165288 -1.5110116 -1.3131118 -0.87484121 -2.0577898 -3.0716119 -6.4335632 -8.0238628][0.28896141 -0.74645281 -1.8393836 -1.7271032 -2.4873986 -1.4595561 -0.32767344 0.25779057 0.860754 0.53240442 0.10442448 -1.3448968 -2.4568596 -5.9848232 -7.7698197][-1.1266699 -1.5747623 -2.2381952 -1.4080081 -1.0831256 -0.39692354 0.39584732 0.57665157 0.86253738 1.2425513 1.4346805 -0.067145824 -1.0572782 -4.172318 -6.1508818][-2.3972239 -2.346493 -2.7293465 -1.4768291 -0.86056519 0.46942282 0.7936244 0.96804523 1.1900454 1.4300337 2.0427122 0.47855997 -0.93405724 -4.0432854 -4.7033205][-3.8604426 -3.9699111 -3.1965063 -0.99996281 0.14783764 0.77461576 1.4810748 1.1806169 0.86989927 1.0911188 1.5134735 0.4020257 0.21682644 -3.1109784 -4.1248565][-3.7141778 -4.0491838 -3.32506 -0.65979862 0.53390884 1.2992129 1.6800647 1.4682169 1.4831438 1.3412395 1.6486712 -0.1088028 -1.2633743 -4.3891621 -4.5581541][-3.7083426 -3.4513648 -3.3039334 -0.85686016 -0.019113064 0.754158 0.44878197 0.13683224 0.055575371 -0.34585905 -0.19846201 -1.4724274 -2.0586376 -5.0716782 -5.2964382][-5.9376025 -4.8321919 -4.6985164 -3.10947 -2.7132742 -2.2320764 -2.4303851 -3.0104933 -2.810637 -2.348552 -2.2198758 -3.9584084 -4.8127432 -7.0753541 -5.8730316][-9.5534554 -8.41507 -6.7428784 -5.1495123 -5.1357 -5.02083 -5.5238457 -5.7197785 -5.1214066 -4.9952154 -4.7148509 -6.2008004 -6.4742761 -8.3278885 -7.1856365][-9.65106 -9.1267366 -7.7920833 -6.6358 -7.0137773 -6.8979378 -6.9366717 -7.1415968 -7.1276741 -6.8293934 -6.8686872 -8.1045656 -8.3333645 -8.5939188 -6.2554727][-7.9640517 -8.0398912 -7.7374592 -6.5193028 -5.4925385 -5.8365445 -6.1520677 -6.4262333 -6.4788914 -6.4316416 -6.6355371 -6.8752532 -6.5700474 -7.5184469 -6.39658][-4.7117529 -5.328619 -4.9115319 -3.9499841 -3.6529162 -3.7131391 -3.7515409 -4.163763 -4.400938 -4.4291015 -4.2515669 -5.611896 -7.2586327 -8.038681 -8.2201157]]...]
INFO - root - 2017-12-15 23:38:47.354313: step 72310, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 47h:02m:53s remains)
INFO - root - 2017-12-15 23:38:53.966069: step 72320, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 49h:03m:54s remains)
INFO - root - 2017-12-15 23:39:00.612705: step 72330, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 48h:10m:05s remains)
INFO - root - 2017-12-15 23:39:07.270478: step 72340, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 49h:00m:40s remains)
INFO - root - 2017-12-15 23:39:13.792955: step 72350, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 46h:38m:26s remains)
INFO - root - 2017-12-15 23:39:20.383747: step 72360, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 47h:33m:56s remains)
INFO - root - 2017-12-15 23:39:27.037993: step 72370, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.681 sec/batch; 49h:14m:19s remains)
INFO - root - 2017-12-15 23:39:33.661534: step 72380, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.675 sec/batch; 48h:45m:54s remains)
INFO - root - 2017-12-15 23:39:40.174357: step 72390, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.634 sec/batch; 45h:48m:45s remains)
INFO - root - 2017-12-15 23:39:46.761619: step 72400, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.652 sec/batch; 47h:06m:53s remains)
2017-12-15 23:39:47.328458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7696457 -8.0881844 -8.8188324 -9.8783512 -11.071377 -11.474093 -12.047209 -11.423136 -11.235223 -11.549135 -10.777481 -12.618496 -14.750748 -14.282196 -13.855652][-8.4982023 -10.185029 -11.236421 -11.782084 -12.302645 -12.481301 -12.782763 -13.83864 -14.622183 -13.952303 -13.537123 -15.490192 -16.40218 -15.551239 -14.491747][-6.251915 -8.9577827 -11.657907 -11.692312 -11.423559 -11.803076 -12.378678 -12.890232 -13.193836 -13.994593 -14.095011 -15.270508 -16.036776 -16.191233 -15.718826][-9.01903 -10.718235 -11.998204 -10.790176 -10.246389 -9.1566648 -9.1497746 -11.476162 -12.839329 -11.64257 -10.916189 -14.22864 -16.784491 -16.375423 -15.031525][-10.345505 -13.116529 -14.598572 -12.126983 -8.3826046 -3.5850925 -1.4203711 -5.4380627 -10.199335 -10.616896 -10.572291 -12.252652 -14.352127 -16.342987 -17.108858][-12.166592 -13.143517 -12.847527 -10.730335 -7.3247294 -0.82360554 5.2579141 3.548697 -0.028120995 -4.8957572 -9.1438351 -10.452908 -12.457239 -13.78952 -15.955999][-14.784536 -14.472586 -12.26021 -8.0284967 -3.3700755 2.0232983 7.3431134 8.598587 7.6702714 0.10982752 -7.2501287 -9.9114809 -12.39105 -13.405967 -14.088783][-14.210939 -13.268562 -11.941883 -7.0062842 -0.39074373 5.1918006 9.2201653 8.0835361 6.5061793 1.9464188 -3.9448695 -9.8644638 -14.206383 -14.396301 -14.636206][-10.246639 -10.954351 -12.165453 -9.6542587 -3.6955664 2.1336188 7.3441834 7.5882697 4.4771171 -0.55034447 -5.6118054 -11.376236 -16.744345 -17.479841 -16.992434][-7.7743311 -8.9158983 -9.7001171 -9.226738 -7.6762509 -4.4729576 1.8056393 3.8203454 2.3796096 -2.134387 -7.8129721 -12.918888 -17.478626 -18.76198 -19.01602][-11.848719 -13.11125 -13.4074 -12.354202 -11.685907 -10.583363 -7.9711323 -6.7110925 -6.1026335 -6.5875735 -9.6116066 -15.405928 -18.494526 -18.426668 -17.663727][-18.015694 -16.170834 -15.664288 -16.539642 -17.094524 -15.940495 -15.384604 -14.823708 -13.514256 -13.262764 -13.948431 -15.04463 -16.638685 -16.909912 -15.882099][-14.607059 -14.320814 -14.542646 -15.15262 -15.553246 -15.014761 -14.615309 -14.647476 -14.894989 -13.530539 -12.69431 -14.395452 -15.368004 -13.022413 -12.255577][-11.494558 -11.215218 -10.596843 -10.085915 -10.418106 -12.016428 -12.627621 -11.978861 -11.928006 -12.055988 -12.140873 -11.305377 -11.122736 -10.728965 -10.778682][-8.8942995 -7.5092192 -5.79887 -5.553267 -5.4773288 -5.5216823 -6.4788208 -8.154274 -8.490037 -8.3249273 -8.7132721 -10.064171 -10.834684 -10.505412 -11.366955]]...]
INFO - root - 2017-12-15 23:39:53.942193: step 72410, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.639 sec/batch; 46h:11m:53s remains)
INFO - root - 2017-12-15 23:40:00.519156: step 72420, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.651 sec/batch; 47h:00m:59s remains)
INFO - root - 2017-12-15 23:40:07.132220: step 72430, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 49h:16m:23s remains)
INFO - root - 2017-12-15 23:40:13.714021: step 72440, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.664 sec/batch; 47h:57m:04s remains)
INFO - root - 2017-12-15 23:40:20.367572: step 72450, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 46h:47m:35s remains)
INFO - root - 2017-12-15 23:40:27.032587: step 72460, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 47h:05m:38s remains)
INFO - root - 2017-12-15 23:40:33.646467: step 72470, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 47h:50m:53s remains)
INFO - root - 2017-12-15 23:40:40.141218: step 72480, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 46h:39m:14s remains)
INFO - root - 2017-12-15 23:40:46.729376: step 72490, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 49h:27m:04s remains)
INFO - root - 2017-12-15 23:40:53.327102: step 72500, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 48h:31m:19s remains)
2017-12-15 23:40:53.919198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2165344 -2.4083855 -1.8465161 -1.6418009 -2.4394197 -3.1707058 -3.1784751 -3.1605325 -3.3504553 -3.5423732 -3.7374225 -6.9637623 -10.459067 -11.297309 -10.63773][-5.2501211 -3.684566 -3.0713644 -3.1054585 -3.295248 -3.4707839 -3.2357361 -3.4235182 -3.8880177 -4.107141 -4.6710548 -7.8482161 -11.354681 -12.262068 -11.745852][-3.1995633 -3.6116538 -4.4874887 -4.3292294 -4.8331776 -4.5473938 -3.7306185 -3.5438974 -3.9118853 -4.5307746 -5.4838891 -7.8989816 -10.554018 -11.039761 -10.623401][-3.1037877 -3.1128821 -4.03425 -4.0418496 -4.5101705 -3.5604033 -2.894136 -3.1521311 -3.4773893 -3.9582615 -4.5953064 -7.52294 -10.55102 -10.561443 -10.370613][-3.1373763 -3.5244062 -4.6852236 -4.44855 -3.8094327 -2.0789 -0.85512877 -1.4107819 -2.5741649 -3.1360095 -3.734921 -6.4806275 -9.4561234 -10.156096 -9.8334913][-4.7702918 -4.3929286 -4.2241364 -3.0483408 -1.834362 0.48575211 2.4635797 2.0160189 0.56090069 -1.0427303 -2.0584617 -4.6115718 -7.5424409 -8.14074 -8.0877333][-6.8435688 -6.4486141 -5.6712871 -3.8976941 -1.6222134 1.3213763 3.6277003 4.2022862 3.3224339 1.1483588 -0.60764837 -3.9167304 -7.0959826 -7.5153437 -6.9978995][-6.9094324 -6.66987 -6.2821474 -3.8320346 -1.5221939 1.2639203 3.585793 3.6157908 2.9305272 1.5347266 0.23789549 -3.1106396 -6.6841416 -7.3900328 -7.4436927][-6.5773907 -6.3732295 -6.0195565 -3.3950307 -1.068181 1.1734543 2.4057946 2.0706224 1.7546372 0.77905416 -0.039241791 -3.6708205 -7.4815326 -8.5239849 -8.5961647][-6.4368777 -6.9720669 -6.9923773 -4.0058112 -1.7567723 -0.0016918182 0.75323391 0.82864761 0.73388672 -0.048285961 -0.92904186 -4.2567334 -7.9475369 -8.7940388 -8.6925917][-9.4470568 -9.6567764 -9.386652 -6.7991405 -4.4070125 -2.7508342 -2.4467642 -2.7761383 -3.2444823 -3.9369636 -4.3527303 -7.6770015 -10.239691 -10.360723 -9.5861549][-13.035847 -12.122543 -10.752398 -8.3949242 -6.6776266 -5.5286756 -5.0488968 -5.905736 -6.7674737 -7.4659958 -8.0886478 -10.234545 -11.74495 -10.693062 -9.1745825][-13.324251 -11.245356 -9.1019974 -8.2707062 -7.7852654 -7.1510906 -6.6974983 -7.7490416 -9.0797577 -9.90334 -10.314117 -11.212229 -11.978472 -10.665606 -9.1729288][-10.171707 -8.5053864 -6.1289487 -4.29754 -3.8097887 -5.1399093 -6.3854084 -7.0557208 -8.097497 -9.6783466 -10.723183 -11.222557 -11.6123 -10.64151 -9.8977642][-7.8029318 -7.0071254 -5.0170803 -2.900748 -1.8658521 -2.5816853 -3.4055738 -4.2879915 -5.7096686 -6.8470049 -8.2023792 -10.233978 -11.618674 -11.51815 -11.171277]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-72500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-72500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 23:41:01.427093: step 72510, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 46h:45m:47s remains)
INFO - root - 2017-12-15 23:41:07.957208: step 72520, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 47h:31m:19s remains)
INFO - root - 2017-12-15 23:41:14.563068: step 72530, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 47h:21m:55s remains)
INFO - root - 2017-12-15 23:41:21.298457: step 72540, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 47h:30m:42s remains)
INFO - root - 2017-12-15 23:41:27.886011: step 72550, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 46h:59m:43s remains)
INFO - root - 2017-12-15 23:41:34.563162: step 72560, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 47h:54m:53s remains)
INFO - root - 2017-12-15 23:41:41.120427: step 72570, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 47h:15m:27s remains)
INFO - root - 2017-12-15 23:41:47.740814: step 72580, loss = 0.19, batch loss = 0.15 (12.6 examples/sec; 0.635 sec/batch; 45h:52m:39s remains)
INFO - root - 2017-12-15 23:41:54.288854: step 72590, loss = 0.21, batch loss = 0.16 (12.4 examples/sec; 0.646 sec/batch; 46h:37m:47s remains)
INFO - root - 2017-12-15 23:42:00.911520: step 72600, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 46h:28m:56s remains)
2017-12-15 23:42:01.441258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9897389 -4.0483036 -3.8980377 -3.5060427 -3.7959986 -4.5788031 -6.0542 -6.1897769 -6.3360529 -6.2777572 -5.7038088 -7.24503 -8.0783863 -8.4703579 -8.5733032][-4.1622124 -3.489831 -2.4807439 -2.3803062 -2.6393471 -2.727685 -3.4188569 -4.6022487 -5.8036227 -5.7434468 -5.4575338 -7.0174842 -7.7794514 -8.0484791 -8.858285][-0.56548071 -2.2338517 -3.5404389 -2.5049777 -1.4581909 -1.8601816 -2.6578162 -3.1525662 -3.7085047 -4.130969 -4.7894011 -6.8014603 -8.0685539 -9.0935211 -9.9207458][-3.6658361 -3.5210922 -3.2379413 -3.0926192 -2.4817936 -1.7663088 -1.4678974 -2.283818 -3.5230951 -3.6914473 -3.9293363 -6.2036963 -7.6817026 -8.6314306 -9.567255][-4.0480642 -5.2438631 -5.0572557 -3.4654577 -1.8934865 -0.3614645 0.38405991 -1.0863729 -3.2087142 -3.8140793 -4.2202716 -6.6176071 -8.1708412 -9.2190523 -10.489979][-7.1589108 -6.9781966 -5.0047364 -2.338207 0.44182205 3.0867124 4.5717511 3.21413 1.5374699 -0.85984945 -3.4699657 -5.6499987 -7.006362 -8.0701141 -9.1654015][-10.233343 -9.291131 -5.6308889 -2.1188147 0.91275883 4.7702994 7.7105193 7.2926936 5.8561034 2.5664907 -0.59793425 -4.0228806 -6.5961409 -8.0939789 -8.9682074][-10.226364 -9.7419262 -7.643012 -3.1987431 0.84864759 4.8515344 7.3514915 7.0821071 6.6915669 4.2544036 1.258935 -3.2701714 -6.7960405 -8.5079212 -9.0996342][-8.46997 -8.664032 -7.0501871 -3.0421569 0.03103447 2.499577 3.7992415 4.4664454 4.6826949 2.3518672 0.260314 -3.5890727 -6.9278789 -9.2069836 -10.795462][-6.55771 -6.3062763 -5.5566649 -3.6655829 -2.4929061 -0.41050911 1.7918129 1.8363509 0.98461866 -0.25034475 -1.6621523 -5.4710259 -8.0692616 -9.521266 -10.600986][-10.706252 -10.082541 -9.8809643 -8.16394 -7.0932989 -5.8114552 -4.0069962 -3.6089427 -3.6219885 -4.5801716 -6.1660523 -9.088747 -9.95117 -10.668403 -10.800779][-14.177324 -14.626207 -13.936342 -12.932004 -12.658596 -11.363739 -10.275188 -10.273722 -9.4213486 -9.6458693 -10.529737 -12.019885 -11.990328 -12.103653 -11.562311][-13.720196 -13.608076 -13.19205 -13.432054 -13.521221 -12.933217 -12.237592 -11.861975 -11.840948 -11.920025 -11.624065 -12.026614 -11.867323 -11.111567 -11.090799][-11.79393 -11.513822 -11.077381 -10.641148 -10.496496 -10.552729 -10.353431 -9.7178841 -9.3658915 -9.6147318 -9.683732 -9.62558 -9.3364086 -9.4475384 -9.7237387][-9.1464262 -8.9111948 -9.5115442 -9.1638422 -8.2367573 -7.78123 -5.9134717 -5.8622465 -6.5149565 -6.0541067 -6.0231819 -7.0137887 -7.3574915 -7.8381271 -9.1865511]]...]
INFO - root - 2017-12-15 23:42:07.966001: step 72610, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 48h:09m:59s remains)
INFO - root - 2017-12-15 23:42:14.617185: step 72620, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 46h:37m:56s remains)
INFO - root - 2017-12-15 23:42:21.224694: step 72630, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 46h:46m:27s remains)
INFO - root - 2017-12-15 23:42:27.913765: step 72640, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 47h:36m:51s remains)
INFO - root - 2017-12-15 23:42:34.493985: step 72650, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 47h:33m:51s remains)
INFO - root - 2017-12-15 23:42:41.096839: step 72660, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 48h:24m:20s remains)
INFO - root - 2017-12-15 23:42:47.653032: step 72670, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 48h:27m:44s remains)
INFO - root - 2017-12-15 23:42:54.225011: step 72680, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 46h:24m:16s remains)
INFO - root - 2017-12-15 23:43:00.786074: step 72690, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 46h:16m:43s remains)
INFO - root - 2017-12-15 23:43:07.379191: step 72700, loss = 0.22, batch loss = 0.17 (12.0 examples/sec; 0.666 sec/batch; 48h:01m:43s remains)
2017-12-15 23:43:07.968638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.5317326 -10.045116 -10.611894 -10.63986 -10.955936 -11.874699 -12.315307 -10.678711 -9.2440987 -7.6994863 -6.1203527 -6.8422241 -8.5968189 -9.2868462 -8.3568764][-8.0446358 -8.7126417 -10.354918 -10.526859 -10.617475 -12.465912 -13.617624 -12.77506 -11.154945 -8.9908152 -7.3603978 -7.7360473 -10.052277 -11.12249 -10.936282][-6.5102725 -8.2566156 -10.236216 -9.7145157 -10.045681 -10.506931 -10.491857 -10.928666 -10.881446 -10.117695 -8.816678 -8.8349714 -10.736149 -11.741053 -11.199946][-6.9557867 -8.1491032 -8.54781 -7.6870589 -8.9003439 -8.4768734 -8.0499506 -8.31822 -8.0905094 -7.578898 -7.6339874 -8.62982 -10.299603 -11.19246 -10.378735][-7.3114791 -9.1991386 -9.0239305 -6.3674889 -5.1187015 -3.5408845 -3.9426386 -5.2703557 -5.1525526 -4.8277125 -5.2209115 -6.5472393 -8.6396675 -9.9362345 -9.425333][-9.5125618 -9.7395954 -8.3168879 -5.3069592 -2.0678792 1.3024178 2.2108541 -0.085127831 -2.457217 -3.7284098 -4.1635313 -4.7485085 -7.3604031 -9.0955763 -8.5610237][-9.6513929 -9.2267294 -6.5320573 -1.6317534 1.8630943 4.2180753 6.3234096 5.6135621 3.2749114 0.049384594 -2.6587818 -3.8377967 -6.0895758 -6.889215 -7.0345917][-8.6439142 -7.0571995 -4.3141656 1.4182534 5.9164176 7.6709876 7.2404666 5.3409257 3.5545506 1.0333672 -1.2121015 -3.358578 -6.2041349 -6.8054056 -4.9603539][-7.7379723 -6.99101 -5.5085044 -0.92906 3.3251233 6.1052413 6.5010877 3.4826293 0.90350437 -0.54143667 -1.1407037 -2.6297028 -5.2481675 -6.1852202 -4.7502861][-8.54806 -7.9665375 -6.4229765 -2.7502329 -0.66211891 1.6009965 2.3943009 -0.1258297 -2.2595949 -3.0835736 -3.76513 -4.1439738 -5.236105 -5.6132159 -4.4867468][-13.152813 -13.011425 -11.092583 -7.0790272 -5.9017105 -4.1915407 -3.7649109 -4.5259109 -5.1822047 -5.7541194 -6.2682667 -6.7586842 -7.7913418 -7.5537314 -5.2684321][-16.628826 -15.602024 -13.749294 -9.9782209 -6.6812925 -6.1578193 -6.8856769 -7.015111 -7.282249 -7.545526 -6.9912992 -7.5574894 -8.77682 -8.387248 -6.0222058][-17.533619 -15.686029 -13.410748 -10.655679 -7.6887589 -6.5650377 -6.15134 -5.9198346 -6.1909866 -7.0171728 -6.8858852 -7.2217422 -7.3506222 -6.7220683 -4.5339918][-16.49227 -14.846779 -12.411057 -8.6057892 -5.8589983 -4.6219034 -4.3510389 -4.4356956 -4.1890135 -4.4165106 -4.3145871 -4.3684788 -4.5063829 -4.6648107 -3.6036918][-11.016328 -10.168966 -8.280365 -4.9988265 -2.8137815 -2.1601374 -2.1499877 -2.1012373 -1.8882372 -1.0647273 -1.0803537 -2.1389265 -2.19481 -2.5935645 -3.4316909]]...]
INFO - root - 2017-12-15 23:43:14.606992: step 72710, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 47h:33m:31s remains)
INFO - root - 2017-12-15 23:43:21.170904: step 72720, loss = 0.14, batch loss = 0.09 (11.5 examples/sec; 0.695 sec/batch; 50h:09m:32s remains)
INFO - root - 2017-12-15 23:43:27.779492: step 72730, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.675 sec/batch; 48h:41m:55s remains)
INFO - root - 2017-12-15 23:43:34.294530: step 72740, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 46h:01m:33s remains)
INFO - root - 2017-12-15 23:43:40.888337: step 72750, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 46h:53m:09s remains)
INFO - root - 2017-12-15 23:43:47.493214: step 72760, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 47h:07m:11s remains)
INFO - root - 2017-12-15 23:43:54.158863: step 72770, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 49h:15m:16s remains)
INFO - root - 2017-12-15 23:44:00.755830: step 72780, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 47h:20m:38s remains)
INFO - root - 2017-12-15 23:44:07.392801: step 72790, loss = 0.18, batch loss = 0.14 (11.6 examples/sec; 0.692 sec/batch; 49h:56m:14s remains)
INFO - root - 2017-12-15 23:44:14.041579: step 72800, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 48h:04m:53s remains)
2017-12-15 23:44:14.643840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1026592 -5.1422634 -2.9974749 -1.7519212 -1.6058102 -0.61098623 -0.62932062 -0.70133066 -0.79241323 -1.0037079 -0.810699 -2.535429 -4.7013655 -5.067482 -4.8380332][-3.0533748 -2.4233959 -1.0573335 -0.47643042 -0.47654915 0.020823479 -0.020076275 -0.46283722 -1.100852 -0.98874807 -0.87791395 -2.5008469 -4.7089052 -4.7217331 -4.1407642][-2.3628221 -2.5328133 -2.4010961 -1.8801 -1.3409476 -1.2592649 -1.2077527 -1.6878457 -2.071321 -2.2233367 -1.62848 -3.4392045 -5.6395717 -5.9944253 -6.5431471][-4.4856791 -4.3657541 -3.9329507 -3.158968 -2.8475561 -2.701417 -3.1924613 -3.1145871 -2.5572474 -2.6980741 -2.2991502 -4.5930262 -7.5976648 -8.1941509 -8.6264791][-5.5140033 -6.3056579 -6.1428275 -4.7069707 -3.7598672 -2.8553431 -3.1273534 -3.7999389 -4.0051289 -3.8385921 -3.8740041 -5.9435396 -8.6142349 -9.146595 -9.9343033][-5.2690296 -4.901669 -3.7555578 -2.5251961 -2.2295091 -1.4084105 -2.1909895 -2.670897 -2.9125657 -3.2514358 -3.3034585 -5.7977057 -8.55222 -8.5356007 -8.3646774][-4.7849379 -2.9178782 -1.1854448 -0.81561279 -0.61864138 -0.14279318 -0.036653042 -1.0378404 -1.897934 -2.5771499 -2.7455339 -5.3293519 -7.9237423 -7.8415852 -7.8272762][-4.7100477 -3.2875345 -0.93075991 0.57601976 0.62251711 1.087441 0.88005018 0.93971586 1.0760612 0.12739849 -0.61809969 -3.7289677 -6.6334934 -7.0405064 -6.8373184][-4.2969131 -4.1209731 -2.687331 -0.82105494 -0.23747587 0.049280167 0.12965107 0.93052483 1.4212132 1.5230403 1.3506656 -2.5505652 -6.1042156 -6.5023255 -6.7365561][-6.2953115 -6.1264167 -4.7855759 -3.0679507 -2.4595261 -1.6444187 -0.43838882 0.67555141 1.4864683 2.386126 2.7711272 -0.36029387 -3.9268563 -5.9837217 -7.4120646][-10.949261 -10.296059 -8.3070374 -6.6668949 -5.5110712 -3.7006044 -2.2910979 -1.1267676 -0.53335333 -0.14509916 -0.14059734 -2.8628132 -5.5052495 -5.990849 -6.6633558][-12.54467 -11.528086 -10.140654 -8.0515594 -6.5890021 -5.7127132 -4.806056 -3.8388042 -3.2802699 -3.1881392 -3.6073346 -5.5236654 -6.650456 -7.2547336 -7.4698539][-12.349987 -10.935342 -9.637887 -7.9016943 -6.5332503 -5.5626912 -5.369494 -5.0413547 -4.8888555 -5.9085298 -6.40372 -7.4905953 -7.8640642 -7.3141351 -6.5513287][-10.54501 -9.933938 -8.0940218 -6.1680069 -5.12842 -4.9730082 -5.1114807 -5.1630177 -5.7730794 -6.1435461 -6.9794521 -7.2526755 -7.2446375 -6.8728738 -5.999927][-7.3965526 -7.792872 -7.4378891 -6.3698215 -5.3377495 -4.736578 -4.7944121 -4.7185459 -4.9171834 -6.1725349 -7.1916842 -6.9304371 -7.750392 -8.12693 -8.2665548]]...]
INFO - root - 2017-12-15 23:44:21.264032: step 72810, loss = 0.14, batch loss = 0.10 (11.3 examples/sec; 0.709 sec/batch; 51h:07m:32s remains)
INFO - root - 2017-12-15 23:44:27.858419: step 72820, loss = 0.21, batch loss = 0.16 (12.4 examples/sec; 0.643 sec/batch; 46h:24m:22s remains)
INFO - root - 2017-12-15 23:44:34.652289: step 72830, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 48h:40m:54s remains)
INFO - root - 2017-12-15 23:44:41.263404: step 72840, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 48h:02m:21s remains)
INFO - root - 2017-12-15 23:44:47.934647: step 72850, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.671 sec/batch; 48h:24m:47s remains)
INFO - root - 2017-12-15 23:44:54.510115: step 72860, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 47h:58m:54s remains)
INFO - root - 2017-12-15 23:45:01.152663: step 72870, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 48h:14m:28s remains)
INFO - root - 2017-12-15 23:45:07.832620: step 72880, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.693 sec/batch; 49h:59m:32s remains)
INFO - root - 2017-12-15 23:45:14.411884: step 72890, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 46h:46m:50s remains)
INFO - root - 2017-12-15 23:45:21.070114: step 72900, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 48h:28m:11s remains)
2017-12-15 23:45:21.642401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.798018 -1.72422 -1.2313557 -1.0022235 -1.543951 -1.7303703 -2.261698 -3.1487868 -3.6130428 -4.4580703 -5.2325788 -8.629343 -10.255159 -9.9407063 -8.3540974][-1.1371808 -0.83535385 -0.36599159 -0.10697842 -0.78052425 -1.428206 -1.9484105 -1.9805613 -2.184849 -3.7168159 -4.5626621 -7.6593132 -9.3537569 -9.5527992 -8.644022][-1.7946024 -1.1056957 -1.3785005 -0.74034977 -1.263339 -1.7623708 -2.1699066 -2.4809656 -2.204761 -2.4105349 -2.6591051 -5.6772346 -7.1321 -8.0280571 -7.3828735][-3.6771159 -2.9075868 -2.5680494 -1.7085981 -2.2426562 -2.4827642 -2.6304562 -2.0773549 -2.0543323 -1.9593849 -1.7191818 -4.4080081 -5.8714781 -7.271852 -7.1214128][-4.1146278 -4.4341617 -4.6145906 -2.6610847 -2.314784 -1.5481772 -1.4040446 -1.4801764 -1.238349 -1.0049958 -0.7567029 -3.7758265 -5.5650058 -6.1002946 -5.7559462][-5.6860476 -5.187871 -3.990027 -1.8432009 -0.8004427 0.69801521 1.979939 1.5662627 0.893394 0.47772312 0.58961678 -2.1778469 -4.1141329 -4.8525953 -4.5275][-5.3991647 -4.7517447 -3.3950467 -0.70527363 0.83082151 2.5997376 4.0246778 4.0325875 3.2920527 2.1278462 1.5427895 -1.789695 -3.9397676 -3.8247879 -3.0078359][-6.13734 -4.8784828 -3.1338127 -0.12358379 1.8090806 3.456882 4.7459445 4.58102 4.1106505 2.9624839 1.875545 -1.2909832 -3.069844 -3.4139929 -2.1868196][-6.2946553 -4.7085605 -2.6833889 0.24191809 1.3869848 2.1522689 3.506186 3.3839488 2.9979081 1.7280483 0.41497946 -3.222055 -4.9315834 -4.6768751 -3.5577502][-7.1462631 -5.9996405 -5.0733209 -1.0724468 -0.059964657 0.14652157 0.79489565 0.0087924 -0.66093922 -1.8579631 -2.7910984 -6.2749176 -7.9392896 -6.4978175 -4.5676689][-11.501334 -10.777199 -9.3693962 -6.7841425 -6.8201952 -5.8099713 -5.2129788 -5.4409695 -5.0582733 -5.7045903 -7.0542474 -10.054701 -10.39949 -8.1422844 -4.9514632][-14.593122 -14.546444 -13.197544 -10.76425 -10.217913 -9.1690426 -8.6807261 -9.0192871 -8.7171879 -8.7331133 -9.5924835 -11.118959 -10.222414 -8.0634937 -4.7760434][-14.487358 -13.270379 -12.577218 -10.785719 -10.079787 -8.6769409 -8.1570225 -8.533124 -8.7155209 -8.91955 -9.5112982 -10.422774 -10.612114 -7.9806547 -4.0350666][-10.217638 -9.0264635 -7.5425229 -6.6882844 -6.2555313 -5.20936 -5.1911626 -5.6128011 -5.8316259 -7.2696323 -9.0913572 -9.5986938 -9.1581364 -8.2148857 -6.4323063][-7.7685061 -6.2979426 -4.2672882 -2.6903281 -2.156492 -1.9356115 -1.8503404 -2.2280588 -2.8918123 -4.113842 -5.2409711 -6.6648812 -7.0908637 -6.592608 -5.8725014]]...]
INFO - root - 2017-12-15 23:45:28.247694: step 72910, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 47h:53m:45s remains)
INFO - root - 2017-12-15 23:45:34.988051: step 72920, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.667 sec/batch; 48h:05m:44s remains)
INFO - root - 2017-12-15 23:45:41.603267: step 72930, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 46h:20m:30s remains)
INFO - root - 2017-12-15 23:45:48.189186: step 72940, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 46h:59m:03s remains)
INFO - root - 2017-12-15 23:45:54.807651: step 72950, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 47h:48m:58s remains)
INFO - root - 2017-12-15 23:46:01.407269: step 72960, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.669 sec/batch; 48h:16m:01s remains)
INFO - root - 2017-12-15 23:46:07.988635: step 72970, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 47h:51m:58s remains)
INFO - root - 2017-12-15 23:46:14.580291: step 72980, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.641 sec/batch; 46h:11m:34s remains)
INFO - root - 2017-12-15 23:46:21.309353: step 72990, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.688 sec/batch; 49h:34m:23s remains)
INFO - root - 2017-12-15 23:46:27.936086: step 73000, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 48h:43m:39s remains)
2017-12-15 23:46:28.467506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.886487 -7.5807295 -8.1694822 -8.556529 -8.9283743 -9.31909 -9.5051746 -10.591037 -11.296416 -11.096409 -10.716712 -10.902842 -9.7817936 -8.2069664 -5.7267394][-8.2598114 -8.6368952 -8.5328321 -9.2559509 -9.8334141 -9.7820644 -9.1740389 -9.4501553 -10.4807 -10.821784 -10.712736 -9.8535042 -8.5767136 -8.492732 -8.1333447][-6.93867 -7.7233982 -9.09879 -8.7509136 -9.0693007 -9.7410669 -9.0722313 -8.3704929 -8.7894344 -9.1502018 -9.9350319 -9.5324926 -7.6356792 -7.6202393 -7.4127445][-9.1586857 -9.55355 -9.144865 -8.78719 -8.8607025 -7.5093212 -5.8080258 -6.96535 -8.34813 -7.8950548 -8.231144 -8.9260492 -8.2360382 -7.8217521 -7.2949915][-7.328166 -9.8412552 -11.849499 -9.6242619 -8.0811224 -4.9252076 -0.50427103 -1.8156815 -4.8739066 -6.1872673 -8.0929441 -8.2680368 -7.1379271 -7.3873892 -7.2264533][-9.5913506 -9.7038031 -9.9051323 -8.5537891 -6.4979057 -1.6028104 3.8359723 3.6612468 2.8534102 -1.3782167 -6.3694553 -6.0686131 -4.6453748 -5.4489217 -5.6271553][-9.0417862 -10.243855 -8.6845627 -6.1216264 -3.784914 1.873106 7.6575017 7.8092303 7.8078647 3.3925557 -2.7750261 -4.8288603 -5.0926375 -5.1139593 -3.8650198][-8.54776 -10.101029 -9.3444347 -5.7748404 -1.5844588 2.7509189 6.6426148 7.6972337 8.13707 3.4242949 -1.4071131 -4.2665687 -5.4725132 -6.2738361 -6.3846827][-7.3271728 -7.46099 -7.9530993 -5.57955 -2.4457493 0.785974 3.7708316 4.3629661 5.5716558 2.46913 -0.91320705 -3.8216348 -6.2725782 -7.318706 -7.6600647][-6.9544616 -6.069304 -6.9223733 -5.6563439 -4.3402271 -1.2674127 1.4700618 1.5777612 1.9813251 0.23301411 -1.7745588 -4.1786494 -5.5233574 -8.0481606 -9.7838821][-9.3535261 -9.5180321 -10.07617 -8.3248987 -7.6078587 -5.3024349 -2.9148273 -4.395741 -4.9934015 -5.6962471 -5.9090285 -7.778842 -9.3070707 -11.172178 -11.462167][-14.024887 -13.145256 -13.370718 -12.355324 -10.90708 -9.3799305 -9.5061817 -9.4485531 -9.4667225 -10.174588 -10.924307 -11.76536 -11.687153 -12.228668 -12.729066][-11.314043 -11.394176 -11.60225 -10.234447 -10.89111 -10.212931 -9.3644571 -9.5117054 -10.69549 -9.8376074 -8.9010382 -9.845336 -10.156675 -11.11548 -10.332417][-10.038187 -9.5431767 -9.1186209 -9.1270752 -7.6404715 -7.9480906 -8.4520082 -8.8485479 -9.5739021 -9.0901489 -8.7350979 -8.9153109 -8.0919743 -7.4315138 -7.3296757][-5.8767476 -6.5164361 -6.1138263 -5.3743515 -4.5070333 -3.5494056 -2.9713316 -3.9514704 -5.9491806 -6.4384065 -6.4205351 -5.9887557 -6.8794284 -7.2838626 -7.2320671]]...]
INFO - root - 2017-12-15 23:46:35.102378: step 73010, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.689 sec/batch; 49h:41m:12s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 23:46:41.678706: step 73020, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 47h:25m:39s remains)
INFO - root - 2017-12-15 23:46:48.212809: step 73030, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 47h:05m:56s remains)
INFO - root - 2017-12-15 23:46:54.828863: step 73040, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 49h:18m:48s remains)
INFO - root - 2017-12-15 23:47:01.478957: step 73050, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.684 sec/batch; 49h:19m:25s remains)
INFO - root - 2017-12-15 23:47:08.119917: step 73060, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 47h:11m:40s remains)
INFO - root - 2017-12-15 23:47:14.712925: step 73070, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 47h:24m:09s remains)
INFO - root - 2017-12-15 23:47:21.281496: step 73080, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 47h:23m:51s remains)
INFO - root - 2017-12-15 23:47:27.984093: step 73090, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 46h:48m:58s remains)
INFO - root - 2017-12-15 23:47:34.602031: step 73100, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.671 sec/batch; 48h:20m:43s remains)
2017-12-15 23:47:35.169419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1268253 -4.2915158 -3.9897103 -3.6170709 -3.7880402 -3.1187863 -2.9303498 -4.0163651 -5.3643842 -5.8991404 -5.5349126 -6.270443 -6.9250412 -7.6718426 -8.5225458][-4.3241396 -4.3309307 -4.0527081 -2.9224403 -2.3519433 -2.2506177 -2.5795453 -3.2190418 -4.7723155 -6.4003477 -6.344079 -7.4340744 -8.3118973 -9.5326042 -9.4509468][-4.405489 -4.3248267 -4.3952627 -4.6299944 -4.6546721 -3.7957187 -3.4728451 -3.1053813 -3.6765163 -6.020534 -7.8070474 -8.8954668 -9.7673168 -10.840003 -11.265373][-5.286706 -4.7573175 -4.6435084 -4.6756582 -4.9964924 -5.0716543 -4.3278065 -4.0670156 -4.6603193 -5.5865908 -6.7112064 -9.6159554 -11.549795 -12.371634 -12.438318][-5.925611 -5.6779666 -5.8484645 -5.0438447 -4.4737167 -4.1339126 -2.9991634 -2.8736861 -3.4052439 -4.7634544 -6.3265157 -8.914691 -10.046824 -11.862856 -13.040312][-5.583056 -5.7949662 -5.5802855 -3.6665921 -2.133533 0.33049965 1.6501608 0.68774652 -0.54696465 -1.9259925 -3.6503074 -5.8792672 -7.7366571 -10.108978 -11.487531][-6.8643322 -5.6937547 -4.8349323 -2.851789 -0.64641 2.3453851 5.0898547 5.2724452 4.6383071 1.2525659 -2.2220657 -3.8713193 -5.3120131 -8.5328369 -10.657763][-6.427238 -5.9272757 -5.2294149 -2.7582283 -0.0038657188 3.847796 6.8846564 6.6145949 5.96779 3.23039 -0.11391592 -3.16601 -5.4812441 -7.7620726 -9.5395594][-5.5596452 -4.7595444 -4.0287848 -2.52014 -0.82935047 2.0769053 4.99348 5.9052052 5.6118131 2.0880065 -1.3223014 -4.6071386 -7.225996 -9.16168 -10.364082][-4.8815045 -4.6123161 -4.0972633 -1.4494958 -0.82261372 0.12810564 1.2394438 1.8099403 1.9412336 -0.040432453 -2.3296883 -6.1404829 -9.6458187 -12.059765 -13.870041][-8.1568909 -7.894702 -8.0245819 -5.351862 -4.7800536 -3.6472 -3.0537293 -3.2350712 -3.1897633 -3.9017956 -5.6534967 -10.004738 -12.857368 -14.445839 -14.611311][-13.333764 -13.285067 -12.819091 -10.842649 -10.275497 -8.8357077 -8.6718216 -9.36336 -9.2982531 -9.4794 -9.9722385 -11.30788 -12.939799 -14.280663 -14.50844][-15.429201 -14.475527 -12.478054 -11.281971 -11.624977 -10.526719 -9.9759874 -10.093671 -10.116478 -10.347165 -10.729396 -12.423388 -12.257137 -11.501619 -10.017][-14.022705 -13.576954 -11.732113 -10.142397 -9.7164326 -9.7485361 -11.207237 -11.00087 -10.12081 -9.8796787 -9.1775122 -9.3631678 -9.0867786 -8.9553 -7.6719885][-8.7662611 -8.6993465 -7.7395673 -7.968339 -7.598248 -6.9497848 -7.4839478 -7.7264891 -8.6151848 -8.1322517 -7.7644277 -8.2704163 -7.9591417 -7.9130073 -7.8059635]]...]
INFO - root - 2017-12-15 23:47:41.786592: step 73110, loss = 0.21, batch loss = 0.16 (11.8 examples/sec; 0.677 sec/batch; 48h:46m:21s remains)
INFO - root - 2017-12-15 23:47:48.356794: step 73120, loss = 0.20, batch loss = 0.15 (12.7 examples/sec; 0.630 sec/batch; 45h:24m:11s remains)
INFO - root - 2017-12-15 23:47:54.953498: step 73130, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 46h:49m:45s remains)
INFO - root - 2017-12-15 23:48:01.541407: step 73140, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 47h:44m:23s remains)
INFO - root - 2017-12-15 23:48:08.222948: step 73150, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 46h:44m:08s remains)
INFO - root - 2017-12-15 23:48:14.846613: step 73160, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 48h:53m:48s remains)
INFO - root - 2017-12-15 23:48:21.455188: step 73170, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 48h:05m:34s remains)
INFO - root - 2017-12-15 23:48:28.105838: step 73180, loss = 0.11, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 46h:51m:39s remains)
INFO - root - 2017-12-15 23:48:34.607850: step 73190, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 46h:31m:58s remains)
INFO - root - 2017-12-15 23:48:41.208496: step 73200, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.685 sec/batch; 49h:20m:11s remains)
2017-12-15 23:48:41.755917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2781532 -4.1153603 -4.7903357 -5.5784726 -7.156702 -7.82013 -8.2421427 -8.3577375 -8.845295 -9.2121429 -9.1631079 -9.9011583 -11.197468 -9.9276028 -7.2308593][-6.5597954 -5.5976734 -5.6448679 -6.2089257 -7.5888228 -9.3124437 -10.110266 -10.128215 -9.9444332 -9.7035542 -9.7910137 -10.088804 -11.218203 -11.299662 -8.9111042][-4.6431694 -5.6654477 -7.0675669 -6.7702031 -7.4458079 -8.5397024 -9.5287933 -9.75573 -9.4488478 -8.9984369 -8.9906368 -8.9303217 -10.2996 -10.36698 -9.6747274][-5.4897165 -5.5251584 -6.5268297 -7.1224227 -8.038949 -8.1884995 -8.4785433 -9.4731884 -9.34164 -8.5660276 -8.617938 -8.3133526 -8.9272423 -8.9310942 -8.8304482][-5.2721581 -6.9356527 -7.9975939 -7.6328688 -7.123776 -6.2426357 -5.9656315 -6.8773332 -7.7592669 -7.6072941 -7.7248793 -7.7323332 -9.3091335 -9.259223 -7.648592][-6.4203544 -6.7707424 -6.0531616 -6.04809 -4.7623744 -1.7465193 0.69690704 0.142941 -1.0851579 -3.0082984 -5.0964584 -5.511569 -7.4212437 -8.2577591 -8.3932467][-7.7571545 -8.1216106 -7.153851 -5.9883375 -3.3685942 0.37694025 4.658627 7.0352597 6.145082 1.8513136 -2.3956389 -3.2870975 -6.2978497 -7.8457866 -8.6823549][-7.9752069 -8.6685009 -7.8320236 -4.6362863 -1.7543635 1.7218547 5.2175469 7.7898908 9.0686493 4.435123 -1.8673425 -4.7504578 -6.9903169 -7.8956623 -8.6906729][-4.9546461 -6.2488985 -7.0203352 -5.3895903 -2.7528954 0.60433865 4.0223517 6.0013156 6.5334058 3.9344592 -0.48277378 -5.0091534 -9.679966 -10.594137 -9.957366][-2.4295547 -2.5000293 -3.9708714 -3.4460745 -1.861666 -0.31308937 1.3137121 2.0964875 2.0900416 -0.51049376 -3.7812185 -6.7852902 -11.096786 -13.176508 -13.405239][-4.9923792 -4.9721313 -4.8721418 -4.97787 -4.5774384 -3.80446 -3.1318786 -3.9851727 -5.3767209 -6.9456911 -9.3664761 -11.125971 -13.881634 -14.521776 -14.428959][-10.269531 -10.307732 -10.067987 -8.5636263 -7.7517037 -7.9214211 -8.1134729 -9.1940145 -10.120678 -11.878055 -14.045313 -14.632736 -15.200205 -14.590202 -12.528456][-11.761179 -11.01976 -10.123902 -9.1280746 -9.8324785 -9.489398 -8.8767071 -10.384361 -11.562294 -12.402075 -12.922199 -13.999666 -14.227552 -12.303961 -9.9833355][-10.202409 -9.5413494 -9.0720243 -9.1388016 -9.7885857 -9.120779 -8.6178465 -8.8015785 -8.922617 -10.053514 -10.413141 -9.97392 -9.9833012 -9.9989624 -8.7933445][-7.7444162 -7.2634206 -6.741487 -5.5910997 -5.9021149 -5.5294609 -6.177731 -5.4563112 -6.2192035 -6.8500714 -7.3220506 -8.0840511 -8.3712139 -7.6015635 -7.6536322]]...]
INFO - root - 2017-12-15 23:48:48.359376: step 73210, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 47h:02m:59s remains)
INFO - root - 2017-12-15 23:48:54.905457: step 73220, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 46h:28m:23s remains)
INFO - root - 2017-12-15 23:49:01.456647: step 73230, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.692 sec/batch; 49h:51m:46s remains)
INFO - root - 2017-12-15 23:49:08.096109: step 73240, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 46h:36m:30s remains)
INFO - root - 2017-12-15 23:49:14.694343: step 73250, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 47h:56m:11s remains)
INFO - root - 2017-12-15 23:49:21.288015: step 73260, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.657 sec/batch; 47h:19m:30s remains)
INFO - root - 2017-12-15 23:49:27.914391: step 73270, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 46h:50m:55s remains)
INFO - root - 2017-12-15 23:49:34.474790: step 73280, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 46h:06m:16s remains)
INFO - root - 2017-12-15 23:49:41.115474: step 73290, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 46h:10m:11s remains)
INFO - root - 2017-12-15 23:49:47.667453: step 73300, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.650 sec/batch; 46h:46m:32s remains)
2017-12-15 23:49:48.260889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4258084 -6.835248 -8.1802158 -8.7202511 -9.8586092 -11.115973 -11.818999 -12.085562 -12.338253 -12.024744 -11.279397 -11.94655 -11.318263 -12.662052 -10.354855][-10.068417 -8.9453907 -9.3322239 -9.2701559 -9.8927431 -11.245389 -12.181791 -13.410524 -13.709006 -13.277815 -12.919672 -12.326593 -10.920621 -12.998777 -9.5155878][-8.1471224 -9.6845379 -11.133179 -10.214821 -10.76892 -11.058507 -11.574312 -12.242435 -12.998074 -13.763977 -13.307293 -13.319735 -12.075996 -13.795588 -10.729165][-8.9803791 -9.2771606 -11.292175 -11.205083 -11.20163 -9.2494755 -8.7031059 -11.00988 -11.786334 -11.314404 -11.682477 -12.614313 -11.353033 -12.843912 -9.8101959][-10.444569 -11.257956 -12.75226 -12.381104 -11.371315 -7.0347743 -4.2349825 -7.5261421 -10.921339 -10.700989 -10.78463 -11.560734 -11.261617 -13.666898 -9.8475323][-12.523481 -11.521413 -12.735794 -11.586933 -9.4287033 -3.2639732 2.1285276 1.0576272 -2.1657035 -7.3370667 -10.873971 -9.2729492 -8.5089417 -12.343513 -11.042823][-15.465075 -13.21892 -11.536855 -8.5716524 -6.2244287 -0.81749678 5.6573625 7.7971082 6.6938148 -1.2306089 -8.6676607 -9.3082619 -9.0638027 -10.659742 -8.5733814][-15.30184 -14.847084 -14.072346 -6.8011703 -1.1379066 3.5224423 7.9109559 9.1013126 9.3414288 3.1035819 -5.1902041 -9.1244888 -9.8210506 -11.400551 -8.7051239][-12.004574 -11.122353 -12.54751 -8.8657684 -4.1133332 3.8157668 8.3578453 6.7439723 6.28107 1.2509055 -5.9008904 -10.709452 -12.410482 -14.18222 -11.5949][-9.0740891 -7.7147083 -10.333457 -9.0896263 -8.1502638 -1.9069898 3.0296884 2.981688 2.2385297 -4.0772381 -9.2682266 -11.9389 -13.562306 -16.426407 -13.977007][-12.97086 -10.948319 -11.578639 -10.664808 -11.466504 -9.6438713 -7.4346228 -5.8952928 -5.7566981 -9.6783609 -13.11202 -15.902842 -14.898567 -16.127901 -14.24415][-16.649254 -15.576859 -15.787701 -13.394765 -13.312578 -13.142534 -14.139482 -13.959291 -12.905895 -14.587479 -15.01796 -16.975883 -15.231934 -15.089655 -11.576248][-16.701643 -16.527582 -16.614555 -15.646347 -15.525761 -13.333593 -13.675686 -14.938425 -14.896809 -14.159 -13.691237 -15.179571 -13.695332 -13.09363 -9.4560785][-12.081496 -12.299959 -12.168501 -11.903618 -12.266763 -11.15654 -11.190104 -10.311121 -10.748591 -11.07744 -10.675913 -10.099932 -10.075117 -9.961545 -7.5567608][-8.6577187 -7.8275375 -6.2578712 -5.7233872 -5.533565 -6.3499637 -7.033155 -6.15105 -6.0498238 -6.1489959 -6.6020546 -7.8394547 -7.4417 -8.1707411 -7.6098652]]...]
INFO - root - 2017-12-15 23:49:54.774027: step 73310, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 45h:38m:22s remains)
INFO - root - 2017-12-15 23:50:01.394886: step 73320, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 46h:16m:27s remains)
INFO - root - 2017-12-15 23:50:07.986503: step 73330, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.697 sec/batch; 50h:12m:11s remains)
INFO - root - 2017-12-15 23:50:14.632584: step 73340, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 46h:30m:59s remains)
INFO - root - 2017-12-15 23:50:21.122339: step 73350, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 46h:17m:42s remains)
INFO - root - 2017-12-15 23:50:27.732108: step 73360, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 48h:07m:03s remains)
INFO - root - 2017-12-15 23:50:34.220697: step 73370, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 47h:15m:14s remains)
INFO - root - 2017-12-15 23:50:40.867697: step 73380, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 46h:23m:06s remains)
INFO - root - 2017-12-15 23:50:47.456482: step 73390, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.665 sec/batch; 47h:50m:30s remains)
INFO - root - 2017-12-15 23:50:54.019278: step 73400, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 47h:06m:08s remains)
2017-12-15 23:50:54.564040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6534731 -3.0904114 -1.6775389 -0.82040119 -1.2444215 -2.0978718 -2.4604352 -3.4382596 -3.7990396 -3.9963942 -4.0749559 -4.81236 -6.4711657 -8.5892353 -8.009594][-3.3907392 -2.0184999 -0.35455751 -0.4393158 -0.84789705 -0.88654804 -1.4375796 -2.2517195 -3.3151648 -2.9079854 -2.5142317 -4.5170078 -5.5328965 -7.732296 -7.2731104][-2.4933398 -1.4308119 -1.0154567 -0.39581966 -0.587739 -0.42929888 -0.81112671 -1.7073722 -2.1825423 -2.0273929 -1.3799 -2.3761613 -3.1153622 -4.9759274 -4.7127624][-4.4254394 -3.9637427 -2.3944485 -1.189826 -0.7507 -0.62637472 -0.86486721 -0.5258913 -0.22535324 0.2913518 0.95127773 -0.86923361 -2.2225418 -4.1220818 -3.4550414][-3.9007916 -4.8512053 -3.3209143 -1.9234281 -2.1895306 -1.6982079 -0.97240973 -0.38286781 -0.45277023 0.39654732 0.25874233 -0.07396841 -1.0422034 -3.1058354 -2.6583323][-4.6039286 -3.9962044 -3.0436707 -1.6115618 -1.1300397 -0.14749002 0.30509996 0.61706305 0.41489697 0.8393383 1.4601479 0.52456 -1.4387197 -4.1227074 -4.2442541][-4.4600806 -2.8776507 -1.9988093 -0.33525658 0.55898 1.3536606 1.7083817 1.2155414 1.3923264 1.5555453 1.359962 0.78290606 -0.7571907 -3.5762591 -4.0615921][-3.8240514 -2.8947654 -1.2988162 0.520309 1.2611117 1.8769484 1.9908185 2.274178 2.9086471 2.3706737 2.3502049 0.59738111 -0.906538 -3.0095296 -3.2630122][-3.5949125 -2.7867649 -1.0294724 0.73086452 0.69809008 0.99538136 1.3166666 0.42643976 0.7299881 0.67052984 1.2799172 -0.15180159 -1.7391164 -4.3924637 -4.5326381][-3.9637096 -4.3983741 -3.0011251 -1.3597803 -0.76100683 0.22754765 0.70125484 -0.15937233 -0.80631304 -1.3518963 -0.90610218 -2.0081334 -2.8228288 -5.1610527 -5.1816087][-6.3036356 -5.662189 -4.5132604 -3.3170698 -2.3754418 -1.7286763 -1.4631367 -1.5552707 -1.200345 -2.0220492 -2.5459478 -3.9512439 -4.2977839 -5.6042314 -4.52877][-8.4534073 -7.6842403 -6.3962283 -4.7443886 -4.0342951 -4.0030956 -3.9389291 -3.3502874 -2.6529262 -2.71029 -2.9921184 -4.1854472 -4.4814639 -5.3687735 -4.6113787][-10.870843 -10.03406 -8.2548676 -6.3032575 -5.5849805 -5.3679905 -5.0254574 -5.1547036 -5.3021488 -5.359498 -5.3422604 -5.3916035 -5.3831758 -5.7360287 -4.5300245][-6.3370161 -6.7312078 -6.5377169 -4.5100012 -3.1743107 -2.7149661 -3.6143587 -3.6811087 -4.0284691 -4.1971931 -4.2087383 -3.3871248 -3.2949927 -4.8930655 -4.6004639][-2.0313113 -2.4539342 -2.9288623 -2.9018362 -2.1741657 -1.5961375 -1.2533636 -1.5082893 -1.5795622 -1.8420179 -2.0707741 -2.5170243 -3.3659935 -4.065793 -4.4478145]]...]
INFO - root - 2017-12-15 23:51:01.191574: step 73410, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 47h:50m:02s remains)
INFO - root - 2017-12-15 23:51:07.756308: step 73420, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 47h:54m:14s remains)
INFO - root - 2017-12-15 23:51:14.305429: step 73430, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 47h:52m:44s remains)
INFO - root - 2017-12-15 23:51:20.903787: step 73440, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 47h:33m:28s remains)
INFO - root - 2017-12-15 23:51:27.501720: step 73450, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 46h:10m:53s remains)
INFO - root - 2017-12-15 23:51:34.115187: step 73460, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.655 sec/batch; 47h:06m:40s remains)
INFO - root - 2017-12-15 23:51:40.723791: step 73470, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 46h:59m:49s remains)
INFO - root - 2017-12-15 23:51:47.349026: step 73480, loss = 0.13, batch loss = 0.09 (11.4 examples/sec; 0.705 sec/batch; 50h:42m:39s remains)
INFO - root - 2017-12-15 23:51:53.931526: step 73490, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.675 sec/batch; 48h:35m:36s remains)
INFO - root - 2017-12-15 23:52:00.579153: step 73500, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.694 sec/batch; 49h:55m:23s remains)
2017-12-15 23:52:01.168827: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0537648 -3.2361495 -3.2124732 -2.6946797 -2.5131884 -2.7335515 -3.2226734 -3.557894 -2.9639211 -2.4121594 -1.3190289 -2.2924187 -4.21868 -6.52925 -7.0628376][-1.2189732 -1.2371564 -0.75358343 -0.16177034 -0.52469206 -0.68749571 -0.69516373 -1.0377264 -1.0657887 -0.75651217 -0.060382366 -1.3967872 -3.5429931 -5.7274647 -6.8203883][-0.64120817 -0.3004818 -0.2923522 0.087311745 -0.36750269 0.15255213 0.38375521 0.14977551 0.33695412 0.90837431 1.147264 -1.0859261 -3.4205766 -6.2439766 -7.9001236][-1.5403156 -1.9075384 -1.6389928 -1.1975307 -1.7089877 -1.2270918 -0.82392836 -0.96438169 -1.1875329 -0.96776915 -0.57759047 -2.5715806 -5.3221111 -7.8539605 -9.8061543][-2.3365457 -2.9916053 -3.4516592 -1.9163837 -0.88672161 -0.10947561 0.56018448 -0.17281818 -1.8435042 -1.3988514 -0.966764 -3.3179984 -6.1477079 -8.8725567 -10.120947][-4.2410083 -4.6677322 -4.5136681 -1.8665154 0.28222418 2.1108713 3.8662095 3.3575873 1.2914381 -0.82803917 -3.1301584 -3.8841558 -5.1593041 -8.5341606 -10.618095][-5.0951614 -5.3405142 -4.844799 -3.1472812 -0.98608637 1.9066458 5.3437591 5.777699 4.0242343 1.0184064 -2.8320944 -5.094326 -6.9840312 -9.2109356 -9.20678][-7.3187094 -6.1785569 -4.5035605 -2.0941749 -0.07447052 2.6836648 5.7055678 6.4532104 5.6958594 3.0515761 -0.36520958 -3.8331928 -7.1102543 -9.6414223 -10.457834][-5.7005677 -5.780879 -4.24526 -1.7478087 -0.1516037 2.6227031 4.8210988 5.7084422 5.9268212 3.2244973 0.17567015 -5.0112476 -9.0528469 -10.918098 -11.16201][-5.4516687 -4.8319187 -4.3915811 -2.2187634 -1.0149541 0.48565435 2.4807925 3.8693662 3.3677087 0.80867481 -1.5997224 -6.2859697 -10.799928 -13.825428 -14.494085][-8.9290552 -9.0230618 -7.5826583 -5.5554328 -4.8084059 -4.5936403 -3.6987276 -3.1593733 -2.4293528 -2.86289 -4.3633289 -9.3122292 -12.581946 -14.081791 -13.648842][-12.077515 -11.701028 -10.421926 -8.1959419 -7.8119221 -7.6002197 -7.8516808 -8.2381048 -7.6513286 -7.8842812 -7.90392 -9.5013981 -11.436861 -14.187088 -14.544037][-12.871349 -12.294352 -11.353765 -11.218405 -11.133576 -9.9406118 -9.8013115 -10.108195 -10.251078 -10.198646 -10.065139 -11.082351 -11.408504 -11.798215 -11.333094][-12.036774 -12.129576 -10.850681 -9.440567 -9.4083233 -10.94791 -11.665243 -10.592388 -10.327532 -10.495556 -10.804871 -10.097755 -9.8108377 -10.423922 -9.5883026][-10.328746 -9.25849 -8.8054028 -8.33715 -8.090807 -7.7419591 -8.2225828 -9.1050911 -9.9164972 -9.5724745 -9.5335846 -10.148554 -10.441555 -10.475248 -10.454075]]...]
INFO - root - 2017-12-15 23:52:07.705156: step 73510, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 47h:55m:04s remains)
INFO - root - 2017-12-15 23:52:14.262379: step 73520, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 47h:12m:21s remains)
INFO - root - 2017-12-15 23:52:20.741101: step 73530, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 47h:09m:32s remains)
INFO - root - 2017-12-15 23:52:27.317385: step 73540, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 47h:08m:24s remains)
INFO - root - 2017-12-15 23:52:33.882376: step 73550, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 47h:42m:36s remains)
INFO - root - 2017-12-15 23:52:40.389696: step 73560, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.635 sec/batch; 45h:41m:38s remains)
INFO - root - 2017-12-15 23:52:47.020729: step 73570, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 47h:03m:50s remains)
INFO - root - 2017-12-15 23:52:53.577569: step 73580, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 46h:26m:36s remains)
INFO - root - 2017-12-15 23:53:00.152872: step 73590, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 46h:59m:25s remains)
INFO - root - 2017-12-15 23:53:06.816408: step 73600, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 47h:57m:10s remains)
2017-12-15 23:53:07.408720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.02234 -7.4818382 -7.6003981 -8.2365389 -9.35469 -8.9822273 -7.9422779 -6.6475663 -6.7213345 -7.3220148 -7.1464233 -8.3626862 -11.184378 -11.480139 -11.954384][-6.833849 -7.5398884 -8.0398254 -8.8038406 -9.7471294 -10.174604 -8.9673967 -7.49349 -6.7788515 -6.8051138 -6.5754571 -7.9264936 -10.036742 -10.703061 -11.559994][-4.6888433 -6.511518 -8.4394512 -9.2818155 -10.225829 -10.345091 -9.25887 -7.5054245 -5.8888478 -6.0563483 -6.2832017 -7.5832157 -9.8123589 -9.9557781 -10.89163][-4.4774256 -6.1053619 -7.7806363 -8.9713659 -10.217032 -8.97052 -7.0352688 -5.804286 -5.4293547 -5.0678363 -5.3708181 -6.5593567 -9.4985313 -10.63619 -11.407978][-4.0028944 -6.87154 -9.2329693 -9.6132336 -9.1592731 -6.3979635 -3.2874718 -2.3450861 -3.494324 -3.8726401 -4.093791 -5.5567765 -8.4611073 -10.31122 -12.222885][-5.4643917 -7.2302246 -8.5843582 -8.7015467 -6.8438978 -3.1783533 0.86935663 2.4362931 1.2013588 -1.707603 -3.4766493 -4.84134 -7.9177313 -9.3513718 -11.039242][-6.7495031 -7.1643205 -7.7873869 -6.1464167 -3.1920285 0.55694628 4.3350348 6.3409114 5.5327573 1.3355575 -2.0660131 -4.6096163 -7.6213913 -8.9951372 -10.198473][-6.4327884 -6.6981711 -6.8600736 -4.9912791 -2.322974 1.8255296 6.3447738 8.3636742 7.0467515 3.6769843 -0.10497808 -3.6334615 -7.6688423 -8.7825832 -10.465685][-4.5411234 -5.5482 -5.8976712 -4.417367 -2.3363962 0.30780411 4.1883492 6.397656 5.2063441 1.6725178 -0.68553495 -3.491127 -8.0731773 -9.8695383 -11.902383][-3.3417966 -4.7939124 -5.744441 -4.7129078 -3.5031941 -1.2661519 1.9479194 2.941339 2.0158916 -0.23157787 -2.307981 -5.314043 -9.1436014 -11.003689 -12.826849][-7.5130796 -7.1803107 -7.5210614 -6.5715065 -6.1601887 -4.8379283 -3.2046747 -2.2996752 -2.9711602 -3.9722059 -5.4250646 -8.1798973 -10.973368 -12.141935 -12.552356][-11.689651 -10.341606 -9.0799179 -7.2573442 -6.7952671 -6.0775347 -5.9691358 -5.924736 -6.1842256 -6.8613367 -7.9156055 -9.8389854 -11.109352 -11.765784 -11.748573][-12.587027 -12.595319 -10.309923 -9.0505285 -8.1864834 -6.575428 -6.6328378 -6.874301 -7.3896747 -8.29885 -8.9402122 -9.6593113 -10.050488 -10.217554 -9.9110527][-10.369692 -10.373482 -8.72453 -8.5963993 -7.7610121 -6.8188329 -6.9827127 -6.7688193 -6.7208567 -7.4861274 -8.0456762 -7.532618 -7.698482 -7.7562771 -8.0897541][-7.2583132 -7.183085 -6.6422472 -6.0302086 -5.5647011 -6.481564 -6.5818434 -6.8382072 -7.1099696 -6.7992735 -6.6732168 -7.1389713 -7.7812657 -8.1566458 -8.38132]]...]
INFO - root - 2017-12-15 23:53:13.938671: step 73610, loss = 0.20, batch loss = 0.15 (11.7 examples/sec; 0.681 sec/batch; 48h:59m:32s remains)
INFO - root - 2017-12-15 23:53:20.584132: step 73620, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.678 sec/batch; 48h:45m:18s remains)
INFO - root - 2017-12-15 23:53:27.214934: step 73630, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 47h:37m:13s remains)
INFO - root - 2017-12-15 23:53:33.790752: step 73640, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 47h:33m:30s remains)
INFO - root - 2017-12-15 23:53:40.361936: step 73650, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 47h:27m:17s remains)
INFO - root - 2017-12-15 23:53:46.965736: step 73660, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 47h:30m:16s remains)
INFO - root - 2017-12-15 23:53:53.581907: step 73670, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 48h:15m:42s remains)
INFO - root - 2017-12-15 23:54:00.081631: step 73680, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 47h:03m:42s remains)
INFO - root - 2017-12-15 23:54:06.661330: step 73690, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 46h:46m:20s remains)
INFO - root - 2017-12-15 23:54:13.256783: step 73700, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 46h:06m:00s remains)
2017-12-15 23:54:13.805182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8012137 -5.856071 -5.5522013 -5.5303531 -6.3730383 -6.6114345 -6.5961127 -6.4786668 -5.793714 -4.5471849 -3.1694756 -4.5351162 -6.3671331 -7.6194668 -7.2021937][-6.3240204 -6.8578963 -7.1765304 -7.5285625 -7.8151321 -7.820159 -7.70849 -7.0852585 -6.3819752 -5.6496844 -5.2699633 -6.4389191 -8.0245295 -8.1961012 -7.7974339][-3.9947836 -5.4217463 -6.4439607 -6.6789007 -7.3109679 -7.8280706 -7.6750984 -7.0830612 -7.0227237 -6.5959024 -6.1493855 -7.7710285 -9.8642721 -10.431904 -9.6517315][-2.1972327 -3.5564055 -5.064343 -5.9117637 -6.8265033 -6.3934627 -6.1431127 -6.544096 -6.4155669 -6.3638787 -6.581625 -9.3975334 -11.838631 -11.014781 -9.8183947][-3.6808329 -4.4518218 -5.5940118 -5.3177452 -4.2900076 -3.3390658 -3.3005178 -3.9981384 -5.4156122 -5.8655996 -5.8933368 -8.4048252 -11.4076 -12.057932 -10.868813][-5.3833618 -5.6817489 -5.4361057 -3.6518033 -1.8851123 0.022511482 1.741219 0.3994875 -1.6173086 -3.1564269 -4.4175239 -7.3830118 -10.494467 -10.786015 -10.052208][-8.5379562 -8.200141 -6.4974971 -3.220428 -0.41883707 2.8116193 6.2104487 5.6757617 3.2436261 -0.31674957 -3.0859296 -6.0784855 -8.7861338 -10.106049 -9.7484341][-8.665556 -8.8185091 -7.294908 -2.3691361 0.82685614 4.9872804 8.0735416 7.1626906 5.7925096 1.8223648 -2.2230721 -6.2903848 -9.8450718 -9.9136734 -8.5350819][-5.1090126 -6.3715262 -5.5756645 -2.9904163 -0.85531855 3.1137166 5.9260497 5.9379458 5.348434 1.7196403 -1.7284997 -6.80549 -10.413795 -10.580463 -8.923317][-4.0207863 -4.5765352 -4.5191746 -2.2742331 -1.6864495 1.1126781 2.2592583 2.4081721 2.1121316 -0.89331675 -3.4636259 -8.12057 -12.038508 -12.0977 -10.46035][-5.8943877 -7.0041914 -7.11547 -6.2101541 -6.1337466 -4.7407961 -4.4045105 -3.7040806 -3.7830467 -4.9032693 -5.9194655 -10.761171 -13.405281 -13.662462 -11.864561][-10.081638 -10.76877 -10.251754 -9.177701 -9.0486984 -8.4773121 -9.112668 -9.2442789 -9.58296 -9.9252357 -10.119458 -11.155439 -11.752141 -13.181406 -11.892648][-12.409496 -12.1992 -11.757217 -11.737762 -11.656013 -10.661665 -10.954054 -11.161844 -11.570276 -11.453437 -11.175483 -11.349106 -11.110043 -10.928756 -8.4722319][-10.38109 -10.902838 -10.404348 -11.055231 -11.226328 -10.415959 -10.092985 -9.6563168 -9.9880562 -10.404648 -10.755375 -9.6060181 -8.754364 -8.7927485 -7.5670643][-7.2166843 -6.754909 -6.40773 -6.84904 -6.9564638 -6.9925632 -7.2489076 -7.0475726 -6.9377303 -6.8491154 -7.4149 -8.1483221 -9.2229729 -8.50141 -7.1343522]]...]
INFO - root - 2017-12-15 23:54:20.366014: step 73710, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 47h:08m:47s remains)
INFO - root - 2017-12-15 23:54:26.980151: step 73720, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 46h:53m:22s remains)
INFO - root - 2017-12-15 23:54:33.560015: step 73730, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 46h:55m:44s remains)
INFO - root - 2017-12-15 23:54:40.198319: step 73740, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 46h:58m:13s remains)
INFO - root - 2017-12-15 23:54:46.819434: step 73750, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 47h:29m:32s remains)
INFO - root - 2017-12-15 23:54:53.401198: step 73760, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 46h:44m:42s remains)
INFO - root - 2017-12-15 23:55:00.053907: step 73770, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 47h:41m:16s remains)
INFO - root - 2017-12-15 23:55:06.630105: step 73780, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 46h:44m:58s remains)
INFO - root - 2017-12-15 23:55:13.291045: step 73790, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 46h:42m:57s remains)
INFO - root - 2017-12-15 23:55:19.830673: step 73800, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 46h:24m:25s remains)
2017-12-15 23:55:20.363864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9137516 -3.3111265 -3.40993 -3.3251102 -3.4932089 -4.042654 -4.7095184 -4.1882181 -3.8799939 -3.2770483 -2.4556541 -2.8051369 -5.5506573 -8.3109264 -10.428713][-2.3705261 -2.3614373 -3.0908349 -5.0218062 -6.117641 -6.5982842 -7.0548096 -7.2123318 -6.5258245 -5.1844354 -4.5470819 -5.1430893 -7.870533 -9.48153 -9.9713478][-1.8911638 -3.3067818 -4.8452554 -6.2406368 -7.4862962 -7.8778114 -7.6217957 -7.6785021 -7.46843 -6.8037271 -6.1035938 -5.9229536 -9.0670443 -10.842785 -12.12243][-2.5224447 -4.0354843 -5.8210526 -7.429647 -8.6964817 -8.5050621 -7.7484803 -7.0177822 -6.6115165 -6.7787747 -7.0290484 -7.9805355 -10.041416 -10.924925 -11.178423][-3.5962353 -3.8666115 -5.2873836 -6.9047375 -7.2843237 -6.5858421 -5.9320588 -5.2394137 -5.1808786 -5.0601845 -4.5155172 -5.7586122 -9.2345524 -11.115541 -11.863499][-4.0189772 -2.9985235 -2.19254 -3.03753 -3.1445754 -1.6516166 -0.16330099 -0.28125334 -1.8315334 -3.8878622 -4.7355676 -4.8277826 -7.475318 -10.110219 -10.404988][-5.2514019 -4.88285 -3.7182643 -2.214052 -0.63010931 1.6199708 4.13409 5.1312776 4.78814 1.7870541 -0.71219397 -2.5182555 -5.520555 -6.9919214 -7.7308555][-6.2504926 -5.4967494 -4.433023 -2.1984429 0.66071892 3.2234445 5.1068788 5.3947282 5.3785844 3.2806067 0.50802279 -1.4386015 -4.7266679 -5.7745123 -5.7021189][-6.0075264 -5.2899342 -4.1712847 -2.5167146 -0.37385654 3.3430991 5.1375136 5.5245919 5.3112845 3.3326364 1.6126962 -0.23557901 -4.45568 -6.1648231 -6.3232927][-4.902051 -5.1060128 -3.8607788 -2.4638503 -1.9597776 -0.19154787 1.6943226 3.0673337 3.7913127 2.0538745 -0.35480356 -1.6602983 -5.5528889 -7.2271571 -7.9152527][-6.3576283 -5.6235404 -4.766252 -3.5456581 -2.9021661 -2.7071931 -3.3448064 -2.480989 -1.3319087 -2.2834785 -4.5989919 -7.25369 -9.0709133 -9.0569592 -8.185132][-9.9347382 -8.7804432 -6.4693942 -3.8409517 -2.5916121 -2.9965904 -4.3242621 -5.6575208 -6.36392 -7.0625734 -7.3710771 -9.6827812 -10.42205 -9.6322432 -8.4846668][-11.539148 -9.482172 -6.3806787 -4.4918923 -3.5451462 -2.8032992 -3.8921905 -5.5751719 -6.7676525 -7.5787578 -8.0916786 -9.0778656 -8.8641891 -7.6699219 -6.0777621][-10.804443 -9.9079742 -8.1739283 -6.1137853 -3.7894182 -3.9574649 -4.4774895 -4.8434286 -5.0979552 -5.2297258 -6.9253912 -6.8279028 -6.0801439 -5.6506219 -5.6601152][-7.90329 -8.59061 -8.039608 -6.4627428 -5.2595329 -4.8719707 -4.7151537 -4.6078978 -4.8591666 -4.5280952 -5.487195 -5.8844624 -6.2882037 -6.2125874 -5.9097977]]...]
INFO - root - 2017-12-15 23:55:26.899349: step 73810, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 46h:34m:21s remains)
INFO - root - 2017-12-15 23:55:33.541733: step 73820, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 47h:02m:02s remains)
INFO - root - 2017-12-15 23:55:40.135546: step 73830, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 47h:19m:03s remains)
INFO - root - 2017-12-15 23:55:46.722708: step 73840, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 46h:28m:50s remains)
INFO - root - 2017-12-15 23:55:53.364099: step 73850, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.648 sec/batch; 46h:32m:10s remains)
INFO - root - 2017-12-15 23:55:59.897877: step 73860, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 47h:12m:43s remains)
INFO - root - 2017-12-15 23:56:06.451362: step 73870, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 46h:27m:12s remains)
INFO - root - 2017-12-15 23:56:13.048665: step 73880, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 48h:20m:20s remains)
INFO - root - 2017-12-15 23:56:19.654594: step 73890, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 48h:26m:57s remains)
INFO - root - 2017-12-15 23:56:26.186868: step 73900, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 47h:48m:44s remains)
2017-12-15 23:56:26.726337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2960553 -5.7369084 -5.1918936 -4.2235718 -4.51806 -4.5342946 -4.4789209 -4.1419439 -3.7997046 -3.5728612 -3.3548114 -5.8738413 -6.7102332 -7.4217715 -7.8902273][-5.9588084 -6.1296911 -6.1588521 -5.5251303 -5.7100363 -5.6004496 -5.6543913 -5.7121243 -5.6114492 -5.1057539 -4.1808863 -5.4649467 -6.4487305 -8.0815887 -8.7454739][-5.4038229 -5.9666629 -6.5113311 -5.0669689 -4.6356692 -4.9948072 -5.194037 -5.3717403 -5.7428913 -5.224566 -4.5414023 -6.1091056 -6.6391835 -7.5881243 -8.8098469][-4.30569 -4.7621031 -5.07158 -4.9246397 -5.3930082 -4.4918818 -4.0063672 -4.0814061 -4.27761 -4.468142 -4.234324 -5.9607286 -7.1269832 -8.5375023 -9.3175783][-5.4582667 -7.0408912 -7.4901481 -5.6621227 -5.035152 -3.2516508 -2.0359907 -2.6291022 -3.1389351 -3.1908453 -3.3244035 -5.1600595 -6.6076088 -8.61496 -10.370707][-7.1015363 -7.010232 -6.0336232 -4.1394939 -2.515573 0.19761419 1.7151694 1.5339146 1.4541035 0.24821424 -0.2852149 -3.0393465 -5.3146296 -7.61834 -8.8428812][-7.1236887 -7.2405787 -7.2257638 -3.4809732 -0.60084248 2.2020979 4.1177754 4.4080148 4.4843869 2.3092589 0.39441919 -2.3960314 -4.1545196 -7.0431442 -8.2936726][-6.2712379 -6.1139517 -5.582026 -2.73474 -0.0447073 2.7220578 3.7886815 4.4691148 4.8860059 2.80339 0.89032841 -2.6247222 -5.0241923 -7.4499578 -8.0382824][-5.2292471 -4.6233606 -4.141283 -1.8988073 -0.076641083 1.9652643 2.985579 3.3126636 3.2459683 1.796597 0.74070644 -3.4627376 -6.628634 -8.3130589 -9.1133957][-3.4239824 -3.0518842 -2.3458948 -0.86815166 -0.5396409 0.89434433 2.0304551 2.1877308 1.0591693 -0.31729746 -1.1940126 -4.4030004 -6.6271787 -8.2257586 -8.9884558][-5.0989265 -4.7616248 -3.9966717 -3.860029 -4.1184678 -3.3948123 -3.0049164 -2.8553059 -3.0798135 -3.6791005 -4.4873514 -7.6791005 -9.0260563 -9.1866226 -8.8966846][-10.032444 -10.233093 -9.1339693 -8.2281542 -7.4425817 -6.3800888 -5.9375572 -5.8844709 -6.0198631 -6.3213758 -6.8086605 -7.8697762 -8.3722305 -8.4371367 -8.03183][-12.303356 -11.174836 -9.0796318 -8.2597055 -8.4819841 -7.12306 -6.3284049 -5.7029371 -5.2108631 -5.3557119 -5.7573895 -6.6690125 -6.9187236 -6.1725464 -5.5238838][-9.7652054 -9.7484779 -8.2630529 -7.3553081 -6.8457084 -5.5389147 -5.4184828 -4.9393578 -4.28606 -4.7148843 -5.4314556 -5.6302176 -5.4913497 -4.8517222 -3.8899012][-8.6126108 -7.767592 -6.6027966 -5.3777146 -4.1991997 -3.0465634 -3.0479095 -2.9022422 -3.0635624 -3.3401752 -3.6937547 -4.5660667 -5.6819191 -6.589571 -6.5346241]]...]
INFO - root - 2017-12-15 23:56:33.279802: step 73910, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 46h:25m:06s remains)
INFO - root - 2017-12-15 23:56:39.826133: step 73920, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.648 sec/batch; 46h:30m:46s remains)
INFO - root - 2017-12-15 23:56:46.414712: step 73930, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 45h:48m:31s remains)
INFO - root - 2017-12-15 23:56:52.923939: step 73940, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 48h:27m:33s remains)
INFO - root - 2017-12-15 23:56:59.558724: step 73950, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 47h:25m:44s remains)
INFO - root - 2017-12-15 23:57:06.134188: step 73960, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 47h:34m:44s remains)
INFO - root - 2017-12-15 23:57:12.829673: step 73970, loss = 0.10, batch loss = 0.06 (11.7 examples/sec; 0.683 sec/batch; 49h:01m:47s remains)
INFO - root - 2017-12-15 23:57:19.351915: step 73980, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 46h:56m:04s remains)
INFO - root - 2017-12-15 23:57:25.950624: step 73990, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 45h:56m:02s remains)
INFO - root - 2017-12-15 23:57:32.460220: step 74000, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.669 sec/batch; 48h:02m:10s remains)
2017-12-15 23:57:33.000293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4981337 -2.283828 -1.8637555 -1.9914329 -3.0874486 -3.2457898 -3.3002093 -3.6248991 -3.695302 -4.2778893 -5.2323904 -8.69489 -10.487185 -9.9751272 -8.3490267][-0.79330206 -0.8165803 -0.57234859 -0.97215748 -2.3922734 -2.89542 -3.0667191 -2.650094 -2.2755661 -3.0994408 -3.9446821 -7.51924 -9.4858055 -9.5162468 -8.40355][-1.4557323 -1.2641687 -1.6423941 -1.1600661 -1.9456775 -2.6626768 -2.7926767 -2.7337787 -2.1845219 -1.9267175 -2.3860154 -5.807445 -7.2588692 -8.1132965 -7.5660095][-3.2169769 -2.8157682 -2.7540123 -2.1965251 -2.9842844 -3.2113397 -3.2501516 -2.5911405 -1.961659 -1.6203961 -1.707715 -4.5067611 -6.1162519 -7.0203114 -7.0823712][-3.311621 -3.9431386 -4.5854836 -2.969382 -2.6785183 -2.0958126 -1.5037322 -1.9115486 -1.6162477 -1.3027582 -1.3535886 -3.9613557 -5.497 -6.135704 -5.3919][-4.8295274 -4.3061938 -3.627449 -1.8893707 -0.99180794 0.45269966 1.642447 1.1403379 0.57231 -0.062811375 -0.22024727 -2.6621785 -4.3120127 -4.7531958 -4.0331988][-4.7430248 -4.0162 -3.0866861 -0.48460531 0.93198681 2.7853913 4.2746654 4.0518346 3.2468286 1.6174459 0.56988955 -2.5358393 -4.3690128 -3.923842 -2.7523885][-5.8284979 -4.405211 -3.2214668 -0.21113014 1.9250288 3.5617414 4.99598 4.8306632 4.2443318 2.6954274 1.2367444 -2.0742111 -3.4489973 -3.1479945 -1.7493517][-5.7464142 -4.2162795 -3.0671649 0.20303249 1.6000295 2.4587865 3.9372668 3.5653319 3.0539231 1.462678 0.13525152 -3.5664334 -5.3090863 -4.7569466 -3.2702136][-6.4267344 -5.2267427 -4.742866 -0.58815813 0.49404144 0.73645163 1.4622545 0.38647079 -0.6433444 -2.1765525 -3.2590985 -6.7625923 -8.1039743 -6.5827842 -4.1918712][-10.947452 -10.067616 -9.1701088 -6.4139438 -6.4873962 -5.32769 -4.590559 -5.0332384 -4.9252911 -5.8458252 -6.8817496 -9.9249144 -10.609827 -8.5299988 -5.4005933][-14.068888 -13.830399 -12.53228 -9.8939095 -9.5418491 -8.4816608 -8.2319984 -8.7301226 -8.672081 -8.8689585 -9.3996 -10.620646 -9.8787327 -8.1174307 -5.1049104][-14.310221 -13.010027 -12.117713 -10.237009 -9.7041626 -8.4094524 -7.8206291 -8.3322477 -8.9593744 -9.2588778 -9.7492056 -10.470701 -10.621489 -8.1245127 -4.4656687][-10.412727 -9.2609806 -7.88766 -6.6304231 -6.3030353 -5.114634 -5.0138927 -5.7058778 -6.2305975 -7.6873331 -9.2961788 -9.7983656 -9.6134033 -8.6946106 -7.237639][-7.9344034 -6.9554582 -4.8400168 -3.0245414 -2.5985827 -2.0694797 -1.8804832 -2.481847 -2.9856889 -4.2231865 -5.6509042 -6.9944649 -7.6688156 -7.6539259 -7.2185655]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-15 23:57:39.589915: step 74010, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 49h:13m:02s remains)
INFO - root - 2017-12-15 23:57:46.218274: step 74020, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 46h:52m:03s remains)
INFO - root - 2017-12-15 23:57:52.875437: step 74030, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 47h:43m:32s remains)
INFO - root - 2017-12-15 23:57:59.351645: step 74040, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 46h:08m:56s remains)
INFO - root - 2017-12-15 23:58:05.977056: step 74050, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 47h:51m:55s remains)
INFO - root - 2017-12-15 23:58:12.583720: step 74060, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.640 sec/batch; 45h:57m:45s remains)
INFO - root - 2017-12-15 23:58:19.158942: step 74070, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.653 sec/batch; 46h:54m:41s remains)
INFO - root - 2017-12-15 23:58:25.714808: step 74080, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 47h:30m:24s remains)
INFO - root - 2017-12-15 23:58:32.344908: step 74090, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 46h:49m:52s remains)
INFO - root - 2017-12-15 23:58:38.975582: step 74100, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 48h:34m:01s remains)
2017-12-15 23:58:39.478244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5350804 -6.045989 -4.2341819 -3.2163863 -4.1955128 -4.6784096 -4.8105927 -4.5456061 -4.5185781 -4.7070036 -5.0513644 -8.6785593 -10.21511 -9.88629 -10.981464][-7.5253668 -7.3724546 -5.6171231 -4.3771954 -4.5055737 -5.6483231 -6.7524567 -6.2608871 -5.9130673 -5.6606464 -5.40979 -9.7670317 -11.814905 -10.810445 -11.231258][-4.7075949 -7.6360321 -7.620389 -6.1177373 -5.5542259 -5.4099617 -5.7869649 -6.4281659 -7.2324142 -6.55915 -5.9233747 -10.55727 -12.365734 -11.200109 -12.447416][-2.4786143 -4.5563807 -5.4719253 -4.8527236 -5.0119753 -5.0834 -4.681355 -4.4773426 -5.0659776 -5.4473925 -5.5974531 -8.8550272 -9.9409466 -10.810878 -12.637836][-4.9933281 -5.9799857 -4.4725704 -1.9732037 -1.1266565 -1.4166002 -1.0272245 -2.1151862 -4.1036329 -2.8392894 -1.6784277 -6.1912713 -8.1937141 -8.6852245 -10.98983][-5.5526271 -6.621088 -6.0334997 -3.0857732 0.010452271 2.5481906 4.6663651 1.4482212 -1.7988966 -0.71051931 -0.71181107 -4.6973886 -5.3481884 -6.7106705 -10.377159][-6.8329949 -6.8867903 -5.5727181 -2.9836774 -0.46504641 2.3811765 5.5580621 4.25853 2.6810346 0.41248274 -2.9823265 -5.1835556 -4.7637973 -5.5411911 -7.606617][-7.411345 -6.085989 -3.8951707 -1.3005629 0.30564451 2.3186517 5.1233611 4.979218 4.9511046 2.6624494 -1.077446 -6.1429219 -8.6956387 -8.1115408 -8.510788][-5.7889161 -5.6405225 -3.3451571 -0.66883135 0.30770636 1.9318552 3.9463534 2.5896249 1.5090814 1.6803412 0.7254076 -6.0540423 -11.190447 -12.109392 -12.964533][-4.5459719 -4.4697952 -3.7479153 -1.7385445 -1.3129616 -0.24260616 0.63444471 -0.086361408 0.10659122 -0.64342976 -2.1258576 -7.0935678 -10.956295 -12.601344 -14.991854][-5.5712032 -5.4181957 -5.5419626 -4.2916088 -3.6204846 -2.7471602 -3.2049382 -2.7465272 -1.9012675 -3.0702629 -4.191649 -8.0782928 -10.503466 -11.590727 -13.253462][-7.652153 -6.3448553 -5.8574224 -5.3569994 -4.7722211 -4.5723896 -4.7937236 -3.8958454 -3.465409 -4.240653 -4.72481 -8.01821 -9.58857 -9.8598461 -9.7791395][-12.583386 -10.961557 -8.4041414 -7.2679977 -7.9078035 -8.0288849 -7.556591 -6.5397825 -6.5053787 -6.0913639 -5.8830209 -7.9184403 -8.9593058 -8.5789547 -7.8669934][-10.435789 -9.2521763 -8.7013426 -8.5132895 -7.0750751 -6.624691 -7.4026394 -7.7649803 -7.257113 -6.1512022 -5.5703635 -5.6120481 -5.8408074 -7.0160542 -7.5496731][-6.3162985 -5.9606485 -5.1565809 -3.3311346 -2.5566411 -4.0492611 -5.6408062 -5.5592184 -5.0260715 -6.0558152 -6.8023453 -6.2232776 -6.0475941 -5.7748876 -5.4252906]]...]
INFO - root - 2017-12-15 23:58:45.987638: step 74110, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 46h:07m:43s remains)
INFO - root - 2017-12-15 23:58:52.603668: step 74120, loss = 0.19, batch loss = 0.14 (11.8 examples/sec; 0.680 sec/batch; 48h:46m:50s remains)
INFO - root - 2017-12-15 23:58:59.258269: step 74130, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 48h:55m:01s remains)
INFO - root - 2017-12-15 23:59:05.830248: step 74140, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 47h:19m:17s remains)
INFO - root - 2017-12-15 23:59:12.532499: step 74150, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 47h:56m:02s remains)
INFO - root - 2017-12-15 23:59:19.115686: step 74160, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 46h:51m:41s remains)
INFO - root - 2017-12-15 23:59:25.681172: step 74170, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.665 sec/batch; 47h:44m:26s remains)
INFO - root - 2017-12-15 23:59:32.257674: step 74180, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 45h:41m:39s remains)
INFO - root - 2017-12-15 23:59:38.823348: step 74190, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 46h:40m:21s remains)
INFO - root - 2017-12-15 23:59:45.386752: step 74200, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 46h:43m:07s remains)
2017-12-15 23:59:45.889498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6430883 -5.3599262 -5.6115065 -5.5909581 -5.4817572 -4.4841833 -4.1235762 -4.8756418 -5.6650929 -5.6018796 -5.4021759 -6.6690764 -7.2145357 -8.0677567 -8.9211178][-2.6514204 -3.5516083 -4.642261 -4.1625347 -3.9302115 -4.0735903 -4.3250494 -4.4156036 -5.1255403 -6.5763636 -7.0042295 -8.3800564 -8.9673309 -10.749381 -10.766706][-3.7862334 -3.994853 -4.1768975 -4.9529309 -5.8118124 -5.8352637 -5.8067546 -5.7024183 -5.2291975 -6.1995006 -8.5685053 -10.595402 -10.820063 -11.71504 -11.376013][-6.5600653 -6.5307822 -5.9757371 -6.6131344 -7.5531125 -7.8533883 -7.1384926 -6.4256697 -6.3425593 -6.5421753 -7.5665522 -10.935857 -12.04944 -12.314489 -11.707245][-7.6459694 -8.0986958 -8.7103615 -7.4127579 -7.1618981 -6.6809764 -5.14229 -4.597333 -4.9684587 -5.0415535 -5.7325168 -8.9458437 -10.443141 -11.908207 -12.606121][-6.4399009 -7.0550628 -7.1912866 -5.3636575 -3.4940896 -0.63032293 1.3945069 0.021368504 -1.2429929 -2.46289 -4.0595651 -6.2230229 -7.7456031 -10.087463 -11.212434][-7.8417187 -7.5360465 -6.4847822 -3.7349167 -0.84762478 3.1640515 7.0702872 7.0266213 5.3913627 1.1325355 -3.3603711 -4.8568039 -5.3490644 -8.3362312 -10.395255][-7.605351 -8.0322514 -6.6196308 -3.8556173 -0.72551775 4.3905387 9.4717865 9.64925 8.5435848 4.3912063 -1.0266972 -5.0574818 -6.6177025 -7.4525666 -8.379097][-6.3165693 -6.6258035 -6.2424107 -4.3112655 -3.1674621 1.2250681 6.4348788 7.5200944 6.7514119 2.1225247 -2.3147025 -6.48768 -9.6001453 -10.703098 -10.58993][-5.6975155 -5.485393 -5.924315 -3.8195105 -2.173279 -0.75800657 0.791008 1.8417163 1.8763714 -1.8883431 -5.6229525 -10.100983 -13.090851 -14.099911 -14.512997][-8.62653 -8.5401173 -8.3654575 -6.0547142 -5.0750232 -4.6607175 -4.0220804 -4.9215612 -5.9055877 -7.6463113 -10.368856 -14.236879 -16.667391 -17.349838 -16.414713][-14.0112 -13.596872 -13.060359 -10.97323 -9.9727745 -9.0944443 -8.6413593 -9.9969635 -11.022886 -12.110815 -13.927856 -15.337347 -16.049028 -15.821671 -15.294106][-15.494883 -14.25745 -12.472916 -11.313742 -11.897324 -11.100681 -10.680601 -10.819645 -10.833467 -11.531143 -12.297327 -13.678919 -13.481071 -12.221382 -10.357855][-14.096827 -13.510769 -12.488466 -10.77386 -10.434753 -10.387492 -11.093738 -11.301809 -10.75331 -10.62362 -10.386217 -9.852253 -8.8601694 -8.5410366 -7.689393][-9.0613213 -8.4949532 -8.1099806 -7.4307084 -6.4010291 -6.269238 -7.180438 -7.891737 -8.3773355 -8.2679214 -7.90779 -8.8047724 -8.7583742 -7.6247187 -7.0651889]]...]
INFO - root - 2017-12-15 23:59:52.514300: step 74210, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 48h:13m:13s remains)
INFO - root - 2017-12-15 23:59:59.154028: step 74220, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 48h:44m:19s remains)
INFO - root - 2017-12-16 00:00:05.741302: step 74230, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 47h:23m:01s remains)
INFO - root - 2017-12-16 00:00:12.340482: step 74240, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 46h:52m:56s remains)
INFO - root - 2017-12-16 00:00:18.983273: step 74250, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 47h:27m:43s remains)
INFO - root - 2017-12-16 00:00:25.644513: step 74260, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 47h:56m:03s remains)
INFO - root - 2017-12-16 00:00:32.179001: step 74270, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 48h:08m:17s remains)
INFO - root - 2017-12-16 00:00:38.799726: step 74280, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 48h:34m:43s remains)
INFO - root - 2017-12-16 00:00:45.390002: step 74290, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 47h:05m:57s remains)
INFO - root - 2017-12-16 00:00:51.958839: step 74300, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 46h:59m:38s remains)
2017-12-16 00:00:52.498678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0777321 -3.4351258 -3.7350729 -3.0500338 -3.510493 -4.2904472 -5.3868084 -6.3348594 -6.4814119 -6.1368294 -5.6434956 -4.4560761 -4.8903556 -6.3383884 -8.0927525][-7.0003152 -6.0674791 -5.3043814 -5.3937531 -5.4401865 -6.3215017 -7.4880757 -7.7957759 -7.6575565 -6.95307 -6.1544356 -6.1515846 -7.8206272 -7.65718 -7.2834616][-7.0175824 -7.8565159 -7.8107748 -7.72162 -8.2225657 -8.1775255 -8.2241526 -8.2051468 -8.005537 -7.7190676 -7.5393262 -5.7289295 -5.2950225 -5.9355478 -6.4696927][-4.4513927 -5.2841244 -5.9688554 -7.673924 -9.0930367 -9.1767673 -9.1882153 -9.3461294 -7.8227983 -6.0399208 -5.4967704 -5.2007637 -6.5454254 -6.3949041 -6.5832386][-5.9010777 -6.1799879 -6.1577139 -6.3030615 -6.2548089 -5.38797 -4.9072976 -5.1185822 -5.1294537 -4.3267894 -4.1424437 -3.1054168 -4.0620556 -5.0906558 -6.5150504][-6.7225089 -5.40728 -3.8388112 -3.6514423 -3.5208933 -2.2582445 0.79384565 0.84727287 0.97841644 -0.54837561 -1.5867381 -1.3620896 -3.6079235 -5.1408114 -7.5035877][-6.4031868 -7.3859482 -5.4070244 -4.2342854 -2.7949419 -0.81797886 2.5337749 5.3149543 6.2985997 3.3344998 -0.23789835 -1.781539 -4.3896961 -5.2357235 -6.1240911][-6.3092041 -6.953784 -5.5504675 -3.7420256 -2.0995598 1.1815763 4.6095529 7.2481637 7.7480235 4.9014134 0.94011736 -2.6585603 -6.6532807 -6.4584379 -7.9996815][-6.1979971 -7.2250996 -5.4094195 -3.8741059 -2.5765028 0.65336752 3.5675797 5.0794978 6.0150857 3.3198524 -0.25601435 -3.6762938 -7.1366415 -7.9416943 -8.0790243][-6.0250983 -6.0391469 -4.9380131 -4.0695977 -2.6919124 -0.10623932 1.8118329 3.0287662 3.1365018 0.88893795 -2.1567843 -4.4635248 -6.2048326 -8.071681 -10.540091][-7.6868124 -6.4673877 -4.9604726 -4.6204596 -3.9726372 -2.9201818 -2.431618 -1.3448739 -1.7939885 -3.8209319 -6.155087 -6.8503442 -8.22263 -7.0291104 -6.3513494][-8.2168159 -7.8720045 -7.0395985 -6.6792541 -5.4603376 -5.766017 -5.5662484 -5.3560781 -5.8774347 -6.9903774 -7.9875441 -7.9164248 -7.6967497 -7.6307011 -7.4720507][-9.456749 -8.7186966 -8.0576782 -7.565681 -6.7988739 -5.9172592 -6.5941987 -7.46127 -8.4738045 -8.6799707 -9.0919952 -8.78559 -8.930047 -7.2572169 -6.6866827][-6.9735618 -7.7937422 -7.2367821 -6.9247117 -5.701138 -6.667697 -7.0150476 -7.3934484 -7.8087978 -8.3904934 -8.5890789 -7.1655645 -6.6648741 -6.6274252 -7.3497305][-6.267168 -7.6898985 -7.3131618 -6.4236512 -5.8380485 -5.3520865 -5.8378935 -5.5918827 -5.7007155 -6.2800722 -7.472374 -7.4719291 -7.5476432 -7.7971168 -7.8945494]]...]
INFO - root - 2017-12-16 00:00:59.138817: step 74310, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 47h:14m:18s remains)
INFO - root - 2017-12-16 00:01:05.678545: step 74320, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 48h:51m:16s remains)
INFO - root - 2017-12-16 00:01:12.280635: step 74330, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 47h:57m:13s remains)
INFO - root - 2017-12-16 00:01:18.902201: step 74340, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 46h:07m:13s remains)
INFO - root - 2017-12-16 00:01:25.520196: step 74350, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 47h:38m:52s remains)
INFO - root - 2017-12-16 00:01:32.097554: step 74360, loss = 0.11, batch loss = 0.06 (12.3 examples/sec; 0.649 sec/batch; 46h:32m:35s remains)
INFO - root - 2017-12-16 00:01:38.651126: step 74370, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 47h:07m:50s remains)
INFO - root - 2017-12-16 00:01:45.243960: step 74380, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 48h:18m:38s remains)
INFO - root - 2017-12-16 00:01:51.859055: step 74390, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.692 sec/batch; 49h:37m:32s remains)
INFO - root - 2017-12-16 00:01:58.424490: step 74400, loss = 0.12, batch loss = 0.07 (11.6 examples/sec; 0.691 sec/batch; 49h:31m:26s remains)
2017-12-16 00:01:59.005889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.401773 -4.8849392 -4.8260312 -4.1524076 -4.5603456 -4.7789803 -5.0531211 -4.5814881 -4.8738627 -5.0679941 -5.082242 -7.731123 -8.72699 -7.938221 -8.1553974][-3.4577162 -3.1227231 -2.7542179 -2.3416867 -3.3460426 -4.1166468 -4.6377373 -4.5256581 -4.6685696 -3.884099 -3.4119465 -5.7299609 -6.5293679 -7.4761372 -8.7276506][-2.1806619 -2.4722731 -2.5550437 -2.4437482 -2.8776102 -2.8784127 -2.9495831 -2.7542541 -2.494931 -2.6466918 -2.8800497 -5.3482904 -6.5295677 -6.845993 -6.9826126][-3.358134 -2.8845644 -2.5696883 -2.1163731 -2.5599051 -2.354429 -2.161576 -1.9978068 -2.064492 -2.1827838 -1.9564421 -4.8472104 -6.3300538 -6.6210814 -6.6158552][-3.0803449 -3.5575531 -3.405534 -2.2418425 -2.0006976 -1.3866072 -0.90326929 -0.77261209 -0.43161631 -1.0061889 -1.3875356 -4.1680617 -5.4209414 -5.7654696 -6.3666782][-3.7996941 -2.9300554 -2.2128973 -1.3415895 -0.53100014 0.29417467 0.81595421 0.62909126 0.47691059 -0.036451817 -0.88343048 -3.5732975 -4.7915893 -4.6799059 -4.6783228][-4.88972 -4.0577607 -2.6357238 -1.5233746 -0.26118135 0.85775328 1.2584491 1.2870517 1.0711389 0.66730595 0.52914286 -2.5505195 -3.6651676 -3.2771788 -3.8721142][-5.8106694 -5.2917452 -3.2676506 -1.472187 -0.43052292 0.54663229 1.3082633 1.5083423 1.3755307 0.93237257 0.67015314 -2.2417414 -3.2744398 -3.3971293 -4.3733835][-4.6315851 -4.760994 -3.7621355 -1.2725811 -0.27612352 0.805007 1.1305385 1.0006695 1.2138305 0.66932631 0.37330103 -2.652004 -3.7844255 -4.0133204 -4.3693752][-5.7011681 -4.7706437 -2.9692733 -0.96610022 -0.59963989 0.54398537 0.98179626 0.4473362 -0.078619 -0.19629288 -0.40896845 -3.1345642 -4.0157056 -4.2685776 -5.0410414][-8.6631889 -7.2128758 -4.3004942 -1.385097 -1.2435536 -0.87306452 -1.2164087 -1.9314101 -2.0752273 -1.9742799 -2.3689759 -5.3032784 -6.2417207 -5.5871024 -5.8563251][-9.8859138 -8.3513317 -6.5385628 -3.4208326 -2.5262341 -2.7163112 -3.7077587 -5.0578742 -5.2829905 -5.0742955 -5.4565854 -7.2422009 -6.9688406 -5.8538175 -5.7103858][-10.090714 -8.50345 -5.9418015 -4.3090658 -4.0003691 -3.8244 -4.3912673 -4.9016066 -5.4526982 -5.7493238 -5.5976744 -6.8456664 -6.8312039 -5.4502783 -4.9389033][-8.2595081 -6.9886317 -5.5492859 -4.7198114 -4.3659358 -4.5893431 -5.1043491 -4.8829918 -4.2749643 -4.3164644 -4.538682 -4.6707606 -4.1314116 -3.702378 -3.7746944][-5.2329717 -5.6336708 -5.8328586 -5.02387 -4.3533039 -3.6582246 -3.70405 -3.1189065 -3.0503561 -3.0469115 -2.8520913 -3.7201147 -4.694962 -5.4966812 -5.59745]]...]
INFO - root - 2017-12-16 00:02:05.692457: step 74410, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 47h:58m:56s remains)
INFO - root - 2017-12-16 00:02:12.268245: step 74420, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 45h:59m:23s remains)
INFO - root - 2017-12-16 00:02:18.810037: step 74430, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 47h:06m:52s remains)
INFO - root - 2017-12-16 00:02:25.392431: step 74440, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 48h:03m:43s remains)
INFO - root - 2017-12-16 00:02:31.909030: step 74450, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 46h:36m:02s remains)
INFO - root - 2017-12-16 00:02:38.419461: step 74460, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 47h:40m:15s remains)
INFO - root - 2017-12-16 00:02:44.991846: step 74470, loss = 0.19, batch loss = 0.15 (11.7 examples/sec; 0.683 sec/batch; 48h:57m:54s remains)
INFO - root - 2017-12-16 00:02:51.551543: step 74480, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 48h:36m:55s remains)
INFO - root - 2017-12-16 00:02:58.155110: step 74490, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 48h:12m:32s remains)
INFO - root - 2017-12-16 00:03:04.858362: step 74500, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 49h:00m:22s remains)
2017-12-16 00:03:05.389471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6055164 -6.3424721 -6.404892 -5.8253665 -5.8012471 -5.50198 -6.0530887 -7.0141234 -7.2702551 -6.9900093 -6.1897588 -5.9629192 -8.1898661 -9.7423592 -10.233053][-4.1386776 -5.9300475 -5.8347821 -4.6341648 -4.6370049 -5.4628944 -6.58859 -7.4370747 -7.7145271 -6.4433064 -4.198595 -4.01004 -6.6516647 -8.89608 -9.36313][-2.7389703 -4.9199133 -6.7199059 -5.9153991 -5.676558 -6.2268744 -7.073926 -7.442781 -7.3967614 -6.7177095 -4.93897 -3.4181838 -5.3377252 -8.8001814 -10.730803][-4.2761259 -6.1721435 -7.9114637 -7.9576654 -7.2195411 -5.9002347 -6.5792141 -7.8857636 -7.6594944 -6.2284288 -4.0910034 -3.6937883 -6.1316333 -9.8427181 -12.526917][-5.52001 -8.230423 -9.8190374 -8.74518 -7.1079321 -3.9117281 -2.5244856 -4.9561286 -6.7893052 -5.5897832 -3.8775992 -2.8361881 -4.9508152 -8.97283 -12.168718][-7.3114939 -7.4822426 -7.5283422 -5.2717237 -1.716476 2.1051197 3.9667106 3.0495467 0.73732662 -2.5230436 -5.0245147 -3.977982 -4.4463806 -7.6498 -11.101471][-9.24778 -7.9929876 -6.4754205 -4.3206968 -1.307693 4.0785995 7.7345777 7.6530213 5.4841542 -0.013157845 -5.3123569 -6.6827936 -8.9245911 -10.541694 -11.896223][-10.53768 -9.5772839 -8.1666327 -4.1101217 -0.14126539 3.7259822 6.9071965 6.8837705 4.6818776 1.1601481 -3.4112024 -7.4023714 -12.320198 -13.653534 -13.894365][-8.8635387 -8.4574747 -8.2363768 -4.5106015 -1.3051429 1.7089963 4.251781 4.064528 1.8091412 -1.765537 -5.2683196 -8.7700481 -13.479666 -14.648489 -14.760273][-6.65603 -5.6740146 -6.0907388 -5.1849294 -4.3820682 -2.0968235 1.1865883 1.1507134 -0.73872423 -4.2957072 -7.6337204 -9.682045 -13.285326 -15.306391 -16.398445][-9.5007477 -8.2971258 -8.5874863 -7.8546286 -8.3837948 -7.6901507 -5.9177637 -6.0080152 -6.6470642 -7.8776312 -10.370319 -12.728547 -14.809885 -14.756212 -14.020367][-15.560062 -15.029165 -14.289808 -13.265816 -13.52059 -13.356845 -12.702572 -12.177351 -11.716806 -11.833424 -12.596753 -13.108548 -13.951555 -14.173555 -13.248885][-14.638899 -13.950388 -14.653801 -14.108526 -13.549835 -12.444101 -11.976084 -12.078426 -12.077351 -11.778474 -11.669157 -11.869701 -11.574651 -10.690083 -10.262731][-12.45767 -12.048132 -12.058512 -10.5606 -9.5271378 -9.7164068 -10.029976 -9.4647961 -9.0871868 -9.2290773 -9.6597271 -9.2285938 -8.821373 -7.9723153 -7.4851475][-9.5868187 -8.6007652 -7.7193589 -6.772532 -6.8554993 -5.9471207 -5.508121 -6.5768385 -7.2442336 -6.8827696 -6.9111528 -7.7428794 -8.3662825 -8.6096878 -9.0329065]]...]
INFO - root - 2017-12-16 00:03:12.044744: step 74510, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 47h:42m:37s remains)
INFO - root - 2017-12-16 00:03:18.610486: step 74520, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.637 sec/batch; 45h:40m:57s remains)
INFO - root - 2017-12-16 00:03:25.360041: step 74530, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 48h:31m:19s remains)
INFO - root - 2017-12-16 00:03:31.879635: step 74540, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 48h:17m:56s remains)
INFO - root - 2017-12-16 00:03:38.435523: step 74550, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 47h:14m:49s remains)
INFO - root - 2017-12-16 00:03:45.070353: step 74560, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 47h:30m:27s remains)
INFO - root - 2017-12-16 00:03:51.621251: step 74570, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 47h:13m:02s remains)
INFO - root - 2017-12-16 00:03:58.192255: step 74580, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 46h:36m:05s remains)
INFO - root - 2017-12-16 00:04:04.794753: step 74590, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 47h:17m:50s remains)
INFO - root - 2017-12-16 00:04:11.349789: step 74600, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 46h:24m:37s remains)
2017-12-16 00:04:11.864242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9906163 -7.1285825 -7.0543213 -6.7235231 -7.5906696 -8.6862278 -9.85053 -10.010193 -9.8721733 -10.166201 -9.7389708 -9.9060411 -11.299681 -11.200845 -9.6131153][-7.0693865 -7.9496784 -7.6660919 -7.2999582 -7.3541794 -8.38717 -9.5286961 -10.467111 -10.956509 -9.9002047 -8.2616215 -9.19463 -10.417997 -11.17299 -11.207582][-5.6661468 -8.0687237 -8.9788094 -8.1371012 -8.0280342 -7.9926662 -8.6913776 -8.70276 -8.6304312 -8.5269985 -7.9043536 -8.1629524 -8.9313431 -11.000834 -11.43598][-7.2276249 -9.523242 -10.533628 -9.2717972 -7.4226651 -5.9619012 -5.4416785 -6.1880651 -6.893662 -6.6128883 -6.6550255 -7.3089905 -8.7502737 -11.169533 -11.245251][-8.32396 -11.263643 -13.280512 -11.293456 -7.0356054 -2.8510118 0.24311733 -1.5321679 -4.1264272 -4.4044766 -5.7112637 -7.0488243 -8.3570747 -9.6283035 -9.419426][-10.541813 -11.863541 -11.899977 -9.68615 -5.2684259 0.12926006 4.5442319 4.8281617 3.7480426 0.15569592 -3.809659 -4.0893521 -4.9773164 -6.8843284 -6.7081738][-10.060573 -11.257122 -10.513167 -7.14724 -2.8030422 3.2201896 8.1640015 7.7803626 6.8544869 2.6090379 -2.3527226 -4.3860183 -6.8695445 -7.4998984 -6.4923072][-10.781904 -11.325611 -9.8245144 -6.4591608 -1.1132569 4.7349324 8.5454369 8.1018028 6.1597629 2.5775723 -0.72455883 -3.9080524 -8.129612 -9.5735216 -9.5147314][-9.1202831 -9.7666092 -9.7822666 -6.9901824 -2.6526947 2.4220929 5.1368556 5.390841 4.3168941 0.52768707 -1.8272502 -4.9620571 -8.18382 -10.993605 -12.575307][-8.106678 -8.4704857 -8.4422922 -7.062067 -3.7139304 -0.38971472 2.2849793 3.5405622 1.4525485 -1.5328054 -3.406517 -6.274425 -9.0256557 -11.752979 -13.653294][-10.275264 -11.488568 -11.466021 -9.3662033 -7.1855116 -5.5490227 -3.4726977 -2.6489666 -4.299458 -5.6654115 -6.8543491 -9.9879665 -11.5446 -13.789717 -14.174046][-15.29451 -14.722094 -13.954226 -12.171005 -11.706631 -10.257799 -9.026021 -9.2220879 -10.391088 -10.053551 -10.471159 -11.890103 -13.526136 -16.101627 -15.804626][-14.819271 -15.290361 -13.881603 -13.001228 -11.982323 -10.959703 -11.210476 -10.972698 -10.569377 -10.714977 -10.733738 -10.723859 -11.491999 -12.15023 -11.717875][-13.350979 -13.054863 -11.752377 -11.048168 -9.9941721 -8.680789 -8.2395363 -8.4441643 -9.2341728 -9.1573925 -8.7528057 -8.1137991 -8.4883833 -9.7292309 -9.2208776][-10.535958 -9.9531422 -8.6757717 -6.801301 -6.1397738 -5.948873 -5.7524185 -5.4249864 -5.9051628 -6.2459054 -7.1496363 -7.9484916 -8.5931377 -8.6497869 -8.40173]]...]
INFO - root - 2017-12-16 00:04:18.419025: step 74610, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 46h:07m:20s remains)
INFO - root - 2017-12-16 00:04:25.001877: step 74620, loss = 0.24, batch loss = 0.20 (12.2 examples/sec; 0.657 sec/batch; 47h:02m:51s remains)
INFO - root - 2017-12-16 00:04:31.582155: step 74630, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.683 sec/batch; 48h:55m:03s remains)
INFO - root - 2017-12-16 00:04:38.231449: step 74640, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.683 sec/batch; 48h:55m:50s remains)
INFO - root - 2017-12-16 00:04:44.918846: step 74650, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 47h:47m:45s remains)
INFO - root - 2017-12-16 00:04:51.465434: step 74660, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 46h:46m:20s remains)
INFO - root - 2017-12-16 00:04:58.069410: step 74670, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 47h:05m:58s remains)
INFO - root - 2017-12-16 00:05:04.663255: step 74680, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 45h:57m:24s remains)
INFO - root - 2017-12-16 00:05:11.270332: step 74690, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.689 sec/batch; 49h:19m:29s remains)
INFO - root - 2017-12-16 00:05:17.882408: step 74700, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 47h:46m:32s remains)
2017-12-16 00:05:18.391104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0306654 -7.2002563 -6.6962867 -6.0402541 -6.129756 -5.5773611 -5.8336368 -5.7043071 -5.7800903 -6.2121096 -6.1925845 -6.9872351 -8.8591194 -9.4364223 -8.98104][-5.7079043 -6.3827124 -5.6130266 -4.9641037 -5.0306821 -5.1423264 -6.098063 -6.7335262 -7.5634685 -7.3938036 -7.1635385 -7.9880028 -10.277822 -11.48127 -10.6451][-3.3229795 -5.7075567 -6.1203728 -4.5658259 -4.580308 -5.3370705 -5.4323053 -6.3122873 -7.3168578 -8.0163212 -7.6625094 -8.1219873 -10.72242 -11.564643 -11.513607][-2.4306569 -4.1807694 -5.0472636 -4.2076426 -4.661891 -4.1250391 -4.2151341 -4.815485 -5.174264 -5.982904 -6.2427526 -6.949893 -9.8014545 -11.519137 -11.410851][-4.5838051 -5.7085657 -4.8562212 -3.972775 -3.515341 -2.851469 -2.7521169 -3.2548394 -2.7325983 -2.943135 -3.6056459 -5.2303381 -8.1141357 -9.4425554 -9.8756371][-5.4416757 -5.6722178 -4.8900704 -4.1255474 -3.3078885 -0.58100557 0.50452662 0.45779514 0.63827467 -1.0917997 -2.5020487 -3.4750149 -6.3419304 -8.09933 -8.975462][-6.4001217 -7.3960524 -5.8506975 -4.7395124 -2.5613897 0.825017 3.6337457 4.8013072 4.8390307 2.0541925 -0.50483656 -2.6674318 -5.6617918 -6.3872166 -6.9299269][-5.5765076 -6.2175469 -5.4304142 -3.0454314 0.15306997 2.7113881 4.3419623 5.5841308 5.3777061 2.7679124 0.29690552 -2.5042572 -5.930644 -6.5343704 -5.9303455][-3.9704218 -5.2628613 -4.6109109 -3.6426122 -1.4022741 2.4318395 4.9475951 5.3030076 3.8164763 2.6432261 0.95470524 -2.7319369 -5.2476635 -5.6429133 -5.7405958][-3.0308433 -3.5519972 -3.1098793 -1.4239516 -0.89439535 0.34970284 1.6414709 2.6878304 1.8484497 0.87882519 -0.26428461 -2.1178145 -4.1866727 -4.9917374 -5.7830062][-5.5636525 -5.2780366 -4.0737839 -3.3898158 -2.9152038 -1.9934928 -1.8788748 -1.4346461 -1.4983478 -2.2493041 -3.2768869 -4.0711288 -4.928967 -5.2316508 -4.7379589][-8.2652912 -9.0473738 -7.1107373 -4.8821859 -4.8282619 -5.7454934 -6.653954 -6.1542726 -5.6282015 -6.09556 -5.4206762 -5.655242 -6.5194392 -7.4174519 -6.7208395][-10.986202 -10.782362 -9.3466234 -7.9607234 -7.8668332 -7.5648351 -8.0069141 -8.61917 -8.8902 -8.0646677 -6.9108205 -7.39533 -7.4132576 -7.6707869 -7.4075108][-9.8414516 -10.525042 -9.56892 -7.374217 -7.2011371 -7.0723009 -7.7317762 -8.0847254 -8.9421425 -8.4466171 -7.4150147 -6.1720104 -5.1937361 -6.0327568 -5.8336349][-9.7329073 -9.0853319 -7.7884274 -6.9778886 -5.547771 -4.5189686 -5.62841 -6.3818865 -6.6866426 -6.2680545 -6.7147946 -7.7314434 -7.8466973 -7.0166922 -6.7182746]]...]
INFO - root - 2017-12-16 00:05:24.976152: step 74710, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 46h:48m:30s remains)
INFO - root - 2017-12-16 00:05:31.503335: step 74720, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 46h:32m:29s remains)
INFO - root - 2017-12-16 00:05:38.071560: step 74730, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 46h:06m:20s remains)
INFO - root - 2017-12-16 00:05:44.770053: step 74740, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 46h:24m:05s remains)
INFO - root - 2017-12-16 00:05:51.288470: step 74750, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 47h:31m:21s remains)
INFO - root - 2017-12-16 00:05:57.879994: step 74760, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 47h:20m:32s remains)
INFO - root - 2017-12-16 00:06:04.439641: step 74770, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 47h:06m:32s remains)
INFO - root - 2017-12-16 00:06:11.014569: step 74780, loss = 0.21, batch loss = 0.17 (12.3 examples/sec; 0.650 sec/batch; 46h:31m:15s remains)
INFO - root - 2017-12-16 00:06:17.603946: step 74790, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 47h:59m:40s remains)
INFO - root - 2017-12-16 00:06:24.277075: step 74800, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 46h:06m:44s remains)
2017-12-16 00:06:24.834780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8709073 -6.79649 -6.7120976 -6.3845797 -7.0592628 -7.0863571 -6.9944983 -7.3975878 -7.4790483 -5.7220964 -3.6119964 -1.362268 -2.6964648 -4.0404358 -4.9555798][-4.0727272 -3.4987025 -2.7410011 -2.5741277 -3.4786241 -4.4240355 -4.5781379 -4.0155296 -3.5542746 -3.1190019 -1.6464982 0.26973486 -1.8497477 -3.7260685 -4.3788757][-2.0637639 -2.6363244 -3.0693226 -2.5737851 -2.3505564 -2.5898075 -2.4882131 -2.5761423 -2.3192773 -1.4731364 -0.400527 -0.065945148 -3.7110481 -5.7565842 -6.5587816][-4.2082577 -4.7219434 -4.7300043 -4.5454507 -3.7282362 -3.3010626 -3.5852513 -3.5094979 -3.3981869 -2.8294945 -1.8318708 -1.3946424 -5.4357171 -7.8805175 -8.7153378][-5.0338793 -6.5813484 -6.1140075 -4.436408 -2.5590599 -1.1396766 -0.82382488 -2.011507 -3.1698232 -2.5267289 -1.9215052 -2.9398797 -7.2427921 -10.123583 -10.959647][-8.3392706 -7.7547245 -6.4667034 -3.7563229 -0.66562462 1.7390451 3.0377965 1.8207026 -0.32743311 -1.1544118 -1.8659382 -2.4528608 -6.7833786 -9.9233017 -10.483591][-9.0576086 -7.6193762 -5.0307441 -2.2330997 0.16717243 3.0956903 5.8681769 5.2430396 3.8845878 1.2511859 -1.8608451 -2.6431661 -6.4670973 -9.5382214 -10.503809][-8.6621437 -7.466002 -6.0525842 -2.5086491 0.56110382 2.1507854 4.0184531 4.1409574 3.457725 0.63903284 -2.008883 -3.3030365 -7.3129106 -9.1894541 -8.6940708][-5.9724355 -5.0383792 -5.5547028 -3.9784198 -2.4504528 0.40735197 2.7170815 1.8616457 0.93904591 -0.68885708 -3.6305151 -5.7622714 -9.19753 -10.878445 -10.901218][-5.5903525 -5.2829514 -6.1074514 -6.1997986 -5.7694387 -3.7319496 -1.2408428 -0.35516262 -1.0346122 -3.8492596 -6.7200065 -7.6667819 -11.217853 -13.454472 -13.097935][-11.923025 -11.71903 -12.01302 -12.161972 -11.681238 -9.7806149 -7.6019073 -6.8337336 -6.5656037 -7.9597349 -10.219505 -11.786381 -14.252541 -14.397917 -12.930927][-17.990082 -17.507292 -16.680426 -15.89777 -15.283978 -14.101297 -12.277311 -11.179976 -10.692467 -11.663258 -12.733328 -14.016975 -15.501007 -15.156202 -13.283587][-17.786587 -16.534681 -14.949142 -14.193371 -13.786442 -12.757803 -12.196597 -11.436539 -11.026449 -11.52079 -12.293018 -13.594729 -14.126114 -12.59068 -10.220689][-13.699371 -12.402407 -10.717209 -9.3768215 -8.9401731 -8.6965847 -8.9275942 -8.3632822 -8.8191614 -9.9330063 -11.405148 -11.746975 -12.12899 -11.493614 -9.133297][-9.5454855 -8.1231413 -5.5278845 -4.142108 -3.5420561 -3.7788696 -4.3135557 -4.8181939 -6.0403943 -6.5985131 -7.896575 -9.4033241 -10.245167 -9.8258286 -8.7131357]]...]
INFO - root - 2017-12-16 00:06:31.404216: step 74810, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 47h:28m:03s remains)
INFO - root - 2017-12-16 00:06:37.972042: step 74820, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 46h:33m:22s remains)
INFO - root - 2017-12-16 00:06:44.611389: step 74830, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 46h:37m:24s remains)
INFO - root - 2017-12-16 00:06:51.154629: step 74840, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 47h:10m:21s remains)
INFO - root - 2017-12-16 00:06:57.689882: step 74850, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 46h:31m:29s remains)
INFO - root - 2017-12-16 00:07:04.234852: step 74860, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 47h:39m:22s remains)
INFO - root - 2017-12-16 00:07:10.763986: step 74870, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 46h:41m:48s remains)
INFO - root - 2017-12-16 00:07:17.435992: step 74880, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 45h:44m:39s remains)
INFO - root - 2017-12-16 00:07:23.967224: step 74890, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 46h:17m:17s remains)
INFO - root - 2017-12-16 00:07:30.609910: step 74900, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 47h:46m:12s remains)
2017-12-16 00:07:31.121921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9473345 -3.8870945 -2.5836823 -1.9560709 -2.5857558 -2.4697249 -2.49589 -2.8169911 -2.6885183 -3.4199808 -4.5591164 -6.7710676 -7.6182985 -7.9102626 -7.3254604][-4.3362474 -4.9039483 -3.885159 -2.2885315 -2.0431759 -2.7898831 -3.1238112 -2.5876448 -1.1573062 -0.26987028 -0.27012682 -3.3064115 -5.1930757 -7.7754431 -8.1449242][-2.80549 -3.96159 -4.0576568 -4.1888146 -4.3620777 -3.6744277 -2.9814417 -2.0019343 -0.16354275 -0.37320471 -1.3646712 -4.3081594 -5.8649416 -6.5492435 -6.5279307][-3.5735 -4.7878933 -4.6739454 -4.2800517 -3.9523494 -3.5455363 -2.8565886 -2.6547132 -2.0928054 -1.0079026 -1.0637355 -4.1479244 -6.0500546 -7.2368279 -7.2065415][-4.4345241 -5.8648453 -5.8239808 -4.6708369 -2.9967294 -1.7644801 -1.8329422 -2.1140509 -1.8988385 -1.0756822 -0.95055723 -3.4765882 -5.6264634 -8.0946808 -8.4419937][-5.872879 -5.2344823 -3.7705541 -2.0845149 -0.60360527 0.96860409 1.6913619 0.25963116 -1.3840261 -1.0799732 -0.76543427 -3.3942351 -5.4582553 -7.5798659 -8.7803965][-4.243875 -4.2895751 -2.7959468 -0.74294043 0.96896696 2.8322549 3.5811791 2.9662881 2.3683648 1.3705263 0.21807909 -2.947566 -5.2413611 -8.0011034 -8.6575794][-4.6310506 -2.9658546 -1.3182116 0.58404016 2.0303121 2.7430692 2.936295 2.5682817 3.426558 3.6353192 2.349874 -1.4610157 -4.5472765 -6.8544693 -6.945096][-4.1206284 -2.7112849 -0.58795309 0.88966322 0.95985079 1.3092594 1.7607279 1.9068041 2.0764055 2.4915967 2.6905761 -1.3556767 -5.087142 -7.9247313 -6.9610271][-4.6836615 -3.6854658 -2.174619 -0.67562008 -0.037524223 0.16677284 0.48401213 0.30546761 0.71153259 1.2081332 0.72472334 -3.1938674 -7.1998405 -10.651052 -11.268768][-5.6547465 -5.6749811 -4.338388 -3.0777798 -2.1256738 -1.1894588 -0.62894773 -0.51954079 -0.96582603 -2.0728352 -4.1893873 -7.880743 -10.781081 -12.716619 -12.070656][-11.642697 -10.652935 -8.4679737 -6.6822777 -4.5999694 -3.8246665 -4.9519982 -6.20956 -6.9076996 -6.654346 -7.1113396 -9.3396645 -11.433277 -13.038906 -12.7901][-10.729396 -10.753769 -8.9647369 -7.4456816 -6.599494 -5.0989203 -4.306849 -5.2628231 -6.9802094 -7.4723086 -8.4679108 -9.3563843 -9.938427 -9.5868034 -7.4034705][-10.806264 -10.545464 -8.2687111 -6.5287561 -4.7820535 -3.9903898 -4.0285606 -4.06138 -4.3452334 -4.9779139 -5.1167502 -5.1389256 -5.5861983 -5.2483826 -4.165278][-6.1096458 -7.0581923 -6.0507183 -4.381135 -3.2511532 -2.1914306 -2.0065432 -2.4662044 -3.2308869 -3.9852567 -4.4542303 -5.5341077 -5.7526059 -4.22571 -3.4187779]]...]
INFO - root - 2017-12-16 00:07:37.657111: step 74910, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 47h:17m:34s remains)
INFO - root - 2017-12-16 00:07:44.256198: step 74920, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.664 sec/batch; 47h:31m:56s remains)
INFO - root - 2017-12-16 00:07:50.919020: step 74930, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 45h:48m:33s remains)
INFO - root - 2017-12-16 00:07:57.535899: step 74940, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 45h:53m:00s remains)
INFO - root - 2017-12-16 00:08:04.080592: step 74950, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 46h:35m:52s remains)
INFO - root - 2017-12-16 00:08:10.710883: step 74960, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 47h:47m:53s remains)
INFO - root - 2017-12-16 00:08:17.356490: step 74970, loss = 0.18, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 46h:28m:05s remains)
INFO - root - 2017-12-16 00:08:23.910795: step 74980, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 47h:19m:18s remains)
INFO - root - 2017-12-16 00:08:30.570104: step 74990, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 46h:40m:55s remains)
INFO - root - 2017-12-16 00:08:37.177228: step 75000, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.647 sec/batch; 46h:16m:46s remains)
2017-12-16 00:08:37.726161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3018761 -6.986722 -5.2201843 -4.1118765 -3.6698384 -4.3282347 -5.6039906 -6.5425425 -5.7882524 -5.3654652 -4.7448907 -6.4138851 -6.713079 -7.4512448 -6.1243114][-4.1025505 -4.948853 -4.9943213 -3.6043384 -3.5376234 -3.9770002 -5.1641874 -6.3309088 -7.1200795 -6.7065153 -5.9771104 -7.3822036 -8.3377457 -9.90448 -9.106432][-5.1397338 -4.6751089 -3.8474684 -3.1710124 -3.497344 -4.3228445 -5.9980044 -7.2298913 -7.497159 -6.7899742 -6.2332573 -7.4471936 -8.7488 -10.142637 -9.7001247][-6.4664145 -6.4651976 -5.8158207 -4.7281532 -4.4697227 -4.5717883 -4.997066 -6.1932926 -6.6069226 -5.9894629 -5.9182105 -7.1718473 -8.8228607 -9.950737 -8.9220686][-8.3013153 -7.4658995 -6.6446447 -4.4602356 -2.9413567 -2.5263321 -2.5322952 -2.7303085 -3.5517795 -3.4630716 -3.8451807 -5.9975123 -8.6627131 -10.409508 -10.230322][-7.0339851 -6.2726941 -5.758924 -3.9420986 -2.6727297 -0.93395472 0.21990538 0.26270676 0.10969448 -1.2607622 -2.9841433 -4.7432656 -7.8610229 -9.6062784 -9.9982166][-6.0192919 -5.11498 -4.1091886 -2.3769579 -1.0722594 0.94976234 2.0531316 1.8078551 2.3314309 1.0796876 -0.26997089 -3.376704 -6.7448521 -8.86269 -8.5068407][-4.7143908 -3.2933748 -2.3473883 -0.97701931 -0.30435514 1.266057 3.2610126 3.6276498 4.2283254 3.3656459 1.5369759 -2.7787502 -6.1951704 -9.2333832 -8.5437326][-3.4709237 -2.1075475 -2.3368206 -1.8765783 -0.53466558 0.67397976 1.7712555 3.4833589 4.1938415 3.6663604 3.0166926 -1.024004 -4.7260723 -7.1236687 -6.9602761][-2.9141936 -2.1727884 -2.2655296 -2.5616117 -2.6783915 -1.4033656 0.28356552 1.362134 2.0479279 2.0628633 2.0551314 0.078605652 -2.3085721 -5.3996887 -5.7363853][-5.9802346 -5.2929373 -5.8922658 -5.0732913 -5.099329 -4.4686742 -2.9626596 -2.5902224 -3.0596266 -2.8554447 -3.1072052 -5.0639467 -5.3910503 -6.1841335 -4.8288097][-8.7365379 -8.4856911 -7.93612 -8.320117 -8.1584816 -6.5921049 -5.7625365 -6.06745 -6.9099593 -7.8555183 -7.86127 -8.31616 -9.042943 -8.2661648 -6.3437963][-9.0063343 -8.4123707 -7.5061097 -7.4673886 -7.2116232 -6.065032 -5.6947193 -6.3273759 -6.8971138 -8.4620342 -8.9870825 -9.3418808 -8.1889458 -7.0995712 -5.4437437][-8.2453365 -7.7868905 -7.6838393 -7.3549428 -5.296422 -4.0355158 -3.7926545 -3.8802123 -5.1481647 -6.4430857 -6.8195624 -6.8846226 -6.1868873 -5.4539204 -5.2214246][-10.091167 -10.211372 -8.31513 -7.5365067 -6.1328378 -4.7525449 -3.5296819 -2.9232609 -3.6413207 -4.7621193 -5.108602 -6.7730474 -7.5936532 -7.2655344 -7.47538]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-75000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-75000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 00:08:45.666361: step 75010, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.653 sec/batch; 46h:42m:37s remains)
INFO - root - 2017-12-16 00:08:52.266203: step 75020, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 48h:09m:09s remains)
INFO - root - 2017-12-16 00:08:58.787703: step 75030, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 46h:47m:09s remains)
INFO - root - 2017-12-16 00:09:05.425785: step 75040, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 47h:24m:27s remains)
INFO - root - 2017-12-16 00:09:12.031299: step 75050, loss = 0.17, batch loss = 0.13 (11.4 examples/sec; 0.702 sec/batch; 50h:13m:22s remains)
INFO - root - 2017-12-16 00:09:18.654324: step 75060, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.668 sec/batch; 47h:46m:43s remains)
INFO - root - 2017-12-16 00:09:25.175023: step 75070, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.654 sec/batch; 46h:48m:04s remains)
INFO - root - 2017-12-16 00:09:31.791423: step 75080, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 48h:56m:42s remains)
INFO - root - 2017-12-16 00:09:38.421077: step 75090, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.678 sec/batch; 48h:28m:11s remains)
INFO - root - 2017-12-16 00:09:45.046891: step 75100, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 47h:43m:47s remains)
2017-12-16 00:09:45.615058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8862786 -6.5727606 -5.451263 -5.0727091 -6.023962 -6.8773646 -7.12899 -6.5884132 -5.923769 -6.0078945 -6.3066049 -8.9694805 -9.8499947 -10.02265 -9.8791008][-6.4767776 -6.7360172 -6.196156 -5.7984953 -6.1993918 -6.7770815 -7.3432794 -8.0743694 -8.1815643 -7.3295493 -6.9923325 -9.9471607 -10.84194 -11.620813 -11.175482][-4.190033 -4.5476556 -4.6191812 -4.5780373 -5.2299871 -6.3998394 -7.488111 -7.9849634 -7.8671083 -8.1302071 -8.3529892 -10.453596 -11.069244 -11.299599 -10.716917][-5.1404767 -4.8955607 -4.0744462 -3.6411307 -4.3638349 -4.7452488 -5.1641359 -6.4850249 -7.4279218 -7.0420303 -7.1825247 -10.509149 -11.627308 -11.895844 -11.471129][-6.7835011 -7.7625275 -6.9857426 -5.2676344 -4.3383656 -2.955272 -2.3466561 -3.7617335 -4.8703847 -5.4207659 -6.0259447 -8.5041809 -9.3310394 -10.586263 -11.233709][-10.2687 -9.3369379 -7.0539374 -4.5164995 -2.108695 0.17019653 1.309443 1.189136 0.53025723 -1.7216506 -3.9239192 -6.87643 -8.023036 -8.3720455 -8.637598][-12.20302 -11.010248 -7.881279 -2.9188676 1.6607437 3.8682723 5.0457959 5.5795569 5.2597537 2.0332594 -1.2345104 -5.5065989 -7.7204666 -8.0822973 -8.0359621][-11.271797 -10.438381 -7.6138716 -2.7126513 1.8945432 6.261416 8.3384037 7.6013236 7.2543006 4.3532472 0.71148586 -4.8358793 -8.6006222 -9.6832561 -9.3832531][-9.1325073 -8.9325533 -6.90059 -2.9941678 0.61630487 4.0620093 6.218864 5.7628512 4.9188066 2.0980282 -0.43940639 -6.2691669 -10.435999 -10.907791 -10.493695][-10.16589 -8.5074482 -6.0598397 -2.2371242 0.25134897 1.7721615 2.8608508 2.2405705 1.3283954 -0.52252388 -2.0354531 -7.6393938 -12.243151 -13.241642 -13.362453][-10.727936 -9.8348885 -8.1074686 -5.6780891 -3.5528097 -2.0710664 -1.3299885 -2.0713661 -2.846859 -3.991483 -5.2073727 -10.203423 -14.067024 -14.486565 -13.583488][-12.813965 -11.968437 -10.344316 -9.4790859 -9.0196247 -7.5328679 -6.6724052 -7.2896948 -7.7030783 -8.60479 -9.3883572 -11.655737 -13.08182 -13.588733 -13.535773][-15.154552 -13.992754 -12.455868 -11.703889 -11.339671 -10.88361 -10.822105 -10.588423 -10.481201 -10.50393 -10.628836 -11.773386 -11.55072 -10.170675 -8.62335][-13.65202 -12.733082 -11.386596 -10.16906 -8.9844351 -9.2733822 -10.00759 -9.50379 -9.3829851 -9.3711967 -9.2335987 -8.7401571 -7.332346 -6.6295152 -6.3719115][-8.86443 -8.226409 -6.5273333 -5.1411324 -4.79609 -4.76999 -4.8353353 -5.4678154 -6.2733736 -6.5601044 -6.2796383 -7.4346948 -8.058917 -6.7200804 -5.7106004]]...]
INFO - root - 2017-12-16 00:09:52.260957: step 75110, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 46h:49m:34s remains)
INFO - root - 2017-12-16 00:09:58.870799: step 75120, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 48h:44m:10s remains)
INFO - root - 2017-12-16 00:10:05.425683: step 75130, loss = 0.23, batch loss = 0.19 (11.8 examples/sec; 0.680 sec/batch; 48h:35m:12s remains)
INFO - root - 2017-12-16 00:10:12.039156: step 75140, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 46h:57m:14s remains)
INFO - root - 2017-12-16 00:10:18.592343: step 75150, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 47h:34m:47s remains)
INFO - root - 2017-12-16 00:10:25.166882: step 75160, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 47h:40m:13s remains)
INFO - root - 2017-12-16 00:10:31.709635: step 75170, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 46h:24m:23s remains)
INFO - root - 2017-12-16 00:10:38.427870: step 75180, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.681 sec/batch; 48h:40m:08s remains)
INFO - root - 2017-12-16 00:10:45.038050: step 75190, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 47h:31m:14s remains)
INFO - root - 2017-12-16 00:10:51.579438: step 75200, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.681 sec/batch; 48h:40m:33s remains)
2017-12-16 00:10:52.160872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.601892 -4.6776648 -6.2232842 -6.5954328 -7.2778935 -8.008173 -9.1989326 -9.9753456 -10.786022 -11.779599 -10.745604 -8.0009727 -7.5772181 -7.9102793 -7.9319944][-6.4988408 -6.5066967 -6.2744455 -7.1422715 -8.7414923 -9.9068375 -10.133905 -9.9942808 -9.8977013 -9.8523045 -9.4857836 -6.9192467 -6.0322671 -7.73333 -6.3796897][-5.4723568 -7.3698244 -9.2092066 -9.6440773 -9.7635021 -11.679987 -12.873685 -10.84858 -9.4859877 -9.8112564 -8.8446121 -5.7121797 -5.2393513 -6.4594331 -6.2203393][-6.759439 -8.5107517 -8.7817078 -9.18366 -10.40391 -9.6749973 -9.854681 -10.782444 -9.9457083 -9.1149025 -9.38578 -5.889195 -4.8414931 -6.8771267 -6.0066347][-7.0105815 -8.1855764 -9.9207735 -9.6717377 -9.007515 -6.5048676 -5.2496872 -5.7826242 -7.1639395 -7.3121343 -6.6750269 -5.5903897 -5.3072243 -6.9207487 -6.5350676][-8.719058 -8.5283995 -8.016284 -7.1202593 -6.5015421 -3.5832582 -0.14964008 0.87673569 -0.90435791 -3.5077059 -6.2430139 -4.3955426 -5.9583888 -8.0777779 -7.2336464][-7.446764 -8.0779295 -6.7273273 -5.2303481 -2.5665164 0.31892014 3.4594722 5.0272155 4.5209279 1.4901075 -1.0450025 -1.7124419 -3.7578969 -6.4804258 -6.7990222][-7.4880719 -6.4716349 -6.656548 -4.5255032 -1.1842666 3.4225707 5.8815761 5.8651204 6.4766145 3.8387218 0.42696857 -0.0096049309 -2.2752006 -4.7366381 -5.8195763][-8.2016258 -7.1355195 -6.4924974 -5.28337 -3.829711 0.68982935 5.4937158 5.0454354 1.9753222 0.43700218 0.22730017 -0.29192781 -2.0790906 -4.3431282 -3.9291403][-7.7926044 -7.6356091 -7.3787594 -5.7075682 -5.1047668 -2.5964017 1.2203536 2.337996 -0.11514521 -3.6802557 -5.7946119 -4.1832047 -5.7547312 -6.7995243 -5.486249][-11.414404 -10.256979 -9.2094631 -7.6861134 -5.7023106 -3.662158 -3.1210561 -3.1228592 -4.5231214 -7.8323946 -9.8466148 -8.5783644 -8.2277546 -8.8996124 -7.502367][-12.405125 -10.690201 -8.0236931 -6.3908057 -4.9316525 -3.5398669 -2.6279116 -4.6277046 -7.948823 -10.042075 -10.494282 -10.113279 -9.3444376 -9.1631289 -6.5839014][-13.16043 -10.312399 -6.4954853 -4.0430741 -2.6029558 -1.9339151 -2.3467877 -3.3806863 -6.47786 -8.9263115 -9.1525164 -7.16243 -6.4451585 -6.91407 -5.6631608][-10.17417 -7.3779073 -4.6147738 -2.1891356 -1.0402451 -1.3027925 -1.5096865 -2.3660028 -4.473711 -6.57246 -6.7433 -4.0831308 -3.6821127 -3.4401736 -3.8509655][-8.0898628 -6.5773935 -3.5163968 -1.8762274 -1.9994557 -2.3842258 -2.753057 -3.5140188 -4.0700192 -4.9080319 -4.7764478 -5.5644197 -4.1929359 -2.9247868 -4.3489141]]...]
INFO - root - 2017-12-16 00:10:58.721417: step 75210, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 45h:57m:53s remains)
INFO - root - 2017-12-16 00:11:05.266624: step 75220, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 46h:20m:42s remains)
INFO - root - 2017-12-16 00:11:11.786834: step 75230, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 47h:02m:52s remains)
INFO - root - 2017-12-16 00:11:18.405975: step 75240, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 47h:10m:56s remains)
INFO - root - 2017-12-16 00:11:25.013239: step 75250, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 46h:41m:53s remains)
INFO - root - 2017-12-16 00:11:31.581489: step 75260, loss = 0.19, batch loss = 0.14 (12.6 examples/sec; 0.637 sec/batch; 45h:31m:58s remains)
INFO - root - 2017-12-16 00:11:38.140297: step 75270, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 46h:45m:04s remains)
INFO - root - 2017-12-16 00:11:44.767200: step 75280, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 46h:53m:36s remains)
INFO - root - 2017-12-16 00:11:51.383117: step 75290, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 46h:33m:11s remains)
INFO - root - 2017-12-16 00:11:58.001259: step 75300, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 47h:09m:00s remains)
2017-12-16 00:11:58.516430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3978515 -5.9343276 -5.262074 -3.5232089 -3.5578265 -4.4597969 -4.8374791 -4.0247812 -3.3617535 -3.3611367 -3.0354822 -3.1596019 -2.5993075 -4.5324755 -5.2697163][-6.1116295 -5.8593783 -5.1088433 -3.6343882 -3.5744295 -4.48388 -3.9471674 -2.8273807 -1.9814794 -1.4988618 -0.69392824 -0.25949478 0.1719594 -3.0799115 -4.3999104][-5.61651 -5.7055039 -5.28853 -4.4464035 -4.7713637 -5.1189137 -4.7204485 -2.6946812 -1.139473 -1.4522176 -1.2688775 -0.61945391 0.23721313 -2.8775394 -3.7622569][-8.2712069 -7.9993706 -6.7939062 -5.6465883 -5.9992552 -5.6506443 -4.6936545 -2.642118 -0.96253633 -1.5614867 -2.271287 -1.8054821 -1.2514009 -3.4888523 -3.6202445][-7.8843441 -8.7410622 -8.6017218 -7.0188351 -5.8334265 -3.2825346 -1.6764507 -0.90426254 -0.12981701 -0.9290185 -2.2861629 -2.641216 -2.5317173 -5.2694683 -5.1111345][-7.913682 -8.2899017 -7.2651134 -5.9967961 -4.10558 -0.47154331 3.0199342 3.7020755 2.9625888 0.1254096 -2.6584578 -3.1568642 -2.891597 -5.7088604 -5.8409085][-7.7775993 -8.2633858 -6.1828036 -3.645453 -1.8504004 1.9087291 6.473412 7.539639 6.7139735 2.8912883 -0.59047127 -2.7218733 -3.2634437 -4.988955 -5.65029][-8.7331514 -8.2806568 -7.2056265 -3.9881887 -0.69630432 3.5785661 7.931457 8.2271042 7.8020988 4.6982846 0.066057205 -2.8959947 -2.707303 -5.111515 -6.1126161][-9.1418371 -8.8895226 -8.0713673 -5.5693817 -3.2441986 1.4353089 4.6484542 4.3975797 3.3546929 1.0994363 -1.2623348 -4.2934709 -4.25443 -5.7553482 -5.8280396][-8.9228792 -8.045969 -7.6984463 -5.5659904 -5.002368 -1.7169638 2.3184171 2.4348245 0.60618114 -2.7961857 -5.7593689 -6.6008964 -6.1786976 -7.9105859 -8.28925][-11.496222 -10.594744 -9.3889418 -7.8444691 -7.3935623 -6.17679 -5.0719018 -4.2652278 -4.4329319 -5.8173165 -8.4239626 -10.017452 -9.078578 -9.2702055 -8.9627924][-13.693551 -12.893581 -10.909554 -9.1777382 -8.7083025 -7.6985793 -7.6665616 -7.8505068 -7.2001486 -7.5956244 -8.5639153 -9.5540485 -9.4614182 -10.006817 -9.1503668][-11.093741 -10.84713 -9.3256922 -7.87243 -7.4994497 -5.9668794 -5.4620357 -6.109014 -6.146452 -5.390429 -5.1183844 -6.19479 -5.9626603 -7.0833035 -7.5495987][-11.574868 -10.600604 -9.74136 -8.9797087 -8.0064945 -6.8967047 -6.547596 -5.4219217 -4.6871037 -5.1899371 -5.3931637 -4.5101948 -3.7667389 -4.5967131 -4.9382534][-7.927103 -7.3884211 -6.5092759 -5.4485 -5.7723465 -5.6481752 -4.2671432 -2.6555705 -2.8016858 -3.2702014 -3.8354259 -4.69775 -4.8391604 -5.5477228 -6.5760894]]...]
INFO - root - 2017-12-16 00:12:05.057377: step 75310, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.673 sec/batch; 48h:03m:56s remains)
INFO - root - 2017-12-16 00:12:11.665919: step 75320, loss = 0.21, batch loss = 0.17 (11.9 examples/sec; 0.675 sec/batch; 48h:13m:26s remains)
INFO - root - 2017-12-16 00:12:18.231315: step 75330, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 47h:33m:30s remains)
INFO - root - 2017-12-16 00:12:24.850208: step 75340, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 46h:38m:45s remains)
INFO - root - 2017-12-16 00:12:31.491032: step 75350, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.683 sec/batch; 48h:47m:00s remains)
INFO - root - 2017-12-16 00:12:38.125697: step 75360, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.653 sec/batch; 46h:39m:08s remains)
INFO - root - 2017-12-16 00:12:44.699821: step 75370, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 48h:18m:28s remains)
INFO - root - 2017-12-16 00:12:51.321089: step 75380, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 48h:33m:25s remains)
INFO - root - 2017-12-16 00:12:57.894626: step 75390, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.681 sec/batch; 48h:36m:46s remains)
INFO - root - 2017-12-16 00:13:04.477851: step 75400, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 48h:06m:05s remains)
2017-12-16 00:13:05.069042: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9868944 -2.78709 -2.4043565 -3.1459866 -4.1232018 -4.4093423 -4.8627634 -4.7613277 -3.7161589 -3.8872657 -4.9643831 -5.4374304 -6.2943354 -7.6477356 -5.4216323][-5.7385468 -5.6219268 -5.364325 -4.5016069 -4.4547014 -5.272748 -5.3258729 -4.3594236 -4.1317892 -3.8077252 -3.1283007 -5.4876003 -6.8205795 -6.9555411 -6.033143][-5.5913053 -5.265089 -5.9828582 -6.7034073 -6.8135252 -5.8420324 -5.085103 -4.9371452 -3.9425282 -3.4747126 -4.3895435 -5.5586534 -5.6305695 -6.7857342 -4.9947758][-4.2725668 -4.0490885 -4.355444 -4.6893063 -5.4747882 -5.8414316 -5.6418438 -5.3391743 -4.6536064 -3.5027232 -2.67024 -4.3420753 -5.0414138 -4.9608622 -3.5672655][-7.4914827 -6.3816524 -5.0485473 -3.8445148 -3.1507947 -2.5570202 -2.3063066 -3.044121 -2.5722816 -2.6968036 -3.2077012 -3.5323267 -4.1282921 -4.7658396 -2.5316508][-6.8446755 -5.7553906 -4.4740095 -2.0405345 -0.43189526 1.3526897 2.6580548 1.736773 0.48490858 -1.7104912 -2.3786883 -2.8743086 -2.7536283 -2.8713744 -1.6813312][-7.8873796 -6.9503374 -5.1983871 -2.2303896 0.12057257 2.9150081 5.0184817 5.4936109 5.566134 2.1273971 -0.45628309 -1.4667301 -2.2538812 -2.4619758 -0.8395896][-8.1645174 -7.1655636 -4.9190526 -1.5047083 1.1773686 3.0956111 3.9800839 4.5064597 4.4514384 3.2475667 1.8847475 -1.4049382 -3.8714573 -3.8132677 -1.9141243][-5.6536336 -5.0748863 -3.6494131 -2.001889 -0.37651205 2.4035621 2.9176946 2.7935233 3.7130094 2.4343791 1.9683757 -0.96325731 -4.6271834 -6.4553957 -5.0740137][-5.6812105 -4.7035475 -3.4345045 -1.1493287 0.087181091 1.0090756 1.6537962 1.2910776 0.2592454 -0.63726234 -0.9463954 -3.0654318 -4.8242803 -6.8956203 -7.2115993][-6.1938615 -5.6120648 -5.1500106 -3.8605747 -2.5513093 -1.8965504 -2.2303078 -1.7910063 -1.6068063 -2.8510046 -3.6090064 -5.7065043 -7.5625024 -8.9010744 -7.8526888][-8.7022953 -8.2432594 -7.444952 -7.1785269 -7.6877074 -7.5737805 -7.3958235 -7.574194 -7.9077044 -7.295887 -6.846179 -8.1846752 -8.5471954 -9.8027105 -9.7297039][-11.629072 -10.641065 -10.008377 -9.9021358 -10.749537 -10.549011 -10.781553 -10.656067 -10.238674 -10.158543 -10.026518 -10.028175 -9.9391422 -9.961853 -9.0008144][-10.555429 -10.234968 -9.3566628 -9.0169392 -9.4937305 -10.037421 -10.435121 -10.222919 -10.447944 -9.9302168 -9.0762711 -8.8018389 -8.1087885 -8.3252792 -8.3439493][-8.3068018 -7.9720507 -7.5337157 -6.4576788 -6.1958866 -6.3087788 -6.704989 -7.0211792 -6.8928151 -7.0257893 -7.4177294 -7.8496294 -8.3231144 -7.6000614 -6.4725876]]...]
INFO - root - 2017-12-16 00:13:11.611337: step 75410, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 47h:13m:00s remains)
INFO - root - 2017-12-16 00:13:18.153935: step 75420, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 47h:01m:07s remains)
INFO - root - 2017-12-16 00:13:24.702046: step 75430, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 46h:16m:52s remains)
INFO - root - 2017-12-16 00:13:31.259674: step 75440, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 46h:27m:34s remains)
INFO - root - 2017-12-16 00:13:37.908930: step 75450, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 46h:47m:08s remains)
INFO - root - 2017-12-16 00:13:44.435158: step 75460, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 45h:56m:16s remains)
INFO - root - 2017-12-16 00:13:51.038733: step 75470, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 47h:57m:28s remains)
INFO - root - 2017-12-16 00:13:57.567624: step 75480, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 45h:28m:45s remains)
INFO - root - 2017-12-16 00:14:04.164411: step 75490, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 46h:11m:20s remains)
INFO - root - 2017-12-16 00:14:10.774064: step 75500, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 46h:07m:00s remains)
2017-12-16 00:14:11.321399: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5936894 -6.7443409 -5.8396192 -4.3575583 -3.9566627 -4.093636 -4.2245932 -4.477334 -4.6136203 -4.6596475 -4.528151 -5.7370291 -7.9824562 -8.8656206 -9.234786][-4.3776274 -4.6598468 -4.8942461 -4.2360406 -4.18055 -4.2022319 -3.9574065 -3.6813078 -3.6131077 -3.8031492 -3.5778749 -4.5786433 -7.876987 -9.0120974 -8.8477917][-2.4716027 -2.5407526 -2.7676105 -2.97194 -3.6182096 -4.0679178 -4.0834608 -4.0107861 -4.0843749 -4.0002966 -3.5642586 -4.8533816 -7.4520721 -8.1608362 -8.2729359][-1.8315835 -1.7161231 -1.9978366 -2.2341883 -3.102668 -3.3651109 -3.5972207 -3.7934504 -3.6629052 -3.6387508 -3.70643 -5.6261024 -8.2690277 -9.0544252 -8.9907713][-1.5798788 -1.8758364 -2.6749501 -2.2675431 -2.9022994 -2.6368997 -2.8318245 -2.5694022 -2.5462151 -2.5735161 -2.8333509 -4.8032184 -7.8610373 -9.0639553 -9.2110834][-4.4284983 -3.8044357 -2.9103484 -1.5563283 -0.94554329 0.018787384 0.63563681 0.65645075 0.082116604 -0.63073206 -1.3067918 -3.7112474 -7.3801255 -8.4995766 -9.260293][-5.2341719 -4.7421131 -3.8805618 -1.9076457 -0.32539988 0.95153 1.8448491 1.6419683 1.0221462 0.24493599 -0.44967079 -3.563158 -7.259304 -8.3943434 -8.9699039][-4.7887397 -4.8269329 -4.0210209 -2.3762093 -1.550869 0.28122616 1.2725339 1.3832407 1.246973 0.51632738 -0.5041647 -3.4865911 -6.9419465 -8.8922491 -9.3091459][-4.2312088 -3.823298 -2.6521478 -1.1956801 -0.98130035 -0.45821238 -0.47463369 -0.0052595139 0.49827671 0.68361092 0.25242996 -2.6495631 -6.4530029 -7.6702647 -7.8714457][-3.7398129 -3.3929765 -2.5870857 -0.930449 -0.35663795 0.034579277 -0.62786245 -0.70080662 -0.96282959 -0.86543512 -1.1305771 -3.461051 -6.3572125 -7.3824935 -7.7732582][-6.7541742 -5.1854873 -3.617281 -2.4002619 -2.6289814 -3.0472147 -3.9037156 -4.2839642 -4.8464961 -4.9345403 -5.3445215 -6.8465319 -8.4201393 -8.8935261 -7.9272718][-9.3092012 -7.8351026 -6.5600882 -5.1275063 -5.3335533 -6.20879 -7.2742333 -8.0982141 -8.2496824 -8.8982716 -9.245738 -9.7163 -9.9576693 -9.4901886 -8.5190325][-10.718061 -8.4039221 -6.7896895 -6.8030119 -7.0018377 -7.17685 -7.9895849 -8.7032928 -8.4584694 -9.452096 -9.9063625 -10.356344 -10.932915 -10.215319 -8.9870262][-9.6980724 -8.7259359 -8.0521755 -6.9249249 -6.2658725 -6.7823443 -7.3502626 -6.8667603 -7.1895256 -8.33752 -9.0453033 -9.0085144 -8.9853525 -9.0538483 -8.818861][-9.1295948 -9.1577215 -8.4264374 -7.0481548 -6.5523329 -5.7677484 -4.768743 -4.6407967 -5.3890052 -5.34341 -5.7946415 -7.185235 -8.4385538 -8.8163 -9.3508015]]...]
INFO - root - 2017-12-16 00:14:17.848060: step 75510, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.636 sec/batch; 45h:24m:30s remains)
INFO - root - 2017-12-16 00:14:24.459154: step 75520, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 46h:19m:53s remains)
INFO - root - 2017-12-16 00:14:31.039995: step 75530, loss = 0.11, batch loss = 0.07 (12.3 examples/sec; 0.648 sec/batch; 46h:16m:00s remains)
INFO - root - 2017-12-16 00:14:37.636649: step 75540, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 46h:23m:35s remains)
INFO - root - 2017-12-16 00:14:44.191237: step 75550, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 47h:27m:27s remains)
INFO - root - 2017-12-16 00:14:50.682639: step 75560, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 45h:33m:41s remains)
INFO - root - 2017-12-16 00:14:57.259127: step 75570, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 46h:25m:56s remains)
INFO - root - 2017-12-16 00:15:03.918106: step 75580, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 47h:53m:36s remains)
INFO - root - 2017-12-16 00:15:10.487130: step 75590, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 46h:17m:41s remains)
INFO - root - 2017-12-16 00:15:17.060154: step 75600, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 46h:39m:46s remains)
2017-12-16 00:15:17.580655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7490442 -3.9689028 -3.9919436 -3.1268697 -2.1623054 -2.1797521 -0.89609718 0.8427124 1.4830012 0.030985832 -1.7331862 -6.3146806 -9.3394508 -13.877028 -14.341221][-2.8927674 -3.1763673 -3.4737723 -3.8753319 -3.5712483 -2.3954287 -0.34094095 0.15121794 0.80470037 0.15406847 -1.6929598 -6.7033076 -9.8921223 -14.732384 -16.950306][-2.7179573 -3.4181564 -3.4950941 -3.2874348 -3.4803164 -3.3651047 -1.6631975 0.45387459 0.20477676 -0.81661654 -1.7819431 -7.1830587 -10.385286 -13.964968 -15.936026][-1.6122417 -1.9543734 -2.1797931 -1.7446561 -1.2581601 -1.0307655 -1.2021012 -0.12258053 0.79127645 -0.39942837 -2.2108786 -6.828826 -9.67454 -14.68433 -15.794683][-1.2821312 -0.92756128 -0.83212805 -1.5339179 -1.3460369 -0.1273551 0.52326775 0.70899439 0.53371906 0.51292372 -2.0338149 -7.6704111 -10.138046 -14.561628 -16.248316][-1.3458252 -2.9039733 -2.061486 -1.3247437 -0.2120409 0.53685 2.0280647 2.6944518 1.6959968 0.53520679 -1.5353017 -6.86767 -11.113985 -14.44681 -15.533726][-2.4826872 -3.750551 -3.6877992 -2.047102 0.023458004 2.1462159 3.511023 3.8276868 4.2601562 2.0508413 -0.90220404 -5.5995159 -8.8080482 -13.773664 -15.476116][-3.7107122 -3.8054304 -3.2578661 -2.1141756 -0.29484653 2.2838979 4.3788781 4.5235 4.7691875 3.7663388 1.0065198 -4.5396762 -7.7386928 -12.543459 -14.341335][-3.8331532 -5.9213715 -4.7963214 -2.771193 -1.5592971 0.578774 3.2306323 4.4508815 4.0614076 2.9112916 0.7419095 -4.1815338 -7.9873853 -12.085198 -13.551817][-3.2412364 -4.6596375 -5.4669237 -4.1073895 -2.6385472 -1.3057771 0.32936049 1.8796763 2.2264781 0.20494556 -1.2209725 -5.0979705 -8.2150936 -11.936398 -13.363037][-5.1679382 -6.36714 -6.7285523 -6.1462359 -5.9062548 -4.625701 -3.3596003 -3.0753322 -2.3206108 -3.022402 -4.533987 -9.4018536 -10.150976 -12.816656 -12.658559][-7.2292109 -7.6692066 -7.6149364 -7.1866488 -7.2072191 -7.2771893 -6.6336651 -6.5783467 -6.7294216 -6.8245568 -8.3279037 -10.464881 -11.303954 -12.712839 -13.184679][-9.1732521 -9.2361641 -8.38127 -8.1730051 -8.3312006 -8.6620207 -8.9238014 -8.96623 -8.4716444 -9.2300911 -9.68944 -11.22445 -12.231234 -12.183915 -11.988591][-8.5459471 -8.5444574 -8.400671 -7.6769252 -7.6303349 -8.4361048 -9.1849041 -9.3221731 -8.9451017 -9.66013 -8.984477 -9.2088718 -8.96928 -10.278754 -10.789771][-9.4788189 -8.9564552 -8.6903782 -8.5816822 -8.6011038 -8.7287922 -9.6141329 -10.441053 -9.66695 -8.71514 -8.9130316 -9.8846226 -10.501959 -11.284758 -11.351871]]...]
INFO - root - 2017-12-16 00:15:24.131521: step 75610, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 47h:15m:43s remains)
INFO - root - 2017-12-16 00:15:30.667176: step 75620, loss = 0.32, batch loss = 0.27 (12.5 examples/sec; 0.638 sec/batch; 45h:32m:04s remains)
INFO - root - 2017-12-16 00:15:37.309463: step 75630, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.691 sec/batch; 49h:17m:42s remains)
INFO - root - 2017-12-16 00:15:43.880862: step 75640, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 46h:15m:48s remains)
INFO - root - 2017-12-16 00:15:50.460825: step 75650, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 48h:06m:43s remains)
INFO - root - 2017-12-16 00:15:57.066683: step 75660, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 45h:59m:23s remains)
INFO - root - 2017-12-16 00:16:03.642913: step 75670, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 47h:44m:01s remains)
INFO - root - 2017-12-16 00:16:10.289376: step 75680, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 48h:39m:59s remains)
INFO - root - 2017-12-16 00:16:16.902954: step 75690, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 46h:48m:29s remains)
INFO - root - 2017-12-16 00:16:23.522780: step 75700, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.641 sec/batch; 45h:41m:52s remains)
2017-12-16 00:16:24.087041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1088338 -8.6549673 -9.355463 -9.3185129 -9.7220745 -9.8484039 -10.401091 -10.459452 -10.110456 -9.5891094 -8.0907021 -9.07549 -9.8979053 -10.552164 -9.8581839][-6.3545027 -7.4364967 -7.618206 -7.8074846 -7.8656392 -8.1331921 -9.2433681 -10.081425 -10.389862 -9.5826874 -7.9887686 -9.5017776 -10.156567 -11.115754 -10.949871][-3.433567 -6.0068741 -8.06917 -7.1900015 -6.5899677 -6.9227571 -7.2576346 -7.8009386 -8.3782959 -8.3030281 -7.5733533 -8.7259884 -9.05546 -10.261 -10.415265][-5.370163 -5.9842215 -6.9977331 -7.3046355 -7.2038345 -6.3943281 -5.3141766 -6.516243 -8.0458212 -7.6691494 -6.9393973 -9.0009041 -9.9472485 -10.055699 -9.353611][-6.536684 -8.6185246 -9.6865177 -7.0035343 -5.2636461 -2.3414338 -0.35856915 -4.0229278 -7.7239628 -7.7431531 -7.0337505 -8.6220837 -9.4007883 -10.915371 -11.022453][-9.6799107 -10.515984 -9.9913445 -6.2707539 -2.8143604 2.2197261 6.2709765 3.66567 -0.17459679 -4.3358936 -7.6686916 -8.82798 -8.6133509 -9.8798923 -10.542362][-10.167824 -11.209604 -10.470935 -6.3908339 -3.0866678 3.1979861 8.58654 7.69365 6.6097512 0.35669756 -6.5320625 -8.7370329 -9.6138172 -10.395262 -9.2621861][-9.9264717 -9.3105116 -9.1370907 -5.8032589 -2.0059869 4.1105361 8.2721367 7.3919806 7.7207723 2.2206106 -3.8242564 -7.4774466 -10.140786 -9.9208374 -9.0207806][-7.6683784 -7.7142391 -7.5368495 -4.3777361 -1.6262507 2.1584272 4.7804742 5.2802634 5.3697219 0.15679312 -3.0119534 -7.5889969 -12.53156 -12.108317 -10.33652][-6.9834538 -7.280777 -7.0031695 -4.7562947 -3.9904637 -1.3660145 1.8711734 2.727962 1.4677544 -2.9343369 -5.5853791 -9.7490034 -12.229992 -12.746559 -12.903118][-11.538031 -11.55116 -11.975119 -10.419262 -9.39402 -7.9904852 -6.2735062 -5.0620685 -5.0092883 -6.7846112 -8.5270119 -12.678112 -14.208496 -13.345909 -11.861206][-17.399815 -16.830812 -16.305435 -14.760603 -13.630723 -12.638629 -12.03196 -11.5173 -10.873005 -11.687147 -13.060028 -13.540407 -13.009628 -12.918634 -11.476028][-18.478296 -17.595673 -16.819815 -14.553768 -13.545527 -12.739386 -12.069208 -12.163775 -12.547634 -12.274364 -12.282051 -12.987593 -12.364073 -10.646234 -9.0270176][-16.330715 -15.331358 -14.518112 -11.965576 -10.108189 -9.8663559 -9.8890858 -9.7005539 -9.7301025 -10.582031 -11.514994 -10.711132 -9.9962711 -9.5266237 -8.8977432][-12.545994 -11.81377 -9.514801 -6.8590064 -5.7032361 -5.5801005 -5.9052505 -6.6233845 -7.0307226 -6.9716253 -7.77149 -8.9193068 -9.7736788 -9.16425 -8.3760557]]...]
INFO - root - 2017-12-16 00:16:30.674811: step 75710, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 48h:20m:15s remains)
INFO - root - 2017-12-16 00:16:37.326007: step 75720, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 47h:06m:50s remains)
INFO - root - 2017-12-16 00:16:43.979819: step 75730, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 46h:38m:59s remains)
INFO - root - 2017-12-16 00:16:50.463007: step 75740, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.633 sec/batch; 45h:08m:19s remains)
INFO - root - 2017-12-16 00:16:57.065580: step 75750, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 46h:59m:22s remains)
INFO - root - 2017-12-16 00:17:03.636082: step 75760, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 46h:57m:56s remains)
INFO - root - 2017-12-16 00:17:10.349967: step 75770, loss = 0.16, batch loss = 0.12 (11.4 examples/sec; 0.703 sec/batch; 50h:07m:45s remains)
INFO - root - 2017-12-16 00:17:17.024239: step 75780, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 46h:26m:51s remains)
INFO - root - 2017-12-16 00:17:23.637459: step 75790, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 47h:19m:07s remains)
INFO - root - 2017-12-16 00:17:30.269643: step 75800, loss = 0.30, batch loss = 0.26 (12.2 examples/sec; 0.655 sec/batch; 46h:41m:55s remains)
2017-12-16 00:17:30.791971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.481566 -10.078958 -7.9314847 -5.9390421 -5.7533278 -5.2731462 -4.41689 -2.9890869 -2.0011511 -1.2675114 -0.56168318 -3.103225 -4.7626276 -5.8472486 -6.0402522][-8.4109173 -8.5616665 -7.7934475 -5.9847164 -5.6776152 -5.753963 -4.895112 -4.41194 -3.8351507 -2.5671363 -1.9228883 -3.8107667 -5.2928543 -6.1487103 -6.2177472][-5.6760583 -6.8424344 -6.9371119 -5.2570338 -5.7487288 -6.2478347 -5.9304595 -5.4381847 -4.1674576 -3.1979554 -3.2615118 -6.0789051 -7.7601833 -8.28028 -8.4009418][-5.6882644 -6.5934548 -6.4340062 -5.7167945 -7.0274258 -6.3487372 -5.6735816 -6.0368743 -5.7365284 -4.3786497 -3.8340282 -6.5533915 -8.7672758 -9.8346891 -10.400898][-6.0156059 -7.4518609 -7.8031106 -6.9486752 -6.4922409 -4.7577014 -4.4735422 -5.012135 -4.3413944 -2.9574046 -3.2945251 -6.2219119 -8.3737259 -10.08783 -10.98918][-8.2750931 -7.9303007 -7.5417461 -5.3574996 -4.1530962 -2.6381721 -0.98989868 -0.11820269 0.16042185 -1.2382612 -2.2942209 -4.0389376 -6.8896513 -10.241986 -11.910848][-9.3617125 -7.7054434 -6.5944948 -2.9173436 -0.82607174 0.7718811 2.2368608 3.7818255 4.4944425 2.3718762 0.26479912 -3.3566976 -7.0007195 -9.0523739 -10.066069][-6.8170481 -5.2779088 -4.496501 -0.26659107 0.98496914 2.5855622 3.9506974 5.2630858 6.2484078 4.6708159 2.5417943 -2.8535681 -7.6941981 -9.7764626 -9.7801142][-4.4180908 -3.17202 -1.837364 1.2140875 1.7410641 2.7840304 3.4855542 3.6041274 3.4498506 4.1909604 3.8481116 -2.3240035 -7.0367217 -9.5877571 -9.0841026][-3.4082015 -2.8599241 -1.645082 0.76720905 1.117023 1.1696 2.0604138 2.6839747 2.8898473 2.8698 2.0096188 -2.2590761 -5.5844188 -8.3785362 -8.4625225][-8.2973633 -6.2691188 -4.1774421 -3.6325965 -2.6780488 -2.8896608 -1.9264045 -1.7520413 -2.0026751 -1.5610662 -1.6318874 -5.3948607 -7.2441483 -8.190937 -6.9610744][-13.104986 -11.342194 -10.192616 -7.6800675 -6.0981593 -6.8018646 -6.593729 -6.0925303 -5.8382778 -6.1480789 -6.188911 -7.5536942 -8.0281467 -6.4563246 -5.3550334][-14.046434 -13.075105 -11.015125 -9.5996 -8.9185467 -8.9880381 -7.9636478 -8.1165285 -8.4278069 -8.50069 -7.5595431 -8.5088272 -8.4667025 -5.8658423 -4.0564361][-11.110823 -10.640997 -9.8206854 -7.8136506 -7.1432714 -7.7819405 -8.2632713 -8.3783941 -7.8181686 -8.53326 -8.7483082 -8.94434 -8.7852821 -7.3804731 -5.683321][-8.41252 -8.3751183 -7.0214524 -6.1133819 -6.1788435 -4.9843607 -5.5517292 -5.7772994 -6.36642 -7.110157 -7.82782 -9.8162813 -11.223375 -10.34026 -8.8557186]]...]
INFO - root - 2017-12-16 00:17:37.405050: step 75810, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 47h:13m:09s remains)
INFO - root - 2017-12-16 00:17:44.004075: step 75820, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 47h:34m:01s remains)
INFO - root - 2017-12-16 00:17:50.679796: step 75830, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 47h:53m:54s remains)
INFO - root - 2017-12-16 00:17:57.306629: step 75840, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 48h:11m:50s remains)
INFO - root - 2017-12-16 00:18:03.987580: step 75850, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.693 sec/batch; 49h:23m:28s remains)
INFO - root - 2017-12-16 00:18:10.613788: step 75860, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 47h:12m:12s remains)
INFO - root - 2017-12-16 00:18:17.199379: step 75870, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 47h:23m:24s remains)
INFO - root - 2017-12-16 00:18:23.842464: step 75880, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 46h:27m:33s remains)
INFO - root - 2017-12-16 00:18:30.447248: step 75890, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.659 sec/batch; 46h:57m:02s remains)
INFO - root - 2017-12-16 00:18:37.013814: step 75900, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 46h:10m:34s remains)
2017-12-16 00:18:37.596997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5451617 -5.9940581 -5.8133588 -5.2203217 -6.470943 -6.852088 -7.2120686 -6.0509462 -4.4605083 -3.1808739 -2.4223607 -5.062891 -7.3772035 -6.3735905 -4.9259353][-5.282186 -5.3134661 -5.0938549 -4.3738551 -4.9729643 -5.2788558 -5.2868853 -5.4424415 -4.4306479 -3.1647351 -2.4417708 -4.8555775 -6.8793674 -7.0078135 -6.5153828][-3.3625424 -4.0148029 -3.697547 -2.3180873 -1.774781 -2.3415282 -3.1376317 -3.153018 -2.265033 -1.8195417 -1.76931 -4.9030414 -7.5918951 -7.6938534 -6.9172921][-5.593864 -4.9345856 -3.8674817 -2.8840697 -2.0599525 -1.640131 -1.7181525 -2.3591082 -2.1845775 -1.7697031 -1.4864912 -4.1806593 -6.7845006 -7.5883875 -7.4923625][-5.17013 -5.9693971 -5.8962369 -3.2369692 -1.0139923 0.10584736 0.0010452271 -0.91562986 -1.7090354 -1.5567079 -1.1837497 -4.3678131 -7.4677124 -7.8142056 -7.3198791][-6.2008533 -5.7659345 -3.8836782 -1.2678866 0.65904903 3.4745231 4.1676011 2.9012427 1.3599849 -0.65475178 -1.8262858 -4.4484363 -6.4453521 -6.8915205 -6.3370795][-8.71692 -7.3117671 -4.5380974 -1.8296726 0.34894466 3.2006478 5.35323 4.9075809 3.3034253 1.0521612 -1.3037043 -5.0148811 -7.1253166 -7.3868494 -6.2233543][-8.792697 -6.6009712 -4.0211525 -1.6351366 -0.55136108 1.3821001 2.8421321 3.6520448 4.1022105 2.1572204 -0.31764126 -5.0240026 -8.4430037 -8.47139 -6.8054338][-7.5298958 -6.1616454 -3.995717 -1.766892 -0.5457592 0.083479881 0.53594685 1.2846284 1.9239454 0.74768972 -0.59252596 -4.9059391 -8.9008284 -9.4599752 -8.5116873][-5.832654 -5.2284603 -4.1099758 -1.6246686 -0.33285761 -0.18654966 -0.19361925 0.24109077 -0.33582592 -0.89830351 -1.3274069 -5.5150042 -9.1128769 -10.10415 -9.6345119][-8.7768755 -8.2222023 -6.989449 -4.8615532 -4.2427664 -4.377861 -3.9648819 -4.4445524 -5.0919619 -5.1187205 -5.658545 -8.0110159 -9.7533026 -10.093332 -10.501436][-12.972055 -11.813299 -9.9427166 -9.0767269 -9.4954185 -8.9045982 -8.2475367 -9.1770315 -9.4965076 -9.105649 -9.2507706 -10.340883 -10.914296 -10.335672 -10.221453][-13.01243 -12.150512 -10.795698 -9.0632038 -8.3647747 -9.0232487 -10.046371 -9.754653 -9.0879068 -9.3127022 -9.5917587 -9.7379665 -9.3195534 -7.7138219 -6.2614985][-12.555688 -11.450105 -9.9794254 -8.0201464 -7.3908081 -8.2121563 -8.2380724 -7.4637208 -8.0587883 -8.4006748 -8.3009348 -9.1839247 -8.97478 -7.3889256 -6.2912393][-9.7630749 -9.953825 -9.6493959 -6.8419867 -5.499269 -5.2247915 -4.97602 -5.708365 -5.4486475 -4.9657421 -5.2997551 -6.99766 -8.2451172 -8.3239965 -6.9928274]]...]
INFO - root - 2017-12-16 00:18:44.199497: step 75910, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 46h:58m:19s remains)
INFO - root - 2017-12-16 00:18:50.793615: step 75920, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 46h:21m:15s remains)
INFO - root - 2017-12-16 00:18:57.433287: step 75930, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.636 sec/batch; 45h:18m:57s remains)
2017-12-16 00:19:00.705711: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 35159634 get requests, put_count=35159640 evicted_count=9000 eviction_rate=0.000255975 and unsatisfied allocation rate=0.000256118
INFO - root - 2017-12-16 00:19:04.012040: step 75940, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 48h:17m:40s remains)
INFO - root - 2017-12-16 00:19:10.754353: step 75950, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 45h:53m:00s remains)
INFO - root - 2017-12-16 00:19:17.311198: step 75960, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 46h:55m:41s remains)
INFO - root - 2017-12-16 00:19:23.854521: step 75970, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 47h:12m:27s remains)
INFO - root - 2017-12-16 00:19:30.494894: step 75980, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 47h:26m:58s remains)
INFO - root - 2017-12-16 00:19:37.081767: step 75990, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 46h:11m:20s remains)
INFO - root - 2017-12-16 00:19:43.648218: step 76000, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 46h:38m:43s remains)
2017-12-16 00:19:44.175064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4467306 -7.6364985 -6.3753638 -6.1427641 -6.2313418 -6.3246193 -6.2316971 -5.71532 -5.0691309 -5.3488393 -4.9717574 -6.8551292 -8.8204184 -8.0319767 -8.6360559][-6.6759253 -5.9402676 -5.6294885 -6.6219888 -7.0674276 -7.6092997 -7.2129264 -6.9697533 -6.3882704 -6.0216436 -5.722322 -8.641078 -9.9192152 -8.8839035 -9.5906725][-2.5209298 -3.4702985 -4.4519429 -5.3782578 -6.4939747 -7.2831807 -7.7899256 -7.7086678 -7.480361 -7.917057 -8.02611 -10.989649 -12.897892 -11.580718 -10.823519][-0.83113194 -1.744673 -1.8691177 -2.6486874 -3.9770026 -4.421977 -4.9840689 -5.05133 -5.1691194 -6.4324207 -7.814126 -11.993629 -14.812742 -13.854389 -13.66263][-0.965466 -1.6279845 -1.6829486 -2.3643734 -2.5152566 -2.5277727 -1.9930675 -1.3827219 -1.9350665 -3.6955361 -5.6476049 -10.448603 -13.887987 -13.202121 -13.110624][-2.3030052 -2.2688646 -1.1272054 -1.7545047 -1.9313757 -1.2521844 -0.77265692 0.57295132 0.86841583 -0.64671278 -2.4456761 -7.6437049 -11.496584 -11.690077 -11.95035][-3.1850896 -2.9603248 -2.3242872 -1.9239638 -1.172411 -0.29014826 0.12357569 1.2487097 1.3316598 0.11520624 -1.4088807 -6.6153469 -10.017489 -10.293701 -11.209673][-3.0210912 -2.209657 -1.3983054 -0.94167423 -1.04492 -0.55821276 -0.0027880669 0.7565999 0.87489653 0.4178195 -1.288785 -6.9480143 -10.991374 -11.232759 -11.523302][-2.2965627 -2.32826 -1.3798256 -0.50615788 -0.57727909 -0.35965729 -0.15256691 0.50872755 1.0412931 0.51396322 -0.52353859 -5.4183917 -9.3142433 -10.008395 -11.566978][-3.4141972 -3.406275 -2.644762 -1.4597273 -0.92024469 -0.22212458 -0.096918583 0.049828529 0.559772 1.0195308 0.808434 -4.271163 -8.17698 -8.50648 -10.464572][-7.56037 -7.135849 -6.3855209 -5.157136 -5.0150032 -3.8630252 -3.1817539 -2.452621 -1.839237 -2.020767 -1.3437428 -3.7418752 -5.7223263 -6.4283247 -7.3317246][-11.454653 -11.258743 -10.663375 -9.0728254 -8.831459 -8.4311333 -7.8347006 -6.3452034 -5.2031288 -4.2673597 -3.4790697 -4.9312558 -4.9910026 -5.5636773 -6.6484275][-14.856936 -14.479851 -14.576294 -12.783909 -12.19932 -12.419497 -11.88644 -10.538641 -10.038219 -8.762989 -7.6841421 -7.2097368 -6.8948021 -6.15623 -6.3329363][-12.894648 -13.056442 -12.574804 -11.881381 -12.148159 -12.299095 -11.810846 -11.561892 -11.049978 -9.6665888 -8.7844267 -8.0373945 -7.9582796 -6.7995319 -7.1555958][-8.9355526 -9.704567 -9.4751062 -9.6136608 -8.76329 -8.6543541 -8.637804 -9.1544142 -9.31041 -9.59763 -9.2373333 -9.1264343 -9.5239038 -9.0783291 -9.5863476]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 00:19:50.828784: step 76010, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 47h:33m:46s remains)
INFO - root - 2017-12-16 00:19:57.433899: step 76020, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.687 sec/batch; 48h:55m:34s remains)
INFO - root - 2017-12-16 00:20:04.034685: step 76030, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.664 sec/batch; 47h:19m:02s remains)
INFO - root - 2017-12-16 00:20:10.620599: step 76040, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 46h:06m:40s remains)
INFO - root - 2017-12-16 00:20:17.154194: step 76050, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 46h:33m:00s remains)
INFO - root - 2017-12-16 00:20:23.815547: step 76060, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 46h:47m:10s remains)
INFO - root - 2017-12-16 00:20:30.459272: step 76070, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.689 sec/batch; 49h:05m:24s remains)
INFO - root - 2017-12-16 00:20:37.072232: step 76080, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 47h:58m:51s remains)
INFO - root - 2017-12-16 00:20:43.830345: step 76090, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.680 sec/batch; 48h:25m:09s remains)
INFO - root - 2017-12-16 00:20:50.414096: step 76100, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 47h:02m:31s remains)
2017-12-16 00:20:50.927020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1071262 -5.79474 -5.4771724 -5.5776453 -7.0548978 -6.6804152 -6.4336295 -5.5946264 -4.7934093 -3.4185848 -1.8921103 -4.7892475 -5.5122232 -6.7704844 -7.6732659][-6.8398271 -6.9687042 -7.1274428 -7.173728 -7.4331188 -7.325635 -6.6719804 -5.7471848 -5.493556 -4.5784559 -3.9214926 -6.8058071 -8.0561581 -8.9463768 -8.4733963][-6.981492 -7.2611985 -7.7008138 -7.3574867 -7.5757189 -7.3325253 -7.4344983 -6.7479753 -6.2786603 -5.8803577 -6.2131748 -8.3139458 -8.7534819 -9.2113457 -8.8825588][-6.8297782 -6.9224916 -6.36028 -6.5980663 -6.4189472 -5.5986786 -5.0690832 -5.5095267 -6.2837315 -6.4004092 -6.6660118 -9.3698492 -11.042391 -10.755073 -9.7460442][-5.5125942 -5.8959246 -6.122869 -4.0538073 -3.5196321 -2.8905175 -1.9032571 -2.6589255 -3.8102403 -4.7090445 -5.2896328 -8.799161 -10.825703 -11.13537 -10.297333][-4.35874 -4.4631147 -4.270793 -2.6203063 -1.0588932 0.68341446 2.5781302 1.3501506 -0.53750277 -2.9511902 -4.4010296 -7.4310446 -8.8172636 -9.12001 -8.3850565][-4.7913942 -4.089036 -3.0204265 -0.92426062 0.22850084 2.300734 4.265799 4.4506392 4.1059537 1.0396786 -1.5252538 -5.7035775 -7.3316021 -8.1127958 -7.4001379][-5.5230203 -4.3406367 -2.4178557 -0.028924942 0.82149076 3.3782687 5.111402 4.8332343 4.6516538 3.1707263 1.0352778 -3.1761351 -4.7314782 -5.4279156 -4.5018749][-5.4620733 -3.8711553 -2.1139436 -0.20356274 1.2722387 3.2104154 4.9510446 5.0957513 4.5144114 2.8897662 1.8393531 -2.4272542 -4.1189756 -4.2718725 -3.0289373][-5.8059363 -4.1655936 -1.9161429 0.024496078 1.5776882 2.5852103 3.2096171 3.1693769 2.561954 0.99043941 -0.48071957 -4.1725264 -5.2240667 -5.2114363 -3.8345156][-9.7899961 -7.5522852 -4.6990929 -2.6801925 -1.8038211 -1.5313511 -1.6730399 -1.6802163 -2.0852025 -2.9429486 -4.1786427 -7.2760839 -8.0330544 -7.0872483 -5.4152493][-13.413437 -10.64537 -7.4931054 -4.6500854 -3.0140922 -3.0840261 -4.0472808 -4.971653 -5.7560129 -6.6361952 -7.7775192 -9.5205631 -9.7284374 -9.0778246 -7.7443542][-14.52973 -11.24938 -7.8275356 -4.7430811 -3.5445926 -3.6185045 -4.3949738 -5.9235187 -7.7255125 -8.8816786 -9.8745317 -10.546941 -10.760853 -9.807127 -8.701829][-10.193696 -8.63044 -6.5993066 -5.0085936 -3.7638731 -3.0016117 -3.5787034 -4.993577 -6.4435153 -8.1257954 -9.8128071 -10.238455 -9.7537956 -9.3816223 -9.4813309][-7.0503597 -6.0307508 -4.553791 -3.4141958 -1.8102632 -2.3999906 -3.0607102 -4.02767 -4.8943877 -6.2861595 -7.5407357 -9.126543 -10.007109 -10.00511 -10.297628]]...]
INFO - root - 2017-12-16 00:20:57.416190: step 76110, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 46h:40m:14s remains)
INFO - root - 2017-12-16 00:21:04.045982: step 76120, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 47h:09m:11s remains)
INFO - root - 2017-12-16 00:21:10.735537: step 76130, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 47h:19m:52s remains)
INFO - root - 2017-12-16 00:21:17.302917: step 76140, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 46h:42m:20s remains)
INFO - root - 2017-12-16 00:21:23.890715: step 76150, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 46h:41m:16s remains)
INFO - root - 2017-12-16 00:21:30.530920: step 76160, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 47h:21m:19s remains)
INFO - root - 2017-12-16 00:21:37.093971: step 76170, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 46h:54m:39s remains)
INFO - root - 2017-12-16 00:21:43.712583: step 76180, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 47h:08m:26s remains)
INFO - root - 2017-12-16 00:21:50.264136: step 76190, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 47h:11m:36s remains)
INFO - root - 2017-12-16 00:21:56.885348: step 76200, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 47h:12m:48s remains)
2017-12-16 00:21:57.420367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3678775 -5.7067723 -5.1753507 -5.0110369 -5.5750442 -5.7089286 -5.4049678 -4.996532 -4.9447236 -5.0015826 -5.296957 -7.8119044 -10.544399 -9.62666 -8.403635][-7.4863243 -6.52011 -5.7198043 -4.7084174 -4.6266265 -4.9921722 -4.7068748 -4.1921477 -3.9432387 -3.699085 -3.8186896 -6.2066455 -8.93876 -9.4256468 -9.6676292][-5.7846813 -5.62671 -5.2819462 -4.4407392 -4.3725128 -3.3072851 -2.417114 -2.5081296 -2.4315977 -3.1567152 -3.9528589 -5.6464171 -7.8000584 -7.7244034 -7.5673566][-6.0426354 -6.1341023 -4.9467278 -3.4846604 -3.3307791 -2.6175723 -1.7218857 -1.529355 -2.1388395 -2.6094935 -3.0103524 -4.9369397 -6.9369254 -7.2388477 -7.5279365][-5.6107368 -6.011507 -5.2522097 -4.2550364 -3.0499206 -1.000114 0.13233423 -0.16594028 -0.42003584 -1.8085754 -2.797971 -4.3398757 -5.8732634 -5.6335464 -5.6931391][-7.34909 -6.771862 -4.4485121 -1.6234379 0.1190753 1.3733144 2.3068023 2.3516746 0.991889 -0.62606525 -1.4803095 -3.1309564 -4.9588647 -5.0465069 -4.8567762][-8.805171 -7.87519 -5.2960854 -2.4068487 -0.15768814 2.6501269 4.6346803 5.1836743 4.820292 2.0569763 -0.10681629 -2.7967787 -5.5012975 -5.5214548 -5.2000828][-8.9920158 -8.5222244 -6.405602 -2.8405676 0.27567625 3.7452769 5.75456 5.752481 5.2632613 3.5295424 1.204608 -2.4379647 -5.5148025 -6.1353803 -6.1972589][-7.8948078 -7.5079708 -5.6248727 -3.0953925 -0.62025547 2.0249887 3.5514188 4.8745112 5.0827765 3.2573314 1.2967033 -2.5891414 -6.3630691 -6.7307 -6.6072254][-6.0954666 -5.6440024 -4.8915968 -2.8644071 -1.4242115 0.60761642 2.3937211 2.8501964 2.6310267 1.6463003 0.18753147 -3.259264 -6.7472692 -8.1653595 -8.8684673][-7.4530253 -7.0763321 -5.6841817 -4.4426117 -3.5196595 -3.0561595 -3.0848055 -2.9289775 -2.671536 -2.6554706 -3.4053528 -6.6945066 -9.0322275 -8.9995546 -8.7720089][-11.047976 -9.2369528 -6.8748903 -5.6763806 -5.2557116 -4.7501788 -5.0932174 -6.1352425 -6.9118428 -7.5313363 -7.9053297 -8.86978 -9.6212711 -9.83434 -10.02384][-10.912523 -8.7521343 -6.7371683 -5.8435574 -5.7479367 -5.8915725 -6.7855892 -7.579689 -8.4045572 -9.3246889 -9.7427559 -10.199041 -10.378998 -8.4489822 -6.8300447][-9.1455765 -8.4112816 -6.8831959 -5.9517746 -5.2987981 -5.7637362 -6.3179884 -6.9649181 -7.9855051 -8.85408 -9.5593224 -8.6848116 -7.2373333 -6.2660875 -5.6864209][-6.1236253 -5.7652693 -4.8297405 -3.8675146 -3.5410941 -3.8966184 -4.9448509 -6.3881531 -7.3258624 -7.7208939 -7.8610535 -8.1896 -8.02314 -6.8441744 -5.74796]]...]
INFO - root - 2017-12-16 00:22:04.033849: step 76210, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 45h:42m:28s remains)
INFO - root - 2017-12-16 00:22:10.638275: step 76220, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 46h:26m:15s remains)
INFO - root - 2017-12-16 00:22:17.284660: step 76230, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 47h:08m:52s remains)
INFO - root - 2017-12-16 00:22:23.820342: step 76240, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 46h:30m:34s remains)
INFO - root - 2017-12-16 00:22:30.596231: step 76250, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.679 sec/batch; 48h:21m:08s remains)
INFO - root - 2017-12-16 00:22:37.319237: step 76260, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.648 sec/batch; 46h:06m:04s remains)
INFO - root - 2017-12-16 00:22:43.936618: step 76270, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 46h:52m:18s remains)
INFO - root - 2017-12-16 00:22:50.636256: step 76280, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 47h:15m:13s remains)
INFO - root - 2017-12-16 00:22:57.255511: step 76290, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 46h:56m:24s remains)
INFO - root - 2017-12-16 00:23:03.850637: step 76300, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.678 sec/batch; 48h:16m:11s remains)
2017-12-16 00:23:04.440070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6271863 -8.7717228 -9.1345091 -8.7532883 -8.9782858 -8.5156231 -6.9334965 -7.0305529 -7.8036079 -8.6579609 -7.701973 -6.7796078 -5.554646 -7.9850283 -9.251421][-7.5165663 -7.9205017 -8.8650036 -8.8798656 -8.9530573 -8.52686 -7.7857647 -6.5730634 -6.8972683 -7.4424806 -7.4737592 -7.5666504 -7.6117458 -9.757575 -9.3828611][-5.8772678 -7.6000924 -9.5695314 -8.973053 -9.1215076 -9.0574589 -9.1392937 -8.3979664 -8.7897215 -8.5449238 -7.3159547 -7.18338 -8.2525091 -9.8577557 -9.9047146][-7.2202368 -7.8410892 -8.1719885 -8.3404512 -8.5678749 -7.240747 -6.8182039 -7.656549 -7.4798994 -7.6511226 -7.3603182 -7.5372844 -9.00065 -11.293252 -10.721914][-6.0748296 -8.3054276 -10.309359 -7.851099 -6.0517449 -3.2371571 -2.0612133 -4.809648 -6.0536966 -4.8619184 -4.8279691 -6.0507779 -8.1004915 -12.004314 -12.28449][-8.2010107 -7.9973373 -8.0666332 -6.6058793 -4.3361497 -0.59934855 2.1945262 1.8128324 -1.8452375 -3.6489472 -3.4495306 -3.1751735 -5.5176268 -10.048307 -12.021624][-8.0997419 -7.8612432 -6.9227819 -3.1794295 -0.15438271 1.889432 4.0435681 4.7830558 4.0536046 0.7212925 -1.9787107 -3.4290886 -4.4845142 -6.7866573 -8.7562618][-7.7516565 -6.4575982 -5.9334369 -2.4924028 1.4163513 5.1810374 5.5058637 4.1584725 3.9916949 2.7995524 0.74535275 -1.2306061 -3.4569743 -7.0266953 -7.4453216][-7.0670137 -5.6813421 -5.4706941 -3.4372942 -1.1971469 2.7869687 5.5896068 4.9205709 2.8396029 0.35872936 -0.43094397 -1.4776654 -3.2336502 -7.4326344 -7.216938][-8.9456358 -7.7219439 -6.1053734 -4.14413 -3.7827706 -1.7804356 0.67191696 1.5767803 0.63873529 -1.3659487 -2.6690319 -3.563947 -3.6724386 -7.3754578 -9.4055109][-12.804099 -12.008156 -10.558264 -8.5979738 -7.3181496 -6.6729693 -6.7329111 -5.0144095 -3.15303 -3.9913158 -6.9225287 -7.4826603 -7.8933563 -9.83721 -10.190876][-15.56671 -12.515085 -10.157072 -8.796134 -7.6141672 -8.1498795 -9.3309345 -8.4856052 -6.9410381 -6.176899 -8.1018877 -9.647131 -9.1911459 -9.84651 -9.3424873][-14.463104 -11.689457 -7.4440975 -5.2850304 -5.7515736 -5.5148435 -6.9636388 -6.2279625 -5.2739329 -4.9293346 -6.050704 -8.6199026 -8.76652 -7.8731637 -6.6715612][-9.9377432 -7.7565393 -4.5142512 -1.4530821 -0.29384041 -1.9633572 -4.1036596 -3.8906133 -2.7329755 -3.0008645 -4.4031186 -4.4496861 -4.7286215 -5.3485656 -5.0851288][-5.4160061 -4.3202109 -2.3551242 -1.1787624 -0.16333103 -0.20962763 -0.27528763 -1.1046548 -1.4541411 -1.7659216 -2.2887528 -3.5113676 -4.535161 -4.6742239 -5.1303482]]...]
INFO - root - 2017-12-16 00:23:11.107782: step 76310, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 48h:21m:32s remains)
INFO - root - 2017-12-16 00:23:17.651454: step 76320, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 45h:41m:51s remains)
INFO - root - 2017-12-16 00:23:24.212497: step 76330, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 47h:55m:50s remains)
INFO - root - 2017-12-16 00:23:30.755167: step 76340, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 45h:32m:28s remains)
INFO - root - 2017-12-16 00:23:37.282096: step 76350, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 46h:14m:25s remains)
INFO - root - 2017-12-16 00:23:43.812852: step 76360, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 48h:13m:06s remains)
INFO - root - 2017-12-16 00:23:50.408969: step 76370, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 48h:29m:01s remains)
INFO - root - 2017-12-16 00:23:56.999139: step 76380, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 47h:14m:25s remains)
INFO - root - 2017-12-16 00:24:03.525790: step 76390, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.653 sec/batch; 46h:27m:35s remains)
INFO - root - 2017-12-16 00:24:10.172189: step 76400, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 48h:23m:46s remains)
2017-12-16 00:24:10.672596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8726435 -7.5878897 -5.9958148 -4.8467689 -5.5470781 -6.0022168 -5.8878155 -5.8215337 -5.2993846 -4.5913959 -3.4048865 -2.7935503 -5.4021544 -6.4872346 -4.7839131][-6.4958982 -6.2532787 -5.634223 -4.6333675 -4.6265445 -4.5937228 -4.6506786 -4.45733 -4.0614986 -2.8346672 -0.91256571 -0.010008335 -2.2063572 -4.0908241 -3.5687134][-5.3583107 -5.3285012 -5.2747216 -4.3845587 -5.2322078 -5.4045143 -5.1282268 -4.34853 -3.6053772 -2.2888312 -1.149703 -0.12261295 -1.8445385 -3.5652604 -3.182302][-6.8097115 -7.4181776 -7.4826207 -7.0179424 -7.0571957 -5.3110061 -4.1444774 -3.6960778 -2.7580733 -2.0631828 -1.589241 -1.2581716 -3.7835135 -4.8236079 -3.8243973][-6.8977652 -8.2191544 -8.9828339 -7.8684254 -6.7628493 -3.5770586 -1.0767546 -1.2160554 -1.6144075 -1.8388252 -1.7614753 -2.0906737 -4.2048721 -6.1993752 -5.5526843][-8.62646 -8.522769 -8.9907188 -7.5147142 -4.6615953 0.38950062 4.1879 3.5013127 0.75428247 -1.8580997 -2.9082417 -2.8053293 -5.2526517 -7.1130509 -5.8155518][-10.686719 -9.1396351 -7.6302385 -4.75322 -1.6665325 3.2105994 7.3152051 7.0032754 4.0675139 -0.40034628 -3.7031763 -4.2332988 -5.93016 -6.9970737 -6.0243359][-11.305179 -9.7215986 -8.848897 -4.6732087 0.53911352 5.7582974 9.095171 7.9107203 4.8520408 0.20913649 -3.7337151 -4.9349494 -7.1071234 -7.8893328 -6.6112494][-11.482918 -10.757375 -9.365303 -6.3709769 -2.5020685 2.9565244 6.9108472 5.51763 1.7321982 -1.549746 -4.0839319 -6.1789 -9.1763973 -9.7985058 -7.8747339][-12.035114 -11.98333 -10.976173 -8.4391518 -5.7042041 -1.4783549 1.5299067 1.9205222 -0.40652704 -4.0935917 -6.79315 -7.9806881 -10.845066 -11.88896 -9.759778][-15.079321 -15.167122 -14.116194 -12.331378 -10.114252 -7.6751561 -5.6334238 -4.6536865 -5.6431227 -7.1078577 -9.195879 -11.090048 -14.119619 -13.506477 -10.201959][-15.741447 -15.898598 -15.789854 -13.978062 -11.949209 -10.375436 -8.9654408 -8.3585835 -8.8195171 -9.8471088 -11.009445 -11.684574 -13.495333 -12.96817 -9.8441277][-14.104664 -13.248611 -13.088898 -12.236126 -12.152503 -10.103748 -9.2565746 -8.850853 -8.3560734 -8.4502668 -9.0897093 -10.11845 -10.855424 -10.105635 -7.804471][-12.625347 -12.519899 -12.103667 -11.019457 -10.48347 -9.6439991 -9.3060665 -8.26949 -7.7836628 -7.8923893 -7.8279014 -7.5997334 -7.5664821 -7.2372084 -5.8932443][-9.7169962 -10.151114 -9.7604771 -8.7780647 -7.20017 -6.5732088 -6.4266977 -5.978549 -5.8794761 -5.4041309 -5.2881203 -5.9347982 -7.263093 -8.13561 -7.77191]]...]
INFO - root - 2017-12-16 00:24:17.199544: step 76410, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 45h:32m:50s remains)
INFO - root - 2017-12-16 00:24:23.778801: step 76420, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 47h:06m:59s remains)
INFO - root - 2017-12-16 00:24:30.349469: step 76430, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 47h:20m:41s remains)
INFO - root - 2017-12-16 00:24:37.024258: step 76440, loss = 0.18, batch loss = 0.14 (11.5 examples/sec; 0.694 sec/batch; 49h:20m:48s remains)
INFO - root - 2017-12-16 00:24:43.663947: step 76450, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 46h:16m:13s remains)
INFO - root - 2017-12-16 00:24:50.309286: step 76460, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.683 sec/batch; 48h:34m:03s remains)
INFO - root - 2017-12-16 00:24:56.942471: step 76470, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 48h:12m:47s remains)
INFO - root - 2017-12-16 00:25:03.560701: step 76480, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 47h:43m:43s remains)
INFO - root - 2017-12-16 00:25:10.084040: step 76490, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 45h:11m:41s remains)
INFO - root - 2017-12-16 00:25:16.766916: step 76500, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.691 sec/batch; 49h:08m:39s remains)
2017-12-16 00:25:17.302144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2418675 -7.9487009 -7.7022204 -6.7716303 -7.0119476 -7.1523376 -7.0904608 -7.0762682 -6.7895617 -7.0750818 -7.1226454 -8.4142246 -8.0153246 -6.6455503 -5.2355113][-5.7419367 -5.4856472 -5.7242689 -6.018219 -6.3424754 -6.3148155 -5.9716864 -6.2710147 -6.9986067 -6.9805765 -6.9782753 -8.9316492 -8.9944677 -8.1862354 -7.6082892][-4.0162716 -4.3501792 -5.2358909 -5.1115613 -5.5902915 -5.6572027 -5.5097871 -5.4614253 -5.246387 -5.5568333 -6.3785219 -8.7502232 -9.5726824 -9.281909 -9.1120625][-3.8872414 -4.8753324 -4.4264927 -4.7665572 -5.4320912 -4.6327538 -4.0556211 -4.3724179 -4.7687883 -5.1636739 -4.8541236 -7.6998539 -8.6721048 -9.4905958 -9.8486061][-7.1719484 -7.7059402 -7.5878863 -5.6140666 -3.8532643 -1.7339871 0.037869453 -1.2092848 -3.104388 -3.6940467 -4.0382013 -6.7607346 -7.78636 -8.8489733 -9.6947746][-7.5345287 -7.5097189 -6.7859511 -4.9670944 -1.6592994 1.7676635 5.0029969 4.3963866 1.779016 -0.37802887 -3.4064393 -5.6118107 -5.49106 -6.9902673 -8.1344795][-9.0743866 -7.2658415 -5.53725 -2.00348 0.067083359 4.1946034 8.0782337 7.5142121 6.4034505 2.6032815 -1.8679743 -5.1874151 -6.5429797 -6.9543896 -7.1143885][-9.2695351 -8.2456 -5.5770359 -1.8235896 0.38866425 4.6936326 8.2698708 8.2935867 7.5425305 3.7451892 0.036296844 -5.0171442 -7.9424982 -8.325798 -8.0689335][-7.041729 -6.5064874 -5.6432438 -1.7170196 0.17458153 3.4959445 6.145062 6.1955886 5.8408971 2.3651528 -0.41159248 -5.5335045 -8.6200371 -9.2883663 -10.20198][-7.673151 -5.315661 -4.0240078 -0.656024 0.28945446 1.5764446 3.9950509 3.2148194 2.331902 0.35713625 -2.5214183 -7.2351317 -9.45968 -11.202235 -12.21871][-10.107519 -9.15731 -7.6495481 -4.903121 -3.7056832 -3.0853837 -2.7836146 -3.7905655 -3.2994652 -4.3039665 -6.2307019 -10.385369 -12.170813 -13.31823 -13.289299][-13.904158 -13.09771 -11.630059 -9.3864479 -8.069315 -7.592824 -7.9984345 -9.5522861 -9.8730764 -10.316973 -10.324044 -12.131386 -12.409948 -12.678961 -12.435072][-14.183851 -12.995514 -11.565077 -10.353802 -9.140276 -8.293417 -8.5153408 -9.51469 -10.329636 -11.008506 -10.27235 -10.979227 -10.784514 -10.898205 -9.744504][-12.927504 -11.284184 -11.167754 -9.4865494 -7.39656 -7.0325956 -7.02941 -6.9261694 -7.4094191 -7.8942757 -8.2822247 -7.8936739 -6.7686448 -7.4476433 -6.8306117][-9.6793995 -8.4200859 -8.0933094 -7.0915079 -6.3956995 -5.4186783 -4.381968 -4.443954 -5.44439 -5.7235708 -5.9823613 -6.7367249 -7.1611471 -5.9401445 -4.8556247]]...]
INFO - root - 2017-12-16 00:25:23.912882: step 76510, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 46h:45m:06s remains)
INFO - root - 2017-12-16 00:25:30.515295: step 76520, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 46h:32m:22s remains)
INFO - root - 2017-12-16 00:25:37.141302: step 76530, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 47h:08m:15s remains)
INFO - root - 2017-12-16 00:25:43.730348: step 76540, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 45h:30m:08s remains)
INFO - root - 2017-12-16 00:25:50.357325: step 76550, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 48h:06m:52s remains)
INFO - root - 2017-12-16 00:25:57.014505: step 76560, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 45h:46m:24s remains)
INFO - root - 2017-12-16 00:26:03.669264: step 76570, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.686 sec/batch; 48h:47m:20s remains)
INFO - root - 2017-12-16 00:26:10.378681: step 76580, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 46h:28m:29s remains)
INFO - root - 2017-12-16 00:26:16.955893: step 76590, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 47h:08m:02s remains)
INFO - root - 2017-12-16 00:26:23.514974: step 76600, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.659 sec/batch; 46h:50m:43s remains)
2017-12-16 00:26:24.062994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.5482483 -7.1780057 -6.5879674 -5.2856746 -4.5293751 -4.089921 -3.9119134 -4.4526076 -4.9897389 -5.1697493 -5.1602716 -6.5864983 -6.7714815 -6.9983053 -7.3469086][-6.5661573 -6.6250257 -6.0362124 -5.2572351 -5.307478 -4.7742643 -3.9838481 -3.8018541 -3.6803067 -4.156776 -4.0491576 -5.6037488 -7.4212365 -7.9479 -7.2575655][-4.7514162 -4.6162291 -4.5055785 -4.3540511 -4.2928596 -4.3744373 -4.1485734 -3.9940145 -3.9972463 -4.0429907 -4.0846653 -5.5416265 -6.1236305 -6.342454 -6.51211][-4.0914922 -3.7430425 -3.2676549 -3.3087928 -3.5808113 -3.9115181 -3.6441653 -3.3399563 -3.362072 -3.4621153 -3.7373209 -5.8795681 -6.7093778 -6.6945076 -6.6607075][-3.9190104 -3.9765792 -3.6161768 -3.3212016 -2.8962567 -2.653671 -2.6157818 -2.0835168 -1.8958812 -2.4294915 -3.0827084 -5.4415445 -6.6045976 -6.8637919 -7.2420559][-6.2071605 -5.6546187 -3.8844681 -2.4901121 -1.644002 -0.15861797 0.84098434 1.1167083 0.341084 -0.67776251 -1.5095124 -4.4484234 -6.2629681 -6.7018108 -7.0748215][-6.700532 -6.0586686 -4.3535585 -1.9435015 -0.36067295 0.89243364 1.9005561 2.3005805 1.9965248 0.60439682 -0.658442 -3.9846168 -6.1034627 -6.827672 -7.1205044][-6.7158713 -6.4125156 -4.7165194 -2.511673 -0.7975173 1.1491146 2.0387712 2.1153636 2.2355838 1.1381493 -0.68797779 -4.0375009 -5.8089652 -6.5243645 -7.1297388][-5.4673619 -5.4268494 -3.3954246 -1.2142711 0.094341755 0.68864155 0.74118805 1.2252812 1.2278905 1.0205822 0.61741352 -2.9843323 -5.4934263 -5.8467021 -5.2638111][-3.7518942 -3.5539072 -2.3547254 -0.79654932 0.033912659 0.66824341 0.78717041 0.46814108 0.0091567039 -0.41243839 -1.0347033 -3.5619276 -4.7315431 -5.3771238 -5.8790984][-6.722003 -4.8807607 -2.6246812 -1.2002807 -1.4004006 -1.6482811 -2.6109536 -3.718029 -4.3734818 -4.3159103 -4.4551978 -6.48415 -6.9817877 -6.4980259 -5.9043159][-8.8393087 -7.2779012 -5.6113319 -3.9937725 -3.5364676 -4.4597826 -5.9006305 -6.8345456 -7.0559425 -7.5886655 -7.9484568 -8.3160486 -8.3584738 -7.4082623 -6.2805495][-9.2727242 -7.3212681 -5.7134204 -4.720315 -5.3281708 -5.9572625 -6.243433 -7.2662697 -8.0628014 -8.141367 -8.1294861 -8.6136513 -8.8197136 -7.5695791 -6.6626635][-8.9536791 -7.628417 -6.6981697 -5.5818777 -4.5990343 -5.7731519 -6.5634308 -6.248826 -6.3198309 -7.332027 -8.10593 -7.7687969 -7.077055 -6.3309627 -5.7592082][-8.43063 -8.0880089 -7.0666947 -5.4667239 -5.5499873 -5.4632583 -4.6913147 -4.601203 -5.2604246 -5.0028381 -5.1013746 -6.1908927 -6.412291 -6.9751348 -6.8400974]]...]
INFO - root - 2017-12-16 00:26:30.661534: step 76610, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 47h:40m:46s remains)
INFO - root - 2017-12-16 00:26:37.249947: step 76620, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.639 sec/batch; 45h:23m:54s remains)
INFO - root - 2017-12-16 00:26:43.802934: step 76630, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 46h:47m:49s remains)
INFO - root - 2017-12-16 00:26:50.374956: step 76640, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.660 sec/batch; 46h:56m:17s remains)
INFO - root - 2017-12-16 00:26:57.025657: step 76650, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 46h:46m:19s remains)
INFO - root - 2017-12-16 00:27:03.619693: step 76660, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.650 sec/batch; 46h:13m:07s remains)
INFO - root - 2017-12-16 00:27:10.316821: step 76670, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 46h:39m:14s remains)
INFO - root - 2017-12-16 00:27:16.914245: step 76680, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 46h:40m:52s remains)
INFO - root - 2017-12-16 00:27:23.517759: step 76690, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 48h:08m:45s remains)
INFO - root - 2017-12-16 00:27:30.083641: step 76700, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 46h:15m:50s remains)
2017-12-16 00:27:30.573722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7276175 -4.0900588 -4.0530195 -3.8932743 -4.3294525 -4.6444902 -4.6667914 -4.4207573 -3.9447856 -3.6630733 -3.0372577 -5.8316479 -7.6641073 -9.3173037 -9.1534462][-5.2239017 -5.3175225 -5.0029726 -4.593956 -4.7891645 -5.274271 -5.53307 -5.7625737 -5.6422234 -4.8869257 -3.9650164 -6.2473989 -7.5554032 -9.9249706 -10.389141][-4.4790063 -5.2036161 -5.6451569 -5.2512875 -5.6924796 -6.1504359 -6.638505 -6.4421024 -6.0950694 -6.0979004 -5.4673367 -7.5053082 -9.0095177 -10.784614 -11.186151][-4.7930293 -5.7693763 -5.5332518 -4.8889818 -5.3550453 -5.436892 -5.4597082 -5.8043427 -5.7717919 -5.2308168 -4.7333407 -7.533112 -9.4082985 -11.665803 -11.878008][-5.7610488 -6.67683 -7.0146246 -5.2704635 -5.1074963 -4.3480539 -4.0586548 -4.4010286 -4.44218 -3.9763575 -3.4066715 -6.3672552 -8.5550632 -11.31246 -12.32349][-6.9504008 -7.2939348 -6.5482159 -4.417829 -2.5156224 -0.85715628 -0.11470032 -0.77092838 -1.5085568 -1.766947 -1.6676044 -4.3836842 -6.6408234 -9.4550018 -10.757929][-8.3987951 -7.7203 -6.0516567 -3.2025309 -0.90011692 0.91180944 2.123189 2.6128983 2.3133254 0.53992414 -0.89477968 -3.9844775 -5.8302917 -8.4101944 -9.8489466][-7.9658327 -7.4523954 -5.1625066 -1.9305992 0.34779119 2.9030776 4.366302 3.5982127 3.4118896 2.2859511 0.22706366 -4.0073557 -6.5432873 -8.8547745 -9.5342255][-5.0694151 -5.0447903 -3.9848151 -1.4639115 -0.18467045 2.4780049 3.6846147 3.3752837 3.6186595 2.0213275 0.34971523 -3.97825 -7.2173061 -9.6170321 -9.66741][-3.58377 -2.537992 -2.155097 -1.0349503 -0.32799006 0.32533503 0.76324463 1.5193734 1.4416842 0.13439083 -0.84600496 -5.0201678 -8.225914 -10.757529 -11.454645][-4.8053551 -3.9107246 -3.1601284 -2.6209896 -2.7489748 -2.6803291 -3.1728494 -3.0089607 -3.2106602 -3.645452 -4.2758479 -8.1997643 -10.816557 -12.292116 -11.997361][-8.9480572 -7.1697097 -5.9649105 -5.7884779 -5.6907716 -6.018352 -7.0426035 -7.8275471 -8.2135515 -8.1607714 -8.3136463 -10.05217 -11.161414 -12.48685 -11.907173][-11.316425 -10.130558 -8.895834 -8.375598 -9.2019176 -8.847455 -8.6666765 -9.35904 -10.55671 -10.751741 -10.608685 -10.948454 -11.261396 -11.411959 -9.9133224][-11.529778 -10.700762 -9.9577618 -9.2264071 -8.8610268 -8.9751759 -9.2200356 -9.3983927 -9.5778666 -10.046221 -10.311646 -9.1790886 -8.4753714 -9.5179873 -9.2477417][-8.9295073 -9.1016626 -8.6266947 -7.5772862 -7.04218 -6.8929472 -7.2334895 -7.633009 -8.0489483 -8.1448936 -8.03276 -8.7073622 -9.6367712 -8.977169 -8.7969332]]...]
INFO - root - 2017-12-16 00:27:37.172135: step 76710, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 46h:31m:34s remains)
INFO - root - 2017-12-16 00:27:43.821091: step 76720, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.666 sec/batch; 47h:19m:11s remains)
INFO - root - 2017-12-16 00:27:50.437826: step 76730, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 47h:38m:36s remains)
INFO - root - 2017-12-16 00:27:57.024977: step 76740, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 46h:13m:56s remains)
INFO - root - 2017-12-16 00:28:03.654202: step 76750, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 48h:16m:24s remains)
INFO - root - 2017-12-16 00:28:10.339759: step 76760, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 46h:51m:34s remains)
INFO - root - 2017-12-16 00:28:16.913669: step 76770, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 48h:18m:00s remains)
INFO - root - 2017-12-16 00:28:23.499422: step 76780, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 48h:16m:11s remains)
INFO - root - 2017-12-16 00:28:30.033360: step 76790, loss = 0.16, batch loss = 0.11 (12.8 examples/sec; 0.626 sec/batch; 44h:26m:34s remains)
INFO - root - 2017-12-16 00:28:36.642799: step 76800, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 46h:30m:59s remains)
2017-12-16 00:28:37.195351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4105506 -6.1430082 -4.9895163 -3.7634034 -4.0213747 -3.6013005 -2.4505978 -1.0621581 -0.40941763 0.29923916 -0.74563885 -4.3312192 -8.6048784 -9.4190063 -7.8780441][-5.929687 -7.1586685 -8.1304359 -7.4622483 -5.5907345 -4.0876064 -2.8263421 -1.0109372 -0.80986881 -1.3453231 -1.8025298 -4.9942245 -9.1695194 -9.738554 -9.168685][-4.4951291 -5.799355 -7.6375437 -7.3612022 -7.0554256 -5.5142431 -2.4677906 -1.3164411 -1.3341098 -2.2642844 -3.6666977 -7.105958 -10.575092 -11.071007 -9.35773][-3.3595057 -4.7178335 -5.5218925 -5.4920321 -6.2628469 -5.3210015 -4.0041852 -2.5929146 -1.8423703 -2.8192508 -4.3760753 -9.0042315 -12.83326 -11.749721 -9.4945784][-4.7288713 -4.73715 -4.7905793 -4.3144565 -4.3116179 -2.7089808 -1.3170104 -2.2343776 -2.9358406 -3.3806288 -4.633173 -8.4478827 -12.407977 -12.101778 -9.9109573][-8.052844 -6.1987114 -5.6105909 -4.114531 -3.4033661 -0.81247234 1.0493441 0.52507973 -0.69561577 -3.1821291 -5.0632749 -8.3335838 -11.682283 -12.427195 -10.144817][-8.2440577 -7.3322668 -6.8605175 -4.7854471 -3.4875183 -0.33153343 2.6640096 3.7126451 2.8669496 -0.8328476 -4.5949655 -7.3403335 -9.8093014 -10.196428 -9.7666817][-6.9710703 -6.3959227 -6.3149347 -3.7774343 -1.8291047 1.3117418 4.0142846 4.9590077 4.579349 1.851397 -1.4645882 -6.7147346 -10.967834 -10.378047 -8.9855537][-5.7512774 -4.7087736 -4.52444 -2.5070107 -1.5880909 1.3854127 3.8244233 5.210855 5.0073161 1.4255481 -1.457859 -6.418097 -11.265243 -12.248487 -10.491728][-5.1144657 -4.6612368 -4.0062819 -2.594991 -2.1506517 -0.3509078 0.67701912 2.5202079 2.8304706 1.3490148 -0.86493635 -6.5664325 -11.353167 -12.650829 -11.209397][-6.2495751 -7.198534 -7.3006268 -6.4424491 -5.1219368 -3.8953204 -2.7407136 -1.5826268 -1.4029264 -1.724921 -2.6212361 -7.6750813 -11.63841 -13.338362 -12.13018][-10.928324 -9.5009174 -8.6655827 -8.0963631 -7.9975338 -7.5550823 -7.4624071 -7.8512549 -7.8595839 -7.1616125 -7.4377179 -9.5580425 -10.836348 -11.253384 -10.185569][-13.617176 -11.813852 -9.1602316 -8.7262173 -8.8097658 -9.2001781 -9.6149092 -8.8625374 -8.655879 -8.2630453 -8.4643736 -9.9349346 -12.025654 -11.107004 -9.0050116][-12.725772 -11.02931 -8.9796753 -8.1368475 -7.886219 -7.1418672 -7.4055891 -7.9626522 -7.317668 -8.0435905 -8.3680973 -6.9375086 -7.2399769 -8.8295078 -9.2021494][-8.7581921 -7.16752 -6.4927611 -5.2761531 -5.58099 -6.378521 -6.5549159 -5.6914535 -5.4747605 -6.2984767 -6.8407955 -9.0443544 -9.5534029 -8.55846 -9.2564631]]...]
INFO - root - 2017-12-16 00:28:43.760041: step 76810, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 48h:12m:22s remains)
INFO - root - 2017-12-16 00:28:50.342165: step 76820, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 47h:41m:13s remains)
INFO - root - 2017-12-16 00:28:57.002863: step 76830, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 47h:17m:39s remains)
INFO - root - 2017-12-16 00:29:03.575887: step 76840, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 46h:06m:40s remains)
INFO - root - 2017-12-16 00:29:10.269095: step 76850, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.676 sec/batch; 48h:00m:28s remains)
INFO - root - 2017-12-16 00:29:16.933379: step 76860, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 47h:13m:22s remains)
INFO - root - 2017-12-16 00:29:23.579484: step 76870, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 47h:27m:41s remains)
INFO - root - 2017-12-16 00:29:30.210990: step 76880, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 47h:08m:45s remains)
INFO - root - 2017-12-16 00:29:36.839195: step 76890, loss = 0.11, batch loss = 0.07 (11.9 examples/sec; 0.675 sec/batch; 47h:55m:53s remains)
INFO - root - 2017-12-16 00:29:43.440777: step 76900, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.668 sec/batch; 47h:26m:12s remains)
2017-12-16 00:29:43.936064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.25637 -5.1775241 -5.101429 -4.5777721 -4.4743433 -4.9316082 -5.3490844 -7.236402 -8.7736568 -8.5749054 -7.7836514 -9.2293758 -10.143916 -10.064598 -8.6917477][-6.3650446 -5.7040157 -4.5690923 -3.8745894 -3.9929264 -4.7700148 -5.4372654 -7.5052805 -9.3940706 -10.272719 -9.6032391 -10.666881 -11.504492 -12.586779 -11.308241][-5.0974274 -5.8089213 -5.8442955 -4.3627958 -4.9556074 -5.2490029 -5.9956303 -7.7153225 -9.4360571 -10.35398 -10.344137 -11.322315 -12.585674 -12.108952 -11.146639][-8.32251 -7.7169476 -7.3359222 -5.5389466 -5.2007446 -4.5127945 -4.915484 -6.0475979 -6.6101274 -7.8718095 -8.7564144 -11.106955 -12.492332 -12.676142 -11.741179][-8.2065735 -8.1811943 -7.1872005 -4.8749704 -4.6438522 -3.1530247 -1.4875278 -3.0723758 -4.9777942 -4.5588226 -4.6131916 -6.9512339 -9.2027769 -10.452332 -10.834642][-9.4266682 -8.026001 -6.5374861 -3.5703318 -2.1576066 0.30363989 2.6917043 2.1190386 1.1473646 -1.2298336 -1.8199298 -2.583178 -4.83232 -6.9315977 -7.8064466][-7.9405847 -6.8759584 -4.7338018 -1.2175064 1.8889208 4.1619763 5.939054 5.6241937 4.9163756 1.3990097 -1.3733153 -2.9302649 -4.567687 -5.1165085 -5.1688442][-6.7163415 -5.5916104 -4.4625797 -0.1627574 2.7460589 5.4073491 7.2761436 6.2117877 5.0932479 2.4321713 0.63367414 -1.8609099 -4.4144783 -5.2863326 -5.5493803][-7.592186 -5.1810808 -2.8323255 -0.4951992 1.2589526 4.0464749 5.9368224 4.6351237 2.884954 1.1171846 0.3166585 -1.777204 -4.3347354 -5.5970678 -6.3483944][-7.3512015 -6.933867 -5.2981038 -3.2075934 -2.0955715 -0.00085306168 1.9981933 1.8967004 0.61800623 -0.2699523 -0.09016943 -2.7116797 -4.7923265 -6.6873908 -8.6922808][-9.5056705 -9.1026592 -8.9318209 -7.2290626 -5.80724 -5.3164563 -5.1800084 -4.1734142 -3.2309046 -2.632906 -3.284574 -5.1781716 -7.6824527 -8.8061867 -8.5320854][-12.009631 -11.745311 -11.399658 -9.248827 -7.7591629 -7.7998762 -8.5265818 -8.1988287 -7.4916482 -6.8751655 -6.3690691 -7.131032 -7.4277682 -8.8326263 -9.730422][-13.263649 -12.291836 -11.177324 -11.217543 -11.012559 -9.815546 -9.7645664 -9.1324482 -8.0084038 -7.6489754 -7.1087627 -6.9758797 -7.3241959 -6.1591725 -5.3329973][-10.924675 -9.919611 -9.044591 -8.05583 -7.5705128 -8.0038214 -8.2419033 -7.5509148 -8.0436125 -7.4592824 -5.6093946 -4.8653364 -4.2296176 -5.5828309 -5.62809][-5.396184 -5.213933 -4.8306818 -3.8150878 -2.8073287 -3.8840747 -4.3066058 -4.38916 -4.4571347 -4.3378925 -4.9932909 -5.4630055 -5.6562014 -5.7335091 -6.410831]]...]
INFO - root - 2017-12-16 00:29:50.571048: step 76910, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 47h:37m:27s remains)
INFO - root - 2017-12-16 00:29:57.105430: step 76920, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 45h:13m:50s remains)
INFO - root - 2017-12-16 00:30:03.745499: step 76930, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 48h:39m:46s remains)
INFO - root - 2017-12-16 00:30:10.352696: step 76940, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 45h:59m:21s remains)
INFO - root - 2017-12-16 00:30:16.976193: step 76950, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 46h:23m:26s remains)
INFO - root - 2017-12-16 00:30:23.671660: step 76960, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.675 sec/batch; 47h:55m:09s remains)
INFO - root - 2017-12-16 00:30:30.341453: step 76970, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 48h:05m:08s remains)
INFO - root - 2017-12-16 00:30:36.893139: step 76980, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 46h:39m:30s remains)
INFO - root - 2017-12-16 00:30:43.533254: step 76990, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 46h:35m:24s remains)
INFO - root - 2017-12-16 00:30:50.108938: step 77000, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 46h:45m:15s remains)
2017-12-16 00:30:50.629855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.89554644 -0.86941385 0.045773983 1.4180589 1.283143 -0.17333841 -2.9056346 -4.9035487 -5.2677989 -5.5857611 -6.0152016 -8.4065971 -11.589802 -12.673161 -10.690192][-0.84935236 1.312984 3.7244859 4.3849053 3.5064588 1.2086506 -1.4275622 -3.361794 -5.744617 -7.1059809 -7.9193058 -11.265072 -14.469175 -15.352341 -14.588415][-0.27513456 0.34912395 0.96951532 3.5428433 4.0559354 2.5248647 0.47512817 -1.8702474 -4.3086987 -6.3242149 -8.3049908 -10.713284 -13.784731 -16.092 -15.661501][-1.5431461 -0.68346977 1.2248325 3.4075933 2.4567943 1.2680769 0.16742325 -1.3601823 -3.3039129 -4.8225751 -6.1228905 -9.0027409 -12.519969 -13.614067 -13.096231][-1.0001287 -1.2733636 -0.57416105 1.4411654 1.71977 2.6853433 2.4861951 0.46645498 -1.1350865 -2.4047396 -3.9147084 -7.3735881 -10.935154 -13.019211 -12.831325][-3.0226889 -2.428766 -1.8388841 1.1753101 2.88761 4.6778531 5.0485997 4.210113 3.1966147 1.5108066 -0.68453693 -3.230685 -6.4832292 -9.0032482 -9.0659828][-3.3856206 -3.1676202 -1.720047 0.74396229 2.5820174 5.1522079 6.6233068 6.93183 6.4896293 3.9304786 1.2928782 -1.728936 -5.8634157 -7.9700823 -8.2199888][-6.8095064 -5.4208336 -3.3366046 0.68881083 3.0033593 4.778378 6.368588 6.5795703 6.3286777 4.9879527 2.8150582 -1.2230749 -6.2125621 -9.0617008 -9.4275856][-7.6116071 -6.0862923 -3.8147666 -0.672987 1.1565785 4.0386491 5.2682748 4.626822 4.4779944 3.6567721 2.1865454 -1.2962937 -6.0501237 -8.5888147 -8.8823452][-8.1910553 -7.2150054 -5.036694 -1.8568385 0.49616718 2.6207957 3.0357776 3.13481 3.4275336 2.0300732 0.20086336 -2.8989916 -6.8186626 -9.1171265 -9.6418924][-12.878786 -11.767973 -10.03825 -6.1144433 -4.2772946 -2.5336406 -1.3981228 -0.9762063 -0.95420551 -1.4568443 -1.802247 -4.8809013 -7.8020864 -9.3179035 -8.3246822][-15.240644 -15.191139 -13.657407 -10.526849 -9.1516476 -7.4100347 -7.3438339 -6.9476991 -6.6188545 -6.3609066 -6.0269494 -6.9407582 -7.7688303 -8.6635275 -7.6768093][-15.032503 -14.950027 -13.903553 -11.311367 -10.178927 -8.4189482 -8.5543222 -9.5951071 -10.034258 -9.2196951 -8.2733479 -8.0093594 -8.1645241 -8.3136368 -6.6993074][-10.891562 -11.012018 -11.126757 -9.2378826 -8.6353416 -7.7883067 -8.4626646 -8.0517559 -7.2504282 -7.7037525 -7.5896053 -6.248004 -5.6432376 -6.0557432 -6.02764][-8.635973 -7.8515534 -6.9072852 -6.0218968 -6.2951069 -5.1085691 -4.6173997 -4.7259688 -5.7175288 -5.5393314 -5.1578913 -5.6869607 -6.3367376 -6.1073093 -6.3645334]]...]
INFO - root - 2017-12-16 00:30:57.210240: step 77010, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 47h:10m:52s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 00:31:03.812868: step 77020, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 48h:22m:02s remains)
INFO - root - 2017-12-16 00:31:10.469605: step 77030, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 46h:11m:21s remains)
INFO - root - 2017-12-16 00:31:16.999680: step 77040, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 45h:14m:30s remains)
INFO - root - 2017-12-16 00:31:23.588936: step 77050, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 46h:58m:30s remains)
INFO - root - 2017-12-16 00:31:30.121165: step 77060, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 46h:35m:50s remains)
INFO - root - 2017-12-16 00:31:36.714565: step 77070, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 45h:43m:20s remains)
INFO - root - 2017-12-16 00:31:43.311580: step 77080, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 46h:53m:35s remains)
INFO - root - 2017-12-16 00:31:49.982494: step 77090, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 46h:35m:32s remains)
INFO - root - 2017-12-16 00:31:56.573206: step 77100, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 46h:23m:35s remains)
2017-12-16 00:31:57.081420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7543273 -6.2213817 -5.874752 -5.9931054 -6.7019286 -6.7242289 -6.6946163 -6.8609838 -6.5199142 -7.7687664 -8.3359222 -8.7169 -10.394343 -11.095742 -10.628812][-6.5030241 -6.3244195 -5.6604123 -5.1344104 -5.7104993 -5.412046 -5.62765 -5.84597 -6.5199251 -7.7432814 -7.4522252 -8.6539507 -10.397776 -10.823988 -10.112193][-5.7972794 -6.727355 -6.8812494 -5.6368809 -5.2420173 -5.3308754 -5.7101741 -5.5251856 -5.4911866 -6.5152264 -6.6647964 -7.0618305 -8.3095894 -8.485733 -8.840169][-7.2301869 -7.4889183 -7.8093872 -7.2945609 -6.8167191 -5.0007496 -3.8147211 -4.4618788 -5.1459093 -5.5977745 -5.5381455 -6.4700637 -7.9905825 -8.2902527 -7.7178736][-8.1907625 -9.022748 -9.0469341 -7.334846 -5.4485755 -2.8031077 -1.3060946 -1.1093063 -2.7124527 -3.6935515 -4.2459307 -5.4338536 -7.3015361 -7.9437265 -7.7288718][-9.4106064 -9.0070639 -7.5491438 -4.8289838 -2.0544899 1.4792752 4.2503085 4.0192227 1.2990046 -1.3293481 -3.4101186 -5.0126739 -7.1736193 -8.1829453 -9.3123636][-10.803243 -9.564477 -6.8875818 -3.0987613 0.066732407 3.3309588 7.0104842 8.2616692 6.9542594 3.2491755 -1.2704597 -4.7165856 -7.7754374 -9.3033438 -9.7883892][-12.620001 -10.650936 -7.4032755 -3.4333265 0.34982157 5.3765521 8.7235947 8.8926926 8.2960949 4.5125575 -0.26151037 -4.3579845 -8.17717 -10.130743 -9.9410954][-13.275276 -12.126717 -8.5484943 -4.2237139 -1.1096597 2.751101 5.4134116 6.772902 5.8659348 1.8867035 -2.2719123 -6.6117907 -10.646963 -10.977341 -9.8626957][-11.195738 -10.674306 -9.1289768 -4.5436497 -2.1822665 -0.42440128 1.9594126 2.7104869 2.1819057 -0.84367752 -5.2489762 -8.7877655 -11.782635 -11.404811 -9.6005192][-12.407881 -11.491045 -9.2857895 -6.6321635 -5.14347 -4.7747264 -4.2910562 -4.4067154 -4.0667143 -5.40381 -8.0435152 -11.467971 -13.351038 -11.513103 -8.4757252][-13.911095 -13.179796 -11.325715 -8.8374214 -8.1340342 -6.8825417 -6.9164114 -8.3098173 -8.5825386 -9.1885 -10.428291 -11.640056 -12.145887 -10.916109 -8.8883181][-13.426514 -11.92395 -11.289907 -9.5556231 -8.4402285 -8.5287132 -8.0599308 -8.59248 -9.1056786 -8.9787016 -8.8193178 -9.8624783 -9.8351536 -8.4155226 -6.1765952][-10.029222 -8.3717337 -7.747324 -7.5009713 -7.9200449 -7.5477891 -8.0426645 -8.0708513 -7.3014 -7.2286873 -7.7202463 -6.5430098 -5.6939082 -5.2829781 -5.4156551][-7.0657158 -6.4582014 -5.6316581 -4.0111275 -4.1166425 -4.8594656 -5.0544667 -5.1950669 -5.8977962 -5.286788 -4.6896057 -4.44163 -4.3721089 -4.3532162 -5.0817747]]...]
INFO - root - 2017-12-16 00:32:03.638951: step 77110, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 47h:03m:53s remains)
INFO - root - 2017-12-16 00:32:10.190977: step 77120, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.645 sec/batch; 45h:47m:11s remains)
INFO - root - 2017-12-16 00:32:16.812446: step 77130, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 45h:37m:57s remains)
INFO - root - 2017-12-16 00:32:23.410163: step 77140, loss = 0.21, batch loss = 0.17 (12.4 examples/sec; 0.644 sec/batch; 45h:41m:29s remains)
INFO - root - 2017-12-16 00:32:29.936353: step 77150, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 45h:11m:47s remains)
INFO - root - 2017-12-16 00:32:36.509415: step 77160, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 47h:29m:26s remains)
INFO - root - 2017-12-16 00:32:43.126732: step 77170, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 46h:36m:49s remains)
INFO - root - 2017-12-16 00:32:49.670818: step 77180, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 45h:45m:56s remains)
INFO - root - 2017-12-16 00:32:56.242898: step 77190, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 46h:38m:09s remains)
INFO - root - 2017-12-16 00:33:02.765161: step 77200, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 47h:40m:58s remains)
2017-12-16 00:33:03.300992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1234827 -6.0605497 -5.2076836 -5.2228594 -5.8764038 -6.1252222 -6.1009517 -5.1938686 -4.5045772 -3.5947807 -2.820956 -5.0880294 -6.4925551 -6.8632703 -7.0124626][-5.0164838 -5.4027009 -4.9130397 -6.1758065 -7.2654648 -7.4386644 -7.6163845 -7.0050688 -6.2203546 -5.2801385 -4.6404018 -5.7915282 -7.3518944 -7.6101141 -6.9272423][-3.8718314 -5.2321124 -5.3733149 -6.4641333 -7.4719577 -8.2833118 -8.84197 -8.3205738 -7.3080826 -6.518568 -5.5994477 -7.4081264 -9.4926739 -9.0873337 -8.9091644][-4.2084336 -4.5282397 -5.0799069 -6.5519462 -7.5023193 -7.1365013 -6.9892054 -6.7644949 -6.6024051 -6.5992107 -6.1742015 -8.3630714 -9.9568958 -9.9270668 -10.04326][-4.3526545 -5.5954404 -6.0865812 -5.4105582 -5.3977175 -4.496345 -4.1128988 -4.4127712 -4.7125745 -5.198761 -5.078711 -7.4492536 -8.970871 -9.5230923 -9.5952053][-5.7562613 -6.0581894 -4.5219827 -3.5161786 -3.1099255 -1.1595478 -0.27753544 0.39453077 0.65114307 -1.1537075 -2.9181609 -5.1915188 -6.8018284 -7.9396219 -8.4405107][-6.5801525 -5.541337 -3.4794936 -1.84534 -0.99513626 0.9662466 2.3828225 3.37246 3.6521735 2.1855497 0.64073277 -2.3802133 -5.0821 -5.9728661 -6.4467168][-5.8062739 -4.8412409 -2.4047625 -0.23579454 1.0675373 2.9241166 3.7298608 4.6314025 5.8417363 3.7968488 1.8214536 -1.1965857 -3.6225657 -4.2176123 -4.3767433][-3.9187136 -3.3163073 -1.8058593 -0.044612408 1.2415686 3.0178018 3.5268245 3.9712749 4.3871074 2.9027896 2.2637181 -0.29793692 -3.1352079 -4.0282531 -4.0124917][-4.19277 -2.7773781 -1.1043024 0.29482889 0.83140087 1.44343 2.042233 2.7411647 2.5248103 1.6650639 1.2188411 -1.5999885 -4.143311 -4.9699068 -5.4192824][-7.81433 -6.62508 -4.1155186 -1.6041818 -1.1444836 -0.84288263 -0.67884636 -1.3799429 -1.890712 -2.3188376 -3.0431423 -5.9268742 -6.819694 -6.4452677 -5.9353461][-12.03861 -10.53303 -7.1636858 -4.8428378 -3.8668058 -3.2465239 -3.819901 -4.8236957 -5.9084849 -6.8920159 -7.4661946 -8.5615349 -8.5589027 -7.4657674 -6.5359044][-12.00597 -10.30315 -7.7257738 -5.6919065 -4.4602604 -4.1367569 -4.7041912 -5.6887 -7.15489 -7.8803678 -8.5882082 -8.8538332 -8.7214212 -7.6859341 -6.879149][-11.053074 -10.093479 -7.9060621 -5.6220508 -3.8275039 -3.48364 -4.302855 -4.9870839 -5.9128327 -7.0136986 -7.9879293 -7.5850277 -7.9560661 -7.0188532 -6.4890752][-7.4124422 -8.09027 -6.28362 -5.6646171 -5.2161951 -4.6327491 -3.8066778 -4.1690197 -5.2652879 -5.8400822 -7.0214043 -7.9277539 -8.7430925 -9.0163069 -9.0831118]]...]
INFO - root - 2017-12-16 00:33:09.943548: step 77210, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 45h:23m:43s remains)
INFO - root - 2017-12-16 00:33:16.613659: step 77220, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 45h:26m:00s remains)
INFO - root - 2017-12-16 00:33:23.269518: step 77230, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 46h:52m:57s remains)
INFO - root - 2017-12-16 00:33:29.935582: step 77240, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.681 sec/batch; 48h:18m:26s remains)
INFO - root - 2017-12-16 00:33:36.491267: step 77250, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 45h:38m:40s remains)
INFO - root - 2017-12-16 00:33:43.144081: step 77260, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 45h:33m:48s remains)
INFO - root - 2017-12-16 00:33:49.719583: step 77270, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 45h:51m:30s remains)
INFO - root - 2017-12-16 00:33:56.368641: step 77280, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 47h:55m:02s remains)
INFO - root - 2017-12-16 00:34:02.930056: step 77290, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 46h:55m:50s remains)
INFO - root - 2017-12-16 00:34:09.504631: step 77300, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 46h:31m:32s remains)
2017-12-16 00:34:09.992430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5141215 -6.7730489 -6.1126537 -5.6243424 -4.7472715 -4.6053376 -4.494978 -4.7532554 -4.8277617 -4.1131444 -3.2737396 -3.6824472 -5.9646535 -6.4503 -6.5874109][-6.4217148 -5.2623496 -3.3223042 -2.2407837 -1.7593367 -1.712225 -2.0920272 -3.2713943 -4.6679764 -4.9092155 -3.7272234 -3.6322742 -5.5201893 -5.8918295 -5.9448161][-1.7429998 -2.5548172 -2.170939 -0.74343443 -0.071015358 -0.48736525 -1.4080067 -2.4783947 -3.3457367 -2.9105031 -1.9901936 -2.615921 -5.1588645 -6.3807335 -5.9609213][-1.5398645 -1.8644617 -1.4229193 -0.42446184 0.066877842 0.15517426 -1.130528 -2.5992718 -3.0132802 -2.8431818 -2.3604357 -2.9548025 -5.7034645 -6.616221 -6.3675327][-1.7639139 -2.5311754 -2.2336383 -1.7017102 -0.31855345 0.76329184 0.50772572 -0.88589478 -1.916532 -1.9266388 -1.8389623 -3.1440477 -6.099885 -7.1733909 -6.849143][-5.2486305 -4.606 -2.4939733 -1.14886 0.92675018 2.9138103 4.2537885 3.0392947 1.1447592 -0.70424652 -2.0424666 -2.7069013 -5.3444996 -6.5197544 -6.8008242][-9.3682461 -6.8741035 -3.6355824 -0.32994318 2.4750276 4.404511 6.1945462 5.9105668 4.8362603 1.5587378 -1.6563683 -3.3368933 -6.5796824 -7.6330624 -7.5379367][-10.359968 -8.3598156 -6.0074306 -1.0583386 2.9457536 5.4215207 7.497211 7.060041 5.627543 2.3317113 -0.92984819 -3.943126 -8.44414 -9.9114552 -9.3126211][-9.3651552 -8.0062914 -6.2452097 -2.7441535 0.45138502 4.1409545 6.1947141 5.2141252 3.2091327 0.87611055 -1.8307261 -5.6603718 -10.472662 -11.610023 -11.198664][-8.5447254 -8.5538883 -7.54847 -4.7072229 -2.7972865 -0.66465712 1.6864543 2.2596846 0.36517525 -1.9159892 -4.4812508 -7.3711338 -11.316521 -13.235136 -13.322748][-12.136806 -11.781119 -11.093452 -9.7427082 -8.5949392 -7.7173319 -6.0636282 -5.590889 -5.99946 -6.370286 -7.8926344 -10.775043 -13.47582 -13.62737 -12.968722][-15.548288 -15.513 -15.260141 -14.698072 -14.161924 -13.395548 -12.167486 -11.132858 -10.513977 -11.19166 -12.140916 -12.888936 -14.428858 -14.063526 -12.971507][-14.84273 -13.90255 -12.900169 -13.390276 -13.52788 -13.111525 -12.725899 -12.57493 -12.411974 -12.060985 -12.054049 -13.066744 -13.099503 -11.904812 -11.184252][-10.726822 -10.630895 -9.7283087 -9.5893316 -9.95015 -10.722959 -11.153585 -10.402811 -10.245981 -9.9787884 -9.7650185 -9.9097157 -10.062819 -9.303834 -8.7713928][-8.181839 -7.51872 -6.620575 -5.9656506 -5.673388 -6.5128813 -7.4569139 -7.5926123 -8.4215221 -7.5729132 -7.1186838 -7.3148217 -7.8840342 -8.5020342 -9.018405]]...]
INFO - root - 2017-12-16 00:34:16.639273: step 77310, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 47h:22m:26s remains)
INFO - root - 2017-12-16 00:34:23.239084: step 77320, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 47h:39m:14s remains)
INFO - root - 2017-12-16 00:34:29.852017: step 77330, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 46h:59m:52s remains)
INFO - root - 2017-12-16 00:34:36.463553: step 77340, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.664 sec/batch; 47h:02m:22s remains)
INFO - root - 2017-12-16 00:34:43.067049: step 77350, loss = 0.18, batch loss = 0.13 (12.7 examples/sec; 0.631 sec/batch; 44h:42m:14s remains)
INFO - root - 2017-12-16 00:34:49.669173: step 77360, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.637 sec/batch; 45h:08m:25s remains)
INFO - root - 2017-12-16 00:34:56.328705: step 77370, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 46h:07m:07s remains)
INFO - root - 2017-12-16 00:35:02.949937: step 77380, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 46h:00m:11s remains)
INFO - root - 2017-12-16 00:35:09.591239: step 77390, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 46h:50m:58s remains)
INFO - root - 2017-12-16 00:35:16.160477: step 77400, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.658 sec/batch; 46h:36m:57s remains)
2017-12-16 00:35:16.664453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6107759 -6.4009771 -6.783823 -6.7017517 -7.39689 -7.9028425 -7.9905682 -7.7276745 -7.3068171 -7.1107574 -7.0322552 -8.7616758 -9.1766357 -10.450933 -9.7874594][-5.84129 -6.0980392 -6.52124 -6.6369414 -7.4942889 -8.1059685 -8.3242855 -8.1636333 -7.8595695 -7.8525305 -7.533978 -8.9669132 -10.28047 -11.305841 -10.380646][-3.6197577 -4.0967054 -5.9518275 -5.9157686 -6.0779848 -7.0005021 -8.5292759 -8.2722111 -7.8140612 -7.4893894 -7.3137655 -9.1735935 -10.338017 -11.389446 -10.426394][-4.3624368 -4.80075 -4.817317 -4.8257132 -6.2564416 -5.8805995 -5.2834268 -6.2746325 -6.527823 -5.794857 -6.5278358 -8.564209 -10.920232 -12.149072 -10.94945][-3.6260133 -5.1514559 -6.2352047 -4.3535156 -4.3027067 -3.2112808 -2.3665428 -3.414223 -4.0044241 -3.9537296 -4.3912544 -7.0933328 -9.1423759 -12.042562 -12.313383][-5.3613844 -5.1814442 -5.8391008 -4.6183119 -3.3370316 -0.58575869 0.76078892 0.23963022 -0.33564997 -1.8378561 -2.3725872 -4.503099 -7.6571417 -10.879347 -11.628746][-5.1208692 -4.4526181 -5.3703289 -3.5496879 -0.80374908 2.2258921 4.071835 3.4012513 2.603476 0.58145475 -0.82267475 -3.2833548 -6.2454553 -8.9681826 -9.6167564][-5.514226 -4.0825367 -3.8281741 -1.8165016 0.62991333 3.7165151 6.4859529 6.35891 5.363153 3.4798865 1.3991194 -2.0369947 -4.8560333 -7.7584639 -8.7368364][-4.5388589 -4.5789981 -4.4897556 -2.1248231 -0.062062263 2.4420772 5.1285148 6.1567788 4.7859235 2.4026246 1.4725122 -1.5678062 -4.8493905 -8.0585747 -8.107481][-6.0086527 -5.467854 -4.5998459 -2.3938847 -1.5606041 1.2439671 3.3554606 3.723228 2.4634805 0.4409709 -0.86932182 -4.0485525 -5.9146209 -9.3746862 -10.758154][-10.466702 -9.5317116 -8.6668968 -6.850585 -5.0571575 -3.0264485 -2.2086296 -2.2640271 -2.7430136 -3.4340296 -4.9238162 -8.3810635 -10.891947 -11.612061 -10.744957][-16.11978 -13.22316 -10.531269 -9.03232 -7.42237 -6.3837862 -6.2060833 -5.7659163 -6.5874848 -6.8885245 -6.85732 -8.6415453 -11.905413 -12.335357 -10.507825][-14.800039 -13.505961 -9.8098536 -7.4942093 -6.3048258 -5.1536913 -5.024662 -4.0773077 -4.4714622 -5.4824448 -6.0604262 -6.9172359 -9.0245762 -9.6319332 -7.94695][-10.593055 -9.524539 -6.6534452 -4.4470935 -3.0805447 -2.8533478 -3.977519 -2.8722825 -1.2247987 -1.907073 -3.9314337 -4.441833 -4.8304138 -6.1877637 -6.6580677][-6.6254749 -6.282795 -5.0700426 -3.3128033 -2.8903825 -1.4199452 -0.68981791 -1.1945009 -1.681685 -1.3553562 -1.744096 -3.9798617 -5.7206664 -6.2221813 -6.5617361]]...]
INFO - root - 2017-12-16 00:35:23.304277: step 77410, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 48h:22m:22s remains)
INFO - root - 2017-12-16 00:35:29.914503: step 77420, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 47h:25m:49s remains)
INFO - root - 2017-12-16 00:35:36.526551: step 77430, loss = 0.21, batch loss = 0.17 (12.1 examples/sec; 0.663 sec/batch; 46h:57m:43s remains)
INFO - root - 2017-12-16 00:35:43.065579: step 77440, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 47h:16m:19s remains)
INFO - root - 2017-12-16 00:35:49.621325: step 77450, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 47h:20m:14s remains)
INFO - root - 2017-12-16 00:35:56.167478: step 77460, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 46h:24m:35s remains)
INFO - root - 2017-12-16 00:36:02.695538: step 77470, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 46h:09m:45s remains)
INFO - root - 2017-12-16 00:36:09.389738: step 77480, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 47h:49m:15s remains)
INFO - root - 2017-12-16 00:36:16.018935: step 77490, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 47h:18m:12s remains)
INFO - root - 2017-12-16 00:36:22.708567: step 77500, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 47h:54m:11s remains)
2017-12-16 00:36:23.299336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5267448 -4.3155575 -3.9116812 -3.3916774 -3.7866635 -3.6807234 -3.9113631 -3.7905359 -3.5001266 -3.2552087 -2.7286296 -3.3521297 -4.4274707 -5.7301154 -6.8890281][-3.7074151 -3.1804662 -3.1533413 -3.0492013 -3.404079 -3.5318017 -3.5109076 -3.1503415 -2.9099422 -2.4814818 -2.0430632 -3.1294677 -3.803678 -5.1268759 -6.5935497][-1.9388657 -2.4499238 -3.4977772 -3.7627635 -4.6753435 -4.37241 -4.0877643 -3.3890939 -2.4089272 -2.1726873 -1.7117553 -2.8762174 -4.0395632 -5.3247056 -6.4235492][-2.7043886 -3.8342381 -4.7667003 -5.0193596 -6.1779819 -5.3986964 -3.8188941 -2.8580174 -2.2174046 -1.4708719 -1.5290642 -3.72397 -5.4335451 -6.4617524 -7.2684207][-3.556016 -5.319922 -6.7140388 -6.3984823 -6.399056 -4.1357784 -2.065938 -1.940074 -2.1817176 -2.121305 -2.6292226 -4.8296194 -6.7578139 -7.7699833 -8.3351059][-5.0806041 -6.3943596 -6.9082046 -5.7282357 -4.2759933 -0.72049856 2.914619 3.4139113 1.8653326 -0.080975056 -2.0420744 -3.7984891 -5.3907094 -7.8863974 -9.257205][-6.6186123 -6.901536 -6.3513346 -4.0873256 -2.5659075 1.3974123 6.2343469 7.8952518 6.5610242 2.3529105 -1.4837322 -4.7595181 -7.2253771 -8.5093117 -9.2135324][-8.29287 -9.1875744 -8.2005615 -4.8505559 -2.4816785 2.2502313 7.5519567 9.2615585 8.13875 4.3285146 0.11042309 -4.7879815 -7.7652569 -8.7316017 -8.9699][-7.5901875 -8.4753189 -8.2667875 -5.1657648 -2.8647518 0.36081743 4.1537814 7.1463847 7.503221 4.15412 0.394598 -5.2993445 -9.5941572 -10.687883 -10.300026][-7.7968206 -8.4368677 -8.5321579 -6.3541694 -4.6776175 -2.5395808 0.26389837 2.89158 3.6642032 2.0063934 -0.98796558 -6.284771 -10.114023 -12.045912 -11.87689][-11.347465 -11.386089 -10.874502 -9.0539474 -7.8000402 -6.7594995 -5.0051188 -3.4894609 -2.1282206 -1.9466145 -3.3100905 -8.0700531 -12.216404 -13.054409 -12.023387][-14.257217 -14.247828 -13.915752 -12.217695 -11.372601 -10.358727 -9.3146553 -8.167305 -7.2214227 -6.9840889 -7.7133183 -9.9686785 -11.970299 -12.760375 -12.373105][-14.899988 -14.398474 -12.580536 -11.377708 -11.272833 -11.129728 -11.082062 -11.06587 -10.439342 -9.6543064 -9.5110922 -10.18247 -11.376192 -11.215108 -10.522829][-13.0116 -12.102638 -11.309526 -10.286425 -9.7773857 -9.7530985 -10.129921 -9.7281837 -8.92727 -9.8562679 -10.814253 -10.344077 -8.8693686 -7.8709764 -7.9764729][-9.26545 -8.9606676 -8.907999 -8.2599487 -7.5335355 -7.0183392 -7.162467 -7.7663026 -8.2339611 -7.737196 -7.4856429 -8.236805 -8.7980566 -8.1998005 -8.3017349]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-77500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-77500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 00:36:30.899327: step 77510, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 47h:53m:33s remains)
INFO - root - 2017-12-16 00:36:37.509368: step 77520, loss = 0.14, batch loss = 0.10 (11.4 examples/sec; 0.702 sec/batch; 49h:41m:17s remains)
INFO - root - 2017-12-16 00:36:44.062411: step 77530, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 46h:19m:36s remains)
INFO - root - 2017-12-16 00:36:50.751747: step 77540, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 46h:15m:30s remains)
INFO - root - 2017-12-16 00:36:57.335296: step 77550, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 45h:47m:40s remains)
INFO - root - 2017-12-16 00:37:03.923072: step 77560, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 47h:22m:58s remains)
INFO - root - 2017-12-16 00:37:10.528739: step 77570, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 47h:25m:00s remains)
INFO - root - 2017-12-16 00:37:17.134450: step 77580, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.657 sec/batch; 46h:31m:57s remains)
INFO - root - 2017-12-16 00:37:23.757572: step 77590, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 46h:58m:23s remains)
INFO - root - 2017-12-16 00:37:30.261440: step 77600, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 47h:26m:23s remains)
2017-12-16 00:37:30.780263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3187618 -6.8544049 -6.9387813 -5.8316774 -5.84917 -6.5639505 -8.2013464 -8.0218868 -6.7624416 -5.1592531 -3.7712836 -4.1897497 -6.63661 -7.4998069 -5.3129473][-5.1443539 -5.7904148 -4.4978466 -3.7996225 -4.2765594 -5.3783641 -5.8918996 -6.1222997 -5.3675675 -4.4392853 -3.9987638 -4.6470318 -6.2423611 -7.1542478 -6.1040411][-4.6177225 -5.3908114 -4.6233459 -3.1279857 -2.6384764 -3.7884641 -4.7998557 -4.6532464 -3.60703 -2.6622219 -2.5824003 -3.3613875 -5.9191704 -6.5257611 -5.5279593][-6.3568163 -6.2462339 -5.1420107 -3.2453918 -2.9629374 -3.9457073 -5.2880445 -5.5192108 -4.3161635 -2.7949858 -2.1615736 -3.7794929 -6.98132 -8.2139826 -7.388751][-8.9257183 -8.859129 -7.1389818 -4.2112732 -2.6122038 -1.918015 -2.1254582 -2.9547017 -3.1213665 -1.5848064 -1.0144024 -2.5330534 -5.6691689 -7.0565009 -6.7105207][-9.9423847 -9.2531986 -6.9860115 -2.5044687 1.4162273 3.0595136 3.2818818 1.0820832 -0.039497375 0.37061262 0.086634159 -0.55172014 -3.41188 -5.7141757 -6.2816329][-9.6445122 -8.9633656 -6.5613928 -1.893496 1.3962331 4.7219071 6.51986 5.1394677 3.7813487 1.5194917 -1.1131482 -1.6642752 -4.3539038 -6.4581847 -7.0485239][-10.244032 -9.3033724 -6.0828161 -2.4913392 0.66109753 4.1357923 6.4938445 6.044867 4.7723937 2.3179736 0.045109749 -2.0345392 -5.8053641 -7.6688485 -7.4955454][-8.2727089 -7.3190112 -6.0253763 -2.9085295 -0.17811966 2.0364642 4.0401597 4.932322 5.3863978 3.2785411 0.557353 -2.6961174 -7.1992378 -8.6408529 -8.20198][-7.3868055 -7.182272 -5.9514356 -3.1812391 -1.283072 0.30556011 1.9335876 2.7851834 3.3758092 1.715735 -0.21614456 -2.9509521 -7.3138342 -10.303252 -10.602653][-9.2766409 -9.15946 -8.0768108 -6.775475 -5.8278761 -4.1738396 -3.3225346 -3.3088264 -2.4112642 -2.5050933 -3.1865721 -6.5487814 -10.619501 -11.500876 -9.8747959][-15.480722 -14.425611 -12.400496 -10.504906 -9.35915 -9.0499086 -9.0422783 -8.9043837 -8.355217 -8.2989531 -8.2331686 -9.18722 -11.131916 -12.829056 -12.328107][-15.04917 -13.862011 -12.901473 -11.943099 -11.113899 -9.9113913 -8.9145451 -9.3883524 -10.022495 -9.9424686 -9.6366806 -10.307337 -10.784735 -11.418877 -9.7366867][-12.693344 -12.577711 -11.657306 -9.6019268 -8.3452959 -8.1002617 -8.8349066 -9.0448074 -8.9146194 -9.1240149 -9.0371714 -8.775032 -8.5045624 -9.3279715 -8.2604637][-10.712936 -9.2490978 -7.6400671 -7.0910559 -6.591711 -5.8995643 -5.7399731 -6.65132 -7.5655255 -7.5712576 -7.2110319 -7.9832134 -8.2440491 -8.4040146 -7.761797]]...]
INFO - root - 2017-12-16 00:37:37.304551: step 77610, loss = 0.20, batch loss = 0.15 (12.6 examples/sec; 0.633 sec/batch; 44h:48m:11s remains)
INFO - root - 2017-12-16 00:37:43.894984: step 77620, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 48h:14m:54s remains)
INFO - root - 2017-12-16 00:37:50.524972: step 77630, loss = 0.24, batch loss = 0.19 (12.0 examples/sec; 0.669 sec/batch; 47h:21m:08s remains)
INFO - root - 2017-12-16 00:37:57.151856: step 77640, loss = 0.14, batch loss = 0.10 (11.4 examples/sec; 0.699 sec/batch; 49h:29m:53s remains)
INFO - root - 2017-12-16 00:38:03.787772: step 77650, loss = 0.19, batch loss = 0.15 (11.8 examples/sec; 0.677 sec/batch; 47h:53m:26s remains)
INFO - root - 2017-12-16 00:38:10.329583: step 77660, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 46h:52m:04s remains)
INFO - root - 2017-12-16 00:38:16.898049: step 77670, loss = 0.27, batch loss = 0.23 (12.4 examples/sec; 0.643 sec/batch; 45h:32m:21s remains)
INFO - root - 2017-12-16 00:38:23.535929: step 77680, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.672 sec/batch; 47h:33m:14s remains)
INFO - root - 2017-12-16 00:38:30.188173: step 77690, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 47h:31m:37s remains)
INFO - root - 2017-12-16 00:38:36.778524: step 77700, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 48h:30m:25s remains)
2017-12-16 00:38:37.284634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8448324 -5.3753095 -4.3010798 -4.1711321 -4.5393963 -4.4425759 -3.89243 -3.2807236 -2.7864337 -2.3997462 -2.5966427 -4.8408132 -6.415791 -7.1396589 -7.6737685][-6.4229903 -5.9597945 -5.0928674 -4.1750441 -4.4302926 -4.5562658 -4.0454788 -3.3255639 -2.429553 -1.9954593 -1.8794861 -3.4634194 -5.2066746 -6.1307349 -6.4600115][-5.5849075 -5.5598993 -4.3673468 -3.6946557 -3.8310013 -4.0056915 -3.7618742 -3.3840322 -3.0580642 -2.9070375 -2.6905901 -4.175055 -5.8045783 -6.0641766 -6.082509][-3.8080344 -3.8825605 -3.1620331 -2.3000863 -2.1104348 -2.5688846 -2.8054597 -3.1842194 -3.4623234 -2.8325033 -2.463263 -4.6920242 -6.3742366 -6.7020378 -6.9787216][-3.3941483 -3.0062659 -2.540508 -1.93504 -1.9304624 -1.6192555 -1.0942783 -1.3325949 -1.4795437 -1.8333559 -2.5452659 -4.4392748 -6.0006061 -6.6396871 -6.7373838][-4.5131893 -3.7806876 -2.200784 -1.7402046 -1.0945535 -0.18081713 0.70030403 1.3412066 1.1017871 -0.0701766 -1.2195468 -3.636637 -5.4790139 -5.9634867 -6.5972519][-5.77885 -4.7940426 -2.5864871 -0.93665266 0.019883156 1.664722 3.1973252 3.6112609 3.5559564 2.4459672 0.77746868 -2.1769276 -4.8106208 -5.8285165 -6.3802781][-5.66887 -4.7059388 -2.9239445 -1.2207828 0.6785326 2.5243058 3.7166829 4.209271 4.1959529 3.3664536 2.281271 -0.97004366 -4.4399633 -5.6832666 -6.179368][-4.6401181 -4.5475802 -4.1224594 -1.5612955 0.29065752 1.8290181 2.8460212 3.640964 4.1132674 3.019177 1.5084486 -1.2591367 -4.0938587 -5.5405855 -6.2877626][-5.0468006 -5.2780662 -4.5335326 -2.6105564 -0.82691765 0.82609797 1.5444221 1.7881918 1.5770435 1.3129516 0.60188818 -2.8091466 -5.9937897 -7.186151 -7.4999356][-6.7554979 -5.6171432 -4.5174217 -2.88482 -1.5929875 -0.7925272 -0.37745476 -1.2904453 -2.4013963 -2.6105869 -3.1143181 -6.0534492 -8.4504728 -8.3547983 -8.6778469][-9.6237659 -7.7319193 -5.6913719 -3.4222972 -2.1992574 -2.7665579 -3.4049885 -3.917634 -5.0264344 -6.2578559 -6.9983878 -7.7234435 -8.4988289 -8.6542387 -8.4722357][-10.095439 -8.1482248 -5.435504 -4.8823004 -4.8451457 -4.4791746 -4.3108907 -5.2861657 -5.9773426 -6.4029293 -7.3150768 -7.401053 -7.3942738 -6.5088091 -5.6677079][-10.535398 -8.9272652 -6.8981423 -5.1137028 -4.187459 -5.2399373 -5.8513608 -5.3262582 -5.1011119 -4.99872 -5.2629991 -5.2682858 -5.075326 -5.0070558 -4.7478528][-8.2418785 -8.043108 -6.4914889 -4.7130575 -4.1195946 -4.5952768 -4.7921596 -4.8028684 -4.8003983 -3.7143998 -3.275183 -4.230474 -4.9262972 -5.6782203 -5.8663468]]...]
INFO - root - 2017-12-16 00:38:43.811531: step 77710, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 46h:04m:44s remains)
INFO - root - 2017-12-16 00:38:50.419013: step 77720, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.667 sec/batch; 47h:13m:10s remains)
INFO - root - 2017-12-16 00:38:57.005736: step 77730, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 46h:13m:07s remains)
INFO - root - 2017-12-16 00:39:03.627315: step 77740, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.636 sec/batch; 44h:58m:38s remains)
INFO - root - 2017-12-16 00:39:10.266236: step 77750, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 47h:13m:35s remains)
INFO - root - 2017-12-16 00:39:16.775907: step 77760, loss = 0.21, batch loss = 0.17 (12.1 examples/sec; 0.664 sec/batch; 46h:57m:21s remains)
INFO - root - 2017-12-16 00:39:23.363374: step 77770, loss = 0.17, batch loss = 0.13 (11.4 examples/sec; 0.705 sec/batch; 49h:51m:21s remains)
INFO - root - 2017-12-16 00:39:29.940491: step 77780, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 47h:28m:21s remains)
INFO - root - 2017-12-16 00:39:36.564515: step 77790, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 46h:35m:36s remains)
INFO - root - 2017-12-16 00:39:43.121388: step 77800, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 45h:36m:50s remains)
2017-12-16 00:39:43.640540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6537828 -4.2075105 -4.2029495 -4.3185172 -5.2294884 -6.1138558 -7.2042713 -6.6390815 -5.2381563 -5.2869773 -6.5987773 -8.7160635 -10.415813 -7.9160509 -5.5439286][-4.7064953 -4.2781873 -3.3391101 -3.2113037 -3.2899661 -4.2254 -4.13437 -4.0213022 -3.9702978 -3.9499335 -3.9233654 -6.3162642 -8.6770258 -6.8578835 -5.1673207][-4.523325 -4.8748646 -4.508451 -3.4880521 -3.8253083 -3.9784184 -3.7969489 -2.9207025 -2.4632375 -3.5922666 -3.2090297 -5.4060812 -7.8195615 -5.7235889 -3.9103637][-5.8711052 -6.1664767 -5.8494339 -5.4151158 -5.530746 -5.584537 -4.8400846 -3.8274956 -2.9592595 -2.4269214 -2.1752439 -4.6622195 -6.9360247 -5.6268816 -3.6513419][-4.9129734 -5.1341515 -5.3484716 -5.0255795 -4.4152746 -4.3935523 -3.8212266 -3.2380238 -2.1213009 -1.1272335 0.11526012 -2.9350736 -5.5875597 -4.7597723 -4.2253237][-5.6049709 -4.8745165 -3.5146666 -1.8722246 -0.69392633 -0.0052194595 0.39656019 0.57522011 -0.49792624 -0.63839722 -0.5327282 -3.3246346 -6.3630328 -5.6888633 -4.6765904][-4.9054465 -3.5391405 -1.4136539 1.057847 2.7693334 3.2645431 4.2067132 3.3666034 2.68298 1.4157763 -0.13445377 -3.5589278 -5.7740688 -5.1361094 -4.5250783][-4.7014766 -3.2218127 -1.7161274 1.2409592 4.3048997 5.6054215 6.511127 5.3288217 3.8587394 2.4000583 1.1105256 -1.9211242 -4.3902006 -4.0656137 -3.4625869][-6.0425935 -3.642993 -1.5826073 0.14578772 1.0729399 3.5293851 5.0965915 3.8770547 2.1642885 1.1158099 0.027224064 -2.4735291 -5.0567532 -3.8922129 -3.0080349][-6.3214788 -4.817811 -3.2363284 -1.2918057 -0.34407091 0.87900925 2.4868217 1.7722793 0.47676611 -0.072163105 -0.25699759 -3.1983593 -6.4260397 -4.9773192 -3.5355847][-7.8870511 -6.8778124 -5.8214645 -3.7273657 -2.6316593 -1.482554 -1.4258771 -1.7357144 -1.6876965 -2.6321208 -2.8204608 -5.4129844 -6.4184031 -6.2328725 -4.7479677][-10.788389 -8.8993149 -7.7368073 -5.8305016 -5.5128126 -4.9912047 -5.2384214 -5.8249168 -6.7200661 -6.2729583 -6.4925027 -7.5883455 -8.18382 -7.4847116 -5.9348083][-10.733763 -9.1424847 -7.7892642 -6.0070844 -5.8357182 -6.8586407 -7.8271313 -7.6941991 -6.8219461 -6.9269629 -7.0501714 -7.0390081 -7.5651655 -6.1278892 -4.3002577][-8.6905994 -7.671309 -6.2484984 -4.8108211 -4.0815163 -4.9268246 -5.0352411 -5.8545804 -5.91277 -5.7428594 -5.179152 -4.9420271 -4.4081526 -4.1194057 -1.9953136][-5.8696637 -4.8895264 -4.4065218 -3.7065687 -3.8762951 -3.459126 -2.6075022 -3.4665458 -3.629868 -3.2599354 -4.2344055 -4.462276 -4.3949461 -4.208746 -4.3457594]]...]
INFO - root - 2017-12-16 00:39:50.205423: step 77810, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 47h:08m:43s remains)
INFO - root - 2017-12-16 00:39:56.908066: step 77820, loss = 0.17, batch loss = 0.12 (11.6 examples/sec; 0.690 sec/batch; 48h:50m:45s remains)
INFO - root - 2017-12-16 00:40:03.435779: step 77830, loss = 0.24, batch loss = 0.20 (12.1 examples/sec; 0.660 sec/batch; 46h:40m:20s remains)
INFO - root - 2017-12-16 00:40:10.087229: step 77840, loss = 0.11, batch loss = 0.07 (11.9 examples/sec; 0.675 sec/batch; 47h:44m:18s remains)
INFO - root - 2017-12-16 00:40:16.692961: step 77850, loss = 0.18, batch loss = 0.13 (11.7 examples/sec; 0.685 sec/batch; 48h:28m:16s remains)
INFO - root - 2017-12-16 00:40:23.341038: step 77860, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 48h:10m:04s remains)
INFO - root - 2017-12-16 00:40:29.980462: step 77870, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 46h:10m:14s remains)
INFO - root - 2017-12-16 00:40:36.541711: step 77880, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.655 sec/batch; 46h:21m:14s remains)
INFO - root - 2017-12-16 00:40:43.082566: step 77890, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 45h:49m:03s remains)
INFO - root - 2017-12-16 00:40:49.682729: step 77900, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.651 sec/batch; 46h:00m:58s remains)
2017-12-16 00:40:50.251729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9644003 -3.6059136 -2.7712455 -2.4245584 -2.9304011 -3.00544 -2.8682966 -2.9273462 -3.6554177 -4.5906506 -5.288177 -8.0007973 -10.875925 -11.366068 -9.9318142][-3.5184369 -2.521807 -1.9787176 -1.8791635 -2.5939543 -2.7739689 -3.0098584 -3.7406054 -4.71951 -5.7176414 -6.1258368 -8.6087608 -11.756027 -12.797043 -11.593229][-1.6743793 -1.3608027 -1.6829453 -2.253515 -3.5853608 -4.0002813 -3.5896955 -3.1679289 -3.9488873 -4.9100337 -5.07533 -7.6585569 -9.9407368 -11.484636 -12.017][-3.8344967 -3.5590951 -3.2438622 -3.3894649 -4.1104131 -4.6932578 -4.1846929 -3.8801246 -4.3795109 -4.8096609 -5.1486998 -7.906796 -10.423607 -11.963067 -11.382665][-4.6278567 -5.638958 -5.7745209 -4.7785497 -4.38048 -4.0756416 -3.4128449 -2.4380491 -1.6754837 -2.3433869 -3.4500153 -6.611197 -8.6983786 -9.9099712 -10.250564][-6.0699534 -5.4402518 -4.7443666 -4.5759721 -4.75277 -3.1198018 -0.89259434 -0.21030045 -0.36018991 -0.70526123 -0.77843904 -4.0773644 -7.4222822 -8.7277374 -8.3836365][-6.8726916 -6.4243407 -5.4670768 -3.9391894 -3.3948855 -2.0313792 0.23250914 2.0046744 2.7254615 0.89533234 -1.151701 -4.6807456 -7.7695122 -8.5485153 -8.0111132][-6.9047976 -6.65338 -4.9517994 -3.8527322 -3.3041444 -1.263526 1.0586381 2.8229098 3.8195825 2.5408654 0.5186429 -4.0543303 -7.4288921 -8.7092037 -9.3395576][-7.23788 -6.3623905 -4.1304035 -2.4589529 -2.8922622 -2.3201907 -0.93577766 1.0704193 2.1858964 1.5833597 0.30617905 -4.670084 -8.9624157 -10.77129 -10.385509][-8.0186272 -8.2962608 -6.9563384 -4.4554119 -3.6715758 -2.2021761 -0.743525 0.97742176 1.788651 0.90389585 -1.0826268 -6.0134978 -10.033787 -12.081485 -12.460394][-11.175341 -11.333624 -10.108827 -8.6098194 -7.6311188 -5.0199485 -2.2737596 -0.79218626 -0.60112572 -1.4039087 -3.0027575 -7.9941545 -11.289969 -12.650599 -12.302176][-14.5033 -14.240112 -12.767311 -10.719362 -9.3815193 -7.0272813 -5.1055713 -3.4187536 -3.0563884 -4.921629 -6.7482638 -9.4272461 -10.520035 -10.45715 -9.7322321][-13.824064 -12.650307 -11.056186 -9.5862141 -8.5728226 -6.285079 -4.5081177 -3.7624874 -3.9717221 -5.2143316 -6.3898211 -8.1337118 -9.4827919 -8.6112537 -7.5970168][-9.6295433 -8.8726311 -7.983736 -6.858366 -6.6636729 -5.9992533 -4.4578791 -3.1763961 -3.2282627 -4.3594346 -5.9032474 -6.7830596 -7.5753045 -7.2657766 -6.7449131][-6.3491669 -5.6009173 -4.8856025 -2.8784544 -2.4795361 -2.8897488 -2.7928855 -2.435806 -3.3022637 -4.6011853 -5.2882967 -6.6521087 -8.2741346 -8.2960415 -8.750494]]...]
INFO - root - 2017-12-16 00:40:56.862934: step 77910, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 47h:50m:41s remains)
INFO - root - 2017-12-16 00:41:03.453378: step 77920, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 45h:39m:31s remains)
INFO - root - 2017-12-16 00:41:10.066652: step 77930, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 46h:49m:22s remains)
INFO - root - 2017-12-16 00:41:16.628429: step 77940, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 46h:36m:46s remains)
INFO - root - 2017-12-16 00:41:23.250272: step 77950, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 47h:20m:03s remains)
INFO - root - 2017-12-16 00:41:29.867793: step 77960, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 46h:40m:35s remains)
INFO - root - 2017-12-16 00:41:36.603832: step 77970, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 46h:53m:15s remains)
INFO - root - 2017-12-16 00:41:43.207122: step 77980, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 44h:57m:22s remains)
INFO - root - 2017-12-16 00:41:49.715252: step 77990, loss = 0.25, batch loss = 0.20 (12.5 examples/sec; 0.642 sec/batch; 45h:23m:17s remains)
INFO - root - 2017-12-16 00:41:56.308151: step 78000, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 47h:35m:06s remains)
2017-12-16 00:41:56.826916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8228588 -4.3014803 -2.8402586 -2.2026868 -1.8363159 -2.782048 -3.5215149 -3.7301407 -3.5731926 -2.5242639 -2.2861037 -3.2182465 -4.6037173 -6.212707 -7.0711441][-3.0806665 -1.466094 -0.84024382 0.24380875 -0.034327984 -1.7377307 -3.2767875 -4.680274 -4.777914 -4.1564255 -3.2872615 -4.7614794 -7.427073 -8.020175 -7.7798452][-2.6660025 -1.7074704 0.21590328 0.86091042 0.22949648 -0.98820972 -2.6021044 -2.7848442 -2.7220893 -2.9406238 -3.4247444 -5.1262789 -7.3301535 -8.4504042 -8.5768232][-3.0193799 -2.7013135 -1.757596 -0.48458147 -0.24947357 -0.899611 -1.8562188 -2.0997322 -2.2963319 -1.5340023 -1.049963 -3.6005723 -6.5313339 -7.133647 -7.7836895][-2.3299494 -1.6826344 -0.60983753 -0.026762486 0.71716166 0.55701494 -0.32210684 -0.89035416 -1.5408769 -0.51337767 -0.0773201 -2.5908015 -5.1386948 -5.9636879 -6.4368477][-3.1406903 -2.5131166 -0.6178751 0.95159817 1.50039 1.7609506 1.5905919 1.6287503 2.0610189 1.231307 0.29626226 -1.2959137 -3.4042258 -4.8654265 -6.0255947][-4.8365741 -3.7961333 -2.4345341 -0.33097076 1.375267 2.2697778 2.7421575 2.5507779 3.359695 3.4730372 2.7832808 0.48305893 -2.739243 -3.1321223 -3.2207406][-5.2985592 -4.2586 -1.7190266 -0.21219254 1.133853 2.5369182 2.9184566 3.1148629 3.5500503 2.9647584 2.0619407 -0.16671562 -2.4233758 -2.3683999 -1.2393522][-4.306932 -3.7781801 -1.9714839 0.33435535 2.4099641 2.7611136 3.1282868 3.3394246 3.0407472 1.7054262 0.94448662 -0.846128 -3.3002484 -2.3128431 -0.87078857][-6.4159608 -5.3111238 -4.6485157 -2.2842913 0.5698843 1.6767635 2.2069573 1.8148975 0.58738708 -0.044911861 -0.69555807 -2.625747 -4.6950846 -3.6649067 -1.80899][-12.057402 -11.056942 -8.7606611 -6.3970432 -4.3692322 -2.6141555 -1.5392332 -1.2192831 -1.9656441 -2.819309 -3.9695745 -5.7654595 -5.81728 -5.1973362 -3.2733538][-14.271183 -14.371675 -12.460514 -9.9259472 -7.7370949 -6.1206889 -4.8275886 -4.7188592 -5.0137506 -5.0003538 -4.7702913 -5.8851237 -6.0164204 -4.8036981 -3.1962416][-12.054331 -11.351873 -10.530201 -8.883811 -7.3347788 -5.8524156 -4.9297323 -4.5978155 -4.5494943 -4.7215776 -4.5895138 -4.2219524 -4.2360454 -3.1227767 -1.1569977][-8.8909531 -8.301178 -8.0841751 -7.0912189 -6.0367641 -5.1172223 -4.6520748 -4.444047 -3.9864683 -3.4796355 -3.7652297 -3.7613726 -2.3987081 -1.2917938 -0.63024807][-4.9946203 -4.8833213 -4.8779612 -5.2079554 -4.8056846 -3.5123932 -1.7875569 -1.4032097 -2.1861379 -2.3616769 -2.5679049 -3.2930052 -4.0293446 -4.028532 -3.1800375]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 00:42:03.461306: step 78010, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 46h:25m:42s remains)
INFO - root - 2017-12-16 00:42:10.031115: step 78020, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 46h:33m:00s remains)
INFO - root - 2017-12-16 00:42:16.600972: step 78030, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 47h:04m:14s remains)
INFO - root - 2017-12-16 00:42:23.259393: step 78040, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 47h:14m:21s remains)
INFO - root - 2017-12-16 00:42:29.762396: step 78050, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 45h:27m:06s remains)
INFO - root - 2017-12-16 00:42:36.391882: step 78060, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 46h:47m:16s remains)
INFO - root - 2017-12-16 00:42:43.009786: step 78070, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 46h:21m:38s remains)
INFO - root - 2017-12-16 00:42:49.664844: step 78080, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.688 sec/batch; 48h:36m:35s remains)
INFO - root - 2017-12-16 00:42:56.275872: step 78090, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 45h:58m:38s remains)
INFO - root - 2017-12-16 00:43:02.903640: step 78100, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.660 sec/batch; 46h:38m:10s remains)
2017-12-16 00:43:03.458003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2763481 -5.8810663 -5.2936664 -4.2403769 -4.339098 -4.9625144 -5.3241739 -5.0624595 -5.1166911 -5.03725 -4.5193443 -4.9100442 -6.8700013 -6.2469277 -5.0833807][-4.774754 -5.8958578 -5.5243711 -4.5632153 -4.8524609 -5.2504311 -5.0995669 -5.344017 -5.831368 -5.5953579 -5.3056612 -6.2001076 -7.422977 -6.8922229 -6.563005][-4.0391684 -5.0928268 -5.2479806 -3.896136 -3.9097033 -4.1443539 -4.1999044 -4.1281285 -4.196784 -4.3798952 -4.7311306 -6.1919403 -7.8382931 -6.657804 -5.5737715][-4.3448024 -4.6667843 -3.8062043 -3.0838342 -3.1064529 -2.5075316 -2.6725872 -3.2391739 -3.7978821 -3.3662856 -3.0820377 -4.6116314 -6.2648692 -5.8395877 -5.5708127][-5.4227915 -5.6916227 -5.609592 -3.6373966 -1.3641582 -0.34833479 -0.59121275 -2.0772898 -3.401345 -3.149775 -3.1077631 -4.3682084 -6.0977979 -5.6801562 -5.2289505][-7.6205049 -6.5802116 -4.8028035 -2.64922 -0.18316936 1.3635936 2.1964602 1.1614814 -0.28332853 -1.4520984 -2.8132644 -4.0194249 -5.791214 -5.2431417 -4.6703815][-7.4389386 -5.5145988 -4.460032 -2.2166889 -0.28909731 1.9289951 3.645371 3.360189 2.5493302 0.71003056 -1.1194057 -2.7788498 -5.1503406 -4.3404546 -3.2165515][-6.1874948 -4.59011 -3.388947 -1.2776208 -0.076583862 1.8629017 3.4232144 3.3433576 3.0740485 2.0339155 1.2248492 -1.4710193 -4.8601723 -4.5162573 -3.9570668][-5.1101737 -3.4018602 -1.7048731 0.26377106 0.398098 1.0666938 2.3023953 3.1457343 2.7853045 1.6667709 0.41143656 -1.8278821 -4.3350258 -4.5578365 -4.9358068][-4.4292021 -3.0738564 -1.512013 0.47871923 0.73328066 1.4040809 1.6772456 2.3008981 2.4810624 1.9026442 1.2126074 -1.2432213 -4.2591329 -4.491261 -5.3393583][-5.9018197 -4.6821003 -2.4619184 -0.47292233 0.099946976 -0.33372927 -0.93089914 -0.62652588 -0.24381399 -0.18820763 -0.65558195 -2.9339011 -4.8334832 -5.58237 -5.8701048][-8.79957 -7.8469276 -5.6886373 -4.3114314 -4.0508823 -3.4008498 -2.7666647 -3.5236437 -4.4170136 -4.0725594 -4.4884644 -5.1330957 -5.9300818 -6.6640077 -7.1979051][-9.67762 -7.8865089 -6.3988566 -5.5471377 -5.359561 -5.1818357 -5.5150037 -5.9000354 -5.650424 -5.8505988 -6.0142908 -5.571528 -5.3947153 -3.9266829 -3.6185277][-8.0895042 -7.11245 -5.2577009 -3.8577559 -3.4551487 -3.8008096 -4.3834219 -4.3402548 -4.7897015 -4.1546631 -3.4871423 -2.8748925 -2.4041123 -1.5973544 -0.99507952][-6.3708162 -5.7865024 -5.2068515 -3.9203744 -3.1680505 -2.8351858 -2.2320447 -2.995528 -3.7416482 -3.5932176 -3.6707346 -3.931066 -4.0450029 -3.5012069 -3.0883784]]...]
INFO - root - 2017-12-16 00:43:10.005199: step 78110, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 46h:05m:40s remains)
INFO - root - 2017-12-16 00:43:16.618009: step 78120, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 46h:41m:38s remains)
INFO - root - 2017-12-16 00:43:23.308237: step 78130, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 46h:30m:53s remains)
INFO - root - 2017-12-16 00:43:29.924572: step 78140, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 46h:21m:39s remains)
INFO - root - 2017-12-16 00:43:36.506665: step 78150, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 46h:31m:34s remains)
INFO - root - 2017-12-16 00:43:43.073392: step 78160, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 46h:07m:45s remains)
INFO - root - 2017-12-16 00:43:49.587565: step 78170, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 45h:05m:39s remains)
INFO - root - 2017-12-16 00:43:56.173087: step 78180, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 46h:11m:34s remains)
INFO - root - 2017-12-16 00:44:02.843904: step 78190, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.684 sec/batch; 48h:20m:37s remains)
INFO - root - 2017-12-16 00:44:09.509580: step 78200, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.675 sec/batch; 47h:41m:10s remains)
2017-12-16 00:44:10.034799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.762043 -5.0747089 -4.1071358 -4.153338 -3.935648 -3.5452771 -3.6471293 -3.3659756 -3.0438943 -3.974894 -4.1098776 -4.2668109 -5.6267591 -7.0704341 -7.2421627][-6.1657891 -5.8752036 -5.5764651 -4.572403 -3.9386771 -4.2186546 -4.2222695 -4.1200933 -4.5524306 -4.0128965 -2.941288 -3.2359927 -4.1784129 -5.9567432 -7.525919][-5.2907166 -5.7531786 -6.7873316 -5.9108052 -5.0271144 -4.6294131 -4.8628016 -4.4069662 -4.6400385 -4.7765508 -4.2772512 -3.819526 -4.7102089 -5.6445913 -6.4813914][-5.3345289 -6.8946857 -7.5228062 -6.4534364 -6.5812664 -5.5428596 -4.9882221 -4.812891 -4.5232687 -4.2916641 -3.9269018 -4.4668479 -6.4916978 -7.9902678 -8.6287689][-5.0870991 -7.1269064 -8.4899864 -7.7811584 -6.9907112 -4.05562 -2.7015448 -4.0185146 -4.5794425 -4.1179924 -3.7230663 -5.1764359 -7.9221039 -10.001368 -11.133272][-6.0073104 -6.263154 -6.1571193 -5.3932748 -3.4971604 -0.21574831 1.5802875 1.7656465 -0.10216665 -2.6639822 -3.7960362 -4.4092441 -7.2041154 -10.216991 -10.70572][-7.9130197 -7.4340925 -6.4028482 -2.2380643 0.91035509 2.7707057 5.27941 6.5568957 4.8689075 0.73792315 -2.5541511 -4.7088938 -7.7601357 -9.8395 -9.4699936][-7.7693558 -7.7990065 -7.0948071 -2.4853582 2.2039609 5.3647037 7.2904372 6.864759 5.9817567 2.8582006 -1.286118 -4.7856336 -7.8062019 -9.6950769 -9.1924028][-8.899992 -8.5655918 -7.3938303 -4.1983938 -0.79178286 3.1401849 5.4506612 5.3374982 4.5503135 1.8700008 -0.7973628 -4.7485733 -8.9936218 -9.7813425 -9.2708759][-11.017038 -10.443504 -8.368844 -5.4693909 -3.4735637 -1.1997957 0.6959939 1.7896633 1.280652 -0.8602891 -3.0494759 -6.4808493 -9.8758144 -11.163879 -10.353605][-12.566154 -12.434429 -11.170797 -7.7050409 -5.6427617 -4.6617613 -4.3113546 -3.4716249 -3.2367444 -4.47141 -6.2183323 -8.9739161 -11.473536 -12.176615 -11.459216][-13.357178 -13.315239 -12.463159 -9.2423944 -6.9125524 -5.4361453 -5.2770395 -6.0796423 -6.9054132 -7.2860775 -8.1783619 -10.719402 -11.311712 -10.894627 -10.433829][-13.769125 -12.689671 -9.9019527 -8.5413046 -8.0175848 -6.4618645 -5.8704376 -7.0916014 -7.955914 -8.3831635 -8.9462557 -9.513751 -10.323017 -9.4715528 -7.3342023][-12.100225 -10.59901 -8.1165791 -6.6913853 -5.5624576 -5.55605 -6.3701835 -6.1891775 -6.5260468 -7.4688411 -7.9623823 -7.5799551 -7.6927204 -7.5137038 -6.6714487][-8.4782515 -8.5296059 -7.2960825 -4.9447479 -3.3994589 -3.8666503 -5.0446758 -6.130106 -6.7528639 -6.5910845 -6.1723018 -6.9763861 -7.3803387 -7.2444077 -7.107636]]...]
INFO - root - 2017-12-16 00:44:16.624960: step 78210, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.675 sec/batch; 47h:39m:21s remains)
INFO - root - 2017-12-16 00:44:23.151413: step 78220, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 45h:28m:59s remains)
INFO - root - 2017-12-16 00:44:29.733124: step 78230, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.668 sec/batch; 47h:10m:50s remains)
INFO - root - 2017-12-16 00:44:36.356239: step 78240, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 46h:59m:26s remains)
INFO - root - 2017-12-16 00:44:43.003885: step 78250, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.681 sec/batch; 48h:04m:00s remains)
INFO - root - 2017-12-16 00:44:49.594803: step 78260, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.648 sec/batch; 45h:44m:39s remains)
INFO - root - 2017-12-16 00:44:56.137084: step 78270, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.648 sec/batch; 45h:47m:19s remains)
INFO - root - 2017-12-16 00:45:02.749190: step 78280, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 46h:31m:10s remains)
INFO - root - 2017-12-16 00:45:09.323356: step 78290, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 46h:02m:08s remains)
INFO - root - 2017-12-16 00:45:15.861407: step 78300, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 45h:13m:03s remains)
2017-12-16 00:45:16.340282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2549634 -6.9562345 -5.8248239 -5.2778978 -5.8036718 -5.9849024 -6.255579 -6.4867826 -6.1549706 -5.5121307 -4.1384859 -4.3930225 -5.41658 -5.4924197 -5.7535729][-6.0378094 -5.6205449 -5.2787333 -5.4318838 -6.4392123 -7.3950715 -8.0843773 -8.9180984 -8.909193 -8.1096134 -6.2756705 -6.4004931 -7.6096945 -7.2292833 -7.1960964][-3.9819841 -5.5097132 -6.9704504 -6.58217 -7.5674429 -8.509798 -9.01763 -9.127841 -9.1908064 -8.44923 -7.2639017 -7.8447614 -9.3964624 -8.893918 -8.60612][-5.7576628 -6.5372639 -8.1650257 -8.4827147 -9.5034447 -9.0040627 -7.9353933 -8.0388117 -7.9999042 -6.9479923 -6.068521 -6.9365664 -9.3612881 -8.7789974 -9.0413532][-7.9812288 -10.198644 -11.650276 -10.952322 -9.964035 -7.2731595 -4.0885553 -4.1045771 -5.5586329 -5.8130455 -5.8184485 -7.4033413 -10.175352 -9.9152412 -10.243114][-11.50692 -12.09144 -11.391517 -9.3146181 -7.1253066 -3.6513188 0.61804056 1.4441428 0.7097621 -1.8578777 -5.0206041 -7.1217933 -9.8218937 -10.402261 -10.801432][-12.269797 -12.229918 -10.772971 -6.7164135 -3.766675 0.13646078 3.9518533 4.9619613 5.4304023 1.9216905 -2.6149485 -5.9564176 -9.8040762 -9.5192442 -9.2984295][-12.045494 -11.019197 -9.2681656 -5.3030996 -1.4411497 2.8009973 5.8621926 6.7187381 6.2147183 2.728313 -0.35149097 -4.4252973 -8.5579529 -7.8581171 -7.2602477][-9.2544708 -8.98119 -8.1290569 -4.5990014 -0.78970957 2.4951243 4.9936652 5.4092813 4.2095103 1.4533772 -0.8858943 -4.663743 -8.1369591 -7.8376923 -7.3604207][-8.659977 -7.422164 -6.3826351 -3.2454343 -1.111742 0.85691929 2.8436074 3.1219954 2.0656438 0.21380424 -1.7983129 -4.354619 -6.7966576 -6.7277384 -7.3089437][-11.689526 -10.488517 -9.2084532 -7.270895 -5.60771 -3.2178583 -1.3531022 -0.85465717 -1.5489693 -2.9237227 -4.5213337 -7.4438515 -8.98023 -9.1363544 -8.0335464][-15.765249 -15.109306 -13.051392 -10.194805 -8.9130764 -8.1155281 -7.5581179 -6.8532343 -7.5117874 -7.8158474 -8.8166075 -9.8735676 -10.818052 -10.215689 -8.8844643][-15.072342 -14.351688 -13.142826 -11.106161 -9.7748775 -9.026742 -8.5848427 -8.7895613 -9.2355843 -9.317647 -10.161699 -10.079009 -10.400978 -9.469161 -8.1551991][-11.441661 -11.230772 -9.9931793 -7.9483805 -6.8523722 -6.7124295 -8.034565 -8.6702366 -8.2345858 -8.4656143 -8.8770466 -8.3177385 -8.5247679 -7.8329453 -6.7346525][-8.0712214 -7.5004253 -6.2898045 -5.7946692 -4.5772586 -4.1095324 -4.1432362 -4.8315964 -6.0858469 -6.21444 -6.3017159 -6.7740345 -7.6906929 -7.9202547 -7.5127821]]...]
INFO - root - 2017-12-16 00:45:22.903913: step 78310, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 45h:52m:11s remains)
INFO - root - 2017-12-16 00:45:29.532422: step 78320, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 46h:30m:15s remains)
INFO - root - 2017-12-16 00:45:36.205930: step 78330, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.655 sec/batch; 46h:14m:08s remains)
INFO - root - 2017-12-16 00:45:42.765267: step 78340, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 46h:29m:46s remains)
INFO - root - 2017-12-16 00:45:49.415370: step 78350, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 46h:53m:26s remains)
INFO - root - 2017-12-16 00:45:56.029024: step 78360, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 47h:42m:37s remains)
INFO - root - 2017-12-16 00:46:02.683081: step 78370, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 47h:54m:08s remains)
INFO - root - 2017-12-16 00:46:09.288473: step 78380, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 47h:36m:26s remains)
INFO - root - 2017-12-16 00:46:15.937767: step 78390, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 46h:42m:38s remains)
INFO - root - 2017-12-16 00:46:22.544268: step 78400, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 46h:05m:15s remains)
2017-12-16 00:46:23.118320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8543825 -3.7570238 -3.9254284 -4.0184846 -4.588675 -4.7346878 -5.658967 -5.2913494 -4.2059312 -2.8911135 -1.7151618 -2.6558185 -4.4564443 -4.2471914 -4.5351934][-1.3874612 -1.9553008 -2.5384331 -2.9881644 -4.0439472 -4.235724 -3.7161636 -3.5200515 -2.9205344 -1.3843007 0.13974857 -1.1738758 -3.2138257 -3.5603554 -3.7711227][1.3829126 -0.65445375 -2.9579427 -2.3363953 -2.457896 -3.0773358 -3.2890496 -2.567075 -0.77450657 0.41621828 0.763309 -0.84632063 -2.4903543 -4.1083117 -5.4648938][-2.1225791 -2.5892229 -2.77736 -2.6976101 -2.791223 -2.2886784 -2.0534475 -1.8994212 -1.2933898 -0.16151571 0.51590157 -1.9035985 -4.7680569 -5.6324916 -5.3644319][-4.1824427 -5.0462909 -5.4192295 -3.8328943 -2.3571727 0.12607861 1.9425745 0.82337523 -0.42963362 -0.843421 -1.2707419 -2.910799 -4.7383327 -5.6889338 -5.6776071][-8.1267395 -7.8181362 -5.7663622 -1.4751577 1.7377229 4.6401191 6.0835233 5.1093535 4.0322022 1.9433255 -0.55152512 -2.502619 -4.7205591 -5.3158569 -4.7378273][-10.646673 -8.1939306 -4.9083643 -0.93022203 3.0074048 7.1418967 9.7985191 9.1211681 7.8230672 4.218224 0.68364573 -2.0452652 -5.6121345 -6.13304 -6.2735362][-9.94282 -9.3851948 -6.91967 -1.9354501 1.5449815 5.9792752 9.1261826 8.2481747 6.90107 3.6365819 0.37850952 -3.3782206 -7.03405 -7.8752365 -8.5552435][-8.5849209 -8.3984919 -6.5362053 -3.3173158 -0.30640697 2.078299 4.2396159 5.4717441 5.323566 1.8725972 -0.9869256 -4.2685833 -8.8353281 -9.9608288 -9.743289][-7.2351074 -7.4044037 -6.766232 -4.9232059 -1.9522669 -0.24689341 1.4297738 2.4950385 2.7461104 0.13149452 -2.5119083 -6.1240726 -9.1600208 -10.271866 -11.051651][-13.344519 -12.62657 -11.680008 -9.6021023 -7.2559052 -5.7748842 -3.8925247 -3.3581493 -3.6871691 -4.4867339 -5.5862274 -9.0581894 -11.100386 -11.335058 -10.826147][-17.467886 -16.483406 -14.831581 -13.138206 -11.286707 -9.8983479 -9.0722485 -8.6680336 -8.7851353 -9.2635746 -9.9866142 -10.514067 -11.146626 -11.321875 -10.659023][-14.888594 -13.995188 -12.345562 -11.172533 -10.608339 -9.3006525 -8.9003582 -8.84844 -9.8058357 -10.124917 -10.054777 -9.8312273 -9.6058311 -8.6669292 -7.793478][-11.949141 -11.136928 -9.4676342 -7.6912632 -6.3322468 -7.0202341 -8.0966253 -8.1963339 -7.9553251 -8.0923023 -8.9196091 -8.3918209 -7.7023325 -6.50468 -6.6310048][-8.5958214 -7.3629107 -5.6945305 -4.4413071 -3.8268781 -2.9966428 -2.6842878 -4.1189222 -5.3451385 -5.3333549 -5.1805615 -6.3407907 -7.275012 -6.9901991 -6.6221113]]...]
INFO - root - 2017-12-16 00:46:29.677979: step 78410, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 45h:54m:23s remains)
INFO - root - 2017-12-16 00:46:36.192534: step 78420, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 45h:20m:43s remains)
INFO - root - 2017-12-16 00:46:42.707086: step 78430, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 46h:38m:13s remains)
INFO - root - 2017-12-16 00:46:49.296160: step 78440, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 46h:22m:03s remains)
INFO - root - 2017-12-16 00:46:55.908273: step 78450, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 48h:21m:22s remains)
INFO - root - 2017-12-16 00:47:02.465463: step 78460, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 46h:24m:18s remains)
INFO - root - 2017-12-16 00:47:09.049530: step 78470, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 46h:20m:28s remains)
INFO - root - 2017-12-16 00:47:15.550477: step 78480, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 47h:09m:43s remains)
INFO - root - 2017-12-16 00:47:22.131630: step 78490, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 47h:26m:20s remains)
INFO - root - 2017-12-16 00:47:28.646721: step 78500, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 46h:12m:30s remains)
2017-12-16 00:47:29.235556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8364744 -9.0578089 -9.3184566 -9.3432646 -8.9594727 -9.320035 -9.5434971 -9.1011829 -7.860733 -6.5019 -5.76523 -6.2949204 -6.3638387 -6.2528625 -6.71241][-5.9607954 -7.0365634 -7.6033678 -7.532546 -8.0176945 -8.0196218 -8.7338991 -8.9445763 -8.6681013 -7.4830804 -5.3621879 -5.4787741 -7.0985775 -8.4230366 -7.8983746][-3.5514963 -4.2371359 -4.7972736 -5.4862556 -5.8815408 -6.0007873 -6.0978155 -6.6262383 -7.0438395 -6.1573572 -4.9834728 -5.915453 -6.3373461 -7.4606991 -8.44933][-3.9855552 -4.0971403 -4.5034366 -4.7822862 -4.0491514 -3.9778295 -4.203032 -3.9992976 -4.0769424 -4.6852932 -4.8437166 -5.1107659 -6.5951395 -7.2141514 -7.614059][-4.4561634 -5.0204058 -4.7569923 -3.3641372 -3.2048235 -1.9993005 -0.81555843 -1.8141897 -2.9514754 -2.5240648 -2.4801788 -3.7339463 -5.0582905 -7.0653148 -7.5306215][-5.0212255 -4.7257633 -4.2908044 -3.0221405 -0.7482543 0.94744205 2.1017685 1.0884676 0.53887653 -0.5354414 -1.1523104 -2.0880764 -3.1669593 -4.8753138 -5.7459359][-4.6534629 -3.9049509 -2.7494729 -0.92399788 0.56756544 2.4400229 4.6883016 3.8901744 2.9326835 0.79753351 -1.3931699 -2.3923044 -3.9715962 -5.3580184 -6.0143771][-4.7437077 -3.3234808 -1.4973116 0.19084597 0.86559057 2.623693 3.8124957 3.6702914 3.3734384 1.7417469 -0.19708681 -3.435014 -6.1594229 -7.2225213 -6.1830206][-5.7175212 -3.9822359 -1.534049 0.34934759 0.78047323 2.0102658 2.6679626 2.7800899 3.4224391 3.1634793 1.9069028 -1.9992802 -7.0839186 -10.300005 -11.151116][-6.1580663 -4.9497123 -3.0941176 -0.081261158 0.22164583 0.1520133 1.0793595 1.5737162 1.5049539 2.2873416 2.5622983 -0.73848009 -4.8799114 -9.4481411 -12.992214][-9.38979 -8.43548 -6.8665752 -4.177074 -3.6081216 -3.6853204 -3.2420037 -2.0655534 -1.7764919 -1.0237203 -1.6451716 -4.0523715 -6.6958165 -9.94076 -11.561343][-11.462938 -11.899591 -11.155928 -8.7256622 -8.5397587 -8.4824066 -8.185236 -7.5014524 -7.0218253 -6.0731454 -5.7522836 -7.3404427 -9.0577488 -10.357819 -10.945542][-11.798141 -11.572317 -11.539145 -10.543833 -10.72925 -10.41655 -10.205188 -8.4868259 -7.533433 -6.2056022 -5.5649762 -7.9032612 -9.6085215 -11.342108 -10.597729][-12.8929 -12.314999 -11.438889 -10.223692 -10.939594 -12.11438 -10.911749 -9.3714809 -9.0062609 -7.1921692 -6.4297757 -7.5547667 -8.1025848 -10.570626 -10.596561][-9.1584387 -10.112555 -9.9639969 -7.4156542 -6.3885489 -6.6058397 -7.1164446 -7.0836787 -7.4151464 -7.7017837 -8.6957035 -8.8125248 -8.2894783 -9.4806843 -10.514505]]...]
INFO - root - 2017-12-16 00:47:35.827172: step 78510, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 45h:58m:21s remains)
INFO - root - 2017-12-16 00:47:42.378344: step 78520, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 45h:07m:34s remains)
INFO - root - 2017-12-16 00:47:49.000454: step 78530, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 45h:29m:39s remains)
INFO - root - 2017-12-16 00:47:55.582611: step 78540, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 47h:00m:26s remains)
INFO - root - 2017-12-16 00:48:02.199587: step 78550, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 46h:15m:53s remains)
INFO - root - 2017-12-16 00:48:08.860421: step 78560, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 45h:44m:18s remains)
INFO - root - 2017-12-16 00:48:15.358803: step 78570, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 44h:52m:47s remains)
INFO - root - 2017-12-16 00:48:22.050278: step 78580, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 47h:14m:52s remains)
INFO - root - 2017-12-16 00:48:28.698664: step 78590, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 46h:23m:44s remains)
INFO - root - 2017-12-16 00:48:35.232356: step 78600, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 46h:33m:17s remains)
2017-12-16 00:48:35.776477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-12.516964 -13.648506 -13.50808 -13.738407 -14.679142 -14.502663 -13.434556 -10.896018 -9.0099859 -8.5807352 -9.503109 -11.002665 -13.6661 -13.896286 -11.15064][-11.163324 -10.800022 -11.279985 -11.859823 -13.297593 -13.965794 -13.76758 -12.593258 -11.746276 -10.725632 -10.320606 -11.514688 -14.085871 -14.276302 -12.884703][-5.4318075 -6.649178 -8.5196609 -10.056274 -11.599588 -11.274816 -10.711634 -9.8001709 -9.3874588 -8.3857708 -8.0673742 -8.886157 -12.023211 -13.138567 -13.201914][-3.1616478 -3.2109931 -4.5197363 -6.6508408 -7.889214 -6.6838779 -5.6690722 -6.1477528 -7.0576549 -7.1405087 -7.0550551 -7.5210104 -10.355141 -11.585034 -11.533792][-4.2899122 -4.9045572 -6.2882309 -5.5063763 -4.3288507 -2.340903 -0.68700981 -1.7545085 -3.4158177 -4.1425962 -4.9182425 -6.5729151 -9.6671782 -11.353069 -10.641615][-6.6767297 -6.9004583 -6.977541 -5.7325029 -3.8451784 0.20890856 3.5781436 3.1483903 1.7027564 -0.62084818 -2.233376 -3.6881714 -7.3483171 -8.9615183 -8.5911608][-8.4558048 -8.771862 -7.6373014 -5.3832722 -2.7661042 1.1545486 5.1871285 6.5450854 7.02224 4.2115836 1.2897859 -1.1728425 -5.5087981 -7.7540474 -6.9787521][-8.0963354 -7.3619814 -6.4646306 -3.9827471 -0.98457432 2.4967122 5.44733 6.2879672 6.3381133 4.224153 1.3253727 -1.5511971 -5.2543631 -6.5843296 -5.565907][-5.5141172 -4.6403513 -4.2607574 -2.0775683 -0.34546757 1.8522582 3.1529765 4.4988704 4.9900823 3.160974 0.93781567 -2.3945138 -6.1293941 -7.4871163 -6.0947919][-4.6754608 -3.3340347 -1.6092901 -0.53039885 -0.29624891 0.62281179 1.7968364 2.0339203 0.95366859 -0.3954587 -1.5592408 -3.4545908 -6.2059026 -6.8707 -6.1813931][-7.6801562 -5.3862057 -4.9275851 -4.6295938 -4.6374021 -3.2829578 -0.93159962 -0.48434305 -2.2805445 -4.0947895 -4.513669 -6.0228066 -8.3817472 -8.84193 -6.6995258][-10.220711 -8.3851223 -7.775259 -7.4328837 -7.8227549 -6.6081805 -5.1104574 -4.4311466 -5.2775168 -5.943151 -5.5789886 -5.9121437 -6.7919021 -7.3053188 -6.760169][-9.95981 -9.8471384 -9.0371418 -8.63017 -7.7633238 -6.1777472 -5.2414632 -4.6554852 -5.0013051 -5.7068877 -5.7607 -5.9853926 -6.6125789 -6.6452942 -5.2899723][-9.6968794 -8.8562975 -8.9295139 -8.2129507 -7.6080217 -6.3593988 -5.7693133 -5.2834826 -5.5391011 -5.5036225 -5.896277 -5.4051504 -5.4896293 -5.6719804 -6.0758586][-6.8933411 -6.1308203 -5.953671 -5.8646622 -5.1645408 -4.3062987 -4.011313 -4.513082 -5.2505732 -6.1634812 -6.4079347 -7.7587018 -8.8114958 -9.0966854 -8.034441]]...]
INFO - root - 2017-12-16 00:48:42.290804: step 78610, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 46h:37m:04s remains)
INFO - root - 2017-12-16 00:48:48.854241: step 78620, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 45h:21m:25s remains)
INFO - root - 2017-12-16 00:48:55.538475: step 78630, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 47h:53m:42s remains)
INFO - root - 2017-12-16 00:49:02.162026: step 78640, loss = 0.21, batch loss = 0.16 (12.2 examples/sec; 0.656 sec/batch; 46h:15m:45s remains)
INFO - root - 2017-12-16 00:49:08.734111: step 78650, loss = 0.20, batch loss = 0.16 (12.5 examples/sec; 0.638 sec/batch; 44h:57m:45s remains)
INFO - root - 2017-12-16 00:49:15.445046: step 78660, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 46h:14m:49s remains)
INFO - root - 2017-12-16 00:49:21.979653: step 78670, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 45h:44m:54s remains)
INFO - root - 2017-12-16 00:49:28.578015: step 78680, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 47h:18m:40s remains)
INFO - root - 2017-12-16 00:49:35.221720: step 78690, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 47h:11m:50s remains)
INFO - root - 2017-12-16 00:49:41.783051: step 78700, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 45h:42m:27s remains)
2017-12-16 00:49:42.299253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5261407 -7.5273924 -7.805933 -8.2190981 -9.8131027 -10.929351 -12.063669 -12.28405 -11.669895 -11.191536 -10.592484 -12.110209 -12.102854 -11.420389 -9.7152882][-9.2558727 -9.35994 -9.2980633 -9.4108591 -10.366669 -11.527378 -12.984915 -13.968336 -14.242014 -13.140823 -12.008849 -13.921509 -14.121374 -13.569099 -11.143828][-7.4021568 -9.0699787 -10.526237 -10.657179 -11.532686 -12.341272 -13.305235 -13.26186 -12.955212 -12.870782 -13.079958 -14.591177 -14.544485 -14.338648 -13.112654][-8.4582081 -9.5043144 -11.315672 -11.582225 -11.852259 -10.822979 -9.9868946 -11.038496 -12.051144 -11.181402 -10.837505 -13.894814 -15.061306 -14.483635 -12.127779][-10.850508 -13.067783 -14.214563 -12.724607 -11.477123 -6.9908857 -4.4494119 -6.7294731 -9.5473232 -10.140211 -10.810154 -12.888103 -13.037683 -13.665845 -13.072937][-12.937512 -13.987253 -14.892317 -12.536475 -10.082698 -3.8669207 2.590075 2.1158376 -1.7594333 -6.1530914 -9.8699741 -11.414154 -12.153097 -12.96496 -12.258736][-14.934193 -14.675169 -13.256279 -9.2357655 -6.1553116 -0.083059311 6.2886033 8.1915436 7.0557227 -0.40847731 -8.74413 -11.856452 -12.016159 -12.188932 -10.809143][-14.878189 -14.88686 -13.732422 -8.2728434 -2.5821946 3.3414245 7.8035369 8.3066006 8.4599171 3.5967107 -4.33294 -10.113091 -13.193548 -13.186874 -11.358332][-12.549902 -12.291302 -13.100725 -10.252098 -5.3515482 1.7889261 6.9851651 6.6374726 5.5726523 0.71311951 -4.556993 -9.66519 -13.286581 -14.54933 -13.132004][-11.223173 -10.876249 -12.186954 -10.155675 -8.1280212 -3.8501189 1.4931622 2.9056554 1.8401256 -2.9125857 -7.9695425 -11.525796 -13.303823 -14.417652 -14.90023][-14.184944 -13.501886 -14.412554 -12.241944 -11.463301 -9.234087 -6.7161283 -5.1193562 -4.96684 -7.2155 -10.369934 -14.41979 -16.267698 -15.498732 -13.875708][-18.429808 -17.270481 -17.766121 -16.804594 -15.737738 -14.228016 -12.705396 -12.233316 -11.880924 -12.560252 -13.597076 -15.288107 -15.157154 -15.510101 -13.003395][-17.697655 -17.133223 -17.935349 -17.044527 -16.815338 -15.468943 -14.256475 -13.901043 -13.989058 -13.694726 -13.164959 -13.861723 -14.429777 -13.445542 -10.716371][-13.92186 -12.999586 -13.005293 -10.964986 -11.616902 -12.988049 -12.741282 -11.795073 -11.07062 -11.634315 -11.853824 -11.478796 -10.466271 -9.7928123 -9.2763653][-8.5728 -7.7664185 -6.613163 -5.3063774 -5.244998 -5.6562085 -6.9291749 -7.6125584 -7.9413834 -7.605175 -7.5887575 -9.5730619 -10.349819 -10.618616 -9.3800583]]...]
INFO - root - 2017-12-16 00:49:48.899075: step 78710, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 46h:45m:53s remains)
INFO - root - 2017-12-16 00:49:55.504019: step 78720, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.634 sec/batch; 44h:42m:59s remains)
INFO - root - 2017-12-16 00:50:02.084406: step 78730, loss = 0.11, batch loss = 0.07 (12.4 examples/sec; 0.644 sec/batch; 45h:25m:53s remains)
INFO - root - 2017-12-16 00:50:08.760186: step 78740, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 44h:52m:32s remains)
INFO - root - 2017-12-16 00:50:15.318154: step 78750, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 45h:47m:04s remains)
INFO - root - 2017-12-16 00:50:21.895076: step 78760, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 47h:37m:10s remains)
INFO - root - 2017-12-16 00:50:28.482193: step 78770, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 45h:59m:51s remains)
INFO - root - 2017-12-16 00:50:35.043157: step 78780, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 46h:42m:00s remains)
INFO - root - 2017-12-16 00:50:41.687078: step 78790, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.636 sec/batch; 44h:50m:22s remains)
INFO - root - 2017-12-16 00:50:48.247100: step 78800, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 47h:32m:24s remains)
2017-12-16 00:50:48.737382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3059411 -4.6223912 -4.7213407 -3.9929914 -3.5272768 -3.2860408 -3.1967313 -2.549674 -1.8598504 -1.9055262 -1.8510671 -4.7755566 -7.544353 -8.4061489 -8.3642235][-4.4103093 -4.4624224 -4.1506963 -3.4151468 -2.6262109 -2.3384652 -1.7443473 -0.94531107 0.01499033 0.51878834 0.33428669 -3.467526 -6.4087887 -7.9290867 -8.6031475][-2.7213595 -4.0105047 -4.8675938 -3.4155135 -2.9753947 -2.2477329 -1.2701244 -0.5959487 0.16797304 -0.082871914 -0.18693447 -3.1830139 -5.6894889 -7.3631163 -8.2161312][-4.418952 -4.9955249 -5.0473547 -3.7988081 -4.294899 -2.9403915 -1.6481242 -1.175065 -0.77152586 -0.26195431 -0.33263254 -3.8400693 -6.3329978 -7.3392091 -7.5788717][-4.8796854 -7.0806141 -7.530921 -6.0471573 -5.055109 -2.4584622 -1.0127335 -1.1499491 -1.5887461 -0.86676741 -0.8401022 -3.7134202 -6.1109667 -7.6673536 -7.9400892][-5.6878114 -6.7639275 -6.1384244 -4.0359144 -2.2214847 0.45617104 2.7865672 1.9723363 1.2128186 -0.040017128 -1.5573759 -3.5631881 -5.7019682 -7.0543222 -7.2691522][-7.3078289 -7.1071124 -5.4615645 -2.7253823 -0.61521769 2.6826549 5.6232104 5.017714 3.8784108 1.2888842 -1.036561 -3.7075746 -6.343576 -7.2547579 -7.9386978][-7.8809824 -7.1344132 -5.9739962 -2.4423406 0.7344265 4.1989608 6.5604081 5.5435538 4.8155637 2.6659427 0.584641 -3.4166632 -6.5268149 -7.5784349 -8.5885735][-6.738019 -6.2851086 -5.8492932 -2.9633646 -0.59665489 1.7897177 3.4189391 3.407208 3.2359452 1.4117017 -0.14696455 -4.0380626 -6.5719624 -7.4835925 -8.038763][-6.2892485 -5.503016 -4.6387739 -2.5789037 -1.7494187 -0.39644384 1.3529963 1.7086253 0.78701258 -0.64564991 -1.9058709 -5.1643472 -7.1530762 -7.847003 -8.62907][-9.1041546 -8.5951605 -7.5425677 -5.0111041 -5.0217214 -4.3345604 -3.5312469 -3.2010679 -3.3702571 -3.4693701 -4.2438745 -7.816412 -9.2898645 -9.0725937 -8.721283][-11.984345 -11.474067 -10.284698 -8.5176735 -8.0781107 -7.0910373 -6.5756512 -7.0368791 -7.2047453 -6.8561792 -7.0250545 -8.7864866 -9.2334328 -9.5098667 -9.5634689][-12.392012 -12.020115 -10.61195 -9.7009277 -9.9940271 -8.22601 -8.4212189 -9.1279516 -9.3026352 -9.0539274 -8.699481 -9.7119732 -9.885973 -9.18746 -8.5838518][-11.046041 -10.818495 -9.2713518 -8.4879894 -8.3564978 -8.3464756 -8.0865021 -7.8798838 -8.4147587 -9.1109095 -9.3331871 -9.0448685 -8.2086258 -8.1994972 -8.2834015][-7.8835182 -8.1906281 -7.3098564 -6.8296146 -6.3352051 -5.7850914 -5.7640948 -5.9702349 -6.5076394 -6.6382957 -7.5315809 -8.4939976 -8.8622112 -9.0655918 -8.9335251]]...]
INFO - root - 2017-12-16 00:50:55.395789: step 78810, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 46h:36m:34s remains)
INFO - root - 2017-12-16 00:51:02.098884: step 78820, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 45h:31m:07s remains)
INFO - root - 2017-12-16 00:51:08.678942: step 78830, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 45h:07m:32s remains)
INFO - root - 2017-12-16 00:51:15.245161: step 78840, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 45h:44m:31s remains)
INFO - root - 2017-12-16 00:51:21.850807: step 78850, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 45h:36m:18s remains)
INFO - root - 2017-12-16 00:51:28.411804: step 78860, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 46h:16m:17s remains)
INFO - root - 2017-12-16 00:51:35.008656: step 78870, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 47h:49m:10s remains)
INFO - root - 2017-12-16 00:51:41.538932: step 78880, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 46h:42m:11s remains)
INFO - root - 2017-12-16 00:51:48.150080: step 78890, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 46h:00m:44s remains)
INFO - root - 2017-12-16 00:51:54.753019: step 78900, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 45h:24m:07s remains)
2017-12-16 00:51:55.263958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8875508 -4.9332 -4.9632697 -4.8694239 -5.4631205 -6.1131182 -7.5217495 -7.6201811 -7.7541647 -8.0725021 -8.9768505 -9.6418934 -11.120417 -7.4069929 -4.5208244][-4.979382 -4.440412 -3.7281604 -3.701663 -3.585613 -4.1519918 -3.5879419 -4.545301 -5.1303482 -5.6599436 -5.54491 -6.49095 -8.3984613 -5.7444592 -3.2905014][-4.703557 -4.6289635 -4.4543929 -2.7914238 -3.4797211 -4.1591725 -4.244031 -3.9827671 -3.4862418 -4.6425004 -3.3311741 -4.8569808 -6.6660776 -4.8092313 -2.7688675][-6.118197 -5.9298048 -5.4493423 -4.5170884 -4.3898497 -4.153183 -3.6478469 -3.705646 -3.7568154 -3.4577889 -3.2116723 -3.6661909 -5.071506 -4.0968347 -2.0272388][-6.1263485 -6.617291 -7.2168379 -6.1173563 -5.0826664 -4.0772133 -3.3858151 -3.6978855 -2.6218324 -2.3654685 -1.4039063 -3.0418439 -5.3613257 -3.9631376 -3.0674202][-7.6963873 -5.872982 -4.5282946 -2.8277776 -1.4767194 0.1354866 1.3028989 0.9439249 -0.88957644 -1.437367 -1.6669073 -2.623491 -4.8605204 -4.3777881 -3.6757486][-6.6132207 -5.8005724 -3.7590151 0.37295914 2.1328635 3.289185 4.7262836 4.1559253 3.2844558 0.9365983 -0.78161907 -2.4009097 -5.5930142 -4.6685615 -3.43889][-4.7594967 -3.6235259 -3.1941762 0.60160208 4.1126695 5.5164542 6.6128488 5.9373565 4.9839664 2.9337211 1.0034838 -1.344418 -3.6980736 -3.0113919 -1.9276562][-5.5513048 -3.5598137 -1.7474771 0.12757254 1.6668324 3.6048961 5.0986381 4.3666635 3.5320096 2.3647056 0.67721987 -1.8096728 -4.6810017 -3.3234711 -1.6432314][-7.76791 -5.4677796 -3.6669884 -0.86503744 -0.46153545 1.000062 2.5699506 2.1412339 0.87690878 -0.19053698 -0.37981319 -2.2663782 -5.6371093 -4.6381779 -3.1315699][-8.4069118 -7.4301486 -6.5896344 -3.9277313 -2.9406064 -1.7968001 -1.707376 -1.5905204 -1.971859 -3.0489845 -3.5582054 -5.2306576 -6.689724 -6.7836094 -5.5395279][-10.787115 -9.463706 -8.4683037 -6.7636976 -6.36536 -5.2862167 -5.2981811 -6.0054474 -6.105021 -6.3491883 -7.2790351 -6.9218163 -7.1524911 -6.492969 -5.2807889][-11.943699 -9.866621 -7.9849315 -6.7511673 -6.7693911 -6.0548849 -6.8130317 -7.1600695 -7.3723936 -7.7765126 -7.8222151 -6.903132 -6.9999571 -5.8649035 -4.0694952][-8.5886135 -7.6253147 -5.9887223 -4.7457571 -4.3192835 -4.5094833 -4.5388045 -5.3449535 -6.2982206 -5.78251 -5.7802496 -4.3575077 -4.3409052 -3.6301138 -2.0960867][-6.6214876 -5.931994 -4.9699969 -3.8454103 -2.2045257 -2.5094194 -2.404557 -2.4073977 -2.648273 -3.2403743 -3.2623444 -2.5333972 -3.21639 -3.468971 -3.4852908]]...]
INFO - root - 2017-12-16 00:52:01.870191: step 78910, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 45h:33m:48s remains)
INFO - root - 2017-12-16 00:52:08.463057: step 78920, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 47h:45m:57s remains)
INFO - root - 2017-12-16 00:52:15.061624: step 78930, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 47h:01m:32s remains)
INFO - root - 2017-12-16 00:52:21.651909: step 78940, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 46h:33m:49s remains)
INFO - root - 2017-12-16 00:52:28.249034: step 78950, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 46h:53m:20s remains)
INFO - root - 2017-12-16 00:52:34.766001: step 78960, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 46h:19m:54s remains)
INFO - root - 2017-12-16 00:52:41.379229: step 78970, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 46h:59m:58s remains)
INFO - root - 2017-12-16 00:52:47.927762: step 78980, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 45h:31m:04s remains)
INFO - root - 2017-12-16 00:52:54.507410: step 78990, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 47h:35m:55s remains)
INFO - root - 2017-12-16 00:53:01.024717: step 79000, loss = 0.19, batch loss = 0.14 (12.5 examples/sec; 0.642 sec/batch; 45h:12m:54s remains)
2017-12-16 00:53:01.593781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.6952229 -6.98005 -4.8971834 -1.6515255 0.30959749 -1.1529083 -2.8952615 -4.3138866 -5.6797385 -7.2059774 -7.1188345 -8.9312725 -11.673429 -12.225758 -9.7996807][-6.5040045 -6.3724866 -7.1172733 -4.5600886 -3.0736523 -2.1023085 -1.1282496 -2.5924349 -4.4142895 -6.2560453 -8.1450939 -10.320217 -11.217643 -13.213772 -11.161177][-4.5850248 -6.5783973 -7.5810828 -5.1221838 -4.810122 -3.0734911 -2.0583689 -2.3080096 -3.6113753 -4.3873749 -4.9844089 -8.0864048 -10.154325 -11.331696 -10.472986][-4.5167527 -6.5051546 -7.7593794 -7.5364771 -7.5178242 -4.8404393 -1.8275476 -0.48635149 -1.8179202 -4.6726785 -8.0471516 -11.266174 -11.782668 -12.713551 -10.770623][-5.6888933 -7.1697488 -7.9133353 -7.3039379 -5.4899879 -1.9258351 -0.28189278 -0.924315 -2.124305 -3.1939876 -5.8819327 -10.722178 -13.133293 -14.964506 -13.188673][-7.2569733 -6.1839638 -6.1132207 -5.7847705 -3.0460212 2.6392951 7.8252578 7.2609315 2.865108 -2.4360418 -6.38052 -9.854744 -12.320448 -14.812685 -13.821857][-8.3038568 -8.0059891 -7.9349737 -4.8702736 -1.5471973 4.5654626 9.5276051 10.622536 9.1352654 1.4485435 -5.0574703 -7.4306393 -9.5502434 -12.615576 -13.07218][-8.6882629 -10.076221 -10.095139 -5.0304236 -0.18226528 6.6748261 11.808199 10.95859 9.49539 3.0002341 -3.3318288 -8.4228458 -10.469887 -10.864746 -10.184214][-7.6817689 -9.7454224 -8.86274 -5.7504129 -2.9840658 3.7606912 8.5563755 9.1848774 7.4645057 0.0097455978 -4.3743529 -9.2076 -12.390811 -13.496131 -11.60165][-6.9991822 -7.9784355 -7.8982658 -5.6503611 -3.3318596 0.69761562 2.9594016 4.0000663 2.4470029 -3.4454293 -9.2231674 -14.431974 -16.218607 -16.028782 -13.825058][-9.8939152 -11.188924 -9.9676666 -7.7874804 -6.5777235 -4.3692651 -3.935729 -3.4589734 -5.2308393 -8.6401072 -11.79524 -15.362677 -18.252613 -18.373619 -15.533165][-14.465847 -14.907475 -14.364664 -11.540539 -7.8358727 -6.417491 -6.9753857 -7.9973392 -9.7787495 -12.122425 -13.770948 -16.088488 -16.22353 -15.240076 -13.980255][-15.891407 -15.450195 -14.005556 -12.7565 -11.787952 -10.613056 -9.6038485 -10.256502 -11.052748 -11.338514 -11.504727 -12.5068 -13.192423 -11.818453 -9.6883564][-12.67048 -11.71939 -11.521681 -11.283115 -10.905116 -10.103086 -9.6856136 -9.7621536 -9.9673376 -10.347466 -10.271107 -8.4487457 -8.2329578 -7.6806951 -7.6944232][-6.3142557 -6.8796053 -5.875916 -5.1113553 -6.038763 -7.4615593 -7.9475083 -6.4942446 -6.4414005 -7.121686 -6.8809271 -7.2854266 -8.614048 -7.5626554 -7.1670861]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 00:53:08.207392: step 79010, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 45h:57m:52s remains)
INFO - root - 2017-12-16 00:53:14.816555: step 79020, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 46h:13m:53s remains)
INFO - root - 2017-12-16 00:53:21.385676: step 79030, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 46h:25m:58s remains)
INFO - root - 2017-12-16 00:53:28.035588: step 79040, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 44h:53m:27s remains)
INFO - root - 2017-12-16 00:53:34.624767: step 79050, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 46h:49m:21s remains)
INFO - root - 2017-12-16 00:53:41.204767: step 79060, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 45h:16m:18s remains)
INFO - root - 2017-12-16 00:53:47.796418: step 79070, loss = 0.11, batch loss = 0.06 (12.5 examples/sec; 0.638 sec/batch; 44h:56m:51s remains)
INFO - root - 2017-12-16 00:53:54.448743: step 79080, loss = 0.20, batch loss = 0.16 (12.3 examples/sec; 0.649 sec/batch; 45h:42m:26s remains)
INFO - root - 2017-12-16 00:54:01.017119: step 79090, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 46h:27m:42s remains)
INFO - root - 2017-12-16 00:54:07.691991: step 79100, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.690 sec/batch; 48h:33m:19s remains)
2017-12-16 00:54:08.243287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.401202 -3.2362216 -3.8059211 -2.8933625 -3.1015594 -3.9724488 -5.236608 -6.9191532 -8.2861729 -8.6817923 -8.5342484 -9.1714277 -10.703581 -11.846077 -10.886669][-1.832258 -2.5654867 -2.5766563 -1.397202 -1.3590155 -2.0723114 -3.3005531 -5.097661 -6.8853688 -7.1804929 -6.37132 -7.2809711 -9.4052658 -9.7787037 -9.7903633][-0.15529537 -1.7155294 -3.6760416 -2.6563225 -2.4502683 -2.3773263 -2.6315544 -3.6537411 -4.7939105 -5.2426457 -5.1159163 -6.3393841 -8.2422848 -9.7672453 -9.6246529][-4.848218 -4.6283789 -4.6430707 -4.4209852 -3.926991 -3.4892094 -3.1723797 -3.3971078 -4.2208242 -4.50938 -4.1921163 -6.1005325 -7.7420254 -8.1546488 -7.9323606][-5.5485959 -6.5572143 -7.1675091 -5.4317441 -3.45951 -1.0824585 0.28111935 -1.6896777 -3.3133593 -3.4727924 -3.82128 -5.0672679 -7.0341239 -8.0921984 -7.9811144][-7.0850348 -7.707963 -7.21645 -4.6906867 -1.8389647 1.4838614 3.8255315 2.5664678 1.0753365 -0.65923405 -2.0385249 -3.3742747 -5.4800286 -6.3338132 -5.4354467][-6.3654375 -6.8058105 -6.25838 -3.4136555 -1.0736246 2.9161248 6.2713704 6.0125918 4.6893344 1.3661056 -1.4174733 -3.2371633 -5.154923 -6.4724178 -6.6149168][-5.5773115 -5.0463223 -4.4512496 -2.0595365 0.031754494 3.3262706 6.1750026 6.5933776 5.9707923 2.9283433 -0.083525181 -2.822947 -4.9380808 -6.1527472 -5.64886][-3.823643 -3.1590106 -3.1915371 -2.2634764 -0.94091845 1.9087987 4.1802459 5.2771 5.0765853 3.4572682 1.0743852 -3.370064 -6.9519882 -7.7474074 -7.1927552][-5.9287724 -6.0900183 -5.0210667 -3.6932659 -3.3529418 -1.6878948 0.45313311 2.1005054 2.2040691 0.12317705 -2.5182195 -5.8181729 -8.332365 -8.7682772 -7.4959836][-11.765512 -11.823147 -10.941542 -8.3227539 -6.8863635 -5.4804935 -4.0558605 -3.6295443 -3.6435263 -4.0683231 -5.6315236 -9.0740089 -10.799341 -9.9451323 -8.1190891][-14.867746 -15.076073 -13.6341 -11.191416 -9.7251663 -7.7186565 -6.5786362 -6.314714 -6.2162623 -7.1434507 -8.2029095 -9.3633909 -9.8239594 -9.7553806 -8.021018][-13.982031 -13.208292 -11.697827 -10.358461 -9.52727 -8.2612953 -8.1914425 -8.312561 -8.5170689 -8.2153425 -8.4624252 -9.1961994 -9.2053366 -7.9720707 -6.5280342][-11.066839 -10.343721 -9.03049 -7.85503 -6.6103745 -6.6970792 -7.0138626 -6.8948283 -7.6794538 -8.5386248 -8.6457872 -7.5072794 -7.194293 -6.7055521 -4.9158163][-6.9336662 -6.3246422 -5.3700266 -4.1090851 -4.1076608 -3.6159663 -3.6302235 -4.9115205 -6.0127854 -6.207345 -6.4628391 -7.0108981 -6.9955292 -6.4807577 -6.3039918]]...]
INFO - root - 2017-12-16 00:54:14.825709: step 79110, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 47h:19m:51s remains)
INFO - root - 2017-12-16 00:54:21.337965: step 79120, loss = 0.20, batch loss = 0.16 (12.6 examples/sec; 0.636 sec/batch; 44h:46m:35s remains)
INFO - root - 2017-12-16 00:54:27.942109: step 79130, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.673 sec/batch; 47h:20m:39s remains)
INFO - root - 2017-12-16 00:54:34.545298: step 79140, loss = 0.18, batch loss = 0.13 (11.6 examples/sec; 0.689 sec/batch; 48h:28m:03s remains)
INFO - root - 2017-12-16 00:54:41.106019: step 79150, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 45h:00m:38s remains)
INFO - root - 2017-12-16 00:54:47.694523: step 79160, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 45h:21m:20s remains)
INFO - root - 2017-12-16 00:54:54.296947: step 79170, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 46h:21m:27s remains)
INFO - root - 2017-12-16 00:55:00.787131: step 79180, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.636 sec/batch; 44h:44m:17s remains)
INFO - root - 2017-12-16 00:55:07.447467: step 79190, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 45h:15m:55s remains)
INFO - root - 2017-12-16 00:55:14.063567: step 79200, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.680 sec/batch; 47h:48m:45s remains)
2017-12-16 00:55:14.635574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1256039 -1.1199374 0.45188332 2.0554495 2.0026441 1.0715709 -0.37314939 -1.4277282 -1.9202075 -2.05225 -1.8488879 -3.3322172 -3.3888247 -4.5021982 -3.9900055][-1.9190624 -0.44688606 1.0777164 1.9621792 1.5760045 0.61382914 -0.39732075 -1.263113 -1.5235662 -1.1984105 -0.45558691 -1.68788 -1.8687093 -2.6430218 -1.9851894][-0.64057636 0.055708885 0.44622135 1.3655705 0.578104 -0.30450869 -0.97510529 -1.3783698 -1.0450702 -0.40115261 0.4372015 -1.3863425 -2.009825 -3.2342033 -3.3036487][-1.7700987 -1.2625866 -0.83958387 -0.41274595 -1.5022426 -1.8079047 -1.6860404 -1.3763242 -0.53662634 0.49840212 1.2368383 -1.2866187 -2.474174 -4.4429073 -4.6335588][-2.9611702 -3.065851 -2.9754846 -1.8741415 -2.1411951 -1.7901218 -1.0490041 -0.090202808 0.69898558 1.4702268 1.6819658 -1.1363316 -2.8392613 -5.042326 -5.2129455][-4.305275 -3.3068256 -2.7102802 -1.4867368 -1.1138687 -0.28179073 0.72863626 1.6596498 2.4474397 2.44494 1.6916213 -1.1585312 -3.064096 -5.3653841 -5.3230371][-5.7059565 -4.2014961 -2.1846826 -0.58119011 0.38602972 1.613843 3.1069903 3.6672616 4.114017 3.5316463 2.1254239 -1.4585643 -3.7936335 -6.0284228 -5.9613156][-4.9248414 -4.2112746 -2.4960046 -0.23697329 0.49771404 1.6973705 2.8270211 3.0871139 3.687222 3.23072 1.9064922 -2.0998023 -4.7229862 -7.3304582 -7.5675673][-4.8468642 -3.9697466 -2.6520898 -0.57165384 0.034061909 1.3335276 2.2131076 2.4920564 2.525847 1.7723722 0.96570444 -2.8140061 -5.3226042 -7.7545776 -7.8341861][-4.4854746 -3.471314 -2.6015604 -0.76513386 0.088398933 0.557065 0.94627 1.3594875 1.6133838 0.97180033 0.043389797 -3.5666196 -5.9781389 -8.7028522 -8.9014263][-7.4643478 -6.0944281 -4.6939263 -3.0677309 -1.9905136 -2.1897502 -1.9753964 -2.1614895 -2.3456247 -2.5683227 -2.9166934 -6.0707273 -8.1920414 -9.9789152 -9.39946][-10.905661 -9.4709272 -7.2748308 -5.5252485 -5.0781522 -5.06318 -5.5288305 -5.8924894 -5.9202743 -5.8788538 -6.0660152 -7.6282539 -8.25586 -9.6147041 -8.7388744][-11.283155 -9.4329863 -7.5651989 -6.1402211 -6.551074 -6.6681876 -7.0593863 -7.2200432 -7.5354257 -7.8401814 -7.9641209 -8.0144682 -8.0164461 -8.22595 -6.1797786][-9.8110571 -7.9693775 -7.1302943 -5.7794995 -5.3882818 -6.0497694 -6.9001589 -7.1504278 -6.749845 -6.5952077 -6.85841 -6.6344709 -6.2510014 -5.4706082 -4.1422157][-6.3409195 -5.4386773 -4.5949831 -3.8686771 -3.8149123 -4.02355 -4.0105257 -4.4420619 -4.502409 -4.411046 -4.366261 -5.0674844 -5.6449575 -5.0042481 -4.5855875]]...]
INFO - root - 2017-12-16 00:55:21.269824: step 79210, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.675 sec/batch; 47h:30m:37s remains)
INFO - root - 2017-12-16 00:55:27.804075: step 79220, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.674 sec/batch; 47h:25m:48s remains)
INFO - root - 2017-12-16 00:55:34.405772: step 79230, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 46h:40m:24s remains)
INFO - root - 2017-12-16 00:55:41.121820: step 79240, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.684 sec/batch; 48h:05m:51s remains)
INFO - root - 2017-12-16 00:55:47.685852: step 79250, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 46h:28m:38s remains)
INFO - root - 2017-12-16 00:55:54.281949: step 79260, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 46h:36m:33s remains)
INFO - root - 2017-12-16 00:56:00.883816: step 79270, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 45h:50m:25s remains)
INFO - root - 2017-12-16 00:56:07.460362: step 79280, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 45h:43m:41s remains)
INFO - root - 2017-12-16 00:56:14.194354: step 79290, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.670 sec/batch; 47h:06m:20s remains)
INFO - root - 2017-12-16 00:56:20.860151: step 79300, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 45h:21m:59s remains)
2017-12-16 00:56:21.384585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3756804 -8.1777287 -8.2060986 -6.9529915 -6.9285822 -7.3287754 -8.378212 -9.368372 -9.469883 -8.36455 -6.9279213 -8.009881 -9.2178059 -9.9320641 -6.9670129][-7.4960351 -7.3174028 -6.5412464 -5.7932334 -6.3534403 -7.1626048 -7.5125856 -7.9174223 -8.5420847 -8.0763168 -7.4680743 -7.7068467 -8.0216045 -9.8156528 -6.9674587][-6.5996466 -8.337697 -7.798007 -5.9311323 -6.3308754 -6.5121727 -7.0859756 -7.6491947 -7.6892567 -7.4830146 -7.49633 -9.0796967 -9.77059 -11.499569 -9.2102938][-8.1819553 -8.0714216 -7.5272069 -6.2687874 -6.3956046 -5.7158484 -5.5478206 -5.9036946 -5.7618637 -5.8440123 -6.6430583 -8.65938 -10.586269 -11.928352 -9.59701][-9.3494034 -10.36693 -10.003731 -8.0299 -6.878449 -4.303061 -2.4160476 -3.1941121 -4.2550287 -4.5076141 -5.8821635 -7.9672966 -9.3484764 -12.309569 -9.879303][-9.9912243 -10.374615 -9.4295731 -6.8632469 -4.665019 -1.0467429 1.9700313 2.7958493 2.0904837 -0.64834261 -3.2466278 -5.3230681 -7.3972821 -9.7741261 -7.8462892][-11.107611 -10.012411 -8.2817421 -5.2690911 -3.2475579 1.4614906 6.0726895 6.682445 5.4016089 1.9275179 -1.8747616 -4.9501991 -7.5146322 -9.6959124 -7.327404][-10.313076 -9.5612192 -7.5846019 -3.2990925 -0.28311872 2.7355752 6.3795543 7.2530808 7.3387647 3.9550653 -0.49163437 -4.6263652 -7.266057 -9.554184 -7.6815929][-8.3101559 -7.0913172 -5.8499484 -3.3436062 -1.7931991 1.9889126 3.6058269 3.5611358 4.0075507 2.0104833 -1.5882144 -5.8943295 -8.7475338 -10.956745 -9.89369][-6.9462223 -5.2511411 -4.1036272 -2.1721487 -1.9054444 0.084775448 0.85055828 0.16015577 -0.69839287 -3.4866631 -5.4384022 -8.0487623 -10.046965 -12.681362 -11.235657][-10.810484 -9.7849751 -8.5774632 -6.3923936 -6.7844825 -5.9334335 -4.8510065 -4.5222363 -5.878592 -8.5828981 -10.785938 -12.755501 -13.376216 -14.914776 -12.815649][-13.562216 -12.904428 -12.504633 -11.009234 -10.680595 -10.437765 -10.418094 -9.3674717 -9.2296619 -11.160374 -12.375591 -13.151001 -13.702641 -14.743029 -12.1214][-13.867271 -14.207727 -13.99173 -13.04875 -14.101406 -12.669392 -11.706197 -11.401711 -11.366329 -11.922716 -12.195566 -12.318523 -10.971909 -11.142372 -8.0974636][-12.028888 -11.27079 -11.312252 -9.0867691 -8.4585018 -8.22409 -9.05364 -8.5475311 -8.7796087 -8.9933081 -8.917963 -8.1957226 -7.8315778 -6.8546343 -4.4817853][-7.9999456 -8.19051 -7.5928197 -6.5832129 -5.3759413 -3.788691 -4.1460919 -4.1803865 -4.532753 -5.1710682 -5.4873314 -5.6761832 -6.1490917 -6.1443644 -4.4882722]]...]
INFO - root - 2017-12-16 00:56:28.049998: step 79310, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 47h:02m:09s remains)
INFO - root - 2017-12-16 00:56:34.621522: step 79320, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 46h:13m:37s remains)
INFO - root - 2017-12-16 00:56:41.272059: step 79330, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 47h:28m:14s remains)
INFO - root - 2017-12-16 00:56:47.739346: step 79340, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 46h:35m:01s remains)
INFO - root - 2017-12-16 00:56:54.382873: step 79350, loss = 0.11, batch loss = 0.06 (12.1 examples/sec; 0.664 sec/batch; 46h:40m:29s remains)
INFO - root - 2017-12-16 00:57:00.930356: step 79360, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 46h:01m:13s remains)
INFO - root - 2017-12-16 00:57:07.607773: step 79370, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 45h:49m:06s remains)
INFO - root - 2017-12-16 00:57:14.179429: step 79380, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.667 sec/batch; 46h:53m:05s remains)
INFO - root - 2017-12-16 00:57:20.764594: step 79390, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 46h:51m:16s remains)
INFO - root - 2017-12-16 00:57:27.455040: step 79400, loss = 0.29, batch loss = 0.25 (11.6 examples/sec; 0.688 sec/batch; 48h:20m:51s remains)
2017-12-16 00:57:28.020179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6833358 -4.0703907 -3.999651 -3.344223 -3.2634752 -3.5494471 -3.9821038 -4.4424472 -4.6034918 -4.8046784 -4.7599263 -6.7642641 -8.5721111 -8.59448 -9.2248859][-3.736547 -4.155302 -3.9422855 -3.7296171 -3.962204 -5.0871973 -6.475316 -7.2764788 -7.5574026 -7.5685196 -7.1266766 -8.5349874 -10.096788 -9.8680515 -9.9252625][-2.1292856 -3.8202629 -5.0195475 -5.3797126 -5.8414159 -6.3331747 -6.9748178 -8.0678139 -8.3145351 -8.0137539 -7.769166 -9.3891163 -10.940413 -10.547853 -11.044237][-3.2360179 -4.1817451 -4.669714 -5.1367207 -5.4744983 -5.6301184 -6.5073633 -7.553359 -8.1327782 -8.0580454 -7.56601 -9.0441093 -10.889742 -10.585981 -10.91747][-4.5361013 -6.2806797 -7.2109747 -6.2704082 -4.7148161 -3.5095141 -3.0462122 -4.2156148 -5.7794237 -6.22497 -6.4420652 -7.7345896 -9.0171928 -9.1513071 -9.8816643][-6.0468712 -6.9146123 -6.5945306 -4.4391503 -2.1686008 -0.1974678 1.8456588 1.1003084 -0.61168337 -2.4647527 -4.0995803 -5.6008658 -7.9589176 -8.3992777 -9.4905338][-6.4953895 -7.2537484 -6.8480587 -4.221992 -1.5754137 1.746026 4.9724011 4.9186072 4.4864326 1.7241802 -1.5781822 -4.5224595 -7.8366089 -8.5054369 -9.388876][-6.6168728 -6.77853 -6.3405218 -3.7183306 -0.876081 2.5447631 4.9390321 5.3722148 5.4468102 3.1138339 0.62714291 -3.3646023 -7.7321081 -8.6094112 -9.2607784][-6.1267457 -6.1962795 -5.3321447 -3.1935222 -1.4681516 1.6339164 3.6294084 4.5797515 5.1288905 3.54108 1.6626949 -2.9791331 -7.4647207 -7.7607956 -8.9385509][-6.4483147 -6.648828 -5.4874849 -3.3878813 -1.832587 0.17479706 1.4035959 2.7114978 2.751718 2.24396 1.9430037 -2.1241913 -6.6150188 -8.0106411 -9.729147][-9.5928364 -8.9841824 -7.1071062 -5.9137268 -4.7070208 -3.7240045 -3.3299658 -2.5561678 -2.1021535 -1.6483264 -2.0246909 -6.0908413 -8.882906 -9.3198433 -10.132654][-13.598614 -13.475181 -11.778715 -10.163996 -8.43023 -7.6857128 -7.2175179 -7.2328014 -7.1629567 -6.6150808 -6.4063654 -8.1304207 -9.7608767 -10.790428 -11.649755][-14.149391 -13.375701 -12.797194 -11.870412 -10.140598 -8.8055649 -8.33814 -8.41822 -7.8607736 -7.2111015 -6.6402149 -7.2517347 -8.01492 -8.35926 -8.5224981][-13.055075 -12.87291 -12.030603 -10.269663 -8.2215748 -7.8971896 -7.3638592 -6.840241 -6.8207979 -6.67508 -6.1966515 -5.7847753 -6.2311988 -6.8600106 -6.8579469][-8.7255974 -8.2981319 -7.6190829 -6.5177593 -4.706356 -4.0371885 -4.5051775 -5.2155323 -5.96998 -5.6684661 -5.89857 -6.684073 -7.1681232 -7.5785942 -7.7515211]]...]
INFO - root - 2017-12-16 00:57:34.621508: step 79410, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 46h:26m:03s remains)
INFO - root - 2017-12-16 00:57:41.207038: step 79420, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 47h:03m:30s remains)
INFO - root - 2017-12-16 00:57:47.850093: step 79430, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 46h:04m:43s remains)
INFO - root - 2017-12-16 00:57:54.449914: step 79440, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 48h:28m:26s remains)
INFO - root - 2017-12-16 00:58:01.052122: step 79450, loss = 0.11, batch loss = 0.07 (12.2 examples/sec; 0.658 sec/batch; 46h:14m:54s remains)
INFO - root - 2017-12-16 00:58:07.734900: step 79460, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 45h:44m:22s remains)
INFO - root - 2017-12-16 00:58:14.305860: step 79470, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 46h:03m:54s remains)
INFO - root - 2017-12-16 00:58:20.871619: step 79480, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 46h:00m:03s remains)
INFO - root - 2017-12-16 00:58:27.469984: step 79490, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 45h:07m:12s remains)
INFO - root - 2017-12-16 00:58:34.112867: step 79500, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 45h:32m:04s remains)
2017-12-16 00:58:34.643996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7004771 -7.2399063 -7.4873786 -7.6056876 -8.1544294 -8.0504789 -6.6922889 -5.1323228 -4.1118126 -3.4621689 -2.4639754 -2.888973 -5.2761807 -6.7684875 -6.77601][-6.3966622 -6.6585197 -6.0172095 -5.3196011 -5.5981321 -5.2524819 -4.7774029 -3.3487291 -2.8628521 -1.7403259 0.00072669983 -1.0163145 -3.8489604 -6.1308265 -6.6694512][-4.7080297 -4.8997173 -4.1993771 -3.1196742 -2.9797106 -3.2341917 -2.889076 -1.1165485 0.16188622 0.77433729 0.79430294 -0.98611689 -4.2417417 -7.1873059 -8.562768][-4.4152017 -4.6319356 -3.4505553 -2.2793744 -2.6055307 -2.792825 -2.8619261 -2.6034036 -2.399555 -2.0613298 -1.7547307 -3.7589891 -7.5557885 -10.191065 -10.668116][-6.324748 -6.6861296 -4.9546084 -2.9124706 -1.6752062 -1.1259546 -1.6488113 -2.2409432 -2.8508053 -2.5698919 -2.7001195 -5.4563313 -9.2900143 -11.979483 -12.294258][-7.9438362 -7.0428138 -5.0234737 -2.1429279 0.21155167 1.3195853 1.203835 0.044721127 -0.91597128 -1.8966656 -3.781888 -5.6543417 -8.9144735 -11.697514 -11.480162][-10.007785 -8.1879139 -4.7604218 -1.314558 0.51071835 1.3680921 2.6041942 2.7351918 2.3344216 0.027642727 -2.9624946 -4.8055959 -7.998929 -10.417364 -10.753216][-9.6760807 -7.6049533 -4.7946563 -1.0572085 0.41170645 1.229579 2.8152194 2.9308333 2.9610543 0.5221076 -2.3111076 -4.6222053 -8.1562214 -10.307767 -9.9925938][-7.91264 -5.7239575 -3.6743107 -1.0831871 -0.50793123 0.2644906 0.96522236 0.41139984 0.11993313 -1.0380101 -3.3909633 -6.8445935 -10.697673 -12.130209 -12.24663][-4.74624 -3.3965766 -2.8462977 -1.3706217 -1.8267832 -1.6443353 -1.2196822 -1.4867635 -2.0530066 -3.8671114 -6.2888651 -8.5918064 -12.09156 -14.483774 -15.047915][-7.612689 -7.0376744 -7.0256324 -6.2512307 -7.2144194 -7.8508921 -7.9784307 -7.8241873 -7.4154916 -8.4524632 -10.627733 -13.431726 -16.089045 -16.9281 -16.214535][-14.519045 -13.974461 -12.249822 -11.359444 -12.108967 -13.189953 -13.824047 -13.938211 -13.263802 -13.406946 -14.294523 -15.491833 -16.648886 -16.800764 -16.501125][-16.923601 -15.559612 -13.538424 -12.959745 -12.529204 -12.238546 -12.394405 -13.37846 -14.144037 -14.050526 -13.760624 -14.528761 -14.702902 -13.983269 -13.389437][-14.835762 -13.945101 -12.31675 -10.725767 -9.6672688 -9.8468971 -10.308689 -10.022852 -10.145681 -10.860807 -11.76753 -11.622396 -11.221583 -11.058092 -10.620857][-10.96126 -10.490923 -9.2150326 -7.9591665 -6.2471151 -5.6151037 -5.6283092 -5.9417825 -6.6887012 -7.0968437 -7.8391562 -9.3733177 -10.061737 -10.374268 -9.9502373]]...]
INFO - root - 2017-12-16 00:58:41.249431: step 79510, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.653 sec/batch; 45h:53m:08s remains)
INFO - root - 2017-12-16 00:58:47.822833: step 79520, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 46h:17m:59s remains)
INFO - root - 2017-12-16 00:58:54.434367: step 79530, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.663 sec/batch; 46h:36m:44s remains)
INFO - root - 2017-12-16 00:59:01.016170: step 79540, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 46h:35m:39s remains)
INFO - root - 2017-12-16 00:59:07.660345: step 79550, loss = 0.19, batch loss = 0.15 (11.7 examples/sec; 0.683 sec/batch; 48h:01m:14s remains)
INFO - root - 2017-12-16 00:59:14.226123: step 79560, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 45h:36m:57s remains)
INFO - root - 2017-12-16 00:59:20.773334: step 79570, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 47h:01m:02s remains)
INFO - root - 2017-12-16 00:59:27.376842: step 79580, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 46h:14m:09s remains)
INFO - root - 2017-12-16 00:59:33.918811: step 79590, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 47h:13m:03s remains)
INFO - root - 2017-12-16 00:59:40.561745: step 79600, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 46h:24m:36s remains)
2017-12-16 00:59:41.073786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7518787 -7.0442209 -7.6954336 -7.6019878 -7.8405352 -7.9858613 -6.6428313 -6.2219434 -5.5375886 -4.773294 -4.1538429 -7.2573214 -9.1864729 -11.827561 -13.040928][-3.2652409 -2.7172496 -3.4447088 -4.4668121 -5.2461495 -6.506279 -6.2079134 -4.9354115 -4.1660614 -3.3211737 -1.5713472 -5.0004649 -7.4851885 -11.096655 -13.246466][-1.8670585 -1.9684916 -2.9177344 -3.4656589 -5.5418739 -6.2439427 -5.7167716 -4.8203454 -2.8970935 -1.5164814 -1.5408812 -4.5118961 -8.0173254 -11.551117 -12.829294][-2.5736437 -3.292443 -4.077343 -5.09926 -6.2035909 -5.1674037 -4.3832245 -4.309443 -3.1434472 -1.5991158 -1.3641062 -5.2319307 -8.0200806 -12.69672 -14.419685][-3.9353547 -5.7461262 -7.4747486 -7.3201952 -6.7179589 -4.60922 -2.5464494 -2.4985559 -2.5006363 -2.3868606 -2.0628757 -4.6700234 -7.5327015 -11.612284 -13.927444][-5.7263975 -6.6008234 -7.9281731 -6.9869514 -5.0318556 -1.752454 1.2382793 1.6413689 0.17900991 -1.1285152 -1.7847283 -4.7297897 -6.3191385 -9.454093 -12.072258][-6.5614405 -6.9868584 -7.4176245 -5.3560691 -2.2644079 1.3300185 4.8331819 5.6592383 4.5574908 2.6654859 0.34292126 -3.8157334 -6.6015482 -9.0342073 -10.590037][-5.6594782 -6.0196886 -5.46111 -2.8039196 -0.046191692 4.6412225 8.2324982 8.6778755 8.2849674 5.7086968 2.3399224 -2.5147667 -7.4496078 -10.984026 -10.805595][-6.1969023 -5.6638846 -5.38577 -2.5550022 -0.58692789 2.216804 5.5606675 6.9830565 6.8319535 4.772965 2.5068812 -2.6835914 -7.55564 -11.607798 -13.508913][-6.690515 -6.9649839 -6.2683673 -3.5069773 -2.7896609 -0.28284168 1.8398509 2.2492151 1.6217847 0.864192 -1.0435247 -6.9631906 -11.833169 -15.767076 -17.289164][-10.682479 -11.856954 -11.761216 -9.2494841 -7.6073165 -5.8146496 -4.1821313 -3.8715096 -5.2240906 -6.1553531 -6.83377 -11.931261 -16.544882 -19.736427 -19.864172][-13.931829 -14.468452 -14.248985 -12.120602 -10.926325 -9.1573753 -9.0782623 -9.8521538 -9.87714 -11.353181 -12.760933 -14.798326 -16.314331 -18.202816 -18.644247][-18.3664 -17.692368 -17.377914 -15.524181 -14.409138 -13.593466 -12.904144 -13.547567 -14.738031 -15.575304 -14.547865 -15.152084 -14.418955 -13.533657 -12.317053][-17.65633 -17.664398 -16.402639 -16.362196 -15.435665 -13.853821 -13.447117 -13.133854 -13.52832 -13.105524 -12.457975 -12.680668 -10.839878 -11.250669 -11.171518][-13.326391 -14.444698 -14.149426 -12.034785 -11.251694 -12.637539 -12.636503 -12.689445 -12.603487 -11.405897 -10.461617 -10.51383 -10.111789 -10.057823 -10.397711]]...]
INFO - root - 2017-12-16 00:59:47.710976: step 79610, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.680 sec/batch; 47h:45m:09s remains)
INFO - root - 2017-12-16 00:59:54.313427: step 79620, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 46h:49m:44s remains)
INFO - root - 2017-12-16 01:00:00.969178: step 79630, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 46h:20m:21s remains)
INFO - root - 2017-12-16 01:00:07.563474: step 79640, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 47h:37m:39s remains)
INFO - root - 2017-12-16 01:00:14.167070: step 79650, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.671 sec/batch; 47h:09m:44s remains)
INFO - root - 2017-12-16 01:00:20.814902: step 79660, loss = 0.19, batch loss = 0.15 (11.8 examples/sec; 0.678 sec/batch; 47h:35m:19s remains)
INFO - root - 2017-12-16 01:00:27.462814: step 79670, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.664 sec/batch; 46h:38m:43s remains)
INFO - root - 2017-12-16 01:00:34.097953: step 79680, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 45h:55m:40s remains)
INFO - root - 2017-12-16 01:00:40.688512: step 79690, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 45h:26m:21s remains)
INFO - root - 2017-12-16 01:00:47.240029: step 79700, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 46h:29m:37s remains)
2017-12-16 01:00:47.745351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.4608183 -8.75205 -8.0653954 -5.9345646 -5.9908495 -6.0580721 -5.1900678 -5.26012 -5.56508 -5.518322 -4.8274069 -6.3938289 -7.429615 -8.2166309 -8.1601219][-8.3787184 -7.7706847 -6.1617284 -5.511507 -6.107451 -6.192872 -5.544807 -5.2904992 -4.9465041 -4.5247726 -3.4413 -4.5695906 -6.0638709 -7.9413128 -7.9217072][-4.0808563 -6.4409595 -7.0528216 -6.0875196 -6.0448456 -5.8615379 -5.1321907 -5.0770936 -5.1656351 -4.8805695 -4.3121305 -6.177372 -7.7770972 -7.6440439 -8.1587286][-3.0493689 -4.1349545 -4.3872924 -4.6548386 -4.7541733 -4.1578321 -3.128782 -3.1842084 -3.1842225 -3.4343085 -3.6610837 -5.779819 -7.6554546 -9.4017487 -10.241211][-3.0388949 -5.1845031 -5.9654055 -4.7887311 -3.8618798 -2.7994769 -0.69126987 -1.2230549 -2.5368645 -3.850626 -4.22415 -7.0346475 -8.3678188 -10.271514 -11.059967][-5.7336345 -6.9707341 -6.74753 -4.9551759 -2.7061925 -0.088245869 2.1504974 2.4794917 1.5751305 -0.2413764 -2.0629339 -5.3320079 -7.476902 -9.7125292 -9.9400606][-6.8653126 -7.6851311 -7.1639013 -4.3122168 -1.2959538 1.4673719 4.5917811 5.04826 3.1964097 0.013784885 -2.6330709 -5.4732056 -7.9435511 -9.6381493 -9.3353767][-6.6689548 -7.6975756 -6.9954486 -3.7374582 -0.4905839 3.3406425 5.6866775 5.2859817 3.655087 1.0175595 -2.1516838 -5.6436033 -8.0347748 -9.5354443 -9.7824945][-4.8251457 -5.7137303 -5.4691548 -2.5699673 -0.24038839 2.55374 4.2902122 3.9945302 1.5193462 -1.3702331 -3.4032261 -6.6694026 -9.6813412 -10.973478 -11.047722][-4.5750504 -5.7596288 -5.8612685 -4.2295227 -2.5444167 -0.66672277 0.9219408 0.96208429 -0.52710104 -2.9167912 -5.02597 -8.50943 -11.035593 -12.875823 -13.13442][-6.3193436 -8.1165571 -7.9325666 -6.897666 -5.9226222 -3.9892683 -2.9003842 -3.3615706 -5.298028 -7.1023655 -8.2344761 -11.067722 -13.576 -14.472843 -13.706326][-9.961525 -10.734919 -9.42995 -8.6488647 -8.1412745 -6.4397383 -6.2320037 -6.8747091 -7.50356 -8.5644388 -10.028399 -11.848109 -12.120329 -12.760872 -12.227501][-11.491049 -11.447948 -10.101107 -9.4256086 -8.3948412 -7.8045845 -7.4046803 -7.5692177 -9.3278513 -10.402353 -10.804233 -10.836038 -10.705309 -11.023168 -9.5117874][-9.7170067 -9.9455595 -8.6518774 -8.4148312 -7.4196348 -6.6596255 -6.5280304 -6.28819 -7.025322 -7.7185593 -8.39559 -7.2557116 -6.8102894 -7.4931812 -6.1853714][-8.5060539 -9.6758108 -9.5159407 -7.5340037 -5.5200105 -5.7045722 -5.356328 -5.8502097 -6.480926 -6.1542048 -5.920476 -6.2968845 -6.7796669 -6.13586 -6.0436683]]...]
INFO - root - 2017-12-16 01:00:54.353562: step 79710, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 46h:30m:27s remains)
INFO - root - 2017-12-16 01:01:01.022146: step 79720, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 48h:01m:31s remains)
INFO - root - 2017-12-16 01:01:07.601957: step 79730, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 46h:22m:29s remains)
INFO - root - 2017-12-16 01:01:14.197820: step 79740, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 46h:21m:57s remains)
INFO - root - 2017-12-16 01:01:20.849214: step 79750, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 47h:35m:32s remains)
INFO - root - 2017-12-16 01:01:27.398047: step 79760, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 45h:54m:43s remains)
INFO - root - 2017-12-16 01:01:34.022144: step 79770, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 46h:17m:27s remains)
INFO - root - 2017-12-16 01:01:40.582772: step 79780, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 45h:09m:01s remains)
INFO - root - 2017-12-16 01:01:47.171490: step 79790, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 45h:09m:43s remains)
INFO - root - 2017-12-16 01:01:53.823124: step 79800, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 44h:54m:54s remains)
2017-12-16 01:01:54.396176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5516882 -6.9370046 -7.5186896 -8.3946686 -9.5860167 -9.78598 -10.318791 -10.322397 -10.236673 -10.041838 -8.9649353 -8.7302723 -8.54264 -8.5673828 -8.7925653][-8.6395321 -9.42217 -9.4699936 -9.405262 -10.077034 -11.228235 -12.576767 -12.856724 -12.791174 -11.544105 -10.484959 -10.9009 -10.211945 -11.333815 -10.560118][-6.4005618 -8.1594791 -10.053877 -10.457237 -10.593578 -11.061457 -12.263572 -13.054377 -12.563873 -11.323068 -10.911478 -11.049404 -11.076743 -12.203789 -11.721627][-7.5791264 -8.7411556 -10.589064 -10.164654 -10.283917 -10.089863 -9.9848518 -10.632434 -11.045498 -10.473929 -10.28195 -11.505129 -12.11517 -12.326172 -11.112288][-8.6284342 -11.070614 -12.544191 -10.387213 -8.7541609 -5.3342395 -3.7915897 -6.7158384 -8.8210049 -8.6471653 -9.6059475 -11.400993 -11.908346 -13.506802 -12.846938][-10.712597 -11.504957 -12.294241 -11.660786 -9.3880262 -1.392746 4.212131 0.83416843 -3.0039856 -6.0595107 -9.2920761 -9.3465767 -9.487215 -12.191303 -12.936012][-12.317181 -12.19776 -11.155603 -9.7820969 -6.9090319 0.064531326 6.2344432 7.7513452 5.7933059 -2.71318 -9.9046688 -9.6396894 -8.9295845 -11.110546 -11.760904][-12.487059 -13.28562 -12.815228 -8.3230972 -2.5266433 2.2811012 6.3937945 8.2858582 8.160202 1.3316398 -6.6501675 -9.3206768 -9.96636 -10.725517 -10.81919][-10.377174 -11.30121 -12.730542 -10.66689 -5.6580524 0.68885279 5.9184937 7.1133628 5.4653449 -0.16352034 -6.1269627 -9.4085722 -11.799221 -13.775801 -13.101875][-8.309309 -9.90079 -11.449507 -10.976569 -8.6655 -4.0838671 1.6290364 4.0156837 2.3999162 -2.6225975 -8.1238356 -11.550105 -12.6147 -14.53718 -15.221611][-11.396343 -12.478765 -13.526511 -12.454372 -10.893442 -9.629488 -7.1691217 -4.7186122 -4.70074 -6.1560569 -10.154221 -12.743562 -14.254499 -14.738892 -14.56975][-13.509406 -15.263084 -15.645678 -13.664328 -11.93915 -11.907642 -11.490192 -10.904219 -10.94292 -11.480366 -13.069407 -13.589595 -14.630013 -14.18556 -13.674689][-11.995506 -13.788181 -14.652643 -13.457005 -12.915762 -12.188473 -11.117998 -11.820694 -12.608931 -12.003881 -11.524569 -12.707247 -13.164379 -11.444456 -10.34009][-10.342562 -10.518698 -11.014725 -10.715593 -11.576698 -11.755863 -10.340739 -9.7653255 -9.7566843 -10.734852 -11.26503 -9.30516 -8.95584 -8.884573 -9.7760267][-5.7624245 -5.9310894 -5.0255756 -4.3043146 -5.4224072 -6.4640265 -6.5675611 -6.0714188 -5.7961378 -6.4749775 -6.8078432 -7.86835 -8.6754265 -8.6985149 -9.382143]]...]
INFO - root - 2017-12-16 01:02:01.019066: step 79810, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 46h:11m:40s remains)
INFO - root - 2017-12-16 01:02:07.599049: step 79820, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 46h:15m:27s remains)
INFO - root - 2017-12-16 01:02:14.207922: step 79830, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.670 sec/batch; 47h:01m:13s remains)
INFO - root - 2017-12-16 01:02:20.804049: step 79840, loss = 0.20, batch loss = 0.16 (11.9 examples/sec; 0.674 sec/batch; 47h:18m:35s remains)
INFO - root - 2017-12-16 01:02:27.351285: step 79850, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 45h:38m:09s remains)
INFO - root - 2017-12-16 01:02:33.971005: step 79860, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 46h:38m:53s remains)
INFO - root - 2017-12-16 01:02:40.499449: step 79870, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 45h:30m:58s remains)
INFO - root - 2017-12-16 01:02:47.138337: step 79880, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 46h:44m:21s remains)
INFO - root - 2017-12-16 01:02:53.788552: step 79890, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 45h:20m:09s remains)
INFO - root - 2017-12-16 01:03:00.389736: step 79900, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 45h:33m:43s remains)
2017-12-16 01:03:00.901805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2173376 -5.9065251 -5.6484179 -3.2137015 -2.574749 -2.5824149 -3.2651248 -3.80195 -4.1919422 -4.7264538 -5.2000232 -7.8204112 -8.9206734 -10.129638 -9.6797333][-6.0546551 -5.718977 -4.9337234 -3.7261157 -2.9681213 -3.1169832 -3.6998847 -4.0437121 -4.1706066 -3.8254261 -3.493453 -6.7197194 -8.2947712 -10.255713 -10.66209][-3.4391446 -4.7390733 -5.3158422 -3.8399451 -3.7885818 -3.7689638 -3.8336668 -4.0657163 -3.9264257 -3.178741 -2.8193111 -5.4948692 -7.1489005 -9.4915934 -9.4970016][-3.8473365 -4.3701277 -4.4528751 -3.4018598 -3.5579631 -3.8185649 -3.4913239 -3.4558437 -3.8066077 -2.8799627 -1.8699698 -4.2485104 -6.0837631 -9.1457767 -9.0985527][-4.1138468 -5.0711856 -5.5784912 -4.3991175 -3.4506338 -2.3781652 -1.5122795 -1.2413983 -1.3387575 -1.1236243 -0.39503765 -2.3806014 -3.9825644 -7.5375795 -8.5800762][-5.6440692 -5.2001472 -5.0122485 -4.1897326 -2.7494349 -0.69862175 0.74756479 1.1320305 0.80347586 0.83292818 0.9182415 -1.2260365 -2.7626092 -6.4109654 -7.3522782][-5.4069295 -5.2930908 -5.7497249 -3.9451234 -1.7537341 0.93180323 2.963985 3.6184878 3.1048236 2.0395575 1.5230751 -1.1076555 -2.9546933 -5.2720013 -5.6739836][-3.6407714 -4.156424 -4.3831573 -2.250221 -0.91668177 1.3874359 3.9433827 5.0303845 4.0623641 2.9536386 2.5429444 -1.0257087 -3.0598438 -5.494698 -5.6029167][-2.200717 -2.827388 -3.0981824 -1.3840494 -0.45658922 1.3339138 3.118669 3.907001 3.4259486 2.3696094 1.7190199 -0.98518515 -2.9104524 -5.4496546 -5.0124354][-2.4596579 -2.5025847 -2.8992405 -2.1786957 -1.757565 -0.1868825 0.24047518 1.1940937 1.6752152 1.225266 0.68504858 -1.8377054 -3.3336902 -5.5982146 -4.3955588][-5.5420465 -4.9786816 -4.9719968 -4.7064066 -4.2596116 -3.4381888 -2.852824 -2.0677452 -1.6025567 -1.3182101 -1.7588322 -3.9038033 -5.5137019 -6.980123 -4.7638931][-10.911007 -9.89824 -8.6720161 -7.0267754 -6.9212914 -6.8159437 -5.6784515 -4.9415832 -5.2033768 -5.1380877 -4.9829855 -5.9539771 -6.5349393 -6.8748827 -5.0842934][-11.864096 -11.448234 -10.000235 -7.9533715 -7.6598148 -7.6600518 -6.9623618 -5.7717228 -6.0115609 -6.2834377 -6.780345 -7.8249092 -7.9523783 -7.2622266 -4.9924383][-8.2795906 -8.088419 -7.7224841 -6.7315879 -5.7722583 -5.2104053 -5.6912355 -5.8713393 -5.3409443 -5.2148933 -6.0273895 -6.0898948 -6.2426038 -5.8585773 -4.0719666][-4.3668213 -4.5606852 -4.9316359 -4.8619862 -4.064086 -3.6365395 -4.1599092 -3.9452312 -4.062386 -4.6650543 -4.5682526 -4.6439023 -5.4476252 -6.0021858 -5.3883619]]...]
INFO - root - 2017-12-16 01:03:07.503241: step 79910, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 45h:59m:24s remains)
INFO - root - 2017-12-16 01:03:14.058055: step 79920, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 45h:15m:20s remains)
INFO - root - 2017-12-16 01:03:20.663883: step 79930, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 45h:16m:28s remains)
INFO - root - 2017-12-16 01:03:27.371507: step 79940, loss = 0.15, batch loss = 0.10 (11.2 examples/sec; 0.716 sec/batch; 50h:13m:11s remains)
INFO - root - 2017-12-16 01:03:33.931319: step 79950, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 46h:21m:23s remains)
INFO - root - 2017-12-16 01:03:40.478483: step 79960, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 47h:02m:07s remains)
INFO - root - 2017-12-16 01:03:47.031475: step 79970, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 45h:44m:54s remains)
INFO - root - 2017-12-16 01:03:53.716198: step 79980, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.687 sec/batch; 48h:09m:33s remains)
INFO - root - 2017-12-16 01:04:00.322418: step 79990, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 45h:44m:39s remains)
INFO - root - 2017-12-16 01:04:06.911025: step 80000, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 45h:28m:41s remains)
2017-12-16 01:04:07.498742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4337006 -4.9181638 -6.1697989 -6.7019396 -7.6452761 -7.6638031 -8.5905418 -9.7900791 -10.255753 -11.032656 -10.520541 -10.368042 -11.592886 -12.535828 -11.173109][-4.2832985 -5.99327 -6.516469 -5.999506 -7.626709 -8.3581247 -8.8611927 -9.3824921 -10.126321 -10.723564 -10.925894 -10.506664 -11.070704 -12.237927 -10.358011][-1.663537 -4.9916563 -7.8106775 -8.11678 -9.3429613 -9.1764174 -9.1296339 -9.1784716 -8.9926262 -9.8242607 -10.233435 -9.8780594 -10.575336 -11.713268 -10.211449][-5.6829538 -7.2159719 -9.2373829 -10.24849 -11.426093 -9.600832 -8.4899845 -9.1653786 -8.5548878 -7.7691259 -8.2828207 -8.9569473 -10.40454 -10.963453 -10.383551][-8.4292526 -12.344662 -14.215761 -13.420963 -13.060547 -8.6756325 -4.3748913 -6.3260508 -7.972887 -7.2094593 -7.457438 -7.9060545 -10.801079 -12.568388 -11.475121][-10.880999 -13.272079 -14.19342 -12.981334 -9.4076719 -3.5411282 1.8202047 1.2532063 0.81868124 -3.0231869 -7.841876 -6.8297949 -9.33741 -11.642643 -11.477547][-12.480737 -13.249026 -12.044807 -8.8614616 -5.1994586 0.71910238 8.3981934 9.4208107 8.9512253 1.7739038 -5.984827 -6.5558243 -10.751417 -12.996998 -12.698475][-12.990108 -13.44186 -12.559665 -7.7051339 -2.9470613 3.061151 11.212618 12.114777 11.903845 4.6877084 -2.6482546 -5.7171283 -11.75318 -13.79018 -13.350595][-12.500689 -13.305596 -13.462724 -8.9572287 -4.1328783 3.2443919 8.9773788 8.455574 9.4291553 3.1045403 -3.9548135 -8.0507765 -14.393164 -15.066214 -13.161953][-10.843845 -11.892345 -12.958207 -11.028507 -9.330946 -2.9957929 3.4185214 4.722898 3.777863 -2.6721907 -6.71294 -8.4739 -13.629923 -17.074366 -16.931387][-14.61327 -16.130896 -16.479706 -12.881016 -12.154129 -9.5619574 -6.9736676 -4.8994222 -4.4702754 -7.7906132 -10.474091 -12.214417 -15.574615 -16.479649 -15.733124][-19.181808 -20.230337 -20.485067 -17.739229 -16.333008 -13.914425 -13.920452 -13.126965 -11.544846 -12.62295 -13.513126 -14.016382 -15.220339 -15.61021 -13.808561][-19.03927 -20.690248 -20.332296 -17.575466 -16.364573 -14.61306 -14.188353 -13.886782 -13.457135 -13.814688 -13.454041 -13.01021 -12.784639 -12.69722 -11.479073][-14.668972 -16.979679 -17.540545 -14.379303 -12.357031 -11.768388 -11.856243 -11.593857 -11.837872 -11.367937 -10.25038 -10.170477 -11.000591 -10.942443 -8.6340532][-11.207945 -11.830603 -12.194878 -9.9337463 -8.3504705 -7.1227188 -6.3755603 -7.4110255 -8.78305 -8.81358 -8.5342722 -8.77212 -9.1010475 -10.200066 -11.370949]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 01:04:15.337469: step 80010, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 46h:14m:16s remains)
INFO - root - 2017-12-16 01:04:21.896915: step 80020, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 47h:01m:22s remains)
INFO - root - 2017-12-16 01:04:28.483916: step 80030, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 45h:56m:28s remains)
INFO - root - 2017-12-16 01:04:35.080902: step 80040, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 45h:30m:57s remains)
INFO - root - 2017-12-16 01:04:41.644095: step 80050, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 46h:07m:29s remains)
INFO - root - 2017-12-16 01:04:48.220502: step 80060, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 44h:49m:30s remains)
INFO - root - 2017-12-16 01:04:54.774201: step 80070, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 45h:41m:43s remains)
INFO - root - 2017-12-16 01:05:01.445929: step 80080, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 46h:35m:58s remains)
INFO - root - 2017-12-16 01:05:08.014691: step 80090, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 45h:01m:38s remains)
INFO - root - 2017-12-16 01:05:14.545006: step 80100, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 44h:25m:35s remains)
2017-12-16 01:05:15.082329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2985373 -5.6254296 -4.6274261 -2.0608096 0.33578444 -0.39393187 -2.3159769 -4.0444031 -4.996572 -5.3248162 -5.2493334 -7.5366168 -10.135315 -11.78627 -10.331879][-4.6172543 -4.3719354 -5.8222933 -5.3078318 -4.07674 -2.3304203 -0.32697153 -1.6937966 -3.5402455 -5.7634568 -8.2996845 -11.040771 -11.203289 -12.459841 -11.190253][-3.3205721 -4.7199855 -6.4323707 -4.8835859 -5.2861414 -4.7918272 -4.07012 -3.8415797 -3.6896577 -4.0123048 -5.2980704 -8.92779 -10.88859 -11.683062 -10.453661][-3.373054 -4.7558951 -6.972846 -7.8088427 -8.4953709 -6.1679969 -3.3401914 -1.8026521 -1.9573288 -4.6272469 -8.4097519 -11.804684 -11.880921 -12.369689 -10.247435][-5.5606456 -6.3736463 -7.8002539 -7.9152212 -7.0232105 -4.1759973 -2.8023293 -2.9264307 -2.933552 -2.9145467 -4.7857375 -10.116261 -12.799541 -14.287682 -12.182264][-7.6937084 -5.8918691 -5.8593769 -6.0792289 -4.40589 0.20002079 5.2693095 5.211432 1.7633491 -2.9434323 -6.1827326 -8.7993813 -10.744618 -13.378298 -12.76683][-8.3260727 -8.2457991 -9.0933495 -6.5713477 -3.0799358 2.3472981 6.8469338 7.9222751 6.72096 0.54419279 -4.7302933 -7.08206 -9.0562592 -11.70892 -11.972274][-8.5036182 -9.4315844 -9.84873 -6.2498112 -2.5347321 3.4309745 8.7299786 8.9938049 7.9973474 2.6065326 -3.3635888 -7.8832369 -9.7511749 -10.097532 -9.5689583][-7.4230623 -10.656708 -10.37059 -7.3716335 -4.3091307 1.6496372 5.9882178 6.8324666 6.4792085 0.51392508 -3.7031817 -8.701086 -12.161472 -13.363907 -11.289476][-7.2578268 -8.1205187 -8.2521544 -6.9119534 -5.245892 -1.3491073 1.7086215 3.0376496 2.6305089 -2.7074339 -7.8373413 -13.088957 -16.048887 -16.058752 -14.454731][-10.314434 -12.118637 -11.161388 -8.1658287 -6.7382092 -4.8189197 -4.4352255 -3.98526 -5.5998206 -9.15052 -11.440378 -14.766212 -17.761889 -18.01236 -15.463425][-13.892546 -14.349653 -14.191248 -11.623878 -8.3726816 -6.5110712 -6.9231548 -7.5702353 -9.04865 -11.917442 -14.265148 -16.315649 -16.231495 -15.343651 -13.965204][-15.452896 -15.009142 -13.294919 -12.571453 -12.033634 -10.537407 -9.3319464 -9.6716413 -10.624159 -10.911146 -11.527575 -12.363426 -13.010397 -11.422413 -9.3655872][-11.02072 -10.159431 -10.404119 -10.914758 -11.055984 -10.382644 -9.8095922 -9.6573315 -9.5285454 -10.568678 -10.434899 -8.7359352 -8.4320869 -7.6366954 -7.3804822][-5.9463515 -5.9805417 -5.0971789 -4.7075052 -6.0534625 -7.8876281 -8.45776 -7.7843218 -6.9731507 -7.039269 -7.0643096 -7.7044544 -8.2872982 -7.3811073 -6.7871828]]...]
INFO - root - 2017-12-16 01:05:21.686728: step 80110, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 45h:55m:34s remains)
INFO - root - 2017-12-16 01:05:28.248091: step 80120, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 46h:25m:35s remains)
INFO - root - 2017-12-16 01:05:34.841411: step 80130, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 47h:23m:04s remains)
INFO - root - 2017-12-16 01:05:41.449851: step 80140, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.687 sec/batch; 48h:11m:04s remains)
INFO - root - 2017-12-16 01:05:48.011057: step 80150, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 46h:02m:53s remains)
INFO - root - 2017-12-16 01:05:54.577797: step 80160, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.654 sec/batch; 45h:48m:49s remains)
INFO - root - 2017-12-16 01:06:01.186520: step 80170, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 45h:50m:02s remains)
INFO - root - 2017-12-16 01:06:07.707777: step 80180, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 45h:37m:37s remains)
INFO - root - 2017-12-16 01:06:14.338787: step 80190, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 45h:34m:12s remains)
INFO - root - 2017-12-16 01:06:20.943389: step 80200, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.681 sec/batch; 47h:42m:30s remains)
2017-12-16 01:06:21.494892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2418785 -6.9094133 -6.355629 -5.6782866 -6.8112736 -7.0911374 -7.2137561 -6.9989643 -6.1294174 -5.4446721 -5.23634 -6.0943732 -8.1097069 -7.9739094 -6.3142877][-6.8992515 -6.3956914 -6.4229288 -5.8316298 -7.2147322 -7.9260855 -8.3945827 -8.3135948 -7.2819185 -5.8977323 -5.4227867 -6.1770663 -8.5700388 -8.8858671 -7.1763868][-5.4708514 -5.6273775 -6.0945077 -5.5691695 -6.459661 -7.45828 -7.9228096 -7.5370584 -6.5147562 -5.9019904 -5.9069624 -6.6737595 -9.3634567 -9.7800055 -8.082386][-6.8106546 -6.5395379 -6.33318 -6.7601795 -8.2700052 -8.0426273 -7.5488725 -7.506309 -7.1981258 -6.3379741 -6.4666014 -7.92017 -11.209989 -11.321855 -9.71959][-6.1738586 -6.8211203 -7.4700756 -6.9994063 -6.5660229 -4.8637919 -3.9841659 -4.6524277 -4.7109537 -4.4437246 -5.4355597 -7.4268284 -11.115157 -12.347765 -11.721947][-9.4446526 -8.7981491 -7.7702589 -6.3162217 -4.7204962 -2.3565633 -0.47336483 -0.25060797 -0.54586983 -2.8500698 -5.5081577 -7.343853 -11.058187 -12.839362 -12.004801][-9.4886122 -9.53356 -7.8983526 -5.018074 -3.0439832 0.33768559 3.1556754 3.0175147 2.213099 -0.77871561 -4.323164 -6.9450035 -11.24988 -11.972267 -10.800961][-8.60688 -7.5171442 -6.6043811 -2.9436781 0.090702534 3.6639447 5.5296617 5.2173228 5.0362573 1.5949917 -2.0015082 -4.5387878 -8.903882 -10.68218 -9.8133392][-6.5968885 -5.9070845 -4.5966697 -1.9325845 0.5212698 3.5532107 5.4350619 5.416532 4.4753594 1.9679065 -0.63114452 -3.1393781 -7.8698826 -8.7467384 -6.9410663][-4.4483576 -4.4410419 -4.0530596 -0.85658455 0.90841818 2.3734651 3.4703355 3.3842273 2.1695728 0.094209194 -1.6773677 -3.1794095 -6.6981058 -8.007329 -6.7841334][-6.816515 -6.3983536 -5.0710578 -3.3219335 -3.0888343 -3.09505 -2.7247636 -3.3006942 -4.0401011 -5.3588057 -6.1612911 -7.1196656 -8.3638449 -8.3527231 -6.5000648][-11.086157 -11.164005 -10.067429 -8.7681284 -9.6270285 -10.110859 -10.728678 -10.911307 -10.742771 -10.931435 -10.298428 -9.006959 -8.9078226 -7.2295647 -4.592504][-12.452795 -12.311066 -11.943783 -11.563769 -12.170208 -12.328806 -13.584809 -14.042479 -14.129736 -13.084198 -11.741396 -9.7732649 -8.5434446 -6.2034988 -3.0882781][-10.110859 -10.735415 -10.319887 -10.251613 -10.583961 -11.290799 -12.285852 -12.046618 -12.302372 -11.422661 -10.830477 -8.458065 -7.4413786 -5.827558 -4.500474][-8.46549 -8.703249 -8.0932732 -7.5742159 -6.7724805 -7.11537 -7.2416935 -7.0946932 -7.9339466 -7.2666526 -7.0228014 -6.4599848 -6.4192009 -5.91858 -5.6973834]]...]
INFO - root - 2017-12-16 01:06:28.114323: step 80210, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 47h:02m:35s remains)
INFO - root - 2017-12-16 01:06:34.700282: step 80220, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 47h:17m:16s remains)
INFO - root - 2017-12-16 01:06:41.329594: step 80230, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 45h:19m:10s remains)
INFO - root - 2017-12-16 01:06:47.951454: step 80240, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 45h:25m:29s remains)
INFO - root - 2017-12-16 01:06:54.631029: step 80250, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 45h:56m:45s remains)
INFO - root - 2017-12-16 01:07:01.223580: step 80260, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 46h:46m:11s remains)
INFO - root - 2017-12-16 01:07:07.800553: step 80270, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 46h:02m:04s remains)
INFO - root - 2017-12-16 01:07:14.341895: step 80280, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 45h:45m:17s remains)
INFO - root - 2017-12-16 01:07:20.928272: step 80290, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.638 sec/batch; 44h:43m:27s remains)
INFO - root - 2017-12-16 01:07:27.538008: step 80300, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.680 sec/batch; 47h:38m:45s remains)
2017-12-16 01:07:28.053810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7255464 -4.015696 -4.008625 -4.8336911 -5.6104393 -6.5941114 -7.12051 -7.19498 -6.9542379 -6.6181951 -5.4769096 -6.9881239 -8.3780184 -10.107264 -9.6535187][-4.8663621 -4.2097697 -4.4292364 -5.4926586 -6.131628 -7.0331359 -7.9158087 -8.0990877 -7.0989585 -6.0360622 -5.9037309 -7.6391511 -9.4883966 -12.406473 -12.028646][-3.4600186 -4.0229549 -5.0154405 -6.1457834 -7.129704 -7.23728 -7.3111992 -6.3513417 -5.6795993 -5.6535096 -6.4309192 -7.8257756 -9.3754263 -10.649342 -10.375078][-4.0050678 -3.952419 -4.8922195 -5.5421772 -6.2608709 -6.0826278 -5.2961292 -5.2957869 -5.3738203 -4.9342208 -5.5610747 -7.9334345 -10.526646 -10.977262 -9.7343235][-4.7124443 -6.0949225 -6.3689127 -6.0696011 -5.2715473 -2.8688312 -1.3104529 -2.4617631 -3.6241992 -3.8451471 -4.8674612 -7.3242764 -9.8768644 -10.877029 -10.271303][-6.214345 -6.5448737 -6.3421969 -4.9570312 -3.9513569 -0.89712572 2.0926418 2.3449683 0.46830654 -2.8252308 -4.884151 -6.3347721 -7.8368979 -9.1629982 -9.1672535][-6.0969176 -7.9275341 -7.5045843 -5.5202856 -3.60178 -0.2513032 3.4527516 5.2177892 5.2127748 1.3376107 -1.8839266 -4.6750741 -7.676475 -8.773632 -8.1973648][-5.8989949 -6.56714 -5.362411 -3.5240214 -1.9670916 1.7096024 4.7169833 6.0487037 5.7979131 4.1283126 1.2908072 -2.4367089 -5.4480495 -6.4427214 -7.07423][-4.8900442 -5.1132326 -3.5023825 -1.8080285 -1.2954712 1.291048 4.019402 6.1065574 6.5828223 4.3534713 2.734406 -0.35535622 -2.9726088 -5.6437812 -6.1673985][-3.4872255 -3.3299406 -1.7410398 -0.16768122 0.4995327 1.1261106 1.9039912 3.061049 3.1454959 1.8488817 0.072025776 -2.6226134 -4.2995281 -4.8229094 -4.7495375][-6.6030841 -6.3679967 -5.433207 -3.9732761 -3.3518469 -2.3454452 -2.0949681 -1.6322794 -2.10522 -2.8362653 -3.6833212 -6.3986254 -9.232605 -8.3808889 -6.21112][-11.607687 -10.907972 -10.930228 -8.8555641 -7.1532841 -6.65727 -6.5913572 -6.4696031 -8.0190954 -9.1299067 -9.7914581 -10.620144 -10.703417 -9.5096359 -7.7512751][-14.136127 -13.946981 -12.010386 -11.133606 -11.068927 -9.6069984 -8.458252 -8.2893467 -9.6382179 -11.460283 -11.616125 -11.660574 -11.495515 -10.493795 -8.2753372][-13.50176 -13.127101 -13.322191 -11.833931 -9.7968063 -8.9520311 -8.4866648 -8.778388 -9.4659538 -10.547136 -11.138548 -10.098336 -9.0050392 -7.5659313 -6.1983619][-9.5082788 -9.6123562 -9.7690525 -9.2682781 -9.0225754 -8.25534 -7.6202126 -6.4353261 -7.1007261 -7.2623897 -6.9437838 -7.6035557 -7.7013674 -6.5860195 -5.8853226]]...]
INFO - root - 2017-12-16 01:07:34.558404: step 80310, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 45h:26m:36s remains)
INFO - root - 2017-12-16 01:07:41.100488: step 80320, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 45h:10m:37s remains)
INFO - root - 2017-12-16 01:07:47.770638: step 80330, loss = 0.17, batch loss = 0.12 (11.6 examples/sec; 0.688 sec/batch; 48h:12m:10s remains)
INFO - root - 2017-12-16 01:07:54.254715: step 80340, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 44h:46m:15s remains)
INFO - root - 2017-12-16 01:08:00.899827: step 80350, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 46h:02m:21s remains)
INFO - root - 2017-12-16 01:08:07.535084: step 80360, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 45h:42m:14s remains)
INFO - root - 2017-12-16 01:08:14.125783: step 80370, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 46h:45m:27s remains)
INFO - root - 2017-12-16 01:08:20.709994: step 80380, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.661 sec/batch; 46h:15m:55s remains)
INFO - root - 2017-12-16 01:08:27.185978: step 80390, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 45h:27m:26s remains)
INFO - root - 2017-12-16 01:08:33.791193: step 80400, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 45h:11m:23s remains)
2017-12-16 01:08:34.324401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.7858534 -8.7875595 -7.7215681 -5.7070122 -5.4028344 -5.4145284 -4.7165141 -4.7310176 -4.4526544 -5.2411461 -6.2488976 -8.4290142 -9.0598755 -9.174633 -9.0124607][-9.142643 -8.0061312 -7.1410565 -5.824152 -5.3142209 -4.6864915 -3.9545245 -3.7821689 -3.9809055 -5.0917149 -5.1433792 -7.4946785 -8.1370087 -8.4006453 -7.9689813][-7.5132971 -7.3853359 -7.2121944 -5.7536306 -5.6033936 -4.9404235 -4.427474 -3.9125562 -2.9027779 -3.1409602 -3.3980887 -6.4041958 -6.8966713 -6.8326235 -7.5072184][-7.7024302 -6.5009913 -6.4996195 -5.9605021 -5.6548896 -4.5770154 -3.6522634 -3.3130736 -2.9666679 -3.0074685 -3.2871253 -5.9954715 -6.4121103 -6.723484 -6.9233227][-7.6975422 -7.649322 -7.1371107 -4.7160039 -3.9920354 -3.3800156 -2.3006775 -2.1310256 -2.1163855 -2.3130012 -2.8872287 -5.2062192 -6.0296307 -6.5224228 -6.8189807][-8.1404552 -7.4574804 -6.4495459 -4.3614645 -2.7469206 -1.2065606 0.69920635 1.1908989 0.54158735 -0.858428 -1.8233566 -3.9218135 -4.8758812 -5.9673033 -6.4849529][-7.2049465 -6.2197924 -5.996079 -3.1465456 -0.42983437 1.8690877 3.4631829 3.7211938 3.5453477 1.6759109 -1.1227446 -4.60744 -5.4152074 -5.8706913 -6.4037004][-7.2436504 -6.2583079 -4.816617 -2.811198 -1.0328469 2.2326326 5.0950408 5.1499133 3.9989629 1.9492993 -0.37212515 -4.2912269 -6.0593262 -7.2009225 -7.1952686][-8.31005 -7.5532188 -5.6988606 -3.2065918 -1.7413015 0.64674568 3.6860871 4.201591 3.8106961 1.6957808 -0.039177418 -3.9382298 -7.0130405 -8.3295193 -9.0696592][-8.3486166 -8.5642281 -7.09755 -4.6415033 -3.6137776 -1.4463115 1.0843697 2.0226908 1.6700149 0.49749565 -0.89257383 -4.4621811 -7.204473 -10.175674 -11.467603][-10.718597 -10.846955 -10.053158 -7.7079248 -6.4878044 -5.2431822 -2.843663 -1.8786335 -1.8241441 -1.8812048 -3.2594492 -6.7824616 -9.5801668 -11.876132 -12.894363][-13.538065 -12.452668 -11.650998 -10.003614 -9.7448406 -8.11462 -5.8299527 -4.9778743 -5.016953 -5.3994436 -6.911027 -8.841917 -10.534986 -11.47801 -11.870888][-13.728998 -11.899025 -10.973772 -9.652833 -8.8167343 -6.9343843 -5.5447078 -4.8275967 -4.6172791 -5.8446136 -7.9257727 -9.0460768 -10.555349 -9.9160824 -9.1757011][-11.080605 -9.8102722 -7.6921721 -6.013669 -5.5715127 -5.1900344 -4.9711022 -4.3502417 -4.4131651 -4.9708347 -5.8670206 -5.349472 -5.4694319 -6.3404531 -6.6719418][-6.21521 -5.8271852 -5.7755833 -3.486912 -2.2431316 -2.8572979 -2.7587843 -2.7515831 -2.92639 -3.1762073 -4.1572595 -5.4783878 -6.2698331 -5.91648 -5.9702616]]...]
INFO - root - 2017-12-16 01:08:40.937489: step 80410, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 46h:11m:21s remains)
INFO - root - 2017-12-16 01:08:47.514965: step 80420, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 45h:05m:38s remains)
INFO - root - 2017-12-16 01:08:54.105018: step 80430, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 45h:41m:49s remains)
INFO - root - 2017-12-16 01:09:00.663898: step 80440, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 45h:29m:46s remains)
INFO - root - 2017-12-16 01:09:07.259982: step 80450, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 44h:37m:34s remains)
INFO - root - 2017-12-16 01:09:13.816404: step 80460, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 45h:59m:40s remains)
INFO - root - 2017-12-16 01:09:20.314683: step 80470, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 45h:26m:01s remains)
INFO - root - 2017-12-16 01:09:26.813213: step 80480, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 46h:35m:23s remains)
INFO - root - 2017-12-16 01:09:33.418308: step 80490, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 47h:27m:07s remains)
INFO - root - 2017-12-16 01:09:39.987205: step 80500, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 44h:59m:52s remains)
2017-12-16 01:09:40.533449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2784548 -3.4709563 -3.3034441 -2.8307133 -3.3657911 -3.6538682 -4.4744353 -5.6966066 -5.5204597 -3.8739104 -2.2085354 -3.6714265 -4.1184974 -6.1513834 -6.2941756][-5.1694989 -5.1724906 -6.3847513 -5.6382151 -6.2761784 -6.9826465 -7.316247 -7.6080923 -7.0708375 -5.2445521 -3.2056448 -5.2425585 -6.9111905 -7.76289 -6.3166995][-5.0801263 -6.1468935 -6.8300815 -6.0977907 -7.6276169 -7.9996548 -8.4040394 -8.20702 -7.6358109 -6.46717 -4.7142873 -7.0891914 -8.7705383 -9.3701477 -7.8257761][-8.1640205 -8.1057148 -8.2932062 -7.5116405 -7.4403219 -6.4715705 -6.3007679 -6.776515 -7.0057044 -5.7279944 -4.1386986 -7.762044 -10.378973 -11.528477 -10.284569][-10.465278 -10.815725 -11.068609 -8.6377954 -6.813 -4.2628183 -3.2235355 -3.7987137 -4.7071066 -4.7034593 -4.9432893 -8.2418613 -10.52453 -12.877699 -12.796358][-12.559937 -12.61166 -10.921598 -7.1360707 -4.6819158 -1.770395 0.33684111 1.0394878 0.5324564 -1.1560249 -3.5957811 -8.5031633 -11.703911 -13.784403 -12.217964][-12.47138 -11.059286 -9.0683918 -5.2990789 -1.7975872 2.5526319 5.1151681 5.3811641 4.7969575 2.2277603 -0.79935694 -6.7319427 -11.174189 -13.412451 -12.37018][-11.472733 -9.4537992 -7.9175596 -3.2119517 1.5119677 6.5464072 9.1158676 8.7546215 7.4958997 4.5495639 2.220366 -3.3792171 -7.5084105 -9.70215 -9.0849466][-10.167643 -9.1503878 -7.7602053 -2.4119089 0.7584691 3.8313556 6.8805175 7.5585513 7.2431521 4.9967637 3.396502 -1.5561872 -5.2623978 -6.7624474 -6.4564018][-10.306047 -9.256403 -7.8616381 -3.4990702 -1.9755642 0.96861362 3.2406812 3.8657575 3.9717021 2.9041867 2.8546834 -1.1230741 -3.7930088 -5.715642 -5.7149515][-11.304543 -10.898472 -11.206949 -8.06626 -6.4257784 -3.765801 -2.1621592 -1.9011931 -1.3524871 -1.871594 -1.5857205 -5.2691188 -6.5406346 -6.785759 -6.1889486][-14.814074 -15.025248 -13.253345 -9.4071417 -8.88942 -8.3322668 -8.7649059 -8.8809385 -7.933918 -7.5458674 -7.0570807 -8.9782572 -9.0527172 -8.7900171 -8.5146227][-14.308434 -13.354525 -11.676323 -9.1541882 -8.4250174 -7.6131325 -7.929986 -8.53135 -9.4282188 -8.7233067 -7.5618944 -8.250597 -7.9341636 -7.8008928 -7.638474][-11.619661 -11.61415 -10.376223 -8.2465 -6.9366312 -6.2122073 -6.5360436 -6.2959609 -6.2391911 -6.3108854 -5.9005833 -6.2851663 -5.5650282 -6.4730568 -5.5447946][-8.6643362 -9.0443707 -7.9025788 -5.4109631 -5.0279117 -4.5385518 -4.8396883 -4.6482811 -4.6362858 -3.8726912 -3.6608925 -5.4170055 -5.1884093 -6.3652048 -6.7925687]]...]
INFO - root - 2017-12-16 01:09:47.155019: step 80510, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 47h:43m:22s remains)
INFO - root - 2017-12-16 01:09:53.675094: step 80520, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 46h:28m:49s remains)
INFO - root - 2017-12-16 01:10:00.206811: step 80530, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 44h:59m:20s remains)
INFO - root - 2017-12-16 01:10:06.777469: step 80540, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 46h:23m:42s remains)
INFO - root - 2017-12-16 01:10:13.420289: step 80550, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.674 sec/batch; 47h:08m:41s remains)
INFO - root - 2017-12-16 01:10:19.992967: step 80560, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 45h:53m:21s remains)
INFO - root - 2017-12-16 01:10:26.515519: step 80570, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 45h:58m:49s remains)
INFO - root - 2017-12-16 01:10:33.147659: step 80580, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.669 sec/batch; 46h:50m:53s remains)
INFO - root - 2017-12-16 01:10:39.748700: step 80590, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 47h:07m:31s remains)
INFO - root - 2017-12-16 01:10:46.313859: step 80600, loss = 0.15, batch loss = 0.11 (12.7 examples/sec; 0.632 sec/batch; 44h:13m:30s remains)
2017-12-16 01:10:46.860208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0744638 -7.0256433 -6.3896255 -5.9413218 -5.9636984 -5.9304395 -6.0095425 -6.3154316 -6.262321 -5.470089 -4.815927 -6.7188053 -7.24799 -7.0552378 -7.3917418][-5.3839498 -4.9094048 -4.5256777 -4.1829629 -3.9463744 -3.8930187 -4.0319109 -4.3985629 -4.9265304 -5.41329 -5.1543722 -6.2809567 -6.9293971 -7.0141559 -6.7318707][-3.3912997 -3.5560994 -3.4119232 -2.7354281 -2.6619027 -2.3594334 -2.4057317 -2.843621 -2.9397411 -3.0181162 -3.3832171 -5.6234193 -6.5038309 -6.7398481 -7.0541067][-2.7725375 -2.7036028 -2.6026826 -2.0874908 -1.6915669 -1.3829756 -1.27145 -1.3183265 -1.4948969 -1.817281 -2.0518165 -4.3401074 -5.4221811 -5.3341279 -5.840641][-2.9112267 -2.8930144 -3.424346 -3.1790676 -3.2039354 -2.149569 -1.1809688 -1.4127841 -1.8956594 -2.1606145 -2.4137025 -5.123733 -6.5603905 -7.15107 -7.64519][-4.8285604 -4.2717113 -3.6764238 -2.95899 -2.2114375 -0.78932476 0.73952532 1.2578669 0.914721 -0.014097691 -1.1763129 -3.9398918 -5.5097346 -6.2033992 -6.9343753][-6.4986815 -5.85422 -4.7618 -2.7914288 -1.1505647 0.33459425 1.6421919 2.5853019 3.1062055 1.3923945 -0.654284 -4.1871076 -6.3482251 -7.1621146 -6.880621][-7.0747304 -6.4662867 -4.8175993 -1.6204906 0.22719383 1.9276738 3.6426768 3.4927459 3.0148292 1.4151568 -0.60186005 -4.5746908 -6.7128372 -6.9099975 -6.7053938][-7.0110703 -5.8054867 -3.7483931 -1.0827074 -0.16556406 1.805357 3.4383016 2.8866029 2.6668162 1.7366476 0.17198324 -4.5117645 -7.0979657 -7.2525082 -7.3854218][-5.2200842 -4.5072222 -3.1537821 -1.0110064 -0.20331097 1.0120177 1.6328635 0.94662333 0.51912451 0.19753742 -0.43382311 -4.7050052 -7.2968125 -7.8101144 -8.7349834][-6.6612158 -5.6603661 -4.1651773 -2.7621517 -2.4158094 -2.0365613 -1.7536566 -1.8634181 -2.1095908 -2.9758525 -3.3996198 -6.6380215 -8.8445835 -8.6851473 -8.6790791][-10.431787 -9.0005941 -7.1005898 -5.8319163 -6.0480318 -5.7606206 -5.6400237 -5.5346336 -5.5939345 -6.2002296 -6.1994352 -8.15484 -8.9005556 -8.69022 -8.2500248][-12.392756 -11.364902 -10.435501 -10.012558 -9.20383 -8.3598757 -8.5762939 -9.0457916 -9.619236 -9.7379723 -9.4025764 -9.61142 -8.8277416 -8.263876 -7.8984032][-10.921183 -11.253107 -10.434998 -8.6792774 -7.9394956 -8.4880629 -9.1993065 -8.7351084 -8.255518 -7.9351931 -7.8581705 -7.7198906 -7.2966771 -7.0982018 -6.7450471][-8.0729332 -7.9437532 -7.7153726 -6.8185978 -5.5337915 -5.3864527 -5.3898149 -6.4164133 -6.8951435 -6.1641688 -5.7895365 -6.6258221 -7.3836765 -7.0247774 -7.4158473]]...]
INFO - root - 2017-12-16 01:10:53.442742: step 80610, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 45h:27m:50s remains)
INFO - root - 2017-12-16 01:11:00.013968: step 80620, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 46h:24m:34s remains)
INFO - root - 2017-12-16 01:11:06.614549: step 80630, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.643 sec/batch; 44h:58m:13s remains)
INFO - root - 2017-12-16 01:11:13.227690: step 80640, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 44h:55m:37s remains)
INFO - root - 2017-12-16 01:11:19.755366: step 80650, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 45h:17m:59s remains)
INFO - root - 2017-12-16 01:11:26.366142: step 80660, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 44h:47m:24s remains)
INFO - root - 2017-12-16 01:11:32.882457: step 80670, loss = 0.13, batch loss = 0.09 (11.5 examples/sec; 0.693 sec/batch; 48h:29m:54s remains)
INFO - root - 2017-12-16 01:11:39.508664: step 80680, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 45h:51m:09s remains)
INFO - root - 2017-12-16 01:11:46.112949: step 80690, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 47h:01m:09s remains)
INFO - root - 2017-12-16 01:11:52.662159: step 80700, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 45h:43m:46s remains)
2017-12-16 01:11:53.321023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.0725193 -9.5323143 -10.276615 -10.305279 -11.51539 -11.140161 -10.685094 -9.540659 -9.2017088 -9.7590523 -10.843466 -14.129717 -15.958521 -14.447044 -9.9754028][-7.6161242 -8.05693 -8.9941521 -8.8861656 -9.9965248 -10.670101 -10.644399 -10.452099 -10.101902 -9.5054531 -9.4494991 -11.80752 -13.910517 -14.402512 -10.810738][-6.7670283 -7.33358 -8.6641979 -9.4926624 -10.222101 -9.83213 -9.5548611 -9.3222389 -9.2541 -9.2914391 -9.1424923 -10.452557 -12.148263 -12.158714 -9.3168755][-6.5167551 -7.1497006 -8.3499165 -7.2290163 -6.8509507 -6.8066888 -6.8346624 -6.8507204 -6.8129253 -7.200428 -8.1788788 -9.9427261 -11.437788 -10.196337 -7.742938][-7.1523647 -7.7896657 -8.1483145 -5.8696795 -4.6609697 -2.7693443 -1.8028226 -2.5403249 -2.8807185 -3.0565047 -4.3240967 -7.12384 -9.5436077 -9.6730366 -7.4967995][-8.1517973 -6.8891454 -5.8263364 -3.2593367 -1.4081998 1.5242181 3.976212 3.3290792 1.4303656 -0.53101444 -1.9263699 -4.1052856 -6.725678 -8.35853 -7.037333][-7.614069 -5.821209 -4.57991 -1.7267776 0.6705246 4.0559239 7.32214 7.37258 6.3326011 3.5864015 0.049246311 -3.4300077 -5.4391489 -5.7104526 -5.1843147][-6.5808744 -5.0091419 -4.1047583 -0.091355324 2.9691072 5.7237067 7.6207557 7.9208703 6.9984069 3.8902831 0.81899929 -2.5403869 -6.1225142 -6.4962344 -4.9007139][-6.0300059 -4.3315182 -3.5218735 -0.90564203 1.6598563 4.6280189 6.2011437 6.433311 5.1069865 2.823998 0.19132185 -4.6204996 -9.4596186 -9.7339945 -7.4548888][-6.6845875 -4.7820163 -3.7827413 -1.7445717 -1.0936208 2.1099224 3.9080033 3.780683 2.0501657 0.096123695 -2.5208442 -7.8485165 -12.166568 -13.133053 -11.634948][-10.745733 -9.3776779 -7.5156136 -4.2315092 -3.4972823 -2.4745295 -2.3871393 -1.6426697 -2.1813776 -3.838531 -6.9042592 -11.73707 -16.223652 -18.060125 -15.353563][-11.431036 -11.181698 -10.319194 -6.8852377 -5.4151487 -4.6982646 -5.3937058 -6.3558006 -7.3604317 -8.6891775 -10.704353 -13.330486 -16.286102 -17.156084 -14.223][-11.369995 -9.3012705 -8.4627495 -6.9545546 -6.1861415 -5.4188981 -5.2513618 -6.0624304 -8.0926352 -9.8189144 -11.360077 -13.328104 -14.556765 -13.681667 -9.9181309][-10.111265 -8.4937172 -7.2151775 -6.652586 -6.2872519 -6.0327854 -5.5560284 -5.8438654 -6.3609018 -7.4729109 -8.85885 -9.5806084 -9.4837971 -8.1791325 -5.3846555][-7.4873867 -6.6428895 -5.1248035 -3.4296703 -2.7180691 -3.4779956 -5.1304331 -6.1479359 -6.8987575 -7.6287355 -8.3629274 -8.4016752 -8.5361509 -8.2817373 -6.4807386]]...]
INFO - root - 2017-12-16 01:11:59.865639: step 80710, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.636 sec/batch; 44h:27m:08s remains)
INFO - root - 2017-12-16 01:12:06.468517: step 80720, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 46h:32m:46s remains)
INFO - root - 2017-12-16 01:12:13.158790: step 80730, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 46h:00m:37s remains)
INFO - root - 2017-12-16 01:12:19.685765: step 80740, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 45h:10m:42s remains)
INFO - root - 2017-12-16 01:12:26.208655: step 80750, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.650 sec/batch; 45h:26m:14s remains)
INFO - root - 2017-12-16 01:12:32.741697: step 80760, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 46h:34m:07s remains)
INFO - root - 2017-12-16 01:12:39.337330: step 80770, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 45h:48m:35s remains)
INFO - root - 2017-12-16 01:12:45.906684: step 80780, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 45h:12m:46s remains)
INFO - root - 2017-12-16 01:12:52.503249: step 80790, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 46h:00m:32s remains)
INFO - root - 2017-12-16 01:12:59.082281: step 80800, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 45h:20m:14s remains)
2017-12-16 01:12:59.580248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8339067 -6.80236 -7.2948852 -7.2131071 -7.6156282 -7.5527277 -7.4922085 -7.587481 -7.4720387 -7.2695808 -6.1051006 -4.4708023 -5.7658587 -6.6544662 -7.472343][-6.1396084 -5.734561 -4.8174367 -5.2888436 -6.0350337 -6.1896429 -6.291925 -6.2401581 -6.8046179 -6.300571 -4.7834606 -4.2819943 -6.3891377 -6.2512922 -7.3086658][-5.4350009 -5.4982114 -5.466805 -5.2753544 -6.1176248 -5.9604683 -5.9144506 -5.7464619 -5.4405613 -5.1624036 -4.9310212 -4.2032056 -6.5040865 -7.5454726 -9.2302][-5.6700792 -5.3807368 -4.745923 -4.5576153 -5.0833354 -4.7788363 -4.454073 -4.4501381 -4.8306766 -4.0501375 -3.1511483 -2.8804474 -5.1270823 -6.7404165 -9.2607822][-6.1731362 -7.4068308 -7.071178 -5.4486914 -4.5938177 -2.4071021 -0.70188427 -2.2342582 -4.254159 -3.9084501 -3.1405902 -2.1827009 -4.0547485 -5.5605021 -7.3367996][-7.4969807 -7.4866705 -6.13974 -4.28186 -2.1363869 0.39850283 3.5784154 3.6515126 1.9943237 -0.20839214 -2.1413405 -1.9870832 -3.7763877 -5.06815 -6.270165][-10.045195 -9.1092148 -6.5993595 -3.3940871 -1.7682209 1.3140893 5.5759816 6.8423047 6.4748979 2.3775344 -1.5739765 -1.7800472 -4.5374646 -5.82439 -6.5183468][-10.87797 -9.1327238 -5.4535742 -1.8253226 1.0889826 4.5386539 6.4134021 6.4597774 6.5921779 4.0298648 1.146853 -1.6945696 -6.8796015 -8.0831013 -8.7623062][-7.3954554 -6.4506693 -4.8625913 -1.8383684 0.49747515 3.0419583 4.376719 5.4794793 4.9055657 2.6709189 0.30053759 -3.1492512 -7.776063 -9.32821 -11.454412][-5.1640725 -3.7525384 -2.6407473 -1.6987581 -0.57697916 0.19954252 1.8182902 2.6480155 1.9462652 1.1455054 -0.56616306 -3.3635726 -8.1370945 -10.273291 -13.219193][-8.5440006 -7.563262 -6.5623074 -5.3090563 -4.8406315 -5.0173445 -4.4060078 -4.0297942 -4.465682 -4.1566815 -4.6989841 -6.0817218 -9.0401669 -11.253677 -12.780553][-11.858881 -11.053764 -9.893693 -9.39699 -8.8854818 -9.0343313 -9.0138388 -9.92062 -9.4716082 -8.1845036 -7.9171114 -8.9591465 -11.027187 -11.869759 -12.188308][-15.118427 -14.784037 -13.860252 -12.336864 -11.987577 -11.47291 -11.254421 -11.438562 -11.676044 -11.728722 -10.84668 -9.1602268 -9.9102993 -10.983574 -10.678762][-13.250391 -13.453605 -12.230406 -11.155878 -10.723075 -10.255899 -9.5902462 -8.7604837 -9.1469669 -9.9838543 -9.5529747 -7.5760226 -7.4498181 -6.7174006 -6.8923111][-9.9981127 -10.512302 -9.7402 -8.013135 -7.263351 -6.2640696 -6.3721757 -6.4040685 -6.9565334 -6.9535503 -7.7061462 -6.5917239 -7.5154796 -7.3030505 -6.7439461]]...]
INFO - root - 2017-12-16 01:13:06.208477: step 80810, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 45h:35m:44s remains)
INFO - root - 2017-12-16 01:13:12.741619: step 80820, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 46h:04m:31s remains)
INFO - root - 2017-12-16 01:13:19.377423: step 80830, loss = 0.21, batch loss = 0.16 (12.0 examples/sec; 0.669 sec/batch; 46h:44m:02s remains)
INFO - root - 2017-12-16 01:13:25.934054: step 80840, loss = 0.29, batch loss = 0.24 (12.3 examples/sec; 0.652 sec/batch; 45h:34m:34s remains)
INFO - root - 2017-12-16 01:13:32.520831: step 80850, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 46h:07m:20s remains)
INFO - root - 2017-12-16 01:13:39.096364: step 80860, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 45h:33m:16s remains)
INFO - root - 2017-12-16 01:13:45.721826: step 80870, loss = 0.32, batch loss = 0.28 (12.2 examples/sec; 0.655 sec/batch; 45h:47m:56s remains)
INFO - root - 2017-12-16 01:13:52.285803: step 80880, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 45h:05m:35s remains)
INFO - root - 2017-12-16 01:13:58.803433: step 80890, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 45h:09m:32s remains)
INFO - root - 2017-12-16 01:14:05.364412: step 80900, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 45h:48m:40s remains)
2017-12-16 01:14:05.906570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2644224 -7.1877928 -7.7478657 -6.7199397 -6.9598575 -7.3218269 -7.8275042 -8.1100254 -7.9918628 -8.77047 -9.22152 -9.4011335 -9.1925526 -7.094285 -3.4455187][-8.5348454 -7.2678771 -6.4250183 -5.0493684 -5.1974983 -5.1499286 -5.1242094 -5.2828021 -5.7916102 -6.8514333 -7.1166363 -7.44426 -8.1473484 -6.9947333 -2.8158269][-7.1335707 -6.7987795 -6.3525324 -4.0364342 -4.534759 -4.5955257 -4.0272331 -3.594775 -3.3095496 -4.4626727 -5.5619678 -6.4836917 -5.418819 -4.6888409 -1.5587392][-6.5584335 -6.0815616 -6.1633482 -5.3217378 -5.6276555 -4.6856503 -4.0195055 -4.3318057 -4.0061154 -3.7797389 -3.9877033 -5.1915178 -6.0127859 -5.0149755 -0.3713131][-8.0129662 -7.7100883 -7.5871711 -7.0097027 -7.2262893 -4.6055903 -2.4426584 -3.0326824 -3.6982615 -2.7890465 -3.1045082 -4.1118193 -5.1479197 -4.8105931 -1.2818232][-8.1129351 -7.5173512 -7.0287642 -5.4299865 -4.0810165 -0.850399 2.0574389 2.0170436 0.6492486 -0.38087273 -1.6947994 -1.9545128 -3.1109219 -4.1155424 -2.1298783][-9.4199667 -8.5842648 -6.7519913 -3.1034434 -1.246489 1.9417005 5.9459395 6.480679 5.684833 1.7309594 -2.1505013 -3.3383229 -4.7587156 -4.9113817 -2.4119163][-8.8500271 -7.8706312 -6.944355 -3.6780908 -0.9768014 3.3429475 7.75219 7.7205129 6.8159785 3.4165874 -0.22050953 -2.9661925 -5.4602275 -5.5537229 -3.5932591][-8.0722713 -5.636651 -4.9562378 -2.4326394 -0.4452405 3.0793214 5.9215074 6.1825156 4.8512158 0.66135216 -3.1461754 -5.4281831 -6.7686076 -7.454793 -5.296443][-8.1318312 -6.6112976 -6.2112722 -4.4758906 -3.3009043 -0.21032619 2.2303724 2.570251 1.2595258 -2.4674671 -5.5252666 -6.9714503 -7.751287 -7.6239939 -5.4268103][-7.9707336 -7.2384634 -6.6030412 -5.3896971 -5.584631 -5.0720439 -4.1627717 -3.0887196 -4.2909393 -6.0555387 -7.0202284 -8.023468 -8.8676682 -8.99505 -6.9992056][-13.337515 -11.938604 -11.088921 -10.946843 -10.893497 -10.337196 -10.474943 -10.334346 -10.075505 -10.422083 -10.89831 -10.874029 -10.514019 -10.077419 -7.4619536][-14.042952 -13.165545 -13.041523 -12.115246 -12.621168 -12.415963 -12.315195 -12.599185 -12.362536 -11.17038 -9.8182459 -9.400279 -9.97048 -9.9051447 -7.2915335][-11.266421 -10.232458 -10.447649 -10.207115 -9.916029 -9.7814121 -10.943695 -10.587257 -10.149661 -9.5469875 -9.3816175 -7.9998789 -7.7800059 -7.0460625 -5.2286329][-9.2531567 -7.6536784 -6.9114113 -5.7841249 -5.5825129 -5.5817537 -5.3302279 -5.334095 -6.2551775 -6.3544693 -6.1668196 -6.0298314 -6.7780671 -6.6831746 -6.1213837]]...]
INFO - root - 2017-12-16 01:14:12.524766: step 80910, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 46h:23m:07s remains)
INFO - root - 2017-12-16 01:14:19.121377: step 80920, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 44h:55m:07s remains)
INFO - root - 2017-12-16 01:14:25.782268: step 80930, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.676 sec/batch; 47h:13m:57s remains)
INFO - root - 2017-12-16 01:14:32.412282: step 80940, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.679 sec/batch; 47h:26m:39s remains)
INFO - root - 2017-12-16 01:14:39.031207: step 80950, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 45h:36m:55s remains)
INFO - root - 2017-12-16 01:14:45.605465: step 80960, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 45h:40m:15s remains)
INFO - root - 2017-12-16 01:14:52.140658: step 80970, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 44h:43m:01s remains)
INFO - root - 2017-12-16 01:14:58.808476: step 80980, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 46h:24m:57s remains)
INFO - root - 2017-12-16 01:15:05.361860: step 80990, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 46h:19m:55s remains)
INFO - root - 2017-12-16 01:15:11.957040: step 81000, loss = 0.16, batch loss = 0.11 (12.8 examples/sec; 0.627 sec/batch; 43h:48m:43s remains)
2017-12-16 01:15:12.447869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3100858 -1.4228206 -1.4590182 -1.3572707 -2.6401689 -3.202785 -3.1133497 -3.0813365 -2.7540424 -1.896981 -1.5350995 -3.9651089 -6.3961487 -8.0949268 -7.8707771][-2.1597648 -1.8393703 -2.6918244 -3.1537941 -4.0714221 -4.1803536 -4.7682071 -5.4244003 -5.3555937 -4.5454745 -3.6307576 -4.9761724 -6.8497386 -8.5339289 -8.3609829][-0.79379749 -1.784148 -2.2846775 -2.420115 -3.3698456 -4.5717463 -5.2860055 -5.3083677 -5.3199573 -5.502161 -4.9565172 -5.7767539 -7.5538483 -8.5857258 -7.6338186][-2.1127234 -2.214025 -2.2595718 -2.3375242 -3.221158 -3.1756098 -3.0541074 -3.9401035 -4.6766443 -4.7614088 -4.948319 -6.0613813 -7.4054451 -8.2657681 -7.5281439][-3.0086212 -2.7866864 -3.3817143 -2.2523115 -1.6294179 -1.0442786 -1.0335875 -1.6935496 -2.4802575 -3.1328969 -3.7048841 -5.2199759 -7.1235456 -8.7059536 -8.044158][-5.2962284 -4.7604094 -4.167697 -1.6137533 0.065094948 1.4395394 2.4714704 2.4444833 1.0903864 -0.81569719 -1.788295 -3.3660951 -5.3089943 -6.1119242 -5.7335691][-7.4664674 -6.6274033 -5.3795953 -2.2891426 0.449965 3.7855439 5.8831048 5.6471858 4.5183978 2.0399098 0.32319593 -1.5605054 -3.267163 -4.2309327 -4.4525957][-7.6251097 -7.4662247 -6.5922265 -3.118129 0.0011978149 4.1130843 6.1981559 5.9601655 5.1783452 3.2414174 1.8117299 -0.40871859 -2.5738828 -3.8620243 -3.7762582][-6.5617132 -6.7651515 -6.6107287 -3.3126912 -0.43076372 2.92946 4.5111442 4.4381108 3.5571208 2.3042426 2.1178927 0.30279779 -1.5160213 -3.4457679 -4.2028151][-6.6104302 -6.9500847 -6.5663142 -3.7749927 -1.692997 1.4903264 3.1056581 3.37291 2.264924 1.3027005 1.7165523 0.42448378 -1.1886559 -3.5997477 -5.4199505][-10.282127 -9.7552748 -9.3728533 -7.3934927 -5.469944 -2.7181911 -1.3263111 -1.3595877 -1.8404765 -1.8847558 -1.6100693 -2.6998928 -3.3857844 -4.9735551 -6.2059131][-12.84348 -12.76338 -12.290909 -10.21633 -8.6803226 -6.8490758 -6.0042844 -5.5683532 -5.4503741 -5.20356 -4.7423759 -4.9935284 -5.3732762 -6.63992 -7.1584768][-13.189058 -12.904394 -11.846228 -11.617048 -11.392019 -10.0742 -9.7183313 -8.7563753 -7.9752083 -7.6303477 -6.5886564 -6.7317133 -6.7234488 -7.8725328 -7.8283844][-10.452147 -10.777399 -10.535254 -10.398455 -10.32081 -10.101357 -10.349169 -9.8320656 -9.2278385 -9.0193 -8.4041939 -8.0439339 -7.5689726 -7.9101992 -8.163002][-8.0049992 -7.5841656 -7.0145297 -6.2592864 -5.8925939 -6.1533175 -6.5018053 -7.0627117 -7.7727909 -7.3955727 -7.672153 -8.8091364 -9.6905079 -9.3404493 -9.4662285]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 01:15:19.044101: step 81010, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.675 sec/batch; 47h:07m:38s remains)
INFO - root - 2017-12-16 01:15:25.646218: step 81020, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 45h:41m:19s remains)
INFO - root - 2017-12-16 01:15:32.326648: step 81030, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 45h:54m:39s remains)
INFO - root - 2017-12-16 01:15:38.923908: step 81040, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.691 sec/batch; 48h:15m:10s remains)
INFO - root - 2017-12-16 01:15:45.517617: step 81050, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 45h:36m:20s remains)
INFO - root - 2017-12-16 01:15:52.045479: step 81060, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 45h:59m:36s remains)
INFO - root - 2017-12-16 01:15:58.568934: step 81070, loss = 0.23, batch loss = 0.18 (12.6 examples/sec; 0.634 sec/batch; 44h:16m:31s remains)
INFO - root - 2017-12-16 01:16:05.171483: step 81080, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 45h:06m:27s remains)
INFO - root - 2017-12-16 01:16:11.815397: step 81090, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 45h:28m:50s remains)
INFO - root - 2017-12-16 01:16:18.447219: step 81100, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 45h:33m:17s remains)
2017-12-16 01:16:18.992531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2614036 -7.1894131 -6.6507244 -5.3817215 -5.088088 -5.2703648 -5.4752254 -5.0910792 -4.9210277 -4.8084354 -4.6401582 -6.0158272 -7.6301785 -7.1343069 -6.9726968][-5.6719975 -6.6654749 -6.2151465 -4.9860773 -4.8348484 -4.1369553 -3.195864 -3.6733663 -4.1174355 -3.456969 -3.1979241 -4.8964834 -6.73628 -6.7893677 -7.3507996][-5.5229435 -4.8169575 -4.5108256 -4.222578 -3.4338605 -3.3105369 -3.2007718 -2.8997042 -3.136189 -3.5542908 -3.5167665 -4.9459505 -6.5691295 -6.336854 -7.0447588][-3.7614584 -4.5517454 -4.9918914 -4.7317643 -4.2531223 -3.6364827 -3.6288078 -3.5130014 -3.6441932 -3.536166 -3.3816168 -5.0738745 -6.2458367 -6.6008739 -7.445425][-3.0726893 -4.4103079 -4.1250496 -3.5343392 -2.5975413 -1.5649853 -1.0312314 -1.6971636 -2.6531219 -2.8769057 -3.5897658 -5.2454929 -6.325387 -6.0487766 -6.005085][-4.079546 -4.5090942 -3.2225924 -2.052974 -0.27688503 1.4870815 1.8870997 1.2685862 0.26515388 -1.3306508 -2.5624437 -3.5756946 -4.8361578 -4.830575 -4.7336836][-3.7390518 -3.9741697 -3.4004097 -0.84080267 1.43577 2.9192958 4.0362706 3.6890769 3.0605769 1.3842359 0.45931673 -1.7601204 -3.6540775 -3.4741538 -3.6458952][-3.5251236 -2.4863019 -0.97993231 0.98544312 2.0744886 3.4853597 4.4839816 4.504312 3.7786908 2.40165 1.7358217 0.040952682 -1.5071301 -1.6155677 -2.2201116][-2.3484871 -1.3422594 0.28999949 2.4396024 3.238667 3.3058906 3.1318336 2.6925797 1.5293546 0.89295864 0.96741104 -0.46691179 -1.8316302 -1.5818262 -1.5156288][-1.6852322 -0.71427965 -0.095856667 2.1121716 3.0390267 2.3974509 2.0891342 1.189342 -0.48393726 -0.70704222 -0.15504837 -1.7300687 -3.3188748 -2.5616009 -2.6575425][-5.0988035 -4.2134204 -2.403832 -1.4562683 -1.2472687 -0.69782829 -0.50241852 -1.6905746 -2.8076308 -2.3258319 -2.6209905 -4.6139259 -5.2909741 -5.110024 -5.0718365][-9.1224117 -8.989645 -7.0385213 -5.218451 -4.4315448 -4.2920961 -4.5336266 -5.1027827 -5.1079736 -5.5246568 -5.7657118 -6.3677416 -6.6600094 -6.543611 -5.8080921][-10.664305 -8.7319307 -7.5320249 -6.60522 -6.09666 -6.0566225 -6.506207 -6.5762429 -7.0329504 -7.0756702 -6.9940872 -7.4898643 -7.4324417 -6.180964 -5.5076361][-8.1502771 -7.5278678 -7.2009387 -5.5238605 -4.1718187 -5.06649 -6.0735373 -6.1747437 -7.1600471 -6.9660425 -7.5534515 -8.0355263 -7.8163152 -6.4166827 -5.8533869][-5.0096111 -4.9721704 -3.8995829 -4.0209465 -4.1750722 -3.3228352 -2.9585032 -4.2455492 -5.5610061 -5.7483287 -6.073627 -6.1615119 -6.5258327 -6.8429732 -7.3038025]]...]
INFO - root - 2017-12-16 01:16:25.544197: step 81110, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 46h:30m:04s remains)
INFO - root - 2017-12-16 01:16:32.122989: step 81120, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.655 sec/batch; 45h:45m:01s remains)
INFO - root - 2017-12-16 01:16:38.765516: step 81130, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 45h:35m:09s remains)
INFO - root - 2017-12-16 01:16:45.369698: step 81140, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 45h:42m:53s remains)
INFO - root - 2017-12-16 01:16:51.878656: step 81150, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 45h:32m:35s remains)
INFO - root - 2017-12-16 01:16:58.418633: step 81160, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 45h:47m:47s remains)
INFO - root - 2017-12-16 01:17:05.046043: step 81170, loss = 0.22, batch loss = 0.17 (12.0 examples/sec; 0.668 sec/batch; 46h:40m:07s remains)
INFO - root - 2017-12-16 01:17:11.647820: step 81180, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 47h:21m:46s remains)
INFO - root - 2017-12-16 01:17:18.315129: step 81190, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 46h:05m:41s remains)
INFO - root - 2017-12-16 01:17:24.896671: step 81200, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 45h:36m:49s remains)
2017-12-16 01:17:25.424946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9509425 -9.196188 -8.74272 -8.49537 -9.006424 -9.9579115 -10.410818 -9.3073292 -8.0843134 -6.822298 -5.5225825 -6.9308872 -8.2083931 -9.1034718 -9.303422][-8.2534533 -10.116161 -9.28169 -8.917902 -8.8021574 -10.620422 -11.696444 -11.197664 -10.820753 -10.032909 -9.2073307 -10.84341 -11.39612 -10.883965 -10.101734][-7.7257471 -9.0636387 -9.1252346 -8.9959764 -9.0923758 -9.3240633 -9.7750883 -9.8326 -9.4300776 -9.4065742 -9.6302347 -11.656879 -13.023311 -13.304037 -12.222704][-10.607012 -11.336121 -11.348066 -10.404469 -9.1685667 -9.0078506 -8.8827534 -9.1212988 -9.3747454 -9.3819265 -10.332556 -12.778933 -14.287369 -14.341486 -12.793428][-11.854918 -13.986904 -14.924736 -11.38496 -6.0141716 -3.7674873 -3.1279113 -4.5332618 -7.5580335 -8.9003057 -9.7605772 -11.798891 -14.233866 -14.952299 -14.034641][-13.607714 -14.529682 -14.49078 -11.47841 -5.9469752 0.97508907 6.1357188 3.7646031 -1.2659726 -4.455111 -8.2194586 -10.769846 -12.651395 -13.32182 -13.328928][-13.762009 -13.879271 -11.563267 -7.4859896 -2.6584339 3.6013608 10.261024 8.7856026 4.942328 -0.97886705 -7.9126873 -9.7685566 -10.892515 -11.443182 -10.832369][-15.063707 -14.355108 -11.531654 -6.7856941 -0.70887566 4.7610135 10.524815 10.46632 7.4253087 0.37427187 -6.7253852 -10.021017 -12.010878 -10.824383 -8.9960127][-12.519468 -13.518946 -11.955969 -7.7444162 -3.4555888 1.3932853 5.6856494 5.0051913 3.5579562 -2.2156844 -7.0259557 -10.349886 -13.538509 -11.846882 -8.9165621][-10.75577 -10.775667 -11.529345 -9.2611 -5.9930305 -1.9417837 2.3111272 1.7490511 0.029798508 -4.7000418 -8.6281939 -12.17436 -14.348579 -13.55345 -12.602919][-12.498957 -14.03986 -14.893507 -13.591912 -11.241051 -7.856709 -4.7242889 -4.4848828 -4.4475822 -7.5247126 -10.908005 -13.979303 -14.587732 -13.693867 -12.088359][-16.66609 -17.760588 -17.434721 -15.401005 -13.361517 -11.373018 -9.8861475 -8.88555 -7.9882407 -10.361298 -12.44375 -13.64194 -13.686729 -13.578909 -12.378429][-17.807013 -18.107328 -17.926744 -17.162422 -15.352798 -13.318729 -11.227908 -10.265358 -10.552906 -10.780128 -11.513232 -13.08872 -12.83271 -12.232866 -11.066501][-13.416824 -13.307201 -13.496347 -13.325975 -12.405947 -12.099823 -12.038979 -11.321062 -10.682224 -10.366417 -11.180123 -10.905291 -11.078977 -11.295124 -10.544628][-8.2876205 -8.5586433 -8.041729 -6.8047657 -5.9451885 -5.352488 -5.5346394 -6.7015915 -8.0274944 -8.4174128 -9.3685417 -10.498395 -11.989643 -12.485812 -12.640009]]...]
INFO - root - 2017-12-16 01:17:32.056234: step 81210, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 46h:16m:59s remains)
INFO - root - 2017-12-16 01:17:38.636181: step 81220, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.638 sec/batch; 44h:31m:27s remains)
INFO - root - 2017-12-16 01:17:45.310954: step 81230, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 47h:01m:07s remains)
INFO - root - 2017-12-16 01:17:51.885803: step 81240, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 44h:27m:38s remains)
INFO - root - 2017-12-16 01:17:58.461473: step 81250, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 45h:07m:04s remains)
INFO - root - 2017-12-16 01:18:04.997639: step 81260, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.640 sec/batch; 44h:40m:59s remains)
INFO - root - 2017-12-16 01:18:11.711084: step 81270, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 45h:01m:58s remains)
INFO - root - 2017-12-16 01:18:18.298830: step 81280, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.675 sec/batch; 47h:06m:37s remains)
INFO - root - 2017-12-16 01:18:24.849411: step 81290, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 46h:22m:21s remains)
INFO - root - 2017-12-16 01:18:31.436616: step 81300, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 45h:54m:12s remains)
2017-12-16 01:18:31.970185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1415014 -5.0870833 -5.6937947 -5.6716046 -5.5914645 -5.3238316 -5.1376352 -4.7125893 -3.8496873 -3.0150907 -2.6051278 -5.6032047 -7.8156333 -7.198307 -8.2027359][-4.1267357 -4.3863773 -3.3063555 -2.5128536 -3.3512616 -4.805088 -5.2291708 -5.6237044 -5.7018914 -4.7766852 -4.5868158 -8.4992218 -10.586926 -9.5996933 -10.747683][-2.8611455 -3.0629945 -3.1917086 -2.320945 -3.039556 -3.1489005 -3.7669032 -4.7471027 -4.6694155 -4.95711 -5.3363028 -8.3353252 -9.8782749 -9.7330666 -10.470914][-2.388382 -2.8373973 -2.2500954 -1.5809898 -2.0517914 -2.5623899 -3.7221975 -4.0697765 -4.5178547 -4.5292234 -4.9192872 -8.6395855 -10.681924 -10.318465 -10.611427][-1.3718443 -1.56812 -1.2736225 -0.43775463 -0.25445986 -1.1784363 -1.9077449 -2.8748305 -3.906883 -3.3299162 -3.4926517 -7.3426242 -9.4380283 -8.8756294 -9.6271877][-1.0260324 -0.61307573 0.23467827 1.5850725 1.7470765 1.6847978 1.7810664 0.22482252 -0.75237274 -0.70160246 -2.3107679 -5.9287925 -7.5724106 -6.845541 -7.9850025][-2.3980756 -0.72209787 0.12769651 1.0027533 1.1749439 3.0110812 4.0806127 2.3173966 2.3909144 1.6955962 -0.49015188 -4.5113277 -7.1252537 -6.6946411 -8.0703182][-2.3581235 -0.50240564 1.7042298 1.8816929 1.5640016 2.7865663 3.2595496 3.5174117 3.5938792 2.1957974 0.76818085 -4.4243059 -8.1204653 -7.4207182 -8.6443138][-0.87514257 -0.14437008 0.98171329 1.7661572 2.1997275 1.966105 2.2539387 3.717648 4.4458547 3.6125522 2.3380847 -3.2813928 -6.8316174 -7.3906527 -8.978137][-1.7924206 -0.70716381 0.080719948 1.5619078 2.7642732 2.0970635 2.6459632 2.9779992 2.6666913 3.7768302 3.3886504 -1.120986 -3.7116532 -5.1590581 -7.6964092][-5.3242702 -4.2264915 -2.2582054 -1.038486 -0.38203859 0.062461376 0.62840271 0.04039526 -0.21698523 0.5579319 0.64591169 -1.8924966 -4.3009472 -5.0043893 -6.4053917][-9.5993519 -8.3873949 -6.4558654 -4.714046 -4.5952034 -4.1407466 -4.19576 -4.1659288 -2.9527655 -2.4590917 -2.7939367 -4.4438839 -6.0524874 -6.5614252 -7.3068233][-10.630625 -9.9233913 -8.33338 -7.0867562 -6.811368 -7.3180323 -8.0393057 -7.0587935 -6.3137984 -5.0701146 -3.7830834 -4.269938 -5.5370317 -5.1040206 -6.0034471][-8.9783487 -8.3823032 -6.6115594 -6.3514519 -5.8814569 -6.7121229 -7.6154566 -7.8668537 -7.5604725 -6.1490488 -5.1035943 -5.395308 -5.696702 -5.3295884 -5.746511][-6.6210017 -5.4057407 -4.192924 -2.7886434 -1.8729281 -3.1307602 -4.18173 -6.2289438 -7.3330812 -7.907146 -7.5870771 -7.9112945 -8.3963127 -8.0213394 -8.7003193]]...]
INFO - root - 2017-12-16 01:18:38.520883: step 81310, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 45h:42m:54s remains)
INFO - root - 2017-12-16 01:18:45.188230: step 81320, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 46h:33m:29s remains)
INFO - root - 2017-12-16 01:18:51.756638: step 81330, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 44h:51m:35s remains)
INFO - root - 2017-12-16 01:18:58.353720: step 81340, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 45h:16m:02s remains)
INFO - root - 2017-12-16 01:19:04.894783: step 81350, loss = 0.10, batch loss = 0.06 (12.6 examples/sec; 0.634 sec/batch; 44h:14m:55s remains)
INFO - root - 2017-12-16 01:19:11.509645: step 81360, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 46h:10m:09s remains)
INFO - root - 2017-12-16 01:19:18.098180: step 81370, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 44h:37m:54s remains)
INFO - root - 2017-12-16 01:19:24.686155: step 81380, loss = 0.21, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 44h:36m:32s remains)
INFO - root - 2017-12-16 01:19:31.286159: step 81390, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 46h:04m:01s remains)
INFO - root - 2017-12-16 01:19:37.903272: step 81400, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 47h:26m:48s remains)
2017-12-16 01:19:38.439991: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.34237 -7.4224858 -6.9862871 -8.2277517 -9.4998169 -9.5055218 -9.3135872 -8.8621244 -8.8651972 -8.6453962 -7.9162445 -9.9034348 -11.329554 -11.996561 -11.062685][-8.3133392 -8.8952484 -8.60984 -9.3328772 -9.4731932 -9.9080467 -9.9299822 -8.8646011 -9.2153339 -9.7616825 -9.8894539 -11.428614 -12.741929 -12.981686 -12.116341][-6.7474642 -7.4451838 -7.9166894 -9.865118 -9.9886246 -9.3573971 -8.4348621 -7.4528041 -7.5953493 -8.5582695 -9.2087526 -10.207891 -11.532896 -12.373191 -12.360687][-6.2415514 -6.2040715 -5.5770283 -6.2070847 -6.1166143 -5.96905 -5.13433 -5.2129135 -6.6608062 -7.3113432 -7.6108 -10.020794 -11.62936 -11.602152 -11.414419][-5.8201323 -5.7565093 -5.2983356 -4.5689831 -3.0036612 -1.4572029 0.25783491 -1.4558349 -4.3179574 -5.6551409 -6.6205759 -8.5228777 -10.638289 -12.101839 -11.880772][-6.6964073 -6.2471704 -5.0571256 -3.8320162 -1.9358773 1.739809 4.6207852 3.1511025 0.88739395 -1.8400855 -3.8398237 -5.7269988 -7.8785563 -9.5298576 -10.456732][-6.5182943 -7.0719352 -6.3165703 -3.7020314 -1.1742868 2.2243638 5.8973861 6.7813354 5.4938254 1.1496654 -2.5652976 -4.8275828 -7.0718689 -8.0326347 -8.00863][-7.3633013 -7.6487732 -5.8112473 -2.0182762 0.74265766 3.8703332 7.4843659 7.7068868 6.2416034 2.9440045 -0.24434853 -3.6576567 -6.2293277 -7.4245572 -7.137228][-5.249444 -5.8184743 -4.2999158 -1.1566229 1.4325752 4.2615008 5.680016 5.4903083 5.1905246 2.4242387 -0.2765317 -3.3577616 -5.8571215 -7.7000561 -7.3181448][-3.4179261 -3.9823012 -3.3106294 -0.61700153 1.3369222 2.4906812 2.8655448 1.6766577 1.1047301 -0.78362989 -2.9780111 -6.5238609 -9.2841587 -10.766777 -10.255121][-7.6013937 -7.3288264 -6.4804134 -4.59914 -3.6643269 -3.5413527 -3.7093461 -4.7933321 -5.0031362 -5.2806721 -6.0054193 -9.2995529 -12.004054 -12.624388 -12.235657][-12.876242 -12.699977 -10.93998 -9.2542038 -8.4611912 -8.6761026 -9.6874046 -10.629517 -11.050053 -10.910656 -10.146385 -10.954778 -11.073599 -11.312296 -11.340668][-15.61327 -14.635437 -12.123605 -11.246904 -11.095908 -10.60203 -11.323088 -12.05652 -11.932437 -11.292967 -10.024207 -9.0851536 -8.8428192 -8.69013 -8.2714653][-11.810978 -12.799728 -12.540869 -10.630144 -9.8130245 -10.347207 -10.926382 -10.429764 -10.049295 -9.4677658 -8.5866795 -6.7461567 -5.7383652 -6.0778475 -5.8451295][-8.4136486 -9.2671242 -8.8340816 -7.3063607 -6.5006185 -7.0576987 -7.2928767 -6.8940463 -6.7341571 -6.18164 -5.736547 -6.1181684 -6.2474122 -5.6822953 -5.3511443]]...]
INFO - root - 2017-12-16 01:19:44.988882: step 81410, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 46h:44m:15s remains)
INFO - root - 2017-12-16 01:19:51.534247: step 81420, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 45h:09m:15s remains)
INFO - root - 2017-12-16 01:19:58.125323: step 81430, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 46h:17m:21s remains)
INFO - root - 2017-12-16 01:20:04.829163: step 81440, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 45h:51m:13s remains)
INFO - root - 2017-12-16 01:20:11.500688: step 81450, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.654 sec/batch; 45h:37m:30s remains)
INFO - root - 2017-12-16 01:20:18.103733: step 81460, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 45h:31m:21s remains)
INFO - root - 2017-12-16 01:20:24.711650: step 81470, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 45h:26m:30s remains)
INFO - root - 2017-12-16 01:20:31.303914: step 81480, loss = 0.24, batch loss = 0.20 (12.3 examples/sec; 0.652 sec/batch; 45h:26m:34s remains)
INFO - root - 2017-12-16 01:20:37.978900: step 81490, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 46h:15m:43s remains)
INFO - root - 2017-12-16 01:20:44.544879: step 81500, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 44h:44m:37s remains)
2017-12-16 01:20:45.036534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3825345 -8.3049583 -8.65449 -8.90285 -10.133953 -10.79939 -11.795402 -11.732741 -10.922359 -10.659496 -9.8377075 -9.366518 -12.422429 -10.913738 -8.4652958][-9.4753466 -9.3142529 -8.9625149 -9.2249737 -10.01509 -10.948645 -12.139177 -12.476124 -12.182681 -10.808914 -9.6618681 -10.364357 -13.119698 -12.051457 -10.933565][-8.6359463 -9.7052183 -10.248036 -9.818922 -9.862524 -10.618653 -11.420344 -11.714751 -11.360851 -10.420385 -9.8948994 -10.517651 -13.588404 -12.752534 -10.948648][-8.8836336 -9.522418 -9.4094925 -8.7865829 -8.69955 -8.5725784 -8.472538 -9.1077356 -9.598362 -8.7124643 -8.17396 -8.9722652 -12.501599 -12.547279 -12.580626][-9.2282848 -11.223362 -11.471753 -9.4823742 -6.5735869 -3.3911717 -1.3562016 -3.4500122 -6.3757415 -7.0751352 -7.8010798 -8.1936808 -11.739267 -11.606152 -11.79974][-10.631062 -12.281185 -11.908623 -9.4450493 -5.105937 1.0362072 5.5610681 4.442492 0.52702856 -3.3957758 -6.6101513 -7.1983533 -10.087279 -9.5752506 -9.9760857][-11.630596 -12.809868 -11.217129 -8.3781319 -4.0687852 2.1238747 7.7350097 10.029362 7.9888453 1.3730626 -4.7198005 -5.7800732 -9.1306562 -8.4990435 -8.2799263][-10.902761 -11.901385 -10.102524 -6.0443144 -1.4963608 3.0335212 7.5998683 9.1481514 9.09255 4.9853158 -1.0381618 -4.9410629 -10.085829 -9.0475922 -8.8050814][-8.8100023 -9.32195 -8.9722137 -5.8089967 -1.6403003 2.8828254 5.5773444 5.830812 5.8805251 4.0405364 0.45372915 -3.8242939 -11.032869 -11.054457 -10.684118][-6.5676079 -7.2357163 -8.05435 -6.7921739 -3.799068 -0.75999594 1.8240709 2.2611442 1.1351695 -0.85388231 -3.0943146 -6.2803583 -11.903643 -13.039886 -13.761414][-9.0268221 -10.895815 -11.379341 -9.3994312 -7.9104366 -6.2270737 -4.087883 -3.9231625 -5.3165417 -6.6273346 -8.533205 -10.644895 -14.292547 -14.694733 -14.232115][-12.269236 -13.053753 -13.393544 -12.693906 -11.671888 -11.246778 -10.675924 -11.358987 -11.776142 -11.616472 -12.0783 -14.091871 -15.966335 -17.034367 -16.274775][-13.680505 -13.301027 -13.827717 -13.823446 -13.193785 -12.466919 -12.64068 -13.789989 -14.837872 -14.017942 -12.889067 -14.090218 -15.527239 -15.489311 -13.519594][-13.771318 -14.21806 -12.966181 -13.366415 -13.271797 -12.346506 -12.136177 -12.522454 -12.781576 -12.486143 -12.503616 -11.862133 -12.248592 -12.152445 -10.992773][-10.735885 -11.35219 -11.039642 -10.196812 -8.7274332 -8.1932764 -8.1958179 -7.87333 -8.3164024 -8.8392868 -9.015851 -9.8424826 -10.461368 -9.6685734 -8.553669]]...]
INFO - root - 2017-12-16 01:20:51.572574: step 81510, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 46h:27m:59s remains)
INFO - root - 2017-12-16 01:20:58.182411: step 81520, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 47h:38m:40s remains)
INFO - root - 2017-12-16 01:21:04.797191: step 81530, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 44h:59m:09s remains)
INFO - root - 2017-12-16 01:21:11.483286: step 81540, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.685 sec/batch; 47h:46m:30s remains)
INFO - root - 2017-12-16 01:21:18.094031: step 81550, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 46h:30m:58s remains)
INFO - root - 2017-12-16 01:21:24.685247: step 81560, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.671 sec/batch; 46h:48m:19s remains)
INFO - root - 2017-12-16 01:21:31.185196: step 81570, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.691 sec/batch; 48h:10m:11s remains)
INFO - root - 2017-12-16 01:21:37.951128: step 81580, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 45h:19m:23s remains)
INFO - root - 2017-12-16 01:21:44.535402: step 81590, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 45h:26m:58s remains)
INFO - root - 2017-12-16 01:21:51.077347: step 81600, loss = 0.20, batch loss = 0.16 (12.2 examples/sec; 0.654 sec/batch; 45h:35m:37s remains)
2017-12-16 01:21:51.587339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.9294882 -11.71288 -12.541839 -10.640341 -9.0366945 -7.6793065 -8.1805124 -8.8385172 -8.2020617 -8.1386576 -6.2142968 -4.9158888 -5.7409577 -8.307126 -7.1670909][-6.6676607 -8.890708 -10.153197 -9.4367476 -8.1267185 -6.446825 -6.555923 -6.604825 -7.511826 -7.5064754 -6.0153227 -5.7785931 -6.0461888 -8.3233795 -8.2896929][-6.0416789 -5.6003957 -6.9747276 -7.2554631 -6.5964055 -6.3676381 -6.703166 -6.4992013 -6.3436155 -6.6101546 -5.7183037 -5.2343431 -5.7463551 -9.21798 -8.8088436][-7.13424 -8.0629539 -8.3750954 -7.4892044 -7.9493475 -7.1622419 -6.6083746 -6.3685284 -6.5552306 -6.0718241 -4.2097154 -4.04633 -5.0805225 -7.7431917 -7.0585494][-7.9235306 -9.5073214 -11.236149 -9.7942219 -8.3881111 -5.5091839 -4.2983074 -4.7132607 -4.7380714 -4.5809646 -4.6777272 -3.9097962 -4.5358429 -7.6207471 -7.4218163][-7.8533754 -8.2562637 -8.9149094 -8.2644329 -7.0888386 -2.5579944 0.48096991 -0.059083462 -0.34027529 -1.5550418 -2.5418255 -1.7963924 -3.1477792 -6.8978319 -6.46733][-8.5855284 -7.5325341 -6.6460171 -4.9609385 -3.9322405 0.519876 2.6379275 3.8553805 4.6837659 1.6071682 -0.037162781 -0.79884005 -2.9769797 -5.6120915 -5.868453][-5.775064 -4.8030014 -3.4221094 -1.2202783 0.55228138 3.5064387 4.296845 5.9423938 6.3628345 4.0723176 1.7760091 -0.23261023 -2.0015507 -5.1583819 -5.4323125][-5.347352 -3.6835072 -1.7900503 0.33239985 1.9857798 3.380652 4.7126288 4.0029311 2.8106828 2.578505 1.4983587 -0.19009161 -2.9334638 -5.6779051 -4.6547003][-4.5774126 -3.3444448 -4.0251217 -2.3737998 -0.040437222 1.98176 2.5705609 1.90377 1.1721263 0.4343667 -0.14931726 -0.47922611 -2.101032 -5.5337591 -5.5009842][-5.3663821 -5.5937924 -5.7653675 -5.3801646 -4.6135111 -2.6279233 -0.79831409 -0.58608675 -1.6426916 -1.8628123 -1.3265667 -1.4400263 -3.1001484 -5.4454794 -4.840951][-10.718556 -9.5816278 -9.0695181 -9.1318874 -9.1478157 -8.2845421 -6.3542662 -5.0004873 -5.2620678 -5.8165288 -5.3635139 -4.1153841 -3.9181614 -5.4950767 -5.8095083][-12.456437 -9.7782621 -9.6164761 -9.4350424 -9.05627 -8.7736883 -8.6327505 -7.2429442 -6.4015822 -6.3583922 -5.7780075 -5.9410095 -6.3649688 -6.26059 -6.3176289][-11.263909 -10.212654 -10.817446 -9.5553446 -8.7626848 -8.4175129 -8.4479694 -7.3124304 -6.4294877 -6.5754142 -5.7421494 -5.0207958 -4.677249 -5.0687351 -5.0132232][-7.7887993 -7.4408846 -7.2815275 -7.1467066 -7.1203203 -6.3700638 -6.7470722 -6.4371552 -6.5102091 -5.8244982 -4.8855867 -3.974484 -4.042098 -4.5846395 -5.4905005]]...]
INFO - root - 2017-12-16 01:21:58.248386: step 81610, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 46h:36m:41s remains)
INFO - root - 2017-12-16 01:22:04.774156: step 81620, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 45h:03m:54s remains)
INFO - root - 2017-12-16 01:22:11.351335: step 81630, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.664 sec/batch; 46h:15m:16s remains)
INFO - root - 2017-12-16 01:22:17.912878: step 81640, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 45h:49m:58s remains)
INFO - root - 2017-12-16 01:22:24.504638: step 81650, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 45h:33m:11s remains)
INFO - root - 2017-12-16 01:22:31.043390: step 81660, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 44h:24m:19s remains)
INFO - root - 2017-12-16 01:22:37.699421: step 81670, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 45h:43m:00s remains)
INFO - root - 2017-12-16 01:22:44.283198: step 81680, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 47h:23m:53s remains)
INFO - root - 2017-12-16 01:22:50.835348: step 81690, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.676 sec/batch; 47h:05m:40s remains)
INFO - root - 2017-12-16 01:22:57.447993: step 81700, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 46h:32m:22s remains)
2017-12-16 01:22:57.958849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.800096 -7.5577707 -7.5293446 -7.0505381 -5.5433378 -4.9498129 -4.6672196 -5.1045575 -5.3662491 -6.6218028 -7.0128345 -9.1144485 -11.381811 -11.99379 -10.9985][-8.1727991 -8.07664 -8.0693569 -7.227829 -6.6042056 -5.6926718 -5.6099482 -5.9374623 -6.7047567 -6.5302925 -6.9004269 -8.8201141 -11.829072 -13.007128 -12.006419][-6.1436453 -7.5866342 -8.5493107 -7.7803178 -6.649879 -6.2232151 -6.06248 -6.7424068 -7.8089161 -8.3245916 -7.7762289 -9.3479443 -11.215385 -11.235451 -10.73661][-6.8079414 -8.5697641 -8.8523731 -8.6621828 -8.3023148 -6.49657 -5.9882779 -6.4783945 -6.6196513 -7.1092205 -7.7759871 -9.4402475 -11.552031 -11.286978 -10.122894][-8.9548292 -10.542414 -11.374658 -9.96594 -7.6240659 -4.4203653 -3.3509991 -3.8515291 -4.2290893 -4.8121638 -5.5312281 -7.5561004 -9.8286829 -10.238882 -10.342035][-9.7446213 -11.57642 -11.271868 -9.6508751 -6.2417192 -1.2080994 1.6477518 0.74799442 -0.52993679 -2.008955 -2.6786983 -4.3404713 -6.9877882 -7.6871595 -7.0694842][-8.8207121 -8.9573088 -8.70408 -6.0501356 -3.2922981 -0.045485973 3.1628881 4.517282 4.4022479 0.92450762 -0.77136278 -2.3046296 -4.6046438 -6.0796437 -6.620523][-7.6316171 -7.0152025 -5.7393312 -1.8158092 1.0478277 3.0481753 4.3648829 4.4745955 4.3125539 2.4238868 0.069194794 -2.543967 -5.7206249 -5.7674332 -5.2541413][-5.5671968 -5.4103189 -4.2327209 -1.3583336 0.9455142 2.8474317 4.0014281 2.7734485 2.416307 1.1011548 -0.17270136 -3.3088734 -6.6891527 -7.8337545 -7.4938083][-1.6099472 -2.1470575 -2.8871739 -2.0925307 -2.0278931 0.31259489 1.6720495 1.1865034 -0.088367939 -1.7814782 -3.1920116 -5.5214682 -8.63058 -9.766161 -9.4472075][-6.3935342 -6.0432959 -6.7649708 -6.5004168 -6.1497512 -5.9952006 -4.886838 -4.5114169 -5.1469736 -6.1797462 -7.1304665 -9.5991554 -12.099535 -11.887941 -10.903129][-9.55068 -9.6723223 -10.200462 -10.393633 -10.147449 -8.9462881 -8.1684628 -8.9092083 -9.1994076 -10.140037 -11.440409 -13.193014 -14.202555 -13.892921 -12.333763][-12.116905 -11.406834 -10.990944 -10.838974 -10.560263 -9.9444294 -9.54814 -10.108303 -10.852648 -11.648859 -12.427677 -13.895554 -14.98671 -13.346523 -10.732938][-11.190966 -10.259904 -9.2958221 -8.1530552 -8.5351257 -7.5196404 -7.9739785 -7.9287415 -8.5664968 -8.881609 -10.104489 -10.090317 -10.530425 -9.676878 -8.4758224][-9.6478348 -9.711875 -8.4499664 -6.743288 -4.7506857 -5.4968648 -6.3271084 -6.2681437 -6.079812 -5.7443709 -5.5631423 -6.6509271 -7.4158735 -7.452033 -7.0617437]]...]
INFO - root - 2017-12-16 01:23:04.476134: step 81710, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 45h:58m:58s remains)
INFO - root - 2017-12-16 01:23:11.047761: step 81720, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.664 sec/batch; 46h:14m:01s remains)
INFO - root - 2017-12-16 01:23:17.564504: step 81730, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 46h:02m:53s remains)
INFO - root - 2017-12-16 01:23:24.114500: step 81740, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 47h:09m:34s remains)
INFO - root - 2017-12-16 01:23:30.687890: step 81750, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.635 sec/batch; 44h:15m:24s remains)
INFO - root - 2017-12-16 01:23:37.271814: step 81760, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 44h:50m:16s remains)
INFO - root - 2017-12-16 01:23:43.802819: step 81770, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 45h:01m:40s remains)
INFO - root - 2017-12-16 01:23:50.352569: step 81780, loss = 0.18, batch loss = 0.14 (11.8 examples/sec; 0.676 sec/batch; 47h:04m:33s remains)
INFO - root - 2017-12-16 01:23:56.951183: step 81790, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.677 sec/batch; 47h:10m:23s remains)
INFO - root - 2017-12-16 01:24:03.552685: step 81800, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 45h:42m:50s remains)
2017-12-16 01:24:04.101433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5158005 -6.35122 -5.5224252 -5.3097997 -6.2694683 -7.7865553 -8.9360065 -8.8807964 -7.9728546 -7.0378227 -6.360754 -8.4558086 -10.355204 -9.470603 -8.471138][-7.59237 -6.1073365 -4.4143119 -4.382144 -5.3309727 -7.3081255 -8.7917166 -9.6442518 -9.2476425 -7.9738417 -6.95954 -9.5471411 -10.831299 -10.44248 -10.041254][-4.8933105 -4.4071078 -3.5797756 -3.0003986 -3.4735243 -5.557056 -7.0611134 -7.8478737 -8.1536188 -8.2758884 -8.1113806 -9.5600529 -10.421577 -10.661894 -10.800397][-5.7239761 -5.3754797 -4.5129743 -3.4329715 -3.4355614 -4.4095058 -5.2287846 -6.5744214 -7.7298613 -8.0441027 -7.6193023 -9.85939 -11.425512 -11.713284 -11.324259][-8.539402 -9.6569195 -8.665904 -5.0957866 -3.0683651 -2.01132 -1.348156 -4.3460674 -6.961236 -7.3617015 -7.4300475 -10.008078 -11.365493 -11.293556 -10.926993][-10.026985 -10.069017 -8.19428 -4.5629458 -1.2980371 1.7321439 4.2012734 2.3386312 0.06704092 -3.0262971 -6.190382 -8.7329607 -9.9403782 -9.7171059 -8.7503757][-11.179581 -9.9520521 -7.0079794 -2.8679118 0.77947283 4.6312375 7.9995761 7.5366817 5.9409719 1.5125589 -2.9311259 -7.0715532 -10.224026 -9.8710318 -9.15377][-10.970573 -9.1394014 -6.9754381 -3.1469188 0.56102943 5.1251884 8.9900761 8.7010708 7.3710837 4.0918984 0.13021898 -6.6075664 -11.565661 -12.219812 -11.835947][-8.9151707 -8.8396292 -7.301682 -4.3296232 -1.2571921 3.0779929 5.9381967 5.5459256 5.0524955 2.2844319 -1.4232879 -6.7184525 -11.375681 -13.357553 -13.846735][-7.7694092 -7.1183467 -7.0976958 -4.0054193 -1.1755252 -0.17028475 1.473897 2.2373943 2.0843549 -0.67197943 -3.55643 -8.4684477 -12.140076 -13.435692 -13.889765][-10.473927 -10.448762 -9.7438507 -7.6888552 -6.5526061 -5.1654067 -3.3096159 -3.4041221 -3.8382473 -5.3565545 -7.1524005 -12.021782 -15.151665 -14.889959 -14.030458][-15.429806 -14.372967 -13.632784 -12.323605 -12.272787 -10.717478 -9.797677 -10.496696 -10.463676 -11.04414 -11.646406 -14.106886 -15.645775 -15.596849 -14.5513][-15.066305 -14.366901 -14.263754 -13.757313 -12.662428 -12.101519 -12.658918 -12.873905 -13.196238 -12.77754 -12.28535 -13.57616 -13.53837 -12.776451 -11.461241][-11.827878 -11.092375 -10.362343 -9.4717941 -9.9812584 -11.14415 -12.078268 -11.330984 -11.057842 -11.500395 -11.570315 -11.246353 -10.74794 -10.144062 -8.8103819][-7.4992061 -6.2859387 -5.150157 -4.4953423 -4.7049236 -5.3887153 -5.9289179 -7.2965779 -8.1208572 -7.4536362 -7.4539394 -8.9139214 -9.2371082 -8.7755852 -8.4905128]]...]
INFO - root - 2017-12-16 01:24:10.698420: step 81810, loss = 0.18, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 44h:04m:01s remains)
INFO - root - 2017-12-16 01:24:17.292022: step 81820, loss = 0.11, batch loss = 0.07 (11.9 examples/sec; 0.669 sec/batch; 46h:37m:07s remains)
INFO - root - 2017-12-16 01:24:23.951682: step 81830, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.693 sec/batch; 48h:14m:29s remains)
INFO - root - 2017-12-16 01:24:30.453199: step 81840, loss = 0.37, batch loss = 0.32 (12.5 examples/sec; 0.640 sec/batch; 44h:34m:34s remains)
INFO - root - 2017-12-16 01:24:37.001313: step 81850, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 46h:45m:51s remains)
INFO - root - 2017-12-16 01:24:43.589162: step 81860, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 45h:37m:53s remains)
INFO - root - 2017-12-16 01:24:50.196243: step 81870, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 45h:53m:04s remains)
INFO - root - 2017-12-16 01:24:56.815198: step 81880, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 45h:21m:47s remains)
INFO - root - 2017-12-16 01:25:03.427542: step 81890, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 45h:41m:35s remains)
INFO - root - 2017-12-16 01:25:10.105149: step 81900, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 46h:38m:53s remains)
2017-12-16 01:25:10.633682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.1879263 -9.0116634 -8.7309208 -8.4563694 -9.4963036 -9.8706951 -9.9516792 -9.2823238 -8.6185961 -7.9226637 -6.2433434 -5.1563692 -5.4511933 -5.2956405 -5.5038157][-9.422 -9.0300131 -7.6711593 -7.455379 -8.4059181 -8.5613413 -8.9610033 -9.2541647 -9.29158 -7.9786444 -5.941227 -5.2933326 -5.5716538 -5.4733248 -6.8388252][-7.4700413 -7.2312469 -7.0333095 -6.0550547 -6.7822189 -7.17418 -7.5721707 -7.2969413 -6.8833666 -7.1328454 -6.7343431 -6.0821915 -6.7329473 -7.0665531 -7.9871941][-8.16092 -7.473567 -6.49268 -6.2274313 -7.0659924 -6.717073 -6.3795371 -6.7550411 -6.8819537 -6.0901604 -5.6133304 -5.8722382 -6.6917648 -7.6510687 -8.6572275][-8.1971283 -9.1905079 -8.339036 -6.8028631 -6.3770928 -4.6647291 -2.9296312 -3.5818114 -5.1457806 -5.4308023 -5.3998661 -5.2446494 -5.7407527 -6.2999878 -6.9175143][-11.079793 -10.005407 -7.9033666 -6.01386 -4.04219 -0.32788658 2.3817182 1.4851561 -0.22896433 -2.4926374 -3.9631197 -3.991869 -4.8109908 -5.2921576 -6.0488138][-12.499159 -11.258004 -8.2275944 -4.7281327 -1.8403149 2.68399 6.1902204 6.4115195 5.0309606 0.35808849 -3.5449662 -4.7526608 -5.796185 -5.932941 -6.5277243][-14.459417 -11.914349 -8.7141733 -4.7133245 -0.7116971 3.2718253 5.9745955 7.3753715 6.98066 2.5163703 -1.6817145 -5.1351175 -8.2344713 -8.3784971 -9.0308542][-12.042355 -10.056701 -7.5151768 -3.4745784 -0.57737684 1.556602 4.0315385 5.2828526 4.45563 1.9239974 -1.0419683 -5.7476487 -9.5468073 -10.395042 -11.376708][-10.92663 -8.970912 -7.018611 -3.1230114 -0.55570078 0.84059286 2.8950715 2.1703744 0.074541092 -0.533823 -2.0051425 -5.5657864 -8.9673309 -11.636681 -14.26111][-8.9938736 -8.8984613 -7.5898967 -4.3805246 -2.8243597 -1.8491883 -0.7088151 -1.4658828 -2.6143186 -3.9539359 -6.0032072 -7.8993034 -10.768661 -12.791506 -14.047218][-13.500451 -12.346588 -10.951497 -10.243876 -9.5812778 -7.9725943 -7.2528648 -8.7643566 -9.2669888 -9.5894194 -10.292793 -10.656784 -12.830671 -14.251829 -15.426947][-14.391602 -13.913517 -12.556875 -11.068436 -11.237619 -10.698422 -10.59428 -11.313631 -11.898216 -11.668653 -11.118982 -10.271404 -11.358505 -11.331415 -11.399874][-14.002777 -13.202883 -11.977097 -10.516305 -9.9893246 -9.30826 -9.9586258 -10.186314 -9.8542938 -9.6785135 -9.6573429 -8.3594437 -7.7712278 -7.8303638 -7.5310574][-9.3986759 -9.1250963 -7.5745468 -5.4230857 -4.7617292 -4.2076511 -4.2089968 -5.4343867 -6.3816018 -6.25336 -6.4419394 -6.0378957 -6.1435785 -5.918664 -5.770648]]...]
INFO - root - 2017-12-16 01:25:17.199943: step 81910, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 45h:19m:34s remains)
INFO - root - 2017-12-16 01:25:23.914162: step 81920, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 45h:56m:25s remains)
INFO - root - 2017-12-16 01:25:30.540278: step 81930, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.639 sec/batch; 44h:28m:34s remains)
INFO - root - 2017-12-16 01:25:37.126185: step 81940, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 47h:05m:52s remains)
INFO - root - 2017-12-16 01:25:43.700465: step 81950, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 44h:42m:08s remains)
INFO - root - 2017-12-16 01:25:50.345877: step 81960, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.685 sec/batch; 47h:42m:06s remains)
INFO - root - 2017-12-16 01:25:56.944885: step 81970, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 46h:06m:12s remains)
INFO - root - 2017-12-16 01:26:03.594251: step 81980, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 45h:03m:45s remains)
INFO - root - 2017-12-16 01:26:10.121153: step 81990, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 45h:43m:37s remains)
INFO - root - 2017-12-16 01:26:16.771477: step 82000, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 46h:23m:09s remains)
2017-12-16 01:26:17.294338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.9421282 -9.5462933 -10.431389 -10.793169 -11.573071 -11.543649 -11.428957 -10.611097 -10.076657 -9.6264229 -8.32232 -9.112606 -10.13843 -10.930655 -11.858392][-9.1502237 -8.7475281 -8.8744488 -9.3459883 -10.112441 -10.296972 -9.9981651 -9.6557388 -10.006043 -9.9064751 -9.3417168 -10.894665 -11.955032 -12.406602 -12.594803][-7.8072472 -8.6644249 -9.9727192 -9.7309189 -10.714996 -10.693007 -10.04331 -9.03818 -8.6752663 -8.7085752 -8.4394836 -9.6203537 -10.779817 -11.589585 -12.239566][-8.77648 -8.9745274 -9.01426 -9.1694078 -10.490572 -9.4899282 -8.1589375 -8.3293629 -9.1002283 -8.006835 -6.7414994 -8.1647358 -9.3105793 -10.478128 -11.764409][-10.754961 -11.797727 -11.437645 -9.6718807 -8.9301643 -5.991528 -3.3829031 -5.5334392 -8.8847084 -8.7882881 -7.5266843 -7.7305236 -8.4728546 -10.120525 -11.608168][-12.747763 -12.548584 -11.611294 -7.2239943 -4.2773113 -0.19808102 4.8839602 2.7399411 -1.7363656 -5.6644392 -9.0276737 -8.330245 -7.8902712 -9.4990549 -11.2262][-13.381615 -12.973633 -11.491147 -4.448616 -0.66012716 3.2828403 8.533844 8.15934 5.4856009 -0.7420969 -6.9597125 -8.3808861 -9.2915649 -10.720911 -11.479036][-12.569578 -11.736925 -10.14408 -4.1584563 -0.2351923 5.59341 10.07527 8.2359486 6.1998725 1.8928003 -3.5990813 -6.9910669 -9.2109632 -10.540364 -11.405308][-9.4013758 -8.7707119 -8.6756678 -5.1126127 -1.8042381 2.6938624 6.4181848 6.116817 4.4246564 0.014623642 -4.1538086 -8.5086241 -10.663414 -10.983702 -12.173504][-7.9806113 -7.0859737 -7.6031814 -5.881166 -4.2680979 -1.4186482 2.0290112 2.7355542 0.8909111 -2.5833347 -6.218492 -9.5339909 -11.371201 -12.538586 -13.765232][-10.73403 -9.9830666 -10.622118 -9.0543213 -8.0625286 -7.1646762 -5.4235549 -5.0572309 -5.3519335 -6.3003058 -8.2146721 -11.757104 -13.470327 -13.253973 -13.239672][-14.95681 -14.153622 -14.142862 -13.273146 -12.621981 -12.337854 -11.846642 -11.583117 -10.734663 -10.619274 -11.627455 -12.995339 -13.20076 -13.749458 -13.468207][-17.99493 -18.317186 -18.261263 -17.158581 -16.98958 -15.666546 -14.462936 -14.553392 -15.050543 -14.238134 -13.134356 -12.855785 -13.088975 -13.085529 -11.950262][-16.31526 -15.322281 -15.210548 -15.730621 -14.898331 -14.50058 -14.191387 -13.141682 -11.792639 -11.911617 -12.030973 -11.59967 -11.182886 -10.713353 -10.117111][-12.083147 -11.359514 -10.130722 -9.5914345 -8.8821354 -9.0932064 -8.9783688 -8.8599129 -9.3644962 -8.9298811 -8.97325 -9.3683748 -10.26967 -10.261135 -10.84832]]...]
INFO - root - 2017-12-16 01:26:23.923720: step 82010, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 46h:52m:39s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 01:26:30.497391: step 82020, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 46h:23m:34s remains)
INFO - root - 2017-12-16 01:26:37.125206: step 82030, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.692 sec/batch; 48h:07m:28s remains)
INFO - root - 2017-12-16 01:26:43.790737: step 82040, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.682 sec/batch; 47h:27m:26s remains)
INFO - root - 2017-12-16 01:26:50.535949: step 82050, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.667 sec/batch; 46h:24m:26s remains)
INFO - root - 2017-12-16 01:26:57.104303: step 82060, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.668 sec/batch; 46h:28m:44s remains)
INFO - root - 2017-12-16 01:27:03.710666: step 82070, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 45h:33m:07s remains)
INFO - root - 2017-12-16 01:27:10.383057: step 82080, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 44h:50m:00s remains)
INFO - root - 2017-12-16 01:27:17.021072: step 82090, loss = 0.17, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 44h:18m:00s remains)
INFO - root - 2017-12-16 01:27:23.668193: step 82100, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 44h:39m:40s remains)
2017-12-16 01:27:24.205599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.2397995 -0.59901524 -1.1824474 -1.1793709 -2.5717068 -3.1739306 -3.4515624 -3.5988345 -3.6524847 -2.9174988 -2.1502123 -5.5652113 -8.8707991 -10.588858 -10.636377][-1.5656939 -1.3522677 -2.408705 -3.1681526 -4.1983819 -5.1011667 -5.9765744 -6.8054094 -6.80242 -5.7133222 -4.8683062 -7.3438454 -9.8700914 -11.548147 -11.232393][-0.32099628 -0.99030161 -2.2651348 -2.7488441 -3.8506594 -5.0164342 -5.8698316 -6.2389879 -6.5123687 -6.744379 -6.36279 -8.1575127 -10.170877 -10.833712 -10.085405][-1.3161087 -1.4243121 -2.1809886 -2.4953029 -3.4831083 -3.6217792 -3.6710122 -4.3815246 -5.4161015 -5.450304 -5.4472289 -7.7380848 -9.7470646 -9.9673576 -9.5987387][-2.2762158 -2.2713804 -3.2834892 -2.7232351 -2.3570285 -1.2008486 -0.77940226 -1.7598147 -3.0124974 -3.6949453 -4.344337 -6.4803023 -8.5307751 -10.012755 -9.9417143][-4.4074407 -4.0050983 -3.9326665 -2.1361024 -0.49182653 1.2950215 2.7025819 2.2461343 0.88823271 -1.2474599 -2.6275282 -4.6831822 -6.886138 -7.5669265 -7.5639229][-6.4094644 -6.0459538 -5.6340208 -2.9320304 -0.36149883 2.8722358 5.9487929 6.2341304 4.8191 1.7101908 -0.1854744 -2.4443245 -4.19443 -4.766571 -5.8994308][-7.1096392 -7.0552769 -6.8397088 -3.6523173 -0.64568806 3.2463746 5.9540076 5.79164 4.9632678 3.1069932 1.9469905 -1.0541615 -3.3888328 -4.2784839 -5.6505237][-5.5066791 -5.8326712 -6.4993625 -4.1882086 -1.7724171 1.6724234 3.63759 3.7905898 3.3690076 2.8114085 2.7924361 -0.37658453 -2.5919797 -3.8826635 -5.5310793][-4.8051343 -5.410852 -5.9148307 -4.0861149 -2.5999866 0.29386044 1.7999582 1.8730717 1.1763701 1.3017974 2.2710757 -0.23241377 -2.1684098 -3.7148273 -6.8497782][-8.704917 -8.1947 -7.6303124 -6.4647164 -5.592308 -3.0602131 -1.7276275 -1.9220052 -2.5604668 -2.3389139 -1.6742258 -3.8908453 -4.4150467 -5.4976649 -8.0133953][-12.510719 -11.82902 -11.257492 -9.93108 -8.99589 -6.9167571 -6.0562615 -6.02546 -6.7306652 -6.7086492 -5.6239409 -6.8488932 -7.2541189 -7.7306337 -8.8196335][-13.651678 -13.25625 -12.161455 -12.061649 -12.203543 -11.22867 -10.878431 -10.007821 -9.0937862 -8.8112888 -8.2248287 -8.5794964 -8.2668209 -8.55115 -8.9813738][-10.41227 -11.141412 -10.547057 -10.441486 -10.441999 -10.713494 -11.167526 -10.386782 -10.094798 -9.8447514 -9.3967972 -9.1102829 -8.7765026 -8.9275284 -9.6118946][-7.2781944 -7.6898055 -7.6135297 -7.0803814 -6.5336256 -6.2664003 -6.7530642 -8.0323725 -9.3712931 -9.1893768 -8.8880281 -10.253243 -11.339469 -11.012094 -11.003046]]...]
INFO - root - 2017-12-16 01:27:30.848542: step 82110, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.655 sec/batch; 45h:32m:40s remains)
INFO - root - 2017-12-16 01:27:37.516242: step 82120, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 44h:28m:01s remains)
INFO - root - 2017-12-16 01:27:44.093645: step 82130, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 45h:36m:26s remains)
INFO - root - 2017-12-16 01:27:50.728149: step 82140, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 45h:25m:30s remains)
INFO - root - 2017-12-16 01:27:57.386448: step 82150, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 46h:22m:28s remains)
INFO - root - 2017-12-16 01:28:04.007014: step 82160, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 46h:01m:27s remains)
INFO - root - 2017-12-16 01:28:10.643048: step 82170, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 46h:15m:20s remains)
INFO - root - 2017-12-16 01:28:17.214778: step 82180, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 44h:44m:43s remains)
INFO - root - 2017-12-16 01:28:23.703679: step 82190, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 45h:21m:27s remains)
INFO - root - 2017-12-16 01:28:30.313865: step 82200, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.689 sec/batch; 47h:54m:20s remains)
2017-12-16 01:28:30.826386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0678234 -4.7194481 -4.1134295 -3.8577166 -4.2014732 -5.0209618 -6.043983 -6.3124557 -6.7086711 -6.8507686 -6.611711 -8.26188 -8.0275 -7.6624131 -6.8542805][-5.2092428 -3.770988 -2.3034778 -1.8996682 -2.5266235 -3.4572 -4.1878791 -5.5673614 -7.0248342 -7.03001 -6.6079855 -8.2760878 -7.8617811 -8.0302362 -7.9054861][-0.73412514 -1.6656432 -2.1235769 -0.85292053 -0.643281 -1.6457186 -2.6344893 -3.6447694 -4.8203473 -5.6082983 -6.222023 -8.0074425 -7.9551182 -8.075654 -7.9640818][-1.737705 -1.8379865 -1.2047462 -0.8538599 -1.5210104 -1.4919205 -1.6283131 -3.061059 -4.4732456 -4.7735586 -5.136106 -7.6170917 -7.837698 -8.1011724 -8.0213432][-2.8597603 -3.7795792 -3.3997674 -2.1900489 -1.9896054 -1.2314205 -0.55859232 -1.9823 -4.0854154 -5.2790189 -5.6549182 -7.699893 -8.2097111 -8.6258469 -8.94799][-6.9678373 -6.0985284 -3.8643684 -1.1466684 0.52355242 2.5505109 3.3740172 2.0938392 0.78285456 -1.8648818 -4.4610376 -6.9494162 -7.0968857 -7.8966923 -8.3365479][-9.8060532 -8.456872 -5.0058994 -0.92594147 1.6845055 4.7179828 6.8974395 6.562614 5.2995868 1.5024319 -1.5588717 -4.9676952 -6.3566985 -7.4381495 -7.544261][-10.931156 -9.8577785 -6.7586079 -1.5387955 2.2765279 5.7923512 8.1202316 7.8345504 6.8756585 3.8458762 0.6411643 -3.6834593 -5.5019574 -7.1132164 -7.5254049][-8.7413025 -8.5541935 -6.740694 -2.1376922 1.3239312 4.5132871 6.7200541 6.4368348 5.5026984 2.8652511 0.30854702 -4.2953391 -6.8401604 -8.7706356 -8.9758663][-6.9266906 -7.1441751 -6.362196 -3.0561674 -0.94733 1.6175852 4.1047616 4.0140252 2.8019385 0.569294 -1.4047365 -5.5663066 -7.2941518 -8.7743254 -9.106657][-10.591873 -10.021605 -8.3763008 -6.277235 -5.4569769 -3.4309347 -1.5995827 -1.5713286 -1.9533181 -3.4504857 -5.37186 -9.0387363 -10.020964 -10.201112 -9.4504089][-13.975779 -13.959969 -13.309462 -11.51265 -10.11923 -8.748127 -7.7273331 -7.3909683 -6.9889555 -7.7337084 -8.8276138 -10.959497 -10.734698 -10.085194 -8.9062128][-14.597486 -12.98173 -11.176476 -10.868225 -11.481788 -10.87232 -10.037233 -9.9814167 -10.229811 -10.13352 -9.8814678 -10.565908 -10.30916 -9.0730658 -7.9250627][-10.996223 -9.9404154 -8.453248 -7.5156369 -7.1565957 -7.9463511 -9.0008659 -8.9314852 -8.6706676 -9.0205727 -9.5663471 -9.33621 -8.5333977 -7.8190446 -6.6072068][-9.0197849 -7.5969782 -6.0334334 -4.4660511 -3.5090086 -3.9065843 -4.3834629 -5.2468419 -6.2479653 -6.3373971 -6.2192273 -7.0941515 -7.7607284 -7.7357807 -7.7291985]]...]
INFO - root - 2017-12-16 01:28:37.521269: step 82210, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 44h:26m:36s remains)
INFO - root - 2017-12-16 01:28:44.079898: step 82220, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 45h:32m:49s remains)
INFO - root - 2017-12-16 01:28:50.709055: step 82230, loss = 0.15, batch loss = 0.10 (11.5 examples/sec; 0.699 sec/batch; 48h:34m:17s remains)
INFO - root - 2017-12-16 01:28:57.291567: step 82240, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 45h:00m:31s remains)
INFO - root - 2017-12-16 01:29:03.909050: step 82250, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 44h:48m:33s remains)
INFO - root - 2017-12-16 01:29:10.477327: step 82260, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 44h:50m:43s remains)
INFO - root - 2017-12-16 01:29:17.120980: step 82270, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 45h:56m:39s remains)
INFO - root - 2017-12-16 01:29:23.677046: step 82280, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 45h:47m:51s remains)
INFO - root - 2017-12-16 01:29:30.218720: step 82290, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 45h:04m:21s remains)
INFO - root - 2017-12-16 01:29:36.779873: step 82300, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 44h:17m:58s remains)
2017-12-16 01:29:37.325077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8350163 -10.312885 -9.959404 -10.136065 -10.658101 -11.36611 -11.378633 -10.213709 -8.5856438 -7.2496352 -5.9060426 -6.2300267 -8.3411846 -9.8640442 -9.7849407][-9.0491152 -10.052886 -10.214174 -9.6073246 -9.9916954 -11.734089 -12.537239 -11.507168 -9.7329721 -8.1242256 -7.3498888 -8.0002575 -9.9938412 -11.371056 -11.12113][-6.964685 -8.2876282 -9.4473734 -8.0592213 -8.1664057 -9.3538561 -10.573837 -10.660486 -10.0875 -8.8237934 -7.8615913 -8.2354488 -10.662418 -11.783827 -10.922317][-7.0808821 -7.666419 -7.9330978 -6.3989973 -6.8258634 -6.6188293 -6.6361074 -7.4959779 -7.9110594 -7.536952 -7.2403746 -8.2460957 -10.31307 -11.634871 -10.937569][-7.8937654 -8.617548 -7.0981703 -4.5863729 -3.3890362 -2.1994607 -3.0516782 -4.7912693 -5.0219145 -5.0727787 -5.6186929 -6.760211 -8.69704 -10.174052 -9.7606277][-10.843229 -10.473524 -7.3178668 -2.6070809 0.090968609 2.4003339 1.8166304 -0.91101122 -3.1026301 -4.3456745 -4.3415871 -5.1632481 -7.4232812 -8.9159575 -8.6514454][-10.741765 -8.7337332 -4.6010447 0.4199028 3.2945523 5.4127517 5.4504857 3.5687947 0.97642803 -2.0278397 -3.7283175 -4.9015775 -6.4234052 -8.0151606 -8.2393732][-8.0564184 -6.6891718 -3.2901814 2.2693782 5.3092418 6.5268359 4.4600139 2.853868 1.9483533 -0.0942173 -2.1507967 -3.9691923 -6.24686 -7.2771912 -5.5856276][-7.1551924 -5.6496873 -3.8521261 0.57719374 3.7176108 4.70073 3.5437045 1.2887506 -0.93637943 -1.6234698 -1.2784705 -2.6985722 -5.4413466 -6.8666062 -5.1177168][-7.569468 -7.1721277 -5.07644 -1.9579983 -0.83004284 0.23245955 0.57459307 -1.0315638 -2.9852719 -3.9221916 -3.6216514 -3.9471912 -4.8885159 -6.4184842 -5.4637942][-12.466112 -10.821935 -9.7149754 -6.5189743 -5.4594216 -4.5350628 -3.7338221 -4.4684138 -5.0606608 -5.8979549 -6.4460206 -7.1788244 -6.8657055 -6.0700674 -3.8489518][-15.911596 -14.586042 -12.280962 -9.4030514 -7.078083 -6.0271735 -6.1471853 -6.6672707 -6.7807117 -7.3654003 -7.851717 -9.1815815 -9.1319561 -7.2960048 -4.680007][-15.458549 -14.537054 -13.34116 -10.60512 -8.0608187 -5.6081376 -4.3223066 -4.6376028 -5.1978812 -5.6249714 -6.359025 -7.5806856 -7.8571587 -6.117981 -3.202183][-13.723978 -12.645859 -11.332201 -9.4007111 -6.8085909 -4.3289461 -2.806525 -1.6705346 -2.4058003 -2.773159 -3.3176079 -4.0914717 -4.9986086 -4.3345819 -3.1068225][-10.098504 -10.025081 -9.2658768 -6.9594755 -4.6612096 -2.7557938 -2.0763607 -0.50066185 -0.0045485497 -0.40476418 -1.0620003 -2.1266131 -2.8264065 -3.5022526 -3.3787494]]...]
INFO - root - 2017-12-16 01:29:43.878280: step 82310, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.646 sec/batch; 44h:51m:49s remains)
INFO - root - 2017-12-16 01:29:50.460752: step 82320, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 44h:36m:16s remains)
INFO - root - 2017-12-16 01:29:57.140088: step 82330, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 46h:45m:17s remains)
INFO - root - 2017-12-16 01:30:03.649928: step 82340, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 46h:45m:09s remains)
INFO - root - 2017-12-16 01:30:10.181431: step 82350, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 45h:13m:37s remains)
INFO - root - 2017-12-16 01:30:16.693016: step 82360, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 46h:04m:30s remains)
INFO - root - 2017-12-16 01:30:23.274373: step 82370, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.668 sec/batch; 46h:24m:24s remains)
INFO - root - 2017-12-16 01:30:29.905560: step 82380, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.675 sec/batch; 46h:55m:26s remains)
INFO - root - 2017-12-16 01:30:36.485831: step 82390, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 46h:00m:43s remains)
INFO - root - 2017-12-16 01:30:43.038242: step 82400, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.635 sec/batch; 44h:07m:03s remains)
2017-12-16 01:30:43.572103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8240352 -7.5358119 -6.9623866 -5.326633 -5.541451 -5.6082373 -5.3519592 -5.059207 -4.4049921 -3.8452737 -3.6847589 -6.7748466 -10.198072 -11.184181 -7.9229946][-6.6035786 -5.3903427 -4.0239863 -2.2686024 -3.0328755 -4.9454331 -5.9967728 -5.8230376 -6.0918064 -5.8552227 -5.8715305 -9.2771511 -12.949076 -14.15053 -10.389605][-5.624836 -4.744391 -3.5864487 -1.5528316 -2.0002043 -3.0465634 -3.7648826 -4.6458139 -4.6073542 -5.396462 -6.0976706 -8.7976589 -11.660712 -12.974216 -10.026583][-5.1171675 -4.313468 -2.6006453 -0.90943575 -1.2838392 -2.1133711 -2.8744564 -3.3107734 -3.4223247 -3.8329697 -4.6052542 -8.106699 -10.650746 -11.965899 -9.7949314][-4.4911346 -3.9887457 -2.786185 -1.0641971 -0.82159376 0.066279411 -0.42408276 -2.2722085 -2.6383884 -3.1337278 -3.8363242 -6.6596785 -9.5587549 -11.488024 -9.3469772][-4.5925274 -3.1302073 -1.8471038 0.39657545 1.6080842 2.5315557 3.0976443 2.3924356 1.7880359 0.01644516 -1.9544055 -4.5630593 -6.3707027 -8.2723541 -6.5892129][-6.0647988 -3.5034862 -1.2936211 1.3352208 2.56889 3.0913224 3.7120366 5.01811 5.751471 2.5673327 -0.45178604 -3.7233765 -6.3659425 -7.7335 -6.1219263][-6.0438547 -3.2871356 -1.0299611 1.3189993 2.3392568 3.1618972 3.2217956 3.2909484 4.3605285 3.4545054 1.860393 -3.0387254 -8.0906429 -9.9728107 -7.7402763][-4.8852654 -3.4738202 -0.93382835 1.5275693 2.1033354 3.8008809 4.3413692 3.5241828 3.38485 2.4835134 2.0452771 -2.447536 -7.0321441 -9.2266655 -8.5723152][-5.2892208 -3.980129 -2.2096379 0.26435041 0.59163904 1.7479162 3.3176045 4.0379996 3.481523 2.627768 1.7206469 -2.2524559 -5.57788 -7.98746 -7.4966016][-7.9662323 -6.3333769 -4.0382619 -2.1860161 -2.1637454 -2.1064084 -1.548265 0.20823574 1.254796 1.3030405 0.66502953 -3.6556683 -6.733912 -9.0815153 -8.8734722][-11.684908 -9.5395613 -7.4350548 -5.9724917 -5.8374181 -6.05345 -6.4568205 -5.5821643 -3.2893767 -1.8681419 -2.152534 -4.7739744 -6.327486 -9.2179241 -9.39337][-11.225172 -10.204288 -9.09536 -8.3563347 -7.6087952 -7.4520636 -8.7834167 -7.9419541 -6.6674056 -4.6654983 -2.8632894 -3.4602606 -4.3633976 -6.4376616 -6.7721338][-8.8461189 -8.5614748 -8.3247967 -7.2553239 -5.9072065 -7.2339811 -9.0537939 -8.0488224 -7.2837634 -6.1925025 -5.475358 -4.5170941 -4.5116487 -6.1164117 -5.4614334][-6.8887239 -6.322114 -5.6111536 -4.0197425 -4.003921 -5.1342616 -5.8101993 -6.3531442 -7.0504837 -6.5851903 -5.6425219 -6.5516405 -7.1938915 -6.7365937 -7.8592653]]...]
INFO - root - 2017-12-16 01:30:50.102139: step 82410, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.638 sec/batch; 44h:20m:38s remains)
INFO - root - 2017-12-16 01:30:56.643191: step 82420, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 45h:39m:46s remains)
INFO - root - 2017-12-16 01:31:03.279167: step 82430, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 47h:10m:43s remains)
INFO - root - 2017-12-16 01:31:09.777302: step 82440, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 44h:10m:37s remains)
INFO - root - 2017-12-16 01:31:16.287452: step 82450, loss = 0.23, batch loss = 0.18 (12.2 examples/sec; 0.658 sec/batch; 45h:41m:38s remains)
INFO - root - 2017-12-16 01:31:22.817211: step 82460, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 44h:55m:35s remains)
INFO - root - 2017-12-16 01:31:29.377490: step 82470, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 45h:35m:42s remains)
INFO - root - 2017-12-16 01:31:35.932535: step 82480, loss = 0.11, batch loss = 0.07 (12.0 examples/sec; 0.668 sec/batch; 46h:24m:46s remains)
INFO - root - 2017-12-16 01:31:42.569892: step 82490, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 46h:15m:34s remains)
INFO - root - 2017-12-16 01:31:49.159775: step 82500, loss = 0.21, batch loss = 0.16 (12.4 examples/sec; 0.645 sec/batch; 44h:46m:08s remains)
2017-12-16 01:31:49.700072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7474165 -4.48271 -4.5279713 -4.1222377 -4.8209577 -5.6400132 -5.340404 -5.6015706 -5.3772359 -5.7084241 -5.5262456 -6.359479 -8.1627035 -6.8623638 -5.3609991][-4.7052727 -5.0983486 -4.1187649 -2.7561979 -2.8165436 -3.1149855 -4.446104 -4.867763 -4.9555755 -4.8144541 -4.4865122 -5.7436967 -6.4178905 -5.8451796 -5.3038611][-2.6440535 -3.9939218 -5.013525 -4.2719669 -3.7142937 -4.0737791 -3.9880805 -3.491895 -3.6196971 -3.8158984 -4.5463347 -5.4483056 -6.6101108 -6.7427096 -6.3698893][-5.0596209 -5.8643274 -6.074789 -4.5789986 -3.4246306 -2.8669217 -2.5319109 -2.9498353 -3.4738328 -2.9507601 -2.6662786 -4.0767322 -5.6795368 -6.5719252 -6.3711729][-5.2442122 -6.6161895 -6.8896937 -4.8521104 -2.5225418 -0.5442791 0.53486872 -0.23963737 -1.3308063 -1.9359334 -2.8929431 -3.1145258 -3.8396206 -4.3811221 -4.2854853][-7.0641208 -7.5000625 -6.5099206 -2.3609576 0.90183592 3.4340405 4.6790051 3.7609868 3.1968446 1.2949591 -0.99000931 -1.3250227 -2.7597229 -3.4300821 -3.3366446][-7.1996627 -6.8438854 -4.8737307 -1.5307479 1.3483996 4.6415496 7.1238704 5.8729386 5.0152831 2.8532214 0.35004663 -1.7603064 -5.6783619 -5.4455843 -5.0701165][-7.4802561 -6.457963 -5.387785 -3.2618568 0.0054659843 3.5890584 5.352798 3.9485288 3.0549884 1.3529172 -0.42526054 -3.2975397 -7.2344713 -8.4762964 -9.683548][-6.3129139 -5.9638991 -5.7277603 -4.0599394 -1.4652257 1.0764928 2.6545272 2.9743409 2.0622864 -0.4303875 -1.7961848 -4.3467755 -7.6191006 -9.0551262 -9.9221659][-5.6274223 -5.8707218 -5.9297585 -4.5114365 -1.700552 -0.62069893 0.68955612 1.4755254 -0.28681755 -2.1419671 -3.3901126 -5.0280557 -7.5313253 -9.6641436 -10.568697][-9.200798 -9.3789253 -9.7425728 -7.9766579 -6.2638712 -4.965137 -2.8460522 -3.2954452 -4.6957245 -4.8468504 -5.7901616 -8.2739754 -9.637536 -9.6721258 -9.1247][-13.587086 -13.344302 -11.962341 -9.7845154 -8.6049118 -7.1959014 -6.6111727 -7.0220447 -6.9623108 -7.0918922 -7.6740723 -8.0669937 -8.8807983 -10.062063 -10.225239][-14.959949 -13.867338 -12.010645 -10.907581 -9.8005257 -7.5791726 -6.9632978 -7.1497431 -7.2529783 -8.0033693 -9.0555649 -8.3246822 -8.3997078 -7.4631519 -6.3269][-12.111869 -12.119276 -11.320631 -9.0649128 -7.2073731 -6.5300221 -6.1443491 -5.2034154 -5.0099812 -5.6388049 -5.98763 -6.1739593 -7.0301514 -6.2550607 -5.6550694][-8.9605837 -8.7906742 -7.5537715 -6.3269181 -5.8503728 -3.923892 -2.7853754 -2.6620252 -2.8497615 -3.2192092 -4.3650932 -5.7808418 -6.7601037 -6.005384 -5.6838036]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-82500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-82500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 01:31:57.353121: step 82510, loss = 0.18, batch loss = 0.13 (12.7 examples/sec; 0.632 sec/batch; 43h:54m:28s remains)
INFO - root - 2017-12-16 01:32:03.962054: step 82520, loss = 0.11, batch loss = 0.07 (11.6 examples/sec; 0.691 sec/batch; 47h:57m:14s remains)
INFO - root - 2017-12-16 01:32:10.510921: step 82530, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 46h:02m:16s remains)
INFO - root - 2017-12-16 01:32:17.112446: step 82540, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 44h:29m:11s remains)
INFO - root - 2017-12-16 01:32:23.747131: step 82550, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.665 sec/batch; 46h:11m:41s remains)
INFO - root - 2017-12-16 01:32:30.226145: step 82560, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 45h:19m:50s remains)
INFO - root - 2017-12-16 01:32:36.697111: step 82570, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 45h:37m:57s remains)
INFO - root - 2017-12-16 01:32:43.285642: step 82580, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 46h:23m:34s remains)
INFO - root - 2017-12-16 01:32:49.806333: step 82590, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 46h:07m:37s remains)
INFO - root - 2017-12-16 01:32:56.465246: step 82600, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 46h:18m:07s remains)
2017-12-16 01:32:56.986528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5202117 -5.8340049 -5.2833033 -5.7366428 -7.0135894 -8.8713417 -8.8384361 -7.3496451 -6.813272 -6.6143513 -6.6871548 -6.9938641 -7.1673517 -7.9915357 -8.3365][-4.7683163 -5.6703434 -6.2415981 -6.6435709 -6.832058 -7.0729465 -6.4366608 -5.18342 -5.0254631 -5.046257 -4.7876291 -5.8086586 -6.8447328 -7.7553921 -8.2215757][-5.4423208 -5.782795 -6.5269809 -6.1542234 -6.9375553 -7.6064658 -6.67355 -5.0631008 -3.6117275 -2.9650559 -2.4504683 -4.2397175 -6.4075246 -8.5797691 -9.2303066][-5.1824751 -6.8283777 -6.9821129 -7.522788 -8.40609 -7.7908611 -6.5984969 -4.5701251 -2.2800636 -1.7812526 -2.7327678 -3.9199066 -5.4221439 -7.0236278 -8.0097132][-6.0552998 -6.4831161 -7.2645235 -7.9672308 -7.7498107 -6.3038335 -4.7513 -2.5591 -1.6815901 -0.76683664 -0.76353836 -2.7623641 -4.2415066 -5.9680595 -7.2046642][-6.7671695 -6.3479586 -6.3399782 -6.097682 -4.4503117 -0.82839346 2.2050719 2.5053096 0.9663949 -0.55533028 -2.2947528 -2.7189388 -4.5657187 -6.2217493 -7.3501253][-5.9806957 -5.8612494 -5.4121423 -3.323889 -0.29301119 2.093019 5.7223 6.7820783 4.9635406 0.88332081 -2.7180419 -3.7774143 -5.1352682 -6.9360414 -7.9009829][-6.7593975 -5.7070413 -5.12091 -2.3027363 -0.78508329 2.6269698 6.8989043 7.4216609 7.185019 3.0200438 -1.7516832 -4.5307903 -6.5610323 -7.1033006 -7.0770397][-6.252892 -4.5925417 -3.5806975 -2.4958158 -1.6050243 1.2005315 3.5114903 4.9367414 4.8551345 1.698391 -1.4219952 -4.77494 -7.7859039 -8.6767759 -8.641572][-6.0217 -5.7599559 -4.7756763 -3.1090496 -1.935638 -0.091858387 1.3304105 2.4157405 2.0328546 -1.3793297 -4.5089793 -7.266778 -10.590179 -11.424938 -11.060732][-7.1121969 -7.5889215 -7.0663061 -5.362648 -5.3558512 -3.9163618 -2.6545255 -2.6144223 -3.4212663 -5.03263 -7.0846381 -9.7028332 -11.880524 -13.440271 -13.466158][-10.948949 -10.833161 -9.8150358 -9.0824585 -9.0961142 -8.1600084 -7.6226988 -7.8906813 -8.2480717 -9.4327316 -10.092359 -10.136626 -10.414646 -11.735049 -12.081895][-13.524731 -12.938835 -11.708857 -10.340176 -10.567396 -11.18243 -11.529303 -11.159767 -10.519993 -10.228378 -10.060726 -10.318117 -10.362387 -10.181537 -9.8596344][-12.561497 -11.911165 -11.093847 -9.5641041 -9.3997669 -9.743557 -10.422921 -10.588598 -10.509922 -9.8935871 -9.1048126 -8.1677284 -7.8841839 -8.1418076 -7.6389709][-8.8620462 -9.111146 -8.9570312 -8.012332 -7.6234078 -7.5046153 -7.4684305 -8.0065432 -8.8886223 -9.1620359 -8.60685 -7.9419537 -7.77152 -7.7216182 -8.0317535]]...]
INFO - root - 2017-12-16 01:33:03.611120: step 82610, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.648 sec/batch; 44h:56m:48s remains)
INFO - root - 2017-12-16 01:33:10.227869: step 82620, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 44h:43m:10s remains)
INFO - root - 2017-12-16 01:33:16.787287: step 82630, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 44h:45m:57s remains)
INFO - root - 2017-12-16 01:33:23.395320: step 82640, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 45h:38m:25s remains)
INFO - root - 2017-12-16 01:33:29.927070: step 82650, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 44h:53m:47s remains)
INFO - root - 2017-12-16 01:33:36.535898: step 82660, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 45h:59m:44s remains)
INFO - root - 2017-12-16 01:33:43.076165: step 82670, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 46h:00m:21s remains)
INFO - root - 2017-12-16 01:33:49.694940: step 82680, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 46h:36m:24s remains)
INFO - root - 2017-12-16 01:33:56.304261: step 82690, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.683 sec/batch; 47h:22m:21s remains)
INFO - root - 2017-12-16 01:34:02.944923: step 82700, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 44h:14m:13s remains)
2017-12-16 01:34:03.496372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1891975 -5.0371346 -4.6592622 -5.3248034 -6.7778325 -7.1530089 -7.1771846 -7.5563955 -8.4719505 -8.5997972 -7.8299065 -6.8258963 -7.3397245 -6.6619425 -5.2962012][-5.8487706 -6.2147427 -5.7560911 -5.19061 -5.738482 -7.2078762 -7.8194284 -7.7884364 -8.425971 -9.0858431 -8.9456253 -7.5952415 -7.3399811 -7.3840032 -6.4408312][-3.6795778 -5.3131642 -6.2805471 -5.8671331 -6.0726576 -6.52361 -6.1876221 -7.0175762 -7.6128106 -8.0005646 -8.56015 -7.3605204 -7.5190825 -7.9646807 -8.2453156][-3.97247 -5.3456283 -5.3021812 -4.8716078 -5.4592128 -5.5795836 -5.108212 -5.8505626 -7.0791259 -7.6965904 -7.7832689 -7.46091 -8.918848 -9.5284414 -9.7403326][-5.4068546 -6.1510572 -6.1685925 -4.9464893 -3.5680315 -1.5749664 0.26033354 -1.6897912 -3.2436888 -4.6083255 -6.1958604 -4.9567356 -6.310637 -7.9370832 -8.7191086][-7.0004959 -6.2590871 -5.1434088 -3.9658604 -1.6063581 1.4240403 4.211144 4.3412118 3.5722442 -0.27707291 -4.5745444 -4.4114394 -5.6198626 -6.0596981 -6.3675647][-7.1994534 -6.6761627 -5.3030314 -2.677599 -0.40789795 2.360456 5.6936145 6.6191859 6.7696576 1.374723 -4.2643089 -4.1707954 -7.2010326 -7.9503493 -6.6257105][-8.5893011 -7.4914107 -6.4992266 -4.0998392 -0.92381716 2.6999307 5.8604741 7.0585446 7.0803523 2.7128396 -1.3432021 -4.02439 -8.4448729 -8.937993 -8.6348114][-6.435606 -6.4152923 -6.9446616 -5.5936928 -3.9455252 -0.66609955 2.8570075 4.336802 4.3458056 1.3100786 -1.7114615 -4.7639837 -9.7725487 -11.186373 -10.4661][-5.55145 -5.6606779 -5.5848541 -5.4853425 -5.9800735 -3.9357684 -1.9384127 -0.30685759 0.84645367 -1.4691839 -4.6837497 -6.8458128 -9.5356455 -11.27564 -11.538282][-9.10987 -9.9029446 -9.370203 -9.36058 -9.66659 -8.8569307 -8.392292 -7.3080959 -6.7317662 -7.085247 -8.0751038 -10.214009 -12.94624 -11.966772 -10.496628][-13.511524 -13.596106 -12.999628 -12.702868 -12.401267 -12.950443 -13.468021 -12.954082 -12.042609 -11.730333 -11.675913 -12.041478 -12.282993 -12.995053 -12.577628][-12.090672 -12.313288 -12.022531 -11.996304 -12.310773 -11.362247 -10.944426 -11.731945 -12.058451 -11.828852 -11.546165 -11.338382 -10.042936 -9.0908813 -8.45834][-11.136308 -10.662655 -9.3004961 -9.1007528 -9.1407871 -8.8098965 -9.0289536 -8.95409 -9.3352451 -10.15469 -10.542633 -9.0403185 -7.9374208 -7.3830647 -5.6550884][-8.2961721 -7.6637869 -6.5102811 -5.3164344 -3.9090629 -3.3554208 -4.3527164 -5.0694532 -5.4373989 -5.7172418 -6.2522058 -6.8309531 -6.701828 -6.0641055 -5.9880061]]...]
INFO - root - 2017-12-16 01:34:10.196395: step 82710, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 46h:07m:43s remains)
INFO - root - 2017-12-16 01:34:16.754890: step 82720, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 46h:03m:53s remains)
INFO - root - 2017-12-16 01:34:23.272381: step 82730, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 45h:23m:29s remains)
INFO - root - 2017-12-16 01:34:29.885732: step 82740, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.672 sec/batch; 46h:36m:14s remains)
INFO - root - 2017-12-16 01:34:36.528641: step 82750, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 44h:44m:23s remains)
INFO - root - 2017-12-16 01:34:43.206838: step 82760, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 46h:35m:46s remains)
INFO - root - 2017-12-16 01:34:49.824055: step 82770, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 46h:24m:34s remains)
INFO - root - 2017-12-16 01:34:56.351713: step 82780, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.659 sec/batch; 45h:41m:05s remains)
INFO - root - 2017-12-16 01:35:02.856055: step 82790, loss = 0.24, batch loss = 0.19 (12.4 examples/sec; 0.643 sec/batch; 44h:34m:39s remains)
INFO - root - 2017-12-16 01:35:09.447264: step 82800, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.674 sec/batch; 46h:46m:31s remains)
2017-12-16 01:35:10.007556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0060792 -6.4223738 -7.484993 -8.2673149 -9.3336458 -10.588334 -11.557482 -10.773621 -10.386217 -10.252569 -9.3454113 -9.858942 -11.545397 -11.178209 -12.384119][-8.077239 -9.1519928 -9.4799843 -10.067678 -10.368988 -11.041515 -11.974457 -13.062414 -13.829103 -12.584488 -11.08111 -11.690049 -12.993567 -12.32332 -12.757421][-4.9745541 -7.3397694 -9.858839 -9.5884352 -9.0775928 -10.886209 -12.266577 -12.202137 -12.085789 -12.534248 -12.354561 -11.809303 -12.599781 -12.663681 -13.15531][-8.059144 -10.00638 -10.903967 -9.5939665 -9.3776789 -9.7657681 -9.423 -11.646301 -13.117434 -11.445255 -9.8232231 -11.896236 -14.008736 -12.573605 -11.740238][-8.6788731 -11.841726 -13.580429 -11.568301 -8.5945244 -4.5189013 -2.6115878 -6.8293905 -11.521608 -11.826598 -11.527963 -11.357145 -12.024276 -12.521551 -13.20092][-11.705759 -13.504807 -13.221497 -10.928186 -7.5833912 -1.7866404 3.6974568 1.5120807 -2.2882676 -7.3202538 -12.038431 -11.262833 -11.571304 -11.274054 -12.252835][-15.062592 -14.517529 -12.74938 -8.9269676 -3.7385817 1.9411273 6.1084018 6.1237884 5.02599 -1.8139031 -8.9365215 -10.519792 -12.324316 -11.192024 -11.707314][-14.781616 -14.029156 -12.315041 -8.0706673 -1.2381392 5.7834191 9.7305908 6.892693 4.6207318 -0.27800798 -5.93136 -9.2692738 -12.087673 -11.947514 -13.212965][-11.762774 -11.320934 -12.323578 -8.7345057 -2.9298091 3.0193105 7.7466884 8.4049454 4.6670976 -2.7602262 -7.6378126 -10.331301 -14.101679 -13.643188 -13.815041][-10.192074 -9.9401131 -10.234316 -7.041749 -4.4403172 -1.7749426 2.4333625 3.7183576 1.5695572 -3.1318142 -8.169425 -12.164881 -15.066919 -14.330645 -15.734579][-10.721809 -12.275909 -12.601027 -10.862901 -8.9215431 -6.1687484 -3.0239229 -3.3340116 -4.4910283 -5.7567763 -8.752429 -13.145458 -15.385477 -15.469103 -15.123692][-16.792576 -16.345737 -16.046782 -15.674574 -14.479259 -12.282739 -11.17534 -11.199591 -11.119495 -11.802794 -13.259159 -14.057964 -14.424162 -14.479553 -14.170181][-15.635468 -13.973379 -14.276085 -16.345722 -15.955259 -14.58733 -12.645899 -12.390452 -13.4251 -12.867464 -12.471348 -14.15937 -14.306942 -12.526484 -11.513441][-13.357786 -12.56327 -12.103714 -10.765449 -11.022828 -12.283118 -12.54105 -11.923852 -11.291101 -11.210836 -11.945568 -11.974085 -11.291452 -9.7387753 -9.8248491][-9.6555586 -8.4290371 -6.828125 -6.4737177 -6.6316442 -6.5372705 -6.7952404 -8.3561029 -9.2693844 -8.6952477 -8.7992649 -10.72312 -10.804793 -10.543825 -11.075388]]...]
INFO - root - 2017-12-16 01:35:16.661200: step 82810, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 45h:38m:35s remains)
INFO - root - 2017-12-16 01:35:23.352364: step 82820, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 45h:44m:54s remains)
INFO - root - 2017-12-16 01:35:29.930111: step 82830, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 45h:16m:57s remains)
INFO - root - 2017-12-16 01:35:36.624765: step 82840, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 45h:20m:28s remains)
INFO - root - 2017-12-16 01:35:43.225305: step 82850, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 46h:43m:10s remains)
INFO - root - 2017-12-16 01:35:49.795209: step 82860, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 45h:04m:41s remains)
INFO - root - 2017-12-16 01:35:56.437855: step 82870, loss = 0.15, batch loss = 0.11 (11.4 examples/sec; 0.703 sec/batch; 48h:44m:20s remains)
INFO - root - 2017-12-16 01:36:03.017266: step 82880, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 45h:18m:53s remains)
INFO - root - 2017-12-16 01:36:09.556492: step 82890, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 45h:02m:20s remains)
INFO - root - 2017-12-16 01:36:16.243646: step 82900, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 45h:38m:21s remains)
2017-12-16 01:36:16.786819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.7047029 -7.196105 -6.4223948 -6.43479 -7.1170292 -7.8927326 -8.9359512 -8.4522076 -7.9158754 -7.6750913 -8.1589718 -10.218292 -11.228157 -9.6010981 -6.89405][-8.4575157 -6.6952314 -5.3357725 -5.1410103 -5.8957939 -7.4281545 -8.640131 -9.046648 -9.3522758 -9.0113344 -8.6443405 -10.043268 -10.903662 -10.664546 -7.9451818][-5.2722526 -4.6362863 -4.5865326 -3.7435074 -4.0743475 -5.9595423 -6.9062595 -7.7345257 -8.6011534 -9.1457539 -9.2074585 -10.542528 -11.496193 -11.226476 -9.69312][-5.7271609 -5.1807966 -4.9783764 -4.337647 -4.5891342 -4.8737893 -5.3486443 -6.9164963 -8.269784 -9.2385645 -9.7058506 -11.541459 -12.930405 -12.494541 -10.659454][-8.2209158 -8.8247929 -8.01151 -5.6094427 -3.9814548 -2.7549574 -1.7687697 -4.2641411 -6.6255083 -7.5619693 -8.3370695 -10.303738 -12.185137 -12.188099 -10.723299][-11.290081 -10.053774 -8.8493881 -5.3150692 -2.5081003 0.98537731 3.5936608 1.8196282 -0.039892197 -2.9725907 -6.2674603 -8.3524084 -10.359604 -10.275393 -8.8007135][-13.567691 -10.884924 -8.2981024 -4.2977219 -0.21811485 4.7644153 8.7253628 7.6876016 6.2232633 1.8283782 -2.7323475 -5.4253025 -9.1350574 -10.001364 -8.6848564][-12.752898 -10.417504 -8.3295307 -2.9257357 1.2377267 5.6366 10.035843 9.8601837 8.4293118 4.4513297 -0.1867919 -5.3807111 -10.851006 -11.847908 -11.031656][-11.056454 -9.3212576 -7.5203304 -3.5054657 -0.052899837 3.7951522 7.0971513 6.8494372 5.8264279 2.6297059 -1.264246 -5.6082797 -10.570694 -13.146849 -13.167095][-9.2670879 -8.3437347 -7.8424945 -3.4985614 -0.86374664 1.0996256 2.5651889 2.0476184 1.5393667 -0.95440674 -4.0335474 -7.89128 -12.224346 -14.337709 -14.051928][-12.427233 -11.870296 -10.562113 -8.1510353 -6.7140908 -4.796514 -3.5921717 -4.2778759 -5.4526787 -7.1696525 -8.5489912 -12.387005 -15.239462 -15.504181 -14.066145][-17.364407 -16.029778 -14.848345 -13.504722 -12.93079 -11.67739 -11.191151 -11.496216 -11.658251 -12.565459 -13.30236 -14.826239 -16.587017 -16.633001 -14.906254][-16.684982 -15.74889 -15.49663 -14.40328 -13.387854 -13.469053 -13.481247 -13.904596 -14.223028 -13.817829 -13.216406 -14.644297 -14.445774 -13.505318 -10.808889][-14.198643 -12.67902 -12.041262 -10.831858 -10.420134 -11.473096 -12.706543 -12.651723 -12.395515 -12.285088 -12.371754 -12.029078 -10.978193 -10.294144 -7.6272678][-9.0319691 -7.4138775 -6.0383906 -5.0406308 -5.3337393 -5.3616676 -6.2381406 -7.0074692 -8.12206 -7.9595704 -8.0263519 -9.413743 -9.2845116 -8.6760721 -6.7288346]]...]
INFO - root - 2017-12-16 01:36:23.371111: step 82910, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 45h:59m:45s remains)
INFO - root - 2017-12-16 01:36:29.875383: step 82920, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.635 sec/batch; 44h:01m:29s remains)
INFO - root - 2017-12-16 01:36:36.429287: step 82930, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 46h:27m:54s remains)
INFO - root - 2017-12-16 01:36:43.054827: step 82940, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.683 sec/batch; 47h:20m:17s remains)
INFO - root - 2017-12-16 01:36:49.668160: step 82950, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 45h:12m:36s remains)
INFO - root - 2017-12-16 01:36:56.217625: step 82960, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 44h:50m:15s remains)
INFO - root - 2017-12-16 01:37:02.803119: step 82970, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 46h:32m:25s remains)
INFO - root - 2017-12-16 01:37:09.437189: step 82980, loss = 0.20, batch loss = 0.15 (11.9 examples/sec; 0.673 sec/batch; 46h:38m:23s remains)
INFO - root - 2017-12-16 01:37:16.036232: step 82990, loss = 0.18, batch loss = 0.14 (11.7 examples/sec; 0.686 sec/batch; 47h:31m:30s remains)
INFO - root - 2017-12-16 01:37:22.548718: step 83000, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 44h:28m:08s remains)
2017-12-16 01:37:23.207385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.634433 -3.3377416 -3.6293461 -3.3734183 -3.96138 -4.5758343 -4.7372217 -4.2130036 -3.0323222 -1.9094033 -1.5549989 -3.7144849 -6.8143096 -7.9305091 -8.2651215][-2.9583864 -3.2066872 -2.9534225 -3.2814224 -4.059309 -4.8224273 -4.7994328 -3.6691551 -2.4553962 -1.7018886 -1.4284787 -3.3152328 -6.0066867 -7.1958251 -6.9099026][-2.3137443 -3.0626066 -4.1222863 -4.4693556 -5.0180316 -5.2290492 -4.8508096 -4.13806 -3.0151148 -2.7266619 -3.0147865 -5.0264945 -7.22983 -7.8055773 -8.16627][-2.6691549 -3.588057 -4.4007568 -4.6274619 -4.9353976 -4.8414192 -4.5554142 -4.2742243 -3.5586505 -2.5810823 -2.0293186 -3.5965919 -5.8036819 -6.9004722 -7.7485957][-3.4532049 -4.7398272 -6.2338338 -5.0156913 -3.7776217 -3.078732 -2.5631888 -2.7812614 -2.8245265 -2.4787335 -2.2800303 -3.4617355 -5.2796059 -5.81198 -6.0721936][-3.8614936 -4.0258512 -4.1860991 -3.5101507 -1.5567212 0.95452595 2.2757483 1.5411057 0.7270999 -0.031569958 -0.40693617 -1.8746157 -4.0180354 -4.6095953 -5.4330034][-4.0757957 -4.010119 -3.9179449 -2.621062 -1.3002272 2.0505023 4.9735866 4.8427339 3.9541144 1.5940809 -0.17967939 -1.9349265 -4.1337805 -4.2862244 -4.574338][-4.0370994 -3.3983593 -2.5533755 -1.5087624 -0.54281378 2.3171878 5.356442 5.6858983 4.9987988 2.7084527 0.20852089 -2.0393279 -4.5420117 -4.7122169 -4.6903963][-3.0696428 -3.0850663 -2.0951331 -1.0934591 -0.010267258 1.7101517 3.3961263 4.5149179 4.2291436 2.0674653 0.12054634 -3.5454018 -7.3415737 -6.801363 -5.7216392][-2.6034358 -2.1827457 -1.8364501 -0.72214365 -0.41589117 0.69058323 2.075562 2.4703498 2.1974792 -0.060915947 -1.9620111 -5.6075807 -9.1383476 -9.1302147 -8.6073914][-5.8039308 -5.9103184 -5.18951 -3.9971061 -3.6887617 -2.905107 -2.6544466 -3.1073534 -3.800849 -4.5685911 -5.5849018 -9.0285482 -11.290853 -11.130749 -10.068047][-9.6176844 -9.6188717 -8.9586582 -7.3994575 -7.0023279 -6.4812784 -6.7546082 -7.3915758 -7.7414083 -8.4538059 -9.2922173 -10.19484 -10.759502 -11.483084 -10.866903][-12.781671 -12.144156 -10.807409 -9.5745087 -9.1401358 -8.7034245 -8.8412619 -9.7023239 -10.469248 -10.759025 -10.479204 -10.673588 -10.676502 -10.347351 -9.8636942][-10.582035 -10.859516 -9.8860531 -9.0323048 -7.7452812 -7.5111279 -7.9658623 -8.4886513 -8.7774363 -9.1016064 -9.0206242 -8.41733 -7.7833948 -8.0099134 -8.3288784][-7.034337 -7.3533411 -6.5531797 -5.4613633 -5.2850046 -5.1316962 -4.7769537 -4.868824 -5.5523882 -6.0322566 -6.5402164 -6.6155853 -6.3941994 -7.0418291 -8.2784729]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 01:37:29.842934: step 83010, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 45h:22m:28s remains)
INFO - root - 2017-12-16 01:37:36.418171: step 83020, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 45h:31m:21s remains)
INFO - root - 2017-12-16 01:37:43.051303: step 83030, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.653 sec/batch; 45h:16m:12s remains)
INFO - root - 2017-12-16 01:37:49.562467: step 83040, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 45h:32m:59s remains)
INFO - root - 2017-12-16 01:37:56.171426: step 83050, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 45h:08m:57s remains)
INFO - root - 2017-12-16 01:38:02.796798: step 83060, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 45h:28m:35s remains)
INFO - root - 2017-12-16 01:38:09.384512: step 83070, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.688 sec/batch; 47h:41m:11s remains)
INFO - root - 2017-12-16 01:38:15.937295: step 83080, loss = 0.20, batch loss = 0.16 (11.6 examples/sec; 0.692 sec/batch; 47h:55m:55s remains)
INFO - root - 2017-12-16 01:38:22.593731: step 83090, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 45h:30m:24s remains)
INFO - root - 2017-12-16 01:38:29.177369: step 83100, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 44h:58m:33s remains)
2017-12-16 01:38:29.747641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0213289 -4.6412582 -4.9709964 -4.5686913 -4.9504232 -5.3732882 -6.1164279 -6.157083 -5.5761046 -5.59067 -5.4487848 -5.4796267 -5.1047597 -4.3346682 -2.21163][-5.9533639 -3.9099886 -4.0586948 -4.0010605 -3.815526 -3.7921391 -4.422368 -5.0909662 -5.1283841 -5.056962 -4.5338154 -4.0849009 -3.8608804 -3.8393178 -1.4909401][-4.0920382 -4.7629647 -5.4975133 -3.4353938 -3.7751465 -3.839994 -3.5005496 -3.0864117 -2.8455794 -3.2359538 -3.00709 -3.5276127 -2.9742126 -3.3085325 -1.8814187][-3.7869787 -4.0869522 -6.1895409 -5.4874592 -6.18497 -4.9328246 -4.2032871 -4.3163075 -3.7372825 -2.8629169 -2.5138187 -2.313766 -2.2912352 -3.7102523 -1.3656549][-7.5851116 -7.3506584 -7.3092022 -6.1862178 -6.8854432 -4.6390328 -2.7374949 -3.0426037 -3.7787552 -3.7632551 -4.0187654 -4.1201949 -3.9690132 -4.7972331 -2.7995589][-7.4424853 -5.9308619 -5.4983845 -4.0100727 -2.7613363 1.3895392 4.1783109 3.5628104 2.033885 -0.586154 -2.7648783 -2.4403255 -3.2234428 -5.432478 -4.222147][-8.3235016 -7.0541668 -6.3717613 -3.6143675 -1.8355174 2.4168019 6.6179423 6.9162879 5.7985024 1.3169556 -2.7463102 -3.8157988 -5.1043029 -6.1014729 -4.719265][-9.5688181 -8.6970968 -8.9437037 -5.7614756 -2.7764122 1.4712272 5.1935019 7.2542748 6.2311893 1.6907883 -1.0214419 -3.6613505 -6.2260613 -7.3037906 -5.6305761][-7.7010098 -5.2873297 -4.4145994 -2.4381623 -1.8517649 1.2919478 4.6914411 4.91923 3.4621654 -0.1343174 -3.261894 -5.5562205 -6.0221715 -7.6755004 -6.6242251][-7.0180712 -5.0521255 -5.006917 -3.1917078 -2.3141701 0.93964767 2.8095469 3.3757653 2.0285139 -1.6292224 -3.7983289 -5.6186247 -6.2671585 -7.7181511 -6.2414365][-7.2502465 -5.2315068 -5.5071268 -4.7675714 -4.5380564 -3.4114881 -2.3836589 -1.6007099 -1.9238484 -3.2257679 -4.7686682 -6.861897 -7.1963053 -9.0853653 -8.0097551][-12.067482 -10.541313 -10.023081 -9.6274014 -10.093384 -9.4344893 -9.1700935 -8.2021465 -8.4728336 -9.3577394 -9.5876732 -9.6355228 -9.4462852 -9.5243435 -7.6963887][-13.227481 -12.515673 -12.456963 -10.8176 -11.256532 -11.059626 -10.58856 -10.9134 -10.995321 -10.384911 -9.6919546 -9.3516817 -8.7513857 -9.2769423 -8.3592968][-7.1732693 -6.68081 -6.5938525 -7.7807021 -8.1907425 -7.7768536 -8.4772167 -8.2507191 -7.9330654 -8.0747833 -7.7428188 -6.2280793 -5.3488889 -6.2364283 -5.9270349][-8.75811 -6.5340714 -4.9239163 -4.8359938 -4.5294743 -5.4590755 -6.4798989 -6.3176246 -6.7136488 -7.0710683 -6.3681245 -5.9582191 -6.6518545 -7.0917792 -6.2608204]]...]
INFO - root - 2017-12-16 01:38:36.317078: step 83110, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 45h:55m:05s remains)
INFO - root - 2017-12-16 01:38:42.931864: step 83120, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 45h:29m:05s remains)
INFO - root - 2017-12-16 01:38:49.482775: step 83130, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 44h:50m:31s remains)
INFO - root - 2017-12-16 01:38:56.118129: step 83140, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 46h:20m:08s remains)
INFO - root - 2017-12-16 01:39:02.728975: step 83150, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 45h:01m:56s remains)
INFO - root - 2017-12-16 01:39:09.299429: step 83160, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 46h:19m:51s remains)
INFO - root - 2017-12-16 01:39:15.854078: step 83170, loss = 0.11, batch loss = 0.07 (12.4 examples/sec; 0.646 sec/batch; 44h:43m:31s remains)
INFO - root - 2017-12-16 01:39:22.500420: step 83180, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 46h:00m:10s remains)
INFO - root - 2017-12-16 01:39:29.125275: step 83190, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 46h:05m:42s remains)
INFO - root - 2017-12-16 01:39:35.769033: step 83200, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.663 sec/batch; 45h:55m:14s remains)
2017-12-16 01:39:36.294038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6884675 -5.6929669 -6.6320758 -7.1754465 -7.5279403 -7.4139061 -7.2278233 -6.7729216 -5.8522067 -4.824996 -4.3653245 -6.6970453 -8.37507 -8.7509651 -7.8727484][-2.466444 -2.8815875 -4.4243736 -5.2889981 -6.4252906 -6.4331207 -5.8812652 -5.6568213 -5.1622071 -4.2817526 -3.80815 -5.701695 -7.7701082 -9.11559 -9.7296238][-0.31149292 -1.2033434 -2.793324 -3.3854296 -5.341774 -5.8681836 -6.2047162 -5.3426037 -4.4450154 -4.3213291 -2.9893143 -5.2492757 -7.5883136 -9.7297115 -11.055429][-1.3723083 -2.4213552 -3.2765145 -3.4425862 -4.5408444 -4.799696 -5.1365733 -5.2726383 -4.4379582 -3.2023551 -2.2548738 -4.75486 -7.4123492 -10.100761 -11.25441][-1.8234515 -3.9573338 -5.4473209 -4.2722859 -3.7287688 -2.094028 -1.7465677 -2.0761552 -2.1003265 -1.8277302 -1.5778446 -3.6458821 -6.78093 -9.6768389 -11.362699][-3.6525087 -4.9939003 -5.892755 -4.1571088 -2.1130331 0.47236967 2.3640351 1.8308234 1.2417736 0.094860554 -0.3533802 -2.97588 -6.4173927 -9.5870323 -10.676389][-5.7628517 -6.9050293 -6.5503306 -3.6683369 -0.81196976 2.9096131 5.9817729 5.4969697 3.9978976 1.9604006 0.19313002 -2.9992142 -5.9557433 -8.3855534 -10.079817][-6.4729824 -6.5298958 -5.1777306 -1.9941254 0.83659744 4.3984981 6.9684129 7.0458684 6.0308595 3.1846547 0.73958683 -3.2021444 -6.1289873 -8.22192 -9.71929][-6.1148448 -5.5946994 -4.5863237 -1.7783518 0.62768745 2.7674508 4.7859817 5.12625 4.5483642 2.7193418 0.10994673 -4.527596 -7.6945028 -9.044693 -9.4794168][-5.7134104 -4.570509 -3.7963226 -2.1680624 -1.2067089 0.2201252 1.6337609 2.0877728 1.9326363 0.47992659 -1.6452398 -5.9550967 -9.2592249 -10.51051 -10.604216][-7.983654 -6.7928925 -5.906939 -5.0951195 -4.2976675 -3.2345252 -2.2353303 -1.4819646 -1.7036486 -3.2008336 -5.1720781 -9.1282272 -11.461712 -11.920321 -11.166373][-10.331648 -9.5742407 -8.9766 -7.2900558 -6.535521 -6.7211914 -6.7081051 -6.3144321 -6.7348933 -7.6495652 -8.8120651 -10.568218 -11.458077 -11.638474 -10.901598][-12.259065 -10.852308 -10.13312 -9.321866 -9.44849 -8.4461193 -8.22414 -8.3322144 -8.79373 -9.2072754 -9.9977179 -10.532219 -10.473907 -9.5245152 -8.6449718][-12.120767 -12.007286 -10.755943 -8.6257372 -7.956912 -7.5112276 -7.9251366 -7.5596976 -7.178196 -7.7270393 -8.0972958 -7.9718866 -8.0724907 -7.633976 -7.4624047][-9.0241871 -9.79138 -8.9980879 -6.6795626 -5.438159 -4.9916682 -5.6124535 -6.0053139 -6.4380069 -6.2735033 -6.09958 -7.0530682 -7.8954515 -7.5099669 -7.3650513]]...]
INFO - root - 2017-12-16 01:39:42.831922: step 83210, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 44h:33m:51s remains)
INFO - root - 2017-12-16 01:39:49.436472: step 83220, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 44h:39m:08s remains)
INFO - root - 2017-12-16 01:39:56.023480: step 83230, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 45h:48m:06s remains)
INFO - root - 2017-12-16 01:40:02.651461: step 83240, loss = 0.18, batch loss = 0.14 (12.0 examples/sec; 0.668 sec/batch; 46h:16m:45s remains)
INFO - root - 2017-12-16 01:40:09.336908: step 83250, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 46h:29m:17s remains)
INFO - root - 2017-12-16 01:40:15.870050: step 83260, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 44h:17m:33s remains)
INFO - root - 2017-12-16 01:40:22.387103: step 83270, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 44h:53m:09s remains)
INFO - root - 2017-12-16 01:40:28.857807: step 83280, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 44h:47m:50s remains)
INFO - root - 2017-12-16 01:40:35.491084: step 83290, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 44h:50m:16s remains)
INFO - root - 2017-12-16 01:40:42.025212: step 83300, loss = 0.11, batch loss = 0.07 (12.5 examples/sec; 0.638 sec/batch; 44h:08m:22s remains)
2017-12-16 01:40:42.537765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9027958 -7.404367 -7.9473491 -7.9059134 -8.9605541 -9.9333363 -9.9021492 -8.9851475 -8.0562449 -7.2078218 -5.6954088 -5.9476004 -6.0406656 -6.0017104 -4.1194692][-6.36714 -6.5245581 -5.6570516 -5.3991961 -6.0434155 -6.9377661 -7.345602 -7.0520916 -6.9918203 -5.8695092 -4.5904789 -5.502008 -5.7477517 -5.9887743 -5.1270013][-2.8367786 -5.3717966 -6.6603851 -4.6388168 -5.0881491 -5.5866632 -4.9531255 -4.689353 -4.8416042 -4.3246779 -3.7420263 -5.5693736 -7.5370641 -8.40011 -8.5997076][-3.1966639 -5.0764112 -5.6383677 -5.0894814 -5.0982556 -4.2531042 -3.8007903 -3.7357159 -3.7742553 -3.870645 -3.4676108 -5.518321 -8.55633 -9.7858086 -10.314896][-3.2967012 -4.7902908 -5.1955686 -4.6502581 -3.68177 -1.5035601 -0.66696548 -2.1334293 -3.3547802 -2.6919968 -3.0149827 -6.5831847 -9.4229937 -10.861041 -12.232428][-6.18664 -5.909709 -4.7404957 -2.7101834 -0.13093948 2.6975179 4.4733262 3.1706796 1.836431 0.19816828 -1.9843884 -5.0244384 -7.6955056 -10.758822 -12.029627][-7.0986629 -6.1581521 -4.7538233 -1.1309867 0.92596054 3.3453393 6.2786832 6.8251853 5.8195443 1.5458789 -2.0771053 -5.8364305 -9.0331974 -11.389832 -12.313531][-6.9503307 -5.9702463 -4.4994059 -0.38179636 1.3182249 3.5511556 5.4518027 5.7167735 5.6511693 2.7994466 -0.688272 -5.7909713 -9.1812725 -11.041767 -10.394205][-6.5745292 -6.3079247 -4.5233259 -1.5692596 -1.1631207 1.5331945 3.1525407 1.8870659 1.270061 0.0277462 -1.7997673 -6.2239785 -9.1163177 -11.301791 -10.855188][-5.18965 -6.0974073 -6.0515242 -4.2516618 -3.7080505 -2.6877441 -1.7152014 -1.7100077 -2.0638387 -3.7884412 -5.1262245 -7.9978876 -10.200667 -10.826674 -10.769583][-10.089518 -10.676689 -11.04303 -9.0850439 -9.6596 -9.61687 -8.7515812 -8.2373323 -7.5009651 -7.9417305 -8.9125156 -12.00037 -12.535078 -12.105043 -10.291049][-13.790606 -14.268299 -14.55435 -14.708519 -14.835421 -14.15593 -14.376827 -14.358694 -13.587063 -13.439952 -13.16243 -14.226921 -13.920694 -13.245256 -11.537203][-14.757708 -14.274214 -13.925966 -13.681042 -14.039965 -13.617097 -13.921856 -14.252911 -14.386209 -14.106245 -13.965771 -13.655539 -12.39141 -10.941772 -9.2837658][-10.113521 -9.4992218 -8.919817 -8.5533819 -9.28833 -9.61232 -9.5187531 -9.2166195 -9.4769878 -9.6249523 -9.7428932 -9.3408289 -8.9110556 -8.0923748 -6.9717488][-7.6911764 -8.525857 -8.3218851 -6.2697353 -5.1342893 -5.5044894 -6.0740156 -6.3427386 -5.7236857 -5.3796883 -5.5710015 -6.1945653 -6.8362904 -6.7742896 -6.7606249]]...]
INFO - root - 2017-12-16 01:40:49.100106: step 83310, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 44h:42m:20s remains)
INFO - root - 2017-12-16 01:40:55.727014: step 83320, loss = 0.16, batch loss = 0.12 (11.7 examples/sec; 0.681 sec/batch; 47h:08m:08s remains)
INFO - root - 2017-12-16 01:41:02.270217: step 83330, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 45h:30m:57s remains)
INFO - root - 2017-12-16 01:41:08.805977: step 83340, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 46h:13m:00s remains)
INFO - root - 2017-12-16 01:41:15.423380: step 83350, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 46h:10m:12s remains)
INFO - root - 2017-12-16 01:41:21.953527: step 83360, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.664 sec/batch; 45h:55m:51s remains)
INFO - root - 2017-12-16 01:41:28.612303: step 83370, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.682 sec/batch; 47h:11m:12s remains)
INFO - root - 2017-12-16 01:41:35.206507: step 83380, loss = 0.14, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 44h:46m:12s remains)
INFO - root - 2017-12-16 01:41:41.848531: step 83390, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 45h:56m:35s remains)
INFO - root - 2017-12-16 01:41:48.482599: step 83400, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.675 sec/batch; 46h:43m:27s remains)
2017-12-16 01:41:48.973342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2009859 -6.9657497 -6.2470474 -5.4730849 -5.3200569 -5.0290136 -4.2094851 -3.0067592 -2.0128343 -1.6332951 -0.89784956 -1.4456062 -3.4994166 -3.2457523 -3.8496037][-7.3658071 -6.9796782 -6.2962761 -5.8528 -5.7694669 -6.071579 -5.8243933 -4.9144573 -3.6271327 -2.2367985 -1.0619841 -1.5340924 -2.8820145 -3.3376682 -3.6387215][-5.5488329 -5.6936793 -6.1489286 -5.2240314 -5.4329739 -5.6148386 -5.4589629 -4.9054365 -4.2447324 -3.2680442 -2.0241809 -1.8924851 -3.3358119 -3.730886 -4.6810365][-5.8590221 -5.6158233 -5.1621203 -4.4118228 -4.9698915 -4.6238346 -3.6950219 -3.5095613 -3.0624914 -2.6603181 -2.3569045 -2.5208063 -3.8555918 -4.281426 -4.7374511][-6.5213552 -7.0249157 -6.4727154 -4.6489925 -3.8559179 -2.4632485 -1.1902251 -1.0516443 -1.0600777 -1.3742242 -1.6420279 -2.0198231 -3.9375086 -4.1303053 -3.8504491][-8.4266806 -8.5109081 -7.1329737 -4.7960048 -2.5691032 0.38467932 2.6960177 2.9903312 2.8442359 1.0740666 0.10521746 -0.84118509 -3.205498 -2.8061688 -2.3083777][-10.689494 -8.9561548 -6.86525 -3.9015379 -2.0118797 1.0955863 4.2146144 5.3489919 5.0755839 2.9780231 1.4772797 -0.34078598 -3.1199725 -2.6873453 -2.707155][-9.9079189 -8.664917 -5.89824 -1.8944342 0.41104221 3.4332328 5.7477927 6.4334922 6.8324428 4.4759336 1.477232 -1.8436186 -4.8092227 -4.5232496 -4.2830582][-8.367486 -7.3732305 -5.7321696 -2.1131425 0.18992376 2.8383183 4.3907523 5.075994 5.4426875 3.8493705 1.8337884 -1.5772529 -5.8164139 -6.58985 -6.3810048][-8.1597948 -6.9462447 -5.6263738 -3.3077345 -1.9427087 0.47899103 2.0718369 2.4362779 2.6615949 1.4095855 -0.21747303 -3.1440232 -6.7526379 -7.8147221 -8.7051487][-11.19993 -9.960495 -8.2445259 -6.0271249 -5.10923 -2.8859468 -1.6782608 -1.3004427 -1.2149644 -2.4447448 -3.9086227 -6.5394011 -8.8355694 -9.3399372 -8.8091516][-14.393637 -12.627592 -10.998562 -9.0393267 -7.41849 -5.563787 -4.9069867 -4.0855923 -3.9453154 -5.316462 -6.8054342 -8.47641 -9.6924877 -9.3656273 -8.3677959][-14.035797 -12.010916 -10.130931 -8.4995995 -7.8038578 -6.805058 -6.1267309 -5.5167637 -5.5690603 -6.0509338 -6.9779882 -8.7362938 -9.0884438 -8.3352642 -6.5611591][-10.731943 -9.8739414 -7.8284435 -6.4956112 -5.9875097 -5.7237458 -4.8100772 -4.1347957 -4.2694931 -4.5320621 -5.3356943 -5.9398603 -6.3891382 -6.1670742 -5.8350372][-7.3623552 -6.7724352 -5.2109637 -3.9983368 -3.1126561 -3.3651624 -3.3840017 -3.4517767 -3.2248454 -3.1463 -3.5588059 -4.0801067 -4.5329046 -5.6248169 -6.0158052]]...]
INFO - root - 2017-12-16 01:41:55.509640: step 83410, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.633 sec/batch; 43h:47m:27s remains)
INFO - root - 2017-12-16 01:42:02.088894: step 83420, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 45h:43m:26s remains)
INFO - root - 2017-12-16 01:42:08.669981: step 83430, loss = 0.30, batch loss = 0.26 (12.1 examples/sec; 0.662 sec/batch; 45h:47m:37s remains)
INFO - root - 2017-12-16 01:42:15.235703: step 83440, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 46h:28m:18s remains)
INFO - root - 2017-12-16 01:42:21.792547: step 83450, loss = 0.23, batch loss = 0.18 (11.9 examples/sec; 0.672 sec/batch; 46h:28m:25s remains)
INFO - root - 2017-12-16 01:42:28.389604: step 83460, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 45h:57m:07s remains)
INFO - root - 2017-12-16 01:42:34.964333: step 83470, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 44h:39m:45s remains)
INFO - root - 2017-12-16 01:42:41.602225: step 83480, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 44h:48m:31s remains)
INFO - root - 2017-12-16 01:42:48.186260: step 83490, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 46h:17m:34s remains)
INFO - root - 2017-12-16 01:42:54.806574: step 83500, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.662 sec/batch; 45h:47m:10s remains)
2017-12-16 01:42:55.308197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2529035 -3.5716887 -2.95995 -2.0224841 -2.1551409 -1.9679093 -2.1409709 -2.0212884 -1.8316104 -2.3123586 -3.156935 -6.7963734 -10.531124 -10.298876 -9.9576244][-4.7785635 -4.785686 -4.5056591 -3.3826118 -2.9501216 -2.7613904 -2.7133934 -2.9173455 -3.0601938 -3.5697718 -4.5112782 -8.2777481 -11.681389 -11.714874 -11.285959][-4.2355013 -4.8815274 -5.307374 -3.8788152 -3.4516709 -3.5911312 -3.4808118 -3.0186825 -2.6099355 -3.9656813 -5.3300838 -9.4216909 -13.214463 -12.894208 -12.700015][-6.5078406 -6.0632968 -5.3826275 -4.4112296 -4.6400146 -4.1187234 -3.6687293 -4.0083008 -4.7096381 -4.7236176 -5.2170024 -9.0376816 -12.695834 -12.913256 -11.65521][-7.9190769 -8.5039406 -8.0237217 -6.177597 -4.5364089 -2.6009541 -2.0818839 -3.1756186 -5.3591409 -5.87747 -5.80415 -8.801589 -12.167776 -12.553518 -11.470068][-8.2889738 -8.505703 -6.9185281 -3.8568764 -1.4247923 1.0995932 2.74433 2.5674329 -0.098475456 -3.376178 -5.911932 -8.660038 -10.932452 -11.244968 -11.106195][-8.957674 -8.84182 -7.6420107 -3.7126446 -0.26232433 3.562798 6.2074904 5.3984408 3.3660522 -0.34793329 -4.4600277 -8.4639015 -11.248506 -10.758749 -10.952007][-9.2947445 -8.5600014 -6.4200354 -3.2127249 0.072782516 5.1002383 8.314785 7.4469686 5.6554217 2.2065992 -1.5134792 -6.6513782 -10.158567 -9.6931028 -9.5176992][-7.6903324 -6.9269891 -5.892252 -2.7122214 -0.45224142 3.1071286 6.0601392 6.7493405 5.3222966 1.2773399 -1.8645713 -6.2901397 -9.9969807 -10.306417 -9.3441753][-6.7919631 -6.2631636 -5.2131033 -3.3227532 -2.75598 -1.4610524 0.88202286 2.7264953 2.3248868 -0.39982891 -3.0362267 -7.05361 -10.482718 -10.236004 -10.257545][-6.9820704 -6.9233961 -6.1007223 -5.4241705 -5.0641847 -3.8307853 -2.6109586 -2.1530573 -2.3116605 -3.0921273 -4.5935488 -8.4912128 -10.438232 -10.097394 -9.921051][-11.790875 -10.774829 -8.707859 -8.1476135 -7.9525957 -7.0126524 -7.4770823 -7.5778341 -7.3994942 -7.7690949 -8.6883793 -9.2981577 -9.29648 -9.4995432 -10.205967][-11.714989 -10.294249 -9.1426029 -8.358427 -7.7159538 -8.0859413 -8.6987171 -8.7768145 -9.7869768 -9.8668537 -9.9741325 -9.3886833 -9.5711288 -8.4412136 -7.9678755][-9.9220238 -8.55492 -7.6484823 -6.2294359 -5.3735342 -6.6044736 -7.4563689 -8.2108364 -8.484189 -8.8037052 -9.3010454 -8.1650639 -7.76966 -6.8220811 -6.4260974][-6.2254467 -5.8393016 -5.1225381 -4.7656426 -5.3236074 -5.3485065 -5.43476 -6.4438152 -6.7203922 -6.8329158 -6.4771347 -7.275701 -7.6202326 -7.544013 -8.2878857]]...]
INFO - root - 2017-12-16 01:43:01.917580: step 83510, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.659 sec/batch; 45h:36m:38s remains)
INFO - root - 2017-12-16 01:43:08.503142: step 83520, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 47h:01m:32s remains)
INFO - root - 2017-12-16 01:43:15.139083: step 83530, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.681 sec/batch; 47h:04m:00s remains)
INFO - root - 2017-12-16 01:43:21.763548: step 83540, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.647 sec/batch; 44h:45m:37s remains)
INFO - root - 2017-12-16 01:43:28.460734: step 83550, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.648 sec/batch; 44h:47m:37s remains)
INFO - root - 2017-12-16 01:43:35.067065: step 83560, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 45h:46m:34s remains)
INFO - root - 2017-12-16 01:43:41.669166: step 83570, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 46h:08m:05s remains)
INFO - root - 2017-12-16 01:43:48.267584: step 83580, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 45h:09m:21s remains)
INFO - root - 2017-12-16 01:43:54.880408: step 83590, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 45h:45m:22s remains)
INFO - root - 2017-12-16 01:44:01.413624: step 83600, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.648 sec/batch; 44h:46m:14s remains)
2017-12-16 01:44:01.914758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6036105 -2.7514105 -2.1955316 -1.2471375 -1.5551219 -1.7641139 -2.0155616 -2.2797749 -2.7160518 -3.2250891 -3.7060573 -5.3600311 -7.1656356 -7.9896059 -5.7465954][-3.6035712 -2.2635603 -0.46522141 -0.25856018 -1.3362041 -1.5176311 -1.2267857 -1.3279119 -1.3681345 -1.7239749 -2.3074427 -4.5499196 -7.3191624 -7.68417 -5.3871884][-1.9715004 -1.6625218 -1.17736 -0.22487545 -0.059614658 -0.30465889 -0.62709236 -0.42268324 -0.019329548 -0.53207684 -0.9764924 -3.1273959 -5.209518 -7.1403294 -5.9420075][-4.3236856 -2.8761492 -1.1922207 -0.34742165 -0.23878956 -0.4750762 -1.022572 -0.7684536 0.13112211 0.83772707 0.29314375 -1.9268689 -4.2620459 -6.03015 -4.9153833][-4.8146873 -5.1597624 -4.6938682 -2.6771548 -1.7901337 -1.3384547 -1.5137463 -1.8851213 -1.4449935 -0.77102232 -0.37265778 -1.6291509 -3.6601985 -5.9464664 -4.6914191][-5.9402037 -5.0643759 -3.6366491 -1.3190498 0.08230257 1.0393081 1.639595 0.91181517 0.15101004 -0.40259266 -0.48110104 -1.2948427 -2.9562132 -4.7770171 -3.1071994][-5.9367452 -4.4470348 -2.3041553 0.18644094 1.4052854 2.8711696 3.4913077 2.7621741 2.221024 1.3214626 0.8653388 -0.17045879 -2.5368755 -3.3959017 -1.5486279][-6.0092645 -4.3827734 -2.429508 0.11642218 1.7024088 3.4661889 4.0925879 3.6091065 3.1535878 2.1342602 1.4146571 -0.35203123 -2.4708123 -3.0498626 -0.75487614][-5.3296366 -3.8903918 -2.4004138 -0.044399261 1.4376869 2.5357013 3.0070558 2.8460679 2.9236579 1.7899094 1.2877192 -0.39148998 -2.5885115 -3.6536205 -1.3236465][-5.7755461 -5.055059 -3.7001281 -0.61292982 0.87236834 1.4147258 1.8991337 1.4254017 1.0592561 -0.40404081 -1.1634703 -2.6548774 -4.5946827 -5.1825795 -2.4872243][-6.8698893 -6.3342876 -5.6829729 -3.2694452 -3.1125629 -2.5534413 -2.0208321 -1.7678094 -1.4661074 -2.4400096 -3.49039 -6.4687915 -7.4795027 -6.1344357 -3.9847865][-9.5545912 -9.6605854 -8.2247486 -6.2569036 -6.639853 -5.9819508 -5.8167887 -5.4739318 -4.7761159 -4.8887758 -5.171205 -6.4428825 -6.3535528 -5.6766372 -3.0997982][-12.250102 -11.502925 -9.7419453 -8.4088669 -7.8051863 -6.6395469 -6.4446464 -6.2879009 -6.608973 -6.9405704 -6.8525171 -6.6349611 -6.8983407 -5.8096614 -3.6665678][-7.6866217 -6.8174238 -5.9063783 -4.4303064 -4.6094904 -4.3428135 -3.5602674 -3.0285528 -3.7635641 -5.0593929 -6.585331 -6.5045424 -5.9178357 -5.2616367 -4.1282525][-3.7334394 -3.850765 -1.8926013 -0.2984519 -0.23210526 -0.45366716 -1.1189032 -1.4333153 -0.97431612 -1.9817302 -3.2884314 -4.4604807 -4.6532278 -4.6649895 -5.1301317]]...]
INFO - root - 2017-12-16 01:44:08.440510: step 83610, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 45h:29m:05s remains)
INFO - root - 2017-12-16 01:44:15.038986: step 83620, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 45h:39m:49s remains)
INFO - root - 2017-12-16 01:44:21.619579: step 83630, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 44h:51m:18s remains)
INFO - root - 2017-12-16 01:44:28.244334: step 83640, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 45h:38m:51s remains)
INFO - root - 2017-12-16 01:44:34.827230: step 83650, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 44h:12m:59s remains)
INFO - root - 2017-12-16 01:44:41.444115: step 83660, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.666 sec/batch; 46h:04m:01s remains)
INFO - root - 2017-12-16 01:44:48.042070: step 83670, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.677 sec/batch; 46h:46m:22s remains)
INFO - root - 2017-12-16 01:44:54.628789: step 83680, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 45h:29m:02s remains)
INFO - root - 2017-12-16 01:45:01.180645: step 83690, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.689 sec/batch; 47h:35m:50s remains)
INFO - root - 2017-12-16 01:45:07.758354: step 83700, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 44h:56m:45s remains)
2017-12-16 01:45:08.294140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5079522 -7.4128156 -8.2805185 -8.4972858 -9.08704 -9.0775652 -8.87391 -9.9862719 -11.280762 -11.011495 -10.187325 -10.417336 -10.768997 -8.6890593 -5.4973392][-8.459959 -8.82388 -8.9281416 -9.5311451 -10.148057 -9.8349323 -9.0895824 -9.3671865 -10.326314 -11.081568 -10.900572 -9.8629456 -9.86916 -9.0715046 -7.1229358][-7.35002 -8.9119034 -10.155435 -9.1995354 -9.1411829 -9.6235189 -9.1886768 -8.7394428 -9.31397 -9.73772 -10.27552 -9.8036537 -9.7051144 -8.4400034 -6.2302132][-9.1374054 -9.9776478 -10.413367 -9.5705385 -8.7925406 -7.0930276 -6.0210009 -7.5846224 -9.3232431 -9.105072 -9.149189 -9.313508 -10.064191 -9.1276073 -6.7056832][-8.187109 -10.863136 -12.037769 -10.256833 -7.9155951 -4.1670442 -0.95390606 -2.7378163 -5.9148183 -7.5064459 -9.1717777 -9.1595068 -9.0196762 -8.4965839 -6.745][-10.214062 -10.714278 -10.511545 -8.8545647 -6.4392357 -1.4790449 3.8170276 3.3400054 1.5119586 -2.2640433 -6.7863369 -6.903821 -6.9821353 -6.9240551 -5.680645][-9.8489075 -11.082811 -9.5606632 -6.2769485 -4.1003051 1.504859 7.4999833 7.6222205 6.9916368 2.5458121 -3.0322802 -5.233057 -7.2870188 -6.6335421 -4.2641649][-9.4480925 -11.26281 -10.438044 -6.1123648 -1.6294446 3.1694036 6.5736318 6.911468 7.4106927 2.6766515 -1.9142077 -4.0512695 -7.4023113 -8.0266571 -6.2549362][-8.8741255 -8.553442 -9.0588856 -6.6296978 -2.0830719 2.4458442 4.71483 4.3773456 5.2340264 2.0334234 -1.2354741 -3.973083 -8.2162 -8.7045908 -8.0194759][-7.7093425 -7.0628715 -7.3794088 -5.8451891 -3.730669 -0.25898552 3.2108531 2.8555064 1.9845767 -0.12709332 -1.9994123 -4.4819479 -7.578794 -9.5234556 -9.9539585][-9.4156075 -10.252403 -10.148891 -8.1285162 -6.9789429 -4.3875704 -1.3236637 -2.4135478 -3.9074683 -5.7788887 -6.7900167 -8.2585669 -9.85871 -11.26894 -10.885657][-14.130297 -13.978008 -13.767321 -12.224427 -10.943851 -9.0607624 -8.8074608 -8.5126724 -8.23454 -10.33334 -11.804735 -12.315157 -12.536897 -12.778835 -12.481193][-12.099556 -11.93907 -11.959599 -10.805897 -10.933538 -9.9934063 -9.4780293 -9.7764816 -10.757994 -10.05533 -9.0596771 -10.197413 -11.481049 -12.28091 -11.114521][-10.032695 -10.252541 -9.8669481 -9.4278049 -7.8237424 -7.84729 -8.2676382 -8.6530657 -9.864996 -9.6279154 -9.0798883 -8.6736288 -8.8706942 -9.0629005 -8.274684][-6.217638 -7.0056372 -6.6878781 -6.118794 -5.0693069 -4.2395773 -3.5932596 -4.602705 -6.6095104 -7.1921687 -7.5681081 -7.0086575 -6.9207673 -7.9472141 -8.1137047]]...]
INFO - root - 2017-12-16 01:45:14.902505: step 83710, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 44h:34m:43s remains)
INFO - root - 2017-12-16 01:45:21.476118: step 83720, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 45h:09m:15s remains)
INFO - root - 2017-12-16 01:45:28.043175: step 83730, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 44h:54m:26s remains)
INFO - root - 2017-12-16 01:45:34.658643: step 83740, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.669 sec/batch; 46h:14m:03s remains)
INFO - root - 2017-12-16 01:45:41.278766: step 83750, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 45h:19m:00s remains)
INFO - root - 2017-12-16 01:45:47.879704: step 83760, loss = 0.17, batch loss = 0.12 (11.8 examples/sec; 0.678 sec/batch; 46h:52m:13s remains)
INFO - root - 2017-12-16 01:45:54.501417: step 83770, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 45h:50m:58s remains)
INFO - root - 2017-12-16 01:46:01.091212: step 83780, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.645 sec/batch; 44h:31m:59s remains)
INFO - root - 2017-12-16 01:46:07.790987: step 83790, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 47h:05m:15s remains)
INFO - root - 2017-12-16 01:46:14.338392: step 83800, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.648 sec/batch; 44h:45m:18s remains)
2017-12-16 01:46:14.792531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9684 -4.7897024 -4.5935917 -5.0119338 -6.3549261 -6.941164 -7.1697097 -7.6962242 -8.7541494 -9.1873846 -8.6755314 -8.0406933 -8.8097563 -8.1433678 -6.9225779][-5.7858529 -6.5405173 -5.9810925 -5.3054075 -5.8009062 -7.159132 -8.0231514 -8.501255 -9.2630491 -9.7647076 -9.7151508 -9.070034 -9.047245 -8.5954666 -7.9228125][-4.3187671 -5.8306046 -6.945261 -6.4876695 -6.4924378 -6.7884502 -6.5742583 -7.7909412 -8.7076464 -9.0130825 -9.392004 -8.9385128 -9.2933922 -9.3945866 -9.4067907][-4.4265265 -6.0770488 -6.2171545 -5.905416 -6.187973 -5.8201008 -5.383853 -6.2421861 -7.5744171 -8.3974991 -8.6242208 -8.8702707 -10.095486 -10.422792 -10.728713][-5.992816 -6.6925888 -7.2330141 -6.5327797 -4.9764085 -2.1714602 -0.18114042 -2.282866 -3.8038731 -5.0477667 -6.7658968 -6.7579794 -7.8415017 -9.0213795 -9.2924347][-7.8349533 -7.0304022 -6.0703716 -5.1295514 -2.6241298 0.5044241 3.3129621 4.0670056 3.6745229 -0.33400297 -4.7890768 -5.5268817 -6.6123891 -6.6517749 -6.4269552][-7.8735566 -7.0627475 -5.6437249 -2.8911014 -0.54195213 1.9449868 5.1425872 6.2376513 6.697969 1.5650473 -3.7896614 -4.9828329 -8.0293894 -8.4895544 -6.7434239][-9.1300068 -8.0189886 -7.1358624 -3.9988828 -0.91019917 2.4012609 5.98517 6.8978152 6.7966228 2.6866164 -1.1366119 -4.64221 -9.4421711 -10.096478 -9.2682734][-6.9266887 -6.9835563 -7.5459347 -6.2212958 -4.2561469 -0.40205669 2.7968545 4.2463288 4.8675847 1.914804 -1.0421958 -4.8093047 -10.086391 -11.865547 -11.659573][-5.9693017 -6.024384 -6.012207 -5.86824 -6.3541374 -4.2763968 -2.0838788 -0.39585495 0.93321657 -0.87053967 -3.6708674 -6.9436121 -9.7853718 -11.298004 -12.186424][-9.1430759 -10.066638 -9.6912642 -9.4859362 -9.660841 -9.0695992 -8.6073027 -7.2812138 -6.7626944 -6.9488058 -7.8531981 -10.787331 -13.500353 -12.902487 -11.390589][-13.173575 -13.712656 -13.072043 -12.690065 -12.320434 -12.661758 -13.145096 -13.125511 -12.413912 -11.543319 -11.48507 -13.083732 -13.331694 -13.874693 -13.464157][-12.572792 -12.978325 -12.809696 -13.027649 -13.262647 -12.113552 -11.658833 -12.406855 -12.661098 -12.261326 -12.052214 -12.121975 -10.894929 -9.9269123 -9.3074131][-11.604445 -11.049595 -9.9516525 -9.7350912 -9.4721813 -9.6603155 -10.101166 -9.6412544 -9.8733482 -10.728724 -11.027393 -9.5193081 -8.7090683 -7.69431 -5.9383397][-8.9266472 -8.3508377 -6.9162173 -5.4911718 -4.1654811 -3.7473378 -4.6939559 -5.7517095 -6.4448123 -6.2141514 -6.7920508 -7.5482492 -7.4157419 -6.4856367 -6.3955359]]...]
INFO - root - 2017-12-16 01:46:21.363863: step 83810, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 47h:07m:42s remains)
INFO - root - 2017-12-16 01:46:27.898725: step 83820, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 43h:58m:29s remains)
INFO - root - 2017-12-16 01:46:34.461253: step 83830, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 45h:24m:57s remains)
INFO - root - 2017-12-16 01:46:41.006379: step 83840, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 44h:56m:17s remains)
INFO - root - 2017-12-16 01:46:47.606619: step 83850, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 45h:45m:53s remains)
INFO - root - 2017-12-16 01:46:54.175503: step 83860, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 45h:19m:03s remains)
INFO - root - 2017-12-16 01:47:00.803957: step 83870, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 47h:09m:59s remains)
INFO - root - 2017-12-16 01:47:07.407413: step 83880, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 46h:01m:21s remains)
INFO - root - 2017-12-16 01:47:14.033356: step 83890, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 46h:27m:17s remains)
INFO - root - 2017-12-16 01:47:20.622886: step 83900, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 45h:25m:20s remains)
2017-12-16 01:47:21.192504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1567879 -3.5324097 -3.89155 -3.2985404 -3.8244247 -4.8187184 -5.5672336 -6.2220273 -6.4294672 -6.2731185 -6.0994482 -5.1178508 -5.6960583 -8.7495384 -9.1944923][-6.6599641 -5.9479723 -5.5895529 -5.7033033 -6.4672608 -7.4053907 -8.2428989 -8.1864519 -8.0577 -7.1876016 -6.77262 -6.58706 -8.5276957 -10.096508 -9.2281628][-6.2754292 -7.3753843 -7.7832956 -7.5403438 -8.4775963 -8.1527786 -8.1801052 -8.2464981 -8.2902412 -7.8836536 -7.5411654 -5.9004173 -5.9083872 -7.18186 -6.9794273][-4.490222 -5.4586778 -6.3406324 -7.6252875 -8.9362 -8.7941151 -8.82359 -8.9646835 -7.6639442 -5.9653854 -5.6075206 -5.3089161 -6.5928154 -7.5004907 -7.3097615][-6.1551704 -6.2525339 -6.6527839 -7.3374691 -7.4261112 -5.8289013 -4.9911942 -5.091517 -5.0462289 -4.2312417 -4.3004508 -3.5050404 -4.9016757 -6.7419534 -7.3875465][-6.5728817 -5.2899909 -4.7468109 -4.4961348 -4.3880796 -1.6227651 1.2477531 1.4699292 1.3488579 -0.50323343 -1.6019163 -1.7688565 -3.8300259 -6.5359993 -7.8620453][-6.4976311 -7.2017775 -6.0345941 -4.6042261 -3.0472882 -0.03745079 3.6440206 6.1475682 6.6220021 3.0717597 -0.15806723 -1.9099934 -4.4526 -6.2282834 -6.9278455][-6.7973008 -7.7224674 -6.815176 -4.253871 -2.4057279 2.0742421 5.5384345 8.2489033 8.2822552 5.2142987 1.2971616 -2.5484295 -6.3646007 -7.5597811 -8.2382851][-7.3248472 -8.0723228 -6.4002981 -4.2425437 -2.8317108 1.2212596 4.2784562 6.0082269 6.7307591 4.3436847 0.47964191 -3.371227 -6.537621 -8.778389 -8.5694695][-6.8058038 -7.307847 -6.25213 -5.0255275 -3.1191602 0.15353727 2.8040833 4.4385848 4.6506267 1.9226341 -1.9261575 -3.7416687 -5.1422663 -8.3341808 -10.52721][-8.11548 -7.6096048 -6.3307033 -5.6884818 -4.2638259 -3.05707 -2.596652 -1.1396375 -0.72108746 -2.7112122 -5.4659386 -6.15425 -7.6857591 -7.7411308 -7.1959376][-9.2253342 -9.0868788 -8.344883 -7.54457 -5.8934674 -5.7994647 -5.4351096 -5.3468981 -5.6702957 -6.7555385 -8.3537569 -8.13636 -7.8582935 -7.7581739 -7.8390303][-10.074411 -9.5198174 -8.9214087 -8.0225315 -7.0337162 -6.406702 -7.184391 -8.1122971 -8.9483032 -9.1976576 -9.908391 -9.8943291 -9.8406124 -8.11749 -7.308814][-7.3234024 -8.0950232 -7.8493328 -7.4712839 -5.9339356 -6.4736052 -7.4361525 -7.382885 -8.0108213 -9.2672634 -9.6951742 -8.3440485 -7.12988 -7.1292462 -8.2171383][-7.2531424 -8.460022 -9.233489 -7.8446617 -6.5312195 -5.8762579 -6.2008667 -5.8186579 -5.5605721 -6.8179607 -7.7071495 -7.9923439 -7.9190021 -8.1801319 -8.9259968]]...]
INFO - root - 2017-12-16 01:47:27.860746: step 83910, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 45h:19m:32s remains)
INFO - root - 2017-12-16 01:47:34.474301: step 83920, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 45h:38m:03s remains)
INFO - root - 2017-12-16 01:47:41.049120: step 83930, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 45h:34m:47s remains)
INFO - root - 2017-12-16 01:47:47.586062: step 83940, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 45h:44m:39s remains)
INFO - root - 2017-12-16 01:47:54.142049: step 83950, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 45h:56m:55s remains)
INFO - root - 2017-12-16 01:48:00.739106: step 83960, loss = 0.23, batch loss = 0.19 (12.2 examples/sec; 0.654 sec/batch; 45h:10m:57s remains)
INFO - root - 2017-12-16 01:48:07.344691: step 83970, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 45h:02m:28s remains)
INFO - root - 2017-12-16 01:48:14.031612: step 83980, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.653 sec/batch; 45h:04m:37s remains)
INFO - root - 2017-12-16 01:48:20.572603: step 83990, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 46h:18m:57s remains)
INFO - root - 2017-12-16 01:48:27.077847: step 84000, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 44h:54m:59s remains)
2017-12-16 01:48:27.674217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.14935 -6.576045 -7.5884233 -8.1081228 -9.4440317 -11.242197 -12.335088 -12.565053 -12.584874 -12.068464 -11.036606 -11.714103 -10.990489 -12.054045 -9.8157616][-9.674386 -8.3966236 -8.6226368 -8.7792568 -9.2303877 -10.491611 -11.783518 -12.849618 -12.897345 -12.575999 -12.227301 -11.658601 -10.276867 -12.426962 -9.5947208][-8.0545216 -9.2740383 -10.500311 -9.7857037 -10.191926 -10.543346 -10.801407 -11.23419 -12.060666 -12.731382 -12.212398 -12.451859 -11.083622 -12.840869 -10.385374][-8.8432665 -9.1832657 -11.165388 -11.286573 -11.160188 -9.1736832 -8.5899849 -10.006247 -10.382153 -10.549634 -11.239288 -11.78619 -10.261912 -11.887659 -9.3848906][-10.355625 -11.311893 -12.684666 -12.404876 -11.124228 -7.08617 -4.6065245 -7.0516353 -9.9534168 -10.175201 -10.39575 -11.492796 -10.857653 -12.743082 -9.3577538][-12.173954 -11.936235 -12.668522 -11.701784 -9.6430473 -3.2810042 2.1627717 1.1031752 -2.5217764 -6.8871379 -9.9744387 -9.5854015 -8.8282967 -11.890907 -10.635517][-14.568947 -13.13447 -11.214747 -8.6882448 -6.64715 -0.95005655 5.6321864 8.03055 6.78145 -1.047823 -8.1386766 -9.30711 -9.1502829 -11.0765 -9.5680428][-14.56319 -14.206932 -13.120295 -6.5417175 -1.1947508 3.0800796 7.0816073 8.7985229 9.2007294 2.9564271 -4.8053226 -8.9678621 -9.7421436 -11.30388 -9.7679386][-11.611349 -10.854074 -11.805934 -8.4393053 -3.8019943 3.6541047 7.7384686 6.10362 5.1853881 0.8577075 -5.1450076 -9.9222889 -11.349175 -13.414001 -11.822142][-8.5503187 -7.7091575 -9.5151548 -8.6704426 -8.1378641 -2.0000787 3.3990245 3.002769 1.4504857 -4.2537794 -8.9030638 -11.154073 -12.151401 -14.607262 -13.486649][-12.469622 -10.736211 -10.874521 -10.438866 -11.307404 -9.3324032 -6.9607182 -5.4985247 -5.1122761 -8.9752769 -12.913816 -14.928589 -13.159264 -14.373785 -12.929327][-16.356258 -15.129923 -15.140425 -13.415327 -13.083072 -12.895901 -13.417859 -12.814596 -11.791456 -13.577032 -14.625011 -16.044744 -14.470156 -13.772717 -10.825457][-16.070732 -15.761179 -16.115326 -15.244301 -14.854752 -12.921104 -13.036797 -14.24806 -14.137621 -13.42309 -13.021597 -14.374958 -13.102747 -12.025016 -8.9162045][-11.56332 -11.354659 -11.302007 -11.698708 -12.22247 -10.815944 -10.12081 -9.6071415 -10.129944 -10.713684 -10.496017 -9.8750553 -9.834198 -9.6976433 -7.6511345][-8.312397 -7.3376265 -5.68033 -5.1542974 -5.5567646 -6.2011747 -6.5619469 -5.7233467 -5.1266589 -5.7059021 -6.4033895 -7.3814721 -7.2077141 -7.6116438 -7.6091948]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 01:48:34.356194: step 84010, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.663 sec/batch; 45h:43m:55s remains)
INFO - root - 2017-12-16 01:48:40.932792: step 84020, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.664 sec/batch; 45h:48m:20s remains)
INFO - root - 2017-12-16 01:48:47.464430: step 84030, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 45h:54m:22s remains)
INFO - root - 2017-12-16 01:48:54.033962: step 84040, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 45h:14m:17s remains)
INFO - root - 2017-12-16 01:49:00.681781: step 84050, loss = 0.23, batch loss = 0.19 (12.5 examples/sec; 0.640 sec/batch; 44h:08m:07s remains)
INFO - root - 2017-12-16 01:49:07.215451: step 84060, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 46h:52m:11s remains)
INFO - root - 2017-12-16 01:49:13.729391: step 84070, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 45h:20m:38s remains)
INFO - root - 2017-12-16 01:49:20.311326: step 84080, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.653 sec/batch; 45h:02m:05s remains)
INFO - root - 2017-12-16 01:49:26.914188: step 84090, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 44h:45m:54s remains)
INFO - root - 2017-12-16 01:49:33.526629: step 84100, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 44h:56m:33s remains)
2017-12-16 01:49:34.133643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7804987 -1.4134526 -1.4980431 -1.3973794 -2.2641976 -3.0084338 -3.0835168 -2.5495741 -2.299839 -2.5603769 -3.067591 -5.7336493 -6.5272036 -7.3497028 -7.200861][-2.468492 -1.3048596 -0.79294109 -0.85089445 -2.1111073 -3.0984569 -3.6955805 -4.2744012 -4.3901334 -3.9830058 -4.1415787 -6.4425836 -7.184289 -8.7741146 -7.6871977][-2.0580344 -0.58900928 -0.069772243 -0.33641195 -1.3313346 -2.0491612 -3.0646746 -4.1932011 -4.9358425 -4.7372518 -4.0211587 -5.9061127 -6.8647766 -8.7805672 -8.6705608][-1.7693224 -1.0543666 0.05300808 0.58224726 0.0517416 -0.94270897 -1.726666 -2.0025408 -2.3271556 -2.1621809 -1.9085677 -3.7797389 -5.6990991 -8.3427563 -8.8501015][-1.562304 -1.3622379 -0.68698168 0.092843056 -0.67782021 -0.89087057 -1.2690425 -1.4387817 -0.814908 -0.43901443 -0.47735167 -2.4799788 -3.9612565 -5.9537954 -6.2780647][-2.1170979 -1.801199 -1.4717188 -0.4024682 0.049627304 -0.12786102 -0.35044289 -0.59742403 -0.63512659 -0.11933184 0.38587236 -1.8101721 -3.7043529 -5.5052161 -5.738883][-4.9246864 -3.2895756 -2.000427 -0.61982775 -0.51640987 0.16718102 0.94036674 0.83026314 0.80393457 0.87916183 0.83495331 -1.7841763 -3.2494621 -4.4354076 -3.7465239][-6.3479419 -5.3619103 -3.7159634 -1.6861897 -0.68486214 0.37394285 1.2870483 1.683352 1.9625602 1.9913278 1.7747426 -0.81560278 -1.9710019 -3.1663148 -2.7002432][-6.4011335 -5.4822 -4.2603726 -1.7547898 -0.18617344 0.44709921 0.75603247 1.4685111 2.3287244 2.3264718 2.1401258 -0.42448092 -2.4259782 -3.458118 -2.0116355][-5.7179623 -5.0281982 -4.0758033 -1.7330682 -0.66418171 -0.058823109 -0.2008853 0.276958 1.4852057 1.1834674 0.59980249 -1.8779526 -2.9949477 -3.8604386 -2.8508334][-6.0291572 -5.1798687 -4.254385 -2.3145373 -2.0049183 -1.2583895 -0.90030241 -0.68426466 -0.13769674 0.0097856522 -0.29919386 -2.8596356 -4.8245015 -5.4041753 -4.9124656][-9.926836 -9.2930737 -7.5203629 -5.5151024 -4.7779431 -4.9108005 -4.9614258 -4.3755646 -3.3157279 -2.7519329 -2.8257034 -5.1536884 -6.393836 -6.7669621 -5.5276566][-12.730749 -11.49291 -9.6581364 -8.2862988 -7.839283 -7.0214591 -6.8899751 -7.0430689 -7.3113594 -6.777576 -6.0089912 -6.5044351 -6.8980188 -6.2803597 -4.8835969][-9.6272392 -9.63276 -8.2966728 -7.0632544 -6.5719628 -6.1062922 -6.2212029 -5.700418 -5.9496965 -6.7597456 -6.9884434 -6.9614449 -6.2356329 -6.15846 -5.0586119][-4.9719548 -5.3287296 -5.0701108 -3.6397 -3.4587574 -3.2592597 -3.3688622 -3.3810804 -3.5618663 -3.9620638 -4.6667957 -5.4432344 -5.8094163 -6.387536 -6.1454959]]...]
INFO - root - 2017-12-16 01:49:40.787205: step 84110, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 44h:19m:16s remains)
INFO - root - 2017-12-16 01:49:47.396090: step 84120, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 45h:23m:48s remains)
INFO - root - 2017-12-16 01:49:53.980395: step 84130, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 45h:22m:17s remains)
INFO - root - 2017-12-16 01:50:00.581700: step 84140, loss = 0.23, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 44h:54m:41s remains)
INFO - root - 2017-12-16 01:50:07.261451: step 84150, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 45h:07m:26s remains)
INFO - root - 2017-12-16 01:50:13.897637: step 84160, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.688 sec/batch; 47h:27m:55s remains)
INFO - root - 2017-12-16 01:50:20.407683: step 84170, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 44h:46m:08s remains)
INFO - root - 2017-12-16 01:50:26.954116: step 84180, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 44h:49m:22s remains)
INFO - root - 2017-12-16 01:50:33.623017: step 84190, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 45h:36m:29s remains)
INFO - root - 2017-12-16 01:50:40.277716: step 84200, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 46h:12m:40s remains)
2017-12-16 01:50:40.900181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2661672 -6.2946033 -5.3910446 -5.0265675 -5.1012974 -6.1272788 -6.6298494 -6.8189054 -7.0418158 -6.4184184 -5.7764559 -7.1792731 -8.2528915 -7.2656097 -7.1050858][-4.9908094 -4.9750834 -4.9019732 -4.4001942 -4.578311 -5.6576886 -6.1062341 -6.6211576 -6.572155 -5.8259969 -4.6402864 -5.6290884 -6.4139619 -6.2086811 -6.6278996][-3.81952 -3.8549433 -3.9107137 -3.4694104 -3.8400502 -4.1090508 -4.1246686 -4.6291475 -4.5231929 -4.3460817 -3.7557268 -4.4745045 -4.7755651 -5.1647158 -5.3809342][-3.0558162 -3.1933649 -3.1374323 -2.2847931 -2.7632775 -2.423636 -2.1272287 -3.0036392 -3.406317 -2.8400686 -1.9663949 -4.2005377 -5.1980267 -4.83952 -4.7079287][-2.8854465 -2.8321455 -2.9532745 -2.8165905 -2.5745354 -1.4958162 -0.66855 -1.3643188 -1.9223616 -2.3058958 -2.6553364 -4.6926384 -5.9482379 -6.0151939 -5.9918036][-3.6032543 -3.079247 -2.6629763 -2.2983665 -1.5837498 0.34714174 1.8096437 1.2318244 0.85419846 -0.21871758 -1.7954006 -4.8502808 -6.4661908 -6.3443551 -6.5091591][-2.6541102 -2.9212153 -3.2199872 -1.3447461 0.50885248 2.2230525 3.6922936 3.8866954 3.0206409 0.48093033 -1.6289558 -4.3005795 -5.9688234 -6.2954588 -6.5424829][-3.288974 -2.5747638 -2.9233646 -0.82474566 0.69674587 2.8631129 5.2205338 4.8667827 3.6844316 1.4576869 -0.47952414 -4.2324047 -6.1775565 -6.310184 -6.1437407][-3.5634422 -2.9382839 -2.133755 -0.85893345 -0.24748325 1.822926 2.7578626 3.3729997 3.3936753 1.2642684 -1.2114491 -4.9405336 -7.1795053 -7.9439173 -7.9908605][-3.0802102 -2.369097 -1.8129225 -1.076746 -1.1781235 0.26438189 1.0912967 1.4668775 1.7020407 0.564888 -1.2203522 -5.6196342 -8.4616318 -9.2855778 -9.2643757][-3.3149085 -3.8861842 -3.518096 -2.5661888 -2.3130646 -1.7836962 -1.8091903 -1.7268624 -2.6759381 -3.1181319 -3.076565 -6.8616848 -9.5842743 -10.2402 -8.9690008][-7.825716 -7.5330086 -6.3361745 -5.904716 -5.4434586 -5.0750751 -5.2574072 -5.8018804 -7.1071672 -6.994709 -6.9699883 -9.0727615 -9.6921444 -9.1089764 -8.04361][-10.794543 -10.429144 -8.0775986 -7.3401632 -7.507988 -7.7951784 -8.227706 -8.1804123 -8.2130871 -8.0206308 -8.5085106 -8.4899168 -8.562479 -7.7820597 -7.3273973][-10.350416 -9.3237 -7.8436527 -7.0167618 -6.6774654 -7.3398218 -8.0259151 -7.6910939 -7.2033644 -7.5021892 -7.6958113 -6.6512022 -6.5881371 -6.0800638 -5.5236959][-8.18778 -7.2786074 -6.1934347 -5.7101474 -5.9773545 -6.2829847 -5.7910686 -6.0603595 -6.4634342 -6.1661019 -4.8257174 -4.8102007 -5.907958 -6.3533607 -6.0772214]]...]
INFO - root - 2017-12-16 01:50:47.505332: step 84210, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 45h:50m:20s remains)
INFO - root - 2017-12-16 01:50:54.087446: step 84220, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 45h:11m:42s remains)
INFO - root - 2017-12-16 01:51:00.734618: step 84230, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 44h:06m:25s remains)
INFO - root - 2017-12-16 01:51:07.400775: step 84240, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 46h:26m:07s remains)
INFO - root - 2017-12-16 01:51:13.945169: step 84250, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 44h:53m:32s remains)
INFO - root - 2017-12-16 01:51:20.482780: step 84260, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 44h:56m:12s remains)
INFO - root - 2017-12-16 01:51:27.070218: step 84270, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 46h:41m:55s remains)
INFO - root - 2017-12-16 01:51:33.653218: step 84280, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 44h:32m:01s remains)
INFO - root - 2017-12-16 01:51:40.169888: step 84290, loss = 0.21, batch loss = 0.17 (11.9 examples/sec; 0.670 sec/batch; 46h:13m:15s remains)
INFO - root - 2017-12-16 01:51:46.758375: step 84300, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 44h:42m:53s remains)
2017-12-16 01:51:47.290659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.77889 -10.709267 -9.437974 -8.9582062 -9.5563145 -9.5027485 -9.3385391 -8.7697735 -8.335288 -8.0639477 -7.7658367 -7.391057 -8.2849884 -9.0211067 -6.2511353][-10.872206 -10.329523 -10.176151 -10.122204 -10.453238 -10.570087 -9.6906881 -8.9026947 -8.8439579 -9.0579176 -9.1398935 -8.8845444 -9.9985962 -10.01597 -6.9339085][-9.3481255 -9.4012423 -10.016462 -9.5517883 -10.189533 -10.363346 -9.85356 -9.16356 -9.2907829 -9.77882 -10.015038 -9.8003807 -10.519909 -11.170254 -8.8699284][-8.0957642 -7.5305238 -7.8946924 -8.3813248 -9.455101 -8.4279919 -7.4331708 -7.1927991 -7.5766168 -8.0031185 -9.3047075 -9.6630859 -10.820594 -10.80381 -7.8275814][-8.7982817 -8.22821 -8.2632065 -6.9055257 -6.3292689 -4.8794479 -3.6620729 -4.6209512 -5.5889111 -6.4475212 -8.1872644 -8.7337942 -10.541677 -11.064661 -8.1612444][-10.187656 -9.1919165 -8.8162441 -6.0052543 -4.6895461 -1.1675544 2.0526271 1.0981054 -0.90848875 -3.803992 -6.0049896 -6.1515555 -8.3002911 -9.8856487 -7.7037106][-10.802314 -10.004885 -8.9786911 -5.1781373 -3.0317743 1.1201997 4.6116862 4.6625037 3.8575711 -0.477911 -4.8555021 -5.2695255 -7.1599026 -7.8988037 -5.5240116][-11.777023 -9.9329967 -8.510067 -4.55196 -1.1102524 3.161365 5.4932637 5.7134919 5.6583753 1.7484865 -2.1841736 -4.3765182 -7.4709835 -7.8417583 -4.64886][-10.507701 -9.1714153 -7.9362197 -4.2391152 -1.5968494 2.1807318 4.2880769 4.2122359 3.2721229 1.3125234 -0.79945278 -3.8192258 -7.2297754 -7.8596659 -4.8174577][-7.552062 -7.5886836 -6.7139845 -4.3240309 -2.333009 0.23408937 2.6663671 2.3973365 1.2889948 -1.0380478 -2.8642838 -4.2202196 -6.7008209 -8.2725773 -6.5982838][-9.13026 -7.9266052 -7.3725467 -5.3623838 -4.5735145 -3.0158241 -2.2217643 -1.3941851 -1.0350962 -3.041048 -4.9239798 -6.7482963 -7.8667412 -8.6517305 -6.0663486][-12.736696 -9.9773655 -9.47182 -8.2020817 -7.9308796 -7.5795183 -7.3515286 -6.392549 -6.1407633 -6.9882622 -8.0038557 -8.228096 -8.3903313 -9.2759247 -6.2717161][-11.960825 -9.6168222 -9.0751247 -7.5836067 -7.4708858 -7.0850134 -7.526545 -7.7818289 -7.8375435 -7.9265513 -7.9421577 -7.5307946 -8.28241 -8.7330914 -5.8516321][-9.2533007 -8.0195112 -7.9271507 -6.6496153 -6.2994304 -6.2330556 -6.7896967 -6.6769915 -6.6507273 -6.8566089 -7.1476183 -5.7472334 -5.6204119 -5.2375593 -3.2480221][-6.419364 -4.7834578 -4.5165558 -3.8807118 -4.5249667 -4.7931571 -4.3136187 -4.4699125 -4.0587978 -4.0202909 -4.6581144 -4.2157221 -5.2635531 -5.1299148 -3.3020833]]...]
INFO - root - 2017-12-16 01:51:53.958161: step 84310, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 47h:14m:15s remains)
INFO - root - 2017-12-16 01:52:00.533103: step 84320, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 44h:45m:08s remains)
INFO - root - 2017-12-16 01:52:07.148214: step 84330, loss = 0.17, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 44h:07m:54s remains)
INFO - root - 2017-12-16 01:52:13.705743: step 84340, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 44h:47m:31s remains)
INFO - root - 2017-12-16 01:52:20.329249: step 84350, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.676 sec/batch; 46h:36m:37s remains)
INFO - root - 2017-12-16 01:52:26.983430: step 84360, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.681 sec/batch; 46h:57m:29s remains)
INFO - root - 2017-12-16 01:52:33.662356: step 84370, loss = 0.20, batch loss = 0.15 (11.8 examples/sec; 0.676 sec/batch; 46h:35m:39s remains)
INFO - root - 2017-12-16 01:52:40.298502: step 84380, loss = 0.15, batch loss = 0.11 (12.8 examples/sec; 0.626 sec/batch; 43h:06m:56s remains)
INFO - root - 2017-12-16 01:52:46.864494: step 84390, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.675 sec/batch; 46h:30m:49s remains)
INFO - root - 2017-12-16 01:52:53.493654: step 84400, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 46h:50m:58s remains)
2017-12-16 01:52:54.030670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6998568 -2.3758495 -2.3129418 -1.6620102 -2.5172136 -3.5664539 -4.8799434 -4.073473 -2.9051249 -3.1185203 -3.577177 -6.5594945 -8.5540428 -9.8768978 -9.6644917][-1.5246406 -0.25972605 -0.28393221 -0.46422243 -2.3597739 -3.2161798 -3.8075809 -4.9448657 -5.9399962 -5.6748538 -5.3629584 -8.4095087 -10.287667 -12.09684 -11.786453][-0.73950911 -0.015230656 0.35786057 1.8249164 1.0903969 -0.21094561 -2.1687522 -3.916667 -5.0251832 -6.0279202 -7.0330687 -9.4634275 -10.201529 -11.457801 -11.080338][-2.0930319 -0.23818874 0.83372211 1.4076123 0.14661217 -0.25991488 -0.50435209 -1.6546559 -2.8954976 -3.9423614 -4.7358732 -7.7007904 -9.40933 -10.364879 -10.274637][-2.7923331 -2.9433105 -2.5637164 -0.5603261 -0.4551053 -0.1648159 -0.061854362 -0.86155319 -1.965049 -3.0263026 -3.8214064 -6.355504 -7.5685306 -9.1334381 -9.6153841][-4.0757246 -3.5427632 -2.74893 -0.820034 -0.054115772 1.4047813 2.015811 1.6839232 1.0027227 -0.57709551 -2.1496 -4.6818714 -6.2631841 -7.4590693 -7.7074904][-5.6504664 -4.7793593 -3.2800672 -0.695611 1.0034332 2.7733884 3.623939 3.7410932 3.2765775 1.5500727 -0.53588057 -3.7587161 -5.8477297 -8.0383873 -8.4318495][-6.9783416 -6.0867729 -4.1649485 -1.2635455 0.12197399 2.3369222 4.3044486 4.4086871 4.3064284 3.7067151 2.0683541 -2.2345669 -5.3984718 -7.5723619 -8.8686619][-7.326416 -6.582727 -5.3784771 -2.6733396 -1.1275153 0.75453043 2.45828 3.1453261 3.5995812 2.7379107 1.3317242 -2.5098271 -5.4791203 -7.4601049 -7.8095722][-8.8013935 -7.4533615 -5.6669083 -2.9326854 -1.5073943 -0.38151789 0.66810131 1.1097326 1.6218519 1.0669994 0.11982632 -2.8839533 -5.1846719 -6.387403 -6.5373006][-11.595715 -9.6276159 -7.7881885 -4.9530153 -3.6142445 -3.0505087 -2.0826712 -1.8840809 -1.8872776 -2.2104123 -3.0907376 -5.9517612 -7.663702 -7.9838181 -7.1670952][-14.530264 -12.82091 -11.124984 -8.9807463 -7.9033346 -6.633523 -5.5563178 -5.5848455 -5.90277 -6.1574383 -6.7007294 -8.2576866 -9.2462788 -9.2811632 -8.280755][-13.99313 -12.658983 -11.07564 -9.9270449 -9.9298248 -9.1442242 -8.295476 -8.0195808 -7.8796244 -7.9971828 -8.3966675 -9.0070057 -8.9926167 -7.9503937 -7.1404548][-11.518786 -10.714421 -9.998765 -8.1634359 -7.2461653 -7.2251916 -7.9135494 -7.7852488 -7.6231928 -7.9126797 -7.849875 -7.9737854 -7.759542 -7.0536404 -6.6338878][-8.4757757 -7.2333431 -6.02661 -4.6000824 -4.1670027 -3.7795119 -3.4855063 -4.3313646 -5.5768375 -6.138607 -6.700767 -8.2012663 -8.728569 -8.3288174 -8.2914753]]...]
INFO - root - 2017-12-16 01:53:00.589857: step 84410, loss = 0.23, batch loss = 0.18 (11.9 examples/sec; 0.670 sec/batch; 46h:10m:40s remains)
INFO - root - 2017-12-16 01:53:07.145561: step 84420, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 46h:10m:46s remains)
INFO - root - 2017-12-16 01:53:13.761491: step 84430, loss = 0.13, batch loss = 0.08 (11.5 examples/sec; 0.694 sec/batch; 47h:50m:32s remains)
INFO - root - 2017-12-16 01:53:20.448327: step 84440, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 45h:03m:34s remains)
INFO - root - 2017-12-16 01:53:27.078939: step 84450, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.658 sec/batch; 45h:19m:11s remains)
INFO - root - 2017-12-16 01:53:33.619269: step 84460, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 44h:14m:14s remains)
INFO - root - 2017-12-16 01:53:40.223474: step 84470, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 44h:39m:45s remains)
INFO - root - 2017-12-16 01:53:46.841019: step 84480, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 45h:01m:05s remains)
INFO - root - 2017-12-16 01:53:53.400562: step 84490, loss = 0.24, batch loss = 0.20 (11.7 examples/sec; 0.683 sec/batch; 47h:01m:55s remains)
INFO - root - 2017-12-16 01:53:59.885688: step 84500, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.657 sec/batch; 45h:17m:38s remains)
2017-12-16 01:54:00.442716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7315223 -0.99260044 -0.089831352 0.76993418 0.92355156 0.22380352 -0.058461666 -0.64985657 -0.1212163 0.12317944 -0.85438776 -3.315455 -5.5826735 -6.9604654 -6.6249743][-2.3436904 -1.9402981 -0.67477703 0.093741417 0.15595341 -0.50170469 -0.344162 -0.38197041 -0.42787647 -1.6785164 -3.0626748 -5.1946616 -7.2720842 -7.8281007 -7.3730164][-2.8317449 -3.6560483 -3.2016251 -1.9675133 -1.2841921 -2.2982018 -2.5273602 -2.8579695 -2.9911819 -3.5578461 -3.6188297 -4.878232 -6.8671832 -7.6470222 -6.979249][-2.5466771 -3.4996183 -3.5912006 -3.3134029 -3.3637927 -3.3093567 -3.4892936 -3.7602954 -3.1934159 -3.2178435 -3.6348958 -4.9265156 -6.5550957 -7.8634052 -7.7637458][-2.4194865 -1.7994382 -2.0378966 -2.1448982 -2.1849182 -1.695456 -1.1838145 -1.4544849 -2.0304093 -2.0137084 -1.8357701 -3.7282162 -6.7365065 -7.378768 -6.8400054][-1.9119186 -2.0341458 -1.4048505 -0.70380545 -1.0229564 -0.064960957 1.1396265 1.4464836 1.6756711 0.12275267 -0.916296 -2.2425883 -5.1413813 -5.8185172 -5.1807327][-2.3888888 -2.4685507 -2.3977878 -0.87862062 -0.029074192 0.50193405 1.7041578 2.5319724 3.5543628 2.7336841 1.0223322 -1.1537261 -3.9234748 -4.4047127 -4.016233][-1.0057845 -1.244668 -0.55697632 0.46036673 1.4278831 2.5893722 3.6184916 3.5667968 3.8210177 3.4240155 2.3049169 -0.67357779 -4.2072906 -5.3748789 -5.9651608][-0.920197 -0.64641 -0.08634901 0.89642334 1.8618364 2.4077706 3.59903 3.523437 4.2695231 3.7312474 2.5444779 -0.23530817 -4.25232 -5.2722187 -5.1474991][-1.7115579 -1.4567871 -0.38509035 0.39176416 2.3753924 3.6612983 4.2930837 3.7232985 2.6867394 2.1309614 1.387722 -2.0630813 -5.1279368 -7.2197342 -8.0051584][-5.3337502 -4.2971063 -2.8694475 -2.1382384 -1.3722157 -0.097803116 1.2578444 1.5067296 -0.81165314 -2.1233518 -2.6561024 -5.3136244 -7.5102854 -7.88087 -6.678081][-10.520252 -8.4618893 -6.3999548 -4.603261 -3.9917583 -3.8328643 -4.1116347 -3.6585755 -4.4525127 -4.8095622 -5.061059 -6.0171843 -6.6548176 -7.6616945 -6.8128009][-11.079178 -10.12748 -8.597764 -7.5262632 -6.4345255 -4.7558255 -5.0715876 -6.0485754 -7.1057444 -6.9628611 -6.9601393 -6.8764334 -7.6936636 -7.1465139 -5.7487068][-9.10572 -8.2516279 -7.5468674 -6.3203173 -4.9682164 -4.3578825 -4.914216 -5.4952087 -7.4313755 -7.4672565 -6.641191 -6.0887589 -6.1902442 -7.2449474 -7.2580986][-5.6374564 -5.0392385 -5.0067573 -4.4887595 -3.8682129 -3.466639 -3.061038 -3.3529017 -4.4353189 -5.11702 -5.7226586 -5.5365424 -6.436142 -7.2734923 -8.412343]]...]
INFO - root - 2017-12-16 01:54:07.082710: step 84510, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 44h:28m:51s remains)
INFO - root - 2017-12-16 01:54:13.702783: step 84520, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 44h:02m:54s remains)
INFO - root - 2017-12-16 01:54:20.311755: step 84530, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 45h:01m:06s remains)
INFO - root - 2017-12-16 01:54:26.924127: step 84540, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 45h:09m:21s remains)
INFO - root - 2017-12-16 01:54:33.545652: step 84550, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.665 sec/batch; 45h:48m:56s remains)
INFO - root - 2017-12-16 01:54:40.177058: step 84560, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.651 sec/batch; 44h:48m:20s remains)
INFO - root - 2017-12-16 01:54:46.816131: step 84570, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 45h:23m:02s remains)
INFO - root - 2017-12-16 01:54:53.353527: step 84580, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 45h:12m:50s remains)
INFO - root - 2017-12-16 01:54:59.919295: step 84590, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.645 sec/batch; 44h:24m:03s remains)
INFO - root - 2017-12-16 01:55:06.566308: step 84600, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.683 sec/batch; 47h:01m:35s remains)
2017-12-16 01:55:07.162685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6542821 -5.1795492 -5.1255636 -4.8243914 -5.1134062 -5.2264438 -5.3207326 -4.9712567 -4.4884992 -3.9016819 -3.2467108 -5.2836742 -7.3215837 -8.5683813 -7.8579469][-6.3266692 -7.2368622 -8.0301476 -7.9880919 -7.8237362 -7.6034184 -7.3387671 -7.0088415 -6.561976 -5.87798 -5.1146278 -6.9047 -9.28841 -10.335355 -9.7077637][-5.9451923 -7.2961812 -8.593626 -8.1974716 -8.3349342 -8.9268608 -8.8353262 -8.0559645 -7.5245028 -6.8682652 -5.9230776 -7.9046855 -10.444672 -11.57137 -11.513532][-6.74248 -7.2177997 -7.6775556 -7.5485444 -8.5434752 -8.0882568 -7.6598859 -7.6393385 -7.2278223 -6.4185557 -5.9961047 -8.1891317 -10.3561 -11.524177 -11.427921][-7.9498591 -9.3886137 -10.258964 -9.0545616 -7.3028607 -4.5820284 -3.5553026 -4.7354875 -6.2522421 -5.79817 -5.2611141 -7.3171172 -9.7188854 -11.494848 -11.365923][-10.592272 -10.1653 -9.2403593 -7.8309751 -5.6294041 -2.0072565 0.652616 0.22177172 -1.2822371 -3.2517238 -4.2884851 -6.0503993 -9.1966391 -11.146629 -11.459568][-10.402302 -10.481291 -9.939621 -6.2098207 -2.846276 0.60903215 3.9757066 5.1255546 4.2970357 0.19948292 -2.9725046 -5.0872173 -7.7861576 -9.5524406 -10.802963][-9.0389023 -8.5461369 -6.9531441 -3.5795972 -1.2069521 2.1036792 3.9402537 4.8136744 5.3165374 2.4926276 0.53230953 -3.7562351 -7.880991 -10.295445 -10.520935][-7.9684391 -6.9644556 -5.8933787 -3.0681841 -1.0869536 1.6475744 3.6741424 4.3896985 3.6546187 1.1096997 -0.7764039 -4.9693279 -9.08412 -11.14913 -10.732098][-5.7136378 -5.288733 -4.6934571 -2.2739434 -1.0024276 0.59745264 2.2385721 2.8111129 1.4554782 -0.769814 -2.4508362 -5.5908651 -9.1186008 -10.873352 -10.809824][-6.3586206 -5.5482159 -4.5653777 -3.7769411 -3.8736055 -3.3940043 -2.2422903 -1.8188338 -2.3378713 -3.9051361 -5.5358844 -8.9851952 -11.200418 -11.996639 -11.22345][-11.225947 -10.470488 -9.3242817 -8.1728134 -8.4661179 -8.3270645 -7.5253892 -6.9302454 -6.4343281 -7.471808 -8.8977928 -10.479474 -11.278858 -11.222345 -10.232195][-14.169537 -12.767087 -11.4191 -10.878492 -11.95771 -11.363613 -10.11633 -9.08184 -8.0936031 -7.4543624 -7.5164318 -9.071414 -9.95105 -9.6943359 -8.5334358][-11.599014 -11.011438 -9.9933491 -9.1293612 -8.9937887 -8.7475252 -8.2535257 -7.26222 -6.4432306 -6.7249975 -7.4130993 -7.5925274 -8.2290516 -7.74336 -6.1498423][-9.893919 -9.5695181 -8.4916687 -6.4749789 -5.0981169 -5.4159307 -5.884872 -5.334619 -5.0016489 -4.8177471 -5.1908579 -6.8628774 -8.18894 -9.3742046 -8.192791]]...]
INFO - root - 2017-12-16 01:55:13.661683: step 84610, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.636 sec/batch; 43h:46m:31s remains)
INFO - root - 2017-12-16 01:55:20.154927: step 84620, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.652 sec/batch; 44h:52m:21s remains)
INFO - root - 2017-12-16 01:55:26.720660: step 84630, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 45h:58m:29s remains)
INFO - root - 2017-12-16 01:55:33.338912: step 84640, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.682 sec/batch; 46h:58m:56s remains)
INFO - root - 2017-12-16 01:55:39.951859: step 84650, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 45h:15m:09s remains)
INFO - root - 2017-12-16 01:55:46.573256: step 84660, loss = 0.22, batch loss = 0.18 (12.1 examples/sec; 0.661 sec/batch; 45h:29m:21s remains)
INFO - root - 2017-12-16 01:55:53.160307: step 84670, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 44h:21m:13s remains)
INFO - root - 2017-12-16 01:55:59.765519: step 84680, loss = 0.18, batch loss = 0.14 (12.6 examples/sec; 0.634 sec/batch; 43h:38m:01s remains)
INFO - root - 2017-12-16 01:56:06.317913: step 84690, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 45h:53m:01s remains)
INFO - root - 2017-12-16 01:56:12.952480: step 84700, loss = 0.10, batch loss = 0.06 (12.1 examples/sec; 0.664 sec/batch; 45h:40m:26s remains)
2017-12-16 01:56:13.464137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.6892376 -9.5893555 -9.6975346 -9.25954 -8.8507109 -8.8805456 -8.7070112 -8.148386 -8.0372467 -8.5002937 -8.9487457 -10.847761 -11.698757 -10.271907 -8.65295][-8.2261372 -8.9675961 -9.1561346 -9.3850965 -10.088682 -10.624583 -10.293018 -9.8070927 -8.6654091 -8.40104 -7.8578711 -9.4534111 -10.809359 -9.7750025 -8.8927574][-5.9441419 -7.2780662 -8.1522388 -7.8304324 -7.6051121 -8.0690775 -8.0709267 -8.0772448 -7.9106731 -7.7201147 -6.9790392 -9.0309753 -10.451649 -9.5750179 -9.8345976][-3.2083764 -3.535013 -4.0062704 -4.8542404 -5.8225827 -5.7579579 -5.5329242 -5.8000746 -5.6264648 -5.8312449 -6.1534281 -8.1632366 -8.7162743 -8.6553583 -8.8201036][-3.4001634 -3.5759432 -3.8872476 -2.4288056 -1.6451249 -0.91740227 -1.2378044 -1.5825667 -1.7371809 -3.0077844 -3.6396258 -6.1518722 -7.920867 -8.2724829 -8.5391474][-5.0838943 -4.0386286 -2.3227689 -1.2981367 -1.2790327 0.27676296 1.1055021 1.1596327 0.37322235 -0.71582603 -1.0895004 -4.2318444 -6.2047176 -5.9730706 -6.211472][-6.0815887 -6.0210366 -4.8346391 -2.5562146 -0.57731628 1.4217124 2.2661252 2.5430856 3.004005 2.4455867 1.5555153 -1.9350247 -3.904068 -4.44715 -5.5895605][-4.1507258 -3.3244371 -2.4493363 -0.65160847 0.31134892 1.8333077 2.2362804 2.8945546 3.4882636 3.1733451 2.3732238 -0.19965649 -2.2481353 -3.2423861 -4.7005634][-2.7972527 -2.3367898 -1.7179441 0.24744892 1.7429223 3.0577121 2.8560348 3.4368424 3.8269858 3.3080878 3.262609 0.44496155 -1.7610612 -2.8813457 -3.6453466][-1.6397753 -1.3955212 -0.62163544 0.5163703 0.84324551 1.2068672 1.3344946 1.4431829 1.0162129 1.0578365 1.5850277 -0.596292 -2.8002572 -3.4520228 -4.6545458][-4.9586153 -3.9815755 -2.9512532 -2.3641346 -2.9072597 -2.581851 -2.7589066 -2.2977424 -2.6953104 -2.5871081 -2.6845183 -4.426208 -5.2889738 -6.1297455 -6.0314884][-9.4620676 -7.9197688 -7.3682704 -6.5530829 -7.2401485 -7.3188286 -8.2612371 -8.6502151 -8.5208321 -8.3411646 -7.744031 -8.8004436 -8.5185909 -7.7074661 -6.7767696][-11.109077 -9.92543 -8.9722948 -8.6238346 -8.4803295 -8.6430883 -9.0911131 -9.3433495 -10.232693 -10.207232 -10.485953 -10.815645 -10.316633 -9.2093105 -6.7461863][-8.8178577 -9.0597219 -8.1149015 -7.4672832 -7.2604418 -6.6820283 -7.3196211 -7.9994445 -8.886425 -8.9325838 -9.2411795 -9.4236479 -9.6251984 -8.5556068 -8.0187082][-7.0218687 -6.8225365 -6.1943493 -6.3346748 -5.4506702 -5.3315239 -5.3789778 -5.7606568 -5.4737291 -5.8682537 -6.5944014 -7.9118013 -8.3197365 -8.1283293 -8.7072992]]...]
INFO - root - 2017-12-16 01:56:20.020557: step 84710, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 45h:34m:32s remains)
INFO - root - 2017-12-16 01:56:26.687456: step 84720, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 46h:19m:36s remains)
INFO - root - 2017-12-16 01:56:33.262576: step 84730, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 44h:20m:32s remains)
INFO - root - 2017-12-16 01:56:39.936550: step 84740, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.679 sec/batch; 46h:42m:43s remains)
INFO - root - 2017-12-16 01:56:46.590842: step 84750, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 45h:58m:19s remains)
INFO - root - 2017-12-16 01:56:53.260193: step 84760, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 45h:56m:56s remains)
INFO - root - 2017-12-16 01:56:59.814337: step 84770, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 46h:06m:17s remains)
INFO - root - 2017-12-16 01:57:06.433295: step 84780, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 45h:09m:17s remains)
INFO - root - 2017-12-16 01:57:13.054051: step 84790, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.686 sec/batch; 47h:10m:50s remains)
INFO - root - 2017-12-16 01:57:19.679418: step 84800, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.687 sec/batch; 47h:14m:18s remains)
2017-12-16 01:57:20.195900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5305314 -6.4975414 -7.8878841 -8.8447981 -9.702035 -9.86217 -9.8972836 -10.271263 -10.761666 -10.601892 -9.6351528 -10.538181 -11.862166 -10.103111 -6.07037][-7.2388821 -7.2055149 -7.8060961 -8.9552479 -10.407036 -10.862346 -10.74614 -10.77906 -11.283403 -10.890622 -9.6380043 -9.7921371 -10.487157 -11.08655 -9.2703171][-5.0752854 -6.7698951 -9.284462 -9.3186636 -9.7357216 -10.007965 -9.9286041 -10.395811 -11.090609 -10.566767 -9.6449871 -9.5629005 -10.050798 -9.8897762 -8.4224434][-7.3375206 -7.5483937 -8.9163361 -9.708271 -9.7206316 -7.5450716 -6.257441 -7.7125263 -9.4751205 -9.5911732 -8.3715878 -8.4914455 -10.365249 -10.876612 -9.2935247][-6.796679 -8.4644747 -10.56139 -11.254729 -9.71893 -5.2622104 -1.4253206 -3.0991488 -6.6692634 -8.1023865 -8.5272388 -7.8929038 -9.0593719 -9.87904 -9.2874632][-8.6654348 -9.2789354 -9.4604073 -9.2736549 -6.7830095 -0.84009123 4.5182261 4.2363353 0.9016099 -3.0162203 -6.1478114 -5.8601747 -6.59032 -7.8718376 -8.3441305][-8.07468 -7.9208164 -7.1327982 -5.6238537 -3.4512653 2.2047095 8.1957817 9.6981812 7.6094108 1.9764462 -3.12455 -4.357522 -6.4916043 -7.615262 -6.8505731][-8.1905909 -9.6283445 -9.4696846 -5.42567 -1.1797705 2.2277284 6.6324668 8.0323105 7.2994819 2.8932633 -1.4569879 -3.5514579 -6.5517988 -8.1763582 -7.6677809][-6.6242628 -7.0344782 -8.32259 -5.9732447 -1.8201962 2.4290395 5.9353204 5.1109605 4.988894 2.4586434 -1.2969899 -3.8172832 -7.992732 -10.222733 -9.4520512][-6.0693169 -5.8716807 -7.4855242 -6.8062439 -4.6317539 -0.9009409 3.2466245 1.871418 1.1670933 -0.37477636 -2.8010969 -4.2249451 -7.4319677 -9.9969873 -11.000893][-10.575068 -10.151225 -10.725155 -9.5220146 -8.8559141 -6.2661376 -3.1005008 -4.2060213 -5.2197556 -6.2780313 -7.1600375 -8.1788855 -10.996881 -12.3917 -11.598892][-14.018095 -13.727152 -14.054276 -12.594096 -11.6906 -10.605436 -9.8477592 -9.931776 -9.0604887 -9.6797905 -10.99456 -12.543492 -12.490061 -12.555059 -12.37605][-13.544811 -13.537402 -13.36618 -12.190903 -12.689001 -12.103634 -11.441022 -11.781067 -11.897724 -11.54566 -10.969173 -12.073442 -12.799736 -12.47323 -11.435432][-10.864856 -10.919746 -10.847443 -9.5522547 -8.2943745 -7.8658361 -8.7815914 -9.9846191 -11.003805 -10.397209 -9.9337349 -10.379286 -10.892605 -10.061531 -9.0379076][-7.7816868 -8.3195381 -6.8596725 -6.0520706 -5.667 -5.2710252 -4.8713045 -4.9803548 -6.2623978 -7.9969225 -8.2457533 -8.5827837 -9.1320324 -9.43374 -9.6282711]]...]
INFO - root - 2017-12-16 01:57:26.797070: step 84810, loss = 0.20, batch loss = 0.16 (12.4 examples/sec; 0.647 sec/batch; 44h:31m:00s remains)
INFO - root - 2017-12-16 01:57:33.323143: step 84820, loss = 0.20, batch loss = 0.15 (12.3 examples/sec; 0.652 sec/batch; 44h:50m:19s remains)
INFO - root - 2017-12-16 01:57:39.936209: step 84830, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 44h:31m:50s remains)
INFO - root - 2017-12-16 01:57:46.646889: step 84840, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 45h:51m:57s remains)
INFO - root - 2017-12-16 01:57:53.360414: step 84850, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.683 sec/batch; 46h:58m:20s remains)
INFO - root - 2017-12-16 01:58:00.021935: step 84860, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 45h:35m:03s remains)
INFO - root - 2017-12-16 01:58:06.646284: step 84870, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 45h:21m:11s remains)
INFO - root - 2017-12-16 01:58:13.236598: step 84880, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.660 sec/batch; 45h:24m:23s remains)
INFO - root - 2017-12-16 01:58:19.857407: step 84890, loss = 0.22, batch loss = 0.17 (12.1 examples/sec; 0.661 sec/batch; 45h:25m:55s remains)
INFO - root - 2017-12-16 01:58:26.441078: step 84900, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 46h:00m:15s remains)
2017-12-16 01:58:27.014045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9915252 -2.548583 -3.1437211 -3.882829 -5.6391625 -6.9979911 -7.8607697 -8.083003 -8.2796869 -7.9195852 -6.2337108 -5.9455776 -6.2761989 -6.5492878 -6.5079432][-4.9421158 -4.4912291 -4.4213834 -6.2430072 -8.2251472 -9.8671494 -10.695162 -11.055631 -10.927593 -9.8797894 -8.23586 -7.9519477 -8.2919207 -8.9288273 -8.5885916][-3.6093719 -5.0536151 -7.1191063 -8.3190022 -10.006648 -11.818853 -12.665909 -12.009442 -11.232027 -10.080104 -8.6468067 -9.0007868 -10.081532 -10.881573 -10.025682][-5.4908409 -6.5860806 -7.6806765 -9.5110168 -11.848804 -11.977695 -11.001907 -10.595633 -10.407375 -9.4811325 -8.4319611 -9.3688812 -11.126238 -13.003311 -12.845505][-6.8044672 -10.044245 -11.771816 -11.29154 -11.091912 -9.1739407 -7.1805296 -7.5719891 -8.2826586 -7.6589766 -6.986671 -8.499279 -10.921413 -13.496558 -13.340924][-9.2311134 -11.035355 -11.834602 -11.442358 -10.040998 -4.246285 0.85612059 0.65826893 -0.66481018 -3.7191529 -6.2703242 -8.1225433 -10.388 -13.25956 -13.973743][-10.315155 -10.593837 -10.425003 -9.0816917 -6.00531 0.0069642067 5.59228 7.6216226 6.5883813 0.776144 -3.6093433 -6.7807312 -10.104376 -12.081627 -11.936192][-10.175957 -9.5084267 -8.9546108 -7.4150696 -3.627677 2.8560014 8.4874573 11.101227 11.158731 4.5393863 -1.3721032 -6.060535 -10.139564 -12.114916 -11.913321][-8.2879639 -7.9126019 -7.5882773 -6.3115125 -4.0228662 0.6185441 5.7283731 7.9556603 7.4437671 3.6994624 0.12029505 -5.3801517 -10.493013 -12.407322 -12.288277][-8.338624 -8.3411627 -8.4927769 -6.7544618 -4.6211929 -1.2277241 2.2201715 4.0232825 3.5482755 -0.22540522 -3.0948353 -6.9101 -11.249056 -14.654215 -15.683014][-10.95289 -10.404348 -10.220312 -9.5137291 -8.3468857 -6.46235 -4.9584451 -3.6410904 -3.4306893 -4.668489 -6.3790078 -10.047551 -13.443356 -14.797407 -14.189766][-15.921322 -13.829981 -12.268642 -11.50774 -10.232227 -9.708662 -9.0894861 -8.3734217 -9.102623 -10.025517 -10.589931 -11.359453 -13.441372 -13.537108 -12.431744][-16.709734 -14.130919 -12.086898 -11.351406 -11.053054 -10.981625 -10.376108 -10.360015 -10.829839 -10.920645 -10.992535 -11.743788 -13.130783 -11.741142 -9.3274479][-11.92115 -11.242463 -10.41939 -9.08688 -8.8408251 -9.1505632 -8.8998117 -8.6824188 -8.7739944 -9.5479536 -9.64884 -8.8795872 -8.8347616 -8.6216946 -8.4014425][-7.1542616 -6.6476765 -5.7123647 -4.4728 -4.7741609 -5.5074205 -5.99732 -6.194943 -6.2564034 -6.6651578 -6.8907943 -8.227478 -9.0509 -8.6444731 -8.5685482]]...]
INFO - root - 2017-12-16 01:58:33.712614: step 84910, loss = 0.12, batch loss = 0.08 (11.5 examples/sec; 0.694 sec/batch; 47h:43m:59s remains)
INFO - root - 2017-12-16 01:58:40.284365: step 84920, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 45h:46m:53s remains)
INFO - root - 2017-12-16 01:58:46.816314: step 84930, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.642 sec/batch; 44h:07m:09s remains)
INFO - root - 2017-12-16 01:58:53.433503: step 84940, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 44h:26m:51s remains)
INFO - root - 2017-12-16 01:58:59.986502: step 84950, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 44h:46m:37s remains)
INFO - root - 2017-12-16 01:59:06.636249: step 84960, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 44h:58m:02s remains)
INFO - root - 2017-12-16 01:59:13.182573: step 84970, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 44h:23m:15s remains)
INFO - root - 2017-12-16 01:59:19.744937: step 84980, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 44h:40m:43s remains)
INFO - root - 2017-12-16 01:59:26.287710: step 84990, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 45h:32m:09s remains)
INFO - root - 2017-12-16 01:59:32.971739: step 85000, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 44h:51m:07s remains)
2017-12-16 01:59:33.534415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4430504 -4.1630807 -3.795361 -2.8774381 -2.9799008 -4.0947251 -5.0358791 -5.867229 -6.2179942 -5.8172112 -5.4281759 -5.0246496 -5.9951372 -7.3358784 -8.2451544][-6.2718582 -5.4725676 -4.7165651 -4.7560186 -5.1358023 -5.6908793 -6.541162 -6.6334271 -6.7823505 -5.7732615 -5.3239613 -5.7030931 -7.1999416 -7.9553561 -6.7788076][-6.4924693 -6.8169756 -6.3054466 -5.8635173 -6.3849535 -6.3883595 -6.594306 -6.480092 -6.1417875 -6.111547 -5.495245 -4.5964136 -5.3189712 -5.9273105 -6.0062156][-3.8031721 -4.4475231 -5.0530682 -6.4055004 -7.8288965 -8.1222811 -8.1192875 -8.20805 -7.1624422 -5.1665993 -5.1722364 -4.9853463 -5.6443386 -5.8843818 -5.6403866][-5.143724 -5.2893286 -5.3819661 -5.254014 -5.1335673 -4.5393167 -4.3355541 -4.4844303 -4.3057528 -3.6491668 -2.8487682 -3.1589377 -4.0758314 -4.8468881 -5.5007415][-6.583889 -5.4849648 -3.7333674 -3.3453643 -3.0235438 -1.6020579 0.3601408 0.49832487 0.75393772 0.10740948 -1.0137877 -0.97291851 -3.6236043 -4.890635 -6.510787][-6.5488811 -7.1042547 -5.7744656 -4.2256637 -2.4950864 0.005150795 2.9299455 5.5365872 6.3536487 3.9096971 1.446394 -0.56351185 -3.6276219 -5.5121737 -5.512876][-6.5450869 -7.0879917 -5.6980138 -3.7926106 -1.5094285 1.0445213 4.1050191 6.6478686 7.287477 4.77231 1.7073822 -1.8876071 -5.7926884 -6.4659591 -7.1183929][-6.72278 -7.88165 -6.3090682 -4.5247984 -2.978267 0.42739773 3.1621213 5.4811206 6.1416583 4.3135495 1.0868154 -3.002938 -6.5073042 -7.7423191 -7.3137116][-6.6474314 -7.056354 -5.9315548 -4.5397425 -2.6923244 -0.14983463 1.7179332 2.81101 2.9051423 0.74122906 -1.7640433 -4.3069453 -6.6337161 -7.9899077 -9.4848042][-8.8542614 -7.9114494 -6.1248441 -5.0343142 -4.013104 -2.7909911 -1.9996376 -1.4205232 -1.9617949 -3.6256282 -5.5114908 -6.7922654 -8.4435186 -7.3699861 -6.3846626][-9.6901779 -9.472105 -7.8116436 -6.6349196 -5.3553438 -5.3388157 -5.0366235 -4.4809761 -4.9886947 -6.4958878 -7.895937 -8.3610077 -8.4730816 -8.3135424 -7.9360209][-8.2139692 -7.9247694 -7.4595995 -7.0678406 -7.0249062 -5.9211245 -5.8011913 -6.2281895 -6.82956 -7.5760264 -8.6993256 -9.4119682 -9.5782013 -8.0530519 -7.3291378][-5.3135276 -5.7392831 -5.4320555 -5.1575389 -4.455493 -5.2727776 -6.2815666 -6.241396 -6.2630324 -7.2547984 -7.959734 -7.0454097 -6.7894258 -7.181457 -8.0867634][-5.4161854 -6.1160855 -6.1768532 -5.9582129 -5.240315 -5.3134956 -4.7262979 -4.8689575 -5.121017 -5.767158 -6.3594685 -6.9948664 -7.1492095 -6.4726315 -7.1757455]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-85000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-85000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 01:59:41.109278: step 85010, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 45h:02m:44s remains)
INFO - root - 2017-12-16 01:59:47.737197: step 85020, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 45h:05m:55s remains)
INFO - root - 2017-12-16 01:59:54.425684: step 85030, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 44h:18m:30s remains)
INFO - root - 2017-12-16 02:00:01.035516: step 85040, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 45h:31m:13s remains)
INFO - root - 2017-12-16 02:00:07.663641: step 85050, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.674 sec/batch; 46h:20m:36s remains)
INFO - root - 2017-12-16 02:00:14.387314: step 85060, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.678 sec/batch; 46h:34m:49s remains)
INFO - root - 2017-12-16 02:00:21.038838: step 85070, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.657 sec/batch; 45h:11m:00s remains)
INFO - root - 2017-12-16 02:00:27.637551: step 85080, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 45h:03m:10s remains)
INFO - root - 2017-12-16 02:00:34.173780: step 85090, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 44h:01m:49s remains)
INFO - root - 2017-12-16 02:00:40.744638: step 85100, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.654 sec/batch; 44h:55m:16s remains)
2017-12-16 02:00:41.282676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5080757 -6.0023341 -5.3903837 -4.6469693 -5.3598132 -5.6367588 -5.8905973 -5.1334548 -5.0903997 -4.976531 -4.4341936 -7.1088414 -10.124644 -8.9067936 -8.838459][-5.371304 -4.492672 -3.3305001 -2.4020505 -3.3863297 -4.4046245 -4.818449 -4.7435374 -4.3876157 -3.830884 -4.4407806 -6.427948 -8.2966652 -9.3248272 -9.8148947][-3.9136703 -3.4917393 -2.6685038 -2.1726582 -2.9801404 -2.8942966 -3.3736496 -3.3562269 -3.2031667 -3.301805 -3.2140789 -5.9615822 -9.32748 -9.3283339 -9.2539387][-4.0610423 -3.7615385 -2.9269423 -1.932843 -2.7216716 -2.8093262 -2.34043 -2.6192756 -3.1491425 -3.0567403 -2.8523138 -5.4441233 -8.4222946 -8.3546076 -7.7247629][-2.4423337 -3.6332572 -2.668844 -1.542954 -1.7438536 -1.1951442 -0.94367218 -0.46598387 0.078575611 -0.094480991 -0.71356106 -3.6308098 -6.3372383 -6.8923082 -7.2491226][-4.3962379 -3.2957628 -2.2806251 -1.1656265 -0.25951958 0.69328642 0.96820545 1.0785923 1.061799 0.63311958 0.098581314 -2.4914613 -5.0993967 -5.1986685 -5.2714868][-6.6955104 -5.6851282 -3.6457925 -1.3606539 0.24133635 1.5619955 2.2775083 2.5959153 2.4340653 1.7206979 1.2326822 -1.4886932 -4.3209715 -4.3761811 -4.6383114][-7.3721557 -6.2637882 -4.2494831 -1.0098243 0.98899174 1.8537946 2.3655438 2.5128665 2.3688111 1.8969879 0.7164731 -1.7191339 -4.3273716 -4.6757665 -5.0306354][-5.8754363 -5.298872 -3.1813803 -0.2807312 1.1204972 1.9569125 1.9547048 1.698875 1.9090104 1.6647544 0.9508543 -2.246716 -5.3102164 -5.6193585 -5.8328276][-6.3307562 -5.0742626 -2.7803957 -0.22148466 0.96629095 1.7017183 1.8796959 1.1466341 0.12175131 0.41771889 0.16806984 -2.7907276 -5.6157494 -6.0232625 -6.2838168][-9.4076948 -8.09327 -5.2106838 -2.0319023 -0.90691376 -0.471035 -0.755671 -1.7299383 -2.5930536 -2.6404011 -2.8728936 -5.6488919 -7.7981529 -7.9247041 -7.6168385][-9.780798 -8.971426 -5.9982586 -3.3168886 -2.4395194 -2.6981325 -3.6867919 -5.0504293 -5.5799508 -5.7136822 -5.6779456 -6.5793738 -7.817863 -7.3242869 -6.8377337][-10.411627 -9.17152 -5.9725614 -4.3781896 -3.9940729 -3.8543055 -5.0233307 -5.4892945 -5.7383089 -5.9505229 -6.0282626 -6.8634157 -7.3520741 -6.3270636 -5.4524579][-9.4792652 -8.2269907 -7.1597672 -6.2651229 -5.5684266 -5.2993717 -5.9650741 -5.800724 -5.9270611 -5.8050418 -5.3118024 -5.1763239 -4.9803309 -4.5438643 -4.4955239][-6.928503 -6.807097 -6.3043346 -5.532968 -5.4263277 -4.9813209 -4.4817257 -4.6194592 -4.4900293 -3.4238811 -3.6435103 -4.4511728 -4.4717011 -5.0409746 -5.7170777]]...]
INFO - root - 2017-12-16 02:00:47.867522: step 85110, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 45h:07m:18s remains)
INFO - root - 2017-12-16 02:00:54.439303: step 85120, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.681 sec/batch; 46h:47m:05s remains)
INFO - root - 2017-12-16 02:01:01.084138: step 85130, loss = 0.19, batch loss = 0.14 (12.0 examples/sec; 0.669 sec/batch; 45h:57m:55s remains)
INFO - root - 2017-12-16 02:01:07.665723: step 85140, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.672 sec/batch; 46h:11m:14s remains)
INFO - root - 2017-12-16 02:01:14.264114: step 85150, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 44h:45m:31s remains)
INFO - root - 2017-12-16 02:01:20.857814: step 85160, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 44h:03m:20s remains)
INFO - root - 2017-12-16 02:01:27.503791: step 85170, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 46h:08m:58s remains)
INFO - root - 2017-12-16 02:01:34.143920: step 85180, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 45h:52m:12s remains)
INFO - root - 2017-12-16 02:01:40.709505: step 85190, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.652 sec/batch; 44h:48m:05s remains)
INFO - root - 2017-12-16 02:01:47.310063: step 85200, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.672 sec/batch; 46h:10m:07s remains)
2017-12-16 02:01:47.813840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7958853 -4.0063124 -4.2931471 -3.0218618 -2.5046804 -2.4602332 -1.1252379 0.66395712 1.7730041 0.1006093 -2.1520822 -6.2779641 -9.540616 -13.823874 -14.243216][-3.1462724 -3.4587083 -3.9898076 -4.2343159 -4.1132092 -2.9593692 -0.85633087 -0.066132545 0.97843504 0.54706383 -1.3074527 -6.2243495 -10.047592 -14.70677 -16.778374][-3.0853698 -4.0591345 -4.004427 -3.417841 -3.9885821 -3.6202331 -1.8015218 0.36854029 0.22416544 -0.76108551 -1.7338738 -6.8754997 -10.518633 -14.112337 -16.12923][-2.1285164 -2.8197465 -2.9341321 -1.8819458 -1.6638169 -1.1610093 -1.2000718 -0.13465309 0.95695734 -0.081716537 -2.0042088 -6.4529452 -9.92585 -14.63591 -15.665352][-1.7192361 -1.5713444 -1.5394845 -1.8154368 -1.5683031 -0.24026585 0.27344322 0.60920048 0.6229 0.68028259 -1.7999716 -7.1867828 -10.186398 -14.414421 -16.203724][-2.0747018 -3.3743339 -2.4153152 -1.5174584 -0.48779345 0.33291149 1.7597032 2.4897065 1.8285384 0.48313189 -1.453949 -6.4992986 -11.226272 -14.344702 -15.396948][-3.0159135 -4.1305189 -4.1657033 -2.3624153 -0.25039721 1.79112 3.1425662 3.7970729 4.20472 2.0122972 -0.81450367 -5.4882355 -8.8171864 -13.340241 -14.955233][-4.5526743 -4.6128011 -4.19841 -2.9140155 -0.85734129 2.044661 3.9968429 4.4508653 4.6496692 3.4958081 0.82938385 -4.5309081 -7.9472675 -12.562825 -14.393873][-4.3276315 -6.2067237 -5.2879138 -2.9261472 -1.6409245 0.36269808 3.0394778 4.4845376 3.9098039 2.6519504 0.37737036 -4.4542942 -8.5421619 -12.375159 -13.711369][-3.7511363 -5.1968956 -5.8884993 -4.5169444 -2.7658553 -1.3692117 0.32155514 1.9037743 2.2924323 0.45583916 -1.1410055 -4.7973685 -8.3275909 -12.039173 -13.624777][-5.9022255 -6.9680624 -7.3382821 -6.6747842 -6.1367211 -4.9204216 -3.2932534 -2.759887 -2.0804758 -2.6547723 -4.1316562 -8.7658157 -10.001268 -12.791143 -12.493622][-7.8567476 -8.5356531 -8.3193569 -7.4251375 -7.3834133 -7.5396628 -6.8253412 -6.5478745 -6.1452465 -6.3654237 -7.916501 -10.133298 -10.85594 -12.505209 -13.058458][-9.1515522 -9.3869877 -8.3615637 -8.1370344 -8.2133617 -8.3340216 -8.46224 -8.4160357 -7.7092423 -8.0945368 -8.5970688 -10.210474 -11.599729 -11.453413 -11.347322][-8.6420822 -8.5555868 -8.0459185 -7.1635742 -6.8589826 -7.9131832 -8.8400536 -8.8806324 -8.2967949 -8.5887318 -8.1047792 -8.1024323 -7.9179678 -9.8045006 -10.269401][-9.3546572 -8.965086 -8.5142326 -8.134696 -8.0497618 -8.0204277 -8.805377 -9.4881992 -9.0633068 -8.2127857 -8.2238989 -9.22444 -9.1697559 -10.300117 -10.838341]]...]
INFO - root - 2017-12-16 02:01:54.341412: step 85210, loss = 0.19, batch loss = 0.15 (11.8 examples/sec; 0.679 sec/batch; 46h:37m:34s remains)
INFO - root - 2017-12-16 02:02:00.865102: step 85220, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.675 sec/batch; 46h:23m:36s remains)
INFO - root - 2017-12-16 02:02:07.543434: step 85230, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 46h:25m:48s remains)
INFO - root - 2017-12-16 02:02:14.230209: step 85240, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.677 sec/batch; 46h:30m:15s remains)
INFO - root - 2017-12-16 02:02:20.834369: step 85250, loss = 0.16, batch loss = 0.12 (12.5 examples/sec; 0.640 sec/batch; 43h:58m:26s remains)
INFO - root - 2017-12-16 02:02:27.335172: step 85260, loss = 0.12, batch loss = 0.07 (12.6 examples/sec; 0.635 sec/batch; 43h:35m:07s remains)
INFO - root - 2017-12-16 02:02:33.856138: step 85270, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 45h:10m:18s remains)
INFO - root - 2017-12-16 02:02:40.371718: step 85280, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 44h:48m:33s remains)
INFO - root - 2017-12-16 02:02:46.985200: step 85290, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 45h:04m:49s remains)
INFO - root - 2017-12-16 02:02:53.496770: step 85300, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.675 sec/batch; 46h:22m:10s remains)
2017-12-16 02:02:54.008321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1221304 -6.8434443 -6.48215 -6.0741353 -7.2192068 -8.4687786 -9.14072 -8.5616283 -7.9767394 -7.2324238 -6.3839583 -8.320672 -9.2682533 -8.9464922 -7.7633162][-7.0056639 -6.9758053 -7.0165014 -6.253655 -6.5733657 -7.7373238 -8.6503134 -8.30082 -7.8162532 -6.6773252 -5.7235818 -8.0033731 -8.54998 -9.0651617 -9.4034042][-5.3640203 -6.6035123 -7.1531358 -6.5992641 -6.5174632 -6.5420036 -7.0440497 -6.4312482 -5.4312143 -5.0097795 -5.1876278 -7.1505079 -8.3376865 -9.3370838 -9.2019939][-8.0275183 -8.8399429 -8.4183922 -6.414361 -5.7379475 -4.6795616 -4.5595026 -4.3503218 -4.0383863 -3.6446447 -3.8654308 -6.81502 -8.5562658 -8.8961964 -8.1812649][-9.3239336 -10.887952 -11.519596 -8.3253031 -5.2577581 -2.0894623 -0.26947355 -1.4280405 -3.128469 -3.326864 -3.9808593 -6.6989117 -7.674346 -8.0681629 -7.4647079][-11.773466 -12.274511 -11.048284 -7.3525481 -3.0738137 1.856564 4.2170196 3.1018138 1.5119729 -0.72934437 -3.077076 -5.0769544 -5.7599525 -5.89763 -4.8356895][-12.865919 -13.304554 -10.179168 -6.3747382 -1.8174384 3.2023273 6.2682719 6.2636561 4.4204783 0.638103 -2.4026721 -5.3111787 -6.8390551 -6.8622279 -5.4395838][-13.168491 -13.496176 -10.843155 -4.8313446 0.1431098 3.9272571 6.1602721 5.5476632 3.6918626 1.0168381 -1.5605669 -5.4515839 -7.4491191 -8.42366 -7.7136478][-11.144877 -11.374443 -10.279816 -5.3667521 -0.2886858 3.0419621 5.0158963 3.5299096 1.1792507 -1.0776386 -2.6606846 -6.0845056 -8.2275915 -9.6079674 -10.008368][-10.096235 -9.6330328 -8.9684515 -6.4533248 -2.5745082 0.74996614 2.8816762 1.3012404 -1.8436639 -3.2570841 -4.2016168 -7.3957682 -8.4515953 -9.4922228 -10.353775][-11.725409 -11.099473 -10.375698 -8.0290146 -6.3626 -3.5639322 -1.5034218 -2.5831068 -4.2149639 -5.4677162 -6.9340591 -9.9897976 -10.819049 -10.410736 -10.658913][-15.575071 -14.652077 -11.884948 -9.7674122 -9.394062 -7.708674 -7.052022 -8.00976 -8.5055437 -8.9070606 -9.8832817 -11.611477 -11.900208 -11.011101 -10.824413][-15.207262 -14.33983 -11.875383 -9.7748556 -8.84156 -8.606802 -8.851881 -8.5794392 -8.6630535 -8.8264246 -9.0543489 -10.704306 -10.880849 -9.4897394 -9.11749][-13.132009 -12.158438 -10.615601 -8.1816683 -7.0386977 -6.8197331 -7.1837473 -6.5788631 -6.9241285 -6.8681717 -6.3595405 -7.6131845 -7.4027138 -7.1737909 -7.2447605][-10.161573 -9.3504677 -8.0287495 -5.859478 -4.7660484 -4.0936847 -3.5248897 -3.490566 -3.620626 -3.903914 -4.1007462 -5.7820969 -6.6664267 -6.8359394 -7.1942358]]...]
INFO - root - 2017-12-16 02:03:00.603253: step 85310, loss = 0.19, batch loss = 0.15 (11.9 examples/sec; 0.674 sec/batch; 46h:15m:01s remains)
INFO - root - 2017-12-16 02:03:07.152759: step 85320, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 46h:01m:52s remains)
INFO - root - 2017-12-16 02:03:13.772395: step 85330, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.687 sec/batch; 47h:10m:20s remains)
INFO - root - 2017-12-16 02:03:20.280190: step 85340, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 44h:20m:54s remains)
INFO - root - 2017-12-16 02:03:26.880158: step 85350, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.651 sec/batch; 44h:41m:37s remains)
INFO - root - 2017-12-16 02:03:33.438183: step 85360, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 44h:12m:36s remains)
INFO - root - 2017-12-16 02:03:40.025672: step 85370, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 44h:45m:25s remains)
INFO - root - 2017-12-16 02:03:46.546545: step 85380, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.661 sec/batch; 45h:21m:12s remains)
INFO - root - 2017-12-16 02:03:53.115631: step 85390, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 44h:58m:26s remains)
INFO - root - 2017-12-16 02:03:59.686422: step 85400, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.677 sec/batch; 46h:26m:31s remains)
2017-12-16 02:04:00.184026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.9607277 -7.9405642 -7.2429657 -6.6508727 -6.9506531 -6.9731636 -7.3818569 -7.3060589 -7.5418396 -7.7840953 -6.7372508 -8.4351273 -9.3261051 -10.103069 -9.6251478][-6.215064 -6.9799633 -6.2206616 -6.2361131 -6.2517891 -6.9628263 -7.8920393 -8.5646591 -9.3595839 -9.2546206 -8.6138077 -10.999693 -12.163812 -12.056308 -11.430756][-3.7248805 -5.3688364 -5.7045889 -5.0396776 -5.65898 -6.6783066 -7.5679193 -7.8546119 -8.2203083 -8.84554 -9.3289709 -11.578621 -13.49548 -13.380352 -12.550013][-3.1104608 -4.9612088 -5.3149962 -5.5233231 -6.2644539 -5.8084383 -5.6114984 -6.8008189 -7.0750542 -6.8125138 -6.7068791 -9.6347389 -12.009431 -13.016413 -13.41507][-3.9945488 -5.3549824 -5.9320197 -5.2148952 -4.9274015 -3.8513761 -3.2623332 -4.0200558 -3.7141995 -3.9668427 -4.1593647 -6.9709044 -9.502265 -10.522863 -10.997192][-5.4293733 -5.7242775 -4.6053743 -3.8296726 -2.8676097 -0.7474246 0.88875055 0.8315959 0.94944572 -1.4186425 -3.264374 -5.838038 -8.0452 -8.7461109 -9.6828508][-6.6463408 -7.2693205 -6.1630282 -3.5106668 -1.3797321 2.1315513 4.9239297 5.8100247 5.2305446 1.7508149 -1.3465662 -5.1023932 -7.4861016 -7.8222589 -8.3106613][-6.2572937 -6.0119891 -5.1714258 -2.0181537 0.80058575 4.1422906 6.3012881 6.7432532 6.1831632 3.1903625 0.29989767 -4.68517 -7.7467079 -7.5164313 -7.6448932][-4.3178511 -4.806807 -4.187706 -1.3239789 1.1245737 3.781033 4.8645148 4.4112763 3.2967067 2.0938282 1.2900314 -3.9748044 -6.8201547 -5.9881229 -5.8186779][-2.2711341 -2.550509 -2.5372751 -0.4998436 0.97118473 1.1766524 1.3809662 1.2935901 0.69324541 0.14624739 -0.070893764 -3.1120541 -4.6070375 -4.9942784 -5.1867][-5.2264476 -4.6286716 -3.6107855 -2.4219964 -2.1435447 -2.6912508 -2.862205 -2.9337678 -3.7749104 -3.9143515 -3.4453676 -5.8571281 -5.8570924 -5.6712985 -5.9435191][-11.424574 -10.879438 -8.8340111 -6.3842826 -5.9171619 -6.9088058 -8.1125135 -8.55227 -8.724514 -8.457222 -8.0489693 -9.0375805 -8.5388536 -7.9286633 -6.9682083][-14.127748 -13.790283 -11.737785 -10.598679 -10.522911 -10.162384 -10.628839 -10.40134 -10.690039 -10.329592 -9.1403656 -10.542485 -10.109359 -8.0352688 -6.4385996][-12.214561 -12.332159 -11.547297 -9.7615833 -8.5421658 -8.5885944 -10.216265 -10.317646 -10.303972 -9.3012905 -8.46661 -7.8409634 -7.0771637 -7.076704 -5.7909784][-10.398148 -9.6572952 -8.6277094 -7.7430687 -6.6416159 -5.5562458 -5.8001571 -6.5042048 -7.8626852 -7.1708765 -6.9755392 -8.5621243 -9.0744953 -7.9437389 -7.0319972]]...]
INFO - root - 2017-12-16 02:04:06.737925: step 85410, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 46h:17m:21s remains)
INFO - root - 2017-12-16 02:04:13.270965: step 85420, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 44h:50m:54s remains)
INFO - root - 2017-12-16 02:04:19.776631: step 85430, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.686 sec/batch; 47h:06m:46s remains)
INFO - root - 2017-12-16 02:04:26.278606: step 85440, loss = 0.20, batch loss = 0.16 (11.9 examples/sec; 0.673 sec/batch; 46h:11m:37s remains)
INFO - root - 2017-12-16 02:04:32.895875: step 85450, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 45h:11m:31s remains)
INFO - root - 2017-12-16 02:04:39.396534: step 85460, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 45h:28m:40s remains)
INFO - root - 2017-12-16 02:04:45.867885: step 85470, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 44h:28m:38s remains)
INFO - root - 2017-12-16 02:04:52.492248: step 85480, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.690 sec/batch; 47h:20m:43s remains)
INFO - root - 2017-12-16 02:04:59.033346: step 85490, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 45h:16m:53s remains)
INFO - root - 2017-12-16 02:05:05.596442: step 85500, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 46h:41m:00s remains)
2017-12-16 02:05:06.134316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9802389 -6.4569964 -7.1913257 -6.376853 -6.3838015 -6.958189 -7.4745946 -6.978507 -6.0211725 -5.3090334 -4.4138355 -6.0827093 -7.3631029 -5.195631 -3.5780854][-5.6291556 -6.2288351 -5.6076784 -5.4510331 -5.9177389 -5.7528534 -5.9533048 -6.3008895 -6.1286445 -4.8223572 -3.7291954 -6.0507712 -7.170507 -5.6583691 -4.5431681][-4.3567467 -4.3710504 -4.5152383 -3.3976269 -3.4138975 -4.0093045 -4.4462576 -4.25289 -3.8082991 -4.0457497 -3.9407964 -6.1148624 -8.0662308 -6.6730185 -5.7144032][-6.2341666 -5.3393946 -4.0494356 -3.7192624 -3.6586199 -3.5150168 -3.7196598 -3.850174 -3.8189101 -3.4415121 -3.4156365 -6.3519979 -8.6201191 -8.2699919 -7.6410489][-4.9108558 -5.9510794 -5.6315613 -3.1518617 -2.7237844 -1.7984822 -0.89816189 -2.0138478 -3.318105 -2.714371 -2.2837811 -5.6093497 -8.5307846 -8.12005 -7.6888218][-6.484334 -5.9438624 -4.9958215 -2.3738363 -0.0099487305 2.6722884 3.9457593 2.375423 0.88100529 -1.106483 -3.2243705 -5.6724243 -7.3698897 -7.213706 -6.3751035][-7.8407965 -6.768446 -4.7397947 -1.9962354 0.94299507 5.1250367 7.20143 5.8695903 4.0045524 0.96370935 -2.1155686 -5.8249764 -8.7531424 -7.7286758 -6.5416608][-8.9364777 -7.8901949 -4.7014241 -2.143609 -0.26769304 3.2948194 5.8602195 5.6055989 4.3292804 1.658349 -0.48445272 -5.5342093 -9.7183962 -8.49053 -6.4922395][-7.1160774 -6.420032 -4.7045908 -2.2625353 -0.90285206 0.78981066 1.9023352 2.4576697 1.9885473 -0.48716259 -2.8315775 -6.858429 -10.114218 -9.2036209 -7.9981766][-5.7723327 -5.5359674 -5.3253269 -2.6855 -0.62001514 -0.80901337 -0.77050924 -0.24915028 -0.87966442 -2.7749753 -4.206727 -8.344985 -11.100618 -10.362921 -9.8611679][-8.2514639 -8.5685434 -7.5823464 -5.7215152 -4.6413693 -4.2356391 -4.1777444 -4.917223 -5.5888028 -6.2139263 -7.171916 -10.345149 -12.072803 -11.179918 -10.063372][-13.327246 -13.399858 -12.145134 -11.270913 -11.027847 -9.6147022 -8.9515533 -10.229173 -10.948145 -11.013303 -11.978161 -13.084595 -12.847029 -11.984663 -10.314224][-13.993763 -12.64484 -11.573395 -10.288802 -9.1888275 -8.9235907 -9.1286716 -9.4398623 -10.086816 -10.138289 -10.263091 -11.571455 -11.726915 -9.5260124 -7.7003717][-11.771602 -10.966846 -9.5360031 -7.471303 -6.4413424 -6.7119083 -7.3142891 -7.2748747 -7.173584 -7.2473116 -7.4385219 -8.4986057 -9.0711927 -7.8749089 -6.0679421][-9.5490408 -8.6337156 -7.2275882 -5.9145355 -5.3878012 -4.55083 -4.3820481 -4.9695354 -5.4286146 -5.0081306 -4.601264 -5.7175732 -7.0044975 -6.7418575 -6.6596909]]...]
INFO - root - 2017-12-16 02:05:12.783104: step 85510, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 44h:58m:27s remains)
INFO - root - 2017-12-16 02:05:19.383022: step 85520, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.679 sec/batch; 46h:33m:55s remains)
INFO - root - 2017-12-16 02:05:25.942635: step 85530, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 45h:05m:20s remains)
INFO - root - 2017-12-16 02:05:32.443332: step 85540, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.652 sec/batch; 44h:43m:47s remains)
INFO - root - 2017-12-16 02:05:38.996408: step 85550, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 44h:52m:52s remains)
INFO - root - 2017-12-16 02:05:45.631341: step 85560, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 45h:18m:06s remains)
INFO - root - 2017-12-16 02:05:52.111308: step 85570, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 44h:12m:02s remains)
INFO - root - 2017-12-16 02:05:58.650087: step 85580, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 45h:40m:24s remains)
INFO - root - 2017-12-16 02:06:05.293546: step 85590, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.689 sec/batch; 47h:16m:03s remains)
INFO - root - 2017-12-16 02:06:11.826600: step 85600, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.655 sec/batch; 44h:56m:39s remains)
2017-12-16 02:06:12.362224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5052886 -6.0345197 -4.8607235 -4.2432737 -4.0694938 -5.0621619 -5.1502042 -5.0621247 -5.0007257 -3.5964055 -2.7453222 -5.0557852 -6.4535694 -7.4779768 -8.3203554][-5.8218894 -5.98979 -4.7378411 -4.4537563 -4.24527 -5.0446444 -4.9760127 -3.9655507 -3.7529156 -3.2261515 -2.4050786 -3.8922706 -5.2768974 -7.1362619 -8.2352419][-3.8243155 -4.7338762 -4.7500687 -4.7401094 -4.6336737 -5.3701177 -4.63602 -4.0544457 -3.6344278 -2.8808236 -2.8198886 -5.2145844 -6.7680063 -8.7833242 -10.084707][-4.1369295 -4.5301552 -5.3375916 -5.8501835 -5.8745451 -5.6345129 -4.6407161 -3.5604968 -2.7525775 -2.6286762 -3.0548885 -5.9965281 -7.3159676 -8.6628275 -10.027006][-2.8606966 -4.8990793 -5.3720407 -5.068655 -5.084415 -4.0941525 -2.569464 -2.3057218 -1.4630613 -1.8576233 -1.9897115 -5.2448254 -7.4212427 -8.2483969 -9.2285576][-5.2782912 -5.9725509 -5.2655859 -2.9305804 -1.8152509 -0.42290974 1.1527162 1.6182742 1.7929955 -0.078505516 -1.1607876 -4.5150948 -6.1710424 -7.454442 -8.7311935][-5.5737596 -5.6192579 -4.4477906 -1.4903326 0.78671408 2.2066841 3.6631894 4.2955775 4.8913379 2.9652495 1.1346083 -3.4961345 -4.954535 -5.9079895 -6.9705496][-4.0635195 -4.0963278 -2.3862085 0.10679483 1.9657044 3.2255988 4.4884953 4.2631011 4.7415633 4.0869 3.0012536 -2.1044583 -4.1695218 -4.9974389 -5.0455642][-4.3836741 -4.425046 -3.3152683 -1.5593157 0.46277857 1.552021 2.3457465 2.6784682 2.6047921 1.8863316 1.3459229 -3.2348518 -4.9602838 -5.16382 -5.0442643][-4.0318551 -5.012702 -3.6588008 -1.8968289 -1.2738533 0.17124939 0.92032433 0.87005234 0.89598989 -0.063372135 -0.86297321 -5.0295277 -5.9424591 -6.2292266 -6.9641514][-6.0840325 -6.760488 -6.7416821 -5.4620504 -3.5612066 -2.5927093 -3.7692416 -4.0999 -4.41421 -4.8975024 -4.90256 -8.0950232 -8.760437 -8.5951643 -8.1193523][-10.564138 -9.4514542 -7.1234317 -6.8225989 -5.0423479 -3.5820611 -4.5982976 -6.1942215 -8.7086964 -9.0276432 -8.7013121 -9.6907673 -9.8083038 -9.8219585 -9.7331619][-12.866796 -11.180904 -7.9951429 -6.3073606 -3.9160607 -2.7916098 -4.5186567 -6.2870073 -8.707716 -9.5531731 -9.5503645 -9.4529638 -8.674098 -7.8760853 -6.81467][-12.074041 -10.815575 -9.2134209 -7.9604406 -6.4837003 -5.0034614 -5.6434345 -6.4885745 -7.1572618 -7.4857407 -7.0768347 -7.0798774 -6.4377275 -6.1445889 -5.3889875][-6.655149 -6.4928837 -5.7007151 -5.4222021 -5.0091457 -5.0881262 -5.1489763 -5.7367363 -6.0773921 -6.3410826 -5.5866408 -5.733849 -6.5148058 -6.84814 -7.5247507]]...]
INFO - root - 2017-12-16 02:06:18.896039: step 85610, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 46h:09m:32s remains)
INFO - root - 2017-12-16 02:06:25.456946: step 85620, loss = 0.16, batch loss = 0.11 (12.8 examples/sec; 0.625 sec/batch; 42h:51m:20s remains)
INFO - root - 2017-12-16 02:06:32.080579: step 85630, loss = 0.14, batch loss = 0.09 (11.6 examples/sec; 0.687 sec/batch; 47h:05m:38s remains)
INFO - root - 2017-12-16 02:06:38.634671: step 85640, loss = 0.11, batch loss = 0.07 (12.6 examples/sec; 0.637 sec/batch; 43h:39m:53s remains)
INFO - root - 2017-12-16 02:06:45.222130: step 85650, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 44h:30m:53s remains)
INFO - root - 2017-12-16 02:06:51.830842: step 85660, loss = 0.16, batch loss = 0.11 (12.7 examples/sec; 0.629 sec/batch; 43h:06m:40s remains)
INFO - root - 2017-12-16 02:06:58.455859: step 85670, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.650 sec/batch; 44h:32m:29s remains)
INFO - root - 2017-12-16 02:07:05.110632: step 85680, loss = 0.19, batch loss = 0.15 (12.2 examples/sec; 0.656 sec/batch; 45h:00m:09s remains)
INFO - root - 2017-12-16 02:07:11.673824: step 85690, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.669 sec/batch; 45h:53m:08s remains)
INFO - root - 2017-12-16 02:07:18.271616: step 85700, loss = 0.31, batch loss = 0.26 (12.3 examples/sec; 0.651 sec/batch; 44h:35m:51s remains)
2017-12-16 02:07:18.774934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1172047 -4.7522597 -4.4182997 -2.5440476 -1.9957385 -1.4244599 -1.33144 -0.95442343 0.016142845 0.24835491 0.45384979 -0.84992361 -1.8311067 -2.5106266 -2.6355314][-3.6604316 -2.9836667 -2.98438 -2.698658 -3.1064386 -2.5817728 -2.1127741 -1.2116756 -0.1084671 0.92230225 2.0241909 0.88068485 -0.59757471 -1.9254241 -2.8910503][-3.4466126 -3.3245194 -3.4121902 -2.2848217 -3.0086174 -3.3011751 -3.4821932 -2.358819 -0.97877979 0.2450552 0.89478636 -1.1963005 -3.3452413 -4.511991 -4.7461934][-6.6627283 -6.2641339 -5.572947 -4.3968754 -4.5867305 -5.097887 -5.7116909 -5.2321239 -3.424788 -1.9166086 -0.72153425 -2.1529682 -4.3259473 -7.0738516 -8.0823593][-8.3886042 -8.6109924 -7.7299089 -5.2448344 -3.4597538 -3.0585418 -4.2617478 -4.9662704 -5.3339992 -4.6555619 -3.4143572 -4.9783282 -6.4476681 -8.5399981 -9.3256121][-6.2848649 -5.7213984 -4.3531361 -2.3444054 -0.51506233 0.96730328 1.1344647 -0.27341986 -2.1354344 -3.2374718 -3.6605897 -4.6605411 -6.0185518 -7.9616594 -8.5584345][-4.2351265 -2.9357193 -1.0551705 1.0965252 1.6632748 2.2852492 2.3770204 2.0357571 1.5176969 -0.77581406 -2.5719304 -5.0964465 -7.4734516 -9.3785458 -9.554039][-4.4981194 -2.2505016 0.59200716 1.8137002 1.316442 0.65038729 0.907341 1.3445482 1.743825 0.66899729 -0.87864208 -4.039609 -6.8570271 -8.3347206 -7.61102][-3.5072403 -2.1181467 -0.66311884 0.9073205 1.8496165 1.1559734 0.20163059 -0.24446058 0.16708422 0.48918104 0.062608719 -3.3167219 -6.2298408 -7.8178549 -7.663157][-4.5941582 -3.7920632 -2.6491294 -0.58472061 0.56523752 1.2097268 1.315434 0.8471818 0.76701307 0.48370075 -0.36653185 -3.5105376 -6.306252 -8.601758 -8.5481167][-10.496156 -9.0388317 -7.1623626 -5.1917648 -3.081578 -1.1678514 0.475389 -0.23284197 -0.96750069 -1.6191788 -2.7304814 -4.8943744 -6.9334488 -8.4724464 -8.9057884][-15.223032 -13.589256 -10.945786 -8.1376724 -7.1299753 -6.6039238 -5.5758843 -4.4889445 -3.8905625 -3.7611589 -4.366353 -6.6033731 -8.3359985 -9.2018309 -9.5691805][-15.121092 -13.379992 -10.618308 -8.6159449 -7.9002295 -7.3174262 -7.2922392 -7.016088 -7.4239182 -7.09757 -6.7716627 -8.0658875 -7.7671375 -7.2596931 -6.4826403][-12.302362 -11.893682 -10.577339 -8.0250216 -7.4053955 -7.9258022 -8.01005 -7.2734532 -6.9775643 -7.4002962 -7.86814 -8.3366289 -8.3711681 -7.4182725 -6.487277][-9.2319069 -9.0080872 -8.1761017 -7.1650662 -7.0441632 -6.7679477 -6.5442405 -6.3731093 -6.1214309 -6.9593945 -8.1267223 -9.5741777 -10.816635 -9.9370365 -8.58781]]...]
INFO - root - 2017-12-16 02:07:25.427958: step 85710, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 44h:40m:45s remains)
INFO - root - 2017-12-16 02:07:32.014395: step 85720, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.688 sec/batch; 47h:08m:42s remains)
INFO - root - 2017-12-16 02:07:38.690938: step 85730, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 45h:24m:47s remains)
INFO - root - 2017-12-16 02:07:45.293912: step 85740, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 44h:19m:02s remains)
INFO - root - 2017-12-16 02:07:51.951279: step 85750, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 44h:10m:30s remains)
INFO - root - 2017-12-16 02:07:58.606712: step 85760, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 46h:01m:06s remains)
INFO - root - 2017-12-16 02:08:05.298306: step 85770, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.687 sec/batch; 47h:03m:25s remains)
INFO - root - 2017-12-16 02:08:12.006293: step 85780, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 46h:01m:32s remains)
INFO - root - 2017-12-16 02:08:18.643284: step 85790, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 46h:00m:56s remains)
INFO - root - 2017-12-16 02:08:25.251622: step 85800, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 45h:03m:12s remains)
2017-12-16 02:08:25.793760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8511693 -3.0390289 -2.8879554 -2.7731881 -3.3321869 -3.4692538 -3.8046019 -3.2711072 -3.3437347 -3.7372477 -4.7467041 -7.0677152 -8.697752 -10.553329 -10.286949][-3.6175964 -3.5069792 -4.0088053 -3.9782786 -4.2383952 -4.1619282 -3.9223685 -3.5951171 -3.923759 -3.8119936 -4.394392 -6.6810617 -8.5273056 -10.180965 -10.228027][-4.4985623 -5.4889731 -5.1494641 -4.2461376 -4.1339893 -3.3506868 -2.6482322 -2.7297146 -3.3597353 -3.5924625 -3.8459148 -6.0224404 -7.2556434 -9.1444893 -8.95978][-6.0012732 -6.1187286 -5.4885988 -4.1970863 -3.681324 -2.7078733 -2.1536856 -2.1031096 -2.5130477 -2.6523485 -2.5549779 -4.7707005 -6.1816254 -8.4461994 -8.0262318][-6.7395382 -6.6424761 -6.0968881 -3.740016 -2.3321033 -0.590642 -0.23821068 -0.46189594 -0.55150843 -1.0059834 -1.2386618 -2.8850605 -3.3817291 -6.3356719 -6.557466][-6.9618287 -6.6580992 -4.7668247 -1.3492646 1.277226 2.6664157 2.8829789 2.4369092 1.973772 0.94806385 0.26944542 -1.908993 -2.9269571 -5.9294777 -6.1069388][-7.6907625 -6.7646456 -4.6463118 -0.84129477 1.8857799 4.3691573 5.2814631 5.0300326 4.44856 2.9089179 1.4303889 -1.6583767 -3.2065773 -5.5737777 -5.1383634][-6.512424 -6.5446219 -4.9185219 -1.3415322 1.5563598 4.8679957 5.99003 5.5518985 5.44842 4.237494 2.490808 -0.73844719 -2.6927078 -5.3360152 -4.6725082][-5.9197884 -5.6622353 -4.7983952 -1.9404685 0.35763693 3.7031875 5.5088954 5.8277516 5.8675456 4.3640018 2.5502887 -1.0911841 -3.2941117 -5.1342716 -4.65165][-4.7862058 -4.7232189 -4.2588034 -1.7482419 -0.46243334 2.0972557 3.5571027 4.0681634 3.7125573 2.3896651 1.689817 -1.6519294 -4.2672992 -6.8577247 -7.2605958][-7.8700876 -7.556026 -6.6750393 -4.4640131 -3.4456205 -1.5742021 -1.1897826 -1.3395033 -1.5573263 -2.5356517 -3.0069833 -5.9384861 -7.0855818 -9.219429 -8.8210144][-11.033855 -9.4955969 -8.5722437 -6.1811328 -5.1441917 -3.748245 -4.3841333 -5.59243 -6.0116568 -6.6057811 -6.8426361 -8.8654737 -8.9657583 -10.805035 -10.760702][-10.653131 -9.7654533 -8.7339907 -7.4124393 -6.2424431 -5.4188762 -5.9680352 -6.2418966 -7.8170786 -8.7501688 -8.739336 -9.3788366 -8.95428 -10.250317 -8.8180571][-10.803427 -10.940977 -9.8719616 -8.3699121 -6.9335704 -6.4419074 -6.6076775 -6.93241 -7.8324747 -8.0032377 -7.9031057 -7.2214966 -6.5358834 -7.2121921 -6.3723364][-8.6923866 -9.1648693 -8.7417212 -7.9660378 -6.9426861 -7.0851197 -7.2459655 -7.2176208 -7.3049593 -6.8298645 -7.1253967 -7.2194753 -6.4733276 -6.1245375 -5.4386644]]...]
INFO - root - 2017-12-16 02:08:32.387184: step 85810, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 46h:58m:12s remains)
INFO - root - 2017-12-16 02:08:39.130079: step 85820, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 46h:11m:01s remains)
INFO - root - 2017-12-16 02:08:45.819729: step 85830, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 45h:58m:37s remains)
INFO - root - 2017-12-16 02:08:52.474745: step 85840, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 45h:38m:56s remains)
INFO - root - 2017-12-16 02:08:59.132177: step 85850, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 46h:05m:20s remains)
INFO - root - 2017-12-16 02:09:05.739716: step 85860, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 45h:10m:19s remains)
INFO - root - 2017-12-16 02:09:12.361576: step 85870, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 44h:02m:29s remains)
INFO - root - 2017-12-16 02:09:18.976008: step 85880, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 46h:00m:02s remains)
INFO - root - 2017-12-16 02:09:25.676143: step 85890, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 45h:40m:26s remains)
INFO - root - 2017-12-16 02:09:32.285384: step 85900, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 45h:25m:07s remains)
2017-12-16 02:09:32.810708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5113845 -5.4720497 -6.4877319 -5.5542164 -5.4285526 -5.0152869 -5.4191623 -6.3348517 -8.12187 -9.7048283 -9.0783768 -9.61902 -10.277063 -10.235613 -10.478394][-5.6946964 -6.09398 -6.646091 -6.2027125 -5.8382893 -6.1158528 -6.9364758 -6.9342031 -7.5935869 -8.1209068 -8.2121143 -8.8447847 -9.54877 -9.8658094 -9.8239021][-4.1296535 -6.182219 -7.2592835 -7.107233 -7.0434856 -6.5513082 -7.0963659 -7.4055262 -8.57381 -9.1931572 -7.9566889 -7.9547796 -8.6220856 -9.480689 -9.9221153][-4.4344459 -6.9777555 -7.1754179 -6.7731881 -7.6937175 -6.3806024 -5.9887753 -7.7361207 -8.2869759 -8.258769 -7.5781474 -7.02241 -7.9595947 -9.6022224 -10.18775][-5.0327206 -7.1454649 -7.8700938 -7.2078457 -6.5096507 -4.6452155 -3.2066858 -4.0548291 -6.5863214 -6.9443183 -6.1501493 -6.721797 -7.6637044 -8.6957216 -10.697044][-7.8071036 -7.4520464 -7.3378472 -6.4327917 -4.1696234 -2.7229636 -0.91816282 -0.22943687 -0.77730179 -4.0396619 -5.3491969 -5.3350372 -7.0701585 -9.1391773 -10.781526][-8.0540686 -7.5296559 -6.4243083 -4.7330208 -0.9493432 1.363677 3.1759038 3.7083249 3.0456386 1.4258437 -1.7427857 -4.3252277 -5.6854959 -8.4193306 -10.757608][-7.5821195 -7.1872444 -5.6001024 -2.5515568 0.04100132 3.7461848 5.878283 6.1997304 4.8576074 3.7859902 1.6452837 -2.0468874 -5.3610663 -7.6166797 -10.37604][-7.2015128 -7.1245208 -5.5325341 -2.4258666 -1.0941229 1.1887822 4.1952825 4.2528214 2.5048823 0.56670189 0.60850382 -1.4100494 -4.6819487 -7.0562897 -9.0328026][-7.6313548 -7.6520529 -6.2890005 -4.0039749 -1.8165188 -0.0034518242 0.41087055 -0.089218616 -0.76054859 -2.0055063 -3.2255805 -3.4899831 -4.6579142 -7.0595994 -8.300457][-11.846687 -10.749722 -9.4085884 -7.9546809 -6.0675993 -3.9611351 -4.1344051 -5.1337757 -5.9954286 -6.0191545 -5.7730894 -7.1309628 -8.0194063 -7.9598207 -8.4897995][-14.284353 -12.834397 -10.776672 -9.1854467 -6.7188311 -5.5439854 -5.6842179 -7.1073966 -8.3335218 -8.982089 -7.6644773 -7.3712397 -9.2744789 -9.1787558 -8.9620428][-13.453083 -12.31139 -9.2561827 -7.029109 -5.7053719 -3.8514404 -3.8920927 -6.4724412 -7.0841403 -7.52637 -6.9683113 -5.9926744 -6.2222729 -6.9728513 -8.3722582][-11.992094 -11.158839 -7.8778825 -3.9055066 -2.5602405 -3.3925068 -4.7677965 -4.9564061 -5.8688087 -6.5922918 -5.3886213 -5.0589695 -4.1260319 -3.1088939 -4.7179685][-11.308909 -10.399639 -7.6778336 -4.7084317 -4.1102934 -4.332521 -5.4927564 -6.4894319 -7.0333662 -6.2366166 -4.5954814 -4.9345236 -6.2984409 -4.9764986 -5.3436375]]...]
INFO - root - 2017-12-16 02:09:39.348419: step 85910, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.659 sec/batch; 45h:08m:49s remains)
INFO - root - 2017-12-16 02:09:45.898097: step 85920, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.664 sec/batch; 45h:28m:39s remains)
INFO - root - 2017-12-16 02:09:52.605272: step 85930, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 45h:56m:54s remains)
INFO - root - 2017-12-16 02:09:59.151298: step 85940, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 44h:23m:15s remains)
INFO - root - 2017-12-16 02:10:05.763929: step 85950, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 44h:29m:52s remains)
INFO - root - 2017-12-16 02:10:12.377610: step 85960, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.672 sec/batch; 45h:59m:21s remains)
INFO - root - 2017-12-16 02:10:19.057077: step 85970, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.681 sec/batch; 46h:38m:19s remains)
INFO - root - 2017-12-16 02:10:25.713813: step 85980, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 45h:54m:04s remains)
INFO - root - 2017-12-16 02:10:32.359909: step 85990, loss = 0.21, batch loss = 0.16 (11.8 examples/sec; 0.678 sec/batch; 46h:24m:57s remains)
INFO - root - 2017-12-16 02:10:38.935154: step 86000, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.657 sec/batch; 44h:58m:38s remains)
2017-12-16 02:10:39.506704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5156221 -4.9598551 -5.369606 -5.0324578 -5.2447553 -6.1424866 -6.10647 -5.5517344 -5.6211472 -5.2607179 -4.0969772 -3.0707083 -2.9694924 -3.129787 -2.0980308][-4.0259943 -4.2883525 -5.078692 -4.9913058 -5.2027173 -5.7003436 -5.4926829 -5.1284461 -4.6782355 -3.7837729 -2.4366887 -0.87273455 -0.24279165 -1.2944107 -1.707716][-4.3488455 -4.6857762 -5.503932 -4.8568611 -5.6448808 -6.0423007 -5.32909 -4.3194833 -4.1673074 -4.0172987 -2.9292667 -1.1767225 -0.019160271 -0.34168243 -0.34375048][-6.8594875 -6.7523136 -6.7512279 -6.63472 -7.2256851 -5.6419344 -4.3317051 -3.5024238 -3.2052567 -3.4309998 -3.2146614 -2.2593927 -1.7919869 -1.360466 -0.40330935][-7.8800659 -8.9301825 -9.49011 -8.1784945 -6.7514563 -4.0984893 -2.4367385 -1.383647 -0.91237068 -1.4712076 -2.3536541 -2.4192343 -2.4001958 -3.0265169 -2.1224387][-7.9566479 -8.4562283 -8.5101271 -7.1571956 -5.1677608 -0.30123138 3.1365266 3.8725514 3.2587857 0.51241732 -1.2919636 -1.6316328 -2.5225029 -3.7545266 -3.1298463][-8.0881157 -7.9204111 -6.8723636 -4.9836073 -2.9276447 1.9422383 6.4951186 8.3605213 7.6216235 3.3203406 -0.068510056 -1.9926736 -3.4138126 -4.269825 -3.8256164][-8.34453 -8.5703669 -8.1527605 -4.2855282 -0.62025213 3.9848676 7.5305238 9.1010761 9.0868149 5.09885 0.5742116 -2.2786551 -3.7872639 -5.0653925 -4.6130013][-10.062668 -9.26943 -8.2582989 -6.0884757 -3.5736268 1.4341755 5.05341 6.2818646 5.5778861 2.71621 -0.23579788 -3.6826425 -5.835361 -6.3991327 -5.1181569][-9.7593441 -9.523344 -9.0873251 -6.1891909 -5.5448413 -1.9687064 1.952024 3.0702643 1.7275867 -1.4171095 -4.5823865 -6.2134938 -7.5537682 -8.4135752 -7.6426129][-13.098696 -12.757942 -11.704811 -9.1832495 -8.2268066 -7.2476792 -6.2914262 -4.9670367 -4.2811804 -5.1723971 -7.6152277 -9.6272545 -10.246206 -9.97302 -8.2391758][-14.262777 -13.592013 -11.797602 -9.48903 -8.4476891 -7.868176 -7.6788831 -7.7510309 -7.826787 -8.3936768 -9.4372034 -10.101131 -9.9008265 -10.152738 -8.8258314][-13.738089 -12.720634 -11.367647 -8.5562878 -7.6074152 -6.2464385 -6.0639839 -7.0940008 -7.2920728 -6.6254396 -5.8229351 -6.543016 -7.019125 -7.6734962 -7.3452621][-11.465765 -10.664581 -9.3559666 -8.3010635 -7.4990134 -6.2400203 -6.1207013 -5.8367848 -5.2223949 -5.4977069 -5.1995525 -4.3659091 -3.8553991 -4.7397728 -5.2606611][-8.30242 -7.5783067 -6.6273322 -5.7219648 -5.4343076 -5.5655479 -5.1268477 -4.1852303 -4.122704 -4.4615951 -4.3890338 -4.3133307 -4.473835 -5.3506036 -6.5203319]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 02:10:46.113225: step 86010, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 44h:50m:28s remains)
INFO - root - 2017-12-16 02:10:52.766305: step 86020, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.681 sec/batch; 46h:36m:00s remains)
INFO - root - 2017-12-16 02:10:59.418314: step 86030, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 44h:30m:42s remains)
INFO - root - 2017-12-16 02:11:06.087197: step 86040, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 45h:01m:23s remains)
INFO - root - 2017-12-16 02:11:12.661339: step 86050, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 45h:00m:45s remains)
INFO - root - 2017-12-16 02:11:19.224131: step 86060, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.648 sec/batch; 44h:20m:19s remains)
INFO - root - 2017-12-16 02:11:25.849923: step 86070, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 45h:59m:58s remains)
INFO - root - 2017-12-16 02:11:32.458006: step 86080, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 45h:11m:43s remains)
INFO - root - 2017-12-16 02:11:39.063790: step 86090, loss = 0.17, batch loss = 0.13 (11.7 examples/sec; 0.682 sec/batch; 46h:38m:52s remains)
INFO - root - 2017-12-16 02:11:45.594655: step 86100, loss = 0.15, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 44h:00m:27s remains)
2017-12-16 02:11:46.116907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0479257 -3.3869967 -3.6144297 -3.0683255 -3.5379732 -4.4337311 -5.2015657 -6.0765095 -6.5482297 -6.2913852 -6.1294184 -5.00858 -5.1101446 -7.6980705 -8.8231163][-6.0202188 -5.20472 -4.754396 -4.9708214 -5.4744411 -6.5635324 -7.4893508 -7.705431 -7.5521832 -7.1425118 -6.8091893 -6.3634729 -7.9921932 -8.7954617 -7.835309][-5.8381553 -6.6176395 -6.6388717 -6.22455 -6.9293337 -6.4564066 -6.528841 -7.1232996 -7.3755836 -7.3636217 -7.1624813 -5.5062618 -5.5299745 -6.63774 -6.5761552][-3.8840094 -4.450521 -5.3808913 -6.1714869 -7.5258527 -7.6511488 -7.9608135 -8.3252058 -7.6079087 -6.1414008 -5.780807 -5.5121441 -6.1488857 -6.7335806 -6.794838][-4.8948064 -5.1254435 -5.9020185 -6.7575879 -6.8665686 -5.2367029 -4.42903 -4.6241336 -4.9551358 -4.3469415 -4.4065132 -3.86053 -4.8470955 -6.5401087 -7.629396][-6.0134711 -4.3219557 -3.0688152 -2.6651571 -2.7125585 -0.977654 1.326755 1.4307199 0.99375057 -0.6807847 -1.8204808 -1.8442342 -3.5637546 -5.8756838 -7.4990382][-5.2827096 -5.9574986 -4.8155179 -3.430527 -1.9309206 0.04442358 3.1892438 5.7469745 6.8140483 3.6976361 0.44452095 -1.4285789 -4.1022754 -6.491992 -7.1695776][-5.4764681 -6.6846261 -5.7516885 -3.5120325 -1.8134584 2.2245121 5.0683494 6.8847671 7.2186151 4.596683 1.2890697 -1.9356763 -5.666389 -7.4070587 -8.1465864][-6.510797 -7.3314285 -5.9031916 -4.0618367 -3.1218548 0.22801256 3.3466086 5.1678348 5.9081578 4.5234733 1.353219 -2.4712732 -5.534265 -7.8628058 -8.3707438][-5.8707724 -6.4235134 -5.3708949 -4.3920965 -3.139245 -0.34350395 2.0306997 3.7197928 3.9607434 1.1974049 -2.1448252 -3.5399418 -4.92801 -7.8008771 -10.605637][-7.4082804 -7.4372587 -5.838316 -5.1611447 -4.2422552 -3.715667 -3.6765833 -2.147053 -1.195961 -2.6392045 -5.26757 -5.7762856 -7.5709314 -7.657896 -6.9870186][-9.0979176 -9.1168375 -8.5371675 -7.7482805 -6.2575541 -6.3430953 -6.1747541 -5.9451723 -6.1245527 -7.1771011 -8.5247526 -8.3737221 -8.1013269 -8.267067 -8.1234093][-9.5757284 -9.4947329 -9.191515 -7.8974924 -7.8195562 -7.1756425 -7.5823727 -8.5303993 -9.1564283 -9.5404291 -9.8796806 -10.15379 -10.416394 -8.6711645 -7.4995375][-6.2018051 -7.0495224 -7.6030288 -7.6231194 -7.1237984 -7.2256646 -7.7527328 -8.2816668 -8.0290117 -9.3568974 -10.506569 -8.638238 -7.3001919 -7.3224883 -9.0093746][-6.3921227 -8.0195761 -9.2005272 -8.7541 -7.1397858 -6.7025347 -6.4422827 -6.1402664 -6.0862503 -6.3725023 -7.4067836 -7.7652273 -7.8739395 -8.3292217 -9.2571812]]...]
INFO - root - 2017-12-16 02:11:52.641703: step 86110, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 45h:11m:54s remains)
INFO - root - 2017-12-16 02:11:59.245096: step 86120, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.660 sec/batch; 45h:11m:04s remains)
INFO - root - 2017-12-16 02:12:05.764017: step 86130, loss = 0.16, batch loss = 0.12 (11.8 examples/sec; 0.675 sec/batch; 46h:13m:12s remains)
INFO - root - 2017-12-16 02:12:12.432064: step 86140, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.675 sec/batch; 46h:12m:39s remains)
INFO - root - 2017-12-16 02:12:19.034069: step 86150, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 45h:02m:10s remains)
INFO - root - 2017-12-16 02:12:25.630344: step 86160, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 43h:40m:15s remains)
INFO - root - 2017-12-16 02:12:32.155511: step 86170, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 44h:50m:28s remains)
INFO - root - 2017-12-16 02:12:38.724851: step 86180, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 45h:44m:52s remains)
INFO - root - 2017-12-16 02:12:45.316062: step 86190, loss = 0.20, batch loss = 0.15 (12.2 examples/sec; 0.658 sec/batch; 44h:59m:39s remains)
INFO - root - 2017-12-16 02:12:51.934401: step 86200, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 46h:01m:35s remains)
2017-12-16 02:12:52.513844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1312766 -6.913599 -6.5857315 -4.8932557 -4.4357548 -4.130991 -4.1957331 -4.1442242 -4.2268715 -4.8365231 -5.2113972 -7.8483047 -10.099039 -9.4359121 -9.5218868][-5.6362238 -4.8332777 -3.9816298 -3.4092839 -4.1163049 -4.1652112 -4.3131671 -4.112937 -4.0694351 -4.257092 -4.5462613 -6.4602766 -8.6502743 -8.3288345 -8.6295023][-3.3285604 -3.524138 -3.4194047 -3.000515 -3.3612328 -3.5904076 -3.5394018 -3.3961561 -3.355607 -3.9050355 -3.8813162 -5.21591 -6.8517771 -6.0019135 -6.6136484][-3.2178044 -3.1230438 -3.2346692 -2.6636195 -3.3230853 -2.8159075 -2.5314417 -2.2279568 -2.0740273 -2.18755 -2.5483327 -4.9464135 -7.1957283 -6.4730787 -6.8609738][-3.227905 -4.3228216 -4.0042195 -2.6474316 -2.343821 -2.3026307 -2.3110826 -2.3828478 -2.0382078 -2.0202737 -2.0150819 -4.1214089 -6.8009424 -6.710834 -7.1839609][-5.0704746 -5.0578184 -4.06286 -2.6843054 -2.3122814 -1.3199806 -0.95389032 -0.31696367 -0.31129265 -0.10804844 -0.18794394 -2.7832434 -5.7048893 -5.7869568 -7.0875034][-5.5886345 -5.9513621 -4.5247145 -2.6214983 -1.3505764 -0.22328472 -0.33756304 -0.17605066 0.25178146 0.30294371 0.25615311 -2.3656659 -5.5204973 -5.4523044 -6.7769971][-5.6787724 -6.1662459 -5.0924683 -3.192291 -1.9000154 -0.43655825 -0.59329844 -0.22508764 0.14826059 0.74069977 0.41142893 -2.6877663 -5.6573734 -5.9355125 -7.4283066][-5.815567 -5.4630332 -4.325614 -1.8170969 -0.35510492 0.37031364 -0.177526 -0.60476351 -0.25946951 0.8402853 0.756093 -1.5894938 -4.0291882 -4.5380149 -5.6982775][-4.8516364 -4.7809186 -3.4339089 -2.0115988 -1.1630964 -0.26740646 -0.86954832 -0.76848507 -0.53939009 0.17179441 0.83958721 -1.4277468 -3.6726658 -4.2134757 -5.8665848][-9.3467083 -8.1997309 -6.1734471 -4.095191 -3.1172502 -2.6304934 -2.906682 -2.4538128 -2.183531 -2.0520716 -2.4787395 -4.5190725 -5.7649136 -5.3519406 -5.9767556][-11.155725 -9.9150486 -7.9576006 -5.3402505 -4.1902018 -4.32737 -5.2114167 -5.1709375 -4.5503278 -4.7604003 -5.0037003 -5.762383 -6.0360317 -5.5964146 -5.8034987][-10.112081 -8.9559441 -7.2341266 -5.7176733 -4.7130661 -4.4305844 -4.9353113 -5.087729 -4.5919456 -4.1692615 -4.7743912 -5.27096 -5.8069973 -5.0110669 -5.1499205][-9.0544167 -7.7409663 -7.0011406 -5.4942832 -3.7857029 -3.75812 -3.9500155 -3.2103813 -3.2546077 -3.8692095 -3.1205759 -3.3711667 -4.381258 -4.3659039 -4.8424473][-7.5133657 -7.3915377 -7.1637211 -5.2656322 -3.6231496 -3.0284507 -2.8217533 -2.6899049 -2.9006052 -2.6376355 -3.2028308 -4.2686715 -5.0255194 -5.4878573 -6.1339455]]...]
INFO - root - 2017-12-16 02:12:59.150303: step 86210, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.667 sec/batch; 45h:38m:54s remains)
INFO - root - 2017-12-16 02:13:05.723902: step 86220, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 46h:03m:42s remains)
INFO - root - 2017-12-16 02:13:12.366833: step 86230, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 44h:24m:27s remains)
INFO - root - 2017-12-16 02:13:19.024149: step 86240, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 44h:53m:55s remains)
INFO - root - 2017-12-16 02:13:25.650901: step 86250, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 45h:04m:20s remains)
INFO - root - 2017-12-16 02:13:32.160838: step 86260, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.674 sec/batch; 46h:06m:20s remains)
INFO - root - 2017-12-16 02:13:38.709963: step 86270, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.639 sec/batch; 43h:43m:04s remains)
INFO - root - 2017-12-16 02:13:45.396631: step 86280, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 44h:41m:08s remains)
INFO - root - 2017-12-16 02:13:52.087916: step 86290, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.688 sec/batch; 47h:04m:48s remains)
INFO - root - 2017-12-16 02:13:58.726990: step 86300, loss = 0.20, batch loss = 0.16 (11.8 examples/sec; 0.677 sec/batch; 46h:17m:08s remains)
2017-12-16 02:13:59.288439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.925849 -7.329731 -7.5632663 -6.7646074 -6.7215052 -6.6932564 -6.4115052 -6.4482727 -6.6093755 -6.7450056 -6.4559479 -7.683207 -9.5825634 -10.688068 -11.848618][-6.2687941 -8.03059 -7.1728911 -5.7186627 -5.36468 -5.6109319 -5.7487226 -6.6661806 -6.6602654 -6.2536917 -5.5012016 -6.2320366 -7.0154786 -10.964622 -12.102547][-2.2936523 -4.9653063 -7.0648513 -6.4291224 -5.3460312 -5.486794 -5.7172561 -6.5487423 -7.1591015 -6.2981377 -5.5505023 -5.4072533 -5.5710554 -9.2079191 -13.014446][-7.2727404 -8.4900656 -8.4966421 -8.2714672 -7.5981884 -6.5092998 -6.3065591 -7.9928331 -8.9524393 -6.9517965 -5.3571796 -5.5558496 -6.2404718 -10.906966 -13.521944][-6.102643 -8.7466269 -9.095459 -7.4171114 -5.9730287 -2.8353231 -1.2062464 -3.5613923 -6.2125258 -6.5399256 -5.5779428 -5.8079329 -6.0941391 -10.898682 -14.386005][-6.9743619 -8.0940456 -7.7010632 -5.4978166 -3.5529637 0.87582016 3.6663003 1.5431409 -0.19153786 -2.7077453 -5.3731856 -5.1457057 -4.3890448 -7.7698369 -12.533781][-8.6787767 -9.1533022 -6.1914082 -2.954931 -1.4086318 2.5160394 6.2158971 6.996388 5.1081567 -1.7112455 -7.1452165 -8.7035141 -8.2183714 -9.3221569 -12.258858][-7.1285491 -8.1924849 -7.687376 -4.0863681 0.33205795 4.0454249 6.58122 5.9316773 5.257443 0.7847476 -4.4302335 -7.7009258 -9.6546116 -11.526138 -12.760836][-7.2632036 -6.8510714 -6.0446243 -5.1358843 -2.2438068 1.2809792 3.7670703 2.7538753 1.20115 -2.2066097 -5.8914032 -9.368948 -11.258651 -12.964552 -14.021433][-6.8849754 -6.5299244 -5.8415527 -4.8045864 -5.5571513 -3.7110233 -0.59287167 0.39216423 -1.6257048 -5.4994359 -9.1509838 -11.267061 -11.874133 -13.228296 -15.595594][-10.433426 -10.083302 -10.942068 -10.1336 -9.7456579 -8.6768274 -7.5576544 -6.5164967 -6.3543062 -7.491838 -10.515648 -13.807346 -15.268831 -14.844063 -14.172821][-16.047474 -16.294327 -15.757059 -15.081257 -14.696768 -13.59907 -13.323313 -13.31633 -12.459518 -12.438162 -13.028479 -13.278067 -12.070129 -13.305756 -13.723423][-15.975874 -14.527637 -15.169746 -14.590117 -13.642467 -12.583316 -11.584858 -11.370426 -10.971712 -11.096985 -11.82356 -11.692566 -11.473861 -10.759058 -11.002825][-12.766995 -12.43714 -12.69305 -11.586507 -9.7265377 -10.009918 -10.497306 -9.7317142 -9.9078064 -10.005234 -10.098976 -9.2191219 -8.0966 -8.2220039 -8.1813431][-10.370903 -9.3221378 -7.8985052 -6.199522 -6.0584555 -6.1026583 -5.8793583 -6.4679594 -6.2748022 -6.2322292 -7.2330084 -8.42222 -9.1298285 -9.40865 -10.240509]]...]
INFO - root - 2017-12-16 02:14:05.968985: step 86310, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 43h:55m:21s remains)
INFO - root - 2017-12-16 02:14:12.543192: step 86320, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 43h:59m:40s remains)
INFO - root - 2017-12-16 02:14:19.211754: step 86330, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 45h:27m:44s remains)
INFO - root - 2017-12-16 02:14:25.784642: step 86340, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 45h:03m:24s remains)
INFO - root - 2017-12-16 02:14:32.416017: step 86350, loss = 0.15, batch loss = 0.10 (11.7 examples/sec; 0.681 sec/batch; 46h:35m:30s remains)
INFO - root - 2017-12-16 02:14:39.012405: step 86360, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.664 sec/batch; 45h:23m:48s remains)
INFO - root - 2017-12-16 02:14:45.603393: step 86370, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 45h:08m:49s remains)
INFO - root - 2017-12-16 02:14:52.090418: step 86380, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 44h:05m:03s remains)
INFO - root - 2017-12-16 02:14:58.704172: step 86390, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 43h:50m:04s remains)
INFO - root - 2017-12-16 02:15:05.275966: step 86400, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 45h:56m:38s remains)
2017-12-16 02:15:05.809522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1093879 -4.4536438 -4.3585162 -3.31339 -3.6298778 -2.8679583 -2.2643166 -2.5536594 -3.5024426 -3.0973969 -2.7616858 -3.7891212 -4.0064855 -2.8242974 -2.5078073][-4.6987319 -4.3972468 -3.5516775 -2.2234745 -2.6214206 -2.5683112 -2.8197434 -3.2761829 -3.1607428 -1.7815232 -1.503408 -3.0746124 -3.1943874 -2.1364145 -2.3507156][-3.6729581 -3.2792284 -2.3333285 -1.7888052 -2.4170671 -2.2239215 -2.3765163 -2.6647253 -2.3846395 -2.4449804 -1.6456923 -1.556788 -1.7497642 -2.01301 -2.2048068][-2.9568176 -2.2953751 -1.7743993 -1.1430378 -1.9370768 -2.1491907 -2.1433089 -2.2037354 -1.5998302 -0.88977814 -0.26247931 -1.3839283 -2.2695239 -1.3690553 -1.3124561][-3.0652006 -2.9890165 -3.1650491 -2.0724883 -1.4817944 -1.1408086 -0.90215683 0.081859589 0.14652967 -0.28895092 0.21649933 -1.217052 -2.4115608 -1.9273753 -1.7630005][-4.2693396 -3.2826676 -2.490128 -2.2250593 -1.5011754 0.47785854 1.3040829 1.4662929 1.6114302 1.3489709 1.2077131 -0.60940838 -2.3335173 -2.3699465 -1.7961588][-4.7013049 -4.1573696 -3.0175874 -0.60499287 0.72715712 1.825933 2.6743646 2.9554095 2.122643 0.88712311 0.34322309 -1.5694127 -2.9496057 -2.5313087 -2.5345423][-5.0238504 -3.6068552 -2.3563056 0.6626091 1.6150875 2.0925055 2.7690749 2.5611081 2.3663116 1.6499133 0.75892591 -1.3507681 -3.1754172 -2.8483613 -2.451544][-4.4176292 -4.03177 -2.2928195 1.1405396 2.4067521 1.6413112 1.8759098 2.2006311 1.485815 0.89962339 0.016852379 -2.8104286 -4.33873 -3.5782392 -2.8947744][-4.7094383 -4.0712104 -2.5443423 0.62050438 1.2346344 1.1091928 1.0891242 1.1645966 1.0175509 0.11843204 -0.7889986 -3.5135329 -5.8520169 -5.7626476 -4.6666861][-6.4305997 -6.0467863 -4.19373 -1.5845795 -0.70044994 0.57163906 0.95615864 -0.51810408 -1.5820761 -1.6566191 -2.073719 -5.2887764 -7.0765672 -6.1443834 -6.0723677][-8.3804932 -8.5908251 -6.5354462 -4.2126956 -3.3149211 -2.2374175 -1.6967807 -2.3958211 -3.3214915 -4.0353866 -4.8539972 -6.1397443 -7.1412492 -6.4885116 -5.630805][-7.3351884 -6.5375628 -4.7923107 -4.15327 -3.6710658 -3.0453241 -2.5785134 -3.0470891 -3.9086289 -4.69907 -5.6877265 -5.9697814 -5.5003333 -4.58556 -4.6399889][-6.0534086 -4.8597293 -3.3980238 -2.6459055 -2.5355601 -2.22926 -2.4782157 -2.9908626 -2.8957884 -3.2236834 -3.7697315 -3.3975096 -3.1550851 -2.0097005 -1.7165971][-4.8655114 -3.9188328 -3.4267116 -3.4827211 -2.5601628 -2.5890899 -3.514554 -3.1752763 -2.7083166 -2.9623606 -3.3735101 -2.9546957 -2.9090652 -3.68472 -4.1476]]...]
INFO - root - 2017-12-16 02:15:12.421742: step 86410, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 45h:03m:14s remains)
INFO - root - 2017-12-16 02:15:18.954660: step 86420, loss = 0.17, batch loss = 0.12 (12.7 examples/sec; 0.632 sec/batch; 43h:11m:17s remains)
INFO - root - 2017-12-16 02:15:25.476512: step 86430, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 45h:11m:40s remains)
INFO - root - 2017-12-16 02:15:31.991923: step 86440, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.642 sec/batch; 43h:54m:39s remains)
INFO - root - 2017-12-16 02:15:38.602985: step 86450, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 45h:44m:00s remains)
INFO - root - 2017-12-16 02:15:45.108984: step 86460, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 44h:35m:05s remains)
INFO - root - 2017-12-16 02:15:51.598709: step 86470, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 43h:55m:14s remains)
INFO - root - 2017-12-16 02:15:58.209493: step 86480, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.673 sec/batch; 46h:01m:04s remains)
INFO - root - 2017-12-16 02:16:04.870098: step 86490, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 45h:02m:30s remains)
INFO - root - 2017-12-16 02:16:11.431012: step 86500, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.633 sec/batch; 43h:13m:59s remains)
2017-12-16 02:16:11.999566: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.353344 -6.2730927 -5.8676519 -4.7193303 -5.1880574 -4.8632851 -4.2909293 -3.1748142 -2.0542283 -1.4982438 -1.6007586 -4.6213813 -7.63886 -8.6637077 -8.2046051][-4.7523713 -4.8517728 -4.4916506 -4.8001652 -5.4221029 -5.1201653 -4.2936554 -2.9722147 -2.0447748 -1.3364048 -1.7706041 -4.1160603 -6.4538269 -7.7726641 -8.1159744][-3.8128619 -3.72873 -4.0128627 -3.8821914 -4.8328543 -4.6927032 -4.1695657 -3.1529632 -1.9933436 -1.3789849 -2.2517996 -5.2986956 -8.3287106 -9.578414 -9.8160877][-3.6552994 -4.1259451 -4.0428638 -4.119216 -4.8707905 -4.8657694 -4.7298579 -4.3134804 -3.6269007 -2.6098216 -2.4246042 -5.3598418 -8.6994343 -10.249369 -10.388326][-3.2343235 -3.9163518 -4.0074635 -3.4331729 -3.0992548 -3.0756273 -3.0224135 -3.663681 -4.1819243 -3.8415036 -3.6524408 -6.1305952 -9.25728 -10.821787 -11.089271][-4.085289 -3.182112 -2.491636 -1.2245054 -0.52852249 -0.7126708 -0.54311562 -0.54203939 -1.2895513 -1.932158 -2.9490778 -5.4073048 -7.8732123 -9.3396683 -10.371931][-4.3431892 -3.7423713 -2.5885296 -0.49144888 0.83258629 1.5653157 2.7780013 2.4003291 1.4151587 -0.19329071 -2.3333309 -5.3336396 -7.1787109 -8.3013525 -9.7011261][-4.5130453 -3.1839941 -1.5434389 -0.22171068 0.68909216 2.6229138 4.5160604 4.0182195 3.2220225 1.423429 -0.519361 -3.6803186 -6.4986267 -8.1086044 -8.814785][-3.7358241 -2.9974351 -1.598659 -0.29154491 -0.11802816 1.1937466 2.3308091 2.6554856 2.4576125 1.8020897 0.64574051 -3.0063243 -6.1534281 -7.2515912 -6.9860134][-2.2671485 -2.4769125 -2.0979178 -0.37848091 0.39702988 -0.24805689 -0.22000837 1.0114045 2.152854 1.7815814 0.64634848 -2.2831852 -5.5903678 -7.4569826 -7.5488434][-6.4621778 -6.1713624 -5.4035535 -4.1802645 -4.0620656 -3.4446111 -3.3872514 -3.3870549 -2.6453934 -2.28161 -1.9770546 -4.0133066 -5.7263961 -6.6747246 -6.9624505][-10.07838 -10.154164 -9.2591763 -8.1088963 -7.66528 -6.8607268 -7.0979524 -7.0725784 -6.7703247 -6.2722178 -5.7730408 -6.2195239 -6.3196135 -7.1656375 -7.3879514][-12.38409 -11.46997 -9.4895916 -8.9527569 -8.7996283 -8.8618031 -8.7418671 -8.4641666 -8.9088821 -9.1952267 -8.6371212 -8.1522055 -7.5297546 -6.5086608 -5.2353554][-10.679317 -10.284771 -10.174324 -8.4975233 -6.3080268 -6.4865265 -7.6025534 -7.9558625 -8.4752178 -8.9358969 -9.35413 -8.8518314 -8.212925 -7.8706851 -6.6639104][-7.39631 -7.2120953 -6.4297328 -5.5464745 -4.7379541 -4.4854221 -4.7119408 -5.9535174 -7.2763662 -8.0966473 -8.4276924 -9.2294731 -9.5304489 -8.9956913 -7.9161692]]...]
INFO - root - 2017-12-16 02:16:18.605846: step 86510, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 44h:06m:27s remains)
INFO - root - 2017-12-16 02:16:25.163958: step 86520, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 44h:57m:20s remains)
INFO - root - 2017-12-16 02:16:31.714765: step 86530, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 44h:55m:24s remains)
INFO - root - 2017-12-16 02:16:38.368132: step 86540, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 45h:02m:56s remains)
INFO - root - 2017-12-16 02:16:44.965051: step 86550, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.684 sec/batch; 46h:44m:07s remains)
INFO - root - 2017-12-16 02:16:51.601086: step 86560, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 45h:48m:16s remains)
INFO - root - 2017-12-16 02:16:58.168831: step 86570, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.644 sec/batch; 43h:58m:44s remains)
INFO - root - 2017-12-16 02:17:04.761127: step 86580, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 45h:21m:45s remains)
INFO - root - 2017-12-16 02:17:11.305561: step 86590, loss = 0.13, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 43h:51m:49s remains)
INFO - root - 2017-12-16 02:17:17.908365: step 86600, loss = 0.18, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 45h:54m:30s remains)
2017-12-16 02:17:18.453371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7584968 -5.0427918 -3.8259232 -3.0251679 -4.9368668 -5.638721 -5.3939872 -4.7536259 -3.9744558 -3.4565971 -3.3334227 -5.6085086 -8.037941 -8.0642939 -6.7653103][-7.0856948 -6.7641592 -5.3545051 -4.3650627 -5.4784074 -5.5995469 -5.5910997 -5.3064957 -4.764822 -4.2123122 -4.0871735 -6.6543221 -9.0202875 -9.1073513 -8.0733423][-6.7605729 -7.5523658 -6.6197963 -4.8783283 -4.9900374 -5.1062288 -5.3582463 -5.0832629 -4.9371343 -4.7849436 -4.8394413 -7.4599376 -10.035972 -9.1753922 -7.8286843][-5.1263208 -5.6826839 -5.5863919 -4.4978809 -4.4149075 -3.8546119 -3.9408994 -3.9731188 -4.1474323 -4.0276656 -4.3291059 -7.2531433 -9.5692177 -8.9262514 -7.7478638][-4.9054003 -5.42159 -4.4260087 -2.824285 -1.8868897 -0.45629883 -0.43713713 -1.8956504 -4.158391 -3.9602127 -3.3654006 -5.6982207 -7.9720888 -8.2979975 -7.3732653][-7.1397724 -6.5867929 -4.6215553 -2.1343577 -0.83682489 1.4243908 3.6541591 2.540668 -1.4276714 -2.8145225 -3.7691445 -5.0743723 -5.8445296 -6.2794609 -6.2628078][-9.31262 -8.22937 -5.3158817 -1.7003465 0.24416828 2.3179374 4.5129695 5.1495757 3.7513804 0.38536453 -3.4342012 -4.1500363 -4.6738825 -4.8624983 -4.2769852][-10.909904 -9.8498993 -6.3005233 -0.94986296 2.775701 4.9399838 5.2059608 5.1615796 5.6026778 3.645 0.56899548 -1.9782958 -4.5092297 -4.5970311 -3.1275771][-9.4213982 -9.1222429 -6.8892 -2.6168842 1.4336109 5.6318192 6.1053386 4.3445773 3.9841304 3.7203202 2.8144488 -0.72824669 -4.4722838 -4.8654733 -4.1407723][-9.1462975 -8.2404051 -6.4630985 -4.1282415 -0.97935724 3.3007379 4.4352927 2.79954 1.4463167 0.61739922 0.50184107 -2.5767305 -6.636425 -7.0270042 -6.683713][-9.2270584 -8.3891907 -6.7492433 -4.6266608 -2.6557968 -0.35372591 0.53601217 0.82197 -0.20471811 -1.7288043 -1.9384007 -5.1185946 -7.7981129 -7.4499884 -7.1768532][-9.1285391 -8.8527985 -6.9431806 -4.6450911 -4.2749434 -3.7656231 -3.8449316 -2.9142604 -2.8670394 -3.3173528 -2.9290242 -5.4575939 -6.9033222 -7.1324916 -6.6735153][-9.9781609 -9.1704922 -8.3408527 -7.3497705 -5.9847927 -5.4616666 -6.70115 -6.9739628 -7.078351 -6.980268 -5.90706 -6.479322 -7.3151426 -6.9790959 -5.8076463][-8.0881147 -7.5699053 -6.8109913 -6.492362 -4.9781766 -4.0022 -5.0165105 -5.5141439 -6.0743723 -5.9930878 -5.87232 -5.9295387 -4.9008203 -5.533627 -5.5939231][-6.0695896 -5.5558186 -4.0502162 -3.1621282 -1.9267254 -1.4924588 -1.897325 -2.0728357 -2.3230233 -1.8959625 -2.2403216 -3.8283787 -4.2302256 -3.6100171 -3.4640698]]...]
INFO - root - 2017-12-16 02:17:25.058116: step 86610, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 44h:17m:29s remains)
INFO - root - 2017-12-16 02:17:31.556994: step 86620, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.660 sec/batch; 45h:05m:01s remains)
INFO - root - 2017-12-16 02:17:38.104155: step 86630, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 45h:20m:53s remains)
INFO - root - 2017-12-16 02:17:44.626712: step 86640, loss = 0.11, batch loss = 0.07 (12.1 examples/sec; 0.661 sec/batch; 45h:09m:01s remains)
INFO - root - 2017-12-16 02:17:51.121019: step 86650, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.645 sec/batch; 44h:03m:36s remains)
INFO - root - 2017-12-16 02:17:57.687893: step 86660, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.642 sec/batch; 43h:52m:07s remains)
INFO - root - 2017-12-16 02:18:04.310488: step 86670, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.678 sec/batch; 46h:16m:52s remains)
INFO - root - 2017-12-16 02:18:10.907772: step 86680, loss = 0.11, batch loss = 0.06 (12.1 examples/sec; 0.661 sec/batch; 45h:07m:48s remains)
INFO - root - 2017-12-16 02:18:17.507206: step 86690, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 44h:53m:52s remains)
INFO - root - 2017-12-16 02:18:24.136903: step 86700, loss = 0.15, batch loss = 0.11 (11.5 examples/sec; 0.693 sec/batch; 47h:18m:35s remains)
2017-12-16 02:18:24.715914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.137475 -8.3488369 -9.1818714 -10.178687 -11.079823 -10.948256 -11.265131 -10.8234 -10.298771 -9.3611746 -8.3580036 -7.082583 -6.8390222 -6.5118208 -4.0661087][-8.06815 -7.6147003 -7.7274837 -8.8717957 -10.606531 -10.845876 -11.366959 -11.777685 -11.836716 -10.523724 -8.7469921 -6.5669222 -4.9026642 -5.2967157 -4.3364878][-6.24495 -7.3542309 -9.5144882 -9.7971649 -9.919117 -9.0979052 -8.7499065 -9.143 -9.3431959 -8.9712677 -8.2270212 -6.1337276 -4.6819868 -4.8717942 -4.544157][-6.3876925 -7.3113155 -8.8249111 -9.6683674 -10.463562 -8.4047346 -6.7002282 -7.6623569 -8.5414619 -8.5624352 -8.3653479 -7.3612595 -6.8108215 -7.5651169 -6.7442913][-6.4578185 -8.9463921 -10.978973 -9.9666195 -8.7394791 -4.6648283 -1.2615886 -3.0128837 -5.5531225 -6.4645791 -7.5829697 -7.3457623 -7.0123377 -7.7141376 -7.0781541][-7.9028053 -8.6248341 -9.7443638 -8.6695719 -6.92561 -1.5134287 3.6139121 3.8522439 1.8636146 -3.17847 -7.19057 -6.2025642 -5.6565766 -7.2705879 -6.6561351][-9.1326981 -8.0755157 -7.21822 -4.95951 -2.9875946 1.7570624 6.6662345 7.7253785 6.571754 0.20469379 -5.8043313 -6.4969726 -7.0472665 -7.9685383 -6.4207292][-11.020687 -9.89765 -7.7890677 -3.7017732 -0.3206749 3.924212 8.3879108 8.8241081 7.5646815 2.3798614 -3.4524493 -5.7962046 -7.4151039 -8.7222509 -7.1892056][-11.126488 -10.087946 -8.0472775 -4.528234 -2.5083113 0.82244205 4.6940074 5.3052897 4.3033519 -0.12937164 -5.0431223 -7.5430145 -10.001904 -11.007768 -9.2514315][-11.038872 -10.575504 -9.4787064 -6.1723208 -4.4760475 -2.6622899 0.26441622 0.72858524 -0.38685513 -3.3590159 -6.8714828 -8.9083242 -11.373667 -13.772846 -12.647453][-15.429991 -14.892878 -13.110304 -9.3873 -7.3118372 -5.5637264 -4.1056542 -4.4552083 -5.5498524 -8.1057863 -10.753954 -11.938696 -12.563585 -13.620193 -12.546827][-19.304152 -17.786528 -14.991722 -10.899014 -8.2912979 -6.3190913 -5.5166106 -5.7120194 -6.4450407 -8.8928728 -11.663845 -12.836658 -12.841349 -12.858831 -11.542288][-18.431007 -17.418928 -14.84864 -11.114176 -8.6427622 -6.6679521 -5.6020346 -6.1269822 -6.9763803 -7.9915485 -9.6579428 -11.458409 -12.093752 -10.76017 -8.8302441][-15.410353 -15.193932 -12.426533 -9.1682167 -6.9151139 -6.791276 -6.3961906 -6.0565529 -6.0312085 -6.6289225 -7.5121922 -7.5178666 -8.2565784 -8.3739824 -7.5137076][-10.950996 -10.504709 -9.4663725 -7.3654213 -5.9749479 -5.262114 -5.4114571 -5.8511539 -5.9574013 -6.1888218 -6.5735736 -7.0058584 -6.9393768 -6.9345679 -6.8279724]]...]
INFO - root - 2017-12-16 02:18:31.255283: step 86710, loss = 0.28, batch loss = 0.24 (12.1 examples/sec; 0.661 sec/batch; 45h:07m:41s remains)
INFO - root - 2017-12-16 02:18:37.768299: step 86720, loss = 0.12, batch loss = 0.07 (12.4 examples/sec; 0.646 sec/batch; 44h:05m:50s remains)
INFO - root - 2017-12-16 02:18:44.321849: step 86730, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 44h:14m:55s remains)
INFO - root - 2017-12-16 02:18:50.870341: step 86740, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 45h:41m:18s remains)
INFO - root - 2017-12-16 02:18:57.442995: step 86750, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.652 sec/batch; 44h:30m:32s remains)
INFO - root - 2017-12-16 02:19:04.104690: step 86760, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.662 sec/batch; 45h:12m:47s remains)
INFO - root - 2017-12-16 02:19:10.803506: step 86770, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.686 sec/batch; 46h:47m:58s remains)
INFO - root - 2017-12-16 02:19:17.367553: step 86780, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 44h:24m:41s remains)
INFO - root - 2017-12-16 02:19:23.925297: step 86790, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 44h:01m:35s remains)
INFO - root - 2017-12-16 02:19:30.479451: step 86800, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 44h:19m:05s remains)
2017-12-16 02:19:31.051885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9826479 -6.8013415 -6.9074955 -7.1018457 -7.9138465 -7.5954642 -6.41376 -4.9211445 -4.0538745 -3.6733134 -3.1486063 -3.3628886 -3.9665349 -7.826086 -7.2360253][-6.7475996 -7.9231339 -7.3939395 -7.2545886 -8.095953 -7.7548075 -6.9889603 -5.7703671 -4.9867091 -4.5297 -3.8548079 -4.5766296 -6.2780609 -9.99453 -8.044404][-6.6260829 -7.403286 -7.9619284 -7.494627 -8.0102606 -8.3544025 -7.965662 -6.632937 -5.6167355 -5.0104704 -4.7900448 -5.1430845 -7.0528488 -11.120356 -8.9200745][-6.6331592 -6.9281397 -7.2682285 -7.5441108 -7.7774897 -7.0100288 -6.2894177 -6.452055 -6.975378 -6.9158363 -6.9301772 -7.0439591 -8.6053219 -11.963839 -10.001979][-6.3543839 -7.0395727 -7.3880811 -5.582665 -5.1702709 -4.1830316 -2.9270508 -3.7935767 -5.3706965 -5.9617791 -6.4023924 -7.381031 -9.7430477 -13.386587 -11.428238][-5.4658628 -5.5848413 -5.7536106 -4.5417857 -3.3707104 -0.8171258 0.57448578 -0.91816664 -2.8106225 -4.8831077 -6.3198605 -6.3545046 -7.5649271 -11.392963 -9.4186134][-5.1458054 -5.1949463 -4.8909979 -2.8600008 -1.5068417 0.51806211 2.2506495 2.6181464 2.2179213 -1.1930428 -4.1280007 -5.3623662 -7.2124248 -10.497728 -8.4277239][-6.1121197 -5.7903814 -4.2447205 -1.5908031 0.55516052 2.9015937 4.0319476 3.8043532 3.4721112 0.70345926 -1.8163874 -3.3053138 -5.6233311 -8.8555479 -6.5708175][-6.07223 -4.5881548 -3.3297579 -2.0006237 -0.88028193 2.3494811 4.6244903 4.84115 3.3047462 0.90272617 -0.76347256 -2.5550067 -4.2070847 -6.8650112 -4.2321253][-6.2828088 -5.5156479 -3.6274457 -1.6751413 -0.49262953 1.2723212 3.1969 4.8293376 3.7885184 0.057496548 -2.2123728 -3.2995157 -5.0781045 -7.6425481 -5.1371803][-9.5169334 -7.4751024 -4.6821842 -2.667592 -1.0773044 -1.0747185 -1.0805359 0.010339737 0.068985939 -2.3243721 -4.5163484 -6.0231433 -7.6931953 -9.6375666 -6.5460029][-12.443746 -11.205898 -8.1926517 -4.77206 -2.8250766 -2.941359 -3.7816041 -3.8322232 -4.4278207 -6.4907589 -7.7519474 -8.1277351 -9.0010042 -10.630489 -8.0201807][-13.685167 -11.483352 -8.3899946 -5.3624115 -3.5845315 -3.5051742 -3.7803068 -5.1983986 -7.0061131 -9.121357 -9.99468 -9.8464794 -9.9100266 -11.53738 -9.3341494][-10.537111 -8.7709427 -7.0361357 -5.4511833 -4.1804838 -3.3975353 -3.4548683 -3.7131066 -5.0424604 -7.4704685 -9.2244978 -8.949049 -8.5667868 -9.4007072 -8.87343][-6.9877825 -6.1456265 -5.3100443 -2.9162543 -1.4623165 -1.3508401 -2.7275341 -3.4450574 -4.1706371 -5.3596077 -6.5000205 -7.7394533 -8.9669313 -9.3573427 -9.3057709]]...]
INFO - root - 2017-12-16 02:19:37.641162: step 86810, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 45h:05m:07s remains)
INFO - root - 2017-12-16 02:19:44.268769: step 86820, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 45h:20m:29s remains)
INFO - root - 2017-12-16 02:19:50.841502: step 86830, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 43h:55m:48s remains)
INFO - root - 2017-12-16 02:19:57.423678: step 86840, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 47h:06m:47s remains)
INFO - root - 2017-12-16 02:20:04.025460: step 86850, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 44h:33m:22s remains)
INFO - root - 2017-12-16 02:20:10.544273: step 86860, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.634 sec/batch; 43h:15m:08s remains)
INFO - root - 2017-12-16 02:20:17.152109: step 86870, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 44h:47m:20s remains)
INFO - root - 2017-12-16 02:20:23.719100: step 86880, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 45h:06m:54s remains)
INFO - root - 2017-12-16 02:20:30.333741: step 86890, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 45h:09m:53s remains)
INFO - root - 2017-12-16 02:20:36.907084: step 86900, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.649 sec/batch; 44h:18m:09s remains)
2017-12-16 02:20:37.451397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.56554 -4.3039932 -3.6768854 -3.1473634 -3.8745093 -3.9818048 -4.092833 -3.7598498 -3.7081387 -3.4147666 -3.5612807 -6.4768782 -8.018508 -8.7643452 -8.3102264][-5.4269614 -4.5032687 -4.5101562 -3.9292111 -3.9831128 -4.1912408 -3.8835087 -3.4242663 -3.0226777 -2.5944202 -2.7627478 -5.3131433 -7.16306 -8.723031 -9.0670843][-4.1773539 -4.2651849 -4.2211266 -3.7020721 -3.8272314 -3.2390115 -2.6635995 -2.1890378 -1.8507106 -2.5712039 -3.1089084 -5.2825742 -6.3439407 -7.854619 -7.9061794][-5.0810142 -4.7563424 -3.7586727 -2.9604018 -3.2108817 -2.3602328 -1.5142498 -1.2922187 -1.4387603 -1.9969873 -2.5493226 -4.8213806 -6.2333469 -7.426538 -7.3013754][-4.3234854 -4.5752964 -4.4586725 -3.4124293 -2.7040448 -0.90856647 0.36575508 0.71356773 0.6794343 -0.286613 -1.1592865 -3.8376434 -5.0898094 -6.4635382 -6.8056269][-5.414144 -5.4797873 -4.0126514 -1.4486132 0.38023138 2.1856809 3.0734296 3.3926597 3.0460591 1.052258 -0.25930643 -2.2861009 -3.7447629 -5.6519256 -5.5121212][-7.0047331 -6.1837177 -4.4227104 -1.4950204 0.76797295 3.6520638 5.9844003 6.54962 5.9775329 3.940906 1.7314749 -1.9537907 -3.9818528 -5.5919027 -5.6623898][-6.6586533 -6.4881072 -4.8812361 -1.7181966 1.0102768 5.1689372 7.0403056 6.8230586 6.6571231 4.9205594 2.8448005 -1.2460461 -3.7747717 -5.9955168 -6.4728036][-5.7774415 -5.442874 -4.1058593 -1.3426943 1.0748205 3.8285871 5.1070056 6.0389609 6.3040462 4.3260722 2.3268228 -1.6914434 -4.7863669 -6.712719 -7.0877662][-4.5985808 -4.2725897 -3.7183084 -1.3413744 -0.07515049 2.0932798 3.6346927 4.3842616 4.0025468 2.6186643 1.3811913 -2.5244548 -5.3760347 -7.9799533 -9.2036076][-7.2544718 -6.3141222 -4.9272208 -3.2691183 -2.5260189 -1.4307303 -1.4890065 -1.6957617 -1.7552087 -2.1822588 -3.0465715 -6.5682154 -8.0443382 -9.3476219 -9.3843689][-9.8419609 -8.0526848 -6.4823875 -4.766099 -3.6274705 -3.4931226 -4.1385989 -4.7787213 -5.4269123 -6.3287525 -6.8810978 -8.4911194 -9.0422029 -10.111682 -9.9801941][-10.054741 -7.9779129 -6.1188626 -5.0068083 -4.9096403 -4.6044254 -5.509429 -6.3414612 -7.4865451 -8.373229 -8.8479128 -9.571209 -9.6056595 -9.5550976 -7.8981428][-8.0224686 -7.5551605 -6.5797143 -5.6250563 -5.0541487 -5.4081 -6.0585346 -6.59006 -7.5184345 -8.4320126 -9.1317883 -8.3707542 -7.3073382 -6.6709104 -5.6858864][-6.3301668 -5.2831931 -4.7718353 -3.9550834 -3.1451321 -3.8252316 -4.6146731 -5.8729954 -7.0681849 -7.18544 -7.229537 -7.8534122 -7.6590891 -7.1581717 -6.501049]]...]
INFO - root - 2017-12-16 02:20:43.980410: step 86910, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 44h:53m:24s remains)
INFO - root - 2017-12-16 02:20:50.572537: step 86920, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 45h:18m:51s remains)
INFO - root - 2017-12-16 02:20:57.160395: step 86930, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 44h:16m:51s remains)
INFO - root - 2017-12-16 02:21:03.745436: step 86940, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.668 sec/batch; 45h:33m:16s remains)
INFO - root - 2017-12-16 02:21:10.318690: step 86950, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.633 sec/batch; 43h:10m:16s remains)
INFO - root - 2017-12-16 02:21:16.892799: step 86960, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 45h:04m:27s remains)
INFO - root - 2017-12-16 02:21:23.552862: step 86970, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 45h:18m:40s remains)
INFO - root - 2017-12-16 02:21:30.163510: step 86980, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 45h:05m:23s remains)
INFO - root - 2017-12-16 02:21:36.712978: step 86990, loss = 0.18, batch loss = 0.14 (12.7 examples/sec; 0.628 sec/batch; 42h:49m:56s remains)
INFO - root - 2017-12-16 02:21:43.265027: step 87000, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 45h:17m:37s remains)
2017-12-16 02:21:43.815250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4858141 -4.5998898 -5.3823452 -4.5359726 -4.8600616 -5.4350748 -5.366 -6.5222116 -7.8008175 -9.0359468 -8.8536634 -7.0193429 -6.36442 -7.38908 -6.5041513][-5.2782917 -5.7428379 -5.5200167 -5.7523389 -4.9892516 -5.2181516 -5.6852164 -6.1598172 -7.4032345 -7.950407 -7.736567 -5.7273293 -5.2137442 -7.3784323 -5.7908187][-3.8565021 -5.5955229 -7.2169981 -6.3444147 -5.6813955 -6.1568522 -6.2413621 -6.880733 -7.8755507 -8.5733318 -7.9044151 -5.1708164 -4.6279483 -6.02515 -5.4936295][-4.3792524 -5.3779817 -5.7800674 -6.6039329 -6.8381495 -5.7652764 -5.1393361 -6.6719189 -7.7601371 -8.1719742 -7.5507431 -5.0933762 -4.6707211 -6.1423068 -5.8163352][-5.1693058 -5.7206755 -6.7011209 -5.4869323 -5.4397259 -4.3331747 -3.3071408 -3.6703362 -5.9252172 -7.0555773 -7.08588 -4.9217515 -3.9577513 -5.9978762 -5.301424][-6.872963 -5.9574494 -6.0996709 -5.0241671 -3.5293503 -1.8656116 -0.30551434 -0.28794765 -1.1963239 -3.9695294 -6.3324633 -4.4031878 -4.185801 -6.6501093 -6.4181442][-5.9632692 -6.0526977 -5.7426496 -4.1826839 -1.682981 0.55927181 2.5983605 3.3022838 2.7851472 0.62794733 -1.82968 -2.1708498 -2.7440488 -5.2908177 -5.7094932][-7.115418 -6.1799378 -5.3139658 -3.8867226 -1.3742194 2.1760445 3.4701934 3.2798924 2.9717612 1.6392899 -0.22547579 -0.34486771 -1.478756 -4.8571877 -5.279932][-6.9251456 -5.8815136 -5.947917 -3.7822771 -3.0957797 -0.88955021 2.7209392 1.7285385 -0.99896145 -1.6352334 -1.4651365 -0.48660803 -1.37043 -4.81071 -4.4051218][-8.4872208 -7.3810086 -6.1149087 -5.2440338 -4.6850309 -2.6797762 -0.90871763 -0.66256428 -3.2640111 -5.0535393 -4.9238 -2.938303 -2.7777455 -4.579267 -4.6415658][-12.056982 -10.303951 -10.20447 -8.2526608 -6.2864976 -5.1770654 -4.4907355 -4.8908372 -5.270155 -6.5667715 -7.0854664 -5.7817483 -4.5684576 -5.3782611 -5.3286338][-14.152897 -11.989302 -9.386343 -7.5609007 -5.9733768 -4.1601934 -4.1116767 -5.2649527 -7.2567339 -6.8978524 -6.1580305 -5.7228293 -5.178854 -5.8886795 -3.7174258][-12.607474 -11.031425 -8.24696 -5.4040856 -3.4947503 -1.8196585 -1.8539855 -3.5722454 -5.6293921 -6.5223074 -5.5407515 -3.9714479 -3.4971571 -4.9196763 -4.9382887][-10.00874 -8.6565762 -6.6777053 -3.7056468 -1.8434515 -1.6159081 -2.1874349 -2.8331447 -4.0645304 -4.7030554 -3.5477905 -2.7995358 -2.9658163 -2.7358694 -2.5047905][-6.32062 -7.2864103 -5.4592533 -2.4323118 -1.9060249 -2.8584423 -3.5543425 -4.869566 -5.0971313 -4.4819422 -3.8632121 -4.2185688 -4.1770773 -3.507664 -3.5862103]]...]
INFO - root - 2017-12-16 02:21:50.385150: step 87010, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 45h:38m:03s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 02:21:56.996973: step 87020, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 43h:56m:53s remains)
INFO - root - 2017-12-16 02:22:03.621447: step 87030, loss = 0.17, batch loss = 0.13 (11.8 examples/sec; 0.677 sec/batch; 46h:09m:16s remains)
INFO - root - 2017-12-16 02:22:10.199297: step 87040, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 45h:06m:51s remains)
INFO - root - 2017-12-16 02:22:16.789617: step 87050, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.683 sec/batch; 46h:35m:14s remains)
INFO - root - 2017-12-16 02:22:23.375913: step 87060, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.662 sec/batch; 45h:07m:17s remains)
INFO - root - 2017-12-16 02:22:29.997802: step 87070, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.651 sec/batch; 44h:24m:48s remains)
INFO - root - 2017-12-16 02:22:36.637328: step 87080, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.687 sec/batch; 46h:50m:30s remains)
INFO - root - 2017-12-16 02:22:43.203652: step 87090, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.662 sec/batch; 45h:07m:18s remains)
INFO - root - 2017-12-16 02:22:49.795549: step 87100, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.675 sec/batch; 46h:02m:39s remains)
2017-12-16 02:22:50.343131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.6564465 -6.7536011 -5.6640148 -4.6970863 -5.9535642 -6.17702 -5.8984795 -5.941102 -5.3643064 -4.7151003 -3.9441488 -2.8506598 -4.8792996 -5.3753242 -4.2898941][-5.9111609 -4.8522539 -4.4188881 -3.846585 -4.0080724 -4.2520056 -4.0366116 -4.0700068 -4.0047412 -3.4475241 -2.3568368 -1.0554543 -2.9478891 -4.5589581 -3.3103769][-4.6385751 -4.4682779 -4.3520212 -4.2751484 -5.0353413 -4.9681768 -4.5631065 -4.1355448 -3.2974758 -2.5217094 -2.0233135 -0.85692263 -2.34474 -3.2655895 -2.8093719][-5.340939 -5.5024629 -6.1657009 -5.8391352 -6.0605607 -4.7289205 -3.1838794 -2.4217894 -2.4370513 -2.588764 -1.88937 -1.0983095 -3.5967097 -4.2037745 -2.7430174][-5.5497942 -6.8493648 -7.9085569 -7.2205873 -6.9223623 -4.4499245 -1.3853664 -0.57633162 -1.6009555 -2.0865979 -2.1260457 -2.2363856 -4.3188457 -5.9053364 -5.346879][-7.9218225 -7.661087 -8.29301 -7.5183139 -5.4575667 -0.84612846 2.719933 3.085917 1.277071 -1.3798985 -3.0588548 -3.3162522 -5.28808 -6.371964 -5.2253981][-9.8983154 -8.900878 -7.0954247 -5.5792313 -3.0609915 1.6047173 5.8035541 6.6732478 4.888185 0.96618462 -2.5726242 -3.4178259 -5.4654303 -6.69309 -5.7770829][-11.24074 -9.6603146 -8.61141 -5.1916618 -0.64253759 4.3511596 8.3746719 8.3919353 6.1967969 1.6544623 -2.3991146 -3.6654553 -6.8937688 -7.4440041 -5.7028861][-11.231279 -10.261236 -9.40052 -7.603282 -3.6442626 1.7292786 6.2650819 6.0331321 3.3058524 0.069791317 -2.453706 -4.9169731 -8.3109417 -8.9270058 -7.2626958][-11.169006 -11.243868 -10.131296 -8.2441959 -6.6866922 -2.4446075 1.7116203 2.1052847 -0.0048775673 -4.1801677 -6.6513624 -6.996027 -10.313742 -11.213991 -8.6168308][-14.417635 -13.886299 -13.323437 -12.089115 -10.165103 -8.137805 -6.0280676 -4.55615 -5.0366273 -7.1437426 -10.100536 -11.483587 -13.296452 -12.441969 -9.7217293][-14.392452 -14.174763 -13.92989 -13.04201 -11.444882 -9.8838186 -9.0931253 -8.2150011 -8.4796724 -9.2467213 -10.447822 -11.466137 -13.648195 -12.699188 -8.9627113][-13.401455 -12.764344 -12.20953 -11.381676 -11.525736 -10.20896 -9.0984087 -8.7924461 -8.6857328 -8.4072685 -8.7063828 -9.7499809 -10.39831 -9.8467789 -7.7384615][-12.573316 -12.438251 -11.804373 -11.112885 -10.013849 -8.9308338 -8.01746 -7.3409376 -6.9559793 -7.0218406 -6.9789438 -6.7133217 -7.0489149 -6.6739516 -5.2932663][-9.6979256 -10.440886 -9.7434473 -8.8415213 -7.3271022 -6.3792005 -5.8298235 -4.9594278 -4.7534142 -4.7392688 -4.6865578 -5.1533504 -6.0623965 -7.3528523 -7.4642615]]...]
INFO - root - 2017-12-16 02:22:56.948393: step 87110, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 44h:30m:43s remains)
INFO - root - 2017-12-16 02:23:03.557926: step 87120, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.671 sec/batch; 45h:42m:54s remains)
INFO - root - 2017-12-16 02:23:10.139904: step 87130, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.654 sec/batch; 44h:35m:06s remains)
INFO - root - 2017-12-16 02:23:16.709928: step 87140, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.649 sec/batch; 44h:15m:04s remains)
INFO - root - 2017-12-16 02:23:23.335283: step 87150, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 46h:18m:53s remains)
INFO - root - 2017-12-16 02:23:29.939663: step 87160, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.646 sec/batch; 44h:01m:09s remains)
INFO - root - 2017-12-16 02:23:36.549933: step 87170, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.677 sec/batch; 46h:08m:03s remains)
INFO - root - 2017-12-16 02:23:43.167094: step 87180, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.665 sec/batch; 45h:20m:19s remains)
INFO - root - 2017-12-16 02:23:49.730909: step 87190, loss = 0.15, batch loss = 0.10 (12.7 examples/sec; 0.628 sec/batch; 42h:48m:55s remains)
INFO - root - 2017-12-16 02:23:56.362548: step 87200, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 45h:29m:58s remains)
2017-12-16 02:23:56.906406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1533761 -5.7093592 -6.7112408 -7.6079454 -9.3815355 -10.146452 -10.709203 -11.146286 -11.419531 -11.366786 -10.540184 -10.824869 -12.235044 -12.408501 -11.139975][-8.1931524 -8.9924755 -9.9275551 -10.429859 -11.507047 -12.358124 -13.074287 -13.988857 -14.917622 -14.533373 -13.718643 -12.995006 -14.10153 -14.436302 -11.834172][-5.9375076 -8.4219465 -11.022171 -11.187369 -11.890034 -12.584684 -13.183015 -13.34868 -13.710167 -14.26959 -13.958452 -13.209952 -14.574221 -15.06489 -13.099956][-7.6773195 -9.2156525 -11.453443 -11.48243 -11.251238 -10.241929 -10.058082 -11.900335 -12.474418 -11.388309 -11.251896 -12.433018 -14.530457 -14.954332 -12.856594][-9.7292023 -12.106837 -13.800995 -12.898566 -10.651129 -5.5978088 -3.6999855 -7.7150278 -10.363898 -9.868103 -9.6669579 -9.74333 -12.92844 -16.29476 -14.859594][-12.955463 -12.976603 -13.242373 -11.641422 -9.0909882 -2.4491415 3.5594335 2.7427955 -1.1232891 -5.9114518 -8.5597477 -7.3731632 -10.552048 -13.916416 -14.943886][-15.068108 -15.024012 -13.683411 -8.8890715 -5.3762531 -0.89798021 5.014257 7.5873961 8.022768 0.39479446 -6.7605548 -7.6201143 -10.936145 -11.9198 -12.13023][-13.627144 -14.491278 -14.741961 -8.4162731 -1.4905596 4.2826886 7.5334029 6.9420314 7.2239919 2.8703723 -3.6485584 -7.4860816 -12.251499 -13.70573 -12.144076][-11.114338 -10.89218 -12.386412 -10.444488 -5.7586036 2.4838023 7.4030309 6.8862472 4.4909196 -0.75279713 -4.94324 -8.717886 -14.458012 -16.943069 -15.344637][-9.0423317 -8.6721077 -10.152012 -9.2774467 -7.9223146 -3.1563296 2.1883826 4.245646 2.3768592 -3.131237 -8.3624678 -11.411755 -15.478106 -18.0701 -18.065256][-12.704749 -12.271875 -12.259893 -11.321291 -11.517199 -10.805519 -8.0686321 -5.3771167 -5.1467438 -7.1703835 -10.556525 -13.930771 -16.877819 -17.536867 -16.806643][-17.868883 -17.1064 -15.943506 -15.142525 -15.133192 -14.440182 -14.710573 -13.689653 -12.764474 -13.32869 -14.925459 -16.127945 -16.251396 -16.334061 -13.866531][-16.173386 -15.425474 -14.810387 -15.624102 -15.557182 -14.337652 -13.721773 -13.683908 -14.13138 -13.518478 -12.825647 -14.019175 -14.979483 -14.126419 -11.593004][-12.087605 -12.189055 -11.478489 -11.346676 -11.864847 -13.101248 -12.648979 -11.175786 -11.401193 -11.862812 -11.92067 -10.831976 -10.774451 -10.307471 -9.9591713][-7.7189169 -7.3091292 -6.3818836 -4.7415991 -4.3749442 -5.6547732 -6.8694711 -6.9158335 -6.687161 -6.51632 -7.6121635 -8.6213255 -9.7912827 -9.9261417 -9.9377384]]...]
INFO - root - 2017-12-16 02:24:03.464951: step 87210, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.641 sec/batch; 43h:39m:19s remains)
INFO - root - 2017-12-16 02:24:10.038330: step 87220, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.658 sec/batch; 44h:49m:58s remains)
INFO - root - 2017-12-16 02:24:16.689998: step 87230, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 45h:08m:56s remains)
INFO - root - 2017-12-16 02:24:23.330387: step 87240, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.674 sec/batch; 45h:56m:03s remains)
INFO - root - 2017-12-16 02:24:29.931421: step 87250, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 44h:48m:47s remains)
INFO - root - 2017-12-16 02:24:36.652687: step 87260, loss = 0.16, batch loss = 0.12 (11.6 examples/sec; 0.689 sec/batch; 46h:54m:53s remains)
INFO - root - 2017-12-16 02:24:43.297958: step 87270, loss = 0.14, batch loss = 0.10 (11.6 examples/sec; 0.690 sec/batch; 47h:02m:10s remains)
INFO - root - 2017-12-16 02:24:49.923497: step 87280, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 44h:36m:40s remains)
INFO - root - 2017-12-16 02:24:56.589250: step 87290, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.649 sec/batch; 44h:14m:17s remains)
INFO - root - 2017-12-16 02:25:03.271810: step 87300, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 46h:03m:45s remains)
2017-12-16 02:25:03.824290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1679878 -6.542161 -4.9980736 -3.7922888 -4.0651779 -3.9326711 -3.7262878 -3.7788033 -3.529866 -2.98585 -2.7750928 -5.3470216 -7.5429358 -8.7518873 -9.3625832][-5.7016072 -6.1417046 -4.8627205 -3.9528048 -4.3737397 -4.468504 -4.5150442 -4.3242068 -3.3524184 -2.2732084 -1.6049447 -3.702492 -6.3406649 -7.1216974 -7.9221897][-4.1859741 -4.6022787 -4.0814009 -3.906745 -4.0326743 -4.2870474 -4.3468466 -4.1631894 -3.7999606 -2.8777459 -2.4282253 -4.6194363 -7.0129323 -7.7438893 -8.6571712][-2.1427102 -3.1995084 -2.8542128 -2.8909395 -3.2430949 -3.0204532 -3.224972 -3.6776371 -3.7846255 -3.1056283 -2.6209452 -5.0820642 -7.2059741 -7.6297917 -7.7756743][-1.9901528 -2.7733395 -2.4983785 -2.0383859 -2.0981803 -1.6985331 -1.6140776 -1.3695254 -1.410933 -1.901551 -2.4734547 -4.9902711 -7.6834679 -8.1530247 -8.3741636][-3.3038294 -3.1747644 -2.2289417 -1.4495139 -0.64775991 0.46829462 1.0019464 0.94696808 0.20414257 -0.31026316 -0.99431038 -4.1093817 -6.6141734 -7.04846 -7.72519][-6.1282735 -5.164443 -2.9339561 -0.97827959 0.6234107 2.1650648 3.0176482 3.1350055 2.3643632 1.3198156 0.25388336 -2.9205251 -5.6692033 -6.0255613 -6.7141752][-6.73586 -6.4073157 -4.5744982 -1.7584355 0.83040524 2.5646691 3.3895459 3.3749413 2.7521052 1.9645972 1.2826896 -2.0447295 -4.8267217 -5.6698351 -6.4797597][-6.3214064 -6.8168049 -5.5907016 -2.4775043 -0.16068935 1.664628 2.607923 3.1090026 3.0756984 2.0237446 0.89078 -2.7835345 -5.5802474 -5.8350325 -6.9109278][-5.8912673 -6.0393224 -4.6585035 -2.5708909 -1.0898371 0.63650036 1.294548 1.27844 0.94459105 0.608222 0.31974649 -3.3813999 -6.7148476 -7.5126581 -8.2985563][-8.2177315 -7.6150751 -6.1798286 -3.9857645 -2.9449193 -1.9755399 -1.8236449 -2.2036016 -2.7137766 -3.4630413 -4.0466561 -7.6067781 -9.4323606 -9.18265 -9.1309109][-10.555534 -8.95994 -6.6927238 -4.8565 -3.6746266 -3.7544053 -4.6406283 -5.5637274 -6.4765978 -7.3386407 -7.5279059 -9.3447647 -10.195488 -9.660368 -9.4191074][-11.109424 -8.9510965 -6.6603818 -5.4203777 -5.02542 -4.7902379 -4.9212322 -6.138546 -7.1950889 -8.5176373 -8.9854555 -9.4826155 -9.5083628 -7.6449337 -6.6620116][-11.517826 -10.277359 -8.2455444 -5.9012365 -4.8895016 -5.8093424 -6.2594337 -6.1026659 -6.2635155 -6.8642883 -6.8783131 -6.9347429 -6.3219051 -5.8432026 -5.4224911][-9.1829538 -8.8237009 -8.0372934 -6.715044 -6.1157584 -5.7412562 -5.841104 -6.0319781 -5.706151 -5.40574 -5.5029273 -6.0234222 -6.0484757 -5.7634673 -5.6523805]]...]
INFO - root - 2017-12-16 02:25:10.465950: step 87310, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 46h:11m:43s remains)
INFO - root - 2017-12-16 02:25:17.101748: step 87320, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.680 sec/batch; 46h:19m:51s remains)
INFO - root - 2017-12-16 02:25:23.738472: step 87330, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.665 sec/batch; 45h:16m:14s remains)
INFO - root - 2017-12-16 02:25:30.336132: step 87340, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 44h:07m:49s remains)
INFO - root - 2017-12-16 02:25:36.955945: step 87350, loss = 0.21, batch loss = 0.16 (12.5 examples/sec; 0.639 sec/batch; 43h:29m:54s remains)
INFO - root - 2017-12-16 02:25:43.683759: step 87360, loss = 0.19, batch loss = 0.14 (11.6 examples/sec; 0.688 sec/batch; 46h:51m:08s remains)
INFO - root - 2017-12-16 02:25:50.308124: step 87370, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 45h:23m:00s remains)
INFO - root - 2017-12-16 02:25:56.918107: step 87380, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.669 sec/batch; 45h:34m:32s remains)
INFO - root - 2017-12-16 02:26:03.394571: step 87390, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.640 sec/batch; 43h:35m:01s remains)
INFO - root - 2017-12-16 02:26:09.939305: step 87400, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 43h:59m:47s remains)
2017-12-16 02:26:10.491712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2347553 -4.2860951 -4.7797604 -3.9791622 -3.16119 -3.3496552 -2.2030346 -0.40546083 0.99927759 -0.872612 -3.3880832 -7.3254542 -8.8544846 -13.457533 -14.623259][-4.4598308 -4.0502567 -4.3533583 -4.5673814 -3.9644337 -3.0561249 -1.3194146 -0.37376118 0.63601017 -0.13662195 -2.1387775 -6.6826205 -8.9672709 -14.41736 -16.437626][-3.0188768 -3.91887 -3.9782925 -3.2640564 -3.4688313 -3.9074616 -2.0649111 0.10294437 -0.18626881 -1.7566447 -2.7624836 -7.201437 -9.4457512 -13.802172 -15.221764][-2.6928504 -3.1417024 -3.7543888 -2.6369159 -1.6978827 -1.2178788 -1.2930923 -0.50992966 0.23302317 -0.93413258 -3.232656 -7.3022041 -9.0230474 -14.053307 -14.813221][-0.98718882 -0.98484039 -0.77026176 -1.3875151 -1.7031255 -0.32176828 0.30691195 0.084385872 0.089349747 0.34652042 -2.1692865 -8.0270576 -9.5453835 -13.753525 -15.244793][-1.7649839 -2.6210132 -1.6566224 -1.181118 -0.54295492 0.72845745 2.065958 2.6520343 2.2327967 0.49245167 -2.0653312 -6.7814136 -9.9058647 -13.742994 -14.259146][-2.8670442 -4.0070581 -4.015986 -2.5341444 -0.58712769 1.7943087 2.9696908 3.4818788 4.2053905 1.5493188 -1.6792088 -5.8096895 -7.6575079 -12.434889 -13.407928][-4.8734379 -4.8960376 -4.146554 -2.7012429 -0.93189621 1.8980074 4.069222 4.1453118 4.3635192 3.2074723 0.39449167 -4.7251415 -6.79177 -10.937376 -12.521844][-4.5179081 -6.2255993 -5.216825 -3.0198712 -1.7212782 0.17178679 2.939302 4.0243297 3.4040198 1.8180666 -0.12576246 -4.9133868 -7.2302232 -11.344556 -12.622051][-4.4718857 -5.5184603 -5.9483876 -4.4539747 -2.9742858 -1.450532 0.19034672 1.343101 1.4027066 -1.1467876 -2.7745774 -5.9979315 -7.49614 -11.120468 -12.101562][-7.0878782 -7.884737 -7.7322431 -6.266211 -6.0164933 -4.2471523 -2.7437046 -2.9011586 -3.0474563 -4.1440945 -5.840682 -10.119223 -10.938089 -13.20749 -12.587624][-9.0511227 -9.4047613 -8.882679 -7.0819221 -6.8458142 -6.75709 -6.0530519 -6.3337774 -6.8157473 -7.4160047 -8.895195 -11.17346 -11.836529 -12.701999 -12.786551][-10.800653 -10.687254 -9.0363884 -7.7052717 -8.12529 -8.5635471 -8.6988811 -8.4577942 -8.1717224 -9.1713238 -9.8851814 -11.388413 -12.140985 -11.926264 -11.524564][-9.050745 -9.1669121 -8.8171711 -7.3039374 -6.7499537 -7.9892788 -8.9167967 -8.7175322 -8.5869808 -9.3681946 -8.9433765 -8.5463171 -7.6495619 -9.1043129 -9.163928][-8.7529888 -8.5609684 -8.7414188 -8.3525572 -7.6599445 -8.0351667 -8.7943707 -9.5618134 -9.2521267 -8.3635693 -8.49704 -9.1458931 -9.8442936 -10.617126 -10.794004]]...]
INFO - root - 2017-12-16 02:26:17.158024: step 87410, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 44h:56m:08s remains)
INFO - root - 2017-12-16 02:26:23.673984: step 87420, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 45h:32m:08s remains)
INFO - root - 2017-12-16 02:26:30.346886: step 87430, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 44h:26m:25s remains)
INFO - root - 2017-12-16 02:26:36.980430: step 87440, loss = 0.23, batch loss = 0.19 (11.9 examples/sec; 0.672 sec/batch; 45h:44m:56s remains)
INFO - root - 2017-12-16 02:26:43.503789: step 87450, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.653 sec/batch; 44h:27m:53s remains)
INFO - root - 2017-12-16 02:26:50.176296: step 87460, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.654 sec/batch; 44h:32m:48s remains)
INFO - root - 2017-12-16 02:26:56.718216: step 87470, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 44h:26m:36s remains)
INFO - root - 2017-12-16 02:27:03.330564: step 87480, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 44h:58m:00s remains)
INFO - root - 2017-12-16 02:27:09.952994: step 87490, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 44h:24m:45s remains)
INFO - root - 2017-12-16 02:27:16.617114: step 87500, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 43h:32m:44s remains)
2017-12-16 02:27:17.152872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2820823 -3.1300993 -2.3681498 -2.4459393 -3.1665182 -2.8682768 -2.5173995 -2.9197719 -3.4976785 -4.8679814 -5.5971403 -7.4349384 -10.201917 -11.787695 -11.854981][-2.7833679 -2.8187838 -2.5810983 -2.5204847 -3.0912161 -3.6030178 -3.4557753 -3.1799774 -4.0783558 -5.130734 -5.5342889 -7.5245972 -10.080223 -11.618334 -12.075171][-0.97883844 -1.8623543 -2.5426304 -3.7337885 -4.9459472 -5.386209 -5.0241375 -4.6485181 -4.4141817 -4.8499589 -5.2644267 -6.5477228 -9.0679741 -10.611421 -12.187604][-2.7749131 -3.9825904 -4.95157 -5.4069138 -5.8096514 -6.4394722 -6.3139486 -6.0230045 -5.9098959 -5.2936358 -5.3953719 -7.5955048 -9.7003307 -11.266611 -12.893925][-3.1027153 -5.1262937 -6.2734113 -6.5837221 -6.493546 -5.98561 -4.8921838 -3.9525967 -3.3095505 -3.4113598 -3.8593512 -6.0850286 -8.9693823 -10.337507 -11.771736][-5.2272382 -5.0660563 -4.9362435 -4.9879503 -4.4019814 -3.5604794 -2.086122 -0.65878344 -0.18839121 -0.87306595 -1.3186932 -3.3609893 -6.3739748 -7.8569431 -9.0238924][-4.3693886 -4.3432817 -3.72372 -3.0247254 -2.6384556 -1.2677841 0.77108335 2.1123109 2.9094548 1.6520071 -0.32229471 -3.680696 -6.6875963 -7.2113028 -8.1394472][-3.0259624 -3.1171854 -2.5807054 -1.6164651 -1.5009708 -0.15072727 1.5104094 2.8068547 3.7635674 2.6210093 0.96122742 -3.2098925 -6.7802453 -8.1849022 -9.2904263][-3.9658453 -3.638737 -2.1402619 -1.1623244 -1.8770227 -1.9934182 -1.4750447 0.053650856 1.3418117 0.45466948 -0.82623434 -4.8029885 -8.90666 -10.525799 -11.16312][-5.0591879 -5.9477844 -4.5903172 -3.1448681 -3.3643706 -2.9336662 -2.6608689 -2.0470865 -1.2764297 -1.1284423 -2.1065066 -6.2008004 -9.8089085 -11.843367 -13.692392][-7.8767939 -8.6976871 -7.9413028 -6.9826212 -6.8944879 -5.9881692 -4.6134467 -3.6433811 -2.9854934 -2.9678988 -3.6304047 -7.9143867 -10.951804 -11.78249 -11.547062][-12.708788 -12.388035 -11.727652 -9.7725887 -8.5027618 -6.6542745 -5.8695645 -5.3343453 -5.01021 -6.353653 -7.5477371 -9.5018091 -9.6280527 -9.8799639 -9.6556883][-11.033113 -10.310866 -9.3230209 -8.8587036 -8.4184589 -5.9976859 -4.6282315 -4.5044012 -5.1651163 -5.8442535 -6.7260604 -8.0515165 -8.0489378 -7.368372 -6.841464][-7.5877671 -6.7621098 -6.8317504 -5.7502728 -5.6774077 -6.0382366 -5.3842945 -4.2493863 -4.8025379 -5.1922283 -5.7924461 -6.2772012 -6.4615693 -6.6681943 -6.5693831][-7.148839 -6.5157452 -5.6465936 -3.3132839 -1.9280789 -2.652602 -3.4499607 -3.850493 -4.2426038 -5.0499864 -5.5848637 -6.9475608 -7.7553825 -7.7508426 -7.7296033]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-87500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-87500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 02:27:25.032662: step 87510, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 45h:05m:04s remains)
INFO - root - 2017-12-16 02:27:31.609128: step 87520, loss = 0.20, batch loss = 0.15 (12.6 examples/sec; 0.636 sec/batch; 43h:16m:07s remains)
INFO - root - 2017-12-16 02:27:38.200191: step 87530, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.690 sec/batch; 46h:56m:47s remains)
INFO - root - 2017-12-16 02:27:44.851433: step 87540, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 44h:08m:33s remains)
INFO - root - 2017-12-16 02:27:51.436791: step 87550, loss = 0.25, batch loss = 0.21 (11.8 examples/sec; 0.679 sec/batch; 46h:12m:29s remains)
INFO - root - 2017-12-16 02:27:58.032781: step 87560, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.664 sec/batch; 45h:09m:39s remains)
INFO - root - 2017-12-16 02:28:04.544881: step 87570, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.635 sec/batch; 43h:13m:13s remains)
INFO - root - 2017-12-16 02:28:11.139295: step 87580, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 45h:22m:37s remains)
INFO - root - 2017-12-16 02:28:17.751277: step 87590, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.692 sec/batch; 47h:02m:41s remains)
INFO - root - 2017-12-16 02:28:24.252647: step 87600, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 44h:11m:28s remains)
2017-12-16 02:28:24.890188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2853284 -4.1832881 -2.630403 -2.2994542 -2.2498596 -2.2716236 -2.5686996 -2.5179574 -2.8624969 -1.9654794 -2.2760663 -3.5944979 -7.251615 -11.646088 -11.003416][-3.1808777 -2.3651533 -0.91251993 -1.2378511 -1.2266021 -2.0520542 -2.2217126 -3.2422674 -3.9134941 -4.5864148 -4.5249071 -5.1769643 -7.7266388 -10.895249 -10.598][-2.7824016 -3.2894258 -2.7559583 -3.13313 -3.5557363 -3.2320759 -3.0743988 -3.5954323 -3.6955216 -4.22776 -5.1177292 -6.7088943 -10.124273 -12.516788 -11.23522][-4.8035493 -4.0614324 -3.9224749 -4.6735916 -4.6650381 -3.9803252 -3.015161 -3.0255466 -3.041384 -3.6932826 -4.4623804 -5.7935762 -8.205759 -10.651661 -10.156439][-2.8037279 -3.6535406 -3.7264028 -3.4088316 -3.6328881 -2.6096883 -1.6726208 -1.279706 -1.3149128 -1.7353518 -2.7001543 -4.9438291 -7.7254019 -9.9743967 -8.5924282][-1.7066226 -2.9072213 -2.948427 -2.375242 -1.3338652 0.40208054 2.1883421 2.3368821 1.7115297 0.43814421 -0.796083 -2.8323698 -5.2197104 -6.6424127 -5.7906222][-1.6412845 -1.8363082 -2.1140881 -1.3740826 -0.77836275 1.7679615 4.6951423 5.8720174 4.843843 1.6611452 -0.70828104 -2.3299584 -3.8177605 -5.4073772 -4.6380682][-2.4825718 -1.8225908 -1.6196795 -0.77693367 0.89519167 3.6607842 6.068296 6.2851262 5.0901742 1.6296816 -1.7069149 -3.8856959 -5.54747 -6.0483661 -4.3158293][-3.6932449 -3.1015635 -2.3823991 -1.1926074 -0.17509556 2.1757388 3.5254731 3.8110995 3.4443316 1.0636344 -0.82027721 -3.5209284 -5.0604968 -5.7825904 -3.8624928][-5.8751664 -5.3993826 -5.2886758 -3.1727157 -1.2183571 -0.33000422 -0.57717228 -0.16217756 -0.24871969 -0.30407667 -0.43006325 -2.3177304 -4.1773138 -5.6339111 -4.4587412][-9.2035885 -9.0757523 -8.5780458 -6.9751272 -6.0518169 -5.9313817 -5.8799691 -6.3392057 -6.7316551 -5.6416273 -3.9922309 -4.5166349 -4.6545238 -5.8624544 -4.1018839][-12.877919 -12.626179 -12.3125 -10.204504 -9.6244907 -9.4976978 -9.5593281 -9.6662331 -9.8333263 -8.4318953 -6.6810913 -5.6189189 -5.5127444 -6.2977848 -4.4079227][-12.597219 -12.75808 -12.123496 -11.51248 -10.957025 -10.460434 -10.42662 -10.282537 -10.032916 -9.8157606 -8.7830172 -8.4261189 -7.6685457 -6.5426679 -2.9818649][-10.264824 -10.570714 -11.652248 -10.658329 -9.691288 -9.16267 -10.003635 -9.5615044 -9.4250479 -9.3217134 -8.8985586 -8.3945446 -8.09755 -7.0871243 -5.7384858][-7.6490049 -7.5654507 -7.3230448 -7.0839138 -5.6342058 -4.8065386 -4.6603107 -5.6328287 -6.1521745 -6.5793567 -7.0753989 -8.3327312 -8.8035164 -8.4276905 -7.1524148]]...]
INFO - root - 2017-12-16 02:28:31.427585: step 87610, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 45h:27m:52s remains)
INFO - root - 2017-12-16 02:28:37.963076: step 87620, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 44h:38m:45s remains)
INFO - root - 2017-12-16 02:28:44.514923: step 87630, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.650 sec/batch; 44h:13m:19s remains)
INFO - root - 2017-12-16 02:28:51.075057: step 87640, loss = 0.17, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 45h:34m:59s remains)
INFO - root - 2017-12-16 02:28:57.592048: step 87650, loss = 0.25, batch loss = 0.21 (12.1 examples/sec; 0.663 sec/batch; 45h:06m:24s remains)
INFO - root - 2017-12-16 02:29:04.139403: step 87660, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.666 sec/batch; 45h:19m:39s remains)
INFO - root - 2017-12-16 02:29:10.725970: step 87670, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.660 sec/batch; 44h:52m:08s remains)
INFO - root - 2017-12-16 02:29:17.282949: step 87680, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 45h:02m:35s remains)
INFO - root - 2017-12-16 02:29:23.816959: step 87690, loss = 0.16, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 44h:00m:04s remains)
INFO - root - 2017-12-16 02:29:30.439190: step 87700, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.676 sec/batch; 45h:56m:28s remains)
2017-12-16 02:29:30.987806: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.98559427 0.54098034 0.957777 1.756958 1.5024962 1.500895 1.0784974 -0.05199194 -1.5535631 -2.9720693 -3.6179457 -5.8116789 -8.6605721 -11.114677 -11.855366][0.73075676 2.1307015 4.4382806 4.9380908 4.1878753 2.9717107 1.5176916 0.27373409 -1.8138709 -3.3634198 -4.6619487 -8.9458914 -12.787397 -15.813158 -16.170904][3.3384757 2.8036809 2.7250972 3.4710507 3.9618764 3.0453134 1.5229716 -0.23352909 -2.37739 -4.6459827 -7.0561533 -10.62874 -14.078886 -18.062399 -19.514511][1.8068757 2.0873284 2.7267613 3.3957372 2.9761462 2.181181 0.71560955 -0.65429831 -2.2771232 -4.57852 -7.0752091 -11.212549 -14.529337 -18.169827 -19.780582][2.3045316 2.3910155 2.1560073 2.1143274 2.1963511 2.3557811 1.926621 0.12696314 -1.9342411 -3.4830296 -5.2423563 -9.3740473 -13.424978 -17.563309 -19.173523][-0.13211775 -0.36148214 0.48971033 3.149838 4.3023934 4.1686444 4.3019252 3.6018414 2.0028763 -0.41151237 -2.4582298 -6.330977 -9.8257751 -12.756392 -14.119631][-0.73124504 -0.41021156 0.66024828 2.781589 4.1488338 5.0243287 5.6297221 4.7978921 3.4853282 1.4529104 -0.37913942 -4.1521163 -7.3959332 -10.375577 -11.475952][-3.7234192 -2.6339934 -0.88207912 1.8969359 4.2797341 6.4434428 7.0353332 5.2621474 3.8443618 2.0260015 0.49120235 -3.1711671 -6.8574314 -10.309834 -11.505459][-3.9778991 -2.9532878 -1.306932 0.76160288 2.7925639 5.0546556 6.0823321 4.2836404 2.4184508 0.59293032 -0.54898405 -3.2814822 -6.293663 -9.9398146 -11.938747][-4.9043055 -4.1494012 -3.2008703 -1.0303006 0.62997437 2.2044921 3.4038434 3.1868787 2.2318697 0.26188135 -0.97164583 -4.2043276 -7.4982791 -11.1418 -12.696924][-10.731024 -10.09593 -9.0942783 -6.4792204 -4.8422723 -3.0441067 -1.7181907 -1.5328689 -1.118382 -1.9204435 -2.8420534 -6.691781 -9.1602612 -11.960273 -12.42944][-12.689881 -13.058284 -12.803616 -10.705155 -9.738925 -8.0457172 -7.5494242 -7.3914261 -6.8515272 -7.0896969 -6.8572421 -8.1136913 -9.1512756 -12.18876 -13.052912][-12.480206 -13.348088 -13.76317 -12.752099 -12.222545 -10.361784 -10.590798 -11.054108 -11.138702 -10.637433 -9.8459492 -10.752843 -11.079208 -13.062168 -12.263052][-8.9012108 -9.360342 -9.6398811 -10.642836 -11.288395 -10.918732 -11.380357 -10.558163 -11.084211 -11.508942 -11.594601 -11.214191 -10.349943 -11.167084 -11.477253][-6.7325473 -6.8725262 -6.5240564 -7.0491071 -6.9310226 -7.3746748 -8.6328278 -9.3243046 -10.292194 -10.138638 -10.145485 -11.194229 -11.884631 -12.254896 -11.740162]]...]
INFO - root - 2017-12-16 02:29:37.493494: step 87710, loss = 0.14, batch loss = 0.09 (12.8 examples/sec; 0.627 sec/batch; 42h:36m:34s remains)
INFO - root - 2017-12-16 02:29:44.094485: step 87720, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 45h:35m:54s remains)
INFO - root - 2017-12-16 02:29:50.558426: step 87730, loss = 0.13, batch loss = 0.09 (12.7 examples/sec; 0.628 sec/batch; 42h:42m:08s remains)
INFO - root - 2017-12-16 02:29:57.023749: step 87740, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 45h:51m:27s remains)
INFO - root - 2017-12-16 02:30:03.549801: step 87750, loss = 0.17, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 45h:01m:14s remains)
INFO - root - 2017-12-16 02:30:10.292197: step 87760, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.676 sec/batch; 45h:56m:44s remains)
INFO - root - 2017-12-16 02:30:16.865738: step 87770, loss = 0.20, batch loss = 0.15 (12.5 examples/sec; 0.641 sec/batch; 43h:33m:41s remains)
INFO - root - 2017-12-16 02:30:23.389498: step 87780, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.669 sec/batch; 45h:30m:23s remains)
INFO - root - 2017-12-16 02:30:29.920040: step 87790, loss = 0.16, batch loss = 0.12 (12.7 examples/sec; 0.630 sec/batch; 42h:50m:05s remains)
INFO - root - 2017-12-16 02:30:36.484659: step 87800, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 43h:44m:57s remains)
2017-12-16 02:30:37.050488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1503887 -5.193471 -5.7429271 -5.5803041 -5.4090986 -5.5142694 -6.0894842 -6.5972266 -7.0282764 -8.3478394 -8.5580559 -9.344697 -10.658007 -10.79021 -8.76286][-4.9968019 -4.1823673 -4.2442894 -4.3780584 -4.7246418 -4.8799195 -4.6884513 -5.47838 -6.5566993 -7.465097 -7.3288236 -8.6040888 -10.149413 -10.365128 -8.44549][-4.7790732 -4.7196741 -5.5777426 -4.4858794 -4.1418977 -4.3901577 -5.0368209 -5.4444509 -5.9156294 -6.7582459 -6.5595903 -7.2082963 -7.8490849 -7.8209963 -6.6676192][-6.1881061 -6.1578894 -6.5215549 -5.7909017 -5.9962974 -4.6102791 -3.7321315 -3.9136295 -4.9409137 -5.851903 -6.0334773 -6.9443626 -7.6287866 -7.6082363 -6.0388656][-7.3830132 -7.5910559 -7.4039607 -6.6002336 -5.5604048 -2.7840807 -1.5287571 -2.0538368 -2.5423114 -3.0062523 -3.547539 -5.3105617 -6.8640842 -7.234539 -6.1086931][-8.4353476 -7.8845396 -6.9851842 -4.5677519 -1.8670065 1.8186731 3.4807 2.1925464 0.47618008 -1.5158224 -3.0823183 -4.4123912 -6.5653334 -7.8217096 -7.5515037][-10.205679 -8.158783 -6.2751408 -3.4379561 -0.32927418 3.2510171 6.1091037 6.8708806 5.3276267 1.4340911 -1.4265761 -3.7548246 -6.6451688 -7.6662083 -7.8269424][-10.843168 -8.9425068 -6.6210957 -2.591615 0.76940584 5.2109 7.8907161 7.9951892 7.3209472 3.2986073 -1.2923708 -5.0516725 -7.6633215 -8.9875908 -8.0529041][-11.862524 -10.276451 -7.2447891 -3.49542 -0.90047884 2.8559566 5.6347842 6.0780358 4.4956326 0.74490833 -2.5323787 -6.3737803 -9.9244423 -10.433373 -8.503912][-9.9784889 -8.9268932 -6.9221759 -4.2003098 -2.9801669 -0.45198107 2.127234 2.9168239 1.4176631 -2.1929593 -5.8256311 -8.8040953 -11.133793 -10.450038 -8.4542665][-12.206303 -10.882992 -8.5707884 -5.9411345 -5.0973353 -5.1775465 -4.8709927 -4.5268822 -4.1498508 -6.3105693 -9.0742435 -12.04454 -12.42002 -10.810768 -7.4558067][-12.801349 -11.958766 -10.424168 -8.4357948 -7.8978767 -7.4987307 -7.3031373 -8.0418663 -8.29538 -9.2381754 -10.515116 -12.236565 -12.075975 -10.447369 -7.5328321][-12.733387 -10.720085 -9.9363785 -8.6503391 -8.6763926 -8.3848038 -8.1115932 -8.3180943 -8.7706347 -8.3938837 -8.751421 -9.7990952 -9.7463379 -8.2188759 -6.0939989][-9.76952 -8.664834 -7.4225397 -7.3547721 -7.7109218 -7.6186161 -8.1812744 -8.28149 -7.1765289 -7.11086 -6.8962164 -6.6381111 -5.8241715 -5.8274794 -5.3952584][-6.820837 -6.7301288 -5.9975519 -5.4761062 -5.4141684 -5.8200183 -5.8082881 -5.7515683 -6.1027684 -5.5772643 -4.9549017 -5.1893392 -5.1625385 -5.2527852 -6.1634855]]...]
INFO - root - 2017-12-16 02:30:43.736961: step 87810, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 44h:36m:21s remains)
INFO - root - 2017-12-16 02:30:50.387053: step 87820, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.653 sec/batch; 44h:22m:55s remains)
INFO - root - 2017-12-16 02:30:56.929406: step 87830, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.683 sec/batch; 46h:24m:04s remains)
INFO - root - 2017-12-16 02:31:03.605997: step 87840, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.661 sec/batch; 44h:56m:40s remains)
INFO - root - 2017-12-16 02:31:10.163099: step 87850, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.661 sec/batch; 44h:54m:20s remains)
INFO - root - 2017-12-16 02:31:16.719697: step 87860, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 43h:22m:34s remains)
INFO - root - 2017-12-16 02:31:23.338069: step 87870, loss = 0.16, batch loss = 0.11 (12.7 examples/sec; 0.632 sec/batch; 42h:57m:37s remains)
INFO - root - 2017-12-16 02:31:29.877730: step 87880, loss = 0.19, batch loss = 0.15 (12.3 examples/sec; 0.649 sec/batch; 44h:07m:10s remains)
INFO - root - 2017-12-16 02:31:36.451975: step 87890, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 45h:17m:56s remains)
INFO - root - 2017-12-16 02:31:43.082474: step 87900, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 43h:47m:24s remains)
2017-12-16 02:31:43.578934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7925615 -6.6022162 -6.0498085 -5.9041557 -6.3537846 -7.1353874 -7.5475035 -7.3287416 -6.0889926 -4.0979595 -2.039115 -2.5500507 -4.1661963 -5.4911752 -5.9184504][-7.0747361 -6.97434 -6.0202394 -5.584589 -6.203361 -7.4777131 -8.3570442 -8.4901772 -7.3545232 -5.5920677 -4.070375 -4.4248037 -6.10542 -7.2993636 -7.0035415][-6.2737174 -6.4109287 -6.8000054 -6.76175 -6.6251936 -7.3420906 -7.760128 -7.5358076 -6.6188574 -5.6110296 -4.5811062 -6.2343311 -8.7964125 -10.071795 -10.003771][-7.6769371 -7.6882048 -6.2863317 -6.2769217 -6.5034714 -6.5047455 -6.5253315 -5.8921666 -5.2537756 -4.8323183 -4.5305047 -6.8801479 -9.9332361 -11.806995 -11.988407][-9.0258255 -8.8542643 -7.6198812 -5.7350831 -4.37816 -3.1432 -2.4485562 -2.6228166 -3.0924513 -2.9867041 -3.6169803 -6.6524591 -10.062636 -12.233932 -12.06535][-10.095429 -9.0669355 -5.8587494 -3.5658352 -1.7243121 0.34264135 1.4541731 1.64427 1.0060239 -0.48809052 -2.2599375 -5.74286 -9.3327312 -11.09572 -10.996614][-9.2009115 -7.6529346 -4.4470086 -1.7074809 0.029669762 2.308639 3.7912774 4.4551749 4.498445 2.5826535 -0.20331335 -4.2759933 -8.1444206 -9.8920956 -9.8343716][-7.5557833 -6.2443118 -3.0331581 -0.43519974 1.2792492 3.1915851 4.4048724 5.1991363 5.6152396 3.9329562 2.3294005 -1.7040076 -6.1308985 -8.0532341 -7.8538809][-5.4613752 -4.7939878 -3.3433149 -1.3164501 -0.37815571 1.1630201 3.2905145 4.2332969 4.4675364 3.6418157 2.1155934 -1.4216776 -4.8738775 -6.3110242 -6.0866237][-3.8345675 -3.856379 -2.9913809 -2.1845431 -2.0726063 -0.42344809 1.2218142 2.0054197 2.22469 1.3212137 0.48911953 -1.1246314 -3.232049 -4.5552378 -4.9750524][-8.2714224 -7.2576561 -5.8772645 -5.072834 -4.5872717 -3.813463 -3.3792076 -3.5713089 -3.6284275 -3.9310045 -4.1567588 -5.3565459 -5.6542645 -5.3408766 -4.2283473][-11.540491 -10.583521 -8.6369343 -7.3825822 -7.8200779 -7.8830576 -7.9783258 -8.1688662 -7.7522097 -7.7987471 -7.7352295 -7.9114351 -8.1817884 -7.3045464 -5.3736711][-11.807707 -10.032501 -8.9734917 -7.9310503 -7.8232927 -7.6760659 -7.4689317 -7.6765862 -7.9007015 -7.3860559 -6.8413405 -7.5467153 -7.9125676 -6.6659713 -4.6992936][-9.4998388 -8.5354109 -7.5429363 -5.9049873 -5.878458 -5.9876976 -6.6455941 -7.1511464 -7.4514675 -7.0924411 -6.9529762 -6.8830986 -6.6244283 -5.9939713 -5.2882261][-7.2542276 -7.6741219 -7.2014642 -5.5019355 -5.1433153 -4.3063049 -4.2015843 -4.6729565 -5.4395032 -5.2606316 -5.4752755 -6.5312939 -7.553556 -8.1685181 -8.2831755]]...]
INFO - root - 2017-12-16 02:31:50.184639: step 87910, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 44h:32m:00s remains)
INFO - root - 2017-12-16 02:31:56.724340: step 87920, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.657 sec/batch; 44h:39m:57s remains)
INFO - root - 2017-12-16 02:32:03.281051: step 87930, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.685 sec/batch; 46h:33m:09s remains)
INFO - root - 2017-12-16 02:32:09.901834: step 87940, loss = 0.18, batch loss = 0.13 (12.2 examples/sec; 0.653 sec/batch; 44h:22m:16s remains)
INFO - root - 2017-12-16 02:32:16.585635: step 87950, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.658 sec/batch; 44h:43m:04s remains)
INFO - root - 2017-12-16 02:32:23.254412: step 87960, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.686 sec/batch; 46h:33m:59s remains)
INFO - root - 2017-12-16 02:32:29.785574: step 87970, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 44h:15m:44s remains)
INFO - root - 2017-12-16 02:32:36.287952: step 87980, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 43h:49m:53s remains)
INFO - root - 2017-12-16 02:32:42.785190: step 87990, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.635 sec/batch; 43h:07m:29s remains)
INFO - root - 2017-12-16 02:32:49.378323: step 88000, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 44h:40m:27s remains)
2017-12-16 02:32:49.885369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1842289 -4.4861822 -4.4791393 -4.243721 -4.868228 -5.2012358 -5.0182352 -4.6491084 -4.632556 -4.2236891 -4.473166 -7.3866563 -10.162267 -9.8127174 -8.6770449][-6.8447928 -5.528419 -5.146852 -4.1123347 -4.2921777 -4.7904949 -4.5263486 -4.1860514 -3.8012064 -3.363266 -3.4729536 -6.3480721 -9.1832294 -9.8251743 -9.9959259][-5.2174368 -5.3347468 -5.3360715 -4.4717937 -4.2713041 -3.1299875 -2.295716 -2.4874284 -2.0438366 -2.5694265 -3.2323558 -5.5524893 -7.742898 -8.2129364 -8.189003][-5.1291027 -5.3589134 -4.4795952 -3.149821 -3.411952 -2.5876553 -1.6932645 -1.4648476 -1.8890898 -2.1312494 -2.4934311 -4.6105604 -6.7152338 -7.3326659 -7.7413645][-4.9303741 -5.1398048 -4.6774511 -3.864574 -3.1815829 -1.1892467 0.32652617 0.18150949 0.022062778 -1.4008636 -2.3436241 -4.2988348 -6.0277119 -5.77999 -5.7443929][-6.5336361 -6.15508 -4.2940855 -1.4414511 0.016321659 1.3864789 2.290792 2.4407678 1.405426 -0.15878487 -1.2046952 -2.9923241 -4.8556213 -5.3047338 -4.9581566][-8.0942011 -7.1985865 -5.0635419 -2.3693254 -0.25802422 2.5759425 4.3627715 5.4908938 5.2618432 2.4593635 0.079880238 -2.9601731 -5.5667591 -5.6176176 -5.3033481][-7.9019136 -7.8107681 -6.1251807 -2.6170039 0.093000889 3.8126655 6.0125833 5.8866773 5.401855 3.7404447 1.397572 -2.8459208 -5.7776761 -6.2330155 -6.3338609][-6.7537928 -6.564291 -5.1122022 -2.8030229 -0.82748652 2.0904875 3.5988202 4.8726792 5.0884824 3.6808515 1.607676 -2.7064924 -6.4616 -6.9130926 -6.8965607][-5.2703876 -4.9120121 -4.5957956 -2.5302927 -1.5101771 0.54495096 2.2900767 2.7942786 2.6418891 1.6453409 0.22317219 -3.4518974 -6.8251438 -8.2396116 -9.084321][-7.1045313 -6.4788752 -5.3341737 -4.102458 -3.5392456 -2.98279 -3.1375315 -3.0769897 -2.9506001 -2.7023406 -3.2496874 -6.826118 -9.1524458 -9.1897526 -9.0270157][-10.470509 -8.573699 -6.4020329 -5.4379034 -4.9981675 -4.7831187 -5.1266661 -6.1052527 -6.9048524 -7.4856887 -7.901444 -8.9529438 -9.6966944 -9.8248959 -10.096145][-12.068943 -9.2482395 -7.2458205 -5.7661138 -6.2282019 -6.4742041 -7.5551009 -8.4291477 -9.236927 -9.9284353 -10.301266 -10.778345 -10.980912 -9.1438332 -7.5154085][-9.1100006 -8.6599369 -7.2265849 -6.2680774 -5.6271863 -5.9771409 -6.6342597 -7.4911995 -8.4973621 -9.4334106 -9.96242 -8.8413754 -7.5183535 -6.875905 -6.2794371][-6.080842 -5.7317762 -5.0106368 -4.0553522 -3.8277979 -4.477809 -5.4735432 -6.53386 -7.4521327 -7.8860989 -7.87827 -8.360322 -8.3913546 -7.2314129 -6.5622339]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 02:32:56.456520: step 88010, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 44h:31m:39s remains)
INFO - root - 2017-12-16 02:33:03.025444: step 88020, loss = 0.18, batch loss = 0.13 (12.3 examples/sec; 0.648 sec/batch; 44h:01m:42s remains)
INFO - root - 2017-12-16 02:33:09.618450: step 88030, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.675 sec/batch; 45h:49m:56s remains)
INFO - root - 2017-12-16 02:33:16.141003: step 88040, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.640 sec/batch; 43h:26m:10s remains)
INFO - root - 2017-12-16 02:33:22.719769: step 88050, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.682 sec/batch; 46h:20m:27s remains)
INFO - root - 2017-12-16 02:33:29.290989: step 88060, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.634 sec/batch; 43h:01m:22s remains)
INFO - root - 2017-12-16 02:33:35.883940: step 88070, loss = 0.14, batch loss = 0.09 (12.7 examples/sec; 0.630 sec/batch; 42h:48m:17s remains)
INFO - root - 2017-12-16 02:33:42.467829: step 88080, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 45h:05m:34s remains)
INFO - root - 2017-12-16 02:33:48.982759: step 88090, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 44h:49m:33s remains)
INFO - root - 2017-12-16 02:33:55.508638: step 88100, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.642 sec/batch; 43h:33m:14s remains)
2017-12-16 02:33:56.046377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6423626 -6.1443133 -6.6082435 -6.1392574 -6.1468577 -6.8243065 -6.320787 -6.1121521 -5.1183214 -4.1528797 -3.96726 -4.8930907 -7.396574 -8.3278255 -9.6441927][-3.3742466 -3.1864634 -2.5975673 -2.727963 -3.5048757 -4.6103282 -4.96229 -4.925746 -4.4703808 -3.6586556 -2.8184378 -3.7346976 -5.7857838 -8.2937059 -10.154865][-1.523777 -2.5633574 -2.4113641 -3.043865 -3.7502346 -4.5723147 -4.7503366 -3.6192117 -2.3030968 -2.2045698 -1.3626862 -3.4152019 -6.0416942 -8.7922268 -10.619465][-2.5348277 -3.2308295 -3.1783612 -4.1124454 -4.2621427 -4.5809908 -4.1569157 -3.5386326 -2.343004 -1.3933005 -1.2672086 -3.2818425 -6.1172576 -9.4614077 -11.99814][-2.4761171 -4.2589746 -5.7122579 -6.1537018 -5.2459884 -4.69691 -2.4378936 -2.4004767 -2.5267088 -2.2065222 -2.007967 -3.5690579 -5.5983253 -7.9039855 -10.114258][-3.5399172 -4.3090081 -4.7724056 -4.6493697 -3.2526903 -1.6663184 1.5546622 1.8265142 0.33288288 -1.4045253 -1.9682448 -3.2751 -5.1256371 -6.7241254 -8.524539][-5.0281787 -5.29426 -4.8466506 -3.2528827 -1.7080717 0.66282558 4.0314984 4.8302979 4.901793 2.3555379 -0.25899458 -2.5152085 -4.9417644 -5.8573518 -7.5828285][-4.5979004 -5.4444776 -3.7054687 -1.79214 -0.5489049 3.7201924 6.440002 6.0359759 7.2020431 5.8065581 2.2333612 -1.6829457 -5.9037061 -7.0665817 -8.1414757][-5.132184 -4.5416207 -3.6543577 -2.6452498 -0.75776768 2.8423257 4.687345 5.0607629 4.8019929 2.8479171 2.636682 -0.96652889 -6.3264089 -8.7550564 -11.076283][-5.6710362 -5.4652328 -3.2208667 -3.1738563 -2.2868991 0.71852827 1.7390137 2.4182496 1.3732872 -0.8528533 -1.4160094 -5.3423743 -9.0519485 -11.477915 -14.474283][-8.5996408 -9.0040073 -9.0810652 -8.0882826 -6.0024514 -4.5206404 -3.3535869 -2.9253225 -4.9528084 -5.3295364 -5.6979909 -9.7765579 -13.196322 -14.580809 -15.423683][-12.74542 -12.382271 -11.004648 -11.394063 -9.937685 -8.5536709 -7.912231 -7.998682 -8.6743584 -9.7486458 -10.678762 -11.194578 -12.665994 -14.502899 -14.669554][-17.092062 -17.296787 -16.025635 -13.801865 -13.103323 -12.938263 -11.479157 -12.226953 -13.006147 -14.24621 -13.898201 -13.379184 -13.295554 -12.076429 -9.6283464][-15.616489 -15.90626 -15.137169 -14.370106 -13.207439 -12.472409 -11.678892 -11.686243 -11.617659 -11.162558 -12.374119 -12.491117 -9.6901064 -9.8513851 -7.5887165][-10.132633 -11.350587 -10.345306 -8.9904251 -8.2998075 -9.0896854 -9.576807 -9.6909809 -10.659042 -9.6877136 -8.1597309 -8.9282808 -9.388936 -9.85954 -8.8735771]]...]
INFO - root - 2017-12-16 02:34:02.596019: step 88110, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.636 sec/batch; 43h:10m:24s remains)
INFO - root - 2017-12-16 02:34:09.229195: step 88120, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 44h:10m:32s remains)
INFO - root - 2017-12-16 02:34:15.818131: step 88130, loss = 0.17, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 45h:38m:11s remains)
INFO - root - 2017-12-16 02:34:22.379381: step 88140, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.633 sec/batch; 42h:59m:12s remains)
INFO - root - 2017-12-16 02:34:28.929888: step 88150, loss = 0.15, batch loss = 0.11 (11.3 examples/sec; 0.707 sec/batch; 48h:00m:56s remains)
INFO - root - 2017-12-16 02:34:35.447488: step 88160, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.684 sec/batch; 46h:23m:31s remains)
INFO - root - 2017-12-16 02:34:42.056501: step 88170, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.665 sec/batch; 45h:08m:49s remains)
INFO - root - 2017-12-16 02:34:48.681300: step 88180, loss = 0.16, batch loss = 0.11 (11.6 examples/sec; 0.687 sec/batch; 46h:36m:59s remains)
INFO - root - 2017-12-16 02:34:55.301879: step 88190, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 45h:18m:37s remains)
INFO - root - 2017-12-16 02:35:01.872546: step 88200, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.638 sec/batch; 43h:17m:18s remains)
2017-12-16 02:35:02.394928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1885662 -3.6547937 -2.4717259 -1.5990334 -2.1161015 -3.1431336 -4.09848 -4.7495027 -5.0266423 -3.8020034 -3.3273153 -6.3467536 -8.6735926 -9.3133869 -9.4527121][-3.6181879 -3.5071774 -2.8063643 -2.7024338 -3.0695047 -3.4861143 -3.5447278 -4.2378511 -4.1676788 -3.409339 -2.1204815 -5.3930554 -8.6946259 -8.7099495 -9.03971][-2.9482291 -3.8589172 -3.7877367 -3.5908811 -3.3634725 -3.533623 -3.5807571 -4.306406 -4.5408578 -3.6815641 -2.7302003 -6.1885471 -9.6420746 -9.7465744 -9.2954035][-4.740602 -5.219502 -4.5051918 -4.743546 -4.4876251 -4.2213211 -3.6588705 -3.7157509 -4.00476 -3.9479465 -3.4309361 -6.3962712 -8.5946522 -9.9594727 -10.905573][-4.8265123 -6.6581321 -6.8254495 -5.3711982 -4.1738749 -3.2682073 -2.4976873 -2.7630816 -3.2702396 -3.537365 -3.5073261 -7.1304488 -9.3623638 -10.374756 -11.223507][-6.3020782 -6.793952 -7.0046029 -5.908658 -3.3993521 -1.0682597 1.2039371 1.242928 -0.00039815903 -0.99912262 -1.4337616 -5.4192376 -7.6272941 -8.4846144 -9.1101475][-5.8893251 -6.479732 -5.059783 -2.77988 -0.45367527 1.5784936 3.3832765 3.2145247 2.8706021 1.1547165 -0.33367252 -3.9408474 -6.3486357 -7.4022875 -8.4292574][-5.3192282 -5.4456096 -4.361176 -1.8341794 0.9215889 3.452126 5.4954686 5.5367579 4.4631267 2.4695792 0.86003828 -3.4223506 -5.7971044 -6.52012 -6.7627645][-6.1472521 -5.0111823 -3.9620781 -1.9241271 -0.19804287 2.4065938 4.2854991 4.3063312 4.0018315 2.2625084 0.18719578 -4.1677427 -7.6903434 -8.6563511 -8.8541555][-6.3499584 -6.8927121 -6.18887 -3.9204597 -2.819346 -1.0332384 0.57911348 1.1258793 1.1981506 0.61450291 -1.0575714 -5.2493234 -8.2280807 -9.4706554 -9.7614651][-10.016172 -9.9947586 -8.2451134 -6.752593 -6.5438743 -5.4701648 -4.83856 -3.9380674 -3.5940912 -3.805594 -4.3962364 -7.9588361 -10.506763 -11.896958 -10.837894][-11.733505 -11.261923 -10.74123 -8.7068882 -7.1014929 -6.172646 -6.0476036 -7.0510707 -7.7333641 -7.3104563 -6.9882455 -8.6695833 -10.244209 -11.058475 -10.223154][-12.541986 -12.522902 -10.527159 -9.4782009 -8.3115759 -6.8949203 -7.7040739 -8.1273651 -8.5023451 -9.5558481 -9.4258652 -9.709012 -9.949482 -9.5952406 -9.2121258][-13.01058 -12.767843 -12.135489 -9.5064468 -7.5235906 -6.3555441 -5.1455197 -6.3382196 -7.6929455 -7.801168 -7.8784828 -7.4673295 -7.2758789 -7.9440436 -6.5636091][-8.8698025 -10.27852 -10.170141 -8.534853 -6.8230562 -6.2457652 -6.6775508 -6.8915415 -6.4034019 -6.7834015 -6.5564818 -6.5991006 -6.9858274 -7.4871063 -8.2101135]]...]
INFO - root - 2017-12-16 02:35:09.017792: step 88210, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 45h:09m:57s remains)
INFO - root - 2017-12-16 02:35:15.618268: step 88220, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 43h:59m:41s remains)
INFO - root - 2017-12-16 02:35:22.208195: step 88230, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.670 sec/batch; 45h:29m:36s remains)
INFO - root - 2017-12-16 02:35:28.762078: step 88240, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 45h:04m:48s remains)
INFO - root - 2017-12-16 02:35:35.295081: step 88250, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.637 sec/batch; 43h:14m:58s remains)
INFO - root - 2017-12-16 02:35:41.902998: step 88260, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 44h:31m:27s remains)
INFO - root - 2017-12-16 02:35:48.456531: step 88270, loss = 0.20, batch loss = 0.15 (12.7 examples/sec; 0.632 sec/batch; 42h:51m:30s remains)
INFO - root - 2017-12-16 02:35:55.039684: step 88280, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 43h:48m:25s remains)
INFO - root - 2017-12-16 02:36:01.684869: step 88290, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 45h:14m:21s remains)
INFO - root - 2017-12-16 02:36:08.354515: step 88300, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.699 sec/batch; 47h:23m:30s remains)
2017-12-16 02:36:08.943232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.26758 -6.4354334 -6.9091625 -6.5739975 -6.4543896 -7.8891563 -8.7743254 -7.5511551 -6.616467 -5.9957681 -5.6344867 -6.4819689 -7.1094184 -5.1415548 -4.0401096][-6.2910409 -5.9523959 -4.1871233 -4.7739315 -5.4615 -6.1237488 -6.6629944 -5.9093466 -5.2736998 -4.8685474 -4.9187098 -5.8528433 -5.8737 -4.7790089 -4.9262433][-4.87193 -6.0110712 -4.7192621 -4.1395383 -4.6896448 -5.1414433 -5.3247643 -4.6787581 -3.9370203 -3.5234354 -3.7189937 -5.86356 -6.6841736 -5.0119276 -4.6891832][-6.587224 -6.6860976 -5.705215 -4.5395908 -3.6863568 -4.4453249 -4.9806461 -4.7817159 -3.956656 -3.2345517 -3.3431342 -6.5385857 -8.3877287 -8.3595867 -8.943511][-9.22333 -9.11516 -8.1172676 -5.5407195 -3.5389876 -2.7388108 -2.6152127 -3.1032372 -3.7371421 -2.5816646 -2.6926095 -6.256062 -9.323452 -9.4218712 -10.665295][-11.397505 -10.7138 -8.1172905 -3.3018339 -0.085136414 1.5575719 2.6565423 1.0794368 0.13174009 0.011646271 -2.100745 -4.5723062 -6.5568495 -8.91579 -11.023767][-9.6602926 -9.9226456 -7.1753836 -2.5387728 0.73317146 3.4787908 5.8662324 4.8084149 4.1733222 2.2718315 -1.3681412 -4.7641211 -7.9622293 -9.4199171 -10.861786][-11.049333 -9.9061565 -6.7957177 -2.0821118 1.0864801 3.4735093 5.78781 4.4732823 3.6739354 1.7433424 -0.4773531 -4.5140333 -8.72826 -10.45327 -11.442837][-10.029202 -9.51306 -6.9279976 -3.5873866 -0.45220947 2.69381 4.9219365 4.12085 3.5479798 1.6523695 -0.45596552 -5.0314894 -9.8103132 -10.984994 -12.130352][-8.0640783 -8.5997972 -7.9776645 -4.6482873 -1.5219631 0.986022 3.4454541 3.4192672 2.7475553 0.82881737 -1.9029357 -6.3583326 -10.521999 -11.868132 -14.067062][-9.6081667 -10.159676 -9.6492825 -8.2980986 -6.6373162 -4.8986382 -3.3311775 -3.101757 -2.7771878 -3.549535 -5.2598052 -10.451385 -14.015408 -14.691185 -14.870296][-14.273695 -14.388254 -13.166367 -11.481701 -10.156943 -9.5066147 -8.8939362 -9.3921051 -9.4010248 -9.1181812 -9.8249445 -12.809874 -14.620798 -16.04007 -17.546944][-14.996284 -14.5317 -14.016197 -13.754044 -12.784895 -11.230253 -9.8166094 -10.261442 -11.092032 -11.299804 -11.444621 -12.965887 -13.864845 -14.072363 -14.039215][-12.235201 -12.071314 -11.903671 -11.14601 -9.83634 -9.6998825 -9.8747368 -9.40379 -9.20832 -9.7666321 -10.621061 -10.914075 -11.623492 -12.688445 -12.6171][-10.677225 -9.2509985 -8.5787926 -7.2643542 -6.5380826 -6.9599652 -7.6917534 -8.4520512 -8.7676067 -8.4020443 -8.4581661 -9.8281527 -11.418238 -11.12267 -11.404172]]...]
INFO - root - 2017-12-16 02:36:15.500842: step 88310, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 44h:23m:55s remains)
INFO - root - 2017-12-16 02:36:22.054074: step 88320, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.644 sec/batch; 43h:42m:42s remains)
INFO - root - 2017-12-16 02:36:28.628834: step 88330, loss = 0.12, batch loss = 0.08 (12.5 examples/sec; 0.642 sec/batch; 43h:33m:51s remains)
INFO - root - 2017-12-16 02:36:35.366246: step 88340, loss = 0.21, batch loss = 0.16 (11.9 examples/sec; 0.673 sec/batch; 45h:39m:27s remains)
INFO - root - 2017-12-16 02:36:41.951097: step 88350, loss = 0.13, batch loss = 0.09 (12.6 examples/sec; 0.633 sec/batch; 42h:55m:40s remains)
INFO - root - 2017-12-16 02:36:48.533394: step 88360, loss = 0.13, batch loss = 0.08 (11.8 examples/sec; 0.681 sec/batch; 46h:10m:09s remains)
INFO - root - 2017-12-16 02:36:55.158902: step 88370, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.653 sec/batch; 44h:16m:41s remains)
INFO - root - 2017-12-16 02:37:01.847163: step 88380, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 46h:01m:47s remains)
INFO - root - 2017-12-16 02:37:08.385731: step 88390, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 44h:46m:55s remains)
INFO - root - 2017-12-16 02:37:14.914567: step 88400, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.662 sec/batch; 44h:54m:11s remains)
2017-12-16 02:37:15.452867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2674117 -4.719955 -3.7790473 -3.8078122 -4.2570915 -4.1448655 -3.6111538 -3.1070154 -2.7062016 -2.3917019 -2.7004488 -5.5494728 -6.3700514 -6.8441453 -7.1293988][-5.8935647 -5.4698644 -4.7096777 -4.0141559 -4.2791986 -4.6008854 -4.3251905 -3.5537527 -2.8465531 -2.3550243 -2.2945921 -4.427608 -5.4742441 -6.1161313 -6.0887709][-5.712996 -5.3405838 -4.1654682 -3.7823989 -3.7719641 -3.8687408 -3.8451028 -3.4515882 -3.2918274 -3.2888474 -3.1776483 -4.9354639 -6.0005565 -6.1285257 -5.8166924][-4.0627623 -4.1315289 -3.4023867 -2.7395141 -2.7474761 -3.1067228 -3.2218285 -3.5446825 -3.9494891 -3.5530434 -3.1726868 -5.56624 -6.5579877 -6.5778646 -6.5936394][-3.7959023 -3.4770095 -3.1942391 -2.731708 -2.8056557 -2.4565027 -2.066541 -1.9364548 -1.9308505 -2.5011208 -3.0584002 -5.3267846 -6.4395127 -6.893755 -6.6668229][-5.0799351 -4.380734 -2.8859091 -2.4399133 -1.98424 -1.0828881 -0.30582237 0.3462882 0.28713512 -0.53279161 -1.4112129 -4.3110747 -5.7236519 -6.0563388 -6.4602866][-6.5804076 -5.6166058 -3.5045047 -1.6718798 -0.46329308 1.1680408 2.4512286 2.9033074 3.0750422 1.8610806 0.35537624 -3.0861576 -5.3762364 -5.8589334 -6.1359644][-6.3239579 -5.4597511 -3.3497496 -1.1728406 0.83291817 2.6781306 3.6947589 3.9348607 3.9236064 3.3365264 2.3741927 -1.766701 -4.8774419 -5.7814317 -6.0537391][-4.9904461 -4.8324223 -4.1131516 -1.4313917 0.2980485 2.1157966 3.1789451 3.9104629 4.35654 3.242722 1.9886575 -1.3272991 -4.0826573 -5.5013833 -6.1418781][-5.3870821 -4.9659419 -3.7879827 -1.8576555 -0.071934223 1.6871572 2.223568 2.4530339 2.5517573 1.9898047 1.209095 -2.3974836 -5.0160327 -6.3878 -7.2048659][-7.337873 -6.1780615 -4.5819941 -2.4987955 -1.4490008 -0.43171692 -0.10078859 -0.71392632 -1.3587971 -1.9930594 -2.7773802 -6.1568608 -8.0528164 -7.8902559 -8.2112312][-10.078321 -7.9883747 -5.928267 -3.4457543 -2.1474562 -2.6695573 -3.4246161 -4.1235414 -4.986042 -5.6410313 -6.3030596 -8.5252514 -9.5826874 -8.9498005 -8.1684647][-10.653721 -8.1229086 -4.9360046 -4.5116405 -4.7950988 -4.62652 -4.2859936 -5.2895246 -6.148345 -7.0350494 -8.0332222 -8.7670822 -8.7769451 -7.2861133 -6.2360964][-10.616449 -9.0150213 -6.68427 -4.5287685 -3.6820438 -4.803782 -5.4400263 -5.4247556 -5.2485933 -5.8476591 -6.3043804 -6.2574654 -6.6116695 -6.3777194 -5.3941607][-8.3139715 -7.7909689 -6.3489895 -4.6813612 -3.9758339 -4.457057 -4.6192446 -5.3086557 -5.4221396 -4.5923023 -4.1700296 -5.0694647 -5.9893336 -6.2932949 -6.455318]]...]
INFO - root - 2017-12-16 02:37:22.022447: step 88410, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.640 sec/batch; 43h:23m:11s remains)
INFO - root - 2017-12-16 02:37:28.634880: step 88420, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.659 sec/batch; 44h:41m:20s remains)
INFO - root - 2017-12-16 02:37:35.131374: step 88430, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.682 sec/batch; 46h:15m:40s remains)
INFO - root - 2017-12-16 02:37:41.685251: step 88440, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 43h:53m:35s remains)
INFO - root - 2017-12-16 02:37:48.276479: step 88450, loss = 0.16, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 46h:13m:06s remains)
INFO - root - 2017-12-16 02:37:54.843530: step 88460, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 44h:25m:27s remains)
INFO - root - 2017-12-16 02:38:01.374636: step 88470, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.652 sec/batch; 44h:12m:52s remains)
INFO - root - 2017-12-16 02:38:08.018253: step 88480, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 45h:54m:48s remains)
INFO - root - 2017-12-16 02:38:14.599587: step 88490, loss = 0.15, batch loss = 0.11 (11.4 examples/sec; 0.702 sec/batch; 47h:36m:08s remains)
INFO - root - 2017-12-16 02:38:21.090322: step 88500, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 44h:19m:55s remains)
2017-12-16 02:38:21.652722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4390974 -5.2570066 -4.5526929 -4.2833261 -4.6482539 -5.1082468 -5.6331859 -5.7652264 -5.9379687 -6.5687385 -7.1607075 -9.1541138 -11.0674 -11.57841 -10.475292][-7.2207241 -7.1658797 -6.542428 -6.0809736 -6.0939846 -6.8650985 -7.5926304 -8.4897614 -9.2929611 -9.2567177 -9.3796368 -11.225829 -13.073555 -13.65062 -11.653286][-5.1371169 -5.8802972 -6.2529025 -5.6087465 -6.2357392 -7.3257532 -7.9731545 -7.9298544 -7.8278403 -8.3907232 -9.0437508 -10.770367 -12.913952 -14.474012 -13.71278][-6.1583233 -5.8136325 -5.5060854 -4.6885834 -5.2419109 -5.2990265 -5.6025267 -6.7570987 -7.0882444 -6.3222542 -6.0448475 -8.92714 -12.365282 -14.006166 -13.311573][-9.52581 -9.7982988 -8.2140856 -5.3645287 -3.9858103 -2.9710257 -2.4718425 -3.0725467 -4.0300908 -4.783823 -5.5109296 -7.1830735 -9.610467 -12.651005 -12.912724][-11.880596 -11.063139 -8.7773647 -5.4422255 -2.1631033 1.0614877 3.4975696 3.584599 2.9147477 0.882185 -2.4777908 -5.7806458 -9.0563822 -10.806562 -10.493525][-12.853626 -11.963329 -8.9283524 -4.9160323 -1.6749892 2.0279007 5.8288913 6.9794526 7.6458678 4.0424609 -0.83118439 -4.8138642 -9.1313953 -11.108879 -10.903332][-12.651899 -11.042251 -8.8923235 -5.2011709 -1.8535058 3.2563062 6.9916816 7.2818408 7.9393153 5.2377667 0.61895514 -4.8921571 -10.448346 -12.904469 -12.428074][-10.806797 -9.2838364 -8.0554752 -4.2696123 -1.7798953 1.8632112 4.4977975 7.0855327 8.6511478 4.6521106 0.45248747 -5.6650124 -11.062508 -12.731835 -12.800455][-8.30434 -7.1122718 -5.70814 -3.3160126 -2.3693459 0.35329676 2.5438371 4.7361255 5.5291991 3.4434543 0.22293425 -5.5053177 -10.893675 -14.139006 -14.796926][-10.088255 -8.7916555 -6.8974342 -5.3210874 -4.9270196 -4.0954113 -3.6452825 -2.7195704 -2.2541142 -3.0974712 -5.0455561 -9.5125637 -12.646578 -13.974003 -12.9276][-15.089273 -12.886602 -10.545561 -10.027397 -9.8282757 -9.1750317 -9.1376266 -9.3314571 -9.5589781 -10.067435 -11.075396 -12.342571 -12.659194 -13.7642 -12.675838][-14.65535 -13.503712 -13.225065 -11.974899 -11.76228 -11.45929 -10.993549 -10.726682 -11.571283 -12.163261 -12.313858 -12.858656 -12.442384 -10.844561 -8.2268677][-11.274361 -10.099667 -9.2636433 -8.1897526 -7.7499228 -8.4237061 -9.5443687 -9.54161 -9.7743359 -9.8962 -10.668869 -9.8762665 -8.6099606 -8.4355555 -7.9233413][-8.11811 -6.3194151 -4.3256645 -3.4080586 -3.006532 -2.9314647 -3.0601823 -4.6065035 -6.3091807 -6.4371796 -6.4451871 -7.5078912 -8.2504988 -7.9750605 -7.6728392]]...]
INFO - root - 2017-12-16 02:38:28.210718: step 88510, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.656 sec/batch; 44h:27m:48s remains)
INFO - root - 2017-12-16 02:38:34.783757: step 88520, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 44h:15m:56s remains)
INFO - root - 2017-12-16 02:38:41.317040: step 88530, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.634 sec/batch; 42h:59m:21s remains)
INFO - root - 2017-12-16 02:38:47.947264: step 88540, loss = 0.19, batch loss = 0.14 (11.9 examples/sec; 0.672 sec/batch; 45h:31m:38s remains)
INFO - root - 2017-12-16 02:38:54.491252: step 88550, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 43h:57m:52s remains)
INFO - root - 2017-12-16 02:39:01.093624: step 88560, loss = 0.16, batch loss = 0.11 (12.6 examples/sec; 0.634 sec/batch; 42h:56m:54s remains)
INFO - root - 2017-12-16 02:39:07.701401: step 88570, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.682 sec/batch; 46h:11m:02s remains)
INFO - root - 2017-12-16 02:39:14.282214: step 88580, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.658 sec/batch; 44h:36m:55s remains)
INFO - root - 2017-12-16 02:39:20.728798: step 88590, loss = 0.13, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 43h:19m:38s remains)
INFO - root - 2017-12-16 02:39:27.328829: step 88600, loss = 0.17, batch loss = 0.13 (12.3 examples/sec; 0.651 sec/batch; 44h:04m:30s remains)
2017-12-16 02:39:27.860219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9045229 -6.2639771 -6.0410256 -5.5933723 -4.2046328 -2.1781402 -0.24700165 -0.24110508 -0.3553071 -0.37157202 -1.3754568 -4.3910007 -8.31586 -9.6026726 -10.049488][-4.2861452 -4.4981194 -3.8664875 -2.5554435 -1.7543232 -0.831985 0.60858345 1.1898804 0.87494516 -0.56124496 -1.5025053 -4.6904244 -8.8976364 -9.1642418 -9.3980608][-1.6916957 -2.6652148 -2.4024892 -1.2011447 -0.47529364 0.16328669 0.41962814 0.4749856 -1.018579 -1.8792474 -2.1030877 -5.3264728 -8.4848156 -10.033529 -11.471505][-1.4993682 -2.0136762 -2.4897318 -2.0491862 -1.9693933 -1.6900821 -1.7338986 -1.5458789 -2.1702268 -3.5542138 -4.4184194 -6.5799952 -10.021694 -11.441132 -12.011178][-1.8786371 -3.1803441 -3.7347004 -3.7887151 -3.7178488 -3.2953341 -2.5894711 -2.0335016 -2.3758183 -2.6205268 -3.3772488 -6.0152736 -9.3278055 -11.263424 -12.709349][-3.3949535 -3.1306841 -3.2020714 -2.4409256 -2.4861732 -1.9073441 -1.090776 -0.78841686 -0.42096043 -0.82197046 -1.5906677 -4.36622 -8.4233227 -9.3703775 -9.2630291][-4.5448132 -3.193599 -1.5237951 -0.85021639 -1.1888151 -0.46875381 0.3258152 1.64956 2.408114 1.9111652 0.87630939 -2.5533295 -6.8591056 -8.1922131 -8.9676285][-3.5916204 -3.0244305 -1.8358493 0.32862806 0.82907152 1.0960484 1.7570677 2.1791539 2.4066262 1.8860598 0.59426928 -2.3954589 -6.1955357 -7.6096892 -8.0626564][-3.7212744 -3.3463438 -1.9054172 -0.75042343 -0.99874115 -0.84863758 -0.036898613 0.83475494 1.6298661 1.3906803 0.72921324 -2.9669905 -7.8816795 -9.2630444 -10.536367][-4.2366066 -3.6494246 -2.6698062 -2.4736283 -2.7602587 -3.026475 -2.0171478 -1.0633898 -0.40565777 -0.35840225 -0.83542061 -4.4760094 -8.9593945 -10.958124 -12.825836][-8.5124836 -8.0168858 -7.6809683 -6.9464493 -7.1581678 -7.6648593 -6.5439005 -4.9098125 -3.7724309 -3.10502 -3.3200057 -7.1092987 -10.856557 -11.685965 -12.409298][-11.847977 -11.679449 -10.832447 -9.7559719 -9.7084188 -9.4630556 -8.6330366 -7.374517 -6.9493752 -6.6574488 -7.2369356 -8.6803865 -10.032938 -10.618931 -11.381366][-12.696487 -11.551233 -10.461904 -10.22776 -10.785072 -9.9808216 -9.0449886 -7.76353 -6.855556 -7.0719986 -7.6844664 -8.3089886 -9.105937 -8.3416739 -7.7839375][-10.896173 -10.516638 -10.141806 -8.9704533 -8.9092836 -9.2332478 -9.0275908 -7.5100365 -6.3030834 -5.8277683 -6.3827491 -6.648891 -7.7230053 -7.6827183 -7.1790633][-7.272037 -7.7643909 -7.7117605 -7.0296421 -6.0466824 -5.1846457 -4.8711386 -4.9159546 -5.3942347 -5.1999841 -5.1596642 -6.2496443 -7.8663206 -8.1872358 -9.0180569]]...]
INFO - root - 2017-12-16 02:39:34.441177: step 88610, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.657 sec/batch; 44h:30m:54s remains)
INFO - root - 2017-12-16 02:39:41.109918: step 88620, loss = 0.11, batch loss = 0.06 (12.0 examples/sec; 0.664 sec/batch; 44h:58m:44s remains)
INFO - root - 2017-12-16 02:39:47.737256: step 88630, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 45h:03m:29s remains)
INFO - root - 2017-12-16 02:39:54.384937: step 88640, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 44h:29m:03s remains)
INFO - root - 2017-12-16 02:40:00.912821: step 88650, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 45h:27m:46s remains)
INFO - root - 2017-12-16 02:40:07.476609: step 88660, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 43h:47m:06s remains)
INFO - root - 2017-12-16 02:40:14.032134: step 88670, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 44h:39m:52s remains)
INFO - root - 2017-12-16 02:40:20.562031: step 88680, loss = 0.17, batch loss = 0.12 (12.7 examples/sec; 0.632 sec/batch; 42h:47m:48s remains)
INFO - root - 2017-12-16 02:40:27.081621: step 88690, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.645 sec/batch; 43h:39m:28s remains)
INFO - root - 2017-12-16 02:40:33.690012: step 88700, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 43h:56m:25s remains)
2017-12-16 02:40:34.181036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8101254 -6.3544569 -6.2873411 -6.1396775 -6.1953325 -6.4927115 -6.8638659 -8.057004 -8.9569874 -8.7495184 -8.300951 -9.4206133 -10.115118 -11.609901 -10.044067][-5.6175184 -6.2047696 -6.4183283 -6.7000461 -7.2913036 -7.6590624 -7.9490004 -8.9471416 -9.8781137 -9.6357555 -9.3095636 -10.201065 -10.769218 -12.284283 -10.645374][-4.6415644 -5.9908891 -7.431303 -7.9325209 -8.1353 -8.69525 -8.7144156 -8.8356829 -9.0328979 -8.8552628 -8.6172237 -9.6859636 -10.839069 -12.431679 -11.084665][-4.6274862 -5.402606 -6.1379042 -6.4128647 -7.0688825 -6.5722909 -5.6659169 -5.8282833 -6.0895276 -6.22033 -6.4495564 -8.7553492 -10.798235 -13.381847 -12.538918][-5.9235764 -6.4792771 -6.2383595 -5.4080172 -4.59848 -3.2965329 -2.17632 -1.9627645 -1.964936 -2.2567959 -3.1339211 -5.9008174 -8.741662 -12.891443 -13.374023][-6.0879955 -5.6672854 -4.6064048 -2.9550242 -1.1194401 1.1235943 2.4045978 2.7090831 2.3685174 0.780375 -1.1960945 -4.4039407 -7.4324093 -10.997014 -10.810595][-5.7849355 -5.5933523 -4.3067675 -1.5719953 0.76413441 3.2481284 6.0268083 7.2088695 6.9632611 3.8388839 0.60128546 -3.0417187 -6.0866671 -8.9670877 -8.9874687][-5.1848555 -4.2739964 -2.839483 -0.83054876 0.88669586 4.0549455 6.683207 7.0553985 7.0850558 5.3695264 3.68498 -0.98577356 -5.1353207 -8.5653572 -8.383111][-5.0568261 -4.3943624 -3.3679621 -0.90919685 0.54847479 2.8959994 4.7182441 5.3509955 5.655108 4.7731853 3.6632037 -0.91411638 -4.9584651 -9.0642385 -9.4332161][-3.8575771 -3.3736317 -2.6131189 -0.77860165 -0.30795002 0.73502064 1.265799 1.4186606 1.9698229 1.3573046 0.70596027 -3.5483282 -7.2257004 -11.377084 -11.781536][-7.0070486 -5.8269863 -5.0372224 -3.6582181 -3.4746413 -2.7918723 -3.0338387 -3.1938162 -3.2572544 -4.1263394 -5.5896363 -8.9997349 -10.734239 -12.99884 -12.036079][-11.245251 -9.9972906 -7.8536663 -6.4826851 -6.4938545 -6.4246573 -7.1699142 -7.5238953 -7.7003489 -8.4629812 -9.3245459 -11.064245 -11.114479 -12.672322 -11.398689][-11.530865 -10.248406 -8.3040609 -7.6812253 -7.2866011 -6.7891965 -7.0132055 -7.3192396 -8.0361443 -9.1436834 -9.9382973 -10.573877 -9.97056 -9.6256361 -7.7512703][-10.037807 -8.9324551 -7.8007069 -5.8940134 -5.0867934 -5.2187591 -5.1929383 -5.0387235 -6.0110464 -7.16337 -8.1803713 -8.27907 -7.7139182 -6.9767838 -6.5176806][-8.4265232 -8.427413 -7.878768 -5.8315191 -4.0271115 -4.2409377 -3.9202995 -4.1578631 -4.450983 -4.9189711 -6.0357227 -6.9093437 -7.238821 -7.1140423 -7.0879478]]...]
INFO - root - 2017-12-16 02:40:40.790635: step 88710, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 44h:27m:54s remains)
INFO - root - 2017-12-16 02:40:47.352637: step 88720, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.653 sec/batch; 44h:11m:22s remains)
INFO - root - 2017-12-16 02:40:53.895738: step 88730, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.682 sec/batch; 46h:09m:29s remains)
INFO - root - 2017-12-16 02:41:00.447510: step 88740, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.645 sec/batch; 43h:39m:14s remains)
INFO - root - 2017-12-16 02:41:07.027719: step 88750, loss = 0.12, batch loss = 0.07 (11.9 examples/sec; 0.670 sec/batch; 45h:20m:25s remains)
INFO - root - 2017-12-16 02:41:13.624106: step 88760, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 44h:37m:11s remains)
INFO - root - 2017-12-16 02:41:20.215187: step 88770, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 44h:33m:44s remains)
INFO - root - 2017-12-16 02:41:26.807329: step 88780, loss = 0.19, batch loss = 0.15 (12.1 examples/sec; 0.661 sec/batch; 44h:44m:46s remains)
INFO - root - 2017-12-16 02:41:33.386345: step 88790, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.649 sec/batch; 43h:56m:45s remains)
INFO - root - 2017-12-16 02:41:39.900800: step 88800, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.643 sec/batch; 43h:32m:32s remains)
2017-12-16 02:41:40.425268: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.48561716 0.12207365 0.52446413 1.4006395 1.1975002 1.2316766 0.8740015 -0.24260092 -1.7189212 -3.1229739 -3.8021026 -6.042829 -8.89872 -11.324082 -12.097143][0.39255619 1.7285309 3.9998803 4.5120931 3.7971482 2.6330581 1.2597318 0.15814543 -1.8945231 -3.5129769 -4.8560123 -9.1877441 -12.927444 -15.875423 -16.257084][3.0582108 2.3282819 2.257153 3.0681119 3.6851945 2.8507657 1.3584957 -0.40578318 -2.5594943 -4.8238869 -7.2069335 -10.722451 -14.05827 -17.921099 -19.344374][1.6373277 1.8196359 2.4510341 3.061419 2.668941 1.9495435 0.51215172 -0.82286787 -2.4748094 -4.7243304 -7.2006631 -11.281804 -14.462217 -18.090336 -19.70746][2.0114355 1.8494773 1.8554029 1.8693557 1.9801664 2.2277288 1.8732028 -0.021359444 -2.2433972 -3.7791889 -5.4666286 -9.5120869 -13.44191 -17.497948 -19.090309][-0.44182587 -0.72865009 0.36583996 2.8175321 4.1750131 3.97259 4.1357942 3.4187493 1.7390132 -0.66570282 -2.708966 -6.4976325 -9.8415413 -12.741102 -14.132093][-0.95110512 -0.72559977 0.36106968 2.4568763 4.1387811 4.9481578 5.4696517 4.584929 3.2507691 1.2084136 -0.6115942 -4.3493266 -7.5324507 -10.433486 -11.484107][-3.7961178 -2.7378113 -0.96253872 1.7321291 4.2929034 6.4489236 6.9692845 5.0853295 3.6239114 1.76438 0.24275732 -3.3177068 -6.9031653 -10.312609 -11.56764][-4.097548 -3.0683646 -1.4414077 0.64380312 2.8812356 5.1016269 6.0587335 4.18348 2.2402935 0.3279767 -0.82737875 -3.4578478 -6.3573427 -9.9286 -11.938101][-4.9125738 -4.1915364 -3.3448799 -1.1955047 0.61794949 2.1929083 3.3646569 3.1216578 2.0823669 0.078606606 -1.1024127 -4.28371 -7.5237179 -11.045904 -12.994022][-10.712603 -10.120673 -9.1278381 -6.5844336 -4.9058332 -3.0634415 -1.7159009 -1.5571113 -1.2096148 -2.0322366 -2.8813827 -6.6531582 -9.0678749 -11.759312 -12.422725][-12.733948 -13.139765 -12.900665 -10.875461 -9.8804455 -8.1091175 -7.5498714 -7.3900852 -6.8557382 -7.064589 -6.8133268 -8.0376892 -9.0239506 -11.943567 -12.99626][-12.606195 -13.601114 -14.111511 -12.904732 -12.224663 -10.457426 -10.672443 -11.101764 -11.158627 -10.633184 -9.8393822 -10.676769 -10.90069 -12.808264 -11.859716][-9.0748854 -9.5833178 -10.006509 -10.843942 -11.498623 -11.025452 -11.443407 -10.589388 -11.088596 -11.521318 -11.581109 -11.176851 -10.278397 -11.053066 -11.440957][-7.0092783 -7.2100506 -6.9262543 -7.3993497 -7.2263494 -7.440876 -8.6943369 -9.3782158 -10.345642 -10.191305 -10.210752 -11.260793 -11.918039 -12.229542 -11.740791]]...]
INFO - root - 2017-12-16 02:41:46.958970: step 88810, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.664 sec/batch; 44h:57m:22s remains)
INFO - root - 2017-12-16 02:41:53.502419: step 88820, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 43h:58m:27s remains)
INFO - root - 2017-12-16 02:42:00.058205: step 88830, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 45h:19m:03s remains)
INFO - root - 2017-12-16 02:42:06.648169: step 88840, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 45h:23m:25s remains)
INFO - root - 2017-12-16 02:42:13.231700: step 88850, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.643 sec/batch; 43h:29m:38s remains)
INFO - root - 2017-12-16 02:42:19.798115: step 88860, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 44h:10m:43s remains)
INFO - root - 2017-12-16 02:42:26.412532: step 88870, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 44h:10m:58s remains)
INFO - root - 2017-12-16 02:42:32.916063: step 88880, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.644 sec/batch; 43h:33m:26s remains)
INFO - root - 2017-12-16 02:42:39.502524: step 88890, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.656 sec/batch; 44h:24m:51s remains)
INFO - root - 2017-12-16 02:42:46.088257: step 88900, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 43h:45m:56s remains)
2017-12-16 02:42:46.615729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5025308 -4.6319408 -4.7233562 -3.6458235 -3.5622859 -3.2958241 -4.4077945 -7.0199986 -7.8514023 -8.0471153 -7.5887938 -6.8104281 -7.4339752 -7.6078053 -5.7157478][-2.3374295 -2.4791205 -2.449194 -2.6796901 -2.8061194 -2.792058 -3.6622658 -5.1017838 -6.4104948 -6.2145333 -5.3100624 -4.5579419 -4.0819674 -6.2413931 -6.6517134][-1.9515233 -2.5243554 -2.7457683 -2.6981072 -3.333046 -2.4592996 -3.1403842 -4.5775719 -5.5931139 -5.0272303 -4.1606312 -2.9299874 -2.4327576 -4.5572119 -4.8381929][-1.7546601 -2.345036 -2.4637618 -2.6899862 -3.5398417 -2.8276608 -2.7930005 -3.8685443 -4.9309497 -4.8131027 -3.6455281 -2.3268592 -2.5300295 -3.6657171 -4.0567865][-2.2315438 -2.4573214 -2.5798736 -2.9859979 -3.1487572 -1.670424 -0.16232491 -1.1527729 -2.1610236 -2.644366 -2.768672 -2.4984064 -2.6460621 -3.2514925 -4.18643][-2.766968 -2.6502078 -3.1374853 -2.5098441 -1.6843662 0.10011768 2.9594502 2.727757 1.0786762 -0.12745142 -1.0142879 -1.322372 -1.804116 -2.9063621 -3.8799016][-4.5177741 -3.1099164 -2.5569246 -1.6768017 -0.97246075 0.78310728 3.28546 4.6745467 3.6530976 1.0695786 -0.44082975 -1.1940298 -2.2807994 -3.5988393 -4.3721242][-5.0056643 -3.3648584 -3.2483633 -1.8482609 -0.63945961 0.64097118 3.0776305 4.265058 2.9461932 1.5950403 -0.021054268 -1.5111289 -2.5202043 -4.3552561 -5.6200047][-5.6717076 -3.5910892 -3.1574392 -1.4428415 -1.374166 -0.19122696 2.0018964 2.4035401 1.9729018 1.04638 -0.92916679 -2.1215389 -3.1949255 -4.8422608 -5.9989238][-8.9623289 -7.465251 -5.903441 -4.032979 -3.5614629 -0.52988386 1.0141916 0.96487379 1.501205 0.49031591 -0.68879271 -1.8085294 -2.5970736 -5.1695247 -6.6761665][-10.460517 -10.131313 -9.7558022 -8.37944 -7.08304 -4.0968938 -2.017761 -1.6206002 -1.5763454 -2.6007776 -3.238477 -4.0649376 -4.8425283 -6.1416888 -6.9219027][-12.131866 -11.282753 -11.027898 -9.7289524 -7.9713945 -7.1104121 -6.2573919 -5.6351972 -5.5743923 -6.0067959 -6.3625507 -6.7852607 -6.8768859 -8.00233 -8.3843689][-10.849144 -10.880471 -9.9761753 -9.2692184 -7.905633 -6.2108154 -5.5555263 -5.5382223 -5.8879895 -6.4513283 -6.7725525 -7.1490822 -6.3309135 -7.1293073 -6.8253016][-8.0459671 -8.1247711 -7.2129159 -6.1925497 -5.4591908 -5.1631021 -5.0579052 -3.7961249 -3.4275012 -4.2293296 -4.7821188 -4.6212416 -3.4715471 -4.63766 -5.1212525][-5.2802219 -4.9915986 -4.226759 -3.4996793 -4.2992444 -4.5385685 -4.382688 -4.2251706 -4.2568159 -4.423852 -4.4639215 -4.341907 -3.9058056 -3.7790675 -3.8984795]]...]
INFO - root - 2017-12-16 02:42:53.217371: step 88910, loss = 0.12, batch loss = 0.07 (11.4 examples/sec; 0.701 sec/batch; 47h:25m:13s remains)
INFO - root - 2017-12-16 02:42:59.807468: step 88920, loss = 0.12, batch loss = 0.08 (11.6 examples/sec; 0.687 sec/batch; 46h:29m:26s remains)
INFO - root - 2017-12-16 02:43:06.437208: step 88930, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.656 sec/batch; 44h:24m:58s remains)
INFO - root - 2017-12-16 02:43:13.048510: step 88940, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.675 sec/batch; 45h:41m:32s remains)
INFO - root - 2017-12-16 02:43:19.563446: step 88950, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.681 sec/batch; 46h:05m:42s remains)
INFO - root - 2017-12-16 02:43:26.173884: step 88960, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 44h:49m:05s remains)
INFO - root - 2017-12-16 02:43:32.766501: step 88970, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 44h:16m:22s remains)
INFO - root - 2017-12-16 02:43:39.327512: step 88980, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.641 sec/batch; 43h:20m:56s remains)
INFO - root - 2017-12-16 02:43:45.940909: step 88990, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.646 sec/batch; 43h:40m:15s remains)
INFO - root - 2017-12-16 02:43:52.488933: step 89000, loss = 0.12, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 43h:57m:16s remains)
2017-12-16 02:43:53.014710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3242278 -3.990844 -4.0379677 -4.2446723 -4.5105906 -4.7556119 -5.0208941 -4.9951215 -4.7817888 -3.9548936 -3.1243856 -5.3167858 -7.2145524 -7.7160673 -7.4244719][-5.3562536 -4.9309974 -5.4503183 -5.4464803 -5.289444 -5.5339227 -5.8282714 -6.0194292 -5.802103 -5.0387011 -4.6092806 -6.7671952 -8.1152058 -9.1384163 -8.8918028][-3.4342127 -4.2737679 -5.5567131 -5.0410852 -5.5303082 -6.0381746 -6.040977 -5.7198229 -5.572298 -5.5190549 -5.5321016 -7.5092959 -9.4621658 -10.530325 -9.842392][-3.1837583 -3.3101616 -3.9665613 -3.9213533 -4.3157616 -4.6299162 -5.3091936 -5.7680845 -5.9764161 -6.1632876 -6.1485791 -8.5592461 -10.536253 -10.771389 -10.272438][-4.4337173 -5.472765 -5.6049914 -3.94093 -3.630209 -2.7923265 -2.8162868 -3.873744 -5.202415 -5.7644949 -6.0339961 -8.7689686 -11.095489 -11.658586 -10.764719][-7.3251681 -7.41076 -5.7678719 -3.4475789 -2.1161702 -0.19732571 0.57202196 0.42047882 -0.3027339 -2.4727502 -4.7473021 -7.5615816 -9.9091749 -10.834475 -10.91602][-8.4469166 -8.1181688 -6.5134254 -3.04314 -0.89483976 1.1773205 2.6683831 3.168509 3.175796 0.85212421 -1.8592346 -6.1704116 -9.6252518 -11.06572 -10.385579][-7.2910337 -6.6849213 -4.7070875 -1.0368013 0.925683 2.6961455 3.58248 3.3925633 3.2075772 1.0689125 -1.1459622 -5.6409178 -8.9772358 -10.406167 -10.521335][-5.5440769 -5.0160294 -3.2701609 -0.1580677 1.3696184 3.0116229 3.3366055 2.3174434 1.569334 0.33854723 -1.0990748 -5.0285807 -8.1616335 -9.9004574 -8.73653][-6.4891658 -5.7258787 -3.935658 -0.39817142 1.2697172 1.8374362 1.9834046 2.0204592 1.4034047 0.31742907 -0.80558586 -3.9229765 -6.5526886 -7.7431207 -6.8879442][-11.006989 -9.2628469 -6.2775283 -3.6597333 -2.1533518 -1.5208426 -0.7419858 -0.9569397 -1.5386348 -1.4635367 -2.0729203 -4.4975042 -6.3523397 -7.2794929 -6.4410286][-13.540005 -12.42238 -10.50478 -6.8165278 -5.0085673 -5.010149 -5.0518408 -5.1402235 -4.9915285 -4.6471157 -4.6742172 -6.3315187 -7.1844196 -7.0814772 -6.6475329][-13.100531 -11.619729 -10.2173 -8.2030373 -6.9222407 -6.3814721 -6.1796074 -6.8572063 -7.3719773 -7.1270742 -6.9536242 -7.5219269 -7.3527651 -6.6789083 -5.3732705][-10.244705 -9.4995451 -8.7617216 -7.278708 -6.6742373 -6.9252357 -7.2881727 -6.785315 -6.0375519 -6.6770525 -7.8548136 -8.1544676 -7.8358345 -7.498147 -6.7403536][-6.9956789 -6.8104258 -6.6525016 -6.756125 -7.0938663 -7.9234962 -8.1069889 -8.0212126 -7.7346082 -7.1257219 -6.7624483 -7.7297974 -8.3121414 -8.50533 -9.1298313]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 02:43:59.580810: step 89010, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 45h:08m:41s remains)
INFO - root - 2017-12-16 02:44:06.158024: step 89020, loss = 0.21, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 44h:47m:26s remains)
INFO - root - 2017-12-16 02:44:12.785257: step 89030, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.653 sec/batch; 44h:08m:25s remains)
INFO - root - 2017-12-16 02:44:19.377377: step 89040, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.666 sec/batch; 45h:02m:39s remains)
INFO - root - 2017-12-16 02:44:25.944876: step 89050, loss = 0.17, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 45h:04m:34s remains)
INFO - root - 2017-12-16 02:44:32.578875: step 89060, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.673 sec/batch; 45h:31m:20s remains)
INFO - root - 2017-12-16 02:44:39.135488: step 89070, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.667 sec/batch; 45h:08m:08s remains)
INFO - root - 2017-12-16 02:44:45.731857: step 89080, loss = 0.16, batch loss = 0.12 (11.5 examples/sec; 0.693 sec/batch; 46h:51m:47s remains)
INFO - root - 2017-12-16 02:44:52.323327: step 89090, loss = 0.17, batch loss = 0.13 (12.5 examples/sec; 0.640 sec/batch; 43h:15m:28s remains)
INFO - root - 2017-12-16 02:44:58.878900: step 89100, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 44h:39m:51s remains)
2017-12-16 02:44:59.418020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7219028 -7.0186419 -6.0720148 -5.9334211 -5.9986825 -5.50479 -5.2674832 -4.6548076 -4.0797853 -3.7109277 -3.8093367 -4.9631548 -6.4488196 -5.4931111 -3.6718044][-6.5798254 -6.0481882 -5.1787853 -4.8000875 -3.7294981 -3.7498059 -4.2275825 -3.6217196 -2.8070965 -2.2829905 -1.5273671 -2.9717679 -4.2842817 -3.7179842 -3.1396852][-2.6055312 -4.3904543 -5.30928 -3.6202445 -3.3986568 -3.1149147 -3.0025389 -2.5194359 -1.9445608 -1.323103 -0.91158962 -1.7455242 -2.6742067 -2.5729389 -1.8278177][-4.7552934 -4.5408044 -3.5550539 -3.5815895 -4.0794392 -3.3959725 -3.3118069 -3.4680748 -3.6846611 -2.465445 -1.891356 -3.0528128 -4.5288672 -4.2466922 -3.2556462][-5.8658705 -6.4033356 -5.6557059 -4.1725049 -2.5907981 -0.96827126 -0.893877 -1.3091817 -1.1975331 -0.57935715 -0.92910624 -2.2972815 -4.1187057 -4.0290928 -3.4635448][-6.7019129 -6.3593946 -4.8248706 -1.1044822 1.3849545 3.0743566 3.6163993 3.1430173 2.7798543 1.4207439 0.13616657 -0.96705151 -3.150203 -3.2885735 -2.6009533][-8.8186255 -7.861444 -4.3092775 -0.73570395 1.6065211 4.1655564 7.08083 6.3650937 4.9445405 3.3312535 2.1084967 -0.7562151 -4.6501503 -4.5347004 -3.7500648][-11.093459 -9.136837 -7.1119404 -3.4150496 -0.36571932 2.4538112 4.5519805 3.8026071 3.2909026 2.0635519 0.25094366 -3.3697939 -6.874414 -6.9498806 -6.6335993][-11.689825 -9.7123747 -6.9736581 -4.0072432 -1.6814489 0.87305069 2.3169551 2.1822152 2.0002327 -0.66877508 -2.9275668 -5.7992282 -8.6816864 -8.7256241 -7.3840308][-12.185726 -9.8197689 -7.3532796 -3.7841749 -1.934957 -0.87432575 0.23088408 0.39659882 -0.62674761 -2.6629682 -3.7717462 -6.3622012 -8.8319416 -8.6948271 -7.8245559][-13.818211 -12.183842 -9.0843878 -6.1246529 -5.6034527 -4.3580518 -3.2379551 -3.3965847 -3.9152393 -4.5092831 -5.2304134 -8.0469751 -9.065855 -8.6872139 -7.8252983][-16.475256 -15.246408 -12.238456 -10.017689 -10.044016 -8.947216 -7.4544754 -7.3027091 -7.2042704 -7.1371121 -7.7571173 -8.5803881 -8.4520245 -8.6289034 -8.2421045][-14.571108 -13.024904 -11.166018 -9.9133863 -9.4323463 -9.1431417 -8.9421816 -8.7585716 -8.3316 -8.4906311 -8.2580585 -7.9426851 -7.5183883 -6.193604 -5.216186][-11.51362 -10.962942 -8.9296789 -7.0514646 -7.1050692 -7.474308 -7.7566838 -7.4367781 -7.28039 -7.5436811 -7.5757976 -6.5257406 -5.1330423 -3.93398 -3.511282][-7.5962577 -7.6585407 -6.9297419 -4.7601743 -3.822386 -3.4373021 -3.57147 -4.2639713 -4.1928072 -3.9670334 -4.0691495 -4.4802885 -4.9882178 -4.71342 -5.3590918]]...]
INFO - root - 2017-12-16 02:45:05.964625: step 89110, loss = 0.17, batch loss = 0.13 (12.0 examples/sec; 0.668 sec/batch; 45h:11m:06s remains)
INFO - root - 2017-12-16 02:45:12.553149: step 89120, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.628 sec/batch; 42h:27m:58s remains)
INFO - root - 2017-12-16 02:45:19.153089: step 89130, loss = 0.17, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 42h:50m:53s remains)
INFO - root - 2017-12-16 02:45:25.779252: step 89140, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 44h:43m:38s remains)
INFO - root - 2017-12-16 02:45:32.390635: step 89150, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.653 sec/batch; 44h:09m:08s remains)
INFO - root - 2017-12-16 02:45:38.973851: step 89160, loss = 0.23, batch loss = 0.19 (12.1 examples/sec; 0.661 sec/batch; 44h:41m:56s remains)
INFO - root - 2017-12-16 02:45:45.581653: step 89170, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.659 sec/batch; 44h:33m:01s remains)
INFO - root - 2017-12-16 02:45:52.145138: step 89180, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.672 sec/batch; 45h:23m:28s remains)
INFO - root - 2017-12-16 02:45:58.717127: step 89190, loss = 0.18, batch loss = 0.14 (12.5 examples/sec; 0.641 sec/batch; 43h:20m:46s remains)
INFO - root - 2017-12-16 02:46:05.267714: step 89200, loss = 0.14, batch loss = 0.10 (12.6 examples/sec; 0.633 sec/batch; 42h:47m:30s remains)
2017-12-16 02:46:05.836314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.7870922 -7.8992577 -7.9901681 -8.3410587 -8.321167 -8.7128553 -8.8562393 -8.786438 -7.8624144 -6.7358541 -5.5372081 -5.8486834 -6.0592055 -6.1245313 -6.5935211][-5.3668232 -6.4310184 -6.8134947 -6.8640804 -7.5255055 -7.8621893 -8.6969709 -8.6656227 -8.7283936 -7.62961 -5.6907187 -5.6344533 -7.1480627 -8.4283905 -8.4055061][-3.7986095 -4.1731291 -4.6574183 -5.5226812 -5.6807818 -5.7874851 -6.1859665 -6.92337 -7.5036311 -6.5851712 -5.4608731 -6.2643204 -6.8795295 -7.72241 -9.6138353][-3.7993302 -3.8290544 -4.1068234 -4.5450945 -4.0181904 -3.9193616 -3.9469652 -3.932982 -4.2977886 -5.1452847 -5.4026432 -5.6654267 -7.1678953 -7.8113327 -8.323801][-4.6785884 -5.1815434 -5.1027365 -3.572309 -3.0692987 -1.966532 -0.86193323 -2.0290544 -3.3264861 -3.3527753 -3.1166785 -4.3895264 -5.6768341 -7.6296535 -8.4521446][-5.0143905 -4.887042 -4.442914 -3.244549 -0.9863801 0.84221268 2.0628815 1.1342154 0.54441738 -0.93063354 -1.5217972 -2.4058378 -3.5349331 -5.0599241 -6.1591511][-4.7558928 -3.8998785 -2.6390998 -0.87567043 0.5542326 2.2988725 4.5606227 4.0476117 3.2568498 1.0091825 -1.3669949 -2.6344607 -4.2249651 -5.3307843 -6.561883][-5.2284923 -3.6195815 -1.7386351 0.070838451 0.83443785 2.4217782 3.5272632 3.6834941 3.9055991 2.2173996 0.17354822 -3.7078381 -6.9244413 -7.8031578 -7.3040357][-6.1884484 -4.4581385 -1.9571285 -0.036756039 0.40890169 1.5826569 2.1758776 2.6111951 3.6353574 3.5051055 1.9938459 -1.8685801 -7.3893976 -10.69865 -12.192229][-6.5543609 -5.2852306 -3.6167178 -0.79977417 -0.26090384 -0.37862444 0.46183825 1.1854329 1.4651337 2.44694 2.698607 -0.85959435 -5.6097269 -9.8868618 -13.934221][-9.30596 -8.6553612 -7.1755552 -4.6803393 -3.9492123 -4.099195 -3.5267978 -2.2789123 -1.9910617 -1.2584829 -1.9651659 -4.2890439 -7.5168934 -11.366188 -13.62453][-11.617226 -12.02394 -11.624643 -9.6807232 -9.1848564 -8.8798275 -8.4986362 -7.9736404 -7.4990215 -5.9816227 -6.1710167 -7.7598372 -9.9335155 -11.409266 -11.986124][-11.640015 -11.43206 -11.425915 -10.466488 -10.875834 -10.434008 -10.265613 -8.44764 -7.2505093 -7.0014615 -6.4924679 -8.8748226 -10.672674 -11.99395 -10.8935][-12.623877 -11.856068 -10.853882 -10.226414 -10.696755 -11.590187 -10.786952 -9.427496 -8.98935 -7.8368115 -6.8664737 -8.2807846 -8.89315 -10.930317 -11.235191][-8.9840088 -9.500392 -9.96834 -8.0863495 -6.9363551 -6.89732 -7.7505617 -7.5288582 -7.8226633 -8.7943764 -9.2545185 -9.0095577 -8.7720833 -9.46706 -10.50422]]...]
INFO - root - 2017-12-16 02:46:12.391318: step 89210, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.650 sec/batch; 43h:55m:39s remains)
INFO - root - 2017-12-16 02:46:18.995809: step 89220, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 45h:44m:40s remains)
INFO - root - 2017-12-16 02:46:25.551916: step 89230, loss = 0.12, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 45h:21m:24s remains)
INFO - root - 2017-12-16 02:46:32.150249: step 89240, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.643 sec/batch; 43h:27m:56s remains)
INFO - root - 2017-12-16 02:46:38.703183: step 89250, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 43h:53m:07s remains)
INFO - root - 2017-12-16 02:46:45.260390: step 89260, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 44h:26m:48s remains)
INFO - root - 2017-12-16 02:46:51.841574: step 89270, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 45h:20m:34s remains)
INFO - root - 2017-12-16 02:46:58.370508: step 89280, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.651 sec/batch; 43h:59m:47s remains)
INFO - root - 2017-12-16 02:47:04.916515: step 89290, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 44h:28m:54s remains)
INFO - root - 2017-12-16 02:47:11.584811: step 89300, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 45h:20m:45s remains)
2017-12-16 02:47:12.167373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-12.568141 -13.614861 -13.311831 -13.415133 -13.980288 -13.831362 -12.500608 -9.8872547 -8.494071 -8.4869146 -9.4295578 -11.465691 -13.874249 -13.476454 -11.154365][-11.159021 -11.046908 -11.022277 -11.544753 -12.624556 -13.310022 -12.734727 -11.630407 -10.771877 -9.68158 -9.8483562 -11.425419 -13.790689 -14.35688 -13.315348][-5.6042166 -6.650528 -8.1772852 -9.8770761 -11.078765 -10.677203 -10.013252 -9.2829428 -8.7343006 -7.9182453 -7.4910936 -8.670289 -11.885548 -13.388167 -13.601733][-3.2293761 -3.0244427 -4.1208553 -5.8710194 -7.1431236 -6.6486292 -5.619482 -5.8636718 -6.7274671 -7.0019665 -6.9237976 -7.6892433 -10.015949 -11.273029 -11.456615][-4.1246457 -4.6289911 -5.5664415 -4.3956304 -3.6039774 -1.9379308 -0.30720091 -1.6863356 -3.4261243 -4.0211086 -5.1271772 -7.014215 -9.8398113 -11.232284 -10.112598][-6.4952927 -6.3324232 -6.1223183 -4.6538343 -3.1513553 0.62268496 3.8926663 3.3871732 1.8412395 -0.3618989 -2.32929 -4.2948923 -7.5168834 -8.9271164 -8.4779615][-7.8983612 -7.9861851 -6.7839546 -4.2656484 -1.6859822 2.1092544 5.533462 6.3102889 7.0376344 4.1894832 0.6599946 -2.029423 -5.8596859 -7.6521764 -6.7595634][-7.7642307 -6.8688617 -5.8005161 -3.2233179 -0.25574541 2.9955783 5.4244075 6.2205253 6.5160661 4.2383714 1.2103443 -1.8066041 -5.1299686 -5.9441438 -4.4394846][-4.4882011 -3.7395763 -3.3284216 -1.2617497 0.24959517 2.0653863 3.4127698 4.7416711 5.0085921 2.7657619 0.20596266 -3.0173635 -6.0515914 -6.3958163 -4.6268015][-4.5028629 -3.3097692 -1.5666919 -0.6011219 -0.33755541 0.716979 2.1079526 2.4703212 1.6069055 -0.15651512 -1.7066035 -3.9012551 -6.5373325 -6.7530165 -5.6225762][-7.7190638 -5.4701533 -4.9148636 -4.6572862 -4.5868111 -3.0349016 -0.9458046 -0.51142168 -2.0945408 -4.1873307 -4.7949491 -6.4964242 -8.4898119 -8.0592079 -6.045599][-10.118992 -8.3737164 -7.1447306 -6.7109275 -6.9746985 -5.7815051 -4.6167288 -4.0465846 -5.1445327 -5.8967676 -5.5899763 -5.7666464 -6.5733862 -7.2816396 -6.7983332][-9.4604979 -9.0362663 -8.2154732 -7.8974667 -7.037137 -5.3472667 -4.4550219 -4.2834172 -5.2615 -6.4975138 -6.5789576 -6.2592816 -6.1718245 -5.9003038 -5.13553][-8.6852093 -7.9732871 -8.562315 -7.7759414 -7.3337407 -5.8620639 -4.7158284 -4.4286442 -5.0865078 -5.817543 -6.1265736 -5.0025988 -4.4265962 -5.2051663 -5.4963226][-6.6492715 -5.9441986 -5.3071971 -5.3938932 -4.7749434 -3.7031376 -3.0939155 -3.8287196 -4.9736352 -6.0171766 -6.3890982 -7.2593775 -7.6961451 -7.1962757 -6.4324145]]...]
INFO - root - 2017-12-16 02:47:18.705711: step 89310, loss = 0.12, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 43h:02m:49s remains)
INFO - root - 2017-12-16 02:47:25.308442: step 89320, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.640 sec/batch; 43h:14m:20s remains)
INFO - root - 2017-12-16 02:47:31.944278: step 89330, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 43h:48m:46s remains)
INFO - root - 2017-12-16 02:47:38.507532: step 89340, loss = 0.12, batch loss = 0.08 (11.7 examples/sec; 0.686 sec/batch; 46h:19m:36s remains)
INFO - root - 2017-12-16 02:47:45.045806: step 89350, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.666 sec/batch; 44h:58m:08s remains)
INFO - root - 2017-12-16 02:47:51.585377: step 89360, loss = 0.17, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 44h:39m:49s remains)
INFO - root - 2017-12-16 02:47:58.198635: step 89370, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.660 sec/batch; 44h:35m:29s remains)
INFO - root - 2017-12-16 02:48:04.759536: step 89380, loss = 0.12, batch loss = 0.07 (12.5 examples/sec; 0.638 sec/batch; 43h:04m:17s remains)
INFO - root - 2017-12-16 02:48:11.401956: step 89390, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.663 sec/batch; 44h:48m:14s remains)
INFO - root - 2017-12-16 02:48:18.038101: step 89400, loss = 0.12, batch loss = 0.07 (12.3 examples/sec; 0.648 sec/batch; 43h:45m:53s remains)
2017-12-16 02:48:18.552164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4033628 -8.0399628 -8.73927 -8.62461 -8.6920233 -8.8288813 -9.2016125 -7.5896983 -6.1105089 -5.7964287 -5.5379639 -8.884799 -11.499001 -11.511816 -11.433702][-5.857667 -5.9681172 -6.2781572 -7.1089191 -8.6173191 -9.117137 -9.8650694 -9.8011894 -8.6762409 -6.9409318 -5.9090505 -9.4455547 -11.824192 -11.89807 -12.28729][-1.4018679 -4.575624 -6.4264603 -6.36988 -7.7244587 -9.1176052 -10.309669 -9.4549637 -8.2531137 -7.6919713 -7.6588974 -11.527147 -13.594328 -12.2709 -11.991765][-1.1970062 -2.1196573 -3.440063 -4.5764327 -6.0554309 -6.4476438 -6.5198307 -7.6201577 -9.2690105 -8.0635262 -6.6252427 -10.818354 -14.294559 -14.165987 -13.64741][-3.1303437 -3.8039551 -5.7917104 -5.5600748 -4.9366951 -3.5768573 -2.1512578 -4.1939783 -7.048501 -7.1302967 -7.4841385 -10.979805 -13.148878 -14.003965 -14.061855][-6.481822 -6.6289358 -7.0765314 -4.7048621 -3.1954575 -0.28869677 3.0947948 2.2318473 -0.57927322 -3.5380466 -7.2515769 -10.078273 -11.38702 -11.673676 -12.405914][-9.2176523 -8.5729275 -6.5871191 -3.1889966 -1.7878475 0.96637058 5.6512942 6.6959205 4.7762656 0.024410248 -5.3125978 -8.81138 -10.86407 -11.240568 -11.554239][-10.731313 -9.4255447 -7.6512957 -3.9643593 -1.6741605 3.1467404 8.1262169 7.0469956 5.5841422 2.6330905 -2.4709656 -7.6074147 -10.514914 -10.471519 -11.313568][-6.9643774 -7.2161942 -6.6656542 -3.1642172 -0.74468374 3.0510144 5.7502618 5.5975747 6.0870528 1.6386395 -3.4575593 -8.0572376 -11.69368 -11.699305 -11.40167][-5.8974919 -5.4777007 -5.7195067 -2.6876576 -0.53896046 0.52167225 2.0105777 3.3063645 3.4693112 -0.42421532 -3.4871044 -8.8920069 -13.271322 -12.885113 -12.48879][-10.306046 -8.6117516 -8.7403069 -6.1616726 -4.7551613 -3.8104916 -2.39403 -2.9137931 -3.7144685 -4.2630372 -5.7478414 -11.725722 -14.680605 -14.172636 -13.390146][-13.001992 -12.357134 -11.860895 -10.760897 -10.024567 -7.9150229 -7.0892344 -8.6956072 -8.8315525 -8.4335489 -8.9225893 -12.465519 -14.077549 -13.936331 -12.840693][-16.28639 -15.054981 -13.30302 -12.652377 -12.601046 -11.562799 -10.820221 -10.685305 -11.183771 -11.599609 -11.211003 -12.684265 -12.688152 -11.892467 -10.388535][-13.810101 -12.796661 -12.194031 -10.814465 -10.729473 -11.425611 -11.417324 -10.600571 -10.378923 -10.574799 -10.697548 -10.78849 -10.691888 -10.346239 -7.97192][-10.659203 -9.9577751 -9.1218243 -7.60679 -7.2431316 -7.4654732 -8.0497713 -9.5446815 -10.444579 -9.4056492 -8.43579 -10.761129 -10.909557 -8.45722 -7.4610791]]...]
INFO - root - 2017-12-16 02:48:25.111474: step 89410, loss = 0.13, batch loss = 0.09 (11.7 examples/sec; 0.686 sec/batch; 46h:19m:55s remains)
INFO - root - 2017-12-16 02:48:31.658405: step 89420, loss = 0.17, batch loss = 0.12 (12.2 examples/sec; 0.654 sec/batch; 44h:09m:06s remains)
INFO - root - 2017-12-16 02:48:38.202414: step 89430, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.679 sec/batch; 45h:49m:27s remains)
INFO - root - 2017-12-16 02:48:44.696049: step 89440, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.641 sec/batch; 43h:14m:53s remains)
INFO - root - 2017-12-16 02:48:51.202187: step 89450, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 43h:56m:20s remains)
INFO - root - 2017-12-16 02:48:57.847302: step 89460, loss = 0.13, batch loss = 0.08 (12.3 examples/sec; 0.652 sec/batch; 44h:01m:19s remains)
INFO - root - 2017-12-16 02:49:04.404003: step 89470, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.678 sec/batch; 45h:47m:07s remains)
INFO - root - 2017-12-16 02:49:10.997618: step 89480, loss = 0.14, batch loss = 0.09 (11.7 examples/sec; 0.681 sec/batch; 45h:57m:58s remains)
INFO - root - 2017-12-16 02:49:17.552782: step 89490, loss = 0.14, batch loss = 0.09 (12.5 examples/sec; 0.639 sec/batch; 43h:08m:46s remains)
INFO - root - 2017-12-16 02:49:24.136151: step 89500, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.651 sec/batch; 43h:58m:22s remains)
2017-12-16 02:49:24.642371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9654245 -5.4532237 -5.4778624 -4.926146 -4.8920341 -4.2827692 -3.5046523 -2.5824556 -1.9507635 -1.8515429 -1.9833539 -4.6885147 -5.7100315 -6.8866487 -7.6734056][-5.0214615 -4.7714868 -4.4076619 -3.6372602 -4.0488124 -3.6642957 -2.7877815 -1.8157983 -0.992033 -0.44216061 -0.67198515 -4.3584118 -5.67288 -7.2571149 -8.434206][-2.7731428 -3.3185689 -3.868268 -2.76952 -3.0768867 -2.6588466 -2.4020975 -1.5989738 -0.90223885 -1.1406236 -1.4416623 -4.1800575 -5.3050194 -7.1319418 -8.3917084][-3.0588148 -3.9659457 -4.2919903 -3.0812488 -3.2933416 -2.2688735 -1.8010983 -1.9056575 -1.9033594 -1.4310722 -1.2452078 -4.5861425 -5.7946367 -6.7928915 -7.3804512][-4.2897997 -6.2541008 -6.2061825 -4.8235006 -4.2106071 -1.8252916 -0.97836971 -1.8169255 -2.091259 -1.4788375 -1.5143447 -4.42704 -5.7159195 -7.0274248 -7.63154][-5.599483 -6.7782421 -6.4489479 -3.9318833 -1.943085 0.64123535 1.993958 1.025682 0.13884974 -0.79546785 -1.6435132 -3.6754489 -4.5367346 -5.9387751 -6.8700132][-7.3203955 -7.3516951 -5.9726081 -2.4837732 -0.45333672 2.2926641 4.0719409 3.6594949 2.9336286 0.59253168 -1.2661662 -3.8537641 -5.1304121 -6.1559091 -7.1165171][-9.1310711 -7.9165878 -6.4214311 -2.3003187 0.36108255 4.077724 5.9779058 4.8428931 4.1459451 2.3215871 0.19797707 -3.5929689 -5.5265808 -7.3724604 -8.8749466][-7.5611711 -6.8097219 -6.42542 -3.4043055 -0.99618053 2.3815589 3.8073792 3.5155311 3.2397723 1.2129345 -0.20383739 -4.2328768 -6.3576508 -7.9333858 -9.0519056][-7.6155958 -6.9023986 -6.0172677 -2.5727074 -2.0045967 -0.069149971 1.4263692 1.5856018 0.86689806 -0.81280041 -2.2140758 -5.7885547 -7.283473 -8.922514 -10.139616][-10.504633 -10.4471 -9.3257637 -5.6810536 -4.9823685 -3.7731624 -3.3212345 -2.9208562 -3.2377889 -3.9847293 -5.0515747 -9.119215 -10.711182 -10.73716 -10.693607][-13.066819 -13.240295 -12.168819 -10.187277 -8.9707365 -7.6541214 -7.4528995 -7.6626325 -8.0866575 -8.15196 -8.6696053 -10.700019 -11.491571 -11.860895 -11.726745][-13.820881 -13.733265 -12.490875 -11.757527 -11.413872 -9.7843876 -9.4329462 -10.281038 -10.514372 -10.382091 -10.163006 -11.155258 -11.631449 -10.851684 -10.193275][-12.702858 -12.366719 -11.879356 -10.563803 -9.63081 -9.9619694 -10.005968 -9.4139338 -9.4507523 -10.185915 -10.864864 -10.093502 -9.2811451 -9.1265926 -8.9681416][-8.0695438 -8.3169355 -8.3722477 -7.0189118 -6.8940411 -6.5087032 -6.129096 -6.6966648 -7.5697637 -7.4867258 -7.5214157 -8.2091637 -8.4054089 -7.9272404 -7.9316692]]...]
INFO - root - 2017-12-16 02:49:31.195560: step 89510, loss = 0.15, batch loss = 0.11 (12.0 examples/sec; 0.668 sec/batch; 45h:06m:15s remains)
INFO - root - 2017-12-16 02:49:37.797478: step 89520, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 44h:22m:15s remains)
INFO - root - 2017-12-16 02:49:44.443067: step 89530, loss = 0.15, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 45h:43m:10s remains)
INFO - root - 2017-12-16 02:49:51.100278: step 89540, loss = 0.15, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 45h:10m:34s remains)
INFO - root - 2017-12-16 02:49:57.680352: step 89550, loss = 0.16, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 43h:57m:45s remains)
INFO - root - 2017-12-16 02:50:04.249254: step 89560, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.669 sec/batch; 45h:08m:44s remains)
INFO - root - 2017-12-16 02:50:10.837523: step 89570, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.673 sec/batch; 45h:23m:21s remains)
INFO - root - 2017-12-16 02:50:17.362519: step 89580, loss = 0.14, batch loss = 0.09 (12.6 examples/sec; 0.637 sec/batch; 43h:00m:04s remains)
INFO - root - 2017-12-16 02:50:23.911076: step 89590, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.647 sec/batch; 43h:37m:22s remains)
INFO - root - 2017-12-16 02:50:30.401112: step 89600, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.655 sec/batch; 44h:11m:19s remains)
2017-12-16 02:50:30.977770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4256091 -5.1880379 -4.7467718 -4.5903587 -5.1816564 -5.987483 -6.6858153 -7.1657486 -7.3353996 -8.365365 -8.7994938 -10.917145 -12.559694 -12.947643 -11.101124][-7.2850838 -6.9620953 -6.4221907 -6.2021575 -6.530674 -7.5592289 -8.4656591 -9.4210157 -10.12319 -10.12368 -10.463948 -12.705135 -13.932386 -14.5299 -12.115828][-5.2959361 -5.9001102 -6.4586763 -5.6889114 -6.3665333 -7.7879782 -8.5912418 -8.68675 -8.7103119 -9.3364182 -10.040769 -12.242348 -14.021326 -15.624773 -14.630178][-7.0718207 -6.6779728 -6.2989836 -5.3096628 -5.8461466 -5.9785476 -6.2212768 -7.3698974 -7.7458458 -7.161818 -7.3485546 -10.714306 -13.486125 -15.458632 -14.342522][-10.180023 -10.547872 -9.02149 -5.9061875 -4.4143343 -2.9623773 -2.3930917 -3.3307247 -4.5143538 -5.4250011 -6.8133864 -9.3518953 -11.369715 -14.387383 -14.176588][-11.988207 -10.966476 -8.6755018 -5.0587378 -1.9619465 1.2409315 3.9789338 3.972611 2.9473119 0.0055747032 -4.0937614 -7.8400745 -10.642057 -12.511147 -12.271654][-13.286276 -12.153582 -9.2683954 -4.3824911 -1.0777502 3.1108212 7.2579465 8.0166264 8.0585346 3.418138 -2.0476453 -6.9299769 -10.931856 -12.598316 -11.943629][-12.96266 -11.184433 -8.8014708 -4.4487448 -1.2930722 4.1985192 8.03104 8.1941624 8.7131214 4.8181777 -0.63510513 -6.53761 -11.452335 -13.958891 -13.248049][-10.265801 -9.0678272 -7.8543396 -3.993968 -1.0946307 2.9921002 5.923512 7.9346986 9.0917683 4.4776073 -0.50698137 -7.1586266 -12.091107 -13.767099 -13.371237][-7.7523556 -6.5524311 -5.4865661 -3.3849835 -2.3489168 0.24141932 2.601254 5.0158725 5.7338672 2.759953 -1.0439796 -7.1491985 -11.826395 -14.748924 -14.357395][-9.6359921 -9.0536423 -7.7014456 -6.7858825 -6.6126528 -5.2319775 -4.42792 -3.3734019 -2.8818352 -3.7979279 -5.9762383 -11.124966 -13.812668 -14.941395 -13.321959][-15.748615 -13.707474 -11.902847 -11.474639 -11.405553 -10.772655 -10.578302 -10.205626 -9.8972464 -10.427115 -11.546503 -13.10231 -13.046122 -14.188707 -12.749885][-15.14535 -13.764624 -13.629684 -13.525256 -13.463576 -12.755751 -12.226391 -11.722263 -12.534762 -12.705444 -12.527328 -12.925497 -12.268347 -11.055499 -9.08431][-11.172635 -10.399413 -9.9595556 -8.9042578 -8.3122931 -9.2625961 -10.158764 -10.013867 -10.391973 -10.48303 -10.868933 -10.145445 -8.9257517 -8.464263 -7.9484267][-8.2876587 -6.189189 -4.4931774 -3.759275 -3.4531181 -3.5027597 -3.7003789 -4.9527326 -6.4104986 -6.4166923 -6.772212 -7.8499122 -8.1893015 -8.0671759 -8.1767654]]...]
INFO - root - 2017-12-16 02:50:37.550128: step 89610, loss = 0.19, batch loss = 0.15 (12.0 examples/sec; 0.664 sec/batch; 44h:49m:46s remains)
INFO - root - 2017-12-16 02:50:44.185456: step 89620, loss = 0.16, batch loss = 0.12 (12.0 examples/sec; 0.667 sec/batch; 44h:59m:49s remains)
INFO - root - 2017-12-16 02:50:50.803075: step 89630, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 44h:48m:02s remains)
INFO - root - 2017-12-16 02:50:57.354257: step 89640, loss = 0.21, batch loss = 0.17 (12.4 examples/sec; 0.643 sec/batch; 43h:23m:18s remains)
INFO - root - 2017-12-16 02:51:03.959094: step 89650, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 44h:56m:47s remains)
INFO - root - 2017-12-16 02:51:10.601745: step 89660, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 44h:47m:01s remains)
INFO - root - 2017-12-16 02:51:17.229378: step 89670, loss = 0.17, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 43h:23m:46s remains)
INFO - root - 2017-12-16 02:51:23.942045: step 89680, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.668 sec/batch; 45h:05m:16s remains)
INFO - root - 2017-12-16 02:51:30.626717: step 89690, loss = 0.12, batch loss = 0.08 (12.0 examples/sec; 0.666 sec/batch; 44h:53m:41s remains)
INFO - root - 2017-12-16 02:51:37.175207: step 89700, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 44h:41m:35s remains)
2017-12-16 02:51:37.755134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9522431 -4.4855895 -4.5187607 -4.7541471 -5.2726278 -5.7943559 -5.9936476 -6.6680651 -6.7315607 -6.2464871 -5.8297806 -8.2709951 -10.288298 -10.848207 -9.5582638][-3.7056954 -3.9878552 -4.78543 -5.1384816 -6.2441359 -7.2151418 -7.7497334 -8.0089779 -7.7671604 -7.1515808 -6.9437938 -8.59805 -10.947331 -11.691204 -10.596436][-3.3155973 -3.8626385 -5.0566754 -5.4886975 -6.9308434 -7.1421814 -6.8442612 -6.248785 -5.6748195 -5.4781389 -4.6442819 -6.1756916 -8.696373 -9.2444515 -7.92607][-3.9234276 -4.4047041 -5.7828064 -6.31929 -7.0135026 -5.6987839 -4.4793644 -4.1416521 -3.8158114 -3.15224 -3.2782938 -5.055696 -7.3271527 -7.84462 -7.3357205][-5.0262771 -5.9052472 -7.0449767 -7.2721348 -6.8708124 -5.1968956 -2.7847781 -1.3988228 -1.2228804 -1.5040994 -2.0064154 -4.65245 -7.8184409 -8.7078953 -8.4535942][-4.79224 -5.6711707 -5.8923903 -4.9979815 -4.4577503 -1.7737825 1.3674312 1.3857856 0.85681486 -0.68507195 -1.2256374 -3.601722 -6.1381426 -7.1556668 -6.461103][-6.0543466 -6.48265 -6.487411 -4.7318406 -3.3003228 0.12795353 2.9617362 4.3884625 3.9432101 2.6409097 0.14635992 -3.3416483 -6.8218951 -6.74138 -6.0926461][-8.1286726 -7.7720108 -7.1481128 -3.743252 -1.5078411 2.2912383 5.0101209 4.2744174 3.5060449 2.1085658 0.55627537 -2.4462557 -6.0244761 -7.0584702 -6.4297166][-8.5927572 -7.8019857 -6.118259 -3.8976331 -1.7756798 1.4674921 3.0782933 3.9254546 3.7735267 1.870955 0.28303051 -3.8731904 -6.8898 -6.9845247 -4.7875195][-7.4242344 -7.0777111 -5.7283425 -2.9991155 -1.7746418 0.3528738 1.745976 2.4310412 2.43895 0.7537117 -1.4800916 -3.8684301 -6.650826 -6.8312936 -6.2374344][-10.57229 -9.606041 -7.5411825 -5.544292 -4.9293742 -3.5928335 -2.9731727 -2.2992322 -2.0277107 -3.1361394 -4.50624 -7.582571 -9.7897539 -8.7649078 -5.9200153][-13.778349 -11.439358 -9.1293869 -6.2786746 -4.9479532 -4.6881719 -5.2966738 -6.8658648 -8.0342617 -8.5599747 -9.5772209 -10.925526 -10.425925 -9.0085039 -6.7788596][-13.901031 -11.399737 -8.09293 -6.1435614 -4.8707042 -4.2185783 -4.7985969 -5.9241762 -7.9503989 -9.236433 -10.517435 -11.057821 -10.936467 -9.040246 -6.0382824][-12.471096 -11.592669 -9.1072826 -6.8268876 -5.2094779 -4.4954996 -4.7233682 -5.6700854 -6.7174864 -8.0015221 -8.8743258 -8.3557 -8.2455206 -7.1986194 -5.6511421][-8.2264128 -7.3387494 -5.2831917 -4.3967638 -3.7859912 -4.4260168 -4.5125012 -5.510685 -6.0475311 -7.2885351 -7.826962 -8.2785816 -8.3072367 -6.9266844 -5.0337219]]...]
INFO - root - 2017-12-16 02:51:44.273506: step 89710, loss = 0.14, batch loss = 0.10 (12.7 examples/sec; 0.631 sec/batch; 42h:31m:42s remains)
INFO - root - 2017-12-16 02:51:50.848784: step 89720, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.655 sec/batch; 44h:11m:11s remains)
INFO - root - 2017-12-16 02:51:57.348897: step 89730, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 44h:11m:15s remains)
INFO - root - 2017-12-16 02:52:03.925933: step 89740, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.656 sec/batch; 44h:14m:19s remains)
INFO - root - 2017-12-16 02:52:10.625843: step 89750, loss = 0.18, batch loss = 0.13 (12.1 examples/sec; 0.661 sec/batch; 44h:35m:49s remains)
INFO - root - 2017-12-16 02:52:17.202329: step 89760, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.649 sec/batch; 43h:44m:26s remains)
INFO - root - 2017-12-16 02:52:23.827475: step 89770, loss = 0.18, batch loss = 0.13 (11.9 examples/sec; 0.672 sec/batch; 45h:20m:02s remains)
INFO - root - 2017-12-16 02:52:30.408479: step 89780, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.648 sec/batch; 43h:42m:18s remains)
INFO - root - 2017-12-16 02:52:36.973855: step 89790, loss = 0.15, batch loss = 0.11 (12.3 examples/sec; 0.651 sec/batch; 43h:54m:18s remains)
INFO - root - 2017-12-16 02:52:43.581472: step 89800, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.662 sec/batch; 44h:39m:14s remains)
2017-12-16 02:52:44.150681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9983277 -6.7963223 -6.3800812 -6.2643909 -6.90549 -7.2178912 -7.1875176 -6.1213593 -4.9823446 -4.2331243 -3.2819898 -5.0035038 -6.2450209 -7.1612058 -6.6252441][-5.9743233 -5.7660561 -5.8213105 -6.2497058 -7.3803844 -7.6782775 -7.4982486 -6.7458372 -6.03245 -5.3943162 -4.9239359 -6.7632594 -8.143177 -9.2579613 -8.6666975][-2.9837694 -4.5242057 -5.9570332 -5.4724731 -6.5574679 -7.2060118 -7.3680325 -6.5630631 -5.8537946 -5.8490491 -6.2460327 -8.4738855 -10.073704 -10.634256 -9.9182835][-4.469492 -4.3266511 -4.96005 -5.7160158 -7.2454123 -6.807816 -5.8609943 -5.3827939 -5.789166 -5.7548771 -5.8943315 -8.3987951 -10.86009 -11.961058 -11.366158][-4.5186825 -5.1850348 -6.2404723 -6.1111565 -6.2346492 -4.0552711 -1.4773455 -2.085649 -4.1146631 -4.5726843 -5.1252308 -7.8031092 -10.378132 -12.083167 -11.863791][-6.6728597 -7.03926 -6.9477377 -5.4595222 -3.4946463 -0.58144903 1.7160029 2.2931285 2.0262728 -0.97096252 -4.1183195 -6.9797564 -9.1891708 -11.160204 -11.510794][-8.15097 -7.8046932 -6.7255497 -3.7897832 -1.485147 1.3949995 4.2309403 4.88514 4.2901435 0.49894094 -2.5716624 -6.2126451 -8.9682493 -10.064865 -9.1454287][-6.3265495 -5.9073157 -5.3750777 -2.0059071 0.14730787 2.7617536 5.1027884 5.3671327 4.8057771 1.4366589 -1.2449384 -4.8914189 -7.107749 -8.1049957 -7.5137553][-3.9534855 -3.4480543 -2.9049442 -0.59946966 0.48238564 3.3968339 4.4087214 3.2793441 2.9336066 1.0888033 -0.70694351 -4.38859 -6.6914248 -7.1869426 -5.4330144][-3.106528 -3.0782576 -3.1915064 -1.4664483 -0.84990311 0.43483305 0.82780027 1.1813998 0.92059231 -0.30407476 -0.72723293 -3.8006406 -6.1212745 -7.2244372 -6.3082776][-9.81044 -8.5236454 -6.7343726 -5.167305 -5.5854669 -5.2587185 -4.7865591 -4.0596833 -3.5233786 -3.6669593 -3.9229286 -6.5421247 -7.8100147 -7.8191614 -6.0170097][-13.373201 -13.243706 -11.546764 -10.49058 -10.170735 -9.4409752 -9.6285648 -9.0103979 -8.5568876 -8.3961649 -7.8624821 -8.90287 -9.04642 -8.6035538 -6.6784415][-14.113396 -13.68778 -12.638885 -12.020409 -10.892298 -9.8865948 -9.84776 -10.398048 -11.040823 -10.052748 -9.4915714 -9.9814968 -9.3275423 -8.2980995 -5.8444614][-10.240279 -10.236734 -9.4780846 -9.2715435 -9.0127249 -8.9130993 -8.8047848 -8.7623091 -8.8750267 -8.9957886 -9.377099 -8.9821444 -8.3850231 -7.5180383 -6.4133205][-8.4630613 -8.4387646 -8.30093 -7.63735 -6.6450462 -6.739852 -6.7368069 -6.6518583 -6.0268269 -5.5414691 -5.2375212 -6.0734124 -7.1350126 -7.0372629 -7.1563063]]...]
INFO - root - 2017-12-16 02:52:50.765633: step 89810, loss = 0.11, batch loss = 0.07 (12.3 examples/sec; 0.650 sec/batch; 43h:48m:10s remains)
INFO - root - 2017-12-16 02:52:57.327796: step 89820, loss = 0.16, batch loss = 0.11 (12.0 examples/sec; 0.664 sec/batch; 44h:46m:20s remains)
INFO - root - 2017-12-16 02:53:03.910111: step 89830, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.680 sec/batch; 45h:51m:41s remains)
INFO - root - 2017-12-16 02:53:10.572530: step 89840, loss = 0.13, batch loss = 0.09 (11.9 examples/sec; 0.671 sec/batch; 45h:15m:28s remains)
INFO - root - 2017-12-16 02:53:17.242895: step 89850, loss = 0.16, batch loss = 0.12 (12.2 examples/sec; 0.655 sec/batch; 44h:08m:32s remains)
INFO - root - 2017-12-16 02:53:23.925840: step 89860, loss = 0.12, batch loss = 0.08 (11.5 examples/sec; 0.694 sec/batch; 46h:46m:37s remains)
INFO - root - 2017-12-16 02:53:30.423564: step 89870, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.667 sec/batch; 44h:57m:31s remains)
INFO - root - 2017-12-16 02:53:37.042060: step 89880, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 44h:29m:11s remains)
INFO - root - 2017-12-16 02:53:43.615731: step 89890, loss = 0.15, batch loss = 0.10 (11.6 examples/sec; 0.688 sec/batch; 46h:22m:40s remains)
INFO - root - 2017-12-16 02:53:50.210828: step 89900, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.659 sec/batch; 44h:25m:12s remains)
2017-12-16 02:53:50.804410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7806034 -6.4030867 -6.8338962 -6.0669966 -6.2243614 -7.1488838 -8.2162933 -9.1970539 -9.3206844 -9.2206755 -8.41633 -8.8158388 -9.1826591 -6.4824085 -3.6627305][-6.4370532 -6.7400508 -5.7545028 -5.7362003 -6.4189415 -7.3081832 -8.087781 -9.7819118 -10.514864 -9.9269037 -9.5090132 -10.458281 -10.255268 -8.7037106 -7.4324579][-3.6956775 -5.1952047 -6.081522 -4.8631849 -5.2368436 -5.6266026 -6.3413205 -7.7704163 -8.5221672 -8.8089237 -9.0111046 -9.7311954 -9.3166952 -8.4280472 -7.4578428][-5.78668 -6.6046381 -6.4143276 -6.0008421 -5.6836176 -5.1896391 -5.0913134 -6.0329065 -7.1724348 -7.109015 -7.1599379 -8.8720379 -9.6417017 -8.8491983 -7.7337685][-7.7070713 -9.013689 -8.2013607 -6.7702522 -5.2262292 -3.4697986 -2.0267551 -3.5878675 -5.7856793 -5.8723879 -6.4378414 -8.4984818 -9.087328 -8.9389677 -8.6879215][-8.7264109 -9.2937355 -8.570632 -6.2213016 -4.0692382 -1.5297666 1.0898914 0.84140444 -0.24419403 -2.6452935 -5.6006422 -7.1756935 -7.8030086 -7.6720629 -7.2206974][-8.745141 -9.59503 -7.9158268 -4.490622 -1.5616412 1.981462 4.5104852 4.4860568 4.2345271 1.6827669 -1.712091 -5.0263276 -6.6947141 -6.2423983 -5.6435318][-10.014111 -9.290391 -7.1705337 -4.2287126 -1.7854807 3.0763764 6.2187304 5.5884166 4.6278644 3.1823163 0.9118228 -3.4666238 -6.0365496 -5.8548589 -6.7242589][-8.6158161 -7.6255746 -6.90038 -4.8218517 -2.6054142 0.32900572 2.7687879 3.2179055 3.5303559 1.149941 -1.4818635 -3.7324705 -5.4715252 -6.2771335 -7.2969408][-8.3730507 -7.6808691 -6.6751785 -4.2265458 -2.7318292 -1.9247196 -0.095762253 1.3023996 1.8685393 0.02338028 -1.4267044 -4.4474211 -6.28675 -6.373672 -6.97453][-8.5497971 -9.0612478 -8.4195976 -5.9424176 -4.8103571 -3.3090181 -1.333509 -1.0551734 -0.9510355 -0.89682579 -1.7035961 -5.7297468 -8.2976475 -8.51502 -8.904191][-10.762543 -12.084621 -12.477758 -10.520411 -9.4795275 -7.5010452 -5.8859248 -5.6033783 -5.3547411 -5.7268653 -6.7955046 -8.4476767 -9.2922068 -9.868947 -11.059341][-10.595395 -10.225435 -10.324469 -10.636337 -10.951967 -9.6992407 -8.352602 -6.5933232 -6.2237744 -6.9143157 -7.0547361 -7.9197364 -8.6034489 -7.7508583 -7.4686079][-8.8611717 -8.6173449 -8.6907949 -8.1908474 -7.6936774 -7.98289 -7.9753885 -6.8306255 -6.5914989 -6.6028051 -7.2557883 -7.5366983 -6.3329515 -5.2756829 -5.108777][-4.776556 -4.53955 -4.0686789 -3.7679048 -3.5512803 -2.974138 -2.5223591 -3.1798358 -4.51464 -4.2559733 -4.6844826 -6.1772318 -7.3537173 -6.951736 -6.8514109]]...]
INFO - root - 2017-12-16 02:53:57.420813: step 89910, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 43h:41m:37s remains)
INFO - root - 2017-12-16 02:54:04.101032: step 89920, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 44h:26m:10s remains)
INFO - root - 2017-12-16 02:54:10.777590: step 89930, loss = 0.19, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 44h:20m:59s remains)
INFO - root - 2017-12-16 02:54:17.327908: step 89940, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 44h:06m:12s remains)
INFO - root - 2017-12-16 02:54:23.922847: step 89950, loss = 0.13, batch loss = 0.08 (11.3 examples/sec; 0.709 sec/batch; 47h:46m:13s remains)
INFO - root - 2017-12-16 02:54:30.490919: step 89960, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 46h:09m:11s remains)
INFO - root - 2017-12-16 02:54:37.076682: step 89970, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.663 sec/batch; 44h:38m:14s remains)
INFO - root - 2017-12-16 02:54:43.713777: step 89980, loss = 0.13, batch loss = 0.08 (11.3 examples/sec; 0.707 sec/batch; 47h:36m:31s remains)
INFO - root - 2017-12-16 02:54:50.413712: step 89990, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.654 sec/batch; 44h:02m:56s remains)
INFO - root - 2017-12-16 02:54:57.017103: step 90000, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 44h:13m:15s remains)
2017-12-16 02:54:57.550147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1882663 -3.4972892 -3.5567069 -3.4253194 -4.7446685 -5.8467455 -6.4506955 -6.7002459 -6.4131994 -6.8218441 -7.84455 -9.833931 -11.428547 -13.554192 -12.280251][-3.4774814 -2.6575236 -3.8718936 -4.70107 -5.6573658 -6.5419178 -6.7315311 -6.391818 -6.6239266 -6.9018588 -7.4703131 -9.9486227 -11.398511 -13.866291 -12.645119][-1.9901783 -2.5077806 -3.5696421 -4.5299163 -6.15324 -6.0831213 -6.2931514 -6.0880313 -5.4899988 -5.8596888 -6.5237312 -8.9205847 -10.141441 -12.830612 -11.940922][-4.2731428 -3.5387769 -4.4469867 -5.3878465 -6.9754481 -5.8483238 -4.1066928 -3.8572545 -3.8927207 -4.1368561 -4.3237472 -6.8091931 -8.430563 -12.134802 -10.833484][-3.3798556 -4.2942915 -6.3091426 -6.8278151 -7.5831122 -5.72932 -2.55929 -1.9070191 -2.9447312 -3.9872198 -4.6694932 -7.2534204 -8.6130791 -12.371159 -11.842125][-4.4348917 -5.3517447 -7.1949949 -7.9206843 -6.460927 -2.8416121 1.6379957 3.2010188 1.7762322 -0.56767082 -2.944752 -6.3278 -7.5073662 -11.804524 -11.243008][-4.2773905 -5.574019 -6.3029547 -6.2386632 -4.4824128 -0.15971756 3.9194398 6.394033 6.302361 2.0846872 -1.6420226 -6.2357197 -8.8174076 -10.874779 -8.1674881][-5.4099221 -6.2100782 -6.3746881 -4.7845354 -2.7820311 2.1764755 6.4306664 7.9911971 7.419064 3.5063443 -0.648715 -5.2429543 -6.1689382 -9.380312 -8.376771][-5.5003281 -6.2436929 -6.8629408 -4.650301 -2.5880525 1.1879625 4.3249078 6.2929807 5.7517104 2.51973 -1.2126374 -6.6016855 -8.8932686 -10.237981 -7.3183775][-7.1854792 -8.0622864 -8.5755663 -6.6468878 -4.6872272 -0.51767254 1.8328977 3.0173688 2.2259707 -1.2913961 -3.9946404 -8.0497952 -9.7734222 -12.907481 -11.123889][-10.408689 -11.161909 -10.769162 -9.7081556 -7.6750712 -5.2408252 -4.1690006 -3.4073114 -3.3660786 -4.839036 -5.9547596 -10.03266 -12.184717 -14.487848 -12.031689][-13.661018 -14.380777 -13.206293 -10.92099 -9.3139715 -6.9933457 -6.0680609 -6.2411051 -5.9971948 -7.5279837 -8.234333 -9.4730492 -9.8254662 -13.149738 -11.520399][-13.521561 -13.978725 -13.352018 -11.872311 -10.741311 -9.16262 -8.6013927 -7.5328865 -7.8942261 -8.2608261 -8.2902622 -9.0230312 -9.4768429 -11.817968 -10.395285][-14.006208 -13.220505 -12.567825 -10.495203 -9.101244 -7.8220234 -7.4488096 -7.3207984 -6.8893385 -7.888442 -9.1869793 -8.1475325 -8.1774025 -9.0343857 -7.6150079][-10.084278 -10.15685 -9.5535879 -8.3936758 -7.3924818 -7.0160484 -6.7079248 -6.5357485 -6.5764756 -7.004199 -6.8567152 -7.8442755 -8.9553394 -9.5398521 -10.096828]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-90000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100/model.ckpt-90000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 02:55:05.013865: step 90010, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.655 sec/batch; 44h:05m:27s remains)
INFO - root - 2017-12-16 02:55:11.634799: step 90020, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 44h:19m:36s remains)
INFO - root - 2017-12-16 02:55:18.287735: step 90030, loss = 0.20, batch loss = 0.15 (12.1 examples/sec; 0.659 sec/batch; 44h:24m:19s remains)
INFO - root - 2017-12-16 02:55:24.890060: step 90040, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 43h:59m:46s remains)
INFO - root - 2017-12-16 02:55:31.432211: step 90050, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 43h:43m:35s remains)
INFO - root - 2017-12-16 02:55:38.091373: step 90060, loss = 0.13, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 44h:38m:47s remains)
INFO - root - 2017-12-16 02:55:44.633687: step 90070, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.664 sec/batch; 44h:43m:18s remains)
INFO - root - 2017-12-16 02:55:51.271494: step 90080, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.690 sec/batch; 46h:27m:43s remains)
INFO - root - 2017-12-16 02:55:57.771216: step 90090, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.646 sec/batch; 43h:29m:53s remains)
INFO - root - 2017-12-16 02:56:04.312001: step 90100, loss = 0.20, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 43h:05m:16s remains)
2017-12-16 02:56:04.852955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5244622 -6.3073578 -5.64278 -4.6082668 -4.7063313 -4.7983904 -4.9950624 -5.60204 -5.8303823 -4.2352519 -3.3929558 -4.7326913 -5.5869427 -5.3299751 -4.9949312][-6.2328653 -7.0005422 -6.8748121 -6.7551084 -7.1295562 -7.452919 -7.5487976 -7.5720396 -7.1572027 -6.6598105 -5.9961419 -7.1148 -8.6080132 -7.6041284 -5.3255095][-5.457056 -6.6323671 -7.1649604 -7.7035618 -8.1586008 -8.3234892 -8.54085 -7.9928708 -7.4736209 -6.996316 -6.3242226 -7.8024359 -9.1969824 -8.848896 -7.9494004][-6.592896 -6.962966 -7.7461133 -8.1607513 -7.9205379 -7.5144391 -7.5160894 -7.48233 -7.0563278 -6.6748428 -6.3222218 -8.0502958 -9.8525648 -9.5043116 -8.1494751][-7.31209 -8.3800793 -10.167297 -9.2667322 -7.5122614 -5.5196571 -4.3617959 -4.6523838 -5.6497073 -5.550282 -5.4366159 -7.9954672 -10.179171 -10.291364 -9.5768023][-10.048791 -10.019619 -9.59395 -7.8674283 -5.1434412 -2.31454 0.11460447 0.52522612 -0.29174089 -1.6184621 -3.3304026 -6.5708408 -9.0220318 -9.8244686 -9.2890053][-11.211111 -10.58849 -8.6821442 -5.7043295 -3.2211049 1.0451097 4.6692042 5.0028567 4.8361707 2.1698585 -1.8736091 -5.6404862 -8.5881653 -9.470665 -8.8978271][-10.758365 -10.620635 -8.3153944 -4.1584587 -1.0028811 2.7769504 6.0427537 6.4768453 6.3424954 3.3621869 0.076574326 -4.2359304 -8.5930786 -8.9327488 -6.9085436][-8.2981424 -8.374382 -6.6738987 -4.4297376 -1.6296015 2.1058207 4.472095 5.3358121 5.4409585 2.8738351 0.021147728 -4.3903737 -8.3672333 -8.66048 -7.4971762][-6.83529 -6.5143685 -6.3515339 -4.0379829 -2.2974143 -0.45085716 1.9536042 2.7507453 2.2702661 1.0776987 -0.11952209 -4.3242378 -7.9742842 -9.318182 -9.7799015][-10.515528 -9.5526781 -8.5217266 -6.4411254 -4.9873681 -3.4101999 -1.6644263 -1.8508191 -2.4399426 -2.9084914 -4.291275 -7.5098348 -8.7618055 -9.8865814 -9.606637][-14.349237 -14.56687 -12.22262 -9.5298834 -9.2689819 -7.6289015 -6.6205325 -6.7788696 -7.6146874 -8.1113758 -8.5049839 -9.7613382 -9.9265614 -10.080902 -9.6402826][-14.160376 -13.838207 -12.096011 -10.714348 -9.6919661 -8.9164667 -8.90979 -8.1633778 -8.074398 -8.8444071 -9.5986423 -10.59333 -10.431311 -8.9546022 -6.6910739][-10.797199 -10.921696 -10.741525 -8.7774477 -7.7768126 -7.4347348 -6.7760181 -6.8912191 -7.7751923 -7.9870243 -8.88105 -9.0258369 -9.0800495 -7.8237896 -6.2574997][-8.3371544 -8.1178093 -7.6190281 -6.28388 -4.7961893 -3.7129657 -3.3893752 -4.3755569 -5.499197 -6.066555 -6.7547407 -7.7692604 -8.6815176 -8.9428453 -7.8882923]]...]
INFO - root - 2017-12-16 02:56:11.445835: step 90110, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 43h:43m:54s remains)
INFO - root - 2017-12-16 02:56:18.085416: step 90120, loss = 0.12, batch loss = 0.08 (11.8 examples/sec; 0.680 sec/batch; 45h:46m:19s remains)
INFO - root - 2017-12-16 02:56:24.717350: step 90130, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 44h:32m:02s remains)
INFO - root - 2017-12-16 02:56:31.248871: step 90140, loss = 0.13, batch loss = 0.08 (12.6 examples/sec; 0.637 sec/batch; 42h:51m:57s remains)
INFO - root - 2017-12-16 02:56:37.862833: step 90150, loss = 0.15, batch loss = 0.11 (11.6 examples/sec; 0.687 sec/batch; 46h:15m:03s remains)
INFO - root - 2017-12-16 02:56:44.522956: step 90160, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.674 sec/batch; 45h:22m:15s remains)
INFO - root - 2017-12-16 02:56:51.146387: step 90170, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.646 sec/batch; 43h:30m:23s remains)
INFO - root - 2017-12-16 02:56:57.714889: step 90180, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.648 sec/batch; 43h:36m:27s remains)
INFO - root - 2017-12-16 02:57:04.375915: step 90190, loss = 0.12, batch loss = 0.07 (11.5 examples/sec; 0.693 sec/batch; 46h:38m:15s remains)
INFO - root - 2017-12-16 02:57:11.112248: step 90200, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.671 sec/batch; 45h:08m:38s remains)
2017-12-16 02:57:11.640269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4225354 -5.9800057 -5.69758 -5.5842767 -6.1546426 -4.8084135 -3.888844 -3.6822512 -3.0443881 -2.0967355 -1.0439606 -1.2589536 -2.6791756 -4.1431637 -5.3321247][-4.4872761 -3.915633 -2.0839744 -1.86198 -2.51802 -1.8458729 -1.1391082 -0.762506 -0.89287758 -0.30238533 0.9188385 0.16438818 -1.788444 -3.4424245 -4.5218749][-2.7735989 -2.7517912 -1.6262236 -1.2351389 -0.92644787 -0.43477392 -0.67306805 -0.67258644 -0.3762126 -0.44716215 -0.3173089 -1.6961098 -3.7089946 -5.8014321 -7.2777796][-5.1518979 -5.056273 -3.005512 -1.8499887 -1.7732081 -1.493072 -1.2030983 -1.090229 -1.4895415 -1.2883196 -0.76081705 -2.6736598 -5.5880861 -7.8690367 -8.5869141][-6.5703664 -6.5046 -4.7006083 -2.4916871 -1.2754626 -0.082810879 0.54428625 -0.38030624 -2.0143034 -1.8873029 -1.4862604 -3.764322 -6.3719831 -8.2481441 -9.6034946][-8.3227634 -7.7531786 -4.5705647 -0.72160578 1.1675172 2.8468919 4.2821679 3.9421458 2.5813937 0.81818008 -1.0139532 -2.3669045 -4.4161406 -7.3691559 -8.6247625][-8.589674 -7.9615231 -5.025084 -1.9584579 0.14896822 3.1323667 5.5289445 5.8656592 5.1484818 2.1990709 -1.2614188 -3.596266 -5.9509034 -7.814693 -9.5275221][-9.1123457 -8.5022945 -6.3373075 -3.2522795 -0.91604185 2.3510118 4.6108632 4.669683 4.2927127 2.1274166 -0.17366076 -2.2875419 -5.3684249 -7.450892 -8.6686382][-7.472105 -7.2057695 -5.3313189 -4.0009012 -2.8585737 0.472476 3.1402259 4.2229466 3.6622548 0.83163595 -1.7345908 -4.1709681 -7.1646118 -8.7534533 -9.2692728][-6.7050529 -6.592453 -6.1885133 -4.6224051 -3.7420495 -2.3131843 -0.0800066 2.2772841 2.3552775 -0.17699051 -2.9683461 -5.11663 -7.5089765 -9.7325029 -11.17647][-11.609249 -10.553282 -9.5894833 -9.0811605 -8.492384 -6.9738207 -5.0748587 -3.6764338 -2.7154558 -2.7959564 -4.6325846 -8.0323668 -10.800985 -11.130693 -10.641316][-15.026743 -14.721558 -12.739271 -11.562297 -10.441802 -9.42254 -9.2624893 -8.7964287 -7.71819 -7.6628652 -8.1924171 -8.8097458 -9.5522308 -10.710585 -11.031118][-14.731443 -13.342477 -11.275891 -11.070143 -10.453037 -9.1618958 -8.1936016 -8.2568989 -8.8630247 -8.6882582 -8.3982544 -8.7840071 -8.8879642 -8.4762239 -7.9184642][-12.707811 -12.023738 -10.194794 -8.0300722 -6.5431213 -7.4240675 -8.1418276 -7.2212653 -7.047893 -7.7287321 -8.9147968 -8.3044748 -7.3725328 -7.1004705 -6.5381083][-9.451436 -8.0311394 -6.4981465 -4.7773042 -3.6481678 -3.1454153 -2.9750018 -3.8233252 -4.9130125 -4.8331332 -5.0590239 -6.2601838 -7.209125 -7.5041122 -7.3047991]]...]
INFO - root - 2017-12-16 02:57:18.240257: step 90210, loss = 0.18, batch loss = 0.14 (12.4 examples/sec; 0.645 sec/batch; 43h:24m:32s remains)
INFO - root - 2017-12-16 02:57:24.855960: step 90220, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.681 sec/batch; 45h:48m:49s remains)
INFO - root - 2017-12-16 02:57:31.446063: step 90230, loss = 0.12, batch loss = 0.07 (11.7 examples/sec; 0.683 sec/batch; 45h:58m:58s remains)
INFO - root - 2017-12-16 02:57:38.083967: step 90240, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.649 sec/batch; 43h:38m:26s remains)
INFO - root - 2017-12-16 02:57:44.646203: step 90250, loss = 0.12, batch loss = 0.08 (12.1 examples/sec; 0.664 sec/batch; 44h:38m:56s remains)
INFO - root - 2017-12-16 02:57:51.137028: step 90260, loss = 0.18, batch loss = 0.14 (12.1 examples/sec; 0.660 sec/batch; 44h:26m:10s remains)
INFO - root - 2017-12-16 02:57:57.797355: step 90270, loss = 0.19, batch loss = 0.14 (12.3 examples/sec; 0.650 sec/batch; 43h:44m:25s remains)
INFO - root - 2017-12-16 02:58:04.394176: step 90280, loss = 0.16, batch loss = 0.12 (12.6 examples/sec; 0.637 sec/batch; 42h:53m:03s remains)
INFO - root - 2017-12-16 02:58:11.099915: step 90290, loss = 0.17, batch loss = 0.13 (11.5 examples/sec; 0.694 sec/batch; 46h:39m:53s remains)
INFO - root - 2017-12-16 02:58:17.686148: step 90300, loss = 0.23, batch loss = 0.18 (12.3 examples/sec; 0.651 sec/batch; 43h:49m:29s remains)
2017-12-16 02:58:18.179700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9110093 -6.0735579 -6.9859629 -6.587265 -6.571559 -7.109643 -6.1313171 -5.4723687 -4.626318 -3.9872882 -3.2306371 -5.8787141 -8.6279964 -10.301814 -10.79373][-3.4274027 -3.7969525 -3.4752436 -3.4864252 -3.9798484 -5.0334339 -5.3512664 -4.7526417 -4.1116161 -3.072777 -2.0290856 -4.3997283 -6.0493884 -9.7872553 -11.957439][-1.4992204 -2.1398737 -2.9339592 -3.6656492 -4.0789628 -4.7530265 -4.3871469 -3.6855373 -2.4444661 -1.716691 -1.3452392 -4.1852083 -7.0589886 -9.9882574 -12.189354][-2.3518903 -3.1841977 -3.4154968 -4.2463245 -4.59313 -4.159523 -3.7726922 -3.512558 -2.5194988 -1.8625894 -1.8379979 -4.7898197 -7.0915017 -10.573133 -12.751865][-2.9087129 -4.1132107 -5.4404626 -5.6255169 -5.0839391 -4.3291416 -1.9733372 -1.9042859 -2.561142 -2.4226589 -1.9209485 -4.9773269 -6.4896936 -8.8376131 -11.728374][-3.68365 -4.5991387 -5.7311516 -4.8070412 -3.8130455 -2.0860956 1.1718092 1.5768023 0.066161633 -1.1759801 -1.9110045 -4.7945127 -6.1033068 -7.9212284 -10.049572][-5.0572042 -4.9536958 -5.4431629 -3.7509637 -1.6706953 0.53406382 3.6924634 4.3915544 4.7121596 2.7484374 -0.29197073 -3.8055809 -6.118453 -7.86708 -9.14711][-4.4302912 -4.8216391 -4.0013952 -2.518291 -0.014286995 3.8409896 6.7536168 6.7090125 7.5554366 5.7812724 2.6075807 -2.5870445 -7.1896162 -8.3764334 -9.1208763][-4.63268 -3.928349 -3.8727903 -2.7596209 -0.63820171 2.8900952 5.4828372 5.8499684 5.5680728 3.9987378 3.2420793 -2.3189564 -6.7527609 -9.3900242 -11.992562][-4.96351 -5.1718173 -3.7229748 -3.2717457 -3.0218785 0.45745277 2.5577817 3.0574412 2.193459 1.0872436 -0.66748667 -5.8637424 -9.7069387 -12.504863 -15.29727][-8.4470119 -8.8791332 -9.1556931 -7.9393511 -6.0510798 -4.6110091 -3.0782998 -2.4155955 -3.7993379 -4.6805239 -4.9179096 -9.6148739 -14.468285 -15.547586 -16.181709][-12.368162 -12.710335 -11.471949 -10.936312 -10.064302 -8.0130978 -7.3407192 -7.4694495 -8.0280685 -9.5119762 -10.003537 -12.04775 -14.007324 -15.943275 -16.292576][-16.443098 -16.36202 -15.695318 -14.59724 -13.772091 -12.848845 -11.760004 -11.554046 -12.487169 -13.522999 -12.915705 -13.613919 -13.345 -13.264168 -11.775912][-15.82991 -15.887884 -14.682442 -15.287102 -14.138351 -12.822832 -12.641819 -11.968963 -11.447253 -10.602716 -11.706308 -11.972237 -10.175976 -11.035713 -10.312016][-11.998713 -12.628484 -11.36659 -10.142855 -9.56579 -11.099532 -11.703641 -12.16178 -11.966618 -10.179355 -8.652956 -9.4256964 -10.143932 -10.321587 -10.258416]]...]
INFO - root - 2017-12-16 02:58:24.732859: step 90310, loss = 0.15, batch loss = 0.11 (12.8 examples/sec; 0.627 sec/batch; 42h:11m:57s remains)
INFO - root - 2017-12-16 02:58:31.300593: step 90320, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.652 sec/batch; 43h:52m:44s remains)
INFO - root - 2017-12-16 02:58:37.913431: step 90330, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.677 sec/batch; 45h:33m:40s remains)
INFO - root - 2017-12-16 02:58:44.472270: step 90340, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 43h:44m:08s remains)
INFO - root - 2017-12-16 02:58:51.174309: step 90350, loss = 0.16, batch loss = 0.11 (11.9 examples/sec; 0.672 sec/batch; 45h:10m:11s remains)
INFO - root - 2017-12-16 02:58:57.817161: step 90360, loss = 0.15, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 42h:55m:07s remains)
INFO - root - 2017-12-16 02:59:04.399303: step 90370, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 43h:42m:55s remains)
INFO - root - 2017-12-16 02:59:10.964057: step 90380, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.648 sec/batch; 43h:33m:37s remains)
INFO - root - 2017-12-16 02:59:17.520284: step 90390, loss = 0.18, batch loss = 0.13 (12.4 examples/sec; 0.643 sec/batch; 43h:16m:37s remains)
INFO - root - 2017-12-16 02:59:24.171716: step 90400, loss = 0.16, batch loss = 0.12 (11.5 examples/sec; 0.697 sec/batch; 46h:50m:54s remains)
2017-12-16 02:59:24.710928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.3634968 -9.3890581 -8.634798 -7.0135975 -6.8072977 -7.51687 -7.824954 -7.4927397 -7.0636225 -7.2912607 -6.74546 -6.337821 -6.9813251 -8.6061249 -8.4626493][-7.969636 -8.67345 -9.3842945 -8.3761559 -7.7853913 -8.2654905 -8.253933 -7.3603544 -6.1384583 -6.0082726 -6.054122 -6.0533838 -7.281601 -9.7609282 -10.273657][-5.8834133 -6.7677073 -8.7683334 -9.6548662 -9.2696495 -7.9975557 -8.8570385 -8.103817 -6.895195 -6.7912974 -6.2316318 -5.5991516 -6.5657825 -9.1730709 -11.229254][-8.3399668 -8.9817238 -10.268954 -11.595211 -10.620472 -7.8580809 -6.7935915 -6.9182286 -7.9197969 -7.874208 -7.0468097 -6.3228397 -7.5501895 -9.2164478 -10.343332][-9.0023108 -11.852716 -13.456798 -12.447842 -8.9164076 -4.360889 -1.7330372 -4.1629243 -7.9482813 -8.1517725 -7.6964049 -7.5768609 -8.7151 -10.292849 -11.598297][-6.86141 -10.685545 -12.41334 -11.65101 -8.4507818 -1.8486986 3.6142354 2.6450992 -2.3532233 -6.2413306 -8.0784969 -7.3563471 -8.5308113 -10.475085 -11.487507][-7.1520796 -9.2119017 -10.423164 -8.0306921 -4.7672262 0.83497477 7.1865 7.7892528 4.9348731 -1.8090467 -7.2779613 -6.9091907 -7.6884851 -9.7014675 -10.476443][-5.5172992 -6.1364117 -6.6954908 -6.3544827 -3.9469972 2.6331773 7.3143392 7.6656642 7.5065017 1.9929662 -2.7291756 -5.0930338 -7.4777541 -8.8166656 -9.3903961][-3.0365443 -4.3806314 -4.570787 -4.6759386 -5.2936826 -0.8022337 3.8241868 4.5980153 3.2615809 -0.92758369 -3.3299334 -5.193305 -8.2724562 -10.158907 -9.5051527][-3.0000062 -3.7121801 -4.2531338 -3.5355158 -4.5686097 -3.2805409 -0.91201162 1.0497928 -0.64380932 -5.0773625 -7.728385 -9.0018749 -10.393578 -11.973632 -13.065514][-7.10521 -6.9950218 -6.6710672 -6.1528192 -6.8317904 -6.0170531 -5.8935575 -4.244669 -5.6090536 -8.9101925 -11.850054 -12.350756 -11.463792 -12.066358 -12.751132][-10.5821 -10.851585 -9.3430882 -8.2388325 -8.7781429 -8.3653755 -7.8039885 -8.4195929 -8.5681343 -10.441887 -13.617596 -13.701633 -12.162538 -12.41339 -11.185127][-10.280212 -10.649993 -9.3482628 -8.5757656 -7.3721304 -7.3400908 -8.2857628 -8.7892933 -9.550024 -9.3333187 -8.8247128 -10.263911 -10.997632 -10.640708 -9.3556881][-8.012476 -9.7514477 -8.8214912 -8.3118267 -7.7282038 -7.4151845 -7.3147955 -7.1579928 -7.1300941 -7.9936419 -8.4949265 -7.3826704 -7.1273584 -7.3975472 -7.3660197][-4.0698056 -5.7572789 -6.1593738 -5.0280423 -5.2340984 -6.1289372 -6.3692074 -6.2865353 -6.4663963 -6.1991324 -5.8861337 -6.2540274 -6.9555683 -6.7467022 -7.1009889]]...]
INFO - root - 2017-12-16 02:59:31.266483: step 90410, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.639 sec/batch; 42h:58m:01s remains)
INFO - root - 2017-12-16 02:59:37.870674: step 90420, loss = 0.17, batch loss = 0.12 (12.3 examples/sec; 0.649 sec/batch; 43h:36m:33s remains)
INFO - root - 2017-12-16 02:59:44.461942: step 90430, loss = 0.15, batch loss = 0.11 (12.4 examples/sec; 0.644 sec/batch; 43h:18m:57s remains)
INFO - root - 2017-12-16 02:59:51.071964: step 90440, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.638 sec/batch; 42h:55m:01s remains)
INFO - root - 2017-12-16 02:59:57.610000: step 90450, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.677 sec/batch; 45h:31m:14s remains)
INFO - root - 2017-12-16 03:00:04.140069: step 90460, loss = 0.14, batch loss = 0.10 (11.7 examples/sec; 0.686 sec/batch; 46h:08m:02s remains)
INFO - root - 2017-12-16 03:00:10.724827: step 90470, loss = 0.19, batch loss = 0.14 (12.1 examples/sec; 0.663 sec/batch; 44h:36m:02s remains)
INFO - root - 2017-12-16 03:00:17.279611: step 90480, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 44h:27m:24s remains)
INFO - root - 2017-12-16 03:00:23.858307: step 90490, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.644 sec/batch; 43h:17m:45s remains)
INFO - root - 2017-12-16 03:00:30.483241: step 90500, loss = 0.12, batch loss = 0.07 (12.1 examples/sec; 0.664 sec/batch; 44h:36m:23s remains)
2017-12-16 03:00:30.975402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.047401 -8.0354385 -8.1105518 -8.6714325 -9.8751545 -10.713184 -11.626791 -11.686132 -11.683939 -11.321122 -10.435421 -10.718302 -10.709958 -11.564507 -11.770689][-9.2402287 -10.084785 -9.8538246 -9.782093 -10.28306 -11.67662 -13.0793 -13.636038 -13.969419 -13.017768 -12.37288 -12.993648 -12.79703 -13.722691 -12.90765][-6.8370028 -9.5574856 -11.561686 -11.366604 -11.405684 -12.104292 -13.253269 -13.581484 -13.140545 -12.453609 -12.274004 -12.87525 -12.952673 -14.067226 -14.544847][-9.0729885 -11.088221 -12.560095 -12.330889 -12.500074 -11.371859 -10.388928 -11.514309 -12.311487 -11.457575 -11.064135 -12.308722 -13.645971 -14.135967 -13.899361][-10.31793 -14.207462 -15.857693 -14.325394 -12.306093 -7.4643764 -5.5307927 -7.9210443 -9.6639538 -10.366264 -11.393488 -12.201652 -12.595957 -14.552084 -15.311829][-12.654009 -14.121494 -14.988958 -15.287039 -12.288462 -3.8489811 1.5717721 0.44584465 -2.4231946 -6.7437487 -9.8136454 -10.576413 -11.927912 -13.922722 -15.072651][-15.016611 -14.978683 -13.046898 -11.158585 -8.3242874 -2.2537858 4.166697 7.0923352 5.990685 -2.35289 -9.3366909 -10.141623 -11.067232 -13.5163 -14.24869][-15.29847 -16.083216 -15.228384 -9.7794828 -3.6493192 0.79223061 5.2027011 7.9433885 7.6452165 1.3353796 -5.761847 -9.8234749 -12.56991 -13.284431 -13.927887][-12.869654 -13.920502 -14.802237 -11.510862 -5.9135633 0.29103756 4.9262433 5.6524959 4.1384339 -0.41599989 -5.3496661 -9.45216 -13.422012 -15.854418 -15.558004][-10.065873 -11.483114 -12.895827 -11.250877 -9.1332722 -4.54177 2.0312796 2.9806247 1.0699768 -2.8276677 -7.5554371 -11.069357 -12.639246 -15.455772 -17.576349][-14.037714 -14.256536 -14.921835 -13.148225 -12.118315 -9.6493912 -6.3217211 -5.4329939 -5.0096636 -6.6785417 -10.542778 -13.594013 -15.541241 -16.14576 -16.092405][-18.247993 -18.173183 -17.648422 -16.022371 -14.883127 -13.776054 -12.912045 -12.890464 -11.648127 -12.101551 -13.792599 -15.384756 -15.670252 -15.808496 -16.039747][-16.166651 -16.50032 -16.45874 -14.588795 -14.268145 -13.685511 -13.166794 -13.857452 -13.59882 -12.825598 -12.478493 -14.33559 -14.909872 -14.063688 -12.895151][-13.660248 -12.740364 -12.46554 -10.968606 -10.489344 -11.142073 -12.095867 -12.107759 -11.309261 -11.386218 -11.564837 -12.085674 -11.486441 -11.395185 -11.763412][-9.137372 -8.4544182 -7.211916 -4.8758364 -5.5836177 -6.1015596 -6.9789767 -7.5708656 -7.3984385 -7.5127678 -7.7562838 -9.2146158 -10.270166 -11.178693 -11.860863]]...]
INFO - root - 2017-12-16 03:00:37.610985: step 90510, loss = 0.20, batch loss = 0.15 (12.0 examples/sec; 0.667 sec/batch; 44h:50m:17s remains)
INFO - root - 2017-12-16 03:00:44.216284: step 90520, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.662 sec/batch; 44h:31m:50s remains)
INFO - root - 2017-12-16 03:00:50.877797: step 90530, loss = 0.14, batch loss = 0.10 (11.5 examples/sec; 0.697 sec/batch; 46h:49m:25s remains)
INFO - root - 2017-12-16 03:00:57.458494: step 90540, loss = 0.14, batch loss = 0.09 (11.8 examples/sec; 0.679 sec/batch; 45h:36m:54s remains)
INFO - root - 2017-12-16 03:01:04.008795: step 90550, loss = 0.15, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 45h:37m:47s remains)
INFO - root - 2017-12-16 03:01:10.626273: step 90560, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 43h:26m:19s remains)
INFO - root - 2017-12-16 03:01:17.246120: step 90570, loss = 0.14, batch loss = 0.09 (11.9 examples/sec; 0.670 sec/batch; 45h:03m:20s remains)
INFO - root - 2017-12-16 03:01:23.889672: step 90580, loss = 0.12, batch loss = 0.07 (11.8 examples/sec; 0.678 sec/batch; 45h:33m:44s remains)
INFO - root - 2017-12-16 03:01:30.500457: step 90590, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 45h:01m:44s remains)
INFO - root - 2017-12-16 03:01:37.101362: step 90600, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.635 sec/batch; 42h:41m:52s remains)
2017-12-16 03:01:37.614239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1804523 -4.7064261 -3.7475641 -2.6300232 -2.5138361 -2.6472332 -2.6267245 -2.8582923 -2.7409511 -2.4711988 -2.3490584 -5.8526344 -8.09774 -7.8785939 -8.4313688][-3.4084597 -3.9994659 -3.6074164 -3.1194372 -3.4712307 -4.1219835 -4.4713669 -4.3694081 -3.9092705 -3.6830826 -4.0180364 -7.3952818 -10.371339 -9.67639 -8.9529972][-2.1599331 -3.7240176 -3.6033084 -3.8253653 -4.2737551 -4.3442168 -4.3835583 -4.41119 -4.4163661 -4.5246205 -4.5870023 -8.1196909 -11.06982 -10.364555 -10.302888][-3.4948754 -4.3564854 -4.419838 -4.3313174 -3.9428856 -3.5617092 -3.6485357 -3.7612014 -4.0676026 -3.590286 -3.0626311 -7.0579462 -10.204175 -10.23978 -10.725603][-4.4892831 -5.703032 -6.4642467 -5.49441 -4.046206 -2.7131059 -1.7568593 -1.4529104 -1.9893079 -1.9820163 -1.4519696 -4.4377317 -7.302496 -8.1646061 -9.2214651][-6.063179 -6.7863216 -6.4367743 -5.4319949 -3.4896736 -1.2317343 0.53928375 0.82057762 1.2112203 1.0975308 0.56550312 -3.0585949 -5.7002668 -6.2687664 -8.5755844][-5.9427361 -6.89798 -6.8476377 -4.8216777 -2.7112117 0.21972561 3.0283322 3.5728354 3.440289 2.4826856 1.3067369 -2.0701249 -5.8406534 -6.4572015 -7.6162457][-4.3486819 -4.9063945 -4.8495507 -2.9858005 -1.594646 0.26175833 2.6323724 4.0483222 4.0940604 2.2818475 1.8114643 -1.8229775 -6.1006012 -6.308259 -6.7699747][-3.1904857 -3.2133071 -3.3627665 -2.1319911 -0.478858 0.62797308 1.8637581 2.92899 3.3383813 2.5569682 2.1881914 -1.9721253 -5.6150575 -5.1784177 -5.0403452][-3.4675505 -3.9315557 -4.0041051 -3.1009526 -1.8858263 -0.13341904 0.904726 1.6426449 1.8328924 1.8344026 1.4776855 -2.5945408 -5.4254112 -5.0682321 -5.2208076][-7.6410837 -7.5276523 -6.4353442 -5.1976461 -3.7521818 -3.0472856 -2.2202909 -1.8157401 -1.5280399 -1.0267849 -1.4078798 -5.5040674 -7.4162331 -5.8360605 -3.9105725][-11.655379 -10.811226 -9.1690578 -6.9003739 -5.6438055 -5.4284973 -4.3059068 -3.8843353 -3.961669 -3.5150144 -3.834321 -6.0309887 -7.1288061 -4.9400768 -2.5517669][-11.256961 -10.307133 -9.2060995 -7.3609538 -5.5417647 -5.1005321 -4.3616562 -4.0841522 -4.2999673 -4.2965689 -4.7684741 -5.3186154 -6.0242357 -4.4488668 -1.3457413][-8.7134094 -8.0779352 -7.5712833 -5.5552211 -4.0991311 -4.8359456 -4.7446504 -4.3112526 -3.8394136 -3.8053877 -4.5956173 -4.80261 -4.6499696 -2.9758198 -1.7035642][-4.6571169 -5.1983261 -5.35237 -4.5226593 -2.9672711 -2.8675418 -3.2966127 -3.7038023 -3.5874407 -3.3124096 -3.9477081 -4.9450779 -4.9432764 -3.8452389 -3.457036]]...]
INFO - root - 2017-12-16 03:01:44.224416: step 90610, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 45h:35m:38s remains)
INFO - root - 2017-12-16 03:01:50.756303: step 90620, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 44h:04m:44s remains)
INFO - root - 2017-12-16 03:01:57.426457: step 90630, loss = 0.20, batch loss = 0.16 (11.9 examples/sec; 0.670 sec/batch; 45h:00m:38s remains)
INFO - root - 2017-12-16 03:02:04.010787: step 90640, loss = 0.16, batch loss = 0.12 (11.5 examples/sec; 0.698 sec/batch; 46h:52m:18s remains)
INFO - root - 2017-12-16 03:02:10.644573: step 90650, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 44h:23m:27s remains)
INFO - root - 2017-12-16 03:02:17.294321: step 90660, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 44h:06m:15s remains)
INFO - root - 2017-12-16 03:02:23.976795: step 90670, loss = 0.12, batch loss = 0.07 (12.2 examples/sec; 0.655 sec/batch; 43h:59m:28s remains)
INFO - root - 2017-12-16 03:02:30.555312: step 90680, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 44h:58m:40s remains)
INFO - root - 2017-12-16 03:02:37.212032: step 90690, loss = 0.16, batch loss = 0.12 (12.4 examples/sec; 0.647 sec/batch; 43h:26m:37s remains)
INFO - root - 2017-12-16 03:02:43.728257: step 90700, loss = 0.13, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 43h:50m:49s remains)
2017-12-16 03:02:44.270185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3302486 -3.271996 -2.4490368 -2.4753344 -3.8948064 -4.7933097 -5.6783204 -5.6925631 -5.5337977 -6.0381136 -6.1693454 -7.9457278 -10.337536 -11.667736 -10.321985][-4.7880588 -3.6485612 -2.3430235 -2.4320982 -3.1883578 -4.1579747 -4.9418941 -5.7391109 -6.0204229 -5.5339551 -5.4880824 -7.8295193 -10.520555 -12.635546 -11.687468][-2.5944543 -2.9726067 -3.1828079 -2.1006193 -2.111321 -3.5334928 -4.6233196 -4.967855 -5.1847734 -5.6999421 -6.0845957 -8.356698 -11.086624 -13.540791 -13.610893][-5.1908402 -4.0896387 -3.0328457 -2.3095384 -2.8442986 -3.3604326 -3.8684537 -5.2584963 -6.3134723 -6.0996022 -6.0873671 -9.1784916 -12.395802 -14.678347 -14.033609][-7.0235047 -6.7707286 -5.2670565 -2.6303229 -0.84412527 0.19710064 0.049372196 -1.9549532 -4.3447504 -5.8199358 -7.5773525 -10.173891 -12.068689 -14.746466 -14.149309][-9.35238 -8.3180094 -5.4757657 -1.1438713 1.9490247 4.6738744 6.4328494 4.3787885 1.650918 -2.25465 -6.4213977 -9.3627453 -11.710478 -13.590128 -12.48033][-12.115902 -9.56791 -6.012629 -1.2166247 2.3724561 6.5166125 9.6437778 8.3707161 6.6911931 1.6708231 -3.9995985 -8.55528 -12.050459 -12.873051 -11.757784][-11.587706 -9.8448925 -6.9983177 -1.576189 2.4000363 7.5725894 10.122869 8.815731 8.1535511 2.9466844 -2.5022302 -7.3833609 -11.629072 -13.434675 -12.645997][-9.015316 -7.7619267 -6.3140707 -2.6723847 0.39370584 4.6120534 6.4094224 6.2455764 5.5427918 0.55677271 -3.354928 -8.4455194 -12.677866 -13.788376 -12.548302][-7.613862 -7.1867995 -6.5546489 -3.9490321 -2.2458005 0.11270714 1.9991703 2.4333434 1.6497726 -1.6965628 -4.8883243 -9.1671515 -12.452213 -14.279711 -13.894356][-11.748636 -11.071875 -9.9924269 -7.6467786 -7.0769634 -5.572618 -4.4821229 -4.4731731 -4.6448555 -6.0431504 -7.652173 -11.826851 -13.803978 -14.729897 -12.965194][-16.778751 -15.178509 -13.928375 -12.184563 -11.534927 -10.702149 -11.029683 -10.777184 -10.161414 -10.69887 -11.05695 -12.211235 -12.245569 -13.452206 -12.025343][-15.335325 -13.855726 -13.144751 -12.735581 -12.304638 -11.782276 -11.775301 -11.858935 -12.581745 -12.151596 -11.698824 -11.725397 -10.9844 -9.9883766 -8.0111389][-11.600641 -10.272661 -9.6396217 -8.334033 -7.8792706 -9.4583 -10.548571 -10.252636 -10.425703 -10.487991 -10.556049 -9.3978634 -8.3990612 -8.02779 -7.0271492][-7.7756453 -5.2137952 -4.1061831 -3.3780622 -3.4087522 -3.7397523 -3.8206654 -5.3537874 -6.7247229 -6.5341296 -6.7065573 -7.6628652 -8.147871 -8.317234 -8.1319647]]...]
INFO - root - 2017-12-16 03:02:50.845297: step 90710, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.658 sec/batch; 44h:12m:00s remains)
INFO - root - 2017-12-16 03:02:57.382173: step 90720, loss = 0.15, batch loss = 0.10 (12.8 examples/sec; 0.625 sec/batch; 41h:59m:25s remains)
INFO - root - 2017-12-16 03:03:03.992814: step 90730, loss = 0.17, batch loss = 0.13 (12.2 examples/sec; 0.658 sec/batch; 44h:09m:28s remains)
INFO - root - 2017-12-16 03:03:10.629072: step 90740, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.671 sec/batch; 45h:04m:18s remains)
INFO - root - 2017-12-16 03:03:17.244564: step 90750, loss = 0.15, batch loss = 0.11 (12.2 examples/sec; 0.653 sec/batch; 43h:52m:11s remains)
INFO - root - 2017-12-16 03:03:23.749862: step 90760, loss = 0.14, batch loss = 0.10 (12.4 examples/sec; 0.645 sec/batch; 43h:17m:56s remains)
INFO - root - 2017-12-16 03:03:30.347520: step 90770, loss = 0.22, batch loss = 0.17 (11.8 examples/sec; 0.676 sec/batch; 45h:23m:36s remains)
INFO - root - 2017-12-16 03:03:37.003330: step 90780, loss = 0.13, batch loss = 0.09 (11.6 examples/sec; 0.691 sec/batch; 46h:24m:45s remains)
INFO - root - 2017-12-16 03:03:43.679800: step 90790, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.661 sec/batch; 44h:22m:00s remains)
INFO - root - 2017-12-16 03:03:50.270902: step 90800, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.650 sec/batch; 43h:37m:37s remains)
2017-12-16 03:03:50.826952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0874238 -6.3622904 -7.0691414 -7.5045834 -7.112885 -6.9224863 -6.3160877 -5.8569584 -5.3443604 -4.7656336 -4.4764147 -5.5180593 -8.6162539 -9.8886719 -9.0549173][-4.9290633 -5.7401032 -7.3695788 -7.7264395 -7.8610411 -7.5834475 -6.7636752 -5.8581681 -5.6549749 -5.3389082 -5.7320795 -7.5267811 -10.545698 -11.052833 -10.251699][-3.7316132 -4.2011852 -5.4928827 -4.4031239 -4.4037967 -4.7381911 -4.4892035 -4.56516 -5.1761742 -5.1547403 -5.3931208 -5.8838549 -8.3507156 -8.81293 -8.0176373][-3.9402814 -3.3533564 -2.4908962 -2.3951585 -2.6736004 -1.7891409 -1.3060455 -1.9035485 -2.6660798 -3.4424255 -4.0507412 -4.3739471 -6.6699257 -7.252584 -6.9938836][-3.8739305 -4.2661371 -4.195219 -3.0293565 -2.2438309 -0.92369032 -0.19553804 -0.89443636 -1.8033042 -2.269721 -2.8121278 -3.6548815 -5.4032235 -5.7313418 -5.9699783][-5.2444358 -4.6486812 -2.7206697 -1.633337 -0.64547062 0.79912996 1.1799145 1.0493746 1.285347 0.38055992 -0.77431631 -1.0430856 -3.4895976 -4.4457169 -4.8649592][-5.8623309 -5.7054963 -4.7813487 -1.6010685 0.27345085 1.5374503 2.3290153 2.2479825 2.1583209 1.5325184 1.2456007 0.18373394 -2.6058953 -3.5533009 -4.0125351][-6.1528873 -6.0457726 -4.3802295 -2.1001952 -0.88681555 1.1277533 2.4732347 2.601212 2.8357673 2.0091863 1.3098369 0.17671108 -2.9509459 -4.8869834 -4.9731846][-7.8274784 -7.3039145 -6.2525892 -3.0602531 -0.98316383 0.85578632 1.9190221 2.1219654 2.3725781 1.8746042 1.5519218 0.12292624 -3.1996651 -4.4519672 -4.7457056][-6.5889444 -6.6434722 -6.261858 -4.0624504 -3.1999359 -0.22377014 0.41657448 0.5834136 0.36996984 -0.22117901 -1.1315131 -2.149781 -4.31503 -4.5065 -4.5796385][-10.62194 -10.018375 -8.8013668 -6.7753849 -5.8661871 -4.1351008 -3.8284278 -3.54188 -4.60108 -5.1962109 -5.6344309 -6.3442593 -6.8897443 -6.1386237 -4.7209554][-12.636862 -11.731654 -10.491667 -9.05964 -8.140502 -7.5276237 -8.1051483 -8.88554 -8.7976646 -9.3181686 -9.9312763 -9.531105 -8.6147346 -7.569005 -6.0321474][-13.515354 -11.967369 -10.088991 -8.0637808 -7.1757717 -7.2378988 -8.4081745 -9.7145939 -10.850196 -11.259632 -10.809106 -9.6900873 -9.12063 -7.4953146 -5.521595][-11.174417 -10.816635 -9.2939529 -7.3293676 -6.4382405 -6.8657422 -7.0962811 -7.1414938 -6.8188581 -7.5452242 -7.7417469 -6.4556394 -6.0758524 -5.6721039 -5.2345691][-8.3550415 -8.5360584 -7.5626802 -6.6850405 -5.5993967 -5.4787908 -5.6324282 -5.1831341 -4.7245626 -4.5082903 -3.492624 -3.6924608 -4.3585262 -4.7574072 -4.600379]]...]
INFO - root - 2017-12-16 03:03:57.350367: step 90810, loss = 0.28, batch loss = 0.23 (12.6 examples/sec; 0.637 sec/batch; 42h:46m:42s remains)
INFO - root - 2017-12-16 03:04:04.003956: step 90820, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.668 sec/batch; 44h:50m:08s remains)
INFO - root - 2017-12-16 03:04:10.634773: step 90830, loss = 0.14, batch loss = 0.09 (12.0 examples/sec; 0.664 sec/batch; 44h:35m:42s remains)
INFO - root - 2017-12-16 03:04:17.095401: step 90840, loss = 0.15, batch loss = 0.11 (12.6 examples/sec; 0.637 sec/batch; 42h:44m:07s remains)
INFO - root - 2017-12-16 03:04:23.633863: step 90850, loss = 0.12, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 43h:51m:56s remains)
INFO - root - 2017-12-16 03:04:30.263315: step 90860, loss = 0.12, batch loss = 0.07 (12.0 examples/sec; 0.666 sec/batch; 44h:43m:03s remains)
INFO - root - 2017-12-16 03:04:36.856262: step 90870, loss = 0.15, batch loss = 0.11 (11.9 examples/sec; 0.670 sec/batch; 44h:56m:25s remains)
INFO - root - 2017-12-16 03:04:43.441605: step 90880, loss = 0.15, batch loss = 0.10 (12.1 examples/sec; 0.661 sec/batch; 44h:20m:09s remains)
INFO - root - 2017-12-16 03:04:50.023008: step 90890, loss = 0.14, batch loss = 0.09 (12.2 examples/sec; 0.653 sec/batch; 43h:51m:04s remains)
INFO - root - 2017-12-16 03:04:56.618861: step 90900, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.647 sec/batch; 43h:24m:00s remains)
2017-12-16 03:04:57.165606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.505394 -6.0283957 -6.0847983 -6.1166077 -6.9542952 -6.6594505 -5.759366 -5.5153751 -5.895473 -6.5074053 -6.6769538 -7.1408763 -7.7135763 -8.2881107 -8.8443623][-5.4152908 -5.4159541 -4.0774894 -3.7082112 -4.4528575 -4.5035987 -4.5203614 -4.4330955 -5.1808085 -5.2358375 -5.0865526 -6.1536431 -5.8896275 -6.8896561 -8.5196867][-3.4396079 -5.3579783 -6.1913624 -5.6696286 -5.7367988 -5.3216705 -4.8895979 -5.3281994 -5.9814563 -5.6214519 -5.2515712 -6.3625836 -6.2751904 -7.7540092 -9.6447639][-5.3909054 -7.1381679 -7.0577984 -6.3817759 -6.3249612 -4.6054077 -3.7924771 -4.0200472 -4.4406104 -4.8156543 -4.7171726 -6.2263417 -7.5253849 -9.5990829 -10.515476][-6.7654881 -8.64906 -8.5740147 -6.6733456 -5.316124 -3.1413059 -1.7124124 -2.4875576 -3.5139554 -3.953974 -4.3536034 -6.7248812 -8.40746 -10.726414 -11.929817][-8.57434 -9.32832 -7.7483368 -5.2337351 -3.2465546 0.24190474 2.3318133 1.718729 0.79401827 -1.3230829 -3.4930444 -5.137434 -6.3128262 -9.1367636 -10.605491][-9.8282423 -8.7651787 -6.1732378 -2.4539587 0.15185404 2.3161941 4.683 4.5182376 3.7588964 1.3624907 -0.87003326 -3.4511042 -5.2421012 -7.6435304 -9.6550426][-8.609211 -7.1837664 -4.7581334 -0.62568378 2.6027465 4.8851333 6.6947618 6.0570016 5.5875621 3.1043315 0.98213339 -2.0698514 -4.6549606 -6.3244543 -7.6379108][-7.6982059 -6.6451306 -4.561861 -0.94524717 1.3770566 3.9048333 5.5647407 4.7757478 3.7369294 2.4676166 0.9066782 -2.6188219 -4.7932892 -6.9191175 -8.1089573][-6.1319065 -6.9907441 -5.8840828 -3.5695245 -2.8751187 -0.42449188 1.6760411 2.1113343 1.4007969 -0.22802687 -1.6432009 -4.5616789 -6.2847991 -8.4711647 -9.6323862][-10.980652 -11.21356 -10.452762 -8.6925707 -7.4412613 -6.187676 -4.8669839 -3.97603 -3.7650988 -4.2449727 -6.2165904 -9.6537981 -10.734 -10.225199 -9.7898855][-12.949203 -13.240604 -12.653158 -10.565049 -8.9360142 -8.1012325 -8.2963238 -8.8619175 -8.6977034 -7.9361119 -7.6789403 -9.6654205 -11.446791 -11.433247 -10.24859][-13.069059 -12.288884 -11.573702 -10.895555 -10.418703 -9.724987 -9.5926094 -10.069621 -10.566108 -9.8307543 -9.0811386 -8.6625662 -9.5114126 -9.1499853 -7.9328446][-9.6340027 -8.5961018 -7.3173285 -6.3666368 -6.2416372 -7.0486751 -7.6254444 -8.1907549 -8.251543 -7.5241394 -7.9927111 -7.026824 -6.1571026 -5.5183887 -5.4468441][-6.4080081 -5.9931779 -5.0080719 -3.1213715 -1.5694098 -2.7451863 -4.2367721 -4.4923649 -4.4222651 -4.7016172 -5.0868635 -5.4303012 -5.8016911 -5.5729375 -5.6410809]]...]
INFO - root - 2017-12-16 03:05:03.786431: step 90910, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.678 sec/batch; 45h:31m:28s remains)
INFO - root - 2017-12-16 03:05:10.364930: step 90920, loss = 0.16, batch loss = 0.12 (12.3 examples/sec; 0.650 sec/batch; 43h:38m:31s remains)
INFO - root - 2017-12-16 03:05:16.943665: step 90930, loss = 0.18, batch loss = 0.13 (12.0 examples/sec; 0.666 sec/batch; 44h:41m:20s remains)
INFO - root - 2017-12-16 03:05:23.528385: step 90940, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.643 sec/batch; 43h:08m:20s remains)
INFO - root - 2017-12-16 03:05:30.115226: step 90950, loss = 0.13, batch loss = 0.08 (11.7 examples/sec; 0.682 sec/batch; 45h:44m:14s remains)
INFO - root - 2017-12-16 03:05:36.672878: step 90960, loss = 0.13, batch loss = 0.08 (11.6 examples/sec; 0.688 sec/batch; 46h:10m:08s remains)
INFO - root - 2017-12-16 03:05:43.281520: step 90970, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.654 sec/batch; 43h:52m:20s remains)
INFO - root - 2017-12-16 03:05:49.796772: step 90980, loss = 0.19, batch loss = 0.15 (12.4 examples/sec; 0.646 sec/batch; 43h:19m:21s remains)
INFO - root - 2017-12-16 03:05:56.423278: step 90990, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.657 sec/batch; 44h:04m:18s remains)
INFO - root - 2017-12-16 03:06:03.000550: step 91000, loss = 0.14, batch loss = 0.10 (12.1 examples/sec; 0.664 sec/batch; 44h:30m:58s remains)
2017-12-16 03:06:03.557050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7590635 -2.0049772 -1.3321195 -0.61997414 -1.3270025 -1.7302907 -1.7720008 -1.9804831 -2.476696 -2.9450822 -3.0679579 -4.896399 -5.6904659 -5.6931171 -4.2753248][-2.3199074 -1.9422255 -1.71632 -1.2791538 -1.6610637 -2.4039412 -2.4526033 -2.5862856 -3.0884111 -3.1305707 -3.1460183 -4.7656956 -5.3935781 -5.8042831 -4.2143312][-1.871304 -2.0273981 -2.0824125 -1.0222106 -1.7857201 -2.222019 -2.2621663 -2.5076742 -2.3639061 -2.323241 -2.6250854 -4.6333013 -5.6835585 -5.5871415 -4.7655625][-2.9366431 -2.4514465 -2.4997253 -1.7083092 -2.7893715 -3.1676636 -3.3745415 -3.3754776 -2.8251355 -2.2511091 -1.9920754 -4.5747371 -6.0012836 -6.7424755 -5.7673635][-3.5058775 -3.0643394 -3.2190115 -1.9785779 -2.0946527 -1.9322081 -2.0399556 -2.1989331 -1.3148327 -0.78128576 -0.69720888 -3.0341976 -4.5522957 -6.3638859 -6.0390754][-4.7554131 -3.6611228 -2.8023295 -0.81589174 -0.0777235 0.44434309 1.193481 1.6034136 2.0828619 1.8930573 1.5796623 -1.3622603 -3.362745 -5.2606468 -5.1178341][-6.4031396 -5.1230211 -3.0684543 -0.29137659 0.789927 2.1955223 3.8062396 4.7139525 5.0347495 3.7614617 2.4206815 -0.68137693 -3.905273 -5.3916969 -5.3515515][-6.3967471 -5.2302604 -3.4801085 -0.15497255 1.4011693 3.7267594 5.3261867 5.7422805 6.4663281 5.3356557 3.2437806 -1.5030084 -4.6945219 -6.6890779 -6.8930612][-5.637363 -4.3618417 -3.127171 -0.27109814 0.93923759 2.8916745 4.1107354 4.7478118 5.181046 3.9484143 2.2641702 -2.2043016 -5.6049705 -7.9064617 -7.8944092][-4.9789782 -3.8658009 -3.3444979 -0.77114677 -0.11524916 1.2709517 2.601707 3.22891 3.4998593 1.7304564 0.45199442 -3.8318458 -7.0476284 -9.0300255 -9.0851316][-7.6975408 -7.10167 -5.703938 -3.9152417 -3.714083 -3.344852 -3.1141047 -2.7071145 -2.6810725 -3.3754787 -3.8607588 -7.8510942 -9.3558636 -10.197924 -8.9745808][-10.728651 -9.7907486 -8.5519066 -7.1304059 -6.9351969 -6.3048635 -6.8373084 -7.5935278 -7.3460217 -7.490416 -7.6197615 -9.4764786 -9.0733757 -9.9113846 -8.5836048][-11.54217 -10.609928 -9.387166 -8.3611126 -8.5171661 -7.5716023 -8.3980017 -8.7992783 -8.9910135 -8.901659 -8.9330912 -9.5922384 -8.7972164 -8.607605 -5.9632034][-10.606775 -9.6583843 -8.7312469 -7.8001575 -7.1844554 -7.6189308 -8.6228542 -8.3607006 -8.4278755 -8.4964237 -8.5925207 -7.6733227 -6.6412811 -6.0498996 -5.0241537][-7.181006 -7.0205526 -6.2141604 -4.9429216 -4.3233356 -4.1732783 -4.5305662 -4.8705072 -5.9554787 -5.7158403 -5.6542592 -6.18466 -6.4781432 -6.1672587 -5.9169655]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-5-5-val-clipgradient100
INFO - root - 2017-12-16 03:06:10.162842: step 91010, loss = 0.17, batch loss = 0.12 (11.7 examples/sec; 0.681 sec/batch; 45h:41m:36s remains)
INFO - root - 2017-12-16 03:06:16.736314: step 91020, loss = 0.35, batch loss = 0.30 (12.6 examples/sec; 0.634 sec/batch; 42h:31m:08s remains)
INFO - root - 2017-12-16 03:06:23.325839: step 91030, loss = 0.14, batch loss = 0.10 (12.2 examples/sec; 0.656 sec/batch; 44h:01m:37s remains)
INFO - root - 2017-12-16 03:06:29.893912: step 91040, loss = 0.13, batch loss = 0.08 (12.4 examples/sec; 0.647 sec/batch; 43h:23m:32s remains)
INFO - root - 2017-12-16 03:06:36.511080: step 91050, loss = 0.16, batch loss = 0.12 (11.9 examples/sec; 0.670 sec/batch; 44h:56m:01s remains)
INFO - root - 2017-12-16 03:06:43.105004: step 91060, loss = 0.20, batch loss = 0.16 (12.5 examples/sec; 0.640 sec/batch; 42h:53m:47s remains)
INFO - root - 2017-12-16 03:06:49.761678: step 91070, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 43h:29m:11s remains)
INFO - root - 2017-12-16 03:06:56.278331: step 91080, loss = 0.18, batch loss = 0.13 (12.5 examples/sec; 0.641 sec/batch; 42h:57m:29s remains)
INFO - root - 2017-12-16 03:07:02.826455: step 91090, loss = 0.13, batch loss = 0.08 (11.9 examples/sec; 0.671 sec/batch; 45h:00m:12s remains)
INFO - root - 2017-12-16 03:07:09.400169: step 91100, loss = 0.15, batch loss = 0.10 (12.2 examples/sec; 0.655 sec/batch; 43h:54m:43s remains)
2017-12-16 03:07:09.950624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.458 -9.1190987 -8.7200556 -7.4797621 -7.361659 -7.4305162 -7.1744637 -7.087574 -6.7865958 -6.7064686 -7.0046382 -6.5036755 -7.9085221 -8.12488 -6.0955071][-8.1906509 -7.7194233 -6.7226844 -6.1469111 -6.8472457 -7.0686755 -6.93978 -6.4659233 -6.1271644 -5.9700589 -5.1349373 -4.7966261 -6.6672006 -7.6673956 -6.5403056][-5.6043081 -6.5842729 -7.199708 -6.7500839 -7.2619 -7.520257 -7.6543751 -6.7549009 -5.6179981 -5.4971814 -5.8377724 -5.9191418 -6.6996145 -8.3319807 -7.7581124][-5.5870533 -6.2094822 -6.2451305 -7.4878731 -8.2847672 -6.6267114 -5.4597597 -4.7422085 -4.4502139 -4.022501 -4.4668307 -5.3267074 -6.9259081 -7.1675982 -6.309917][-5.9167023 -6.2996063 -8.0000439 -8.7416344 -8.6226988 -5.3586521 -2.8130813 -2.1421702 -2.3831136 -3.2278476 -4.5028543 -5.4611673 -6.5708275 -6.9978714 -5.08925][-7.09346 -6.662406 -6.5534859 -6.6554747 -5.7237897 -2.4014165 1.2677264 1.707828 0.46702003 -1.985589 -3.9904244 -3.8022144 -4.9138737 -6.0141025 -4.7650833][-6.5011063 -6.4948812 -6.5208168 -4.77912 -3.1579719 0.05399847 3.8990054 4.9548945 3.4632049 -0.20549011 -3.4716687 -3.2550819 -3.947021 -3.732043 -2.9008837][-5.6457758 -6.1496315 -6.6815076 -5.161046 -3.4562213 0.66895914 4.6041694 5.1694427 3.9209008 1.1369338 -2.158112 -3.7051723 -5.7499733 -4.7949133 -3.1515863][-5.0434208 -6.0977044 -6.2566304 -4.2859516 -2.9743593 -0.10466766 3.4167724 4.1007428 2.6733556 0.23236942 -2.2112679 -4.0885687 -6.8702846 -6.8677454 -5.5820456][-5.10408 -6.2678785 -5.9395027 -4.7712717 -3.9291351 -1.1351314 1.6545296 1.9801536 1.2788391 -1.2593493 -3.6896365 -5.02148 -6.4930911 -6.6350865 -5.536932][-7.5111055 -8.7773266 -8.561985 -6.9252586 -6.9387031 -6.4054351 -4.0567827 -2.2632065 -1.8764517 -3.3478603 -6.0969296 -7.8559942 -8.6292953 -8.5363712 -6.2589068][-11.035099 -11.202967 -11.84746 -10.106739 -8.9622784 -7.7184505 -6.8498178 -5.8648291 -5.6909308 -6.4551263 -8.0216656 -8.3987236 -8.331646 -7.6529837 -5.10073][-13.028285 -12.507814 -11.164083 -10.231462 -9.8327923 -7.6511436 -6.7209072 -7.1320324 -7.2545881 -7.2288423 -7.9512706 -7.75493 -7.45516 -6.8083715 -5.37789][-10.831203 -10.384587 -8.538 -7.1851854 -6.8172693 -6.0973868 -5.1432838 -4.9203715 -5.2493477 -5.32719 -6.1135058 -6.4685478 -6.5343351 -5.6000614 -5.6908169][-5.87863 -6.4426732 -5.0852966 -2.8960021 -1.7225387 -2.0137317 -2.7845757 -2.5835392 -2.8206794 -2.9576242 -3.5308537 -4.6612573 -6.9949713 -6.8182039 -6.1690598]]...]
INFO - root - 2017-12-16 03:07:16.466656: step 91110, loss = 0.14, batch loss = 0.10 (12.5 examples/sec; 0.642 sec/batch; 43h:04m:35s remains)
INFO - root - 2017-12-16 03:07:23.079995: step 91120, loss = 0.11, batch loss = 0.07 (11.8 examples/sec; 0.677 sec/batch; 45h:23m:39s remains)
INFO - root - 2017-12-16 03:07:29.712420: step 91130, loss = 0.12, batch loss = 0.08 (12.4 examples/sec; 0.646 sec/batch; 43h:19m:36s remains)
INFO - root - 2017-12-16 03:07:36.347085: step 91140, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.658 sec/batch; 44h:07m:46s remains)
INFO - root - 2017-12-16 03:07:42.953951: step 91150, loss = 0.15, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 42h:45m:59s remains)
INFO - root - 2017-12-16 03:07:49.609426: step 91160, loss = 0.18, batch loss = 0.13 (11.8 examples/sec; 0.680 sec/batch; 45h:33m:50s remains)
INFO - root - 2017-12-16 03:07:56.182574: step 91170, loss = 0.17, batch loss = 0.12 (12.4 examples/sec; 0.643 sec/batch; 43h:05m:33s remains)
INFO - root - 2017-12-16 03:08:02.796333: step 91180, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.662 sec/batch; 44h:21m:00s remains)
INFO - root - 2017-12-16 03:08:09.548903: step 91190, loss = 0.16, batch loss = 0.11 (12.1 examples/sec; 0.660 sec/batch; 44h:14m:44s remains)
INFO - root - 2017-12-16 03:08:16.097980: step 91200, loss = 0.14, batch loss = 0.10 (11.8 examples/sec; 0.679 sec/batch; 45h:29m:25s remains)
2017-12-16 03:08:16.585808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0842414 -6.8337274 -7.0582066 -6.4338222 -6.4118896 -6.5614595 -6.3813968 -5.8440294 -4.8767176 -3.8969958 -3.1401873 -5.4329181 -5.9218473 -6.2521091 -6.9203992][-6.3195448 -6.7953839 -6.5777516 -6.3025904 -5.6898942 -6.3459339 -7.3540292 -6.8583279 -5.8399706 -4.8228559 -3.7053795 -5.2552428 -5.7494574 -7.1932487 -8.1306133][-7.0975995 -7.0299878 -6.3084564 -5.8697853 -6.5086064 -7.1146307 -7.2495141 -6.7123494 -5.5550008 -5.5871615 -5.0699649 -6.684278 -7.4441853 -7.501296 -8.8805647][-6.1004877 -6.7550316 -5.8223953 -4.8666444 -5.2927976 -6.4451714 -6.4610543 -6.1990957 -5.7013388 -4.8017597 -4.6953831 -7.4755192 -8.2460661 -8.4377584 -8.9798317][-7.8382387 -8.196331 -6.691865 -5.1940236 -4.3814945 -3.1166849 -2.9480641 -4.0837789 -4.1963515 -4.2565484 -5.1171246 -7.5337734 -8.5876 -9.2697525 -10.14572][-8.7969589 -8.5197048 -6.5522141 -4.0354109 -2.2467604 -0.8121171 0.28165245 -0.082539558 -0.79655313 -2.2119346 -3.7778244 -6.4639454 -8.0621881 -8.4957781 -9.44305][-9.2953176 -8.3043652 -5.919919 -2.5702424 -0.040515423 1.3796754 2.8625655 3.1128526 3.634563 1.2992144 -1.9591956 -5.3656054 -7.1772718 -8.1459475 -9.2135839][-7.9520178 -6.7873478 -3.8692021 -0.81930161 1.7321477 4.5961967 4.6689706 3.4856172 4.4169049 2.6885018 -0.13629055 -4.4618464 -6.8751712 -7.6662464 -8.3361835][-5.9250517 -4.4766188 -2.7912357 -1.3760505 1.3203449 3.0139585 4.0346732 3.2183328 2.0824556 1.2199802 0.27503157 -3.419641 -6.3886957 -7.1250467 -7.2780352][-6.050674 -4.1017032 -1.9987803 -0.62533331 -0.49585438 -1.1852231 0.5674634 1.7384429 1.1997018 -0.57980156 -1.8659425 -5.0853896 -7.101881 -7.2199712 -7.6062021][-8.8682594 -7.5312204 -5.3775387 -2.937712 -3.0277586 -3.4274857 -2.8931603 -3.1508811 -2.2506206 -2.5616243 -3.8878298 -6.1105857 -6.8463154 -6.5251093 -6.6109834][-12.043821 -10.874004 -8.3344069 -6.6756191 -5.5630822 -6.0242758 -7.1737928 -6.5456753 -5.4373341 -4.8264341 -4.2937012 -5.9091511 -6.8996081 -8.0318871 -8.2546806][-13.491973 -12.262829 -10.870897 -10.199678 -9.9245777 -8.5229855 -7.7753162 -7.97761 -7.7802029 -7.143549 -6.3493443 -6.8874826 -6.1911621 -6.0163937 -6.0714135][-11.328272 -11.454134 -11.166127 -9.77416 -9.2010078 -9.1847267 -8.481843 -6.5981493 -6.445693 -6.4053946 -6.0931735 -5.4833059 -4.6139355 -4.9201336 -5.7216091][-10.53341 -10.229344 -8.7733593 -8.1885357 -8.34441 -8.5734291 -8.4472866 -7.3475389 -5.8522053 -5.441113 -5.3118849 -5.7858081 -6.0616708 -4.81818 -5.1909995]]...]
INFO - root - 2017-12-16 03:08:23.196955: step 91210, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.659 sec/batch; 44h:09m:39s remains)
INFO - root - 2017-12-16 03:08:29.762507: step 91220, loss = 0.14, batch loss = 0.10 (12.3 examples/sec; 0.650 sec/batch; 43h:34m:13s remains)
INFO - root - 2017-12-16 03:08:36.332062: step 91230, loss = 0.16, batch loss = 0.11 (12.2 examples/sec; 0.657 sec/batch; 44h:01m:35s remains)
INFO - root - 2017-12-16 03:08:42.995054: step 91240, loss = 0.14, batch loss = 0.09 (12.1 examples/sec; 0.663 sec/batch; 44h:25m:59s remains)
INFO - root - 2017-12-16 03:08:49.625189: step 91250, loss = 0.14, batch loss = 0.10 (12.0 examples/sec; 0.669 sec/batch; 44h:49m:57s remains)
INFO - root - 2017-12-16 03:08:56.220517: step 91260, loss = 0.13, batch loss = 0.09 (12.4 examples/sec; 0.644 sec/batch; 43h:09m:09s remains)
INFO - root - 2017-12-16 03:09:02.702873: step 91270, loss = 0.15, batch loss = 0.11 (12.1 examples/sec; 0.663 sec/batch; 44h:24m:10s remains)
INFO - root - 2017-12-16 03:09:09.334810: step 91280, loss = 0.14, batch loss = 0.10 (11.9 examples/sec; 0.670 sec/batch; 44h:52m:53s remains)
INFO - root - 2017-12-16 03:09:15.825456: step 91290, loss = 0.13, batch loss = 0.08 (12.1 examples/sec; 0.661 sec/batch; 44h:17m:15s remains)
INFO - root - 2017-12-16 03:09:22.470421: step 91300, loss = 0.16, batch loss = 0.11 (11.8 examples/sec; 0.676 sec/batch; 45h:17m:30s remains)
2017-12-16 03:09:23.113092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.5431328 -8.5153 -7.1713796 -5.7757673 -4.7385941 -4.3267436 -4.7902846 -5.1149397 -5.3433757 -5.6000528 -5.6519055 -8.1056376 -10.104685 -10.549476 -8.97292][-9.1385078 -9.48041 -8.5423527 -6.8010254 -5.8799815 -5.1123209 -5.2419443 -5.9801865 -6.737679 -6.3321929 -5.8953638 -8.7519045 -10.904047 -11.040226 -9.105196][-8.0115108 -8.6619139 -8.87764 -7.5082669 -5.9267631 -5.6646304 -6.000205 -6.7882428 -7.5797634 -7.8589458 -7.5619888 -9.8625965 -11.459308 -10.46385 -8.0379791][-8.1734943 -9.72318 -9.3120985 -7.5772724 -6.5181794 -5.2335391 -5.4749851 -7.3046527 -7.9301562 -7.5473604 -7.5106487 -10.09096 -11.426279 -10.520067 -8.1454248][-9.7607918 -11.426858 -10.935848 -7.4832325 -5.1233344 -2.6260862 -1.9480278 -4.9475279 -6.5189533 -6.5129719 -6.5582795 -8.6111107 -10.043846 -9.9769144 -8.1912918][-10.615602 -11.588012 -10.567436 -6.5211182 -2.9348769 0.20448256 1.8212972 -0.48678589 -2.4262638 -3.7572231 -4.4384913 -6.2896562 -7.507658 -7.7762909 -6.4163609][-10.007383 -9.69289 -8.2065916 -3.1129179 0.37871456 2.6190114 3.74542 2.4510961 1.096561 -0.96038818 -2.2460613 -4.0520082 -5.5924621 -5.9490414 -5.1261106][-8.0732756 -7.5033517 -5.886518 -0.83710241 3.1622577 5.7977071 5.847877 2.6840262 1.4507699 0.19617128 -1.0953255 -3.3091638 -5.249279 -5.3563271 -4.1797228][-6.557075 -5.4417233 -4.5142078 -0.46231222 2.499352 4.3410211 5.0645852 2.7706466 1.2378912 -0.47297239 -1.7142401 -4.0129709 -6.1587315 -6.289804 -4.7518325][-3.9253149 -3.9316425 -4.2309484 -2.1205623 -0.2588582 1.8084946 3.3453317 1.5453329 -0.51767635 -1.9786198 -2.9162014 -5.2946568 -7.287755 -7.8067684 -6.4571629][-6.7026734 -6.4095926 -7.1476579 -6.4426832 -4.9019237 -3.4714203 -2.0774837 -2.4405198 -3.733073 -4.8495517 -6.2423754 -9.4431791 -10.71953 -10.260501 -8.2237587][-10.917873 -10.703698 -10.898214 -10.164543 -8.5511808 -7.0206032 -6.528656 -7.8626003 -8.7163773 -9.4873219 -10.848471 -12.810589 -13.225019 -12.055876 -9.1666727][-11.477434 -10.714481 -10.366446 -9.7150908 -9.25462 -8.155405 -7.695137 -8.4330235 -9.7830105 -10.658446 -11.704563 -12.691306 -13.514791 -11.735152 -7.7659597][-9.7027349 -8.8123837 -7.9668694 -6.6136293 -6.8457804 -6.5284185 -7.1973553 -7.7562394 -7.5179811 -8.054759 -9.44545 -9.6279869 -9.3159227 -8.1015768 -5.7790284][-7.6716185 -7.4116745 -6.692512 -5.6161532 -4.0666056 -4.5851836 -5.2147126 -5.4694071 -5.2565384 -4.8552461 -4.8510447 -5.7210736 -6.3449726 -6.35719 -5.1068511]]...]
INFO - root - 2017-12-16 03:09:29.663105: step 91310, loss = 0.15, batch loss = 0.11 (12.7 examples/sec; 0.632 sec/batch; 42h:19m:29s remains)
INFO - root - 2017-12-16 03:09:36.220970: step 91320, loss = 0.16, batch loss = 0.12 (12.1 examples/sec; 0.663 sec/batch; 44h:25m:56s remains)
INFO - root - 2017-12-16 03:09:42.890937: step 91330, loss = 0.14, batch loss = 0.09 (12.3 examples/sec; 0.653 sec/batch; 43h:44m:46s remains)
INFO - root - 2017-12-16 03:09:49.490685: step 91340, loss = 0.13, batch loss = 0.09 (12.2 examples/sec; 0.656 sec/batch; 43h:55m:45s remains)
INFO - root - 2017-12-16 03:09:56.101367: step 91350, loss = 0.18, batch loss = 0.14 (12.2 examples/sec; 0.658 sec/batch; 44h:04m:02s remains)
INFO - root - 2017-12-16 03:10:02.653256: step 91360, loss = 0.19, batch loss = 0.14 (12.4 examples/sec; 0.647 sec/batch; 43h:19m:00s remains)
INFO - root - 2017-12-16 03:10:09.279814: step 91370, loss = 0.13, batch loss = 0.09 (11.8 examples/sec; 0.677 sec/batch; 45h:22m:26s remains)
INFO - root - 2017-12-16 03:10:15.883027: step 91380, loss = 0.15, batch loss = 0.10 (11.9 examples/sec; 0.672 sec/batch; 45h:02m:05s remains)
INFO - root - 2017-12-16 03:10:22.450479: step 91390, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.665 sec/batch; 44h:30m:27s remains)
INFO - root - 2017-12-16 03:10:28.986834: step 91400, loss = 0.15, batch loss = 0.11 (11.7 examples/sec; 0.685 sec/batch; 45h:54m:15s remains)
2017-12-16 03:10:29.478378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5739322 -8.0059052 -9.2587681 -9.9214678 -11.09691 -10.555897 -9.6820717 -8.7832565 -8.40258 -9.1828346 -9.9460545 -10.621923 -12.992197 -16.250158 -14.380901][-5.2325692 -5.3704462 -6.8002653 -8.1551285 -9.9325991 -11.038209 -10.19355 -8.6559639 -9.1333427 -8.4259491 -7.9030952 -8.4484854 -11.184064 -15.688494 -15.776239][-3.6678531 -4.5338483 -7.0746627 -7.9729824 -9.4223995 -10.589602 -10.509102 -9.5513935 -8.2713947 -7.8989196 -7.8453145 -8.0600147 -9.7456741 -13.022823 -12.368122][-3.067987 -5.137249 -6.1255708 -6.3479409 -8.0990982 -7.2195835 -6.03266 -6.8165522 -7.4070425 -6.9666505 -6.7956424 -6.5322943 -8.8407345 -12.935046 -11.197636][-3.9732132 -4.5733042 -6.7792358 -6.9568868 -5.8040853 -2.9069328 -1.0796595 -1.5618701 -2.6051092 -3.789957 -4.6028633 -4.491322 -6.7492771 -12.252003 -13.17975][-6.3966737 -6.925838 -7.0231342 -5.6059203 -3.5638452 0.14429426 3.5364976 4.9081864 3.5647321 0.26831675 -2.0439405 -2.1747253 -3.7406836 -8.3300133 -10.669834][-7.9655485 -7.0600748 -6.5305185 -4.9978151 -2.4551086 1.6884751 6.0235877 8.60029 8.5043049 4.5490718 -0.24419117 -0.93908405 -2.8024168 -6.9720669 -7.527956][-7.678813 -7.5360527 -7.3478646 -4.619297 -2.3889608 3.1745248 7.888124 9.1992168 9.2701664 6.4642968 2.5318942 0.612761 -3.6701505 -6.6453834 -5.6584077][-6.5932455 -5.8926253 -5.5854955 -3.6525061 -3.597682 -0.15606689 4.2835488 7.409245 8.3380489 4.5542712 0.15881109 -2.440465 -5.4776015 -9.4053679 -10.529676][-4.3097138 -5.5991826 -6.1322489 -4.39814 -3.6674931 -1.1706743 0.67100906 2.9914489 3.4853024 1.560976 -1.050458 -5.551445 -9.8070488 -13.739475 -13.861187][-8.9501238 -8.7645512 -9.0901279 -7.3793449 -7.3428588 -7.2081981 -6.5421119 -3.9515531 -2.1103494 -3.6807735 -6.3297067 -9.8634224 -14.787136 -19.293242 -17.088194][-8.7656593 -9.9385452 -10.053236 -8.6401415 -8.8526411 -7.7139916 -7.3846779 -7.2666779 -7.8821845 -8.4942026 -9.6457834 -10.986042 -12.922791 -18.590687 -18.332251][-12.163475 -10.707233 -9.19157 -8.0237446 -9.3793659 -9.0836449 -8.0623446 -7.5308442 -8.1163492 -9.1536722 -10.446117 -10.897052 -12.85751 -15.253065 -15.488218][-10.617249 -10.439493 -8.00183 -8.041543 -7.5814776 -7.9378281 -8.5107536 -7.4055614 -6.0325885 -7.1380806 -8.0431919 -8.2506313 -7.58646 -9.2443161 -10.079278][-7.8616686 -7.6115189 -6.8055248 -5.4594197 -4.5743065 -4.9914222 -5.8975868 -7.3845606 -8.9371424 -8.3523989 -7.1041837 -7.4016418 -7.8391161 -7.3240156 -6.0663586]]...]
INFO - root - 2017-12-16 03:10:35.977035: step 91410, loss = 0.13, batch loss = 0.08 (12.0 examples/sec; 0.667 sec/batch; 44h:40m:27s remains)
INFO - root - 2017-12-16 03:10:42.496960: step 91420, loss = 0.16, batch loss = 0.11 (12.5 examples/sec; 0.638 sec/batch; 42h:44m:03s remains)
INFO - root - 2017-12-16 03:10:49.037607: step 91430, loss = 0.20, batch loss = 0.16 (12.1 examples/sec; 0.662 sec/batch; 44h:17m:51s remains)
INFO - root - 2017-12-16 03:10:55.554638: step 91440, loss = 0.11, batch loss = 0.06 (12.2 examples/sec; 0.653 sec/batch; 43h:44m:06s remains)
INFO - root - 2017-12-16 03:11:02.143882: step 91450, loss = 0.13, batch loss = 0.08 (12.2 examples/sec; 0.653 sec/batch; 43h:43m:44s remains)
INFO - root - 2017-12-16 03:11:08.760622: step 91460, loss = 0.15, batch loss = 0.10 (12.3 examples/sec; 0.648 sec/batch; 43h:24m:56s remains)
INFO - root - 2017-12-16 03:11:15.388602: step 91470, loss = 0.13, batch loss = 0.09 (12.0 examples/sec; 0.669 sec/batch; 44h:47m:52s remains)
