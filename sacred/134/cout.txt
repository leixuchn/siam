INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "134"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 03:23:26.249099: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:23:26.249138: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:23:26.249144: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:23:26.249148: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:23:26.249152: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:23:29.626943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-07 03:23:29.626976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 03:23:29.626982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 03:23:29.626990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
sdiufhasudf Tensor("siamese_fc/conv2/split:0", shape=(8, 29, 29, 48), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose:0", shape=(8, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_1:0", shape=(8, 200, 29, 29), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose_2:0", shape=(8, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_3:0", shape=(8, 200, 29, 29), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv2/split:0", shape=(8, 57, 57, 48), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose:0", shape=(8, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_1:0", shape=(8, 200, 57, 57), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose_2:0", shape=(8, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_3:0", shape=(8, 200, 57, 57), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 03:23:46.571628: step 0, loss = 0.63, batch loss = 0.55 (0.6 examples/sec; 12.585 sec/batch; 1162h:22m:41s remains)
2017-12-07 03:23:47.407463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7442641 -2.7620788 -2.7687068 -2.8921251 -3.019011 -3.0925324 -3.2219448 -3.2570851 -3.1855326 -3.1898823 -3.0777667 -2.90521 -2.7753081 -2.6176224 -2.4820375][-2.841465 -2.8242769 -2.8300545 -3.0149956 -3.1326408 -3.1223733 -3.2053726 -3.1977825 -3.0688548 -3.0648494 -2.9251137 -2.7331376 -2.6484504 -2.5327349 -2.4157798][-2.8387918 -2.8213282 -2.9032798 -3.1994386 -3.2695391 -3.1076989 -3.0540872 -2.8910499 -2.6774111 -2.7293894 -2.6512485 -2.5197544 -2.5476446 -2.485353 -2.3666723][-2.8067026 -2.8910236 -3.1622512 -3.6132221 -3.6529911 -3.3489413 -3.0894291 -2.6752052 -2.3908696 -2.5801575 -2.6134839 -2.5535853 -2.6612463 -2.5918489 -2.4064474][-3.0039606 -3.2075272 -3.6150908 -4.0857024 -4.0119481 -3.5264792 -2.9900208 -2.2150691 -1.8220179 -2.1866794 -2.434267 -2.5617597 -2.8151889 -2.7625592 -2.5138345][-3.3040144 -3.5007944 -3.8568125 -4.1324921 -3.8219719 -3.095109 -2.1672959 -0.92283964 -0.37527466 -0.99848938 -1.6456316 -2.1663692 -2.7082253 -2.7762046 -2.5528247][-3.3736606 -3.440865 -3.6724162 -3.7503972 -3.2972651 -2.4831018 -1.3392293 0.17770529 0.764832 -0.15054798 -1.1665926 -2.0093055 -2.7270811 -2.8374491 -2.6132197][-3.2834105 -3.2856126 -3.5009089 -3.4896088 -3.0716562 -2.3891442 -1.3582044 0.00041103363 0.40565634 -0.5831368 -1.6026871 -2.4430914 -3.0695188 -3.0609903 -2.7774279][-3.3223062 -3.4216809 -3.7085161 -3.6030295 -3.1726661 -2.5881362 -1.775423 -0.79475713 -0.6479249 -1.4910789 -2.2376311 -2.864068 -3.3110023 -3.1877537 -2.8803327][-3.4506724 -3.6553147 -3.9327111 -3.6948 -3.2351794 -2.7304635 -2.1299157 -1.5407252 -1.5840857 -2.2012696 -2.6127188 -3.0024161 -3.3382912 -3.1828127 -2.8987257][-3.5041628 -3.7129693 -3.88725 -3.5823336 -3.2135143 -2.8910313 -2.5556102 -2.3003383 -2.4091818 -2.7908134 -2.9170549 -3.1091876 -3.3556948 -3.1680825 -2.8988333][-3.5189176 -3.6597204 -3.7387652 -3.4982646 -3.3212624 -3.189013 -3.0627861 -2.975306 -3.028635 -3.2118986 -3.1453621 -3.1984487 -3.3823621 -3.1817925 -2.924257][-3.5998836 -3.6663501 -3.693023 -3.5745325 -3.5478058 -3.4992707 -3.4297562 -3.3460627 -3.283812 -3.3360403 -3.1950388 -3.2182937 -3.3971996 -3.2017112 -2.94691][-3.79869 -3.7970538 -3.767009 -3.702503 -3.7104909 -3.6770225 -3.6408811 -3.5669622 -3.4413881 -3.4445758 -3.3145208 -3.3496559 -3.5164254 -3.2892647 -2.9969151][-4.0963135 -4.0838809 -4.021244 -3.9557521 -3.9374406 -3.9082592 -3.9187384 -3.8636324 -3.6923637 -3.6586628 -3.5357611 -3.5272489 -3.6173089 -3.3321853 -2.9997659]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 03:23:55.752393: step 10, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.786 sec/batch; 72h:36m:44s remains)
INFO - root - 2017-12-07 03:24:03.458805: step 20, loss = 0.66, batch loss = 0.58 (10.2 examples/sec; 0.786 sec/batch; 72h:32m:52s remains)
INFO - root - 2017-12-07 03:24:10.820079: step 30, loss = 0.75, batch loss = 0.68 (13.7 examples/sec; 0.585 sec/batch; 53h:59m:07s remains)
INFO - root - 2017-12-07 03:24:18.420181: step 40, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.753 sec/batch; 69h:31m:55s remains)
INFO - root - 2017-12-07 03:24:26.127190: step 50, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.769 sec/batch; 71h:01m:05s remains)
INFO - root - 2017-12-07 03:24:33.805142: step 60, loss = 1.00, batch loss = 0.92 (10.0 examples/sec; 0.802 sec/batch; 74h:04m:00s remains)
INFO - root - 2017-12-07 03:24:41.476086: step 70, loss = 0.88, batch loss = 0.80 (10.4 examples/sec; 0.772 sec/batch; 71h:16m:04s remains)
INFO - root - 2017-12-07 03:24:49.133779: step 80, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.767 sec/batch; 70h:47m:36s remains)
INFO - root - 2017-12-07 03:24:56.826125: step 90, loss = 0.90, batch loss = 0.83 (10.7 examples/sec; 0.749 sec/batch; 69h:07m:39s remains)
INFO - root - 2017-12-07 03:25:04.520948: step 100, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.784 sec/batch; 72h:22m:03s remains)
2017-12-07 03:25:05.158466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8046088 -2.6926613 -2.6108663 -2.6293831 -2.5791814 -2.5205946 -2.6324027 -2.770349 -2.6885619 -2.5878882 -2.6568885 -2.6585298 -2.599072 -2.6761539 -2.7564166][-2.8449793 -2.7618113 -2.6529737 -2.6326013 -2.5909214 -2.5599351 -2.714983 -2.8742042 -2.7682846 -2.6693368 -2.7701094 -2.755373 -2.6953285 -2.8341265 -2.9434774][-2.8771551 -2.8061738 -2.6674478 -2.6180654 -2.5959325 -2.5787666 -2.7315855 -2.8806493 -2.7658949 -2.6786375 -2.7986612 -2.7539244 -2.6938038 -2.9001143 -3.0529485][-2.8900113 -2.8207521 -2.6524897 -2.5788403 -2.5757437 -2.5598068 -2.6793196 -2.7928312 -2.6642118 -2.5943108 -2.7358491 -2.6977286 -2.6681523 -2.9254408 -3.1189213][-2.8958786 -2.8268795 -2.6287096 -2.519825 -2.5240295 -2.5174489 -2.5942681 -2.6419415 -2.4753737 -2.4117019 -2.5704193 -2.5818276 -2.6171908 -2.9090345 -3.1159797][-2.89369 -2.8382483 -2.6389689 -2.5049748 -2.5155678 -2.5181992 -2.5406151 -2.5042949 -2.2926221 -2.2199464 -2.3902149 -2.4788852 -2.5851455 -2.8698878 -3.0454197][-2.8706384 -2.8410604 -2.6771665 -2.5597625 -2.5877047 -2.5980325 -2.5649824 -2.4701233 -2.2473538 -2.1492021 -2.2964504 -2.4357653 -2.5823798 -2.817831 -2.9440172][-2.8507595 -2.851702 -2.7434177 -2.6627874 -2.701576 -2.711422 -2.6421497 -2.5406883 -2.3500128 -2.2167006 -2.2825139 -2.3941822 -2.5147903 -2.6727693 -2.7701788][-2.8543348 -2.8781388 -2.8218994 -2.7806575 -2.8287787 -2.8304336 -2.7297182 -2.6511488 -2.5202425 -2.3636453 -2.318861 -2.3558981 -2.4282982 -2.525162 -2.6327543][-2.8869548 -2.9201627 -2.9002528 -2.8848875 -2.9331925 -2.9016366 -2.7502861 -2.6680822 -2.579689 -2.4354122 -2.3329651 -2.3177 -2.3453581 -2.3980517 -2.5029223][-2.9007576 -2.9089713 -2.8883371 -2.8887796 -2.9458513 -2.8874631 -2.6979759 -2.6119437 -2.5710545 -2.4726181 -2.3665 -2.3394909 -2.335485 -2.3352644 -2.4067559][-2.9001942 -2.8673079 -2.8126516 -2.797096 -2.8475523 -2.7961845 -2.6286695 -2.5788693 -2.5978119 -2.5394793 -2.4279242 -2.3730183 -2.3211777 -2.2672849 -2.3031411][-2.91045 -2.8550084 -2.7757454 -2.7396665 -2.7697003 -2.7439814 -2.6510286 -2.6606569 -2.7253962 -2.6874638 -2.5611382 -2.4660444 -2.3814127 -2.3174062 -2.3655221][-2.9222684 -2.8765407 -2.8058958 -2.7752194 -2.801847 -2.8110385 -2.7952261 -2.8431222 -2.9049587 -2.8609557 -2.7310071 -2.6319361 -2.566134 -2.5242565 -2.5852127][-2.9294868 -2.9129729 -2.8776317 -2.8734379 -2.9046307 -2.9271924 -2.9440923 -2.9882646 -3.0157075 -2.9693246 -2.8697414 -2.8035049 -2.77943 -2.7647555 -2.8098569]]...]
INFO - root - 2017-12-07 03:25:12.853892: step 110, loss = 1.12, batch loss = 1.05 (10.0 examples/sec; 0.803 sec/batch; 74h:06m:43s remains)
INFO - root - 2017-12-07 03:25:20.567040: step 120, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.780 sec/batch; 72h:03m:11s remains)
INFO - root - 2017-12-07 03:25:28.024391: step 130, loss = 0.84, batch loss = 0.77 (14.2 examples/sec; 0.562 sec/batch; 51h:50m:38s remains)
INFO - root - 2017-12-07 03:25:35.594093: step 140, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.755 sec/batch; 69h:43m:44s remains)
INFO - root - 2017-12-07 03:25:43.268221: step 150, loss = 1.02, batch loss = 0.95 (10.5 examples/sec; 0.761 sec/batch; 70h:13m:35s remains)
INFO - root - 2017-12-07 03:25:50.869776: step 160, loss = 0.81, batch loss = 0.73 (10.5 examples/sec; 0.764 sec/batch; 70h:32m:40s remains)
INFO - root - 2017-12-07 03:25:58.542751: step 170, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.781 sec/batch; 72h:06m:52s remains)
INFO - root - 2017-12-07 03:26:06.186179: step 180, loss = 0.68, batch loss = 0.61 (10.8 examples/sec; 0.739 sec/batch; 68h:13m:41s remains)
INFO - root - 2017-12-07 03:26:13.951906: step 190, loss = 1.01, batch loss = 0.94 (10.2 examples/sec; 0.782 sec/batch; 72h:08m:56s remains)
INFO - root - 2017-12-07 03:26:21.605339: step 200, loss = 0.91, batch loss = 0.84 (10.3 examples/sec; 0.778 sec/batch; 71h:46m:53s remains)
2017-12-07 03:26:22.244093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.297111 -4.2903738 -4.3115435 -4.3098483 -4.2446361 -4.0644155 -3.8123262 -3.6283631 -3.6559825 -3.9035628 -4.1806564 -4.3737612 -4.5201082 -4.5725393 -4.473115][-4.0108066 -4.0053892 -3.9508615 -3.9172873 -3.9166374 -3.8725271 -3.8423092 -3.9384034 -4.1790009 -4.50978 -4.717154 -4.691709 -4.5806417 -4.3864422 -4.0859151][-3.8311844 -3.8057666 -3.7313106 -3.7865319 -3.9801846 -4.1861258 -4.4294524 -4.7812939 -5.1363463 -5.4023676 -5.4629774 -5.2173676 -4.8783712 -4.4720478 -3.9598796][-3.7548463 -3.7132227 -3.68088 -3.8748074 -4.2308707 -4.5445075 -4.8222923 -5.1197996 -5.3345652 -5.4505043 -5.433001 -5.1292467 -4.7356434 -4.3097086 -3.7891483][-3.8729067 -3.7716887 -3.6374264 -3.6891959 -3.8729482 -3.9622889 -3.9497921 -3.8954592 -3.8174918 -3.8433785 -3.9647522 -3.9098268 -3.78141 -3.6149807 -3.3394856][-4.0909238 -3.8612201 -3.4286611 -3.0299771 -2.729027 -2.3397374 -1.8681819 -1.4039695 -1.1202052 -1.1945486 -1.6007774 -2.0035841 -2.3663595 -2.624918 -2.7129195][-4.0015206 -3.5500686 -2.7783203 -1.9680259 -1.2796967 -0.53961158 0.25079632 0.946445 1.2655263 1.0938692 0.49950552 -0.25734138 -1.0348437 -1.6451628 -2.0348248][-3.29355 -2.7212462 -1.883338 -1.067543 -0.44343233 0.18097782 0.84485912 1.3782926 1.5287223 1.3111162 0.78520393 0.020517349 -0.78640795 -1.4387572 -1.9348388][-2.5965152 -2.1122396 -1.5857365 -1.1983202 -1.0050509 -0.83975363 -0.60970521 -0.40905094 -0.3455224 -0.39499474 -0.64430141 -1.161062 -1.7570579 -2.2728121 -2.7517142][-2.6734474 -2.4203823 -2.2986262 -2.349988 -2.4965389 -2.6240888 -2.6440206 -2.5564888 -2.4322755 -2.3131459 -2.3890018 -2.7469051 -3.211843 -3.6519053 -4.0820394][-3.58524 -3.4814572 -3.5216599 -3.6892538 -3.8670673 -4.0157356 -4.1077256 -4.0531955 -3.8953247 -3.7595139 -3.8222122 -4.0877886 -4.3870015 -4.6716223 -4.9318619][-4.3549924 -4.2934046 -4.3367352 -4.4629364 -4.5731831 -4.6625953 -4.7838254 -4.7954192 -4.6984348 -4.6394696 -4.6963291 -4.7981772 -4.8466816 -4.8887863 -4.9094958][-4.3389988 -4.3144374 -4.3887796 -4.498951 -4.5685062 -4.6577635 -4.8376393 -4.9271803 -4.8797417 -4.8487306 -4.8449464 -4.7651706 -4.6167336 -4.47709 -4.3408518][-3.8844504 -3.9375975 -4.0802565 -4.1808767 -4.1897321 -4.2744222 -4.4651718 -4.5588541 -4.5197434 -4.5049114 -4.4896226 -4.3614984 -4.1810341 -4.0067921 -3.8615181][-3.4551754 -3.5759754 -3.7538805 -3.8215108 -3.7922764 -3.9057961 -4.0803256 -4.129602 -4.1091609 -4.1526279 -4.1995072 -4.146699 -4.0200911 -3.8497744 -3.7162867]]...]
INFO - root - 2017-12-07 03:26:29.891519: step 210, loss = 0.74, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 70h:47m:43s remains)
INFO - root - 2017-12-07 03:26:37.504565: step 220, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.754 sec/batch; 69h:37m:06s remains)
INFO - root - 2017-12-07 03:26:44.931856: step 230, loss = 0.62, batch loss = 0.55 (13.4 examples/sec; 0.596 sec/batch; 54h:58m:56s remains)
INFO - root - 2017-12-07 03:26:52.449148: step 240, loss = 0.59, batch loss = 0.52 (10.7 examples/sec; 0.750 sec/batch; 69h:11m:22s remains)
INFO - root - 2017-12-07 03:27:00.085798: step 250, loss = 0.98, batch loss = 0.91 (10.8 examples/sec; 0.738 sec/batch; 68h:04m:02s remains)
INFO - root - 2017-12-07 03:27:07.629189: step 260, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.762 sec/batch; 70h:20m:20s remains)
INFO - root - 2017-12-07 03:27:15.322976: step 270, loss = 0.88, batch loss = 0.80 (10.3 examples/sec; 0.773 sec/batch; 71h:20m:55s remains)
INFO - root - 2017-12-07 03:27:23.002462: step 280, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 71h:50m:47s remains)
INFO - root - 2017-12-07 03:27:30.659771: step 290, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.772 sec/batch; 71h:16m:56s remains)
INFO - root - 2017-12-07 03:27:38.308583: step 300, loss = 1.13, batch loss = 1.06 (10.3 examples/sec; 0.780 sec/batch; 72h:00m:36s remains)
2017-12-07 03:27:38.929146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.42839909 -0.4117589 -0.52662969 -0.6505506 -0.93403053 -1.3106518 -1.3612645 -1.0651326 -0.76099157 -0.44739175 -0.096319675 0.21007109 0.25448513 0.33270407 0.58392096][-1.3841894 -1.4355528 -1.3147645 -1.0136638 -0.90506554 -1.0294249 -1.0231583 -0.88435316 -0.79803371 -0.66075253 -0.521996 -0.37386131 -0.34553862 -0.17294645 0.15164948][-2.1746612 -2.3232863 -2.0051446 -1.3373477 -0.9165349 -0.89287472 -0.94512558 -0.99232793 -1.0652683 -1.0521789 -1.0419939 -0.9787302 -0.96810031 -0.80180979 -0.4873662][-2.69533 -2.8750296 -2.408565 -1.5266526 -0.92165661 -0.80810761 -0.90575361 -1.0565095 -1.1753576 -1.2316957 -1.348892 -1.4259539 -1.508182 -1.4122543 -1.1124797][-3.1827736 -3.4293976 -3.0066891 -2.1763589 -1.5411589 -1.3165658 -1.3607013 -1.5001204 -1.5902941 -1.6382756 -1.8015053 -1.9634452 -2.069366 -1.963294 -1.60203][-3.3097413 -3.5559716 -3.2965097 -2.8372388 -2.4872415 -2.3151281 -2.302072 -2.386436 -2.3628397 -2.2530482 -2.2954562 -2.4166284 -2.4917631 -2.366312 -2.0267985][-3.0832367 -3.1189647 -2.9235306 -2.7836938 -2.6981864 -2.5726967 -2.4738054 -2.5163193 -2.4998446 -2.3902366 -2.4136336 -2.5459955 -2.607224 -2.451757 -2.1315975][-2.8459496 -2.6524663 -2.4764977 -2.4758224 -2.4739909 -2.313426 -2.1077759 -2.0908113 -2.1516056 -2.1617186 -2.2376864 -2.3358088 -2.3182786 -2.0759392 -1.7054408][-2.8306236 -2.5564284 -2.467324 -2.5671976 -2.6619565 -2.5630131 -2.2981751 -2.0734897 -1.9682562 -1.8770669 -1.88866 -1.9063523 -1.8443139 -1.6071715 -1.2863245][-2.9275222 -2.6963379 -2.7046885 -2.8411219 -2.9949975 -3.0150294 -2.8270497 -2.5206318 -2.2594759 -2.0017748 -1.8293309 -1.7038982 -1.6393647 -1.5119343 -1.334949][-2.4234087 -2.4421556 -2.7049112 -3.0103774 -3.2842093 -3.4364445 -3.3474004 -3.0333147 -2.665194 -2.2335985 -1.8662777 -1.6155906 -1.5614533 -1.5253856 -1.4801867][-1.2655134 -1.5365582 -2.0280073 -2.5039277 -2.9242907 -3.2274258 -3.2903905 -3.0776231 -2.6814702 -2.0879846 -1.5003376 -1.1018562 -1.0314593 -1.0632074 -1.121273][-0.37748528 -0.67613482 -1.1400354 -1.6130714 -2.0683024 -2.4226451 -2.5681696 -2.4733291 -2.1358535 -1.5292346 -0.91299915 -0.5277884 -0.47502971 -0.51294971 -0.53693557][-0.14454222 -0.32042408 -0.63538694 -1.0129225 -1.4535136 -1.8523176 -2.0650685 -2.0605998 -1.7840412 -1.223947 -0.62799048 -0.23336935 -0.13339663 -0.13577032 -0.11449146][-0.0056562424 -0.01965332 -0.18277264 -0.4390955 -0.80305862 -1.181062 -1.4351776 -1.5160024 -1.3205378 -0.85496807 -0.33534145 0.026652336 0.12793159 0.10457134 0.08448267]]...]
INFO - root - 2017-12-07 03:27:46.618023: step 310, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.765 sec/batch; 70h:37m:20s remains)
INFO - root - 2017-12-07 03:27:54.281309: step 320, loss = 0.66, batch loss = 0.59 (10.3 examples/sec; 0.780 sec/batch; 71h:56m:05s remains)
INFO - root - 2017-12-07 03:28:01.677045: step 330, loss = 0.94, batch loss = 0.87 (15.6 examples/sec; 0.511 sec/batch; 47h:10m:22s remains)
INFO - root - 2017-12-07 03:28:09.508852: step 340, loss = 0.81, batch loss = 0.74 (10.0 examples/sec; 0.799 sec/batch; 73h:45m:14s remains)
INFO - root - 2017-12-07 03:28:17.195441: step 350, loss = 0.75, batch loss = 0.67 (10.5 examples/sec; 0.760 sec/batch; 70h:06m:45s remains)
INFO - root - 2017-12-07 03:28:24.760965: step 360, loss = 0.89, batch loss = 0.81 (10.7 examples/sec; 0.750 sec/batch; 69h:10m:20s remains)
INFO - root - 2017-12-07 03:28:32.332351: step 370, loss = 0.98, batch loss = 0.90 (10.6 examples/sec; 0.754 sec/batch; 69h:34m:43s remains)
INFO - root - 2017-12-07 03:28:40.004716: step 380, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 70h:25m:59s remains)
INFO - root - 2017-12-07 03:28:47.679755: step 390, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.771 sec/batch; 71h:05m:19s remains)
INFO - root - 2017-12-07 03:28:55.324075: step 400, loss = 0.69, batch loss = 0.61 (10.6 examples/sec; 0.754 sec/batch; 69h:34m:35s remains)
2017-12-07 03:28:55.903343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.473531 -2.5500932 -2.7045877 -2.8453555 -2.8917117 -2.8216956 -2.7388704 -2.7539511 -2.9466507 -3.3813996 -3.8852561 -4.1633253 -4.2061553 -4.0891395 -3.7494092][-2.7335114 -2.8768821 -3.0635846 -3.2113924 -3.2572689 -3.1489086 -3.015408 -3.00441 -3.2221279 -3.6762414 -4.1284757 -4.2831693 -4.1925049 -4.0003228 -3.6231689][-2.8746712 -3.09991 -3.3154211 -3.4623535 -3.5174587 -3.3916693 -3.2134895 -3.1679082 -3.3742242 -3.8069654 -4.1871572 -4.2478523 -4.0409813 -3.7409463 -3.3242087][-2.6091273 -2.8828449 -3.12528 -3.315439 -3.4547749 -3.4288628 -3.3256745 -3.3073115 -3.48206 -3.837029 -4.1322136 -4.1545243 -3.8969812 -3.4945879 -3.0209808][-2.0539796 -2.2270443 -2.3399861 -2.435138 -2.5715046 -2.6728 -2.789659 -2.9916902 -3.2923841 -3.645119 -3.8481655 -3.7955062 -3.5017281 -3.0824866 -2.6490359][-1.9040949 -1.9080315 -1.7669089 -1.6118543 -1.5815823 -1.6704929 -1.919157 -2.3305209 -2.8287678 -3.2680001 -3.4310381 -3.2977786 -2.9809756 -2.6275849 -2.3221378][-2.4008439 -2.3803411 -2.1459682 -1.8679206 -1.7093344 -1.675245 -1.8082912 -2.1293092 -2.6044881 -3.0401134 -3.1795468 -2.9984019 -2.6766906 -2.4058938 -2.2222724][-2.9676411 -3.0088444 -2.808948 -2.5601878 -2.4086537 -2.2936385 -2.2346663 -2.3116782 -2.5766306 -2.8558662 -2.9391139 -2.7738733 -2.5208068 -2.3709471 -2.3025115][-3.1703854 -3.2831018 -3.1466622 -2.9838319 -2.9201593 -2.8331823 -2.7040644 -2.5975308 -2.600271 -2.6130767 -2.5671113 -2.4334636 -2.3036234 -2.3026922 -2.3624074][-2.7366815 -2.9100502 -2.9116116 -2.9161124 -3.0060434 -3.0589156 -3.0299242 -2.9434123 -2.8369305 -2.6677561 -2.4971714 -2.3444247 -2.2600334 -2.3122356 -2.4350245][-2.35514 -2.5184994 -2.6152287 -2.7493 -2.9394455 -3.0985513 -3.2006636 -3.253756 -3.2345669 -3.1112266 -2.9521837 -2.7814598 -2.6349306 -2.5846062 -2.6289537][-2.7872233 -2.9344907 -3.0672581 -3.2137561 -3.3484781 -3.4620368 -3.5647645 -3.6632226 -3.7034993 -3.6563659 -3.5565035 -3.3917947 -3.1792417 -2.9971824 -2.9203358][-3.6096663 -3.6785231 -3.7317872 -3.793772 -3.8245125 -3.8452206 -3.8848822 -3.9351096 -3.9456069 -3.9041185 -3.8351371 -3.7025135 -3.4974127 -3.2826071 -3.1586802][-4.0144806 -4.0416341 -4.0278654 -4.0056286 -3.9501526 -3.9034071 -3.8931856 -3.9115839 -3.9067941 -3.8682876 -3.8118901 -3.7067437 -3.5415545 -3.3597302 -3.2556081][-3.7493184 -3.7966523 -3.7880328 -3.7494121 -3.676697 -3.6169848 -3.6078978 -3.6405957 -3.6672485 -3.6666119 -3.6377831 -3.5592918 -3.4407961 -3.3174617 -3.2570651]]...]
INFO - root - 2017-12-07 03:29:03.677576: step 410, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.770 sec/batch; 71h:00m:18s remains)
INFO - root - 2017-12-07 03:29:11.361378: step 420, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.771 sec/batch; 71h:09m:10s remains)
INFO - root - 2017-12-07 03:29:18.812850: step 430, loss = 0.88, batch loss = 0.81 (16.0 examples/sec; 0.500 sec/batch; 46h:06m:42s remains)
INFO - root - 2017-12-07 03:29:26.457074: step 440, loss = 0.97, batch loss = 0.90 (10.8 examples/sec; 0.740 sec/batch; 68h:15m:07s remains)
INFO - root - 2017-12-07 03:29:34.098346: step 450, loss = 0.95, batch loss = 0.88 (10.6 examples/sec; 0.757 sec/batch; 69h:50m:40s remains)
INFO - root - 2017-12-07 03:29:41.771482: step 460, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.760 sec/batch; 70h:06m:57s remains)
INFO - root - 2017-12-07 03:29:49.441139: step 470, loss = 0.78, batch loss = 0.70 (10.6 examples/sec; 0.754 sec/batch; 69h:34m:51s remains)
INFO - root - 2017-12-07 03:29:57.150911: step 480, loss = 0.73, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 70h:19m:50s remains)
INFO - root - 2017-12-07 03:30:04.755268: step 490, loss = 0.94, batch loss = 0.87 (10.5 examples/sec; 0.760 sec/batch; 70h:02m:47s remains)
INFO - root - 2017-12-07 03:30:12.419428: step 500, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.756 sec/batch; 69h:40m:46s remains)
2017-12-07 03:30:13.026568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4767401 -3.4721143 -3.4871287 -3.400928 -3.224726 -3.0413733 -2.9400663 -2.9056935 -2.8281569 -2.7917233 -2.8366659 -2.8833246 -2.867578 -2.6319046 -2.2134762][-3.3316567 -3.3407638 -3.3255377 -3.1593771 -2.9358144 -2.8218074 -2.8826678 -2.9723117 -2.9891374 -3.0461659 -3.1370072 -3.1057878 -2.9548445 -2.496531 -1.7997665][-2.9621005 -2.9142935 -2.8196492 -2.6250119 -2.470645 -2.5183959 -2.8031681 -3.0045371 -2.9565568 -2.8742623 -2.7987609 -2.6256309 -2.4516244 -2.0183709 -1.4006634][-2.5858464 -2.5049098 -2.3483427 -2.1706743 -2.0856018 -2.1979728 -2.5897732 -2.8272028 -2.6948652 -2.4276271 -2.0672772 -1.8045697 -1.8984363 -1.8685863 -1.67542][-2.2151062 -2.1387138 -1.9804447 -1.8524139 -1.7368143 -1.6818774 -1.8889964 -2.0841248 -2.011956 -1.7323303 -1.2408433 -1.0483885 -1.4548097 -1.8032012 -1.9715564][-1.8382215 -1.7899323 -1.7603567 -1.7962115 -1.6170435 -1.1954806 -0.93204117 -0.90875077 -1.0355597 -0.9736867 -0.48636365 -0.3134613 -0.80834293 -1.2863188 -1.643959][-1.7254455 -1.6451418 -1.6650217 -1.8017101 -1.6049209 -0.87321043 -0.13121319 0.09844017 -0.32726479 -0.61672187 -0.22095346 0.024644375 -0.33367252 -0.71044469 -1.1174641][-1.7069013 -1.4017723 -1.2280495 -1.3319821 -1.2714536 -0.6780324 0.060630322 0.36056328 -0.13925457 -0.56984568 -0.287539 -0.033185482 -0.27570677 -0.54251051 -0.90559196][-1.4044728 -0.98650432 -0.64778042 -0.7067318 -0.90781808 -0.68041158 -0.23408175 -0.057592869 -0.41077948 -0.70922232 -0.4930675 -0.33862925 -0.52864051 -0.73321366 -1.0396249][-1.014924 -0.78267932 -0.57615733 -0.71670818 -1.0761898 -1.0483057 -0.79262757 -0.72624421 -0.94895124 -1.1495168 -1.0777941 -0.99036193 -1.0693474 -1.1514709 -1.3386736][-0.94525409 -1.0524743 -1.0850441 -1.2702897 -1.5734887 -1.5438163 -1.3594708 -1.3725514 -1.5172126 -1.6391032 -1.6407809 -1.5668728 -1.537535 -1.497133 -1.5789578][-1.1301882 -1.4957497 -1.727896 -1.95168 -2.1513929 -2.07724 -1.8984733 -1.8982375 -1.9509597 -1.9548509 -1.943156 -1.9018459 -1.8633792 -1.8092079 -1.8645041][-1.5685093 -1.9272437 -2.1508567 -2.3001482 -2.4304643 -2.4516454 -2.4079916 -2.4348295 -2.42731 -2.3174691 -2.2277796 -2.1928079 -2.1645145 -2.162523 -2.2523007][-2.2778287 -2.4604225 -2.5612464 -2.5692167 -2.5539184 -2.5478516 -2.5144482 -2.5043287 -2.4673128 -2.3457854 -2.3063452 -2.353868 -2.3581622 -2.394264 -2.4890091][-2.7015078 -2.6975346 -2.6986272 -2.6182065 -2.5220232 -2.4840693 -2.4173098 -2.3296604 -2.242888 -2.1642504 -2.244714 -2.3967137 -2.4318776 -2.4622135 -2.5449517]]...]
INFO - root - 2017-12-07 03:30:20.721335: step 510, loss = 0.70, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 70h:48m:34s remains)
INFO - root - 2017-12-07 03:30:28.343746: step 520, loss = 0.79, batch loss = 0.71 (10.7 examples/sec; 0.745 sec/batch; 68h:39m:37s remains)
INFO - root - 2017-12-07 03:30:35.969305: step 530, loss = 0.60, batch loss = 0.52 (13.2 examples/sec; 0.604 sec/batch; 55h:40m:38s remains)
INFO - root - 2017-12-07 03:30:43.705280: step 540, loss = 0.77, batch loss = 0.70 (10.1 examples/sec; 0.790 sec/batch; 72h:51m:06s remains)
INFO - root - 2017-12-07 03:30:51.361180: step 550, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 71h:01m:51s remains)
INFO - root - 2017-12-07 03:30:59.033832: step 560, loss = 0.70, batch loss = 0.62 (10.5 examples/sec; 0.761 sec/batch; 70h:09m:46s remains)
INFO - root - 2017-12-07 03:31:06.595624: step 570, loss = 0.85, batch loss = 0.78 (10.7 examples/sec; 0.745 sec/batch; 68h:39m:23s remains)
INFO - root - 2017-12-07 03:31:14.206013: step 580, loss = 0.84, batch loss = 0.76 (10.6 examples/sec; 0.758 sec/batch; 69h:52m:41s remains)
INFO - root - 2017-12-07 03:31:21.810346: step 590, loss = 0.99, batch loss = 0.91 (10.5 examples/sec; 0.760 sec/batch; 70h:04m:17s remains)
INFO - root - 2017-12-07 03:31:29.453470: step 600, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.767 sec/batch; 70h:45m:30s remains)
2017-12-07 03:31:30.069845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0315285 -4.1443896 -4.1298766 -4.1059828 -4.1487408 -4.2293949 -4.3356142 -4.4264936 -4.3545818 -4.0582023 -3.7163205 -3.4832568 -3.3907986 -3.3321323 -3.3398223][-4.1583529 -4.35615 -4.409595 -4.4149613 -4.4915075 -4.5797381 -4.6327357 -4.583827 -4.3221574 -3.8825922 -3.4376855 -3.1328614 -3.0365319 -3.0293956 -3.0926156][-4.077291 -4.2138257 -4.196784 -4.1207385 -4.1824751 -4.2758231 -4.2877064 -4.1429677 -3.7940292 -3.3799047 -3.0407352 -2.8737216 -2.9169888 -3.0011394 -3.1018016][-3.7189181 -3.7562675 -3.6626406 -3.5542884 -3.6021054 -3.6758103 -3.6337967 -3.4187651 -3.0623589 -2.789408 -2.7394705 -2.8921065 -3.1811719 -3.35148 -3.4331412][-3.3486691 -3.3409352 -3.2483265 -3.1660452 -3.148958 -3.1233058 -3.0007939 -2.746326 -2.4231029 -2.3279066 -2.561034 -2.9816604 -3.4526799 -3.6398544 -3.6185365][-3.2717323 -3.2287607 -3.0669312 -2.8842092 -2.7379727 -2.6150255 -2.4660773 -2.2373166 -2.0516326 -2.1879194 -2.6032143 -3.0866914 -3.559571 -3.6743565 -3.5085828][-3.3458667 -3.3252158 -3.1412735 -2.8852353 -2.6538811 -2.5369024 -2.42974 -2.2324283 -2.1410506 -2.4092064 -2.8700359 -3.2535148 -3.5564873 -3.4888215 -3.1711507][-3.3123198 -3.2290759 -3.0670185 -2.8306885 -2.5653744 -2.4667225 -2.4129672 -2.2199461 -2.1005666 -2.3321109 -2.7120376 -2.963707 -3.0604451 -2.8722019 -2.562763][-3.2794142 -3.1568391 -3.064146 -2.9219189 -2.6990752 -2.6580024 -2.682313 -2.4849367 -2.2930965 -2.4003832 -2.6518664 -2.8097086 -2.8039808 -2.6109357 -2.3528388][-3.2588558 -3.2262094 -3.2441783 -3.237237 -3.1511292 -3.2533312 -3.4008131 -3.2718189 -3.0680242 -3.0764642 -3.2020779 -3.3165832 -3.2774382 -3.0625091 -2.7445426][-3.1679268 -3.1859903 -3.1949768 -3.2167168 -3.2290874 -3.430882 -3.673075 -3.6609 -3.5811803 -3.635077 -3.7477827 -3.8687787 -3.8313732 -3.616894 -3.2907424][-3.1801791 -3.1958065 -3.0753808 -2.9528618 -2.92221 -3.1230865 -3.3959134 -3.4918213 -3.5568185 -3.733449 -3.9093449 -4.036932 -3.9942181 -3.788003 -3.5289578][-3.2431903 -3.1447482 -2.8787103 -2.6336765 -2.5427628 -2.6801696 -2.9104373 -3.029705 -3.1265717 -3.3150804 -3.4975493 -3.6222746 -3.6057959 -3.4706771 -3.3351212][-3.421515 -3.1384139 -2.7872124 -2.5633039 -2.5084438 -2.5922084 -2.7152271 -2.7517004 -2.7660055 -2.8536496 -2.9642534 -3.0490942 -3.0573385 -2.9933569 -2.9567437][-3.555855 -3.1598821 -2.766346 -2.5690637 -2.537138 -2.5901575 -2.6359272 -2.6089826 -2.5624428 -2.5733037 -2.6219175 -2.6602254 -2.6451509 -2.5838137 -2.5505123]]...]
INFO - root - 2017-12-07 03:31:37.644502: step 610, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.762 sec/batch; 70h:17m:27s remains)
INFO - root - 2017-12-07 03:31:45.333346: step 620, loss = 0.63, batch loss = 0.56 (10.4 examples/sec; 0.770 sec/batch; 70h:58m:03s remains)
INFO - root - 2017-12-07 03:31:52.782688: step 630, loss = 0.73, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 49h:53m:42s remains)
INFO - root - 2017-12-07 03:32:00.422356: step 640, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.758 sec/batch; 69h:54m:29s remains)
INFO - root - 2017-12-07 03:32:08.074345: step 650, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.769 sec/batch; 70h:52m:33s remains)
INFO - root - 2017-12-07 03:32:15.697424: step 660, loss = 0.89, batch loss = 0.81 (10.4 examples/sec; 0.772 sec/batch; 71h:10m:40s remains)
INFO - root - 2017-12-07 03:32:23.258363: step 670, loss = 0.99, batch loss = 0.92 (10.6 examples/sec; 0.751 sec/batch; 69h:15m:26s remains)
INFO - root - 2017-12-07 03:32:30.862414: step 680, loss = 1.03, batch loss = 0.95 (10.3 examples/sec; 0.774 sec/batch; 71h:17m:45s remains)
INFO - root - 2017-12-07 03:32:38.490385: step 690, loss = 0.81, batch loss = 0.73 (10.6 examples/sec; 0.752 sec/batch; 69h:20m:47s remains)
INFO - root - 2017-12-07 03:32:46.108938: step 700, loss = 0.92, batch loss = 0.84 (10.4 examples/sec; 0.767 sec/batch; 70h:42m:43s remains)
2017-12-07 03:32:46.691219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0823464 -3.0962682 -3.110059 -3.109787 -3.0878954 -3.0708671 -3.0489054 -3.0306337 -3.0099511 -2.978194 -2.9377337 -2.8772001 -2.8183093 -2.7382498 -2.6566038][-3.3286538 -3.3755555 -3.4101052 -3.4170382 -3.3812151 -3.3300242 -3.2581725 -3.1987009 -3.1499999 -3.1062646 -3.0768304 -3.0267258 -2.9919481 -2.9353347 -2.8446307][-3.29112 -3.3769126 -3.421514 -3.4355383 -3.3959217 -3.3181648 -3.203295 -3.1167002 -3.0481243 -2.9913445 -2.9684262 -2.9175749 -2.9022844 -2.8830581 -2.8149748][-3.0294833 -3.1485198 -3.2051148 -3.2411833 -3.2220497 -3.1510756 -3.0408053 -2.9697843 -2.91087 -2.855509 -2.8325481 -2.7421389 -2.6826019 -2.6475525 -2.5719512][-2.6461434 -2.7578809 -2.8156838 -2.8841827 -2.9106708 -2.8932292 -2.8563461 -2.8633716 -2.8735981 -2.8736451 -2.8806791 -2.7573292 -2.6176734 -2.5160363 -2.3824215][-2.2597144 -2.3076141 -2.3321764 -2.4116328 -2.4764237 -2.5056152 -2.5329866 -2.59903 -2.6804152 -2.7820067 -2.8801951 -2.8093076 -2.66722 -2.5496364 -2.3877163][-2.0941961 -2.0737598 -2.0334718 -2.0776567 -2.1295521 -2.1531527 -2.1810277 -2.2202952 -2.2903118 -2.445889 -2.6252604 -2.65965 -2.6067805 -2.5617795 -2.4500489][-2.1819396 -2.1612391 -2.0795238 -2.0627463 -2.0535088 -2.0225375 -2.0053034 -1.9703636 -1.9692819 -2.1067772 -2.2959027 -2.3992629 -2.4447169 -2.4934165 -2.4757829][-2.2908156 -2.3459969 -2.3184905 -2.3243475 -2.3191917 -2.2889955 -2.2735281 -2.1881487 -2.0918908 -2.1366379 -2.2370608 -2.2997923 -2.3397102 -2.3955734 -2.42285][-2.4077759 -2.5140605 -2.5586948 -2.6257117 -2.6823163 -2.7103925 -2.7500739 -2.6861787 -2.5678239 -2.5411277 -2.530318 -2.4882421 -2.4394095 -2.4153538 -2.4103496][-2.571455 -2.6442642 -2.6825385 -2.7428393 -2.7935221 -2.819591 -2.86451 -2.8335333 -2.7651219 -2.7516639 -2.7241881 -2.6537189 -2.574157 -2.5015521 -2.4535942][-2.6091835 -2.6199183 -2.6283021 -2.6682081 -2.6969471 -2.6959603 -2.7048149 -2.6702058 -2.629231 -2.6203504 -2.5969276 -2.5485237 -2.5106747 -2.4606128 -2.4104149][-2.5102077 -2.4812019 -2.4661865 -2.4802661 -2.4811983 -2.4576216 -2.4393573 -2.4058893 -2.3820629 -2.3746068 -2.3693342 -2.3582485 -2.3690906 -2.3539436 -2.3222489][-2.3323789 -2.3102415 -2.3023903 -2.3116913 -2.2947106 -2.2599165 -2.221163 -2.1751297 -2.14448 -2.1176775 -2.1148102 -2.1260304 -2.1673291 -2.1793866 -2.164654][-2.3159075 -2.3222685 -2.3444159 -2.3708208 -2.365495 -2.3541751 -2.3301601 -2.2926834 -2.2612317 -2.2137961 -2.1869051 -2.1872718 -2.2252955 -2.2398446 -2.2259896]]...]
INFO - root - 2017-12-07 03:32:54.332899: step 710, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.774 sec/batch; 71h:19m:53s remains)
INFO - root - 2017-12-07 03:33:01.890305: step 720, loss = 0.70, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 71h:17m:22s remains)
INFO - root - 2017-12-07 03:33:09.416000: step 730, loss = 0.85, batch loss = 0.78 (13.7 examples/sec; 0.584 sec/batch; 53h:50m:14s remains)
INFO - root - 2017-12-07 03:33:17.086114: step 740, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.793 sec/batch; 73h:03m:43s remains)
INFO - root - 2017-12-07 03:33:24.688193: step 750, loss = 0.61, batch loss = 0.53 (10.4 examples/sec; 0.772 sec/batch; 71h:09m:45s remains)
INFO - root - 2017-12-07 03:33:32.379965: step 760, loss = 1.03, batch loss = 0.95 (10.4 examples/sec; 0.770 sec/batch; 70h:54m:57s remains)
INFO - root - 2017-12-07 03:33:40.035742: step 770, loss = 0.83, batch loss = 0.75 (10.2 examples/sec; 0.783 sec/batch; 72h:07m:30s remains)
INFO - root - 2017-12-07 03:33:47.773783: step 780, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.775 sec/batch; 71h:24m:37s remains)
INFO - root - 2017-12-07 03:33:55.540273: step 790, loss = 0.93, batch loss = 0.86 (10.8 examples/sec; 0.743 sec/batch; 68h:29m:20s remains)
INFO - root - 2017-12-07 03:34:03.336767: step 800, loss = 1.01, batch loss = 0.94 (10.2 examples/sec; 0.784 sec/batch; 72h:12m:33s remains)
2017-12-07 03:34:03.929370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.681299 -0.95849419 -1.1255817 -1.7250624 -1.7732983 -1.8112419 -1.7032433 -1.2583137 -1.0561745 -0.94611788 -0.94022083 -1.0352283 -0.89101124 -0.82669353 -0.90864396][-1.3045652 -0.81210566 -1.1272962 -1.6390862 -1.6970952 -1.7859168 -1.6317151 -1.178694 -0.95127773 -0.7913332 -0.66824889 -0.62667751 -0.4393127 -0.45098448 -0.69962525][-1.3351812 -1.1278932 -1.5275354 -1.8694181 -1.9393206 -1.9176764 -1.5849807 -1.0934687 -0.83945084 -0.71260118 -0.60727143 -0.52859712 -0.32455111 -0.3111043 -0.66873741][-1.3753448 -1.4217317 -1.8239524 -2.0878549 -2.315933 -2.186862 -1.6703665 -1.0514445 -0.63922954 -0.59002328 -0.58513641 -0.44785333 -0.12634563 0.0031919479 -0.43408418][-1.3583508 -1.5420294 -1.8062878 -1.9621089 -2.3385534 -2.157238 -1.6601667 -0.98828435 -0.42400837 -0.52733517 -0.66976786 -0.43369198 0.086981773 0.42469454 0.011467934][-1.4199734 -1.6848867 -1.682518 -1.6358833 -1.9751203 -1.7039251 -1.3492663 -0.62942791 0.10127211 -0.1549964 -0.52314806 -0.38966846 0.1048336 0.60288668 0.36808872][-1.3231788 -1.7208786 -1.4418175 -1.1967525 -1.4309955 -1.101676 -0.90237665 -0.06731081 0.88290882 0.5430131 -0.04208231 -0.23218679 -0.060770035 0.424603 0.42203808][-0.85185051 -1.4638228 -1.0565753 -0.74363232 -0.94333577 -0.69608068 -0.71100712 0.11185312 1.0777235 0.74061584 0.094738007 -0.33289003 -0.43618608 -0.021901608 0.17094755][-0.4648366 -1.2985601 -0.95111156 -0.69063067 -0.86683369 -0.74163485 -0.98092842 -0.37672853 0.3756814 0.11432981 -0.47523117 -1.0041461 -1.1946316 -0.74727345 -0.33224392][-0.34199238 -1.3052974 -1.1516104 -1.0313802 -1.2026875 -1.1739216 -1.5204837 -1.1266072 -0.58888841 -0.76476622 -1.3138418 -1.9370055 -2.1515472 -1.6171081 -0.94261861][-0.41564989 -1.4016745 -1.4537358 -1.4520602 -1.6269758 -1.6929402 -2.021095 -1.7424333 -1.314631 -1.4190915 -1.90625 -2.5256386 -2.7426305 -2.250644 -1.5093088][-0.63444591 -1.5878444 -1.7767007 -1.7959583 -1.9329197 -2.0486717 -2.3209636 -2.1055472 -1.72386 -1.7520738 -2.1395869 -2.6413286 -2.8609941 -2.5656219 -2.0502822][-0.76977324 -1.6692166 -1.9399011 -1.9275115 -1.994113 -2.1207008 -2.3313236 -2.1642313 -1.8044496 -1.7634091 -2.0306945 -2.4347787 -2.7559838 -2.7288821 -2.5395012][-0.81711435 -1.6034639 -1.9303305 -1.9127891 -1.9333279 -2.0361311 -2.184135 -2.0603137 -1.7241216 -1.6124151 -1.7509315 -2.139961 -2.6259215 -2.8376846 -2.8415914][-0.98714972 -1.553896 -1.8752494 -1.8920836 -1.9258289 -2.018471 -2.1201639 -2.0447795 -1.7830844 -1.6310563 -1.6368654 -1.9657838 -2.5618896 -2.9591641 -3.0606272]]...]
INFO - root - 2017-12-07 03:34:11.601245: step 810, loss = 0.81, batch loss = 0.74 (11.4 examples/sec; 0.701 sec/batch; 64h:33m:44s remains)
INFO - root - 2017-12-07 03:34:19.350458: step 820, loss = 0.59, batch loss = 0.52 (10.5 examples/sec; 0.763 sec/batch; 70h:16m:14s remains)
INFO - root - 2017-12-07 03:34:26.800096: step 830, loss = 0.71, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 48h:23m:10s remains)
INFO - root - 2017-12-07 03:34:34.464599: step 840, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.780 sec/batch; 71h:51m:23s remains)
INFO - root - 2017-12-07 03:34:42.058074: step 850, loss = 0.66, batch loss = 0.59 (10.3 examples/sec; 0.777 sec/batch; 71h:37m:17s remains)
INFO - root - 2017-12-07 03:34:49.655013: step 860, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.755 sec/batch; 69h:31m:35s remains)
INFO - root - 2017-12-07 03:34:57.357828: step 870, loss = 0.72, batch loss = 0.65 (10.7 examples/sec; 0.751 sec/batch; 69h:11m:39s remains)
INFO - root - 2017-12-07 03:35:04.959503: step 880, loss = 0.92, batch loss = 0.85 (10.8 examples/sec; 0.741 sec/batch; 68h:15m:43s remains)
INFO - root - 2017-12-07 03:35:12.693287: step 890, loss = 0.88, batch loss = 0.81 (10.7 examples/sec; 0.749 sec/batch; 68h:57m:29s remains)
INFO - root - 2017-12-07 03:35:20.472993: step 900, loss = 0.66, batch loss = 0.59 (10.2 examples/sec; 0.783 sec/batch; 72h:08m:22s remains)
2017-12-07 03:35:21.063964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8856976 -1.5051754 -1.143549 -1.0839515 -0.6887567 0.36624241 0.9175539 0.056534767 -1.3725789 -1.8535757 -1.404072 -1.2853773 -1.7468221 -1.8723061 -1.5110211][-0.57722831 -0.42943025 -0.37186861 -0.646225 -0.46549582 0.46992302 0.97802162 0.037246704 -1.5251775 -2.1384406 -1.6960504 -1.4097829 -1.7330809 -1.7977107 -1.400322][0.30100584 0.12303734 -0.21262789 -0.93321705 -1.0127761 -0.16727018 0.46773624 -0.18749571 -1.5470452 -2.2062566 -1.8898947 -1.4957361 -1.6145875 -1.615921 -1.3030088][0.051390648 -0.2505908 -0.74262619 -1.593451 -1.6922727 -0.7881813 0.079452991 -0.1138072 -1.1084592 -1.8024714 -1.752012 -1.4061158 -1.2933292 -1.1830261 -0.92708516][-0.61447 -0.81548667 -1.2257161 -1.8781128 -1.8014596 -0.80691934 0.24791574 0.48884153 -0.21994305 -1.0071838 -1.3104734 -1.1834526 -0.93048096 -0.69781852 -0.4313488][-0.9359355 -1.0277331 -1.240262 -1.5208361 -1.1947374 -0.13434792 1.0140505 1.4304371 0.728518 -0.26169157 -0.9609704 -1.1035602 -0.79266334 -0.36175728 0.051556587][-1.0026336 -0.9746213 -0.935621 -0.86125493 -0.33133125 0.70200777 1.7588096 1.9670234 1.045104 -0.14633131 -1.0889597 -1.3144751 -0.80193949 -0.086672306 0.468565][-1.1315794 -1.0467424 -0.86839032 -0.64834404 -0.20687437 0.56801653 1.3116469 1.2145815 0.29720592 -0.72053528 -1.5671775 -1.6535408 -0.89976215 -0.014360428 0.54661226][-1.5924177 -1.5445342 -1.3550205 -1.0774963 -0.75024343 -0.19272566 0.35107183 0.23488522 -0.368515 -0.97605586 -1.6199348 -1.6553791 -0.89771986 -0.11497402 0.28642654][-2.2122014 -2.2276435 -2.0343685 -1.5743847 -1.1021643 -0.5917511 -0.15627384 -0.2433176 -0.5618875 -0.84702229 -1.3095679 -1.2936447 -0.60519218 0.026553154 0.2098093][-2.6156619 -2.6669545 -2.4416807 -1.7640843 -1.0679388 -0.56466341 -0.31882524 -0.47195554 -0.66565108 -0.84634995 -1.1500399 -1.0190296 -0.35001612 0.21406174 0.30748606][-2.9407668 -3.0943904 -2.9177704 -2.1909232 -1.4092581 -0.897259 -0.678138 -0.69296694 -0.698421 -0.81226873 -1.0262508 -0.8818717 -0.31491709 0.17369413 0.36124611][-2.7782755 -3.0683804 -3.1075704 -2.7403383 -2.2728934 -1.9317892 -1.7117991 -1.5161965 -1.2850354 -1.2789567 -1.402668 -1.3225679 -0.91366434 -0.482049 -0.065690517][-1.9436624 -2.2433791 -2.5343122 -2.7086835 -2.7978787 -2.8620105 -2.895349 -2.8300428 -2.6251225 -2.5392575 -2.4982328 -2.4262638 -2.1467981 -1.7339005 -1.0562294][-1.4138942 -1.5461128 -1.8127689 -2.1776149 -2.5689559 -2.9661932 -3.3269148 -3.5425959 -3.5039787 -3.4181228 -3.3211746 -3.3084741 -3.158987 -2.811132 -1.948514]]...]
INFO - root - 2017-12-07 03:35:28.716084: step 910, loss = 0.80, batch loss = 0.72 (10.4 examples/sec; 0.772 sec/batch; 71h:07m:29s remains)
INFO - root - 2017-12-07 03:35:36.385447: step 920, loss = 1.03, batch loss = 0.96 (10.3 examples/sec; 0.778 sec/batch; 71h:38m:00s remains)
INFO - root - 2017-12-07 03:35:43.933230: step 930, loss = 0.92, batch loss = 0.85 (13.5 examples/sec; 0.593 sec/batch; 54h:34m:42s remains)
INFO - root - 2017-12-07 03:35:51.606045: step 940, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.771 sec/batch; 70h:59m:52s remains)
INFO - root - 2017-12-07 03:35:59.267996: step 950, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.774 sec/batch; 71h:16m:38s remains)
INFO - root - 2017-12-07 03:36:06.911348: step 960, loss = 0.85, batch loss = 0.78 (10.8 examples/sec; 0.740 sec/batch; 68h:07m:41s remains)
INFO - root - 2017-12-07 03:36:14.728375: step 970, loss = 0.83, batch loss = 0.76 (9.9 examples/sec; 0.807 sec/batch; 74h:16m:53s remains)
INFO - root - 2017-12-07 03:36:22.317934: step 980, loss = 0.69, batch loss = 0.61 (10.5 examples/sec; 0.763 sec/batch; 70h:13m:55s remains)
INFO - root - 2017-12-07 03:36:30.106618: step 990, loss = 0.63, batch loss = 0.56 (10.5 examples/sec; 0.764 sec/batch; 70h:22m:05s remains)
INFO - root - 2017-12-07 03:36:37.923287: step 1000, loss = 0.59, batch loss = 0.52 (10.4 examples/sec; 0.768 sec/batch; 70h:42m:27s remains)
2017-12-07 03:36:38.571423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5892506 -3.5653286 -3.5774803 -3.5824511 -3.6094985 -3.6562507 -3.684042 -3.5682642 -3.4343259 -3.4079177 -3.4053607 -3.3007565 -3.0132732 -2.6795008 -2.4004176][-3.6097059 -3.5647812 -3.5454874 -3.5292006 -3.5488002 -3.5807226 -3.588902 -3.4735708 -3.348536 -3.3074341 -3.2941043 -3.2426286 -3.0033455 -2.6854439 -2.4241817][-3.4905181 -3.4932592 -3.5283175 -3.5808868 -3.6673183 -3.7252698 -3.7211044 -3.6047935 -3.4552565 -3.3437903 -3.2685866 -3.2197926 -3.0078931 -2.6938939 -2.4560685][-3.2759736 -3.2680552 -3.2879176 -3.348165 -3.453321 -3.5272002 -3.5380478 -3.4560232 -3.3290429 -3.2161374 -3.152271 -3.1326275 -2.9499288 -2.6603494 -2.4757698][-3.3516874 -3.3404641 -3.3078966 -3.2661707 -3.2463737 -3.2124023 -3.1418731 -3.026556 -2.9380789 -2.9113841 -2.9723704 -3.0572567 -2.9249792 -2.6776233 -2.5450883][-3.6278009 -3.6662345 -3.6921935 -3.6572728 -3.5952551 -3.5037551 -3.3371706 -3.1566556 -3.0564618 -3.0162425 -3.0670457 -3.1487112 -3.0022879 -2.7473998 -2.6312745][-3.5278811 -3.5194368 -3.5040393 -3.4193645 -3.3071382 -3.1999526 -3.0240567 -2.9131804 -2.9679067 -3.0354893 -3.1394033 -3.2485056 -3.0994518 -2.8133471 -2.6904652][-3.2500744 -3.1606183 -3.0466328 -2.8516269 -2.6471038 -2.4898176 -2.2871735 -2.2656662 -2.5078969 -2.7139561 -2.9106712 -3.11735 -3.0564609 -2.8133628 -2.7116268][-3.4569325 -3.3815598 -3.2680955 -3.0590324 -2.8492572 -2.7122386 -2.5191717 -2.4978309 -2.709218 -2.8170018 -2.9159112 -3.0798559 -3.0347528 -2.825346 -2.7430043][-3.7671192 -3.7151492 -3.6155314 -3.4308817 -3.2641401 -3.1961813 -3.0699039 -3.03688 -3.1419587 -3.096488 -3.0763195 -3.1542573 -3.0653729 -2.8676372 -2.7935805][-3.827301 -3.7751338 -3.6903267 -3.5608637 -3.4332886 -3.385097 -3.2732353 -3.2053967 -3.2149096 -3.0852413 -3.0448625 -3.1357617 -3.0694213 -2.9124017 -2.8510575][-3.7947395 -3.7475462 -3.7100563 -3.6715946 -3.6068351 -3.567064 -3.4476812 -3.3318532 -3.2611299 -3.0991518 -3.0800426 -3.2063856 -3.1614513 -3.0168352 -2.9349098][-3.6184502 -3.5982161 -3.6105323 -3.6471951 -3.641103 -3.6197023 -3.5229316 -3.4055452 -3.317034 -3.1793518 -3.1862979 -3.3097463 -3.2570963 -3.1022711 -2.9850707][-3.6043622 -3.5797877 -3.5791261 -3.5988355 -3.5791218 -3.5422459 -3.4676929 -3.383116 -3.3328481 -3.2600431 -3.2877312 -3.3694024 -3.2932448 -3.1254196 -2.9837065][-3.7813091 -3.7315202 -3.6899779 -3.6608882 -3.6039937 -3.5476642 -3.487762 -3.4274392 -3.3983643 -3.36441 -3.3849525 -3.3989031 -3.2908092 -3.1190748 -2.9765286]]...]
INFO - root - 2017-12-07 03:36:46.335339: step 1010, loss = 0.74, batch loss = 0.67 (10.1 examples/sec; 0.795 sec/batch; 73h:13m:57s remains)
INFO - root - 2017-12-07 03:36:53.927782: step 1020, loss = 0.63, batch loss = 0.56 (10.5 examples/sec; 0.762 sec/batch; 70h:11m:36s remains)
INFO - root - 2017-12-07 03:37:01.303965: step 1030, loss = 0.77, batch loss = 0.70 (15.1 examples/sec; 0.530 sec/batch; 48h:49m:57s remains)
INFO - root - 2017-12-07 03:37:08.954860: step 1040, loss = 0.67, batch loss = 0.60 (10.6 examples/sec; 0.751 sec/batch; 69h:09m:44s remains)
INFO - root - 2017-12-07 03:37:16.552545: step 1050, loss = 0.90, batch loss = 0.82 (10.6 examples/sec; 0.752 sec/batch; 69h:11m:36s remains)
INFO - root - 2017-12-07 03:37:24.271146: step 1060, loss = 0.70, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 72h:20m:29s remains)
INFO - root - 2017-12-07 03:37:31.968550: step 1070, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.760 sec/batch; 69h:56m:51s remains)
INFO - root - 2017-12-07 03:37:39.635415: step 1080, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.760 sec/batch; 69h:57m:53s remains)
INFO - root - 2017-12-07 03:37:47.231989: step 1090, loss = 0.80, batch loss = 0.72 (10.8 examples/sec; 0.742 sec/batch; 68h:15m:58s remains)
INFO - root - 2017-12-07 03:37:54.887582: step 1100, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.781 sec/batch; 71h:52m:55s remains)
2017-12-07 03:37:55.491612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0015819 -1.8469553 -1.8174226 -2.0507181 -2.4275215 -2.8978939 -3.3080685 -3.6527169 -4.119009 -4.3951592 -4.6466026 -4.7839656 -4.28318 -3.5468798 -2.9668989][-2.181247 -1.9733958 -1.6509304 -1.509692 -1.7004836 -2.1755672 -2.5888157 -2.9883451 -3.6046247 -4.0384717 -4.4081421 -4.6358004 -4.2353358 -3.5995903 -3.0949714][-2.3779471 -2.2074866 -1.8215194 -1.5109575 -1.6264606 -2.1843677 -2.6195908 -2.978785 -3.47819 -3.8339503 -4.1685195 -4.3923974 -4.0775061 -3.5558836 -3.1428485][-2.5993185 -2.5256169 -2.2515774 -1.9487581 -2.0352483 -2.5550447 -2.8950624 -3.1379852 -3.445895 -3.67628 -3.9914029 -4.2532854 -4.0289621 -3.5929391 -3.2058821][-2.7767611 -2.8900876 -2.8127775 -2.5971746 -2.6224754 -2.9240847 -3.0094309 -3.0594275 -3.1756291 -3.2855484 -3.6616855 -4.1028485 -4.07752 -3.75987 -3.3656023][-2.4932294 -2.7043409 -2.7397738 -2.5972285 -2.6185799 -2.8070078 -2.7873144 -2.795758 -2.8663192 -2.94353 -3.3753219 -3.9707494 -4.1537514 -3.98119 -3.5733297][-1.8494606 -2.0712926 -2.1351993 -2.020788 -2.0479884 -2.2044184 -2.239203 -2.3605223 -2.5248365 -2.6643577 -3.1652272 -3.812197 -4.131712 -4.0963168 -3.6994805][-1.7907863 -1.9160755 -1.8899755 -1.7231941 -1.6849651 -1.7771475 -1.8607337 -2.0667031 -2.2717354 -2.4593706 -2.9719296 -3.5642347 -3.9259996 -4.0055766 -3.6883435][-2.1559339 -2.2100489 -2.086565 -1.8727503 -1.7747562 -1.8251288 -1.9594762 -2.2298155 -2.4051638 -2.5715525 -2.9918203 -3.4264107 -3.7108445 -3.8328307 -3.6188107][-2.2367871 -2.1975114 -1.9799769 -1.7527316 -1.6212151 -1.6315691 -1.8284872 -2.158756 -2.3419135 -2.5119243 -2.8504696 -3.165534 -3.3963218 -3.5789754 -3.5273948][-2.2316539 -2.1145837 -1.8115671 -1.5670133 -1.3724122 -1.3016567 -1.5171442 -1.8645408 -2.0653682 -2.2654958 -2.5559096 -2.8118792 -3.0117698 -3.2580695 -3.393899][-2.0288708 -1.9748392 -1.7620647 -1.627223 -1.471595 -1.3986938 -1.5801702 -1.8479762 -2.042341 -2.2843502 -2.5537014 -2.7753186 -2.9106946 -3.1117067 -3.3109493][-2.0783172 -2.1062615 -2.0353782 -2.0333209 -1.9484427 -1.9018567 -2.0105369 -2.119607 -2.2401741 -2.4551344 -2.7079945 -2.9330232 -3.0182605 -3.1289883 -3.30657][-2.5443783 -2.5751219 -2.5342484 -2.5282345 -2.4000833 -2.315933 -2.3233984 -2.2836885 -2.29653 -2.4251497 -2.656527 -2.9224429 -3.0054379 -3.0711479 -3.2223034][-2.7255588 -2.7181964 -2.6890707 -2.6869874 -2.5490675 -2.4515045 -2.3824441 -2.2150135 -2.1046858 -2.1189165 -2.3420491 -2.6620362 -2.7605777 -2.8081052 -2.9317186]]...]
INFO - root - 2017-12-07 03:38:03.059251: step 1110, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.767 sec/batch; 70h:38m:30s remains)
INFO - root - 2017-12-07 03:38:10.676117: step 1120, loss = 0.87, batch loss = 0.80 (10.7 examples/sec; 0.748 sec/batch; 68h:52m:35s remains)
INFO - root - 2017-12-07 03:38:18.200085: step 1130, loss = 0.71, batch loss = 0.64 (13.1 examples/sec; 0.610 sec/batch; 56h:09m:30s remains)
INFO - root - 2017-12-07 03:38:25.892316: step 1140, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 68h:39m:24s remains)
INFO - root - 2017-12-07 03:38:33.545893: step 1150, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.748 sec/batch; 68h:49m:25s remains)
INFO - root - 2017-12-07 03:38:41.353663: step 1160, loss = 0.71, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 71h:39m:52s remains)
INFO - root - 2017-12-07 03:38:48.986278: step 1170, loss = 0.67, batch loss = 0.59 (10.1 examples/sec; 0.789 sec/batch; 72h:34m:41s remains)
INFO - root - 2017-12-07 03:38:56.667194: step 1180, loss = 0.82, batch loss = 0.74 (10.4 examples/sec; 0.769 sec/batch; 70h:44m:35s remains)
INFO - root - 2017-12-07 03:39:04.347372: step 1190, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.753 sec/batch; 69h:20m:32s remains)
INFO - root - 2017-12-07 03:39:11.982114: step 1200, loss = 0.73, batch loss = 0.66 (10.7 examples/sec; 0.747 sec/batch; 68h:45m:42s remains)
2017-12-07 03:39:12.645168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6777372 -4.4902558 -4.3814387 -4.1766281 -3.9360869 -3.6907828 -3.8763227 -4.3280582 -4.7729411 -5.0387015 -4.9374361 -4.6766629 -4.1552753 -3.2751789 -2.6849961][-4.00957 -3.9857128 -3.9038825 -3.7242012 -3.6412177 -3.4839456 -3.6415224 -4.0964022 -4.6337285 -5.0122895 -5.0119104 -4.7331367 -4.0503631 -3.0268633 -2.3734884][-3.4674895 -3.5987654 -3.4463379 -3.2236843 -3.1165335 -2.847281 -2.8477664 -3.2817473 -3.9607592 -4.5354533 -4.7901058 -4.611939 -3.871315 -2.826107 -2.2070878][-2.9522946 -3.243906 -3.1305542 -2.9184353 -2.7040648 -2.2155898 -1.9345188 -2.2513115 -3.0906906 -4.0046215 -4.6594348 -4.7400317 -4.1169095 -3.12037 -2.44927][-2.3350227 -2.7154515 -2.6954684 -2.5730724 -2.3456089 -1.7154868 -1.1088414 -1.1643922 -2.0090647 -3.2027159 -4.2343354 -4.6928639 -4.4500113 -3.6639364 -2.8872435][-1.8657341 -2.2352715 -2.2637722 -2.1141789 -1.8204157 -0.97510076 0.087444305 0.35433102 -0.5576272 -2.0186784 -3.3923693 -4.34805 -4.7287292 -4.4245138 -3.6868548][-2.2402697 -2.7231612 -2.8243577 -2.4397998 -1.7856336 -0.45855141 1.3043213 2.071826 1.1178436 -0.53835392 -2.1237259 -3.4383824 -4.40932 -4.7988133 -4.4773192][-3.1118939 -3.9447267 -4.2649317 -3.7230189 -2.7548585 -1.1200764 1.0432763 2.211071 1.6313076 0.31709146 -1.0529025 -2.3439 -3.5383825 -4.3958173 -4.5709782][-3.4254508 -4.6059413 -5.2302208 -4.8321795 -3.8699386 -2.3095403 -0.42215681 0.59692574 0.48820019 -0.072923183 -0.82128525 -1.729327 -2.7317233 -3.6023397 -4.0309353][-3.2596169 -4.5120625 -5.3384342 -5.2723174 -4.510335 -3.2094541 -1.9020987 -1.2945251 -1.1950223 -1.0986254 -1.0876403 -1.3828323 -1.9133811 -2.5119081 -3.0559816][-3.1972134 -4.1585984 -4.8938847 -5.1190386 -4.6055784 -3.6731305 -2.9607549 -2.7339516 -2.5611973 -2.0864637 -1.5512371 -1.3152006 -1.363241 -1.5908625 -2.0652642][-2.9999967 -3.420733 -3.8814206 -4.2135377 -3.9577045 -3.4984751 -3.3712592 -3.4884334 -3.3583617 -2.8553467 -2.2379096 -1.8593836 -1.7283649 -1.7408183 -1.9958687][-2.8996859 -2.9084249 -3.1961255 -3.5713358 -3.4908395 -3.3615661 -3.5506072 -3.7787642 -3.6759284 -3.3559809 -2.9286368 -2.6001015 -2.5201178 -2.6061239 -2.8202617][-3.48978 -3.4223835 -3.7672668 -4.2583184 -4.2883511 -4.1906495 -4.2716765 -4.2649665 -4.0242348 -3.7878404 -3.482017 -3.2105417 -3.1916897 -3.3728595 -3.6934292][-4.1170874 -4.0228596 -4.3150516 -4.7890205 -4.8823028 -4.7612948 -4.6633477 -4.4445462 -4.1340375 -3.9504268 -3.7626693 -3.6253765 -3.7154603 -3.9152107 -4.2330589]]...]
INFO - root - 2017-12-07 03:39:20.083543: step 1210, loss = 0.80, batch loss = 0.72 (10.6 examples/sec; 0.752 sec/batch; 69h:10m:02s remains)
INFO - root - 2017-12-07 03:39:27.598433: step 1220, loss = 0.70, batch loss = 0.62 (10.6 examples/sec; 0.757 sec/batch; 69h:40m:07s remains)
INFO - root - 2017-12-07 03:39:35.023526: step 1230, loss = 0.75, batch loss = 0.68 (13.5 examples/sec; 0.593 sec/batch; 54h:33m:14s remains)
INFO - root - 2017-12-07 03:39:42.563957: step 1240, loss = 0.85, batch loss = 0.77 (10.8 examples/sec; 0.743 sec/batch; 68h:20m:04s remains)
INFO - root - 2017-12-07 03:39:50.160174: step 1250, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.768 sec/batch; 70h:42m:02s remains)
INFO - root - 2017-12-07 03:39:57.844017: step 1260, loss = 0.55, batch loss = 0.48 (10.3 examples/sec; 0.776 sec/batch; 71h:23m:55s remains)
INFO - root - 2017-12-07 03:40:05.499471: step 1270, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.755 sec/batch; 69h:25m:24s remains)
INFO - root - 2017-12-07 03:40:13.153066: step 1280, loss = 0.99, batch loss = 0.92 (10.7 examples/sec; 0.748 sec/batch; 68h:49m:53s remains)
INFO - root - 2017-12-07 03:40:20.825110: step 1290, loss = 0.69, batch loss = 0.62 (10.7 examples/sec; 0.747 sec/batch; 68h:46m:04s remains)
INFO - root - 2017-12-07 03:40:28.528983: step 1300, loss = 0.95, batch loss = 0.88 (10.3 examples/sec; 0.774 sec/batch; 71h:14m:12s remains)
2017-12-07 03:40:29.151935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7535405 -2.8174844 -2.8786864 -2.9341941 -2.9150865 -2.8702083 -2.8659511 -2.6369581 -2.4235258 -2.6479173 -2.8349407 -2.8164887 -2.8247223 -2.9042249 -2.888536][-2.699513 -2.7493474 -2.7945285 -2.8418763 -2.8349686 -2.81733 -2.8568563 -2.6622162 -2.4545069 -2.6962066 -2.8944883 -2.8342237 -2.7829385 -2.8508224 -2.8479652][-2.6439643 -2.6744998 -2.6916215 -2.7058477 -2.7245007 -2.7835822 -2.8868151 -2.7230928 -2.521059 -2.8028708 -3.0420697 -2.9552176 -2.8401661 -2.8646059 -2.8369765][-2.5762262 -2.5838747 -2.5687168 -2.5232573 -2.5725474 -2.7519469 -2.9258428 -2.7606165 -2.5440083 -2.8701806 -3.1677146 -3.0826349 -2.9405713 -2.9327257 -2.8651733][-2.5459597 -2.5279751 -2.4727957 -2.3310106 -2.3783948 -2.6860952 -2.9318113 -2.7423677 -2.5081394 -2.8790011 -3.2274077 -3.1592431 -3.0264056 -3.0121841 -2.911525][-2.5721006 -2.530735 -2.4341607 -2.196758 -2.1958311 -2.5761771 -2.8719335 -2.6434937 -2.3992059 -2.8232512 -3.2173839 -3.1854444 -3.0895886 -3.0820041 -2.9559517][-2.660861 -2.5866354 -2.4656949 -2.1624296 -2.0762486 -2.44564 -2.7429709 -2.452127 -2.1922202 -2.6753459 -3.1151347 -3.1429832 -3.1114655 -3.1220496 -2.9814334][-2.8586726 -2.7498243 -2.6172068 -2.2803626 -2.084374 -2.3687382 -2.61165 -2.2312093 -1.9392424 -2.4635968 -2.9407218 -3.0339599 -3.0837259 -3.1278439 -2.9870663][-3.1273608 -3.0187023 -2.9020658 -2.5613642 -2.2816377 -2.4791961 -2.6441064 -2.1659708 -1.8018365 -2.3014729 -2.7711213 -2.8984749 -3.0165811 -3.1019053 -2.9720688][-3.3108695 -3.2315259 -3.1700377 -2.8842158 -2.5880091 -2.72817 -2.8280773 -2.2996025 -1.8689828 -2.299289 -2.723794 -2.8391387 -2.974885 -3.0750222 -2.9476721][-3.3999228 -3.3551369 -3.3515048 -3.1576192 -2.9089699 -3.0267196 -3.0889342 -2.5681152 -2.1221511 -2.4831204 -2.8494036 -2.9179492 -3.0224853 -3.0999036 -2.9544656][-3.4295983 -3.4057033 -3.4336078 -3.3229275 -3.1417186 -3.2501695 -3.3002687 -2.8405282 -2.4306159 -2.7349262 -3.0452852 -3.0593281 -3.106317 -3.137356 -2.9730949][-3.3866014 -3.3849959 -3.4336376 -3.3893816 -3.2687993 -3.3824635 -3.4474244 -3.0737808 -2.7272801 -2.9860518 -3.239536 -3.2095609 -3.19163 -3.155077 -2.9690289][-3.3164287 -3.345428 -3.4062943 -3.4142218 -3.3589478 -3.4841297 -3.5756929 -3.3020175 -3.0347524 -3.2487836 -3.4395127 -3.3745904 -3.3003833 -3.1869445 -2.973896][-3.2260017 -3.2621632 -3.3181524 -3.3572969 -3.3527653 -3.4713686 -3.57611 -3.4027443 -3.2241502 -3.4007046 -3.532146 -3.4553239 -3.3609793 -3.2086396 -2.9939165]]...]
INFO - root - 2017-12-07 03:40:36.788016: step 1310, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.750 sec/batch; 68h:57m:09s remains)
INFO - root - 2017-12-07 03:40:44.456975: step 1320, loss = 0.69, batch loss = 0.61 (10.6 examples/sec; 0.753 sec/batch; 69h:15m:24s remains)
INFO - root - 2017-12-07 03:40:51.906157: step 1330, loss = 0.81, batch loss = 0.74 (13.4 examples/sec; 0.598 sec/batch; 55h:02m:18s remains)
INFO - root - 2017-12-07 03:40:59.632789: step 1340, loss = 0.83, batch loss = 0.75 (10.0 examples/sec; 0.796 sec/batch; 73h:14m:01s remains)
INFO - root - 2017-12-07 03:41:07.392553: step 1350, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.774 sec/batch; 71h:11m:54s remains)
INFO - root - 2017-12-07 03:41:15.077308: step 1360, loss = 0.83, batch loss = 0.75 (10.7 examples/sec; 0.748 sec/batch; 68h:47m:56s remains)
INFO - root - 2017-12-07 03:41:22.714204: step 1370, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.774 sec/batch; 71h:10m:49s remains)
INFO - root - 2017-12-07 03:41:30.396281: step 1380, loss = 0.78, batch loss = 0.70 (10.5 examples/sec; 0.763 sec/batch; 70h:11m:56s remains)
INFO - root - 2017-12-07 03:41:37.935584: step 1390, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.781 sec/batch; 71h:52m:33s remains)
INFO - root - 2017-12-07 03:41:45.547353: step 1400, loss = 0.62, batch loss = 0.55 (10.3 examples/sec; 0.777 sec/batch; 71h:25m:32s remains)
2017-12-07 03:41:46.244177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8913231 -4.0393081 -4.1019163 -4.1134858 -4.0315681 -3.9303346 -3.9983623 -4.16326 -4.1080441 -3.97646 -3.9532864 -3.9653361 -3.9344993 -3.9076881 -3.9226763][-3.507448 -3.6867886 -3.8198786 -3.8724091 -3.77947 -3.6459894 -3.6814437 -3.8271527 -3.7396894 -3.62841 -3.6689341 -3.7030647 -3.7034583 -3.7322979 -3.7807908][-3.0188308 -3.1942661 -3.3780942 -3.458281 -3.3175168 -3.1292763 -3.1262496 -3.2583432 -3.1812136 -3.0920181 -3.1568084 -3.1757107 -3.1916578 -3.2548308 -3.2778344][-2.6175146 -2.8113089 -3.0780909 -3.2211523 -3.0641954 -2.8591375 -2.8156013 -2.8910646 -2.8105659 -2.6979771 -2.7081614 -2.6433253 -2.6278963 -2.6720204 -2.5932646][-2.1257355 -2.3172903 -2.679883 -2.9770448 -2.9615297 -2.8776612 -2.8530676 -2.8585472 -2.7246928 -2.5026214 -2.3824763 -2.2001941 -2.1173513 -2.0790968 -1.8498819][-1.9348719 -2.0236063 -2.3367257 -2.65872 -2.7299018 -2.7473345 -2.7663743 -2.789258 -2.6892312 -2.4127798 -2.2097008 -2.0081496 -1.913265 -1.7897398 -1.4138][-2.4057527 -2.2938073 -2.3685672 -2.5184646 -2.5181241 -2.5090818 -2.5066628 -2.5994892 -2.6648147 -2.5348701 -2.4760423 -2.4718127 -2.5114689 -2.4074578 -2.0029721][-2.9195204 -2.6348872 -2.4836314 -2.4641237 -2.4026003 -2.3790436 -2.3389223 -2.415133 -2.5727077 -2.6657996 -2.9113607 -3.2585869 -3.5809832 -3.6463804 -3.3829231][-3.1481082 -2.7567859 -2.4942596 -2.4357758 -2.4868283 -2.618361 -2.6583123 -2.6676967 -2.7575786 -2.8929768 -3.2279286 -3.7260931 -4.1890984 -4.3754358 -4.2541108][-3.4798453 -3.0429368 -2.7242069 -2.6533332 -2.825829 -3.1365948 -3.3185215 -3.31699 -3.2972367 -3.2982032 -3.4513111 -3.8137691 -4.2030396 -4.4203234 -4.3936849][-3.9913757 -3.58781 -3.2113123 -3.0306425 -3.1143813 -3.3927522 -3.5569975 -3.5029838 -3.4030709 -3.3033986 -3.2894392 -3.4902163 -3.7723703 -3.9933827 -4.0356326][-4.4472456 -4.185153 -3.80561 -3.5024359 -3.3960714 -3.482805 -3.4887567 -3.335017 -3.1662564 -3.0042958 -2.9054773 -2.9832506 -3.1934137 -3.4442613 -3.5765262][-4.6210837 -4.58095 -4.339931 -4.032074 -3.7841036 -3.6809394 -3.5358858 -3.3193259 -3.1301022 -2.9753432 -2.9061399 -2.9593458 -3.1623883 -3.4617648 -3.6696131][-4.5562272 -4.711206 -4.6491971 -4.44458 -4.1882148 -3.9811318 -3.7105014 -3.4277852 -3.2467575 -3.1635981 -3.2179971 -3.3156362 -3.5281727 -3.8337955 -4.0383544][-4.2073059 -4.4941292 -4.5606685 -4.5116887 -4.4023352 -4.2606173 -3.9585381 -3.6045702 -3.3725929 -3.2747567 -3.3646903 -3.4598694 -3.6423864 -3.9131131 -4.0799141]]...]
INFO - root - 2017-12-07 03:41:53.832199: step 1410, loss = 0.89, batch loss = 0.81 (10.8 examples/sec; 0.741 sec/batch; 68h:08m:51s remains)
INFO - root - 2017-12-07 03:42:01.464967: step 1420, loss = 0.84, batch loss = 0.77 (11.0 examples/sec; 0.726 sec/batch; 66h:46m:10s remains)
INFO - root - 2017-12-07 03:42:08.789607: step 1430, loss = 0.92, batch loss = 0.85 (13.6 examples/sec; 0.589 sec/batch; 54h:09m:34s remains)
INFO - root - 2017-12-07 03:42:16.444563: step 1440, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.766 sec/batch; 70h:27m:06s remains)
INFO - root - 2017-12-07 03:42:24.135977: step 1450, loss = 0.78, batch loss = 0.71 (10.0 examples/sec; 0.799 sec/batch; 73h:28m:14s remains)
INFO - root - 2017-12-07 03:42:31.810595: step 1460, loss = 1.04, batch loss = 0.96 (10.3 examples/sec; 0.777 sec/batch; 71h:26m:58s remains)
INFO - root - 2017-12-07 03:42:39.597048: step 1470, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.783 sec/batch; 71h:57m:28s remains)
INFO - root - 2017-12-07 03:42:47.244796: step 1480, loss = 0.98, batch loss = 0.91 (10.3 examples/sec; 0.776 sec/batch; 71h:20m:01s remains)
INFO - root - 2017-12-07 03:42:54.921370: step 1490, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.758 sec/batch; 69h:43m:29s remains)
INFO - root - 2017-12-07 03:43:02.575576: step 1500, loss = 0.94, batch loss = 0.87 (10.3 examples/sec; 0.777 sec/batch; 71h:26m:31s remains)
2017-12-07 03:43:03.197242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3727646 -2.073139 -2.5578246 -3.4434726 -4.211235 -4.5634971 -4.4466934 -3.9037344 -3.0999188 -2.3892639 -2.4223902 -3.4370394 -4.6019135 -4.5976844 -3.450259][-1.9022613 -1.665308 -2.3890266 -3.594027 -4.5693331 -4.888186 -4.5363584 -3.7434289 -2.7323139 -1.9810286 -2.1776569 -3.4665771 -4.87409 -4.9228735 -3.6012557][-1.5389938 -1.3707128 -2.1836555 -3.4577413 -4.529933 -4.879909 -4.4441366 -3.5234942 -2.3527372 -1.5611122 -1.8842199 -3.3245692 -4.8371663 -4.9383264 -3.5760412][-1.2710304 -1.1958361 -1.9855754 -3.1486254 -4.1984553 -4.5976224 -4.2034268 -3.3342927 -2.2363181 -1.5449762 -1.9675477 -3.4236732 -4.8512063 -4.9021468 -3.5140972][-1.3658669 -1.4929528 -2.22695 -3.188545 -4.0419989 -4.3549075 -3.9633694 -3.2060146 -2.3481557 -1.8740764 -2.4026253 -3.8119678 -5.0728955 -5.029655 -3.6048462][-1.7257485 -2.117476 -2.8455162 -3.6294441 -4.1818337 -4.3190327 -3.8972046 -3.25212 -2.6171503 -2.3223267 -2.9089689 -4.247117 -5.3337064 -5.1850724 -3.7471764][-2.15627 -2.628058 -3.285306 -3.9341607 -4.2834721 -4.3124485 -3.8874557 -3.2803602 -2.711175 -2.4635053 -3.103714 -4.4610114 -5.4848528 -5.2900753 -3.873754][-2.3095624 -2.6949835 -3.1388984 -3.5888858 -3.8219266 -3.8786592 -3.6323509 -3.1496935 -2.5653656 -2.18951 -2.7589922 -4.1032391 -5.113091 -4.9715228 -3.7110736][-2.4857483 -2.8190794 -3.012866 -3.1719527 -3.2962976 -3.4646206 -3.5251057 -3.3172066 -2.8314071 -2.3318231 -2.7139778 -3.8616676 -4.6603193 -4.4375257 -3.3107319][-3.0178795 -3.2369783 -3.2141168 -3.1726213 -3.2893651 -3.6194477 -3.9126654 -3.8793421 -3.4646044 -2.8850067 -3.0355115 -3.9111075 -4.4603863 -4.0935855 -3.0321193][-3.5930972 -3.705802 -3.5669193 -3.4360564 -3.5581076 -3.9798191 -4.3927946 -4.4539704 -4.0963097 -3.466851 -3.3621988 -3.9356203 -4.2684445 -3.8260355 -2.8702121][-3.6432757 -3.6911263 -3.5551977 -3.4341564 -3.4883199 -3.8083901 -4.199141 -4.348453 -4.1725693 -3.6702354 -3.4548471 -3.7822862 -3.9649706 -3.5520782 -2.7812724][-3.2931843 -3.3174193 -3.2248189 -3.1626472 -3.203697 -3.4245627 -3.7552147 -3.9551594 -3.9053948 -3.5304735 -3.3078642 -3.4928799 -3.6439166 -3.3845849 -2.8538487][-3.0968094 -3.1271837 -3.0704231 -3.0387022 -3.09505 -3.2674017 -3.5562716 -3.7670975 -3.7444863 -3.4331017 -3.2182562 -3.3102956 -3.44738 -3.3419833 -3.0247302][-3.0890663 -3.121834 -3.1117804 -3.1152387 -3.1702437 -3.2648678 -3.4311564 -3.5494204 -3.4950414 -3.2672653 -3.1406369 -3.2340136 -3.3621774 -3.3426473 -3.1777117]]...]
INFO - root - 2017-12-07 03:43:10.801757: step 1510, loss = 0.86, batch loss = 0.78 (10.5 examples/sec; 0.764 sec/batch; 70h:14m:29s remains)
INFO - root - 2017-12-07 03:43:18.352885: step 1520, loss = 0.70, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 70h:15m:43s remains)
INFO - root - 2017-12-07 03:43:25.800772: step 1530, loss = 0.82, batch loss = 0.74 (14.5 examples/sec; 0.550 sec/batch; 50h:33m:20s remains)
INFO - root - 2017-12-07 03:43:33.424073: step 1540, loss = 0.93, batch loss = 0.86 (10.7 examples/sec; 0.750 sec/batch; 68h:58m:35s remains)
INFO - root - 2017-12-07 03:43:41.192764: step 1550, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.762 sec/batch; 70h:03m:50s remains)
INFO - root - 2017-12-07 03:43:48.947095: step 1560, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.774 sec/batch; 71h:07m:51s remains)
INFO - root - 2017-12-07 03:43:56.585988: step 1570, loss = 0.81, batch loss = 0.73 (10.5 examples/sec; 0.762 sec/batch; 70h:02m:13s remains)
INFO - root - 2017-12-07 03:44:04.332562: step 1580, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 71h:30m:28s remains)
INFO - root - 2017-12-07 03:44:11.983460: step 1590, loss = 1.01, batch loss = 0.94 (10.5 examples/sec; 0.759 sec/batch; 69h:46m:32s remains)
INFO - root - 2017-12-07 03:44:19.728087: step 1600, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.771 sec/batch; 70h:50m:03s remains)
2017-12-07 03:44:20.401210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3110313 -4.3650765 -4.4252415 -4.5312095 -4.6627412 -4.7710137 -4.8291235 -4.8545666 -4.8431153 -4.830389 -4.8306565 -4.8100982 -4.7472072 -4.6106424 -4.4451628][-4.4223785 -4.4898009 -4.5258808 -4.5691037 -4.6397972 -4.6985388 -4.7197518 -4.7181449 -4.6870756 -4.6847897 -4.73845 -4.7955823 -4.8186255 -4.7216258 -4.5512762][-4.3893056 -4.4367518 -4.4811254 -4.574791 -4.753572 -4.9013042 -4.966486 -4.9682775 -4.8981953 -4.8610616 -4.9381604 -5.0638771 -5.1430187 -5.0100837 -4.7266135][-4.0025911 -3.8287425 -3.7080698 -3.7593875 -4.0414052 -4.3280716 -4.5219088 -4.6100364 -4.5873957 -4.6050515 -4.7922616 -5.0511212 -5.2445726 -5.1350346 -4.7874813][-3.5355809 -3.0513546 -2.6719923 -2.6016359 -2.8729918 -3.1384685 -3.2841344 -3.3345566 -3.3574972 -3.543011 -3.9593928 -4.4216642 -4.7316403 -4.636416 -4.2738862][-3.2509513 -2.5195129 -1.8889437 -1.6073849 -1.7074006 -1.8107243 -1.8032317 -1.7516613 -1.828768 -2.2406678 -2.9234381 -3.622597 -4.1006932 -4.0786 -3.7527204][-3.320653 -2.5165443 -1.7140224 -1.1288731 -0.77964687 -0.39176035 0.029514313 0.32162666 0.21145821 -0.42389774 -1.3493671 -2.2643669 -2.9142151 -3.0744345 -2.9647541][-3.6334977 -3.0106213 -2.3775144 -1.8387823 -1.3451071 -0.71199465 -0.011590958 0.5456953 0.56877232 -0.042070866 -0.9596858 -1.8375466 -2.4433575 -2.6403985 -2.6508727][-3.9238873 -3.5123591 -3.0919259 -2.70127 -2.320384 -1.8542259 -1.31145 -0.80328584 -0.64225435 -0.97248197 -1.5755734 -2.1667404 -2.6253395 -2.8535938 -2.9551897][-4.0397058 -3.83605 -3.6270516 -3.3964863 -3.1573706 -2.8695393 -2.5176229 -2.1358948 -1.8814855 -1.9210608 -2.1349406 -2.391556 -2.7158976 -2.9989386 -3.1952305][-3.9041376 -3.8103147 -3.7446628 -3.6830511 -3.6541746 -3.5879369 -3.4353397 -3.191716 -2.912724 -2.7443459 -2.6718926 -2.7492802 -3.0404429 -3.3419759 -3.5405264][-3.5218902 -3.2942705 -3.1269283 -3.0645666 -3.1204588 -3.1958456 -3.1977181 -3.0993369 -2.9090085 -2.7314467 -2.5962996 -2.6789665 -3.0146551 -3.3314843 -3.5386043][-3.3000236 -2.8588157 -2.4783421 -2.2829676 -2.3075392 -2.4360719 -2.5188384 -2.5061846 -2.4387069 -2.3761177 -2.3311863 -2.470818 -2.756639 -2.9536219 -3.0794029][-3.4266124 -2.8928638 -2.3896053 -2.0854864 -2.0467629 -2.1705332 -2.2860007 -2.3179696 -2.3412225 -2.3901453 -2.4426463 -2.586905 -2.7430789 -2.7668853 -2.7598009][-3.6871934 -3.2134752 -2.7471302 -2.4425504 -2.3579304 -2.4374957 -2.5438933 -2.6081719 -2.6798499 -2.766057 -2.8399849 -2.9363747 -2.9850278 -2.9159567 -2.8343375]]...]
INFO - root - 2017-12-07 03:44:28.075551: step 1610, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.769 sec/batch; 70h:41m:57s remains)
INFO - root - 2017-12-07 03:44:35.805688: step 1620, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.772 sec/batch; 70h:57m:36s remains)
INFO - root - 2017-12-07 03:44:43.284905: step 1630, loss = 0.92, batch loss = 0.84 (14.3 examples/sec; 0.561 sec/batch; 51h:32m:01s remains)
INFO - root - 2017-12-07 03:44:51.117371: step 1640, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.758 sec/batch; 69h:37m:13s remains)
INFO - root - 2017-12-07 03:44:58.813325: step 1650, loss = 0.87, batch loss = 0.79 (10.5 examples/sec; 0.765 sec/batch; 70h:17m:53s remains)
INFO - root - 2017-12-07 03:45:06.623256: step 1660, loss = 0.80, batch loss = 0.72 (10.1 examples/sec; 0.792 sec/batch; 72h:46m:31s remains)
INFO - root - 2017-12-07 03:45:14.260572: step 1670, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.775 sec/batch; 71h:13m:11s remains)
INFO - root - 2017-12-07 03:45:22.020241: step 1680, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.765 sec/batch; 70h:17m:19s remains)
INFO - root - 2017-12-07 03:45:29.663758: step 1690, loss = 0.78, batch loss = 0.71 (10.7 examples/sec; 0.749 sec/batch; 68h:51m:43s remains)
INFO - root - 2017-12-07 03:45:37.260341: step 1700, loss = 0.77, batch loss = 0.70 (10.8 examples/sec; 0.738 sec/batch; 67h:49m:16s remains)
2017-12-07 03:45:37.880690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5999424 -2.1266041 -1.508672 -1.525048 -2.0033267 -2.4145133 -2.6245961 -2.7087624 -2.7883348 -2.6274912 -2.4553552 -2.5973558 -2.799634 -2.7499876 -2.3248379][-2.0197663 -1.5159705 -1.0450153 -1.3231218 -2.0998728 -2.8107648 -3.231648 -3.4221771 -3.3932343 -3.1734803 -3.043932 -3.1356306 -3.1941428 -2.9967291 -2.4290078][-1.5061505 -0.86146212 -0.47286153 -0.89925194 -1.8277352 -2.6498606 -3.1265152 -3.3106446 -3.2037263 -3.0346179 -3.0470231 -3.1825104 -3.144258 -2.7766747 -2.0979385][-1.5412967 -0.77240968 -0.40846825 -0.80511761 -1.6717932 -2.4166782 -2.7616169 -2.7574759 -2.4991703 -2.3628767 -2.5358953 -2.8153684 -2.8304715 -2.478672 -1.8857484][-2.0092032 -1.3033307 -0.98198366 -1.2408967 -1.8529646 -2.337502 -2.4040952 -2.0808051 -1.6535828 -1.6172125 -1.9809754 -2.4498739 -2.6294079 -2.479012 -2.1287711][-2.5649538 -2.0522087 -1.8645031 -1.9829392 -2.20859 -2.2396824 -1.8673644 -1.201019 -0.67565918 -0.82573485 -1.4224346 -2.0690897 -2.4197905 -2.5043907 -2.370636][-2.7543206 -2.4115663 -2.3448269 -2.3266244 -2.1447406 -1.6656115 -0.80742216 0.15744305 0.65544319 0.18146658 -0.74643183 -1.605798 -2.1931415 -2.5543203 -2.5696578][-2.8356695 -2.626265 -2.612627 -2.4804678 -2.0335495 -1.2013464 0.063430309 1.3455238 1.8204913 1.0390992 -0.1874094 -1.2335868 -2.0038571 -2.5431542 -2.5716691][-3.1352482 -3.1266236 -3.2052679 -3.0340242 -2.5469694 -1.7439764 -0.54279709 0.65056849 1.116569 0.53565788 -0.46981263 -1.3133519 -1.9833376 -2.5043807 -2.5076928][-3.57507 -3.7246528 -3.9324121 -3.8359575 -3.4785547 -2.9595258 -2.2426443 -1.5624223 -1.2972045 -1.5720091 -2.0736172 -2.4392772 -2.7313433 -3.0243173 -2.9551411][-3.8797574 -3.9939132 -4.1195436 -4.0126863 -3.7920995 -3.5299568 -3.2298813 -2.9987857 -2.9656246 -3.0842414 -3.2543 -3.3094168 -3.3038905 -3.4029346 -3.3032987][-3.6972044 -3.7606537 -3.7936616 -3.7217383 -3.6173363 -3.4573524 -3.2860596 -3.1623533 -3.1469798 -3.1690052 -3.2324183 -3.2580338 -3.1762643 -3.1934505 -3.1041028][-3.2663627 -3.2758245 -3.294456 -3.3644032 -3.4374957 -3.3644264 -3.2242565 -3.0572588 -2.924284 -2.8203726 -2.813457 -2.8551469 -2.762284 -2.7303295 -2.6432986][-2.9894753 -2.8726239 -2.8255856 -2.9267743 -3.0412018 -2.9953022 -2.9020884 -2.7694566 -2.638377 -2.5184746 -2.457628 -2.4579058 -2.3284447 -2.2582731 -2.185658][-2.9983625 -2.849606 -2.7567105 -2.803072 -2.8491821 -2.7940207 -2.7336476 -2.6637893 -2.6102223 -2.5707057 -2.5302758 -2.505312 -2.3739169 -2.2885194 -2.218967]]...]
INFO - root - 2017-12-07 03:45:45.607834: step 1710, loss = 1.13, batch loss = 1.06 (10.4 examples/sec; 0.771 sec/batch; 70h:52m:21s remains)
INFO - root - 2017-12-07 03:45:53.263444: step 1720, loss = 0.64, batch loss = 0.56 (10.1 examples/sec; 0.795 sec/batch; 73h:03m:11s remains)
INFO - root - 2017-12-07 03:46:00.747903: step 1730, loss = 0.80, batch loss = 0.73 (15.3 examples/sec; 0.523 sec/batch; 48h:01m:02s remains)
INFO - root - 2017-12-07 03:46:08.365230: step 1740, loss = 0.88, batch loss = 0.81 (10.2 examples/sec; 0.788 sec/batch; 72h:23m:12s remains)
INFO - root - 2017-12-07 03:46:16.022557: step 1750, loss = 0.99, batch loss = 0.92 (10.5 examples/sec; 0.763 sec/batch; 70h:04m:24s remains)
INFO - root - 2017-12-07 03:46:23.737392: step 1760, loss = 1.01, batch loss = 0.94 (10.4 examples/sec; 0.767 sec/batch; 70h:29m:22s remains)
INFO - root - 2017-12-07 03:46:31.545008: step 1770, loss = 0.91, batch loss = 0.83 (10.2 examples/sec; 0.785 sec/batch; 72h:04m:22s remains)
INFO - root - 2017-12-07 03:46:39.306003: step 1780, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.775 sec/batch; 71h:13m:21s remains)
INFO - root - 2017-12-07 03:46:47.085860: step 1790, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.765 sec/batch; 70h:14m:34s remains)
INFO - root - 2017-12-07 03:46:54.901242: step 1800, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.782 sec/batch; 71h:52m:36s remains)
2017-12-07 03:46:55.444445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1656454 -2.8616457 -2.3499327 -1.6697471 -1.2438219 -1.4758997 -1.7020419 -1.4846613 -1.1249261 -1.3033669 -1.8606369 -1.4698865 -0.96985817 -1.6126692 -2.2559853][-3.1781809 -2.7804008 -2.1319728 -1.5398297 -1.4186838 -1.6340568 -1.6098909 -1.3491819 -1.1670959 -1.6158612 -2.147367 -1.5305924 -1.004427 -1.7032063 -2.4345746][-3.1549773 -2.6848783 -2.003577 -1.6540129 -1.7669969 -1.7876611 -1.4952526 -1.2178292 -1.2407486 -1.9323168 -2.2993693 -1.4008155 -1.0227189 -1.8839276 -2.5245404][-3.1055512 -2.6396904 -2.033565 -1.9350181 -2.0952902 -1.8588235 -1.4374154 -1.1874993 -1.4037449 -2.1532083 -2.0720208 -0.89676046 -0.93282557 -2.0565314 -2.4824383][-3.0702479 -2.7017007 -2.2726469 -2.3692591 -2.4175954 -1.9921546 -1.603862 -1.3774095 -1.7065217 -2.1899004 -1.3733847 -0.17156696 -0.89061 -2.1190407 -2.19505][-3.0877805 -2.8402793 -2.5815883 -2.7518606 -2.599628 -2.1141524 -1.8540745 -1.6660099 -1.9995763 -1.8931849 -0.35797691 0.3230443 -1.0578704 -2.0003972 -1.6755266][-3.1490841 -2.9416943 -2.7176261 -2.8215761 -2.4556623 -2.024348 -1.8378899 -1.6530833 -1.9177883 -1.1743913 0.59338379 0.2859149 -1.3599739 -1.731704 -1.3651183][-3.1598158 -2.886518 -2.5755568 -2.5847678 -2.1311948 -1.7655528 -1.5185442 -1.3369277 -1.559875 -0.46548104 0.79704952 -0.43107581 -1.7114401 -1.4603825 -1.4577498][-3.1200781 -2.7633574 -2.3497431 -2.269845 -1.812741 -1.451658 -1.1326537 -1.0495441 -1.2500119 -0.17487288 0.18914557 -1.3778069 -1.8977468 -1.3806531 -1.9478083][-3.0752757 -2.7051535 -2.2097542 -2.0384309 -1.6438694 -1.2380071 -0.91418576 -0.96964765 -1.0732694 -0.27034473 -0.70320463 -2.0478806 -1.8833733 -1.5790436 -2.574172][-3.0485649 -2.7111917 -2.154429 -1.9278839 -1.6677649 -1.2022212 -0.88659763 -1.0293679 -1.0197835 -0.64780235 -1.5122106 -2.3695471 -1.9819095 -2.2036083 -3.261308][-2.9802155 -2.6802511 -2.1029353 -1.8974669 -1.7754855 -1.1567712 -0.82769608 -0.99998927 -0.99992537 -1.036118 -1.9396303 -2.4026608 -2.2452767 -3.0198209 -3.8013375][-2.8773179 -2.5742936 -2.0325153 -1.9412353 -1.8867731 -1.0715525 -0.7156961 -0.96573853 -1.1198883 -1.4159303 -2.1273446 -2.3448522 -2.5602698 -3.631984 -3.9585178][-2.7573633 -2.4289577 -1.9986892 -2.0709865 -1.9305813 -0.89233184 -0.57493711 -1.0196178 -1.3619208 -1.7536483 -2.3439047 -2.5625606 -3.1035988 -4.2358127 -4.2290125][-2.6540232 -2.3196123 -2.0139935 -2.1312482 -1.6701956 -0.47100854 -0.41511822 -1.1500764 -1.5755472 -2.0006809 -2.6593266 -3.0126185 -3.726912 -4.7380409 -4.5248795]]...]
INFO - root - 2017-12-07 03:47:03.175660: step 1810, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.772 sec/batch; 70h:56m:25s remains)
INFO - root - 2017-12-07 03:47:10.929408: step 1820, loss = 0.90, batch loss = 0.83 (9.9 examples/sec; 0.805 sec/batch; 73h:55m:30s remains)
INFO - root - 2017-12-07 03:47:18.682516: step 1830, loss = 0.63, batch loss = 0.56 (13.4 examples/sec; 0.595 sec/batch; 54h:40m:30s remains)
INFO - root - 2017-12-07 03:47:26.472025: step 1840, loss = 0.69, batch loss = 0.62 (10.6 examples/sec; 0.757 sec/batch; 69h:33m:54s remains)
INFO - root - 2017-12-07 03:47:34.350655: step 1850, loss = 0.75, batch loss = 0.68 (10.2 examples/sec; 0.785 sec/batch; 72h:08m:05s remains)
INFO - root - 2017-12-07 03:47:42.175559: step 1860, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.784 sec/batch; 71h:59m:43s remains)
INFO - root - 2017-12-07 03:47:50.003692: step 1870, loss = 0.91, batch loss = 0.84 (10.2 examples/sec; 0.785 sec/batch; 72h:08m:07s remains)
INFO - root - 2017-12-07 03:47:57.661544: step 1880, loss = 0.75, batch loss = 0.68 (10.6 examples/sec; 0.755 sec/batch; 69h:18m:22s remains)
INFO - root - 2017-12-07 03:48:05.355685: step 1890, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.772 sec/batch; 70h:53m:04s remains)
INFO - root - 2017-12-07 03:48:12.943127: step 1900, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 69h:20m:17s remains)
2017-12-07 03:48:13.521109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.54056931 0.26259375 -0.055876732 -0.97324586 -1.6530917 -2.2090685 -2.6408262 -2.867558 -2.9745345 -2.6441169 -1.9376819 -1.3184347 -1.2888575 -1.9387674 -2.4616075][0.54639435 1.3457627 0.93001938 -0.090156555 -0.82073331 -1.4396536 -1.9154685 -2.2235355 -2.4276485 -2.1863611 -1.5008333 -0.91763496 -0.95255995 -1.6422052 -2.1167245][1.4267936 2.1053476 1.5911531 0.59097147 -0.05967617 -0.67560863 -1.199569 -1.5857773 -1.9192104 -1.8464158 -1.2990916 -0.82342315 -0.89288545 -1.5267363 -1.9474027][2.1843829 2.651082 1.9707861 1.0321317 0.58136177 0.1663866 -0.28119087 -0.71587873 -1.2181537 -1.3363612 -0.934849 -0.50648546 -0.4878068 -0.97956491 -1.3229439][2.1573591 2.3881612 1.6305666 0.81651926 0.59314251 0.50969362 0.23180151 -0.27584839 -0.98009443 -1.2813718 -0.971761 -0.50706005 -0.33848286 -0.62081337 -0.82311583][1.081996 1.1173983 0.45180798 -0.12987709 -0.15088415 0.053730011 -0.015613556 -0.47863054 -1.2700713 -1.6945398 -1.4478703 -0.90305448 -0.51982641 -0.53756404 -0.54768968][-0.23650169 -0.31804991 -0.76799917 -1.0836949 -0.98428249 -0.63407159 -0.4945364 -0.75814295 -1.4905429 -2.012558 -1.9216027 -1.4109817 -0.88488936 -0.73718405 -0.686069][-1.0287054 -1.0784552 -1.2287395 -1.3328011 -1.2646074 -0.98082852 -0.79845524 -0.91219306 -1.5258331 -2.1157062 -2.2488627 -1.925637 -1.4756114 -1.31987 -1.2285295][-1.2004924 -1.1193235 -0.98624277 -0.96996307 -1.0632687 -1.0348728 -1.0236971 -1.1290038 -1.5977452 -2.1319559 -2.3929677 -2.2798522 -2.0197477 -1.9225554 -1.8115444][-1.2122781 -1.0756648 -0.82708478 -0.74936438 -0.91315174 -1.0474944 -1.1444671 -1.2201254 -1.4829395 -1.8527946 -2.1584873 -2.2484093 -2.2239943 -2.2513945 -2.2187054][-1.2749825 -1.1365943 -0.89735174 -0.8394897 -1.005481 -1.1553395 -1.2376661 -1.2489464 -1.3096979 -1.4792273 -1.7095885 -1.8425999 -1.893352 -1.9653146 -2.0347576][-1.5277836 -1.4376278 -1.3040881 -1.3187375 -1.467906 -1.5607178 -1.5556302 -1.4953904 -1.4309685 -1.4328253 -1.5340464 -1.6093352 -1.6482472 -1.704097 -1.789696][-2.1122656 -2.0733478 -2.0150607 -2.0564179 -2.1542675 -2.1754611 -2.1114128 -2.0526786 -2.0188851 -2.0307832 -2.1181555 -2.2019489 -2.2453337 -2.2533271 -2.1983488][-2.6831684 -2.6440306 -2.6047029 -2.6297166 -2.6782138 -2.6818991 -2.6408868 -2.6403615 -2.685926 -2.7388892 -2.8137431 -2.8948257 -2.9546278 -2.9385667 -2.7894237][-3.1386669 -3.0900798 -3.0617905 -3.0760825 -3.0916247 -3.076761 -3.0438228 -3.0560479 -3.1111062 -3.1556139 -3.1905341 -3.2274375 -3.2558112 -3.230401 -3.1037641]]...]
INFO - root - 2017-12-07 03:48:21.243603: step 1910, loss = 0.61, batch loss = 0.54 (10.6 examples/sec; 0.757 sec/batch; 69h:30m:49s remains)
INFO - root - 2017-12-07 03:48:29.061993: step 1920, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.768 sec/batch; 70h:30m:07s remains)
INFO - root - 2017-12-07 03:48:36.699843: step 1930, loss = 1.03, batch loss = 0.95 (13.3 examples/sec; 0.601 sec/batch; 55h:12m:58s remains)
INFO - root - 2017-12-07 03:48:44.410547: step 1940, loss = 0.63, batch loss = 0.56 (10.6 examples/sec; 0.754 sec/batch; 69h:14m:17s remains)
INFO - root - 2017-12-07 03:48:52.083875: step 1950, loss = 0.95, batch loss = 0.88 (10.2 examples/sec; 0.782 sec/batch; 71h:50m:22s remains)
INFO - root - 2017-12-07 03:48:59.661532: step 1960, loss = 0.95, batch loss = 0.88 (10.8 examples/sec; 0.743 sec/batch; 68h:15m:33s remains)
INFO - root - 2017-12-07 03:49:07.287578: step 1970, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.769 sec/batch; 70h:38m:47s remains)
INFO - root - 2017-12-07 03:49:14.943368: step 1980, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.769 sec/batch; 70h:38m:34s remains)
INFO - root - 2017-12-07 03:49:22.604170: step 1990, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.774 sec/batch; 71h:01m:19s remains)
INFO - root - 2017-12-07 03:49:30.275692: step 2000, loss = 0.77, batch loss = 0.70 (10.8 examples/sec; 0.742 sec/batch; 68h:08m:49s remains)
2017-12-07 03:49:30.861952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.55779529 -1.3166878 -1.4793272 -1.3995159 -1.2421329 -1.0330608 -1.1169882 -1.4983275 -1.9298999 -2.2898571 -2.5614192 -2.5332484 -2.2350307 -1.6930988 -1.0892994][-0.64465404 -1.3856483 -1.5609407 -1.491214 -1.328438 -1.0323706 -1.0882897 -1.5242469 -2.0520139 -2.4122469 -2.4889064 -2.2312195 -1.7869861 -1.2434061 -0.67443347][-0.84516573 -1.4003742 -1.499958 -1.4116793 -1.2620182 -0.92595911 -0.96652126 -1.4283316 -2.0468409 -2.425786 -2.3675768 -1.952765 -1.4031129 -0.9141922 -0.50198579][-0.88178205 -1.1722891 -1.1700766 -1.0786803 -0.9998672 -0.68341613 -0.66261339 -1.0567617 -1.655231 -2.0117886 -1.8998244 -1.4596522 -0.9325583 -0.561959 -0.42669582][-0.9544704 -0.98969364 -0.85818458 -0.69944763 -0.64360666 -0.3293848 -0.18077755 -0.41445065 -0.9257381 -1.2683992 -1.2018225 -0.85957146 -0.43947983 -0.17973995 -0.29745722][-1.1487646 -1.0426583 -0.81913567 -0.56736827 -0.4661963 -0.12117195 0.1899395 0.098932743 -0.39295673 -0.80320716 -0.83598232 -0.59443712 -0.25395203 -0.0079250336 -0.17232847][-1.2512817 -1.0297143 -0.7556088 -0.41780853 -0.20552444 0.19856024 0.69001675 0.74067593 0.25331306 -0.24844265 -0.38539076 -0.23591375 0.013834953 0.23774767 0.080599785][-1.2985432 -0.96015835 -0.58274865 -0.17726851 0.088239193 0.46313286 1.0188713 1.1520643 0.65619135 0.071252346 -0.15522337 -0.074908257 0.12144947 0.31589794 0.13374805][-1.4183412 -1.0983412 -0.68934393 -0.30955124 -0.10476398 0.12197733 0.62555456 0.77666473 0.32364845 -0.2396059 -0.47650385 -0.4252224 -0.25268984 -0.053230286 -0.2395072][-1.5626397 -1.3437757 -1.0141613 -0.76325321 -0.65632772 -0.51097178 -0.00996685 0.21593809 -0.091709614 -0.4859879 -0.59597373 -0.512619 -0.35025406 -0.15235472 -0.35025167][-1.5072415 -1.258666 -0.99372721 -0.92194915 -0.9401536 -0.84939528 -0.38238907 -0.11919451 -0.28450298 -0.45201826 -0.38290215 -0.25305605 -0.14112854 0.032853127 -0.17995691][-1.4250731 -1.1205606 -0.88809776 -0.9790411 -1.1056755 -1.0596578 -0.66781259 -0.37928867 -0.38331079 -0.3276515 -0.046080112 0.11675739 0.056039333 0.1099143 -0.12146807][-1.4353738 -1.2442994 -1.1444097 -1.391448 -1.5712078 -1.4881911 -1.1141653 -0.75250769 -0.56047082 -0.34344053 0.11599064 0.32538557 0.10459375 0.0440526 -0.15317965][-1.5052328 -1.4754972 -1.5391431 -1.8650436 -1.9998381 -1.7777498 -1.3556561 -0.97049856 -0.66710806 -0.432101 0.033139706 0.25211334 -0.10001564 -0.28563118 -0.50411153][-1.8727505 -1.9736645 -2.1013782 -2.3534138 -2.3441682 -1.9629734 -1.5443001 -1.2415915 -0.91626167 -0.64972019 -0.14704418 0.14972067 -0.2289505 -0.47007823 -0.67753386]]...]
INFO - root - 2017-12-07 03:49:38.434015: step 2010, loss = 0.87, batch loss = 0.79 (10.9 examples/sec; 0.736 sec/batch; 67h:31m:18s remains)
INFO - root - 2017-12-07 03:49:46.068474: step 2020, loss = 0.88, batch loss = 0.80 (10.3 examples/sec; 0.778 sec/batch; 71h:23m:35s remains)
INFO - root - 2017-12-07 03:49:53.530537: step 2030, loss = 0.67, batch loss = 0.60 (14.9 examples/sec; 0.538 sec/batch; 49h:22m:31s remains)
INFO - root - 2017-12-07 03:50:01.259794: step 2040, loss = 0.81, batch loss = 0.74 (10.1 examples/sec; 0.794 sec/batch; 72h:53m:47s remains)
INFO - root - 2017-12-07 03:50:08.948021: step 2050, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 69h:03m:23s remains)
INFO - root - 2017-12-07 03:50:16.508937: step 2060, loss = 0.77, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 70h:14m:22s remains)
INFO - root - 2017-12-07 03:50:24.167428: step 2070, loss = 0.68, batch loss = 0.61 (10.4 examples/sec; 0.766 sec/batch; 70h:20m:46s remains)
INFO - root - 2017-12-07 03:50:31.851148: step 2080, loss = 0.85, batch loss = 0.77 (10.9 examples/sec; 0.736 sec/batch; 67h:33m:23s remains)
INFO - root - 2017-12-07 03:50:39.555579: step 2090, loss = 0.66, batch loss = 0.59 (10.5 examples/sec; 0.761 sec/batch; 69h:47m:58s remains)
INFO - root - 2017-12-07 03:50:47.189875: step 2100, loss = 0.95, batch loss = 0.88 (10.3 examples/sec; 0.775 sec/batch; 71h:05m:21s remains)
2017-12-07 03:50:47.773726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.28226757 -0.43002248 -0.81310868 -1.4483838 -2.3480217 -3.0937493 -3.4984934 -3.597774 -3.560246 -3.6747046 -3.9607348 -4.2762127 -4.5387683 -4.5353527 -4.2333632][-0.16839409 -0.5098083 -1.1795363 -1.9138801 -2.5959418 -2.8960485 -2.9361033 -2.9514399 -3.0932827 -3.5849903 -4.1850858 -4.5904255 -4.7099876 -4.45879 -4.07466][-0.32385349 -0.63638067 -1.316972 -1.9616704 -2.3086629 -2.1655555 -1.9334347 -1.9719393 -2.3998935 -3.330709 -4.2659416 -4.763875 -4.6932359 -4.1922774 -3.7370381][-0.72465849 -0.91096187 -1.4332805 -1.9186254 -1.9760821 -1.4984558 -1.0284023 -1.0357482 -1.6916833 -3.0710845 -4.4415402 -5.1037164 -4.8086033 -3.9339137 -3.3154564][-1.192344 -1.2563193 -1.4968221 -1.6931303 -1.4098151 -0.4541862 0.5846591 0.93594313 0.079904556 -2.0171325 -4.1956758 -5.3749142 -5.0896506 -3.9090946 -3.0414104][-1.8566077 -1.8815892 -1.9289017 -1.8755412 -1.2255638 0.45065308 2.52879 3.7074394 2.896162 0.084284782 -3.0977561 -5.0960045 -5.19622 -4.1119719 -3.2431703][-2.3660336 -2.4118335 -2.5805533 -2.6773655 -1.9954519 0.16412687 3.1625648 5.2763166 4.8524904 1.8764405 -1.7950587 -4.3047614 -4.7778559 -3.9928148 -3.3407853][-2.5155721 -2.4933765 -2.7253916 -2.9823956 -2.4014404 -0.18396425 3.0173473 5.4354277 5.2814713 2.5813441 -0.98032594 -3.5970438 -4.2527528 -3.681036 -3.1699696][-2.5781717 -2.4952395 -2.6871986 -2.8371038 -2.2050598 -0.17963219 2.7204523 4.9662189 4.9932232 2.698565 -0.5946095 -3.2108967 -4.0217705 -3.6027713 -3.1397533][-2.642235 -2.5864964 -2.69493 -2.6403332 -1.9900007 -0.32626104 2.0735025 4.0243673 4.2892714 2.5585089 -0.27204084 -2.7229698 -3.665489 -3.4751256 -3.1381707][-2.8483453 -2.6494403 -2.4544673 -2.13398 -1.579073 -0.48057723 1.2524414 2.7797384 3.1871972 1.9521866 -0.29497194 -2.318543 -3.2401042 -3.2601209 -3.0384898][-3.2751715 -2.9187131 -2.4443984 -1.9768419 -1.5801249 -0.90097427 0.34172344 1.5183301 2.0355582 1.2549601 -0.36352348 -1.8977146 -2.7731285 -2.9411774 -2.7440119][-3.4617524 -3.1488805 -2.6393209 -2.1591623 -1.8484821 -1.4401984 -0.7038126 0.016002178 0.56005573 0.35376596 -0.43462992 -1.3392425 -2.0700328 -2.3120739 -2.2182121][-3.5696647 -3.3859346 -2.9131496 -2.3443758 -1.9130828 -1.559154 -1.2627342 -1.1067495 -0.81922674 -0.71442604 -0.86383271 -1.1429064 -1.4666858 -1.4714129 -1.3669744][-3.6783636 -3.6136019 -3.2966642 -2.7404037 -2.1836431 -1.7550118 -1.5766892 -1.6320133 -1.5521958 -1.4329131 -1.3736098 -1.3594873 -1.3351994 -1.0305407 -0.74043918]]...]
INFO - root - 2017-12-07 03:50:55.388059: step 2110, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.778 sec/batch; 71h:25m:04s remains)
INFO - root - 2017-12-07 03:51:03.050014: step 2120, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.754 sec/batch; 69h:11m:57s remains)
INFO - root - 2017-12-07 03:51:10.450554: step 2130, loss = 0.59, batch loss = 0.52 (15.3 examples/sec; 0.524 sec/batch; 48h:07m:18s remains)
INFO - root - 2017-12-07 03:51:18.104683: step 2140, loss = 0.80, batch loss = 0.73 (10.8 examples/sec; 0.742 sec/batch; 68h:05m:40s remains)
INFO - root - 2017-12-07 03:51:25.901426: step 2150, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 71h:22m:29s remains)
INFO - root - 2017-12-07 03:51:33.642877: step 2160, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.766 sec/batch; 70h:16m:16s remains)
INFO - root - 2017-12-07 03:51:41.354739: step 2170, loss = 0.81, batch loss = 0.73 (10.3 examples/sec; 0.776 sec/batch; 71h:12m:15s remains)
INFO - root - 2017-12-07 03:51:48.995588: step 2180, loss = 0.81, batch loss = 0.74 (10.3 examples/sec; 0.779 sec/batch; 71h:28m:37s remains)
INFO - root - 2017-12-07 03:51:56.720112: step 2190, loss = 0.83, batch loss = 0.76 (10.7 examples/sec; 0.750 sec/batch; 68h:47m:01s remains)
INFO - root - 2017-12-07 03:52:04.391246: step 2200, loss = 0.59, batch loss = 0.52 (10.5 examples/sec; 0.763 sec/batch; 69h:58m:22s remains)
2017-12-07 03:52:05.031307: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21605253 -0.22084236 -1.1003058 -1.7654085 -1.8144915 -1.4559987 -1.1612546 -1.2011702 -1.4581132 -1.7231498 -1.9096906 -1.9514675 -1.8870695 -1.8074362 -1.7846291][0.54571629 -0.0069084167 -0.94992661 -1.5481844 -1.4835272 -0.97710657 -0.5835495 -0.67091608 -1.1224575 -1.6089563 -1.9126716 -1.9591 -1.8518534 -1.732708 -1.7098997][1.0828571 0.42967033 -0.62244344 -1.225493 -1.0940676 -0.50685167 -0.10330868 -0.27078295 -0.924171 -1.6207507 -1.9845574 -1.9695466 -1.7838335 -1.6288676 -1.617049][1.4789076 0.83058786 -0.23628712 -0.78677583 -0.54658461 0.065074921 0.4045 0.11683369 -0.76683521 -1.6962912 -2.1392479 -2.0611446 -1.8088796 -1.6418238 -1.6166761][1.3116221 0.7504468 -0.21037388 -0.64015007 -0.25453424 0.44551229 0.81980324 0.5165391 -0.47228503 -1.5356276 -2.0621386 -2.0022788 -1.7602482 -1.649493 -1.6381736][1.2214398 0.70972157 -0.14320803 -0.54277396 -0.13540554 0.64612436 1.1008368 0.79352236 -0.222085 -1.2573493 -1.7733171 -1.750638 -1.5418723 -1.5102835 -1.5646825][1.12042 0.578104 -0.14073944 -0.39533138 0.13418722 1.0624571 1.6307292 1.2529454 0.11453581 -0.94689751 -1.5064564 -1.5719831 -1.4159577 -1.404902 -1.479363][0.96515703 0.27250147 -0.41318178 -0.47081923 0.22206068 1.2821093 2.007247 1.6783962 0.54214096 -0.51287413 -1.1155066 -1.3019001 -1.2449026 -1.2574227 -1.3616667][0.9401741 0.081104755 -0.65670156 -0.63204074 0.0918808 1.1002436 1.8292556 1.6041226 0.61843443 -0.35758448 -0.9302814 -1.146744 -1.1169164 -1.0998905 -1.2302663][0.9653883 -0.0072832108 -0.75905466 -0.70718336 0.013130665 0.963779 1.613493 1.4178896 0.47823524 -0.5525291 -1.1561594 -1.3022752 -1.1584485 -1.0202372 -1.119441][1.1924324 0.21117163 -0.57931161 -0.63861442 -0.03306675 0.83375072 1.362956 1.1515489 0.25321579 -0.8001399 -1.3976169 -1.4105351 -1.119612 -0.91999269 -1.0350554][1.5417371 0.63368082 -0.20138836 -0.38900852 0.092794895 0.82112312 1.1370597 0.79025984 -0.12385464 -1.1346223 -1.6644957 -1.5672116 -1.2071183 -1.0229867 -1.1394005][1.5628195 0.77605915 -0.043189526 -0.25067949 0.24833775 0.93832684 1.1058106 0.63277054 -0.33101749 -1.2776997 -1.75828 -1.6708667 -1.3812892 -1.2665677 -1.3469129][1.401566 0.74093819 -0.034667015 -0.26326418 0.24473238 0.94737148 1.1149893 0.61838388 -0.34773636 -1.1931038 -1.5819964 -1.4973958 -1.2606437 -1.2218943 -1.3490818][1.2833543 0.72927523 0.067650318 -0.1079073 0.43392467 1.2068191 1.4675207 0.91147137 -0.17809772 -1.0522275 -1.4349737 -1.3421667 -1.0616896 -0.99062514 -1.1684933]]...]
INFO - root - 2017-12-07 03:52:12.793799: step 2210, loss = 0.94, batch loss = 0.87 (10.2 examples/sec; 0.785 sec/batch; 72h:00m:37s remains)
INFO - root - 2017-12-07 03:52:20.524076: step 2220, loss = 0.79, batch loss = 0.71 (10.3 examples/sec; 0.774 sec/batch; 70h:58m:52s remains)
INFO - root - 2017-12-07 03:52:28.004635: step 2230, loss = 0.95, batch loss = 0.88 (14.3 examples/sec; 0.559 sec/batch; 51h:14m:17s remains)
INFO - root - 2017-12-07 03:52:35.664521: step 2240, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.778 sec/batch; 71h:21m:30s remains)
INFO - root - 2017-12-07 03:52:43.386318: step 2250, loss = 0.97, batch loss = 0.89 (10.8 examples/sec; 0.742 sec/batch; 68h:02m:56s remains)
INFO - root - 2017-12-07 03:52:51.057276: step 2260, loss = 0.63, batch loss = 0.56 (10.3 examples/sec; 0.780 sec/batch; 71h:31m:10s remains)
INFO - root - 2017-12-07 03:52:58.836128: step 2270, loss = 0.90, batch loss = 0.83 (10.0 examples/sec; 0.802 sec/batch; 73h:35m:57s remains)
INFO - root - 2017-12-07 03:53:06.595434: step 2280, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.758 sec/batch; 69h:30m:49s remains)
INFO - root - 2017-12-07 03:53:14.354064: step 2290, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.748 sec/batch; 68h:34m:41s remains)
INFO - root - 2017-12-07 03:53:21.976619: step 2300, loss = 0.81, batch loss = 0.73 (10.2 examples/sec; 0.783 sec/batch; 71h:48m:41s remains)
2017-12-07 03:53:22.545940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9012437 -1.612139 -1.3589666 -1.3479025 -1.3333452 -1.3183167 -1.3648741 -1.2982476 -1.1588039 -1.0590558 -0.91324425 -0.9149437 -0.97758031 -1.242784 -1.7342329][-1.6047289 -1.0603678 -0.5557971 -0.49128366 -0.58491564 -0.61890531 -0.59038782 -0.41095877 -0.20049143 -0.089683533 -0.067015648 -0.47459269 -1.0591736 -1.6605182 -2.1741703][-1.3617775 -0.6054368 0.10406446 0.17982101 -0.020189285 -0.11070395 -0.010513783 0.26618338 0.52159739 0.64596272 0.49205875 -0.2549839 -1.2066271 -2.0025473 -2.4811215][-1.2629347 -0.52408719 0.20080566 0.29776144 0.1220994 0.099387646 0.27722836 0.57664204 0.73006439 0.6980648 0.29948997 -0.68029594 -1.7471898 -2.4603865 -2.798985][-1.3693607 -0.82572436 -0.18640041 -0.017732143 -0.020341873 0.18255281 0.50518036 0.9093008 0.90583611 0.5114336 -0.24586153 -1.3684084 -2.2908204 -2.7101572 -2.8541412][-1.5459383 -1.239192 -0.689908 -0.38580751 -0.13818455 0.38173389 1.0380378 1.7847009 1.7116504 0.793777 -0.47015452 -1.7356546 -2.4689832 -2.6709423 -2.7512574][-1.5817995 -1.3732479 -0.80879235 -0.37140036 0.010157108 0.72731352 1.6383662 2.6237478 2.4167342 0.98318815 -0.7028594 -1.9754446 -2.5127857 -2.5834098 -2.6567917][-1.3925078 -1.1422997 -0.58703685 -0.16667509 0.21164179 0.91058779 1.6950827 2.4533544 2.0795708 0.47914934 -1.1988766 -2.2062571 -2.4654109 -2.4491925 -2.5423052][-0.98472357 -0.69252062 -0.32198381 -0.12512779 0.088698387 0.56481266 1.0090637 1.3734474 0.89104557 -0.48345757 -1.8026721 -2.4403257 -2.5035293 -2.46701 -2.5774577][-0.71927547 -0.44531679 -0.31472492 -0.39302206 -0.3713665 -0.134089 0.086128235 0.1870122 -0.33070707 -1.3401477 -2.2289827 -2.6413064 -2.6971903 -2.7009873 -2.783608][-0.75035572 -0.68700886 -0.76653838 -0.93464971 -0.91831946 -0.7496562 -0.60493517 -0.68263292 -1.2229347 -1.9174578 -2.4561372 -2.7621727 -2.8765013 -2.9149756 -2.9455051][-1.184027 -1.3613558 -1.4857461 -1.4992943 -1.3167279 -1.0893595 -0.9937892 -1.2260406 -1.7710423 -2.2546878 -2.5763714 -2.8187726 -2.956012 -2.9921341 -2.981817][-1.7402561 -1.9316142 -1.9287882 -1.7660041 -1.5241492 -1.3955605 -1.4817095 -1.8262973 -2.2904246 -2.6171544 -2.7860234 -2.9228911 -2.9857507 -2.9823236 -2.9410188][-1.8219676 -2.0059681 -1.9656398 -1.793118 -1.6699436 -1.7675979 -2.0537512 -2.4328241 -2.791039 -3.0000858 -3.0413594 -3.0405402 -3.0079019 -2.9605811 -2.9047098][-1.6222682 -1.8904562 -1.9752512 -1.9483685 -1.9761274 -2.2040646 -2.5433502 -2.8708267 -3.1112151 -3.2045603 -3.157012 -3.0753465 -2.9904659 -2.9212871 -2.8711705]]...]
INFO - root - 2017-12-07 03:53:30.120285: step 2310, loss = 0.94, batch loss = 0.86 (10.3 examples/sec; 0.777 sec/batch; 71h:18m:18s remains)
INFO - root - 2017-12-07 03:53:37.911948: step 2320, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.770 sec/batch; 70h:36m:06s remains)
INFO - root - 2017-12-07 03:53:45.463633: step 2330, loss = 1.12, batch loss = 1.05 (12.5 examples/sec; 0.640 sec/batch; 58h:40m:17s remains)
INFO - root - 2017-12-07 03:53:53.092036: step 2340, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.771 sec/batch; 70h:42m:51s remains)
INFO - root - 2017-12-07 03:54:00.713744: step 2350, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.777 sec/batch; 71h:13m:56s remains)
INFO - root - 2017-12-07 03:54:08.380577: step 2360, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.774 sec/batch; 71h:00m:31s remains)
INFO - root - 2017-12-07 03:54:15.987117: step 2370, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 69h:48m:13s remains)
INFO - root - 2017-12-07 03:54:23.566683: step 2380, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.766 sec/batch; 70h:14m:52s remains)
INFO - root - 2017-12-07 03:54:31.180788: step 2390, loss = 0.67, batch loss = 0.60 (10.1 examples/sec; 0.788 sec/batch; 72h:16m:51s remains)
INFO - root - 2017-12-07 03:54:38.816734: step 2400, loss = 0.69, batch loss = 0.62 (10.6 examples/sec; 0.755 sec/batch; 69h:11m:23s remains)
2017-12-07 03:54:39.455411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9811485 -2.93591 -2.9328673 -2.8983603 -2.7898841 -2.607564 -2.5225809 -2.5365586 -2.606082 -2.8311644 -3.0684819 -3.3617172 -3.6404567 -3.7895443 -4.1572046][-2.9339442 -2.8492765 -2.7614026 -2.7111073 -2.6359801 -2.5162172 -2.5262246 -2.6086674 -2.6896722 -2.9140034 -3.1065907 -3.3626909 -3.6822062 -3.9028296 -4.3321905][-3.1075242 -3.0462577 -2.9550741 -2.9693532 -2.9952502 -2.9752145 -3.0363336 -3.0687013 -3.071568 -3.1999564 -3.2851796 -3.4439249 -3.7175674 -3.9279304 -4.3000965][-3.2772503 -3.2619119 -3.247431 -3.3967452 -3.5586557 -3.6198318 -3.6709282 -3.5961559 -3.5097086 -3.541265 -3.5309463 -3.5909393 -3.7669888 -3.9103961 -4.16815][-3.3400679 -3.3212755 -3.3597922 -3.5656765 -3.7203803 -3.7259002 -3.6555307 -3.4654593 -3.3578377 -3.4231381 -3.4481673 -3.5296967 -3.7056088 -3.8296971 -4.0283689][-3.2892976 -3.2275286 -3.2675612 -3.4281278 -3.428288 -3.2264149 -2.9142809 -2.535063 -2.3911343 -2.598443 -2.8212748 -3.0784886 -3.4000797 -3.6058874 -3.8287213][-3.2792993 -3.1324263 -3.1341462 -3.1829281 -2.9715385 -2.550781 -2.0520246 -1.5338106 -1.3644178 -1.7285802 -2.1713665 -2.6213741 -3.1078093 -3.4272671 -3.6888838][-3.3745365 -3.1965096 -3.1579757 -3.0662003 -2.6624749 -2.1242526 -1.6193488 -1.1511493 -1.0560882 -1.5192676 -2.0412621 -2.5236893 -3.0593481 -3.4499817 -3.7038651][-3.4403858 -3.3505311 -3.3304563 -3.1690784 -2.7240329 -2.2823398 -1.9838722 -1.7537167 -1.8089745 -2.2344398 -2.6345458 -2.9519856 -3.3610792 -3.7019682 -3.8521671][-3.5460296 -3.5708389 -3.6140146 -3.4792914 -3.1408424 -2.8881896 -2.7896242 -2.7242546 -2.8296013 -3.1242037 -3.331795 -3.4559102 -3.7261329 -3.9913452 -4.0516052][-3.645988 -3.7242515 -3.8000879 -3.7149823 -3.5195036 -3.4269209 -3.4300373 -3.3863482 -3.4092851 -3.5332158 -3.5625036 -3.5727162 -3.7976165 -4.0388184 -4.0898747][-3.6709871 -3.6982186 -3.73127 -3.6808257 -3.6550984 -3.7115741 -3.7590015 -3.6698804 -3.5502417 -3.4723189 -3.31593 -3.2280929 -3.4513283 -3.7274036 -3.833425][-3.6853418 -3.6272635 -3.5795786 -3.5744545 -3.7133303 -3.9004188 -3.9914875 -3.8759897 -3.620209 -3.3268352 -2.9346924 -2.6765633 -2.8480167 -3.1628947 -3.3624966][-3.6004107 -3.468806 -3.3438065 -3.3616333 -3.5939405 -3.8512685 -3.9744916 -3.8887579 -3.5947673 -3.2154169 -2.7173061 -2.3508773 -2.4591546 -2.7799654 -3.0675101][-3.3760607 -3.1786249 -2.9959579 -3.0144901 -3.2540376 -3.4927356 -3.6260822 -3.6091318 -3.3661418 -3.0540257 -2.6422219 -2.3278565 -2.3963299 -2.67699 -3.0214825]]...]
INFO - root - 2017-12-07 03:54:47.020765: step 2410, loss = 0.86, batch loss = 0.79 (10.7 examples/sec; 0.751 sec/batch; 68h:50m:20s remains)
INFO - root - 2017-12-07 03:54:54.588124: step 2420, loss = 0.88, batch loss = 0.81 (11.2 examples/sec; 0.716 sec/batch; 65h:39m:39s remains)
INFO - root - 2017-12-07 03:55:02.059858: step 2430, loss = 0.70, batch loss = 0.63 (13.3 examples/sec; 0.601 sec/batch; 55h:03m:51s remains)
INFO - root - 2017-12-07 03:55:09.843739: step 2440, loss = 0.69, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 72h:41m:32s remains)
INFO - root - 2017-12-07 03:55:17.426882: step 2450, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 69h:31m:09s remains)
INFO - root - 2017-12-07 03:55:25.164141: step 2460, loss = 0.64, batch loss = 0.57 (10.1 examples/sec; 0.795 sec/batch; 72h:53m:42s remains)
INFO - root - 2017-12-07 03:55:32.800364: step 2470, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.806 sec/batch; 73h:55m:50s remains)
INFO - root - 2017-12-07 03:55:40.477876: step 2480, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.762 sec/batch; 69h:52m:30s remains)
INFO - root - 2017-12-07 03:55:48.219130: step 2490, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.756 sec/batch; 69h:18m:35s remains)
INFO - root - 2017-12-07 03:55:55.975562: step 2500, loss = 0.72, batch loss = 0.65 (10.7 examples/sec; 0.746 sec/batch; 68h:23m:30s remains)
2017-12-07 03:55:56.579008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9806609 -2.5413976 -2.1450989 -1.7289422 -1.3843176 -1.2273467 -1.319561 -1.2263792 -1.178961 -1.3131359 -1.5265799 -1.8559153 -1.8271585 -1.5370176 -1.1658659][-1.3608282 -1.9297767 -1.8772132 -1.9466381 -1.8395581 -1.5748403 -1.2991667 -0.88548517 -0.731426 -0.94437623 -1.3906431 -1.8798966 -1.7978818 -1.3255272 -0.69365978][-0.70912051 -1.1917899 -1.4785681 -1.9469566 -2.0085366 -1.6721427 -1.0715168 -0.45149374 -0.34411049 -0.72502422 -1.3785579 -1.875989 -1.7218471 -1.1583562 -0.49669003][-0.230999 -0.48822737 -0.86608315 -1.4389522 -1.6033332 -1.2958012 -0.53189588 0.131948 0.069731236 -0.46155286 -1.0999405 -1.3827786 -1.1915369 -0.75758362 -0.35030556][0.16250324 0.0334754 -0.40468502 -0.95412993 -1.2044792 -0.95075059 -0.054198742 0.72430849 0.64875793 0.12689877 -0.37284803 -0.47916484 -0.40314293 -0.31738091 -0.34286118][0.19668674 0.03917408 -0.47979617 -0.93716717 -1.1597614 -0.83858252 0.28654385 1.1770163 1.1416521 0.63695669 0.28778219 0.30918932 0.2440486 -0.02982235 -0.49471688][0.14695358 -0.11069632 -0.5175097 -0.76772094 -0.91120005 -0.44433904 0.80771923 1.6070223 1.3619266 0.74866152 0.49183369 0.57309055 0.36326694 -0.16667604 -0.803463][0.084070206 -0.0703578 -0.22405958 -0.3248477 -0.46209145 0.010832787 1.2018728 1.7050357 1.1422935 0.48990297 0.29790163 0.3678503 0.032643318 -0.63167214 -1.2120817][-0.302598 -0.2509656 -0.14971781 -0.25547743 -0.5138638 -0.081285477 0.91243172 1.0515938 0.31591749 -0.28953934 -0.3553648 -0.23345423 -0.57347441 -1.2508118 -1.6397681][-0.707036 -0.53616548 -0.43041134 -0.74813938 -1.1026409 -0.508466 0.43598413 0.3597517 -0.43629622 -0.96964383 -0.90971088 -0.74854755 -1.0740435 -1.6896803 -1.9357221][-0.57487082 -0.58905339 -0.85240555 -1.4335544 -1.6833251 -0.78778887 0.20033216 0.069372177 -0.68953681 -1.1055963 -0.96935511 -0.87874389 -1.2047873 -1.7315536 -1.8835027][-0.6264565 -1.0365674 -1.7355149 -2.3938844 -2.2428274 -1.0522351 -0.18032837 -0.3911376 -0.99165988 -1.1690233 -0.98669934 -1.0810106 -1.3756921 -1.6307962 -1.6040711][-1.1967738 -1.8234742 -2.6270413 -3.0346646 -2.4757795 -1.3497806 -0.93274 -1.3343503 -1.6771567 -1.3599761 -1.0492554 -1.3307297 -1.5404403 -1.4977446 -1.2521012][-1.4615145 -2.1251104 -2.885376 -3.0192757 -2.2718461 -1.4986906 -1.5381916 -2.008852 -1.928005 -0.91933155 -0.44270372 -1.0000994 -1.3153727 -1.2443812 -0.99566221][-1.0752792 -1.811877 -2.6459255 -2.6810112 -1.902204 -1.5680161 -1.9441969 -2.3309808 -1.7517388 -0.063714027 0.51325464 -0.47968531 -1.1402287 -1.2279847 -1.043901]]...]
INFO - root - 2017-12-07 03:56:04.301186: step 2510, loss = 0.77, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 71h:14m:52s remains)
INFO - root - 2017-12-07 03:56:12.049779: step 2520, loss = 0.81, batch loss = 0.74 (10.3 examples/sec; 0.774 sec/batch; 70h:57m:12s remains)
INFO - root - 2017-12-07 03:56:19.539547: step 2530, loss = 0.84, batch loss = 0.77 (14.0 examples/sec; 0.572 sec/batch; 52h:28m:07s remains)
INFO - root - 2017-12-07 03:56:27.210989: step 2540, loss = 0.98, batch loss = 0.90 (10.1 examples/sec; 0.793 sec/batch; 72h:40m:11s remains)
INFO - root - 2017-12-07 03:56:34.960055: step 2550, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.788 sec/batch; 72h:12m:58s remains)
INFO - root - 2017-12-07 03:56:42.642599: step 2560, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.771 sec/batch; 70h:37m:29s remains)
INFO - root - 2017-12-07 03:56:50.314483: step 2570, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.764 sec/batch; 70h:01m:09s remains)
INFO - root - 2017-12-07 03:56:58.018475: step 2580, loss = 0.86, batch loss = 0.79 (10.0 examples/sec; 0.800 sec/batch; 73h:16m:13s remains)
INFO - root - 2017-12-07 03:57:05.792640: step 2590, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.759 sec/batch; 69h:35m:09s remains)
INFO - root - 2017-12-07 03:57:13.568755: step 2600, loss = 1.00, batch loss = 0.93 (10.2 examples/sec; 0.787 sec/batch; 72h:08m:40s remains)
2017-12-07 03:57:14.352547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6471274 -3.6730294 -3.8253536 -3.984165 -3.9542949 -3.6946826 -3.4447708 -3.3931069 -3.3037589 -3.0783815 -2.8938575 -2.9351315 -2.9003854 -2.7031622 -2.8695951][-3.6887605 -3.7084441 -3.7235298 -3.7574773 -3.7006383 -3.5269451 -3.4313052 -3.4804215 -3.4440334 -3.1917143 -2.8844957 -2.7594228 -2.6341271 -2.4115582 -2.5900068][-3.6956344 -3.7105496 -3.6406131 -3.6365142 -3.6202354 -3.516001 -3.5326855 -3.6768091 -3.6925104 -3.4500723 -3.0499125 -2.7582991 -2.5495117 -2.3662577 -2.6061709][-3.6952963 -3.7490377 -3.6968107 -3.7524333 -3.7855554 -3.6487303 -3.6031604 -3.732017 -3.7983747 -3.6704419 -3.2839813 -2.8688138 -2.5777898 -2.454787 -2.764183][-3.6609294 -3.7608607 -3.7752881 -3.8840075 -3.8799253 -3.5566144 -3.3155737 -3.3977325 -3.6089597 -3.7289166 -3.5155187 -3.0614991 -2.6597672 -2.5172529 -2.8020272][-3.6435475 -3.7707109 -3.8371825 -3.9353342 -3.8144205 -3.2278924 -2.6933193 -2.6454291 -2.9838853 -3.4451582 -3.5875521 -3.2764139 -2.8478498 -2.6584096 -2.8366282][-3.6882591 -3.8588846 -3.9652085 -3.9977946 -3.7474117 -3.0137208 -2.2758973 -2.0508831 -2.3423235 -2.9810174 -3.4478712 -3.4035139 -3.0756161 -2.855401 -2.8981006][-3.7608995 -3.9751554 -4.0689206 -3.9727633 -3.614764 -2.922138 -2.2250078 -1.9158866 -2.0530038 -2.5831509 -3.1091657 -3.2655721 -3.1214719 -2.9420366 -2.8702326][-3.7151866 -3.9544103 -3.9965308 -3.7788885 -3.3589816 -2.8031795 -2.3150342 -2.0998211 -2.1608832 -2.4308906 -2.7592349 -2.9305823 -2.9181986 -2.8487692 -2.7657027][-3.5232522 -3.8089206 -3.8397837 -3.5586462 -3.1399174 -2.7358685 -2.4797802 -2.4238286 -2.4677277 -2.5015187 -2.5464945 -2.6003003 -2.6082716 -2.6254153 -2.5625792][-3.3716 -3.7251019 -3.850348 -3.6490736 -3.3008637 -2.9941871 -2.8352616 -2.8282413 -2.8813882 -2.8049886 -2.6469374 -2.5550392 -2.5007353 -2.5316169 -2.5072479][-3.0551565 -3.463007 -3.7561979 -3.7563334 -3.6197524 -3.485306 -3.3566508 -3.2808533 -3.2972171 -3.1763325 -2.9267371 -2.7568071 -2.6653771 -2.7129149 -2.7717552][-2.4951441 -2.8037014 -3.1626196 -3.3495975 -3.4658728 -3.5578451 -3.5182619 -3.4216566 -3.4050446 -3.2823963 -3.039835 -2.873661 -2.776402 -2.8417575 -2.9815729][-2.2038519 -2.3048246 -2.5497386 -2.7256918 -2.8860102 -3.0484157 -3.0683656 -3.0111859 -3.0165935 -2.968998 -2.8419766 -2.7449183 -2.6608696 -2.7044342 -2.8349466][-2.4063556 -2.3834071 -2.4719415 -2.526123 -2.5810103 -2.6645503 -2.6385074 -2.5488155 -2.5351443 -2.5611238 -2.5747862 -2.5837646 -2.5503478 -2.5630622 -2.6239944]]...]
INFO - root - 2017-12-07 03:57:22.011685: step 2610, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.785 sec/batch; 71h:58m:25s remains)
INFO - root - 2017-12-07 03:57:29.650520: step 2620, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.773 sec/batch; 70h:48m:44s remains)
INFO - root - 2017-12-07 03:57:37.149991: step 2630, loss = 0.66, batch loss = 0.59 (13.4 examples/sec; 0.595 sec/batch; 54h:30m:19s remains)
INFO - root - 2017-12-07 03:57:44.715109: step 2640, loss = 0.84, batch loss = 0.77 (10.8 examples/sec; 0.739 sec/batch; 67h:41m:01s remains)
INFO - root - 2017-12-07 03:57:52.266499: step 2650, loss = 0.93, batch loss = 0.85 (10.6 examples/sec; 0.757 sec/batch; 69h:21m:06s remains)
INFO - root - 2017-12-07 03:57:59.867400: step 2660, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.762 sec/batch; 69h:48m:58s remains)
INFO - root - 2017-12-07 03:58:07.391604: step 2670, loss = 0.95, batch loss = 0.87 (10.7 examples/sec; 0.748 sec/batch; 68h:33m:48s remains)
INFO - root - 2017-12-07 03:58:14.969853: step 2680, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.762 sec/batch; 69h:50m:22s remains)
INFO - root - 2017-12-07 03:58:22.619726: step 2690, loss = 1.00, batch loss = 0.93 (10.7 examples/sec; 0.750 sec/batch; 68h:39m:58s remains)
INFO - root - 2017-12-07 03:58:30.272968: step 2700, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.758 sec/batch; 69h:25m:58s remains)
2017-12-07 03:58:30.918919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.984755 -3.0999529 -3.1662211 -3.2126298 -3.1930642 -3.1665342 -3.1466498 -3.1301894 -3.1297517 -3.1393209 -3.1576302 -3.1699748 -3.1660762 -3.1528935 -3.1206868][-2.78401 -2.7128839 -2.7056427 -2.7644286 -2.8249555 -2.8330388 -2.8001301 -2.7204459 -2.668045 -2.6407394 -2.62953 -2.6168203 -2.5931253 -2.5673079 -2.5250962][-2.4663258 -2.2960534 -2.2884257 -2.407156 -2.566581 -2.6298742 -2.6200829 -2.5192609 -2.4348855 -2.3590579 -2.2927239 -2.2343771 -2.1840317 -2.1219914 -2.0364919][-2.1947246 -2.1353855 -2.193435 -2.3499985 -2.55426 -2.6691184 -2.7051833 -2.6348433 -2.5659473 -2.4822531 -2.3848658 -2.3148954 -2.3003075 -2.2526453 -2.1430633][-2.1337402 -2.2377076 -2.3445344 -2.4998052 -2.6705575 -2.7515955 -2.7299769 -2.6316817 -2.593812 -2.5788012 -2.5175123 -2.5010543 -2.5779371 -2.5794392 -2.487412][-1.9693451 -2.0718143 -2.0826514 -2.118659 -2.2055047 -2.2915697 -2.3001254 -2.2527194 -2.330862 -2.4733441 -2.5030828 -2.5624161 -2.7217522 -2.7678127 -2.7329583][-1.077132 -1.1211634 -1.0678089 -1.0397782 -1.0859873 -1.2074475 -1.3268924 -1.4119921 -1.6489289 -1.9359057 -2.0305071 -2.1154544 -2.2480216 -2.2377043 -2.1990666][0.051117897 0.13119698 0.26974821 0.40868521 0.50362349 0.45787525 0.28774977 0.05892849 -0.36978054 -0.82224679 -1.0627239 -1.2633491 -1.4282413 -1.4194782 -1.4081788][-0.0046768188 0.14141178 0.30700731 0.48706865 0.67933559 0.73441696 0.66403008 0.46141672 0.003262043 -0.46641564 -0.75776052 -0.9822638 -1.0825117 -0.986428 -0.90180206][-1.0737479 -1.0359254 -0.9783361 -0.87839723 -0.76118827 -0.80757236 -0.9394958 -1.1499856 -1.4829223 -1.7050488 -1.7815964 -1.8373168 -1.7783544 -1.5650265 -1.3441048][-1.8351431 -1.9323971 -1.9933548 -1.9559505 -1.9009018 -1.9936383 -2.1191623 -2.2784238 -2.4729569 -2.4832156 -2.3916261 -2.3404553 -2.2671206 -2.1278675 -1.9332116][-2.0043015 -2.1954067 -2.3541811 -2.4083538 -2.4455109 -2.5516176 -2.6119585 -2.6858649 -2.7745547 -2.6737213 -2.4845572 -2.3351893 -2.2090387 -2.0531011 -1.8359172][-1.8257618 -2.0058649 -2.1980305 -2.3419926 -2.5013568 -2.6841073 -2.7481194 -2.8049009 -2.8572924 -2.7770534 -2.6397536 -2.5096619 -2.4008541 -2.2709219 -2.134455][-1.7035856 -1.8273039 -2.0152042 -2.196547 -2.4064586 -2.5931082 -2.6067126 -2.5771859 -2.5270295 -2.4034352 -2.285146 -2.1903191 -2.1675911 -2.2384734 -2.4069271][-1.7753699 -1.7795761 -1.8491218 -1.9046278 -2.0335131 -2.1724558 -2.1766961 -2.1676972 -2.1746991 -2.1607952 -2.186403 -2.1980085 -2.2489424 -2.4478774 -2.7435508]]...]
INFO - root - 2017-12-07 03:58:38.530384: step 2710, loss = 1.00, batch loss = 0.92 (10.4 examples/sec; 0.771 sec/batch; 70h:36m:07s remains)
INFO - root - 2017-12-07 03:58:46.184451: step 2720, loss = 0.98, batch loss = 0.91 (10.1 examples/sec; 0.795 sec/batch; 72h:50m:29s remains)
INFO - root - 2017-12-07 03:58:53.593192: step 2730, loss = 0.66, batch loss = 0.59 (14.8 examples/sec; 0.539 sec/batch; 49h:21m:21s remains)
INFO - root - 2017-12-07 03:59:01.255810: step 2740, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.755 sec/batch; 69h:10m:50s remains)
INFO - root - 2017-12-07 03:59:08.912996: step 2750, loss = 0.76, batch loss = 0.69 (10.1 examples/sec; 0.791 sec/batch; 72h:29m:33s remains)
INFO - root - 2017-12-07 03:59:16.567072: step 2760, loss = 0.83, batch loss = 0.76 (10.0 examples/sec; 0.797 sec/batch; 73h:02m:24s remains)
INFO - root - 2017-12-07 03:59:24.164879: step 2770, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.763 sec/batch; 69h:52m:38s remains)
INFO - root - 2017-12-07 03:59:31.778404: step 2780, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.759 sec/batch; 69h:32m:54s remains)
INFO - root - 2017-12-07 03:59:39.375006: step 2790, loss = 0.87, batch loss = 0.80 (10.7 examples/sec; 0.750 sec/batch; 68h:40m:30s remains)
INFO - root - 2017-12-07 03:59:46.995817: step 2800, loss = 0.61, batch loss = 0.54 (10.5 examples/sec; 0.764 sec/batch; 69h:57m:05s remains)
2017-12-07 03:59:47.612377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3180604 -4.3560786 -4.3694096 -4.3513756 -4.2920918 -4.24407 -4.1792674 -4.112762 -4.0656419 -4.0374541 -4.0295811 -4.0228763 -4.0238771 -4.010438 -3.9915104][-4.2155061 -4.1933575 -4.1608176 -4.1053128 -4.0105696 -3.9624605 -3.8999515 -3.8497536 -3.8412828 -3.8391609 -3.8615215 -3.891293 -3.94819 -3.988194 -4.0085158][-3.7044666 -3.5950742 -3.5155888 -3.4355316 -3.329318 -3.3162827 -3.2943172 -3.3008752 -3.3736949 -3.4194264 -3.4799423 -3.5592394 -3.689867 -3.8114848 -3.9048133][-2.990247 -2.8320045 -2.7356505 -2.6626179 -2.5964022 -2.6466763 -2.66584 -2.6991668 -2.8120327 -2.9036117 -3.02378 -3.1768732 -3.3782413 -3.5703924 -3.7312245][-2.3112543 -2.1728971 -2.0761306 -2.007694 -1.9650307 -2.0131342 -1.996448 -1.9759097 -2.0598612 -2.1681857 -2.333288 -2.5554268 -2.8490362 -3.158659 -3.4541993][-1.7238157 -1.6360183 -1.546176 -1.4710007 -1.4127791 -1.3784287 -1.25126 -1.1290584 -1.1619711 -1.281697 -1.4723783 -1.7159958 -2.0710828 -2.5045304 -2.9631872][-1.2277293 -1.1376469 -1.0042043 -0.87283325 -0.74768782 -0.56676984 -0.29666567 -0.071690559 -0.055611134 -0.21844053 -0.47467256 -0.75801182 -1.1849439 -1.7774501 -2.4227531][-0.9609592 -0.82141614 -0.62583017 -0.43561077 -0.25082731 0.022550583 0.34025955 0.59556818 0.63265896 0.45517254 0.17935228 -0.10658932 -0.57951021 -1.3016355 -2.0929382][-1.2919312 -1.1811404 -1.047673 -0.913188 -0.75760126 -0.51087856 -0.28506279 -0.13323736 -0.14818287 -0.31879568 -0.53173137 -0.72935009 -1.123143 -1.7714469 -2.439878][-2.066869 -2.0174572 -1.9865596 -1.9628146 -1.892092 -1.7413576 -1.6405931 -1.600219 -1.656755 -1.7923555 -1.9334221 -2.076854 -2.3922293 -2.8810077 -3.2945662][-2.8564405 -2.8263748 -2.8340518 -2.8553536 -2.8406291 -2.781775 -2.7827973 -2.8252087 -2.9060025 -3.0205278 -3.1346135 -3.257973 -3.488066 -3.7810431 -3.9557033][-3.6101823 -3.5798571 -3.575696 -3.5567334 -3.5106421 -3.4666176 -3.4859116 -3.5378425 -3.624903 -3.74222 -3.865608 -3.9865606 -4.1391225 -4.2691908 -4.2737293][-4.4211125 -4.4000945 -4.3839197 -4.317554 -4.2154088 -4.1412063 -4.1274786 -4.1374097 -4.1785722 -4.2552958 -4.350347 -4.4378533 -4.5188026 -4.5363274 -4.4342489][-5.0226965 -5.0088973 -4.9741564 -4.876771 -4.7439523 -4.6561394 -4.6312647 -4.627943 -4.6481118 -4.6868863 -4.7270551 -4.7330184 -4.7073164 -4.622632 -4.4431963][-5.1918993 -5.1991429 -5.1642647 -5.0754261 -4.9553728 -4.872776 -4.8415117 -4.832756 -4.8438258 -4.8600616 -4.8625908 -4.8181863 -4.7298722 -4.5873704 -4.3834925]]...]
INFO - root - 2017-12-07 03:59:55.194708: step 2810, loss = 0.93, batch loss = 0.86 (10.5 examples/sec; 0.761 sec/batch; 69h:39m:29s remains)
INFO - root - 2017-12-07 04:00:02.833787: step 2820, loss = 0.71, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 70h:47m:48s remains)
INFO - root - 2017-12-07 04:00:10.249939: step 2830, loss = 0.87, batch loss = 0.79 (14.6 examples/sec; 0.546 sec/batch; 50h:00m:33s remains)
INFO - root - 2017-12-07 04:00:17.966251: step 2840, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.764 sec/batch; 69h:56m:24s remains)
INFO - root - 2017-12-07 04:00:25.600579: step 2850, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.755 sec/batch; 69h:07m:33s remains)
INFO - root - 2017-12-07 04:00:33.222122: step 2860, loss = 0.74, batch loss = 0.67 (10.9 examples/sec; 0.736 sec/batch; 67h:23m:21s remains)
INFO - root - 2017-12-07 04:00:40.833075: step 2870, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.780 sec/batch; 71h:23m:05s remains)
INFO - root - 2017-12-07 04:00:48.390279: step 2880, loss = 1.00, batch loss = 0.92 (10.6 examples/sec; 0.757 sec/batch; 69h:16m:27s remains)
INFO - root - 2017-12-07 04:00:56.098643: step 2890, loss = 0.56, batch loss = 0.49 (10.4 examples/sec; 0.770 sec/batch; 70h:31m:01s remains)
INFO - root - 2017-12-07 04:01:03.742926: step 2900, loss = 1.07, batch loss = 0.99 (10.4 examples/sec; 0.771 sec/batch; 70h:32m:53s remains)
2017-12-07 04:01:04.371250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9902894 -3.9921572 -3.8993132 -3.8602037 -3.8698776 -3.9133906 -3.9277105 -4.0080767 -4.1140928 -4.150322 -4.0525174 -3.8798046 -3.798929 -3.8002791 -3.883116][-3.9440932 -3.9548433 -3.8945808 -3.9298706 -4.0113139 -4.0951371 -4.0744114 -4.1219034 -4.2245536 -4.1892781 -3.9669101 -3.7436543 -3.7229705 -3.8121252 -3.9569702][-4.0415039 -4.0636697 -4.0490494 -4.1464257 -4.2565565 -4.3405452 -4.2829556 -4.2859039 -4.3540292 -4.22931 -3.9144082 -3.6712024 -3.7038288 -3.8615689 -4.0389137][-4.0650587 -4.1413989 -4.2062821 -4.3846927 -4.52871 -4.61102 -4.5005603 -4.4150429 -4.4060159 -4.197125 -3.8500628 -3.6423864 -3.7387552 -3.9496553 -4.1227465][-4.1158738 -4.2875972 -4.4522133 -4.6978283 -4.8821058 -4.9650264 -4.8086329 -4.6299896 -4.5000691 -4.1775937 -3.7983649 -3.6280384 -3.7733045 -4.0196304 -4.1790195][-4.3502526 -4.5881147 -4.7881031 -5.0075865 -5.188839 -5.2907076 -5.1359315 -4.8946362 -4.6440864 -4.2263708 -3.8207908 -3.64848 -3.76914 -4.0007763 -4.1779609][-4.7660947 -4.9770684 -5.1122675 -5.1973066 -5.2834697 -5.3554988 -5.2009897 -4.9351954 -4.6569719 -4.26795 -3.9456372 -3.8156037 -3.8915577 -4.0656652 -4.2249131][-5.1563506 -5.3162928 -5.3611388 -5.3149981 -5.278089 -5.2835784 -5.1298108 -4.8767824 -4.651927 -4.3755574 -4.1829386 -4.13266 -4.2170486 -4.3766479 -4.51254][-5.242125 -5.3763952 -5.3830242 -5.2995291 -5.2233663 -5.2173533 -5.121542 -4.928091 -4.780571 -4.5952091 -4.4723148 -4.4894409 -4.6300755 -4.8247929 -4.951251][-5.2181516 -5.3023462 -5.2824049 -5.1887765 -5.0837555 -5.0649333 -5.0193634 -4.8900323 -4.8103271 -4.6719294 -4.5552831 -4.607161 -4.827239 -5.0929365 -5.248117][-5.2896714 -5.3518677 -5.3325596 -5.1916265 -4.95222 -4.8249316 -4.7239013 -4.5748444 -4.5109844 -4.39383 -4.2887774 -4.3860307 -4.6990929 -5.0513616 -5.2954297][-5.3203673 -5.3746076 -5.3550768 -5.1114321 -4.6821971 -4.4502749 -4.328619 -4.2236962 -4.2357292 -4.1697063 -4.0971889 -4.2064404 -4.517581 -4.8481779 -5.1319313][-5.2691126 -5.3253689 -5.2883668 -4.9562197 -4.4243231 -4.2009659 -4.1812015 -4.2188721 -4.3339262 -4.2490516 -4.091784 -4.076097 -4.215693 -4.4046135 -4.6747947][-5.2294822 -5.315341 -5.2986627 -4.9731274 -4.4815106 -4.3381028 -4.3841729 -4.4509311 -4.5251317 -4.3258076 -4.0379467 -3.8752997 -3.8658679 -3.9658573 -4.2396164][-5.2006826 -5.3537989 -5.3656278 -5.06965 -4.6531067 -4.5613589 -4.58906 -4.6036806 -4.62163 -4.3936067 -4.0658016 -3.8157022 -3.7165205 -3.7710404 -4.0328951]]...]
INFO - root - 2017-12-07 04:01:11.980949: step 2910, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.764 sec/batch; 69h:58m:00s remains)
INFO - root - 2017-12-07 04:01:19.711672: step 2920, loss = 0.90, batch loss = 0.83 (10.1 examples/sec; 0.789 sec/batch; 72h:13m:35s remains)
INFO - root - 2017-12-07 04:01:27.251816: step 2930, loss = 1.03, batch loss = 0.95 (13.9 examples/sec; 0.576 sec/batch; 52h:42m:27s remains)
INFO - root - 2017-12-07 04:01:34.871106: step 2940, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.766 sec/batch; 70h:08m:34s remains)
INFO - root - 2017-12-07 04:01:42.514737: step 2950, loss = 1.02, batch loss = 0.95 (10.7 examples/sec; 0.750 sec/batch; 68h:42m:01s remains)
INFO - root - 2017-12-07 04:01:50.228015: step 2960, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.760 sec/batch; 69h:32m:32s remains)
INFO - root - 2017-12-07 04:01:57.956006: step 2970, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.756 sec/batch; 69h:14m:44s remains)
INFO - root - 2017-12-07 04:02:05.588610: step 2980, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.768 sec/batch; 70h:18m:27s remains)
INFO - root - 2017-12-07 04:02:13.240498: step 2990, loss = 0.81, batch loss = 0.73 (10.1 examples/sec; 0.789 sec/batch; 72h:15m:41s remains)
INFO - root - 2017-12-07 04:02:20.991074: step 3000, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.765 sec/batch; 70h:01m:07s remains)
2017-12-07 04:02:21.618087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5259175 -2.5339961 -2.7165523 -2.686192 -2.483732 -2.3872302 -2.2047241 -2.019407 -1.7371037 -1.6955163 -1.7088153 -1.4801219 -1.638073 -1.9691648 -1.9083972][-2.4066935 -2.4280896 -2.6060123 -2.6169405 -2.4851055 -2.4804282 -2.3187509 -2.090343 -1.8410511 -1.8114681 -1.788779 -1.5253065 -1.6376164 -1.8865793 -1.772449][-1.9024897 -1.8863175 -2.0192349 -2.0615709 -2.0908461 -2.2977705 -2.2792256 -2.1234562 -2.0447512 -2.1233962 -2.1306629 -1.9128346 -1.9598966 -2.091383 -1.9543388][-1.4600706 -1.4405818 -1.5146191 -1.5558522 -1.6891601 -2.0511024 -2.1384046 -2.0341547 -2.0977767 -2.2858822 -2.3599124 -2.2099721 -2.1884074 -2.2043948 -2.0663581][-1.1733816 -1.1635246 -1.2126276 -1.3124378 -1.5231071 -1.8851955 -1.918555 -1.7233829 -1.8038387 -2.05903 -2.1927524 -2.0960493 -2.0205803 -1.9810677 -1.8807249][-0.86181188 -0.68958616 -0.5854373 -0.71548796 -1.0178201 -1.3152232 -1.1897991 -0.87286997 -0.9897716 -1.4147375 -1.7289641 -1.7840629 -1.8101134 -1.8372166 -1.8595788][-0.87931657 -0.44604659 -0.05267477 -0.11383772 -0.43351769 -0.56737781 -0.15723276 0.39076471 0.25916386 -0.33687019 -0.7886765 -0.96307921 -1.1013386 -1.2619526 -1.4616108][-1.4156048 -0.88292623 -0.39219093 -0.40715027 -0.66348696 -0.65234041 -0.1063695 0.52646351 0.46831894 -0.099806309 -0.47619557 -0.56611443 -0.63350105 -0.75140572 -0.96101308][-2.2660804 -1.8350737 -1.4655726 -1.4852324 -1.6441393 -1.6488411 -1.3294675 -0.93500113 -0.97050214 -1.3979878 -1.6180282 -1.5666745 -1.4959104 -1.4365456 -1.4749002][-3.081244 -2.8949194 -2.7706704 -2.835989 -2.9274621 -2.97755 -2.8293636 -2.52711 -2.4842694 -2.7580056 -2.7764502 -2.4735565 -2.1941755 -2.0085535 -1.9368432][-3.6205206 -3.6375005 -3.7352993 -3.9070592 -4.0478816 -4.1495538 -4.0470052 -3.7234397 -3.5761585 -3.6817076 -3.4927843 -3.0094862 -2.7002041 -2.589278 -2.4606342][-3.8654397 -3.9642792 -4.1375985 -4.363421 -4.5702753 -4.71703 -4.6432161 -4.2967033 -3.9962673 -3.8518522 -3.47699 -3.0066633 -2.9312272 -3.0887866 -3.0828547][-3.8278439 -3.9563518 -4.1246705 -4.3249221 -4.5062623 -4.6105189 -4.5232754 -4.1862116 -3.8267326 -3.5358636 -3.1144466 -2.6857324 -2.6309223 -2.7782159 -2.7819333][-3.6208773 -3.7112851 -3.8235416 -3.9656448 -4.10514 -4.2048292 -4.1928763 -3.9751995 -3.6633396 -3.3664651 -3.0952091 -2.9225287 -2.9946969 -3.139143 -3.0880675][-3.4130583 -3.4395435 -3.4738202 -3.5289881 -3.6117995 -3.7230179 -3.8056095 -3.7277584 -3.5257056 -3.3351955 -3.2571917 -3.3317657 -3.5667863 -3.82677 -3.8887944]]...]
INFO - root - 2017-12-07 04:02:29.263710: step 3010, loss = 0.99, batch loss = 0.92 (10.1 examples/sec; 0.788 sec/batch; 72h:09m:52s remains)
INFO - root - 2017-12-07 04:02:36.967637: step 3020, loss = 0.97, batch loss = 0.89 (10.4 examples/sec; 0.772 sec/batch; 70h:38m:05s remains)
INFO - root - 2017-12-07 04:02:44.383824: step 3030, loss = 0.72, batch loss = 0.65 (13.3 examples/sec; 0.600 sec/batch; 54h:56m:22s remains)
INFO - root - 2017-12-07 04:02:52.033273: step 3040, loss = 0.71, batch loss = 0.64 (10.7 examples/sec; 0.748 sec/batch; 68h:25m:17s remains)
INFO - root - 2017-12-07 04:02:59.652485: step 3050, loss = 0.78, batch loss = 0.70 (10.6 examples/sec; 0.758 sec/batch; 69h:21m:04s remains)
INFO - root - 2017-12-07 04:03:07.392361: step 3060, loss = 0.94, batch loss = 0.87 (10.2 examples/sec; 0.781 sec/batch; 71h:26m:46s remains)
INFO - root - 2017-12-07 04:03:15.070752: step 3070, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.760 sec/batch; 69h:33m:09s remains)
INFO - root - 2017-12-07 04:03:22.794830: step 3080, loss = 0.87, batch loss = 0.80 (10.2 examples/sec; 0.781 sec/batch; 71h:28m:38s remains)
INFO - root - 2017-12-07 04:03:30.489249: step 3090, loss = 0.93, batch loss = 0.86 (10.7 examples/sec; 0.751 sec/batch; 68h:43m:30s remains)
INFO - root - 2017-12-07 04:03:38.096958: step 3100, loss = 0.91, batch loss = 0.83 (10.5 examples/sec; 0.765 sec/batch; 69h:57m:31s remains)
2017-12-07 04:03:38.717966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6873178 -5.5750837 -5.1834736 -4.5746322 -3.9599707 -3.3563466 -2.7393322 -2.2307518 -1.9832287 -2.0877559 -2.3776152 -2.5727575 -2.674861 -2.7912052 -2.8317618][-5.6423554 -5.4786873 -5.04864 -4.4112411 -3.7455678 -3.1011257 -2.5344126 -2.10845 -1.9295158 -2.0390964 -2.2686658 -2.4025292 -2.4804034 -2.5899949 -2.6435783][-5.2188549 -5.0700555 -4.6840158 -4.0783787 -3.3949318 -2.7501163 -2.2745521 -1.9808381 -1.9103079 -2.0388141 -2.195159 -2.2513428 -2.2736731 -2.3437169 -2.4045386][-4.6676221 -4.4510026 -4.0923514 -3.5503473 -2.923254 -2.35511 -2.0206521 -1.9004104 -1.9685252 -2.1364143 -2.2533929 -2.2686779 -2.2482011 -2.2637787 -2.3011341][-4.17224 -3.8862929 -3.5031412 -2.958715 -2.3653104 -1.8922541 -1.7278154 -1.8032253 -2.0068254 -2.2084377 -2.3131745 -2.3238649 -2.28824 -2.2670827 -2.2908449][-3.8255458 -3.5037656 -3.0831847 -2.5070224 -1.9193685 -1.5053284 -1.4511554 -1.6563826 -1.93413 -2.1569207 -2.2902343 -2.3350823 -2.3128154 -2.2850556 -2.3150041][-3.5054986 -3.1835608 -2.7534971 -2.1752653 -1.6203806 -1.3001647 -1.3487515 -1.6401575 -1.9230969 -2.134846 -2.2978337 -2.3772986 -2.371645 -2.3413303 -2.3588097][-3.1966991 -2.885469 -2.4508929 -1.9232199 -1.4708469 -1.315697 -1.5042288 -1.8378475 -2.0681398 -2.221741 -2.3601048 -2.4247224 -2.4101975 -2.363735 -2.3599057][-2.9411869 -2.5987756 -2.1394877 -1.673526 -1.3678477 -1.3888228 -1.6794076 -2.007015 -2.1866195 -2.2987773 -2.4153495 -2.4617743 -2.4380741 -2.3699284 -2.3265331][-2.6895804 -2.2902653 -1.8200035 -1.4395969 -1.3000216 -1.4593239 -1.7920313 -2.0919411 -2.2595251 -2.3807738 -2.4989681 -2.526701 -2.4833148 -2.3840487 -2.2912302][-2.5119812 -2.0671194 -1.6254044 -1.3528068 -1.3604593 -1.5995324 -1.9101992 -2.1640177 -2.3200905 -2.4531374 -2.5628781 -2.5604472 -2.4993563 -2.3804474 -2.255831][-2.3599925 -1.8968933 -1.5260682 -1.3920374 -1.5306747 -1.8157411 -2.0792894 -2.2741024 -2.4090233 -2.5328972 -2.6167758 -2.5945888 -2.527945 -2.4024851 -2.2610977][-2.2521241 -1.8197882 -1.5561574 -1.5514233 -1.7746003 -2.0636506 -2.2827029 -2.4426866 -2.56168 -2.6533046 -2.7013264 -2.6536536 -2.5664361 -2.4313841 -2.2914631][-2.2418432 -1.8847713 -1.7565136 -1.8510599 -2.0777574 -2.301007 -2.4409771 -2.562494 -2.6614633 -2.715291 -2.7361097 -2.680253 -2.5900364 -2.4725347 -2.3592072][-2.3175824 -2.0746014 -2.0920389 -2.2523572 -2.4305267 -2.5434237 -2.5888612 -2.6809571 -2.775918 -2.8080139 -2.8173094 -2.7663944 -2.689218 -2.6024015 -2.5237164]]...]
INFO - root - 2017-12-07 04:03:46.487225: step 3110, loss = 0.60, batch loss = 0.53 (10.3 examples/sec; 0.778 sec/batch; 71h:09m:05s remains)
INFO - root - 2017-12-07 04:03:54.067989: step 3120, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.758 sec/batch; 69h:18m:27s remains)
INFO - root - 2017-12-07 04:04:01.548277: step 3130, loss = 0.97, batch loss = 0.90 (13.6 examples/sec; 0.590 sec/batch; 53h:58m:04s remains)
INFO - root - 2017-12-07 04:04:09.155902: step 3140, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.783 sec/batch; 71h:37m:29s remains)
INFO - root - 2017-12-07 04:04:16.824786: step 3150, loss = 0.79, batch loss = 0.71 (10.6 examples/sec; 0.752 sec/batch; 68h:48m:49s remains)
INFO - root - 2017-12-07 04:04:24.537835: step 3160, loss = 0.67, batch loss = 0.60 (10.2 examples/sec; 0.784 sec/batch; 71h:42m:08s remains)
INFO - root - 2017-12-07 04:04:32.189443: step 3170, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.753 sec/batch; 68h:51m:17s remains)
INFO - root - 2017-12-07 04:04:39.807824: step 3180, loss = 0.93, batch loss = 0.86 (11.0 examples/sec; 0.729 sec/batch; 66h:43m:23s remains)
INFO - root - 2017-12-07 04:04:47.389503: step 3190, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 69h:41m:46s remains)
INFO - root - 2017-12-07 04:04:55.130102: step 3200, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.764 sec/batch; 69h:54m:45s remains)
2017-12-07 04:04:55.763918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6976647 -2.9195957 -3.0420692 -2.8996773 -2.5357389 -2.0800798 -1.6077869 -1.2018962 -0.93932247 -0.9613502 -1.0913725 -1.2328539 -1.4098532 -1.7094142 -1.9948175][-2.3408616 -2.4746046 -2.6009297 -2.5340297 -2.2044287 -1.7389171 -1.2841811 -0.94839263 -0.78003907 -0.88351941 -1.0444479 -1.1813049 -1.3089879 -1.511404 -1.7705696][-1.9472311 -2.1285491 -2.3436441 -2.4040148 -2.1932631 -1.8413036 -1.5051618 -1.2649288 -1.1598425 -1.2639766 -1.3752542 -1.4131799 -1.3757145 -1.3814132 -1.5654259][-1.9307308 -2.1805358 -2.3979375 -2.4759364 -2.3398955 -2.1096914 -1.8951218 -1.694963 -1.55253 -1.566443 -1.5798268 -1.5102487 -1.3450432 -1.2545114 -1.4650681][-2.3355312 -2.566077 -2.6916661 -2.717104 -2.5944157 -2.4085798 -2.1838865 -1.8705971 -1.5797875 -1.482506 -1.477164 -1.4639313 -1.3914797 -1.3835495 -1.6620533][-2.8494878 -2.9764357 -2.9924579 -2.9885721 -2.8594134 -2.6036439 -2.2228487 -1.7319574 -1.3700671 -1.3537278 -1.5155523 -1.6986394 -1.78669 -1.8171456 -2.0385995][-3.1983438 -3.1771474 -3.08857 -3.0636935 -2.8969641 -2.5382383 -2.0204461 -1.4999275 -1.2732742 -1.4925447 -1.898874 -2.2547095 -2.4220738 -2.4190233 -2.5183048][-3.1777132 -3.0633969 -2.8951271 -2.8151627 -2.6317053 -2.3344581 -1.9453223 -1.6405177 -1.6348488 -2.0202968 -2.5323262 -2.8742635 -2.9534137 -2.8445849 -2.797709][-2.8680363 -2.7856019 -2.6386292 -2.5792794 -2.4872954 -2.4173605 -2.360656 -2.3542163 -2.4565289 -2.7922225 -3.2002702 -3.3623857 -3.2485135 -2.9871945 -2.7506142][-2.692667 -2.7248366 -2.6580725 -2.6622834 -2.6434352 -2.7378433 -2.9236073 -3.0901377 -3.1652515 -3.3344893 -3.5434263 -3.5298193 -3.299607 -2.9574068 -2.5658054][-2.6504741 -2.7331131 -2.7300611 -2.7978373 -2.7930026 -2.8729072 -3.0857937 -3.255125 -3.2491345 -3.256927 -3.2961345 -3.1545897 -2.89014 -2.5278959 -2.1110547][-2.6625137 -2.7287607 -2.7968054 -2.8997087 -2.8611827 -2.8202963 -2.9320655 -3.0577102 -3.0146856 -2.9259677 -2.8475184 -2.6338048 -2.3315573 -1.9775038 -1.6688564][-2.8397551 -2.91601 -2.9956746 -3.0419836 -2.95191 -2.8475351 -2.8682926 -2.9197006 -2.8489671 -2.7260647 -2.6602402 -2.5043576 -2.2278309 -1.9208469 -1.7735443][-3.0601652 -3.1508298 -3.1912804 -3.1651111 -3.1058149 -3.0371785 -2.9803455 -2.8757572 -2.6999686 -2.5281353 -2.5376625 -2.5217888 -2.3271689 -2.08789 -2.0724378][-3.1692371 -3.2360315 -3.2249665 -3.1666172 -3.1576004 -3.1279204 -2.9909031 -2.7262053 -2.4373274 -2.2411034 -2.3189476 -2.4345975 -2.3708606 -2.2215788 -2.2976933]]...]
INFO - root - 2017-12-07 04:05:03.425489: step 3210, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.776 sec/batch; 70h:59m:42s remains)
INFO - root - 2017-12-07 04:05:11.005981: step 3220, loss = 0.86, batch loss = 0.78 (10.5 examples/sec; 0.764 sec/batch; 69h:55m:29s remains)
INFO - root - 2017-12-07 04:05:18.407407: step 3230, loss = 0.92, batch loss = 0.85 (14.6 examples/sec; 0.548 sec/batch; 50h:07m:17s remains)
INFO - root - 2017-12-07 04:05:26.090501: step 3240, loss = 0.90, batch loss = 0.83 (10.4 examples/sec; 0.770 sec/batch; 70h:24m:25s remains)
INFO - root - 2017-12-07 04:05:33.758554: step 3250, loss = 0.91, batch loss = 0.83 (10.4 examples/sec; 0.768 sec/batch; 70h:17m:07s remains)
INFO - root - 2017-12-07 04:05:41.395773: step 3260, loss = 0.75, batch loss = 0.68 (10.8 examples/sec; 0.744 sec/batch; 67h:59m:50s remains)
INFO - root - 2017-12-07 04:05:49.083985: step 3270, loss = 0.95, batch loss = 0.88 (10.1 examples/sec; 0.793 sec/batch; 72h:32m:21s remains)
INFO - root - 2017-12-07 04:05:56.817475: step 3280, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.777 sec/batch; 71h:01m:42s remains)
INFO - root - 2017-12-07 04:06:04.413695: step 3290, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 69h:14m:53s remains)
INFO - root - 2017-12-07 04:06:12.013973: step 3300, loss = 0.51, batch loss = 0.44 (10.5 examples/sec; 0.759 sec/batch; 69h:22m:30s remains)
2017-12-07 04:06:12.584739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7817676 -2.8191819 -2.917042 -3.0531225 -3.1590519 -3.2374439 -3.2770197 -3.3107219 -3.32085 -3.3139639 -3.3259597 -3.3285308 -3.3198628 -3.2967544 -3.2176027][-2.6182485 -2.6145062 -2.6719973 -2.7806332 -2.8628185 -2.9402494 -3.0093613 -3.0744348 -3.1083679 -3.1215384 -3.1476269 -3.1684356 -3.2067146 -3.2254968 -3.1625931][-2.692275 -2.7030807 -2.7213578 -2.7626553 -2.7716875 -2.792628 -2.8405094 -2.896317 -2.9155459 -2.9169855 -2.9330714 -2.947557 -2.9969354 -3.0374415 -3.0045371][-2.8516765 -2.8641987 -2.8283 -2.7829075 -2.7006435 -2.6390519 -2.621963 -2.61812 -2.5859947 -2.5622768 -2.5759456 -2.6187024 -2.720027 -2.8323298 -2.87482][-2.8962407 -2.888833 -2.8149729 -2.7020588 -2.5400591 -2.3925669 -2.2922156 -2.2084532 -2.1239142 -2.0820422 -2.0970521 -2.1727655 -2.3354261 -2.5432477 -2.6731813][-2.9703445 -2.9470816 -2.8389158 -2.6528506 -2.40148 -2.1747518 -2.0028386 -1.8634968 -1.7783527 -1.7732458 -1.8271399 -1.9401023 -2.1286213 -2.367595 -2.5087833][-3.0977426 -3.057168 -2.9209878 -2.6783638 -2.35988 -2.0816009 -1.8607721 -1.6807733 -1.6180944 -1.681643 -1.8156908 -2.0034695 -2.2341292 -2.4876013 -2.6223693][-3.0417781 -2.9971604 -2.8783875 -2.6492658 -2.3378611 -2.0647533 -1.8399539 -1.6532419 -1.6190047 -1.7290583 -1.9055519 -2.1342831 -2.382571 -2.6221979 -2.7351322][-3.0010409 -2.9509916 -2.8600323 -2.6834812 -2.4337549 -2.2147734 -2.0371461 -1.9057922 -1.9165661 -2.0338817 -2.1757228 -2.34732 -2.5300851 -2.687021 -2.7410994][-3.1696849 -3.110707 -3.0328014 -2.8979135 -2.7078 -2.5439453 -2.430687 -2.372359 -2.423322 -2.5267406 -2.6088946 -2.6855922 -2.7811539 -2.8679733 -2.9060888][-3.2416177 -3.2004557 -3.163286 -3.107054 -3.0134668 -2.9293242 -2.8864582 -2.8912618 -2.9389896 -2.9840221 -2.9822054 -2.9658742 -2.9848526 -3.0282645 -3.0801725][-3.1738281 -3.1584129 -3.1474166 -3.1482596 -3.1378279 -3.1256943 -3.1411786 -3.194375 -3.2263122 -3.2100298 -3.1340415 -3.0413477 -2.9883704 -2.9846 -3.0331273][-3.0802264 -3.0739317 -3.0521045 -3.0464721 -3.0525007 -3.0717878 -3.1200585 -3.2016716 -3.2267916 -3.1814208 -3.0893278 -2.9767084 -2.8901272 -2.8591266 -2.9103172][-2.957489 -2.9890575 -2.9956553 -3.0066516 -3.0384209 -3.0856538 -3.144259 -3.2145472 -3.2168412 -3.1582379 -3.0752778 -2.9739056 -2.8801551 -2.829155 -2.8598883][-2.8011842 -2.867496 -2.9076633 -2.9402053 -2.9936457 -3.0539849 -3.104305 -3.1483183 -3.1429188 -3.1081147 -3.0602002 -2.9889202 -2.9063268 -2.8427629 -2.840565]]...]
INFO - root - 2017-12-07 04:06:20.397030: step 3310, loss = 0.75, batch loss = 0.68 (10.0 examples/sec; 0.799 sec/batch; 73h:04m:43s remains)
INFO - root - 2017-12-07 04:06:28.118369: step 3320, loss = 1.04, batch loss = 0.97 (10.1 examples/sec; 0.792 sec/batch; 72h:26m:30s remains)
INFO - root - 2017-12-07 04:06:35.607201: step 3330, loss = 0.87, batch loss = 0.80 (14.3 examples/sec; 0.558 sec/batch; 51h:03m:46s remains)
INFO - root - 2017-12-07 04:06:43.169479: step 3340, loss = 0.73, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 71h:12m:33s remains)
INFO - root - 2017-12-07 04:06:50.761365: step 3350, loss = 0.91, batch loss = 0.83 (10.3 examples/sec; 0.779 sec/batch; 71h:14m:46s remains)
INFO - root - 2017-12-07 04:06:58.341389: step 3360, loss = 0.74, batch loss = 0.67 (10.8 examples/sec; 0.743 sec/batch; 67h:54m:53s remains)
INFO - root - 2017-12-07 04:07:06.050172: step 3370, loss = 0.68, batch loss = 0.61 (10.4 examples/sec; 0.772 sec/batch; 70h:37m:01s remains)
INFO - root - 2017-12-07 04:07:13.699179: step 3380, loss = 0.77, batch loss = 0.70 (10.9 examples/sec; 0.736 sec/batch; 67h:18m:08s remains)
INFO - root - 2017-12-07 04:07:21.393891: step 3390, loss = 0.90, batch loss = 0.82 (10.4 examples/sec; 0.769 sec/batch; 70h:19m:14s remains)
INFO - root - 2017-12-07 04:07:29.031176: step 3400, loss = 0.69, batch loss = 0.61 (10.2 examples/sec; 0.784 sec/batch; 71h:37m:35s remains)
2017-12-07 04:07:29.645299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7970831 -1.8785129 -2.0686793 -2.3080804 -2.4101465 -2.4641166 -2.7026711 -2.9906702 -3.132031 -3.0057044 -2.5764503 -1.9002259 -1.3036642 -1.0753937 -1.0140274][-1.3500669 -1.4674222 -1.6528802 -1.8737547 -2.1026912 -2.3107336 -2.6406682 -2.9228702 -2.9997129 -2.7859368 -2.2780244 -1.633765 -1.1177959 -0.98935509 -1.0103409][-1.2287011 -1.2600236 -1.406081 -1.577533 -1.9141092 -2.1974056 -2.4469314 -2.5898247 -2.6180639 -2.507199 -2.2154841 -1.825942 -1.5038543 -1.4368639 -1.3743532][-1.8234231 -1.7600999 -1.7821565 -1.7450383 -1.9017591 -2.0532544 -2.0667126 -1.9720209 -1.9136164 -1.9438531 -1.9785504 -1.9876328 -1.9990458 -2.0786989 -1.8997312][-2.2915168 -2.1559954 -2.0574493 -1.8441281 -1.8171594 -1.8817787 -1.7462101 -1.4505794 -1.2948577 -1.4043255 -1.6830931 -2.0351338 -2.3633568 -2.6300964 -2.5027235][-2.5711713 -2.2344694 -1.9477437 -1.6818542 -1.6308601 -1.6966932 -1.4662058 -0.96631455 -0.7412703 -0.96554017 -1.5266101 -2.1788347 -2.6741543 -3.0220096 -3.0542402][-2.2873344 -1.8939011 -1.600091 -1.4692366 -1.4092946 -1.3035238 -0.78411126 -0.011815548 0.26794147 -0.10109091 -0.93412519 -1.7947035 -2.4048982 -2.8687711 -3.1598604][-1.6426539 -1.420027 -1.3730159 -1.4983537 -1.4596906 -1.2312217 -0.55445457 0.33943367 0.776206 0.54063845 -0.19900513 -0.92881775 -1.523535 -2.1345184 -2.6566052][-1.2561707 -1.3349707 -1.5409923 -1.7912974 -1.8221118 -1.7570717 -1.3188019 -0.58576632 0.014076233 0.1800828 -0.11975288 -0.4581089 -0.91206861 -1.5496163 -2.0721884][-0.95277762 -1.2766564 -1.5872688 -1.8040681 -1.9193206 -2.1527293 -2.1482215 -1.7345173 -1.1884313 -0.78236723 -0.69103146 -0.68001151 -0.91779947 -1.343369 -1.6227612][-1.1548297 -1.4886839 -1.7314861 -1.7346106 -1.7475007 -2.0884967 -2.390496 -2.2881491 -1.9358709 -1.5623105 -1.3418562 -1.1242766 -1.099489 -1.2354834 -1.2784693][-2.0098507 -2.2129822 -2.2621431 -1.9916639 -1.7607925 -1.9568558 -2.2885995 -2.3536298 -2.157094 -1.8749313 -1.6479828 -1.3658586 -1.1702156 -1.1242573 -1.0851419][-2.8623576 -2.8139997 -2.6007214 -2.1114602 -1.7310309 -1.7631991 -2.0223298 -2.1832917 -2.1219847 -1.9452612 -1.780611 -1.5706792 -1.38573 -1.3135719 -1.3121846][-3.5753784 -3.3093781 -2.8679471 -2.289475 -1.8718348 -1.7750623 -1.9114492 -2.117013 -2.2297869 -2.1972909 -2.117799 -2.0345364 -1.9530797 -1.9088812 -1.9329839][-3.9713373 -3.7299333 -3.2959371 -2.7933764 -2.4101551 -2.2538197 -2.286303 -2.4754052 -2.6484141 -2.6496329 -2.5973444 -2.6275625 -2.6664162 -2.6713905 -2.7242155]]...]
INFO - root - 2017-12-07 04:07:37.241286: step 3410, loss = 0.73, batch loss = 0.66 (10.2 examples/sec; 0.786 sec/batch; 71h:49m:24s remains)
INFO - root - 2017-12-07 04:07:44.872404: step 3420, loss = 0.82, batch loss = 0.75 (9.8 examples/sec; 0.816 sec/batch; 74h:33m:07s remains)
INFO - root - 2017-12-07 04:07:52.343708: step 3430, loss = 0.83, batch loss = 0.76 (14.5 examples/sec; 0.550 sec/batch; 50h:16m:56s remains)
INFO - root - 2017-12-07 04:07:59.994885: step 3440, loss = 0.71, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 72h:08m:30s remains)
INFO - root - 2017-12-07 04:08:07.749736: step 3450, loss = 0.99, batch loss = 0.91 (10.0 examples/sec; 0.801 sec/batch; 73h:14m:44s remains)
INFO - root - 2017-12-07 04:08:15.447712: step 3460, loss = 1.14, batch loss = 1.06 (10.5 examples/sec; 0.759 sec/batch; 69h:22m:47s remains)
INFO - root - 2017-12-07 04:08:23.153047: step 3470, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.781 sec/batch; 71h:24m:37s remains)
INFO - root - 2017-12-07 04:08:30.935254: step 3480, loss = 1.01, batch loss = 0.94 (10.6 examples/sec; 0.757 sec/batch; 69h:13m:44s remains)
INFO - root - 2017-12-07 04:08:38.492556: step 3490, loss = 0.78, batch loss = 0.70 (10.7 examples/sec; 0.749 sec/batch; 68h:25m:36s remains)
INFO - root - 2017-12-07 04:08:46.164635: step 3500, loss = 0.68, batch loss = 0.61 (10.4 examples/sec; 0.769 sec/batch; 70h:17m:25s remains)
2017-12-07 04:08:46.776868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4244971 -4.0502014 -3.9200711 -3.1516092 -2.9623854 -3.3640454 -3.8037248 -3.7658334 -3.2787018 -2.9203794 -2.9540803 -3.3041532 -3.6034646 -3.5341163 -3.0213318][-2.83032 -3.5578265 -3.2915454 -2.3300896 -2.1346848 -2.6920319 -3.2282591 -3.2284055 -2.7546334 -2.3512864 -2.3493252 -2.6940515 -2.9435058 -2.7963839 -2.1973126][-1.9743013 -2.7772141 -2.3874335 -1.3095939 -1.1411951 -1.8261089 -2.4724774 -2.6114621 -2.2786727 -1.8703957 -1.7817674 -1.9401858 -1.9750407 -1.766402 -1.2297792][-1.44735 -2.3793466 -1.9826937 -0.89741778 -0.78777647 -1.5209417 -2.1621962 -2.4244945 -2.3170009 -1.9740534 -1.7461526 -1.5887213 -1.2957127 -1.0164769 -0.60802126][-1.3488493 -2.4177856 -2.187732 -1.2753007 -1.2468832 -1.8909786 -2.4004552 -2.7227378 -2.873435 -2.6626277 -2.3361542 -1.9125855 -1.3500896 -0.97298074 -0.64780855][-1.3661923 -2.5137217 -2.5317292 -1.9389219 -1.9587235 -2.4244516 -2.7424068 -3.0231309 -3.3365507 -3.2777953 -2.9602613 -2.4150264 -1.7060077 -1.2221808 -0.87557578][-1.5149415 -2.6044526 -2.7951622 -2.4901938 -2.5097971 -2.7677393 -2.8294921 -2.9073658 -3.1933711 -3.2077007 -3.0094488 -2.5433435 -1.8245168 -1.27899 -0.89107943][-1.7221165 -2.6058831 -2.8303933 -2.7358236 -2.787358 -2.9145496 -2.7981832 -2.6651196 -2.8043687 -2.7841625 -2.6748281 -2.319885 -1.6733789 -1.1892898 -0.87106347][-1.5635347 -2.1461914 -2.2938643 -2.3535419 -2.55892 -2.7273455 -2.6001039 -2.4309115 -2.5096831 -2.4408648 -2.2768633 -1.902869 -1.3226256 -0.99452853 -0.85148811][-1.1427565 -1.4375515 -1.4735432 -1.6260302 -2.035835 -2.4118233 -2.4877491 -2.4931104 -2.5818434 -2.457047 -2.2315145 -1.8071609 -1.2791302 -1.0590837 -1.0708642][-1.0648921 -1.2213812 -1.2016978 -1.374579 -1.9130213 -2.473444 -2.7809439 -2.9860964 -3.1281285 -3.0418854 -2.8294678 -2.376091 -1.843411 -1.6346099 -1.6975672][-1.3319993 -1.4975576 -1.4889064 -1.6350121 -2.189543 -2.827899 -3.2911544 -3.6061294 -3.7522764 -3.711904 -3.5208337 -3.0796223 -2.6014934 -2.4300461 -2.5057213][-1.8124778 -2.0520048 -2.0718791 -2.1442263 -2.5755196 -3.1310182 -3.6081064 -3.8702869 -3.8864951 -3.7879729 -3.547179 -3.1428261 -2.8209591 -2.7887416 -2.9726372][-2.571394 -2.8194218 -2.8117723 -2.8140185 -3.0664651 -3.4295847 -3.80466 -3.9636831 -3.85579 -3.6894925 -3.4259565 -3.1197364 -2.9709537 -3.0656404 -3.3674555][-3.1658282 -3.3455253 -3.2975473 -3.2494364 -3.3555632 -3.5096931 -3.7376308 -3.8179898 -3.6834452 -3.5009284 -3.2570581 -3.074964 -3.0371418 -3.1519194 -3.4865484]]...]
INFO - root - 2017-12-07 04:08:54.499393: step 3510, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.754 sec/batch; 68h:53m:52s remains)
INFO - root - 2017-12-07 04:09:02.233367: step 3520, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.772 sec/batch; 70h:33m:47s remains)
INFO - root - 2017-12-07 04:09:09.604302: step 3530, loss = 0.72, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 50h:19m:48s remains)
INFO - root - 2017-12-07 04:09:17.353675: step 3540, loss = 0.99, batch loss = 0.92 (10.3 examples/sec; 0.777 sec/batch; 70h:59m:12s remains)
INFO - root - 2017-12-07 04:09:25.168623: step 3550, loss = 0.72, batch loss = 0.65 (10.7 examples/sec; 0.745 sec/batch; 68h:06m:13s remains)
INFO - root - 2017-12-07 04:09:32.841535: step 3560, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.780 sec/batch; 71h:14m:03s remains)
INFO - root - 2017-12-07 04:09:40.592685: step 3570, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.786 sec/batch; 71h:47m:55s remains)
INFO - root - 2017-12-07 04:09:48.409621: step 3580, loss = 0.82, batch loss = 0.74 (10.1 examples/sec; 0.791 sec/batch; 72h:16m:23s remains)
INFO - root - 2017-12-07 04:09:55.984784: step 3590, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.786 sec/batch; 71h:51m:04s remains)
INFO - root - 2017-12-07 04:10:03.592635: step 3600, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.777 sec/batch; 71h:01m:55s remains)
2017-12-07 04:10:04.203364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1209726 -2.7722859 -2.3417795 -2.01586 -1.743778 -1.511528 -1.2627051 -0.76710272 -0.34402275 -0.35914278 -0.675277 -1.2868798 -1.9775643 -2.3262022 -2.4183178][-3.4850574 -3.2574704 -2.9953532 -2.7932823 -2.5711117 -2.3875442 -2.3136141 -2.0270681 -1.7306414 -1.6835194 -1.7685387 -2.084064 -2.5352037 -2.793932 -2.9678359][-3.0021377 -2.9647572 -2.9312403 -2.8596439 -2.6771815 -2.4956567 -2.5673366 -2.5367951 -2.4220583 -2.3681483 -2.2758615 -2.3202684 -2.5049181 -2.6571379 -2.9349284][-2.1271877 -2.345778 -2.5653493 -2.5934007 -2.4047568 -2.2082722 -2.348367 -2.4445014 -2.3953121 -2.341006 -2.2575243 -2.2240174 -2.1899593 -2.2002118 -2.5629249][-1.5775506 -1.8332453 -2.0409188 -1.9737566 -1.6508288 -1.3121247 -1.3198125 -1.4086199 -1.5287812 -1.7627087 -2.0168595 -2.1646912 -2.0894012 -2.0630591 -2.5372052][-1.3126895 -1.3843651 -1.4690442 -1.2625682 -0.70211673 -0.021144867 0.28217793 0.20239353 -0.30758095 -1.0852859 -1.8226531 -2.2216148 -2.2181754 -2.2439098 -2.793221][-0.8016746 -0.66121745 -0.78412175 -0.74000597 -0.18324232 0.73362684 1.3455729 1.3663926 0.56374836 -0.622067 -1.5784979 -1.9538622 -1.8965073 -1.9972131 -2.6809216][-0.38976479 -0.25555325 -0.59011221 -0.93965316 -0.72786474 0.069326878 0.82658052 1.0914903 0.41248035 -0.76669121 -1.6704295 -1.8244896 -1.5081577 -1.5578928 -2.319319][-0.23119926 -0.33789062 -0.9988997 -1.7403119 -1.897362 -1.37374 -0.60969687 -0.10535479 -0.42367363 -1.3176794 -2.098202 -2.1409671 -1.6564195 -1.620527 -2.3529582][-0.37321568 -0.672452 -1.4871893 -2.3025663 -2.5453134 -2.1892145 -1.5612552 -1.1137362 -1.3086824 -1.9707682 -2.5759702 -2.5268645 -2.010987 -1.9713011 -2.6420107][-0.73418474 -1.0702307 -1.842942 -2.5051639 -2.6140759 -2.3259177 -1.9636312 -1.7962747 -2.0550604 -2.5259218 -2.9205427 -2.8290377 -2.4118035 -2.3828781 -2.8603871][-0.79090381 -1.0453429 -1.6995308 -2.2162137 -2.1932712 -1.8670921 -1.6241775 -1.6506393 -1.9820516 -2.3555729 -2.6356044 -2.6744361 -2.6172795 -2.8283489 -3.2798572][-0.3607111 -0.57388973 -1.2088673 -1.7669227 -1.7632527 -1.4138806 -1.1721597 -1.2171428 -1.5220582 -1.8257849 -2.0012345 -2.0623262 -2.2352593 -2.7287049 -3.4227858][0.069153309 -0.15730333 -0.85558772 -1.5833075 -1.7605021 -1.4930804 -1.2496729 -1.2381778 -1.4140744 -1.5628963 -1.5183985 -1.302907 -1.3206792 -1.8427889 -2.7862258][0.10977364 -0.068376064 -0.72813416 -1.5543506 -1.9527149 -1.8410342 -1.6396003 -1.5963457 -1.6728578 -1.72522 -1.5031042 -0.95014024 -0.66714644 -1.0234563 -2.0159175]]...]
INFO - root - 2017-12-07 04:10:11.955146: step 3610, loss = 0.95, batch loss = 0.87 (10.2 examples/sec; 0.785 sec/batch; 71h:40m:48s remains)
INFO - root - 2017-12-07 04:10:19.662377: step 3620, loss = 0.60, batch loss = 0.53 (10.0 examples/sec; 0.799 sec/batch; 73h:00m:45s remains)
INFO - root - 2017-12-07 04:10:27.340350: step 3630, loss = 0.75, batch loss = 0.67 (14.8 examples/sec; 0.540 sec/batch; 49h:21m:59s remains)
INFO - root - 2017-12-07 04:10:35.103159: step 3640, loss = 0.64, batch loss = 0.57 (10.3 examples/sec; 0.779 sec/batch; 71h:10m:52s remains)
INFO - root - 2017-12-07 04:10:42.899096: step 3650, loss = 0.97, batch loss = 0.89 (10.6 examples/sec; 0.754 sec/batch; 68h:53m:23s remains)
INFO - root - 2017-12-07 04:10:50.663291: step 3660, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.786 sec/batch; 71h:49m:59s remains)
INFO - root - 2017-12-07 04:10:58.376641: step 3670, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.760 sec/batch; 69h:22m:29s remains)
INFO - root - 2017-12-07 04:11:06.163737: step 3680, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.761 sec/batch; 69h:32m:36s remains)
INFO - root - 2017-12-07 04:11:13.740956: step 3690, loss = 0.82, batch loss = 0.74 (10.8 examples/sec; 0.738 sec/batch; 67h:25m:34s remains)
INFO - root - 2017-12-07 04:11:21.395143: step 3700, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.771 sec/batch; 70h:23m:42s remains)
2017-12-07 04:11:21.997142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1546154 -4.4325104 -4.2724352 -3.6490104 -3.1190758 -3.0278265 -3.0494528 -2.8818429 -2.6503143 -2.5716834 -2.6277022 -2.6553347 -2.7389069 -2.8777189 -2.9644434][-3.8147562 -4.104804 -4.0219865 -3.6206431 -3.2858768 -3.2827983 -3.2743979 -3.0472243 -2.5959735 -2.1792262 -2.0936198 -2.231174 -2.5357285 -2.84128 -2.9286041][-3.4230289 -3.6429157 -3.6419892 -3.5000157 -3.4074001 -3.4913554 -3.4820158 -3.2072353 -2.6228452 -2.0073166 -1.895355 -2.1936376 -2.6646867 -3.0583451 -3.0765944][-2.7766194 -2.7518511 -2.7204332 -2.7314081 -2.8359079 -2.9882035 -3.0351975 -2.8481956 -2.4042377 -2.0145423 -2.1026213 -2.5552216 -2.9769258 -3.2033091 -3.017724][-2.6994619 -2.474977 -2.3551519 -2.4420536 -2.6950266 -2.8920746 -2.98067 -2.9005613 -2.7507477 -2.7445109 -3.0306745 -3.4600716 -3.5849347 -3.3887844 -2.8727784][-3.1501856 -2.8621426 -2.6329236 -2.7362609 -3.0763946 -3.2510495 -3.2683287 -3.1610742 -3.1546175 -3.3375525 -3.6005898 -3.8192863 -3.6072712 -3.1198387 -2.5688572][-3.0492187 -2.6846302 -2.3050897 -2.3193302 -2.6270962 -2.7337148 -2.600451 -2.3923609 -2.4181089 -2.6929665 -2.9265249 -3.0366008 -2.7543225 -2.4260736 -2.3313577][-2.0543942 -1.5555999 -1.0062859 -0.88689566 -1.1423495 -1.2501252 -1.1011896 -0.97398734 -1.1502142 -1.5595744 -1.8224981 -1.9336023 -1.7925429 -1.860863 -2.3713734][-1.5286295 -0.90521646 -0.18490267 0.088722706 -0.1409483 -0.35130739 -0.30962372 -0.28314543 -0.50984573 -0.90084887 -1.0830331 -1.0736637 -0.97714233 -1.3232894 -2.2587152][-2.5428846 -2.0543635 -1.4638147 -1.2465441 -1.4868884 -1.793575 -1.8530715 -1.8197083 -1.8933434 -1.9857986 -1.8685966 -1.5376854 -1.2345226 -1.4371331 -2.2427161][-3.7315643 -3.489253 -3.1431909 -3.0181522 -3.1995547 -3.4289517 -3.464848 -3.4136796 -3.4038534 -3.3611817 -3.1551073 -2.7366681 -2.3436649 -2.3167903 -2.7545969][-4.0013795 -3.9085903 -3.7325175 -3.6538444 -3.7467868 -3.8431907 -3.8202202 -3.7763348 -3.7830873 -3.7710607 -3.6606073 -3.3941827 -3.1181874 -3.0519974 -3.2685809][-3.7958162 -3.8101368 -3.7894785 -3.799758 -3.8727984 -3.9010699 -3.8561649 -3.8230939 -3.8376279 -3.8448951 -3.7859373 -3.6391926 -3.4755082 -3.4018021 -3.4771008][-3.5705197 -3.6241388 -3.6617835 -3.7195122 -3.7865429 -3.7859695 -3.7272429 -3.6752732 -3.6679416 -3.6715972 -3.6527438 -3.61421 -3.5747056 -3.5541382 -3.5622647][-3.8218224 -3.8215466 -3.8064387 -3.8178768 -3.82694 -3.7851739 -3.6976533 -3.604188 -3.5462008 -3.5079865 -3.4955473 -3.5166082 -3.5591626 -3.6011391 -3.6098468]]...]
INFO - root - 2017-12-07 04:11:29.702446: step 3710, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 69h:46m:11s remains)
INFO - root - 2017-12-07 04:11:37.355700: step 3720, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.788 sec/batch; 71h:57m:14s remains)
INFO - root - 2017-12-07 04:11:44.832736: step 3730, loss = 0.63, batch loss = 0.55 (13.7 examples/sec; 0.585 sec/batch; 53h:26m:17s remains)
INFO - root - 2017-12-07 04:11:52.557336: step 3740, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.768 sec/batch; 70h:06m:09s remains)
INFO - root - 2017-12-07 04:12:00.199690: step 3750, loss = 1.09, batch loss = 1.02 (10.5 examples/sec; 0.762 sec/batch; 69h:37m:31s remains)
INFO - root - 2017-12-07 04:12:07.791824: step 3760, loss = 0.76, batch loss = 0.69 (10.8 examples/sec; 0.742 sec/batch; 67h:46m:40s remains)
INFO - root - 2017-12-07 04:12:15.448727: step 3770, loss = 0.83, batch loss = 0.76 (10.8 examples/sec; 0.741 sec/batch; 67h:39m:31s remains)
INFO - root - 2017-12-07 04:12:23.195457: step 3780, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 70h:55m:27s remains)
INFO - root - 2017-12-07 04:12:30.820320: step 3790, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.775 sec/batch; 70h:48m:31s remains)
INFO - root - 2017-12-07 04:12:38.593545: step 3800, loss = 0.82, batch loss = 0.75 (9.8 examples/sec; 0.813 sec/batch; 74h:15m:46s remains)
2017-12-07 04:12:39.282544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5669875 -4.50827 -4.3380919 -4.1314955 -3.9939704 -3.9369617 -3.9095685 -3.8073373 -3.6649144 -3.674082 -3.8335118 -3.9424927 -3.6518106 -2.9915359 -2.3645182][-5.071229 -5.0586028 -4.8707914 -4.6513243 -4.5030818 -4.41012 -4.2828288 -4.0758934 -3.8833184 -3.9360509 -4.17958 -4.2932053 -3.7936821 -2.7610123 -1.7656271][-5.299047 -5.2921352 -5.1090837 -4.9604445 -4.8864622 -4.813827 -4.6680813 -4.5062718 -4.4525347 -4.6433644 -4.9948368 -5.0777264 -4.3522859 -2.9742861 -1.611022][-4.9165578 -4.8247929 -4.6164427 -4.4863076 -4.4213271 -4.3153119 -4.1101875 -3.9867482 -4.1208549 -4.5669127 -5.1970968 -5.4198008 -4.6384568 -3.088783 -1.5005815][-4.3787823 -4.062767 -3.7018402 -3.5176713 -3.4577103 -3.3583398 -3.0714083 -2.9226117 -3.1593266 -3.8218293 -4.7526064 -5.1710768 -4.4805322 -2.9456446 -1.3051639][-3.8896315 -3.3280587 -2.7384763 -2.3596444 -2.1497688 -1.9738636 -1.7008812 -1.8121331 -2.44296 -3.4083252 -4.5294089 -4.9583178 -4.26313 -2.7590439 -1.1497993][-3.8702536 -3.2123322 -2.4091892 -1.5774031 -0.76193047 0.011519432 0.50470877 0.010775566 -1.3039618 -2.9083147 -4.4704294 -5.091188 -4.4743891 -3.0168777 -1.4291775][-4.5510612 -4.1194248 -3.4480219 -2.4291971 -1.0498958 0.63240957 2.0627794 1.9716368 0.60954762 -1.2763851 -3.2610114 -4.314199 -4.1044531 -3.0707374 -1.8665276][-4.9397454 -4.8457446 -4.5975046 -4.0418019 -3.0079885 -1.4357009 0.21913338 0.75325251 0.2001152 -0.94113111 -2.3852961 -3.1620975 -3.0207944 -2.4131334 -1.8591745][-4.5741911 -4.6110973 -4.6277113 -4.4824424 -4.0044975 -3.1377907 -2.1863434 -1.9179733 -2.1944351 -2.6566405 -3.2464085 -3.2664099 -2.7185714 -2.134927 -1.9292488][-3.6221559 -3.6339922 -3.802743 -3.9255955 -3.7799594 -3.3464885 -2.90497 -2.8978 -3.1449134 -3.3888879 -3.6246269 -3.3922462 -2.8304152 -2.4106376 -2.402638][-2.7227263 -2.5218229 -2.6521254 -2.9549074 -3.1176977 -3.0369725 -2.9302325 -3.0074282 -3.1148925 -3.2001686 -3.2550378 -3.0431175 -2.7550864 -2.6796203 -2.8749242][-3.018605 -2.7628741 -2.7593973 -2.9069924 -3.0420861 -3.0852518 -3.2037864 -3.393774 -3.4731305 -3.4566731 -3.358624 -3.1485724 -3.0270441 -3.0977583 -3.3091996][-3.6135201 -3.5524335 -3.5784326 -3.6105828 -3.6116431 -3.5326519 -3.5974767 -3.7371342 -3.7762213 -3.7370744 -3.6260602 -3.4642282 -3.409184 -3.475982 -3.59084][-3.7286911 -3.7315111 -3.7878754 -3.8410087 -3.8633416 -3.7839286 -3.7846956 -3.8310227 -3.815167 -3.7680621 -3.6783009 -3.5614264 -3.5195093 -3.5450146 -3.5840182]]...]
INFO - root - 2017-12-07 04:12:46.910943: step 3810, loss = 0.77, batch loss = 0.70 (11.0 examples/sec; 0.726 sec/batch; 66h:17m:18s remains)
INFO - root - 2017-12-07 04:12:54.515362: step 3820, loss = 0.87, batch loss = 0.80 (10.2 examples/sec; 0.783 sec/batch; 71h:29m:49s remains)
INFO - root - 2017-12-07 04:13:01.855274: step 3830, loss = 0.97, batch loss = 0.90 (13.9 examples/sec; 0.574 sec/batch; 52h:22m:02s remains)
INFO - root - 2017-12-07 04:13:09.406782: step 3840, loss = 0.88, batch loss = 0.80 (10.8 examples/sec; 0.743 sec/batch; 67h:50m:06s remains)
INFO - root - 2017-12-07 04:13:16.922813: step 3850, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.764 sec/batch; 69h:42m:59s remains)
INFO - root - 2017-12-07 04:13:24.609113: step 3860, loss = 0.82, batch loss = 0.74 (10.5 examples/sec; 0.760 sec/batch; 69h:25m:13s remains)
INFO - root - 2017-12-07 04:13:32.275018: step 3870, loss = 0.92, batch loss = 0.84 (10.3 examples/sec; 0.774 sec/batch; 70h:40m:49s remains)
INFO - root - 2017-12-07 04:13:39.966855: step 3880, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.787 sec/batch; 71h:48m:55s remains)
INFO - root - 2017-12-07 04:13:47.631109: step 3890, loss = 0.95, batch loss = 0.88 (10.4 examples/sec; 0.772 sec/batch; 70h:29m:39s remains)
INFO - root - 2017-12-07 04:13:55.496376: step 3900, loss = 0.72, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 72h:53m:54s remains)
2017-12-07 04:13:56.123736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1973472 -2.3481452 -2.0694985 -2.923182 -4.1304617 -4.3995037 -4.2153397 -3.889302 -3.3325119 -3.0030332 -3.0082426 -3.0925927 -3.2085042 -3.2600479 -3.2734056][-3.2258711 -2.1688762 -1.6185129 -2.2513857 -3.3364131 -3.6144264 -3.5006206 -3.2192216 -2.7470975 -2.7228541 -3.0797448 -3.23386 -3.2897227 -3.3739946 -3.409183][-3.2562649 -2.18146 -1.3536632 -1.4969919 -2.3291852 -2.8149376 -3.0082569 -2.9544826 -2.7183971 -2.9474013 -3.3806465 -3.3432016 -3.2285867 -3.3349767 -3.5044518][-3.4117332 -2.4416275 -1.4499552 -1.1248331 -1.7363768 -2.5035238 -2.9028568 -2.8632717 -2.6855679 -2.9985285 -3.3749886 -3.2293005 -3.1167057 -3.3562381 -3.6454592][-3.3155477 -2.4822721 -1.4912889 -0.89177895 -1.2716064 -2.1061106 -2.469255 -2.325913 -2.2015355 -2.6055124 -3.0067925 -2.9620638 -3.0355706 -3.3745742 -3.6418128][-3.0758662 -2.5208354 -1.8006747 -1.2021906 -1.3943901 -2.1201727 -2.3450651 -2.0711684 -1.915478 -2.2351532 -2.5297036 -2.5415187 -2.7823865 -3.2061763 -3.5574417][-2.9098294 -2.6187322 -2.2244596 -1.7473621 -1.8215823 -2.385617 -2.5632014 -2.2791266 -2.0486941 -2.1603894 -2.2674146 -2.2690392 -2.5231597 -2.8617942 -3.1962566][-2.7372766 -2.5669246 -2.3920159 -2.0249152 -2.0451155 -2.4407954 -2.6077352 -2.4276419 -2.2452161 -2.3002038 -2.3410933 -2.3079417 -2.4275415 -2.6396227 -2.8762007][-2.5666301 -2.4342284 -2.4346223 -2.2675397 -2.3715868 -2.6820908 -2.8322096 -2.7072048 -2.5211763 -2.5105462 -2.4725294 -2.29837 -2.2158756 -2.4070785 -2.7405806][-2.4154105 -2.2237 -2.3117549 -2.3611114 -2.6333861 -2.9732018 -3.1054664 -2.9535682 -2.6927223 -2.6132617 -2.523262 -2.2242711 -1.9869576 -2.1905119 -2.611424][-2.4302487 -2.1314607 -2.1791179 -2.3277607 -2.7095866 -3.1021836 -3.2333035 -3.0235319 -2.6721563 -2.476712 -2.342694 -2.0894456 -1.9085276 -2.1774559 -2.6193128][-2.6279593 -2.2370198 -2.1974311 -2.3744988 -2.8125639 -3.2685335 -3.426754 -3.1614058 -2.6947224 -2.3011625 -2.0370929 -1.7903113 -1.6915174 -2.03085 -2.5475097][-2.8377481 -2.4066072 -2.2770276 -2.4377775 -2.8841252 -3.3880477 -3.5962365 -3.3652449 -2.8819861 -2.3974009 -2.0301843 -1.7423699 -1.6247156 -1.8885813 -2.3873634][-3.091037 -2.7487044 -2.5830927 -2.6689072 -3.03018 -3.475702 -3.6749792 -3.5241861 -3.1687593 -2.803225 -2.5392904 -2.3269129 -2.247678 -2.431829 -2.7903743][-3.4140892 -3.2056336 -3.0322104 -3.0253663 -3.2554951 -3.5750635 -3.7273724 -3.6603646 -3.470341 -3.2478952 -3.0612187 -2.8660178 -2.7673352 -2.8757849 -3.1297791]]...]
INFO - root - 2017-12-07 04:14:03.753333: step 3910, loss = 1.00, batch loss = 0.92 (10.9 examples/sec; 0.734 sec/batch; 66h:57m:44s remains)
INFO - root - 2017-12-07 04:14:11.398414: step 3920, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.746 sec/batch; 68h:03m:05s remains)
INFO - root - 2017-12-07 04:14:18.882076: step 3930, loss = 0.78, batch loss = 0.71 (14.0 examples/sec; 0.572 sec/batch; 52h:11m:06s remains)
INFO - root - 2017-12-07 04:14:26.638224: step 3940, loss = 0.93, batch loss = 0.86 (10.4 examples/sec; 0.770 sec/batch; 70h:14m:00s remains)
INFO - root - 2017-12-07 04:14:34.182634: step 3950, loss = 1.06, batch loss = 0.98 (10.4 examples/sec; 0.771 sec/batch; 70h:22m:51s remains)
INFO - root - 2017-12-07 04:14:41.999257: step 3960, loss = 0.69, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 71h:25m:31s remains)
INFO - root - 2017-12-07 04:14:49.847427: step 3970, loss = 0.79, batch loss = 0.72 (9.9 examples/sec; 0.807 sec/batch; 73h:37m:57s remains)
INFO - root - 2017-12-07 04:14:57.681648: step 3980, loss = 1.01, batch loss = 0.94 (10.3 examples/sec; 0.779 sec/batch; 71h:07m:44s remains)
INFO - root - 2017-12-07 04:15:05.476083: step 3990, loss = 0.76, batch loss = 0.68 (10.3 examples/sec; 0.779 sec/batch; 71h:06m:04s remains)
INFO - root - 2017-12-07 04:15:13.172596: step 4000, loss = 1.04, batch loss = 0.97 (10.6 examples/sec; 0.756 sec/batch; 69h:01m:38s remains)
2017-12-07 04:15:13.784283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7707057 -3.4401126 -3.8544502 -4.033514 -4.0592613 -4.0897288 -4.248096 -4.4008904 -4.3815293 -4.2139935 -3.879817 -3.4600296 -3.3862753 -3.5412881 -3.7306437][-2.8478754 -3.6115093 -4.12589 -4.2507167 -4.0977397 -3.950773 -3.979053 -4.117599 -4.2160554 -4.1849246 -3.8803 -3.442009 -3.364356 -3.4879103 -3.6072061][-2.9785249 -3.6100476 -4.0421147 -4.0786171 -3.8583219 -3.6201024 -3.5163448 -3.6272984 -3.8911352 -4.0824265 -3.9196033 -3.5515175 -3.4772511 -3.5602961 -3.592984][-3.0712771 -3.4649107 -3.753974 -3.7176723 -3.4626074 -3.1609941 -2.9919472 -3.1589489 -3.6042476 -3.9646823 -3.921288 -3.6283183 -3.5673585 -3.631716 -3.5927238][-3.0484257 -3.242656 -3.4377272 -3.4222617 -3.1829624 -2.8449678 -2.625093 -2.8143587 -3.3361297 -3.7634163 -3.8047209 -3.6052518 -3.6186934 -3.7292948 -3.6395054][-3.0919206 -3.1063044 -3.2342205 -3.3232195 -3.1428106 -2.7556391 -2.4383008 -2.5480313 -3.0389969 -3.4619434 -3.5648367 -3.4846489 -3.6248624 -3.8307264 -3.7365465][-3.3172035 -3.1458707 -3.2065997 -3.4039483 -3.2718153 -2.7699833 -2.245389 -2.18455 -2.65769 -3.1508269 -3.3801055 -3.4348447 -3.6697106 -3.9289479 -3.8705473][-3.6541879 -3.29945 -3.2563956 -3.5018673 -3.3944769 -2.7720582 -2.0002723 -1.7281539 -2.2258158 -2.8998911 -3.3694191 -3.6045194 -3.8647237 -4.0628853 -3.9818535][-3.8936639 -3.4431698 -3.3134043 -3.553967 -3.479146 -2.840903 -1.9336944 -1.4945261 -1.9827058 -2.7752533 -3.4530349 -3.8549924 -4.1169815 -4.2141085 -4.0382743][-3.81165 -3.3929706 -3.2813966 -3.5503669 -3.5748446 -3.0912933 -2.2641609 -1.7584941 -2.0945683 -2.8093843 -3.5511725 -4.04766 -4.2682996 -4.2519355 -3.9425068][-3.4696155 -3.1897092 -3.2124157 -3.5659397 -3.7246127 -3.4653378 -2.8299 -2.2934918 -2.3733063 -2.8581934 -3.5460615 -4.0907984 -4.2659459 -4.1845717 -3.76223][-3.004746 -2.9372637 -3.1447208 -3.5740459 -3.8261988 -3.7369525 -3.2927141 -2.7714038 -2.633553 -2.8986034 -3.5074105 -4.0743222 -4.2135139 -4.0798821 -3.5526109][-2.672121 -2.7565107 -3.0547023 -3.4849854 -3.7620883 -3.7643542 -3.5181189 -3.1264219 -2.9056606 -3.0250483 -3.5418873 -4.0705333 -4.1570439 -3.968452 -3.3502998][-2.6200838 -2.7295675 -2.964952 -3.3005273 -3.5576539 -3.63344 -3.580332 -3.3806026 -3.1879253 -3.2342789 -3.6201887 -4.0374284 -4.0509715 -3.8666565 -3.2697678][-2.7702849 -2.7762852 -2.8498383 -3.0776916 -3.3632932 -3.5555203 -3.6508503 -3.574456 -3.4126139 -3.4196768 -3.6840572 -3.9532907 -3.9018459 -3.7760043 -3.2996881]]...]
INFO - root - 2017-12-07 04:15:21.566846: step 4010, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.786 sec/batch; 71h:45m:09s remains)
INFO - root - 2017-12-07 04:15:29.224888: step 4020, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.760 sec/batch; 69h:22m:58s remains)
INFO - root - 2017-12-07 04:15:36.645201: step 4030, loss = 0.83, batch loss = 0.75 (14.9 examples/sec; 0.539 sec/batch; 49h:08m:55s remains)
INFO - root - 2017-12-07 04:15:44.288340: step 4040, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 69h:48m:51s remains)
INFO - root - 2017-12-07 04:15:51.920590: step 4050, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.761 sec/batch; 69h:28m:30s remains)
INFO - root - 2017-12-07 04:15:59.612137: step 4060, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.760 sec/batch; 69h:20m:58s remains)
INFO - root - 2017-12-07 04:16:07.372769: step 4070, loss = 0.76, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 74h:58m:39s remains)
INFO - root - 2017-12-07 04:16:15.079374: step 4080, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.769 sec/batch; 70h:06m:52s remains)
INFO - root - 2017-12-07 04:16:22.689911: step 4090, loss = 0.63, batch loss = 0.56 (10.6 examples/sec; 0.752 sec/batch; 68h:36m:07s remains)
INFO - root - 2017-12-07 04:16:30.312074: step 4100, loss = 1.03, batch loss = 0.95 (10.1 examples/sec; 0.791 sec/batch; 72h:10m:14s remains)
2017-12-07 04:16:31.030569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9430144 -3.8620558 -3.8398657 -3.876303 -3.9341614 -3.9355111 -3.89466 -3.9261732 -4.0289955 -4.1383438 -4.151691 -4.03532 -3.83821 -3.7275121 -3.6918256][-3.607523 -3.5200527 -3.4545908 -3.4964652 -3.5498409 -3.4905899 -3.4198105 -3.454808 -3.5905929 -3.7223835 -3.7081394 -3.5583134 -3.3800783 -3.4022086 -3.4922652][-2.9445658 -2.7757378 -2.5615377 -2.5255399 -2.5226278 -2.4603081 -2.4479446 -2.5070348 -2.6547918 -2.7816215 -2.7669964 -2.6514893 -2.5443482 -2.6931319 -2.9049783][-1.9866412 -1.6832986 -1.3526657 -1.2650299 -1.2282715 -1.2506802 -1.363028 -1.4459269 -1.5720179 -1.6682358 -1.6580644 -1.6120594 -1.5946565 -1.8720846 -2.1928108][-1.0691538 -0.69265413 -0.39837122 -0.326056 -0.25631666 -0.27157927 -0.29166126 -0.21318197 -0.27480888 -0.42160559 -0.61672235 -0.82173967 -1.0287786 -1.4570208 -1.8043947][-0.60412908 -0.18099833 0.085948944 0.21975899 0.37698317 0.5245204 0.80213404 1.0595126 0.88934469 0.43319988 -0.18955898 -0.75937128 -1.1838169 -1.6291115 -1.8151913][-0.65585184 -0.20328236 0.18329048 0.56152916 0.94117165 1.3737373 1.962893 2.2032585 1.6261559 0.71258974 -0.30269432 -1.0855563 -1.530575 -1.7766759 -1.6791844][-1.0236666 -0.6288805 -0.14283657 0.40963364 0.94787312 1.5586438 2.2128835 2.2137079 1.3231053 0.22550011 -0.85455036 -1.6027906 -1.8750422 -1.8281856 -1.5525427][-1.5947349 -1.3876393 -0.9302156 -0.32008886 0.28094244 0.92737007 1.408318 1.1510253 0.26632547 -0.63371658 -1.4714148 -2.0229425 -2.0899363 -1.8983274 -1.7088366][-2.1844833 -2.1223216 -1.6897106 -1.0909879 -0.52775836 -0.0089731216 0.2100563 -0.15511227 -0.78295493 -1.249006 -1.6809144 -1.9908359 -1.8740253 -1.7519362 -1.9198263][-2.5074797 -2.4459012 -1.9905896 -1.5009477 -1.1374123 -0.89640546 -0.96554947 -1.3811355 -1.7249715 -1.7801073 -1.7908041 -1.7620928 -1.4724474 -1.5364256 -2.0718479][-2.2956407 -2.1885262 -1.7768166 -1.4914055 -1.3671663 -1.3298266 -1.5322533 -1.9338653 -2.1597571 -2.0748465 -1.8869989 -1.650511 -1.3776448 -1.6899536 -2.4262431][-1.5749168 -1.3874822 -1.0734632 -0.9998858 -1.0489728 -1.1137736 -1.3568001 -1.719676 -1.92995 -1.8647668 -1.6548202 -1.4131868 -1.3368073 -1.8227766 -2.6197262][-0.55662131 -0.2771244 -0.10116148 -0.22381639 -0.37871218 -0.52085805 -0.79401731 -1.1050293 -1.3301587 -1.3411541 -1.2019637 -1.0019484 -0.98729181 -1.4104412 -2.1658347][0.27119827 0.54232883 0.52143574 0.26995516 0.097567081 -0.10974646 -0.45072484 -0.72869873 -0.98349547 -1.0560172 -0.93528676 -0.62711787 -0.41033506 -0.64542007 -1.4463017]]...]
INFO - root - 2017-12-07 04:16:38.625703: step 4110, loss = 0.84, batch loss = 0.77 (10.1 examples/sec; 0.789 sec/batch; 72h:00m:50s remains)
INFO - root - 2017-12-07 04:16:46.367837: step 4120, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.769 sec/batch; 70h:06m:08s remains)
INFO - root - 2017-12-07 04:16:53.844003: step 4130, loss = 0.83, batch loss = 0.76 (16.9 examples/sec; 0.474 sec/batch; 43h:14m:25s remains)
INFO - root - 2017-12-07 04:17:01.543389: step 4140, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.760 sec/batch; 69h:18m:09s remains)
INFO - root - 2017-12-07 04:17:09.053828: step 4150, loss = 0.68, batch loss = 0.61 (10.7 examples/sec; 0.750 sec/batch; 68h:23m:43s remains)
INFO - root - 2017-12-07 04:17:16.784628: step 4160, loss = 1.01, batch loss = 0.94 (10.1 examples/sec; 0.789 sec/batch; 71h:56m:54s remains)
INFO - root - 2017-12-07 04:17:24.524662: step 4170, loss = 1.30, batch loss = 1.23 (10.4 examples/sec; 0.771 sec/batch; 70h:18m:51s remains)
INFO - root - 2017-12-07 04:17:32.178174: step 4180, loss = 0.88, batch loss = 0.80 (10.3 examples/sec; 0.780 sec/batch; 71h:06m:24s remains)
INFO - root - 2017-12-07 04:17:39.881166: step 4190, loss = 1.11, batch loss = 1.04 (10.1 examples/sec; 0.791 sec/batch; 72h:09m:44s remains)
INFO - root - 2017-12-07 04:17:47.516321: step 4200, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.758 sec/batch; 69h:09m:32s remains)
2017-12-07 04:17:48.160849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8169191 -3.7606208 -4.0248218 -4.1751208 -4.3124514 -4.2892432 -3.9532211 -3.2738843 -2.7022169 -2.8381963 -3.6306775 -4.5237975 -5.0012493 -4.6641736 -3.7669156][-3.6281574 -3.6218991 -3.8590877 -3.9335132 -4.0325308 -4.0703049 -3.7771363 -3.1351638 -2.598814 -2.7768674 -3.6424916 -4.5757456 -5.0534625 -4.681829 -3.7197618][-3.5853612 -3.6018195 -3.7932105 -3.8296092 -3.9289455 -4.0242519 -3.7867866 -3.1621947 -2.5933943 -2.7629752 -3.6347556 -4.5721545 -5.0715828 -4.7214346 -3.7564342][-3.7221465 -3.7152789 -3.8628426 -3.9253442 -4.0263567 -4.1203623 -3.9259746 -3.2918816 -2.6773043 -2.8435078 -3.6762836 -4.5682898 -5.0697155 -4.7740846 -3.8374279][-3.8351724 -3.878406 -4.0423932 -4.1292162 -4.1539555 -4.1529446 -3.9753547 -3.3210697 -2.6807032 -2.8733373 -3.6615391 -4.5049839 -5.0229373 -4.7982793 -3.9131351][-3.7018709 -3.8357911 -4.06852 -4.1797729 -4.0836186 -3.9006357 -3.7049494 -3.0619984 -2.4845862 -2.8078578 -3.6060665 -4.4415479 -4.9888058 -4.8012838 -3.9432013][-3.1315002 -3.3184986 -3.5531111 -3.6658561 -3.4739084 -3.1218805 -2.9032357 -2.2936301 -1.902591 -2.4923573 -3.3917704 -4.2908335 -4.9007607 -4.7185645 -3.8818245][-2.5201325 -2.6334572 -2.7653372 -2.8167999 -2.5582514 -2.1028254 -1.8539159 -1.2382107 -1.064461 -1.9766433 -3.0257146 -4.0285845 -4.7237291 -4.5507274 -3.7558327][-2.7567654 -2.7106998 -2.6415422 -2.5425911 -2.2279541 -1.7749364 -1.5175173 -0.83053088 -0.75049567 -1.8235233 -2.930397 -3.9686751 -4.6557193 -4.4341469 -3.6582191][-3.4861786 -3.3759041 -3.2011819 -3.0002232 -2.7076859 -2.3996544 -2.2491682 -1.5303659 -1.3151658 -2.2030196 -3.1658235 -4.1294641 -4.7263927 -4.4202576 -3.6433566][-4.2314205 -4.099288 -3.9038105 -3.694757 -3.4919722 -3.3697157 -3.3611631 -2.6793895 -2.2642367 -2.8167226 -3.5984352 -4.4639049 -4.9532895 -4.54475 -3.7225165][-4.7345657 -4.6274157 -4.4753675 -4.3408251 -4.2862577 -4.2969537 -4.3297634 -3.6597164 -3.0664377 -3.3197222 -3.9587963 -4.7635374 -5.1778722 -4.6892066 -3.8163538][-4.7204924 -4.698092 -4.641767 -4.6340394 -4.7408009 -4.7922387 -4.7699547 -4.0675945 -3.3437824 -3.4252405 -4.016603 -4.8191814 -5.2110882 -4.7137504 -3.8238053][-4.4121504 -4.4670653 -4.4674206 -4.5180192 -4.70661 -4.7072558 -4.6078119 -3.9014249 -3.1348679 -3.1717348 -3.8104694 -4.6603408 -5.0773735 -4.6341391 -3.7554531][-3.8221295 -3.9304476 -3.9835374 -4.0939927 -4.3427291 -4.2919168 -4.1573849 -3.4871659 -2.7354796 -2.8219924 -3.5578933 -4.4320579 -4.8815742 -4.5221267 -3.6603475]]...]
INFO - root - 2017-12-07 04:17:55.932196: step 4210, loss = 0.96, batch loss = 0.89 (10.3 examples/sec; 0.774 sec/batch; 70h:36m:39s remains)
INFO - root - 2017-12-07 04:18:03.584379: step 4220, loss = 1.01, batch loss = 0.93 (10.5 examples/sec; 0.764 sec/batch; 69h:40m:34s remains)
INFO - root - 2017-12-07 04:18:10.970359: step 4230, loss = 0.69, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 49h:24m:24s remains)
INFO - root - 2017-12-07 04:18:18.646252: step 4240, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.762 sec/batch; 69h:31m:30s remains)
INFO - root - 2017-12-07 04:18:26.442306: step 4250, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 72h:43m:00s remains)
INFO - root - 2017-12-07 04:18:34.140809: step 4260, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.783 sec/batch; 71h:24m:50s remains)
INFO - root - 2017-12-07 04:18:41.929543: step 4270, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 73h:06m:25s remains)
INFO - root - 2017-12-07 04:18:49.702819: step 4280, loss = 0.70, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 70h:11m:02s remains)
INFO - root - 2017-12-07 04:18:57.304857: step 4290, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.758 sec/batch; 69h:07m:29s remains)
INFO - root - 2017-12-07 04:19:05.130159: step 4300, loss = 0.80, batch loss = 0.73 (9.9 examples/sec; 0.812 sec/batch; 74h:01m:58s remains)
2017-12-07 04:19:05.731033: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4007616 0.36650991 0.37465191 0.43725729 0.5406909 0.64911413 0.71673536 0.6870079 0.60870218 0.52394104 0.44014835 0.38899183 0.37847853 0.38465929 0.37228775][0.53942108 0.49544954 0.4912138 0.54492426 0.69211006 0.87111712 0.94323778 0.85550356 0.72164059 0.60374594 0.51223421 0.467072 0.47167206 0.4823122 0.46222353][0.71545649 0.68356037 0.6926775 0.73873234 0.90286589 1.1212726 1.1611776 1.0076947 0.83339882 0.6942873 0.593534 0.5533886 0.5799427 0.60083103 0.58325005][0.87707472 0.85829258 0.90448809 0.9663291 1.1251221 1.3292685 1.3114147 1.0983601 0.91791105 0.78441095 0.66845417 0.62168074 0.66130209 0.69624949 0.70196342][1.0251727 1.0458407 1.1283278 1.1996531 1.31604 1.4490156 1.3723016 1.1184874 0.94920969 0.84701729 0.72454786 0.674078 0.7240963 0.7780652 0.82551432][1.1174588 1.1997008 1.3107276 1.3971786 1.482502 1.5373321 1.410327 1.1096869 0.91853952 0.83371067 0.71845293 0.70716381 0.81045341 0.91075373 1.0143142][1.1583962 1.2921367 1.4130902 1.5255284 1.6364698 1.660161 1.4957337 1.1342034 0.864974 0.73611021 0.61763954 0.67381716 0.86120319 1.0239124 1.1857429][1.2264214 1.3593411 1.4130702 1.4745274 1.5731335 1.586133 1.4392853 1.089787 0.79260921 0.6295476 0.49398947 0.58208847 0.8116293 1.0191789 1.2502098][1.3088026 1.3969502 1.3593583 1.3215928 1.3271441 1.259624 1.0878344 0.79623604 0.59450006 0.51004982 0.42606258 0.52817917 0.72525597 0.91264772 1.1644216][1.3642616 1.3839321 1.2865605 1.219296 1.1923637 1.0737872 0.83688259 0.54496765 0.40931511 0.4147439 0.42470694 0.54001093 0.69853687 0.86761713 1.112987][1.3752837 1.3339224 1.1882076 1.0886168 1.0245738 0.8921175 0.67953348 0.46139908 0.39172459 0.43658209 0.47313976 0.5498786 0.67412567 0.86634493 1.1289434][1.4227929 1.3457532 1.1630478 0.996192 0.82011318 0.6082902 0.4347167 0.330822 0.35434246 0.461977 0.51464319 0.53744459 0.6170311 0.8027606 1.0627565][1.4116387 1.323957 1.1585159 0.98364878 0.75749588 0.48586559 0.31306887 0.20665312 0.20071745 0.34078979 0.43888283 0.47649527 0.57162094 0.73594522 0.95484686][1.4058971 1.3256493 1.1962891 1.0578904 0.86107683 0.59742928 0.439157 0.28428698 0.19186687 0.31378889 0.42569923 0.49652719 0.62802219 0.75874472 0.90496111][1.4378428 1.3365045 1.2167044 1.0837274 0.909781 0.68187523 0.56017113 0.39553404 0.27750158 0.40892076 0.52805471 0.61932993 0.77336693 0.86167192 0.92125893]]...]
INFO - root - 2017-12-07 04:19:13.384806: step 4310, loss = 1.09, batch loss = 1.02 (10.8 examples/sec; 0.743 sec/batch; 67h:43m:06s remains)
INFO - root - 2017-12-07 04:19:21.046987: step 4320, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.758 sec/batch; 69h:06m:02s remains)
INFO - root - 2017-12-07 04:19:28.470407: step 4330, loss = 0.81, batch loss = 0.74 (13.4 examples/sec; 0.597 sec/batch; 54h:23m:11s remains)
INFO - root - 2017-12-07 04:19:36.246778: step 4340, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.765 sec/batch; 69h:44m:24s remains)
INFO - root - 2017-12-07 04:19:43.892225: step 4350, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.761 sec/batch; 69h:21m:24s remains)
INFO - root - 2017-12-07 04:19:51.638516: step 4360, loss = 0.91, batch loss = 0.83 (10.5 examples/sec; 0.763 sec/batch; 69h:32m:15s remains)
INFO - root - 2017-12-07 04:19:59.326293: step 4370, loss = 0.94, batch loss = 0.86 (10.2 examples/sec; 0.782 sec/batch; 71h:15m:25s remains)
INFO - root - 2017-12-07 04:20:06.998926: step 4380, loss = 0.88, batch loss = 0.81 (10.7 examples/sec; 0.749 sec/batch; 68h:15m:59s remains)
INFO - root - 2017-12-07 04:20:14.684053: step 4390, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.762 sec/batch; 69h:28m:19s remains)
INFO - root - 2017-12-07 04:20:22.359709: step 4400, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.753 sec/batch; 68h:37m:30s remains)
2017-12-07 04:20:23.018328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3873873 -3.3221235 -3.2309508 -3.2669058 -3.6073303 -4.0409079 -4.2222533 -3.9605203 -3.6940475 -3.8567946 -4.0894322 -4.445755 -4.6896706 -4.1626773 -3.1822095][-3.343184 -3.4069774 -3.3701782 -3.3376679 -3.5723815 -3.9183505 -4.1072974 -3.9017427 -3.5736027 -3.6671982 -3.7992933 -4.1006289 -4.4652615 -4.0530434 -3.16708][-3.0067623 -3.1398015 -3.1540942 -3.0331204 -3.0583725 -3.1871052 -3.2963257 -3.1836166 -2.9406958 -3.1060579 -3.3002486 -3.6094916 -4.0897255 -3.8682604 -3.1211133][-2.6686206 -2.8643904 -2.9708564 -2.8598433 -2.736722 -2.6260691 -2.5670373 -2.5100946 -2.393734 -2.6549273 -2.9893179 -3.3261528 -3.8233652 -3.7870526 -3.151463][-2.5628314 -2.8316951 -3.0676856 -3.0655575 -2.9164882 -2.6081634 -2.3303943 -2.2626908 -2.2605736 -2.5818291 -3.0264859 -3.3520308 -3.7422359 -3.7904351 -3.2091768][-2.5960703 -2.8970485 -3.1836309 -3.2289767 -3.0619798 -2.6316142 -2.1905231 -2.1455989 -2.3029816 -2.7059109 -3.2391486 -3.5793943 -3.8625576 -3.9146128 -3.3139243][-2.7527657 -2.9482195 -3.1538746 -3.1583121 -2.961338 -2.4803324 -1.9310336 -1.8972771 -2.2359636 -2.7423868 -3.3725717 -3.8176274 -4.0519485 -4.0503983 -3.3778229][-2.9551916 -3.0493529 -3.1762176 -3.1650605 -2.9659166 -2.4867897 -1.8393519 -1.7446687 -2.2004294 -2.7643929 -3.4466112 -4.0280752 -4.2447896 -4.1566949 -3.4103966][-2.86932 -2.922472 -3.0382657 -3.0823967 -2.9171934 -2.4777703 -1.8137372 -1.6848273 -2.2455578 -2.8481784 -3.5255811 -4.2158127 -4.4032264 -4.1877837 -3.403965][-2.7017131 -2.7725618 -2.8928418 -2.9721971 -2.7829337 -2.3545408 -1.723381 -1.5948193 -2.2123425 -2.88442 -3.5651274 -4.3001819 -4.4248886 -4.0863791 -3.3086576][-2.6823199 -2.7930503 -2.8849096 -2.9468148 -2.7237284 -2.3134773 -1.7572396 -1.6694882 -2.2889431 -2.9844654 -3.6295087 -4.3012881 -4.3355012 -3.9369459 -3.2179816][-2.5969844 -2.7715588 -2.8449056 -2.8802676 -2.7312002 -2.4381173 -2.0126014 -1.9983349 -2.5846756 -3.231482 -3.799134 -4.3358946 -4.2699647 -3.8486643 -3.1842494][-2.1811819 -2.4022429 -2.5381727 -2.6081479 -2.6424079 -2.5391808 -2.2702913 -2.307111 -2.7904372 -3.3201618 -3.8543482 -4.3699703 -4.2895 -3.8814855 -3.2112136][-1.6473181 -1.7836237 -1.9849634 -2.1086588 -2.2730906 -2.2858016 -2.1278448 -2.198808 -2.6406956 -3.1319418 -3.7053137 -4.2981257 -4.3042283 -3.9332831 -3.1816111][-1.5077806 -1.4728079 -1.7081115 -1.8624873 -2.0005929 -1.9853146 -1.8342245 -1.8159015 -2.2243359 -2.8109121 -3.5325084 -4.2215953 -4.347291 -3.996078 -3.1158359]]...]
INFO - root - 2017-12-07 04:20:30.620173: step 4410, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.762 sec/batch; 69h:28m:58s remains)
INFO - root - 2017-12-07 04:20:38.276787: step 4420, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.755 sec/batch; 68h:48m:46s remains)
INFO - root - 2017-12-07 04:20:45.770717: step 4430, loss = 0.81, batch loss = 0.74 (13.9 examples/sec; 0.575 sec/batch; 52h:24m:59s remains)
INFO - root - 2017-12-07 04:20:53.404022: step 4440, loss = 0.65, batch loss = 0.58 (10.6 examples/sec; 0.755 sec/batch; 68h:47m:12s remains)
INFO - root - 2017-12-07 04:21:01.207779: step 4450, loss = 0.74, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 70h:59m:14s remains)
INFO - root - 2017-12-07 04:21:08.959375: step 4460, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.763 sec/batch; 69h:31m:14s remains)
INFO - root - 2017-12-07 04:21:16.546468: step 4470, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.749 sec/batch; 68h:13m:15s remains)
INFO - root - 2017-12-07 04:21:24.193783: step 4480, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.768 sec/batch; 69h:59m:43s remains)
INFO - root - 2017-12-07 04:21:31.878604: step 4490, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.771 sec/batch; 70h:16m:03s remains)
INFO - root - 2017-12-07 04:21:39.456561: step 4500, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.767 sec/batch; 69h:54m:26s remains)
2017-12-07 04:21:40.113165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9207592 -2.4357252 -2.0290084 -2.2385828 -2.7933669 -3.0172844 -2.8717246 -2.4985518 -2.2892315 -2.3303936 -2.4622889 -2.8061402 -2.8397303 -2.4106965 -2.06186][-2.8296702 -2.3327641 -1.8250465 -2.1523931 -2.9203506 -3.1925824 -2.911608 -2.2415619 -1.892487 -2.0765479 -2.4650517 -3.0020227 -2.9941771 -2.3843744 -1.8757155][-2.9333928 -2.3753469 -1.7260213 -2.073792 -2.8851366 -3.2145858 -2.9481115 -2.0658498 -1.6143992 -1.9908168 -2.6395483 -3.3039951 -3.2434657 -2.5774422 -1.990133][-2.9250636 -2.2933815 -1.5645568 -1.9309239 -2.6619058 -2.9524393 -2.5615711 -1.3532448 -0.94789124 -1.7542582 -2.6924543 -3.320992 -3.1466689 -2.5150802 -1.9515946][-2.6812406 -1.9969592 -1.2930801 -1.659867 -2.2588081 -2.4464235 -1.7398763 0.00066900253 0.26879883 -1.1744814 -2.4982548 -3.0930643 -2.8870878 -2.40698 -1.9584234][-2.3469875 -1.6630511 -1.0570109 -1.3662806 -1.7417433 -1.7263181 -0.49552441 1.9178376 1.8984828 -0.45778918 -2.3455169 -2.9092541 -2.6137662 -2.3304987 -2.077271][-1.959204 -1.4266284 -1.0794294 -1.3875287 -1.5689247 -1.3431118 0.3610878 3.385541 3.0938449 -0.07738018 -2.51337 -3.0828786 -2.5896144 -2.3316674 -2.1509871][-1.6181056 -1.2876081 -1.2425549 -1.6302373 -1.8795362 -1.8980436 -0.28972578 2.9475069 2.8561821 -0.25245714 -2.6817136 -3.1715088 -2.5800886 -2.3123274 -2.0707178][-1.312402 -1.0435445 -1.168591 -1.6744704 -2.1710095 -2.6817219 -1.5607288 1.3504128 1.4834776 -0.97326422 -2.8310757 -2.9796448 -2.291461 -2.0485611 -1.8303208][-0.94684339 -0.58759928 -0.68476105 -1.2304947 -1.9487789 -2.875206 -2.3095315 -0.024215221 0.18879175 -1.5926723 -2.8906279 -2.731931 -2.0143716 -1.839447 -1.7521389][-0.79150796 -0.2858057 -0.17015648 -0.59092259 -1.3862469 -2.5654116 -2.5859365 -1.1341019 -1.0039544 -2.1501698 -2.8878958 -2.4986706 -1.8920023 -1.8972623 -1.9752789][-0.89642906 -0.29933596 0.054243088 -0.14729834 -0.77015066 -1.8888235 -2.3466725 -1.8139069 -1.9603596 -2.5999165 -2.7203703 -2.0562067 -1.6325529 -1.8858066 -2.1499221][-1.1237502 -0.48995662 -0.0085840225 0.019311905 -0.26992512 -1.0721588 -1.625308 -1.6716049 -1.9853723 -2.276701 -1.9457552 -1.2154357 -1.1273758 -1.6938055 -2.2001474][-1.6747789 -1.0401516 -0.50999975 -0.30533886 -0.28225422 -0.70155287 -1.1291862 -1.4034204 -1.7090647 -1.6969709 -1.0478933 -0.40422964 -0.6966567 -1.5450387 -2.2420018][-2.4003913 -1.9209201 -1.4554999 -1.1541951 -0.93082547 -1.0384007 -1.2486234 -1.5022824 -1.7103651 -1.5003283 -0.70244384 -0.1529727 -0.6032989 -1.4912791 -2.1592989]]...]
INFO - root - 2017-12-07 04:21:47.799837: step 4510, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.771 sec/batch; 70h:15m:59s remains)
INFO - root - 2017-12-07 04:21:55.514454: step 4520, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.783 sec/batch; 71h:19m:39s remains)
INFO - root - 2017-12-07 04:22:03.059811: step 4530, loss = 0.94, batch loss = 0.87 (13.4 examples/sec; 0.598 sec/batch; 54h:28m:54s remains)
INFO - root - 2017-12-07 04:22:10.758869: step 4540, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.766 sec/batch; 69h:46m:21s remains)
INFO - root - 2017-12-07 04:22:18.462937: step 4550, loss = 0.83, batch loss = 0.75 (10.2 examples/sec; 0.784 sec/batch; 71h:25m:30s remains)
INFO - root - 2017-12-07 04:22:26.198972: step 4560, loss = 1.02, batch loss = 0.95 (10.3 examples/sec; 0.775 sec/batch; 70h:35m:48s remains)
INFO - root - 2017-12-07 04:22:33.981199: step 4570, loss = 0.67, batch loss = 0.59 (10.5 examples/sec; 0.763 sec/batch; 69h:27m:35s remains)
INFO - root - 2017-12-07 04:22:41.626329: step 4580, loss = 0.93, batch loss = 0.86 (10.2 examples/sec; 0.783 sec/batch; 71h:17m:44s remains)
INFO - root - 2017-12-07 04:22:49.394837: step 4590, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.776 sec/batch; 70h:43m:11s remains)
INFO - root - 2017-12-07 04:22:57.151639: step 4600, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.773 sec/batch; 70h:26m:52s remains)
2017-12-07 04:22:57.879318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3485916 -1.3947752 -1.404362 -1.3917336 -1.3674853 -1.3530467 -1.3572698 -1.3607054 -1.3453956 -1.3496394 -1.4215548 -1.5516386 -1.6594236 -1.7108967 -1.7453923][-2.1084852 -2.195425 -2.2444582 -2.2607517 -2.2627537 -2.2768133 -2.3115525 -2.3451514 -2.3594036 -2.4058411 -2.5257277 -2.64033 -2.6429467 -2.5213675 -2.3936603][-2.5852604 -2.7018695 -2.79358 -2.8308589 -2.8287578 -2.8376184 -2.88997 -2.9650111 -3.0264831 -3.1015744 -3.1775548 -3.1515088 -2.9467063 -2.6132176 -2.3357184][-2.8033457 -2.9728971 -3.1280987 -3.1977019 -3.185147 -3.1619878 -3.18253 -3.2276855 -3.2739446 -3.3382173 -3.3271704 -3.1393003 -2.7433758 -2.2353849 -1.8190212][-3.40678 -3.5931971 -3.7356491 -3.7788453 -3.7648098 -3.7386866 -3.7240725 -3.7073832 -3.6764197 -3.6797948 -3.5717747 -3.2763014 -2.7978239 -2.23566 -1.7543168][-3.9564524 -3.9022689 -3.7938206 -3.6659849 -3.576715 -3.5341663 -3.5100214 -3.4832711 -3.442328 -3.4580927 -3.3820474 -3.1372852 -2.7553616 -2.3211005 -1.9116895][-3.1469395 -2.8289094 -2.4997916 -2.2507863 -2.0948861 -2.0086265 -1.9727371 -1.9968333 -2.105633 -2.3352125 -2.5195661 -2.5243306 -2.3523214 -2.1192336 -1.8425062][-1.3431551 -0.91682935 -0.48942947 -0.17428732 0.007692337 0.13638449 0.22471857 0.23780632 0.099792 -0.20033121 -0.52514076 -0.77019072 -0.90861535 -1.0577879 -1.1613998][-0.61996531 -0.23018551 0.16014576 0.40936804 0.51294327 0.62520218 0.80992174 1.0042109 1.0898414 1.0453005 0.90093374 0.7121067 0.4907856 0.12889814 -0.28986788][-1.1466103 -1.0155027 -0.90898561 -0.9545114 -1.0852916 -1.0890319 -0.89543867 -0.615468 -0.39466429 -0.26801872 -0.25374365 -0.268713 -0.26520586 -0.37015104 -0.55327296][-1.5864265 -1.6304636 -1.7076371 -1.9026594 -2.1488085 -2.2288547 -2.0671263 -1.7826574 -1.5256014 -1.4109666 -1.4261854 -1.436511 -1.3437531 -1.2791748 -1.2245071][-1.9656518 -1.9250603 -1.8990583 -1.9637432 -2.0950325 -2.1565042 -2.0604074 -1.8740692 -1.6698415 -1.6033099 -1.6582937 -1.6556432 -1.5223069 -1.416908 -1.337069][-2.152158 -2.1315939 -2.1221824 -2.1486473 -2.1972077 -2.2051013 -2.115633 -2.0351994 -1.9457405 -1.9436922 -2.0068061 -1.9319489 -1.7165859 -1.5237293 -1.3935778][-2.5753458 -2.5860085 -2.6242437 -2.6608033 -2.7000141 -2.6803317 -2.5502849 -2.4715619 -2.4002793 -2.3756292 -2.3763328 -2.2395911 -1.992362 -1.769366 -1.6435258][-3.0402818 -3.0558882 -3.0902324 -3.1098125 -3.1449842 -3.1480904 -3.0574207 -3.0098891 -2.90409 -2.7657313 -2.634927 -2.4172707 -2.1258614 -1.8634884 -1.7468171]]...]
INFO - root - 2017-12-07 04:23:05.533133: step 4610, loss = 0.65, batch loss = 0.58 (10.3 examples/sec; 0.778 sec/batch; 70h:52m:24s remains)
INFO - root - 2017-12-07 04:23:13.241360: step 4620, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.770 sec/batch; 70h:08m:20s remains)
INFO - root - 2017-12-07 04:23:20.869957: step 4630, loss = 0.81, batch loss = 0.73 (10.3 examples/sec; 0.773 sec/batch; 70h:24m:11s remains)
INFO - root - 2017-12-07 04:23:28.704101: step 4640, loss = 0.68, batch loss = 0.61 (9.7 examples/sec; 0.828 sec/batch; 75h:24m:10s remains)
INFO - root - 2017-12-07 04:23:36.327156: step 4650, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.779 sec/batch; 70h:54m:31s remains)
INFO - root - 2017-12-07 04:23:44.105784: step 4660, loss = 0.93, batch loss = 0.85 (10.3 examples/sec; 0.774 sec/batch; 70h:31m:20s remains)
INFO - root - 2017-12-07 04:23:51.795299: step 4670, loss = 0.58, batch loss = 0.51 (10.4 examples/sec; 0.766 sec/batch; 69h:43m:57s remains)
INFO - root - 2017-12-07 04:23:59.398627: step 4680, loss = 0.88, batch loss = 0.81 (11.0 examples/sec; 0.728 sec/batch; 66h:15m:10s remains)
INFO - root - 2017-12-07 04:24:07.041551: step 4690, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.770 sec/batch; 70h:09m:30s remains)
INFO - root - 2017-12-07 04:24:14.744658: step 4700, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.773 sec/batch; 70h:22m:51s remains)
2017-12-07 04:24:15.361787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.77637672 -0.96276188 -1.528492 -1.9924641 -2.5424795 -3.263195 -3.8007016 -4.0493937 -4.0903206 -3.8846295 -3.4708014 -3.3212178 -3.6016405 -4.0855474 -4.4265022][-1.208621 -1.3639827 -1.8442945 -2.0955133 -2.4266481 -3.0622191 -3.6179905 -3.928863 -3.9873445 -3.7003145 -3.2087452 -3.0208266 -3.3381042 -3.8990405 -4.3887095][-0.90658283 -1.1721981 -1.698971 -1.7966082 -1.7944763 -2.2060788 -2.826488 -3.4118779 -3.614414 -3.2884016 -2.7422669 -2.6080093 -3.0762949 -3.8376205 -4.4998016][-0.56040215 -0.95762181 -1.5836608 -1.5553334 -1.1378074 -1.1302557 -1.7540267 -2.75561 -3.2905293 -3.085737 -2.532618 -2.4174662 -2.991364 -3.9376366 -4.8341146][-0.37623882 -0.72157407 -1.2855923 -0.973048 0.011478424 0.54595375 -0.078587532 -1.4999046 -2.4571013 -2.4759722 -1.9203658 -1.7940805 -2.4149108 -3.4786587 -4.5557103][0.11062765 -0.29727364 -0.72244596 0.0080976486 1.5588865 2.4814925 1.7276115 -0.11566591 -1.4152327 -1.4926639 -0.77362275 -0.560235 -1.3576038 -2.6561465 -3.8979228][0.43862677 -0.1167469 -0.41795158 0.69382334 2.6153426 3.6467085 2.589057 0.31519556 -1.1920712 -1.0902746 0.022398472 0.40706444 -0.59055519 -2.2495515 -3.6985109][0.014551163 -0.616364 -0.80931115 0.53154707 2.6635675 3.7333832 2.4847708 -0.053925991 -1.6968679 -1.3802977 0.17605162 0.7920537 -0.27769566 -2.2993574 -4.0189214][-0.92239261 -1.4955201 -1.6060836 -0.22411013 2.0307899 3.2840223 2.1900096 -0.32735682 -2.0508881 -1.6346362 0.2062521 1.0187597 -0.053346634 -2.2820418 -4.1617951][-1.5930681 -2.0746107 -2.1855187 -1.0154777 1.1030359 2.4450741 1.729363 -0.4092226 -2.1039588 -1.6985116 0.26990652 1.2886333 0.26422071 -1.971246 -3.796587][-1.6195221 -2.0034373 -2.1575849 -1.3815234 0.26828051 1.4655266 1.1487556 -0.37986183 -1.8368227 -1.4375696 0.54099989 1.7046533 0.80157995 -1.2286417 -2.8215659][-1.2111421 -1.4878912 -1.6877723 -1.368402 -0.39517498 0.48509455 0.49196005 -0.40820169 -1.4974303 -1.1441638 0.61469984 1.721581 0.99887609 -0.62902474 -1.8477376][-0.79024768 -0.99479604 -1.2309172 -1.2905209 -0.95308852 -0.46617961 -0.35220766 -0.82085872 -1.5138137 -1.2121227 0.12088203 0.97404003 0.46933413 -0.57667923 -1.2620325][-0.50402784 -0.65418911 -0.89202809 -1.1551266 -1.2382023 -1.1282051 -1.10849 -1.3400412 -1.6716137 -1.4310744 -0.56274438 0.016486645 -0.22090435 -0.67513824 -0.85463738][-0.38185453 -0.47390771 -0.65607953 -0.95192647 -1.2264676 -1.3589907 -1.4596624 -1.5698752 -1.6905704 -1.552654 -1.0632682 -0.70595956 -0.74201369 -0.79686666 -0.66411376]]...]
INFO - root - 2017-12-07 04:24:23.080973: step 4710, loss = 0.72, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 70h:54m:38s remains)
INFO - root - 2017-12-07 04:24:30.793360: step 4720, loss = 0.65, batch loss = 0.58 (10.3 examples/sec; 0.773 sec/batch; 70h:25m:23s remains)
INFO - root - 2017-12-07 04:24:38.307462: step 4730, loss = 0.83, batch loss = 0.75 (10.7 examples/sec; 0.749 sec/batch; 68h:10m:06s remains)
INFO - root - 2017-12-07 04:24:45.926331: step 4740, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.781 sec/batch; 71h:06m:08s remains)
INFO - root - 2017-12-07 04:24:53.563070: step 4750, loss = 0.64, batch loss = 0.57 (10.5 examples/sec; 0.761 sec/batch; 69h:19m:38s remains)
INFO - root - 2017-12-07 04:25:01.333955: step 4760, loss = 0.94, batch loss = 0.87 (10.0 examples/sec; 0.797 sec/batch; 72h:35m:53s remains)
INFO - root - 2017-12-07 04:25:09.038380: step 4770, loss = 0.98, batch loss = 0.91 (10.5 examples/sec; 0.762 sec/batch; 69h:22m:30s remains)
INFO - root - 2017-12-07 04:25:16.609858: step 4780, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.767 sec/batch; 69h:50m:35s remains)
INFO - root - 2017-12-07 04:25:24.251064: step 4790, loss = 0.66, batch loss = 0.59 (10.5 examples/sec; 0.760 sec/batch; 69h:11m:48s remains)
INFO - root - 2017-12-07 04:25:31.830918: step 4800, loss = 0.63, batch loss = 0.55 (10.6 examples/sec; 0.755 sec/batch; 68h:43m:31s remains)
2017-12-07 04:25:32.464059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.535897 -2.69408 -2.8289123 -2.8773582 -2.8078623 -2.6762886 -2.4716949 -2.2647657 -2.2946942 -2.5038795 -2.6823611 -2.5857058 -2.1917732 -1.779695 -1.5956643][-2.2638214 -2.6739483 -3.0124903 -3.132184 -2.9975204 -2.7788959 -2.5350497 -2.2009361 -2.1293194 -2.3708129 -2.6785774 -2.7482398 -2.4616356 -1.9662316 -1.6213877][-1.6491129 -2.4316194 -3.047327 -3.1845477 -2.9341183 -2.6304724 -2.3279693 -1.8426647 -1.6792209 -2.0454514 -2.61566 -2.9714017 -2.8938375 -2.4002256 -1.9028816][-0.97943187 -2.0554767 -2.8788731 -3.0328653 -2.7702003 -2.4482791 -1.9825995 -1.2384119 -0.90238452 -1.4043634 -2.3254049 -3.078939 -3.3317809 -2.9787803 -2.3911343][-0.871361 -1.9868553 -2.7691681 -2.9112206 -2.7298872 -2.4400053 -1.7944448 -0.731302 -0.12524796 -0.63240886 -1.8324172 -2.9617331 -3.5644026 -3.4561117 -2.8719876][-1.1370718 -2.1558557 -2.8048 -2.8968024 -2.7152603 -2.385386 -1.573822 -0.22892809 0.66387081 0.21423578 -1.2138553 -2.6806808 -3.5916762 -3.7238054 -3.156261][-1.5394523 -2.4298248 -2.9747233 -2.9974346 -2.7085638 -2.2724965 -1.2948408 0.33790541 1.5346885 1.1511402 -0.4700067 -2.2440486 -3.4212666 -3.7661111 -3.248858][-2.1516795 -2.8315992 -3.171176 -3.0496736 -2.6986151 -2.2665753 -1.2667556 0.52710581 1.9612079 1.7324138 0.18438625 -1.6364977 -2.9294562 -3.4542794 -3.1299219][-2.81176 -3.2332315 -3.3145227 -3.0589509 -2.7244382 -2.3692596 -1.4927571 0.17594719 1.5815105 1.5436583 0.340405 -1.2060895 -2.3830497 -2.9434314 -2.8399696][-3.3597212 -3.60754 -3.5164025 -3.1571496 -2.7745681 -2.3954704 -1.6568346 -0.34353113 0.78722954 0.82965517 -0.030063629 -1.20895 -2.1505556 -2.599237 -2.597363][-3.5352747 -3.7150562 -3.6268756 -3.2782784 -2.8470392 -2.4145842 -1.8249342 -0.96944022 -0.23505974 -0.21791506 -0.803565 -1.6298923 -2.3310518 -2.634347 -2.6735706][-3.3101273 -3.4260225 -3.4500985 -3.2522426 -2.8258886 -2.3615148 -1.9273064 -1.5221961 -1.2319324 -1.3168302 -1.7024441 -2.2112648 -2.7019484 -2.955174 -3.0927901][-2.8951705 -2.9189446 -3.05264 -3.0790515 -2.7462969 -2.2832499 -1.9617441 -1.8850431 -1.9725945 -2.1648841 -2.3982134 -2.6584044 -2.9964352 -3.3064861 -3.6237874][-2.4451056 -2.3557036 -2.5359588 -2.7761948 -2.6039398 -2.1994793 -1.937284 -2.0403636 -2.3955188 -2.6885562 -2.816252 -2.8460045 -3.0082879 -3.3594108 -3.8915954][-2.1148496 -1.8602552 -1.9840448 -2.3352127 -2.3148553 -1.9732502 -1.6558046 -1.7538488 -2.2628143 -2.7117767 -2.8283384 -2.6730127 -2.6245451 -2.9345417 -3.6210134]]...]
INFO - root - 2017-12-07 04:25:40.331819: step 4810, loss = 0.63, batch loss = 0.56 (10.4 examples/sec; 0.768 sec/batch; 69h:52m:40s remains)
INFO - root - 2017-12-07 04:25:48.005696: step 4820, loss = 0.88, batch loss = 0.81 (10.3 examples/sec; 0.776 sec/batch; 70h:40m:14s remains)
INFO - root - 2017-12-07 04:25:55.486692: step 4830, loss = 0.71, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 72h:18m:05s remains)
INFO - root - 2017-12-07 04:26:03.102712: step 4840, loss = 0.96, batch loss = 0.89 (10.8 examples/sec; 0.743 sec/batch; 67h:39m:44s remains)
INFO - root - 2017-12-07 04:26:11.002279: step 4850, loss = 0.80, batch loss = 0.73 (9.8 examples/sec; 0.813 sec/batch; 74h:01m:43s remains)
INFO - root - 2017-12-07 04:26:18.656797: step 4860, loss = 0.82, batch loss = 0.75 (10.8 examples/sec; 0.739 sec/batch; 67h:14m:33s remains)
INFO - root - 2017-12-07 04:26:26.376446: step 4870, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.758 sec/batch; 69h:00m:54s remains)
INFO - root - 2017-12-07 04:26:34.062057: step 4880, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.773 sec/batch; 70h:22m:50s remains)
INFO - root - 2017-12-07 04:26:41.816938: step 4890, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.745 sec/batch; 67h:47m:13s remains)
INFO - root - 2017-12-07 04:26:49.445985: step 4900, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.760 sec/batch; 69h:11m:13s remains)
2017-12-07 04:26:50.054855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1288686 -3.5540032 -3.7201681 -3.5581563 -2.8462796 -2.1812873 -1.7972164 -1.8366761 -2.7802792 -3.8000832 -4.2836094 -4.6621404 -4.6028352 -4.5593019 -4.6419444][-3.0958724 -3.5090022 -3.6936202 -3.6169138 -3.0090952 -2.4234369 -1.9871366 -1.8777862 -2.7962229 -3.8096585 -4.2674637 -4.6461725 -4.5590739 -4.4775839 -4.4682837][-3.0140724 -3.4262195 -3.6732502 -3.6756139 -3.1619053 -2.6692924 -2.1810408 -1.93174 -2.7625246 -3.66385 -4.1020055 -4.50694 -4.3864036 -4.2522268 -4.1363931][-2.9020472 -3.3576803 -3.7119162 -3.7633314 -3.2552705 -2.7670097 -2.1865795 -1.8385282 -2.5725355 -3.3383312 -3.7822218 -4.2570992 -4.1495852 -4.0236163 -3.8777304][-2.8225284 -3.3312449 -3.7599549 -3.7351546 -3.1021924 -2.5464816 -1.8888893 -1.4900279 -2.1370404 -2.7628226 -3.1991148 -3.8077736 -3.8257329 -3.7841473 -3.7498827][-2.7587512 -3.2939353 -3.7204993 -3.5455413 -2.7432621 -2.1077428 -1.377527 -0.87499046 -1.4272735 -1.9456806 -2.3920107 -3.170681 -3.314168 -3.3441687 -3.5011256][-2.7531815 -3.244869 -3.6175485 -3.339098 -2.4845865 -1.8296702 -1.0384846 -0.40883017 -0.90350509 -1.4150603 -1.9173892 -2.8300509 -2.9501941 -2.9050584 -3.1720686][-2.8259356 -3.2093625 -3.4551585 -3.1390738 -2.4251246 -1.9075949 -1.1704295 -0.55009627 -1.0814393 -1.678762 -2.2161009 -3.1167228 -3.0699637 -2.8213084 -3.0486112][-2.9433222 -3.1748052 -3.2491255 -2.9499226 -2.4228494 -2.0723352 -1.5529358 -1.1755211 -1.8408358 -2.5349212 -3.0200799 -3.7665517 -3.5332065 -3.0861506 -3.2398944][-3.090493 -3.2275696 -3.18681 -2.9199073 -2.4832082 -2.1715345 -1.9196203 -1.8743241 -2.6872537 -3.441509 -3.8178399 -4.3353524 -3.9986351 -3.45158 -3.5819616][-3.2928958 -3.4119072 -3.3446789 -3.1186683 -2.7017817 -2.3624816 -2.2630322 -2.4066422 -3.2070451 -3.895195 -4.1739764 -4.5141053 -4.1756344 -3.6513453 -3.788846][-3.4808016 -3.5670145 -3.508956 -3.367084 -3.0238805 -2.714222 -2.659812 -2.8060989 -3.4307208 -3.9545763 -4.1504736 -4.3701863 -4.0714941 -3.6051984 -3.6761627][-3.512922 -3.4955091 -3.4237118 -3.4078164 -3.2466364 -3.0689511 -3.0242224 -3.0733128 -3.4681594 -3.7992139 -3.9364 -4.0804434 -3.8605118 -3.4942408 -3.42941][-3.2909455 -3.180932 -3.0886073 -3.1998658 -3.1854525 -3.0810533 -3.0074725 -2.9426618 -3.1587954 -3.349838 -3.4536135 -3.5625236 -3.4476569 -3.2483745 -3.1350443][-2.8346276 -2.7405076 -2.6553369 -2.8051553 -2.8002672 -2.6776152 -2.5859842 -2.47972 -2.5913644 -2.7032728 -2.7624953 -2.8165381 -2.7822924 -2.7492259 -2.6700878]]...]
INFO - root - 2017-12-07 04:26:57.712268: step 4910, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.745 sec/batch; 67h:45m:47s remains)
INFO - root - 2017-12-07 04:27:05.358477: step 4920, loss = 0.73, batch loss = 0.66 (10.7 examples/sec; 0.747 sec/batch; 67h:58m:16s remains)
INFO - root - 2017-12-07 04:27:12.809922: step 4930, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.766 sec/batch; 69h:43m:01s remains)
INFO - root - 2017-12-07 04:27:20.499402: step 4940, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.772 sec/batch; 70h:16m:20s remains)
INFO - root - 2017-12-07 04:27:28.152884: step 4950, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.760 sec/batch; 69h:08m:55s remains)
INFO - root - 2017-12-07 04:27:35.792538: step 4960, loss = 1.04, batch loss = 0.96 (10.3 examples/sec; 0.777 sec/batch; 70h:39m:19s remains)
INFO - root - 2017-12-07 04:27:43.587303: step 4970, loss = 0.71, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 70h:46m:08s remains)
INFO - root - 2017-12-07 04:27:51.185683: step 4980, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.770 sec/batch; 70h:02m:18s remains)
INFO - root - 2017-12-07 04:27:58.801581: step 4990, loss = 0.64, batch loss = 0.57 (10.5 examples/sec; 0.764 sec/batch; 69h:31m:27s remains)
INFO - root - 2017-12-07 04:28:06.384638: step 5000, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.757 sec/batch; 68h:54m:00s remains)
2017-12-07 04:28:07.004973: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6592793 -2.5286121 -2.4550929 -2.4353461 -2.4237187 -2.5655289 -2.5898232 -2.5678203 -2.8037238 -3.0157213 -3.0677755 -3.0130265 -2.3557689 -1.6840045 -1.5744193][-2.668961 -2.5900974 -2.6308007 -2.7092063 -2.7078829 -2.8071 -2.7524352 -2.613086 -2.7574778 -2.99185 -3.0626607 -3.0427523 -2.4002576 -1.7404683 -1.7288768][-2.7325807 -2.7480383 -2.9076266 -3.0584044 -3.0478625 -3.0938854 -2.9789109 -2.824357 -2.9950972 -3.3244991 -3.4199576 -3.4107957 -2.7838268 -2.1207874 -2.1110709][-2.9244199 -3.0414329 -3.2574549 -3.4118207 -3.3415136 -3.276052 -3.0727298 -2.9153562 -3.1695111 -3.5667534 -3.6421282 -3.6641955 -3.0944657 -2.4219253 -2.3795183][-3.1950464 -3.2789993 -3.4405141 -3.5574014 -3.4141045 -3.17763 -2.7712922 -2.5074058 -2.8742158 -3.4189606 -3.5907114 -3.7530975 -3.3893156 -2.8224685 -2.7669172][-3.3637407 -3.2562728 -3.2874606 -3.3520088 -3.0949512 -2.6133647 -1.9335532 -1.6006081 -2.1713805 -2.9265075 -3.3175216 -3.7347364 -3.6967833 -3.3704844 -3.3456001][-3.2260082 -2.9352679 -2.9403286 -2.98533 -2.5667922 -1.7540095 -0.735095 -0.41028309 -1.3515184 -2.3986578 -3.0379262 -3.6297073 -3.8023076 -3.6880977 -3.6899643][-2.8034973 -2.4540215 -2.5542431 -2.6232038 -2.1589279 -1.2339249 -0.055582523 0.18312693 -1.0712008 -2.2786503 -3.0543351 -3.6623511 -3.8482244 -3.8165789 -3.737196][-2.3428612 -2.0350883 -2.2662199 -2.4585209 -2.2616117 -1.6500993 -0.67194796 -0.49324775 -1.6387563 -2.596725 -3.214829 -3.6603012 -3.7543793 -3.7417383 -3.6084585][-1.9260085 -1.7451844 -2.1469588 -2.5485082 -2.7470844 -2.5657685 -1.8226466 -1.6166465 -2.4399652 -2.9458976 -3.2024345 -3.3838782 -3.3812349 -3.4030266 -3.2802][-1.83493 -1.8221867 -2.3170528 -2.7828019 -3.1982031 -3.2585344 -2.6589475 -2.3725393 -2.885869 -3.0630112 -3.0541744 -3.0438092 -2.9849353 -3.0554543 -2.9793258][-2.2328081 -2.3619037 -2.7884469 -3.0949116 -3.4188516 -3.4659529 -2.9373062 -2.6068571 -2.8909936 -2.9178634 -2.844516 -2.8027291 -2.7344749 -2.8360934 -2.7895293][-2.8197911 -2.9598241 -3.1765676 -3.2230644 -3.3099937 -3.2319007 -2.772532 -2.4818251 -2.6198878 -2.5949125 -2.5535054 -2.5421495 -2.4680829 -2.5521016 -2.5338387][-3.1959972 -3.2390518 -3.2559128 -3.121511 -3.0195365 -2.856024 -2.5067039 -2.2959065 -2.3420408 -2.2948959 -2.310354 -2.3310022 -2.2251294 -2.2687936 -2.2998121][-3.1733212 -3.1485596 -3.1117997 -2.9742761 -2.8319685 -2.6979485 -2.5036321 -2.3988216 -2.4102838 -2.3643055 -2.4000869 -2.4124944 -2.2886629 -2.2870493 -2.3222425]]...]
INFO - root - 2017-12-07 04:28:14.654381: step 5010, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.760 sec/batch; 69h:08m:12s remains)
INFO - root - 2017-12-07 04:28:22.306556: step 5020, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.766 sec/batch; 69h:41m:15s remains)
INFO - root - 2017-12-07 04:28:29.720385: step 5030, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.754 sec/batch; 68h:36m:11s remains)
INFO - root - 2017-12-07 04:28:37.370021: step 5040, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.765 sec/batch; 69h:35m:35s remains)
INFO - root - 2017-12-07 04:28:45.039690: step 5050, loss = 0.85, batch loss = 0.77 (10.5 examples/sec; 0.759 sec/batch; 69h:01m:22s remains)
INFO - root - 2017-12-07 04:28:52.698162: step 5060, loss = 0.71, batch loss = 0.63 (10.7 examples/sec; 0.746 sec/batch; 67h:50m:01s remains)
INFO - root - 2017-12-07 04:29:00.323160: step 5070, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.772 sec/batch; 70h:10m:16s remains)
INFO - root - 2017-12-07 04:29:08.089435: step 5080, loss = 0.85, batch loss = 0.78 (10.1 examples/sec; 0.791 sec/batch; 71h:56m:23s remains)
INFO - root - 2017-12-07 04:29:15.902955: step 5090, loss = 0.78, batch loss = 0.70 (10.2 examples/sec; 0.785 sec/batch; 71h:21m:06s remains)
INFO - root - 2017-12-07 04:29:23.586256: step 5100, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.777 sec/batch; 70h:42m:09s remains)
2017-12-07 04:29:24.232711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6774306 -1.1577399 -1.184988 -1.4682081 -1.8731935 -2.1018178 -2.2500231 -2.1997054 -1.818198 -1.8162069 -1.771908 -1.1963484 -0.77407479 -0.64902759 -0.47454619][-2.1303036 -1.411417 -1.3777771 -1.62222 -2.0038402 -2.1855326 -2.3372357 -2.3448014 -2.047513 -2.1269169 -2.0065832 -1.370115 -0.95612168 -0.74667931 -0.50502324][-2.2502522 -1.481832 -1.3948963 -1.4748607 -1.7119827 -1.8705554 -2.1827385 -2.4453089 -2.3458426 -2.4563785 -2.26145 -1.6124465 -1.2527087 -1.0187395 -0.72263241][-1.9435148 -1.349952 -1.3500955 -1.3214147 -1.4005194 -1.5590088 -2.1085539 -2.7043962 -2.8024025 -2.856627 -2.5431437 -1.8604558 -1.5373015 -1.3228064 -0.99193764][-1.7294686 -1.4493012 -1.6392353 -1.6249835 -1.6590126 -1.8401389 -2.5118961 -3.2463832 -3.3816161 -3.239944 -2.7519469 -2.0195007 -1.7281773 -1.5399871 -1.1629562][-1.8615341 -1.9814961 -2.3995328 -2.4371102 -2.4812121 -2.6507866 -3.1355505 -3.6689868 -3.7182517 -3.3589797 -2.7639589 -2.0427439 -1.8148937 -1.649967 -1.2499371][-2.146827 -2.5923061 -3.2069142 -3.3494215 -3.3351574 -3.2767408 -3.2365441 -3.3678584 -3.4058626 -3.0683961 -2.6369259 -2.0754955 -1.8628197 -1.6268802 -1.1555693][-2.3596981 -3.0187862 -3.7100825 -3.8756602 -3.6283972 -3.1028357 -2.325098 -1.9490178 -2.0616677 -2.0194016 -2.0302932 -1.8400073 -1.6810553 -1.3629191 -0.82721853][-2.735914 -3.4744964 -4.0134211 -3.9683404 -3.1965079 -1.962429 -0.45088315 0.40654945 0.17165804 -0.30711937 -0.88296509 -1.1783719 -1.1260436 -0.80856323 -0.28029871][-3.0104723 -3.6345813 -3.9629321 -3.6716337 -2.4311032 -0.62605 1.2573667 2.3445325 1.9283352 1.0526295 0.19003153 -0.39000511 -0.41869831 -0.15565348 0.30750084][-3.1714816 -3.5830593 -3.8018661 -3.4106069 -1.9141543 0.17449236 1.9985433 3.0054932 2.5480089 1.6567049 0.93197823 0.29293823 0.099758625 0.17267466 0.44448519][-3.302525 -3.5638697 -3.7629254 -3.2979364 -1.6236644 0.49672747 1.9851384 2.6927981 2.2440529 1.5380139 1.1227369 0.58022261 0.25353384 0.15429497 0.21461296][-3.4644113 -3.5758326 -3.7188666 -3.0772285 -1.2222846 0.77173805 1.804543 2.1730995 1.7404041 1.2382393 1.1209941 0.72225475 0.38512468 0.23264551 0.2007637][-3.5608325 -3.5731108 -3.5398602 -2.5843468 -0.52442265 1.2695956 1.8850117 1.9435577 1.5366879 1.1891446 1.2164111 0.8884716 0.57044888 0.41290379 0.3549695][-3.3128836 -3.2240987 -2.9470272 -1.7019925 0.33379269 1.7432723 1.8998718 1.6498623 1.2433019 1.020709 1.1090379 0.86030149 0.61457682 0.49213648 0.43822241]]...]
INFO - root - 2017-12-07 04:29:31.770460: step 5110, loss = 0.86, batch loss = 0.78 (10.4 examples/sec; 0.767 sec/batch; 69h:47m:02s remains)
INFO - root - 2017-12-07 04:29:39.438347: step 5120, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.766 sec/batch; 69h:37m:52s remains)
INFO - root - 2017-12-07 04:29:46.971127: step 5130, loss = 0.87, batch loss = 0.79 (10.5 examples/sec; 0.759 sec/batch; 69h:00m:45s remains)
INFO - root - 2017-12-07 04:29:54.654804: step 5140, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.769 sec/batch; 69h:57m:20s remains)
INFO - root - 2017-12-07 04:30:02.320603: step 5150, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.767 sec/batch; 69h:47m:14s remains)
INFO - root - 2017-12-07 04:30:09.919128: step 5160, loss = 1.01, batch loss = 0.94 (10.6 examples/sec; 0.754 sec/batch; 68h:31m:33s remains)
INFO - root - 2017-12-07 04:30:17.641385: step 5170, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.786 sec/batch; 71h:25m:57s remains)
INFO - root - 2017-12-07 04:30:25.339449: step 5180, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.769 sec/batch; 69h:54m:50s remains)
INFO - root - 2017-12-07 04:30:33.011566: step 5190, loss = 0.65, batch loss = 0.58 (10.1 examples/sec; 0.791 sec/batch; 71h:53m:12s remains)
INFO - root - 2017-12-07 04:30:40.722026: step 5200, loss = 0.85, batch loss = 0.78 (10.2 examples/sec; 0.788 sec/batch; 71h:37m:08s remains)
2017-12-07 04:30:41.347958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2257433 -3.2653828 -3.3953266 -3.2784791 -2.8981943 -2.7635729 -2.670331 -2.4419177 -1.9479201 -1.2820866 -1.0844119 -1.2620173 -1.6455808 -2.4166875 -2.9852693][-3.2132573 -3.2613778 -3.4023471 -3.3017316 -2.961977 -2.8312049 -2.7153955 -2.4497218 -1.9492352 -1.4147658 -1.3666997 -1.460149 -1.583679 -2.1622405 -2.6697385][-3.2239542 -3.3062637 -3.4778407 -3.4246645 -3.1586709 -3.0677423 -2.928329 -2.5935545 -2.0862284 -1.6873958 -1.712779 -1.6907349 -1.5803571 -1.9333656 -2.3168182][-3.2990415 -3.4369466 -3.6509657 -3.660156 -3.479167 -3.4156494 -3.2438343 -2.8537488 -2.36536 -2.0931466 -2.1707702 -2.07425 -1.8245177 -1.9630039 -2.1790769][-3.3785484 -3.5727527 -3.8282113 -3.9068465 -3.8088419 -3.7811878 -3.6397266 -3.2546697 -2.7767596 -2.5640767 -2.6225226 -2.4336443 -2.0975056 -2.0896204 -2.1287706][-3.4002628 -3.6030343 -3.8215504 -3.8734369 -3.7835746 -3.762141 -3.682796 -3.3789477 -2.9515109 -2.7666614 -2.791151 -2.4822011 -2.0456121 -1.9083362 -1.7847872][-3.358604 -3.4471428 -3.4440269 -3.2706618 -3.0619516 -3.0225816 -3.0516586 -2.8898058 -2.5726597 -2.4428887 -2.4709308 -2.0678854 -1.5059249 -1.2174978 -0.95061851][-3.3359206 -3.2616179 -2.913456 -2.3340077 -1.8236139 -1.6517212 -1.7396584 -1.7344236 -1.5635486 -1.5404909 -1.6541131 -1.3006916 -0.72195745 -0.35124969 0.00043725967][-3.3814363 -3.2626472 -2.7149496 -1.8544514 -1.0790911 -0.7296989 -0.78558135 -0.85561776 -0.78094697 -0.83716869 -1.0292435 -0.80969477 -0.3733263 -0.087898731 0.24608564][-3.30469 -3.3239737 -2.9025254 -2.1890681 -1.5079184 -1.153445 -1.1566892 -1.2279866 -1.1978645 -1.2828956 -1.4970949 -1.387224 -1.1086121 -0.90690875 -0.52920389][-3.0888162 -3.3581176 -3.277916 -3.0142007 -2.7417579 -2.6046295 -2.6351905 -2.6948304 -2.6613655 -2.7106419 -2.8626542 -2.7791476 -2.5542698 -2.3382664 -1.8895929][-3.0031826 -3.410049 -3.557647 -3.6706238 -3.8270321 -3.9853473 -4.12242 -4.1929531 -4.1475234 -4.1354833 -4.1759748 -4.0341911 -3.770587 -3.503315 -3.087194][-3.3444433 -3.683744 -3.7893233 -3.9557292 -4.2646508 -4.554193 -4.7585735 -4.8536921 -4.823801 -4.7981997 -4.8043408 -4.6702504 -4.4081969 -4.1315527 -3.7949495][-4.0290542 -4.1051254 -3.9565043 -3.9217134 -4.0978503 -4.3145266 -4.4806037 -4.5617328 -4.5557079 -4.5591516 -4.58045 -4.4958 -4.2968569 -4.1020212 -3.889195][-4.6101866 -4.4064054 -4.0316157 -3.8061016 -3.7954507 -3.8444428 -3.8902314 -3.9031856 -3.8818707 -3.8962941 -3.9422386 -3.9248614 -3.8297567 -3.7589712 -3.6694102]]...]
INFO - root - 2017-12-07 04:30:48.894972: step 5210, loss = 0.70, batch loss = 0.62 (10.5 examples/sec; 0.762 sec/batch; 69h:17m:29s remains)
INFO - root - 2017-12-07 04:30:56.513609: step 5220, loss = 0.77, batch loss = 0.70 (10.8 examples/sec; 0.739 sec/batch; 67h:12m:24s remains)
INFO - root - 2017-12-07 04:31:04.064566: step 5230, loss = 0.98, batch loss = 0.90 (10.4 examples/sec; 0.771 sec/batch; 70h:03m:12s remains)
INFO - root - 2017-12-07 04:31:11.699959: step 5240, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.747 sec/batch; 67h:52m:26s remains)
INFO - root - 2017-12-07 04:31:19.255424: step 5250, loss = 0.72, batch loss = 0.65 (10.8 examples/sec; 0.738 sec/batch; 67h:02m:45s remains)
INFO - root - 2017-12-07 04:31:26.918194: step 5260, loss = 0.92, batch loss = 0.85 (10.6 examples/sec; 0.755 sec/batch; 68h:35m:09s remains)
INFO - root - 2017-12-07 04:31:34.569687: step 5270, loss = 1.05, batch loss = 0.98 (10.1 examples/sec; 0.793 sec/batch; 72h:04m:24s remains)
INFO - root - 2017-12-07 04:31:42.239777: step 5280, loss = 0.91, batch loss = 0.84 (10.3 examples/sec; 0.777 sec/batch; 70h:38m:31s remains)
INFO - root - 2017-12-07 04:31:49.842904: step 5290, loss = 0.85, batch loss = 0.78 (10.7 examples/sec; 0.750 sec/batch; 68h:09m:19s remains)
INFO - root - 2017-12-07 04:31:57.463409: step 5300, loss = 0.81, batch loss = 0.73 (10.5 examples/sec; 0.761 sec/batch; 69h:07m:29s remains)
2017-12-07 04:31:58.117480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5060866 -3.3901544 -3.3240526 -3.3024876 -3.2986841 -3.3109994 -3.404263 -3.4856737 -3.5713539 -3.6540613 -3.7055783 -3.6811776 -3.6179376 -3.5108256 -3.274159][-3.5522912 -3.4285882 -3.3862703 -3.3712397 -3.2952127 -3.191947 -3.1835093 -3.1710696 -3.2682774 -3.4464161 -3.5644815 -3.5550606 -3.5160019 -3.4089079 -3.140672][-3.4653273 -3.3755531 -3.4067011 -3.4857035 -3.480557 -3.4083943 -3.3358433 -3.1690879 -3.1996346 -3.3901176 -3.4596653 -3.3609819 -3.2710557 -3.1274462 -2.8348169][-3.0826883 -3.0251696 -3.0090137 -3.0718923 -3.1607838 -3.2532592 -3.2578876 -3.0577862 -3.097806 -3.3307102 -3.3673186 -3.2038827 -3.0811892 -2.8806605 -2.5533395][-2.4355021 -2.3319151 -2.0846174 -1.9745865 -2.0369573 -2.2964845 -2.4308534 -2.3228705 -2.5173051 -2.9439874 -3.1166964 -3.0931349 -3.0989983 -2.9323168 -2.5845976][-1.7288444 -1.4866536 -0.9177506 -0.4880867 -0.32063389 -0.54991889 -0.66160274 -0.62087655 -1.0618193 -1.8428237 -2.36879 -2.7026415 -2.9980555 -2.9840341 -2.6967349][-1.1296799 -0.71787882 0.065162182 0.69564676 1.0421391 0.94280338 0.98719072 1.080482 0.51245308 -0.51028347 -1.3678916 -2.0810983 -2.7058487 -2.9118307 -2.7711034][-1.3066168 -0.76423097 0.017419815 0.52623177 0.67866468 0.49105692 0.67708063 1.0101738 0.73180151 -0.021196365 -0.74813843 -1.4526353 -2.1573415 -2.556035 -2.6673672][-2.1163914 -1.5847247 -0.91326332 -0.53123879 -0.50565767 -0.72709894 -0.5095861 -0.099873066 -0.17572784 -0.60515761 -1.0379117 -1.5075774 -2.038214 -2.4406483 -2.68208][-2.6790123 -2.2761729 -1.7416189 -1.3847883 -1.3074436 -1.4631066 -1.3042381 -1.0563376 -1.1852698 -1.5127645 -1.7780647 -2.0422156 -2.3508732 -2.6670566 -2.8767812][-2.4738784 -2.2788575 -1.9211011 -1.649461 -1.5156941 -1.5340459 -1.4406092 -1.3931625 -1.6604791 -1.9987252 -2.1876397 -2.3172562 -2.4439781 -2.6808665 -2.8813386][-1.8000221 -1.7468777 -1.5732019 -1.4738748 -1.3608544 -1.2326849 -1.1077757 -1.1063366 -1.3841941 -1.6929343 -1.8921847 -2.06195 -2.2355752 -2.5323398 -2.7947781][-1.580442 -1.5236628 -1.4358251 -1.4453137 -1.3657596 -1.1875927 -1.0333717 -0.97892404 -1.1297731 -1.2930861 -1.4527347 -1.6695368 -1.9605782 -2.3543293 -2.68697][-1.6980546 -1.6038225 -1.5137846 -1.4995642 -1.4004469 -1.2545121 -1.1440926 -1.0997946 -1.1916656 -1.2768025 -1.3921311 -1.5885177 -1.8881001 -2.2822111 -2.6081395][-2.3200736 -2.2297938 -2.1103222 -2.0279837 -1.9092972 -1.8054531 -1.7368708 -1.690469 -1.7118404 -1.7331116 -1.8079083 -1.9618151 -2.2049983 -2.5050352 -2.7094512]]...]
INFO - root - 2017-12-07 04:32:05.804092: step 5310, loss = 1.22, batch loss = 1.15 (10.3 examples/sec; 0.779 sec/batch; 70h:45m:36s remains)
INFO - root - 2017-12-07 04:32:13.405644: step 5320, loss = 0.64, batch loss = 0.57 (10.7 examples/sec; 0.750 sec/batch; 68h:07m:13s remains)
INFO - root - 2017-12-07 04:32:20.866992: step 5330, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.786 sec/batch; 71h:24m:08s remains)
INFO - root - 2017-12-07 04:32:28.506348: step 5340, loss = 0.68, batch loss = 0.61 (10.6 examples/sec; 0.752 sec/batch; 68h:20m:44s remains)
INFO - root - 2017-12-07 04:32:36.130819: step 5350, loss = 0.76, batch loss = 0.69 (10.2 examples/sec; 0.781 sec/batch; 70h:59m:52s remains)
INFO - root - 2017-12-07 04:32:43.675943: step 5360, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.754 sec/batch; 68h:33m:44s remains)
INFO - root - 2017-12-07 04:32:51.371878: step 5370, loss = 0.64, batch loss = 0.56 (10.5 examples/sec; 0.764 sec/batch; 69h:24m:42s remains)
INFO - root - 2017-12-07 04:32:59.077461: step 5380, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.770 sec/batch; 69h:59m:35s remains)
INFO - root - 2017-12-07 04:33:06.678114: step 5390, loss = 0.81, batch loss = 0.74 (10.8 examples/sec; 0.742 sec/batch; 67h:23m:46s remains)
INFO - root - 2017-12-07 04:33:14.259087: step 5400, loss = 0.73, batch loss = 0.66 (10.7 examples/sec; 0.746 sec/batch; 67h:48m:34s remains)
2017-12-07 04:33:14.879154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2490239 -3.4706602 -3.6319442 -3.6026716 -3.6298337 -3.8005662 -3.9699242 -4.1773262 -4.4035664 -4.3918495 -4.1954861 -3.9644547 -3.78187 -3.6579084 -3.6207428][-3.1159191 -3.2077749 -3.2497637 -3.0935979 -3.029469 -3.1776338 -3.3671341 -3.7612803 -4.345602 -4.61409 -4.5259686 -4.2867928 -4.0158782 -3.7868352 -3.6943116][-2.9579263 -2.8597031 -2.7994704 -2.600359 -2.5802944 -2.8443613 -3.09054 -3.616174 -4.4321537 -4.8790288 -4.8494806 -4.5762525 -4.1845436 -3.8074815 -3.6100576][-2.7808592 -2.5268741 -2.4098797 -2.1860611 -2.1733704 -2.4344361 -2.5307846 -2.9045963 -3.6463485 -4.1007357 -4.172585 -4.0201974 -3.6684327 -3.2477677 -3.0062408][-2.563251 -2.1677575 -1.9930604 -1.7077434 -1.6135561 -1.7332864 -1.5025578 -1.5570261 -2.132452 -2.532558 -2.7360339 -2.8190172 -2.7197962 -2.5190907 -2.4542551][-2.3985088 -1.8813672 -1.598546 -1.186085 -0.93759274 -0.8478179 -0.27384472 -0.052316189 -0.49899387 -0.886667 -1.2611711 -1.7338316 -2.084048 -2.3441679 -2.64595][-2.3057733 -1.69017 -1.2397573 -0.64488149 -0.22592211 -0.0027747154 0.72833061 1.1015491 0.74429035 0.35688591 -0.22137117 -1.1165388 -1.8756247 -2.4801431 -3.0459132][-2.2864225 -1.7959766 -1.4255266 -0.84213138 -0.45184779 -0.32584715 0.20400143 0.504148 0.27337074 0.12761641 -0.21482754 -1.0527451 -1.8506281 -2.5547457 -3.2481737][-2.4953809 -2.2171202 -2.0344474 -1.5938816 -1.3186009 -1.3853736 -1.1537282 -0.9883697 -1.1049049 -0.90729761 -0.82433176 -1.3394303 -1.9650362 -2.6409814 -3.4370422][-2.7759805 -2.6349325 -2.6261134 -2.4219849 -2.3274271 -2.503561 -2.3338873 -2.1290793 -2.159735 -1.8465614 -1.5247455 -1.7382178 -2.0938513 -2.607234 -3.3860407][-2.9528079 -2.840982 -2.9252038 -2.9517481 -3.062542 -3.3030226 -3.1725183 -2.9856977 -3.0152256 -2.7447019 -2.3392935 -2.2757089 -2.3513865 -2.6526084 -3.3180766][-3.2045887 -3.0964479 -3.1608741 -3.2552404 -3.4410517 -3.679744 -3.631145 -3.5209818 -3.5615406 -3.3920066 -3.0493374 -2.8471775 -2.738771 -2.8144305 -3.2675045][-3.3746991 -3.2225938 -3.2194283 -3.3286366 -3.5260804 -3.7305272 -3.7242527 -3.6406484 -3.6760883 -3.6412783 -3.457757 -3.27321 -3.0993505 -2.9744287 -3.1564479][-3.4086866 -3.2245426 -3.1948791 -3.3424656 -3.5245283 -3.6727571 -3.6847842 -3.6322203 -3.678431 -3.7361302 -3.6327925 -3.4626822 -3.2777433 -3.1099086 -3.1845303][-3.4549356 -3.2942114 -3.2451291 -3.3641984 -3.4815869 -3.5605674 -3.5855684 -3.5706539 -3.6211448 -3.6913102 -3.6055987 -3.439575 -3.2977529 -3.1826072 -3.2580094]]...]
INFO - root - 2017-12-07 04:33:22.509422: step 5410, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.769 sec/batch; 69h:54m:54s remains)
INFO - root - 2017-12-07 04:33:30.140763: step 5420, loss = 0.64, batch loss = 0.57 (10.5 examples/sec; 0.762 sec/batch; 69h:14m:32s remains)
INFO - root - 2017-12-07 04:33:37.509890: step 5430, loss = 0.70, batch loss = 0.63 (10.7 examples/sec; 0.748 sec/batch; 68h:00m:01s remains)
INFO - root - 2017-12-07 04:33:45.259080: step 5440, loss = 0.60, batch loss = 0.53 (10.3 examples/sec; 0.775 sec/batch; 70h:27m:11s remains)
INFO - root - 2017-12-07 04:33:52.917590: step 5450, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.768 sec/batch; 69h:43m:53s remains)
INFO - root - 2017-12-07 04:34:00.542662: step 5460, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.765 sec/batch; 69h:28m:49s remains)
INFO - root - 2017-12-07 04:34:08.203933: step 5470, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.751 sec/batch; 68h:13m:40s remains)
INFO - root - 2017-12-07 04:34:15.822265: step 5480, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.782 sec/batch; 71h:01m:44s remains)
INFO - root - 2017-12-07 04:34:23.501897: step 5490, loss = 0.71, batch loss = 0.64 (10.7 examples/sec; 0.746 sec/batch; 67h:43m:45s remains)
INFO - root - 2017-12-07 04:34:31.124769: step 5500, loss = 0.89, batch loss = 0.82 (10.8 examples/sec; 0.743 sec/batch; 67h:28m:10s remains)
2017-12-07 04:34:31.731721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6252093 -1.8356228 -1.9826226 -1.9017277 -1.8160505 -1.8760943 -1.8766518 -1.9268768 -2.0834017 -2.3531501 -2.7742662 -3.0568805 -3.2607708 -3.1957307 -2.9581559][-1.1186953 -1.3913968 -1.4947855 -1.2795012 -1.1414747 -1.2159193 -1.2082431 -1.2901013 -1.5549405 -1.8845558 -2.3485975 -2.7405643 -3.077024 -3.1074843 -2.8770473][-1.0309093 -1.2269421 -1.1899328 -0.83429456 -0.67023396 -0.730361 -0.67584825 -0.78987765 -1.2114992 -1.6499157 -2.1482701 -2.6239338 -3.0490987 -3.10141 -2.8358097][-1.1021409 -1.1714118 -1.0032227 -0.58339071 -0.45808864 -0.51829076 -0.41577005 -0.573524 -1.0631571 -1.4949152 -1.9597697 -2.4443843 -2.8707461 -2.9033279 -2.6319118][-0.72317457 -0.80139184 -0.68572187 -0.39166355 -0.38496304 -0.47203445 -0.39044237 -0.61371708 -1.1054718 -1.4566517 -1.8203189 -2.2104371 -2.513936 -2.4637849 -2.1578383][0.3067255 0.13448048 0.05546093 0.13572454 0.11077881 0.15228605 0.26349783 -0.071764469 -0.63538933 -1.0163057 -1.3795619 -1.7143176 -1.9083626 -1.800051 -1.4867086][1.4199796 1.1435528 0.84079123 0.77744246 0.9327178 1.3976669 1.6576505 1.200892 0.49397469 -0.019381046 -0.46126437 -0.81539917 -0.99155021 -0.94652128 -0.77182388][1.6454301 1.4154992 1.1104112 1.139112 1.6855359 2.7362895 3.2992344 2.7040329 1.8398619 1.1250272 0.593658 0.18960905 -0.13054371 -0.33514738 -0.49714136][0.81096458 0.68879318 0.50430393 0.62124252 1.2657566 2.4077258 3.0784593 2.6853719 2.0110173 1.3482461 0.84066391 0.4921999 0.11108398 -0.25966215 -0.65336657][0.088569641 0.037566185 -0.080360413 -0.089635372 0.22956085 0.90286541 1.2814093 1.1594539 0.88464928 0.49537802 0.16092253 -0.036010742 -0.31764269 -0.69072366 -1.1558812][-0.66291475 -0.64456654 -0.68798447 -0.8073287 -0.78115773 -0.54614735 -0.4254427 -0.44634891 -0.47464585 -0.57226896 -0.60519624 -0.55524874 -0.63794208 -0.90102077 -1.353323][-1.1501224 -1.0051515 -0.96597719 -1.0905058 -1.1573381 -1.0903497 -1.0458875 -1.0114963 -0.95458126 -0.89084387 -0.688159 -0.47584629 -0.4619863 -0.69578147 -1.1773837][-0.86050606 -0.63983679 -0.619395 -0.7841413 -0.87788296 -0.89352131 -0.8724339 -0.82067585 -0.73949051 -0.58038545 -0.21880198 0.075654507 0.026286602 -0.36108112 -0.96905208][-0.29315186 -0.21130705 -0.41744137 -0.77293634 -0.983042 -1.1020682 -1.0734828 -0.9724865 -0.86613846 -0.67112875 -0.26765251 0.055638313 -0.018947601 -0.46801138 -1.0795202][-0.42257833 -0.55205488 -0.92763495 -1.390949 -1.6732278 -1.858706 -1.8658416 -1.7730258 -1.6952956 -1.5663421 -1.2913177 -1.0601399 -1.1296985 -1.4601038 -1.8642986]]...]
INFO - root - 2017-12-07 04:34:39.419151: step 5510, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.755 sec/batch; 68h:34m:53s remains)
INFO - root - 2017-12-07 04:34:47.008922: step 5520, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.775 sec/batch; 70h:25m:05s remains)
INFO - root - 2017-12-07 04:34:54.465754: step 5530, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.758 sec/batch; 68h:49m:57s remains)
INFO - root - 2017-12-07 04:35:02.133961: step 5540, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.777 sec/batch; 70h:35m:55s remains)
INFO - root - 2017-12-07 04:35:09.738215: step 5550, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.786 sec/batch; 71h:23m:55s remains)
INFO - root - 2017-12-07 04:35:17.493177: step 5560, loss = 0.93, batch loss = 0.86 (10.6 examples/sec; 0.754 sec/batch; 68h:26m:57s remains)
INFO - root - 2017-12-07 04:35:25.163610: step 5570, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.764 sec/batch; 69h:20m:42s remains)
INFO - root - 2017-12-07 04:35:32.816511: step 5580, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.775 sec/batch; 70h:22m:01s remains)
INFO - root - 2017-12-07 04:35:40.428284: step 5590, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.766 sec/batch; 69h:32m:30s remains)
INFO - root - 2017-12-07 04:35:48.056709: step 5600, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.761 sec/batch; 69h:08m:24s remains)
2017-12-07 04:35:48.650468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1150923 -3.0235896 -2.8483272 -2.7451985 -2.7534654 -2.9178548 -3.1523991 -3.3835423 -3.6088767 -3.7544394 -3.7720468 -3.6387396 -3.44193 -3.2935066 -3.2287402][-3.2439909 -3.120636 -2.8405032 -2.6132598 -2.4983463 -2.5588784 -2.7469 -3.0153275 -3.3937545 -3.7937002 -4.000236 -3.9264793 -3.6881933 -3.442102 -3.3076339][-3.4013541 -3.2910371 -2.9575582 -2.5914993 -2.2431448 -2.014365 -1.9976861 -2.2255647 -2.7597451 -3.5238233 -4.0580897 -4.1490207 -3.9323537 -3.6009173 -3.413552][-3.38693 -3.3117628 -2.9825511 -2.532979 -1.9799595 -1.4629841 -1.2141247 -1.3238845 -1.9361989 -3.0654368 -4.0021353 -4.3430514 -4.2125878 -3.8166256 -3.5723071][-3.2174039 -3.2178264 -2.9690895 -2.5262995 -1.8549304 -1.1245189 -0.66318917 -0.53969932 -1.0330629 -2.3962862 -3.698314 -4.3464241 -4.3929152 -4.0171022 -3.7342455][-2.9151301 -2.9972878 -2.8272028 -2.4227514 -1.6864245 -0.86384773 -0.23846769 0.25027132 0.026935577 -1.4775031 -3.0992882 -4.0886021 -4.3951292 -4.1422729 -3.8657639][-2.6184926 -2.7315271 -2.5432312 -2.1214702 -1.342556 -0.53997946 0.18690348 1.0751624 1.1431866 -0.45791698 -2.3223553 -3.6051457 -4.1748047 -4.1093373 -3.9162476][-2.4592209 -2.5120316 -2.2156963 -1.7520812 -0.98273563 -0.26313496 0.5156951 1.6979451 1.940114 0.3273859 -1.6117117 -3.0581722 -3.8210611 -3.9358084 -3.8560755][-2.4239259 -2.3330355 -1.9192612 -1.4523103 -0.81096697 -0.31175613 0.33165407 1.5216327 1.8525863 0.53471184 -1.1707697 -2.6100497 -3.4831412 -3.7304087 -3.7400336][-2.6815739 -2.4848742 -1.9967115 -1.5231409 -1.0409718 -0.846751 -0.45925951 0.56763172 1.0185537 0.21845961 -1.044235 -2.3291755 -3.2147837 -3.5226698 -3.5612564][-3.1169226 -2.9157341 -2.4329457 -1.9391956 -1.5875862 -1.6423507 -1.4504271 -0.5924077 -0.022306919 -0.29529285 -1.1263816 -2.2325642 -3.0925012 -3.423665 -3.4674528][-3.4232273 -3.3121881 -2.924377 -2.4653714 -2.2074094 -2.4061911 -2.3311141 -1.6349747 -0.98216581 -0.83192849 -1.2826383 -2.2044611 -3.0390303 -3.4123816 -3.471771][-3.5422168 -3.5142579 -3.2165432 -2.8156824 -2.6229987 -2.8478365 -2.844871 -2.3626966 -1.7882471 -1.4321659 -1.6233621 -2.341799 -3.0958016 -3.467881 -3.5026143][-3.5372777 -3.5716081 -3.3587496 -3.0468512 -2.935401 -3.135076 -3.1614084 -2.8602624 -2.4406281 -2.0865989 -2.1817904 -2.735867 -3.336453 -3.6017432 -3.54484][-3.472446 -3.56937 -3.4666572 -3.2729747 -3.2446222 -3.4247818 -3.4729519 -3.3030319 -3.0168185 -2.7319293 -2.7560229 -3.1021914 -3.4721432 -3.59989 -3.4926629]]...]
INFO - root - 2017-12-07 04:35:56.334118: step 5610, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.775 sec/batch; 70h:24m:37s remains)
INFO - root - 2017-12-07 04:36:03.846685: step 5620, loss = 0.94, batch loss = 0.87 (10.7 examples/sec; 0.746 sec/batch; 67h:44m:44s remains)
INFO - root - 2017-12-07 04:36:11.331391: step 5630, loss = 1.02, batch loss = 0.95 (10.3 examples/sec; 0.774 sec/batch; 70h:15m:43s remains)
INFO - root - 2017-12-07 04:36:18.973898: step 5640, loss = 0.83, batch loss = 0.75 (10.5 examples/sec; 0.762 sec/batch; 69h:12m:57s remains)
INFO - root - 2017-12-07 04:36:26.736136: step 5650, loss = 0.71, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 70h:40m:39s remains)
INFO - root - 2017-12-07 04:36:34.397039: step 5660, loss = 0.96, batch loss = 0.89 (10.1 examples/sec; 0.794 sec/batch; 72h:04m:36s remains)
INFO - root - 2017-12-07 04:36:42.024774: step 5670, loss = 1.00, batch loss = 0.93 (10.4 examples/sec; 0.769 sec/batch; 69h:51m:09s remains)
INFO - root - 2017-12-07 04:36:49.763246: step 5680, loss = 0.82, batch loss = 0.75 (9.8 examples/sec; 0.817 sec/batch; 74h:09m:03s remains)
INFO - root - 2017-12-07 04:36:57.618883: step 5690, loss = 0.64, batch loss = 0.57 (10.2 examples/sec; 0.785 sec/batch; 71h:16m:07s remains)
INFO - root - 2017-12-07 04:37:05.330002: step 5700, loss = 0.66, batch loss = 0.59 (10.6 examples/sec; 0.754 sec/batch; 68h:26m:33s remains)
2017-12-07 04:37:05.961132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3455229 -1.9241483 -1.274678 -0.90894532 -0.98096538 -1.3704259 -1.7955348 -2.0472994 -2.195271 -2.1996212 -2.2620707 -2.4209609 -2.4814112 -2.292156 -2.093442][-2.4507585 -2.1129405 -1.4465885 -0.94676733 -0.819216 -1.086257 -1.5043421 -1.7147348 -1.7598217 -1.7174928 -1.781054 -1.9787946 -2.0642576 -1.8668296 -1.6942825][-2.5683155 -2.26931 -1.5626564 -0.92502141 -0.63692021 -0.82518053 -1.2267547 -1.3653674 -1.3364282 -1.2943718 -1.3514061 -1.5113645 -1.5641034 -1.4008384 -1.326591][-2.6222072 -2.3717694 -1.6933985 -1.0152028 -0.67419004 -0.84973407 -1.2394679 -1.3229401 -1.3061271 -1.3303339 -1.3813744 -1.4828088 -1.4856234 -1.3507357 -1.3379807][-2.677989 -2.5422235 -2.0008333 -1.4229441 -1.1210406 -1.2943416 -1.6529226 -1.7359958 -1.8075693 -1.9029119 -1.9267328 -1.9497004 -1.8597162 -1.6942565 -1.6668451][-2.7573762 -2.7402391 -2.3628612 -1.9473011 -1.7419078 -1.9303567 -2.288363 -2.4147079 -2.5482202 -2.6445031 -2.6044483 -2.5339241 -2.3513675 -2.1459529 -2.0740788][-2.812571 -2.8461542 -2.5705166 -2.2866385 -2.1968048 -2.4609294 -2.8708029 -3.0383129 -3.1540382 -3.1808302 -3.0806398 -2.9647794 -2.7589052 -2.5707123 -2.4857922][-2.8110795 -2.8259466 -2.5629807 -2.3211417 -2.3046544 -2.6416955 -3.1105642 -3.3063879 -3.3613992 -3.3159628 -3.2075505 -3.128679 -2.9818149 -2.8715787 -2.8224921][-2.7652042 -2.7123561 -2.3791263 -2.0792327 -2.0760765 -2.4676614 -2.9985476 -3.2321608 -3.2406211 -3.163461 -3.0904331 -3.0836499 -3.02528 -3.0007529 -2.9899316][-2.6474023 -2.4978375 -2.0475359 -1.6349876 -1.6089211 -2.0478992 -2.6607354 -2.9664016 -2.9813952 -2.923676 -2.9129477 -2.9891315 -3.0199103 -3.0608354 -3.0731277][-2.4930239 -2.2727094 -1.741178 -1.2326994 -1.1613333 -1.6048861 -2.2702503 -2.6438904 -2.6891763 -2.6696696 -2.7298026 -2.893358 -3.012588 -3.1035113 -3.1327462][-2.4518635 -2.2424974 -1.6994598 -1.1410828 -1.0046215 -1.3844452 -2.01977 -2.4081404 -2.4641662 -2.470118 -2.5820067 -2.8026857 -2.9760857 -3.0947971 -3.1394587][-2.5451345 -2.3977647 -1.894964 -1.3282521 -1.133456 -1.4152863 -1.9649787 -2.3322804 -2.3922927 -2.4324174 -2.5780778 -2.8045352 -2.9823308 -3.094424 -3.1383634][-2.6436343 -2.5716529 -2.1515055 -1.6334264 -1.4207683 -1.6133621 -2.0593734 -2.3793509 -2.4298124 -2.4959013 -2.6554539 -2.8651021 -3.0233259 -3.1152725 -3.1464477][-2.6436706 -2.6391954 -2.3119261 -1.8654401 -1.6692014 -1.8070135 -2.1766522 -2.4550424 -2.4737172 -2.5355663 -2.6904685 -2.8777339 -3.0157962 -3.0868797 -3.1105747]]...]
INFO - root - 2017-12-07 04:37:13.597649: step 5710, loss = 0.58, batch loss = 0.51 (10.5 examples/sec; 0.765 sec/batch; 69h:28m:04s remains)
INFO - root - 2017-12-07 04:37:21.202316: step 5720, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.777 sec/batch; 70h:30m:02s remains)
INFO - root - 2017-12-07 04:37:28.629222: step 5730, loss = 0.76, batch loss = 0.68 (10.6 examples/sec; 0.757 sec/batch; 68h:42m:49s remains)
INFO - root - 2017-12-07 04:37:36.229413: step 5740, loss = 0.85, batch loss = 0.77 (10.7 examples/sec; 0.749 sec/batch; 67h:56m:36s remains)
INFO - root - 2017-12-07 04:37:43.846555: step 5750, loss = 0.93, batch loss = 0.86 (10.4 examples/sec; 0.772 sec/batch; 70h:06m:09s remains)
INFO - root - 2017-12-07 04:37:51.464505: step 5760, loss = 1.04, batch loss = 0.96 (10.6 examples/sec; 0.753 sec/batch; 68h:23m:10s remains)
INFO - root - 2017-12-07 04:37:59.334863: step 5770, loss = 0.88, batch loss = 0.81 (10.2 examples/sec; 0.783 sec/batch; 71h:04m:35s remains)
INFO - root - 2017-12-07 04:38:07.041678: step 5780, loss = 0.99, batch loss = 0.92 (10.2 examples/sec; 0.788 sec/batch; 71h:30m:30s remains)
INFO - root - 2017-12-07 04:38:14.865902: step 5790, loss = 0.98, batch loss = 0.91 (9.8 examples/sec; 0.819 sec/batch; 74h:17m:46s remains)
INFO - root - 2017-12-07 04:38:22.662206: step 5800, loss = 1.11, batch loss = 1.04 (10.3 examples/sec; 0.780 sec/batch; 70h:47m:19s remains)
2017-12-07 04:38:23.345949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0795927 -4.0439262 -4.0354557 -3.8936312 -3.548146 -3.4318824 -3.5621321 -3.6766248 -3.7913411 -3.6383915 -3.2657671 -3.1014233 -3.0326843 -2.9302964 -2.8530321][-3.7276311 -3.7052579 -3.7715311 -3.7804816 -3.5669346 -3.4827468 -3.5264106 -3.5692389 -3.7071102 -3.7048688 -3.5871701 -3.6181388 -3.6002274 -3.4975502 -3.4569936][-3.5114217 -3.3909159 -3.3305678 -3.3133869 -3.1524477 -3.116951 -3.1584671 -3.1964169 -3.3501072 -3.4614315 -3.5586739 -3.7471061 -3.7495584 -3.6317258 -3.5889812][-3.2405505 -2.942028 -2.6071551 -2.3952169 -2.1259449 -2.0415034 -2.1639724 -2.3599777 -2.6572242 -2.9465761 -3.3011749 -3.7013035 -3.7913177 -3.7048018 -3.6598873][-2.9122272 -2.5449581 -2.0877533 -1.7749662 -1.3879364 -1.1687839 -1.2663138 -1.5368752 -1.9476688 -2.3658309 -2.9137907 -3.5558786 -3.8344057 -3.7841454 -3.6566741][-2.401504 -2.0705388 -1.6932716 -1.4858065 -1.0975661 -0.72362065 -0.64380121 -0.81063652 -1.2103708 -1.5936837 -2.1539881 -3.0094132 -3.6560009 -3.852653 -3.7348037][-2.0722685 -1.6714349 -1.2963939 -1.1455128 -0.796618 -0.31744146 0.0077090263 0.027829647 -0.33799839 -0.6709671 -1.1049564 -2.0207634 -3.0430584 -3.688755 -3.8339882][-2.4665236 -1.9300079 -1.4011087 -1.0978208 -0.7603147 -0.30645323 0.12571669 0.35286522 0.14964342 -0.060949326 -0.28200293 -1.0361383 -2.1277719 -2.9945898 -3.3342733][-3.1306572 -2.5680156 -1.9847391 -1.6140847 -1.3460286 -1.0221941 -0.69316125 -0.37622738 -0.29754782 -0.27813864 -0.28901672 -0.78621888 -1.711694 -2.5293646 -2.8871856][-3.6932702 -3.2745709 -2.8128238 -2.5250688 -2.3739476 -2.2221291 -2.086864 -1.8315883 -1.5939798 -1.3976443 -1.2840118 -1.5327346 -2.2007322 -2.8640056 -3.197176][-4.0648236 -3.8152633 -3.4942067 -3.2620785 -3.1370745 -3.0703752 -3.0802364 -3.0056753 -2.838943 -2.6915879 -2.6024761 -2.6902144 -3.1193042 -3.6080368 -3.8659627][-4.2355928 -4.121244 -3.9323926 -3.7563069 -3.619725 -3.5113785 -3.4887393 -3.5057127 -3.4745498 -3.4396181 -3.4219632 -3.4563086 -3.7519469 -4.1289129 -4.302794][-4.2099586 -4.2101703 -4.1639514 -4.0970387 -4.0204391 -3.8920379 -3.7828338 -3.7434976 -3.7064514 -3.6900253 -3.6989262 -3.7104332 -3.9033506 -4.174561 -4.28835][-3.9820783 -4.0397816 -4.0862589 -4.1186862 -4.134829 -4.0974607 -4.0344706 -4.0255628 -4.0113306 -3.9937882 -3.9842157 -3.9440114 -3.996968 -4.092618 -4.1060653][-3.8408043 -3.8880005 -3.9386852 -3.9931083 -4.0538149 -4.0991044 -4.1297164 -4.1844082 -4.2191448 -4.2159529 -4.1880026 -4.1166048 -4.0750165 -4.0420303 -3.9774032]]...]
INFO - root - 2017-12-07 04:38:30.969547: step 5810, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.752 sec/batch; 68h:15m:52s remains)
INFO - root - 2017-12-07 04:38:38.567535: step 5820, loss = 0.88, batch loss = 0.80 (10.5 examples/sec; 0.762 sec/batch; 69h:09m:09s remains)
INFO - root - 2017-12-07 04:38:45.981522: step 5830, loss = 0.64, batch loss = 0.57 (10.1 examples/sec; 0.793 sec/batch; 71h:55m:27s remains)
INFO - root - 2017-12-07 04:38:53.584118: step 5840, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.754 sec/batch; 68h:25m:28s remains)
INFO - root - 2017-12-07 04:39:01.116057: step 5850, loss = 0.69, batch loss = 0.62 (10.7 examples/sec; 0.744 sec/batch; 67h:33m:09s remains)
INFO - root - 2017-12-07 04:39:08.784934: step 5860, loss = 0.79, batch loss = 0.72 (10.0 examples/sec; 0.802 sec/batch; 72h:46m:38s remains)
INFO - root - 2017-12-07 04:39:16.425867: step 5870, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.763 sec/batch; 69h:12m:21s remains)
INFO - root - 2017-12-07 04:39:24.118362: step 5880, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 70h:06m:32s remains)
INFO - root - 2017-12-07 04:39:31.724616: step 5890, loss = 0.66, batch loss = 0.59 (10.3 examples/sec; 0.779 sec/batch; 70h:40m:33s remains)
INFO - root - 2017-12-07 04:39:39.418015: step 5900, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.773 sec/batch; 70h:06m:41s remains)
2017-12-07 04:39:40.009803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4717102 -2.7786555 -2.6810627 -2.4504414 -2.2251313 -1.9484637 -1.9338982 -2.1315758 -2.2703257 -2.3196278 -2.4179208 -2.5119343 -2.4795666 -2.3513296 -2.272995][-3.3336859 -3.6952739 -3.633954 -3.3886187 -3.0533667 -2.6078811 -2.4733882 -2.6806078 -2.8662891 -2.940496 -3.0956936 -3.2997482 -3.4009778 -3.3637629 -3.2815347][-3.630918 -3.9337616 -3.8605759 -3.6202455 -3.2465563 -2.7673163 -2.6583695 -2.976933 -3.2795777 -3.4140046 -3.6093059 -3.8514171 -3.9830146 -3.9247186 -3.7411323][-3.7048471 -3.9667618 -3.87911 -3.5923014 -3.0946665 -2.5354042 -2.4372747 -2.8421912 -3.2782557 -3.5407658 -3.8609829 -4.2190981 -4.4367781 -4.4358082 -4.235918][-3.46398 -3.7246535 -3.6684575 -3.3500209 -2.746563 -2.1700306 -2.1559212 -2.6838574 -3.2712636 -3.650919 -4.0386744 -4.4473653 -4.6767073 -4.6694531 -4.4224997][-2.7868116 -3.0314927 -3.0504088 -2.7783482 -2.1744614 -1.6580205 -1.6914217 -2.204689 -2.7882559 -3.1518502 -3.5079784 -3.9154377 -4.131433 -4.09875 -3.8308425][-1.9485178 -2.1916385 -2.3482625 -2.2232685 -1.7807219 -1.4737468 -1.6245744 -2.0625138 -2.4683435 -2.6371064 -2.8050611 -3.0759401 -3.1614063 -3.0479646 -2.8065772][-1.3831058 -1.68414 -1.9951749 -1.9925909 -1.7102408 -1.6181262 -1.9284642 -2.3617175 -2.6597934 -2.7039714 -2.7201447 -2.7991309 -2.6700664 -2.4062366 -2.1655657][-0.93712187 -1.3453915 -1.7898018 -1.8337562 -1.5892107 -1.5317633 -1.8329041 -2.198595 -2.410054 -2.4051807 -2.4166086 -2.498014 -2.3736892 -2.1279547 -1.9537709][-0.5257051 -0.97636771 -1.5205221 -1.692513 -1.6492963 -1.792299 -2.1909282 -2.5497906 -2.6578574 -2.5093288 -2.411164 -2.4283218 -2.2969692 -2.0425937 -1.8585367][-0.85335612 -1.3047585 -1.8683703 -2.13293 -2.2591531 -2.5642867 -3.0319238 -3.3787746 -3.4031191 -3.1421192 -2.94519 -2.881289 -2.6999059 -2.4090343 -2.2008753][-1.3711271 -1.8132153 -2.3144579 -2.5403757 -2.6417789 -2.8719647 -3.237792 -3.5056417 -3.4975033 -3.2511342 -3.0863895 -3.0508304 -2.9123683 -2.6865644 -2.5358138][-1.5403028 -1.9123065 -2.3093827 -2.5172095 -2.6279705 -2.7976422 -3.0618215 -3.2798011 -3.295418 -3.1202686 -3.0041637 -2.9860835 -2.8876638 -2.711242 -2.5919788][-1.6664104 -1.9217651 -2.1864164 -2.3670907 -2.4978065 -2.644062 -2.84616 -3.0382884 -3.0866542 -2.9788978 -2.9019604 -2.9004087 -2.841249 -2.6948392 -2.5806618][-1.7352686 -1.8929231 -2.0490668 -2.185955 -2.3182189 -2.4400015 -2.5777225 -2.7105703 -2.7406414 -2.6520429 -2.575408 -2.5575523 -2.50904 -2.3981628 -2.320004]]...]
INFO - root - 2017-12-07 04:39:47.793818: step 5910, loss = 0.72, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 69h:58m:17s remains)
INFO - root - 2017-12-07 04:39:55.422416: step 5920, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.749 sec/batch; 67h:57m:06s remains)
INFO - root - 2017-12-07 04:40:02.920307: step 5930, loss = 0.84, batch loss = 0.76 (10.7 examples/sec; 0.744 sec/batch; 67h:31m:31s remains)
INFO - root - 2017-12-07 04:40:10.628490: step 5940, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 68h:57m:46s remains)
INFO - root - 2017-12-07 04:40:18.171980: step 5950, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.759 sec/batch; 68h:49m:12s remains)
INFO - root - 2017-12-07 04:40:25.779521: step 5960, loss = 0.53, batch loss = 0.46 (10.5 examples/sec; 0.762 sec/batch; 69h:05m:47s remains)
INFO - root - 2017-12-07 04:40:33.405428: step 5970, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.745 sec/batch; 67h:32m:20s remains)
INFO - root - 2017-12-07 04:40:41.096656: step 5980, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.753 sec/batch; 68h:18m:19s remains)
INFO - root - 2017-12-07 04:40:48.648283: step 5990, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.756 sec/batch; 68h:34m:32s remains)
INFO - root - 2017-12-07 04:40:56.320147: step 6000, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 69h:23m:44s remains)
2017-12-07 04:40:56.911078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.951071 -0.78616285 -0.68875313 -0.90664721 -1.0157287 -1.1780143 -1.2771754 -1.1827326 -1.0641451 -0.46983767 0.19982004 0.07010603 -0.48465109 -0.620239 -0.581233][-0.99987721 -0.8754344 -0.73436332 -0.8911891 -0.99197483 -1.0571611 -1.0346358 -0.98821712 -1.0386813 -0.645041 -0.12651682 -0.17907667 -0.5176301 -0.55611587 -0.63519144][-1.2685418 -1.1280584 -0.93945265 -1.0274239 -1.0845993 -1.0516763 -1.0016754 -1.1142843 -1.334585 -1.1086917 -0.74748206 -0.7390008 -0.90011358 -0.94579029 -1.1876314][-1.4452443 -1.2535849 -1.0774801 -1.1754894 -1.1631365 -1.0260782 -0.97841191 -1.2491603 -1.6350906 -1.6113586 -1.473563 -1.4548223 -1.4155409 -1.3957908 -1.7487314][-1.3721282 -1.1444697 -0.97292018 -1.0294313 -0.90087819 -0.72155643 -0.72467113 -1.0804257 -1.5739467 -1.8016918 -2.0129423 -2.1050711 -1.907769 -1.7305186 -2.0278711][-1.1994483 -0.9359479 -0.634707 -0.44283581 -0.17351103 -0.093317509 -0.26187325 -0.64970231 -1.0952024 -1.4797523 -2.0309143 -2.3399944 -2.1750004 -1.9426181 -2.1583846][-1.0761888 -0.94572425 -0.582252 -0.08792305 0.34568548 0.30913258 0.029973507 -0.22463608 -0.43100429 -0.86223626 -1.6702967 -2.1802318 -2.1538036 -1.9778562 -2.1932249][-0.92860579 -1.1621726 -0.95860362 -0.26023865 0.36931562 0.36307621 0.12542772 0.12912273 0.25701571 -0.13107347 -1.1228745 -1.8641176 -2.1241889 -2.0967321 -2.3129542][-0.76893282 -1.3923385 -1.4473872 -0.706969 0.020429611 0.12352324 0.040965557 0.35846567 0.81307364 0.49978971 -0.57674122 -1.5739694 -2.2210484 -2.4222577 -2.6320369][-0.57903886 -1.5611341 -1.9280014 -1.333756 -0.64826536 -0.42484522 -0.28716946 0.29264259 0.95175123 0.75580025 -0.22095537 -1.2923708 -2.1757171 -2.6108918 -2.8871756][-0.43250346 -1.6672325 -2.2690208 -1.8868039 -1.3142416 -0.9963088 -0.71308136 -0.052425861 0.66726542 0.63617182 -0.084511757 -1.0534713 -2.008152 -2.6466541 -3.0498633][-0.34185076 -1.6167004 -2.2654631 -2.0231664 -1.6074657 -1.3372204 -1.1036012 -0.51078939 0.21841574 0.35595322 -0.12768459 -0.93212986 -1.8345261 -2.58339 -3.1026793][-0.2980628 -1.3781066 -1.9270856 -1.7824981 -1.5017691 -1.317673 -1.2521636 -0.8486774 -0.22285271 -0.038244247 -0.38527441 -1.0372643 -1.7709763 -2.4768248 -3.0787644][-0.41369963 -1.1202931 -1.4091809 -1.2478504 -1.0335405 -0.95838928 -1.0993977 -0.96001291 -0.548136 -0.42736244 -0.75539517 -1.3320851 -1.9085948 -2.4834518 -3.092118][-0.72114468 -1.1180325 -1.1413391 -0.87006426 -0.63968992 -0.66413045 -0.99334645 -1.0980544 -0.90266681 -0.83141971 -1.1627922 -1.7566843 -2.2642548 -2.7053862 -3.2186315]]...]
INFO - root - 2017-12-07 04:41:04.648959: step 6010, loss = 0.98, batch loss = 0.90 (10.4 examples/sec; 0.767 sec/batch; 69h:33m:23s remains)
INFO - root - 2017-12-07 04:41:12.376855: step 6020, loss = 0.90, batch loss = 0.83 (10.5 examples/sec; 0.765 sec/batch; 69h:25m:19s remains)
INFO - root - 2017-12-07 04:41:19.896417: step 6030, loss = 0.59, batch loss = 0.51 (10.6 examples/sec; 0.755 sec/batch; 68h:29m:55s remains)
INFO - root - 2017-12-07 04:41:27.693611: step 6040, loss = 0.73, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 69h:55m:40s remains)
INFO - root - 2017-12-07 04:41:35.454702: step 6050, loss = 0.75, batch loss = 0.68 (9.8 examples/sec; 0.813 sec/batch; 73h:43m:23s remains)
INFO - root - 2017-12-07 04:41:43.246229: step 6060, loss = 1.04, batch loss = 0.97 (10.4 examples/sec; 0.767 sec/batch; 69h:31m:07s remains)
INFO - root - 2017-12-07 04:41:50.973360: step 6070, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 69h:08m:55s remains)
INFO - root - 2017-12-07 04:41:58.636588: step 6080, loss = 0.66, batch loss = 0.59 (10.8 examples/sec; 0.741 sec/batch; 67h:10m:48s remains)
INFO - root - 2017-12-07 04:42:06.305492: step 6090, loss = 0.79, batch loss = 0.71 (10.4 examples/sec; 0.767 sec/batch; 69h:35m:12s remains)
INFO - root - 2017-12-07 04:42:13.982398: step 6100, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.776 sec/batch; 70h:19m:20s remains)
2017-12-07 04:42:14.614540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3995645 -2.4248908 -2.4529374 -2.4877188 -2.5215192 -2.5479817 -2.5694396 -2.5857935 -2.5893683 -2.5780129 -2.5491366 -2.5019143 -2.4470332 -2.4141204 -2.4080453][-2.4329805 -2.4670317 -2.5162611 -2.5804763 -2.6528094 -2.717154 -2.7727802 -2.8094745 -2.8174992 -2.7825828 -2.702632 -2.5976472 -2.4898996 -2.4193401 -2.3870816][-2.5000069 -2.5454876 -2.609678 -2.6810145 -2.7506564 -2.7915401 -2.8207984 -2.846946 -2.8722606 -2.8609488 -2.7852139 -2.6768796 -2.5558751 -2.4606705 -2.3964524][-2.5609441 -2.601054 -2.6547608 -2.6965823 -2.7090688 -2.6722422 -2.6376286 -2.6276331 -2.68256 -2.7464345 -2.7363229 -2.6661172 -2.5577407 -2.4568954 -2.3749037][-2.5529368 -2.5516443 -2.5659504 -2.5693779 -2.5233784 -2.4406266 -2.3932674 -2.3853023 -2.4863806 -2.6278739 -2.68784 -2.64148 -2.5366716 -2.4187403 -2.3430197][-2.4265358 -2.380846 -2.3756292 -2.3859267 -2.3342738 -2.2948768 -2.3221314 -2.3843353 -2.5617642 -2.786279 -2.9142473 -2.8580832 -2.7170014 -2.5172067 -2.4031444][-2.2089152 -2.1217742 -2.1181824 -2.1844935 -2.2328284 -2.308677 -2.4162362 -2.4984307 -2.6692371 -2.9041941 -3.0220323 -2.9309311 -2.746701 -2.4787762 -2.3575127][-2.0588825 -1.8760045 -1.8003056 -1.8797665 -2.0436535 -2.2292991 -2.4136534 -2.5109081 -2.6528268 -2.8370309 -2.8449163 -2.7003593 -2.514751 -2.2684329 -2.2181995][-1.9494872 -1.6496887 -1.454741 -1.4693968 -1.6596291 -1.8941519 -2.148576 -2.2949066 -2.4309223 -2.5587175 -2.4780927 -2.3391976 -2.2202246 -2.0895903 -2.1208777][-1.6658232 -1.3641353 -1.1488035 -1.1259232 -1.2970715 -1.5143156 -1.7836869 -1.9294903 -2.0270884 -2.08417 -1.947315 -1.8414025 -1.7847316 -1.7695959 -1.853498][-1.7222338 -1.6170204 -1.517144 -1.4980631 -1.5723679 -1.6500576 -1.7744367 -1.8266425 -1.8723996 -1.8836844 -1.7796526 -1.7626619 -1.7738407 -1.8351557 -1.9188542][-2.4862967 -2.612103 -2.6320057 -2.5954533 -2.5284224 -2.4396026 -2.368993 -2.289957 -2.2654445 -2.248868 -2.2138982 -2.2765126 -2.33204 -2.3940213 -2.4256749][-3.4120321 -3.5837076 -3.5845354 -3.4679103 -3.2907205 -3.1444016 -3.0217214 -2.9306018 -2.9018521 -2.8974335 -2.9518843 -3.0590706 -3.107914 -3.0949097 -3.021244][-3.6609941 -3.708832 -3.5827489 -3.4075875 -3.2473664 -3.210495 -3.2049735 -3.1805961 -3.1430898 -3.1129887 -3.2005482 -3.3341565 -3.4031296 -3.3550782 -3.1900482][-2.9448338 -2.8128314 -2.5182054 -2.33318 -2.271013 -2.3668025 -2.4816966 -2.4827476 -2.3792679 -2.2844388 -2.3372054 -2.4693785 -2.5867152 -2.580091 -2.3902555]]...]
INFO - root - 2017-12-07 04:42:22.542383: step 6110, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.773 sec/batch; 70h:04m:16s remains)
INFO - root - 2017-12-07 04:42:30.309907: step 6120, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.767 sec/batch; 69h:30m:55s remains)
INFO - root - 2017-12-07 04:42:37.847332: step 6130, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.755 sec/batch; 68h:27m:41s remains)
INFO - root - 2017-12-07 04:42:45.500442: step 6140, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.773 sec/batch; 70h:01m:54s remains)
INFO - root - 2017-12-07 04:42:53.277774: step 6150, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 69h:10m:44s remains)
INFO - root - 2017-12-07 04:43:01.080492: step 6160, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.782 sec/batch; 70h:53m:55s remains)
INFO - root - 2017-12-07 04:43:08.969311: step 6170, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.769 sec/batch; 69h:40m:12s remains)
INFO - root - 2017-12-07 04:43:16.741581: step 6180, loss = 0.92, batch loss = 0.85 (10.3 examples/sec; 0.774 sec/batch; 70h:08m:34s remains)
INFO - root - 2017-12-07 04:43:24.637210: step 6190, loss = 0.94, batch loss = 0.86 (10.4 examples/sec; 0.769 sec/batch; 69h:39m:51s remains)
INFO - root - 2017-12-07 04:43:32.296667: step 6200, loss = 0.68, batch loss = 0.61 (10.5 examples/sec; 0.764 sec/batch; 69h:13m:50s remains)
2017-12-07 04:43:32.908807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.575902 -2.2202568 -1.6953802 -1.486227 -1.3648245 -1.107578 -0.693048 -0.2693429 -0.47706795 -1.1692364 -1.6880505 -1.6395705 -1.384001 -1.0589409 -0.7305553][-2.8305025 -2.2927759 -1.5596871 -1.2134674 -1.027967 -0.80350447 -0.47701478 -0.1188097 -0.34294271 -1.0716679 -1.5661061 -1.4510422 -1.2228892 -0.93800855 -0.60893512][-2.6769779 -2.0065188 -1.2512932 -0.89039111 -0.73245144 -0.69962025 -0.70258641 -0.6275847 -0.88354325 -1.4482238 -1.6429813 -1.3388214 -1.0549698 -0.77111006 -0.41261816][-2.6718054 -1.9184823 -1.3138561 -1.0616062 -0.90245295 -0.9000237 -1.0379519 -1.1400096 -1.4640589 -1.920511 -1.8876815 -1.4980969 -1.202759 -0.89370632 -0.418015][-2.760694 -2.1197934 -1.8090262 -1.7106652 -1.5631268 -1.4934762 -1.4526434 -1.3418522 -1.5187359 -1.8378308 -1.7536893 -1.4580178 -1.2640195 -1.0510123 -0.55506706][-2.5366259 -2.2204518 -2.1977704 -2.1409843 -1.9588957 -1.8944519 -1.7655637 -1.4717522 -1.4669363 -1.6450918 -1.5727971 -1.3361731 -1.0511851 -0.76059628 -0.23444366][-2.1666484 -2.1891291 -2.2697344 -1.9620464 -1.4375203 -1.1604276 -0.99679923 -0.76615405 -0.84438467 -1.1100614 -1.2818685 -1.2393043 -0.84962368 -0.33734989 0.28481245][-1.8019962 -2.0637474 -2.1864216 -1.6963923 -0.85526252 -0.25789642 0.077462673 0.44059753 0.50353432 0.29313469 -0.13569832 -0.42114067 -0.18525934 0.25395393 0.741786][-1.2423308 -1.5377889 -1.6416454 -1.2805037 -0.680007 -0.30821323 -0.077421188 0.40439034 0.85518169 1.0933313 0.86275673 0.57279968 0.72153473 1.0174365 1.2644119][-0.87999249 -1.0949457 -1.0543604 -0.79054904 -0.52184367 -0.54017544 -0.64024138 -0.41198397 0.009475708 0.41607714 0.39459181 0.30930567 0.53920889 0.9060688 1.2321353][-0.87259388 -0.94087744 -0.7395246 -0.4204464 -0.17926884 -0.28137016 -0.50266171 -0.48575449 -0.31948519 -0.16255951 -0.30436325 -0.34229755 -0.11066532 0.25914764 0.68494463][-0.945632 -0.81597018 -0.56646752 -0.30180597 -0.13976669 -0.2936554 -0.48663735 -0.40659666 -0.18743563 -0.067222595 -0.15141964 -0.089731216 0.095949173 0.28691769 0.50916243][-1.01741 -0.76891971 -0.64151335 -0.61587977 -0.68502855 -0.96104813 -1.1374788 -0.96564674 -0.58788657 -0.26092291 0.0010795593 0.2859354 0.3936553 0.34507322 0.25615597][-1.3322029 -1.1698403 -1.2718387 -1.4586329 -1.6124997 -1.7852306 -1.7833257 -1.4864695 -1.0713742 -0.7663064 -0.44250607 -0.13815212 -0.16408634 -0.3617425 -0.640676][-1.7750106 -1.7706296 -2.0208814 -2.2738669 -2.4083664 -2.4214172 -2.207324 -1.7714641 -1.3838499 -1.2309656 -1.0745711 -0.95260382 -1.1177559 -1.3285718 -1.5906487]]...]
INFO - root - 2017-12-07 04:43:40.614585: step 6210, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.751 sec/batch; 68h:06m:24s remains)
INFO - root - 2017-12-07 04:43:48.375750: step 6220, loss = 0.90, batch loss = 0.83 (10.5 examples/sec; 0.762 sec/batch; 69h:04m:16s remains)
INFO - root - 2017-12-07 04:43:55.796813: step 6230, loss = 0.91, batch loss = 0.84 (10.3 examples/sec; 0.775 sec/batch; 70h:16m:34s remains)
INFO - root - 2017-12-07 04:44:03.572686: step 6240, loss = 0.70, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 69h:07m:00s remains)
INFO - root - 2017-12-07 04:44:11.353537: step 6250, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.771 sec/batch; 69h:53m:13s remains)
INFO - root - 2017-12-07 04:44:19.044257: step 6260, loss = 0.93, batch loss = 0.86 (10.4 examples/sec; 0.767 sec/batch; 69h:32m:37s remains)
INFO - root - 2017-12-07 04:44:26.749141: step 6270, loss = 0.74, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 71h:41m:52s remains)
INFO - root - 2017-12-07 04:44:34.537755: step 6280, loss = 1.00, batch loss = 0.93 (10.5 examples/sec; 0.763 sec/batch; 69h:08m:49s remains)
INFO - root - 2017-12-07 04:44:42.296777: step 6290, loss = 0.72, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 70h:28m:59s remains)
INFO - root - 2017-12-07 04:44:49.970775: step 6300, loss = 1.03, batch loss = 0.96 (10.5 examples/sec; 0.759 sec/batch; 68h:48m:41s remains)
2017-12-07 04:44:50.629536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2589459 -2.1890028 -2.6525364 -1.9154437 -0.15110779 1.7157445 2.0881195 0.78590107 -0.72612691 -1.5652099 -1.5632946 -1.0840714 -0.54862738 -0.27200222 -0.33700991][-1.5518765 -2.5728838 -3.0197039 -2.1573005 -0.30157089 1.7954278 2.6130557 1.4257708 -0.33184004 -1.4976478 -1.6895027 -1.1847141 -0.53091311 -0.15990734 -0.21575403][-1.7824934 -2.885231 -3.3147454 -2.359689 -0.49535751 1.71169 2.9676542 2.1223297 0.32294035 -1.1035202 -1.5212741 -1.0428209 -0.33575916 0.052715778 -0.0061774254][-1.9321833 -3.1099412 -3.5493181 -2.543684 -0.78297687 1.3185048 2.8268204 2.436873 0.81738281 -0.74264121 -1.3911545 -1.0401797 -0.38877535 -0.059195518 -0.12053633][-2.0183244 -3.225158 -3.6568091 -2.6095262 -0.98039627 0.84780455 2.3072333 2.235167 0.86482525 -0.72377133 -1.5884628 -1.4731443 -0.95445132 -0.64844441 -0.60786319][-2.0740876 -3.2451642 -3.6183865 -2.5438266 -1.0426638 0.43027449 1.6230788 1.7241769 0.77905226 -0.56094623 -1.472595 -1.6103396 -1.2950504 -0.99468875 -0.82397556][-2.1102755 -3.2088509 -3.4801617 -2.4043934 -1.0363045 0.063163757 0.85497284 0.96950245 0.51122952 -0.35092735 -1.1257825 -1.4867687 -1.4667528 -1.293469 -1.1425107][-2.1035383 -3.1238382 -3.3046956 -2.2438071 -1.024785 -0.28842211 -0.030076981 -0.13219547 -0.20996618 -0.46519876 -0.9457047 -1.4482713 -1.7420425 -1.7720051 -1.7039771][-2.0773742 -3.0723052 -3.2450235 -2.2257063 -1.1167724 -0.61315942 -0.82408023 -1.2748351 -1.176878 -0.85407686 -0.96523857 -1.5235062 -2.0912097 -2.3397393 -2.3514318][-2.0848739 -3.1082032 -3.3542891 -2.4064968 -1.2888846 -0.781662 -1.1714947 -1.9036398 -1.7658343 -0.99217987 -0.74422717 -1.3532312 -2.2341237 -2.7934294 -2.9446235][-2.1031506 -3.1806679 -3.5758655 -2.7849531 -1.6951632 -1.167068 -1.5951672 -2.4814816 -2.3299787 -1.1960554 -0.52231312 -1.0239754 -2.095412 -2.9547412 -3.3029616][-2.0167847 -3.1121485 -3.6725936 -3.1301982 -2.1636298 -1.6835599 -2.1190994 -3.0125666 -2.8553689 -1.5285387 -0.54812694 -0.89741182 -2.014256 -3.0372744 -3.5168438][-1.7840705 -2.8330221 -3.5120521 -3.2305121 -2.4246488 -2.0007994 -2.3912694 -3.1867342 -3.057518 -1.7514329 -0.68887544 -0.93827367 -2.0197303 -3.0609269 -3.560338][-1.4486115 -2.37987 -3.0770583 -3.0030124 -2.3733337 -1.9930482 -2.2811267 -2.9024115 -2.8044066 -1.6943944 -0.72789454 -0.89198995 -1.8354602 -2.7520256 -3.2003968][-1.1197345 -1.8840768 -2.5154138 -2.6059413 -2.2023797 -1.8955417 -2.0233285 -2.3667638 -2.2130353 -1.3558645 -0.58449674 -0.66921663 -1.4020865 -2.1377213 -2.5164869]]...]
INFO - root - 2017-12-07 04:44:58.241176: step 6310, loss = 0.87, batch loss = 0.79 (10.6 examples/sec; 0.755 sec/batch; 68h:24m:06s remains)
INFO - root - 2017-12-07 04:45:05.875719: step 6320, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.758 sec/batch; 68h:41m:01s remains)
INFO - root - 2017-12-07 04:45:13.263742: step 6330, loss = 0.64, batch loss = 0.57 (10.1 examples/sec; 0.789 sec/batch; 71h:31m:17s remains)
INFO - root - 2017-12-07 04:45:20.892864: step 6340, loss = 1.06, batch loss = 0.99 (10.2 examples/sec; 0.782 sec/batch; 70h:52m:25s remains)
INFO - root - 2017-12-07 04:45:28.451357: step 6350, loss = 0.70, batch loss = 0.62 (10.6 examples/sec; 0.751 sec/batch; 68h:03m:35s remains)
INFO - root - 2017-12-07 04:45:36.075527: step 6360, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.771 sec/batch; 69h:51m:22s remains)
INFO - root - 2017-12-07 04:45:43.677580: step 6370, loss = 0.70, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 69h:28m:56s remains)
INFO - root - 2017-12-07 04:45:51.268560: step 6380, loss = 0.69, batch loss = 0.62 (10.9 examples/sec; 0.733 sec/batch; 66h:21m:34s remains)
INFO - root - 2017-12-07 04:45:58.833762: step 6390, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.787 sec/batch; 71h:19m:26s remains)
INFO - root - 2017-12-07 04:46:06.478784: step 6400, loss = 0.72, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 70h:40m:28s remains)
2017-12-07 04:46:07.187986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5057716 -2.4260936 -2.4515758 -2.7198715 -2.8324409 -2.7889981 -2.8178606 -2.9080176 -3.0136294 -3.0156035 -2.8457253 -2.4661112 -2.3076832 -2.4990716 -2.4235649][-2.5476367 -2.3764179 -2.2704093 -2.5508416 -2.7655702 -2.7663059 -2.8173671 -2.9345403 -3.1390214 -3.3382795 -3.2621369 -2.776051 -2.4684513 -2.5584941 -2.5322874][-2.7130096 -2.3863583 -2.0943491 -2.3276365 -2.6098936 -2.6427107 -2.6300883 -2.7361159 -3.1159325 -3.5464816 -3.5759299 -3.0529284 -2.6250138 -2.6196589 -2.6618493][-3.0581779 -2.5493064 -2.0248282 -2.0526087 -2.213042 -2.1498044 -1.970571 -1.9854205 -2.5062249 -3.1556766 -3.3415132 -2.8769217 -2.3725219 -2.3181529 -2.4585168][-3.1859493 -2.6643658 -2.0107152 -1.7723651 -1.7063582 -1.5500646 -1.2552371 -1.1810222 -1.7754223 -2.5829632 -2.9114244 -2.5239639 -1.9086413 -1.6897745 -1.8156829][-3.144 -2.7401474 -2.0667512 -1.629735 -1.4091542 -1.2744548 -0.980942 -0.85359383 -1.4733441 -2.3912191 -2.8485992 -2.5498214 -1.7775943 -1.2437794 -1.145597][-3.0618577 -2.7679553 -2.1092489 -1.5927186 -1.3748157 -1.3662498 -1.1573796 -1.0095863 -1.5420184 -2.4326944 -2.987463 -2.8287072 -1.996475 -1.1789019 -0.83329582][-2.9615402 -2.7245946 -2.1326084 -1.6792665 -1.5644031 -1.6554265 -1.5044119 -1.3566229 -1.7617567 -2.5223987 -3.1520207 -3.2106757 -2.5513463 -1.701195 -1.2533929][-2.7809238 -2.5803902 -2.091074 -1.7594743 -1.748704 -1.8603206 -1.7165158 -1.6012547 -1.9321291 -2.5729198 -3.2307577 -3.5079627 -3.2067771 -2.6161942 -2.2518895][-2.6783619 -2.5415397 -2.1653538 -1.9218197 -1.9602849 -2.0693519 -1.9762096 -1.9292777 -2.2204702 -2.7240224 -3.3138914 -3.6434832 -3.5784101 -3.2650986 -3.030246][-2.9489725 -2.9245839 -2.6419637 -2.3964834 -2.3937769 -2.4780102 -2.4546821 -2.4833436 -2.7493284 -3.1294699 -3.5466495 -3.7045705 -3.5832167 -3.3424842 -3.1968858][-3.5043216 -3.5379522 -3.2697668 -2.9362035 -2.8240876 -2.89353 -2.9725204 -3.1029706 -3.3375168 -3.5844784 -3.739928 -3.5695491 -3.2166283 -2.9937172 -3.0374596][-3.8493149 -3.8802528 -3.6248412 -3.2831807 -3.1531634 -3.2613282 -3.4499698 -3.6323836 -3.7578635 -3.8054922 -3.6776242 -3.2306442 -2.7424424 -2.5841165 -2.8120975][-3.8539357 -3.865958 -3.6600494 -3.4214015 -3.4009514 -3.6034627 -3.8616929 -4.0361233 -4.057714 -3.9683716 -3.6966524 -3.1734133 -2.7006052 -2.5756564 -2.7735839][-3.9235282 -3.9013004 -3.7225535 -3.5761919 -3.6281919 -3.8501046 -4.0994735 -4.2623391 -4.2729559 -4.1871243 -3.9079266 -3.4157629 -3.0429711 -2.929872 -2.9767129]]...]
INFO - root - 2017-12-07 04:46:14.771653: step 6410, loss = 0.70, batch loss = 0.62 (10.7 examples/sec; 0.748 sec/batch; 67h:42m:32s remains)
INFO - root - 2017-12-07 04:46:22.375603: step 6420, loss = 0.81, batch loss = 0.73 (10.7 examples/sec; 0.747 sec/batch; 67h:38m:12s remains)
INFO - root - 2017-12-07 04:46:29.747282: step 6430, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.759 sec/batch; 68h:43m:07s remains)
INFO - root - 2017-12-07 04:46:37.403478: step 6440, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.757 sec/batch; 68h:31m:44s remains)
INFO - root - 2017-12-07 04:46:45.022844: step 6450, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.762 sec/batch; 68h:59m:30s remains)
INFO - root - 2017-12-07 04:46:52.626838: step 6460, loss = 1.03, batch loss = 0.95 (10.4 examples/sec; 0.766 sec/batch; 69h:22m:28s remains)
INFO - root - 2017-12-07 04:47:00.229895: step 6470, loss = 0.92, batch loss = 0.84 (10.2 examples/sec; 0.787 sec/batch; 71h:18m:49s remains)
INFO - root - 2017-12-07 04:47:07.825901: step 6480, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.763 sec/batch; 69h:05m:26s remains)
INFO - root - 2017-12-07 04:47:15.402412: step 6490, loss = 0.91, batch loss = 0.84 (10.7 examples/sec; 0.747 sec/batch; 67h:36m:29s remains)
INFO - root - 2017-12-07 04:47:23.008544: step 6500, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.764 sec/batch; 69h:10m:59s remains)
2017-12-07 04:47:23.662610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5970778 -4.00872 -4.3153062 -4.3016891 -3.8430488 -3.240731 -3.0342546 -2.9311824 -2.730968 -2.609581 -2.1659286 -1.6783643 -2.1088259 -3.0373197 -3.5089781][-3.8597987 -4.3244267 -4.5679479 -4.3534608 -3.6265652 -2.8387971 -2.6865835 -2.7591481 -2.6578846 -2.422209 -1.8472636 -1.3188157 -1.8851805 -3.0623693 -3.6230493][-3.972882 -4.33696 -4.4484997 -4.1112347 -3.2842846 -2.4383578 -2.2719603 -2.4013529 -2.3156164 -1.8899231 -1.2814605 -0.91527581 -1.7207472 -3.1325164 -3.7037463][-4.0787306 -4.2233953 -4.0881896 -3.6667681 -2.945189 -2.2329578 -2.022444 -2.0567429 -1.8520169 -1.2093077 -0.64980221 -0.59269071 -1.6979666 -3.3198779 -3.8634169][-4.2305059 -4.1570191 -3.7638044 -3.2485666 -2.6869886 -2.1958146 -1.9342678 -1.7787347 -1.4098756 -0.65332294 -0.20444536 -0.48758459 -1.8480656 -3.5329144 -4.01581][-4.3306074 -4.147944 -3.6148109 -3.041949 -2.5804729 -2.2594938 -1.9404659 -1.5869889 -1.0730207 -0.32103062 -0.032032013 -0.61392641 -2.1380787 -3.6950047 -4.0181603][-4.3500094 -4.1779919 -3.6938381 -3.0644417 -2.5369329 -2.3094163 -1.9877367 -1.4538202 -0.79893136 -0.090111732 0.0074391365 -0.85991383 -2.4627295 -3.742218 -3.8580956][-4.3198371 -4.2354455 -3.8859992 -3.1911297 -2.5056324 -2.2242727 -1.8499436 -1.2149248 -0.51758409 0.066113472 -0.041928768 -1.1362588 -2.6946905 -3.5737526 -3.5040359][-4.2444854 -4.196444 -3.9077454 -3.1584802 -2.3060291 -1.8698621 -1.4206536 -0.79604554 -0.2408843 0.08017683 -0.27551794 -1.5468287 -2.9557302 -3.3978424 -3.1655867][-4.1685205 -4.0121312 -3.6271682 -2.8519917 -1.9476368 -1.4147322 -0.92753434 -0.41099119 -0.13661146 -0.14547634 -0.70752716 -2.0280597 -3.2282724 -3.3694286 -3.0779142][-4.1112318 -3.7692904 -3.1823137 -2.3371682 -1.4294188 -0.92674327 -0.55476618 -0.27369785 -0.35125017 -0.595171 -1.1729128 -2.3833065 -3.3693833 -3.3495808 -3.1000245][-3.9645371 -3.465344 -2.668191 -1.7469273 -0.97808909 -0.64186692 -0.45986581 -0.45894504 -0.80339289 -1.0948963 -1.6024613 -2.6743345 -3.4497898 -3.3743615 -3.2369738][-3.6750331 -3.0507498 -2.1268439 -1.2579474 -0.80607629 -0.72655034 -0.76217222 -0.99184728 -1.4475954 -1.7127185 -2.1677177 -3.0589895 -3.6014738 -3.5413136 -3.5361779][-3.3988826 -2.7107396 -1.7969675 -1.1293006 -0.95880485 -1.0695524 -1.2732313 -1.6646833 -2.11754 -2.3482454 -2.7579613 -3.4265981 -3.7457771 -3.7061772 -3.7581787][-3.252758 -2.65351 -1.9586637 -1.5409727 -1.4722817 -1.5820792 -1.8539042 -2.2558231 -2.5985646 -2.796968 -3.1527009 -3.6046498 -3.7499092 -3.6913869 -3.6842523]]...]
INFO - root - 2017-12-07 04:47:31.234631: step 6510, loss = 0.58, batch loss = 0.51 (10.6 examples/sec; 0.754 sec/batch; 68h:18m:23s remains)
INFO - root - 2017-12-07 04:47:38.843381: step 6520, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.752 sec/batch; 68h:04m:38s remains)
INFO - root - 2017-12-07 04:47:46.250243: step 6530, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.787 sec/batch; 71h:16m:06s remains)
INFO - root - 2017-12-07 04:47:53.972457: step 6540, loss = 0.73, batch loss = 0.65 (10.6 examples/sec; 0.753 sec/batch; 68h:11m:26s remains)
INFO - root - 2017-12-07 04:48:01.664329: step 6550, loss = 0.99, batch loss = 0.92 (10.6 examples/sec; 0.753 sec/batch; 68h:10m:54s remains)
INFO - root - 2017-12-07 04:48:09.489966: step 6560, loss = 0.84, batch loss = 0.77 (9.7 examples/sec; 0.826 sec/batch; 74h:46m:51s remains)
INFO - root - 2017-12-07 04:48:17.081771: step 6570, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.764 sec/batch; 69h:10m:27s remains)
INFO - root - 2017-12-07 04:48:24.828099: step 6580, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.766 sec/batch; 69h:19m:26s remains)
INFO - root - 2017-12-07 04:48:32.600985: step 6590, loss = 0.79, batch loss = 0.72 (10.1 examples/sec; 0.794 sec/batch; 71h:50m:42s remains)
INFO - root - 2017-12-07 04:48:40.261127: step 6600, loss = 0.77, batch loss = 0.69 (10.6 examples/sec; 0.751 sec/batch; 68h:01m:01s remains)
2017-12-07 04:48:40.846119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0488539 -4.7937865 -3.8998284 -2.7975862 -1.943171 -1.3512282 -0.89986634 -0.33954906 0.20919609 0.33848429 0.36361313 0.36942768 -0.13894463 -1.1326232 -1.7833874][-4.9683843 -4.6872211 -3.7526116 -2.6384788 -1.843668 -1.4197187 -1.1373444 -0.59948516 0.11097813 0.44221449 0.541872 0.41334677 -0.2820549 -1.3863535 -2.0833759][-4.8329506 -4.5556116 -3.6934295 -2.7539711 -2.1895032 -2.0510809 -2.0419524 -1.5780239 -0.71372914 -0.15003633 0.058661938 -0.096609592 -0.60387468 -1.2957528 -1.7093589][-4.6682467 -4.3547797 -3.5465808 -2.7698567 -2.4223022 -2.5196891 -2.6948652 -2.3832955 -1.7106051 -1.2663648 -1.0583839 -0.96408463 -0.86498237 -0.84074807 -0.80227256][-4.5286689 -4.11062 -3.2634461 -2.5349088 -2.2720251 -2.4164896 -2.5428524 -2.3827667 -2.2543054 -2.3555574 -2.3423469 -2.008877 -1.3994577 -0.87150359 -0.49054456][-4.3722978 -3.7709649 -2.875025 -2.195658 -1.9422371 -1.8770173 -1.7001317 -1.5977678 -2.086714 -2.8472733 -3.2047939 -3.0061793 -2.4314704 -1.8602757 -1.3399823][-4.210547 -3.442733 -2.5744915 -2.0480046 -1.8118529 -1.464812 -0.89432335 -0.67882204 -1.5032835 -2.7060432 -3.383028 -3.4383364 -3.1096356 -2.6895733 -2.2324457][-4.0755725 -3.2206717 -2.4662309 -2.1665597 -1.9809368 -1.4338238 -0.45630741 0.11473227 -0.68342209 -2.0181315 -2.8735073 -3.111084 -2.9805276 -2.7956753 -2.6756873][-3.9994776 -3.1176436 -2.4352155 -2.3342445 -2.3135345 -1.8675861 -0.76742268 0.10758257 -0.44203544 -1.6545422 -2.5240421 -2.7999842 -2.6310768 -2.489923 -2.6491613][-4.0704107 -3.1418843 -2.3703558 -2.2864897 -2.440274 -2.3048491 -1.4295788 -0.57248926 -0.98154545 -1.9943416 -2.7266519 -2.8213134 -2.3220208 -1.895988 -2.060555][-4.3238678 -3.3655989 -2.3861337 -2.0861697 -2.2146375 -2.2954056 -1.8355188 -1.3157361 -1.7550988 -2.6193066 -3.1342318 -2.8665977 -1.8185027 -0.87701869 -0.93940687][-4.6560345 -3.7662015 -2.6491661 -2.0423977 -1.9605198 -2.1164589 -2.0465956 -1.8990905 -2.3844 -3.1133204 -3.4315414 -2.891418 -1.4784575 -0.19693184 -0.21046829][-4.8469453 -4.174962 -3.1566696 -2.3906138 -2.1336865 -2.3409655 -2.5761168 -2.6415749 -3.0254116 -3.5566123 -3.74416 -3.2219443 -1.9343164 -0.73208952 -0.73140407][-4.7460594 -4.3578491 -3.6572742 -3.0057292 -2.7877655 -3.1094742 -3.5136113 -3.6288576 -3.7927086 -4.0638242 -4.1512437 -3.7905869 -2.9030733 -2.0316803 -1.9571283][-4.412755 -4.1922817 -3.75004 -3.2920957 -3.186389 -3.526607 -3.9032223 -3.9920011 -4.0298214 -4.1367936 -4.198266 -4.0445938 -3.5788131 -3.0295269 -2.8684225]]...]
INFO - root - 2017-12-07 04:48:48.526927: step 6610, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.774 sec/batch; 70h:05m:05s remains)
INFO - root - 2017-12-07 04:48:56.146879: step 6620, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.780 sec/batch; 70h:38m:42s remains)
INFO - root - 2017-12-07 04:49:03.612934: step 6630, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.763 sec/batch; 69h:01m:37s remains)
INFO - root - 2017-12-07 04:49:11.327391: step 6640, loss = 0.92, batch loss = 0.85 (11.0 examples/sec; 0.730 sec/batch; 66h:05m:09s remains)
INFO - root - 2017-12-07 04:49:19.082277: step 6650, loss = 1.02, batch loss = 0.95 (10.6 examples/sec; 0.757 sec/batch; 68h:33m:49s remains)
INFO - root - 2017-12-07 04:49:26.639828: step 6660, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.770 sec/batch; 69h:39m:07s remains)
INFO - root - 2017-12-07 04:49:34.302980: step 6670, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.766 sec/batch; 69h:18m:15s remains)
INFO - root - 2017-12-07 04:49:41.911981: step 6680, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.761 sec/batch; 68h:51m:59s remains)
INFO - root - 2017-12-07 04:49:49.604041: step 6690, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.780 sec/batch; 70h:34m:29s remains)
INFO - root - 2017-12-07 04:49:57.416802: step 6700, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.763 sec/batch; 69h:01m:14s remains)
2017-12-07 04:49:58.083391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4361382 -3.0580058 -3.6131845 -3.6128645 -3.0566468 -2.3354952 -1.8200159 -1.9996448 -2.5977015 -3.0776687 -3.2984257 -3.6082191 -3.6873872 -3.30059 -2.4942217][-2.3855176 -3.0894976 -3.6194041 -3.624896 -3.1859481 -2.5425682 -2.0507965 -2.1825426 -2.6477873 -2.9823465 -3.0764217 -3.3120179 -3.3458424 -2.8576629 -2.0648382][-2.2323618 -2.9442716 -3.3993134 -3.52217 -3.2966762 -2.7637293 -2.2532818 -2.2002089 -2.4319322 -2.5578485 -2.4999743 -2.7117984 -2.8169365 -2.4119852 -1.7966402][-1.940717 -2.745786 -3.2582722 -3.5376811 -3.3585458 -2.7292469 -2.1388659 -1.9786029 -2.2873948 -2.5378087 -2.4267879 -2.4894967 -2.4865746 -2.1494813 -1.7489264][-1.6895056 -2.5805967 -3.3195963 -3.7699795 -3.4948707 -2.517468 -1.703639 -1.4362552 -2.0051467 -2.6806355 -2.7421157 -2.668813 -2.4775698 -2.161478 -1.9233725][-1.6918099 -2.529876 -3.4490023 -4.0697656 -3.7669909 -2.5465159 -1.494478 -1.0868013 -1.7730217 -2.7212572 -2.9877827 -2.8871229 -2.5813131 -2.2420588 -2.070363][-1.840152 -2.5958371 -3.5212147 -4.1049461 -3.7651067 -2.5436285 -1.4953189 -1.1404064 -1.8081989 -2.7506785 -3.1305375 -3.0593886 -2.6993966 -2.2846723 -2.1038167][-1.7857804 -2.6425443 -3.5411224 -3.9528196 -3.4895792 -2.3524623 -1.4588182 -1.3320291 -1.9661283 -2.6920943 -3.0139275 -2.9554663 -2.6046267 -2.1960793 -2.0442886][-1.6070848 -2.5685163 -3.4356151 -3.7150683 -3.1863978 -2.1473911 -1.3975344 -1.4698801 -2.1122718 -2.6501744 -2.8020813 -2.6485729 -2.3140788 -2.0375867 -2.0389047][-1.7997923 -2.6587882 -3.3428085 -3.5061815 -3.0251927 -2.0909486 -1.4520481 -1.6434774 -2.3171897 -2.8146052 -2.8448229 -2.5668187 -2.2325718 -2.0887396 -2.2523065][-2.1883509 -2.8507085 -3.345469 -3.4862461 -3.1665988 -2.4254618 -1.9150407 -2.0920985 -2.6396232 -3.0682979 -3.0462685 -2.7427492 -2.4756703 -2.4485893 -2.7147303][-2.3760536 -2.8148136 -3.2298422 -3.4341271 -3.330075 -2.8457594 -2.534332 -2.6969173 -3.0134931 -3.2493091 -3.184453 -2.9807029 -2.8380105 -2.8732193 -3.1204915][-2.4818194 -2.7681072 -3.1251917 -3.311089 -3.2575383 -2.9583993 -2.8429487 -3.0135145 -3.1456099 -3.1839013 -3.0962415 -3.022048 -2.9920301 -3.0308392 -3.1837823][-2.5154541 -2.7029815 -2.9798207 -3.1040816 -3.0191898 -2.7961261 -2.779562 -2.9691863 -3.0449209 -2.9998174 -2.9058852 -2.8948624 -2.9037652 -2.9134188 -2.9824224][-2.6036973 -2.6915047 -2.8509135 -2.9227018 -2.8635712 -2.7347789 -2.7636113 -2.9437854 -3.0287805 -2.9910498 -2.9062262 -2.8782902 -2.8624053 -2.8463883 -2.874366]]...]
INFO - root - 2017-12-07 04:50:05.685264: step 6710, loss = 0.70, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 69h:32m:40s remains)
INFO - root - 2017-12-07 04:50:13.359176: step 6720, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.757 sec/batch; 68h:31m:56s remains)
INFO - root - 2017-12-07 04:50:20.806810: step 6730, loss = 0.63, batch loss = 0.55 (10.2 examples/sec; 0.784 sec/batch; 70h:55m:10s remains)
INFO - root - 2017-12-07 04:50:28.470260: step 6740, loss = 0.69, batch loss = 0.61 (10.4 examples/sec; 0.772 sec/batch; 69h:52m:23s remains)
INFO - root - 2017-12-07 04:50:36.199412: step 6750, loss = 1.07, batch loss = 1.00 (10.4 examples/sec; 0.766 sec/batch; 69h:20m:34s remains)
INFO - root - 2017-12-07 04:50:43.885238: step 6760, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.753 sec/batch; 68h:08m:22s remains)
INFO - root - 2017-12-07 04:50:51.602828: step 6770, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.770 sec/batch; 69h:38m:10s remains)
INFO - root - 2017-12-07 04:50:59.292159: step 6780, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.762 sec/batch; 68h:58m:58s remains)
INFO - root - 2017-12-07 04:51:06.928470: step 6790, loss = 1.00, batch loss = 0.93 (10.5 examples/sec; 0.763 sec/batch; 69h:02m:17s remains)
INFO - root - 2017-12-07 04:51:14.530710: step 6800, loss = 0.78, batch loss = 0.71 (10.7 examples/sec; 0.745 sec/batch; 67h:25m:58s remains)
2017-12-07 04:51:15.284169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5768571 -3.8298674 -3.3745136 -2.9566541 -2.9566255 -2.974669 -3.1803598 -3.3535256 -3.5315311 -3.5944481 -3.5003443 -3.7429941 -3.5057805 -2.7575417 -2.3941565][-3.3767583 -3.5706091 -3.0004032 -2.5472426 -2.6176887 -2.6898646 -2.8272865 -2.9357913 -3.0805807 -3.113647 -3.136723 -3.4347959 -3.1455777 -2.4283288 -2.2185998][-2.9192448 -3.0870609 -2.4604287 -1.9806185 -2.0629685 -2.0775688 -2.0586908 -2.1726191 -2.36193 -2.4001985 -2.6377277 -3.003643 -2.645421 -2.0074449 -2.0490065][-2.9011674 -3.0736609 -2.4347749 -1.9037426 -1.8980818 -1.7155294 -1.428539 -1.4897103 -1.7235596 -1.8647809 -2.4056549 -2.8664243 -2.4647069 -1.9919558 -2.2767222][-3.476649 -3.5449569 -2.8446853 -2.2339172 -2.0737851 -1.620512 -0.98402405 -0.91449714 -1.223948 -1.6504331 -2.5829492 -3.1923079 -2.8210363 -2.5296369 -2.9509444][-4.1707716 -4.16298 -3.4701343 -2.8105855 -2.4367437 -1.5871139 -0.45873952 -0.12019825 -0.60378146 -1.5934403 -2.9990199 -3.7673659 -3.430902 -3.2755206 -3.7016077][-4.197454 -4.2675486 -3.7426097 -3.125972 -2.53048 -1.2553353 0.34769773 0.99719715 0.27065992 -1.3373859 -3.115061 -3.9842539 -3.6787174 -3.57303 -3.9314446][-4.0142932 -4.2150736 -3.8691823 -3.2669411 -2.4590313 -0.89465308 0.96591043 1.7894454 0.81677914 -1.1841967 -3.0745883 -3.9857907 -3.7881017 -3.7178826 -3.9665797][-4.09455 -4.3639169 -4.1501927 -3.583889 -2.7008066 -1.0747097 0.83204174 1.7283902 0.62919712 -1.4698088 -3.2153287 -4.047287 -4.0266113 -4.06901 -4.23159][-4.05562 -4.3068089 -4.1259637 -3.6496012 -2.8867741 -1.4807112 0.18479824 0.91468477 -0.22645521 -2.1533558 -3.5162058 -4.0854893 -4.1291652 -4.2233744 -4.3486352][-3.9959042 -4.2348576 -4.0503707 -3.6224611 -2.9962177 -1.8717122 -0.58252215 -0.20706367 -1.3504982 -2.9313827 -3.8489919 -4.1615486 -4.2355137 -4.3213792 -4.4398108][-4.0613008 -4.3214951 -4.2030721 -3.8419518 -3.3070841 -2.4053671 -1.4604847 -1.3973055 -2.3900123 -3.5594625 -4.1586323 -4.3292103 -4.3775964 -4.3933659 -4.5104523][-4.0627775 -4.2497458 -4.1723938 -3.9302733 -3.6145115 -3.0647521 -2.5201855 -2.6168189 -3.2399035 -3.9709251 -4.368104 -4.4631357 -4.4488611 -4.3675594 -4.45131][-3.6375008 -3.7996783 -3.8284345 -3.759747 -3.6797688 -3.4871387 -3.2702217 -3.2886281 -3.3363047 -3.6147063 -3.924077 -4.0755367 -4.1576743 -4.1775603 -4.3032651][-3.1210761 -3.4150953 -3.7102118 -3.8924603 -3.9380991 -3.8619184 -3.7344797 -3.4847579 -2.9616029 -2.9141059 -3.2598908 -3.556489 -3.8691645 -4.0939741 -4.2359676]]...]
INFO - root - 2017-12-07 04:51:22.875295: step 6810, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.750 sec/batch; 67h:49m:38s remains)
INFO - root - 2017-12-07 04:51:30.533039: step 6820, loss = 0.80, batch loss = 0.73 (10.1 examples/sec; 0.789 sec/batch; 71h:20m:54s remains)
INFO - root - 2017-12-07 04:51:37.997712: step 6830, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.774 sec/batch; 70h:02m:37s remains)
INFO - root - 2017-12-07 04:51:45.708256: step 6840, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 69h:32m:32s remains)
INFO - root - 2017-12-07 04:51:53.306174: step 6850, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.762 sec/batch; 68h:58m:24s remains)
INFO - root - 2017-12-07 04:52:00.957510: step 6860, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.760 sec/batch; 68h:44m:39s remains)
INFO - root - 2017-12-07 04:52:08.527476: step 6870, loss = 0.72, batch loss = 0.65 (10.7 examples/sec; 0.744 sec/batch; 67h:20m:29s remains)
INFO - root - 2017-12-07 04:52:16.167454: step 6880, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.747 sec/batch; 67h:36m:12s remains)
INFO - root - 2017-12-07 04:52:23.805691: step 6890, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.757 sec/batch; 68h:26m:58s remains)
INFO - root - 2017-12-07 04:52:31.474469: step 6900, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.760 sec/batch; 68h:45m:14s remains)
2017-12-07 04:52:32.094984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.856266 -2.7668338 -2.813673 -2.8392053 -2.7215304 -2.5841088 -2.7846341 -3.2834222 -3.6139286 -3.5714369 -3.326335 -3.2929926 -3.375217 -3.0853057 -2.4598479][-2.9443693 -2.9026368 -3.1200964 -3.2689557 -3.1328449 -2.9342442 -3.1247869 -3.634804 -3.9393437 -3.8771329 -3.6631017 -3.6647997 -3.7078409 -3.3285532 -2.5616393][-3.2661781 -3.3213215 -3.7243357 -3.9367082 -3.6857014 -3.2408476 -3.1319036 -3.3793285 -3.5294976 -3.4616587 -3.4825964 -3.7632618 -3.917069 -3.5420394 -2.7107217][-3.6034298 -3.7348998 -4.1878619 -4.3439765 -3.8968463 -3.0782111 -2.523385 -2.5690532 -2.7589092 -2.862124 -3.2155755 -3.8393195 -4.1667805 -3.8285043 -2.9600856][-3.8723488 -3.9633064 -4.3474131 -4.3678169 -3.6387579 -2.3204446 -1.2214129 -1.2438319 -1.8634856 -2.4466126 -3.1626916 -4.0141993 -4.487957 -4.2643375 -3.4575939][-3.9177475 -3.8370187 -4.0413747 -3.9090784 -2.842864 -0.95830274 0.61159849 0.38902998 -0.93489909 -2.1647639 -3.1926153 -4.0680256 -4.5954213 -4.5241032 -3.8768237][-3.776504 -3.482924 -3.5146995 -3.2610326 -1.9837396 0.20508194 1.9228725 1.2860222 -0.70393395 -2.3956604 -3.4225116 -3.997438 -4.4429364 -4.5362878 -4.1186867][-3.5697205 -3.1032557 -3.083909 -2.9168284 -1.7932317 0.14492464 1.5424051 0.71505928 -1.3193395 -2.9293544 -3.6715865 -3.8349819 -4.1527019 -4.4097891 -4.256937][-3.4190347 -2.9043326 -2.8843422 -2.7949438 -2.0738933 -0.90175462 -0.13926983 -0.87721848 -2.3402095 -3.3202803 -3.5694859 -3.3577094 -3.5199823 -3.8462389 -3.8991332][-3.4727035 -3.0868099 -3.0515058 -2.9190192 -2.5333734 -2.0988314 -1.8769836 -2.4707901 -3.2591395 -3.5432708 -3.3583164 -2.9229345 -2.9077134 -3.1316378 -3.1744421][-3.6677356 -3.4922934 -3.400095 -3.1247249 -2.8912039 -2.9159622 -3.0250301 -3.4510531 -3.7314248 -3.5558755 -3.2019658 -2.8267946 -2.828918 -2.984468 -2.8772545][-3.7981954 -3.8727694 -3.8132985 -3.4501994 -3.2319534 -3.4161315 -3.6656187 -3.921895 -3.8351398 -3.4116549 -3.0829616 -2.9246728 -3.1060925 -3.3368354 -3.1578457][-3.8524485 -4.1257143 -4.1925359 -3.9045229 -3.7174671 -3.855854 -4.0420561 -4.07806 -3.7666774 -3.29388 -3.0980158 -3.1324081 -3.4396586 -3.7541623 -3.6264226][-3.8759356 -4.2303848 -4.3993392 -4.2627983 -4.1530242 -4.2109017 -4.2614 -4.098022 -3.6877952 -3.3445992 -3.3556783 -3.5079682 -3.7842784 -4.05888 -3.9917912][-3.90695 -4.1891241 -4.3579993 -4.3616352 -4.3337593 -4.3045268 -4.228375 -4.0036683 -3.687314 -3.5498598 -3.7138312 -3.9166718 -4.0996575 -4.2824726 -4.2456951]]...]
INFO - root - 2017-12-07 04:52:39.800374: step 6910, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.760 sec/batch; 68h:43m:06s remains)
INFO - root - 2017-12-07 04:52:47.543073: step 6920, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.773 sec/batch; 69h:54m:07s remains)
INFO - root - 2017-12-07 04:52:54.971042: step 6930, loss = 0.75, batch loss = 0.68 (10.7 examples/sec; 0.751 sec/batch; 67h:55m:01s remains)
INFO - root - 2017-12-07 04:53:02.647156: step 6940, loss = 0.82, batch loss = 0.74 (10.5 examples/sec; 0.763 sec/batch; 69h:01m:24s remains)
INFO - root - 2017-12-07 04:53:10.470196: step 6950, loss = 0.96, batch loss = 0.88 (10.2 examples/sec; 0.784 sec/batch; 70h:56m:04s remains)
INFO - root - 2017-12-07 04:53:18.118295: step 6960, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.774 sec/batch; 70h:01m:08s remains)
INFO - root - 2017-12-07 04:53:25.714571: step 6970, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.772 sec/batch; 69h:50m:28s remains)
INFO - root - 2017-12-07 04:53:33.307348: step 6980, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.745 sec/batch; 67h:23m:00s remains)
INFO - root - 2017-12-07 04:53:40.889386: step 6990, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.767 sec/batch; 69h:23m:06s remains)
INFO - root - 2017-12-07 04:53:48.588376: step 7000, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.768 sec/batch; 69h:26m:56s remains)
2017-12-07 04:53:49.249377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0746603 -0.067851543 -0.11368275 -0.16702461 -0.32019472 -0.78195715 -1.4251692 -1.666815 -1.5123475 -1.3017175 -1.093534 -1.0042026 -0.75373578 -0.68682313 -1.0496452][-0.21627712 -0.30373049 -0.40383005 -0.41370487 -0.43503332 -0.63910365 -0.885829 -0.86271739 -0.80061007 -0.92426014 -1.0015345 -0.93706059 -0.62618136 -0.580724 -1.0384829][-0.24138641 -0.43854952 -0.61723161 -0.65728259 -0.5599792 -0.41944313 -0.17459488 0.11709547 0.019817352 -0.43667841 -0.70315647 -0.5541389 -0.14857626 -0.12346029 -0.663167][-0.25702143 -0.54164243 -0.84240985 -1.0138533 -0.83119655 -0.31697512 0.44636726 0.96379471 0.66705084 -0.11498547 -0.5591073 -0.36109304 0.098801613 0.14120245 -0.32652903][-0.18311119 -0.51594639 -0.982424 -1.3538029 -1.1730433 -0.33639574 0.87954092 1.5658007 1.0094676 -0.13559055 -0.79852748 -0.65593457 -0.20248556 -0.078922272 -0.30040026][0.086755276 -0.27662134 -0.89352417 -1.4445095 -1.2959211 -0.26511002 1.2542191 2.0028934 1.139689 -0.31605339 -1.0989728 -0.99244308 -0.52298355 -0.32099676 -0.39177942][0.6221776 0.13282108 -0.68085885 -1.4479387 -1.4424193 -0.3725605 1.31675 2.0469661 0.95322323 -0.59743953 -1.3229425 -1.1808786 -0.71105742 -0.45977688 -0.47717166][0.84520721 0.21230888 -0.72556305 -1.560781 -1.6098504 -0.58738136 0.97483206 1.4968972 0.41495895 -0.92092609 -1.4015565 -1.186007 -0.71863246 -0.41736412 -0.43243027][0.54699564 -0.19976902 -1.0339851 -1.6699569 -1.5942485 -0.64974 0.59566593 0.90616989 0.044766426 -0.86306548 -1.0992737 -0.99310565 -0.70826483 -0.4470315 -0.41531658][0.17717552 -0.61155105 -1.2161944 -1.5283065 -1.3101399 -0.56543279 0.26506853 0.46624708 -0.024674892 -0.5036366 -0.7244792 -0.95192695 -0.92388654 -0.67699289 -0.46766973][-0.10807896 -0.81982756 -1.1668465 -1.2242472 -1.0036395 -0.58418679 -0.13793182 0.026334763 -0.14016438 -0.36613321 -0.64510655 -1.0342374 -1.0628114 -0.74686265 -0.36668444][-0.49978805 -1.0299819 -1.1112697 -1.0151923 -0.875201 -0.71473074 -0.49552751 -0.31624222 -0.26791859 -0.33673954 -0.60339808 -0.94654441 -0.90101719 -0.57213211 -0.20568609][-1.0955882 -1.4705729 -1.3262365 -1.1437156 -1.0654414 -1.0100622 -0.85688353 -0.64408875 -0.49266744 -0.47936487 -0.65149117 -0.85675263 -0.73619246 -0.5053091 -0.37238169][-1.457818 -1.7262998 -1.4535935 -1.2381315 -1.1840003 -1.1137419 -0.91459584 -0.63422823 -0.50192261 -0.55222058 -0.69335032 -0.77017736 -0.56598353 -0.37143326 -0.37585068][-1.4360223 -1.6283584 -1.3604319 -1.1901839 -1.1529627 -1.0225127 -0.72466588 -0.36455727 -0.32055712 -0.53353286 -0.6869843 -0.61341739 -0.23865795 0.061170578 0.032951355]]...]
INFO - root - 2017-12-07 04:53:56.935126: step 7010, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.765 sec/batch; 69h:12m:14s remains)
INFO - root - 2017-12-07 04:54:04.531769: step 7020, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 69h:00m:06s remains)
INFO - root - 2017-12-07 04:54:11.928631: step 7030, loss = 0.63, batch loss = 0.56 (10.8 examples/sec; 0.741 sec/batch; 67h:00m:07s remains)
INFO - root - 2017-12-07 04:54:19.634094: step 7040, loss = 0.73, batch loss = 0.66 (10.7 examples/sec; 0.749 sec/batch; 67h:42m:21s remains)
INFO - root - 2017-12-07 04:54:27.272162: step 7050, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.762 sec/batch; 68h:54m:26s remains)
INFO - root - 2017-12-07 04:54:34.932559: step 7060, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.771 sec/batch; 69h:40m:56s remains)
INFO - root - 2017-12-07 04:54:42.662436: step 7070, loss = 0.86, batch loss = 0.79 (10.7 examples/sec; 0.747 sec/batch; 67h:32m:58s remains)
INFO - root - 2017-12-07 04:54:50.283583: step 7080, loss = 0.70, batch loss = 0.63 (10.7 examples/sec; 0.749 sec/batch; 67h:44m:44s remains)
INFO - root - 2017-12-07 04:54:58.019274: step 7090, loss = 1.00, batch loss = 0.93 (10.6 examples/sec; 0.756 sec/batch; 68h:21m:56s remains)
INFO - root - 2017-12-07 04:55:05.683048: step 7100, loss = 0.96, batch loss = 0.88 (10.4 examples/sec; 0.769 sec/batch; 69h:31m:18s remains)
2017-12-07 04:55:06.315414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3587368 -2.1869268 -1.75932 -1.4687631 -1.4234457 -1.5039508 -1.7865016 -1.8614457 -1.8086095 -2.1998219 -2.6988521 -2.8097932 -2.6803308 -2.6445324 -2.796762][-2.2138591 -2.1752737 -1.7054753 -1.3081689 -1.2147937 -1.3067911 -1.6397424 -1.8272059 -1.8134987 -2.0808394 -2.4601822 -2.4726424 -2.2612393 -2.2874382 -2.5242114][-2.1307616 -2.1616976 -1.6457014 -1.1489353 -1.0614388 -1.2937841 -1.7360208 -2.0627093 -2.1698816 -2.3788252 -2.6307642 -2.5313082 -2.1996238 -2.210722 -2.5088103][-1.9541056 -2.0349436 -1.597151 -1.1093459 -1.1121757 -1.5264549 -2.0464506 -2.4270549 -2.6837878 -2.965878 -3.1737738 -2.985697 -2.5806928 -2.5061083 -2.6997948][-1.7174444 -1.8639941 -1.6598387 -1.2813394 -1.2718983 -1.6031003 -1.8891828 -2.0813997 -2.4251335 -2.9267027 -3.2618713 -3.1263809 -2.7852774 -2.6925251 -2.7887635][-1.623383 -1.7260587 -1.6268671 -1.264689 -1.0854187 -1.0572419 -0.81619263 -0.57772636 -0.926111 -1.799422 -2.5108809 -2.6554556 -2.6070166 -2.7176597 -2.9037082][-1.6556478 -1.6265149 -1.4828827 -1.1524162 -0.90729213 -0.56901884 0.20375156 0.97808266 0.82257891 -0.35383368 -1.5253322 -2.0895157 -2.4572227 -2.9137497 -3.3430562][-1.8481779 -1.7339761 -1.4737535 -1.2117639 -1.1065302 -0.79958463 0.10152435 1.2053609 1.4647665 0.44842196 -0.82824969 -1.6638281 -2.3567917 -3.0486879 -3.6868954][-1.9815416 -1.9498708 -1.7015345 -1.5549936 -1.6410813 -1.5732381 -0.97655511 0.094230652 0.75465631 0.37205839 -0.43932629 -1.1547604 -1.9175792 -2.6669846 -3.4662902][-1.8144782 -1.8961561 -1.7719483 -1.8236141 -2.1854467 -2.5278716 -2.3563597 -1.4064221 -0.43805575 -0.071542263 -0.068754196 -0.33648777 -0.99069405 -1.7670269 -2.7334466][-1.4397085 -1.5716252 -1.5688174 -1.7344012 -2.2415457 -2.8724213 -3.0657334 -2.4152493 -1.5456522 -0.8005724 -0.11001301 0.20772648 -0.077110291 -0.6950779 -1.6566327][-0.95490956 -0.97193146 -1.0020859 -1.1990232 -1.6944311 -2.3566756 -2.7386124 -2.5513244 -2.2163224 -1.6762662 -0.79808068 -0.1067605 0.062525749 -0.19769049 -0.89521074][-0.43381667 -0.28117085 -0.28277636 -0.46257234 -0.9084208 -1.4680789 -1.8803537 -2.0987046 -2.3516521 -2.2355828 -1.547919 -0.78703046 -0.30046272 -0.22549152 -0.61528087][-0.3540144 -0.2008419 -0.22548532 -0.36750078 -0.66704416 -0.95182204 -1.1316395 -1.420898 -1.9511702 -2.178246 -1.82707 -1.226855 -0.62500477 -0.37809896 -0.5512445][-1.0910678 -1.0084465 -1.0116818 -1.0238771 -1.089823 -1.0016098 -0.7819581 -0.83975697 -1.2571862 -1.6010015 -1.6047673 -1.4042544 -1.0051532 -0.77543187 -0.8033762]]...]
INFO - root - 2017-12-07 04:55:13.985361: step 7110, loss = 1.08, batch loss = 1.01 (10.6 examples/sec; 0.752 sec/batch; 68h:00m:10s remains)
INFO - root - 2017-12-07 04:55:21.768178: step 7120, loss = 0.84, batch loss = 0.77 (10.1 examples/sec; 0.789 sec/batch; 71h:19m:16s remains)
INFO - root - 2017-12-07 04:55:29.234222: step 7130, loss = 0.76, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 71h:04m:48s remains)
INFO - root - 2017-12-07 04:55:36.853081: step 7140, loss = 0.78, batch loss = 0.70 (10.6 examples/sec; 0.752 sec/batch; 67h:58m:05s remains)
INFO - root - 2017-12-07 04:55:44.555740: step 7150, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.770 sec/batch; 69h:35m:32s remains)
INFO - root - 2017-12-07 04:55:52.167535: step 7160, loss = 0.90, batch loss = 0.83 (10.7 examples/sec; 0.747 sec/batch; 67h:27m:53s remains)
INFO - root - 2017-12-07 04:55:59.915704: step 7170, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.772 sec/batch; 69h:46m:51s remains)
INFO - root - 2017-12-07 04:56:07.485088: step 7180, loss = 0.67, batch loss = 0.60 (10.5 examples/sec; 0.762 sec/batch; 68h:53m:30s remains)
INFO - root - 2017-12-07 04:56:15.130523: step 7190, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.755 sec/batch; 68h:15m:47s remains)
INFO - root - 2017-12-07 04:56:22.791440: step 7200, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.755 sec/batch; 68h:15m:25s remains)
2017-12-07 04:56:23.377943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8721161 -2.8457379 -2.8822017 -2.8983989 -2.9023895 -2.9530118 -2.9835777 -3.0486608 -3.1644793 -3.1890059 -3.1792595 -3.1968007 -3.2060812 -3.2234545 -3.2595429][-2.57098 -2.4037211 -2.3357034 -2.3217318 -2.3714495 -2.5173748 -2.6327357 -2.7700889 -2.9131925 -2.9332132 -2.960845 -3.0709252 -3.1745067 -3.2798188 -3.3891919][-2.318207 -2.0285568 -1.8514476 -1.8022518 -1.9005167 -2.1140144 -2.2880502 -2.4650073 -2.5586407 -2.4637303 -2.4601569 -2.6291678 -2.8270738 -3.0388391 -3.2517323][-2.1195033 -1.7291961 -1.4413431 -1.336318 -1.4298482 -1.6270647 -1.7652194 -1.9242415 -1.9767129 -1.76407 -1.6982889 -1.8813851 -2.1749368 -2.5251904 -2.8770785][-2.1132946 -1.639168 -1.2200758 -1.0073991 -1.0260789 -1.1405604 -1.1815534 -1.2995119 -1.3414738 -1.090271 -0.976655 -1.1426041 -1.54024 -2.0736907 -2.5601494][-2.2013886 -1.7020631 -1.1940632 -0.88758755 -0.87404847 -0.98619723 -0.96692824 -1.0060039 -0.96973348 -0.69536543 -0.53500366 -0.65236259 -1.1434269 -1.8099539 -2.3382015][-2.1488667 -1.6903422 -1.1789646 -0.831089 -0.86291909 -1.1436388 -1.228816 -1.1875916 -1.0366983 -0.76187134 -0.56524277 -0.57306671 -1.0321834 -1.7522907 -2.2338428][-1.9357021 -1.5256793 -1.046917 -0.68372917 -0.73002505 -1.1110075 -1.2936299 -1.1685338 -0.9598515 -0.756757 -0.52911782 -0.37908983 -0.68994069 -1.40257 -1.9357166][-1.7910006 -1.3815041 -0.88406515 -0.47210217 -0.46086097 -0.77966666 -0.92213869 -0.72109747 -0.56581712 -0.52869129 -0.32578945 -0.037321091 -0.18285894 -0.81962132 -1.4217393][-2.1583753 -1.8404264 -1.3896136 -0.99674582 -0.94194365 -1.1257918 -1.134088 -0.8479085 -0.71862793 -0.83586931 -0.7743299 -0.56224823 -0.65295768 -1.0909808 -1.5293059][-2.9290731 -2.8644214 -2.6117792 -2.3606846 -2.3116548 -2.3814614 -2.2787819 -1.9652464 -1.8487792 -2.0131867 -2.0438919 -1.9348114 -1.9969854 -2.1572306 -2.2437415][-3.5851333 -3.7676265 -3.7618244 -3.7383943 -3.7961345 -3.8601377 -3.7431297 -3.4856267 -3.4293704 -3.5923605 -3.6017282 -3.4641991 -3.3800049 -3.2298694 -2.9522243][-3.86202 -4.1201105 -4.2279396 -4.3139629 -4.4282432 -4.5047908 -4.4218717 -4.2481003 -4.225502 -4.3521042 -4.3488812 -4.2240968 -4.0590377 -3.7399817 -3.2896366][-3.8107734 -4.0307322 -4.1285858 -4.18764 -4.2535987 -4.2819114 -4.208384 -4.0819468 -4.0439887 -4.093852 -4.0834346 -4.0073981 -3.8796055 -3.6179905 -3.2589068][-3.5710561 -3.7406573 -3.8384137 -3.8858061 -3.9146011 -3.9218092 -3.8866115 -3.8342636 -3.813978 -3.8103788 -3.7584181 -3.6729763 -3.5624995 -3.3832793 -3.161232]]...]
INFO - root - 2017-12-07 04:56:31.083563: step 7210, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.767 sec/batch; 69h:18m:23s remains)
INFO - root - 2017-12-07 04:56:38.754192: step 7220, loss = 0.96, batch loss = 0.89 (10.3 examples/sec; 0.779 sec/batch; 70h:20m:33s remains)
INFO - root - 2017-12-07 04:56:46.256639: step 7230, loss = 0.97, batch loss = 0.90 (10.5 examples/sec; 0.760 sec/batch; 68h:40m:36s remains)
INFO - root - 2017-12-07 04:56:53.977289: step 7240, loss = 0.60, batch loss = 0.53 (9.8 examples/sec; 0.818 sec/batch; 73h:54m:19s remains)
INFO - root - 2017-12-07 04:57:01.591171: step 7250, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.753 sec/batch; 68h:00m:37s remains)
INFO - root - 2017-12-07 04:57:09.256231: step 7260, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.785 sec/batch; 70h:54m:40s remains)
INFO - root - 2017-12-07 04:57:16.983646: step 7270, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.762 sec/batch; 68h:49m:11s remains)
INFO - root - 2017-12-07 04:57:24.755169: step 7280, loss = 0.91, batch loss = 0.83 (10.7 examples/sec; 0.744 sec/batch; 67h:14m:49s remains)
INFO - root - 2017-12-07 04:57:32.516410: step 7290, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.761 sec/batch; 68h:44m:11s remains)
INFO - root - 2017-12-07 04:57:40.250266: step 7300, loss = 0.90, batch loss = 0.83 (10.5 examples/sec; 0.763 sec/batch; 68h:57m:12s remains)
2017-12-07 04:57:40.932112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5327535 -3.7751832 -3.9378791 -3.9807351 -3.9496243 -3.9537635 -3.9088702 -3.8539815 -3.74354 -3.6502011 -3.5312397 -3.3555441 -3.3168063 -3.5230618 -3.7745802][-3.350131 -3.5658915 -3.6203787 -3.5924523 -3.5809486 -3.6917772 -3.8218331 -3.8802228 -3.7734365 -3.6460285 -3.4414554 -3.0379612 -2.7981515 -3.0927539 -3.55856][-3.0036049 -3.2387364 -3.2657864 -3.2155685 -3.105782 -3.099731 -3.2412889 -3.3281541 -3.257895 -3.1942458 -2.9806123 -2.4533658 -2.1075876 -2.587796 -3.2929835][-2.754683 -2.8323357 -2.7665098 -2.8040471 -2.8088379 -2.7510362 -2.7830963 -2.7017579 -2.533844 -2.5760386 -2.4735241 -2.01383 -1.7212315 -2.310133 -3.0210538][-2.8894262 -2.6598339 -2.1608052 -2.0077839 -2.1639681 -2.2504876 -2.243367 -2.0524559 -1.900265 -2.0901475 -2.1093416 -1.8371198 -1.7641766 -2.3902807 -2.8035526][-3.0141065 -2.7048602 -1.885577 -1.402055 -1.446228 -1.3548825 -1.0174708 -0.77498126 -0.96261597 -1.5426559 -1.7914474 -1.6849675 -1.8371234 -2.48762 -2.6181083][-2.9134245 -2.66499 -1.8028979 -1.1244407 -0.82813644 -0.13817835 0.77388859 0.89239168 0.017148495 -1.1105335 -1.5846555 -1.4386122 -1.5129342 -2.0058229 -2.0720232][-2.7729154 -2.6019926 -1.8396635 -1.1376281 -0.4434104 0.99844694 2.5011973 2.4030132 0.8149538 -0.76519489 -1.5090456 -1.3953552 -1.2813547 -1.5073359 -1.6010432][-2.850626 -2.7627215 -2.1432376 -1.6306291 -0.90579414 0.82191277 2.5143137 2.4771357 0.99112844 -0.47556162 -1.3927293 -1.5781484 -1.6187329 -1.8303947 -1.9625587][-3.0229769 -2.929431 -2.3980825 -2.1799631 -1.8387666 -0.54788876 0.67973661 0.78004742 0.12406635 -0.50343847 -1.099206 -1.4329519 -1.7515023 -2.2411258 -2.5553489][-2.984961 -2.901262 -2.5269146 -2.5390568 -2.502223 -1.7622321 -1.1861525 -1.1741905 -1.1858349 -1.0237257 -0.983047 -0.98868322 -1.3068047 -2.0950222 -2.7177806][-2.7480073 -2.7406101 -2.5724511 -2.6889517 -2.7242374 -2.2832742 -2.1237259 -2.2980676 -2.1876554 -1.8251143 -1.4301538 -0.99443078 -1.0448694 -1.879477 -2.6782551][-2.4863911 -2.5700676 -2.4652512 -2.5385385 -2.5933714 -2.3964047 -2.4487877 -2.6402254 -2.5390339 -2.3244557 -2.0017986 -1.5202341 -1.4342549 -2.1218722 -2.7892132][-2.3556788 -2.4933357 -2.2058489 -2.0411339 -2.0928683 -2.1681354 -2.4473374 -2.680655 -2.6263316 -2.4736366 -2.2350535 -1.9477637 -1.9853263 -2.5855742 -3.0094724][-2.5809069 -2.6521697 -2.1647801 -1.7910631 -1.7496617 -1.8969529 -2.2939904 -2.6672421 -2.7813892 -2.7146745 -2.5399046 -2.3892469 -2.5333908 -3.0328364 -3.1893673]]...]
INFO - root - 2017-12-07 04:57:48.541084: step 7310, loss = 0.91, batch loss = 0.84 (10.7 examples/sec; 0.748 sec/batch; 67h:35m:55s remains)
INFO - root - 2017-12-07 04:57:56.062871: step 7320, loss = 0.83, batch loss = 0.76 (11.0 examples/sec; 0.730 sec/batch; 65h:53m:44s remains)
INFO - root - 2017-12-07 04:58:03.722454: step 7330, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.774 sec/batch; 69h:57m:07s remains)
INFO - root - 2017-12-07 04:58:11.424019: step 7340, loss = 1.07, batch loss = 1.00 (10.1 examples/sec; 0.790 sec/batch; 71h:23m:36s remains)
INFO - root - 2017-12-07 04:58:19.071051: step 7350, loss = 0.79, batch loss = 0.72 (10.9 examples/sec; 0.735 sec/batch; 66h:22m:57s remains)
INFO - root - 2017-12-07 04:58:26.751569: step 7360, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.762 sec/batch; 68h:49m:36s remains)
INFO - root - 2017-12-07 04:58:34.447342: step 7370, loss = 0.65, batch loss = 0.58 (10.3 examples/sec; 0.774 sec/batch; 69h:53m:36s remains)
INFO - root - 2017-12-07 04:58:42.083470: step 7380, loss = 0.75, batch loss = 0.67 (10.5 examples/sec; 0.760 sec/batch; 68h:38m:39s remains)
INFO - root - 2017-12-07 04:58:49.761944: step 7390, loss = 0.89, batch loss = 0.81 (10.6 examples/sec; 0.757 sec/batch; 68h:23m:08s remains)
INFO - root - 2017-12-07 04:58:57.515815: step 7400, loss = 0.93, batch loss = 0.85 (10.5 examples/sec; 0.764 sec/batch; 68h:57m:57s remains)
2017-12-07 04:58:58.167980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4348831 -1.8217306 -1.9892025 -1.6377749 -1.6224427 -1.617449 -1.2836912 -1.0129104 -0.84043241 -1.140192 -1.7191854 -2.0038161 -2.213141 -2.362973 -2.3634818][-1.8620405 -2.2570579 -2.3165665 -1.9915659 -2.0434761 -1.9849203 -1.677382 -1.3676894 -0.98075986 -1.1130145 -1.5643303 -1.7986927 -2.0719404 -2.3354366 -2.4438415][-2.2803574 -2.5765932 -2.4627151 -2.1629937 -2.1933968 -2.0557656 -1.8629055 -1.6104207 -1.0674446 -1.064014 -1.4239998 -1.6758997 -2.099488 -2.4838476 -2.6575303][-2.4907105 -2.6847522 -2.4539504 -2.1915457 -2.1316907 -1.9083753 -1.7811234 -1.576865 -1.0243752 -0.9935751 -1.3401334 -1.6498971 -2.2489805 -2.7443666 -2.957386][-2.8364348 -2.9567804 -2.6119363 -2.241214 -1.9052393 -1.4636946 -1.2379026 -1.0220735 -0.57925272 -0.67656231 -1.1522257 -1.6523197 -2.4515066 -3.0192175 -3.2301786][-3.4406354 -3.5115571 -3.0690951 -2.4678364 -1.7113049 -0.93668818 -0.40518713 0.0099864006 0.3269062 -0.0802989 -0.90528584 -1.76899 -2.7533569 -3.3040843 -3.4725955][-3.9490149 -4.0094686 -3.5638804 -2.7844739 -1.642653 -0.50062323 0.47109795 1.2705302 1.5404816 0.730453 -0.54263973 -1.8020406 -2.9556983 -3.4898138 -3.6583898][-4.1025257 -4.1575437 -3.7780089 -2.9326715 -1.5695035 -0.14420128 1.2255664 2.4025183 2.6325312 1.4799647 -0.11912632 -1.6517048 -2.9377043 -3.5204253 -3.7402349][-4.0386882 -4.0700822 -3.7778826 -2.9742405 -1.6268728 -0.16819143 1.2932439 2.58208 2.8053713 1.5939903 -0.022315979 -1.5767956 -2.8460677 -3.4370761 -3.6944566][-3.9328237 -3.9517083 -3.7669909 -3.1270351 -2.02424 -0.77829051 0.53170586 1.768321 2.0564308 1.0403094 -0.3574605 -1.7310631 -2.8336425 -3.3763618 -3.6613321][-3.8430958 -3.8349817 -3.7163303 -3.244828 -2.4549203 -1.5376813 -0.52696609 0.52243567 0.83572626 0.10149908 -0.96524572 -2.0367405 -2.8797889 -3.3330843 -3.6081362][-3.7849705 -3.7394936 -3.6160913 -3.2426109 -2.691364 -2.0657957 -1.3940327 -0.66116095 -0.41799879 -0.9085319 -1.6403723 -2.3670697 -2.9203663 -3.2361045 -3.4555976][-3.6979434 -3.6320124 -3.5004938 -3.189348 -2.8029189 -2.3980529 -1.9903302 -1.556036 -1.4113944 -1.7104635 -2.1713505 -2.637193 -2.9757621 -3.163893 -3.3019674][-3.5520873 -3.4842951 -3.3695717 -3.1354489 -2.8835545 -2.6411862 -2.4233706 -2.2162225 -2.1358495 -2.2888379 -2.5609722 -2.8353133 -3.0250468 -3.1201098 -3.1985149][-3.4535379 -3.3913777 -3.3002071 -3.1242695 -2.9463525 -2.798352 -2.7035017 -2.6547105 -2.6374221 -2.6992354 -2.8299069 -2.9495888 -3.0287416 -3.0686958 -3.126955]]...]
INFO - root - 2017-12-07 04:59:05.728180: step 7410, loss = 0.78, batch loss = 0.70 (10.7 examples/sec; 0.745 sec/batch; 67h:15m:51s remains)
INFO - root - 2017-12-07 04:59:13.432180: step 7420, loss = 0.71, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 70h:34m:37s remains)
INFO - root - 2017-12-07 04:59:20.873925: step 7430, loss = 0.94, batch loss = 0.86 (10.6 examples/sec; 0.758 sec/batch; 68h:26m:18s remains)
INFO - root - 2017-12-07 04:59:28.515414: step 7440, loss = 0.61, batch loss = 0.54 (10.4 examples/sec; 0.772 sec/batch; 69h:40m:59s remains)
INFO - root - 2017-12-07 04:59:36.090853: step 7450, loss = 0.96, batch loss = 0.88 (10.7 examples/sec; 0.746 sec/batch; 67h:22m:33s remains)
INFO - root - 2017-12-07 04:59:43.775400: step 7460, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.765 sec/batch; 69h:04m:29s remains)
INFO - root - 2017-12-07 04:59:51.434986: step 7470, loss = 0.75, batch loss = 0.68 (10.7 examples/sec; 0.745 sec/batch; 67h:17m:08s remains)
INFO - root - 2017-12-07 04:59:59.130338: step 7480, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.754 sec/batch; 68h:05m:52s remains)
INFO - root - 2017-12-07 05:00:06.870908: step 7490, loss = 0.80, batch loss = 0.72 (10.2 examples/sec; 0.781 sec/batch; 70h:30m:39s remains)
INFO - root - 2017-12-07 05:00:14.649215: step 7500, loss = 0.78, batch loss = 0.71 (10.1 examples/sec; 0.792 sec/batch; 71h:31m:09s remains)
2017-12-07 05:00:15.335576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.305723 -3.447717 -3.6664579 -3.9335127 -3.9929109 -3.9039428 -3.8839452 -3.9269619 -3.9532123 -3.9422195 -3.8785467 -3.7271535 -3.5693665 -3.5044584 -3.5675261][-3.5507841 -3.6507282 -3.7908211 -3.8770919 -3.7005517 -3.4278512 -3.2754664 -3.2780313 -3.3522329 -3.3895454 -3.4353685 -3.4433782 -3.4334614 -3.43858 -3.4949536][-3.5641873 -3.6747451 -3.8275239 -3.7888019 -3.405405 -2.9898493 -2.7327747 -2.6922197 -2.8461204 -2.9674437 -3.0929046 -3.1885457 -3.2450757 -3.3143425 -3.4067397][-3.4768236 -3.6944432 -3.9500256 -3.8882866 -3.3848622 -2.8181605 -2.3683672 -2.2321067 -2.5096557 -2.7747827 -3.0265546 -3.2386136 -3.394347 -3.5430279 -3.6611183][-3.4197583 -3.7304335 -4.0381975 -3.9372077 -3.3917615 -2.8039985 -2.2652366 -2.0385261 -2.32518 -2.600049 -2.8156948 -3.0480509 -3.3652196 -3.6715398 -3.854718][-3.2880626 -3.5302925 -3.7193842 -3.4777064 -2.8503916 -2.3191807 -1.9392157 -1.9330609 -2.3927631 -2.6814721 -2.720583 -2.7728348 -3.092937 -3.4485135 -3.6217897][-2.9765043 -3.0543194 -3.0712037 -2.6418791 -1.7656112 -1.0079992 -0.58670568 -0.78218961 -1.5475671 -2.1197581 -2.3208122 -2.4165184 -2.6872711 -2.9464598 -3.0492082][-2.39194 -2.4428015 -2.4834404 -2.0910783 -1.1418409 -0.13688946 0.62789249 0.70652962 0.058729649 -0.5753634 -1.0649877 -1.393656 -1.7051842 -1.932487 -2.0669446][-1.6427977 -1.7671566 -1.9274216 -1.8113112 -1.2302566 -0.462327 0.261631 0.55022526 0.38988209 0.14742184 -0.26848745 -0.62397957 -0.87560964 -1.0470819 -1.2158401][-1.512542 -1.6953874 -1.8754396 -1.9242775 -1.6596112 -1.2691731 -0.93900704 -0.84647775 -0.8373003 -0.78895044 -0.9260807 -1.0575197 -1.0914578 -1.0585582 -1.0328903][-1.7477961 -1.9620862 -2.0816302 -2.0887995 -1.8665597 -1.6701791 -1.6756973 -1.9123659 -2.1394398 -2.1551492 -2.1323888 -1.9907165 -1.7748182 -1.4360614 -1.0360248][-1.9455168 -2.1029418 -2.1301129 -1.9693215 -1.6549041 -1.5288184 -1.7584622 -2.267554 -2.7508788 -2.9620135 -3.0290532 -2.894896 -2.60124 -2.0188947 -1.2802482][-2.3420186 -2.4576554 -2.4176974 -2.1509719 -1.7946143 -1.6578858 -1.8585148 -2.3602562 -2.903826 -3.28398 -3.5593755 -3.6840112 -3.5762482 -3.0982046 -2.3747537][-2.8954778 -3.0345731 -3.0297954 -2.8401456 -2.5844038 -2.4211853 -2.4145372 -2.6282802 -2.952266 -3.2791367 -3.5862422 -3.862817 -4.0207696 -3.9075723 -3.5827293][-3.3536041 -3.48764 -3.5357676 -3.501776 -3.4083543 -3.2987924 -3.2185953 -3.2157826 -3.2981949 -3.4251056 -3.5453343 -3.7002368 -3.8744388 -3.9635339 -3.9676561]]...]
INFO - root - 2017-12-07 05:00:23.034437: step 7510, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.752 sec/batch; 67h:54m:15s remains)
INFO - root - 2017-12-07 05:00:30.522058: step 7520, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.746 sec/batch; 67h:22m:18s remains)
INFO - root - 2017-12-07 05:00:38.040203: step 7530, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.769 sec/batch; 69h:25m:22s remains)
INFO - root - 2017-12-07 05:00:45.607926: step 7540, loss = 0.99, batch loss = 0.92 (10.5 examples/sec; 0.762 sec/batch; 68h:49m:25s remains)
INFO - root - 2017-12-07 05:00:53.165578: step 7550, loss = 0.97, batch loss = 0.90 (10.8 examples/sec; 0.742 sec/batch; 66h:58m:10s remains)
INFO - root - 2017-12-07 05:01:00.996160: step 7560, loss = 0.79, batch loss = 0.71 (10.3 examples/sec; 0.777 sec/batch; 70h:05m:41s remains)
INFO - root - 2017-12-07 05:01:08.765269: step 7570, loss = 0.96, batch loss = 0.88 (10.1 examples/sec; 0.789 sec/batch; 71h:12m:37s remains)
INFO - root - 2017-12-07 05:01:16.411328: step 7580, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.763 sec/batch; 68h:53m:12s remains)
INFO - root - 2017-12-07 05:01:24.006393: step 7590, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.767 sec/batch; 69h:11m:59s remains)
INFO - root - 2017-12-07 05:01:31.672769: step 7600, loss = 0.63, batch loss = 0.56 (10.4 examples/sec; 0.768 sec/batch; 69h:16m:33s remains)
2017-12-07 05:01:32.283833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9764435 -2.7605546 -2.1260273 -1.261754 -0.944386 -1.3175652 -1.8343935 -2.0926845 -2.2565703 -2.5110245 -2.6678514 -2.7645323 -2.8599396 -2.81244 -2.722646][-3.1061816 -2.8710976 -2.3008823 -1.5360637 -1.1999953 -1.3659146 -1.5707326 -1.5765467 -1.6680095 -2.0602417 -2.4927003 -2.8519921 -3.0683527 -3.0570769 -2.999393][-3.1362481 -2.8687532 -2.4759283 -1.9838133 -1.7802658 -1.8218317 -1.7684517 -1.5588558 -1.5031283 -1.7928944 -2.2551765 -2.7190447 -3.0661035 -3.1758366 -3.1866217][-2.9977388 -2.7827382 -2.6787398 -2.4820316 -2.3583734 -2.2994339 -2.1255567 -1.8505843 -1.6941602 -1.7808771 -2.0840678 -2.5013473 -2.8373718 -2.9768658 -2.9726915][-2.8721361 -2.7464185 -2.8208609 -2.7999763 -2.8000536 -2.8746734 -2.8545747 -2.6688251 -2.4529762 -2.2657907 -2.2144465 -2.3382823 -2.4694033 -2.52062 -2.4652989][-3.0718172 -2.9566765 -2.9244552 -2.7993331 -2.8214841 -3.048883 -3.1748455 -3.0430422 -2.7993565 -2.4641626 -2.1566215 -2.0179255 -1.978929 -2.0035658 -1.9914954][-3.4337485 -3.2343764 -2.9447863 -2.553149 -2.3532732 -2.4324939 -2.4981184 -2.4659395 -2.4208243 -2.2377551 -1.9269025 -1.7312038 -1.681613 -1.7817981 -1.886801][-3.5468581 -3.3576303 -3.1108613 -2.7176416 -2.4085562 -2.2423871 -2.0820456 -1.9892249 -1.968998 -1.7914288 -1.5173128 -1.4165881 -1.495863 -1.6980753 -1.9052424][-3.368808 -3.2670245 -3.2727106 -3.1533482 -2.9702396 -2.6734307 -2.2892342 -2.0549262 -1.8904021 -1.6295371 -1.4489934 -1.4757771 -1.5519383 -1.6809249 -1.8867362][-3.3625159 -3.3190889 -3.4386845 -3.4692113 -3.40548 -3.1397276 -2.8054471 -2.674921 -2.5159476 -2.2171907 -2.0594466 -2.0033219 -1.8597825 -1.8074515 -1.9684019][-3.5513651 -3.5539432 -3.6374805 -3.6377356 -3.5794239 -3.3502493 -3.1238403 -3.109678 -3.0166221 -2.7738743 -2.6134534 -2.374409 -2.0128767 -1.8465984 -1.9547784][-3.5755093 -3.6103134 -3.6288311 -3.5825806 -3.4953647 -3.2445521 -3.0594907 -3.1141136 -3.1731086 -3.1465464 -3.0772114 -2.7609942 -2.3112204 -2.0878594 -2.0910554][-3.4353747 -3.4371588 -3.3572359 -3.2815766 -3.2433331 -3.1208107 -3.1035967 -3.2684116 -3.4232397 -3.5062335 -3.4777932 -3.2102821 -2.8752151 -2.698575 -2.6049237][-3.2731409 -3.22657 -3.0499966 -2.9002495 -2.8402498 -2.8211184 -2.9534893 -3.2150788 -3.4361939 -3.5662112 -3.5572782 -3.3923278 -3.2201276 -3.1226099 -3.0108488][-3.1551595 -3.1520674 -3.0027525 -2.837822 -2.7224989 -2.6776009 -2.7808819 -2.9651735 -3.135365 -3.2555628 -3.2701654 -3.1989346 -3.1212382 -3.0450783 -2.9382477]]...]
INFO - root - 2017-12-07 05:01:39.892436: step 7610, loss = 0.69, batch loss = 0.62 (10.7 examples/sec; 0.745 sec/batch; 67h:16m:17s remains)
INFO - root - 2017-12-07 05:01:47.543024: step 7620, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.769 sec/batch; 69h:25m:16s remains)
INFO - root - 2017-12-07 05:01:55.047671: step 7630, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.755 sec/batch; 68h:06m:07s remains)
INFO - root - 2017-12-07 05:02:02.654407: step 7640, loss = 0.75, batch loss = 0.68 (10.6 examples/sec; 0.757 sec/batch; 68h:19m:54s remains)
INFO - root - 2017-12-07 05:02:10.322460: step 7650, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.785 sec/batch; 70h:48m:05s remains)
INFO - root - 2017-12-07 05:02:18.094061: step 7660, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 69h:53m:45s remains)
INFO - root - 2017-12-07 05:02:25.715456: step 7670, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.749 sec/batch; 67h:34m:32s remains)
INFO - root - 2017-12-07 05:02:33.368134: step 7680, loss = 0.74, batch loss = 0.67 (9.9 examples/sec; 0.809 sec/batch; 72h:57m:24s remains)
INFO - root - 2017-12-07 05:02:42.598698: step 7690, loss = 0.69, batch loss = 0.62 (7.7 examples/sec; 1.037 sec/batch; 93h:33m:57s remains)
INFO - root - 2017-12-07 05:02:52.889698: step 7700, loss = 0.87, batch loss = 0.79 (7.8 examples/sec; 1.025 sec/batch; 92h:29m:58s remains)
2017-12-07 05:02:53.692202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5779328 -3.7256107 -3.6670582 -3.4286542 -3.1064796 -2.6593866 -2.4318366 -2.4391761 -2.5677752 -2.8534207 -2.8143003 -2.569813 -2.4997933 -2.3963006 -2.3133435][-3.1057043 -3.401041 -3.3503168 -2.9624162 -2.4567785 -1.8720663 -1.7113621 -1.895803 -2.2377162 -2.7618282 -2.8676007 -2.6337481 -2.5604846 -2.398432 -2.2095325][-2.9062834 -3.2772226 -3.1914158 -2.6173482 -1.9262879 -1.2826042 -1.2389796 -1.5697768 -2.0565329 -2.7339659 -2.9792314 -2.8254371 -2.8099542 -2.6775942 -2.5283189][-2.6429892 -2.9782758 -2.8703911 -2.220432 -1.5382335 -1.0934145 -1.2465348 -1.6518846 -2.1444182 -2.762835 -3.0639157 -3.0567155 -3.1872005 -3.157536 -3.099062][-2.6670361 -2.7897024 -2.599762 -1.9761155 -1.5073335 -1.3764293 -1.592221 -1.8555379 -2.1333196 -2.4721663 -2.6530089 -2.7873492 -3.1073198 -3.2102537 -3.1790252][-2.9509993 -2.8784435 -2.5597291 -1.9736009 -1.749568 -1.8544743 -1.9559486 -1.8803449 -1.8576241 -1.916285 -1.8464634 -1.9554799 -2.3489943 -2.5793576 -2.665786][-2.8435302 -2.7148621 -2.3426454 -1.8233709 -1.762996 -1.9240417 -1.8214545 -1.4783595 -1.3583026 -1.3142457 -0.97534537 -0.84290552 -1.0550013 -1.280221 -1.5845032][-2.7638142 -2.65843 -2.3284161 -1.8401887 -1.703279 -1.6628845 -1.3131437 -0.80999136 -0.72181916 -0.59565973 0.075107574 0.49397659 0.48850346 0.1703 -0.53497767][-3.111764 -3.108891 -2.8426218 -2.2470491 -1.7836785 -1.3993776 -0.92087984 -0.5407939 -0.61632752 -0.33374214 0.67883444 1.3847833 1.5311575 0.96118546 -0.25726748][-3.5455651 -3.6556861 -3.369379 -2.5430079 -1.7557802 -1.1916401 -0.87812448 -0.89689088 -1.2061024 -0.88597488 0.14495897 0.8886714 1.1058125 0.55591106 -0.7019372][-3.7683024 -3.9301872 -3.5700357 -2.61286 -1.7535965 -1.2961047 -1.3493788 -1.7690387 -2.2103045 -1.9540617 -1.2399538 -0.80762625 -0.66019773 -0.93310452 -1.6423562][-3.6661203 -3.9011962 -3.6146965 -2.8243332 -2.1640222 -1.8888695 -2.1413372 -2.676662 -3.0791345 -2.9037194 -2.4939265 -2.3109753 -2.2189486 -2.2665534 -2.4976678][-3.2447026 -3.5961037 -3.5862355 -3.250525 -2.9452708 -2.7648876 -2.9172306 -3.2300978 -3.3806562 -3.189862 -2.9283962 -2.7827311 -2.6071305 -2.4918442 -2.5019975][-2.5395405 -3.0140183 -3.3366828 -3.4827495 -3.504818 -3.3723149 -3.3384118 -3.3646469 -3.2416036 -2.9915247 -2.7807298 -2.6022816 -2.3363085 -2.1064651 -1.9868562][-1.6897318 -2.249644 -2.8343716 -3.2972536 -3.5051472 -3.3962965 -3.2426353 -3.1192591 -2.876961 -2.6187997 -2.4424264 -2.2575159 -1.9976535 -1.758364 -1.6050613]]...]
INFO - root - 2017-12-07 05:03:04.022777: step 7710, loss = 0.92, batch loss = 0.84 (7.8 examples/sec; 1.027 sec/batch; 92h:36m:48s remains)
INFO - root - 2017-12-07 05:03:14.406793: step 7720, loss = 0.73, batch loss = 0.65 (7.6 examples/sec; 1.055 sec/batch; 95h:09m:05s remains)
INFO - root - 2017-12-07 05:03:24.556289: step 7730, loss = 0.75, batch loss = 0.68 (7.5 examples/sec; 1.072 sec/batch; 96h:43m:19s remains)
INFO - root - 2017-12-07 05:03:34.883693: step 7740, loss = 1.02, batch loss = 0.94 (7.7 examples/sec; 1.038 sec/batch; 93h:37m:11s remains)
INFO - root - 2017-12-07 05:03:45.174277: step 7750, loss = 0.99, batch loss = 0.91 (7.8 examples/sec; 1.024 sec/batch; 92h:22m:33s remains)
INFO - root - 2017-12-07 05:03:55.513606: step 7760, loss = 1.06, batch loss = 0.98 (7.8 examples/sec; 1.023 sec/batch; 92h:16m:59s remains)
INFO - root - 2017-12-07 05:04:05.868719: step 7770, loss = 0.87, batch loss = 0.80 (7.8 examples/sec; 1.020 sec/batch; 92h:00m:44s remains)
INFO - root - 2017-12-07 05:04:16.235991: step 7780, loss = 0.98, batch loss = 0.91 (7.8 examples/sec; 1.028 sec/batch; 92h:46m:00s remains)
INFO - root - 2017-12-07 05:04:26.661788: step 7790, loss = 0.87, batch loss = 0.80 (7.7 examples/sec; 1.045 sec/batch; 94h:16m:08s remains)
INFO - root - 2017-12-07 05:04:37.075443: step 7800, loss = 0.82, batch loss = 0.74 (7.8 examples/sec; 1.020 sec/batch; 91h:57m:13s remains)
2017-12-07 05:04:37.865133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2728691 -2.3865545 -2.1120834 -1.5915716 -1.649899 -2.3848977 -2.9892311 -3.2632751 -3.4140966 -3.62236 -3.813098 -3.8658047 -3.8864987 -3.97476 -3.9874573][-1.7881649 -1.9475145 -1.7501884 -1.1643929 -1.1299994 -1.7834353 -2.3225873 -2.5693831 -2.6925511 -2.9561245 -3.2504783 -3.391248 -3.5222073 -3.6900864 -3.7825236][-1.3977497 -1.4784665 -1.1569281 -0.49440145 -0.37063217 -0.83990884 -1.256032 -1.5198357 -1.6895511 -2.0431745 -2.4027445 -2.5871816 -2.79883 -3.0139632 -3.1811409][-1.0121033 -0.90255809 -0.29015446 0.50939226 0.68308163 0.26207209 -0.20160866 -0.58189774 -0.86863422 -1.268069 -1.5623882 -1.7186511 -1.9863758 -2.2825298 -2.5558927][-0.65964031 -0.32431889 0.55367422 1.4322553 1.593286 1.1072888 0.52325678 0.095994473 -0.22589445 -0.5433054 -0.61600423 -0.66812825 -1.0209789 -1.5522852 -2.0280964][-0.48146319 0.035203934 0.978426 1.7939324 1.8914924 1.3123045 0.71431732 0.43138075 0.27412891 0.20982552 0.41994619 0.48055649 0.031774044 -0.81214213 -1.506541][-0.40635395 0.092680931 0.887444 1.5219846 1.5451832 1.0055146 0.49329233 0.36168528 0.40443039 0.5609684 0.87893915 1.0064316 0.62906551 -0.30737734 -1.093534][-0.49882436 -0.18485928 0.32352018 0.7254734 0.74459267 0.4063015 0.08299017 -0.00701046 0.064387321 0.23585987 0.43453503 0.55007887 0.38614416 -0.39083147 -1.1697884][-0.84234738 -0.64508176 -0.37183142 -0.20797253 -0.22341251 -0.3780961 -0.54669261 -0.66783857 -0.64211345 -0.52025986 -0.47117162 -0.39710331 -0.36231136 -0.86366487 -1.5701659][-1.459326 -1.2229083 -1.1095788 -1.141927 -1.1852846 -1.2037523 -1.2665317 -1.4357896 -1.5041068 -1.4301813 -1.4043939 -1.3112335 -1.1306961 -1.3664534 -1.940546][-2.1179588 -1.776988 -1.7341905 -1.8744872 -1.9447353 -1.9284499 -1.9522641 -2.107631 -2.2438314 -2.237747 -2.1948586 -2.0703371 -1.8313494 -1.8753562 -2.2822258][-2.6123619 -2.3192935 -2.3833046 -2.603281 -2.6998763 -2.6952338 -2.6784458 -2.7318773 -2.8390713 -2.8744252 -2.8487952 -2.7453437 -2.5197458 -2.4630141 -2.7157607][-2.9891129 -2.8444366 -2.9969916 -3.2406449 -3.3402057 -3.3394065 -3.2919457 -3.2682569 -3.3297472 -3.389029 -3.3944497 -3.3377261 -3.1848774 -3.0932665 -3.2026167][-3.4309092 -3.3227754 -3.3842411 -3.5105433 -3.5559492 -3.5604687 -3.5331028 -3.4915183 -3.51853 -3.5769789 -3.6110582 -3.5992908 -3.5295653 -3.4552491 -3.4961655][-3.6692705 -3.5390594 -3.4801145 -3.4732308 -3.4491086 -3.4310026 -3.4194098 -3.3903654 -3.3948252 -3.4370313 -3.4789791 -3.5013866 -3.5010936 -3.4788146 -3.5124321]]...]
INFO - root - 2017-12-07 05:04:48.240746: step 7810, loss = 0.59, batch loss = 0.52 (7.5 examples/sec; 1.063 sec/batch; 95h:50m:58s remains)
INFO - root - 2017-12-07 05:04:58.666969: step 7820, loss = 0.93, batch loss = 0.85 (7.9 examples/sec; 1.013 sec/batch; 91h:20m:05s remains)
INFO - root - 2017-12-07 05:05:08.909400: step 7830, loss = 0.81, batch loss = 0.74 (7.7 examples/sec; 1.035 sec/batch; 93h:22m:12s remains)
INFO - root - 2017-12-07 05:05:19.384659: step 7840, loss = 0.55, batch loss = 0.48 (7.5 examples/sec; 1.074 sec/batch; 96h:50m:11s remains)
INFO - root - 2017-12-07 05:05:29.692452: step 7850, loss = 0.75, batch loss = 0.68 (7.9 examples/sec; 1.009 sec/batch; 90h:57m:06s remains)
INFO - root - 2017-12-07 05:05:40.129888: step 7860, loss = 0.69, batch loss = 0.62 (7.6 examples/sec; 1.054 sec/batch; 95h:00m:53s remains)
INFO - root - 2017-12-07 05:05:50.545484: step 7870, loss = 0.77, batch loss = 0.70 (7.7 examples/sec; 1.037 sec/batch; 93h:27m:59s remains)
INFO - root - 2017-12-07 05:06:01.013174: step 7880, loss = 0.71, batch loss = 0.64 (7.8 examples/sec; 1.023 sec/batch; 92h:12m:50s remains)
INFO - root - 2017-12-07 05:06:11.253756: step 7890, loss = 0.93, batch loss = 0.85 (7.7 examples/sec; 1.040 sec/batch; 93h:49m:02s remains)
INFO - root - 2017-12-07 05:06:21.525624: step 7900, loss = 0.74, batch loss = 0.67 (7.9 examples/sec; 1.017 sec/batch; 91h:44m:01s remains)
2017-12-07 05:06:22.343571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8507781 -3.7588587 -3.7122555 -3.6912727 -3.713882 -3.8350425 -3.9797952 -4.0430021 -4.0886946 -4.1226983 -4.0916266 -4.0299706 -3.9773302 -3.9751379 -4.0090528][-3.7276919 -3.6517267 -3.6361237 -3.6440203 -3.7052665 -3.8811436 -4.0749555 -4.163744 -4.2129107 -4.2543283 -4.2182264 -4.1311841 -4.0445671 -4.0050364 -4.0353866][-3.5061409 -3.4638557 -3.4974542 -3.5565355 -3.6911328 -3.9458268 -4.1992354 -4.3043303 -4.312099 -4.30186 -4.2276573 -4.102531 -3.9659708 -3.8662665 -3.8743436][-3.5086491 -3.4868009 -3.543412 -3.6173906 -3.8107252 -4.1433697 -4.4574385 -4.593 -4.5699692 -4.503931 -4.3887415 -4.22727 -4.0483904 -3.9094214 -3.8993015][-3.3614731 -3.3806353 -3.4578223 -3.5178864 -3.7333002 -4.1115708 -4.4619331 -4.6475062 -4.6571054 -4.6053438 -4.5138068 -4.3635755 -4.1636438 -3.9991493 -3.9726849][-3.1339116 -3.2129807 -3.323298 -3.37294 -3.601099 -3.9869313 -4.3103595 -4.4809489 -4.47464 -4.3753834 -4.2510285 -4.0995865 -3.8960128 -3.7397094 -3.734621][-2.9088638 -3.0152621 -3.1535728 -3.2404773 -3.5244088 -3.9232602 -4.2130818 -4.3710985 -4.3723221 -4.2298617 -4.0583653 -3.8731227 -3.6426628 -3.4896641 -3.5129154][-2.6767983 -2.8141851 -3.019733 -3.1947351 -3.5266185 -3.8879907 -4.1000438 -4.2180443 -4.2462139 -4.1111951 -3.9203858 -3.694268 -3.4053917 -3.2146254 -3.241991][-2.53013 -2.6669593 -2.9085352 -3.1615186 -3.5242376 -3.8629019 -4.046629 -4.1834903 -4.2633934 -4.1417694 -3.9200234 -3.6431768 -3.299788 -3.0602057 -3.0462012][-2.3997707 -2.5193894 -2.7853425 -3.1135287 -3.5172935 -3.8851695 -4.0939851 -4.2784085 -4.4329405 -4.3550582 -4.0971971 -3.7524261 -3.3544769 -3.0519853 -2.9664755][-2.3470659 -2.4732428 -2.7810462 -3.184052 -3.6303332 -4.0269127 -4.2437153 -4.4181867 -4.5896983 -4.5657086 -4.3355889 -3.9764698 -3.5539312 -3.1799879 -2.992579][-2.36027 -2.5283208 -2.8913107 -3.3389304 -3.7843413 -4.1838694 -4.4121933 -4.5730596 -4.7204666 -4.7058291 -4.5122514 -4.1837306 -3.7761154 -3.3485708 -3.0546412][-2.3882604 -2.5717072 -2.9466205 -3.4063392 -3.8146091 -4.1677966 -4.3863049 -4.5343075 -4.6270013 -4.5673113 -4.373004 -4.0967417 -3.7600117 -3.3739734 -3.0627637][-2.4744787 -2.6414309 -2.9818821 -3.4028945 -3.7456546 -4.0393667 -4.2421551 -4.3834162 -4.4551888 -4.36977 -4.1746831 -3.9224522 -3.6331778 -3.3293419 -3.0674381][-2.6696134 -2.8258605 -3.1374526 -3.4989529 -3.7558255 -3.9623659 -4.1244841 -4.2489934 -4.3218493 -4.2596097 -4.09482 -3.8570056 -3.5783038 -3.3395417 -3.146934]]...]
INFO - root - 2017-12-07 05:06:32.566272: step 7910, loss = 0.82, batch loss = 0.74 (8.1 examples/sec; 0.989 sec/batch; 89h:09m:30s remains)
INFO - root - 2017-12-07 05:06:42.914574: step 7920, loss = 0.90, batch loss = 0.83 (7.4 examples/sec; 1.088 sec/batch; 98h:04m:46s remains)
INFO - root - 2017-12-07 05:06:53.071161: step 7930, loss = 1.01, batch loss = 0.94 (7.9 examples/sec; 1.019 sec/batch; 91h:52m:16s remains)
INFO - root - 2017-12-07 05:07:03.482403: step 7940, loss = 0.90, batch loss = 0.83 (7.8 examples/sec; 1.032 sec/batch; 93h:00m:41s remains)
INFO - root - 2017-12-07 05:07:13.872332: step 7950, loss = 0.80, batch loss = 0.73 (7.4 examples/sec; 1.087 sec/batch; 97h:57m:05s remains)
INFO - root - 2017-12-07 05:07:24.383129: step 7960, loss = 0.85, batch loss = 0.78 (7.7 examples/sec; 1.045 sec/batch; 94h:12m:02s remains)
INFO - root - 2017-12-07 05:07:34.775626: step 7970, loss = 0.84, batch loss = 0.77 (7.7 examples/sec; 1.043 sec/batch; 94h:03m:27s remains)
INFO - root - 2017-12-07 05:07:45.085839: step 7980, loss = 0.78, batch loss = 0.70 (7.7 examples/sec; 1.044 sec/batch; 94h:07m:53s remains)
INFO - root - 2017-12-07 05:07:55.481213: step 7990, loss = 0.87, batch loss = 0.80 (7.6 examples/sec; 1.058 sec/batch; 95h:22m:19s remains)
INFO - root - 2017-12-07 05:08:06.036504: step 8000, loss = 0.71, batch loss = 0.63 (7.7 examples/sec; 1.044 sec/batch; 94h:06m:21s remains)
2017-12-07 05:08:06.876971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.594053 -3.5185339 -3.4911456 -3.4533119 -3.3989387 -3.4192364 -3.4651184 -3.4294109 -3.4104772 -3.4960515 -3.5204341 -3.4678531 -3.4099221 -3.2242756 -2.9796872][-4.0350957 -3.9644787 -3.9254117 -3.8633552 -3.7615845 -3.691967 -3.6207578 -3.4924114 -3.4747317 -3.5409236 -3.4637761 -3.3819504 -3.3377225 -3.1282773 -2.8288407][-4.2934022 -4.2520881 -4.2297664 -4.1454473 -3.9805548 -3.7787311 -3.5438359 -3.3406861 -3.3426228 -3.3987079 -3.2779789 -3.2143435 -3.1986821 -2.9795232 -2.6574981][-4.2651153 -4.2664523 -4.2716408 -4.1796002 -3.9886305 -3.6716273 -3.2512169 -2.9609423 -2.9733157 -3.0473106 -3.0066175 -3.0508904 -3.102443 -2.9326587 -2.6545973][-3.8152642 -3.8883967 -3.9338603 -3.8728189 -3.7031436 -3.3519506 -2.8337483 -2.5149534 -2.5539861 -2.6792042 -2.7765369 -2.9735742 -3.1438973 -3.1060669 -2.9045553][-3.1398225 -3.3288174 -3.4354606 -3.4083595 -3.2550874 -2.9066486 -2.4015341 -2.1430101 -2.2667499 -2.4758065 -2.6795528 -2.9904928 -3.2840014 -3.3884664 -3.2404461][-2.6773665 -3.0127912 -3.2176833 -3.2049735 -3.0037971 -2.6117387 -2.135083 -1.9388421 -2.1283731 -2.395551 -2.6364245 -2.9868832 -3.3678565 -3.5796676 -3.4767673][-2.4081159 -2.8209753 -3.0870888 -3.0497837 -2.77699 -2.3701794 -1.9544528 -1.8199124 -2.0537567 -2.3498445 -2.5786722 -2.8762379 -3.2576404 -3.5529971 -3.5650377][-2.3084371 -2.6890378 -2.9461026 -2.8649521 -2.547421 -2.1511712 -1.7798924 -1.6975958 -1.9821496 -2.3185697 -2.5502672 -2.7556357 -3.0534568 -3.3979261 -3.5618305][-2.2810922 -2.5416083 -2.7441099 -2.6724176 -2.410985 -2.0761502 -1.7421477 -1.6676736 -1.9242985 -2.2623544 -2.5260682 -2.6827989 -2.8917363 -3.2551174 -3.5466208][-2.2360404 -2.4165542 -2.5546923 -2.5217705 -2.3649886 -2.1291013 -1.8612623 -1.7570128 -1.8937049 -2.1660941 -2.4601364 -2.6353822 -2.8114066 -3.1493058 -3.4858193][-2.2472498 -2.3975134 -2.4719229 -2.4458165 -2.3630767 -2.2225261 -2.0290647 -1.8971477 -1.9104612 -2.1121213 -2.4292493 -2.6673975 -2.8515284 -3.104455 -3.385746][-2.1979549 -2.2614412 -2.2664573 -2.2342939 -2.2094815 -2.1720586 -2.0790985 -1.9765439 -1.9522796 -2.1152654 -2.4187686 -2.6919365 -2.8696454 -2.991734 -3.1519465][-2.0595145 -1.9916062 -1.9288864 -1.8921666 -1.9174905 -1.9760926 -1.9868631 -1.9512229 -1.9518619 -2.0889409 -2.3357003 -2.5844536 -2.7192991 -2.7306743 -2.7883677][-2.1227112 -1.9401646 -1.7943797 -1.7072849 -1.7313426 -1.8422501 -1.9223812 -1.9298308 -1.9380951 -2.0331335 -2.1972947 -2.3567851 -2.4148092 -2.3783369 -2.382519]]...]
INFO - root - 2017-12-07 05:08:17.219653: step 8010, loss = 0.85, batch loss = 0.78 (7.9 examples/sec; 1.018 sec/batch; 91h:45m:39s remains)
INFO - root - 2017-12-07 05:08:27.477399: step 8020, loss = 0.80, batch loss = 0.72 (7.7 examples/sec; 1.042 sec/batch; 93h:54m:08s remains)
INFO - root - 2017-12-07 05:08:37.791732: step 8030, loss = 0.83, batch loss = 0.76 (7.7 examples/sec; 1.037 sec/batch; 93h:27m:37s remains)
INFO - root - 2017-12-07 05:08:48.237656: step 8040, loss = 0.81, batch loss = 0.74 (7.8 examples/sec; 1.029 sec/batch; 92h:45m:24s remains)
INFO - root - 2017-12-07 05:08:58.491614: step 8050, loss = 0.75, batch loss = 0.68 (7.6 examples/sec; 1.052 sec/batch; 94h:46m:25s remains)
INFO - root - 2017-12-07 05:09:08.659557: step 8060, loss = 0.66, batch loss = 0.59 (7.9 examples/sec; 1.019 sec/batch; 91h:50m:22s remains)
INFO - root - 2017-12-07 05:09:19.027557: step 8070, loss = 0.77, batch loss = 0.69 (7.8 examples/sec; 1.029 sec/batch; 92h:41m:39s remains)
INFO - root - 2017-12-07 05:09:29.444348: step 8080, loss = 0.78, batch loss = 0.70 (7.6 examples/sec; 1.051 sec/batch; 94h:40m:11s remains)
INFO - root - 2017-12-07 05:09:39.872498: step 8090, loss = 0.77, batch loss = 0.70 (7.8 examples/sec; 1.025 sec/batch; 92h:23m:53s remains)
INFO - root - 2017-12-07 05:09:50.258737: step 8100, loss = 1.03, batch loss = 0.96 (7.7 examples/sec; 1.043 sec/batch; 93h:57m:47s remains)
2017-12-07 05:09:51.056284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7626741 -3.8913436 -4.019558 -4.0165448 -3.8914683 -3.7565694 -3.7416527 -3.6647513 -3.5240767 -3.5630536 -3.5971322 -3.5159032 -3.3420675 -3.097362 -3.0048079][-3.9989774 -4.1323805 -4.2995472 -4.3133316 -4.1205726 -3.8716571 -3.8634 -3.8071094 -3.6223598 -3.6932559 -3.7930923 -3.7435002 -3.4784539 -3.0103564 -2.7670457][-4.1782103 -4.2007456 -4.2713957 -4.2276731 -3.9771512 -3.7143927 -3.8223896 -3.7961178 -3.506387 -3.5102811 -3.6463456 -3.6774046 -3.4121211 -2.8025174 -2.4603052][-4.230463 -4.1848187 -4.2014551 -4.139637 -3.8032148 -3.5187845 -3.6955914 -3.6277044 -3.1957963 -3.0651572 -3.1909084 -3.3145843 -3.0810528 -2.4246104 -2.0951135][-4.2039213 -4.1580749 -4.2072558 -4.1874928 -3.7985172 -3.4527462 -3.5853722 -3.4085984 -2.891912 -2.612119 -2.6253397 -2.7899919 -2.60109 -1.9710653 -1.7602229][-4.1486692 -4.0607185 -4.0878572 -4.0775509 -3.6552813 -3.2745154 -3.3755183 -3.2070117 -2.8011003 -2.4757159 -2.2771237 -2.3509834 -2.12548 -1.4738078 -1.39639][-3.8532307 -3.781249 -3.9149389 -4.0402403 -3.6600845 -3.2695069 -3.2954764 -3.1424305 -2.848969 -2.4953678 -2.0821681 -2.129267 -1.9358597 -1.2514913 -1.2239811][-3.3611703 -3.3626707 -3.6603794 -3.9275746 -3.5525756 -3.1168013 -3.0783906 -2.8998775 -2.5994637 -2.1534157 -1.6604707 -1.9105892 -1.9316814 -1.2823777 -1.2705898][-3.0367179 -3.0443258 -3.3363757 -3.5792744 -3.1965485 -2.7735324 -2.7465458 -2.577795 -2.2587602 -1.6936262 -1.1973829 -1.684952 -1.9476919 -1.40995 -1.4355042][-2.9686608 -2.9768515 -3.1590433 -3.2536905 -2.9083331 -2.621773 -2.720808 -2.6041865 -2.2556517 -1.6636109 -1.2288961 -1.831094 -2.1581726 -1.6780739 -1.7259698][-3.0665786 -3.1143582 -3.1324306 -2.960326 -2.5928426 -2.4750776 -2.7005649 -2.6469154 -2.3291876 -1.8598919 -1.6331017 -2.2786565 -2.540339 -2.0524659 -2.0785775][-3.2468543 -3.3787227 -3.3147283 -2.979054 -2.6108789 -2.586503 -2.804162 -2.7168875 -2.4166472 -2.11365 -2.1391935 -2.7693167 -2.935148 -2.45114 -2.4413109][-3.3052144 -3.4944949 -3.4912581 -3.2622318 -3.0452666 -3.0643377 -3.1685965 -3.0664096 -2.8820028 -2.7646766 -2.905478 -3.3588452 -3.3619955 -2.8870049 -2.8033991][-3.1515222 -3.3250971 -3.3686957 -3.3082657 -3.3362737 -3.4929807 -3.5430908 -3.4460347 -3.3597267 -3.3484256 -3.4594855 -3.6748672 -3.548799 -3.1604171 -3.0566602][-3.1076398 -3.2156515 -3.2361088 -3.2199893 -3.3327866 -3.5375586 -3.5784116 -3.4857738 -3.4200015 -3.4170585 -3.4553792 -3.5078688 -3.3850687 -3.1701818 -3.1450105]]...]
INFO - root - 2017-12-07 05:10:01.419190: step 8110, loss = 0.73, batch loss = 0.66 (7.9 examples/sec; 1.010 sec/batch; 91h:01m:34s remains)
INFO - root - 2017-12-07 05:10:11.821271: step 8120, loss = 0.86, batch loss = 0.79 (7.8 examples/sec; 1.025 sec/batch; 92h:21m:23s remains)
INFO - root - 2017-12-07 05:10:22.010542: step 8130, loss = 0.86, batch loss = 0.78 (7.6 examples/sec; 1.051 sec/batch; 94h:39m:21s remains)
INFO - root - 2017-12-07 05:10:32.458395: step 8140, loss = 0.58, batch loss = 0.51 (7.6 examples/sec; 1.047 sec/batch; 94h:21m:17s remains)
INFO - root - 2017-12-07 05:10:42.749138: step 8150, loss = 0.93, batch loss = 0.85 (7.6 examples/sec; 1.055 sec/batch; 95h:00m:31s remains)
INFO - root - 2017-12-07 05:10:53.083623: step 8160, loss = 0.76, batch loss = 0.69 (7.7 examples/sec; 1.040 sec/batch; 93h:41m:12s remains)
INFO - root - 2017-12-07 05:11:03.472265: step 8170, loss = 0.69, batch loss = 0.62 (7.8 examples/sec; 1.028 sec/batch; 92h:38m:17s remains)
INFO - root - 2017-12-07 05:11:13.954102: step 8180, loss = 0.97, batch loss = 0.90 (7.7 examples/sec; 1.046 sec/batch; 94h:11m:24s remains)
INFO - root - 2017-12-07 05:11:24.273299: step 8190, loss = 0.66, batch loss = 0.59 (8.9 examples/sec; 0.898 sec/batch; 80h:51m:13s remains)
INFO - root - 2017-12-07 05:11:34.691147: step 8200, loss = 0.97, batch loss = 0.90 (7.8 examples/sec; 1.031 sec/batch; 92h:53m:40s remains)
2017-12-07 05:11:35.513739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2569883 -1.52195 -1.5966771 -1.3318286 -0.89892149 -0.56324792 -0.61867619 -0.92111254 -1.1283708 -1.1150331 -1.0033927 -1.0448284 -1.2257524 -1.4868274 -1.8495789][-1.4337287 -1.5470812 -1.3379378 -0.73237824 0.026357174 0.68805838 0.91828394 0.79341984 0.72332668 0.82257986 0.93274593 0.84212446 0.63143444 0.41774702 0.11494875][-1.3864179 -1.5561349 -1.4500604 -1.0043981 -0.47818613 0.0091004372 0.1495285 -0.030112267 -0.14819431 -0.11487341 -0.0913105 -0.20472574 -0.31329346 -0.29743862 -0.30946016][-1.1740768 -1.4609163 -1.5377812 -1.2796247 -1.0052135 -0.73337269 -0.7137413 -0.96726489 -1.1680925 -1.2397711 -1.3031306 -1.4053543 -1.4249105 -1.3026671 -1.2077651][-1.0380812 -1.3859324 -1.5827279 -1.3902833 -1.2036335 -0.97862267 -0.89277959 -0.98818064 -1.024514 -0.98142362 -0.9226892 -0.848191 -0.71575713 -0.51762867 -0.3935318][-1.0051744 -1.2932451 -1.5083005 -1.2928157 -1.127676 -0.9811523 -0.98302078 -1.0899217 -1.0708547 -0.98659062 -0.88161397 -0.74688673 -0.56197047 -0.29362059 -0.053566933][-1.0340469 -1.3499396 -1.5607567 -1.1616671 -0.6946528 -0.21616411 0.090275288 0.27371883 0.429739 0.4083519 0.30346251 0.17631721 0.10333109 0.20554876 0.4021492][-1.2591133 -1.7635236 -2.0637875 -1.6386809 -0.99863029 -0.19955492 0.52760887 1.1832876 1.5801377 1.4686465 1.1876287 0.86251354 0.57605553 0.51493359 0.60815811][-1.5246155 -2.0248513 -2.1919279 -1.6359878 -0.84720922 0.064081669 0.87013769 1.5853157 1.8277903 1.4177704 0.93895578 0.52439928 0.22475767 0.16197824 0.18217182][-1.6767616 -2.057049 -2.0748911 -1.4267859 -0.4958992 0.46254778 1.2249446 1.7931094 1.7513857 1.1493559 0.61695576 0.2458868 0.098127365 0.18478584 0.19865894][-1.6062937 -1.9022393 -2.0530159 -1.7153552 -1.1157169 -0.566422 -0.15648556 0.14179564 -0.014399529 -0.39540815 -0.5631988 -0.583179 -0.39472008 -0.044717312 0.085553646][-1.2976406 -1.557822 -1.9422426 -1.9733505 -1.7818475 -1.6383059 -1.4468145 -1.2778094 -1.4081767 -1.4853058 -1.2560432 -0.93168879 -0.50615048 -0.064121246 0.049123764][-0.97769094 -1.3703732 -2.077384 -2.3777165 -2.4035814 -2.3457806 -2.0683403 -1.8468139 -1.8582723 -1.7220941 -1.3195856 -0.85960817 -0.34940243 0.11308193 0.22501802][-0.66504741 -1.0559716 -1.7831919 -2.000591 -2.0173342 -1.9802458 -1.782886 -1.76156 -1.8730032 -1.7509224 -1.4443705 -1.0700431 -0.59063554 -0.12800312 0.047632694][-0.63253093 -0.95148087 -1.4962442 -1.3766677 -1.1199386 -0.82996178 -0.52295136 -0.54971385 -0.6567831 -0.533201 -0.34013367 -0.12244844 0.18544292 0.45017147 0.49270773]]...]
INFO - root - 2017-12-07 05:11:45.993382: step 8210, loss = 0.78, batch loss = 0.70 (7.7 examples/sec; 1.043 sec/batch; 93h:54m:42s remains)
INFO - root - 2017-12-07 05:11:56.580108: step 8220, loss = 0.80, batch loss = 0.73 (7.3 examples/sec; 1.101 sec/batch; 99h:09m:10s remains)
INFO - root - 2017-12-07 05:12:06.950312: step 8230, loss = 0.85, batch loss = 0.77 (7.9 examples/sec; 1.013 sec/batch; 91h:15m:29s remains)
INFO - root - 2017-12-07 05:12:17.257800: step 8240, loss = 0.91, batch loss = 0.84 (7.8 examples/sec; 1.027 sec/batch; 92h:32m:35s remains)
INFO - root - 2017-12-07 05:12:27.805073: step 8250, loss = 0.80, batch loss = 0.73 (7.4 examples/sec; 1.082 sec/batch; 97h:26m:26s remains)
INFO - root - 2017-12-07 05:12:38.214111: step 8260, loss = 0.77, batch loss = 0.70 (7.7 examples/sec; 1.040 sec/batch; 93h:37m:46s remains)
INFO - root - 2017-12-07 05:12:48.546778: step 8270, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.061 sec/batch; 95h:31m:41s remains)
INFO - root - 2017-12-07 05:12:58.983373: step 8280, loss = 0.91, batch loss = 0.83 (7.5 examples/sec; 1.069 sec/batch; 96h:14m:00s remains)
INFO - root - 2017-12-07 05:13:09.397699: step 8290, loss = 0.82, batch loss = 0.75 (7.7 examples/sec; 1.037 sec/batch; 93h:22m:33s remains)
INFO - root - 2017-12-07 05:13:19.741456: step 8300, loss = 0.77, batch loss = 0.70 (7.8 examples/sec; 1.026 sec/batch; 92h:26m:00s remains)
2017-12-07 05:13:20.575490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6430147 -1.8876104 -2.134913 -1.8817043 -1.6445966 -1.9765906 -2.2479591 -2.658392 -3.3009377 -3.6695249 -3.7024288 -3.4996421 -3.2648296 -3.1805124 -3.0371227][-1.737591 -1.9769545 -2.1549046 -1.9411376 -1.7990968 -2.1715286 -2.4388962 -2.797308 -3.3083177 -3.5333402 -3.4915767 -3.217144 -2.8730512 -2.6941481 -2.4377713][-1.6302445 -1.8308485 -1.9633784 -1.8592033 -1.8817813 -2.2903945 -2.5517821 -2.8756301 -3.2691669 -3.3424933 -3.2259295 -2.8818369 -2.4159546 -2.1428685 -1.7911763][-1.4065385 -1.5139511 -1.5815303 -1.5805686 -1.771672 -2.1937749 -2.40894 -2.6794312 -2.969811 -2.9495659 -2.8617237 -2.5529304 -2.0591431 -1.739646 -1.3078666][-1.3014555 -1.3324158 -1.3203897 -1.3563967 -1.6309042 -2.0257413 -2.1380575 -2.2873917 -2.4521692 -2.394603 -2.4074907 -2.1916025 -1.7307856 -1.4200418 -0.95073223][-1.2278545 -1.2399707 -1.2248464 -1.3197351 -1.6681535 -2.0490623 -2.0212014 -1.9627278 -1.9564774 -1.8942211 -2.0336156 -1.8832357 -1.4200983 -1.153964 -0.75244689][-1.0803642 -1.1309783 -1.166888 -1.3163228 -1.7132287 -2.1420407 -2.0844591 -1.8616409 -1.7590294 -1.7749052 -2.035887 -1.8890312 -1.3965592 -1.2270184 -1.0082293][-0.80420995 -0.94719481 -1.1207283 -1.3501291 -1.7586374 -2.2624996 -2.287528 -1.990845 -1.8497615 -2.0029676 -2.3825 -2.1841755 -1.6152313 -1.4950931 -1.4180496][-0.60252643 -0.76123905 -1.0752611 -1.4544115 -1.8773005 -2.3847284 -2.4788342 -2.1518683 -2.0189168 -2.35612 -2.835113 -2.5625167 -1.9140389 -1.7755604 -1.7178688][-0.5848875 -0.65373516 -1.0426676 -1.5909915 -2.0143032 -2.3855565 -2.4269769 -2.0729427 -1.9922388 -2.51706 -3.0829778 -2.8004854 -2.1713867 -2.0247576 -1.8976555][-0.49962664 -0.41466188 -0.8055191 -1.5193005 -2.0107894 -2.2581711 -2.2028184 -1.8449821 -1.8219275 -2.4183633 -2.9728003 -2.723597 -2.2261736 -2.1499097 -1.9979074][-0.34983063 -0.10207033 -0.45317245 -1.3017571 -1.9264326 -2.1386008 -1.9968512 -1.6874158 -1.7620609 -2.3384523 -2.7952595 -2.6051011 -2.2922313 -2.3410738 -2.2248855][-0.41538715 -0.054330826 -0.37610197 -1.3095994 -2.0211804 -2.1823852 -1.9076936 -1.600383 -1.7251561 -2.2195013 -2.561024 -2.4745693 -2.39478 -2.560292 -2.4494348][-0.76139069 -0.30743551 -0.55460739 -1.4620085 -2.1643834 -2.2502151 -1.8908885 -1.6133449 -1.7855544 -2.1754303 -2.4119325 -2.4368973 -2.5352085 -2.731107 -2.5620871][-1.2690091 -0.74476004 -0.894439 -1.7338674 -2.3760445 -2.3732009 -1.951071 -1.6671627 -1.8273342 -2.1094749 -2.2790432 -2.3792679 -2.5659714 -2.7365112 -2.5141444]]...]
INFO - root - 2017-12-07 05:13:31.152900: step 8310, loss = 0.70, batch loss = 0.62 (7.6 examples/sec; 1.046 sec/batch; 94h:10m:54s remains)
INFO - root - 2017-12-07 05:13:41.430487: step 8320, loss = 0.72, batch loss = 0.65 (7.8 examples/sec; 1.027 sec/batch; 92h:30m:23s remains)
INFO - root - 2017-12-07 05:13:51.561519: step 8330, loss = 0.88, batch loss = 0.80 (7.8 examples/sec; 1.027 sec/batch; 92h:28m:00s remains)
INFO - root - 2017-12-07 05:14:01.892776: step 8340, loss = 1.06, batch loss = 0.99 (7.9 examples/sec; 1.018 sec/batch; 91h:38m:54s remains)
INFO - root - 2017-12-07 05:14:12.258876: step 8350, loss = 0.93, batch loss = 0.86 (7.6 examples/sec; 1.051 sec/batch; 94h:40m:13s remains)
INFO - root - 2017-12-07 05:14:22.620804: step 8360, loss = 0.90, batch loss = 0.82 (7.7 examples/sec; 1.039 sec/batch; 93h:31m:08s remains)
INFO - root - 2017-12-07 05:14:32.935791: step 8370, loss = 0.82, batch loss = 0.75 (8.1 examples/sec; 0.988 sec/batch; 88h:59m:32s remains)
INFO - root - 2017-12-07 05:14:43.209099: step 8380, loss = 0.81, batch loss = 0.74 (7.8 examples/sec; 1.025 sec/batch; 92h:19m:26s remains)
INFO - root - 2017-12-07 05:14:53.509185: step 8390, loss = 0.66, batch loss = 0.58 (7.9 examples/sec; 1.012 sec/batch; 91h:07m:30s remains)
INFO - root - 2017-12-07 05:15:03.959100: step 8400, loss = 0.84, batch loss = 0.77 (8.1 examples/sec; 0.987 sec/batch; 88h:52m:45s remains)
2017-12-07 05:15:04.821306: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.61807013 0.2431407 -0.19361544 -0.797235 -1.2189548 -1.120266 -0.5612731 0.079023361 0.34993553 0.034367561 -0.72024536 -1.3461404 -1.5248804 -1.2696266 -0.76275396][0.60172844 0.11551619 -0.44386053 -1.1833639 -1.7516282 -1.7151117 -1.1283586 -0.4157362 0.031555653 0.00392437 -0.56374 -1.2608826 -1.6306615 -1.5166154 -1.0473382][0.57148981 0.017322063 -0.61766696 -1.453651 -2.1489499 -2.1858802 -1.5919952 -0.7751174 -0.061556816 0.27309275 -0.078362465 -0.875782 -1.5214245 -1.687464 -1.4083958][0.49668884 -0.095549583 -0.7569263 -1.6194584 -2.3612156 -2.438869 -1.8624463 -0.98351 -0.053007126 0.62592936 0.51193523 -0.32597256 -1.1889126 -1.6619442 -1.7077572][0.40933084 -0.20451212 -0.858552 -1.6763673 -2.3590677 -2.3970275 -1.8023186 -0.86213374 0.24432945 1.1711521 1.1574473 0.2042284 -0.81712174 -1.5064487 -1.8741033][0.37634277 -0.21777058 -0.840662 -1.5761187 -2.1002584 -2.000447 -1.309659 -0.2632041 1.0434747 2.2010236 2.1804523 1.0047655 -0.21786451 -1.1274543 -1.8304985][0.36493254 -0.18274975 -0.75307822 -1.3638086 -1.6916912 -1.4713919 -0.73980141 0.33793688 1.7275176 2.9855075 2.9454384 1.6959872 0.43170214 -0.62794161 -1.670938][0.35254192 -0.15326929 -0.66315341 -1.1226017 -1.2589221 -0.96305895 -0.27040863 0.69504976 1.8550849 2.8241835 2.6404705 1.570797 0.57119322 -0.38808966 -1.5683925][0.32062292 -0.17771196 -0.66860747 -1.0296164 -1.0578332 -0.74929619 -0.1179328 0.68610287 1.469533 1.9986668 1.7131209 0.9695673 0.34207058 -0.40321207 -1.5254467][0.2923317 -0.20390368 -0.6960144 -1.0304465 -1.0707483 -0.822165 -0.28871536 0.30586481 0.67306757 0.79800463 0.53854322 0.18160439 -0.11033201 -0.66349816 -1.6419222][0.30215597 -0.17617893 -0.66040421 -1.0025446 -1.0958662 -0.90347481 -0.43516326 -0.0024561882 0.040819645 -0.12708807 -0.33065462 -0.38346243 -0.42480612 -0.83854055 -1.6919649][0.37430143 -0.028376579 -0.45742488 -0.80146527 -0.96506429 -0.85097814 -0.46733189 -0.15890265 -0.29032469 -0.57633018 -0.69434023 -0.58278632 -0.52402425 -0.91304922 -1.7315531][0.45365667 0.16456842 -0.1535182 -0.43696141 -0.62044573 -0.59864378 -0.35584831 -0.19378614 -0.39991426 -0.68705463 -0.74842358 -0.63262844 -0.65874529 -1.1409142 -1.9502454][0.50542927 0.3194375 0.12922144 -0.041401863 -0.16849566 -0.19694138 -0.089467049 -0.050349236 -0.26689768 -0.53453946 -0.64114046 -0.70416951 -0.98234844 -1.6134377 -2.3970754][0.57755947 0.46570778 0.36175489 0.27437162 0.20232582 0.15468931 0.16959238 0.12059927 -0.11327648 -0.429801 -0.73308611 -1.1226888 -1.7036004 -2.4329579 -3.1004229]]...]
INFO - root - 2017-12-07 05:15:15.025919: step 8410, loss = 0.78, batch loss = 0.70 (7.8 examples/sec; 1.026 sec/batch; 92h:20m:18s remains)
INFO - root - 2017-12-07 05:15:25.463746: step 8420, loss = 0.73, batch loss = 0.66 (7.5 examples/sec; 1.062 sec/batch; 95h:38m:42s remains)
INFO - root - 2017-12-07 05:15:35.713100: step 8430, loss = 0.89, batch loss = 0.82 (7.9 examples/sec; 1.014 sec/batch; 91h:15m:58s remains)
INFO - root - 2017-12-07 05:15:46.062841: step 8440, loss = 1.04, batch loss = 0.97 (7.7 examples/sec; 1.037 sec/batch; 93h:19m:14s remains)
INFO - root - 2017-12-07 05:15:56.526835: step 8450, loss = 0.75, batch loss = 0.68 (7.5 examples/sec; 1.060 sec/batch; 95h:24m:30s remains)
INFO - root - 2017-12-07 05:16:06.898791: step 8460, loss = 0.99, batch loss = 0.92 (7.8 examples/sec; 1.023 sec/batch; 92h:03m:54s remains)
INFO - root - 2017-12-07 05:16:17.295754: step 8470, loss = 0.72, batch loss = 0.64 (7.7 examples/sec; 1.039 sec/batch; 93h:30m:03s remains)
INFO - root - 2017-12-07 05:16:27.703999: step 8480, loss = 0.80, batch loss = 0.73 (7.8 examples/sec; 1.032 sec/batch; 92h:51m:35s remains)
INFO - root - 2017-12-07 05:16:38.068134: step 8490, loss = 0.76, batch loss = 0.69 (7.8 examples/sec; 1.027 sec/batch; 92h:26m:49s remains)
INFO - root - 2017-12-07 05:16:48.420174: step 8500, loss = 0.71, batch loss = 0.64 (7.5 examples/sec; 1.069 sec/batch; 96h:13m:50s remains)
2017-12-07 05:16:49.226739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.89266038 -0.926836 -0.93371034 -0.95682931 -0.99469805 -1.0396857 -1.1000128 -1.1570017 -1.2012589 -1.2303278 -1.251282 -1.2949274 -1.3346896 -1.3450565 -1.3292351][-0.52512121 -0.53852487 -0.545594 -0.57114029 -0.60164404 -0.6252625 -0.64774323 -0.66413736 -0.66106224 -0.63416576 -0.60674572 -0.60928988 -0.63105249 -0.63849139 -0.62472916][-0.62804914 -0.64259195 -0.63687325 -0.62977052 -0.62357593 -0.60700941 -0.58700109 -0.5725162 -0.55138946 -0.50578618 -0.4627986 -0.45498729 -0.47874355 -0.49400878 -0.47902608][-1.1955445 -1.2726219 -1.2654297 -1.1973991 -1.1007917 -0.96837831 -0.83147 -0.728034 -0.65223503 -0.57627368 -0.52966666 -0.54883862 -0.61111569 -0.66363788 -0.66305828][-1.5267458 -1.6865833 -1.7583838 -1.7655618 -1.7300501 -1.6238589 -1.4713206 -1.3225005 -1.190243 -1.0475037 -0.95815063 -0.96656108 -1.0289679 -1.0880475 -1.0812795][-1.5000987 -1.5931292 -1.6516402 -1.7169876 -1.7889359 -1.8228533 -1.8097723 -1.7626989 -1.6906965 -1.5743513 -1.5225716 -1.5692835 -1.6470931 -1.7089465 -1.6837723][-1.9345131 -2.018 -2.0073502 -1.9306302 -1.8120601 -1.6578326 -1.5020149 -1.359895 -1.2422044 -1.1368685 -1.1677785 -1.3237252 -1.4826458 -1.6064422 -1.6155972][-2.5810776 -2.8269296 -2.8747764 -2.7103431 -2.3800712 -1.9309182 -1.4520111 -0.99161363 -0.614547 -0.3469348 -0.32135773 -0.48869467 -0.68446803 -0.86845016 -0.93663883][-2.5869844 -2.9019771 -2.9864411 -2.8770511 -2.6386833 -2.29381 -1.8736119 -1.4184389 -1.0018568 -0.65574527 -0.53493404 -0.60607243 -0.72132254 -0.86314416 -0.9185276][-2.2665555 -2.5505743 -2.585489 -2.4862428 -2.3468146 -2.1923792 -2.0130153 -1.8267336 -1.659903 -1.4738181 -1.4117243 -1.4643693 -1.5267801 -1.6221724 -1.6501269][-2.3493381 -2.6035819 -2.6353064 -2.5512784 -2.4126105 -2.2337942 -2.0324872 -1.8664865 -1.7571075 -1.6422899 -1.6411026 -1.7421873 -1.8484302 -1.9757028 -2.0268543][-2.5915756 -2.8232455 -2.8662329 -2.8137512 -2.6726489 -2.4487772 -2.1552975 -1.8668559 -1.6226888 -1.4066217 -1.3336272 -1.3798349 -1.4697433 -1.598196 -1.6741257][-2.380487 -2.5623541 -2.55011 -2.4405715 -2.2506537 -2.0502412 -1.8317883 -1.6148436 -1.4174755 -1.2370484 -1.1617775 -1.1477146 -1.1771224 -1.2572565 -1.3246427][-1.8397844 -1.9893637 -1.9799626 -1.8607883 -1.6479051 -1.4592261 -1.297874 -1.1639776 -1.0675867 -0.99328113 -0.9876194 -0.98550797 -0.98989677 -1.0228925 -1.0583684][-1.3329465 -1.4297726 -1.457957 -1.4176331 -1.2962067 -1.1822274 -1.0835416 -0.99675989 -0.93439293 -0.88998985 -0.89803338 -0.89956117 -0.88868475 -0.89010906 -0.89993238]]...]
INFO - root - 2017-12-07 05:16:59.394232: step 8510, loss = 1.00, batch loss = 0.93 (7.8 examples/sec; 1.032 sec/batch; 92h:53m:48s remains)
INFO - root - 2017-12-07 05:17:09.736782: step 8520, loss = 0.78, batch loss = 0.71 (7.8 examples/sec; 1.028 sec/batch; 92h:32m:01s remains)
INFO - root - 2017-12-07 05:17:19.889609: step 8530, loss = 0.91, batch loss = 0.84 (7.7 examples/sec; 1.033 sec/batch; 92h:59m:24s remains)
INFO - root - 2017-12-07 05:17:30.255211: step 8540, loss = 0.89, batch loss = 0.82 (7.8 examples/sec; 1.023 sec/batch; 92h:03m:15s remains)
INFO - root - 2017-12-07 05:17:40.556373: step 8550, loss = 0.79, batch loss = 0.72 (7.9 examples/sec; 1.018 sec/batch; 91h:36m:53s remains)
INFO - root - 2017-12-07 05:17:50.843141: step 8560, loss = 0.83, batch loss = 0.75 (7.8 examples/sec; 1.027 sec/batch; 92h:26m:13s remains)
INFO - root - 2017-12-07 05:18:01.122151: step 8570, loss = 0.75, batch loss = 0.68 (7.3 examples/sec; 1.099 sec/batch; 98h:52m:20s remains)
INFO - root - 2017-12-07 05:18:11.251060: step 8580, loss = 0.82, batch loss = 0.75 (7.8 examples/sec; 1.025 sec/batch; 92h:13m:04s remains)
INFO - root - 2017-12-07 05:18:21.535095: step 8590, loss = 0.76, batch loss = 0.69 (7.7 examples/sec; 1.036 sec/batch; 93h:10m:34s remains)
INFO - root - 2017-12-07 05:18:31.735886: step 8600, loss = 0.64, batch loss = 0.56 (8.2 examples/sec; 0.972 sec/batch; 87h:28m:45s remains)
2017-12-07 05:18:32.529108: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.934818 -3.9402189 -3.9404747 -3.9529786 -3.9556541 -3.9403474 -3.9487624 -4.0008774 -4.0440617 -4.0359659 -4.0176282 -4.0264416 -4.010087 -3.9400592 -3.884212][-3.8697922 -3.8544893 -3.8188765 -3.8206336 -3.8295953 -3.8121827 -3.8208222 -3.8809352 -3.926486 -3.8783386 -3.793566 -3.7727544 -3.8057723 -3.8134832 -3.7964211][-3.5991216 -3.551944 -3.4884806 -3.4964917 -3.5292113 -3.5317693 -3.5418777 -3.6054769 -3.6952386 -3.6827643 -3.5612042 -3.4416149 -3.4200535 -3.4851649 -3.5568509][-3.1465015 -3.0436206 -2.9489059 -2.9637749 -3.0467486 -3.101037 -3.1135137 -3.1714406 -3.3177109 -3.4003398 -3.3612275 -3.2405686 -3.1612291 -3.232636 -3.3462074][-2.8115165 -2.5827174 -2.3811491 -2.3245113 -2.3968062 -2.4585323 -2.44306 -2.5134792 -2.7652111 -2.9449263 -2.9827228 -2.9391856 -2.8849769 -2.9778314 -3.100461][-2.7097557 -2.3556674 -1.9982965 -1.7623506 -1.7126384 -1.6662521 -1.5260544 -1.5667796 -1.942791 -2.2647529 -2.407937 -2.4958076 -2.5020876 -2.5399661 -2.5984406][-2.5282123 -2.0472038 -1.5189784 -1.0867922 -0.8976686 -0.72693443 -0.37367439 -0.20144749 -0.61721563 -1.1755919 -1.5669715 -1.9077742 -2.0287485 -1.9864147 -1.9121413][-2.2966516 -1.826936 -1.2517576 -0.68687534 -0.42266607 -0.24459219 0.22928953 0.72306347 0.55537844 -0.034174919 -0.6457386 -1.2578838 -1.5924006 -1.5691197 -1.3981838][-2.2960656 -2.0041623 -1.5525742 -0.95977116 -0.65476418 -0.60519719 -0.34141016 0.15149117 0.33950377 0.17243719 -0.20505095 -0.79162073 -1.2752614 -1.3837421 -1.2336092][-2.6837945 -2.5228686 -2.1898246 -1.6144676 -1.2664449 -1.2812796 -1.3240373 -1.1602936 -0.9575181 -0.83633327 -0.78932023 -0.97714162 -1.2697654 -1.3800104 -1.2873211][-3.2195711 -3.1244936 -2.8609438 -2.3583972 -2.0213614 -2.0133355 -2.1709447 -2.2487004 -2.2081952 -2.1145797 -1.9231906 -1.8368502 -1.8267434 -1.7597427 -1.6392834][-3.5312333 -3.531589 -3.3413382 -2.9198084 -2.5955799 -2.5414085 -2.6680818 -2.7985449 -2.8876591 -2.9529881 -2.8801551 -2.8061397 -2.6878071 -2.4621015 -2.2326946][-3.6943455 -3.8091598 -3.7469501 -3.457464 -3.1921475 -3.13974 -3.2298331 -3.3353953 -3.4624286 -3.6415634 -3.7227757 -3.7376056 -3.5933833 -3.2425723 -2.858458][-4.094512 -4.268024 -4.3284893 -4.1655335 -3.938803 -3.8753624 -3.923501 -3.978081 -4.0515614 -4.1821489 -4.2700357 -4.2962966 -4.1230121 -3.6875994 -3.1590281][-4.5462661 -4.6838775 -4.7656622 -4.6935024 -4.5498075 -4.4769187 -4.4792943 -4.4731636 -4.4576325 -4.4703908 -4.4609308 -4.4284568 -4.2251353 -3.7911551 -3.2747464]]...]
INFO - root - 2017-12-07 05:18:42.811547: step 8610, loss = 0.77, batch loss = 0.70 (7.8 examples/sec; 1.027 sec/batch; 92h:25m:59s remains)
INFO - root - 2017-12-07 05:18:53.165601: step 8620, loss = 0.78, batch loss = 0.71 (7.8 examples/sec; 1.029 sec/batch; 92h:34m:16s remains)
INFO - root - 2017-12-07 05:19:03.300281: step 8630, loss = 0.80, batch loss = 0.73 (8.2 examples/sec; 0.981 sec/batch; 88h:15m:53s remains)
INFO - root - 2017-12-07 05:19:13.534602: step 8640, loss = 0.71, batch loss = 0.64 (7.9 examples/sec; 1.017 sec/batch; 91h:28m:08s remains)
INFO - root - 2017-12-07 05:19:23.855866: step 8650, loss = 0.59, batch loss = 0.51 (7.7 examples/sec; 1.033 sec/batch; 92h:53m:28s remains)
INFO - root - 2017-12-07 05:19:34.184922: step 8660, loss = 0.69, batch loss = 0.62 (7.9 examples/sec; 1.018 sec/batch; 91h:37m:02s remains)
INFO - root - 2017-12-07 05:19:44.497600: step 8670, loss = 0.88, batch loss = 0.81 (7.8 examples/sec; 1.022 sec/batch; 91h:57m:16s remains)
INFO - root - 2017-12-07 05:19:54.767437: step 8680, loss = 0.78, batch loss = 0.71 (7.7 examples/sec; 1.035 sec/batch; 93h:06m:15s remains)
INFO - root - 2017-12-07 05:20:05.038850: step 8690, loss = 0.98, batch loss = 0.91 (7.4 examples/sec; 1.074 sec/batch; 96h:36m:26s remains)
INFO - root - 2017-12-07 05:20:15.422338: step 8700, loss = 0.74, batch loss = 0.66 (7.8 examples/sec; 1.030 sec/batch; 92h:41m:15s remains)
2017-12-07 05:20:16.222247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4814088 -3.3072591 -3.206418 -3.2597189 -3.468493 -3.6454279 -3.5994575 -3.4485514 -3.4560056 -3.3581367 -3.2178428 -3.290832 -3.5034485 -3.6960402 -3.7688751][-3.1863163 -2.7836609 -2.5213737 -2.5340669 -2.8074729 -3.0927911 -3.0157361 -2.7318678 -2.7254257 -2.6134028 -2.423939 -2.4748101 -2.7868829 -3.1305079 -3.2948632][-2.6788666 -2.0600502 -1.7403507 -1.8188183 -2.2049809 -2.6005197 -2.4154658 -1.9402215 -1.9457409 -1.9275584 -1.8010764 -1.8846154 -2.3069046 -2.7898264 -2.9824343][-2.07188 -1.4232581 -1.303659 -1.7090218 -2.4026611 -2.9389791 -2.4202027 -1.5065007 -1.3900208 -1.4481673 -1.4164813 -1.5372095 -2.045922 -2.6451116 -2.8759956][-1.5344992 -1.0355036 -1.2648537 -2.0573707 -3.013876 -3.4299235 -2.2267697 -0.743438 -0.67554116 -1.1398337 -1.4580953 -1.6551452 -2.1254396 -2.6292195 -2.7265618][-1.2552516 -0.98438859 -1.4818852 -2.399971 -3.1517277 -2.9308167 -0.79255366 1.1043806 0.64781427 -0.66370559 -1.5787234 -1.9948001 -2.5211365 -2.7992096 -2.5548477][-1.2475579 -1.1717014 -1.7450137 -2.4015841 -2.5220513 -1.452863 1.4347358 3.2760272 1.9985456 -0.091200352 -1.2883816 -1.775322 -2.5153942 -2.78865 -2.4003153][-1.3420076 -1.3624363 -1.8186281 -1.9167492 -1.1882155 0.65915775 3.8799067 5.1652479 2.9915295 0.39998436 -0.69155264 -1.11618 -2.1685719 -2.6304936 -2.3015859][-1.7098701 -1.7683449 -2.060848 -1.5743377 -0.20254707 2.0305419 5.1607809 5.9002972 3.3796177 0.75777912 -0.12108231 -0.56390905 -1.9601824 -2.6121984 -2.3563561][-2.336199 -2.3613806 -2.4895697 -1.6305017 -0.14028358 1.8189688 4.305582 4.7576923 2.8162661 0.85604954 0.33035135 -0.23192596 -1.9526501 -2.8016841 -2.6000586][-2.9762621 -3.2235394 -3.3575444 -2.3553889 -1.0084898 0.39828968 2.0259008 2.2991753 1.1667204 0.035808563 -0.067099571 -0.56534672 -2.3411424 -3.2733314 -3.0515735][-3.5022557 -4.0819254 -4.407104 -3.5841279 -2.534039 -1.6113372 -0.56752706 -0.29142427 -0.88622379 -1.5259676 -1.3802116 -1.6533418 -3.1084023 -3.8369126 -3.4887657][-4.1058016 -4.7098603 -5.0153461 -4.4278336 -3.8300886 -3.4430194 -2.8526788 -2.5220048 -2.7091434 -2.9662671 -2.6605139 -2.7634115 -3.7979865 -4.2261224 -3.7574129][-4.3005705 -4.7087512 -4.8028164 -4.395669 -4.2087975 -4.2677555 -4.0807323 -3.8656642 -3.8152013 -3.7373595 -3.3528342 -3.4139795 -4.1338124 -4.3218 -3.8275805][-4.0949054 -4.3135767 -4.2251091 -3.9707105 -4.0630846 -4.3392577 -4.3414178 -4.2050138 -4.0725212 -3.8550971 -3.5596175 -3.6606896 -4.1472178 -4.2197132 -3.8178964]]...]
INFO - root - 2017-12-07 05:20:26.539239: step 8710, loss = 0.92, batch loss = 0.84 (7.6 examples/sec; 1.050 sec/batch; 94h:26m:58s remains)
INFO - root - 2017-12-07 05:20:36.907206: step 8720, loss = 0.89, batch loss = 0.81 (7.4 examples/sec; 1.088 sec/batch; 97h:51m:23s remains)
INFO - root - 2017-12-07 05:20:47.001133: step 8730, loss = 0.86, batch loss = 0.79 (8.0 examples/sec; 1.003 sec/batch; 90h:13m:36s remains)
INFO - root - 2017-12-07 05:20:57.248440: step 8740, loss = 0.75, batch loss = 0.68 (7.4 examples/sec; 1.088 sec/batch; 97h:52m:27s remains)
INFO - root - 2017-12-07 05:21:07.595861: step 8750, loss = 0.95, batch loss = 0.88 (8.0 examples/sec; 1.003 sec/batch; 90h:14m:20s remains)
INFO - root - 2017-12-07 05:21:17.837051: step 8760, loss = 0.81, batch loss = 0.74 (8.0 examples/sec; 1.001 sec/batch; 89h:58m:35s remains)
INFO - root - 2017-12-07 05:21:28.082230: step 8770, loss = 0.75, batch loss = 0.68 (7.9 examples/sec; 1.013 sec/batch; 91h:04m:32s remains)
INFO - root - 2017-12-07 05:21:38.355785: step 8780, loss = 0.86, batch loss = 0.79 (7.9 examples/sec; 1.017 sec/batch; 91h:25m:32s remains)
INFO - root - 2017-12-07 05:21:48.640745: step 8790, loss = 0.60, batch loss = 0.53 (7.8 examples/sec; 1.027 sec/batch; 92h:21m:30s remains)
INFO - root - 2017-12-07 05:21:58.964182: step 8800, loss = 0.73, batch loss = 0.66 (7.7 examples/sec; 1.039 sec/batch; 93h:26m:54s remains)
2017-12-07 05:21:59.823817: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21621656 0.63681746 0.78116703 0.35857487 -0.15372801 -0.24462318 -0.15735054 -0.091505051 -0.12855959 -0.37706518 -0.75170207 -1.1002591 -1.4505153 -1.5782287 -1.5371809][0.22397757 0.62857008 0.85920382 0.53207445 0.044551849 -0.22271252 -0.34739113 -0.35126591 -0.33337545 -0.34483385 -0.38528347 -0.50702906 -0.78997874 -1.0179799 -1.0770791][0.24776363 0.52550268 0.74095869 0.5377059 0.17697334 -0.18333817 -0.43740177 -0.49773145 -0.50632548 -0.34089947 -0.044604778 0.020716667 -0.29791737 -0.72123885 -0.99013066][-0.03369379 0.14842033 0.34364605 0.28192854 0.1427927 -0.10002613 -0.25457144 -0.28637505 -0.41086435 -0.284513 0.11099434 0.23190403 -0.12085676 -0.6393106 -1.0114281][-0.57072473 -0.37533665 -0.11425924 0.054944515 0.27257776 0.396245 0.51976347 0.52574587 0.18584347 0.016281128 0.16745996 0.12920618 -0.2711606 -0.77564526 -1.1158361][-0.9668982 -0.7757194 -0.47309422 -0.051748753 0.55877638 1.1325397 1.6129479 1.7145867 1.1755252 0.61554289 0.35507679 0.06482172 -0.38476324 -0.8448894 -1.1639934][-1.0932662 -0.9678669 -0.74448562 -0.2570529 0.54162455 1.4985361 2.4367995 2.7876887 2.1743984 1.3060584 0.70951796 0.22830725 -0.29938412 -0.834723 -1.2577932][-1.1122744 -1.056313 -0.98832512 -0.64590979 0.053652287 1.1390662 2.4679523 3.167253 2.6429381 1.7193003 1.0543847 0.5384841 -0.059574604 -0.76361632 -1.3766379][-1.2540612 -1.2829623 -1.3337989 -1.1889246 -0.69915891 0.26646376 1.6500673 2.5051398 2.2430015 1.5479369 1.0400214 0.61909866 0.071199894 -0.68316674 -1.41803][-1.293153 -1.3960161 -1.5015421 -1.465359 -1.138757 -0.41537189 0.69643116 1.4407735 1.3397145 0.94265223 0.65408087 0.36723185 -0.0075302124 -0.63906431 -1.3513997][-0.913697 -1.2064829 -1.394278 -1.43518 -1.2343655 -0.74541235 0.050290585 0.64453793 0.61390114 0.38495684 0.16980457 -0.042490482 -0.23778296 -0.72043037 -1.3814011][-0.18173742 -0.66182232 -1.007282 -1.2313643 -1.2404623 -1.0157223 -0.40831375 0.17605829 0.25072956 0.084145546 -0.16465282 -0.37491798 -0.5062294 -0.96902728 -1.6762991][0.47960234 -0.081588268 -0.590359 -1.063138 -1.3768477 -1.3899312 -0.865299 -0.22243738 -0.010015965 -0.14260912 -0.4373126 -0.67626095 -0.85215783 -1.3206432 -2.0148709][0.62499285 0.096986294 -0.41476154 -0.98660517 -1.5115139 -1.6544578 -1.1806154 -0.56502175 -0.30581856 -0.45724702 -0.79439569 -1.0229294 -1.1632965 -1.504401 -2.0091784][0.31556702 -0.088656425 -0.455441 -0.94590521 -1.53176 -1.6855314 -1.2753596 -0.76305246 -0.51353335 -0.70571351 -1.0874891 -1.2527902 -1.2985218 -1.4520531 -1.6542559]]...]
INFO - root - 2017-12-07 05:22:10.140172: step 8810, loss = 0.76, batch loss = 0.69 (7.7 examples/sec; 1.034 sec/batch; 92h:58m:35s remains)
INFO - root - 2017-12-07 05:22:20.582108: step 8820, loss = 0.71, batch loss = 0.64 (7.9 examples/sec; 1.013 sec/batch; 91h:05m:47s remains)
INFO - root - 2017-12-07 05:22:30.794418: step 8830, loss = 0.79, batch loss = 0.71 (7.5 examples/sec; 1.069 sec/batch; 96h:06m:47s remains)
INFO - root - 2017-12-07 05:22:41.157607: step 8840, loss = 0.75, batch loss = 0.68 (7.8 examples/sec; 1.027 sec/batch; 92h:19m:31s remains)
INFO - root - 2017-12-07 05:22:51.639092: step 8850, loss = 0.96, batch loss = 0.89 (7.5 examples/sec; 1.072 sec/batch; 96h:21m:31s remains)
INFO - root - 2017-12-07 05:23:01.961995: step 8860, loss = 0.72, batch loss = 0.65 (7.8 examples/sec; 1.023 sec/batch; 91h:59m:42s remains)
INFO - root - 2017-12-07 05:23:12.358409: step 8870, loss = 0.88, batch loss = 0.81 (7.6 examples/sec; 1.057 sec/batch; 95h:03m:42s remains)
INFO - root - 2017-12-07 05:23:22.617720: step 8880, loss = 0.80, batch loss = 0.72 (8.1 examples/sec; 0.984 sec/batch; 88h:28m:43s remains)
INFO - root - 2017-12-07 05:23:33.102654: step 8890, loss = 0.86, batch loss = 0.79 (7.4 examples/sec; 1.078 sec/batch; 96h:56m:23s remains)
INFO - root - 2017-12-07 05:23:43.367213: step 8900, loss = 0.65, batch loss = 0.58 (7.7 examples/sec; 1.032 sec/batch; 92h:48m:19s remains)
2017-12-07 05:23:44.181812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.301738 -1.0505502 -1.0128078 -0.79901671 -0.43205047 -0.42561054 -0.45198131 -0.48374677 -0.57962465 -0.43828583 -0.48007441 -0.74634385 -0.86133623 -0.839561 -0.79879212][-1.2494617 -1.0901003 -1.0367715 -0.62692404 -0.14926147 -0.279109 -0.49376297 -0.56083751 -0.64294314 -0.61636376 -0.80454016 -1.0521266 -1.0533342 -0.99058723 -0.93974948][-1.1685817 -1.2141118 -1.2882209 -0.88269687 -0.43259096 -0.70180774 -1.0636921 -1.1300945 -1.1105266 -1.0204284 -1.1219118 -1.174427 -0.98668408 -0.89284682 -0.86992645][-0.44882727 -0.76208854 -1.095845 -0.92126036 -0.66705632 -1.1030939 -1.5484214 -1.5861516 -1.4969237 -1.2966335 -1.1806526 -0.96680236 -0.57143283 -0.3951211 -0.3676157][0.21088886 -0.20514536 -0.70375371 -0.74535465 -0.7136066 -1.2489026 -1.6200695 -1.5241697 -1.3922629 -1.1897855 -0.96244907 -0.60161591 -0.15599966 0.041535854 0.0626421][0.44446659 0.13220263 -0.36543322 -0.47127271 -0.60325265 -1.1471491 -1.2485414 -0.90765548 -0.77729082 -0.666945 -0.45334983 -0.17463684 0.10510683 0.27969885 0.32188511][0.47278452 0.29989243 -0.10063505 -0.10467243 -0.22163534 -0.55233741 -0.1785531 0.43121147 0.39025259 0.23129845 0.20147991 0.15400267 0.19124985 0.40980673 0.56587076][0.32571745 0.2535224 0.077946663 0.40829515 0.54159117 0.57504749 1.3875203 2.0819874 1.6140299 1.0622382 0.77990437 0.42841482 0.33052778 0.62369728 0.84067249][0.041428089 -0.0017185211 0.037918568 0.559443 0.77383375 0.91914225 1.7804728 2.3248172 1.6608758 1.1011562 0.84716749 0.39097595 0.25784588 0.52926683 0.66053867][-0.43531394 -0.49240804 -0.34851217 0.11305237 0.1610384 0.16058588 0.84841204 1.32056 0.946538 0.75341797 0.63127613 0.12987375 -0.092766762 -0.012838364 -0.065610409][-0.98152161 -1.0113683 -0.80755925 -0.44173861 -0.50422978 -0.58931947 -0.092254162 0.26114941 0.14442253 0.24458504 0.15228176 -0.35919952 -0.60011888 -0.6481235 -0.76830363][-1.6083281 -1.5708716 -1.3238816 -1.0275271 -1.1209338 -1.2535579 -1.0126891 -0.9323616 -1.0109031 -0.80480886 -0.87142754 -1.2311041 -1.312906 -1.3320422 -1.3813088][-2.5720816 -2.5060353 -2.2817566 -2.0411687 -2.1104753 -2.2214653 -2.1681561 -2.2810471 -2.3323822 -2.0831316 -2.0783792 -2.2448406 -2.1844544 -2.1648223 -2.1595445][-3.5435245 -3.5034003 -3.3398433 -3.1281879 -3.12235 -3.1247559 -3.0773759 -3.182569 -3.1978092 -3.0089569 -3.0027518 -3.1099248 -3.0600553 -3.0465055 -3.0209918][-3.8100247 -3.7799597 -3.6783996 -3.5320373 -3.5049722 -3.45925 -3.4162374 -3.49969 -3.5344265 -3.4701738 -3.5250037 -3.6355968 -3.6341183 -3.6233389 -3.5636711]]...]
INFO - root - 2017-12-07 05:23:54.416743: step 8910, loss = 0.72, batch loss = 0.65 (7.7 examples/sec; 1.044 sec/batch; 93h:50m:31s remains)
INFO - root - 2017-12-07 05:24:04.609328: step 8920, loss = 0.80, batch loss = 0.73 (7.9 examples/sec; 1.010 sec/batch; 90h:46m:09s remains)
INFO - root - 2017-12-07 05:24:14.743222: step 8930, loss = 0.91, batch loss = 0.84 (7.7 examples/sec; 1.045 sec/batch; 93h:57m:56s remains)
INFO - root - 2017-12-07 05:24:25.114941: step 8940, loss = 0.73, batch loss = 0.66 (7.6 examples/sec; 1.046 sec/batch; 94h:02m:59s remains)
INFO - root - 2017-12-07 05:24:35.472014: step 8950, loss = 0.81, batch loss = 0.73 (7.6 examples/sec; 1.058 sec/batch; 95h:04m:03s remains)
INFO - root - 2017-12-07 05:24:45.859153: step 8960, loss = 0.87, batch loss = 0.80 (7.3 examples/sec; 1.092 sec/batch; 98h:10m:43s remains)
INFO - root - 2017-12-07 05:24:56.241730: step 8970, loss = 0.87, batch loss = 0.80 (7.7 examples/sec; 1.043 sec/batch; 93h:44m:15s remains)
INFO - root - 2017-12-07 05:25:06.499504: step 8980, loss = 0.83, batch loss = 0.76 (8.0 examples/sec; 1.005 sec/batch; 90h:17m:43s remains)
INFO - root - 2017-12-07 05:25:16.722827: step 8990, loss = 0.67, batch loss = 0.60 (7.9 examples/sec; 1.010 sec/batch; 90h:44m:11s remains)
INFO - root - 2017-12-07 05:25:27.059137: step 9000, loss = 0.63, batch loss = 0.55 (8.0 examples/sec; 1.000 sec/batch; 89h:51m:42s remains)
2017-12-07 05:25:27.885872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8369977 -1.9474213 -1.6471472 -1.3878384 -1.2389269 -1.1263788 -0.92957377 -0.82805443 -0.85204768 -0.91761255 -1.0777104 -1.3928299 -1.7535772 -1.8663709 -1.773319][-1.7044022 -1.7044475 -1.39519 -1.144136 -0.98457 -0.82339621 -0.61403775 -0.46747518 -0.38494253 -0.40857363 -0.60258412 -1.0142412 -1.4861779 -1.6611974 -1.5918574][-1.8685889 -1.7836304 -1.5155551 -1.328469 -1.2048228 -1.0340271 -0.83943653 -0.65354061 -0.46747446 -0.43282104 -0.59495544 -1.0093279 -1.5269222 -1.7675951 -1.7677848][-2.0444384 -1.9514215 -1.7547541 -1.6291776 -1.5155213 -1.3285661 -1.1557245 -0.950717 -0.72237754 -0.681947 -0.83490109 -1.242949 -1.8128469 -2.200572 -2.3943183][-2.0731022 -2.0322671 -1.933558 -1.8532269 -1.7226722 -1.5075407 -1.34656 -1.1373787 -0.94136262 -0.975718 -1.1258566 -1.4775667 -2.0147738 -2.4645386 -2.7743032][-1.6969764 -1.7428679 -1.7900813 -1.8067679 -1.7184753 -1.5095537 -1.3205733 -1.073307 -0.946455 -1.1238601 -1.3020637 -1.5911562 -2.0694518 -2.5174003 -2.8746262][-1.3184583 -1.4320118 -1.5711546 -1.6549172 -1.6303756 -1.4616241 -1.2024436 -0.79628205 -0.63819337 -0.88552594 -1.0660045 -1.2757277 -1.6560843 -2.0820394 -2.4896007][-1.6921663 -1.8212979 -1.945533 -1.9760604 -1.9155033 -1.7535079 -1.424542 -0.88955092 -0.64502788 -0.82825756 -0.93042541 -1.0155022 -1.2153695 -1.4960375 -1.8149478][-2.1297574 -2.2744052 -2.4212842 -2.4777074 -2.4606605 -2.394685 -2.1894026 -1.7826784 -1.5637445 -1.626945 -1.5598078 -1.4295917 -1.3457527 -1.3612552 -1.4520054][-2.3204789 -2.4304531 -2.5711675 -2.6562259 -2.6981256 -2.7263772 -2.6774049 -2.4827144 -2.397033 -2.4757974 -2.4267154 -2.2848215 -2.1378689 -2.0789943 -2.0659907][-2.7089267 -2.7771044 -2.8736095 -2.9332039 -2.9710534 -3.0135763 -3.0284591 -2.9454665 -2.8982964 -2.9467165 -2.9296079 -2.856813 -2.7672358 -2.7274985 -2.6813536][-3.1648443 -3.2149496 -3.2768159 -3.2922492 -3.2926044 -3.3222857 -3.3699212 -3.360992 -3.314168 -3.279624 -3.219595 -3.1726055 -3.1375847 -3.1075926 -3.0256958][-3.4305379 -3.4818871 -3.5392313 -3.5558338 -3.5414011 -3.5530484 -3.5930588 -3.6097302 -3.5740342 -3.51303 -3.43619 -3.411828 -3.4384055 -3.4557712 -3.383153][-3.3648949 -3.4288902 -3.5057514 -3.5541539 -3.5809004 -3.6216946 -3.6817589 -3.7342587 -3.7556348 -3.7451906 -3.7113376 -3.7098722 -3.7515855 -3.7858603 -3.7274868][-3.2460394 -3.3272481 -3.4179211 -3.4572997 -3.4795666 -3.5254841 -3.5875969 -3.6531634 -3.7025695 -3.7238302 -3.7351069 -3.7775054 -3.856091 -3.9374321 -3.9468708]]...]
INFO - root - 2017-12-07 05:25:37.994049: step 9010, loss = 0.90, batch loss = 0.83 (7.9 examples/sec; 1.016 sec/batch; 91h:19m:15s remains)
INFO - root - 2017-12-07 05:25:48.210292: step 9020, loss = 0.81, batch loss = 0.74 (7.8 examples/sec; 1.019 sec/batch; 91h:35m:04s remains)
INFO - root - 2017-12-07 05:25:58.408685: step 9030, loss = 0.85, batch loss = 0.78 (8.2 examples/sec; 0.980 sec/batch; 88h:04m:08s remains)
INFO - root - 2017-12-07 05:26:08.625644: step 9040, loss = 0.70, batch loss = 0.63 (7.3 examples/sec; 1.089 sec/batch; 97h:50m:37s remains)
INFO - root - 2017-12-07 05:26:18.942153: step 9050, loss = 0.79, batch loss = 0.71 (7.8 examples/sec; 1.027 sec/batch; 92h:16m:20s remains)
INFO - root - 2017-12-07 05:26:29.317351: step 9060, loss = 0.97, batch loss = 0.90 (7.9 examples/sec; 1.009 sec/batch; 90h:36m:33s remains)
INFO - root - 2017-12-07 05:26:39.792482: step 9070, loss = 0.86, batch loss = 0.79 (7.5 examples/sec; 1.070 sec/batch; 96h:05m:14s remains)
INFO - root - 2017-12-07 05:26:50.131668: step 9080, loss = 0.79, batch loss = 0.71 (8.1 examples/sec; 0.988 sec/batch; 88h:43m:50s remains)
INFO - root - 2017-12-07 05:27:00.462204: step 9090, loss = 0.82, batch loss = 0.75 (8.0 examples/sec; 0.998 sec/batch; 89h:38m:41s remains)
INFO - root - 2017-12-07 05:27:10.839208: step 9100, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.060 sec/batch; 95h:11m:16s remains)
2017-12-07 05:27:11.654859: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.61937 -2.8271327 -2.7304056 -2.198036 -1.6233187 -1.4351525 -1.7776432 -2.3835602 -2.7368298 -2.8104563 -2.7832134 -2.7319684 -2.814949 -3.065469 -3.3209102][-2.7597857 -2.9630451 -2.772727 -2.0784247 -1.3509724 -1.0112796 -1.2506731 -1.9202218 -2.4391499 -2.7078121 -2.8243957 -2.8759606 -2.9627073 -3.0091381 -2.9323335][-2.9231687 -3.1492472 -2.8853836 -2.0652208 -1.256767 -0.8133781 -0.899812 -1.5448351 -2.1970417 -2.5927143 -2.7618256 -2.8499963 -2.8983665 -2.750628 -2.3912961][-3.0707321 -3.3392191 -3.0579431 -2.2005577 -1.3851252 -0.85825348 -0.71622086 -1.2008367 -1.9247837 -2.4537902 -2.7190909 -2.8368483 -2.8161371 -2.5310268 -2.0368843][-3.1460724 -3.4751728 -3.284934 -2.5158203 -1.6953597 -0.94911551 -0.35344505 -0.48016524 -1.2359152 -2.0561604 -2.6531596 -2.965179 -2.9582191 -2.6225309 -2.1128571][-3.1404958 -3.5637126 -3.5403914 -2.9219437 -2.0790558 -1.0102203 0.19843626 0.57348728 -0.18059397 -1.3712938 -2.3946497 -2.979588 -3.1134875 -2.8859615 -2.4619889][-3.0633197 -3.607192 -3.7878482 -3.3387055 -2.4658384 -1.0469434 0.76477528 1.6645708 0.9812727 -0.45513177 -1.748281 -2.5602469 -2.9502466 -3.0097797 -2.7515228][-2.9421535 -3.5900664 -3.9706268 -3.7474277 -2.9388709 -1.3124781 0.86660433 2.1799493 1.6811528 0.2357049 -1.1116667 -2.0172932 -2.5794578 -2.8583932 -2.7769763][-2.8465085 -3.5305278 -4.034637 -4.0204239 -3.3690705 -1.8141985 0.25582027 1.5384493 1.1935205 0.00053024292 -1.156471 -1.939163 -2.4336107 -2.6962237 -2.6816359][-2.816968 -3.473124 -3.9664848 -4.0215931 -3.4711294 -2.1166594 -0.43911147 0.47684526 0.13620901 -0.79010773 -1.7112379 -2.3135605 -2.6455655 -2.7915974 -2.76687][-2.7840555 -3.3821151 -3.7993746 -3.8151636 -3.2910485 -2.1312606 -0.83862543 -0.28171873 -0.63460326 -1.3876731 -2.1466448 -2.5942249 -2.7704191 -2.8491924 -2.8459742][-2.7082987 -3.2330427 -3.5554988 -3.5036225 -3.0583146 -2.1425512 -1.1312506 -0.7313149 -1.034631 -1.7287321 -2.4471169 -2.7670417 -2.7927642 -2.7751918 -2.7210631][-2.6075 -3.05473 -3.3340483 -3.2966466 -2.9969225 -2.3317816 -1.4836977 -1.042944 -1.2360568 -1.9305804 -2.6995094 -3.0223718 -3.003983 -2.8786259 -2.6810672][-2.5098879 -2.8982198 -3.1995718 -3.2262895 -3.0692582 -2.6166866 -1.8890128 -1.4008355 -1.4579177 -2.0297952 -2.7489007 -3.1307876 -3.2097282 -3.0654268 -2.7604113][-2.461884 -2.8295891 -3.1989555 -3.3152952 -3.2477856 -2.9217377 -2.304637 -1.8561676 -1.8506629 -2.2921994 -2.903368 -3.2936628 -3.4259658 -3.2780175 -2.937021]]...]
INFO - root - 2017-12-07 05:27:22.051330: step 9110, loss = 0.79, batch loss = 0.72 (7.8 examples/sec; 1.023 sec/batch; 91h:53m:56s remains)
INFO - root - 2017-12-07 05:27:32.479098: step 9120, loss = 0.71, batch loss = 0.64 (7.6 examples/sec; 1.052 sec/batch; 94h:27m:15s remains)
INFO - root - 2017-12-07 05:27:42.672383: step 9130, loss = 0.62, batch loss = 0.55 (7.8 examples/sec; 1.025 sec/batch; 92h:03m:23s remains)
INFO - root - 2017-12-07 05:27:53.082971: step 9140, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.052 sec/batch; 94h:32m:04s remains)
INFO - root - 2017-12-07 05:28:03.411936: step 9150, loss = 0.64, batch loss = 0.57 (7.6 examples/sec; 1.058 sec/batch; 95h:00m:13s remains)
INFO - root - 2017-12-07 05:28:13.745322: step 9160, loss = 0.63, batch loss = 0.56 (7.8 examples/sec; 1.029 sec/batch; 92h:26m:54s remains)
INFO - root - 2017-12-07 05:28:24.080377: step 9170, loss = 0.76, batch loss = 0.69 (8.0 examples/sec; 0.995 sec/batch; 89h:19m:23s remains)
INFO - root - 2017-12-07 05:28:34.458127: step 9180, loss = 0.81, batch loss = 0.74 (7.7 examples/sec; 1.036 sec/batch; 93h:02m:08s remains)
INFO - root - 2017-12-07 05:28:44.817713: step 9190, loss = 0.96, batch loss = 0.89 (7.9 examples/sec; 1.018 sec/batch; 91h:26m:30s remains)
INFO - root - 2017-12-07 05:28:55.030169: step 9200, loss = 0.75, batch loss = 0.68 (8.4 examples/sec; 0.948 sec/batch; 85h:06m:50s remains)
2017-12-07 05:28:55.851513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4875121 -4.438509 -4.3841681 -4.4998174 -4.6390595 -4.6719437 -4.5753789 -4.3828011 -4.3077235 -4.392808 -4.4940739 -4.5405092 -4.5163083 -4.4971385 -4.530035][-4.677897 -4.6287122 -4.5331521 -4.5986342 -4.4908466 -4.2075143 -3.9008479 -3.6076422 -3.5630147 -3.77634 -3.9848907 -4.1260815 -4.1756377 -4.1852064 -4.2355428][-4.8586378 -4.6904526 -4.3875618 -4.2167854 -3.7161174 -3.1541333 -2.8087912 -2.5548871 -2.6335878 -2.9888978 -3.2211943 -3.4118404 -3.5524511 -3.5963178 -3.6745353][-4.9556575 -4.5042534 -3.8250914 -3.3106489 -2.468255 -1.8056986 -1.5123789 -1.2261472 -1.3641756 -1.831157 -2.0264149 -2.2162049 -2.478405 -2.6292248 -2.7809639][-5.0840144 -4.474597 -3.6544616 -3.0849929 -2.1632097 -1.4118562 -0.88889956 -0.20426989 -0.23145342 -0.81914806 -0.97339249 -1.1194022 -1.4861574 -1.7572331 -1.942486][-5.2907553 -4.7460284 -4.0253768 -3.5603154 -2.5586116 -1.4856706 -0.40360689 0.87837458 0.90396357 -0.0094122887 -0.31668854 -0.49162984 -0.93099833 -1.2593393 -1.3920813][-5.4896269 -5.026289 -4.3166003 -3.7157459 -2.3761463 -0.82333517 0.85999775 2.6626725 2.5917921 1.1691728 0.46133137 0.098526 -0.42338657 -0.79145527 -0.91043496][-5.5532055 -5.1369929 -4.3863339 -3.5788331 -1.9465594 -0.1355567 1.7773781 3.7090054 3.3697305 1.6102481 0.70701551 0.26887226 -0.19728327 -0.50169921 -0.58967662][-5.37962 -4.9798446 -4.22564 -3.3418932 -1.7383595 -0.11191702 1.4356337 2.8618817 2.3568769 0.77719688 -0.021917343 -0.33583689 -0.47622085 -0.46622586 -0.42488766][-5.2502866 -4.8344994 -4.0581203 -3.1735036 -1.7310624 -0.33688402 0.79674959 1.670599 1.133081 -0.065029621 -0.6099906 -0.73864484 -0.519928 -0.21100473 -0.10138178][-5.2049966 -4.73616 -3.9010067 -3.0172677 -1.6871574 -0.40199041 0.45000505 0.87044239 0.290133 -0.63347673 -1.0262191 -1.06953 -0.67395663 -0.24843693 -0.15791512][-5.1857977 -4.6517906 -3.8034174 -2.9950349 -1.9316189 -0.93220472 -0.36537695 -0.22567129 -0.73517108 -1.4386725 -1.7810578 -1.7897103 -1.3313537 -0.89523196 -0.82275367][-5.13587 -4.5822382 -3.7434916 -3.0473902 -2.3049645 -1.6908748 -1.3680539 -1.348742 -1.7426329 -2.2403989 -2.4600878 -2.3542485 -1.834717 -1.4007556 -1.3266263][-5.0304518 -4.6386318 -3.9961984 -3.4482358 -2.926496 -2.5226569 -2.3096263 -2.3618541 -2.7482796 -3.1892073 -3.3889577 -3.250526 -2.7548931 -2.3215919 -2.1480875][-4.7679558 -4.5594583 -4.1657104 -3.8073814 -3.4779277 -3.2493024 -3.15304 -3.2437127 -3.5824809 -3.937835 -4.0845537 -3.9528167 -3.550307 -3.1762469 -2.9739041]]...]
INFO - root - 2017-12-07 05:29:06.324861: step 9210, loss = 0.75, batch loss = 0.67 (7.9 examples/sec; 1.019 sec/batch; 91h:29m:22s remains)
INFO - root - 2017-12-07 05:29:16.777208: step 9220, loss = 0.69, batch loss = 0.62 (7.6 examples/sec; 1.058 sec/batch; 95h:00m:02s remains)
INFO - root - 2017-12-07 05:29:27.032414: step 9230, loss = 0.70, batch loss = 0.62 (7.6 examples/sec; 1.050 sec/batch; 94h:16m:39s remains)
INFO - root - 2017-12-07 05:29:37.448282: step 9240, loss = 0.96, batch loss = 0.89 (7.5 examples/sec; 1.071 sec/batch; 96h:12m:40s remains)
INFO - root - 2017-12-07 05:29:47.776616: step 9250, loss = 0.68, batch loss = 0.60 (7.7 examples/sec; 1.043 sec/batch; 93h:37m:03s remains)
INFO - root - 2017-12-07 05:29:58.083090: step 9260, loss = 0.75, batch loss = 0.68 (7.8 examples/sec; 1.022 sec/batch; 91h:47m:12s remains)
INFO - root - 2017-12-07 05:30:08.393045: step 9270, loss = 0.89, batch loss = 0.81 (7.8 examples/sec; 1.023 sec/batch; 91h:52m:30s remains)
INFO - root - 2017-12-07 05:30:18.752661: step 9280, loss = 0.95, batch loss = 0.88 (7.6 examples/sec; 1.059 sec/batch; 95h:05m:14s remains)
INFO - root - 2017-12-07 05:30:29.272428: step 9290, loss = 0.82, batch loss = 0.75 (7.5 examples/sec; 1.063 sec/batch; 95h:28m:21s remains)
INFO - root - 2017-12-07 05:30:39.599448: step 9300, loss = 0.84, batch loss = 0.76 (7.8 examples/sec; 1.022 sec/batch; 91h:44m:04s remains)
2017-12-07 05:30:40.417391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7248588 -3.1957793 -3.755662 -4.2572446 -4.2721553 -3.9157012 -3.5538812 -3.0295839 -2.406872 -2.1558263 -2.2011316 -2.3119552 -2.7044365 -3.5480566 -4.4043188][-2.1810479 -2.8316817 -3.5629814 -4.1930618 -4.3621345 -4.1455445 -3.8874912 -3.2711308 -2.5078044 -2.2410975 -2.2504992 -2.2939107 -2.6593428 -3.3848412 -4.0037069][-1.6291778 -2.1898975 -2.8093138 -3.3156128 -3.5608466 -3.5587661 -3.5639749 -3.1685095 -2.6389003 -2.5302272 -2.4353137 -2.1859887 -2.2860911 -2.6914761 -2.98618][-1.339164 -1.7381632 -2.1854215 -2.519377 -2.7614474 -2.856782 -2.9432292 -2.6614437 -2.3506494 -2.4263146 -2.3054833 -1.8789968 -1.7748792 -1.9610388 -2.0496309][-1.2858019 -1.6008852 -1.9455135 -2.1560359 -2.3309398 -2.3772404 -2.3779275 -2.106508 -1.9512908 -2.2277915 -2.2400806 -1.8769603 -1.7412941 -1.8363597 -1.8017416][-1.3636146 -1.5529454 -1.7604172 -1.8053808 -1.8423097 -1.8075941 -1.7632077 -1.5447195 -1.5399494 -2.0563483 -2.3380373 -2.1559296 -2.076937 -2.1401441 -2.0356505][-1.6598799 -1.6884143 -1.7508547 -1.6121914 -1.4401686 -1.2574036 -1.0702858 -0.76211548 -0.73253965 -1.3256311 -1.7863622 -1.8055828 -1.9235306 -2.1005208 -2.0256295][-2.0113456 -1.7845905 -1.6543567 -1.3061275 -0.88363743 -0.53458476 -0.19813538 0.25134134 0.46462393 0.034545422 -0.44323945 -0.6642487 -1.0963366 -1.5533824 -1.7134202][-2.2072551 -1.7651901 -1.4669719 -0.99941516 -0.47558856 -0.12873459 0.13407898 0.446661 0.6580739 0.45939875 0.11036491 -0.21004772 -0.77899575 -1.3142908 -1.5665033][-2.4491911 -1.9439294 -1.557658 -1.1068738 -0.67124343 -0.45378733 -0.40114737 -0.41576195 -0.35715628 -0.33319426 -0.36164665 -0.44624543 -0.73006058 -0.97297931 -1.0627024][-2.8496909 -2.4611554 -2.0647352 -1.698926 -1.4604573 -1.4098232 -1.4966218 -1.6888411 -1.7005086 -1.5197356 -1.2672651 -1.0176404 -0.90544415 -0.82332444 -0.77075648][-3.1919193 -3.0386927 -2.7191958 -2.425133 -2.3163335 -2.3181145 -2.3829453 -2.5228615 -2.4634752 -2.2160034 -1.9126949 -1.5740509 -1.2987611 -1.0986457 -1.0270514][-2.9216051 -2.973731 -2.7993813 -2.6227703 -2.6431284 -2.70488 -2.7706537 -2.8419785 -2.738224 -2.5349922 -2.334614 -2.0841088 -1.8120108 -1.5860729 -1.4890907][-2.266891 -2.4508448 -2.441546 -2.43829 -2.6041808 -2.760931 -2.8801043 -2.9472649 -2.8613255 -2.7425318 -2.652143 -2.5044875 -2.2840292 -2.0743334 -1.9599938][-2.1384795 -2.3743277 -2.4544141 -2.5526476 -2.7629058 -2.9286566 -3.0519702 -3.1156881 -3.0638 -3.0059884 -2.9689112 -2.8750954 -2.7116601 -2.548157 -2.4406223]]...]
INFO - root - 2017-12-07 05:30:50.731294: step 9310, loss = 0.84, batch loss = 0.77 (7.8 examples/sec; 1.019 sec/batch; 91h:30m:37s remains)
INFO - root - 2017-12-07 05:31:01.190494: step 9320, loss = 0.76, batch loss = 0.69 (7.8 examples/sec; 1.029 sec/batch; 92h:24m:14s remains)
INFO - root - 2017-12-07 05:31:11.301283: step 9330, loss = 0.81, batch loss = 0.74 (8.0 examples/sec; 1.005 sec/batch; 90h:13m:34s remains)
INFO - root - 2017-12-07 05:31:21.703122: step 9340, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.054 sec/batch; 94h:37m:00s remains)
INFO - root - 2017-12-07 05:31:32.023857: step 9350, loss = 0.82, batch loss = 0.75 (7.7 examples/sec; 1.036 sec/batch; 92h:58m:26s remains)
INFO - root - 2017-12-07 05:31:42.367656: step 9360, loss = 0.90, batch loss = 0.82 (7.5 examples/sec; 1.063 sec/batch; 95h:25m:54s remains)
INFO - root - 2017-12-07 05:31:52.677207: step 9370, loss = 1.19, batch loss = 1.11 (7.8 examples/sec; 1.029 sec/batch; 92h:23m:43s remains)
INFO - root - 2017-12-07 05:32:03.073367: step 9380, loss = 0.68, batch loss = 0.61 (7.4 examples/sec; 1.087 sec/batch; 97h:36m:29s remains)
INFO - root - 2017-12-07 05:32:13.407396: step 9390, loss = 0.72, batch loss = 0.65 (7.8 examples/sec; 1.019 sec/batch; 91h:29m:24s remains)
INFO - root - 2017-12-07 05:32:24.068866: step 9400, loss = 0.91, batch loss = 0.84 (7.7 examples/sec; 1.045 sec/batch; 93h:46m:11s remains)
2017-12-07 05:32:24.956286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7970378 -1.2704844 -0.98838878 -1.1784744 -1.5703621 -1.6233153 -1.3928058 -1.2740638 -1.4585183 -1.8366613 -1.8824267 -1.8901765 -2.5028567 -3.3466988 -3.747751][-1.8824909 -1.4899695 -1.4486864 -1.8419423 -2.360204 -2.4546819 -2.1695731 -2.02342 -2.2365751 -2.7089412 -2.7170963 -2.5190811 -2.9623139 -3.7074752 -3.9496334][-1.7373359 -1.5075865 -1.7082551 -2.2708824 -2.8551412 -2.9740181 -2.7072444 -2.5953016 -2.8172717 -3.272758 -3.1983829 -2.8774309 -3.3172297 -4.08364 -4.245759][-1.1181858 -0.96351957 -1.2991388 -1.9319549 -2.5882437 -2.811106 -2.6572778 -2.5787442 -2.6758752 -2.9075751 -2.6665561 -2.3594935 -2.9888878 -3.9646239 -4.2823935][-0.24046564 -0.096116543 -0.40432549 -1.0223181 -1.8020861 -2.2420759 -2.2837067 -2.25997 -2.1839139 -2.1125817 -1.7044833 -1.4674313 -2.2247283 -3.290148 -3.7585969][0.18976927 0.30239534 0.14347553 -0.439636 -1.3606844 -1.9428208 -2.0293086 -1.8583488 -1.4777098 -1.1604021 -0.88218188 -1.0531254 -2.0538156 -3.1373615 -3.6123116][0.058752537 0.22427368 0.36393929 -0.020487309 -0.786175 -1.2211468 -1.17576 -0.75427294 -0.027319908 0.46343279 0.42459774 -0.32109785 -1.6655169 -2.8567438 -3.4274776][-0.4321115 -0.15035152 0.33904934 0.2539587 -0.24145412 -0.51264334 -0.5000875 -0.095211983 0.69568253 1.2056084 0.9549737 0.042787552 -1.1474135 -2.1046481 -2.6182175][-0.9714756 -0.6204617 -0.011538506 0.081319809 -0.23989677 -0.48116279 -0.68750763 -0.6036005 -0.12666416 0.15180731 -0.20707655 -0.96142888 -1.6612248 -2.0943553 -2.3161502][-1.433847 -1.0999796 -0.51584387 -0.33138657 -0.63708711 -1.0147483 -1.4078615 -1.5270584 -1.3290963 -1.251616 -1.6242762 -2.1550944 -2.4301982 -2.4628234 -2.5017853][-2.3372147 -2.1591992 -1.6569684 -1.3961365 -1.6971898 -2.14561 -2.5240426 -2.6584244 -2.57519 -2.5193355 -2.7572966 -3.0222063 -2.9709253 -2.7384658 -2.6683416][-3.1728206 -3.0848503 -2.6866221 -2.5326116 -2.8917446 -3.3148859 -3.5803347 -3.6903906 -3.6662655 -3.5869293 -3.6544454 -3.7629731 -3.5752738 -3.2012634 -2.9857609][-3.6249256 -3.4510987 -3.154027 -3.2302623 -3.712611 -4.0880923 -4.2383118 -4.2677908 -4.2001753 -4.0296254 -3.918695 -3.9794757 -3.8462195 -3.5001266 -3.2481198][-3.681293 -3.3929262 -3.2144868 -3.5225227 -4.0724535 -4.4039555 -4.5027671 -4.4767075 -4.335887 -4.0118361 -3.6664295 -3.6390073 -3.6033103 -3.3633986 -3.1705666][-3.3790579 -3.1025732 -3.0029712 -3.3323228 -3.7860239 -4.0443869 -4.1287169 -4.1219978 -4.03062 -3.706377 -3.2558336 -3.1980071 -3.2618842 -3.124506 -2.9870944]]...]
INFO - root - 2017-12-07 05:32:35.452028: step 9410, loss = 0.81, batch loss = 0.74 (7.6 examples/sec; 1.052 sec/batch; 94h:24m:52s remains)
INFO - root - 2017-12-07 05:32:45.809782: step 9420, loss = 0.82, batch loss = 0.75 (7.6 examples/sec; 1.048 sec/batch; 94h:05m:00s remains)
INFO - root - 2017-12-07 05:32:56.068145: step 9430, loss = 0.87, batch loss = 0.80 (7.6 examples/sec; 1.047 sec/batch; 93h:59m:40s remains)
INFO - root - 2017-12-07 05:33:06.459921: step 9440, loss = 0.83, batch loss = 0.75 (8.1 examples/sec; 0.991 sec/batch; 88h:58m:31s remains)
INFO - root - 2017-12-07 05:33:16.837779: step 9450, loss = 0.90, batch loss = 0.83 (7.7 examples/sec; 1.039 sec/batch; 93h:16m:01s remains)
INFO - root - 2017-12-07 05:33:27.201505: step 9460, loss = 0.80, batch loss = 0.72 (7.6 examples/sec; 1.054 sec/batch; 94h:36m:22s remains)
INFO - root - 2017-12-07 05:33:37.460270: step 9470, loss = 0.76, batch loss = 0.69 (7.9 examples/sec; 1.015 sec/batch; 91h:03m:00s remains)
INFO - root - 2017-12-07 05:33:47.682137: step 9480, loss = 0.75, batch loss = 0.67 (7.8 examples/sec; 1.023 sec/batch; 91h:45m:34s remains)
INFO - root - 2017-12-07 05:33:58.049838: step 9490, loss = 0.67, batch loss = 0.59 (7.7 examples/sec; 1.034 sec/batch; 92h:49m:01s remains)
INFO - root - 2017-12-07 05:34:08.248274: step 9500, loss = 0.80, batch loss = 0.72 (7.9 examples/sec; 1.010 sec/batch; 90h:38m:02s remains)
2017-12-07 05:34:09.041165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2048106 -3.8952696 -3.7188714 -3.4303334 -2.9379065 -2.0400608 -1.125577 -0.52631855 -0.26605558 -0.45035124 -1.0258889 -1.7259662 -2.61154 -3.7997475 -5.10055][-3.066046 -3.881989 -3.7556195 -3.446173 -2.9331048 -1.9244518 -0.77788591 0.071886539 0.46664715 0.23930931 -0.43395829 -1.187695 -2.1868663 -3.4918175 -4.8377271][-2.8916855 -3.7608743 -3.6415286 -3.2916007 -2.8309596 -1.8667395 -0.64575672 0.43957472 1.0579686 0.87009573 0.10190868 -0.724298 -1.7111397 -2.974354 -4.309444][-3.0403252 -3.8279481 -3.6462386 -3.2743034 -2.8618422 -1.9929118 -0.78571177 0.48141003 1.3943224 1.3833656 0.58423615 -0.47583508 -1.7085738 -3.1049285 -4.4632096][-3.227664 -3.810251 -3.5280809 -3.1075878 -2.6760702 -1.941046 -0.89791179 0.36957788 1.4579024 1.6680565 1.0354576 -0.026420116 -1.4285965 -2.9798179 -4.32969][-3.2117577 -3.6301825 -3.3823404 -2.8633361 -2.2768607 -1.6334918 -0.81722236 0.28002834 1.4365201 1.8199062 1.3591223 0.40726042 -1.0470884 -2.6945765 -3.9932792][-2.9707184 -3.3910437 -3.2354226 -2.5244508 -1.6102419 -0.97705483 -0.44641185 0.36812782 1.4815898 2.1364226 1.9395051 1.1674042 -0.34551954 -2.18325 -3.5736909][-3.1780887 -3.5319378 -3.3850825 -2.4428153 -1.1634409 -0.31855488 0.13195133 0.65774059 1.5141068 2.2847748 2.35121 1.7758751 0.31327105 -1.6277728 -3.1188703][-3.7094915 -3.7777114 -3.5093606 -2.5131943 -1.1085315 0.024053574 0.56487989 0.76401424 1.1056747 1.6582718 1.8186355 1.3727932 0.087910652 -1.7249515 -3.125855][-3.7594423 -3.6196592 -3.3193164 -2.4353175 -1.1782353 -0.066018581 0.47657204 0.40389729 0.30258083 0.6489501 0.91630936 0.69467068 -0.32795238 -1.9234724 -3.1656103][-3.1969388 -3.0752068 -2.9615171 -2.361017 -1.4393156 -0.69338059 -0.34402323 -0.52646112 -0.77946496 -0.48229647 -0.1347084 -0.20110941 -0.92659545 -2.0935047 -2.9842288][-2.6485944 -2.6432753 -2.6448793 -2.1747918 -1.4665358 -1.053802 -0.96641564 -1.1919646 -1.4253702 -1.1504116 -0.88648963 -1.0391564 -1.6073987 -2.3018026 -2.7571936][-2.5016847 -2.5901248 -2.588114 -2.1639388 -1.6357927 -1.3956654 -1.4339609 -1.64311 -1.7774618 -1.501792 -1.3334167 -1.6099448 -2.0351453 -2.3570642 -2.5155232][-2.3608189 -2.4822054 -2.4851213 -2.2480114 -1.965668 -1.8375411 -1.9028728 -2.0938118 -2.1805086 -1.9457932 -1.8058684 -1.9857552 -2.1508992 -2.1694138 -2.1272678][-2.140384 -2.2399752 -2.2634618 -2.2155032 -2.131321 -2.0598221 -2.0943754 -2.2476885 -2.3140266 -2.1737328 -2.0783343 -2.1326857 -2.1236994 -2.0208182 -1.9089277]]...]
INFO - root - 2017-12-07 05:34:19.299630: step 9510, loss = 0.67, batch loss = 0.60 (8.0 examples/sec; 1.000 sec/batch; 89h:42m:50s remains)
INFO - root - 2017-12-07 05:34:29.538250: step 9520, loss = 1.00, batch loss = 0.92 (8.0 examples/sec; 0.994 sec/batch; 89h:10m:19s remains)
INFO - root - 2017-12-07 05:34:39.642447: step 9530, loss = 0.88, batch loss = 0.81 (7.6 examples/sec; 1.049 sec/batch; 94h:04m:43s remains)
INFO - root - 2017-12-07 05:34:49.758243: step 9540, loss = 0.92, batch loss = 0.85 (8.0 examples/sec; 1.004 sec/batch; 90h:01m:45s remains)
INFO - root - 2017-12-07 05:35:00.027669: step 9550, loss = 0.77, batch loss = 0.70 (7.8 examples/sec; 1.032 sec/batch; 92h:34m:48s remains)
INFO - root - 2017-12-07 05:35:10.085970: step 9560, loss = 0.93, batch loss = 0.86 (7.9 examples/sec; 1.012 sec/batch; 90h:46m:36s remains)
INFO - root - 2017-12-07 05:35:20.433283: step 9570, loss = 0.82, batch loss = 0.75 (8.0 examples/sec; 1.003 sec/batch; 90h:00m:24s remains)
INFO - root - 2017-12-07 05:35:30.772968: step 9580, loss = 0.61, batch loss = 0.54 (7.6 examples/sec; 1.059 sec/batch; 94h:59m:30s remains)
INFO - root - 2017-12-07 05:35:41.107816: step 9590, loss = 0.69, batch loss = 0.62 (8.0 examples/sec; 1.005 sec/batch; 90h:10m:34s remains)
INFO - root - 2017-12-07 05:35:51.375526: step 9600, loss = 0.82, batch loss = 0.75 (7.5 examples/sec; 1.062 sec/batch; 95h:13m:52s remains)
2017-12-07 05:35:52.261950: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0797074 -2.2397258 -2.3604949 -2.4166861 -2.5999155 -3.0047522 -3.5097985 -3.7790546 -3.8768952 -3.8615849 -3.6212621 -3.1855116 -2.7003515 -2.4111969 -2.3440397][-1.8917108 -2.2068076 -2.549685 -2.7479463 -2.9416282 -3.323844 -3.7820725 -3.9775 -3.9936223 -3.9185662 -3.687794 -3.2650287 -2.8097897 -2.556253 -2.522615][-2.9283104 -3.2273436 -3.5415869 -3.624542 -3.5938826 -3.7453485 -3.9849539 -4.0533504 -3.9966996 -3.9259269 -3.8126037 -3.5437341 -3.2432156 -3.1157808 -3.1908002][-3.9899962 -4.2351809 -4.4252453 -4.27305 -3.951221 -3.8283229 -3.8093507 -3.7318585 -3.6395686 -3.6439533 -3.7102823 -3.6376731 -3.4937563 -3.4779086 -3.6356096][-4.2109818 -4.4455328 -4.5875506 -4.2950706 -3.8423719 -3.6228738 -3.5224874 -3.4150314 -3.3118882 -3.3867056 -3.6146545 -3.7725663 -3.8416221 -3.9382844 -4.1248784][-3.4278915 -3.6917763 -3.8405814 -3.4636316 -2.9316154 -2.7758193 -2.8364396 -2.8656068 -2.8399529 -2.9551606 -3.2400506 -3.5141077 -3.7409248 -3.9445207 -4.1530094][-2.1655316 -2.4528058 -2.6556153 -2.2240386 -1.5520914 -1.3557465 -1.5516355 -1.7219548 -1.7860897 -1.901269 -2.1232777 -2.3568187 -2.6207151 -2.8725834 -3.0762782][-1.7284482 -1.8935056 -2.1122744 -1.7803209 -1.1556981 -0.88656116 -1.0561144 -1.2864196 -1.4189825 -1.4906378 -1.5696325 -1.7050683 -1.9510658 -2.2124102 -2.4048367][-2.1718063 -2.1249723 -2.2287722 -2.0921237 -1.8527377 -1.7944543 -2.0081179 -2.2765393 -2.3874531 -2.3269429 -2.2126663 -2.2336421 -2.3923688 -2.5936031 -2.7580748][-2.720531 -2.6633973 -2.791995 -2.8964882 -2.9985819 -3.0846648 -3.2550964 -3.4289975 -3.4451776 -3.3263419 -3.1537974 -3.117847 -3.1374102 -3.2115469 -3.2815895][-3.3149364 -3.4393461 -3.7044115 -3.981741 -4.225141 -4.2742143 -4.2328224 -4.1665325 -4.07831 -4.020226 -3.9694476 -4.0054669 -3.9416184 -3.8438108 -3.7012031][-3.7202709 -3.9542727 -4.2534962 -4.51542 -4.729888 -4.7239962 -4.5259213 -4.2736058 -4.1473389 -4.1757889 -4.2781954 -4.4268622 -4.3894806 -4.2148213 -3.9237392][-3.7894087 -4.0155649 -4.3053889 -4.5466189 -4.7160592 -4.6650648 -4.38705 -4.0919652 -4.0008836 -4.0863872 -4.2454529 -4.4223065 -4.378572 -4.1256442 -3.7516928][-3.5488269 -3.7163658 -3.9927 -4.2591243 -4.4372029 -4.3949475 -4.1283617 -3.8542316 -3.8069797 -3.9460301 -4.1584444 -4.3613586 -4.3306422 -4.0583968 -3.6859589][-2.882242 -2.9404278 -3.155715 -3.3983955 -3.5718923 -3.5931082 -3.4507179 -3.2925854 -3.2922225 -3.4581103 -3.6907027 -3.9146488 -3.9426157 -3.7723451 -3.5254977]]...]
INFO - root - 2017-12-07 05:36:02.666344: step 9610, loss = 0.82, batch loss = 0.75 (7.9 examples/sec; 1.014 sec/batch; 90h:55m:58s remains)
INFO - root - 2017-12-07 05:36:13.002241: step 9620, loss = 0.75, batch loss = 0.68 (7.7 examples/sec; 1.043 sec/batch; 93h:34m:35s remains)
INFO - root - 2017-12-07 05:36:23.184588: step 9630, loss = 0.70, batch loss = 0.63 (7.8 examples/sec; 1.021 sec/batch; 91h:33m:34s remains)
INFO - root - 2017-12-07 05:36:33.593311: step 9640, loss = 0.90, batch loss = 0.82 (7.8 examples/sec; 1.029 sec/batch; 92h:16m:14s remains)
INFO - root - 2017-12-07 05:36:43.960789: step 9650, loss = 0.71, batch loss = 0.63 (7.8 examples/sec; 1.032 sec/batch; 92h:32m:54s remains)
INFO - root - 2017-12-07 05:36:54.321297: step 9660, loss = 0.86, batch loss = 0.79 (7.9 examples/sec; 1.018 sec/batch; 91h:17m:48s remains)
INFO - root - 2017-12-07 05:37:04.600154: step 9670, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.074 sec/batch; 96h:20m:51s remains)
INFO - root - 2017-12-07 05:37:14.866175: step 9680, loss = 0.80, batch loss = 0.73 (7.9 examples/sec; 1.007 sec/batch; 90h:15m:29s remains)
INFO - root - 2017-12-07 05:37:25.177909: step 9690, loss = 1.05, batch loss = 0.98 (8.0 examples/sec; 0.999 sec/batch; 89h:33m:20s remains)
INFO - root - 2017-12-07 05:37:35.538154: step 9700, loss = 0.84, batch loss = 0.77 (7.7 examples/sec; 1.043 sec/batch; 93h:32m:45s remains)
2017-12-07 05:37:36.413810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3509088 -3.0878692 -3.1863503 -3.5960722 -4.0952468 -4.2813239 -3.9525304 -3.5644848 -3.4628167 -3.7095785 -4.2774205 -4.6862378 -4.6476536 -4.3232827 -3.9761021][-2.8767452 -2.5230227 -2.6148524 -3.1434903 -3.733706 -3.9117067 -3.4426892 -2.92903 -2.8548689 -3.2458944 -4.13799 -4.8483682 -4.9471188 -4.5551667 -4.1174617][-2.2505267 -1.760392 -1.8753753 -2.4609151 -2.9370975 -2.9536271 -2.2513928 -1.5345111 -1.506943 -2.2044637 -3.6000218 -4.7344341 -5.0498705 -4.6685095 -4.193697][-1.9602561 -1.1369483 -1.108578 -1.629499 -1.9502976 -1.8121984 -0.86808419 0.13660812 0.17798042 -0.8522048 -2.8053608 -4.4056654 -5.0106893 -4.7364588 -4.2539473][-2.2115808 -1.0612116 -0.67028737 -0.82851768 -0.81473684 -0.5092628 0.49628305 1.5123496 1.3342772 -0.021787643 -2.2943783 -4.1366596 -4.9426661 -4.8221626 -4.3524561][-2.9655378 -1.7088838 -0.94133806 -0.488096 0.2478056 1.0985379 2.3594174 3.2986846 2.6565223 0.79024744 -1.8144104 -3.8319187 -4.7673488 -4.7978487 -4.3853087][-4.1016293 -2.9484732 -2.027595 -1.1637106 0.12954378 1.4766488 3.117177 4.2139864 3.3134418 1.209455 -1.4672034 -3.5014997 -4.4620261 -4.6245008 -4.3199959][-4.4388309 -3.6205921 -2.8692455 -2.0716696 -0.8636744 0.37956715 1.9466906 3.1531663 2.5369411 0.73728466 -1.5681677 -3.3791375 -4.2519121 -4.4396386 -4.2275414][-3.8174448 -3.2701681 -2.7744808 -2.2812176 -1.4989543 -0.66547918 0.58247089 1.8489199 1.6999383 0.366364 -1.5592749 -3.1243541 -3.9166205 -4.1504407 -4.0702648][-3.0042148 -2.646755 -2.3901515 -2.2155774 -1.8964424 -1.5329118 -0.66120338 0.56435156 0.73547935 -0.2863636 -1.87727 -3.1173973 -3.7547903 -3.9852748 -3.9889045][-1.9929187 -1.9002137 -1.9267743 -2.035183 -2.0390041 -1.9830794 -1.3998275 -0.38640308 -0.22397614 -1.158361 -2.5109451 -3.4215479 -3.8657737 -4.0247817 -4.0181508][-1.9826355 -2.0750005 -2.2971969 -2.5140224 -2.6039677 -2.5763037 -2.0446174 -1.2383454 -1.2315664 -2.1487908 -3.2842193 -3.8665409 -4.0473304 -4.0628691 -4.0113788][-2.8999183 -3.0557361 -3.3229866 -3.5079911 -3.5426717 -3.4092026 -2.7664344 -2.0447595 -2.1831989 -3.1423104 -4.1632733 -4.5047174 -4.3739824 -4.1520705 -3.9800124][-3.2672338 -3.4318042 -3.6817112 -3.8347945 -3.9037178 -3.8506577 -3.253617 -2.6556888 -2.9274333 -3.9051995 -4.8187537 -4.9926138 -4.6147614 -4.1894326 -3.8999894][-3.4771562 -3.5883965 -3.7463744 -3.8263803 -3.9022651 -3.9547992 -3.5122514 -3.1289954 -3.5510895 -4.50713 -5.262145 -5.2824368 -4.7514896 -4.2060671 -3.844873]]...]
INFO - root - 2017-12-07 05:37:46.636797: step 9710, loss = 0.62, batch loss = 0.55 (8.0 examples/sec; 1.003 sec/batch; 89h:54m:48s remains)
INFO - root - 2017-12-07 05:37:56.995338: step 9720, loss = 0.84, batch loss = 0.76 (8.0 examples/sec; 1.003 sec/batch; 89h:57m:30s remains)
INFO - root - 2017-12-07 05:38:07.112825: step 9730, loss = 0.88, batch loss = 0.80 (7.7 examples/sec; 1.040 sec/batch; 93h:16m:56s remains)
INFO - root - 2017-12-07 05:38:17.254372: step 9740, loss = 0.79, batch loss = 0.72 (7.9 examples/sec; 1.016 sec/batch; 91h:07m:44s remains)
INFO - root - 2017-12-07 05:38:27.469963: step 9750, loss = 0.90, batch loss = 0.83 (7.9 examples/sec; 1.016 sec/batch; 91h:04m:46s remains)
INFO - root - 2017-12-07 05:38:37.761652: step 9760, loss = 0.69, batch loss = 0.61 (7.8 examples/sec; 1.025 sec/batch; 91h:51m:29s remains)
INFO - root - 2017-12-07 05:38:47.945386: step 9770, loss = 0.73, batch loss = 0.66 (7.9 examples/sec; 1.015 sec/batch; 91h:00m:49s remains)
INFO - root - 2017-12-07 05:38:58.263929: step 9780, loss = 0.78, batch loss = 0.70 (8.0 examples/sec; 1.001 sec/batch; 89h:41m:45s remains)
INFO - root - 2017-12-07 05:39:08.518772: step 9790, loss = 0.80, batch loss = 0.73 (7.9 examples/sec; 1.015 sec/batch; 91h:01m:12s remains)
INFO - root - 2017-12-07 05:39:18.802180: step 9800, loss = 0.87, batch loss = 0.80 (8.0 examples/sec; 0.999 sec/batch; 89h:33m:52s remains)
2017-12-07 05:39:19.639506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8168707 -2.8254633 -2.8646474 -2.9027665 -2.9144826 -2.8938782 -2.8313589 -2.7642951 -2.6929145 -2.6210566 -2.5586824 -2.4478841 -2.2629051 -2.0400279 -1.8018358][-3.8001711 -3.782268 -3.8246906 -3.8806446 -3.9195807 -3.9551237 -3.9544411 -3.9377058 -3.8855977 -3.8390088 -3.8284903 -3.7268789 -3.4544244 -3.0360079 -2.5342417][-3.5148587 -3.4542222 -3.4497914 -3.4667363 -3.4963913 -3.5964761 -3.6866856 -3.7598789 -3.778656 -3.8399324 -3.9896245 -3.9979122 -3.7244194 -3.2152395 -2.5794296][-2.2643392 -2.1915021 -2.1176977 -2.0264583 -1.9580858 -2.0496144 -2.1999595 -2.3699377 -2.4773431 -2.6762357 -3.0479052 -3.2826908 -3.1570678 -2.7297211 -2.1386614][-0.68498659 -0.65200377 -0.49557185 -0.2190547 0.04710722 0.058823586 -0.095632076 -0.33392763 -0.51679111 -0.87344933 -1.5291078 -2.0781374 -2.2073293 -1.996763 -1.582052][0.44329786 0.37978935 0.5953331 1.0822911 1.6244044 1.8768744 1.8850656 1.7400641 1.6069655 1.123764 0.10334349 -0.87140942 -1.3734782 -1.4979358 -1.3764811][0.73249388 0.57651472 0.80074549 1.3845811 2.0747671 2.5154991 2.7698951 2.8978038 3.0175352 2.6069326 1.403151 0.13994646 -0.68071389 -1.1553481 -1.3866763][0.27085829 0.042187691 0.18351793 0.63781118 1.1727214 1.5151706 1.8412075 2.1642237 2.5501709 2.4189034 1.4335847 0.31602526 -0.48855567 -1.0787139 -1.4963479][0.067837715 -0.22674179 -0.24220514 -0.058456421 0.14099312 0.18769741 0.3462677 0.61269045 1.0561824 1.1785903 0.57861423 -0.15636253 -0.6766963 -1.1198237 -1.4951656][0.43341017 0.13892746 0.024565697 0.0058279037 -0.065492153 -0.27852345 -0.34335041 -0.26446438 0.047262669 0.22286224 -0.16282415 -0.5830493 -0.74661756 -0.897213 -1.100147][0.8757391 0.70188808 0.64994526 0.619081 0.45537329 0.10844994 -0.14451265 -0.28774023 -0.2034893 -0.14724064 -0.53035855 -0.82278275 -0.71776605 -0.59564781 -0.6206677][0.88456726 0.87734079 0.99475861 1.1172962 1.062439 0.76534843 0.45631456 0.15590954 0.02884531 -0.071840763 -0.5217247 -0.78385377 -0.53224158 -0.26736546 -0.21878242][0.62878656 0.75542164 1.0078044 1.3362956 1.5401154 1.5074773 1.3517151 1.0761485 0.85417795 0.6359129 0.092761517 -0.22384882 0.026594639 0.27115774 0.25561571][0.057232857 0.28201437 0.6125145 1.0912423 1.5326724 1.8195519 1.9334259 1.8469033 1.6861162 1.4345918 0.7916131 0.34357595 0.49991226 0.65575886 0.56112337][-0.24244261 0.081703663 0.49959612 1.0756726 1.6392126 2.1104074 2.4250798 2.547318 2.5589824 2.3801951 1.6975937 1.0868416 1.0209489 0.97949409 0.78571606]]...]
INFO - root - 2017-12-07 05:39:29.938871: step 9810, loss = 0.97, batch loss = 0.90 (7.7 examples/sec; 1.044 sec/batch; 93h:33m:34s remains)
INFO - root - 2017-12-07 05:39:40.308365: step 9820, loss = 0.61, batch loss = 0.54 (8.0 examples/sec; 1.005 sec/batch; 90h:02m:52s remains)
INFO - root - 2017-12-07 05:39:49.987455: step 9830, loss = 0.67, batch loss = 0.59 (7.9 examples/sec; 1.009 sec/batch; 90h:28m:16s remains)
INFO - root - 2017-12-07 05:40:00.172924: step 9840, loss = 0.94, batch loss = 0.87 (8.1 examples/sec; 0.988 sec/batch; 88h:31m:06s remains)
INFO - root - 2017-12-07 05:40:10.338315: step 9850, loss = 0.55, batch loss = 0.48 (8.1 examples/sec; 0.989 sec/batch; 88h:36m:44s remains)
INFO - root - 2017-12-07 05:40:20.520679: step 9860, loss = 0.73, batch loss = 0.65 (7.8 examples/sec; 1.024 sec/batch; 91h:44m:23s remains)
INFO - root - 2017-12-07 05:40:30.693475: step 9870, loss = 1.02, batch loss = 0.94 (7.7 examples/sec; 1.035 sec/batch; 92h:44m:08s remains)
INFO - root - 2017-12-07 05:40:40.944668: step 9880, loss = 0.92, batch loss = 0.84 (7.9 examples/sec; 1.018 sec/batch; 91h:13m:17s remains)
INFO - root - 2017-12-07 05:40:51.160963: step 9890, loss = 0.72, batch loss = 0.64 (7.7 examples/sec; 1.033 sec/batch; 92h:36m:42s remains)
INFO - root - 2017-12-07 05:41:01.359674: step 9900, loss = 0.73, batch loss = 0.66 (7.9 examples/sec; 1.013 sec/batch; 90h:49m:15s remains)
2017-12-07 05:41:02.169505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0029778 -4.213604 -4.22418 -4.1613665 -4.1299634 -4.1091142 -4.1353312 -4.1977777 -4.1454649 -4.0398593 -3.9205029 -3.89766 -4.0548744 -4.1965141 -4.1796131][-4.0658989 -4.2332497 -4.2367744 -4.18301 -4.1319079 -4.0438657 -3.9748435 -3.92563 -3.7524073 -3.6204391 -3.5896645 -3.6371126 -3.8171873 -3.9792764 -4.000814][-3.8678851 -3.9355295 -3.930598 -3.9329035 -3.9795449 -3.9802251 -3.9062431 -3.7267704 -3.4105248 -3.3354788 -3.4717352 -3.5546153 -3.6733422 -3.8360443 -3.8535471][-3.6980271 -3.5948215 -3.5172865 -3.5136333 -3.5434542 -3.5471745 -3.5400145 -3.4113419 -3.1605155 -3.2123361 -3.4439759 -3.5084848 -3.5767558 -3.7417738 -3.6860235][-3.6328127 -3.4831386 -3.462913 -3.4902384 -3.4092698 -3.3212094 -3.3475072 -3.2590704 -3.0175328 -3.0606751 -3.2968187 -3.4178782 -3.593472 -3.7451711 -3.5146968][-3.6140676 -3.4984572 -3.5333815 -3.5734882 -3.4331219 -3.3417492 -3.4674945 -3.401068 -3.0746579 -2.9610529 -3.1075077 -3.3546653 -3.6345408 -3.7091341 -3.3034377][-3.7168584 -3.7265429 -3.7235327 -3.5849261 -3.2954803 -3.2020836 -3.4416857 -3.4673336 -3.1315088 -2.9294047 -2.982163 -3.2821693 -3.5305147 -3.4253616 -2.9132645][-3.6449318 -3.8256662 -3.8399255 -3.6150784 -3.2420154 -3.10508 -3.3247609 -3.404377 -3.1313529 -2.912535 -2.8415723 -3.0140643 -3.0841141 -2.8498521 -2.3989451][-3.3395491 -3.5540435 -3.589926 -3.4431882 -3.1111951 -2.9081779 -2.9810514 -3.0346253 -2.9159665 -2.812995 -2.6597276 -2.6556854 -2.6273422 -2.4535637 -2.1866922][-2.9943576 -3.1890879 -3.2291005 -3.180058 -2.9116039 -2.6393795 -2.5447388 -2.5525746 -2.6205323 -2.6283956 -2.4200163 -2.3297439 -2.3793957 -2.4273009 -2.2849324][-3.1106243 -3.2148342 -3.1435037 -3.0440631 -2.7497537 -2.3835809 -2.1143656 -2.0681033 -2.2901869 -2.3479018 -2.0657179 -1.9781148 -2.1962419 -2.4828651 -2.4504552][-3.3901296 -3.35876 -3.091188 -2.8045332 -2.4454699 -2.0589681 -1.7576935 -1.7575965 -2.056015 -2.1396515 -1.8975549 -1.9126489 -2.1930823 -2.5023694 -2.5018692][-3.3510907 -3.2306926 -2.8284903 -2.4029663 -2.0663958 -1.8488479 -1.7323093 -1.8079932 -2.0113285 -2.0747733 -1.9732482 -2.1123536 -2.3232732 -2.4388735 -2.3741164][-2.9084318 -2.7615688 -2.3762128 -2.0059369 -1.8111298 -1.813601 -1.8927844 -2.0152068 -2.0844617 -2.1238937 -2.1571622 -2.3065524 -2.3507013 -2.2636094 -2.1556354][-2.3200395 -2.1827767 -1.950345 -1.7526681 -1.6868799 -1.7914007 -1.9899411 -2.1716702 -2.2196205 -2.2688918 -2.3307223 -2.3502219 -2.2260258 -2.0552702 -1.9465389]]...]
INFO - root - 2017-12-07 05:41:12.469763: step 9910, loss = 0.91, batch loss = 0.84 (7.6 examples/sec; 1.048 sec/batch; 93h:56m:41s remains)
INFO - root - 2017-12-07 05:41:22.613797: step 9920, loss = 1.08, batch loss = 1.01 (7.9 examples/sec; 1.018 sec/batch; 91h:13m:26s remains)
INFO - root - 2017-12-07 05:41:32.740019: step 9930, loss = 0.73, batch loss = 0.66 (7.3 examples/sec; 1.089 sec/batch; 97h:35m:47s remains)
INFO - root - 2017-12-07 05:41:42.972172: step 9940, loss = 0.78, batch loss = 0.71 (8.0 examples/sec; 1.004 sec/batch; 89h:58m:32s remains)
INFO - root - 2017-12-07 05:41:53.214451: step 9950, loss = 1.02, batch loss = 0.95 (7.8 examples/sec; 1.030 sec/batch; 92h:18m:21s remains)
INFO - root - 2017-12-07 05:42:03.441752: step 9960, loss = 0.79, batch loss = 0.72 (7.6 examples/sec; 1.046 sec/batch; 93h:45m:15s remains)
INFO - root - 2017-12-07 05:42:13.570882: step 9970, loss = 1.00, batch loss = 0.92 (7.7 examples/sec; 1.040 sec/batch; 93h:12m:49s remains)
INFO - root - 2017-12-07 05:42:23.858925: step 9980, loss = 0.70, batch loss = 0.63 (7.7 examples/sec; 1.039 sec/batch; 93h:05m:28s remains)
INFO - root - 2017-12-07 05:42:34.141176: step 9990, loss = 0.71, batch loss = 0.64 (8.0 examples/sec; 0.998 sec/batch; 89h:25m:36s remains)
INFO - root - 2017-12-07 05:42:44.378100: step 10000, loss = 0.91, batch loss = 0.84 (7.8 examples/sec; 1.028 sec/batch; 92h:03m:23s remains)
2017-12-07 05:42:45.216407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4767118 -4.5335288 -4.6327496 -4.5878048 -4.4778843 -4.4281549 -4.3083696 -4.1699862 -4.222311 -4.3533916 -4.4927244 -4.4298549 -3.9721527 -3.7536972 -4.0044122][-4.1921306 -4.291647 -4.4092717 -4.3324709 -4.1676869 -4.047441 -3.8223641 -3.606467 -3.7322309 -3.991924 -4.227325 -4.2976651 -3.9705141 -3.811408 -4.0327454][-4.0815706 -4.2496247 -4.3208766 -4.1450057 -3.8965337 -3.690084 -3.3430896 -2.9837031 -3.1118844 -3.4915111 -3.8736892 -4.1765776 -4.1131659 -4.0537138 -4.2104621][-4.0780997 -4.2146797 -4.0724459 -3.6413858 -3.2600098 -2.9922583 -2.5064201 -1.9260974 -1.9435298 -2.4476507 -3.0520713 -3.6769748 -3.9479825 -4.019208 -4.1273327][-3.973443 -4.00253 -3.6191905 -3.0209076 -2.5988958 -2.2601702 -1.5365808 -0.7041018 -0.67143321 -1.386133 -2.27657 -3.2121453 -3.7937522 -3.9488471 -3.9306164][-3.3984206 -3.3269827 -2.9038625 -2.4022532 -2.0459797 -1.4901633 -0.36455011 0.69866562 0.60224771 -0.49407268 -1.7061932 -2.838275 -3.5764873 -3.8062556 -3.7268567][-2.4662595 -2.395786 -2.0376897 -1.6974142 -1.3587606 -0.489259 1.0942774 2.3673029 2.0672603 0.69085646 -0.70695615 -1.9325197 -2.7665615 -3.1436734 -3.2246246][-1.5551219 -1.5898676 -1.4392762 -1.3020203 -1.0490458 -0.15856838 1.3909388 2.5121698 2.1681466 0.93863153 -0.27973413 -1.313592 -2.0439208 -2.4793463 -2.7544861][-0.80590296 -1.0363469 -1.2077954 -1.3227923 -1.1999836 -0.52691627 0.53075695 1.2246466 1.0233593 0.28233337 -0.506212 -1.2291539 -1.8514013 -2.3391974 -2.7155089][-0.67330647 -1.0116484 -1.3920009 -1.6628187 -1.6115437 -1.1336274 -0.48414612 -0.085811138 -0.13025999 -0.42783022 -0.7791605 -1.1636696 -1.7112215 -2.2641468 -2.706126][-1.0088274 -1.4355836 -1.9121525 -2.2414372 -2.2152855 -1.8426323 -1.4168746 -1.1671002 -1.104197 -1.1065593 -1.1676548 -1.3314168 -1.7693343 -2.3579996 -2.8435967][-1.3571985 -1.8464096 -2.3653622 -2.690258 -2.6392081 -2.3039095 -2.0115857 -1.8484061 -1.7135344 -1.511796 -1.3869231 -1.4446802 -1.876452 -2.5838928 -3.2158241][-1.8098171 -2.203588 -2.6117153 -2.7782087 -2.5703354 -2.1853034 -1.9983475 -1.953326 -1.8356025 -1.5593264 -1.3074944 -1.2416229 -1.5722969 -2.334451 -3.1400146][-1.9991996 -2.4042146 -2.806159 -2.8836854 -2.4859943 -1.9093313 -1.6836216 -1.7553849 -1.7945149 -1.617187 -1.3604929 -1.1567454 -1.2671273 -1.9158387 -2.7538018][-1.8017316 -2.3133085 -2.8330011 -2.9270983 -2.5131259 -1.9150956 -1.7000175 -1.8569217 -2.0132432 -1.944864 -1.7157023 -1.3926759 -1.2812383 -1.7387893 -2.4944637]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 05:42:56.011904: step 10010, loss = 0.74, batch loss = 0.67 (8.0 examples/sec; 1.003 sec/batch; 89h:49m:28s remains)
INFO - root - 2017-12-07 05:43:06.246228: step 10020, loss = 1.14, batch loss = 1.07 (8.0 examples/sec; 1.000 sec/batch; 89h:37m:14s remains)
INFO - root - 2017-12-07 05:43:16.323058: step 10030, loss = 0.86, batch loss = 0.79 (7.8 examples/sec; 1.026 sec/batch; 91h:54m:30s remains)
INFO - root - 2017-12-07 05:43:26.755675: step 10040, loss = 0.95, batch loss = 0.87 (7.7 examples/sec; 1.037 sec/batch; 92h:54m:44s remains)
INFO - root - 2017-12-07 05:43:36.934620: step 10050, loss = 0.82, batch loss = 0.75 (7.8 examples/sec; 1.025 sec/batch; 91h:49m:12s remains)
INFO - root - 2017-12-07 05:43:47.315885: step 10060, loss = 0.89, batch loss = 0.82 (7.8 examples/sec; 1.031 sec/batch; 92h:20m:25s remains)
INFO - root - 2017-12-07 05:43:57.668027: step 10070, loss = 0.91, batch loss = 0.84 (7.8 examples/sec; 1.024 sec/batch; 91h:41m:46s remains)
INFO - root - 2017-12-07 05:44:08.081461: step 10080, loss = 0.86, batch loss = 0.79 (7.8 examples/sec; 1.025 sec/batch; 91h:49m:42s remains)
INFO - root - 2017-12-07 05:44:18.449724: step 10090, loss = 0.70, batch loss = 0.63 (7.6 examples/sec; 1.048 sec/batch; 93h:51m:27s remains)
INFO - root - 2017-12-07 05:44:27.355626: step 10100, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.774 sec/batch; 69h:20m:48s remains)
2017-12-07 05:44:28.014631: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6921895 -2.9239497 -3.1461277 -3.2045259 -3.0989645 -3.0945306 -3.1571794 -3.1641493 -3.1251216 -3.0484805 -2.9794464 -2.9675069 -3.0052052 -3.0591841 -3.052][-2.6584592 -2.8356519 -2.9869361 -3.0228431 -2.92548 -2.9518113 -3.1080303 -3.19251 -3.1914091 -3.1373334 -3.0071406 -2.8969874 -2.8850095 -2.90206 -2.91502][-2.5704119 -2.6820207 -2.7357965 -2.7018137 -2.5991049 -2.65842 -2.9401238 -3.1845675 -3.2935305 -3.2709188 -3.0397511 -2.7581549 -2.6315546 -2.6177149 -2.7326555][-2.3276091 -2.3360267 -2.2831371 -2.1642025 -2.0373216 -2.1070485 -2.5026751 -2.9531639 -3.2156675 -3.2107463 -2.8981705 -2.4962983 -2.2672613 -2.2097337 -2.3967986][-2.1543076 -2.0390041 -1.8514125 -1.571131 -1.3034017 -1.2314889 -1.6006048 -2.2351053 -2.7294424 -2.8876762 -2.7377765 -2.466953 -2.2456388 -2.0998976 -2.1835413][-2.2127612 -2.0108762 -1.6822684 -1.1538532 -0.5997684 -0.20698643 -0.34373713 -1.0244355 -1.7414105 -2.1769333 -2.3983469 -2.506613 -2.4797802 -2.3273966 -2.269161][-2.3695376 -2.2164574 -1.8605289 -1.117722 -0.26411343 0.45440674 0.62045574 0.012078762 -0.880198 -1.5837781 -2.1299994 -2.5820773 -2.7588172 -2.6742969 -2.5492623][-2.3731282 -2.4207304 -2.2705302 -1.5853219 -0.727371 -0.028543472 0.24332094 -0.15781736 -0.96621037 -1.6317961 -2.1208572 -2.625519 -2.9453704 -3.0562785 -3.053277][-2.2146714 -2.4987285 -2.6890054 -2.3224437 -1.7395356 -1.3047054 -1.0711038 -1.2308474 -1.7108138 -1.9753962 -2.0314906 -2.2700653 -2.5968928 -2.9244466 -3.1214797][-2.0536716 -2.462908 -2.9260564 -2.9316831 -2.6966243 -2.5537002 -2.441849 -2.4820542 -2.6615429 -2.5407262 -2.1587029 -1.9846373 -2.0822203 -2.3590658 -2.502048][-1.9541361 -2.3670366 -2.9452684 -3.177248 -3.1514206 -3.1911736 -3.2464218 -3.3408852 -3.4027429 -3.1186564 -2.5425773 -2.0499587 -1.8363757 -1.8980243 -1.8459883][-2.0126162 -2.3907204 -2.929781 -3.1464486 -3.1075449 -3.1870728 -3.3726082 -3.6065221 -3.6943517 -3.4187362 -2.8596139 -2.2348506 -1.7987716 -1.6642261 -1.4515865][-2.2440825 -2.6130638 -3.0572929 -3.1473904 -2.9766147 -2.9493258 -3.0947943 -3.3490443 -3.4594858 -3.2703795 -2.8755832 -2.3767552 -1.9515138 -1.7508228 -1.4993699][-2.63827 -3.0075054 -3.3638544 -3.3640931 -3.10359 -2.9252877 -2.871067 -2.9397531 -2.9763508 -2.8928926 -2.7389045 -2.5099311 -2.2756081 -2.151809 -2.002248][-3.1420586 -3.4924209 -3.7674313 -3.7400599 -3.4730732 -3.1923318 -2.9414556 -2.8074832 -2.7766371 -2.779089 -2.7699943 -2.7015016 -2.5920796 -2.5199323 -2.4632838]]...]
INFO - root - 2017-12-07 05:44:35.633585: step 10110, loss = 0.75, batch loss = 0.67 (10.5 examples/sec; 0.763 sec/batch; 68h:22m:20s remains)
INFO - root - 2017-12-07 05:44:43.227299: step 10120, loss = 0.96, batch loss = 0.89 (10.6 examples/sec; 0.757 sec/batch; 67h:49m:55s remains)
INFO - root - 2017-12-07 05:44:50.592612: step 10130, loss = 0.89, batch loss = 0.81 (10.5 examples/sec; 0.763 sec/batch; 68h:19m:59s remains)
INFO - root - 2017-12-07 05:44:58.190637: step 10140, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.768 sec/batch; 68h:46m:44s remains)
INFO - root - 2017-12-07 05:45:06.034856: step 10150, loss = 0.91, batch loss = 0.84 (9.8 examples/sec; 0.814 sec/batch; 72h:52m:58s remains)
INFO - root - 2017-12-07 05:45:13.750827: step 10160, loss = 0.62, batch loss = 0.55 (10.6 examples/sec; 0.756 sec/batch; 67h:42m:03s remains)
INFO - root - 2017-12-07 05:45:21.403179: step 10170, loss = 0.84, batch loss = 0.77 (10.7 examples/sec; 0.745 sec/batch; 66h:43m:09s remains)
INFO - root - 2017-12-07 05:45:29.020316: step 10180, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.766 sec/batch; 68h:35m:59s remains)
INFO - root - 2017-12-07 05:45:36.598627: step 10190, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 69h:02m:20s remains)
INFO - root - 2017-12-07 05:45:44.293112: step 10200, loss = 0.82, batch loss = 0.75 (10.2 examples/sec; 0.784 sec/batch; 70h:12m:36s remains)
2017-12-07 05:45:44.979153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0034237 -2.8702874 -2.9370227 -3.0082493 -3.0687337 -3.0642519 -2.9645648 -2.7994936 -2.7512949 -2.8300395 -2.9460735 -3.0473042 -3.1024947 -3.1399288 -3.0943024][-2.9351592 -2.8304896 -2.9068112 -2.9787023 -3.0760632 -3.1749427 -3.1318054 -2.9567122 -2.9116437 -3.0136213 -3.153728 -3.2463944 -3.2306275 -3.1912804 -3.0997355][-2.9046574 -2.8639719 -2.9720457 -3.0699444 -3.1911345 -3.34217 -3.3011451 -3.0692537 -2.9885716 -3.0792391 -3.2073479 -3.2673228 -3.1972384 -3.1327906 -3.0507336][-2.9230878 -2.9184277 -3.0440507 -3.17772 -3.3149979 -3.4680316 -3.3632021 -3.0189571 -2.8867488 -2.9988232 -3.1697993 -3.2477355 -3.189992 -3.1558619 -3.1048827][-2.9800124 -3.0069013 -3.1533918 -3.296895 -3.3963623 -3.4654996 -3.207521 -2.691102 -2.4782691 -2.6513662 -2.9521837 -3.153033 -3.1985343 -3.24044 -3.2214141][-3.0323575 -3.1154442 -3.3039911 -3.4234314 -3.410356 -3.299077 -2.8092518 -2.1073563 -1.8745301 -2.205451 -2.7221508 -3.0951998 -3.2482445 -3.3146977 -3.2595847][-3.1307743 -3.2333803 -3.390362 -3.3791614 -3.1582918 -2.8015571 -2.0929089 -1.3362322 -1.2858107 -1.9395266 -2.7200937 -3.2280331 -3.4095783 -3.4029732 -3.2452185][-3.348805 -3.3761806 -3.3778849 -3.1475182 -2.7084112 -2.1856143 -1.4366345 -0.84694004 -1.1017165 -2.0247366 -2.9066262 -3.3902447 -3.5328336 -3.4509199 -3.2294364][-3.6305845 -3.4552402 -3.2260289 -2.8225851 -2.3332551 -1.9037375 -1.4219372 -1.1872654 -1.6479001 -2.5596504 -3.2688518 -3.5473208 -3.5680451 -3.4179146 -3.2125487][-3.8351316 -3.462074 -3.0857663 -2.6567588 -2.2932792 -2.1436021 -2.0382748 -2.0714862 -2.4831491 -3.1248174 -3.5337 -3.5961623 -3.505784 -3.330435 -3.2168734][-3.8583717 -3.4454036 -3.0976348 -2.7702575 -2.605643 -2.7359076 -2.8856995 -2.9711111 -3.1668282 -3.4489429 -3.5689878 -3.4947488 -3.3711896 -3.2516701 -3.28168][-3.6679294 -3.37394 -3.1804352 -3.0014074 -3.0130014 -3.3123379 -3.5556467 -3.5998793 -3.612669 -3.6400592 -3.6032381 -3.4977367 -3.3783278 -3.3124633 -3.4252403][-3.3958688 -3.2734241 -3.216001 -3.1549444 -3.2801666 -3.6371164 -3.8831656 -3.8950253 -3.819535 -3.7304828 -3.6755285 -3.6214862 -3.5354671 -3.4951262 -3.6015024][-3.1735806 -3.1768556 -3.1656728 -3.1439438 -3.2954445 -3.6147237 -3.8303318 -3.8414876 -3.7354338 -3.6163015 -3.5966177 -3.6295795 -3.6141605 -3.5983295 -3.6459737][-3.0323777 -3.1044645 -3.0542326 -2.9814024 -3.0427356 -3.2195663 -3.3534193 -3.3577521 -3.2548387 -3.1840394 -3.2523108 -3.3992577 -3.4907603 -3.5099537 -3.4839449]]...]
INFO - root - 2017-12-07 05:45:52.652832: step 10210, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 70h:55m:31s remains)
INFO - root - 2017-12-07 05:46:00.283582: step 10220, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.765 sec/batch; 68h:27m:07s remains)
INFO - root - 2017-12-07 05:46:07.734142: step 10230, loss = 0.94, batch loss = 0.87 (10.5 examples/sec; 0.763 sec/batch; 68h:16m:51s remains)
INFO - root - 2017-12-07 05:46:15.357131: step 10240, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 68h:12m:37s remains)
INFO - root - 2017-12-07 05:46:23.043756: step 10250, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.774 sec/batch; 69h:19m:26s remains)
INFO - root - 2017-12-07 05:46:30.643189: step 10260, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.760 sec/batch; 68h:01m:43s remains)
INFO - root - 2017-12-07 05:46:38.370310: step 10270, loss = 0.91, batch loss = 0.83 (10.4 examples/sec; 0.773 sec/batch; 69h:10m:47s remains)
INFO - root - 2017-12-07 05:46:46.121427: step 10280, loss = 0.73, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 67h:39m:02s remains)
INFO - root - 2017-12-07 05:46:53.717949: step 10290, loss = 0.75, batch loss = 0.67 (10.9 examples/sec; 0.732 sec/batch; 65h:32m:18s remains)
INFO - root - 2017-12-07 05:47:01.354633: step 10300, loss = 0.85, batch loss = 0.77 (10.3 examples/sec; 0.775 sec/batch; 69h:23m:53s remains)
2017-12-07 05:47:01.961815: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.8933692 1.7277918 1.7645116 1.8765306 2.0009427 2.096561 2.1147518 2.075038 1.992981 1.7994647 1.2373695 0.29453325 -0.53647161 -0.81499553 -0.60744023][0.62880945 0.39191675 0.331748 0.33785582 0.35827637 0.39465618 0.40133619 0.43645096 0.55256271 0.58021688 0.16134834 -0.6372776 -1.3190408 -1.425498 -1.0460098][-0.70054841 -1.0399013 -1.21315 -1.2867472 -1.3406358 -1.3587315 -1.4276257 -1.4099371 -1.1866531 -1.0039444 -1.2636666 -1.8621278 -2.3737195 -2.3379846 -1.8020573][-1.2540548 -1.5468693 -1.7345066 -1.8476644 -1.8854744 -1.8322012 -1.8405688 -1.784292 -1.5903904 -1.5280247 -1.9130256 -2.5732386 -3.069083 -2.9751973 -2.3101268][-1.1993215 -1.3717234 -1.4983695 -1.6438084 -1.6845233 -1.603744 -1.5441 -1.4169641 -1.2402897 -1.2830369 -1.7373812 -2.48565 -3.0714293 -3.0803571 -2.4468551][-0.77056789 -0.92962027 -1.0797765 -1.2288415 -1.1766937 -1.0350549 -0.98967695 -1.0068877 -1.0574417 -1.2406595 -1.6666453 -2.3078234 -2.8368151 -2.874352 -2.3187494][-0.40217972 -0.54258752 -0.76033354 -0.93557811 -0.80157495 -0.53033996 -0.3812027 -0.43516421 -0.64564133 -0.99460936 -1.5198009 -2.1610055 -2.6956196 -2.7842832 -2.3096662][-0.93826747 -1.2439806 -1.6081572 -1.8934245 -1.8913209 -1.7192035 -1.4913621 -1.3062825 -1.2394528 -1.3792088 -1.7856481 -2.3267336 -2.8403809 -3.0247567 -2.6806765][-1.8015797 -2.2067826 -2.5416188 -2.7531967 -2.79078 -2.7825265 -2.6565683 -2.4851069 -2.3582897 -2.3160443 -2.4656925 -2.7526395 -3.1328983 -3.3495498 -3.0687437][-2.8475156 -3.1275935 -3.19352 -3.1517549 -3.0236096 -2.9603171 -2.7718608 -2.6223683 -2.5755389 -2.457973 -2.4932508 -2.703414 -3.1304779 -3.4904842 -3.3169656][-3.7773895 -4.1010561 -4.0702038 -3.9938834 -3.8542473 -3.7094264 -3.3179302 -2.9410024 -2.6797044 -2.2632775 -2.0895081 -2.2898812 -2.8739178 -3.3983979 -3.3585448][-3.8063207 -4.2784882 -4.349359 -4.4285817 -4.373806 -4.1524181 -3.6562314 -3.2162905 -2.8702424 -2.3093855 -1.9957652 -2.1360345 -2.6717603 -3.1461792 -3.1383486][-3.5453422 -4.1851883 -4.4642134 -4.7274237 -4.7626724 -4.48508 -3.9294748 -3.4685073 -3.1050811 -2.5244422 -2.1962502 -2.2886989 -2.6552467 -2.9479849 -2.9030874][-3.2531347 -3.9745302 -4.3848338 -4.7615614 -4.9336233 -4.7253232 -4.2368979 -3.801456 -3.4122133 -2.8085356 -2.4965816 -2.5462894 -2.730134 -2.8755565 -2.8243289][-3.0247841 -3.731503 -4.1299138 -4.4924593 -4.7172475 -4.5912161 -4.2172723 -3.8542285 -3.5371754 -3.0306334 -2.8125474 -2.8281527 -2.8766115 -2.9539211 -2.9287884]]...]
INFO - root - 2017-12-07 05:47:09.698087: step 10310, loss = 0.81, batch loss = 0.74 (9.8 examples/sec; 0.817 sec/batch; 73h:05m:18s remains)
INFO - root - 2017-12-07 05:47:17.582331: step 10320, loss = 0.80, batch loss = 0.73 (9.9 examples/sec; 0.811 sec/batch; 72h:32m:34s remains)
INFO - root - 2017-12-07 05:47:25.090336: step 10330, loss = 0.85, batch loss = 0.78 (10.2 examples/sec; 0.788 sec/batch; 70h:28m:54s remains)
INFO - root - 2017-12-07 05:47:32.715812: step 10340, loss = 0.92, batch loss = 0.85 (10.7 examples/sec; 0.747 sec/batch; 66h:51m:28s remains)
INFO - root - 2017-12-07 05:47:40.482362: step 10350, loss = 0.96, batch loss = 0.88 (10.5 examples/sec; 0.763 sec/batch; 68h:14m:25s remains)
INFO - root - 2017-12-07 05:47:48.187116: step 10360, loss = 0.88, batch loss = 0.80 (10.0 examples/sec; 0.797 sec/batch; 71h:20m:23s remains)
INFO - root - 2017-12-07 05:47:55.953570: step 10370, loss = 0.69, batch loss = 0.62 (10.2 examples/sec; 0.788 sec/batch; 70h:31m:06s remains)
INFO - root - 2017-12-07 05:48:03.726809: step 10380, loss = 0.81, batch loss = 0.74 (10.3 examples/sec; 0.779 sec/batch; 69h:42m:01s remains)
INFO - root - 2017-12-07 05:48:11.254036: step 10390, loss = 0.91, batch loss = 0.83 (10.8 examples/sec; 0.744 sec/batch; 66h:32m:45s remains)
INFO - root - 2017-12-07 05:48:18.909040: step 10400, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.768 sec/batch; 68h:43m:59s remains)
2017-12-07 05:48:19.546509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1560082 -3.1213484 -3.0887597 -3.0573933 -3.0639875 -3.1406534 -3.2359052 -3.3454247 -3.4413714 -3.4686108 -3.4229562 -3.3312776 -3.2439013 -3.1976848 -3.1894746][-3.1535549 -3.1190524 -3.0881824 -3.0566869 -3.0702682 -3.151438 -3.2591195 -3.3830948 -3.4807181 -3.5057154 -3.4515872 -3.3481021 -3.25282 -3.201632 -3.1908443][-3.1551547 -3.1218491 -3.0954008 -3.0729775 -3.1021967 -3.1867371 -3.3095052 -3.4560564 -3.5541363 -3.5631497 -3.4887104 -3.3657141 -3.257947 -3.1980987 -3.1823256][-3.1561356 -3.1218388 -3.0996554 -3.0806603 -3.1143143 -3.1924782 -3.3370466 -3.5223656 -3.6423914 -3.6520443 -3.5676279 -3.4300125 -3.3084705 -3.2269588 -3.1846337][-3.144485 -3.1121573 -3.0911002 -3.0510035 -3.0465484 -3.0953012 -3.2603912 -3.4923112 -3.662081 -3.7103233 -3.6572094 -3.5364747 -3.4092155 -3.2999332 -3.218842][-3.1165924 -3.1042063 -3.1069622 -3.0552456 -3.0086739 -3.040946 -3.2057936 -3.4366972 -3.6268291 -3.7093687 -3.7137856 -3.6466839 -3.5294051 -3.3929296 -3.2692199][-3.075793 -3.0689676 -3.0983181 -3.0607355 -3.0131521 -3.0413351 -3.1648645 -3.3311918 -3.5045617 -3.6408117 -3.7536497 -3.7845383 -3.6937327 -3.5222387 -3.336098][-3.1115255 -3.1016448 -3.1579878 -3.1511159 -3.1062145 -3.0942969 -3.1239328 -3.1929758 -3.3316054 -3.5188403 -3.7630115 -3.9203873 -3.8778653 -3.680114 -3.4249601][-3.2273231 -3.2468593 -3.3514488 -3.4040208 -3.3690429 -3.2753284 -3.1820798 -3.1516821 -3.2383971 -3.4491527 -3.7997947 -4.071444 -4.0856509 -3.8804283 -3.562324][-3.3381751 -3.4037104 -3.5589375 -3.6876106 -3.6841111 -3.5257628 -3.3278251 -3.2341285 -3.2838867 -3.4916494 -3.8887973 -4.2213755 -4.2626057 -4.0591478 -3.710161][-3.4283147 -3.5316339 -3.7081661 -3.8807268 -3.9077663 -3.7343607 -3.4808116 -3.339412 -3.3402627 -3.5175533 -3.9216626 -4.2690969 -4.3164067 -4.1140208 -3.7655478][-3.4699271 -3.567781 -3.72089 -3.8952556 -3.9554739 -3.8166335 -3.5691328 -3.4137502 -3.3705125 -3.4994333 -3.8638592 -4.1855927 -4.2321739 -4.0380993 -3.7164221][-3.4557509 -3.5076199 -3.5942073 -3.72077 -3.7934623 -3.7199759 -3.5398064 -3.4375818 -3.4048514 -3.4888592 -3.7756526 -4.03016 -4.0609665 -3.8891735 -3.6184039][-3.4284539 -3.4460618 -3.4781375 -3.5565622 -3.6249774 -3.6016576 -3.4972415 -3.4533513 -3.4427605 -3.4918694 -3.6862657 -3.8586683 -3.8698163 -3.7269297 -3.5143516][-3.3910191 -3.4033494 -3.4177141 -3.4707346 -3.5241151 -3.5239534 -3.4718409 -3.4574268 -3.458056 -3.4803963 -3.5937157 -3.6948373 -3.6869156 -3.5740805 -3.4207554]]...]
INFO - root - 2017-12-07 05:48:27.227844: step 10410, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.753 sec/batch; 67h:23m:41s remains)
INFO - root - 2017-12-07 05:48:34.909778: step 10420, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.770 sec/batch; 68h:55m:07s remains)
INFO - root - 2017-12-07 05:48:42.182182: step 10430, loss = 0.85, batch loss = 0.78 (11.2 examples/sec; 0.713 sec/batch; 63h:46m:43s remains)
INFO - root - 2017-12-07 05:48:49.781646: step 10440, loss = 0.92, batch loss = 0.85 (10.8 examples/sec; 0.738 sec/batch; 65h:58m:53s remains)
INFO - root - 2017-12-07 05:48:57.453069: step 10450, loss = 1.01, batch loss = 0.94 (10.5 examples/sec; 0.764 sec/batch; 68h:21m:49s remains)
INFO - root - 2017-12-07 05:49:05.102420: step 10460, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 67h:41m:31s remains)
INFO - root - 2017-12-07 05:49:12.793351: step 10470, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.762 sec/batch; 68h:07m:36s remains)
INFO - root - 2017-12-07 05:49:20.496805: step 10480, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 69h:40m:31s remains)
INFO - root - 2017-12-07 05:49:28.216704: step 10490, loss = 0.76, batch loss = 0.68 (10.2 examples/sec; 0.781 sec/batch; 69h:53m:33s remains)
INFO - root - 2017-12-07 05:49:35.997981: step 10500, loss = 0.77, batch loss = 0.70 (10.1 examples/sec; 0.794 sec/batch; 71h:02m:18s remains)
2017-12-07 05:49:36.720367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1530895 -3.2511821 -3.2875719 -3.2087383 -3.0525689 -2.8403182 -2.7478833 -2.7498064 -2.7554774 -3.0002398 -3.4030704 -3.6405289 -3.4562902 -2.6274137 -1.4231105][-3.2103965 -3.363224 -3.4944479 -3.4679649 -3.2919381 -3.0390956 -3.0119658 -3.1275635 -3.1942003 -3.4382312 -3.7497025 -3.7855129 -3.3795383 -2.4011195 -1.1849544][-3.1239362 -3.4453211 -3.7247143 -3.7398226 -3.4914854 -3.1926818 -3.2295299 -3.3981636 -3.45518 -3.6276665 -3.8081405 -3.6577537 -3.1162384 -2.1563251 -1.1039662][-3.0647721 -3.5912232 -4.041368 -4.1052895 -3.7500443 -3.3799603 -3.3833485 -3.4435928 -3.3652716 -3.4329591 -3.5082669 -3.3002095 -2.7922673 -2.0018063 -1.2387474][-2.9145555 -3.5778174 -4.1351476 -4.2206621 -3.7594292 -3.3216136 -3.2386837 -3.1116457 -2.9041324 -2.9347837 -2.9802082 -2.8280919 -2.4794426 -1.9444933 -1.5026941][-2.8655267 -3.6161888 -4.2207379 -4.2786565 -3.6892204 -3.1222177 -2.8283157 -2.4269419 -2.1335638 -2.2267885 -2.3606682 -2.3607409 -2.2441463 -1.9866397 -1.8245802][-2.7996833 -3.516362 -4.0998693 -4.174643 -3.5728149 -2.881227 -2.2724845 -1.5488617 -1.2399924 -1.523632 -1.8617771 -2.0739822 -2.2012262 -2.1976557 -2.2355602][-2.6888461 -3.1773126 -3.6235271 -3.7147026 -3.2138016 -2.4726741 -1.5964227 -0.65056968 -0.45285463 -1.0622532 -1.6789465 -2.0872216 -2.3738821 -2.5138094 -2.6468282][-2.9695349 -3.1390629 -3.3073163 -3.2837958 -2.8383002 -2.0606496 -1.0303924 -0.025989056 -0.049749374 -0.99256897 -1.8107865 -2.2593367 -2.5387812 -2.7428441 -2.9443388][-3.2722359 -3.2609885 -3.2152848 -3.0726976 -2.6692114 -1.9438112 -0.97789907 -0.16470051 -0.44208956 -1.4702389 -2.1541157 -2.3769999 -2.5170808 -2.7870731 -3.1356516][-3.2176681 -3.1209173 -2.9598925 -2.7782335 -2.4732547 -1.9220619 -1.20525 -0.731236 -1.18893 -2.1344426 -2.5452342 -2.4396009 -2.3382807 -2.585644 -3.068027][-3.0924006 -2.8849876 -2.653883 -2.5024066 -2.3364613 -1.9879773 -1.5230525 -1.3126273 -1.8034978 -2.6143541 -2.8347678 -2.5524769 -2.3163254 -2.5067108 -3.0103827][-3.0662336 -2.7861047 -2.5285683 -2.4359431 -2.420233 -2.2791908 -2.0613959 -1.9866965 -2.3366351 -2.8770285 -2.9125023 -2.5990214 -2.4372952 -2.6957958 -3.216424][-3.0808668 -2.7455764 -2.4219937 -2.3172805 -2.3919039 -2.4842377 -2.5677514 -2.6056416 -2.7453525 -2.9590402 -2.8171036 -2.5241246 -2.4792209 -2.7827568 -3.2790136][-3.0892696 -2.7783923 -2.4481964 -2.3310862 -2.4303503 -2.6425655 -2.9181767 -3.0333686 -3.0264978 -3.0378878 -2.8288193 -2.5602946 -2.5312433 -2.7566729 -3.1014037]]...]
INFO - root - 2017-12-07 05:49:44.443212: step 10510, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.789 sec/batch; 70h:31m:42s remains)
INFO - root - 2017-12-07 05:49:52.165541: step 10520, loss = 0.63, batch loss = 0.55 (10.5 examples/sec; 0.760 sec/batch; 67h:57m:12s remains)
INFO - root - 2017-12-07 05:49:59.700356: step 10530, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.772 sec/batch; 69h:03m:00s remains)
INFO - root - 2017-12-07 05:50:07.311704: step 10540, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.772 sec/batch; 69h:02m:30s remains)
INFO - root - 2017-12-07 05:50:14.954705: step 10550, loss = 0.74, batch loss = 0.66 (10.6 examples/sec; 0.753 sec/batch; 67h:19m:35s remains)
INFO - root - 2017-12-07 05:50:22.499454: step 10560, loss = 0.92, batch loss = 0.84 (10.6 examples/sec; 0.754 sec/batch; 67h:24m:46s remains)
INFO - root - 2017-12-07 05:50:30.092465: step 10570, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.761 sec/batch; 68h:01m:32s remains)
INFO - root - 2017-12-07 05:50:37.726258: step 10580, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.753 sec/batch; 67h:22m:28s remains)
INFO - root - 2017-12-07 05:50:45.438241: step 10590, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 70h:56m:52s remains)
INFO - root - 2017-12-07 05:50:53.163817: step 10600, loss = 0.97, batch loss = 0.90 (10.2 examples/sec; 0.786 sec/batch; 70h:18m:02s remains)
2017-12-07 05:50:53.873800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7624116 -2.0121613 -1.2979009 -0.96826243 -1.3916926 -1.9350243 -2.140367 -2.1317103 -2.009624 -1.6864581 -1.1924763 -0.69847226 -0.5575664 -1.0319896 -1.6221931][-2.5656409 -1.8666005 -1.2929378 -1.1851859 -1.6586776 -2.0064476 -2.0220094 -1.9319124 -1.7295706 -1.3594792 -0.99963117 -0.73542261 -0.82386351 -1.4412863 -1.9837945][-2.6583576 -2.0980749 -1.7638795 -1.8332431 -2.1795061 -2.2128026 -2.018518 -1.8402264 -1.5685916 -1.2426453 -1.1279993 -1.1701741 -1.42999 -1.9675734 -2.3289742][-2.9938502 -2.5659268 -2.4327745 -2.5719752 -2.7213287 -2.5352991 -2.2760458 -2.102031 -1.8531971 -1.6365418 -1.6827054 -1.8314612 -1.9908757 -2.2082224 -2.3316321][-3.3587675 -2.9845266 -2.9623556 -3.1171579 -3.1107972 -2.8050096 -2.5495558 -2.4424524 -2.354182 -2.3634214 -2.4653707 -2.46379 -2.3469884 -2.2150021 -2.1963227][-3.6022453 -3.228327 -3.2447004 -3.3792758 -3.26734 -2.9445407 -2.7388659 -2.6930926 -2.7960687 -2.9816003 -2.981657 -2.6932287 -2.3154762 -2.0303226 -2.1100323][-3.7186961 -3.3228645 -3.3105092 -3.3448606 -3.1696439 -2.9134781 -2.796638 -2.798852 -3.0527573 -3.2644842 -2.9933593 -2.4431071 -1.9762731 -1.7817397 -2.1727519][-3.6161523 -3.1238596 -2.9942136 -2.8949547 -2.6989808 -2.5169647 -2.4224274 -2.4470725 -2.8404031 -2.9742684 -2.4063852 -1.7789159 -1.5013971 -1.6407275 -2.4326112][-3.292809 -2.5980139 -2.2915194 -2.0795071 -1.8941979 -1.7371862 -1.606889 -1.677609 -2.169467 -2.1802795 -1.4503293 -0.98552442 -1.1367614 -1.7151272 -2.7917156][-2.9167943 -2.0222468 -1.5735145 -1.3191316 -1.1711228 -1.0384724 -0.93672204 -1.109596 -1.6104414 -1.5279126 -0.83541107 -0.66221476 -1.1818309 -1.9688871 -3.0125926][-2.6622057 -1.707597 -1.2598376 -1.0629609 -1.0268879 -1.0670435 -1.1309676 -1.3677747 -1.7365754 -1.5610969 -1.0473807 -1.1409986 -1.7627351 -2.4370642 -3.1673889][-2.5824552 -1.7033989 -1.386323 -1.3212526 -1.4188156 -1.6605413 -1.8913891 -2.1073773 -2.2778213 -2.0490949 -1.7387149 -1.9700651 -2.518544 -2.951479 -3.302002][-2.5811861 -1.8008332 -1.6483212 -1.7801051 -2.0093787 -2.3307953 -2.5604143 -2.6536312 -2.6304162 -2.4133315 -2.3111033 -2.6188045 -3.0464621 -3.2673254 -3.3125973][-2.6637125 -1.9566455 -1.914639 -2.2346854 -2.5689225 -2.8563962 -3.0010114 -2.9799223 -2.8615136 -2.7112646 -2.7435665 -3.0366061 -3.3101034 -3.361187 -3.2116036][-2.7672443 -2.1282732 -2.142427 -2.580255 -2.9492011 -3.1393697 -3.2038069 -3.1559982 -3.0314267 -2.9448385 -3.0327196 -3.2678585 -3.41051 -3.3224096 -3.0619993]]...]
INFO - root - 2017-12-07 05:51:01.531759: step 10610, loss = 0.82, batch loss = 0.74 (10.6 examples/sec; 0.756 sec/batch; 67h:37m:34s remains)
INFO - root - 2017-12-07 05:51:09.143113: step 10620, loss = 0.72, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 72h:31m:22s remains)
INFO - root - 2017-12-07 05:51:16.569254: step 10630, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.764 sec/batch; 68h:17m:03s remains)
INFO - root - 2017-12-07 05:51:24.324254: step 10640, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.784 sec/batch; 70h:05m:34s remains)
INFO - root - 2017-12-07 05:51:31.942440: step 10650, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.771 sec/batch; 68h:53m:29s remains)
INFO - root - 2017-12-07 05:51:39.635261: step 10660, loss = 0.84, batch loss = 0.76 (10.5 examples/sec; 0.764 sec/batch; 68h:18m:06s remains)
INFO - root - 2017-12-07 05:51:47.299645: step 10670, loss = 0.67, batch loss = 0.60 (10.7 examples/sec; 0.750 sec/batch; 67h:04m:17s remains)
INFO - root - 2017-12-07 05:51:54.929203: step 10680, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.759 sec/batch; 67h:49m:49s remains)
INFO - root - 2017-12-07 05:52:02.677916: step 10690, loss = 0.65, batch loss = 0.57 (10.4 examples/sec; 0.770 sec/batch; 68h:47m:31s remains)
INFO - root - 2017-12-07 05:52:10.536732: step 10700, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.760 sec/batch; 67h:54m:50s remains)
2017-12-07 05:52:11.134293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.086624146 -0.62935877 -1.2482455 -1.9055104 -2.5012376 -2.952898 -3.2797096 -3.5156941 -3.7016857 -3.8199801 -3.856344 -3.8383811 -3.7775626 -3.6702552 -3.4139438][-1.0716419 -1.5382154 -1.9781981 -2.4033496 -2.766305 -3.0398607 -3.262136 -3.4547634 -3.6204233 -3.7245374 -3.7548292 -3.7283602 -3.6667061 -3.615953 -3.4908223][-1.7087297 -2.0385025 -2.2932696 -2.5279522 -2.7331762 -2.9069839 -3.0813909 -3.2585874 -3.4237223 -3.5348856 -3.5775526 -3.5591822 -3.5042477 -3.4916615 -3.469872][-1.9376729 -2.1289167 -2.2399645 -2.3495364 -2.4600635 -2.5748205 -2.7358913 -2.9272945 -3.127867 -3.2902408 -3.3918123 -3.4263391 -3.4084249 -3.4226246 -3.449523][-2.0396051 -2.1285648 -2.1317313 -2.1305892 -2.1411228 -2.1956096 -2.3495686 -2.5749435 -2.835638 -3.0733411 -3.25388 -3.3598542 -3.3930583 -3.4265876 -3.4658391][-2.0455155 -2.0793591 -2.0274014 -1.9744604 -1.9400105 -1.9784379 -2.1463008 -2.4112456 -2.7161474 -2.996984 -3.2214193 -3.3721476 -3.4464028 -3.4940042 -3.5356765][-2.0470984 -2.066853 -2.0161066 -1.9730973 -1.9567585 -2.0216208 -2.2121263 -2.4891672 -2.7874265 -3.0497284 -3.2588091 -3.4100707 -3.5023012 -3.5591083 -3.6030414][-2.0406053 -2.0785606 -2.065239 -2.0661771 -2.0929153 -2.1861784 -2.3794041 -2.6352332 -2.8923426 -3.1061823 -3.2757301 -3.4118483 -3.5135639 -3.5823636 -3.6354642][-2.0104706 -2.0726776 -2.0963144 -2.1383343 -2.202009 -2.3159769 -2.5022984 -2.7303996 -2.9485564 -3.1187482 -3.2519031 -3.3715205 -3.4759109 -3.5543189 -3.6213067][-2.0275061 -2.114996 -2.1723914 -2.2514613 -2.3453496 -2.4741549 -2.650584 -2.8539505 -3.0402191 -3.1696029 -3.2582269 -3.3422418 -3.4274745 -3.4994922 -3.5680017][-2.0352721 -2.1631331 -2.2667496 -2.3859787 -2.5077939 -2.6420922 -2.8032384 -2.9841981 -3.15064 -3.2575548 -3.3145034 -3.3627963 -3.4180813 -3.4715071 -3.5279183][-1.9087896 -2.0740144 -2.2166779 -2.3698661 -2.51703 -2.6591744 -2.814719 -2.9840262 -3.1444192 -3.255228 -3.3162827 -3.35888 -3.3973768 -3.4287746 -3.4601238][-1.5977595 -1.7921209 -1.9646158 -2.1481533 -2.3277528 -2.49526 -2.6678331 -2.8419042 -3.0049057 -3.1296868 -3.2122588 -3.2722759 -3.3162661 -3.3382497 -3.3437815][-1.1485147 -1.3598447 -1.5500314 -1.751945 -1.9549983 -2.1438444 -2.3355615 -2.5173509 -2.685266 -2.8278487 -2.9346817 -3.0157816 -3.0766568 -3.1094389 -3.1122208][-0.631402 -0.85188246 -1.0512145 -1.2592609 -1.4719484 -1.6714387 -1.8760631 -2.0611908 -2.2266014 -2.3766072 -2.4934688 -2.58036 -2.6459551 -2.6885846 -2.6987786]]...]
INFO - root - 2017-12-07 05:52:18.908130: step 10710, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.764 sec/batch; 68h:15m:45s remains)
INFO - root - 2017-12-07 05:52:26.644407: step 10720, loss = 0.71, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 69h:16m:13s remains)
INFO - root - 2017-12-07 05:52:34.078809: step 10730, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.768 sec/batch; 68h:37m:46s remains)
INFO - root - 2017-12-07 05:52:41.777220: step 10740, loss = 0.87, batch loss = 0.79 (10.6 examples/sec; 0.756 sec/batch; 67h:36m:21s remains)
INFO - root - 2017-12-07 05:52:49.418979: step 10750, loss = 0.69, batch loss = 0.62 (10.7 examples/sec; 0.748 sec/batch; 66h:49m:32s remains)
INFO - root - 2017-12-07 05:52:57.208521: step 10760, loss = 0.83, batch loss = 0.76 (9.8 examples/sec; 0.813 sec/batch; 72h:42m:12s remains)
INFO - root - 2017-12-07 05:53:04.819734: step 10770, loss = 0.67, batch loss = 0.60 (10.6 examples/sec; 0.751 sec/batch; 67h:08m:11s remains)
INFO - root - 2017-12-07 05:53:12.515248: step 10780, loss = 0.78, batch loss = 0.70 (10.4 examples/sec; 0.773 sec/batch; 69h:02m:59s remains)
INFO - root - 2017-12-07 05:53:20.157479: step 10790, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.780 sec/batch; 69h:43m:20s remains)
INFO - root - 2017-12-07 05:53:28.010474: step 10800, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.779 sec/batch; 69h:37m:01s remains)
2017-12-07 05:53:28.635441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3916311 -2.1439056 -1.9203222 -1.7409837 -1.4763608 -1.3086174 -1.0995631 -0.79467511 -0.68973255 -0.55339813 -0.28959322 -0.18735933 -0.22811842 -0.16350842 -0.093860149][-1.823154 -1.5623114 -1.2876539 -1.0856509 -0.87106752 -0.8110919 -0.74324179 -0.5821557 -0.65905952 -0.65745926 -0.49432731 -0.49268627 -0.51860023 -0.37888622 -0.21691132][-1.2047405 -1.0068381 -0.75976682 -0.66587615 -0.65145826 -0.832144 -1.0121725 -1.0660207 -1.3513336 -1.5176237 -1.4530828 -1.4862278 -1.3792927 -1.0480971 -0.75063443][-0.82054114 -0.65282106 -0.43721414 -0.47072768 -0.64607716 -1.0238907 -1.416364 -1.670212 -2.151222 -2.4679828 -2.4596019 -2.4785945 -2.2727473 -1.8514652 -1.5652351][-0.75655365 -0.56725097 -0.31591845 -0.36988974 -0.56144309 -0.94587874 -1.3668058 -1.6133029 -2.13894 -2.5570474 -2.592618 -2.6977649 -2.6274214 -2.3683708 -2.199271][-1.0580029 -0.89773989 -0.61390972 -0.55389571 -0.48301244 -0.49510074 -0.5051477 -0.36317921 -0.70562696 -1.1908283 -1.379281 -1.7586472 -2.0385363 -2.1141951 -2.2156374][-1.5190265 -1.3214602 -0.93121862 -0.60113883 -0.0596838 0.49969435 1.0816407 1.7463622 1.5702658 0.82952023 0.26334667 -0.50070882 -1.1794178 -1.67138 -2.11367][-1.6817873 -1.3256207 -0.795465 -0.27621412 0.51700068 1.3156328 2.0700283 2.8726768 2.6899738 1.7745976 1.0369725 0.16301346 -0.6767385 -1.454762 -2.2306271][-1.5118103 -1.065243 -0.56901431 -0.11824512 0.53266382 1.135242 1.5615044 2.0264807 1.899045 1.2877617 0.79632521 0.12873793 -0.61355877 -1.4015017 -2.268708][-1.2202072 -0.79653406 -0.49062514 -0.31819248 -0.092370987 0.068818092 0.025645256 0.14009953 0.12126589 -0.0783782 -0.172194 -0.48413849 -0.93900681 -1.4746828 -2.1136951][-0.74810576 -0.51713133 -0.52937293 -0.71407676 -0.91473985 -1.1154728 -1.4143913 -1.4901109 -1.4784439 -1.4959342 -1.3436096 -1.3287995 -1.4322586 -1.5971324 -1.8713913][-0.28521395 -0.35851622 -0.73925519 -1.2339909 -1.6476843 -1.9433243 -2.2376437 -2.3449123 -2.382817 -2.4125252 -2.2618225 -2.0948334 -1.9644194 -1.8916936 -1.9615145][-0.037600517 -0.53271866 -1.2074788 -1.7645779 -2.0760553 -2.2615941 -2.4756968 -2.6197891 -2.7416224 -2.8327284 -2.7871683 -2.6963773 -2.5967183 -2.5464983 -2.610497][-0.26868343 -0.99549603 -1.6213748 -1.9019628 -1.9300618 -2.0307772 -2.2747629 -2.5421576 -2.7626696 -2.897881 -2.9440756 -2.9788623 -3.0297134 -3.1319175 -3.2874179][-0.79785633 -1.3101602 -1.6308651 -1.627301 -1.5489249 -1.7402322 -2.1152859 -2.5135174 -2.8240495 -2.9666924 -3.0195551 -3.0851769 -3.1761329 -3.3147159 -3.4962511]]...]
INFO - root - 2017-12-07 05:53:36.240182: step 10810, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.783 sec/batch; 69h:56m:34s remains)
INFO - root - 2017-12-07 05:53:44.023492: step 10820, loss = 0.92, batch loss = 0.84 (10.2 examples/sec; 0.787 sec/batch; 70h:17m:11s remains)
INFO - root - 2017-12-07 05:53:51.498475: step 10830, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.760 sec/batch; 67h:55m:25s remains)
INFO - root - 2017-12-07 05:53:59.214849: step 10840, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.787 sec/batch; 70h:16m:43s remains)
INFO - root - 2017-12-07 05:54:06.982323: step 10850, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.763 sec/batch; 68h:12m:00s remains)
INFO - root - 2017-12-07 05:54:14.836868: step 10860, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.758 sec/batch; 67h:44m:07s remains)
INFO - root - 2017-12-07 05:54:22.559530: step 10870, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.773 sec/batch; 69h:05m:36s remains)
INFO - root - 2017-12-07 05:54:30.256511: step 10880, loss = 0.82, batch loss = 0.75 (10.9 examples/sec; 0.735 sec/batch; 65h:40m:22s remains)
INFO - root - 2017-12-07 05:54:37.908662: step 10890, loss = 1.00, batch loss = 0.93 (10.7 examples/sec; 0.749 sec/batch; 66h:54m:42s remains)
INFO - root - 2017-12-07 05:54:45.565437: step 10900, loss = 0.88, batch loss = 0.80 (10.5 examples/sec; 0.764 sec/batch; 68h:14m:34s remains)
2017-12-07 05:54:46.303124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5175357 -3.0425749 -2.385896 -2.2679806 -2.6650929 -2.8887339 -3.1305037 -3.4974537 -3.6526921 -3.6708262 -3.4708393 -2.93045 -2.3132913 -1.6142058 -1.3684807][-3.016252 -2.605607 -1.9382694 -1.811348 -2.1355579 -2.2045305 -2.4272907 -2.8871756 -3.046068 -3.1269045 -3.0560982 -2.5889409 -2.0593233 -1.4594879 -1.2840104][-2.3679357 -2.1332669 -1.6576476 -1.6462395 -1.9464757 -1.9626336 -2.1920664 -2.549293 -2.461277 -2.3998938 -2.4197176 -2.1346536 -1.7604742 -1.3071492 -1.272953][-1.8837776 -1.8338668 -1.6602037 -1.785562 -2.1023972 -2.2287281 -2.5728106 -2.8554821 -2.5921066 -2.3516078 -2.27889 -1.9839294 -1.6165063 -1.2644279 -1.3928659][-1.6236742 -1.75595 -1.8075199 -1.9639866 -2.2396839 -2.4660227 -2.8216848 -3.0318956 -2.7302649 -2.3929398 -2.2334232 -1.9259927 -1.5537581 -1.3055973 -1.5841267][-1.5792639 -1.8180647 -1.9494109 -2.055475 -2.2352557 -2.4665351 -2.6866877 -2.7707 -2.3791325 -1.835011 -1.5591187 -1.3951807 -1.2525334 -1.2638714 -1.7472503][-1.652262 -1.8433421 -1.8802474 -1.9013655 -2.0333664 -2.2664616 -2.3127587 -2.2314661 -1.7180822 -0.91208267 -0.47977161 -0.50376344 -0.66881704 -0.95225167 -1.6711662][-1.6917224 -1.8423426 -1.7332578 -1.6721318 -1.8032155 -2.0421629 -1.9472611 -1.7321041 -1.1534393 -0.21177769 0.26591873 0.060166836 -0.31772995 -0.69092107 -1.5042591][-1.6434896 -1.9891808 -1.9492726 -1.8914025 -2.0090399 -2.1815677 -1.9470906 -1.6020155 -0.96496797 -0.025061607 0.37586308 -0.0051136017 -0.50828195 -0.84627485 -1.5765035][-1.7070048 -2.3488984 -2.4368238 -2.3781967 -2.4602613 -2.5507722 -2.2623591 -1.9023075 -1.313447 -0.49434924 -0.25259733 -0.7419045 -1.2244766 -1.4209816 -1.9548485][-1.8950622 -2.7444992 -2.897074 -2.7696517 -2.7625852 -2.7660584 -2.5252085 -2.280653 -1.841234 -1.2392187 -1.2286043 -1.7918003 -2.1968653 -2.192775 -2.387984][-2.005043 -3.0002806 -3.259825 -3.0569425 -2.9285297 -2.8683977 -2.7253847 -2.6149278 -2.2470508 -1.7485321 -1.9147236 -2.5324364 -2.8528333 -2.6536064 -2.5325663][-1.8105056 -2.8496268 -3.2877135 -3.1110659 -2.9426546 -2.9081273 -2.9144495 -2.9045236 -2.5092514 -1.9848742 -2.1570315 -2.6962795 -2.8666489 -2.4564028 -2.156801][-1.7749698 -2.7531657 -3.2603221 -3.056798 -2.8544598 -2.8634841 -3.0023067 -3.0696239 -2.6604834 -2.1130681 -2.2179642 -2.6197324 -2.6054907 -1.9671125 -1.5699649][-2.2035391 -3.1047099 -3.6007862 -3.3690767 -3.1445103 -3.1323409 -3.2365422 -3.2551069 -2.8547711 -2.349942 -2.3701994 -2.63183 -2.4677558 -1.6793194 -1.2425752]]...]
INFO - root - 2017-12-07 05:54:53.903863: step 10910, loss = 0.82, batch loss = 0.74 (10.2 examples/sec; 0.783 sec/batch; 69h:54m:55s remains)
INFO - root - 2017-12-07 05:55:01.454368: step 10920, loss = 0.71, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 70h:39m:14s remains)
INFO - root - 2017-12-07 05:55:08.893628: step 10930, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.759 sec/batch; 67h:46m:27s remains)
INFO - root - 2017-12-07 05:55:16.678553: step 10940, loss = 0.83, batch loss = 0.76 (10.1 examples/sec; 0.792 sec/batch; 70h:46m:57s remains)
INFO - root - 2017-12-07 05:55:24.432715: step 10950, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.768 sec/batch; 68h:37m:27s remains)
INFO - root - 2017-12-07 05:55:32.110989: step 10960, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.785 sec/batch; 70h:05m:12s remains)
INFO - root - 2017-12-07 05:55:39.764046: step 10970, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 68h:15m:33s remains)
INFO - root - 2017-12-07 05:55:47.453413: step 10980, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.769 sec/batch; 68h:43m:06s remains)
INFO - root - 2017-12-07 05:55:55.255823: step 10990, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.768 sec/batch; 68h:36m:02s remains)
INFO - root - 2017-12-07 05:56:02.863298: step 11000, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.757 sec/batch; 67h:35m:54s remains)
2017-12-07 05:56:03.479202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6771235 -2.572597 -2.5298948 -2.5196667 -2.6301038 -2.8556056 -2.9924481 -3.0193241 -2.9993711 -2.8944616 -2.7954392 -2.7364304 -2.6949615 -2.6736326 -2.7070045][-2.43607 -2.1482902 -1.9111547 -1.8340592 -2.0567498 -2.5141511 -2.9057746 -3.1440873 -3.1663413 -2.9992054 -2.8338084 -2.7132447 -2.6728325 -2.665379 -2.6333036][-1.7959499 -1.3230116 -0.88988066 -0.77297926 -1.1050603 -1.7490864 -2.4194102 -2.9360764 -3.0644946 -2.9047232 -2.7215757 -2.5350127 -2.4812739 -2.5279555 -2.4557276][-1.3289087 -0.85289264 -0.20504332 0.12630177 -0.010632992 -0.52575254 -1.2757642 -1.976011 -2.2290921 -2.2193689 -2.1941106 -2.0796883 -2.0764241 -2.1451452 -2.0011952][-1.4107625 -1.263422 -0.601748 0.031315327 0.49039459 0.61250687 0.2212286 -0.41047573 -0.80227733 -1.0720441 -1.3338728 -1.4328251 -1.5964279 -1.7300618 -1.6205602][-1.9156034 -2.0339606 -1.3811133 -0.50237441 0.50209904 1.2793407 1.3867202 0.94122648 0.39348793 -0.10597467 -0.53324461 -0.76545668 -1.0277753 -1.213953 -1.3181541][-1.9710388 -2.2251244 -1.7999485 -0.96804619 0.24782991 1.3099141 1.7089105 1.3795443 0.65814114 0.010014534 -0.43702221 -0.67238 -0.86164474 -0.95146155 -1.2734179][-1.6827688 -2.0928385 -2.1450198 -1.6798983 -0.592937 0.53525162 1.2296276 1.2033067 0.57308674 -0.1215148 -0.67492986 -1.063971 -1.1887746 -1.050426 -1.290185][-1.9318736 -2.3640647 -2.7269552 -2.5135181 -1.6579957 -0.68729806 0.076942444 0.25165796 -0.22174835 -0.83391666 -1.4041643 -1.9310482 -2.072674 -1.8057556 -1.7543666][-2.4558749 -2.7074871 -3.0312965 -2.9262364 -2.4124026 -1.7834747 -1.2243185 -1.099906 -1.4780881 -1.8848479 -2.2454188 -2.7199173 -2.9634647 -2.8340878 -2.6834087][-2.6533988 -2.6865768 -2.8401313 -2.8141932 -2.6272368 -2.3068597 -1.9579866 -1.9153681 -2.1304345 -2.253154 -2.3483331 -2.7060161 -3.1048117 -3.2714837 -3.2258909][-2.2694867 -2.2840493 -2.3850112 -2.4161634 -2.383249 -2.213696 -1.9832151 -1.8952606 -1.853035 -1.7507825 -1.7386484 -2.0671041 -2.6169708 -3.0969396 -3.2820082][-2.0141735 -2.1125839 -2.2643032 -2.3496895 -2.2911534 -2.0302911 -1.5776408 -1.1199658 -0.70839286 -0.49759412 -0.6385808 -1.1247942 -1.8349893 -2.485002 -2.8822627][-2.0297771 -2.1612995 -2.341985 -2.423373 -2.2771978 -1.8420217 -1.1463616 -0.44349051 0.12446404 0.29209185 -0.045595646 -0.64115214 -1.3712347 -2.0145645 -2.4965353][-2.1441357 -2.2547054 -2.3831916 -2.4274585 -2.2695436 -1.8063114 -1.1352081 -0.60008812 -0.2141242 -0.12309408 -0.36055088 -0.56440187 -0.84308767 -1.2371466 -1.763562]]...]
INFO - root - 2017-12-07 05:56:11.162245: step 11010, loss = 0.77, batch loss = 0.70 (10.7 examples/sec; 0.749 sec/batch; 66h:52m:02s remains)
INFO - root - 2017-12-07 05:56:18.818578: step 11020, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.768 sec/batch; 68h:34m:33s remains)
INFO - root - 2017-12-07 05:56:26.313947: step 11030, loss = 0.82, batch loss = 0.74 (10.7 examples/sec; 0.747 sec/batch; 66h:44m:55s remains)
INFO - root - 2017-12-07 05:56:33.959748: step 11040, loss = 0.76, batch loss = 0.68 (10.6 examples/sec; 0.758 sec/batch; 67h:40m:02s remains)
INFO - root - 2017-12-07 05:56:41.693420: step 11050, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.774 sec/batch; 69h:05m:19s remains)
INFO - root - 2017-12-07 05:56:49.348811: step 11060, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.768 sec/batch; 68h:33m:18s remains)
INFO - root - 2017-12-07 05:56:56.979608: step 11070, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 69h:09m:56s remains)
INFO - root - 2017-12-07 05:57:04.648489: step 11080, loss = 1.13, batch loss = 1.06 (10.3 examples/sec; 0.774 sec/batch; 69h:06m:32s remains)
INFO - root - 2017-12-07 05:57:12.336623: step 11090, loss = 0.99, batch loss = 0.92 (10.3 examples/sec; 0.775 sec/batch; 69h:10m:13s remains)
INFO - root - 2017-12-07 05:57:20.055089: step 11100, loss = 0.85, batch loss = 0.78 (10.2 examples/sec; 0.781 sec/batch; 69h:45m:08s remains)
2017-12-07 05:57:20.729970: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7135606 -2.8418651 -2.9851632 -3.067914 -3.0721893 -3.1424904 -3.3097386 -3.4254353 -3.4037235 -3.3348308 -3.2299247 -3.0818696 -2.9922113 -2.8508434 -2.6439996][-2.0504375 -2.260339 -2.3491151 -2.3727396 -2.4043715 -2.5410495 -2.7068055 -2.6728239 -2.5362251 -2.4992476 -2.4737577 -2.3592041 -2.2256916 -2.0306962 -1.8272846][-1.6463859 -1.9258065 -1.968643 -1.9246404 -2.0140541 -2.2986276 -2.5818286 -2.5814781 -2.4643853 -2.4089589 -2.2786365 -2.0104582 -1.7333825 -1.5511737 -1.5008996][-1.6361308 -1.805866 -1.7297525 -1.5380814 -1.5746105 -1.9596899 -2.42179 -2.6204271 -2.6650319 -2.5915694 -2.2771194 -1.7660451 -1.3515432 -1.2856979 -1.4561555][-1.7101946 -1.7037556 -1.5559211 -1.2359054 -1.1259503 -1.499701 -1.9809036 -2.2270653 -2.4399605 -2.5170956 -2.2507386 -1.669677 -1.1838489 -1.1151326 -1.380928][-1.6895623 -1.5603526 -1.410116 -1.0315351 -0.7959168 -1.0211437 -1.2055566 -1.2335827 -1.5559821 -1.9858205 -2.016324 -1.5170934 -1.0134432 -0.86700463 -1.1446135][-1.6607397 -1.4786942 -1.3178523 -0.9606111 -0.70170355 -0.65581894 -0.32883596 0.030241013 -0.31609678 -1.0807586 -1.3950663 -1.0278986 -0.62893367 -0.53383923 -0.86065006][-1.6907542 -1.5513744 -1.4175863 -1.11955 -0.82886982 -0.5366807 0.13217211 0.7117691 0.44022369 -0.3310709 -0.72055864 -0.53529882 -0.45411539 -0.59830236 -0.91319656][-1.782465 -1.6929216 -1.6090002 -1.3747752 -1.0571165 -0.69474721 -0.1019001 0.34403515 0.15766096 -0.39776802 -0.62209892 -0.46128607 -0.55082726 -0.751853 -0.88470745][-1.8974957 -1.7813842 -1.7008457 -1.524014 -1.2874048 -1.0679982 -0.73687792 -0.45237422 -0.56906581 -0.97725415 -1.03303 -0.75921631 -0.75737834 -0.82932281 -0.75889564][-2.14958 -1.9768422 -1.9107542 -1.8240094 -1.735491 -1.6831563 -1.5596268 -1.3502953 -1.3985505 -1.7529569 -1.7337477 -1.4515884 -1.4485364 -1.4868991 -1.2710397][-2.6342552 -2.4548552 -2.42118 -2.470804 -2.5371628 -2.5477209 -2.4732132 -2.2811654 -2.2979665 -2.5727367 -2.4927242 -2.3117418 -2.4966009 -2.6617265 -2.4277785][-3.3100355 -3.2007625 -3.1811836 -3.2681499 -3.3781219 -3.3666768 -3.2681017 -3.0757442 -3.0436087 -3.1490269 -3.0201764 -3.0086737 -3.3886638 -3.6969645 -3.5769615][-3.8710563 -3.8453541 -3.8168283 -3.8627465 -3.9310741 -3.8788557 -3.7908721 -3.6510162 -3.6040709 -3.5664139 -3.3927534 -3.4235721 -3.7614622 -4.0647607 -4.0867286][-4.0719957 -4.0954309 -4.0700841 -4.0679011 -4.0903683 -4.0415058 -3.9925749 -3.9286261 -3.898154 -3.8062871 -3.6284318 -3.5821137 -3.6929488 -3.8404102 -3.9320512]]...]
INFO - root - 2017-12-07 05:57:28.384385: step 11110, loss = 1.09, batch loss = 1.02 (10.3 examples/sec; 0.776 sec/batch; 69h:18m:51s remains)
INFO - root - 2017-12-07 05:57:36.112422: step 11120, loss = 0.63, batch loss = 0.56 (10.5 examples/sec; 0.763 sec/batch; 68h:05m:18s remains)
INFO - root - 2017-12-07 05:57:43.566226: step 11130, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.762 sec/batch; 68h:03m:43s remains)
INFO - root - 2017-12-07 05:57:51.249504: step 11140, loss = 0.74, batch loss = 0.67 (10.1 examples/sec; 0.793 sec/batch; 70h:45m:54s remains)
INFO - root - 2017-12-07 05:57:59.124297: step 11150, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.769 sec/batch; 68h:39m:57s remains)
INFO - root - 2017-12-07 05:58:06.739969: step 11160, loss = 0.93, batch loss = 0.86 (10.7 examples/sec; 0.750 sec/batch; 66h:57m:08s remains)
INFO - root - 2017-12-07 05:58:14.402320: step 11170, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.766 sec/batch; 68h:23m:13s remains)
INFO - root - 2017-12-07 05:58:22.145500: step 11180, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.757 sec/batch; 67h:31m:24s remains)
INFO - root - 2017-12-07 05:58:29.840951: step 11190, loss = 0.74, batch loss = 0.67 (10.1 examples/sec; 0.790 sec/batch; 70h:31m:26s remains)
INFO - root - 2017-12-07 05:58:37.560321: step 11200, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.768 sec/batch; 68h:32m:50s remains)
2017-12-07 05:58:38.166458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7211413 -2.6475568 -2.5318253 -2.4278436 -2.3208015 -2.147367 -2.0702615 -2.0487552 -1.9745862 -1.9010954 -1.8458335 -1.7566655 -1.6587317 -1.6163521 -1.6374748][-2.6938241 -2.5904298 -2.4400496 -2.3074298 -2.2250228 -2.1068723 -2.1234164 -2.2376752 -2.2717013 -2.2466586 -2.1887505 -2.0696812 -1.9306514 -1.8612487 -1.8767288][-2.3899844 -2.2835081 -2.0841808 -1.8922997 -1.8082333 -1.7198288 -1.767853 -1.9638944 -2.1072128 -2.1865873 -2.2185521 -2.1749835 -2.0682743 -1.9882128 -1.9414248][-2.2513969 -2.1022794 -1.7760441 -1.4211006 -1.2515841 -1.1308358 -1.172585 -1.4426517 -1.712795 -1.8731794 -1.9564328 -1.976145 -1.9122815 -1.824326 -1.7350373][-2.3459165 -2.2225871 -1.8272824 -1.2923365 -0.93904066 -0.6269753 -0.49059582 -0.70299959 -1.0588055 -1.3139114 -1.4663317 -1.5953236 -1.6507151 -1.6339254 -1.5803113][-2.4573128 -2.4378443 -2.1037295 -1.5033684 -0.98459482 -0.39124489 0.11217213 0.17397404 -0.175004 -0.60193086 -0.92388749 -1.2141168 -1.4801965 -1.644469 -1.7247214][-2.4928124 -2.5800219 -2.3689504 -1.8028576 -1.20755 -0.38953304 0.52210474 0.99480915 0.73993731 0.10000944 -0.46602416 -0.94401789 -1.4053383 -1.7456892 -1.929369][-2.4399834 -2.5985813 -2.5098681 -2.0203962 -1.4582541 -0.61180639 0.49506283 1.2878394 1.256372 0.57225275 -0.12586069 -0.69688129 -1.2160246 -1.6533163 -1.9198656][-2.3776536 -2.5845139 -2.5979762 -2.2142322 -1.785033 -1.1430094 -0.22559071 0.53279257 0.7229166 0.35446978 -0.12870169 -0.494498 -0.77825761 -1.0865538 -1.398159][-2.3614473 -2.6225371 -2.7298045 -2.4659107 -2.2093608 -1.8447046 -1.2651923 -0.69004273 -0.34377384 -0.29539156 -0.35525942 -0.33827877 -0.20733595 -0.23885202 -0.59438705][-2.3897402 -2.7106106 -2.8973246 -2.7367063 -2.6078386 -2.4170442 -2.0880835 -1.6996329 -1.2893684 -0.9491353 -0.67358565 -0.30951166 0.26929808 0.61612988 0.26972151][-2.4139593 -2.7805855 -3.0271106 -2.9257436 -2.8272867 -2.672019 -2.4768486 -2.2569652 -1.9044902 -1.4759059 -1.1386333 -0.70730972 0.075149536 0.75219536 0.66643333][-2.3986323 -2.7660232 -3.040484 -2.9900095 -2.8938031 -2.7170668 -2.5751882 -2.4814858 -2.2380095 -1.8687174 -1.6409574 -1.3879662 -0.74212575 -0.016578674 0.23165989][-2.3565941 -2.6751742 -2.9438357 -2.9441304 -2.8748989 -2.701298 -2.5956173 -2.5766606 -2.4018772 -2.0978069 -1.9973664 -1.9674413 -1.6055052 -0.99977374 -0.52961159][-2.334553 -2.6031051 -2.8523831 -2.8766813 -2.8174181 -2.641901 -2.5708377 -2.6175809 -2.5060198 -2.2433565 -2.1645138 -2.2023044 -2.0569785 -1.6417229 -1.1501546]]...]
INFO - root - 2017-12-07 05:58:45.932963: step 11210, loss = 0.78, batch loss = 0.70 (10.7 examples/sec; 0.745 sec/batch; 66h:31m:42s remains)
INFO - root - 2017-12-07 05:58:53.595836: step 11220, loss = 0.94, batch loss = 0.87 (10.7 examples/sec; 0.745 sec/batch; 66h:28m:56s remains)
INFO - root - 2017-12-07 05:59:01.145459: step 11230, loss = 0.82, batch loss = 0.74 (10.3 examples/sec; 0.774 sec/batch; 69h:03m:55s remains)
INFO - root - 2017-12-07 05:59:08.903953: step 11240, loss = 0.74, batch loss = 0.67 (10.0 examples/sec; 0.796 sec/batch; 71h:02m:54s remains)
INFO - root - 2017-12-07 05:59:16.727978: step 11250, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.759 sec/batch; 67h:42m:34s remains)
INFO - root - 2017-12-07 05:59:24.562425: step 11260, loss = 0.91, batch loss = 0.84 (10.0 examples/sec; 0.802 sec/batch; 71h:31m:31s remains)
INFO - root - 2017-12-07 05:59:32.381061: step 11270, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.775 sec/batch; 69h:10m:05s remains)
INFO - root - 2017-12-07 05:59:40.099451: step 11280, loss = 0.79, batch loss = 0.72 (10.9 examples/sec; 0.735 sec/batch; 65h:33m:56s remains)
INFO - root - 2017-12-07 05:59:47.810076: step 11290, loss = 0.83, batch loss = 0.75 (10.4 examples/sec; 0.767 sec/batch; 68h:27m:58s remains)
INFO - root - 2017-12-07 05:59:55.382064: step 11300, loss = 0.61, batch loss = 0.54 (10.4 examples/sec; 0.771 sec/batch; 68h:47m:46s remains)
2017-12-07 05:59:55.994328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.407711 -3.3998334 -3.559871 -3.4581029 -3.2137461 -3.2015665 -3.2707918 -3.083138 -2.9124165 -3.0801587 -3.3252311 -3.3975873 -3.5688078 -3.7727747 -3.6460586][-3.1327579 -3.1361194 -3.2668362 -3.1765742 -3.0962043 -3.2303448 -3.3183548 -3.130794 -3.0017147 -3.1563482 -3.2967358 -3.3674593 -3.6101506 -3.8098907 -3.6102676][-2.8654282 -2.906534 -3.0216579 -3.0012398 -3.0376577 -3.1881876 -3.2267256 -3.0705886 -2.992734 -3.0993156 -3.13968 -3.2130651 -3.4648013 -3.6044867 -3.3722234][-2.7912989 -2.8204203 -2.8119395 -2.7523155 -2.7443662 -2.7760918 -2.7707462 -2.7486987 -2.7962332 -2.9230938 -2.9766269 -3.0952911 -3.2936273 -3.3396845 -3.1486166][-2.822958 -2.7662592 -2.5897903 -2.4520121 -2.3624744 -2.2828879 -2.31555 -2.4604454 -2.6343327 -2.8350995 -2.9664452 -3.1629829 -3.37355 -3.4343343 -3.3874316][-2.5505085 -2.4134507 -2.2811637 -2.3072925 -2.3636448 -2.346365 -2.4190476 -2.5404906 -2.6138549 -2.6969919 -2.7207296 -2.8019261 -2.9558554 -3.0883288 -3.2118359][-2.1857452 -2.1052325 -2.1528709 -2.3981936 -2.58034 -2.5749416 -2.5126176 -2.3891821 -2.2271814 -2.1103468 -1.9312556 -1.7204373 -1.6593599 -1.828516 -2.179775][-2.1632171 -2.2821367 -2.4188602 -2.6286473 -2.6803432 -2.5238113 -2.2153132 -1.8436544 -1.5095768 -1.2672706 -0.97090459 -0.5874455 -0.42001581 -0.66725516 -1.1887195][-2.5465627 -2.7585988 -2.8721168 -2.9401441 -2.8098764 -2.4834914 -2.031822 -1.6305768 -1.3679435 -1.210603 -0.98306036 -0.67588305 -0.6302228 -0.92735219 -1.286473][-3.1763668 -3.2388816 -3.2716374 -3.3025968 -3.1103458 -2.7113872 -2.2514989 -1.9706302 -1.9150617 -1.9996667 -2.0285337 -2.0021489 -2.2339802 -2.5405807 -2.5836596][-3.5458665 -3.4462447 -3.444932 -3.4944816 -3.3122768 -2.9851174 -2.6842234 -2.6131384 -2.7667527 -2.9938197 -3.0922933 -3.1699262 -3.5096493 -3.7721393 -3.6082942][-3.5237293 -3.2940578 -3.2774303 -3.3442001 -3.2115338 -3.0941026 -3.1123672 -3.2743075 -3.4915586 -3.5546732 -3.4195292 -3.3654618 -3.6117787 -3.7680583 -3.4867358][-3.3383818 -3.0476279 -3.0417547 -3.109272 -3.0408893 -3.1367903 -3.4225292 -3.6744814 -3.7802095 -3.5776434 -3.2321858 -3.0953388 -3.2168736 -3.2722826 -3.0627694][-2.8835139 -2.6943321 -2.7299964 -2.737329 -2.6989133 -2.9668694 -3.3922782 -3.5940943 -3.5476346 -3.2909036 -3.0690227 -3.1230206 -3.2730703 -3.2701635 -3.1109862][-2.6557302 -2.6173649 -2.641885 -2.5086775 -2.4133129 -2.706301 -3.0830145 -3.1572945 -3.047894 -2.933372 -2.9751515 -3.2242379 -3.3885193 -3.3754756 -3.2814713]]...]
INFO - root - 2017-12-07 06:00:03.593611: step 11310, loss = 0.66, batch loss = 0.59 (10.4 examples/sec; 0.772 sec/batch; 68h:54m:50s remains)
INFO - root - 2017-12-07 06:00:11.198821: step 11320, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 68h:52m:35s remains)
INFO - root - 2017-12-07 06:00:18.506257: step 11330, loss = 0.56, batch loss = 0.49 (10.6 examples/sec; 0.758 sec/batch; 67h:35m:55s remains)
INFO - root - 2017-12-07 06:00:26.107994: step 11340, loss = 0.64, batch loss = 0.57 (10.3 examples/sec; 0.774 sec/batch; 69h:02m:54s remains)
INFO - root - 2017-12-07 06:00:33.754299: step 11350, loss = 0.97, batch loss = 0.89 (10.0 examples/sec; 0.797 sec/batch; 71h:03m:55s remains)
INFO - root - 2017-12-07 06:00:41.359567: step 11360, loss = 0.68, batch loss = 0.60 (10.6 examples/sec; 0.753 sec/batch; 67h:12m:48s remains)
INFO - root - 2017-12-07 06:00:49.032110: step 11370, loss = 0.89, batch loss = 0.82 (10.3 examples/sec; 0.778 sec/batch; 69h:25m:12s remains)
INFO - root - 2017-12-07 06:00:56.599838: step 11380, loss = 0.88, batch loss = 0.80 (10.6 examples/sec; 0.756 sec/batch; 67h:25m:57s remains)
INFO - root - 2017-12-07 06:01:04.198141: step 11390, loss = 0.80, batch loss = 0.72 (10.5 examples/sec; 0.759 sec/batch; 67h:39m:39s remains)
INFO - root - 2017-12-07 06:01:11.822566: step 11400, loss = 0.96, batch loss = 0.89 (10.5 examples/sec; 0.758 sec/batch; 67h:38m:11s remains)
2017-12-07 06:01:12.499551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.76127 -2.689497 -2.7061143 -2.7717476 -2.8702598 -2.9770317 -3.1371009 -3.2165651 -3.1376421 -2.9456706 -2.72336 -2.6455884 -2.7146678 -2.7701907 -2.7255282][-2.6785283 -2.5058599 -2.4487972 -2.4568095 -2.535542 -2.6999536 -2.9447587 -3.0392318 -2.833322 -2.4286473 -2.009011 -1.9275656 -2.1868529 -2.4528861 -2.476686][-2.64553 -2.3957107 -2.2311864 -2.0973043 -2.0330384 -2.1480505 -2.3971136 -2.445416 -2.1126249 -1.5868409 -1.1859338 -1.3338649 -1.9129562 -2.4034352 -2.4273772][-2.6956396 -2.4520698 -2.2166176 -1.9334221 -1.6945381 -1.7160206 -1.9016426 -1.8116248 -1.3167505 -0.79468155 -0.65536547 -1.1983175 -2.0919957 -2.70545 -2.6290991][-2.5853233 -2.3261814 -2.0615442 -1.7550311 -1.5181375 -1.5698214 -1.7286966 -1.5068836 -0.84200907 -0.34630632 -0.50328493 -1.3761706 -2.441236 -3.0342622 -2.7956071][-2.4570208 -2.1589503 -1.8693211 -1.6541071 -1.600631 -1.7875369 -1.8692682 -1.4096832 -0.49213576 -0.0031385422 -0.41452837 -1.4431384 -2.4875641 -2.9884338 -2.6504045][-2.1928303 -1.7196541 -1.3322258 -1.2058778 -1.3152668 -1.5366557 -1.3884671 -0.58989549 0.56996822 0.96629763 0.24579191 -0.92102861 -1.9745505 -2.5143659 -2.264395][-1.5375514 -0.91015124 -0.5334487 -0.50651884 -0.65978479 -0.7945509 -0.41288185 0.63886118 1.8937693 2.1260362 1.1111922 -0.23022461 -1.3709471 -2.0369637 -1.9443588][-0.979856 -0.42107582 -0.25621939 -0.39966011 -0.53391075 -0.57073545 -0.14287996 0.80866003 1.8511276 1.9320126 0.90010595 -0.3853898 -1.4488668 -2.0518293 -1.97819][-1.1791017 -0.92939162 -1.0877507 -1.4044056 -1.5320988 -1.481406 -1.1732996 -0.65432072 -0.085371971 0.010976315 -0.6423943 -1.5181468 -2.2229564 -2.541976 -2.3465114][-2.2565453 -2.2585244 -2.5001128 -2.8062639 -2.929461 -2.8665102 -2.7155485 -2.5876493 -2.3281176 -2.13941 -2.3842573 -2.795588 -3.0786219 -3.0699184 -2.760673][-3.3449116 -3.3731098 -3.4448349 -3.5808749 -3.6177628 -3.5186656 -3.4709997 -3.5331562 -3.3947573 -3.2136045 -3.3224142 -3.5231912 -3.5876679 -3.4179883 -3.0759718][-3.7885418 -3.7571082 -3.6806614 -3.6995416 -3.7058353 -3.6463535 -3.6960216 -3.8182464 -3.6951361 -3.5351849 -3.6002586 -3.7661994 -3.7963912 -3.5831428 -3.2504191][-4.1457677 -4.0677328 -3.9502406 -3.9216897 -3.9232266 -3.9458418 -4.0497975 -4.1317248 -3.9703739 -3.7746265 -3.7576628 -3.8825071 -3.8838046 -3.6290271 -3.3015034][-4.5458765 -4.4206357 -4.3049579 -4.2424111 -4.20749 -4.2234092 -4.2933483 -4.3157954 -4.1589489 -3.9526224 -3.8510795 -3.8994343 -3.8531501 -3.5830517 -3.2786622]]...]
INFO - root - 2017-12-07 06:01:20.249253: step 11410, loss = 1.04, batch loss = 0.97 (10.7 examples/sec; 0.748 sec/batch; 66h:42m:18s remains)
INFO - root - 2017-12-07 06:01:27.807502: step 11420, loss = 0.75, batch loss = 0.68 (10.2 examples/sec; 0.783 sec/batch; 69h:52m:37s remains)
INFO - root - 2017-12-07 06:01:35.219553: step 11430, loss = 0.92, batch loss = 0.85 (10.5 examples/sec; 0.759 sec/batch; 67h:42m:26s remains)
INFO - root - 2017-12-07 06:01:42.794309: step 11440, loss = 0.75, batch loss = 0.68 (10.8 examples/sec; 0.738 sec/batch; 65h:51m:01s remains)
INFO - root - 2017-12-07 06:01:50.383312: step 11450, loss = 0.61, batch loss = 0.54 (10.7 examples/sec; 0.748 sec/batch; 66h:40m:34s remains)
INFO - root - 2017-12-07 06:01:58.089354: step 11460, loss = 0.62, batch loss = 0.54 (10.3 examples/sec; 0.774 sec/batch; 69h:00m:38s remains)
INFO - root - 2017-12-07 06:02:05.665834: step 11470, loss = 0.86, batch loss = 0.79 (10.7 examples/sec; 0.747 sec/batch; 66h:36m:49s remains)
INFO - root - 2017-12-07 06:02:13.351925: step 11480, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 69h:30m:25s remains)
INFO - root - 2017-12-07 06:02:20.998199: step 11490, loss = 1.03, batch loss = 0.96 (10.5 examples/sec; 0.760 sec/batch; 67h:44m:39s remains)
INFO - root - 2017-12-07 06:02:28.645201: step 11500, loss = 1.17, batch loss = 1.10 (10.3 examples/sec; 0.776 sec/batch; 69h:10m:10s remains)
2017-12-07 06:02:29.284211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6978936 -2.6165528 -2.4950325 -2.5011368 -2.6991553 -2.9753187 -3.1815977 -3.2946062 -3.3575311 -3.2633948 -3.0381634 -2.7887735 -2.6330411 -2.5409124 -2.3752346][-2.3865039 -2.3477933 -2.3506353 -2.5081489 -2.8118739 -3.0599775 -3.06404 -2.9522243 -2.9939654 -3.0953982 -3.0648227 -2.8925235 -2.7801957 -2.7399073 -2.5533247][-2.1918771 -2.4059963 -2.6795435 -2.9608884 -3.2013972 -3.2183626 -2.8804374 -2.4531178 -2.4790802 -2.8793969 -3.1406531 -3.0992594 -3.0401559 -3.0396152 -2.8415914][-2.4052551 -2.8317862 -3.213448 -3.4282284 -3.475508 -3.2337432 -2.6147952 -1.9879394 -2.1109533 -2.8316994 -3.3493927 -3.3958702 -3.3369198 -3.3370302 -3.1502438][-2.8310657 -3.1478121 -3.3554633 -3.4174938 -3.3238158 -2.9119253 -2.116972 -1.4613426 -1.8271348 -2.8668594 -3.5911915 -3.6926219 -3.5946009 -3.5500531 -3.3752527][-3.0674624 -3.0519342 -2.9788921 -2.9569111 -2.8122737 -2.2060056 -1.1754279 -0.59571528 -1.3575914 -2.7341051 -3.6172087 -3.7783146 -3.65077 -3.5352433 -3.3402][-2.9218941 -2.5956593 -2.3007233 -2.2255516 -2.016892 -1.1794338 0.10602283 0.53884411 -0.64752316 -2.2656927 -3.1864843 -3.3732512 -3.2689896 -3.1285751 -2.919683][-2.5092442 -2.1073968 -1.73614 -1.598968 -1.3300946 -0.45638728 0.8474741 1.1721382 -0.0576005 -1.4593065 -2.1801615 -2.4063933 -2.5100145 -2.58741 -2.5507877][-2.2031915 -1.9280169 -1.6062477 -1.4512033 -1.2587004 -0.7453649 0.091632843 0.29418421 -0.4686079 -1.1785402 -1.4640877 -1.7090507 -2.11217 -2.5586641 -2.8104157][-2.3925302 -2.1287105 -1.7991071 -1.6486073 -1.6618638 -1.6438332 -1.3073158 -1.2104478 -1.5407677 -1.6031489 -1.4737463 -1.6684475 -2.2614427 -2.9256587 -3.3174977][-2.7467866 -2.3188045 -1.8910232 -1.812768 -2.0752006 -2.3064535 -2.1027937 -1.9692614 -2.0326638 -1.8029542 -1.542191 -1.7673478 -2.4414351 -3.1563013 -3.578862][-3.0245194 -2.5098855 -2.0778465 -2.1072392 -2.4926629 -2.719368 -2.4582329 -2.2480397 -2.1715243 -1.9080942 -1.7197125 -2.006988 -2.6307981 -3.2376657 -3.5772314][-3.2189207 -2.7996836 -2.4688194 -2.5523434 -2.9104488 -3.0688367 -2.8247809 -2.6376631 -2.5048306 -2.258189 -2.1132765 -2.3153243 -2.7064772 -3.0466237 -3.2005939][-3.0764902 -2.8628993 -2.7029824 -2.7996473 -3.0740745 -3.20924 -3.0930195 -2.9677329 -2.8083367 -2.5617247 -2.3963583 -2.4421263 -2.6146092 -2.7636235 -2.8146682][-2.7904823 -2.7553859 -2.7434816 -2.8468215 -3.0335703 -3.1517625 -3.1334944 -3.0649929 -2.9398079 -2.7531381 -2.6079521 -2.5617259 -2.5951052 -2.6397 -2.6471481]]...]
INFO - root - 2017-12-07 06:02:37.000318: step 11510, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.758 sec/batch; 67h:36m:19s remains)
INFO - root - 2017-12-07 06:02:44.546562: step 11520, loss = 0.71, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 69h:44m:07s remains)
INFO - root - 2017-12-07 06:02:51.916730: step 11530, loss = 0.68, batch loss = 0.61 (10.5 examples/sec; 0.763 sec/batch; 68h:00m:47s remains)
INFO - root - 2017-12-07 06:02:59.500859: step 11540, loss = 0.83, batch loss = 0.75 (10.4 examples/sec; 0.767 sec/batch; 68h:23m:11s remains)
INFO - root - 2017-12-07 06:03:07.194916: step 11550, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.763 sec/batch; 68h:02m:55s remains)
INFO - root - 2017-12-07 06:03:14.885081: step 11560, loss = 0.73, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 68h:26m:17s remains)
INFO - root - 2017-12-07 06:03:22.454406: step 11570, loss = 0.61, batch loss = 0.54 (10.9 examples/sec; 0.734 sec/batch; 65h:23m:32s remains)
INFO - root - 2017-12-07 06:03:30.142318: step 11580, loss = 1.16, batch loss = 1.09 (10.4 examples/sec; 0.767 sec/batch; 68h:21m:24s remains)
INFO - root - 2017-12-07 06:03:37.828126: step 11590, loss = 0.84, batch loss = 0.76 (10.2 examples/sec; 0.784 sec/batch; 69h:52m:02s remains)
INFO - root - 2017-12-07 06:03:45.694698: step 11600, loss = 0.76, batch loss = 0.68 (9.9 examples/sec; 0.806 sec/batch; 71h:50m:03s remains)
2017-12-07 06:03:46.324878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6014349 -3.1038661 -3.5581164 -3.80296 -3.788753 -3.6325343 -3.4614902 -3.3326726 -3.2868519 -3.3033693 -3.3578553 -3.4121566 -3.4384632 -3.4586284 -3.4333696][-2.8398411 -3.5690086 -4.2493606 -4.6513538 -4.6734009 -4.4684153 -4.2464762 -4.1103749 -4.0907106 -4.1458626 -4.2474017 -4.3536253 -4.4041181 -4.4222131 -4.3604722][-2.9028702 -3.7872055 -4.6046891 -5.112154 -5.1636915 -4.9206614 -4.6706319 -4.6002264 -4.7050209 -4.8800817 -5.0684114 -5.2504215 -5.3299851 -5.31608 -5.168015][-2.6879492 -3.5640135 -4.3369889 -4.8235955 -4.8620753 -4.5590148 -4.2835016 -4.3222985 -4.6466317 -5.0538983 -5.4084883 -5.6501517 -5.6935811 -5.5801811 -5.3701706][-2.4314554 -3.1491795 -3.6935587 -3.9844613 -3.9316528 -3.544775 -3.2339718 -3.3459654 -3.8724074 -4.5530624 -5.1411362 -5.4664507 -5.4462018 -5.2021241 -4.9703655][-2.3850365 -2.9154305 -3.1558537 -3.125524 -2.8666859 -2.3720145 -2.0154006 -2.1673086 -2.8183641 -3.6896687 -4.480319 -4.935348 -4.9171476 -4.6013088 -4.3900838][-2.5079165 -2.8724761 -2.8827481 -2.5961008 -2.1230824 -1.4796693 -1.0302529 -1.1790838 -1.9147584 -2.9081202 -3.8554981 -4.4134011 -4.4275289 -4.1270981 -3.9797091][-2.5766563 -2.7657249 -2.6935534 -2.3511014 -1.761375 -0.9998014 -0.45937014 -0.59352875 -1.3665957 -2.4088073 -3.4381497 -3.9857652 -3.9523032 -3.6747594 -3.6169198][-2.5070572 -2.5621786 -2.50075 -2.2005823 -1.5887153 -0.839725 -0.34804344 -0.50037861 -1.2186863 -2.1508405 -3.0655637 -3.4833772 -3.39187 -3.1654067 -3.1907229][-2.443953 -2.4645414 -2.4226792 -2.1170404 -1.5122206 -0.90044737 -0.54586005 -0.68878365 -1.3013179 -2.0821819 -2.79181 -3.0299129 -2.9123559 -2.7571535 -2.8116446][-2.4886074 -2.5447474 -2.4826639 -2.1329803 -1.5909138 -1.1593404 -0.89317703 -0.93927026 -1.4468572 -2.1255877 -2.6354952 -2.7082729 -2.576354 -2.4893978 -2.564991][-2.5403719 -2.6405153 -2.5660353 -2.2371697 -1.8348284 -1.5519123 -1.290915 -1.2147782 -1.6207516 -2.2222171 -2.5904179 -2.5767779 -2.4408333 -2.3824687 -2.4735665][-2.421495 -2.5393448 -2.4722 -2.2337406 -1.980783 -1.7895143 -1.5265799 -1.4054041 -1.7168391 -2.176429 -2.4027231 -2.3618126 -2.2618959 -2.2524362 -2.380614][-2.2083418 -2.2939856 -2.2361138 -2.0877717 -1.9349544 -1.7726364 -1.5291731 -1.4276698 -1.6415486 -1.9313469 -2.0586033 -2.0543747 -2.0283492 -2.080451 -2.2375228][-2.2055027 -2.2237494 -2.1418035 -2.024781 -1.9153123 -1.7751839 -1.5947349 -1.5525501 -1.6921079 -1.8525472 -1.9294629 -1.9568274 -1.9637928 -2.0123405 -2.1371763]]...]
INFO - root - 2017-12-07 06:03:54.073315: step 11610, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.777 sec/batch; 69h:15m:34s remains)
INFO - root - 2017-12-07 06:04:01.697375: step 11620, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.771 sec/batch; 68h:43m:11s remains)
INFO - root - 2017-12-07 06:04:09.182240: step 11630, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.776 sec/batch; 69h:10m:17s remains)
INFO - root - 2017-12-07 06:04:16.966702: step 11640, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.773 sec/batch; 68h:55m:52s remains)
INFO - root - 2017-12-07 06:04:24.675952: step 11650, loss = 0.85, batch loss = 0.78 (10.9 examples/sec; 0.736 sec/batch; 65h:34m:11s remains)
INFO - root - 2017-12-07 06:04:32.331544: step 11660, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.754 sec/batch; 67h:09m:32s remains)
INFO - root - 2017-12-07 06:04:40.056070: step 11670, loss = 1.05, batch loss = 0.98 (10.8 examples/sec; 0.744 sec/batch; 66h:16m:11s remains)
INFO - root - 2017-12-07 06:04:47.603259: step 11680, loss = 1.06, batch loss = 0.99 (10.2 examples/sec; 0.784 sec/batch; 69h:53m:12s remains)
INFO - root - 2017-12-07 06:04:55.214265: step 11690, loss = 0.87, batch loss = 0.79 (10.8 examples/sec; 0.744 sec/batch; 66h:17m:10s remains)
INFO - root - 2017-12-07 06:05:02.767348: step 11700, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.751 sec/batch; 66h:53m:53s remains)
2017-12-07 06:05:03.422679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7144828 -2.8888369 -2.8715088 -3.0440483 -3.4042239 -3.3852439 -2.9418006 -2.5586696 -2.7038355 -2.753794 -2.211971 -1.8180103 -2.1921117 -2.6624832 -2.3982794][-2.4124866 -2.6112852 -2.774672 -3.0971608 -3.5098453 -3.5545597 -3.2710037 -3.0582 -3.1726792 -3.0363305 -2.382102 -1.9621654 -2.3898258 -2.9079134 -2.6549149][-2.1958261 -2.4669168 -2.8951683 -3.3773274 -3.7935553 -3.8433096 -3.6102273 -3.3905311 -3.3839257 -3.1105034 -2.4447534 -2.0878587 -2.5968244 -3.1842804 -2.9866161][-1.8902009 -2.2879732 -2.9321928 -3.4763227 -3.775269 -3.7061987 -3.4009042 -3.0682569 -2.9601488 -2.6979289 -2.1846771 -2.0282748 -2.6676869 -3.3636088 -3.2834895][-1.6435945 -2.2216856 -3.0449314 -3.5487759 -3.59882 -3.2497401 -2.8366294 -2.5712042 -2.6059117 -2.4967334 -2.0967922 -2.0691323 -2.7606215 -3.5070288 -3.552845][-1.5009363 -2.0339274 -2.7662983 -3.0067928 -2.698905 -2.0966053 -1.7676878 -1.9251359 -2.344847 -2.3591888 -1.9115026 -1.9000187 -2.6051626 -3.3550668 -3.5469489][-1.4628634 -1.7954826 -2.2504485 -2.1299765 -1.4712899 -0.72391224 -0.62256551 -1.3044436 -2.0427058 -2.0477386 -1.5046227 -1.5121536 -2.207314 -2.8288827 -3.0531621][-2.0005977 -2.1360166 -2.3997388 -2.1089551 -1.3422611 -0.58570528 -0.62213326 -1.560344 -2.36599 -2.2562969 -1.6828055 -1.7170575 -2.2797353 -2.5625019 -2.5850191][-2.8293862 -2.8888392 -3.0630264 -2.7885051 -2.1518478 -1.5091746 -1.5628161 -2.4056895 -3.0096874 -2.7875354 -2.3203285 -2.4119663 -2.7650826 -2.7220573 -2.5904233][-3.1922903 -3.2680097 -3.384284 -3.178637 -2.692838 -2.1538343 -2.115941 -2.6748071 -2.9908335 -2.7905662 -2.6348643 -2.8596768 -3.0367103 -2.8474851 -2.7847867][-3.3272023 -3.3938076 -3.5082006 -3.44514 -3.1035855 -2.658484 -2.5797257 -2.8741322 -2.8703828 -2.6712718 -2.7678261 -3.053237 -2.9954724 -2.6912217 -2.7175491][-3.5421596 -3.6085241 -3.7967405 -3.8887482 -3.5789247 -3.1380405 -3.0697713 -3.1910391 -2.8661387 -2.5790308 -2.8344135 -3.1934118 -2.9960322 -2.6103053 -2.6833792][-3.5484638 -3.5538993 -3.7967031 -3.9663835 -3.6102803 -3.1371522 -3.0688915 -3.081212 -2.6058469 -2.342993 -2.8211017 -3.3432324 -3.1881821 -2.8182077 -2.9445491][-3.3574026 -3.335947 -3.5858772 -3.7042613 -3.2540183 -2.7560906 -2.6312685 -2.5143967 -2.0386722 -1.9297218 -2.6735911 -3.3613369 -3.3228354 -3.0570252 -3.2429554][-3.1139965 -3.1722841 -3.4169955 -3.4509697 -2.9760838 -2.5750003 -2.4127107 -2.1320705 -1.6650529 -1.6935084 -2.5900211 -3.3234265 -3.3506279 -3.1119237 -3.204215]]...]
INFO - root - 2017-12-07 06:05:11.098393: step 11710, loss = 0.66, batch loss = 0.58 (10.1 examples/sec; 0.789 sec/batch; 70h:15m:44s remains)
INFO - root - 2017-12-07 06:05:18.852084: step 11720, loss = 0.96, batch loss = 0.88 (10.2 examples/sec; 0.782 sec/batch; 69h:40m:09s remains)
INFO - root - 2017-12-07 06:05:26.323503: step 11730, loss = 0.86, batch loss = 0.79 (10.7 examples/sec; 0.745 sec/batch; 66h:21m:15s remains)
INFO - root - 2017-12-07 06:05:34.000317: step 11740, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 67h:36m:55s remains)
INFO - root - 2017-12-07 06:05:41.767525: step 11750, loss = 0.84, batch loss = 0.76 (10.5 examples/sec; 0.762 sec/batch; 67h:53m:35s remains)
INFO - root - 2017-12-07 06:05:49.522091: step 11760, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.761 sec/batch; 67h:46m:19s remains)
INFO - root - 2017-12-07 06:05:57.228493: step 11770, loss = 0.88, batch loss = 0.81 (10.1 examples/sec; 0.788 sec/batch; 70h:14m:04s remains)
INFO - root - 2017-12-07 06:06:04.949962: step 11780, loss = 0.93, batch loss = 0.86 (10.3 examples/sec; 0.776 sec/batch; 69h:06m:33s remains)
INFO - root - 2017-12-07 06:06:12.632776: step 11790, loss = 0.73, batch loss = 0.66 (10.7 examples/sec; 0.750 sec/batch; 66h:47m:56s remains)
INFO - root - 2017-12-07 06:06:20.352934: step 11800, loss = 1.09, batch loss = 1.01 (10.1 examples/sec; 0.795 sec/batch; 70h:47m:57s remains)
2017-12-07 06:06:21.088944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5719376 -2.6155729 -2.7349892 -2.8122964 -2.8259621 -2.7815263 -2.672298 -2.5292 -2.4222898 -2.4050987 -2.4081986 -2.4213195 -2.5473223 -2.6582608 -2.6453695][-2.5773969 -2.6099911 -2.7572517 -2.827517 -2.7954197 -2.6828694 -2.519783 -2.3703 -2.2851741 -2.3336244 -2.4136508 -2.4781203 -2.5596871 -2.54941 -2.4315722][-2.6559849 -2.6640239 -2.7512231 -2.78564 -2.7614074 -2.6575994 -2.50987 -2.3834846 -2.33824 -2.4366958 -2.5827239 -2.7031059 -2.749155 -2.6261244 -2.3998537][-2.5956826 -2.5765419 -2.5700872 -2.5861964 -2.6047802 -2.5268302 -2.3792138 -2.2635725 -2.272639 -2.4291718 -2.634347 -2.8012981 -2.8679328 -2.7223716 -2.4527571][-2.666441 -2.6972451 -2.6991711 -2.75122 -2.80408 -2.7130749 -2.5047345 -2.3320851 -2.3131604 -2.4633892 -2.679493 -2.8346915 -2.9154148 -2.8076279 -2.5707181][-2.3074608 -2.3764625 -2.4157906 -2.4830658 -2.5060987 -2.3453012 -2.0628672 -1.8404381 -1.780966 -1.914078 -2.1291404 -2.277231 -2.420208 -2.4636123 -2.4233654][-1.5232093 -1.5228431 -1.5485921 -1.5962892 -1.6347053 -1.5505354 -1.4227259 -1.315469 -1.2774284 -1.3689272 -1.4937973 -1.5474083 -1.6685376 -1.8097591 -1.9621751][-1.2324915 -1.1829021 -1.1717112 -1.1828899 -1.2543283 -1.3535547 -1.5450358 -1.717643 -1.771594 -1.8273902 -1.8173389 -1.7112529 -1.6692314 -1.6751535 -1.742445][-1.5395565 -1.5643184 -1.5744803 -1.5377805 -1.525182 -1.6179652 -1.9073668 -2.206908 -2.341439 -2.4258635 -2.4011338 -2.2621789 -2.1595316 -2.0388894 -1.9345055][-1.5995758 -1.7500994 -1.879535 -1.9097369 -1.8515131 -1.8198314 -1.9548781 -2.147083 -2.2701423 -2.4144564 -2.4608936 -2.3952012 -2.3619645 -2.2755477 -2.1562054][-1.4564178 -1.6400237 -1.8577831 -1.9886627 -1.9834619 -1.9062531 -1.8795633 -1.8872521 -1.918134 -2.0614345 -2.1647105 -2.1945214 -2.2734437 -2.2954073 -2.2573771][-1.7814505 -1.8771045 -2.0374491 -2.165015 -2.2182474 -2.207114 -2.1841741 -2.1404715 -2.1036921 -2.1715207 -2.2368078 -2.2752078 -2.3751502 -2.4246604 -2.4149544][-2.4547973 -2.4971685 -2.5834146 -2.6634693 -2.7235351 -2.7640512 -2.7966397 -2.8029113 -2.7972515 -2.8461013 -2.8720267 -2.8577151 -2.8683114 -2.8172474 -2.7199235][-2.8886573 -2.9351425 -2.9834664 -3.0157239 -3.0397353 -3.0601716 -3.0852342 -3.1112137 -3.1353436 -3.1972113 -3.2345295 -3.2265246 -3.1964474 -3.0931158 -2.9554687][-3.0861387 -3.125278 -3.1427333 -3.1402624 -3.1300197 -3.1230431 -3.1293554 -3.1547952 -3.1896415 -3.2493091 -3.2879767 -3.2907538 -3.260123 -3.1744709 -3.0642247]]...]
INFO - root - 2017-12-07 06:06:28.744298: step 11810, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.786 sec/batch; 70h:01m:42s remains)
INFO - root - 2017-12-07 06:06:36.339585: step 11820, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.757 sec/batch; 67h:26m:07s remains)
INFO - root - 2017-12-07 06:06:43.718871: step 11830, loss = 0.64, batch loss = 0.57 (10.5 examples/sec; 0.763 sec/batch; 67h:57m:47s remains)
INFO - root - 2017-12-07 06:06:51.396144: step 11840, loss = 1.02, batch loss = 0.95 (10.3 examples/sec; 0.777 sec/batch; 69h:13m:42s remains)
INFO - root - 2017-12-07 06:06:59.043562: step 11850, loss = 0.78, batch loss = 0.70 (10.6 examples/sec; 0.751 sec/batch; 66h:54m:55s remains)
INFO - root - 2017-12-07 06:07:06.697155: step 11860, loss = 0.87, batch loss = 0.79 (10.3 examples/sec; 0.778 sec/batch; 69h:15m:07s remains)
INFO - root - 2017-12-07 06:07:14.483358: step 11870, loss = 0.67, batch loss = 0.60 (9.0 examples/sec; 0.888 sec/batch; 79h:03m:25s remains)
INFO - root - 2017-12-07 06:07:22.148287: step 11880, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 69h:00m:32s remains)
INFO - root - 2017-12-07 06:07:29.910368: step 11890, loss = 0.77, batch loss = 0.70 (10.0 examples/sec; 0.800 sec/batch; 71h:13m:47s remains)
INFO - root - 2017-12-07 06:07:37.504276: step 11900, loss = 0.88, batch loss = 0.81 (10.7 examples/sec; 0.751 sec/batch; 66h:52m:52s remains)
2017-12-07 06:07:38.160281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0189402 -2.93628 -2.4675374 -2.6871481 -3.4665895 -3.5885971 -2.955833 -2.2700763 -1.7260673 -1.0913424 -0.30080318 0.028506756 -0.60633636 -1.519912 -1.770875][-3.0724952 -2.9789968 -2.7079155 -2.9759719 -3.3078189 -3.0701051 -2.5777526 -2.0680108 -1.5642903 -0.97797346 -0.38335371 -0.24983883 -0.9125886 -1.750391 -1.8102911][-2.9924021 -2.7922683 -2.6210141 -2.878722 -2.8107948 -2.3913646 -2.2087946 -1.8505244 -1.1995249 -0.52060819 -0.21391582 -0.42632961 -1.1091077 -1.9358182 -1.8621464][-2.8695135 -2.5884876 -2.4878235 -2.7072482 -2.3734944 -1.9770696 -2.1377621 -1.7516556 -0.71547985 0.1475234 0.0666275 -0.56279564 -1.2387066 -2.0969074 -1.9250236][-2.7475243 -2.4939766 -2.4863448 -2.6527233 -2.1731911 -1.8918152 -2.2973728 -1.699801 -0.13286924 0.99764156 0.46623325 -0.68664074 -1.4873409 -2.5055051 -2.2199795][-2.5331869 -2.3684766 -2.4647679 -2.5207467 -1.9617567 -1.8369679 -2.3566794 -1.4089689 0.84025145 2.3047352 1.248332 -0.55572391 -1.7100341 -3.0550656 -2.6614575][-2.371969 -2.3447323 -2.4696226 -2.2880485 -1.6489906 -1.680284 -2.2971151 -1.0729835 1.8162694 3.6250353 2.0629816 -0.37558794 -1.9632545 -3.6565616 -3.1205454][-2.3648632 -2.4903989 -2.6240168 -2.2440844 -1.5697808 -1.7214506 -2.3749251 -1.0591726 2.2039471 4.3382769 2.5683622 -0.19999552 -2.1313758 -4.0351148 -3.2897511][-2.4362035 -2.8996522 -3.1776226 -2.6959739 -1.9871163 -2.1473124 -2.7230213 -1.4090848 1.9756894 4.3393984 2.6945214 -0.10893488 -2.2525601 -4.1471276 -3.1391273][-2.4931474 -3.3094363 -3.7543297 -3.2002242 -2.3944745 -2.4779792 -2.9588895 -1.7403052 1.4844685 3.841959 2.4929843 -0.13272905 -2.3419197 -4.0516715 -2.8043315][-2.1996174 -3.1625195 -3.6736808 -3.0727568 -2.2777336 -2.4042456 -2.9010143 -1.9191356 0.77522373 2.7866535 1.8137445 -0.38103342 -2.4410057 -3.8336692 -2.4287984][-1.8744907 -2.7645085 -3.2329888 -2.6661625 -1.9777243 -2.1664002 -2.717135 -2.1081696 -0.24669361 1.2138262 0.70298147 -0.88499188 -2.6716537 -3.6935716 -2.2225251][-1.7054212 -2.4006147 -2.7405705 -2.2432747 -1.7218173 -1.9891908 -2.55648 -2.3123982 -1.300138 -0.37594938 -0.42814922 -1.3922131 -2.8622189 -3.5579133 -2.142211][-1.7159631 -2.2281592 -2.4397831 -2.0710976 -1.7810681 -2.1017776 -2.609632 -2.5836496 -2.1876264 -1.6328201 -1.3068204 -1.7642579 -2.9560466 -3.4389238 -2.1826916][-1.6841638 -1.9960768 -2.1064768 -1.8996687 -1.8228886 -2.1367223 -2.556078 -2.6841969 -2.6927254 -2.3620121 -1.822727 -1.9792361 -2.9919758 -3.378958 -2.3196704]]...]
INFO - root - 2017-12-07 06:07:45.822737: step 11910, loss = 0.91, batch loss = 0.84 (10.3 examples/sec; 0.774 sec/batch; 68h:57m:24s remains)
INFO - root - 2017-12-07 06:07:53.457404: step 11920, loss = 1.03, batch loss = 0.96 (10.4 examples/sec; 0.771 sec/batch; 68h:39m:22s remains)
INFO - root - 2017-12-07 06:08:00.942164: step 11930, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 68h:39m:47s remains)
INFO - root - 2017-12-07 06:08:08.600076: step 11940, loss = 0.66, batch loss = 0.59 (10.4 examples/sec; 0.770 sec/batch; 68h:34m:40s remains)
INFO - root - 2017-12-07 06:08:16.314333: step 11950, loss = 0.61, batch loss = 0.54 (10.6 examples/sec; 0.754 sec/batch; 67h:06m:43s remains)
INFO - root - 2017-12-07 06:08:23.992348: step 11960, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.771 sec/batch; 68h:40m:22s remains)
INFO - root - 2017-12-07 06:08:31.564309: step 11970, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.769 sec/batch; 68h:28m:48s remains)
INFO - root - 2017-12-07 06:08:39.134108: step 11980, loss = 0.97, batch loss = 0.90 (9.9 examples/sec; 0.806 sec/batch; 71h:44m:11s remains)
INFO - root - 2017-12-07 06:08:46.751469: step 11990, loss = 0.81, batch loss = 0.73 (10.6 examples/sec; 0.756 sec/batch; 67h:17m:59s remains)
INFO - root - 2017-12-07 06:08:54.461537: step 12000, loss = 0.71, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 70h:35m:25s remains)
2017-12-07 06:08:55.166979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6150131 -2.5110981 -2.7030468 -2.9905224 -3.2330546 -3.3680272 -3.2997823 -3.0619469 -2.6874223 -2.4679732 -2.5425231 -2.7486479 -2.9754424 -3.0869384 -3.0371976][-2.49497 -2.2890508 -2.4831476 -2.7616341 -3.0377808 -3.2682056 -3.2509758 -2.9880803 -2.4393358 -2.0397904 -2.0195584 -2.2274938 -2.5680809 -2.791647 -2.8239212][-2.1447458 -1.7962809 -1.9517832 -2.216949 -2.5073133 -2.7687292 -2.7737589 -2.5074756 -1.8679435 -1.3913476 -1.3421926 -1.5958636 -2.0339549 -2.2722468 -2.2894843][-1.6011157 -1.1502001 -1.3183773 -1.6131926 -1.8780353 -2.0759568 -2.0782063 -1.8149235 -1.1586914 -0.6395731 -0.58068395 -0.91983151 -1.4767215 -1.7221847 -1.6840193][-1.1400373 -0.66770363 -0.82868576 -1.1041579 -1.2852049 -1.435467 -1.5312481 -1.3609183 -0.75880527 -0.12244558 0.089826584 -0.19204998 -0.81790709 -1.1273046 -1.1552112][-1.0590298 -0.62621856 -0.72644949 -0.86891961 -0.84979677 -0.93485618 -1.1674378 -1.1607692 -0.68094206 0.046818256 0.51233006 0.39286232 -0.28038216 -0.71559405 -0.90256929][-1.3534195 -0.988235 -0.99869823 -0.92215705 -0.64410949 -0.68536735 -1.0000217 -1.0690224 -0.662514 0.090249062 0.71775675 0.67719412 -0.012283802 -0.48411202 -0.78449416][-1.8299692 -1.5354795 -1.4376824 -1.1476309 -0.67429948 -0.71617961 -1.0475838 -1.0309958 -0.57882524 0.12915325 0.68768835 0.52232504 -0.1436305 -0.4818542 -0.73735976][-2.3102679 -2.0018981 -1.7857785 -1.2858953 -0.68004847 -0.758425 -1.0741074 -0.93178225 -0.42916298 0.18271208 0.5055604 0.18572712 -0.35656786 -0.49892235 -0.71140909][-2.6036949 -2.2771235 -2.0194445 -1.3668666 -0.63297176 -0.63787627 -0.86256218 -0.67853689 -0.23515558 0.19121408 0.25934362 -0.058473587 -0.39826822 -0.46863985 -0.75655389][-2.5072012 -2.239799 -2.0395527 -1.3115208 -0.47582054 -0.35277367 -0.47385621 -0.3281312 -0.034688473 0.1117363 0.0050635338 -0.15239 -0.28023529 -0.44397807 -0.92797184][-2.2453489 -2.0656357 -1.848372 -1.0476298 -0.16860485 0.034789085 -0.028487682 0.065782547 0.14650202 -0.060586452 -0.31762505 -0.30725288 -0.27723455 -0.5359261 -1.1680701][-2.0378606 -2.042593 -1.7889099 -0.87756371 0.0559783 0.348979 0.36445189 0.43845415 0.29717684 -0.21733236 -0.55846906 -0.48106742 -0.38849115 -0.65224385 -1.2443986][-1.8291247 -2.0418499 -1.8124628 -0.88645959 0.037963867 0.38052416 0.52665424 0.65487289 0.37076139 -0.21489096 -0.46799302 -0.40590096 -0.39586163 -0.66354942 -1.0956497][-1.651859 -1.9400902 -1.7686388 -0.97281814 -0.20237303 0.12282038 0.41281366 0.6068182 0.28391647 -0.14401627 -0.23893785 -0.23807096 -0.34668207 -0.60093665 -0.85927868]]...]
INFO - root - 2017-12-07 06:09:02.793113: step 12010, loss = 0.64, batch loss = 0.57 (10.4 examples/sec; 0.769 sec/batch; 68h:28m:02s remains)
INFO - root - 2017-12-07 06:09:10.460450: step 12020, loss = 0.77, batch loss = 0.70 (9.8 examples/sec; 0.815 sec/batch; 72h:34m:10s remains)
INFO - root - 2017-12-07 06:09:17.908752: step 12030, loss = 0.73, batch loss = 0.65 (10.8 examples/sec; 0.744 sec/batch; 66h:12m:48s remains)
INFO - root - 2017-12-07 06:09:25.629969: step 12040, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.773 sec/batch; 68h:47m:23s remains)
INFO - root - 2017-12-07 06:09:33.322300: step 12050, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.767 sec/batch; 68h:16m:35s remains)
INFO - root - 2017-12-07 06:09:41.059051: step 12060, loss = 0.84, batch loss = 0.76 (10.4 examples/sec; 0.770 sec/batch; 68h:32m:58s remains)
INFO - root - 2017-12-07 06:09:48.681845: step 12070, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.751 sec/batch; 66h:48m:45s remains)
INFO - root - 2017-12-07 06:09:56.283299: step 12080, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.768 sec/batch; 68h:21m:08s remains)
INFO - root - 2017-12-07 06:10:03.929598: step 12090, loss = 0.97, batch loss = 0.90 (10.6 examples/sec; 0.753 sec/batch; 67h:03m:32s remains)
INFO - root - 2017-12-07 06:10:11.655216: step 12100, loss = 0.71, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 70h:29m:47s remains)
2017-12-07 06:10:12.247534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7077632 -2.8716793 -2.8525159 -2.7581491 -2.7774177 -2.4124889 -1.8600843 -1.466959 -1.3530021 -1.7361386 -2.341975 -2.859832 -3.2475471 -3.4102988 -3.5132377][-2.5522072 -2.7620842 -2.7508442 -2.6814606 -2.8144484 -2.6273704 -2.2606483 -1.9808738 -1.7721844 -1.8544328 -2.0832791 -2.3093648 -2.5840564 -2.8025322 -3.037641][-2.4670095 -2.641084 -2.512229 -2.3823171 -2.5792046 -2.6588659 -2.6560984 -2.7041559 -2.5787807 -2.4754906 -2.3667903 -2.23792 -2.221061 -2.2389967 -2.3624024][-2.5812366 -2.8479161 -2.6410177 -2.3508294 -2.4108403 -2.5375371 -2.7705398 -3.1540346 -3.306653 -3.3381085 -3.1532807 -2.7829909 -2.4395339 -2.142514 -1.9967132][-2.6838312 -3.1163511 -3.0539818 -2.7400546 -2.5748539 -2.4166067 -2.4784408 -2.8817987 -3.2947724 -3.7015631 -3.7931986 -3.5138826 -3.0510073 -2.5300703 -2.1133971][-2.6309304 -3.0548773 -3.1760752 -2.9784434 -2.7131617 -2.2978117 -1.9592688 -2.0003881 -2.40057 -3.1129241 -3.5959504 -3.7308135 -3.5636334 -3.182198 -2.6915927][-2.58904 -2.8523674 -3.0238183 -2.9259844 -2.6490507 -2.1149631 -1.3764617 -0.88525128 -1.0492463 -1.8035285 -2.5283294 -3.0714071 -3.3556788 -3.3983192 -3.2042511][-2.6681862 -2.7687321 -2.91639 -2.8520851 -2.5732503 -2.0040319 -0.96337271 -0.0012364388 0.14115191 -0.42806959 -1.1949043 -1.9958596 -2.5791159 -2.9812312 -3.1733146][-2.6773872 -2.7336659 -2.85174 -2.8450212 -2.6874161 -2.2406359 -1.1579518 -0.012645245 0.47198296 0.37214041 -0.13816261 -0.96700263 -1.7072663 -2.2970774 -2.7054768][-2.6130512 -2.6373138 -2.7241 -2.7841651 -2.7551785 -2.4624648 -1.5812581 -0.65035415 -0.13782644 0.19767952 0.17467117 -0.36679602 -1.0577304 -1.6837523 -2.1586866][-2.6928444 -2.5993907 -2.5742383 -2.6146274 -2.6051013 -2.3710058 -1.6772039 -1.07776 -0.78817964 -0.43151665 -0.27061367 -0.55597925 -0.9886694 -1.3913286 -1.7721324][-3.0121393 -2.8274143 -2.6669121 -2.5760341 -2.4679132 -2.195713 -1.6125119 -1.1969755 -1.0517511 -0.85732937 -0.82352424 -1.120435 -1.3908226 -1.4769998 -1.6202776][-3.3008361 -3.1527028 -2.9496698 -2.7295959 -2.5018883 -2.1800005 -1.6567063 -1.2465146 -1.0365472 -0.95209455 -1.1495047 -1.6341598 -1.8781435 -1.7689209 -1.6916029][-3.4068336 -3.3767705 -3.2477546 -3.02909 -2.7873168 -2.4796696 -2.0079799 -1.4841511 -1.0773222 -0.97038889 -1.3148768 -1.965287 -2.2643926 -2.1231587 -1.9485967][-3.4491677 -3.4710059 -3.4138145 -3.284339 -3.1572957 -2.9373446 -2.5183764 -1.8805723 -1.3329275 -1.172899 -1.4741349 -2.0836885 -2.3619282 -2.2878783 -2.1688504]]...]
INFO - root - 2017-12-07 06:10:19.877082: step 12110, loss = 1.00, batch loss = 0.93 (10.2 examples/sec; 0.783 sec/batch; 69h:38m:59s remains)
INFO - root - 2017-12-07 06:10:27.476656: step 12120, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.762 sec/batch; 67h:46m:09s remains)
INFO - root - 2017-12-07 06:10:35.008377: step 12130, loss = 0.79, batch loss = 0.71 (10.4 examples/sec; 0.771 sec/batch; 68h:37m:38s remains)
INFO - root - 2017-12-07 06:10:42.659636: step 12140, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.750 sec/batch; 66h:46m:59s remains)
INFO - root - 2017-12-07 06:10:50.271293: step 12150, loss = 0.73, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 68h:16m:30s remains)
INFO - root - 2017-12-07 06:10:58.044053: step 12160, loss = 0.81, batch loss = 0.74 (10.1 examples/sec; 0.795 sec/batch; 70h:46m:09s remains)
INFO - root - 2017-12-07 06:11:05.741469: step 12170, loss = 0.64, batch loss = 0.57 (10.4 examples/sec; 0.768 sec/batch; 68h:19m:23s remains)
INFO - root - 2017-12-07 06:11:13.487337: step 12180, loss = 0.71, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 70h:15m:17s remains)
INFO - root - 2017-12-07 06:11:21.219280: step 12190, loss = 0.68, batch loss = 0.60 (10.6 examples/sec; 0.751 sec/batch; 66h:50m:19s remains)
INFO - root - 2017-12-07 06:11:28.920701: step 12200, loss = 0.83, batch loss = 0.75 (10.7 examples/sec; 0.749 sec/batch; 66h:39m:54s remains)
2017-12-07 06:11:29.512500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3538105 -3.4888608 -3.6031168 -3.7017281 -3.757679 -3.7519157 -3.7120717 -3.6908238 -3.6236463 -3.5256391 -3.4955347 -3.4982374 -3.4793358 -3.4236312 -3.3238184][-3.6498356 -3.7780044 -3.8588066 -3.9331148 -3.9762495 -3.9244194 -3.8141046 -3.7810585 -3.7222767 -3.6207652 -3.6042695 -3.6376889 -3.6292708 -3.5693164 -3.441833][-3.734699 -3.8085008 -3.7909665 -3.8018777 -3.846761 -3.7754736 -3.6159983 -3.6363051 -3.7033687 -3.6967003 -3.7552271 -3.88204 -3.9239569 -3.8535106 -3.6566489][-3.6194134 -3.6368339 -3.4867251 -3.3774688 -3.3816628 -3.2623963 -3.0144153 -3.0595708 -3.2839866 -3.4010448 -3.5460134 -3.8020415 -3.9711337 -3.9552407 -3.7254457][-3.3502212 -3.3526812 -3.1041241 -2.8726339 -2.8072619 -2.6084242 -2.1973963 -2.1466279 -2.4690156 -2.6956635 -2.9060671 -3.2700787 -3.5671182 -3.6310406 -3.4033072][-2.9642448 -2.9355052 -2.583061 -2.2129192 -2.0548992 -1.7734799 -1.1697869 -0.95394754 -1.3744047 -1.8028529 -2.1654804 -2.6627049 -3.0784714 -3.2272315 -3.0190883][-2.5939655 -2.5006795 -2.0174992 -1.4632258 -1.1819773 -0.81857324 -0.0508399 0.35985374 -0.11208248 -0.76095581 -1.3090315 -1.9195466 -2.4441996 -2.7382722 -2.6612191][-2.2837765 -2.1043189 -1.5319481 -0.84605813 -0.52006388 -0.22835493 0.48416185 0.97603416 0.59654856 -0.069922447 -0.63977075 -1.2005789 -1.6914694 -2.0483429 -2.1338036][-2.1346939 -1.8845179 -1.3045957 -0.60838795 -0.3659544 -0.34108686 0.046467781 0.43761635 0.2730937 -0.16162205 -0.53669071 -0.87408233 -1.1841176 -1.4563158 -1.6078436][-2.1508241 -1.9022813 -1.4309278 -0.84985185 -0.72728395 -0.94595766 -0.89294052 -0.64525676 -0.61546493 -0.76044726 -0.88317752 -0.97510481 -1.0838246 -1.2162108 -1.3517635][-2.2453284 -2.0636561 -1.7591639 -1.3509133 -1.3011849 -1.5975182 -1.7294226 -1.6174583 -1.5205815 -1.4917834 -1.433533 -1.3469458 -1.2860944 -1.2608695 -1.3146002][-2.32519 -2.2299855 -2.098033 -1.8698595 -1.8405476 -2.0694838 -2.224467 -2.2081895 -2.1707127 -2.1450636 -2.0695896 -1.9468045 -1.7941356 -1.6342525 -1.5613496][-2.4059341 -2.3740563 -2.3536868 -2.2459164 -2.2089336 -2.3223805 -2.4200513 -2.4484048 -2.4874907 -2.5365381 -2.5497139 -2.510129 -2.4046218 -2.2357225 -2.0975449][-2.4977169 -2.4899764 -2.5055196 -2.4631817 -2.4305806 -2.4686921 -2.5151997 -2.550097 -2.6134388 -2.700057 -2.7714832 -2.8046303 -2.7784493 -2.6817679 -2.5648007][-2.5805953 -2.5794654 -2.5994768 -2.5900035 -2.5780454 -2.5943847 -2.6192384 -2.6476157 -2.6971335 -2.7689979 -2.8350148 -2.8728795 -2.8680439 -2.8174677 -2.7437143]]...]
INFO - root - 2017-12-07 06:11:37.216036: step 12210, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.783 sec/batch; 69h:37m:07s remains)
INFO - root - 2017-12-07 06:11:44.973023: step 12220, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.746 sec/batch; 66h:19m:43s remains)
INFO - root - 2017-12-07 06:11:52.462179: step 12230, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.758 sec/batch; 67h:23m:50s remains)
INFO - root - 2017-12-07 06:12:00.114421: step 12240, loss = 0.55, batch loss = 0.48 (10.8 examples/sec; 0.742 sec/batch; 66h:00m:13s remains)
INFO - root - 2017-12-07 06:12:07.767942: step 12250, loss = 0.93, batch loss = 0.86 (10.3 examples/sec; 0.775 sec/batch; 68h:55m:49s remains)
INFO - root - 2017-12-07 06:12:15.478474: step 12260, loss = 0.84, batch loss = 0.76 (10.4 examples/sec; 0.767 sec/batch; 68h:12m:19s remains)
INFO - root - 2017-12-07 06:12:23.156369: step 12270, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.761 sec/batch; 67h:42m:46s remains)
INFO - root - 2017-12-07 06:12:30.818290: step 12280, loss = 1.00, batch loss = 0.92 (10.5 examples/sec; 0.765 sec/batch; 68h:03m:43s remains)
INFO - root - 2017-12-07 06:12:38.548530: step 12290, loss = 0.65, batch loss = 0.58 (10.0 examples/sec; 0.802 sec/batch; 71h:19m:34s remains)
INFO - root - 2017-12-07 06:12:46.162970: step 12300, loss = 0.78, batch loss = 0.70 (10.6 examples/sec; 0.754 sec/batch; 67h:01m:42s remains)
2017-12-07 06:12:46.780432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.056109 -4.5476723 -3.5818243 -2.1232584 -0.64035511 -0.14189768 -1.3670208 -3.6084907 -5.3332791 -5.32902 -4.0017319 -2.613728 -2.3419392 -3.2236042 -4.1710567][-5.0236297 -4.1634083 -2.7912102 -0.95962119 0.55913973 0.78051043 -0.653389 -2.9251912 -4.4798546 -4.3681707 -3.1677718 -2.2389021 -2.4883268 -3.5496361 -4.3373637][-5.0467677 -3.9315817 -2.305491 -0.29529476 1.0645947 1.045239 -0.3584156 -2.4240572 -3.60184 -3.264061 -2.2373245 -1.8483543 -2.6193225 -3.864459 -4.5536237][-5.0143361 -3.7194154 -2.0095878 -0.12059402 0.90395832 0.79315376 -0.21279812 -1.8109376 -2.5057478 -1.9282606 -1.1941679 -1.3657718 -2.61974 -4.0247355 -4.6326919][-5.0204911 -3.6934347 -2.0168014 -0.36177158 0.412529 0.37229156 -0.06912756 -1.0162067 -1.1497483 -0.40107203 -0.15017653 -0.93712854 -2.6358652 -4.1574535 -4.660677][-5.275166 -4.1073275 -2.5328991 -0.97438645 -0.0670166 0.21485424 0.28807688 -0.16324234 0.035943031 0.62086058 0.22840834 -1.0115783 -2.8725495 -4.3227878 -4.6821237][-5.5760684 -4.7467103 -3.3261991 -1.6910901 -0.30912924 0.55388451 1.0348601 0.74412775 0.69687843 0.59847355 -0.45654702 -1.8461642 -3.4706554 -4.5765738 -4.6717114][-5.9011183 -5.4342384 -4.1944628 -2.4500313 -0.57791829 0.98312378 1.9030981 1.6212234 0.92717648 -0.12267876 -1.8055735 -3.181798 -4.3178897 -4.90641 -4.6622281][-6.2206545 -6.07969 -5.0404067 -3.2574117 -1.0406888 1.1525798 2.4351025 2.142756 0.857697 -1.0368152 -3.1891103 -4.51375 -5.1433835 -5.2255144 -4.7401114][-6.2700682 -6.3382463 -5.5219431 -3.8219161 -1.4571595 1.0887117 2.4950752 2.11091 0.44930744 -1.9188182 -4.25235 -5.451014 -5.6570454 -5.3849468 -4.8617706][-6.0764275 -6.2025938 -5.5395064 -4.0014362 -1.7130661 0.80072069 2.0069289 1.4140711 -0.36967421 -2.7708914 -4.8836102 -5.7680874 -5.6355624 -5.19725 -4.8278155][-5.7380748 -5.8567247 -5.2982903 -3.9629695 -1.9353533 0.20757103 1.0684986 0.34679747 -1.3815496 -3.5529816 -5.2008505 -5.6469107 -5.278316 -4.8575492 -4.7265334][-5.1994538 -5.3303661 -4.8972273 -3.8439155 -2.2279518 -0.613497 -0.080513954 -0.76123452 -2.1964347 -3.8992233 -4.9863524 -5.1014266 -4.7527461 -4.5502782 -4.6853056][-4.5402832 -4.6946931 -4.4346447 -3.7724447 -2.6837435 -1.6379304 -1.4011805 -1.9466176 -2.9578533 -3.9951439 -4.4175076 -4.2756257 -4.084796 -4.1696825 -4.6016579][-3.8807328 -4.0213375 -3.9003537 -3.5877881 -3.0134547 -2.4849529 -2.513309 -2.9446354 -3.5815914 -4.0097461 -3.8302021 -3.52236 -3.4935198 -3.8081779 -4.4527922]]...]
INFO - root - 2017-12-07 06:12:54.395210: step 12310, loss = 1.09, batch loss = 1.01 (10.4 examples/sec; 0.771 sec/batch; 68h:32m:16s remains)
INFO - root - 2017-12-07 06:13:02.129572: step 12320, loss = 0.88, batch loss = 0.80 (10.5 examples/sec; 0.763 sec/batch; 67h:51m:10s remains)
INFO - root - 2017-12-07 06:13:09.548870: step 12330, loss = 0.87, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 67h:05m:52s remains)
INFO - root - 2017-12-07 06:13:17.284875: step 12340, loss = 1.05, batch loss = 0.98 (10.2 examples/sec; 0.783 sec/batch; 69h:38m:14s remains)
INFO - root - 2017-12-07 06:13:25.077679: step 12350, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.770 sec/batch; 68h:27m:05s remains)
INFO - root - 2017-12-07 06:13:32.698579: step 12360, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.756 sec/batch; 67h:13m:09s remains)
INFO - root - 2017-12-07 06:13:40.312439: step 12370, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.766 sec/batch; 68h:09m:19s remains)
INFO - root - 2017-12-07 06:13:48.046048: step 12380, loss = 0.69, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 69h:42m:09s remains)
INFO - root - 2017-12-07 06:13:55.767743: step 12390, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.757 sec/batch; 67h:19m:44s remains)
INFO - root - 2017-12-07 06:14:03.534641: step 12400, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 68h:35m:54s remains)
2017-12-07 06:14:04.196616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4779024 -3.3657341 -3.3105688 -3.2732558 -3.2538857 -3.3033316 -3.3734679 -3.3980141 -3.4291229 -3.4990525 -3.575691 -3.6162479 -3.6308706 -3.6217604 -3.5692091][-3.429893 -3.3909996 -3.3887682 -3.3829038 -3.3909783 -3.4864013 -3.5830593 -3.5679212 -3.4934456 -3.4237068 -3.3579478 -3.3025 -3.3117905 -3.3757195 -3.4337018][-3.8555405 -3.8159585 -3.7845647 -3.7138727 -3.6729515 -3.7335637 -3.7838712 -3.7054145 -3.5516424 -3.4058404 -3.2660913 -3.1494226 -3.158258 -3.2844987 -3.4117997][-4.2383842 -4.1425357 -4.0714169 -3.9595578 -3.9090924 -3.9363313 -3.9257607 -3.8049297 -3.6325498 -3.5117543 -3.4601498 -3.4256487 -3.5186484 -3.7447839 -3.9068127][-4.0038185 -3.8368821 -3.7653484 -3.7018843 -3.7318864 -3.7735763 -3.7228572 -3.5825181 -3.4333498 -3.3931413 -3.5138958 -3.6558592 -3.8868508 -4.2023354 -4.3429947][-3.1938868 -3.0161352 -2.9814649 -3.0040669 -3.1479483 -3.2227447 -3.1115222 -2.8845921 -2.6716352 -2.5990481 -2.7661865 -2.9992557 -3.3092933 -3.6821933 -3.8128862][-2.2948039 -2.1510112 -2.1578529 -2.2448606 -2.4451642 -2.5195806 -2.3371952 -2.0021935 -1.6729865 -1.4618742 -1.4906309 -1.6141393 -1.8296726 -2.1377594 -2.2523077][-2.3475804 -2.3195734 -2.3786633 -2.4810491 -2.6331484 -2.6592274 -2.4452507 -2.0515609 -1.6193559 -1.227304 -0.99543357 -0.84507227 -0.799247 -0.886235 -0.8895247][-3.2857621 -3.3824368 -3.4917474 -3.5805888 -3.6685493 -3.6881013 -3.6010916 -3.3643489 -3.0175796 -2.5431926 -2.0528193 -1.5685544 -1.1799471 -0.97616124 -0.84706426][-3.9103889 -4.0634403 -4.1772523 -4.2434087 -4.3013253 -4.3422103 -4.3954782 -4.3666191 -4.2381167 -3.9255562 -3.4502845 -2.9149442 -2.4137659 -2.0348115 -1.7650437][-3.8678374 -4.0158844 -4.1599431 -4.2481613 -4.3201404 -4.4039154 -4.5240116 -4.56532 -4.5418262 -4.4065151 -4.1063952 -3.723557 -3.3076644 -2.8960123 -2.5072169][-3.8066251 -3.9335594 -4.1079426 -4.2171936 -4.2876334 -4.3867559 -4.503602 -4.5001259 -4.4405971 -4.3596177 -4.1768818 -3.9163902 -3.5797346 -3.183918 -2.7518287][-3.983269 -4.1087809 -4.2744765 -4.3594685 -4.3757992 -4.4446826 -4.5544114 -4.5490189 -4.4749517 -4.3945317 -4.2685747 -4.052166 -3.7047274 -3.2844939 -2.8382587][-4.2449331 -4.3192225 -4.364275 -4.2875123 -4.13387 -4.0863876 -4.1805897 -4.2958393 -4.4093575 -4.5140333 -4.5805268 -4.5032969 -4.2034683 -3.7809639 -3.303951][-4.3661489 -4.4311714 -4.3660836 -4.0935636 -3.725208 -3.4655149 -3.4454064 -3.601985 -3.9046121 -4.3072872 -4.6818724 -4.8802729 -4.7997694 -4.5358014 -4.0984507]]...]
INFO - root - 2017-12-07 06:14:11.871381: step 12410, loss = 0.97, batch loss = 0.90 (10.2 examples/sec; 0.786 sec/batch; 69h:54m:20s remains)
INFO - root - 2017-12-07 06:14:19.448434: step 12420, loss = 0.77, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 67h:47m:00s remains)
INFO - root - 2017-12-07 06:14:26.844616: step 12430, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.760 sec/batch; 67h:35m:21s remains)
INFO - root - 2017-12-07 06:14:34.637260: step 12440, loss = 0.58, batch loss = 0.50 (10.3 examples/sec; 0.774 sec/batch; 68h:51m:08s remains)
INFO - root - 2017-12-07 06:14:42.300617: step 12450, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.770 sec/batch; 68h:28m:47s remains)
INFO - root - 2017-12-07 06:14:49.929646: step 12460, loss = 0.77, batch loss = 0.70 (10.9 examples/sec; 0.735 sec/batch; 65h:21m:36s remains)
INFO - root - 2017-12-07 06:14:57.653496: step 12470, loss = 0.64, batch loss = 0.56 (10.4 examples/sec; 0.772 sec/batch; 68h:39m:26s remains)
INFO - root - 2017-12-07 06:15:05.401086: step 12480, loss = 0.73, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 69h:42m:47s remains)
INFO - root - 2017-12-07 06:15:13.034317: step 12490, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.770 sec/batch; 68h:24m:11s remains)
INFO - root - 2017-12-07 06:15:20.704021: step 12500, loss = 0.71, batch loss = 0.64 (10.7 examples/sec; 0.750 sec/batch; 66h:42m:01s remains)
2017-12-07 06:15:21.378037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9342918 -3.5217884 -3.9197135 -3.9830675 -3.738673 -3.3178458 -2.9330773 -2.6147008 -2.4864035 -2.65815 -2.6340413 -2.2382004 -2.1127689 -2.3379815 -2.5213585][-2.6350079 -3.1128321 -3.4170561 -3.4026532 -3.1437814 -2.7479737 -2.3595526 -2.0137382 -1.9215651 -2.1311369 -1.9663143 -1.3501749 -1.0428493 -1.2207177 -1.5184796][-2.42763 -2.7673073 -2.9577503 -2.8981524 -2.7096136 -2.4193411 -2.0327957 -1.6912522 -1.7429364 -2.0885105 -1.9381218 -1.2120082 -0.66914988 -0.65019155 -0.89915419][-2.5698588 -2.6663659 -2.5362732 -2.3105054 -2.1837165 -2.0641122 -1.7822709 -1.5657749 -1.8584967 -2.41646 -2.4177032 -1.702657 -0.96953106 -0.74036288 -0.80452704][-3.214993 -3.1826649 -2.720571 -2.3087547 -2.171155 -2.085314 -1.7818274 -1.5856745 -2.0282078 -2.7200556 -2.8313897 -2.1483068 -1.3432539 -1.001724 -0.95479226][-3.9275422 -3.8369095 -3.2249289 -2.8050036 -2.6586738 -2.3978317 -1.7942441 -1.4033375 -1.8996344 -2.75361 -3.0098 -2.4643302 -1.7844226 -1.5185108 -1.4845207][-4.0915403 -3.8001289 -3.087081 -2.7119946 -2.5736151 -2.1277075 -1.1778631 -0.56751561 -1.1937633 -2.3675823 -2.9032197 -2.6629629 -2.2920675 -2.2056491 -2.1777141][-3.5936303 -3.0638714 -2.3177111 -1.9298661 -1.7410915 -1.1247239 0.11517286 0.9342432 0.14584494 -1.3592687 -2.1817577 -2.2730393 -2.2392151 -2.3571019 -2.3428979][-3.0230532 -2.4419904 -1.7774971 -1.388634 -1.1616225 -0.47848105 0.87560749 1.8297477 1.0315857 -0.49843287 -1.4050994 -1.7167006 -1.9591444 -2.2544022 -2.251724][-2.701822 -2.25332 -1.7838488 -1.4982839 -1.3957353 -0.8974762 0.24464893 1.1115675 0.479702 -0.78024483 -1.5510983 -1.9118803 -2.2576482 -2.5694757 -2.5144923][-2.6430612 -2.3189759 -2.0159659 -1.8561301 -1.9098713 -1.6789248 -0.94294143 -0.3581171 -0.80974865 -1.7020001 -2.2112403 -2.4746213 -2.7328725 -2.8989291 -2.7585413][-2.9165506 -2.5562139 -2.2033043 -2.0156691 -2.1082277 -2.0712886 -1.708365 -1.4181867 -1.7216675 -2.2622781 -2.5064483 -2.5970612 -2.6838813 -2.6450369 -2.4545541][-3.4556894 -2.983521 -2.4604571 -2.1054239 -2.0737083 -2.0632429 -1.9328537 -1.8556311 -2.0803044 -2.3864233 -2.469851 -2.4478948 -2.413996 -2.2145634 -1.9768405][-3.8986492 -3.343756 -2.7408576 -2.3012176 -2.1344578 -2.0614209 -2.0258191 -2.0798507 -2.3072259 -2.5113921 -2.5276377 -2.4850667 -2.4266555 -2.1561394 -1.8897083][-3.9438102 -3.4122229 -2.9207551 -2.5988989 -2.4456153 -2.38488 -2.3748915 -2.4702652 -2.6729808 -2.800386 -2.7802334 -2.735728 -2.6817617 -2.3769767 -2.1161044]]...]
INFO - root - 2017-12-07 06:15:29.069238: step 12510, loss = 0.97, batch loss = 0.89 (10.7 examples/sec; 0.746 sec/batch; 66h:18m:21s remains)
INFO - root - 2017-12-07 06:15:36.872639: step 12520, loss = 0.87, batch loss = 0.80 (10.1 examples/sec; 0.795 sec/batch; 70h:42m:21s remains)
INFO - root - 2017-12-07 06:15:44.404117: step 12530, loss = 0.87, batch loss = 0.80 (10.1 examples/sec; 0.793 sec/batch; 70h:27m:35s remains)
INFO - root - 2017-12-07 06:15:52.109171: step 12540, loss = 0.91, batch loss = 0.84 (10.2 examples/sec; 0.781 sec/batch; 69h:22m:43s remains)
INFO - root - 2017-12-07 06:15:59.844446: step 12550, loss = 0.93, batch loss = 0.85 (10.3 examples/sec; 0.775 sec/batch; 68h:55m:03s remains)
INFO - root - 2017-12-07 06:16:07.531415: step 12560, loss = 0.79, batch loss = 0.72 (10.0 examples/sec; 0.799 sec/batch; 71h:01m:37s remains)
INFO - root - 2017-12-07 06:16:15.136986: step 12570, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.758 sec/batch; 67h:19m:14s remains)
INFO - root - 2017-12-07 06:16:22.900735: step 12580, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.758 sec/batch; 67h:20m:03s remains)
INFO - root - 2017-12-07 06:16:30.576048: step 12590, loss = 0.66, batch loss = 0.59 (10.5 examples/sec; 0.758 sec/batch; 67h:23m:15s remains)
INFO - root - 2017-12-07 06:16:38.139975: step 12600, loss = 0.77, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 69h:20m:01s remains)
2017-12-07 06:16:38.871175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5606847 -2.1942852 -1.9481981 -1.9722097 -2.2758584 -2.5724711 -2.7831936 -2.8838112 -2.6694441 -1.8991208 -1.1924076 -1.3226368 -1.9384649 -2.2621961 -2.4511042][-2.8377976 -2.2262952 -1.7269514 -1.6686628 -2.0362031 -2.3627956 -2.6065788 -2.79528 -2.5744162 -1.7168119 -1.0108361 -1.1533108 -1.7077708 -2.027261 -2.366905][-2.7410231 -2.1713104 -1.5182526 -1.2917018 -1.4652882 -1.5534899 -1.6858678 -2.0340564 -1.9982901 -1.2701814 -0.736572 -0.99543047 -1.4619713 -1.7574129 -2.199137][-2.9414387 -2.6425853 -1.9816656 -1.5229256 -1.2523851 -0.8755641 -0.80712104 -1.3354309 -1.5756357 -1.1411364 -0.92116094 -1.3404171 -1.6216922 -1.7466435 -2.1258574][-3.4063315 -3.2757111 -2.6253896 -2.0529487 -1.5076108 -0.84002995 -0.70640731 -1.3646343 -1.7335463 -1.4538476 -1.460603 -1.9361982 -2.0081134 -1.9428127 -2.171365][-3.7646873 -3.5308855 -2.7562406 -2.2107639 -1.7178693 -1.0937698 -1.0975645 -1.8490672 -2.2021704 -1.8817976 -1.9518769 -2.3842151 -2.3465266 -2.1850088 -2.2487726][-3.7040086 -3.3515582 -2.515944 -2.0708427 -1.716902 -1.1680365 -1.2587619 -2.0222201 -2.3340106 -2.0141191 -2.1857045 -2.6904485 -2.7668598 -2.6277537 -2.4744556][-3.4143982 -3.0238905 -2.2731221 -1.9695709 -1.6935441 -1.1053939 -1.089798 -1.730175 -1.9405651 -1.6948826 -2.0146413 -2.6988626 -3.0872602 -3.0872731 -2.7546105][-3.3501258 -2.9220934 -2.2663753 -2.0464344 -1.773555 -1.1640837 -1.1126409 -1.6492581 -1.7578075 -1.476742 -1.7110567 -2.4035606 -3.028652 -3.1897688 -2.8199806][-3.5597844 -3.1984429 -2.6563406 -2.447005 -2.1579776 -1.6475096 -1.6679158 -2.1096129 -2.1367474 -1.7722766 -1.7840807 -2.33251 -3.036428 -3.2839084 -2.9626026][-3.7034998 -3.5072942 -3.1200409 -2.912591 -2.6060042 -2.2350254 -2.3214211 -2.6286795 -2.5871964 -2.2048929 -2.0837283 -2.5358396 -3.2756782 -3.6044106 -3.4005303][-3.765656 -3.644275 -3.327662 -3.0847263 -2.7970371 -2.5638916 -2.6749234 -2.8760164 -2.8144512 -2.5121288 -2.4065309 -2.8199828 -3.5198884 -3.8832033 -3.8012846][-3.936121 -3.8293767 -3.5569205 -3.3047676 -3.0808668 -2.953289 -3.0253184 -3.1115122 -3.0453691 -2.871793 -2.843045 -3.1903548 -3.7432184 -4.0336862 -3.9714532][-3.9644227 -3.8886561 -3.7065964 -3.529876 -3.3938515 -3.3333254 -3.3522768 -3.3474281 -3.27669 -3.1940207 -3.2033629 -3.4436545 -3.8103185 -3.9892113 -3.9147108][-3.7380681 -3.7076402 -3.6286094 -3.5433297 -3.4789338 -3.456439 -3.4634736 -3.4500427 -3.416574 -3.3987544 -3.4305997 -3.5800676 -3.7831416 -3.8568249 -3.7703211]]...]
INFO - root - 2017-12-07 06:16:46.575831: step 12610, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.775 sec/batch; 68h:49m:55s remains)
INFO - root - 2017-12-07 06:16:54.201607: step 12620, loss = 0.75, batch loss = 0.67 (10.6 examples/sec; 0.757 sec/batch; 67h:16m:25s remains)
INFO - root - 2017-12-07 06:17:01.572964: step 12630, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.772 sec/batch; 68h:36m:34s remains)
INFO - root - 2017-12-07 06:17:09.201102: step 12640, loss = 0.70, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 69h:43m:36s remains)
INFO - root - 2017-12-07 06:17:16.889541: step 12650, loss = 0.90, batch loss = 0.82 (10.1 examples/sec; 0.790 sec/batch; 70h:13m:33s remains)
INFO - root - 2017-12-07 06:17:24.430740: step 12660, loss = 0.90, batch loss = 0.83 (10.4 examples/sec; 0.767 sec/batch; 68h:06m:16s remains)
INFO - root - 2017-12-07 06:17:32.083969: step 12670, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.750 sec/batch; 66h:35m:43s remains)
INFO - root - 2017-12-07 06:17:39.751177: step 12680, loss = 1.09, batch loss = 1.02 (10.8 examples/sec; 0.744 sec/batch; 66h:05m:10s remains)
INFO - root - 2017-12-07 06:17:47.413114: step 12690, loss = 0.92, batch loss = 0.85 (10.5 examples/sec; 0.763 sec/batch; 67h:48m:43s remains)
INFO - root - 2017-12-07 06:17:55.101752: step 12700, loss = 1.00, batch loss = 0.93 (10.2 examples/sec; 0.788 sec/batch; 69h:58m:56s remains)
2017-12-07 06:17:55.752829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6230986 -3.9537437 -4.1013741 -4.1681933 -4.2874126 -4.350924 -4.3250017 -4.2412214 -4.2267785 -4.12455 -3.7920194 -3.0866795 -2.4408555 -2.2942488 -2.5973959][-4.271246 -4.5979977 -4.6604142 -4.688127 -4.8554125 -4.9838371 -4.9719319 -4.8330278 -4.7015395 -4.4142237 -3.8547673 -2.993135 -2.3306136 -2.3051312 -2.7242117][-4.4912548 -4.7261252 -4.6768613 -4.6581278 -4.8603773 -5.0162416 -4.9850593 -4.78984 -4.5859752 -4.2443261 -3.6838944 -2.9188857 -2.4054675 -2.5113096 -2.9317095][-4.5588412 -4.7036262 -4.5421586 -4.4323287 -4.5998707 -4.6788454 -4.55652 -4.3014631 -4.1011677 -3.8075511 -3.3603992 -2.799798 -2.4851356 -2.6786404 -3.0329094][-4.3097057 -4.3325357 -4.0414839 -3.8526175 -4.0144963 -4.0443382 -3.8443613 -3.5315173 -3.3289785 -3.0440829 -2.6552052 -2.2601178 -2.0785847 -2.2908843 -2.6210365][-3.7093487 -3.592062 -3.2396624 -3.0587337 -3.2526577 -3.2371097 -2.8818333 -2.3446257 -2.039875 -1.832459 -1.6044402 -1.4264114 -1.3577008 -1.5760658 -1.9444993][-2.9390407 -2.7429237 -2.4458251 -2.3037496 -2.4061277 -2.1443152 -1.4380481 -0.53710389 -0.24855566 -0.43414879 -0.65943956 -0.852401 -0.92264628 -1.055455 -1.3061657][-2.1877058 -1.9494667 -1.6638362 -1.4481287 -1.3034654 -0.80740714 0.0508523 0.96844721 0.79381132 -0.032805443 -0.71346545 -1.1064861 -1.1624043 -1.0654855 -1.0229738][-1.5336964 -1.2194855 -0.90742755 -0.6825068 -0.5118072 -0.22468281 0.1567688 0.44529152 -0.08322525 -1.0655897 -1.7380724 -2.0108151 -1.9393244 -1.6933289 -1.4940827][-1.1992664 -0.99789238 -0.90969992 -0.95330238 -1.0086269 -1.0234635 -1.0403068 -1.1093199 -1.6454508 -2.4365263 -2.9141655 -3.0247571 -2.877346 -2.6025434 -2.438467][-1.5629253 -1.6196625 -1.8188078 -2.0752165 -2.2032394 -2.243516 -2.3150315 -2.4331007 -2.8407912 -3.3951683 -3.7100403 -3.7330844 -3.5428224 -3.2716224 -3.1855903][-2.3242414 -2.5058193 -2.7154722 -2.8662472 -2.8516636 -2.8040357 -2.8667271 -3.0198379 -3.3542657 -3.7331963 -3.9040635 -3.8447781 -3.6300459 -3.3934171 -3.3740106][-2.6794071 -2.8283441 -2.9312983 -2.93264 -2.8142843 -2.7535012 -2.8128533 -2.931495 -3.1119323 -3.2800684 -3.3028247 -3.1816425 -3.0203528 -2.9259324 -3.0123365][-2.7019978 -2.7958162 -2.8404183 -2.8127782 -2.7055883 -2.6644254 -2.6846228 -2.6914086 -2.6905026 -2.6661239 -2.6159396 -2.5198221 -2.4814272 -2.5293114 -2.6786342][-2.8557498 -2.926435 -2.979516 -2.982271 -2.9153125 -2.8962426 -2.8841851 -2.8182261 -2.7360632 -2.6403332 -2.5891962 -2.5624769 -2.6258318 -2.7626059 -2.9199963]]...]
INFO - root - 2017-12-07 06:18:03.397462: step 12710, loss = 0.63, batch loss = 0.56 (10.6 examples/sec; 0.758 sec/batch; 67h:18m:23s remains)
INFO - root - 2017-12-07 06:18:11.066455: step 12720, loss = 0.88, batch loss = 0.81 (10.7 examples/sec; 0.751 sec/batch; 66h:43m:26s remains)
INFO - root - 2017-12-07 06:18:18.543015: step 12730, loss = 0.81, batch loss = 0.73 (10.5 examples/sec; 0.765 sec/batch; 67h:56m:25s remains)
INFO - root - 2017-12-07 06:18:26.160904: step 12740, loss = 0.73, batch loss = 0.66 (10.9 examples/sec; 0.732 sec/batch; 64h:59m:04s remains)
INFO - root - 2017-12-07 06:18:33.831183: step 12750, loss = 0.78, batch loss = 0.71 (10.8 examples/sec; 0.742 sec/batch; 65h:51m:54s remains)
INFO - root - 2017-12-07 06:18:41.591582: step 12760, loss = 1.11, batch loss = 1.04 (10.3 examples/sec; 0.775 sec/batch; 68h:47m:51s remains)
INFO - root - 2017-12-07 06:18:49.238228: step 12770, loss = 0.75, batch loss = 0.68 (10.7 examples/sec; 0.748 sec/batch; 66h:25m:31s remains)
INFO - root - 2017-12-07 06:18:56.903618: step 12780, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.776 sec/batch; 68h:52m:28s remains)
INFO - root - 2017-12-07 06:19:04.638626: step 12790, loss = 1.09, batch loss = 1.02 (10.5 examples/sec; 0.762 sec/batch; 67h:40m:28s remains)
INFO - root - 2017-12-07 06:19:12.473312: step 12800, loss = 0.82, batch loss = 0.75 (10.0 examples/sec; 0.801 sec/batch; 71h:06m:49s remains)
2017-12-07 06:19:13.149642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7510881 -3.1080921 -3.4199295 -3.4975495 -3.4297924 -3.2289057 -3.0217662 -2.8813646 -2.7755971 -2.6265717 -2.7117429 -3.1276114 -3.2203689 -2.9153371 -2.8900003][-2.6486893 -2.9986763 -3.2805691 -3.3634648 -3.2273536 -2.9616125 -2.7686834 -2.6468515 -2.4931827 -2.3039117 -2.4718585 -3.0031781 -3.0002389 -2.4255702 -2.320473][-2.456264 -2.7104506 -2.954246 -3.1111593 -2.9793239 -2.7131252 -2.5500212 -2.3376215 -2.0032797 -1.7712471 -2.0628777 -2.731102 -2.6486049 -1.8258977 -1.6751971][-2.2298255 -2.3295751 -2.5657043 -2.9065754 -2.9341733 -2.7917173 -2.6871428 -2.2607169 -1.6191638 -1.3576705 -1.7663944 -2.4887156 -2.33472 -1.3065546 -1.1384726][-2.0906065 -2.0113428 -2.2458451 -2.7902436 -3.0370588 -3.0579932 -2.9783998 -2.2616937 -1.3183558 -1.1589577 -1.7120447 -2.4206758 -2.1941936 -0.99157763 -0.80544448][-2.1005979 -1.8365149 -2.0137415 -2.667994 -3.0592422 -3.1829433 -3.0379992 -2.0328245 -0.967237 -1.0859973 -1.8088062 -2.4558792 -2.1577425 -0.81464624 -0.59153438][-2.1226315 -1.6646419 -1.7146053 -2.3507645 -2.8072772 -2.9838266 -2.7538614 -1.5895925 -0.6361897 -1.12129 -1.9479473 -2.4751205 -2.117635 -0.7202487 -0.49305224][-2.1698763 -1.5974948 -1.5347219 -2.0694869 -2.5316668 -2.6966555 -2.3742101 -1.2278047 -0.55526042 -1.3156745 -2.0506849 -2.3698616 -1.9650996 -0.6011889 -0.44142509][-2.4466882 -1.851239 -1.6927395 -2.0689135 -2.4774246 -2.6076522 -2.2567925 -1.3012869 -0.99344635 -1.8629766 -2.35363 -2.4240503 -1.9741273 -0.72240877 -0.66617608][-2.7636189 -2.2299168 -2.0165315 -2.2317433 -2.5430722 -2.6405458 -2.309376 -1.6204126 -1.5842371 -2.3641436 -2.5847661 -2.4667549 -2.0113187 -0.95968294 -1.060607][-2.9038889 -2.4816012 -2.2731571 -2.3757753 -2.5795181 -2.6293082 -2.336307 -1.8817804 -1.9625788 -2.4910331 -2.4861526 -2.2768095 -1.8801844 -1.1045554 -1.3533473][-2.9144864 -2.6080229 -2.4552293 -2.507308 -2.63577 -2.6557732 -2.4221761 -2.1330421 -2.1934941 -2.4337835 -2.310647 -2.1388421 -1.8613155 -1.3636885 -1.6788611][-2.9530215 -2.7489047 -2.6689992 -2.7076392 -2.7867651 -2.7884779 -2.610836 -2.4454885 -2.4664559 -2.495882 -2.348984 -2.2629356 -2.1020994 -1.8132241 -2.1082256][-2.9896178 -2.8615932 -2.8366818 -2.8692036 -2.9121351 -2.9185133 -2.8032911 -2.7243547 -2.7403233 -2.6840219 -2.5618825 -2.534127 -2.4591837 -2.3110313 -2.5288744][-2.9364657 -2.8675442 -2.8766518 -2.9069748 -2.9261165 -2.9347868 -2.8631096 -2.8306572 -2.8391962 -2.7665572 -2.6922452 -2.7025542 -2.6802368 -2.6238914 -2.7711887]]...]
INFO - root - 2017-12-07 06:19:20.742870: step 12810, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 69h:15m:11s remains)
INFO - root - 2017-12-07 06:19:28.327376: step 12820, loss = 0.87, batch loss = 0.80 (10.9 examples/sec; 0.734 sec/batch; 65h:08m:35s remains)
INFO - root - 2017-12-07 06:19:35.711457: step 12830, loss = 0.80, batch loss = 0.72 (10.6 examples/sec; 0.751 sec/batch; 66h:43m:23s remains)
INFO - root - 2017-12-07 06:19:43.526026: step 12840, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.774 sec/batch; 68h:43m:33s remains)
INFO - root - 2017-12-07 06:19:51.203866: step 12850, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.791 sec/batch; 70h:16m:30s remains)
INFO - root - 2017-12-07 06:19:58.926163: step 12860, loss = 0.62, batch loss = 0.55 (10.7 examples/sec; 0.748 sec/batch; 66h:26m:15s remains)
INFO - root - 2017-12-07 06:20:06.550558: step 12870, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.752 sec/batch; 66h:46m:21s remains)
INFO - root - 2017-12-07 06:20:14.145088: step 12880, loss = 0.79, batch loss = 0.71 (10.4 examples/sec; 0.770 sec/batch; 68h:21m:33s remains)
INFO - root - 2017-12-07 06:20:21.786931: step 12890, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.755 sec/batch; 66h:59m:39s remains)
INFO - root - 2017-12-07 06:20:29.479216: step 12900, loss = 0.85, batch loss = 0.78 (10.1 examples/sec; 0.793 sec/batch; 70h:25m:10s remains)
2017-12-07 06:20:30.158931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6504955 -2.7417746 -2.9883165 -3.2763371 -3.4397762 -3.548722 -3.6476126 -3.6888418 -3.6435378 -3.4821727 -3.3107755 -3.1722679 -3.0570774 -3.1317558 -3.3432138][-2.5383098 -2.625144 -3.0118279 -3.4727564 -3.7395496 -3.8471208 -3.8886058 -3.8469737 -3.7787273 -3.6607313 -3.5927334 -3.5634189 -3.5302799 -3.6812782 -3.9833524][-2.4131799 -2.4001408 -2.9409726 -3.6133542 -4.0164189 -4.1201782 -4.0739341 -3.9149916 -3.8294406 -3.7926874 -3.8443942 -3.9234314 -3.9711919 -4.1809888 -4.5037322][-2.2672815 -2.08426 -2.7878165 -3.7377653 -4.2882662 -4.3269887 -4.106164 -3.8160579 -3.7920737 -3.9258256 -4.0831981 -4.1810746 -4.2334595 -4.3997149 -4.628746][-1.923456 -1.5238059 -2.3554313 -3.5949442 -4.2379518 -4.0712109 -3.5001745 -3.0085638 -3.1361895 -3.5887237 -3.9808412 -4.2122393 -4.3668675 -4.4794497 -4.52739][-1.5601561 -1.00859 -1.9105768 -3.3084898 -3.9092898 -3.4299548 -2.376755 -1.6187968 -1.9798687 -2.9028134 -3.6165977 -4.0109429 -4.2955613 -4.4018612 -4.2723][-1.4190395 -0.88133264 -1.8310266 -3.2281098 -3.6502535 -2.817534 -1.268533 -0.24933767 -0.93062234 -2.4591582 -3.5563478 -4.0692372 -4.391767 -4.4066944 -4.0341358][-1.5077157 -1.1609552 -2.1530035 -3.4271536 -3.6254711 -2.4896703 -0.53942132 0.7607851 -0.10358238 -2.0585093 -3.410305 -3.9843295 -4.3374362 -4.3586135 -3.9299419][-1.7230706 -1.6789792 -2.7021677 -3.8052526 -3.8414402 -2.5819006 -0.52603292 0.8672924 0.0423584 -1.9234164 -3.2223997 -3.6826818 -3.9812975 -4.0918465 -3.8249702][-2.2571576 -2.4608936 -3.4018266 -4.2347479 -4.1399717 -2.9474974 -1.136395 0.057291508 -0.59948468 -2.2596498 -3.316855 -3.6209588 -3.8086381 -3.9518709 -3.8634269][-2.9601665 -3.2534308 -3.948498 -4.4288335 -4.1851573 -3.1534953 -1.7760925 -0.9234035 -1.3847356 -2.5937688 -3.3464103 -3.5621452 -3.721164 -3.8579793 -3.8777273][-3.2489479 -3.4704883 -3.9211552 -4.1606874 -3.8735142 -3.1123276 -2.275573 -1.8258996 -2.1187205 -2.806016 -3.160111 -3.2466357 -3.3378029 -3.3725204 -3.3613486][-3.2375383 -3.2786736 -3.5472507 -3.6928015 -3.454283 -2.998225 -2.6607442 -2.5747519 -2.7528257 -2.9879212 -3.0157495 -3.0436382 -3.1683292 -3.2184954 -3.2159317][-3.3569851 -3.2740045 -3.4682136 -3.60448 -3.4264393 -3.1296849 -3.016221 -3.068574 -3.1285844 -3.0613186 -2.8929236 -2.9160244 -3.1259632 -3.3121018 -3.4285944][-3.5307579 -3.3630495 -3.5831685 -3.8231397 -3.7820027 -3.5762897 -3.4963696 -3.5276756 -3.480001 -3.2932994 -3.0849884 -3.1178155 -3.3343661 -3.5488877 -3.6776915]]...]
INFO - root - 2017-12-07 06:20:37.836128: step 12910, loss = 1.01, batch loss = 0.94 (10.6 examples/sec; 0.753 sec/batch; 66h:49m:07s remains)
INFO - root - 2017-12-07 06:20:45.526922: step 12920, loss = 0.89, batch loss = 0.82 (10.9 examples/sec; 0.736 sec/batch; 65h:18m:19s remains)
INFO - root - 2017-12-07 06:20:52.933140: step 12930, loss = 1.02, batch loss = 0.94 (10.8 examples/sec; 0.738 sec/batch; 65h:28m:17s remains)
INFO - root - 2017-12-07 06:21:00.614913: step 12940, loss = 0.68, batch loss = 0.60 (10.1 examples/sec; 0.795 sec/batch; 70h:31m:56s remains)
INFO - root - 2017-12-07 06:21:08.319973: step 12950, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.761 sec/batch; 67h:31m:24s remains)
INFO - root - 2017-12-07 06:21:16.117977: step 12960, loss = 1.05, batch loss = 0.98 (10.3 examples/sec; 0.777 sec/batch; 68h:55m:45s remains)
INFO - root - 2017-12-07 06:21:23.741631: step 12970, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.758 sec/batch; 67h:16m:35s remains)
INFO - root - 2017-12-07 06:21:31.419950: step 12980, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.766 sec/batch; 67h:59m:16s remains)
INFO - root - 2017-12-07 06:21:39.029956: step 12990, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.768 sec/batch; 68h:12m:01s remains)
INFO - root - 2017-12-07 06:21:46.613881: step 13000, loss = 0.96, batch loss = 0.88 (10.6 examples/sec; 0.754 sec/batch; 66h:54m:34s remains)
2017-12-07 06:21:47.233330: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24333763 0.20802784 0.18824863 0.16281414 0.14709663 0.13744926 0.12550783 0.12517405 0.13751173 0.15335512 0.15052271 0.14248753 0.12624788 0.12463951 0.14936209][0.25384903 0.21089792 0.17300892 0.12541533 0.087227821 0.059346676 0.037847042 0.026814461 0.045605183 0.079232216 0.087622166 0.10619831 0.12070751 0.14443445 0.20348978][0.18582916 0.14110231 0.090153694 0.025898457 -0.029318333 -0.063466072 -0.085445404 -0.1072278 -0.0816741 -0.02781868 -0.013602257 0.024857521 0.065475941 0.11029196 0.21137094][0.11752892 0.10154867 0.07166481 0.020684719 -0.026359558 -0.049959183 -0.067032337 -0.10346365 -0.082839966 -0.032075405 -0.038379669 0.0039353371 0.055474281 0.10352325 0.23215532][-0.035868168 0.011113167 0.0680604 0.10092163 0.12836361 0.1728344 0.19197655 0.13515711 0.092409134 0.048877716 -0.062607765 -0.067448616 -0.021892548 0.024813175 0.18301201][-0.21592569 -0.081779 0.086687565 0.22782803 0.35949755 0.50009155 0.57904053 0.513144 0.37364626 0.16004562 -0.13489389 -0.24315405 -0.23453283 -0.18505192 0.011940002][-0.60785985 -0.39955378 -0.17190742 0.0086064339 0.15212202 0.28741598 0.35408497 0.27302313 0.10120726 -0.16390419 -0.51642823 -0.63701248 -0.59634852 -0.47863531 -0.20157051][-1.1580808 -0.96259785 -0.76886749 -0.60843778 -0.49217272 -0.40398836 -0.39527893 -0.50177479 -0.6473012 -0.85168576 -1.1257374 -1.1562884 -1.0215256 -0.80043769 -0.42999721][-1.7917905 -1.6479652 -1.5081928 -1.3324583 -1.1602302 -0.98836637 -0.89248562 -0.91271949 -0.95633173 -1.0589421 -1.2539196 -1.2358408 -1.0740252 -0.83930588 -0.47180247][-2.6950557 -2.5506501 -2.3770003 -2.0850091 -1.7415011 -1.3653731 -1.0646696 -0.91689587 -0.83798862 -0.8413372 -0.97852111 -0.97034788 -0.83938336 -0.65227318 -0.35324621][-3.1114817 -2.9753723 -2.8104336 -2.4760039 -2.0801244 -1.6083505 -1.1524544 -0.84788656 -0.65244555 -0.57153821 -0.65934992 -0.69116664 -0.61836433 -0.4935081 -0.27835608][-2.9967194 -2.935714 -2.8635278 -2.6016672 -2.3135808 -1.9153519 -1.428582 -1.0103006 -0.68155193 -0.47838187 -0.4638176 -0.48920441 -0.4442234 -0.36349916 -0.21804285][-3.0437486 -3.0734112 -3.0528893 -2.8281519 -2.6235878 -2.2788661 -1.7615328 -1.2480826 -0.80086422 -0.49565268 -0.39877462 -0.41761541 -0.39932442 -0.35380936 -0.23930025][-3.1309004 -3.2279234 -3.2206712 -3.0246325 -2.8756497 -2.536252 -1.9775221 -1.3992286 -0.86710215 -0.49171448 -0.36645651 -0.40468884 -0.42992306 -0.43364382 -0.32587767][-2.7918818 -2.9201074 -2.9498253 -2.8468738 -2.786778 -2.4920592 -1.9519455 -1.3755181 -0.80532694 -0.36514044 -0.21848965 -0.26996708 -0.34122086 -0.41420937 -0.32682133]]...]
INFO - root - 2017-12-07 06:21:54.949047: step 13010, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.786 sec/batch; 69h:43m:32s remains)
INFO - root - 2017-12-07 06:22:02.658394: step 13020, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 66h:10m:08s remains)
INFO - root - 2017-12-07 06:22:10.081660: step 13030, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.764 sec/batch; 67h:50m:19s remains)
INFO - root - 2017-12-07 06:22:17.850054: step 13040, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.790 sec/batch; 70h:07m:46s remains)
INFO - root - 2017-12-07 06:22:25.548851: step 13050, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.750 sec/batch; 66h:32m:12s remains)
INFO - root - 2017-12-07 06:22:33.223460: step 13060, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.760 sec/batch; 67h:25m:55s remains)
INFO - root - 2017-12-07 06:22:40.891451: step 13070, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.775 sec/batch; 68h:43m:57s remains)
INFO - root - 2017-12-07 06:22:48.588279: step 13080, loss = 0.66, batch loss = 0.59 (10.2 examples/sec; 0.782 sec/batch; 69h:24m:54s remains)
INFO - root - 2017-12-07 06:22:56.231667: step 13090, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.765 sec/batch; 67h:54m:23s remains)
INFO - root - 2017-12-07 06:23:03.791326: step 13100, loss = 0.88, batch loss = 0.80 (10.5 examples/sec; 0.762 sec/batch; 67h:34m:07s remains)
2017-12-07 06:23:04.412953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7472334 -3.7540081 -3.7475736 -3.724328 -3.6901143 -3.7983222 -3.96734 -4.0670862 -4.1424279 -4.054265 -4.0033374 -4.1819763 -4.2514496 -4.2310247 -4.0482664][-3.6462164 -3.6488073 -3.66993 -3.603581 -3.4646981 -3.5113413 -3.7554474 -3.941895 -4.0815148 -3.9621897 -3.8435364 -4.0270667 -4.0865993 -4.07525 -3.8778229][-3.3979859 -3.3821964 -3.4003716 -3.2200277 -2.8646405 -2.7445483 -3.01119 -3.372159 -3.7190974 -3.69251 -3.5011756 -3.5438778 -3.4980979 -3.561873 -3.6186149][-3.0249476 -2.9397569 -2.8446383 -2.4269822 -1.8776026 -1.6784024 -1.9916081 -2.528101 -3.1388764 -3.3348675 -3.2208281 -3.1869001 -3.0931723 -3.2489524 -3.5576649][-2.5800683 -2.4106543 -2.2494004 -1.7138004 -1.1372566 -0.92891479 -1.1718879 -1.7411859 -2.5298698 -3.034379 -3.2650266 -3.4349859 -3.4074893 -3.5343094 -3.7851379][-2.1670871 -2.0422425 -1.93718 -1.3703275 -0.72893238 -0.33596992 -0.27245617 -0.74874377 -1.6813881 -2.5419822 -3.2252355 -3.6832933 -3.7244685 -3.7584696 -3.8040347][-1.9802117 -1.9880686 -1.8769174 -1.2262478 -0.47387576 0.12943697 0.46165419 0.0046033859 -1.0617073 -2.1527069 -3.0761967 -3.6968465 -3.8383393 -3.8320022 -3.7005429][-1.9334764 -1.9607112 -1.7501056 -1.1696556 -0.59521461 -0.017179966 0.36798859 -0.051218987 -0.9605248 -1.901721 -2.7976584 -3.5907967 -4.0267076 -4.1541395 -3.9526234][-1.6824448 -1.6303136 -1.4626448 -1.2321105 -1.088197 -0.67469549 -0.27741718 -0.43618631 -0.87967062 -1.4374001 -2.1943305 -3.1547575 -3.9236069 -4.25143 -4.1532083][-1.4720869 -1.4292932 -1.4584568 -1.5422137 -1.6303988 -1.2830179 -0.88723063 -0.84478879 -0.94206357 -1.2467825 -1.91201 -2.8938565 -3.7482607 -4.1240849 -4.1396503][-1.6948469 -1.679275 -1.8168094 -2.0190752 -2.1714635 -1.9784508 -1.7758424 -1.767365 -1.784375 -1.9902356 -2.4844046 -3.1735177 -3.7739103 -4.0478096 -4.1179118][-2.1520438 -2.0325944 -2.0985553 -2.3168705 -2.5682781 -2.6727858 -2.7370644 -2.8015609 -2.7665431 -2.8104837 -2.9743876 -3.1821284 -3.4441686 -3.6664026 -3.8743215][-2.5609384 -2.32489 -2.2886984 -2.4703932 -2.7556503 -3.0135784 -3.1821864 -3.2233348 -3.12771 -3.035902 -3.0012112 -3.0098727 -3.1581059 -3.365737 -3.6288671][-3.074789 -2.8704665 -2.816942 -2.922019 -3.102787 -3.32253 -3.4741027 -3.4865911 -3.3925319 -3.2981129 -3.292974 -3.36836 -3.5171754 -3.6331668 -3.7332554][-3.6454422 -3.5718367 -3.5367675 -3.5390944 -3.5574028 -3.6637797 -3.7561498 -3.7478647 -3.6854906 -3.6459448 -3.7002661 -3.8071141 -3.8836226 -3.8624439 -3.7995985]]...]
INFO - root - 2017-12-07 06:23:12.004696: step 13110, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.754 sec/batch; 66h:51m:28s remains)
INFO - root - 2017-12-07 06:23:19.610369: step 13120, loss = 0.82, batch loss = 0.74 (10.2 examples/sec; 0.781 sec/batch; 69h:15m:22s remains)
INFO - root - 2017-12-07 06:23:27.134648: step 13130, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.785 sec/batch; 69h:39m:55s remains)
INFO - root - 2017-12-07 06:23:34.863978: step 13140, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.770 sec/batch; 68h:18m:04s remains)
INFO - root - 2017-12-07 06:23:42.587573: step 13150, loss = 0.95, batch loss = 0.88 (10.2 examples/sec; 0.783 sec/batch; 69h:27m:33s remains)
INFO - root - 2017-12-07 06:23:50.355296: step 13160, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.752 sec/batch; 66h:40m:34s remains)
INFO - root - 2017-12-07 06:23:58.042329: step 13170, loss = 1.03, batch loss = 0.96 (10.0 examples/sec; 0.799 sec/batch; 70h:52m:56s remains)
INFO - root - 2017-12-07 06:24:05.679588: step 13180, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.766 sec/batch; 67h:58m:37s remains)
INFO - root - 2017-12-07 06:24:13.438963: step 13190, loss = 0.76, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 72h:10m:15s remains)
INFO - root - 2017-12-07 06:24:21.217256: step 13200, loss = 0.85, batch loss = 0.78 (10.2 examples/sec; 0.785 sec/batch; 69h:38m:14s remains)
2017-12-07 06:24:21.862985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.198956 -3.1467738 -3.041307 -2.8684049 -2.7603557 -2.673893 -2.5196865 -2.3392017 -2.105989 -1.9090281 -1.9189529 -2.0060854 -2.0002007 -2.0281658 -2.1669159][-3.31497 -3.3125234 -3.2240739 -3.049737 -2.919208 -2.7577248 -2.4896188 -2.1981058 -1.8597906 -1.6089032 -1.6693845 -1.8800802 -1.9926388 -2.0450339 -2.0933502][-3.2945473 -3.3571706 -3.2976859 -3.1330709 -2.9353454 -2.6434915 -2.2620637 -1.9308307 -1.612783 -1.4106209 -1.5379725 -1.844641 -2.061841 -2.1297336 -2.0721779][-3.1487775 -3.2066295 -3.10116 -2.9046693 -2.613487 -2.1840317 -1.7611897 -1.4936178 -1.3138881 -1.222038 -1.4127898 -1.7902484 -2.0706844 -2.1223044 -1.9719543][-2.9747381 -2.959384 -2.7421675 -2.4978302 -2.147336 -1.6022542 -1.1357923 -0.89174485 -0.76018906 -0.69742155 -0.91715932 -1.4070208 -1.7927272 -1.8594582 -1.6971052][-2.6809375 -2.5989344 -2.2594895 -1.9908025 -1.6723742 -1.1020513 -0.5844717 -0.25989723 -0.050301075 0.031993866 -0.25080776 -0.89863539 -1.4145989 -1.5077376 -1.3902326][-2.1844466 -2.0818465 -1.7101102 -1.4970374 -1.2631252 -0.72379756 -0.18372393 0.19895935 0.48604822 0.57013512 0.2136364 -0.51451874 -1.0396974 -1.0926452 -1.0362332][-1.838052 -1.6994474 -1.2747838 -1.0551143 -0.86906314 -0.4234271 0.0064892769 0.29245996 0.53594637 0.59542418 0.26420879 -0.33541822 -0.68193388 -0.61427236 -0.62060356][-1.695138 -1.4463053 -0.87056112 -0.58541179 -0.49599195 -0.31805992 -0.20792675 -0.16850853 -0.044219971 0.029850483 -0.071917057 -0.31099796 -0.34963036 -0.14514494 -0.22050524][-1.3361809 -0.97141027 -0.29037714 -0.080008984 -0.26260281 -0.4730525 -0.69476342 -0.83072376 -0.75173354 -0.61433196 -0.45493841 -0.34665442 -0.16583061 0.10063076 0.042166233][-0.823617 -0.40796566 0.25607204 0.23629284 -0.32465506 -0.87084532 -1.2685289 -1.376699 -1.2027764 -1.0180814 -0.76780987 -0.5408318 -0.37030029 -0.14668941 -0.045619011][-0.57551527 -0.11989498 0.46662331 0.25941896 -0.4837234 -1.0914409 -1.4829197 -1.4684579 -1.1821702 -1.0235963 -0.85383606 -0.68967676 -0.68928003 -0.60674882 -0.36604118][-0.78104472 -0.33333826 0.24109077 0.12758112 -0.41496325 -0.79189086 -1.1025743 -1.0674236 -0.81389785 -0.82568526 -0.86060834 -0.86629772 -1.0450163 -1.0789773 -0.7418735][-1.0392463 -0.57569385 0.057505608 0.16230297 -0.076066494 -0.22649097 -0.53183627 -0.61120439 -0.50264454 -0.69450951 -0.90668726 -1.0530572 -1.3318157 -1.4066505 -1.0206397][-1.1421301 -0.67228055 -0.055129051 0.16106653 0.051794052 -0.083405495 -0.49211788 -0.70951581 -0.6993649 -0.89772534 -1.131114 -1.2965076 -1.5733562 -1.6693654 -1.3254044]]...]
INFO - root - 2017-12-07 06:24:29.431780: step 13210, loss = 0.70, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 69h:44m:05s remains)
INFO - root - 2017-12-07 06:24:37.081182: step 13220, loss = 0.75, batch loss = 0.68 (10.2 examples/sec; 0.783 sec/batch; 69h:24m:45s remains)
INFO - root - 2017-12-07 06:24:44.552647: step 13230, loss = 0.66, batch loss = 0.58 (10.4 examples/sec; 0.769 sec/batch; 68h:14m:34s remains)
INFO - root - 2017-12-07 06:24:52.195702: step 13240, loss = 0.87, batch loss = 0.80 (10.7 examples/sec; 0.746 sec/batch; 66h:08m:43s remains)
INFO - root - 2017-12-07 06:24:59.848143: step 13250, loss = 0.96, batch loss = 0.88 (10.3 examples/sec; 0.774 sec/batch; 68h:36m:38s remains)
INFO - root - 2017-12-07 06:25:07.466105: step 13260, loss = 0.90, batch loss = 0.83 (10.7 examples/sec; 0.748 sec/batch; 66h:18m:13s remains)
INFO - root - 2017-12-07 06:25:15.140995: step 13270, loss = 1.03, batch loss = 0.96 (10.6 examples/sec; 0.754 sec/batch; 66h:53m:07s remains)
INFO - root - 2017-12-07 06:25:22.808233: step 13280, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.788 sec/batch; 69h:53m:32s remains)
INFO - root - 2017-12-07 06:25:30.461988: step 13290, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.764 sec/batch; 67h:43m:15s remains)
INFO - root - 2017-12-07 06:25:38.189901: step 13300, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.782 sec/batch; 69h:20m:40s remains)
2017-12-07 06:25:38.894240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.99126 -3.074923 -3.2808247 -3.5085783 -3.5236578 -3.3266439 -3.0149641 -2.5523169 -2.1982257 -2.1687288 -2.4797187 -2.9966655 -3.4944339 -3.6223664 -3.3968043][-3.4881783 -3.651665 -3.9107461 -4.1464448 -4.1175089 -3.8728852 -3.5075624 -3.0506961 -2.8402343 -2.9772842 -3.3663454 -3.8096843 -4.1257215 -4.0574083 -3.7466908][-3.9780855 -4.1849437 -4.4146361 -4.5536714 -4.4081545 -4.0813875 -3.6931691 -3.3062873 -3.2029953 -3.4194038 -3.8842428 -4.3384156 -4.5516553 -4.3392472 -3.9796071][-4.345819 -4.4564643 -4.5306735 -4.47446 -4.157548 -3.7299516 -3.4076767 -3.1860647 -3.1568913 -3.3674664 -3.8392045 -4.2785573 -4.3581176 -3.9924355 -3.5842671][-4.67191 -4.6408362 -4.5563364 -4.3401241 -3.8805256 -3.3758764 -3.1216807 -2.9800887 -2.887918 -2.9945574 -3.4778752 -3.9907072 -4.0760512 -3.6857758 -3.2878344][-4.5442972 -4.4127135 -4.2681351 -4.0465045 -3.6028857 -3.1357951 -2.963495 -2.8693881 -2.6566441 -2.5901451 -3.0322065 -3.6778517 -3.9614322 -3.7434645 -3.4573514][-4.2357187 -4.0729446 -3.9222536 -3.7443137 -3.3840251 -3.021059 -2.9474761 -2.8304052 -2.4262431 -2.1729169 -2.6209111 -3.4903283 -4.1057677 -4.1440945 -3.9626493][-4.1925364 -4.06231 -3.9045279 -3.7443931 -3.5228686 -3.3477178 -3.3443632 -3.0629277 -2.3695631 -1.9350505 -2.3809421 -3.4672225 -4.3663282 -4.500845 -4.17959][-4.1385412 -4.09535 -3.9128454 -3.7375948 -3.6986487 -3.7680035 -3.8314049 -3.3605318 -2.4317575 -1.8881176 -2.2964354 -3.4176602 -4.367394 -4.4199977 -3.8439097][-3.8162587 -3.8706033 -3.6548367 -3.4759953 -3.591604 -3.8178275 -3.9017231 -3.4013057 -2.5175138 -2.0294592 -2.3425648 -3.2805326 -4.0888796 -4.0339417 -3.316998][-3.3235488 -3.3425021 -3.0469074 -2.8345957 -2.9184942 -3.0523992 -3.0511534 -2.7346478 -2.241637 -2.010901 -2.2557971 -2.9722729 -3.6529212 -3.6025565 -2.8994679][-2.7950926 -2.6478949 -2.2109239 -1.8853166 -1.8066561 -1.7818761 -1.760129 -1.6907854 -1.5849793 -1.5925462 -1.8182619 -2.3793 -3.0039423 -3.1229424 -2.6599803][-2.5185134 -2.2946014 -1.8846531 -1.5350592 -1.3007927 -1.1255589 -1.1196814 -1.241874 -1.3704431 -1.4898672 -1.6797936 -2.1345861 -2.7428055 -3.0789204 -2.9407787][-2.506598 -2.3957734 -2.2413061 -2.0473092 -1.7645526 -1.5033693 -1.4748306 -1.6311917 -1.7707987 -1.8888273 -2.0840204 -2.4594922 -2.9220147 -3.2331865 -3.2600486][-2.716399 -2.7564385 -2.8535495 -2.8618963 -2.6674643 -2.3807273 -2.2603455 -2.3566644 -2.5146871 -2.6859241 -2.8976769 -3.1527705 -3.3599119 -3.4578578 -3.4545276]]...]
INFO - root - 2017-12-07 06:25:46.451970: step 13310, loss = 0.90, batch loss = 0.83 (10.7 examples/sec; 0.749 sec/batch; 66h:23m:26s remains)
INFO - root - 2017-12-07 06:25:54.167034: step 13320, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.781 sec/batch; 69h:13m:15s remains)
INFO - root - 2017-12-07 06:26:01.548394: step 13330, loss = 0.96, batch loss = 0.89 (10.5 examples/sec; 0.761 sec/batch; 67h:26m:18s remains)
INFO - root - 2017-12-07 06:26:09.185614: step 13340, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.769 sec/batch; 68h:07m:55s remains)
INFO - root - 2017-12-07 06:26:16.820169: step 13350, loss = 0.82, batch loss = 0.75 (10.1 examples/sec; 0.789 sec/batch; 69h:56m:29s remains)
INFO - root - 2017-12-07 06:26:24.502543: step 13360, loss = 1.05, batch loss = 0.98 (10.5 examples/sec; 0.764 sec/batch; 67h:45m:26s remains)
INFO - root - 2017-12-07 06:26:32.225323: step 13370, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.757 sec/batch; 67h:06m:44s remains)
INFO - root - 2017-12-07 06:26:39.915061: step 13380, loss = 1.14, batch loss = 1.07 (10.3 examples/sec; 0.778 sec/batch; 68h:57m:10s remains)
INFO - root - 2017-12-07 06:26:47.559617: step 13390, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.764 sec/batch; 67h:42m:38s remains)
INFO - root - 2017-12-07 06:26:55.213424: step 13400, loss = 0.81, batch loss = 0.74 (10.1 examples/sec; 0.788 sec/batch; 69h:52m:40s remains)
2017-12-07 06:26:55.815559: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.815516 0.71683168 0.57315779 0.42050171 0.26912975 0.11480856 -0.0070548058 -0.073783875 -0.081665039 -0.0790987 -0.046782017 0.061918259 0.203403 0.36585569 0.53249311][0.57635546 0.43996429 0.24578571 0.020108223 -0.23567677 -0.48920631 -0.667171 -0.77632785 -0.82593846 -0.84406567 -0.80009556 -0.62741566 -0.40632486 -0.1476922 0.1346426][0.43505955 0.23012304 -0.051564217 -0.35667896 -0.69285822 -0.9816072 -1.1164284 -1.2106559 -1.2884305 -1.3201756 -1.292115 -1.0907352 -0.82394457 -0.48865986 -0.095387936][0.40069866 0.13130569 -0.23108339 -0.57536054 -0.9050374 -1.1011155 -1.0802326 -1.1307645 -1.2591672 -1.3532214 -1.4309108 -1.3031836 -1.0584033 -0.68101168 -0.19823408][0.51357174 0.21922779 -0.18531466 -0.541106 -0.82263947 -0.89105105 -0.74640012 -0.85838151 -1.1238368 -1.3142037 -1.5070333 -1.4481404 -1.1858699 -0.73898697 -0.19794083][0.76164436 0.53936958 0.18837643 -0.1210885 -0.31440639 -0.28603649 -0.13466835 -0.4397409 -0.9225843 -1.253072 -1.5357003 -1.5050428 -1.1792653 -0.65327191 -0.093340874][0.97093391 0.82976151 0.49237108 0.14354467 -0.075699329 -0.094304085 -0.069826126 -0.54524636 -1.1141872 -1.4438143 -1.6693997 -1.56548 -1.1406193 -0.57213688 -0.066500664][0.99081373 0.8491044 0.41101027 -0.089897633 -0.42585707 -0.54109955 -0.61161923 -1.0336831 -1.4061623 -1.5284159 -1.5848231 -1.4071918 -0.96417761 -0.44889641 -0.061504841][0.88253117 0.70563269 0.17214251 -0.42857075 -0.8061645 -0.90572691 -0.93684292 -1.1797013 -1.2962275 -1.2292478 -1.1949811 -1.0723209 -0.73594546 -0.32794189 -0.0403018][0.68237591 0.47432566 -0.084993839 -0.67605519 -1.005996 -1.0420253 -1.025269 -1.1227181 -1.0705028 -0.91464949 -0.86235332 -0.81476736 -0.58324575 -0.2711153 -0.0666337][0.55383539 0.34720755 -0.16540861 -0.66362906 -0.90047383 -0.90082908 -0.86230516 -0.88751292 -0.78607893 -0.64572 -0.62153959 -0.62245131 -0.46841025 -0.24762964 -0.12211704][0.61411238 0.44367647 0.0095329285 -0.37503338 -0.54240561 -0.5534339 -0.51391959 -0.5044055 -0.41467047 -0.33312845 -0.343472 -0.34778833 -0.23065758 -0.099844456 -0.0594697][0.8295269 0.69281578 0.32028913 0.012872696 -0.12772799 -0.15890932 -0.10813427 -0.064962387 0.0089125633 0.054300785 0.033697605 0.054869175 0.1574707 0.21467876 0.18841267][0.97749996 0.837873 0.50568247 0.25413561 0.13909721 0.11664534 0.18516302 0.25172663 0.31300735 0.3280015 0.30024862 0.3345809 0.43119431 0.44773722 0.38406467][0.7631197 0.61344957 0.33476448 0.15204954 0.083550453 0.082341671 0.1415453 0.21175194 0.27955246 0.28923416 0.26948929 0.30199957 0.36980438 0.35094309 0.27731419]]...]
INFO - root - 2017-12-07 06:27:03.499094: step 13410, loss = 0.90, batch loss = 0.83 (11.0 examples/sec; 0.727 sec/batch; 64h:27m:21s remains)
INFO - root - 2017-12-07 06:27:11.306445: step 13420, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.777 sec/batch; 68h:52m:41s remains)
INFO - root - 2017-12-07 06:27:18.796477: step 13430, loss = 0.99, batch loss = 0.91 (10.7 examples/sec; 0.746 sec/batch; 66h:09m:40s remains)
INFO - root - 2017-12-07 06:27:26.501984: step 13440, loss = 0.68, batch loss = 0.60 (10.4 examples/sec; 0.773 sec/batch; 68h:29m:56s remains)
INFO - root - 2017-12-07 06:27:34.057492: step 13450, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.764 sec/batch; 67h:42m:54s remains)
INFO - root - 2017-12-07 06:27:41.753655: step 13460, loss = 0.70, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 67h:38m:19s remains)
INFO - root - 2017-12-07 06:27:49.293895: step 13470, loss = 1.07, batch loss = 1.00 (10.3 examples/sec; 0.774 sec/batch; 68h:34m:17s remains)
INFO - root - 2017-12-07 06:27:56.884179: step 13480, loss = 0.68, batch loss = 0.61 (10.2 examples/sec; 0.783 sec/batch; 69h:24m:28s remains)
INFO - root - 2017-12-07 06:28:04.558966: step 13490, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.752 sec/batch; 66h:37m:37s remains)
INFO - root - 2017-12-07 06:28:12.200216: step 13500, loss = 1.18, batch loss = 1.11 (10.5 examples/sec; 0.765 sec/batch; 67h:47m:31s remains)
2017-12-07 06:28:12.806277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6975429 -2.5170276 -2.2733474 -2.1608579 -2.2386813 -2.3434398 -2.37004 -2.410104 -2.5200195 -2.6121392 -2.6342826 -2.6797552 -2.8046947 -2.9168434 -2.951019][-3.1482396 -2.8640079 -2.4782014 -2.2194877 -2.2398961 -2.3073943 -2.3078601 -2.3488069 -2.4798751 -2.637682 -2.6789541 -2.7209466 -2.9008973 -3.091409 -3.182497][-3.7206936 -3.346168 -2.8526871 -2.4487593 -2.4125276 -2.4477336 -2.4171116 -2.4407434 -2.5507283 -2.7673569 -2.8684807 -2.9509475 -3.2144029 -3.4918227 -3.6105771][-4.2520332 -3.8287148 -3.2482183 -2.7158766 -2.6231256 -2.6199713 -2.50078 -2.4602923 -2.5371168 -2.80855 -3.015687 -3.176388 -3.5238242 -3.8667524 -3.9885716][-4.61758 -4.1807075 -3.5368488 -2.9184737 -2.7336516 -2.6353619 -2.3656549 -2.2148142 -2.2625644 -2.5542846 -2.8658061 -3.1292605 -3.549109 -3.9377344 -4.077373][-4.6925163 -4.2033195 -3.4784203 -2.7977 -2.4604139 -2.2143023 -1.8029401 -1.5311449 -1.5830774 -1.9157307 -2.3372507 -2.746335 -3.2689097 -3.7322283 -3.9393728][-4.4782476 -3.9502184 -3.258055 -2.6401896 -2.1912355 -1.7711036 -1.2384851 -0.83962965 -0.90680909 -1.3012636 -1.8171618 -2.3871491 -3.0178838 -3.5406482 -3.8045435][-4.2905488 -3.8494534 -3.3287385 -2.8626809 -2.37229 -1.7824841 -1.1370909 -0.62157226 -0.68953919 -1.1635237 -1.7159064 -2.3452625 -3.0054564 -3.5095079 -3.7745953][-4.3137841 -4.0315332 -3.6964495 -3.3712778 -2.9273005 -2.2806885 -1.6116867 -1.0982606 -1.1435769 -1.6126628 -2.079325 -2.6178875 -3.2152934 -3.6345417 -3.8464422][-4.3740311 -4.2258139 -4.0268755 -3.8209233 -3.5155816 -2.9569826 -2.3787322 -1.9913108 -2.0122855 -2.3965981 -2.7179513 -3.0714874 -3.546953 -3.8635468 -4.002274][-4.2952037 -4.2030497 -4.1016417 -4.0008545 -3.8462076 -3.4353323 -3.0005312 -2.7991524 -2.8465924 -3.1400075 -3.3338776 -3.5072327 -3.8563879 -4.0873728 -4.1524444][-4.2612662 -4.1804423 -4.1157322 -4.0396676 -3.9385176 -3.6391582 -3.3175447 -3.2547312 -3.3392956 -3.5565004 -3.676734 -3.7521722 -4.0186424 -4.1941609 -4.2047982][-4.5012941 -4.4141941 -4.318665 -4.1741185 -4.0303431 -3.7883615 -3.5293741 -3.5091345 -3.5876596 -3.71917 -3.7982264 -3.8577969 -4.1007261 -4.2493105 -4.2163558][-4.7744365 -4.6388168 -4.4699359 -4.2536116 -4.0974336 -3.9313967 -3.7435157 -3.7192826 -3.7499769 -3.7931075 -3.8440342 -3.9253709 -4.1586947 -4.2757292 -4.2106204][-4.5966473 -4.4514313 -4.3002157 -4.1355143 -4.063241 -4.0153923 -3.9234185 -3.9017549 -3.8840399 -3.8555064 -3.8761959 -3.9609673 -4.165699 -4.252645 -4.1735816]]...]
INFO - root - 2017-12-07 06:28:20.461006: step 13510, loss = 0.73, batch loss = 0.66 (10.7 examples/sec; 0.749 sec/batch; 66h:21m:15s remains)
INFO - root - 2017-12-07 06:28:28.087199: step 13520, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.771 sec/batch; 68h:18m:51s remains)
INFO - root - 2017-12-07 06:28:35.496752: step 13530, loss = 0.74, batch loss = 0.66 (10.7 examples/sec; 0.748 sec/batch; 66h:14m:19s remains)
INFO - root - 2017-12-07 06:28:43.233386: step 13540, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.771 sec/batch; 68h:16m:39s remains)
INFO - root - 2017-12-07 06:28:50.804383: step 13550, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.757 sec/batch; 67h:01m:41s remains)
INFO - root - 2017-12-07 06:28:58.444282: step 13560, loss = 0.68, batch loss = 0.61 (10.8 examples/sec; 0.743 sec/batch; 65h:49m:01s remains)
INFO - root - 2017-12-07 06:29:06.035452: step 13570, loss = 0.64, batch loss = 0.56 (10.7 examples/sec; 0.748 sec/batch; 66h:18m:17s remains)
INFO - root - 2017-12-07 06:29:13.697535: step 13580, loss = 0.71, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 68h:49m:41s remains)
INFO - root - 2017-12-07 06:29:21.243427: step 13590, loss = 0.84, batch loss = 0.77 (10.9 examples/sec; 0.736 sec/batch; 65h:13m:21s remains)
INFO - root - 2017-12-07 06:29:28.846535: step 13600, loss = 0.74, batch loss = 0.67 (10.9 examples/sec; 0.736 sec/batch; 65h:14m:24s remains)
2017-12-07 06:29:29.503456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6454415 -2.6840925 -2.2689774 -2.0163121 -1.861331 -1.6327043 -1.4823098 -1.0515203 -0.89457726 -1.0054319 -0.96816874 -1.1927454 -1.5628302 -1.4478123 -1.5606987][-2.6400604 -2.4968181 -1.8496184 -1.5723982 -1.5704885 -1.500766 -1.5089486 -1.2262356 -1.0471559 -1.2072248 -1.2623084 -1.3621366 -1.4955041 -1.2460418 -1.197484][-2.7643085 -2.4698348 -1.5751188 -1.1564441 -1.2053478 -1.1811328 -1.133554 -0.9632678 -0.99972963 -1.3075359 -1.4121091 -1.3518014 -1.2474535 -0.90820241 -0.77135825][-2.9110308 -2.5199213 -1.3957198 -0.75219965 -0.69793582 -0.49902368 -0.12614012 -0.053260803 -0.60479212 -1.3150806 -1.6158557 -1.402061 -0.92197943 -0.36526918 -0.14403343][-2.9250345 -2.4412684 -1.213851 -0.49629593 -0.32394171 0.24084044 1.0908418 1.1889124 0.17100096 -0.98426747 -1.6408117 -1.5363886 -0.76244354 0.0988574 0.46757221][-2.7712476 -2.1850092 -1.0135179 -0.45829844 -0.23099709 0.78215122 2.2117724 2.4887376 1.1304903 -0.46690917 -1.5370555 -1.7321107 -0.87773061 0.25768709 0.72559547][-2.4923387 -1.8122602 -0.8215704 -0.50607157 -0.19949436 1.1217813 2.9025955 3.2956944 1.6951709 -0.183846 -1.5299239 -2.0573924 -1.3818445 -0.15429783 0.3807416][-2.213721 -1.5356176 -0.79672742 -0.6332438 -0.29666281 0.98011589 2.591248 2.913424 1.6213841 0.018525124 -1.2634988 -2.0694854 -1.8659651 -0.8629272 -0.37669325][-2.0098743 -1.5167332 -1.1224558 -1.1008322 -0.81093478 0.14840031 1.2996578 1.611578 0.97112989 -0.0019030571 -0.97682357 -1.8779249 -2.1257069 -1.4538395 -0.99849033][-1.9337413 -1.6483684 -1.60828 -1.7829347 -1.6678488 -1.107286 -0.41462183 -0.0195055 -0.089293957 -0.47357011 -0.99429893 -1.7003489 -2.1606276 -1.8069613 -1.3460059][-1.9054193 -1.7610059 -1.9471028 -2.2403567 -2.2351265 -2.0126829 -1.7380326 -1.4339182 -1.269299 -1.359174 -1.5143161 -1.8098345 -2.1873078 -2.025955 -1.5750167][-1.8818641 -1.7320669 -1.9933548 -2.3254085 -2.3363822 -2.2546585 -2.2493589 -2.1242604 -2.11559 -2.3382761 -2.4244092 -2.3653421 -2.4883099 -2.3659608 -1.8604522][-1.7043166 -1.5349436 -1.815222 -2.0978372 -2.0312126 -1.9259732 -2.0163908 -2.075891 -2.3451073 -2.8392112 -3.0355117 -2.8430181 -2.8689754 -2.8154044 -2.2607393][-1.3463402 -1.2424674 -1.4950588 -1.6415613 -1.3917308 -1.1987588 -1.3019121 -1.4679639 -1.9254837 -2.5706367 -2.8242226 -2.6793046 -2.862771 -3.0466166 -2.6036315][-0.82431579 -0.90362954 -1.1501815 -1.1571112 -0.81918716 -0.62314034 -0.63897991 -0.69815135 -1.2004695 -1.9110281 -2.13901 -2.060302 -2.4621949 -2.8964963 -2.674875]]...]
INFO - root - 2017-12-07 06:29:37.188087: step 13610, loss = 0.77, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 68h:07m:01s remains)
INFO - root - 2017-12-07 06:29:44.800414: step 13620, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.762 sec/batch; 67h:30m:56s remains)
INFO - root - 2017-12-07 06:29:52.203853: step 13630, loss = 0.91, batch loss = 0.84 (9.7 examples/sec; 0.824 sec/batch; 73h:01m:37s remains)
INFO - root - 2017-12-07 06:29:59.841846: step 13640, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.763 sec/batch; 67h:34m:33s remains)
INFO - root - 2017-12-07 06:30:07.602869: step 13650, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.767 sec/batch; 67h:55m:05s remains)
INFO - root - 2017-12-07 06:30:15.155631: step 13660, loss = 0.92, batch loss = 0.85 (10.3 examples/sec; 0.773 sec/batch; 68h:29m:06s remains)
INFO - root - 2017-12-07 06:30:22.830310: step 13670, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.763 sec/batch; 67h:36m:24s remains)
INFO - root - 2017-12-07 06:30:30.478442: step 13680, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.762 sec/batch; 67h:28m:05s remains)
INFO - root - 2017-12-07 06:30:38.120905: step 13690, loss = 0.82, batch loss = 0.74 (10.3 examples/sec; 0.773 sec/batch; 68h:27m:08s remains)
INFO - root - 2017-12-07 06:30:45.793590: step 13700, loss = 1.07, batch loss = 1.00 (10.5 examples/sec; 0.759 sec/batch; 67h:14m:55s remains)
2017-12-07 06:30:46.485603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6466646 -2.7897537 -2.7682714 -2.569494 -2.4279435 -2.3476474 -2.245857 -2.1209564 -2.0097866 -1.9895613 -2.1186805 -2.27214 -2.416409 -2.533287 -2.6376624][-3.1194229 -3.2363725 -3.110373 -2.8263974 -2.7108097 -2.6902914 -2.5983391 -2.4499607 -2.291343 -2.2097328 -2.2870038 -2.3957903 -2.5274336 -2.6451783 -2.7774734][-3.3294063 -3.3866465 -3.1925886 -2.9377303 -2.9369192 -2.9695544 -2.8585551 -2.7197409 -2.5931249 -2.4942236 -2.4914322 -2.5120418 -2.5878134 -2.652216 -2.7492814][-3.2373857 -3.227622 -3.0323722 -2.9072804 -3.0525842 -3.11344 -2.9808807 -2.9038661 -2.8966987 -2.8330312 -2.7496071 -2.6556048 -2.6266327 -2.5872421 -2.5961671][-2.9739423 -2.8864932 -2.7230597 -2.7472243 -2.9842381 -3.0685833 -2.9518704 -2.959523 -3.0514407 -3.0142031 -2.8800011 -2.7335966 -2.6181746 -2.4783359 -2.3892534][-2.7321937 -2.6032374 -2.5147529 -2.6603847 -2.9487119 -3.0539479 -2.9617572 -2.9937413 -3.0624371 -2.9933038 -2.8781011 -2.7662449 -2.6201651 -2.4301391 -2.292197][-2.7775788 -2.6955404 -2.6908498 -2.8737254 -3.1278729 -3.2088332 -3.11475 -3.0980477 -3.0511723 -2.9188936 -2.8515074 -2.766398 -2.5972624 -2.3946514 -2.2542639][-2.9365926 -2.9662151 -3.0589919 -3.2436616 -3.4356158 -3.4625394 -3.3338304 -3.2527452 -3.107064 -2.9280543 -2.9089675 -2.829067 -2.645637 -2.4422474 -2.2927759][-2.9740667 -3.0946002 -3.2893291 -3.5329592 -3.7518375 -3.8104632 -3.6645396 -3.49134 -3.2544489 -3.0577159 -3.0819881 -3.0167685 -2.8116443 -2.584249 -2.3857219][-2.7746506 -2.9720488 -3.2945762 -3.6582475 -3.9661775 -4.1094136 -3.9625213 -3.7075026 -3.4448328 -3.3046432 -3.371943 -3.326113 -3.0895286 -2.786746 -2.4896429][-2.4392848 -2.7194917 -3.1469202 -3.6132243 -3.9980152 -4.20436 -4.0817971 -3.821718 -3.6657712 -3.6515455 -3.7388613 -3.6703184 -3.384037 -2.9678121 -2.5700288][-2.2399378 -2.5224869 -2.9099164 -3.3348327 -3.7393296 -4.0200386 -3.9930356 -3.8231673 -3.8107371 -3.8832872 -3.943604 -3.8273153 -3.5040212 -3.0196683 -2.5921082][-2.518266 -2.7019215 -2.8976984 -3.1450505 -3.4719384 -3.7461061 -3.7625647 -3.6434093 -3.6844544 -3.7795789 -3.8138962 -3.6896865 -3.3943748 -2.9381957 -2.5750732][-3.1944382 -3.2870541 -3.3295174 -3.3833556 -3.5094535 -3.6039324 -3.4976721 -3.3146772 -3.3029432 -3.3906126 -3.439229 -3.368052 -3.1502972 -2.7818375 -2.5239329][-3.8376796 -3.8693628 -3.8284559 -3.7316313 -3.6381404 -3.4961281 -3.2214613 -2.9465714 -2.8664641 -2.9400196 -3.0241785 -3.0188155 -2.8838978 -2.6079092 -2.4447808]]...]
INFO - root - 2017-12-07 06:30:54.211187: step 13710, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 68h:16m:05s remains)
INFO - root - 2017-12-07 06:31:01.863722: step 13720, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.770 sec/batch; 68h:09m:06s remains)
INFO - root - 2017-12-07 06:31:09.222235: step 13730, loss = 0.94, batch loss = 0.87 (10.6 examples/sec; 0.756 sec/batch; 66h:58m:26s remains)
INFO - root - 2017-12-07 06:31:16.883383: step 13740, loss = 0.70, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 70h:53m:46s remains)
INFO - root - 2017-12-07 06:31:24.517937: step 13750, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 67h:04m:01s remains)
INFO - root - 2017-12-07 06:31:32.159372: step 13760, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.752 sec/batch; 66h:34m:19s remains)
INFO - root - 2017-12-07 06:31:39.892767: step 13770, loss = 0.93, batch loss = 0.86 (10.5 examples/sec; 0.764 sec/batch; 67h:39m:52s remains)
INFO - root - 2017-12-07 06:31:47.538628: step 13780, loss = 0.82, batch loss = 0.74 (10.7 examples/sec; 0.750 sec/batch; 66h:22m:32s remains)
INFO - root - 2017-12-07 06:31:55.182993: step 13790, loss = 0.74, batch loss = 0.67 (10.0 examples/sec; 0.801 sec/batch; 70h:54m:03s remains)
INFO - root - 2017-12-07 06:32:02.799871: step 13800, loss = 0.77, batch loss = 0.70 (10.1 examples/sec; 0.788 sec/batch; 69h:47m:34s remains)
2017-12-07 06:32:03.481372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.525753 -2.4168487 -2.4290142 -2.5970984 -2.7597885 -2.8656645 -2.9973025 -3.0483394 -3.1100054 -3.1894827 -3.1735811 -3.0705509 -3.0805824 -3.2242432 -3.4217477][-2.2167153 -2.0427172 -2.081984 -2.3791127 -2.6108027 -2.6933103 -2.7274399 -2.6480885 -2.7009897 -2.9664788 -3.1236444 -3.0926638 -3.1560423 -3.2588396 -3.3989153][-2.4297938 -2.2693672 -2.3080053 -2.5791192 -2.709681 -2.6773419 -2.5612574 -2.2391329 -2.2063293 -2.6814294 -3.0817728 -3.2167373 -3.367609 -3.3346276 -3.2790928][-2.8391671 -2.7882142 -2.8765001 -3.032052 -2.8947177 -2.5867047 -2.2200389 -1.635438 -1.561708 -2.3102973 -2.9865592 -3.2878594 -3.5187314 -3.3651004 -3.1255395][-3.0922627 -3.1180811 -3.3387578 -3.4725676 -3.0948758 -2.4833782 -1.8091228 -0.94666362 -0.84052157 -1.886961 -2.8536115 -3.2654552 -3.5284688 -3.3448272 -3.0406656][-3.1222258 -3.1228771 -3.4445734 -3.6171966 -3.1439989 -2.3688571 -1.4405012 -0.17938089 0.17092562 -1.0507231 -2.3486063 -2.8870173 -3.1216395 -2.9646335 -2.7549381][-3.0061553 -2.9668145 -3.2493885 -3.367723 -2.8897233 -2.1681237 -1.1936779 0.43834162 1.272615 0.046870708 -1.6633806 -2.4316139 -2.5966218 -2.4339957 -2.3569751][-2.7454467 -2.7549863 -2.9372602 -2.933651 -2.4636779 -1.9035165 -1.0829899 0.61375284 1.8072109 0.79838371 -1.1046169 -2.0089455 -2.082346 -1.9148812 -1.9588585][-2.4115539 -2.5712311 -2.7452862 -2.6925273 -2.2937031 -1.9339149 -1.3757989 0.0032019615 1.2213626 0.66370344 -0.88685274 -1.5628269 -1.4845631 -1.3290887 -1.5141726][-1.993469 -2.251024 -2.4420581 -2.4230318 -2.2077205 -2.0508323 -1.7216368 -0.79664636 0.24279213 0.18539476 -0.66787767 -0.91440058 -0.73943663 -0.70178986 -1.1181462][-1.7242925 -2.0023489 -2.2210107 -2.2841671 -2.2986071 -2.2776625 -2.0233133 -1.3768013 -0.51613045 -0.22583818 -0.50478721 -0.39561796 -0.108634 -0.15505791 -0.79148889][-1.7540658 -1.9510295 -2.1650631 -2.3715482 -2.5961647 -2.5759864 -2.2670689 -1.7291524 -1.0402033 -0.72117448 -0.82403493 -0.54173017 -0.0600276 -0.018481255 -0.74229455][-2.1754336 -2.1707695 -2.2587109 -2.5270798 -2.862994 -2.8627708 -2.5710731 -2.1748719 -1.7469718 -1.6132488 -1.7193489 -1.3364265 -0.64349914 -0.35982609 -0.9377687][-2.8721619 -2.6713128 -2.6078274 -2.8142605 -3.1160517 -3.1531305 -3.0083611 -2.8533182 -2.6685433 -2.6423941 -2.7232907 -2.3215091 -1.5607674 -1.0819483 -1.3695636][-3.3579156 -3.0771189 -2.9212255 -3.0388851 -3.2664373 -3.3345294 -3.3353598 -3.3566937 -3.3210866 -3.309402 -3.3490782 -3.0287457 -2.3826888 -1.918011 -2.0073926]]...]
INFO - root - 2017-12-07 06:32:11.112204: step 13810, loss = 0.78, batch loss = 0.70 (10.6 examples/sec; 0.754 sec/batch; 66h:44m:15s remains)
INFO - root - 2017-12-07 06:32:18.773637: step 13820, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.760 sec/batch; 67h:15m:38s remains)
INFO - root - 2017-12-07 06:32:26.161705: step 13830, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.775 sec/batch; 68h:37m:49s remains)
INFO - root - 2017-12-07 06:32:33.805263: step 13840, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.751 sec/batch; 66h:31m:12s remains)
INFO - root - 2017-12-07 06:32:41.428750: step 13850, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 67h:16m:53s remains)
INFO - root - 2017-12-07 06:32:49.074895: step 13860, loss = 0.80, batch loss = 0.73 (10.9 examples/sec; 0.735 sec/batch; 65h:01m:32s remains)
INFO - root - 2017-12-07 06:32:56.634433: step 13870, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.767 sec/batch; 67h:51m:01s remains)
INFO - root - 2017-12-07 06:33:04.289236: step 13880, loss = 0.75, batch loss = 0.68 (11.1 examples/sec; 0.723 sec/batch; 63h:58m:05s remains)
INFO - root - 2017-12-07 06:33:11.889837: step 13890, loss = 0.71, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 68h:53m:27s remains)
INFO - root - 2017-12-07 06:33:19.513316: step 13900, loss = 0.62, batch loss = 0.55 (10.7 examples/sec; 0.746 sec/batch; 66h:01m:06s remains)
2017-12-07 06:33:20.199831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18250179 0.076580048 0.18362045 0.2453208 0.36309624 0.21809244 0.19544268 0.37397337 0.23075056 0.26822853 0.53722858 0.52203465 0.49526262 0.4498 0.45341492][-0.050117493 0.24914598 0.381804 0.43984509 0.52478647 0.36093283 0.31991673 0.44830847 0.30473042 0.44549417 0.69515324 0.65041542 0.66755676 0.65580988 0.67199707][-0.054252148 0.18891859 0.32667351 0.46517849 0.5692749 0.39142609 0.29052925 0.3712039 0.25728321 0.41789532 0.60390997 0.6024394 0.6718626 0.67618132 0.74269676][-0.12620449 0.023687363 0.15963793 0.38889122 0.52433062 0.37325811 0.21891499 0.26062393 0.1980896 0.31454754 0.41512585 0.46653652 0.53970289 0.53628206 0.6664052][-0.16894197 -0.088726521 0.07228756 0.36383772 0.51241684 0.39695597 0.18935251 0.21327114 0.23130655 0.26279354 0.2468586 0.3187294 0.38588142 0.38096189 0.54976654][-0.13836575 -0.096754074 0.097276688 0.41564226 0.58233404 0.523211 0.27314043 0.31130743 0.40514231 0.29768944 0.12429523 0.15384054 0.24229002 0.25998831 0.40465927][-0.13955879 -0.12024307 0.077773094 0.37992573 0.598649 0.62903881 0.39655542 0.44927502 0.57260752 0.31168747 -0.021206379 -0.056302071 0.034904003 0.062950134 0.18136406][-0.3895483 -0.39101171 -0.24294233 0.0077967644 0.27859402 0.39794636 0.22986317 0.29028034 0.39420795 0.015996933 -0.435858 -0.51139188 -0.42208815 -0.37986231 -0.22530985][-0.8976233 -0.917104 -0.82871103 -0.64875174 -0.3879652 -0.25242996 -0.34963942 -0.26621294 -0.16520023 -0.56307387 -1.0274167 -1.0800636 -0.99155807 -0.94456458 -0.77020049][-1.4761586 -1.5480525 -1.5209279 -1.3842349 -1.1747556 -1.0802732 -1.134203 -1.0205092 -0.88659906 -1.2085791 -1.6100888 -1.635237 -1.5731633 -1.5587547 -1.4372017][-1.9479277 -2.1275434 -2.2072618 -2.1185114 -1.9455595 -1.8695714 -1.8962433 -1.8077865 -1.6816752 -1.8926525 -2.1694846 -2.1734326 -2.1545959 -2.1817424 -2.1382248][-2.3184114 -2.58594 -2.7690439 -2.7463579 -2.6232796 -2.5587547 -2.5608144 -2.5202765 -2.4358335 -2.5633187 -2.7361293 -2.745167 -2.7792058 -2.8311467 -2.8188055][-2.5267735 -2.7862287 -3.0207038 -3.079463 -3.0408092 -3.0139489 -3.0221219 -3.0158923 -2.9590106 -3.0286779 -3.1410713 -3.1608968 -3.2209668 -3.2752361 -3.2644205][-2.4589438 -2.6489525 -2.8867831 -3.0084708 -3.0464106 -3.0530334 -3.080039 -3.0897398 -3.0363846 -3.063601 -3.1480532 -3.1738641 -3.2288146 -3.2599721 -3.2406898][-2.3259041 -2.4735732 -2.6974745 -2.8416617 -2.9102619 -2.9246445 -2.94603 -2.9545951 -2.9140704 -2.9272223 -2.9921079 -3.016937 -3.0561333 -3.0639086 -3.0305147]]...]
INFO - root - 2017-12-07 06:33:27.893648: step 13910, loss = 0.62, batch loss = 0.55 (10.6 examples/sec; 0.752 sec/batch; 66h:30m:28s remains)
INFO - root - 2017-12-07 06:33:35.531676: step 13920, loss = 0.68, batch loss = 0.60 (10.6 examples/sec; 0.757 sec/batch; 66h:59m:57s remains)
INFO - root - 2017-12-07 06:33:42.903874: step 13930, loss = 0.86, batch loss = 0.78 (10.4 examples/sec; 0.772 sec/batch; 68h:16m:38s remains)
INFO - root - 2017-12-07 06:33:50.450747: step 13940, loss = 0.58, batch loss = 0.51 (10.5 examples/sec; 0.764 sec/batch; 67h:37m:24s remains)
INFO - root - 2017-12-07 06:33:58.082125: step 13950, loss = 0.64, batch loss = 0.57 (10.9 examples/sec; 0.736 sec/batch; 65h:05m:45s remains)
INFO - root - 2017-12-07 06:34:05.740048: step 13960, loss = 0.98, batch loss = 0.91 (10.6 examples/sec; 0.758 sec/batch; 67h:02m:24s remains)
INFO - root - 2017-12-07 06:34:13.418999: step 13970, loss = 0.75, batch loss = 0.67 (10.7 examples/sec; 0.749 sec/batch; 66h:15m:22s remains)
INFO - root - 2017-12-07 06:34:21.047949: step 13980, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.753 sec/batch; 66h:36m:14s remains)
INFO - root - 2017-12-07 06:34:28.690299: step 13990, loss = 0.73, batch loss = 0.66 (10.8 examples/sec; 0.742 sec/batch; 65h:40m:05s remains)
INFO - root - 2017-12-07 06:34:36.316523: step 14000, loss = 0.84, batch loss = 0.76 (10.4 examples/sec; 0.767 sec/batch; 67h:53m:32s remains)
2017-12-07 06:34:36.943529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4767227 -3.907352 -3.9975581 -3.2786794 -2.3697917 -1.9057755 -2.0694895 -2.6258812 -2.9445138 -2.6466827 -1.7137492 -0.86048222 -0.85270095 -1.191025 -1.4443669][-3.5087132 -3.7833266 -3.5286605 -2.5262032 -1.5731869 -1.3107281 -1.8518693 -2.6775177 -2.7563934 -1.7678692 -0.30856276 0.40774775 -0.0763154 -0.85476732 -1.4695623][-3.7133911 -3.735414 -3.1184797 -1.9611986 -1.1290896 -1.2103772 -2.1131399 -2.9244852 -2.3856268 -0.57387066 1.1186433 1.2555046 0.013531685 -1.1920764 -1.9289014][-3.6181118 -3.3038006 -2.5897689 -1.7536647 -1.3703864 -1.8409593 -2.7212138 -2.9178395 -1.4030228 1.0677147 2.4620991 1.6469054 -0.34880733 -1.8010802 -2.4428995][-3.6045849 -3.038588 -2.3772962 -2.0095651 -2.0740125 -2.6002097 -2.8636296 -2.1124489 0.032115936 2.3136258 2.7099962 0.96914625 -1.2603781 -2.4358053 -2.6916997][-3.5354109 -2.9612238 -2.5455389 -2.5553493 -2.6396379 -2.5662503 -1.7769461 -0.31802797 1.6154256 2.8388224 1.953959 -0.35219145 -2.246428 -2.8013752 -2.6238003][-3.0877821 -2.5908473 -2.4104986 -2.5766158 -2.3351266 -1.416934 0.19900846 1.6472182 2.5948658 2.51795 0.839396 -1.3393183 -2.5258031 -2.4977605 -2.1216984][-2.9452846 -2.6783307 -2.6222343 -2.5903563 -1.7234025 -0.07916975 1.6565843 2.1698279 1.7652836 0.9209013 -0.63596606 -2.11851 -2.5495033 -2.0975838 -1.6334846][-3.0657916 -3.1691732 -3.2100015 -2.9510722 -1.610903 0.37401485 1.596745 0.9571414 -0.3204937 -1.0891283 -1.8543661 -2.5213296 -2.4773815 -1.9237955 -1.5690007][-3.2062798 -3.5465152 -3.5734043 -3.1364927 -1.6577456 0.11221075 0.4741931 -1.0213318 -2.4493279 -2.6586688 -2.5919666 -2.6192346 -2.3759074 -2.0113049 -1.9585521][-3.5389671 -4.1443939 -4.1573734 -3.5358026 -2.060631 -0.64163351 -0.87758088 -2.5853562 -3.6907296 -3.405432 -2.8516083 -2.5406981 -2.2883196 -2.2399642 -2.5088363][-3.5147851 -4.2638855 -4.2654533 -3.5743594 -2.2860417 -1.2591679 -1.7170534 -3.1492054 -3.791923 -3.2348738 -2.5390191 -2.1425562 -1.9852767 -2.2231116 -2.687284][-3.3154659 -3.9197822 -3.8566809 -3.1873422 -2.1162839 -1.4101756 -1.9352903 -3.0662804 -3.4513996 -2.866905 -2.2265997 -1.881074 -1.8390846 -2.2208555 -2.7022462][-3.3789122 -3.7884459 -3.7505724 -3.2413683 -2.4035501 -1.8916912 -2.3022258 -3.0647829 -3.2643952 -2.7724557 -2.3244529 -2.1573277 -2.2191603 -2.5542984 -2.8809602][-3.3197491 -3.5107102 -3.5209754 -3.2726407 -2.7797894 -2.4691424 -2.6769857 -3.0412974 -3.08844 -2.761399 -2.5311675 -2.5257702 -2.6309617 -2.829401 -2.9529533]]...]
INFO - root - 2017-12-07 06:34:44.646769: step 14010, loss = 0.81, batch loss = 0.74 (9.9 examples/sec; 0.807 sec/batch; 71h:22m:35s remains)
INFO - root - 2017-12-07 06:34:52.269225: step 14020, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.778 sec/batch; 68h:49m:11s remains)
INFO - root - 2017-12-07 06:34:59.639736: step 14030, loss = 0.92, batch loss = 0.84 (10.6 examples/sec; 0.754 sec/batch; 66h:44m:27s remains)
INFO - root - 2017-12-07 06:35:07.295394: step 14040, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.773 sec/batch; 68h:20m:25s remains)
INFO - root - 2017-12-07 06:35:14.955680: step 14050, loss = 0.90, batch loss = 0.83 (10.7 examples/sec; 0.748 sec/batch; 66h:11m:15s remains)
INFO - root - 2017-12-07 06:35:22.673922: step 14060, loss = 0.62, batch loss = 0.55 (10.3 examples/sec; 0.776 sec/batch; 68h:37m:53s remains)
INFO - root - 2017-12-07 06:35:30.284323: step 14070, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.767 sec/batch; 67h:50m:21s remains)
INFO - root - 2017-12-07 06:35:38.043355: step 14080, loss = 0.81, batch loss = 0.74 (9.9 examples/sec; 0.812 sec/batch; 71h:48m:55s remains)
INFO - root - 2017-12-07 06:35:45.700287: step 14090, loss = 0.70, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 70h:43m:03s remains)
INFO - root - 2017-12-07 06:35:53.334664: step 14100, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.763 sec/batch; 67h:30m:29s remains)
2017-12-07 06:35:53.915285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9957447 -2.0275304 -2.0391023 -2.0509276 -2.0359821 -1.9710007 -1.9474142 -1.9662645 -1.9727492 -1.9293036 -1.8049645 -1.7406149 -1.9065335 -2.091027 -2.1444874][-1.5230231 -1.5610125 -1.6038604 -1.6492572 -1.6701665 -1.6339967 -1.6284618 -1.6624315 -1.6681957 -1.6103449 -1.4531393 -1.354306 -1.5401857 -1.7231014 -1.7383878][-1.2312829 -1.3208323 -1.4046392 -1.4200664 -1.4167132 -1.3848574 -1.3829226 -1.4229999 -1.4659576 -1.4789212 -1.3723059 -1.2496014 -1.3957624 -1.5359061 -1.479615][-0.84620166 -0.96796894 -1.0877209 -1.062578 -1.0044944 -0.98058081 -0.95411348 -0.917166 -0.91122723 -0.95205855 -0.93703771 -0.86505294 -1.0328255 -1.2177258 -1.1844773][-0.24081039 -0.39109182 -0.57916665 -0.57955384 -0.5159936 -0.54403782 -0.49855185 -0.37000179 -0.329216 -0.41015673 -0.48028493 -0.47913289 -0.70352435 -0.96107793 -0.99436212][0.34974861 0.18882465 -0.077368259 -0.19698668 -0.22068453 -0.36817312 -0.36987543 -0.1724906 -0.14935684 -0.31225014 -0.38972425 -0.39879417 -0.6403594 -0.95876 -1.0453084][0.79209518 0.68007135 0.4216423 0.25250483 0.1950078 0.0037331581 0.004693985 0.29529524 0.27524376 -0.021585941 -0.14491129 -0.23517656 -0.58253884 -1.0225604 -1.2221556][1.3732262 1.3137822 1.1528273 1.0676756 1.0621891 0.92510605 0.96370888 1.3013635 1.1895137 0.71497536 0.47984982 0.25205421 -0.250659 -0.87126946 -1.2729466][1.8021569 1.734817 1.653913 1.6400862 1.6366825 1.5433025 1.5684738 1.835958 1.6602793 1.1308942 0.88335133 0.624845 0.073722363 -0.63037109 -1.1920042][1.7743578 1.7209468 1.7078681 1.7011728 1.6538205 1.6155663 1.6565018 1.8882418 1.7708211 1.3367624 1.1429586 0.90825653 0.3450551 -0.39658689 -1.0543721][1.5482931 1.5238738 1.5305057 1.4686561 1.3306065 1.2823033 1.3063331 1.4978318 1.5058894 1.2497492 1.1622763 0.98389435 0.43554306 -0.30590725 -0.97214603][0.95050621 0.93772364 0.94018269 0.83280754 0.64117718 0.55992937 0.53257895 0.65263367 0.72207785 0.60524988 0.59509611 0.48497772 0.0036950111 -0.6432848 -1.1931326][0.023542404 0.034230232 0.034738064 -0.083549976 -0.25808096 -0.34581423 -0.3930316 -0.32521915 -0.25785732 -0.32013369 -0.34040642 -0.43861389 -0.846215 -1.3490882 -1.7213435][-0.87142205 -0.85735965 -0.88331819 -1.0048718 -1.1413829 -1.2150159 -1.2506368 -1.2093198 -1.1476216 -1.1712611 -1.2157824 -1.3255975 -1.6524773 -2.0227959 -2.2560847][-1.5622702 -1.5736222 -1.621244 -1.7273757 -1.8175936 -1.873687 -1.9156599 -1.9141722 -1.877588 -1.877573 -1.9143548 -1.9941466 -2.2073748 -2.4459119 -2.5930276]]...]
INFO - root - 2017-12-07 06:36:01.566831: step 14110, loss = 0.90, batch loss = 0.83 (10.7 examples/sec; 0.746 sec/batch; 66h:01m:03s remains)
INFO - root - 2017-12-07 06:36:09.193656: step 14120, loss = 1.12, batch loss = 1.04 (10.1 examples/sec; 0.795 sec/batch; 70h:19m:06s remains)
INFO - root - 2017-12-07 06:36:16.670886: step 14130, loss = 0.67, batch loss = 0.59 (10.5 examples/sec; 0.763 sec/batch; 67h:29m:17s remains)
INFO - root - 2017-12-07 06:36:24.319600: step 14140, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.775 sec/batch; 68h:34m:26s remains)
INFO - root - 2017-12-07 06:36:32.075805: step 14150, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.779 sec/batch; 68h:51m:36s remains)
INFO - root - 2017-12-07 06:36:39.690717: step 14160, loss = 0.96, batch loss = 0.89 (10.6 examples/sec; 0.751 sec/batch; 66h:25m:30s remains)
INFO - root - 2017-12-07 06:36:47.321725: step 14170, loss = 1.11, batch loss = 1.04 (10.2 examples/sec; 0.783 sec/batch; 69h:11m:39s remains)
INFO - root - 2017-12-07 06:36:54.950694: step 14180, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.757 sec/batch; 66h:56m:26s remains)
INFO - root - 2017-12-07 06:37:02.584416: step 14190, loss = 0.95, batch loss = 0.88 (10.8 examples/sec; 0.738 sec/batch; 65h:16m:53s remains)
INFO - root - 2017-12-07 06:37:10.147350: step 14200, loss = 0.75, batch loss = 0.68 (10.6 examples/sec; 0.754 sec/batch; 66h:37m:37s remains)
2017-12-07 06:37:10.764886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3738139 -3.6349752 -3.8109219 -3.744144 -3.487936 -3.3361335 -3.3859959 -3.4589715 -3.4869149 -3.4854674 -3.5179691 -3.6058393 -3.6598351 -3.6006088 -3.4613705][-3.2298152 -3.5350144 -3.709048 -3.5020194 -3.0535798 -2.8137922 -2.8944378 -3.0421436 -3.1378593 -3.1607621 -3.16954 -3.2694635 -3.35111 -3.2803655 -3.1220307][-3.2760019 -3.6155117 -3.7848308 -3.4652722 -2.8625355 -2.54503 -2.6068592 -2.7494721 -2.8961926 -2.9839346 -2.9939008 -3.0765657 -3.1460147 -3.0645912 -2.9191751][-3.4308095 -3.7484488 -3.8736901 -3.4795327 -2.8288398 -2.5289469 -2.5948925 -2.6887765 -2.8137932 -2.9261684 -2.963593 -3.0496948 -3.089992 -2.9916112 -2.8939779][-3.7198873 -4.0501347 -4.1014924 -3.5729945 -2.7948341 -2.4623122 -2.5704012 -2.7059283 -2.8335516 -2.9004273 -2.8847241 -2.9579306 -3.0167394 -2.9482992 -2.9265165][-3.9956348 -4.3891115 -4.4196134 -3.8231623 -2.9173832 -2.410346 -2.401602 -2.5691648 -2.8227327 -2.9624112 -2.8748488 -2.8799868 -2.9559894 -2.9823484 -3.0607991][-4.1286378 -4.5449328 -4.5900993 -4.0553589 -3.1481736 -2.4101264 -2.0447364 -2.0095654 -2.3954225 -2.8582933 -2.9079432 -2.8471236 -2.8498712 -2.9026842 -3.0539007][-4.2223854 -4.6424346 -4.7030239 -4.2654743 -3.4604154 -2.6712761 -1.9641581 -1.4469843 -1.6505997 -2.3519425 -2.6682658 -2.6557615 -2.5868185 -2.5912566 -2.7547379][-4.1945853 -4.593276 -4.6630116 -4.3104911 -3.6430888 -3.0496621 -2.4167469 -1.6838343 -1.488374 -2.0230849 -2.38758 -2.4731665 -2.4466987 -2.4162326 -2.5413055][-4.0348639 -4.4196086 -4.5172997 -4.2031689 -3.6137228 -3.1854973 -2.8120079 -2.2618816 -2.0248761 -2.3292439 -2.5045507 -2.538079 -2.5487757 -2.4722774 -2.4980171][-3.9453597 -4.4020748 -4.5982342 -4.3095059 -3.7201257 -3.2666891 -3.028131 -2.7450442 -2.6414459 -2.8569117 -2.8948989 -2.8302393 -2.8019643 -2.6521754 -2.5421906][-3.8506207 -4.3570409 -4.6441588 -4.4280758 -3.8792655 -3.3265009 -3.0598185 -2.9857407 -3.0095685 -3.1676328 -3.1487341 -2.9711392 -2.8395343 -2.6334324 -2.490546][-3.6911633 -4.1631584 -4.4694052 -4.3674364 -3.9523385 -3.3456581 -2.9370661 -2.9167094 -3.0140419 -3.2014754 -3.2545848 -3.0514164 -2.7956042 -2.4925427 -2.3584573][-3.5913115 -3.9701667 -4.2098808 -4.1408854 -3.8690515 -3.372761 -2.935473 -2.8714221 -2.8846538 -3.0357184 -3.1911378 -3.0762105 -2.8153763 -2.4653931 -2.2930179][-3.5955153 -3.8973927 -4.06016 -3.9202926 -3.6885188 -3.3739767 -3.1215153 -3.1462712 -3.1082742 -3.1440544 -3.2598546 -3.1892171 -2.9615545 -2.6144068 -2.3933661]]...]
INFO - root - 2017-12-07 06:37:18.327044: step 14210, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.761 sec/batch; 67h:14m:55s remains)
INFO - root - 2017-12-07 06:37:26.022573: step 14220, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.760 sec/batch; 67h:10m:53s remains)
INFO - root - 2017-12-07 06:37:33.488846: step 14230, loss = 1.08, batch loss = 1.01 (10.1 examples/sec; 0.793 sec/batch; 70h:04m:43s remains)
INFO - root - 2017-12-07 06:37:41.129431: step 14240, loss = 0.68, batch loss = 0.61 (10.5 examples/sec; 0.764 sec/batch; 67h:31m:12s remains)
INFO - root - 2017-12-07 06:37:48.892149: step 14250, loss = 0.74, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 69h:19m:22s remains)
INFO - root - 2017-12-07 06:37:56.523303: step 14260, loss = 0.72, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 68h:24m:53s remains)
INFO - root - 2017-12-07 06:38:04.199605: step 14270, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.787 sec/batch; 69h:36m:13s remains)
INFO - root - 2017-12-07 06:38:11.809786: step 14280, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.758 sec/batch; 66h:58m:45s remains)
INFO - root - 2017-12-07 06:38:19.435941: step 14290, loss = 0.89, batch loss = 0.81 (10.4 examples/sec; 0.769 sec/batch; 67h:58m:35s remains)
INFO - root - 2017-12-07 06:38:27.075895: step 14300, loss = 0.91, batch loss = 0.84 (10.7 examples/sec; 0.746 sec/batch; 65h:58m:02s remains)
2017-12-07 06:38:27.667322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.519731 -2.4132378 -2.4897842 -2.6250811 -2.6361318 -2.7163925 -2.8820376 -2.9790781 -3.093817 -3.0643728 -2.9423985 -3.04191 -3.3211918 -3.5135534 -3.5697274][-2.3856232 -2.2752419 -2.37765 -2.5514915 -2.6139131 -2.7469931 -2.8259315 -2.8141265 -2.9969354 -3.1316609 -3.1676009 -3.3469443 -3.5262532 -3.5977664 -3.6465662][-2.1117136 -2.0390189 -2.1190727 -2.3116961 -2.4498644 -2.5813093 -2.4419115 -2.2703133 -2.5841813 -3.0080976 -3.2786584 -3.4990482 -3.5108981 -3.4846125 -3.5879669][-1.8077939 -1.7441022 -1.7484441 -1.9994731 -2.2817249 -2.3818362 -1.9875946 -1.6977308 -2.1968873 -2.9174066 -3.3618119 -3.5452909 -3.3893871 -3.317822 -3.5007825][-1.7258215 -1.6781335 -1.5311491 -1.7278359 -2.0643816 -2.0336721 -1.3937101 -1.0599778 -1.7735667 -2.7885461 -3.4045529 -3.6032956 -3.3667912 -3.3040128 -3.5274348][-1.6886387 -1.6877046 -1.37657 -1.3647144 -1.5837181 -1.3567214 -0.53618813 -0.2481432 -1.1398323 -2.3429797 -3.0870872 -3.3759108 -3.1790318 -3.1973429 -3.4870014][-1.7720621 -1.7085731 -1.2483883 -0.99306059 -0.99987125 -0.57559872 0.33073807 0.43109179 -0.76362586 -2.1178415 -2.8815417 -3.1566517 -3.0026822 -3.1191182 -3.4405134][-1.989578 -1.6905856 -1.1728275 -0.82351279 -0.67216206 -0.095690727 0.87734747 0.740448 -0.75334096 -2.2035675 -2.856401 -3.0015016 -2.9006114 -3.1410007 -3.4807019][-2.1053128 -1.6355886 -1.2396076 -0.978194 -0.76907992 -0.15215635 0.71046972 0.34261131 -1.2427227 -2.5787554 -2.9690535 -2.8976533 -2.7946823 -3.1041362 -3.4290109][-2.1219227 -1.6743009 -1.5094256 -1.3842874 -1.1673768 -0.63273907 -0.053099632 -0.58598065 -2.0506589 -3.1102207 -3.1892211 -2.84266 -2.6474369 -2.9145415 -3.2172556][-2.1509733 -1.8785043 -1.8723133 -1.8146999 -1.6837595 -1.3575296 -1.050406 -1.5707796 -2.6931682 -3.397068 -3.2702091 -2.7859168 -2.5219576 -2.7383809 -3.0282803][-2.3176219 -2.2061794 -2.2226784 -2.1687248 -2.1756952 -2.1402693 -2.0625491 -2.4329333 -3.1051941 -3.3940427 -3.1496747 -2.7333517 -2.5192428 -2.725368 -3.0079641][-2.6403661 -2.5675368 -2.5517821 -2.5101306 -2.641562 -2.85947 -2.9380286 -3.1543205 -3.41461 -3.3393559 -3.0479231 -2.8091404 -2.7343907 -2.9309859 -3.164598][-3.0206726 -2.9793186 -2.9668622 -2.9449143 -3.0773807 -3.3362527 -3.4440618 -3.5082369 -3.4911046 -3.2015617 -2.9363275 -2.8853302 -2.9604933 -3.1358275 -3.3094518][-3.2664385 -3.2678266 -3.2829123 -3.2948346 -3.3708115 -3.5307667 -3.6144364 -3.639744 -3.5604048 -3.3009217 -3.1112208 -3.1223731 -3.2017641 -3.2741718 -3.3658919]]...]
INFO - root - 2017-12-07 06:38:35.252185: step 14310, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.752 sec/batch; 66h:28m:27s remains)
INFO - root - 2017-12-07 06:38:42.944683: step 14320, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.774 sec/batch; 68h:26m:57s remains)
INFO - root - 2017-12-07 06:38:50.339391: step 14330, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.758 sec/batch; 66h:58m:31s remains)
INFO - root - 2017-12-07 06:38:57.913901: step 14340, loss = 0.85, batch loss = 0.78 (10.8 examples/sec; 0.740 sec/batch; 65h:22m:16s remains)
INFO - root - 2017-12-07 06:39:05.506658: step 14350, loss = 0.85, batch loss = 0.77 (10.4 examples/sec; 0.769 sec/batch; 67h:59m:43s remains)
INFO - root - 2017-12-07 06:39:13.159445: step 14360, loss = 0.94, batch loss = 0.87 (10.0 examples/sec; 0.798 sec/batch; 70h:32m:44s remains)
INFO - root - 2017-12-07 06:39:20.760249: step 14370, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.775 sec/batch; 68h:27m:00s remains)
INFO - root - 2017-12-07 06:39:28.382497: step 14380, loss = 0.88, batch loss = 0.81 (10.3 examples/sec; 0.775 sec/batch; 68h:28m:59s remains)
INFO - root - 2017-12-07 06:39:36.154968: step 14390, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 66h:37m:40s remains)
INFO - root - 2017-12-07 06:39:43.800676: step 14400, loss = 0.85, batch loss = 0.78 (10.7 examples/sec; 0.749 sec/batch; 66h:09m:48s remains)
2017-12-07 06:39:44.376380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2448311 -4.5077615 -4.4641409 -4.1758828 -3.6659665 -3.339263 -3.3538504 -3.4963527 -3.6029229 -3.6492248 -3.6998925 -3.7625341 -3.8010135 -3.7964592 -3.6917055][-4.45855 -4.7547097 -4.8003445 -4.5844016 -4.115674 -3.8219805 -3.8039351 -3.8699265 -3.9306824 -3.9376092 -3.9529095 -4.013638 -4.0472312 -3.9941735 -3.7965171][-4.4765162 -4.7689595 -4.8567615 -4.660964 -4.2196031 -4.0227966 -4.0874748 -4.2637048 -4.4711967 -4.54926 -4.5106139 -4.4796324 -4.4163918 -4.2767229 -4.0116749][-4.2366252 -4.5209618 -4.6315517 -4.3817358 -3.8458133 -3.5385427 -3.4480572 -3.6268542 -4.0447655 -4.3576365 -4.4657159 -4.4942751 -4.4047704 -4.2280669 -3.9936328][-4.0340223 -4.3611865 -4.5319262 -4.2864804 -3.7135997 -3.2750516 -2.9402885 -2.9893498 -3.4569867 -3.9057355 -4.2081451 -4.39144 -4.3332367 -4.139998 -3.9381547][-4.0165815 -4.2982121 -4.3587728 -3.9431024 -3.2028162 -2.5359077 -1.987298 -2.0041757 -2.5860515 -3.1782405 -3.656688 -3.9551125 -3.8927424 -3.6313858 -3.3581777][-4.0522704 -4.3021574 -4.1815681 -3.4990051 -2.4639115 -1.392431 -0.53898478 -0.53398967 -1.2849677 -2.1036479 -2.830112 -3.3214247 -3.3654046 -3.1227446 -2.8322096][-4.1716518 -4.4860663 -4.34002 -3.6313195 -2.6001666 -1.4281096 -0.50419283 -0.49263406 -1.1995075 -1.9715393 -2.6657267 -3.11635 -3.1666229 -3.0017679 -2.8799367][-4.4254789 -4.8343797 -4.7609091 -4.2457032 -3.5062342 -2.6126947 -1.9389753 -1.9839568 -2.512248 -3.0275738 -3.4122529 -3.5400624 -3.377291 -3.1430326 -3.1035061][-4.4938006 -4.9652295 -4.9982142 -4.68981 -4.2189908 -3.6278265 -3.2280746 -3.3076634 -3.6492329 -3.9221506 -4.0263562 -3.912118 -3.6273851 -3.3831716 -3.4075823][-4.2630758 -4.7444487 -4.9321766 -4.8518434 -4.5894933 -4.2095914 -3.9905186 -4.0804586 -4.2967868 -4.3629417 -4.2171512 -3.917006 -3.5646379 -3.3412793 -3.4536076][-3.8751197 -4.19114 -4.3774433 -4.3845549 -4.2279744 -4.0035563 -3.954869 -4.1503325 -4.3814187 -4.3785682 -4.1250296 -3.7493172 -3.3861878 -3.1780496 -3.2857261][-3.62878 -3.7456148 -3.8396666 -3.8507752 -3.7799473 -3.7179315 -3.8162594 -4.0767126 -4.3040652 -4.267477 -4.0047445 -3.6635923 -3.3717394 -3.2244034 -3.3244853][-3.7608941 -3.7588172 -3.7618723 -3.7240543 -3.6727047 -3.6780946 -3.8137314 -4.0299911 -4.1693931 -4.0929356 -3.8651824 -3.6030774 -3.4000137 -3.2988586 -3.3581226][-4.2135105 -4.1842308 -4.0922284 -3.9323082 -3.805438 -3.7820787 -3.8765056 -4.0078115 -4.0653472 -3.9949188 -3.8497562 -3.7093601 -3.6299829 -3.6247919 -3.6892221]]...]
INFO - root - 2017-12-07 06:39:52.088325: step 14410, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.761 sec/batch; 67h:15m:11s remains)
INFO - root - 2017-12-07 06:39:59.763728: step 14420, loss = 1.01, batch loss = 0.93 (10.3 examples/sec; 0.778 sec/batch; 68h:41m:58s remains)
INFO - root - 2017-12-07 06:40:07.207804: step 14430, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.782 sec/batch; 69h:03m:53s remains)
INFO - root - 2017-12-07 06:40:14.872464: step 14440, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 66h:52m:18s remains)
INFO - root - 2017-12-07 06:40:22.544114: step 14450, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 66h:26m:37s remains)
INFO - root - 2017-12-07 06:40:30.107717: step 14460, loss = 0.65, batch loss = 0.58 (10.6 examples/sec; 0.755 sec/batch; 66h:41m:40s remains)
INFO - root - 2017-12-07 06:40:37.976418: step 14470, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.785 sec/batch; 69h:22m:40s remains)
INFO - root - 2017-12-07 06:40:45.799676: step 14480, loss = 1.05, batch loss = 0.98 (10.3 examples/sec; 0.775 sec/batch; 68h:29m:52s remains)
INFO - root - 2017-12-07 06:40:53.675319: step 14490, loss = 0.97, batch loss = 0.90 (10.2 examples/sec; 0.783 sec/batch; 69h:11m:27s remains)
INFO - root - 2017-12-07 06:41:01.328400: step 14500, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.772 sec/batch; 68h:09m:03s remains)
2017-12-07 06:41:02.024076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1602354 -4.7653852 -4.6415777 -4.6080832 -4.1466417 -3.27488 -2.7475979 -2.621851 -2.7975829 -2.8854203 -2.4196515 -2.0556297 -1.9750485 -2.0634077 -2.5630498][-5.1707458 -4.8357897 -5.0427351 -5.3453331 -4.8686886 -3.7498558 -2.9858606 -2.7321124 -2.9883475 -3.1407266 -2.4846225 -1.8594744 -1.6290574 -1.6986017 -2.2848282][-4.6344109 -4.297379 -4.6655831 -5.13549 -4.6786523 -3.5332737 -2.6691887 -2.2867889 -2.6716731 -2.9891248 -2.2554865 -1.4465303 -1.1743655 -1.3519328 -2.0411839][-3.9705889 -3.6869769 -4.1108694 -4.5266824 -3.9121401 -2.5498662 -1.3358626 -0.67581606 -1.3275824 -2.0920095 -1.6497953 -1.0506892 -1.0450027 -1.4679186 -2.1631174][-3.5974107 -3.4530871 -3.9782708 -4.3835068 -3.6388402 -1.9276719 -0.06403017 1.0856228 0.17476273 -1.145951 -1.1853533 -1.0254929 -1.4473515 -2.1953449 -2.8748348][-3.5773473 -3.4028594 -3.7979395 -4.0970807 -3.3547294 -1.5840218 0.63840437 2.0312514 0.85421419 -0.86632943 -1.2794743 -1.4276481 -2.0959935 -3.0087805 -3.619957][-3.6339567 -3.3482256 -3.5145235 -3.6422412 -2.9061224 -1.2310152 0.926342 2.0954971 0.61181545 -1.2664895 -1.8489056 -2.0457761 -2.5721674 -3.2771854 -3.6941526][-3.5393152 -3.2294059 -3.3689017 -3.5227232 -2.9246922 -1.4432614 0.3814764 1.1670709 -0.21553421 -1.7742038 -2.2577415 -2.3966947 -2.6547241 -2.9661844 -3.1228628][-3.5019588 -3.1954932 -3.3806825 -3.7139876 -3.4121885 -2.2419839 -0.88014817 -0.41067076 -1.362725 -2.3402131 -2.5559962 -2.5544071 -2.4915004 -2.4146733 -2.4347477][-3.6278868 -3.195626 -3.238802 -3.6224384 -3.5646324 -2.7489357 -1.8588605 -1.629081 -2.2084002 -2.78481 -2.855485 -2.70879 -2.353049 -2.0439754 -2.1374767][-3.9023175 -3.3308806 -3.1839962 -3.4712925 -3.4680457 -2.8466203 -2.2270808 -2.0698607 -2.3808353 -2.7714925 -2.9045081 -2.8864565 -2.619771 -2.3883073 -2.6051917][-4.2517705 -3.7504935 -3.7076666 -4.0484457 -3.9665363 -3.2824087 -2.5339661 -2.1362147 -2.2352855 -2.5583808 -2.7821574 -2.9342527 -2.8928864 -2.8314145 -3.0667472][-4.4173484 -4.057282 -4.2341952 -4.749258 -4.6894193 -4.0605116 -3.2350855 -2.5979271 -2.5549078 -2.7680397 -2.8856156 -3.0585895 -3.199264 -3.2713628 -3.3885295][-4.3478007 -4.1788287 -4.5124931 -5.0751295 -5.0768623 -4.6264052 -3.8672533 -3.138937 -3.1182997 -3.3587468 -3.4535427 -3.7019057 -4.0430417 -4.1917834 -4.0799222][-4.25588 -4.4015284 -4.8562264 -5.30377 -5.19997 -4.7026272 -3.9331615 -3.2424335 -3.3873172 -3.8043981 -4.0525866 -4.4307394 -4.82179 -4.8633709 -4.488997]]...]
INFO - root - 2017-12-07 06:41:09.669236: step 14510, loss = 0.71, batch loss = 0.63 (10.6 examples/sec; 0.755 sec/batch; 66h:39m:48s remains)
INFO - root - 2017-12-07 06:41:17.305734: step 14520, loss = 0.69, batch loss = 0.62 (10.7 examples/sec; 0.751 sec/batch; 66h:20m:11s remains)
INFO - root - 2017-12-07 06:41:24.654618: step 14530, loss = 0.92, batch loss = 0.85 (10.6 examples/sec; 0.756 sec/batch; 66h:46m:03s remains)
INFO - root - 2017-12-07 06:41:32.240917: step 14540, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.750 sec/batch; 66h:11m:53s remains)
INFO - root - 2017-12-07 06:41:39.952006: step 14550, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.774 sec/batch; 68h:21m:15s remains)
INFO - root - 2017-12-07 06:41:47.577380: step 14560, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.768 sec/batch; 67h:47m:00s remains)
INFO - root - 2017-12-07 06:41:55.257921: step 14570, loss = 0.81, batch loss = 0.73 (10.2 examples/sec; 0.785 sec/batch; 69h:20m:19s remains)
INFO - root - 2017-12-07 06:42:02.895821: step 14580, loss = 1.03, batch loss = 0.96 (10.4 examples/sec; 0.768 sec/batch; 67h:49m:58s remains)
INFO - root - 2017-12-07 06:42:10.576978: step 14590, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.756 sec/batch; 66h:44m:00s remains)
INFO - root - 2017-12-07 06:42:18.146860: step 14600, loss = 0.63, batch loss = 0.56 (10.4 examples/sec; 0.767 sec/batch; 67h:42m:21s remains)
2017-12-07 06:42:18.780117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6924076 -2.610249 -2.3807364 -1.8005919 -0.70075941 -0.099935532 -0.47325802 -1.0611207 -1.5659022 -2.0638168 -2.6114104 -3.168355 -3.0707381 -2.760607 -2.335166][-2.8622952 -2.8452172 -2.6273725 -2.0405354 -1.123831 -0.48918247 -0.54762077 -0.74667692 -0.73334932 -0.83873463 -1.3816049 -2.0649519 -2.185262 -2.2042336 -1.9849172][-2.971899 -3.0009503 -2.7751369 -2.2134724 -1.4497716 -0.82303357 -0.77113056 -0.82013106 -0.47610664 -0.26842356 -0.66479278 -1.2092397 -1.3479326 -1.5922029 -1.564132][-3.0070624 -3.0050111 -2.6681442 -2.0661335 -1.3470705 -0.73316741 -0.74983144 -0.88676882 -0.5102756 -0.30473375 -0.57239342 -0.79983377 -0.82988834 -1.1736276 -1.2559698][-2.9413304 -2.8595145 -2.3910406 -1.6806142 -0.87015748 -0.23437309 -0.40979481 -0.73722196 -0.4997499 -0.51703739 -0.74783516 -0.69864535 -0.724396 -1.1193843 -1.2415535][-2.6766253 -2.5194912 -1.9843419 -1.2286334 -0.35200691 0.31713057 0.021644592 -0.41017723 -0.28986263 -0.59662175 -0.89209914 -0.70934081 -0.84046865 -1.2888792 -1.398628][-2.4313097 -2.1728632 -1.6496282 -0.96768522 -0.12681198 0.57108641 0.335783 0.068959713 0.27724838 -0.28613806 -0.77574658 -0.64194179 -0.96023893 -1.4964197 -1.6240165][-2.2673807 -1.8518088 -1.3729572 -0.84278727 -0.11580896 0.54789019 0.45628262 0.537735 0.99820328 0.2688241 -0.43573737 -0.43314767 -0.9650836 -1.6640697 -1.8629127][-1.8766968 -1.3336902 -0.95199776 -0.61495137 -0.075455189 0.37908888 0.2793088 0.58358717 1.20365 0.42344379 -0.39195156 -0.46387935 -1.0819631 -1.8798447 -2.1084857][-1.2752752 -0.75222158 -0.55361986 -0.41864204 -0.093749046 0.045813084 -0.20688009 0.062965393 0.56495 -0.12718964 -0.835433 -0.86494946 -1.4212849 -2.1883285 -2.3506272][-0.89694262 -0.57944345 -0.61668229 -0.63240576 -0.44238091 -0.49053979 -0.81129813 -0.65272617 -0.4377985 -1.0570486 -1.5347722 -1.4640603 -1.8302031 -2.435353 -2.5152183][-0.90091038 -0.90528536 -1.1671555 -1.2578654 -1.1161759 -1.2005465 -1.4344769 -1.3217747 -1.3436401 -1.8662214 -2.1392477 -2.0311029 -2.1940482 -2.5729747 -2.5803523][-1.2004313 -1.4620388 -1.8363039 -1.9486799 -1.8764589 -1.951714 -2.0381403 -1.8866038 -1.9903088 -2.3789065 -2.5184965 -2.4514639 -2.4989924 -2.686384 -2.6603699][-1.6495495 -2.007755 -2.3591094 -2.4637332 -2.4612696 -2.5362744 -2.5096493 -2.3096192 -2.3836067 -2.61658 -2.7011223 -2.7035728 -2.7167163 -2.7995694 -2.7860041][-2.1937497 -2.4943161 -2.7168159 -2.774595 -2.8135729 -2.8798957 -2.8006566 -2.6042669 -2.6160817 -2.7101779 -2.7694395 -2.8298531 -2.8620965 -2.9193788 -2.9227204]]...]
INFO - root - 2017-12-07 06:42:26.480169: step 14610, loss = 0.71, batch loss = 0.64 (10.8 examples/sec; 0.740 sec/batch; 65h:19m:43s remains)
INFO - root - 2017-12-07 06:42:34.109372: step 14620, loss = 0.77, batch loss = 0.70 (10.7 examples/sec; 0.751 sec/batch; 66h:18m:47s remains)
INFO - root - 2017-12-07 06:42:41.424538: step 14630, loss = 0.74, batch loss = 0.66 (10.5 examples/sec; 0.764 sec/batch; 67h:26m:53s remains)
INFO - root - 2017-12-07 06:42:49.011310: step 14640, loss = 0.89, batch loss = 0.81 (10.7 examples/sec; 0.747 sec/batch; 65h:57m:43s remains)
INFO - root - 2017-12-07 06:42:56.695812: step 14650, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.761 sec/batch; 67h:10m:40s remains)
INFO - root - 2017-12-07 06:43:04.288366: step 14660, loss = 0.94, batch loss = 0.87 (10.6 examples/sec; 0.752 sec/batch; 66h:24m:35s remains)
INFO - root - 2017-12-07 06:43:11.899586: step 14670, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.775 sec/batch; 68h:24m:38s remains)
INFO - root - 2017-12-07 06:43:19.478363: step 14680, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.771 sec/batch; 68h:02m:45s remains)
INFO - root - 2017-12-07 06:43:27.118292: step 14690, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.781 sec/batch; 68h:55m:49s remains)
INFO - root - 2017-12-07 06:43:34.861080: step 14700, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.753 sec/batch; 66h:27m:50s remains)
2017-12-07 06:43:35.459386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5983918 -3.5298452 -3.5101604 -3.4029632 -3.199404 -3.1194632 -3.0841041 -3.0177541 -2.9277282 -2.8877664 -2.9818735 -3.0715165 -3.0523524 -3.025188 -2.9762068][-4.0586748 -4.07181 -4.0877337 -4.0240421 -3.81146 -3.6831119 -3.6354079 -3.5696664 -3.4946892 -3.5044942 -3.6337972 -3.6992655 -3.6458516 -3.5914726 -3.5161185][-4.0999365 -4.1508846 -4.2258811 -4.2647686 -4.1241345 -4.0002356 -3.9540765 -3.8589892 -3.7634096 -3.7670403 -3.8567443 -3.868273 -3.8053322 -3.764081 -3.7243893][-3.6249104 -3.6060257 -3.68997 -3.84571 -3.862247 -3.8166287 -3.7868643 -3.6641951 -3.5502064 -3.5185809 -3.5091188 -3.451623 -3.3743119 -3.3435552 -3.332927][-2.9051757 -2.7978158 -2.851336 -3.0790625 -3.2509308 -3.3506603 -3.41859 -3.331274 -3.219887 -3.1281202 -2.9820044 -2.8160644 -2.6602724 -2.5812054 -2.5342951][-2.10361 -1.9501357 -1.9898098 -2.2374172 -2.4445322 -2.5716953 -2.672451 -2.6238308 -2.5066731 -2.3286915 -2.047621 -1.7865641 -1.5293846 -1.3758991 -1.3274124][-1.5471377 -1.3466175 -1.3425803 -1.5093961 -1.610826 -1.623661 -1.6319842 -1.573698 -1.493994 -1.3586297 -1.1115983 -0.88771558 -0.62541914 -0.45687866 -0.42854643][-1.5253091 -1.2735333 -1.20299 -1.2447684 -1.173806 -1.0220675 -0.92200994 -0.86888218 -0.92295265 -0.99393535 -0.99392295 -0.97839808 -0.82511306 -0.67749214 -0.63398743][-1.720073 -1.3490043 -1.2087929 -1.1953232 -1.0611827 -0.87306857 -0.76904607 -0.78773403 -1.0132077 -1.3192761 -1.5859015 -1.7888303 -1.7585342 -1.6223185 -1.50928][-1.7199485 -1.2513561 -1.096148 -1.1480973 -1.1377275 -1.0740209 -1.0650687 -1.1496501 -1.3961127 -1.6849039 -1.9443209 -2.1661863 -2.1924398 -2.0719826 -1.9356594][-1.7029579 -1.2417645 -1.1373756 -1.270613 -1.3828108 -1.4807761 -1.5720131 -1.6273868 -1.726408 -1.813437 -1.8971252 -2.0207648 -2.0634766 -2.0013788 -1.9294035][-1.941102 -1.5187955 -1.4140236 -1.5072668 -1.6230707 -1.7806921 -1.8993943 -1.8891175 -1.8386478 -1.746382 -1.6820812 -1.7207067 -1.7873566 -1.8152726 -1.8495927][-2.3178406 -1.956296 -1.8265858 -1.8365734 -1.8934164 -2.0274549 -2.124094 -2.0916593 -1.9920013 -1.8123529 -1.6443317 -1.5677402 -1.5595951 -1.6005356 -1.6959314][-2.5748467 -2.2746468 -2.1285298 -2.0851438 -2.1026134 -2.2073693 -2.2945418 -2.2858787 -2.1819749 -1.9460316 -1.7210932 -1.559644 -1.4556077 -1.4701414 -1.5834138][-2.5703943 -2.2973931 -2.1139851 -2.0466735 -2.0614519 -2.1487248 -2.2310958 -2.242748 -2.15411 -1.9223862 -1.7121458 -1.5770307 -1.4908991 -1.5287497 -1.6672781]]...]
INFO - root - 2017-12-07 06:43:43.277770: step 14710, loss = 1.03, batch loss = 0.96 (10.4 examples/sec; 0.772 sec/batch; 68h:09m:59s remains)
INFO - root - 2017-12-07 06:43:50.957629: step 14720, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.758 sec/batch; 66h:55m:45s remains)
INFO - root - 2017-12-07 06:43:58.238740: step 14730, loss = 0.97, batch loss = 0.89 (10.6 examples/sec; 0.756 sec/batch; 66h:44m:30s remains)
INFO - root - 2017-12-07 06:44:05.828808: step 14740, loss = 0.76, batch loss = 0.68 (10.7 examples/sec; 0.748 sec/batch; 66h:02m:54s remains)
INFO - root - 2017-12-07 06:44:13.484830: step 14750, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.761 sec/batch; 67h:07m:51s remains)
INFO - root - 2017-12-07 06:44:21.088448: step 14760, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.750 sec/batch; 66h:10m:04s remains)
INFO - root - 2017-12-07 06:44:28.580846: step 14770, loss = 0.69, batch loss = 0.62 (11.0 examples/sec; 0.725 sec/batch; 64h:00m:37s remains)
INFO - root - 2017-12-07 06:44:36.225923: step 14780, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.774 sec/batch; 68h:16m:00s remains)
INFO - root - 2017-12-07 06:44:43.843186: step 14790, loss = 0.86, batch loss = 0.78 (10.7 examples/sec; 0.748 sec/batch; 66h:02m:27s remains)
INFO - root - 2017-12-07 06:44:51.492595: step 14800, loss = 0.80, batch loss = 0.72 (10.1 examples/sec; 0.790 sec/batch; 69h:42m:28s remains)
2017-12-07 06:44:52.078041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4053259 -2.0098693 -2.6335564 -3.0267835 -3.2525821 -3.3249421 -3.2864075 -3.3395987 -3.4247923 -3.4942589 -3.5438275 -3.5913062 -3.6699514 -3.7439806 -3.7962813][-1.7059257 -2.3009584 -2.9006045 -3.1813169 -3.3018618 -3.3156409 -3.294796 -3.4419129 -3.640518 -3.7781076 -3.883894 -3.9571171 -3.9754877 -3.9438388 -3.8957837][-2.0496092 -2.738111 -3.3267498 -3.4646306 -3.4527042 -3.4429917 -3.4524238 -3.5629292 -3.7255006 -3.8219938 -3.897635 -3.9571111 -3.9082739 -3.7890689 -3.6908631][-2.3053308 -3.0465662 -3.5342391 -3.5276024 -3.4701841 -3.5358095 -3.5943539 -3.6020148 -3.6488032 -3.6825209 -3.7301621 -3.7657297 -3.670392 -3.5096617 -3.3929543][-2.4622877 -3.0000935 -3.2223985 -3.1265292 -3.1141987 -3.2764707 -3.3319917 -3.1995828 -3.164211 -3.2091765 -3.2761846 -3.3583751 -3.3490019 -3.2992961 -3.2618179][-2.341146 -2.4272988 -2.3024178 -2.1793797 -2.2359939 -2.4090285 -2.3593206 -2.087966 -2.0471296 -2.2247 -2.45223 -2.7399154 -2.966964 -3.1342292 -3.2505021][-1.8298681 -1.4919291 -1.1136875 -1.0350986 -1.127197 -1.2046776 -1.0054159 -0.65417647 -0.712697 -1.0828583 -1.5192208 -2.0298216 -2.4508862 -2.7881122 -3.0775819][-1.2458742 -0.80342412 -0.40928078 -0.32052946 -0.32542896 -0.23584795 0.094430447 0.36430645 0.058837891 -0.515991 -1.0369828 -1.5334938 -1.9085367 -2.2342615 -2.6241241][-1.0422273 -0.75989127 -0.45188046 -0.28998709 -0.17803717 -0.05866003 0.166533 0.20289516 -0.27225304 -0.7702508 -1.001256 -1.1366348 -1.2651615 -1.5127335 -1.9741151][-1.2881196 -1.2350447 -0.9564836 -0.67953873 -0.525336 -0.55056787 -0.58000755 -0.78552318 -1.2243869 -1.3954368 -1.1610799 -0.86087418 -0.74734831 -0.95390892 -1.4315231][-1.8522611 -1.9585159 -1.694164 -1.3532605 -1.1639714 -1.2279499 -1.3210084 -1.5334082 -1.8228352 -1.7794406 -1.4426801 -1.1299293 -1.0316434 -1.210006 -1.5486593][-2.6474891 -2.8097329 -2.5817904 -2.2729232 -2.0849462 -1.9948421 -1.8362136 -1.7872791 -1.8748393 -1.8471956 -1.819699 -1.8700061 -1.9884272 -2.1570508 -2.2611489][-3.369205 -3.5821357 -3.4611332 -3.2394552 -3.0750122 -2.8322854 -2.435822 -2.1870306 -2.1725965 -2.237077 -2.4734159 -2.7706237 -3.0167387 -3.1362247 -3.0377212][-3.8621716 -4.0860047 -4.0626297 -3.9092903 -3.755161 -3.43931 -3.0241485 -2.8360338 -2.8954527 -3.0944998 -3.4409344 -3.7774842 -4.0049758 -4.0110512 -3.750572][-4.0935822 -4.2417793 -4.2450242 -4.1426749 -4.0691328 -3.8454907 -3.5691252 -3.5077171 -3.5913405 -3.81122 -4.1389713 -4.4002004 -4.53801 -4.446857 -4.1129847]]...]
INFO - root - 2017-12-07 06:44:59.863024: step 14810, loss = 0.91, batch loss = 0.84 (10.2 examples/sec; 0.785 sec/batch; 69h:14m:32s remains)
INFO - root - 2017-12-07 06:45:07.472718: step 14820, loss = 0.78, batch loss = 0.71 (10.9 examples/sec; 0.734 sec/batch; 64h:43m:39s remains)
INFO - root - 2017-12-07 06:45:14.885995: step 14830, loss = 0.97, batch loss = 0.90 (10.5 examples/sec; 0.762 sec/batch; 67h:15m:01s remains)
INFO - root - 2017-12-07 06:45:22.569097: step 14840, loss = 0.95, batch loss = 0.88 (10.0 examples/sec; 0.797 sec/batch; 70h:18m:37s remains)
INFO - root - 2017-12-07 06:45:30.361682: step 14850, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.768 sec/batch; 67h:46m:46s remains)
INFO - root - 2017-12-07 06:45:37.963207: step 14860, loss = 0.87, batch loss = 0.79 (10.6 examples/sec; 0.758 sec/batch; 66h:51m:55s remains)
INFO - root - 2017-12-07 06:45:45.681800: step 14870, loss = 0.79, batch loss = 0.72 (10.1 examples/sec; 0.790 sec/batch; 69h:43m:34s remains)
INFO - root - 2017-12-07 06:45:53.340543: step 14880, loss = 0.81, batch loss = 0.73 (10.7 examples/sec; 0.750 sec/batch; 66h:10m:34s remains)
INFO - root - 2017-12-07 06:46:01.057942: step 14890, loss = 0.92, batch loss = 0.85 (10.8 examples/sec; 0.743 sec/batch; 65h:32m:46s remains)
INFO - root - 2017-12-07 06:46:08.784726: step 14900, loss = 0.88, batch loss = 0.81 (10.1 examples/sec; 0.788 sec/batch; 69h:32m:24s remains)
2017-12-07 06:46:09.455943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1332755 -4.1177506 -4.0722079 -3.7833767 -2.8884263 -1.6988721 -0.45419216 0.81641865 1.7370071 1.5985394 0.32357788 -1.3297954 -2.6103587 -3.3843753 -3.7280941][-3.5911272 -3.5718944 -3.7607348 -4.0332675 -3.8850653 -3.4236493 -2.6113119 -1.1031554 0.88644791 2.00108 1.365623 -0.38233185 -1.9452558 -2.8147144 -3.1478636][-2.510942 -2.476316 -2.801625 -3.5080624 -4.0994191 -4.4694948 -4.4886794 -3.4215975 -1.0178285 1.2421556 1.7398214 0.4595933 -1.1686413 -2.1566317 -2.4637325][-1.2010672 -1.0702758 -1.2708437 -2.0945456 -3.1464608 -4.1979132 -5.1124535 -5.0416212 -3.1419911 -0.40036964 1.263979 0.91422176 -0.50410962 -1.6923921 -2.1245677][-0.34247494 -0.034579277 0.18521452 -0.20447969 -1.1473382 -2.4635754 -4.1285257 -5.3594503 -4.7703271 -2.4393878 0.029401302 0.85343456 -0.043301105 -1.4526873 -2.2128713][-0.36993361 0.10550261 0.83525944 1.3145919 1.2191172 0.25779772 -1.8303332 -4.3110132 -5.415554 -4.2401814 -1.6788247 0.084948063 -0.02863884 -1.32495 -2.3830361][-1.2076395 -0.74110413 0.23866987 1.5174899 2.7165685 2.9000735 1.1134462 -2.1739471 -4.9359341 -5.4021554 -3.5501783 -1.4500089 -0.82219887 -1.6597052 -2.8079445][-2.6771946 -2.5097561 -1.7199667 -0.15439749 2.168417 3.9770746 3.5292892 0.46288729 -3.3103445 -5.356112 -4.7240524 -3.0173049 -2.1020191 -2.483645 -3.4772613][-4.2300854 -4.5597715 -4.29356 -3.0178306 -0.245049 2.9195347 4.3646622 2.7127833 -0.96812677 -4.0760922 -4.786943 -3.977366 -3.2385058 -3.3700418 -4.0754957][-4.6596346 -5.4182191 -5.8741827 -5.3911533 -3.0240583 0.52675295 3.3628554 3.5766506 0.95219326 -2.4067478 -4.1517744 -4.2755351 -3.9006326 -3.9615865 -4.4050469][-3.909039 -4.7485294 -5.7491922 -6.2083716 -4.9081311 -1.9733565 1.3435311 3.1177793 2.0185723 -0.8800807 -3.1321549 -3.9712496 -3.9632185 -4.0822067 -4.3779368][-2.9862747 -3.5623193 -4.612813 -5.6367908 -5.4367867 -3.5601909 -0.52167296 2.0374341 2.2479997 0.1962018 -2.0325894 -3.2337322 -3.5825334 -3.9091051 -4.2088685][-2.2801921 -2.5554297 -3.3346124 -4.4528923 -4.98338 -4.0434065 -1.6725733 0.86291027 1.6898665 0.43233252 -1.4024351 -2.5309954 -2.9857571 -3.4729419 -3.8911195][-2.4815078 -2.4571431 -2.759975 -3.50318 -4.0836997 -3.5614653 -1.8248293 0.12413025 0.85708809 0.012473583 -1.3246601 -2.0468726 -2.1902199 -2.5214205 -3.0393739][-2.743453 -2.3760464 -2.2429638 -2.5694003 -2.9765191 -2.5932422 -1.3126717 0.0094056129 0.46231508 -0.21218681 -1.2590597 -1.6760302 -1.3722649 -1.1679087 -1.5347054]]...]
INFO - root - 2017-12-07 06:46:17.030095: step 14910, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.744 sec/batch; 65h:40m:16s remains)
INFO - root - 2017-12-07 06:46:24.669760: step 14920, loss = 0.70, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 66h:53m:05s remains)
INFO - root - 2017-12-07 06:46:32.250164: step 14930, loss = 0.86, batch loss = 0.78 (10.2 examples/sec; 0.784 sec/batch; 69h:08m:48s remains)
INFO - root - 2017-12-07 06:46:39.956109: step 14940, loss = 0.59, batch loss = 0.52 (10.0 examples/sec; 0.797 sec/batch; 70h:20m:09s remains)
INFO - root - 2017-12-07 06:46:47.611040: step 14950, loss = 0.73, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 68h:13m:41s remains)
INFO - root - 2017-12-07 06:46:55.259049: step 14960, loss = 0.88, batch loss = 0.81 (10.3 examples/sec; 0.774 sec/batch; 68h:13m:54s remains)
INFO - root - 2017-12-07 06:47:02.889730: step 14970, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.777 sec/batch; 68h:34m:38s remains)
INFO - root - 2017-12-07 06:47:10.555133: step 14980, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.762 sec/batch; 67h:11m:45s remains)
INFO - root - 2017-12-07 06:47:18.168254: step 14990, loss = 0.82, batch loss = 0.74 (10.4 examples/sec; 0.769 sec/batch; 67h:49m:43s remains)
INFO - root - 2017-12-07 06:47:25.836855: step 15000, loss = 0.67, batch loss = 0.60 (10.2 examples/sec; 0.787 sec/batch; 69h:22m:17s remains)
2017-12-07 06:47:26.428735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4476652 -2.4271135 -2.3432293 -2.2095547 -2.3621452 -2.8210258 -3.0374019 -2.6484904 -1.8241882 -1.1193235 -0.93264174 -1.1840041 -1.6377027 -2.3210506 -2.9225702][-2.513809 -2.3523571 -2.3109624 -2.2600944 -2.3269 -2.6424136 -2.8018923 -2.3760643 -1.4703674 -0.77106 -0.75492263 -1.2346332 -1.8559566 -2.5621176 -3.0716808][-2.5408564 -2.2665944 -2.2453029 -2.3108735 -2.3618612 -2.5059743 -2.5726414 -2.2154138 -1.4602222 -0.9013083 -1.0074947 -1.584801 -2.2793784 -2.9371235 -3.2856584][-2.7436094 -2.334796 -2.2783394 -2.4401402 -2.5655465 -2.6287124 -2.6094198 -2.3741229 -1.8987288 -1.5182197 -1.6039183 -2.0796413 -2.680882 -3.209655 -3.4081349][-2.9218109 -2.4407997 -2.3663666 -2.5594623 -2.7623024 -2.8000808 -2.6823757 -2.4734554 -2.1763976 -1.9424653 -1.9689989 -2.266088 -2.701251 -3.1367676 -3.3064055][-2.9996109 -2.5657299 -2.5271757 -2.7072415 -2.8904815 -2.798779 -2.4420176 -2.0753343 -1.8411717 -1.8041887 -1.8913152 -2.0706067 -2.3903506 -2.8425422 -3.1146536][-3.2641773 -2.9658499 -2.8449249 -2.8077145 -2.8142819 -2.5037222 -1.8424866 -1.1501687 -0.83422422 -1.0772433 -1.4007971 -1.5695207 -1.8733191 -2.4862647 -2.9568243][-3.3808241 -3.2173896 -2.8721752 -2.4697528 -2.2246604 -1.6892629 -0.75258279 0.29817247 0.73100567 0.14687872 -0.52422071 -0.85955238 -1.3311594 -2.2185273 -2.8829019][-3.4962804 -3.4821107 -2.9863002 -2.3418179 -1.9495568 -1.3384023 -0.33326149 0.87100792 1.3309636 0.54665327 -0.23819542 -0.60691 -1.2119305 -2.2796819 -2.9859896][-3.641021 -3.7433829 -3.2848849 -2.6824646 -2.3739066 -1.9640102 -1.2216179 -0.2060194 0.21778059 -0.40987921 -0.89027023 -1.0505154 -1.6052234 -2.6135139 -3.1748891][-3.5491316 -3.7657845 -3.4918082 -3.047327 -2.8229237 -2.6254225 -2.2243805 -1.5477386 -1.2470627 -1.6099045 -1.7407029 -1.7678123 -2.2671566 -3.0757809 -3.3951859][-3.2274175 -3.5511403 -3.555666 -3.3417706 -3.169065 -3.0702796 -2.9290779 -2.6119266 -2.4480529 -2.5501716 -2.4588311 -2.4935513 -2.93625 -3.4736638 -3.5566282][-2.7865109 -3.1742115 -3.4618509 -3.5331001 -3.4590368 -3.4173036 -3.4427347 -3.3617897 -3.2426434 -3.1618831 -3.0439572 -3.1492043 -3.4668317 -3.7144651 -3.6245036][-2.6396718 -2.9522486 -3.3297591 -3.5682139 -3.5730958 -3.5569303 -3.6394467 -3.6972556 -3.6506939 -3.5716624 -3.5649145 -3.708992 -3.8495452 -3.8594477 -3.6690247][-2.9298165 -3.0831809 -3.3601346 -3.6000757 -3.6118124 -3.5867491 -3.6878705 -3.8367667 -3.8751671 -3.8663876 -3.9405453 -4.0425944 -4.0223346 -3.8967388 -3.6803341]]...]
INFO - root - 2017-12-07 06:47:34.073693: step 15010, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.777 sec/batch; 68h:30m:41s remains)
INFO - root - 2017-12-07 06:47:41.810696: step 15020, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.782 sec/batch; 69h:00m:11s remains)
INFO - root - 2017-12-07 06:47:49.306893: step 15030, loss = 0.83, batch loss = 0.76 (10.8 examples/sec; 0.742 sec/batch; 65h:25m:35s remains)
INFO - root - 2017-12-07 06:47:57.023886: step 15040, loss = 0.82, batch loss = 0.74 (10.9 examples/sec; 0.733 sec/batch; 64h:39m:01s remains)
INFO - root - 2017-12-07 06:48:04.612410: step 15050, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.751 sec/batch; 66h:11m:04s remains)
INFO - root - 2017-12-07 06:48:12.322036: step 15060, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.764 sec/batch; 67h:23m:38s remains)
INFO - root - 2017-12-07 06:48:19.937907: step 15070, loss = 0.97, batch loss = 0.89 (10.7 examples/sec; 0.746 sec/batch; 65h:44m:47s remains)
INFO - root - 2017-12-07 06:48:27.636932: step 15080, loss = 0.72, batch loss = 0.65 (10.7 examples/sec; 0.751 sec/batch; 66h:11m:10s remains)
INFO - root - 2017-12-07 06:48:35.305902: step 15090, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.781 sec/batch; 68h:50m:25s remains)
INFO - root - 2017-12-07 06:48:42.987915: step 15100, loss = 0.72, batch loss = 0.64 (10.6 examples/sec; 0.753 sec/batch; 66h:25m:48s remains)
2017-12-07 06:48:43.605093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7228014 -1.8047504 -1.1532183 -1.0468826 -2.1376076 -1.7611511 -0.056339264 -0.27223206 -0.92318821 -0.66282105 -0.51832151 -0.87722373 -0.68770528 -0.16862345 -0.34813023][-1.4982054 -1.342345 -0.76262832 -0.8575027 -1.7887051 -1.2466214 0.33066988 -0.20427084 -1.029572 -0.69553065 -0.38380671 -0.65708804 -0.50765634 -0.14732838 -0.44649768][-0.88669252 -0.66490364 -0.26327848 -0.4758904 -1.1482778 -0.64386797 0.50040627 -0.32933426 -1.1890562 -0.71097088 -0.19624138 -0.38502693 -0.28686476 0.021162033 -0.2976594][-0.75631166 -0.74296021 -0.53934431 -0.6883142 -1.0118141 -0.66661239 0.016219139 -0.83530045 -1.5712864 -0.98622632 -0.36700916 -0.57090545 -0.5636723 -0.1788187 -0.32462502][-1.4023585 -1.5157449 -1.2511814 -1.1953309 -1.2410979 -1.0550859 -0.57581067 -1.0931284 -1.6317043 -1.0975153 -0.53178525 -0.87797666 -1.0633137 -0.67598057 -0.7095592][-1.8914673 -1.7628291 -1.1733658 -0.92882109 -0.70680952 -0.4131999 0.28119802 0.26965046 -0.24234772 -0.028027534 0.22045612 -0.45176697 -1.0336423 -0.81658196 -0.89360094][-1.8088567 -1.2422669 -0.4256649 -0.18312788 0.34441423 1.046864 2.1722302 2.5298047 1.6963124 1.3837743 1.2018895 0.1028161 -0.88161778 -0.8202877 -0.87142539][-1.724946 -0.98009253 -0.392581 -0.57836342 -0.0019659996 0.87617016 2.0493689 2.4376922 1.3920927 1.07476 1.0540147 0.064390659 -0.88348269 -0.82624626 -0.83217788][-1.88939 -1.3981762 -1.3436551 -2.0453 -1.6050999 -0.8365581 0.029308319 0.25052214 -0.43382215 -0.15566111 0.43400335 -0.048726559 -0.6729672 -0.64090157 -0.79624295][-2.0787189 -1.9192264 -2.1324215 -2.9443016 -2.4701436 -1.7634513 -1.1500144 -1.0220766 -1.2857306 -0.60731673 0.24803257 0.052230835 -0.36234665 -0.44119835 -0.84037447][-1.9290247 -1.8822122 -1.9929974 -2.5798063 -2.0383816 -1.3814929 -1.020776 -1.0226936 -1.1895781 -0.68609095 -0.14695883 -0.3686738 -0.70305204 -0.71901321 -1.0130227][-1.5784905 -1.4858186 -1.335156 -1.6730993 -1.1345067 -0.59291577 -0.52081275 -0.702261 -1.0316672 -1.04459 -1.0890017 -1.5057247 -1.81144 -1.6542938 -1.5791976][-1.4066684 -1.2217638 -0.83820176 -0.95546055 -0.51120067 -0.20767212 -0.42066669 -0.75689626 -1.1757205 -1.5393004 -2.0180182 -2.612783 -2.9430285 -2.7254515 -2.3919866][-1.4196751 -1.2030723 -0.69330049 -0.68604469 -0.47837377 -0.56860352 -0.98217511 -1.2923667 -1.491533 -1.7416351 -2.2825828 -2.9148684 -3.2974949 -3.2186894 -2.9350462][-1.6322327 -1.4969723 -1.0088882 -0.96593189 -0.98899961 -1.3259614 -1.7613938 -1.9962807 -1.9410818 -1.9032288 -2.2734163 -2.7831275 -3.19093 -3.3181825 -3.2414403]]...]
INFO - root - 2017-12-07 06:48:51.243933: step 15110, loss = 0.58, batch loss = 0.51 (10.5 examples/sec; 0.763 sec/batch; 67h:17m:17s remains)
INFO - root - 2017-12-07 06:48:58.912432: step 15120, loss = 0.93, batch loss = 0.85 (10.7 examples/sec; 0.750 sec/batch; 66h:07m:56s remains)
INFO - root - 2017-12-07 06:49:06.315864: step 15130, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.782 sec/batch; 68h:56m:31s remains)
INFO - root - 2017-12-07 06:49:14.000785: step 15140, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.760 sec/batch; 67h:01m:30s remains)
INFO - root - 2017-12-07 06:49:21.622928: step 15150, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.767 sec/batch; 67h:35m:04s remains)
INFO - root - 2017-12-07 06:49:29.216526: step 15160, loss = 0.81, batch loss = 0.73 (10.6 examples/sec; 0.755 sec/batch; 66h:35m:17s remains)
INFO - root - 2017-12-07 06:49:36.819630: step 15170, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.763 sec/batch; 67h:16m:30s remains)
INFO - root - 2017-12-07 06:49:44.421849: step 15180, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.757 sec/batch; 66h:44m:15s remains)
INFO - root - 2017-12-07 06:49:52.065237: step 15190, loss = 0.72, batch loss = 0.65 (10.8 examples/sec; 0.739 sec/batch; 65h:08m:14s remains)
INFO - root - 2017-12-07 06:49:59.689790: step 15200, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.771 sec/batch; 67h:57m:07s remains)
2017-12-07 06:50:00.287672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0974407 -4.0403218 -3.910428 -3.8335259 -3.8094287 -3.7043889 -3.5654743 -3.5165796 -3.5002105 -3.4683738 -3.4376924 -3.3881102 -3.3488226 -3.3375206 -3.3190162][-3.9578168 -3.8969526 -3.758534 -3.7081389 -3.72224 -3.6258144 -3.5081289 -3.5142503 -3.5312786 -3.5031672 -3.4810953 -3.4639902 -3.4550838 -3.4503098 -3.4233179][-3.8870528 -3.795615 -3.6456838 -3.6233191 -3.6619046 -3.5764453 -3.4942338 -3.5083683 -3.4824066 -3.424356 -3.4287665 -3.4703159 -3.4855793 -3.4778252 -3.4566469][-3.90615 -3.7989194 -3.6700625 -3.6841402 -3.7175128 -3.6109004 -3.5157099 -3.4595022 -3.3499155 -3.2843413 -3.3456478 -3.4324608 -3.4439809 -3.4325392 -3.4312971][-3.8851385 -3.8013432 -3.7358844 -3.7803056 -3.7729542 -3.6171679 -3.4677978 -3.32012 -3.1753898 -3.1724517 -3.2933612 -3.3729093 -3.3476439 -3.3264055 -3.3363962][-3.8436365 -3.8150058 -3.8172929 -3.8395061 -3.7398691 -3.5077791 -3.2704105 -3.0459669 -2.940774 -3.0529339 -3.2294471 -3.2891269 -3.257319 -3.2647073 -3.2951519][-3.8440957 -3.8705735 -3.9011936 -3.8462186 -3.6341035 -3.3225317 -2.9934974 -2.723618 -2.705898 -2.9340053 -3.1575196 -3.23594 -3.2682567 -3.3440197 -3.4069147][-3.7312548 -3.778125 -3.7956066 -3.7000206 -3.4933386 -3.2295396 -2.9065926 -2.6854486 -2.772944 -3.0402198 -3.2481914 -3.3317471 -3.401911 -3.4901874 -3.5597279][-3.5089133 -3.5751877 -3.5977287 -3.5435147 -3.4443259 -3.2955947 -3.0571184 -2.9427824 -3.0761547 -3.2764535 -3.392065 -3.4309716 -3.4795871 -3.52718 -3.5862563][-3.4727347 -3.5679657 -3.6005306 -3.5745203 -3.5057361 -3.3558164 -3.136133 -3.0711107 -3.2044296 -3.334733 -3.3904252 -3.40472 -3.4379404 -3.4616585 -3.5028281][-3.6439862 -3.7355306 -3.7411492 -3.6629939 -3.5092273 -3.2830653 -3.053158 -3.0144846 -3.1615772 -3.2896147 -3.3502831 -3.3766956 -3.41959 -3.446332 -3.4605246][-3.790467 -3.8349864 -3.7769043 -3.6281643 -3.4095404 -3.16917 -2.992403 -3.0023603 -3.1769989 -3.3344502 -3.416779 -3.4587002 -3.5075619 -3.5353284 -3.5214739][-3.829365 -3.8256054 -3.7219093 -3.54471 -3.3340328 -3.1381698 -3.0234084 -3.0701144 -3.256418 -3.4341178 -3.5275126 -3.5825956 -3.6353421 -3.6594684 -3.6260889][-3.8217471 -3.8038905 -3.6875234 -3.5143139 -3.3388982 -3.1888704 -3.1035919 -3.1565638 -3.3364086 -3.5161748 -3.6096737 -3.660593 -3.6991904 -3.7074232 -3.6854639][-3.8128695 -3.8170018 -3.7206554 -3.5602303 -3.3980174 -3.2596996 -3.1732335 -3.2091949 -3.3686137 -3.5381896 -3.6254997 -3.6619744 -3.6776266 -3.6717837 -3.6819341]]...]
INFO - root - 2017-12-07 06:50:07.952779: step 15210, loss = 0.86, batch loss = 0.78 (10.3 examples/sec; 0.775 sec/batch; 68h:18m:04s remains)
INFO - root - 2017-12-07 06:50:15.631418: step 15220, loss = 0.83, batch loss = 0.75 (10.6 examples/sec; 0.753 sec/batch; 66h:24m:18s remains)
INFO - root - 2017-12-07 06:50:22.981770: step 15230, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.772 sec/batch; 68h:03m:08s remains)
INFO - root - 2017-12-07 06:50:30.617788: step 15240, loss = 0.72, batch loss = 0.65 (10.7 examples/sec; 0.749 sec/batch; 66h:00m:56s remains)
INFO - root - 2017-12-07 06:50:38.297396: step 15250, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.763 sec/batch; 67h:11m:45s remains)
INFO - root - 2017-12-07 06:50:45.981792: step 15260, loss = 0.70, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 66h:14m:19s remains)
INFO - root - 2017-12-07 06:50:53.643093: step 15270, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.756 sec/batch; 66h:34m:41s remains)
INFO - root - 2017-12-07 06:51:01.232904: step 15280, loss = 0.92, batch loss = 0.85 (10.3 examples/sec; 0.774 sec/batch; 68h:10m:21s remains)
INFO - root - 2017-12-07 06:51:09.111862: step 15290, loss = 0.86, batch loss = 0.78 (10.8 examples/sec; 0.744 sec/batch; 65h:32m:00s remains)
INFO - root - 2017-12-07 06:51:16.954031: step 15300, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.757 sec/batch; 66h:40m:23s remains)
2017-12-07 06:51:17.550040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6165853 -3.2344575 -3.0894258 -2.5983191 -1.9741907 -1.2948639 -1.2401447 -1.4982109 -1.6284182 -1.5986617 -1.3267136 -0.92369008 -1.0113597 -1.734973 -1.8571963][-3.0087128 -3.6016784 -3.2588022 -2.7336287 -2.2315767 -1.7043452 -1.7135353 -1.8898392 -1.8397179 -1.5712967 -1.1576204 -0.76737022 -1.0248966 -1.8766017 -2.0986526][-3.04001 -3.5243497 -3.0872731 -2.6655622 -2.3259697 -1.9186654 -1.9939001 -2.1537757 -2.0086186 -1.5962052 -1.0668855 -0.65820909 -1.0101516 -1.9554799 -2.3158312][-2.7816305 -3.138931 -2.709177 -2.4505424 -2.1868641 -1.7901945 -1.8828056 -2.060719 -1.943049 -1.5835209 -1.0639486 -0.6532495 -1.0266628 -2.0042839 -2.5058706][-2.3639061 -2.6333914 -2.254847 -2.1030807 -1.7694235 -1.2814915 -1.3361015 -1.4939125 -1.516027 -1.4330549 -1.1014552 -0.74772048 -1.0948458 -2.0644157 -2.6966043][-1.917779 -2.1506417 -1.9125886 -1.8195226 -1.3579795 -0.78580046 -0.73492432 -0.77995133 -0.91050267 -1.132488 -1.0164752 -0.70904946 -1.0310659 -2.0411904 -2.8342268][-1.5920417 -1.8371615 -1.7939847 -1.7324762 -1.2134984 -0.64734674 -0.44146585 -0.27667189 -0.394032 -0.78550696 -0.77830124 -0.47741008 -0.82960534 -1.9410653 -2.9251466][-1.2619653 -1.5260618 -1.6223037 -1.6186781 -1.2145886 -0.78527808 -0.46846533 -0.10444021 -0.12562513 -0.50123024 -0.49265361 -0.22469854 -0.63540936 -1.8094394 -2.88585][-0.903404 -1.117456 -1.2271452 -1.3225064 -1.1853843 -0.99503636 -0.63589025 -0.16587782 -0.07486248 -0.29684305 -0.20793343 0.018170834 -0.41641188 -1.527602 -2.5511179][-0.84269214 -0.9388504 -0.98041677 -1.1225693 -1.2205276 -1.2081983 -0.82742405 -0.34847927 -0.23428202 -0.30653191 -0.12320662 0.10829401 -0.24011707 -1.1510427 -2.0174086][-1.2161219 -1.1493633 -1.0653694 -1.1418548 -1.3040593 -1.3386033 -0.95618677 -0.54197717 -0.47290874 -0.49549913 -0.2943716 -0.066358089 -0.31172895 -0.98781729 -1.6589451][-1.7402537 -1.5469627 -1.3604813 -1.3370106 -1.4396207 -1.4306951 -1.1143746 -0.83243012 -0.84399581 -0.86830974 -0.72712278 -0.5601511 -0.73593926 -1.2387092 -1.7178707][-2.3295016 -2.0503509 -1.8122091 -1.7438085 -1.8094685 -1.8013055 -1.6181653 -1.4475105 -1.4553533 -1.4518964 -1.3532732 -1.2532866 -1.3872814 -1.7361512 -2.0396268][-2.8974934 -2.6294765 -2.4082389 -2.3480837 -2.4011669 -2.413986 -2.3343704 -2.2225628 -2.1873806 -2.1426966 -2.0913229 -2.0711219 -2.181618 -2.3858235 -2.5493822][-3.1628256 -2.9665084 -2.8045144 -2.7421103 -2.7478426 -2.7482738 -2.720336 -2.6433864 -2.5956554 -2.5687885 -2.574574 -2.6140079 -2.7022233 -2.8134794 -2.8846581]]...]
INFO - root - 2017-12-07 06:51:25.224834: step 15310, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.770 sec/batch; 67h:52m:33s remains)
INFO - root - 2017-12-07 06:51:32.857988: step 15320, loss = 1.03, batch loss = 0.96 (10.8 examples/sec; 0.743 sec/batch; 65h:26m:49s remains)
INFO - root - 2017-12-07 06:51:40.328308: step 15330, loss = 0.59, batch loss = 0.51 (10.2 examples/sec; 0.781 sec/batch; 68h:47m:04s remains)
INFO - root - 2017-12-07 06:51:48.145016: step 15340, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.755 sec/batch; 66h:31m:29s remains)
INFO - root - 2017-12-07 06:51:55.758971: step 15350, loss = 0.94, batch loss = 0.86 (10.5 examples/sec; 0.763 sec/batch; 67h:12m:31s remains)
INFO - root - 2017-12-07 06:52:03.393552: step 15360, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.758 sec/batch; 66h:48m:37s remains)
INFO - root - 2017-12-07 06:52:11.225300: step 15370, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 68h:32m:57s remains)
INFO - root - 2017-12-07 06:52:19.000061: step 15380, loss = 0.65, batch loss = 0.58 (10.7 examples/sec; 0.745 sec/batch; 65h:39m:05s remains)
INFO - root - 2017-12-07 06:52:26.600227: step 15390, loss = 0.72, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 66h:22m:43s remains)
INFO - root - 2017-12-07 06:52:34.343131: step 15400, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.779 sec/batch; 68h:36m:06s remains)
2017-12-07 06:52:34.944756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4811575 -3.5054224 -3.5169787 -3.5043049 -3.4675615 -3.3948202 -3.2843423 -3.2080445 -3.2543297 -3.4212792 -3.6010115 -3.6815422 -3.6616116 -3.5681405 -3.3771753][-3.4608917 -3.4892583 -3.5296013 -3.5526428 -3.5282311 -3.41333 -3.1880937 -2.96732 -2.9344447 -3.1497161 -3.4545951 -3.5926175 -3.4989696 -3.2933044 -3.0131571][-3.4342742 -3.4482493 -3.500406 -3.5481894 -3.5179713 -3.3453097 -3.001761 -2.6359782 -2.5362606 -2.849515 -3.3239665 -3.5412376 -3.3467603 -2.9821086 -2.59575][-3.4046092 -3.383729 -3.3935392 -3.3908091 -3.2957182 -3.0482392 -2.5906467 -2.1044283 -1.9850266 -2.4308877 -3.0681524 -3.3614244 -3.0879738 -2.5749359 -2.0279617][-3.4369798 -3.3669069 -3.2711453 -3.1432574 -2.9430747 -2.6305275 -2.1026697 -1.5243719 -1.4138968 -2.003335 -2.7870574 -3.2172089 -2.983438 -2.4198267 -1.7529089][-3.5475569 -3.4163485 -3.1457891 -2.785429 -2.4076126 -2.0614729 -1.5738165 -0.9943049 -0.96953869 -1.7454877 -2.6869831 -3.2883773 -3.2244697 -2.7791858 -2.1492312][-3.657373 -3.4420033 -2.9581895 -2.2998483 -1.6419649 -1.156774 -0.66481709 -0.1459918 -0.32419538 -1.3443332 -2.4191773 -3.1326184 -3.2763824 -3.0820956 -2.669632][-3.7811711 -3.5790377 -2.9916477 -2.1380789 -1.2567351 -0.56357455 0.11282921 0.73902082 0.45100355 -0.70178008 -1.832098 -2.6391535 -2.9704089 -2.9772987 -2.7810578][-3.8326056 -3.7537248 -3.2592812 -2.4583073 -1.6091013 -0.88917184 -0.12697887 0.5465703 0.37188244 -0.5686605 -1.5998785 -2.4729934 -2.93714 -3.0104635 -2.8315077][-3.7412653 -3.7393029 -3.3568568 -2.7283888 -2.1134119 -1.5356991 -0.8165729 -0.21657324 -0.34628677 -1.0233476 -1.8601861 -2.7763677 -3.3318434 -3.3698568 -3.0437524][-3.6529975 -3.6524718 -3.3118145 -2.7915568 -2.3470428 -1.9109652 -1.3165386 -0.93628979 -1.1814132 -1.6711962 -2.1856396 -2.9091806 -3.4272501 -3.4437516 -3.0045161][-3.6620004 -3.6413715 -3.3565216 -2.9265 -2.5233312 -2.1068616 -1.6544993 -1.6139548 -2.1622586 -2.6564643 -2.8534303 -3.1407413 -3.3180826 -3.13972 -2.5901613][-3.6734116 -3.6236374 -3.371851 -2.9888842 -2.561846 -2.1092217 -1.7322011 -1.9258206 -2.7776418 -3.4761934 -3.6153831 -3.5515294 -3.2827244 -2.7775755 -2.0645792][-3.5808642 -3.4625921 -3.1767893 -2.7412968 -2.2360642 -1.7756677 -1.4827933 -1.7858942 -2.7588971 -3.589783 -3.7514846 -3.4744692 -2.8682537 -2.118099 -1.2494991][-3.38107 -3.1735077 -2.7791312 -2.2307768 -1.6354847 -1.1757355 -1.0123084 -1.5193908 -2.6240454 -3.5490823 -3.7277637 -3.3361685 -2.5303333 -1.6317317 -0.64997864]]...]
INFO - root - 2017-12-07 06:52:42.660344: step 15410, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.767 sec/batch; 67h:31m:31s remains)
INFO - root - 2017-12-07 06:52:50.214887: step 15420, loss = 0.93, batch loss = 0.86 (10.7 examples/sec; 0.746 sec/batch; 65h:41m:11s remains)
INFO - root - 2017-12-07 06:52:57.656232: step 15430, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.771 sec/batch; 67h:51m:49s remains)
INFO - root - 2017-12-07 06:53:05.444748: step 15440, loss = 0.79, batch loss = 0.72 (10.9 examples/sec; 0.734 sec/batch; 64h:39m:50s remains)
INFO - root - 2017-12-07 06:53:13.134313: step 15450, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.755 sec/batch; 66h:31m:38s remains)
INFO - root - 2017-12-07 06:53:20.749160: step 15460, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.772 sec/batch; 67h:56m:56s remains)
INFO - root - 2017-12-07 06:53:28.311962: step 15470, loss = 0.67, batch loss = 0.59 (10.6 examples/sec; 0.754 sec/batch; 66h:25m:33s remains)
INFO - root - 2017-12-07 06:53:35.968377: step 15480, loss = 0.95, batch loss = 0.88 (10.7 examples/sec; 0.747 sec/batch; 65h:44m:53s remains)
INFO - root - 2017-12-07 06:53:43.595847: step 15490, loss = 0.76, batch loss = 0.68 (10.7 examples/sec; 0.746 sec/batch; 65h:39m:51s remains)
INFO - root - 2017-12-07 06:53:51.337293: step 15500, loss = 0.75, batch loss = 0.67 (9.9 examples/sec; 0.804 sec/batch; 70h:48m:45s remains)
2017-12-07 06:53:51.964842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8437755 -2.3765063 -1.7246981 -1.0498948 -0.76543665 -0.87657928 -1.0578611 -1.4115491 -1.945024 -2.0191379 -1.8962615 -1.9168959 -1.9283845 -1.9912941 -2.2058361][-2.003217 -1.5202053 -0.99252868 -0.55982208 -0.42768526 -0.6462419 -0.99067092 -1.4534705 -1.9726794 -2.0114706 -1.9133682 -1.9374585 -1.9217744 -1.9205446 -2.126405][-1.0612497 -0.75163007 -0.59688067 -0.63044119 -0.81381226 -1.242255 -1.8121755 -2.3844492 -2.775816 -2.6801434 -2.5059977 -2.4316921 -2.2984185 -2.1029854 -2.1278379][-0.61897349 -0.61256862 -0.86007881 -1.2835662 -1.6629744 -2.0916088 -2.6455331 -3.1668105 -3.3639765 -3.1561849 -2.9161892 -2.7525563 -2.5237613 -2.1519763 -1.9584434][-1.1162686 -1.3059666 -1.6646783 -2.0979471 -2.357492 -2.4681926 -2.668129 -2.8997376 -2.8843656 -2.6587248 -2.4723911 -2.3696034 -2.181242 -1.7949088 -1.5831821][-1.9198139 -2.0609372 -2.1506026 -2.262394 -2.2512298 -2.0174122 -1.8272557 -1.7336731 -1.5696619 -1.3921673 -1.3475673 -1.4337361 -1.4132349 -1.2069945 -1.2144861][-2.1732512 -2.1072721 -1.8157096 -1.5209954 -1.2639534 -0.84298754 -0.4028635 -0.13733625 -0.093720436 -0.17575455 -0.40827847 -0.77333188 -0.98912477 -1.0162785 -1.2367728][-2.0815034 -1.7834671 -1.2101827 -0.67857528 -0.41073704 -0.074478626 0.40392256 0.67106533 0.46116447 0.094508171 -0.30082321 -0.79113245 -1.1766779 -1.4130409 -1.732064][-2.251009 -1.8184521 -1.1990883 -0.6986506 -0.60430837 -0.52771854 -0.21485758 0.040597439 -0.1330781 -0.516727 -0.87520409 -1.3155699 -1.7672951 -2.1330483 -2.4686933][-2.6112638 -2.1819611 -1.686749 -1.3217986 -1.3163998 -1.290283 -1.0303235 -0.74980545 -0.84461212 -1.2295904 -1.6222742 -2.0614259 -2.5076046 -2.8346252 -3.051168][-2.6517611 -2.1974065 -1.7821736 -1.5291173 -1.5652287 -1.5906472 -1.4795184 -1.2989705 -1.3545661 -1.6678789 -2.0406013 -2.4566863 -2.860898 -3.1073837 -3.2244117][-2.2977781 -1.7811067 -1.3133297 -1.0434251 -1.0850658 -1.2460349 -1.418679 -1.5267904 -1.6880679 -1.920362 -2.1405814 -2.421386 -2.7503748 -2.9781983 -3.1190696][-1.6742561 -1.0957291 -0.55255938 -0.25484848 -0.34977722 -0.68413973 -1.0991101 -1.4498107 -1.7202399 -1.9426877 -2.0336254 -2.1513603 -2.3731577 -2.5836844 -2.7868061][-1.2091305 -0.64182782 -0.12097979 0.13166904 -0.041471958 -0.47029757 -0.9152298 -1.2457957 -1.4507394 -1.5835507 -1.531471 -1.4811037 -1.5667822 -1.7515929 -2.0119915][-1.3252478 -0.91608834 -0.55107141 -0.39575958 -0.53839517 -0.84726238 -1.1310105 -1.2633584 -1.2308397 -1.1377702 -0.90325642 -0.69852805 -0.6615715 -0.8270452 -1.1313918]]...]
INFO - root - 2017-12-07 06:53:59.643495: step 15510, loss = 0.67, batch loss = 0.60 (9.9 examples/sec; 0.808 sec/batch; 71h:07m:09s remains)
INFO - root - 2017-12-07 06:54:07.439901: step 15520, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.768 sec/batch; 67h:34m:54s remains)
INFO - root - 2017-12-07 06:54:14.958392: step 15530, loss = 0.90, batch loss = 0.82 (10.3 examples/sec; 0.777 sec/batch; 68h:25m:10s remains)
INFO - root - 2017-12-07 06:54:22.750357: step 15540, loss = 0.84, batch loss = 0.77 (10.3 examples/sec; 0.777 sec/batch; 68h:25m:32s remains)
INFO - root - 2017-12-07 06:54:30.495493: step 15550, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.762 sec/batch; 67h:06m:56s remains)
INFO - root - 2017-12-07 06:54:38.164702: step 15560, loss = 0.76, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 69h:18m:30s remains)
INFO - root - 2017-12-07 06:54:45.933820: step 15570, loss = 0.86, batch loss = 0.78 (10.1 examples/sec; 0.795 sec/batch; 69h:59m:54s remains)
INFO - root - 2017-12-07 06:54:53.756482: step 15580, loss = 0.68, batch loss = 0.61 (10.2 examples/sec; 0.781 sec/batch; 68h:44m:17s remains)
INFO - root - 2017-12-07 06:55:01.459710: step 15590, loss = 0.83, batch loss = 0.75 (10.3 examples/sec; 0.775 sec/batch; 68h:12m:43s remains)
INFO - root - 2017-12-07 06:55:09.155535: step 15600, loss = 0.67, batch loss = 0.59 (10.1 examples/sec; 0.790 sec/batch; 69h:31m:50s remains)
2017-12-07 06:55:09.873896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6093619 -2.6463521 -2.6641824 -2.6209018 -2.6467996 -3.0451329 -3.6986308 -4.1231661 -4.2019815 -3.9096012 -3.4039845 -2.7850657 -1.7630284 -0.95725584 -1.261095][-2.6444449 -2.6646891 -2.6406412 -2.5445869 -2.5147386 -2.883172 -3.5293789 -3.9832735 -4.1448689 -3.9280348 -3.4832675 -2.9739041 -2.006691 -1.0970175 -1.3044693][-2.7819686 -2.721828 -2.6129918 -2.457057 -2.37028 -2.6777675 -3.2369432 -3.6219115 -3.8138444 -3.7224758 -3.4048512 -2.9977167 -2.0830107 -1.1561232 -1.3420813][-2.8969905 -2.7669139 -2.5819383 -2.39757 -2.3249781 -2.6587496 -3.1902761 -3.4870555 -3.607635 -3.5403118 -3.2928622 -2.9564443 -2.1182561 -1.2530324 -1.447428][-2.883852 -2.7791207 -2.5664258 -2.3745744 -2.3395092 -2.7368567 -3.2896307 -3.5208073 -3.5648341 -3.4764109 -3.2136312 -2.8974438 -2.1524315 -1.3958659 -1.6011832][-2.9891019 -2.9560575 -2.6535752 -2.2834888 -2.0457845 -2.2448406 -2.652513 -2.8412986 -2.9872155 -3.0775762 -2.9211783 -2.6908245 -2.0829027 -1.4563122 -1.677614][-3.3352332 -3.3825617 -3.0012431 -2.4105117 -1.8090749 -1.5214729 -1.531287 -1.6425347 -2.0288088 -2.4666314 -2.5448613 -2.4458523 -1.9953611 -1.4859054 -1.7179494][-3.6391447 -3.8261034 -3.570411 -3.024272 -2.2603426 -1.5363677 -1.1178672 -1.1157451 -1.6241412 -2.19591 -2.309201 -2.2136657 -1.8762956 -1.5046344 -1.7837212][-3.6878035 -4.0873775 -4.1286511 -3.882863 -3.2918596 -2.4456444 -1.8115144 -1.7037456 -2.1076303 -2.5038524 -2.4340296 -2.2057965 -1.8761742 -1.5664809 -1.8563466][-3.5630643 -4.1387258 -4.4450164 -4.5007744 -4.1755137 -3.4114885 -2.8099027 -2.740767 -3.056653 -3.209147 -2.8782635 -2.4539657 -2.0153224 -1.6380692 -1.8843789][-3.3433952 -3.9785259 -4.3791614 -4.5364585 -4.2826781 -3.5225904 -2.9943004 -3.1283002 -3.5864425 -3.6864569 -3.2447882 -2.7436345 -2.2234039 -1.7373714 -1.9078095][-3.0752783 -3.6271832 -3.9420283 -4.0489244 -3.7153888 -2.8610134 -2.3564003 -2.7219625 -3.4251914 -3.6345496 -3.2784266 -2.903842 -2.4143703 -1.8462415 -1.9323735][-2.9234333 -3.3443363 -3.5137122 -3.5418625 -3.13023 -2.1847465 -1.6618843 -2.1827042 -3.0867767 -3.4114449 -3.1697381 -2.9527657 -2.5177915 -1.8817189 -1.9020212][-2.969528 -3.2970488 -3.3909016 -3.3786821 -2.9054458 -1.9011242 -1.3078253 -1.8465514 -2.8498926 -3.2715268 -3.1134341 -2.9956541 -2.5827484 -1.8929424 -1.8706906][-2.9172654 -3.2036326 -3.3268139 -3.3232203 -2.798511 -1.7420392 -1.0248156 -1.4431603 -2.4656107 -2.9919426 -2.92774 -2.9007673 -2.5466738 -1.8792989 -1.8791707]]...]
INFO - root - 2017-12-07 06:55:17.418430: step 15610, loss = 0.98, batch loss = 0.91 (10.5 examples/sec; 0.761 sec/batch; 66h:57m:42s remains)
INFO - root - 2017-12-07 06:55:24.997035: step 15620, loss = 0.94, batch loss = 0.87 (10.3 examples/sec; 0.778 sec/batch; 68h:28m:52s remains)
INFO - root - 2017-12-07 06:55:32.475997: step 15630, loss = 1.04, batch loss = 0.97 (10.4 examples/sec; 0.770 sec/batch; 67h:46m:01s remains)
INFO - root - 2017-12-07 06:55:40.140780: step 15640, loss = 0.92, batch loss = 0.85 (10.7 examples/sec; 0.750 sec/batch; 66h:02m:47s remains)
INFO - root - 2017-12-07 06:55:47.692106: step 15650, loss = 0.97, batch loss = 0.90 (10.7 examples/sec; 0.748 sec/batch; 65h:49m:09s remains)
INFO - root - 2017-12-07 06:55:55.322611: step 15660, loss = 0.91, batch loss = 0.83 (10.6 examples/sec; 0.756 sec/batch; 66h:33m:28s remains)
INFO - root - 2017-12-07 06:56:02.893263: step 15670, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 66h:13m:28s remains)
INFO - root - 2017-12-07 06:56:10.545456: step 15680, loss = 0.84, batch loss = 0.76 (10.4 examples/sec; 0.773 sec/batch; 68h:00m:57s remains)
INFO - root - 2017-12-07 06:56:18.089297: step 15690, loss = 0.86, batch loss = 0.79 (10.1 examples/sec; 0.795 sec/batch; 69h:56m:52s remains)
INFO - root - 2017-12-07 06:56:25.690849: step 15700, loss = 0.76, batch loss = 0.68 (10.5 examples/sec; 0.763 sec/batch; 67h:10m:37s remains)
2017-12-07 06:56:26.353256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6917925 -3.705677 -3.7569609 -3.8189418 -3.879056 -3.9897733 -4.1512308 -4.2880869 -4.3464794 -4.2925277 -4.2147827 -4.1749744 -4.1327634 -4.0609908 -3.9858353][-3.354723 -3.2057984 -3.1584144 -3.2055037 -3.3042126 -3.4845033 -3.7381067 -3.9577236 -4.0567493 -3.9559453 -3.7870297 -3.7150974 -3.7172277 -3.7017953 -3.7068439][-2.9888289 -2.6903181 -2.550777 -2.5698295 -2.6690898 -2.8598447 -3.137816 -3.3622775 -3.4745266 -3.3623433 -3.1133051 -3.0312269 -3.1045289 -3.1215398 -3.1460214][-2.6034298 -2.2766604 -2.1482496 -2.2118676 -2.3226516 -2.4842052 -2.6879356 -2.7733352 -2.82676 -2.7712941 -2.5532186 -2.5487051 -2.7427232 -2.7717628 -2.7249613][-2.5672579 -2.355283 -2.300436 -2.3890774 -2.4520848 -2.4854722 -2.4471869 -2.1959157 -2.1646941 -2.3449812 -2.3660171 -2.543942 -2.897846 -2.8970366 -2.6413207][-2.7086575 -2.6616657 -2.6694028 -2.7354999 -2.6785488 -2.4846437 -2.0278802 -1.2710595 -1.0931151 -1.6415534 -2.1453977 -2.7111778 -3.3411288 -3.3592818 -2.8710957][-2.9081161 -2.8514214 -2.8130479 -2.8832664 -2.8233967 -2.4797151 -1.5960627 -0.28426886 0.15416288 -0.63514853 -1.5195899 -2.4754882 -3.4263387 -3.5907943 -3.0367496][-3.1186318 -2.8651948 -2.6824718 -2.7275627 -2.7150669 -2.3784983 -1.4002044 0.028193951 0.51281452 -0.23426771 -1.1100476 -2.1377563 -3.154593 -3.3791971 -2.8650315][-3.2665024 -2.8783867 -2.6490145 -2.7308829 -2.8039737 -2.6368871 -1.9445012 -0.94066238 -0.66514277 -1.1332636 -1.6403418 -2.4325135 -3.2636735 -3.3640475 -2.8601575][-3.5288868 -3.1096897 -2.8760819 -2.9611721 -3.1227002 -3.2118754 -2.9892764 -2.5698118 -2.5550358 -2.7658224 -2.9030528 -3.3985023 -3.8751438 -3.7049055 -3.1399441][-4.0513153 -3.693296 -3.3787115 -3.2962217 -3.4051685 -3.6168158 -3.7252598 -3.7636378 -4.0094695 -4.1645465 -4.1446171 -4.4148803 -4.5450497 -4.1163549 -3.4841356][-4.4332743 -4.2654438 -3.9835358 -3.8051009 -3.8380954 -4.0114174 -4.1485567 -4.2789159 -4.58408 -4.7752986 -4.7841616 -4.9396248 -4.8439932 -4.29155 -3.6870229][-4.5630093 -4.555088 -4.3733778 -4.2270069 -4.2526464 -4.371829 -4.4581304 -4.5150213 -4.7188621 -4.8485708 -4.8162274 -4.808979 -4.5821524 -4.0923448 -3.6513081][-4.4889574 -4.5774784 -4.5082603 -4.4412017 -4.4754725 -4.5632715 -4.62491 -4.6505733 -4.7776179 -4.845355 -4.7388649 -4.5853214 -4.3071942 -3.94032 -3.6677909][-4.1855555 -4.2642875 -4.2754745 -4.287425 -4.3339782 -4.3902392 -4.4151549 -4.3964119 -4.4338393 -4.4476614 -4.3548641 -4.2261033 -4.05079 -3.8605123 -3.7266023]]...]
INFO - root - 2017-12-07 06:56:33.933267: step 15710, loss = 0.63, batch loss = 0.55 (10.2 examples/sec; 0.788 sec/batch; 69h:18m:15s remains)
INFO - root - 2017-12-07 06:56:41.581606: step 15720, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.761 sec/batch; 66h:55m:43s remains)
INFO - root - 2017-12-07 06:56:48.910639: step 15730, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.766 sec/batch; 67h:23m:20s remains)
INFO - root - 2017-12-07 06:56:56.605816: step 15740, loss = 0.74, batch loss = 0.66 (10.5 examples/sec; 0.762 sec/batch; 67h:00m:35s remains)
INFO - root - 2017-12-07 06:57:04.249996: step 15750, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.768 sec/batch; 67h:33m:28s remains)
INFO - root - 2017-12-07 06:57:11.891852: step 15760, loss = 0.74, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 69h:35m:51s remains)
INFO - root - 2017-12-07 06:57:19.517054: step 15770, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 66h:55m:39s remains)
INFO - root - 2017-12-07 06:57:27.232993: step 15780, loss = 0.94, batch loss = 0.87 (10.5 examples/sec; 0.759 sec/batch; 66h:47m:48s remains)
INFO - root - 2017-12-07 06:57:34.833296: step 15790, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.761 sec/batch; 66h:59m:13s remains)
INFO - root - 2017-12-07 06:57:42.491733: step 15800, loss = 1.24, batch loss = 1.17 (10.4 examples/sec; 0.769 sec/batch; 67h:40m:12s remains)
2017-12-07 06:57:43.090683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7021298 -1.5715725 -1.5595295 -1.6491511 -1.6679611 -1.6433764 -1.9075453 -2.47163 -2.9443178 -3.057668 -2.8555818 -2.6454227 -2.5605569 -2.4590619 -2.3993216][-1.5747852 -1.3641746 -1.3957074 -1.646672 -1.7593744 -1.6807835 -1.8877342 -2.5147176 -3.0980585 -3.1938705 -2.9226208 -2.7184446 -2.6579533 -2.5932288 -2.5518646][-1.1877542 -1.0175002 -1.1813748 -1.5509324 -1.7607224 -1.7734773 -1.9881976 -2.5401845 -3.0597394 -3.0934739 -2.8628521 -2.7619009 -2.7588279 -2.7141137 -2.6848803][-1.1184251 -1.0116668 -1.213048 -1.5642416 -1.8271189 -1.969702 -2.1807656 -2.4977455 -2.7933745 -2.8489866 -2.8575099 -2.9467466 -3.0013952 -2.99338 -2.9698596][-1.5406673 -1.5415058 -1.6990135 -1.9201546 -2.1409962 -2.2778993 -2.2954247 -2.2407463 -2.3175588 -2.5892313 -2.9851239 -3.2700064 -3.3392386 -3.3351245 -3.2870102][-1.9237585 -2.0137844 -2.1956 -2.3807752 -2.5558753 -2.5537543 -2.2803252 -1.9418833 -1.9762502 -2.535799 -3.2445407 -3.6285577 -3.667711 -3.5909922 -3.4369671][-1.8481977 -1.8357098 -2.0511322 -2.3278372 -2.5618575 -2.5033989 -2.1030669 -1.7699237 -1.9875495 -2.7590928 -3.5244393 -3.8773856 -3.8892643 -3.7201884 -3.4444218][-1.4635205 -1.1676345 -1.270324 -1.5744817 -1.9148273 -2.0128605 -1.8319342 -1.7906189 -2.2553685 -2.9991012 -3.6183634 -3.9593873 -4.0194931 -3.7988214 -3.4528627][-1.2289383 -0.59319067 -0.45722365 -0.66632581 -1.0411501 -1.3204305 -1.4556944 -1.7125773 -2.2522347 -2.8165584 -3.2954867 -3.6954849 -3.8798151 -3.7558861 -3.4742005][-1.1526446 -0.39034557 -0.14004946 -0.23392391 -0.50750637 -0.79968333 -1.0267069 -1.2888629 -1.6607416 -2.0152669 -2.4296572 -2.9222457 -3.228374 -3.2706578 -3.1892869][-1.1589901 -0.56273246 -0.4321785 -0.5449636 -0.73395872 -0.92718863 -1.0341852 -1.0808656 -1.1951647 -1.3757038 -1.688705 -2.0964088 -2.3627961 -2.4639874 -2.5324605][-1.3717976 -1.2271125 -1.3639972 -1.573035 -1.745508 -1.8500359 -1.8429489 -1.7426419 -1.6876235 -1.6775694 -1.7300019 -1.8628318 -1.9597533 -1.9772096 -1.999414][-1.9647646 -2.2269073 -2.5001745 -2.7021117 -2.839565 -2.8899927 -2.872313 -2.8103862 -2.75893 -2.67421 -2.5334811 -2.4314632 -2.3550014 -2.2280354 -2.0560935][-2.6239314 -2.9741805 -3.1940141 -3.3344126 -3.4518635 -3.4987242 -3.5319743 -3.5794058 -3.6018827 -3.5267339 -3.3342879 -3.1368232 -2.9691248 -2.7693982 -2.4948423][-3.0108943 -3.2250013 -3.3045897 -3.3688908 -3.4856651 -3.5590034 -3.6287241 -3.7207508 -3.7772663 -3.7509832 -3.6137781 -3.4208632 -3.2264104 -3.0312679 -2.7800488]]...]
INFO - root - 2017-12-07 06:57:50.712977: step 15810, loss = 0.98, batch loss = 0.90 (10.7 examples/sec; 0.745 sec/batch; 65h:32m:23s remains)
INFO - root - 2017-12-07 06:57:58.284874: step 15820, loss = 0.80, batch loss = 0.72 (10.2 examples/sec; 0.783 sec/batch; 68h:53m:30s remains)
INFO - root - 2017-12-07 06:58:05.710722: step 15830, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.753 sec/batch; 66h:13m:01s remains)
INFO - root - 2017-12-07 06:58:13.355627: step 15840, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.760 sec/batch; 66h:50m:26s remains)
INFO - root - 2017-12-07 06:58:21.060832: step 15850, loss = 0.66, batch loss = 0.59 (10.2 examples/sec; 0.784 sec/batch; 68h:56m:57s remains)
INFO - root - 2017-12-07 06:58:28.726618: step 15860, loss = 0.72, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 68h:33m:43s remains)
INFO - root - 2017-12-07 06:58:36.472071: step 15870, loss = 0.99, batch loss = 0.92 (10.4 examples/sec; 0.770 sec/batch; 67h:44m:45s remains)
INFO - root - 2017-12-07 06:58:44.074017: step 15880, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.751 sec/batch; 66h:04m:51s remains)
INFO - root - 2017-12-07 06:58:51.696269: step 15890, loss = 0.92, batch loss = 0.84 (10.6 examples/sec; 0.754 sec/batch; 66h:16m:20s remains)
INFO - root - 2017-12-07 06:58:59.306001: step 15900, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.756 sec/batch; 66h:31m:10s remains)
2017-12-07 06:58:59.900357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7787101 -1.7461145 -1.7565763 -1.7276583 -1.6897802 -1.6893368 -1.7561111 -1.7909775 -1.7451985 -1.6739929 -1.6505344 -1.6165876 -1.6403451 -1.6606696 -1.7404802][-1.6262932 -1.5805566 -1.6127727 -1.5965016 -1.5435569 -1.5412679 -1.6687043 -1.8000283 -1.8024662 -1.7586863 -1.759635 -1.7606828 -1.8298497 -1.8455522 -1.8896492][-1.3675983 -1.3014324 -1.3785729 -1.4018614 -1.3464327 -1.3383131 -1.4966629 -1.6869476 -1.7399302 -1.7377481 -1.754581 -1.8475342 -2.0214143 -2.1075633 -2.1139598][-0.88143039 -0.851542 -1.0476916 -1.1679378 -1.1338191 -1.0991156 -1.2178264 -1.3747089 -1.4672513 -1.5207992 -1.5405262 -1.7003143 -1.9861083 -2.178262 -2.1690085][-0.39073086 -0.4728415 -0.83355 -1.0366514 -0.97503662 -0.84653139 -0.85355783 -0.91359305 -1.0568237 -1.2155082 -1.2659202 -1.4614708 -1.7896006 -2.0018866 -1.9675915][-0.075447083 -0.29298067 -0.77667856 -0.98078918 -0.798717 -0.44963837 -0.2473917 -0.14796686 -0.38825274 -0.77178097 -0.98881316 -1.3048933 -1.6949773 -1.8868396 -1.8457525][0.093187332 -0.27974987 -0.88663578 -1.1172121 -0.83393097 -0.2037034 0.32567549 0.65930748 0.2821064 -0.39241028 -0.83450651 -1.2673626 -1.6857643 -1.8537471 -1.8752551][0.16013288 -0.36818504 -1.0849676 -1.4047415 -1.1365747 -0.36063385 0.4111042 0.97826242 0.5738368 -0.20821381 -0.6464777 -0.97275567 -1.2652366 -1.4404747 -1.6322229][0.14434052 -0.43284225 -1.1082273 -1.4497256 -1.2885144 -0.670352 -0.066328526 0.39661694 0.10076284 -0.45938683 -0.62330723 -0.659554 -0.71372366 -0.8731811 -1.2139604][-0.039328575 -0.52908421 -1.0200388 -1.3125393 -1.270066 -0.95282054 -0.74539566 -0.58350849 -0.78440356 -1.0109782 -0.85143661 -0.60840988 -0.45476055 -0.56413126 -0.92069411][-0.43662596 -0.80923796 -1.1583838 -1.4217014 -1.4542685 -1.3694611 -1.4497185 -1.4747479 -1.5718269 -1.5709023 -1.3168724 -1.0457644 -0.83483863 -0.85698676 -1.0555773][-0.84006262 -1.0139177 -1.2966487 -1.6608558 -1.8121238 -1.8376448 -1.9980354 -2.0105217 -1.9481442 -1.8619182 -1.7491758 -1.7087955 -1.5907629 -1.5255196 -1.4706047][-1.1330059 -1.0582922 -1.2771184 -1.7222753 -1.957484 -1.9921594 -2.0962775 -2.0268543 -1.879283 -1.8438447 -1.9471209 -2.1614437 -2.1598439 -2.0525916 -1.8327479][-1.4882731 -1.2403021 -1.3868017 -1.8145833 -2.0668361 -2.0973737 -2.168844 -2.1136463 -2.0250432 -2.1188087 -2.32338 -2.5734496 -2.5452976 -2.3617542 -2.053477][-2.0009921 -1.7715778 -1.8857145 -2.2210798 -2.4154081 -2.4179261 -2.4453979 -2.4020336 -2.3998287 -2.5789611 -2.7516539 -2.87115 -2.720808 -2.4500928 -2.1279087]]...]
INFO - root - 2017-12-07 06:59:07.486403: step 15910, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.783 sec/batch; 68h:51m:46s remains)
INFO - root - 2017-12-07 06:59:15.202062: step 15920, loss = 0.86, batch loss = 0.78 (10.3 examples/sec; 0.776 sec/batch; 68h:14m:38s remains)
INFO - root - 2017-12-07 06:59:22.587618: step 15930, loss = 0.74, batch loss = 0.66 (10.9 examples/sec; 0.735 sec/batch; 64h:37m:57s remains)
INFO - root - 2017-12-07 06:59:30.183078: step 15940, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.768 sec/batch; 67h:31m:47s remains)
INFO - root - 2017-12-07 06:59:37.811466: step 15950, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.761 sec/batch; 66h:54m:44s remains)
INFO - root - 2017-12-07 06:59:45.391991: step 15960, loss = 0.78, batch loss = 0.71 (11.0 examples/sec; 0.728 sec/batch; 63h:58m:25s remains)
INFO - root - 2017-12-07 06:59:53.092100: step 15970, loss = 0.94, batch loss = 0.87 (10.1 examples/sec; 0.793 sec/batch; 69h:40m:59s remains)
INFO - root - 2017-12-07 07:00:00.742360: step 15980, loss = 0.73, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 68h:50m:36s remains)
INFO - root - 2017-12-07 07:00:08.574341: step 15990, loss = 0.96, batch loss = 0.89 (10.4 examples/sec; 0.768 sec/batch; 67h:32m:32s remains)
INFO - root - 2017-12-07 07:00:16.316946: step 16000, loss = 0.70, batch loss = 0.63 (10.6 examples/sec; 0.754 sec/batch; 66h:19m:08s remains)
2017-12-07 07:00:16.932660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5507617 -3.8357859 -3.3202133 -3.3181345 -3.6275034 -3.6557858 -3.2623365 -2.8667169 -2.6633811 -2.8171897 -3.3438795 -3.698246 -3.5746078 -2.9697866 -2.2778363][-4.0575304 -3.5062475 -3.0979173 -3.2456522 -3.7584264 -3.8825104 -3.5164378 -3.0319684 -2.6315017 -2.7530413 -3.272789 -3.4580112 -3.1585407 -2.6663978 -2.178093][-3.3888946 -2.9128122 -2.6067944 -2.9150434 -3.6283488 -3.8070378 -3.3497686 -2.8097572 -2.4807644 -2.7120476 -3.0841928 -2.889415 -2.3133636 -1.8509338 -1.5165024][-3.1757193 -2.4922071 -2.092597 -2.463191 -3.27136 -3.4643238 -2.9321866 -2.4159067 -2.2428317 -2.5396767 -2.8101037 -2.4608493 -1.7809772 -1.1980121 -0.78630066][-3.2820711 -2.3587754 -1.8228045 -2.1672652 -2.8832011 -2.97658 -2.413949 -1.9590442 -1.8616619 -2.2168055 -2.5965245 -2.4566443 -1.9368815 -1.2845271 -0.76276135][-3.2699523 -2.2744291 -1.6908064 -1.9235022 -2.330653 -2.2212305 -1.6285052 -1.130923 -0.97901964 -1.4800456 -2.1773148 -2.3910384 -2.0967224 -1.4032528 -0.8334074][-2.9372635 -2.0530379 -1.6249647 -1.8522449 -1.9865429 -1.6806633 -0.97980094 -0.14060354 0.31970978 -0.32789421 -1.3817034 -1.912328 -1.8010285 -1.1278532 -0.58191562][-2.4879718 -1.5801427 -1.3224597 -1.666501 -1.745472 -1.4907985 -0.851069 0.25644827 0.95798016 0.28867245 -0.81108332 -1.353138 -1.261256 -0.65810513 -0.263515][-2.3130596 -1.2115519 -0.803061 -1.0165431 -1.0984147 -1.126684 -0.92426348 -0.085477352 0.49318314 8.0108643e-05 -0.7754221 -1.1142893 -0.95909095 -0.49982548 -0.35475731][-2.5659065 -1.3935783 -0.69984674 -0.56151867 -0.53254247 -0.72593713 -0.85933328 -0.43849707 -0.067161083 -0.33332491 -0.82475948 -1.12485 -0.99311018 -0.60657549 -0.58366036][-2.7789025 -1.8593304 -1.1234136 -0.79762125 -0.66879368 -0.79543829 -0.93811989 -0.74134278 -0.4959178 -0.56254268 -0.94319105 -1.3248839 -1.2382739 -0.84316468 -0.7910614][-2.4594591 -1.9623244 -1.3776593 -0.98150253 -0.76160812 -0.82338548 -0.98880744 -0.99775982 -0.91458559 -0.93819642 -1.2910829 -1.6904113 -1.6239195 -1.2809076 -1.2215168][-1.9478068 -1.7403138 -1.3424349 -0.95317054 -0.68931603 -0.80002189 -1.1100366 -1.3771527 -1.481863 -1.5234494 -1.8092322 -2.1175394 -2.0890617 -1.9445312 -1.9880402][-1.681459 -1.5793219 -1.3565578 -1.104986 -0.95860028 -1.2000706 -1.6340756 -2.0161495 -2.1584988 -2.1704187 -2.3662927 -2.5684519 -2.5981653 -2.6566181 -2.800648][-1.8306863 -1.8290997 -1.7800119 -1.6904025 -1.6463745 -1.8891687 -2.2618644 -2.5627131 -2.6429405 -2.62187 -2.7409 -2.8398476 -2.9226377 -3.1219146 -3.3293409]]...]
INFO - root - 2017-12-07 07:00:24.648056: step 16010, loss = 0.97, batch loss = 0.90 (10.4 examples/sec; 0.771 sec/batch; 67h:47m:22s remains)
INFO - root - 2017-12-07 07:00:32.257773: step 16020, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.762 sec/batch; 67h:01m:09s remains)
INFO - root - 2017-12-07 07:00:39.806574: step 16030, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.781 sec/batch; 68h:38m:41s remains)
INFO - root - 2017-12-07 07:00:47.516879: step 16040, loss = 0.83, batch loss = 0.75 (10.0 examples/sec; 0.796 sec/batch; 69h:59m:57s remains)
INFO - root - 2017-12-07 07:00:55.279303: step 16050, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.786 sec/batch; 69h:07m:04s remains)
INFO - root - 2017-12-07 07:01:02.989154: step 16060, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.772 sec/batch; 67h:49m:13s remains)
INFO - root - 2017-12-07 07:01:10.685743: step 16070, loss = 0.73, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 69h:02m:50s remains)
INFO - root - 2017-12-07 07:01:18.292945: step 16080, loss = 0.86, batch loss = 0.78 (10.2 examples/sec; 0.786 sec/batch; 69h:03m:23s remains)
INFO - root - 2017-12-07 07:01:25.899730: step 16090, loss = 0.99, batch loss = 0.92 (10.8 examples/sec; 0.743 sec/batch; 65h:20m:41s remains)
INFO - root - 2017-12-07 07:01:33.520429: step 16100, loss = 0.65, batch loss = 0.58 (10.5 examples/sec; 0.761 sec/batch; 66h:54m:41s remains)
2017-12-07 07:01:34.119421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7688305 -3.8545678 -4.0152736 -4.2027297 -4.3300185 -4.313035 -4.3704138 -4.621243 -4.742816 -4.56394 -4.314188 -4.2326503 -4.2245359 -4.1143551 -3.9115462][-3.4785357 -3.4594097 -3.6029148 -3.8631411 -4.0417981 -4.0066972 -4.1188221 -4.541698 -4.7655973 -4.5561347 -4.2207665 -4.0649743 -3.9993067 -3.8640695 -3.6403069][-3.0216742 -2.7991936 -2.7919302 -2.981904 -3.1450694 -3.1681414 -3.4902751 -4.1158366 -4.3219895 -3.9912772 -3.6102767 -3.4571052 -3.391181 -3.3004603 -3.1411061][-2.4883802 -2.0476282 -1.9120984 -2.0557897 -2.2573056 -2.4520593 -3.0464916 -3.7119429 -3.5980344 -2.9343359 -2.4820166 -2.4269478 -2.4404993 -2.4726508 -2.4814129][-1.9438162 -1.3068669 -1.1438298 -1.3664007 -1.6417735 -1.9281569 -2.5435631 -2.985455 -2.4170721 -1.4431365 -1.0396082 -1.1541653 -1.2985716 -1.495543 -1.7284129][-1.4435267 -0.70277429 -0.62163877 -1.0116925 -1.3963025 -1.7352428 -2.173147 -2.2134507 -1.3254235 -0.38531065 -0.28931952 -0.60743117 -0.85910892 -1.1677701 -1.4683948][-1.099966 -0.4631958 -0.56203008 -1.0642684 -1.4751291 -1.7563088 -1.8643925 -1.3496609 -0.18510437 0.44765234 -0.068717 -0.7507863 -1.1557181 -1.529319 -1.7397087][-1.3346126 -1.0027478 -1.3177228 -1.7974885 -1.9910579 -1.9022093 -1.4923275 -0.48379707 0.7854228 0.98906326 -0.03048563 -0.96849823 -1.483479 -1.844286 -1.8846085][-1.9379308 -1.9372258 -2.4107265 -2.7728906 -2.603744 -2.0565155 -1.2907488 -0.27346611 0.61352253 0.41254473 -0.62757039 -1.4283524 -1.8197622 -1.9901292 -1.7814398][-2.1982663 -2.3362777 -2.7139187 -2.8121314 -2.3280954 -1.512192 -0.68525267 -0.047732353 0.21850252 -0.22085762 -0.99981451 -1.4891136 -1.7038276 -1.6978498 -1.3941424][-2.3754172 -2.4786677 -2.5242383 -2.207293 -1.4758799 -0.65719414 5.2452087e-06 0.25477314 0.082060814 -0.40138626 -0.8339889 -0.99531579 -1.0764399 -1.0436609 -0.85660863][-2.6692 -2.4940634 -2.0978754 -1.4468312 -0.7378726 -0.20684767 0.090241909 0.050604343 -0.26607895 -0.61780548 -0.74059176 -0.6170764 -0.55110383 -0.54752254 -0.56326032][-2.7500253 -2.2086234 -1.5106716 -0.82329941 -0.38147831 -0.22849226 -0.30147266 -0.54426932 -0.83739328 -0.99247432 -0.91066194 -0.65858269 -0.5437398 -0.59498262 -0.74342489][-2.494072 -1.8519745 -1.1709158 -0.66049576 -0.50211334 -0.62145591 -0.91744351 -1.2758362 -1.5490417 -1.6424158 -1.541254 -1.3200519 -1.2108369 -1.2647195 -1.4101045][-2.4398317 -1.925261 -1.4363163 -1.1510229 -1.172101 -1.4086916 -1.7528212 -2.0902996 -2.3154304 -2.4185109 -2.4062598 -2.2970569 -2.2153988 -2.2080796 -2.2589436]]...]
INFO - root - 2017-12-07 07:01:41.923079: step 16110, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.770 sec/batch; 67h:40m:53s remains)
INFO - root - 2017-12-07 07:01:49.672551: step 16120, loss = 0.70, batch loss = 0.63 (10.8 examples/sec; 0.739 sec/batch; 64h:58m:09s remains)
INFO - root - 2017-12-07 07:01:57.206633: step 16130, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.787 sec/batch; 69h:07m:54s remains)
INFO - root - 2017-12-07 07:02:04.905216: step 16140, loss = 0.88, batch loss = 0.81 (10.8 examples/sec; 0.742 sec/batch; 65h:11m:13s remains)
INFO - root - 2017-12-07 07:02:12.637642: step 16150, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.753 sec/batch; 66h:09m:50s remains)
INFO - root - 2017-12-07 07:02:20.351755: step 16160, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.772 sec/batch; 67h:47m:48s remains)
INFO - root - 2017-12-07 07:02:28.037434: step 16170, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.786 sec/batch; 69h:05m:45s remains)
INFO - root - 2017-12-07 07:02:35.715421: step 16180, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.758 sec/batch; 66h:36m:42s remains)
INFO - root - 2017-12-07 07:02:43.403972: step 16190, loss = 1.15, batch loss = 1.07 (10.7 examples/sec; 0.746 sec/batch; 65h:32m:31s remains)
INFO - root - 2017-12-07 07:02:50.970743: step 16200, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.776 sec/batch; 68h:12m:30s remains)
2017-12-07 07:02:51.578636: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.46142387 -0.63293123 -0.71468353 -0.79113317 -1.1367795 -1.0501385 -0.51538897 -0.38236046 -0.67851567 -0.607394 -0.26132059 -0.36074114 -0.40212536 -0.2871232 -0.41639662][-0.20662832 -0.37729597 -0.50442648 -0.68359518 -1.0044427 -0.94560766 -0.48672867 -0.33959866 -0.69384694 -0.68515992 -0.46736145 -0.7133069 -0.55899596 -0.24534512 -0.44868279][-0.023182869 -0.1742835 -0.25568724 -0.4293189 -0.63800693 -0.57118177 -0.18160486 -0.015855789 -0.381423 -0.41624737 -0.42952466 -0.87706947 -0.61454368 -0.2058239 -0.46870327][0.20548153 0.21454954 0.25045061 0.077783108 -0.089219093 -0.03094244 0.38515234 0.62499 0.29908657 0.20450592 -0.061198235 -0.67850351 -0.47673917 -0.14583302 -0.4659524][0.47466707 0.730917 0.85765362 0.62462091 0.44509315 0.50534821 1.0329528 1.4087725 1.1601901 0.97934961 0.52835131 -0.15429306 -0.12284803 0.017889977 -0.35031652][0.7293458 1.1150846 1.1873798 0.86953783 0.77211046 0.96616888 1.6911278 2.2269678 2.015142 1.7107921 1.1843543 0.56931257 0.42743111 0.30431032 -0.14690971][0.90077734 1.2516074 1.2052579 0.90419436 0.97691393 1.2993164 2.1721816 2.8265519 2.6679974 2.2883992 1.7559085 1.2891383 1.0400424 0.69355726 0.20183897][1.2125769 1.4514513 1.3220382 1.1100574 1.3045139 1.6086893 2.4443698 3.0527129 3.0000973 2.684411 2.2244511 1.9227381 1.5744495 1.0678639 0.56256962][1.7209287 1.8921213 1.7512045 1.6528516 1.8212233 1.9371133 2.5387135 2.8818884 2.8742414 2.6732841 2.2923174 2.1205683 1.7638507 1.2225404 0.7663455][1.8236256 2.0264473 1.9589615 1.9930687 2.0464602 1.9617352 2.3353195 2.4569554 2.4521084 2.382966 2.1067009 2.0417504 1.7267532 1.2424603 0.89127064][1.3699379 1.576642 1.6010399 1.755362 1.7327895 1.5693493 1.8586202 1.8409452 1.7690215 1.72825 1.5126152 1.5078497 1.243196 0.83078 0.51471519][0.338274 0.58787966 0.70191288 0.92988873 0.8631897 0.74394131 1.0248485 0.94803619 0.78069735 0.69228745 0.48062086 0.47688627 0.23052311 -0.10709476 -0.4097147][-0.67071795 -0.40473461 -0.22393179 0.056325436 0.0061120987 -0.057105064 0.15012598 0.017158985 -0.21477509 -0.38038158 -0.57369947 -0.5783546 -0.8029654 -1.0674357 -1.3041813][-1.5467117 -1.3045223 -1.1107082 -0.82333374 -0.7993207 -0.82250667 -0.72637439 -0.92948556 -1.2259915 -1.448637 -1.6442347 -1.6837063 -1.8966367 -2.1050451 -2.2386763][-2.2753458 -2.1081057 -1.9617844 -1.7418115 -1.7043216 -1.7626255 -1.7955608 -2.0205584 -2.2688258 -2.4417171 -2.5615005 -2.5943704 -2.7643776 -2.9063203 -2.9505954]]...]
INFO - root - 2017-12-07 07:02:59.377301: step 16210, loss = 0.87, batch loss = 0.79 (10.3 examples/sec; 0.779 sec/batch; 68h:24m:08s remains)
INFO - root - 2017-12-07 07:03:06.995022: step 16220, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.775 sec/batch; 68h:06m:46s remains)
INFO - root - 2017-12-07 07:03:14.323297: step 16230, loss = 1.07, batch loss = 1.00 (10.2 examples/sec; 0.782 sec/batch; 68h:42m:49s remains)
INFO - root - 2017-12-07 07:03:22.081666: step 16240, loss = 0.67, batch loss = 0.59 (10.6 examples/sec; 0.755 sec/batch; 66h:17m:45s remains)
INFO - root - 2017-12-07 07:03:29.619260: step 16250, loss = 0.79, batch loss = 0.71 (11.0 examples/sec; 0.729 sec/batch; 64h:02m:05s remains)
INFO - root - 2017-12-07 07:03:37.301458: step 16260, loss = 0.65, batch loss = 0.57 (10.2 examples/sec; 0.781 sec/batch; 68h:33m:50s remains)
INFO - root - 2017-12-07 07:03:44.901451: step 16270, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.781 sec/batch; 68h:38m:36s remains)
INFO - root - 2017-12-07 07:03:52.557534: step 16280, loss = 0.68, batch loss = 0.60 (10.6 examples/sec; 0.751 sec/batch; 66h:00m:00s remains)
INFO - root - 2017-12-07 07:04:00.200836: step 16290, loss = 0.66, batch loss = 0.59 (10.4 examples/sec; 0.766 sec/batch; 67h:15m:16s remains)
INFO - root - 2017-12-07 07:04:07.856721: step 16300, loss = 0.63, batch loss = 0.56 (10.5 examples/sec; 0.759 sec/batch; 66h:37m:55s remains)
2017-12-07 07:04:08.446324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.94433 -2.2371888 -2.3601623 -2.3962555 -2.2686551 -1.9903562 -1.7527556 -1.652014 -1.6539202 -1.6284878 -1.5935597 -1.6941457 -1.9035313 -2.0796402 -2.1800995][-2.1990771 -2.7006323 -2.9745884 -3.07635 -2.8336382 -2.2939291 -1.8136487 -1.6599286 -1.7710853 -1.8585577 -1.8852258 -2.0106571 -2.2090297 -2.4628091 -2.7020228][-2.6104836 -3.2770967 -3.6388454 -3.734673 -3.3388505 -2.5023761 -1.7339191 -1.5588017 -1.8999209 -2.2770729 -2.4757586 -2.5866032 -2.6038985 -2.7859383 -3.0889034][-3.0917957 -3.8775196 -4.2520642 -4.2374454 -3.609005 -2.4755964 -1.4464991 -1.266603 -1.9052165 -2.6845045 -3.1348758 -3.2166102 -2.9881587 -3.0246763 -3.3153014][-3.5492473 -4.3673959 -4.6862803 -4.5142059 -3.6421323 -2.2629387 -1.0320854 -0.841609 -1.7677927 -2.941834 -3.6370385 -3.7152095 -3.2921576 -3.1904621 -3.4535882][-3.8122594 -4.5826254 -4.8200283 -4.5088615 -3.44693 -1.8982859 -0.53487086 -0.30886936 -1.4503279 -2.9522367 -3.8733835 -4.0331006 -3.5001392 -3.2314987 -3.405302][-3.8829556 -4.5426273 -4.7226548 -4.3488941 -3.1958427 -1.5177269 -0.029724598 0.27855539 -0.94497538 -2.6841483 -3.8527145 -4.2075057 -3.7245014 -3.31003 -3.353148][-3.9210272 -4.38597 -4.491076 -4.1167059 -3.0252151 -1.3728993 0.1116786 0.46615314 -0.65949392 -2.4389968 -3.809238 -4.4114466 -4.0873289 -3.5805583 -3.4771652][-4.0470142 -4.3311224 -4.3665614 -4.0584917 -3.1693335 -1.790333 -0.52724338 -0.21306372 -1.0653708 -2.6323862 -4.0144134 -4.7087226 -4.474854 -3.9176741 -3.6989198][-4.2595181 -4.4321866 -4.4106278 -4.1639581 -3.538007 -2.6007955 -1.7176723 -1.4831057 -1.9816189 -3.1222682 -4.2945967 -4.8936596 -4.6883135 -4.1377354 -3.8661225][-4.3648582 -4.476253 -4.4395576 -4.2692819 -3.9023511 -3.3857131 -2.8848155 -2.7325854 -2.9434772 -3.6591735 -4.51709 -4.8956203 -4.6593051 -4.1704125 -3.9093137][-4.1414976 -4.2040014 -4.1678843 -4.0909867 -3.968405 -3.7823596 -3.5698466 -3.4730144 -3.5020251 -3.869895 -4.3814883 -4.4987879 -4.2285714 -3.8653827 -3.6767139][-3.5786653 -3.5939591 -3.5502727 -3.533443 -3.5639048 -3.5817752 -3.5552671 -3.5063829 -3.4490571 -3.5776699 -3.7807457 -3.7003829 -3.4596128 -3.2476912 -3.1510563][-2.9400282 -2.9416242 -2.9097 -2.9177198 -2.9900799 -3.0621006 -3.104033 -3.094954 -3.0437341 -3.0648398 -3.0945983 -2.9398332 -2.7560639 -2.6448154 -2.6065242][-2.3902233 -2.3996761 -2.3886054 -2.389348 -2.4329052 -2.5002408 -2.5720828 -2.5975409 -2.5591598 -2.5120978 -2.4431686 -2.3086543 -2.2195308 -2.1979823 -2.2005007]]...]
INFO - root - 2017-12-07 07:04:16.082281: step 16310, loss = 0.95, batch loss = 0.88 (10.9 examples/sec; 0.735 sec/batch; 64h:32m:11s remains)
INFO - root - 2017-12-07 07:04:23.734406: step 16320, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.754 sec/batch; 66h:12m:30s remains)
INFO - root - 2017-12-07 07:04:31.121867: step 16330, loss = 0.86, batch loss = 0.78 (10.3 examples/sec; 0.775 sec/batch; 68h:02m:31s remains)
INFO - root - 2017-12-07 07:04:38.715931: step 16340, loss = 0.74, batch loss = 0.67 (11.0 examples/sec; 0.724 sec/batch; 63h:35m:49s remains)
INFO - root - 2017-12-07 07:04:46.430766: step 16350, loss = 0.75, batch loss = 0.67 (10.7 examples/sec; 0.747 sec/batch; 65h:36m:31s remains)
INFO - root - 2017-12-07 07:04:54.138737: step 16360, loss = 0.96, batch loss = 0.89 (10.4 examples/sec; 0.772 sec/batch; 67h:48m:32s remains)
INFO - root - 2017-12-07 07:05:01.899774: step 16370, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.770 sec/batch; 67h:35m:31s remains)
INFO - root - 2017-12-07 07:05:09.569552: step 16380, loss = 1.07, batch loss = 1.00 (10.2 examples/sec; 0.782 sec/batch; 68h:38m:55s remains)
INFO - root - 2017-12-07 07:05:17.292102: step 16390, loss = 0.92, batch loss = 0.85 (10.6 examples/sec; 0.753 sec/batch; 66h:09m:35s remains)
INFO - root - 2017-12-07 07:05:25.025625: step 16400, loss = 0.84, batch loss = 0.76 (10.1 examples/sec; 0.791 sec/batch; 69h:27m:50s remains)
2017-12-07 07:05:25.627855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3886123 -2.3108075 -2.4161351 -2.3920934 -2.4783719 -2.3956804 -2.0663917 -1.7600589 -1.2572014 -0.93329048 -1.1350029 -1.4428923 -1.8427067 -2.2146311 -2.3591833][-2.1016965 -1.8932655 -1.9457183 -1.8826151 -1.9208703 -1.7692869 -1.4143667 -1.0669901 -0.42677021 -0.13730669 -0.54199195 -0.98934388 -1.5200608 -1.9472477 -2.1276252][-2.0546913 -1.6379397 -1.5956318 -1.4939523 -1.5467877 -1.4381716 -1.1285555 -0.66223216 0.20618296 0.43097448 -0.19011831 -0.77693152 -1.403105 -1.7538016 -1.8820677][-2.2437487 -1.7775364 -1.7192883 -1.6168396 -1.7169499 -1.7454808 -1.528388 -0.95365262 0.10055733 0.26920223 -0.43928146 -1.0946023 -1.7412236 -1.8366776 -1.7472696][-2.3428628 -2.041235 -2.0336161 -1.9407947 -2.0302181 -2.1379759 -2.0034144 -1.3700867 -0.21560431 -0.096033573 -0.77529955 -1.4260004 -2.1529856 -2.0800352 -1.7829909][-2.1948831 -2.0567853 -1.9981906 -1.8528798 -1.9232783 -2.0432689 -1.8375256 -1.0502164 0.16652822 0.1262517 -0.5828805 -1.2198684 -2.1393921 -2.1588364 -1.8483362][-2.0013516 -1.92676 -1.7525115 -1.5287678 -1.5552819 -1.5049083 -0.95938444 0.14263391 1.360631 0.89446449 -0.022954464 -0.69145584 -1.7931709 -2.022356 -1.8115716][-1.7355149 -1.71959 -1.4876566 -1.2000194 -1.1585779 -0.85950685 0.090085983 1.5524683 2.6983428 1.6696835 0.45908785 -0.27699518 -1.4397528 -1.7774851 -1.6655858][-1.1624181 -1.2447448 -1.0852287 -0.80882311 -0.78423214 -0.48126674 0.44368792 1.8973637 2.8367686 1.6519394 0.52551174 -0.13368368 -1.2345769 -1.5543602 -1.5105376][-0.54790521 -0.8143611 -0.80463243 -0.61208916 -0.69112825 -0.62132645 -0.10350037 0.96807384 1.665503 0.8508172 0.25856018 -0.19679499 -1.1946955 -1.4713938 -1.4732857][-0.37694454 -0.73702621 -0.7826426 -0.59040689 -0.70245671 -0.79425383 -0.62381673 0.098734856 0.59405851 0.10531521 0.0044922829 -0.35444307 -1.3137648 -1.567425 -1.5566933][-0.67384839 -0.921123 -0.81662226 -0.51781297 -0.58076191 -0.6357882 -0.51406693 0.049594402 0.37817097 0.019937992 0.11910534 -0.34187412 -1.3522117 -1.6638522 -1.6765237][-1.0018902 -1.1221778 -0.82159615 -0.4334383 -0.43140793 -0.39565849 -0.19075346 0.35042286 0.64193678 0.3697958 0.47885847 -0.12090349 -1.1762364 -1.62433 -1.7685921][-0.97788334 -1.0387216 -0.68944836 -0.38512135 -0.41926384 -0.33416986 -0.085343838 0.41255569 0.75103092 0.624753 0.6805954 -0.037827015 -1.0813758 -1.6421678 -1.9001215][-0.71668339 -0.71024895 -0.40535498 -0.27981853 -0.45569658 -0.4411006 -0.26447105 0.11186504 0.46519804 0.47124672 0.45555592 -0.36694908 -1.3543253 -1.8946037 -2.1254587]]...]
INFO - root - 2017-12-07 07:05:33.277670: step 16410, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 67h:25m:35s remains)
INFO - root - 2017-12-07 07:05:41.010649: step 16420, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.758 sec/batch; 66h:33m:40s remains)
INFO - root - 2017-12-07 07:05:48.436824: step 16430, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.763 sec/batch; 66h:57m:42s remains)
INFO - root - 2017-12-07 07:05:56.022303: step 16440, loss = 0.81, batch loss = 0.73 (10.7 examples/sec; 0.748 sec/batch; 65h:41m:35s remains)
INFO - root - 2017-12-07 07:06:03.709568: step 16450, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.754 sec/batch; 66h:10m:52s remains)
INFO - root - 2017-12-07 07:06:11.427902: step 16460, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.771 sec/batch; 67h:40m:41s remains)
INFO - root - 2017-12-07 07:06:19.126705: step 16470, loss = 0.69, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 68h:51m:02s remains)
INFO - root - 2017-12-07 07:06:27.022379: step 16480, loss = 1.06, batch loss = 0.99 (10.3 examples/sec; 0.778 sec/batch; 68h:18m:39s remains)
INFO - root - 2017-12-07 07:06:34.613933: step 16490, loss = 0.92, batch loss = 0.84 (11.4 examples/sec; 0.703 sec/batch; 61h:40m:33s remains)
INFO - root - 2017-12-07 07:06:42.205389: step 16500, loss = 1.07, batch loss = 1.00 (10.4 examples/sec; 0.766 sec/batch; 67h:13m:56s remains)
2017-12-07 07:06:42.830994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4608314 -2.4079351 -2.1994061 -1.9777505 -2.2770252 -3.0803604 -3.4409659 -3.1922469 -3.0671196 -2.8156137 -2.1530116 -1.8909514 -2.1339378 -2.6454859 -3.1573806][-2.666358 -2.522851 -2.1751888 -1.8270092 -2.1691823 -3.0321033 -3.409817 -3.270998 -3.2765749 -3.0480194 -2.3251362 -1.8681746 -1.996058 -2.7383909 -3.5384464][-2.5399604 -2.3022521 -1.7943218 -1.3689132 -1.8233998 -2.7254207 -3.1006203 -3.0810156 -3.1414042 -2.9014211 -2.2479172 -1.7163291 -1.7398431 -2.6404862 -3.5693324][-2.0527937 -1.7315681 -1.0699661 -0.66380644 -1.3004489 -2.1817167 -2.4556146 -2.4607353 -2.4174573 -2.1017361 -1.5600643 -1.0628619 -1.0609422 -2.0792027 -3.0580339][-1.7259066 -1.4287982 -0.67911863 -0.40453291 -1.2103078 -1.9276245 -1.9749455 -1.8674734 -1.6189241 -1.1760597 -0.72331953 -0.30987215 -0.35206985 -1.3749881 -2.2167048][-1.6917367 -1.4580545 -0.71391678 -0.62467694 -1.4241061 -1.7724752 -1.5602303 -1.3064339 -0.81044745 -0.24032354 0.11213589 0.38217258 0.20793629 -0.82804704 -1.5119364][-1.8943481 -1.6544144 -0.91743469 -0.84178352 -1.2515471 -1.0362656 -0.65986991 -0.37036514 0.27772522 0.79125357 0.83086395 0.84859371 0.51079178 -0.58560419 -1.2346802][-2.1352336 -1.7937839 -1.0591154 -0.84473991 -0.68958378 0.000228405 0.28035831 0.45749283 1.1142888 1.3852305 1.0583925 0.91820955 0.56882477 -0.552778 -1.284297][-2.1844749 -1.7266726 -1.0420432 -0.68877792 -0.13854408 0.57941961 0.45842981 0.44015694 1.037909 1.1472626 0.76456451 0.74121141 0.52467346 -0.57467556 -1.4282274][-2.1036296 -1.51664 -0.91597223 -0.52434278 0.061407089 0.39654875 -0.096968174 -0.16199636 0.4715271 0.63984632 0.47113657 0.63269043 0.48786974 -0.59432578 -1.572722][-2.1263731 -1.4824793 -1.0264213 -0.72297049 -0.33225298 -0.42961645 -1.0520005 -0.97624588 -0.2464838 0.098864079 0.25830889 0.539124 0.35706568 -0.7099402 -1.7628186][-2.3072648 -1.690479 -1.455091 -1.322324 -1.1443977 -1.4658074 -1.9871225 -1.8048234 -1.120507 -0.68269634 -0.29041529 0.054005146 -0.16807461 -1.1180954 -2.1283052][-2.7242248 -2.1977499 -2.1621041 -2.188009 -2.1439805 -2.4660623 -2.7889135 -2.5849729 -2.0569692 -1.6435022 -1.175678 -0.85005546 -1.0982649 -1.8936236 -2.7610948][-3.1972389 -2.7750621 -2.8662944 -2.9911656 -3.0347555 -3.287683 -3.4586337 -3.297075 -2.9150352 -2.5559764 -2.1712751 -1.9814954 -2.2611053 -2.8782756 -3.5306797][-3.4908848 -3.1821756 -3.3285077 -3.482893 -3.5516109 -3.6915972 -3.7273552 -3.6006467 -3.3405335 -3.1051896 -2.9269209 -2.9269738 -3.2315297 -3.6890154 -4.120265]]...]
INFO - root - 2017-12-07 07:06:50.615868: step 16510, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.783 sec/batch; 68h:44m:46s remains)
INFO - root - 2017-12-07 07:06:58.360716: step 16520, loss = 0.81, batch loss = 0.73 (10.0 examples/sec; 0.803 sec/batch; 70h:26m:46s remains)
INFO - root - 2017-12-07 07:07:05.866287: step 16530, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.776 sec/batch; 68h:07m:26s remains)
INFO - root - 2017-12-07 07:07:13.662970: step 16540, loss = 0.81, batch loss = 0.74 (10.0 examples/sec; 0.798 sec/batch; 70h:00m:31s remains)
INFO - root - 2017-12-07 07:07:21.313743: step 16550, loss = 0.68, batch loss = 0.61 (10.2 examples/sec; 0.782 sec/batch; 68h:38m:21s remains)
INFO - root - 2017-12-07 07:07:29.163699: step 16560, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.765 sec/batch; 67h:07m:20s remains)
INFO - root - 2017-12-07 07:07:36.811097: step 16570, loss = 0.72, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 67h:09m:36s remains)
INFO - root - 2017-12-07 07:07:44.533291: step 16580, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.760 sec/batch; 66h:39m:59s remains)
INFO - root - 2017-12-07 07:07:52.807877: step 16590, loss = 0.79, batch loss = 0.72 (7.7 examples/sec; 1.034 sec/batch; 90h:44m:09s remains)
INFO - root - 2017-12-07 07:08:03.258679: step 16600, loss = 0.84, batch loss = 0.76 (7.8 examples/sec; 1.027 sec/batch; 90h:05m:44s remains)
2017-12-07 07:08:04.065951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3318353 -1.8703265 -1.5763454 -1.3175507 -1.0779228 -0.98116016 -1.1192596 -1.0753365 -1.0955725 -1.402611 -1.3994896 -1.4890924 -1.9941874 -2.4820223 -2.8658741][-2.3977404 -2.0825305 -1.7722809 -1.428551 -1.1365612 -1.0013244 -1.0654099 -0.97442889 -1.0443246 -1.4106019 -1.4060802 -1.512388 -2.0103729 -2.4834371 -2.994221][-2.9222341 -2.7251844 -2.4489551 -2.127085 -1.9165521 -1.8520029 -1.8669329 -1.7504182 -1.8190575 -2.0779934 -1.9708962 -2.0459578 -2.5539932 -3.0620551 -3.6642828][-3.67163 -3.5022793 -3.184288 -2.8162947 -2.60268 -2.5012925 -2.3880672 -2.234652 -2.2864549 -2.3913176 -2.1862423 -2.1625733 -2.5529232 -3.002739 -3.5251436][-4.1068015 -3.8609219 -3.4910808 -3.1014223 -2.8529372 -2.6781621 -2.4443421 -2.2465851 -2.2850266 -2.3300042 -2.1539412 -2.0607514 -2.2147973 -2.4621451 -2.7691298][-3.9182866 -3.5226572 -3.050622 -2.6822658 -2.5815 -2.5554788 -2.3611326 -2.0696468 -2.0284388 -2.0229397 -1.8750527 -1.708931 -1.6539569 -1.7276227 -1.8781042][-3.5548036 -3.0775762 -2.4918697 -2.0622766 -1.9889688 -2.0026703 -1.6676121 -1.0221045 -0.82072687 -0.84990549 -0.77593231 -0.60884881 -0.469903 -0.49502468 -0.62231135][-3.3248668 -2.7524021 -1.9308572 -1.262387 -0.99612331 -0.74028468 -0.026418209 1.0191336 1.152153 0.72223711 0.44084644 0.27529573 0.11848831 -0.13802767 -0.45535874][-3.139359 -2.4041052 -1.3603058 -0.49385285 -0.12881947 0.16405964 0.91220188 1.9170537 1.7754693 0.91649294 0.31473017 -0.17741013 -0.59955168 -0.98892426 -1.4093375][-3.1079566 -2.266778 -1.1920471 -0.3359313 -0.10639191 -0.14996576 0.08127737 0.37826204 -0.08756876 -0.86120224 -1.2998221 -1.6407835 -1.811347 -1.9235842 -2.1736708][-3.2612071 -2.5215063 -1.6920373 -1.0373292 -0.97797394 -1.3496783 -1.5376122 -1.6981905 -2.202054 -2.6265268 -2.6992798 -2.6959562 -2.4461353 -2.1899514 -2.2209876][-3.3959675 -2.8618021 -2.4091361 -2.0238743 -2.0551145 -2.5153704 -2.8039927 -3.0378213 -3.343724 -3.3765841 -3.2236881 -3.0353737 -2.5805349 -2.1597154 -2.0919466][-3.4567494 -3.0642772 -2.8497081 -2.631443 -2.661283 -3.0707347 -3.3106189 -3.443625 -3.5172205 -3.3537486 -3.1677456 -2.9853024 -2.5750191 -2.2205915 -2.2032175][-3.6156273 -3.2241354 -3.0292041 -2.861979 -2.823245 -3.1029625 -3.2729363 -3.2528846 -3.0918953 -2.8406429 -2.7562661 -2.7934978 -2.6581845 -2.4788115 -2.4876747][-3.8438065 -3.4140725 -3.1787071 -3.0411136 -2.9513104 -3.1388755 -3.33757 -3.2970407 -3.081023 -2.8002081 -2.6772656 -2.7334509 -2.7636752 -2.7585719 -2.8258231]]...]
INFO - root - 2017-12-07 07:08:14.486110: step 16610, loss = 0.69, batch loss = 0.62 (7.6 examples/sec; 1.052 sec/batch; 92h:21m:13s remains)
INFO - root - 2017-12-07 07:08:24.921694: step 16620, loss = 0.74, batch loss = 0.67 (7.5 examples/sec; 1.060 sec/batch; 93h:02m:15s remains)
INFO - root - 2017-12-07 07:08:35.157367: step 16630, loss = 0.84, batch loss = 0.76 (7.8 examples/sec; 1.025 sec/batch; 89h:55m:52s remains)
INFO - root - 2017-12-07 07:08:45.451897: step 16640, loss = 0.80, batch loss = 0.72 (7.9 examples/sec; 1.007 sec/batch; 88h:22m:48s remains)
INFO - root - 2017-12-07 07:08:55.762465: step 16650, loss = 0.86, batch loss = 0.79 (7.7 examples/sec; 1.041 sec/batch; 91h:21m:24s remains)
INFO - root - 2017-12-07 07:09:06.191378: step 16660, loss = 0.79, batch loss = 0.72 (7.8 examples/sec; 1.029 sec/batch; 90h:18m:49s remains)
INFO - root - 2017-12-07 07:09:16.620048: step 16670, loss = 0.80, batch loss = 0.72 (7.8 examples/sec; 1.031 sec/batch; 90h:29m:22s remains)
INFO - root - 2017-12-07 07:09:26.976118: step 16680, loss = 0.88, batch loss = 0.81 (7.7 examples/sec; 1.042 sec/batch; 91h:23m:57s remains)
INFO - root - 2017-12-07 07:09:37.311548: step 16690, loss = 0.68, batch loss = 0.60 (7.7 examples/sec; 1.044 sec/batch; 91h:37m:38s remains)
INFO - root - 2017-12-07 07:09:47.569283: step 16700, loss = 0.88, batch loss = 0.81 (7.8 examples/sec; 1.020 sec/batch; 89h:27m:57s remains)
2017-12-07 07:09:48.468810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4748728 -1.7232215 -2.2508073 -2.2562747 -1.6672902 -0.73241329 -0.17359781 -1.1528234 -2.1137288 -1.7578926 -1.2067502 -0.52279353 -0.13799477 -0.16766071 -0.022536278][-1.1832602 -1.5174654 -2.2055418 -2.1567709 -1.2592726 0.17165041 1.0768223 -0.25269318 -1.7164309 -1.5002713 -1.0348346 -0.22655439 0.44792175 0.52630854 0.62672949][-1.146291 -1.5553913 -2.4229808 -2.3429186 -1.0971587 1.0045695 2.4892578 0.90396976 -1.2277076 -1.3773983 -1.0681198 -0.029366493 1.0674715 1.2761874 1.3204031][-1.4793487 -1.917474 -2.9770017 -2.937923 -1.3301468 1.5432277 3.7295704 1.9560976 -0.86637521 -1.4666502 -1.3427024 -0.054049492 1.4540129 1.7765584 1.7445955][-1.8599722 -2.2729037 -3.4738383 -3.4931178 -1.5771146 2.0044966 4.8642359 3.0049529 -0.458529 -1.5804577 -1.6831071 -0.13369083 1.7681942 2.1785884 2.0231781][-2.0471814 -2.3705812 -3.6122093 -3.6906183 -1.6217854 2.4203758 5.794529 4.0280075 0.16092157 -1.411921 -1.7087305 0.069659233 2.2495294 2.6762419 2.4121327][-2.1401365 -2.3547845 -3.5887766 -3.787765 -1.8125331 2.3290315 5.9567461 4.4473772 0.4534874 -1.4244027 -1.7487917 0.27960253 2.6005325 2.982944 2.5868492][-2.3906009 -2.5030141 -3.6656454 -4.0549774 -2.3684974 1.5846128 5.2293653 4.1206493 0.29969931 -1.712568 -1.9628396 0.24373484 2.5341163 2.8274264 2.2735453][-2.6642981 -2.7524567 -3.811029 -4.3295393 -2.9508698 0.66194963 4.1490822 3.390377 -0.14185524 -2.1761062 -2.3192844 -0.034422398 2.1325274 2.3697567 1.6950936][-2.97074 -3.066288 -3.9606385 -4.4371991 -3.3307447 -0.21199131 2.9737635 2.5391879 -0.58971691 -2.5935421 -2.688612 -0.50086975 1.4543786 1.6591072 0.91308784][-3.15892 -3.22623 -3.9412067 -4.3190465 -3.4614117 -0.96634746 1.7289171 1.5141916 -1.127928 -3.0254269 -3.1140828 -1.1816621 0.515079 0.75130272 0.038944244][-3.1944575 -3.2619851 -3.8627148 -4.1802554 -3.5187643 -1.6122062 0.52119923 0.3862586 -1.7864964 -3.4854794 -3.4954545 -1.8273807 -0.35849714 -0.078621387 -0.75828624][-3.3288195 -3.3848805 -3.886692 -4.1640496 -3.6802704 -2.3348403 -0.81334233 -0.89809728 -2.5193064 -3.8681965 -3.8048794 -2.4268882 -1.160526 -0.86321616 -1.5248089][-3.3831234 -3.4459462 -3.8841472 -4.1094537 -3.7575779 -2.8669872 -1.878907 -1.9639142 -3.123951 -4.1635671 -4.1123853 -3.0100589 -1.8213077 -1.3898351 -1.9469178][-3.3499327 -3.4055529 -3.7867389 -3.98288 -3.744915 -3.1699615 -2.5250502 -2.5473113 -3.339541 -4.1858492 -4.2590122 -3.4777436 -2.3964188 -1.8180027 -2.1918004]]...]
INFO - root - 2017-12-07 07:09:58.696030: step 16710, loss = 1.06, batch loss = 0.99 (7.8 examples/sec; 1.032 sec/batch; 90h:29m:51s remains)
INFO - root - 2017-12-07 07:10:08.935799: step 16720, loss = 0.71, batch loss = 0.64 (8.0 examples/sec; 0.998 sec/batch; 87h:33m:43s remains)
INFO - root - 2017-12-07 07:10:19.089077: step 16730, loss = 0.75, batch loss = 0.68 (7.9 examples/sec; 1.012 sec/batch; 88h:43m:46s remains)
INFO - root - 2017-12-07 07:10:29.401745: step 16740, loss = 0.83, batch loss = 0.75 (7.4 examples/sec; 1.076 sec/batch; 94h:22m:10s remains)
INFO - root - 2017-12-07 07:10:39.700286: step 16750, loss = 0.68, batch loss = 0.60 (7.9 examples/sec; 1.017 sec/batch; 89h:09m:52s remains)
INFO - root - 2017-12-07 07:10:50.020896: step 16760, loss = 0.92, batch loss = 0.85 (7.7 examples/sec; 1.036 sec/batch; 90h:53m:59s remains)
INFO - root - 2017-12-07 07:11:00.380290: step 16770, loss = 0.81, batch loss = 0.74 (7.7 examples/sec; 1.045 sec/batch; 91h:40m:48s remains)
INFO - root - 2017-12-07 07:11:10.661105: step 16780, loss = 0.85, batch loss = 0.78 (7.9 examples/sec; 1.016 sec/batch; 89h:05m:43s remains)
INFO - root - 2017-12-07 07:11:21.146523: step 16790, loss = 0.97, batch loss = 0.90 (7.5 examples/sec; 1.068 sec/batch; 93h:41m:55s remains)
INFO - root - 2017-12-07 07:11:31.438715: step 16800, loss = 0.58, batch loss = 0.51 (7.6 examples/sec; 1.057 sec/batch; 92h:40m:18s remains)
2017-12-07 07:11:32.244653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6831968 -3.6007588 -3.5085588 -3.4075723 -3.2541392 -3.0520627 -2.9395137 -3.0040143 -3.1792006 -3.1766441 -3.0500517 -3.1245542 -3.2511394 -3.2874207 -3.1989484][-3.8490682 -3.7179577 -3.5643859 -3.2716885 -2.8459544 -2.4419661 -2.3445518 -2.6398902 -3.0317249 -3.0609975 -2.8805532 -2.9544382 -3.0850596 -3.0899792 -3.0092959][-3.8410714 -3.7267127 -3.6275969 -3.2147927 -2.5068557 -1.8950696 -1.78841 -2.3529222 -3.0260618 -3.1550894 -2.970541 -2.9793324 -2.9986446 -2.9199431 -2.8762681][-3.4851205 -3.4060826 -3.480437 -3.1552446 -2.3643596 -1.574213 -1.3053398 -2.0098572 -2.9614735 -3.2620902 -3.2161744 -3.2534924 -3.1537952 -2.9304519 -2.8516715][-2.904573 -2.8799572 -3.1548474 -2.9721072 -2.2012012 -1.2364995 -0.71617794 -1.4798999 -2.6339941 -3.0633032 -3.1952438 -3.3314934 -3.195456 -2.8480482 -2.6386442][-2.3840048 -2.4468853 -2.94201 -2.8633869 -1.9491065 -0.60823035 0.23545837 -0.639987 -2.0732925 -2.6761351 -2.9086266 -3.000421 -2.77919 -2.3815413 -2.1021988][-2.0701408 -2.1433644 -2.7125869 -2.6061831 -1.461514 0.25180149 1.3465967 0.34610319 -1.3428743 -2.2243943 -2.5850532 -2.5455184 -2.1458852 -1.7433574 -1.5282354][-1.9606242 -1.9530385 -2.2940395 -1.933466 -0.65794444 1.0401325 1.9616494 0.76991796 -0.92454243 -1.8210335 -2.1462023 -1.9752464 -1.491528 -1.2198675 -1.2252669][-1.9427142 -1.8263535 -1.7774105 -1.2021096 -0.16708469 0.919672 1.1642146 -0.071341991 -1.3830931 -1.8603077 -1.857806 -1.5175681 -1.1228576 -1.2065632 -1.6024265][-1.8941424 -1.6391432 -1.3530033 -0.91490841 -0.46888351 -0.14432621 -0.39589119 -1.4082425 -2.1954191 -2.3308938 -2.1853225 -1.8515048 -1.5972157 -1.8802273 -2.4124868][-2.001157 -1.6774225 -1.4462171 -1.3652267 -1.4432259 -1.500597 -1.7508922 -2.3016112 -2.6553984 -2.7726383 -2.8400018 -2.7583311 -2.6579881 -2.89882 -3.259069][-2.3094122 -2.1414886 -2.1706238 -2.3671396 -2.5810118 -2.586813 -2.5962052 -2.7638469 -2.9233232 -3.1190987 -3.2522182 -3.2011292 -3.1182985 -3.2709312 -3.5122445][-2.4553246 -2.4947515 -2.7107985 -2.9679024 -3.0935855 -2.9481025 -2.7619448 -2.75064 -2.9306331 -3.246922 -3.3648596 -3.1904941 -2.9758809 -2.9939647 -3.1795194][-2.4952488 -2.5315816 -2.72904 -2.9446044 -3.0141315 -2.8744121 -2.70914 -2.6970062 -2.9041121 -3.2116449 -3.3086028 -3.1437285 -2.947602 -2.9155688 -3.0447476][-2.8276901 -2.7885995 -2.8725064 -2.98845 -3.0319088 -2.9890971 -2.9530177 -3.0043323 -3.1637583 -3.3473616 -3.3874211 -3.2702088 -3.1514688 -3.1275179 -3.1976452]]...]
INFO - root - 2017-12-07 07:11:42.629033: step 16810, loss = 0.74, batch loss = 0.66 (7.6 examples/sec; 1.049 sec/batch; 91h:59m:00s remains)
INFO - root - 2017-12-07 07:11:53.007080: step 16820, loss = 0.80, batch loss = 0.73 (7.8 examples/sec; 1.025 sec/batch; 89h:54m:40s remains)
INFO - root - 2017-12-07 07:12:03.321424: step 16830, loss = 0.84, batch loss = 0.77 (7.4 examples/sec; 1.086 sec/batch; 95h:15m:59s remains)
INFO - root - 2017-12-07 07:12:13.690149: step 16840, loss = 1.00, batch loss = 0.93 (7.7 examples/sec; 1.042 sec/batch; 91h:23m:48s remains)
INFO - root - 2017-12-07 07:12:24.090399: step 16850, loss = 0.87, batch loss = 0.80 (7.5 examples/sec; 1.072 sec/batch; 93h:58m:26s remains)
INFO - root - 2017-12-07 07:12:34.419001: step 16860, loss = 0.76, batch loss = 0.69 (7.8 examples/sec; 1.023 sec/batch; 89h:40m:38s remains)
INFO - root - 2017-12-07 07:12:44.718880: step 16870, loss = 0.81, batch loss = 0.74 (7.9 examples/sec; 1.019 sec/batch; 89h:18m:17s remains)
INFO - root - 2017-12-07 07:12:55.105778: step 16880, loss = 1.03, batch loss = 0.96 (7.6 examples/sec; 1.047 sec/batch; 91h:48m:28s remains)
INFO - root - 2017-12-07 07:13:05.431794: step 16890, loss = 0.90, batch loss = 0.82 (7.8 examples/sec; 1.020 sec/batch; 89h:25m:05s remains)
INFO - root - 2017-12-07 07:13:15.815244: step 16900, loss = 0.91, batch loss = 0.84 (7.7 examples/sec; 1.035 sec/batch; 90h:43m:27s remains)
2017-12-07 07:13:16.621274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3883619 -1.4283557 -1.5080247 -1.6085415 -1.6870763 -1.7171078 -1.6941671 -1.6385272 -1.5879338 -1.5573342 -1.550071 -1.5388024 -1.539458 -1.5897942 -1.6430354][-1.3543327 -1.3939426 -1.521904 -1.685431 -1.7969127 -1.83339 -1.7989986 -1.6951609 -1.5787914 -1.512557 -1.5324123 -1.56199 -1.5956745 -1.6864572 -1.7846615][-1.2262948 -1.2507286 -1.4277902 -1.6580977 -1.8266087 -1.944181 -1.9850876 -1.8764229 -1.6911092 -1.5804894 -1.6059904 -1.6367066 -1.6502075 -1.7471557 -1.895757][-0.976156 -0.92971849 -1.0868089 -1.3263686 -1.5318396 -1.7485144 -1.8987951 -1.8447199 -1.6718948 -1.607933 -1.6929212 -1.7307308 -1.695456 -1.7822714 -1.9558325][-0.71339417 -0.55805659 -0.60811353 -0.73217344 -0.84969425 -1.0131688 -1.1235795 -1.0415168 -0.93653131 -1.0903187 -1.4163458 -1.5938044 -1.568593 -1.6018102 -1.6828747][-0.54894018 -0.32514238 -0.24771738 -0.18640041 -0.1204443 -0.067845821 0.10371971 0.45148611 0.62199593 0.30915308 -0.2636857 -0.63920093 -0.72479343 -0.76754618 -0.74144745][-0.40407658 -0.18200445 -0.019656181 0.17183638 0.35887957 0.58842373 1.0234871 1.6112452 1.9084034 1.5982528 0.94927406 0.47031689 0.309906 0.30707502 0.44169378][-0.33384418 -0.18303537 -0.019515514 0.17925978 0.341053 0.55658293 1.0156364 1.597333 1.9432011 1.7512283 1.2549844 0.89794922 0.807219 0.88090324 1.015419][-0.28942919 -0.20522594 -0.080656528 0.044580936 0.0867157 0.14607096 0.43785763 0.81210709 1.0562959 0.98867369 0.79888582 0.74294281 0.81505489 0.90156984 0.86518192][-0.090714931 -0.036962509 0.030843735 0.062253 0.020048618 0.010504723 0.16397858 0.32554293 0.39773321 0.29813814 0.22081995 0.30604172 0.43960238 0.47093964 0.26981115][0.26204205 0.27821493 0.26067114 0.19858265 0.13445807 0.13907433 0.20799494 0.2323904 0.16354561 -0.013586044 -0.089293 0.0055742264 0.11435223 0.066716671 -0.20222569][0.57480145 0.54275656 0.42483187 0.27745295 0.22093868 0.26264906 0.2802248 0.22799969 0.084456444 -0.11963797 -0.18971682 -0.11122179 -0.020915985 -0.0805459 -0.2735796][0.47436953 0.40522003 0.21157217 0.016003609 -0.02460289 0.047867298 0.072641373 0.013307571 -0.14723063 -0.33239126 -0.37600565 -0.29539585 -0.1893692 -0.21725321 -0.32210493][-0.17775488 -0.25479889 -0.43580389 -0.5903151 -0.59415531 -0.51826906 -0.48409986 -0.54312587 -0.70519066 -0.86122394 -0.8859241 -0.78937674 -0.62239671 -0.58575296 -0.64238524][-0.80895138 -0.86462665 -0.98176932 -1.0536277 -1.0299995 -0.98438 -0.99299812 -1.0929446 -1.2660017 -1.4026942 -1.4279914 -1.3253453 -1.1383405 -1.0646896 -1.0945663]]...]
INFO - root - 2017-12-07 07:13:27.036642: step 16910, loss = 0.68, batch loss = 0.61 (7.9 examples/sec; 1.010 sec/batch; 88h:30m:33s remains)
INFO - root - 2017-12-07 07:13:37.361730: step 16920, loss = 0.91, batch loss = 0.84 (8.0 examples/sec; 1.004 sec/batch; 88h:00m:18s remains)
INFO - root - 2017-12-07 07:13:47.575104: step 16930, loss = 0.74, batch loss = 0.67 (7.6 examples/sec; 1.047 sec/batch; 91h:47m:08s remains)
INFO - root - 2017-12-07 07:13:57.823531: step 16940, loss = 0.74, batch loss = 0.67 (7.6 examples/sec; 1.059 sec/batch; 92h:47m:32s remains)
INFO - root - 2017-12-07 07:14:08.179704: step 16950, loss = 0.97, batch loss = 0.90 (7.6 examples/sec; 1.046 sec/batch; 91h:41m:20s remains)
INFO - root - 2017-12-07 07:14:18.392171: step 16960, loss = 0.78, batch loss = 0.71 (7.7 examples/sec; 1.035 sec/batch; 90h:41m:05s remains)
INFO - root - 2017-12-07 07:14:28.737273: step 16970, loss = 0.77, batch loss = 0.70 (7.9 examples/sec; 1.011 sec/batch; 88h:37m:00s remains)
INFO - root - 2017-12-07 07:14:39.120369: step 16980, loss = 0.81, batch loss = 0.74 (7.7 examples/sec; 1.040 sec/batch; 91h:10m:35s remains)
INFO - root - 2017-12-07 07:14:49.472531: step 16990, loss = 0.70, batch loss = 0.63 (7.5 examples/sec; 1.073 sec/batch; 94h:03m:11s remains)
INFO - root - 2017-12-07 07:14:59.895391: step 17000, loss = 0.65, batch loss = 0.57 (7.8 examples/sec; 1.029 sec/batch; 90h:12m:16s remains)
2017-12-07 07:15:00.677728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9180374 -1.9653387 -1.9924974 -2.0313411 -2.0070262 -1.9315398 -1.822279 -1.8662088 -2.1088791 -2.227078 -2.1301887 -1.9984148 -1.8396881 -1.6616116 -1.5360713][-2.0911479 -2.203337 -2.2859578 -2.2732093 -2.1499453 -1.9372087 -1.6843088 -1.6656063 -1.8082414 -1.7752397 -1.6945949 -1.7368841 -1.7834446 -1.7217951 -1.6139677][-2.3375812 -2.4832497 -2.5740004 -2.5187993 -2.3711565 -2.1648257 -1.9142599 -1.9146228 -1.9737659 -1.7872341 -1.6664846 -1.7319944 -1.7902923 -1.7414391 -1.6708834][-2.4480128 -2.6054549 -2.6665316 -2.5938411 -2.49883 -2.3882849 -2.2593184 -2.3912287 -2.4576387 -2.1764119 -2.0055046 -2.018405 -1.999568 -1.9146686 -1.8473947][-2.2213035 -2.3538997 -2.3967195 -2.3670805 -2.3514426 -2.3034408 -2.2864223 -2.5802755 -2.6991076 -2.3765545 -2.1871037 -2.1932929 -2.1721234 -2.0970871 -2.0238495][-1.7863858 -1.8451169 -1.8754914 -1.905277 -1.8945374 -1.796155 -1.844101 -2.2971506 -2.523437 -2.273077 -2.148082 -2.1952617 -2.2095525 -2.1305943 -2.0293586][-1.5856156 -1.5146241 -1.5075698 -1.5221374 -1.4113734 -1.1742477 -1.200846 -1.6760662 -1.9098275 -1.7423811 -1.7447326 -1.8788714 -1.9728322 -1.9263117 -1.8602765][-1.7882404 -1.5575442 -1.4875212 -1.437701 -1.2114484 -0.86324835 -0.83695316 -1.1729109 -1.2802455 -1.1541061 -1.2837765 -1.5024855 -1.6639612 -1.6883299 -1.7434375][-2.1424644 -1.865453 -1.759177 -1.620472 -1.3337176 -1.0163634 -1.0204318 -1.2394226 -1.2226446 -1.1069734 -1.2855389 -1.513763 -1.6644421 -1.7370729 -1.8792634][-2.4807143 -2.2414937 -2.1149471 -1.9181907 -1.6541882 -1.4664266 -1.5785706 -1.7744133 -1.7150764 -1.5997965 -1.7442744 -1.926033 -2.0345075 -2.1303596 -2.2739065][-2.7589025 -2.5174098 -2.3519633 -2.1808803 -2.0279329 -1.9916821 -2.1927652 -2.4085679 -2.3801532 -2.3094251 -2.4129481 -2.5161328 -2.560524 -2.6479645 -2.7299609][-3.0814872 -2.7752414 -2.5500717 -2.4401922 -2.4030452 -2.4670286 -2.6799893 -2.8726521 -2.8725803 -2.8573005 -2.9181242 -2.9146781 -2.8795207 -2.9319015 -2.9418998][-3.3092442 -2.9731183 -2.7360682 -2.6978045 -2.7399046 -2.8496771 -3.0189464 -3.1391554 -3.1264355 -3.1249962 -3.1329589 -3.0448437 -2.9674082 -2.9974346 -2.9655378][-3.2856066 -3.0213861 -2.8398266 -2.8302469 -2.871489 -2.9861035 -3.1020472 -3.1561344 -3.1368451 -3.1265073 -3.0867434 -2.9571767 -2.8534334 -2.8489003 -2.7989655][-3.1144066 -2.9302297 -2.7958362 -2.7567558 -2.7450006 -2.8377287 -2.9188952 -2.9428453 -2.9364552 -2.9224634 -2.8638179 -2.7252574 -2.6039453 -2.5587106 -2.5131979]]...]
INFO - root - 2017-12-07 07:15:11.076076: step 17010, loss = 0.78, batch loss = 0.71 (7.7 examples/sec; 1.033 sec/batch; 90h:29m:46s remains)
INFO - root - 2017-12-07 07:15:21.438297: step 17020, loss = 1.11, batch loss = 1.04 (7.9 examples/sec; 1.009 sec/batch; 88h:24m:53s remains)
INFO - root - 2017-12-07 07:15:31.755923: step 17030, loss = 0.87, batch loss = 0.79 (7.7 examples/sec; 1.039 sec/batch; 91h:05m:20s remains)
INFO - root - 2017-12-07 07:15:42.203987: step 17040, loss = 0.82, batch loss = 0.75 (7.6 examples/sec; 1.059 sec/batch; 92h:45m:31s remains)
INFO - root - 2017-12-07 07:15:52.424491: step 17050, loss = 0.96, batch loss = 0.89 (8.1 examples/sec; 0.989 sec/batch; 86h:37m:36s remains)
INFO - root - 2017-12-07 07:16:02.792317: step 17060, loss = 0.65, batch loss = 0.58 (7.7 examples/sec; 1.036 sec/batch; 90h:45m:47s remains)
INFO - root - 2017-12-07 07:16:13.185767: step 17070, loss = 0.81, batch loss = 0.73 (7.7 examples/sec; 1.040 sec/batch; 91h:09m:33s remains)
INFO - root - 2017-12-07 07:16:23.636768: step 17080, loss = 0.89, batch loss = 0.82 (7.5 examples/sec; 1.067 sec/batch; 93h:31m:01s remains)
INFO - root - 2017-12-07 07:16:34.081678: step 17090, loss = 0.97, batch loss = 0.89 (7.8 examples/sec; 1.029 sec/batch; 90h:09m:28s remains)
INFO - root - 2017-12-07 07:16:44.715668: step 17100, loss = 0.76, batch loss = 0.69 (7.7 examples/sec; 1.040 sec/batch; 91h:04m:59s remains)
2017-12-07 07:16:45.519590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8450706 -3.8790412 -3.7099836 -3.5693841 -3.5420303 -3.6308076 -3.7918978 -3.8849192 -3.843452 -3.6673152 -3.4703388 -3.3441997 -3.258925 -3.199738 -3.1350279][-3.7246764 -3.6054428 -3.1906013 -2.8686852 -2.743994 -2.8779826 -3.1813126 -3.4592593 -3.6153677 -3.5914397 -3.5093164 -3.4066525 -3.2270894 -3.0360796 -2.8610754][-3.5782592 -3.3117385 -2.6646485 -2.1378539 -1.8676028 -1.9849854 -2.4076557 -2.8930593 -3.256218 -3.4116504 -3.4948587 -3.4791694 -3.2486143 -2.9368079 -2.644474][-3.6192052 -3.3827977 -2.6950271 -1.9731178 -1.3884933 -1.2015247 -1.5110536 -2.0984251 -2.6560159 -3.0296459 -3.3562398 -3.4955099 -3.2877288 -2.8832581 -2.4480827][-3.7905424 -3.6510034 -3.0394487 -2.2030511 -1.324687 -0.676852 -0.53579855 -0.92796564 -1.4907236 -2.0850735 -2.7803292 -3.1715913 -3.0508468 -2.6112885 -2.185061][-3.7831392 -3.8581297 -3.3592792 -2.51321 -1.5378976 -0.61762285 -0.029887676 -0.14533329 -0.639698 -1.3914175 -2.3569131 -2.9504373 -2.9892735 -2.7143025 -2.566195][-3.3662748 -3.6540167 -3.2990489 -2.5277781 -1.5301816 -0.48745131 0.27470398 0.11103868 -0.59403086 -1.4607751 -2.3165002 -2.707881 -2.7113175 -2.6706042 -2.9716556][-3.0513763 -3.3872032 -3.1603265 -2.5323687 -1.610635 -0.52309394 0.25934935 -0.095057011 -1.1008105 -1.9955032 -2.5036066 -2.3923168 -2.0646858 -2.0764987 -2.6376204][-2.9463589 -3.2843997 -3.2539041 -2.9224505 -2.2779155 -1.3356383 -0.63022995 -1.0171797 -1.9994435 -2.6832738 -2.7202368 -2.11691 -1.5091414 -1.5748444 -2.2504706][-2.9674535 -3.1899881 -3.3600452 -3.4253764 -3.1738186 -2.4985409 -1.9690578 -2.2420671 -2.9283402 -3.2248254 -2.8238363 -1.9729788 -1.3405387 -1.4914939 -2.1046052][-3.079833 -3.0176492 -3.2144113 -3.5396411 -3.6202631 -3.2656238 -2.9615011 -3.1774311 -3.6022832 -3.6328776 -3.0653346 -2.234148 -1.7456374 -1.9363518 -2.325995][-2.9834194 -2.7315974 -2.8818645 -3.32406 -3.6314418 -3.5679531 -3.4189239 -3.577929 -3.8331294 -3.7806134 -3.343431 -2.7929587 -2.5653129 -2.7530289 -2.8444784][-2.7545836 -2.5302296 -2.68508 -3.1335196 -3.5705113 -3.7162871 -3.6826093 -3.7970316 -3.9054363 -3.7784727 -3.4966874 -3.2592597 -3.3368979 -3.5914006 -3.4942815][-2.9727955 -2.814569 -2.9212084 -3.2833595 -3.7352562 -3.9760833 -4.0058026 -4.0998483 -4.1122904 -3.946254 -3.7366235 -3.6743443 -3.8825011 -4.1029916 -3.9338224][-3.4699388 -3.386622 -3.4022856 -3.6032567 -3.9669454 -4.2207637 -4.2837071 -4.392756 -4.4098363 -4.2795877 -4.1381321 -4.1113253 -4.2254138 -4.2941031 -4.0907173]]...]
INFO - root - 2017-12-07 07:16:56.044077: step 17110, loss = 0.70, batch loss = 0.63 (7.4 examples/sec; 1.080 sec/batch; 94h:36m:46s remains)
INFO - root - 2017-12-07 07:17:06.475519: step 17120, loss = 0.85, batch loss = 0.78 (7.5 examples/sec; 1.063 sec/batch; 93h:09m:48s remains)
INFO - root - 2017-12-07 07:17:16.637617: step 17130, loss = 0.72, batch loss = 0.64 (7.6 examples/sec; 1.053 sec/batch; 92h:16m:53s remains)
INFO - root - 2017-12-07 07:17:27.040224: step 17140, loss = 0.96, batch loss = 0.89 (7.9 examples/sec; 1.018 sec/batch; 89h:10m:55s remains)
INFO - root - 2017-12-07 07:17:37.472074: step 17150, loss = 0.86, batch loss = 0.78 (7.8 examples/sec; 1.029 sec/batch; 90h:09m:27s remains)
INFO - root - 2017-12-07 07:17:47.818658: step 17160, loss = 0.83, batch loss = 0.75 (7.7 examples/sec; 1.043 sec/batch; 91h:19m:21s remains)
INFO - root - 2017-12-07 07:17:58.258886: step 17170, loss = 1.01, batch loss = 0.94 (8.2 examples/sec; 0.979 sec/batch; 85h:45m:39s remains)
INFO - root - 2017-12-07 07:18:08.649353: step 17180, loss = 0.77, batch loss = 0.70 (7.7 examples/sec; 1.037 sec/batch; 90h:51m:20s remains)
INFO - root - 2017-12-07 07:18:19.044813: step 17190, loss = 0.78, batch loss = 0.71 (7.9 examples/sec; 1.017 sec/batch; 89h:04m:20s remains)
INFO - root - 2017-12-07 07:18:29.427908: step 17200, loss = 0.87, batch loss = 0.80 (7.7 examples/sec; 1.038 sec/batch; 90h:57m:16s remains)
2017-12-07 07:18:30.217142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2430074 -3.1183982 -3.0662041 -3.0943913 -3.1120949 -3.076246 -3.0174456 -2.9764459 -2.9901235 -3.0541396 -3.1327119 -3.2043185 -3.2200718 -3.2271042 -3.2852323][-3.5134087 -3.3801503 -3.2539182 -3.2303443 -3.1982002 -3.0722523 -2.9399786 -2.8874865 -2.8992193 -2.9364605 -2.9822445 -3.0453124 -3.0591536 -3.0624554 -3.1568623][-3.6416445 -3.4521136 -3.1869133 -3.0270476 -2.8876734 -2.6663494 -2.5242898 -2.5588076 -2.6809006 -2.7919335 -2.8883996 -3.0005662 -3.0835931 -3.1606903 -3.2935295][-3.3174005 -3.0002065 -2.5934992 -2.3348787 -2.1659925 -1.9747961 -1.9198725 -2.0846567 -2.3504441 -2.5916886 -2.8145268 -3.0260804 -3.2036576 -3.3752069 -3.5262253][-2.5045705 -2.0716822 -1.5467756 -1.1887136 -1.0173903 -0.96463847 -1.0991311 -1.4104142 -1.8104713 -2.2052395 -2.6102052 -2.9940395 -3.2917128 -3.5093946 -3.6246567][-1.7007399 -1.3827484 -0.92569351 -0.53275585 -0.3042717 -0.28999662 -0.47270131 -0.77881908 -1.177984 -1.5990832 -2.0515232 -2.5294995 -2.9343631 -3.1636481 -3.2129626][-1.2523959 -1.1850691 -1.0085282 -0.77700949 -0.548733 -0.4648447 -0.51432252 -0.64915419 -0.89541674 -1.1614871 -1.4099998 -1.7257428 -2.0867898 -2.3012974 -2.33251][-1.0690384 -1.1334205 -1.2388504 -1.2812221 -1.1400414 -0.95382714 -0.789129 -0.69445324 -0.75980091 -0.90505338 -0.9479835 -1.0128574 -1.2120309 -1.3752811 -1.478698][-1.6895287 -1.7146823 -1.9274809 -2.1646287 -2.1846578 -2.0327795 -1.8058889 -1.5485415 -1.3928955 -1.3694654 -1.2528389 -1.1157527 -1.0946927 -1.1234012 -1.2280154][-2.8891163 -2.7958198 -2.9368424 -3.1718318 -3.2843504 -3.2756131 -3.200701 -3.0151639 -2.7575371 -2.5820112 -2.3841178 -2.1790607 -2.0178807 -1.891499 -1.8737044][-3.7972097 -3.6511688 -3.6606607 -3.7399468 -3.7777414 -3.7966576 -3.8364549 -3.8073413 -3.6429255 -3.5030415 -3.4304452 -3.3396051 -3.1895065 -3.0249443 -2.940268][-4.3283072 -4.2351084 -4.1353025 -4.0009542 -3.8807809 -3.8226676 -3.8562703 -3.9156041 -3.9047785 -3.9075761 -4.00318 -4.0549893 -3.9530406 -3.8253605 -3.7670429][-4.2456388 -4.2590003 -4.1283522 -3.898474 -3.71063 -3.6274405 -3.6449718 -3.7365561 -3.8420327 -3.9451723 -4.1173668 -4.2197151 -4.1479321 -4.0645866 -4.0475492][-3.6001282 -3.7620566 -3.7397525 -3.6087561 -3.4712403 -3.3982327 -3.3938084 -3.4633155 -3.5803785 -3.6900268 -3.8350923 -3.9202158 -3.8609638 -3.8151259 -3.8306682][-3.0180411 -3.2907796 -3.4033508 -3.4168389 -3.3467193 -3.2759576 -3.2703962 -3.3385286 -3.4308863 -3.4900737 -3.5484076 -3.5653799 -3.4845243 -3.4514451 -3.4664283]]...]
INFO - root - 2017-12-07 07:18:40.675292: step 17210, loss = 0.56, batch loss = 0.49 (7.7 examples/sec; 1.035 sec/batch; 90h:40m:39s remains)
INFO - root - 2017-12-07 07:18:50.982762: step 17220, loss = 0.79, batch loss = 0.71 (7.8 examples/sec; 1.031 sec/batch; 90h:17m:59s remains)
INFO - root - 2017-12-07 07:19:01.263540: step 17230, loss = 0.86, batch loss = 0.79 (7.7 examples/sec; 1.034 sec/batch; 90h:33m:24s remains)
INFO - root - 2017-12-07 07:19:11.719006: step 17240, loss = 0.79, batch loss = 0.72 (7.8 examples/sec; 1.025 sec/batch; 89h:46m:00s remains)
INFO - root - 2017-12-07 07:19:22.152772: step 17250, loss = 0.98, batch loss = 0.91 (7.6 examples/sec; 1.054 sec/batch; 92h:15m:45s remains)
INFO - root - 2017-12-07 07:19:32.602828: step 17260, loss = 0.75, batch loss = 0.67 (7.6 examples/sec; 1.049 sec/batch; 91h:49m:45s remains)
INFO - root - 2017-12-07 07:19:43.021414: step 17270, loss = 0.89, batch loss = 0.82 (7.7 examples/sec; 1.040 sec/batch; 91h:03m:04s remains)
INFO - root - 2017-12-07 07:19:53.474970: step 17280, loss = 0.66, batch loss = 0.58 (7.7 examples/sec; 1.038 sec/batch; 90h:51m:08s remains)
INFO - root - 2017-12-07 07:20:04.131024: step 17290, loss = 1.09, batch loss = 1.02 (7.4 examples/sec; 1.086 sec/batch; 95h:04m:17s remains)
INFO - root - 2017-12-07 07:20:14.577233: step 17300, loss = 0.89, batch loss = 0.82 (7.8 examples/sec; 1.025 sec/batch; 89h:45m:06s remains)
2017-12-07 07:20:15.408314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2319975 -3.0111816 -2.7343135 -2.4883289 -2.5636876 -2.881319 -3.1581585 -3.2858324 -3.4294558 -3.6514406 -3.7639117 -3.7331889 -3.6218081 -3.4194448 -3.2242146][-3.3439984 -3.0728188 -2.7520571 -2.4207551 -2.3804989 -2.5970767 -2.8109331 -2.9472945 -3.134901 -3.366399 -3.4336927 -3.3682985 -3.2225842 -2.9515119 -2.6850624][-3.3555667 -2.9730515 -2.5807209 -2.2043118 -2.0924037 -2.2660198 -2.5016661 -2.70562 -2.9564435 -3.2048473 -3.2411308 -3.1357923 -2.9631808 -2.6843219 -2.4232225][-3.3070457 -2.816659 -2.3998423 -2.0623345 -1.9543982 -2.1068912 -2.3522968 -2.5524211 -2.7211418 -2.8272502 -2.7422309 -2.5876994 -2.4564023 -2.2550232 -2.0848639][-3.5037477 -3.0597112 -2.6904526 -2.3793151 -2.1691628 -2.0998542 -2.117888 -2.1608703 -2.2138152 -2.2315819 -2.1248918 -2.0159338 -1.9995537 -1.9047563 -1.8206518][-3.8502531 -3.4755652 -3.0957303 -2.7602053 -2.4467854 -2.1786304 -1.9650691 -1.8661542 -1.8553445 -1.8538671 -1.7973208 -1.788492 -1.893322 -1.8898513 -1.8687215][-3.7840898 -3.4282427 -3.101089 -2.9017658 -2.6921258 -2.4235308 -2.1201584 -1.9160433 -1.8237531 -1.7628806 -1.7078424 -1.7442555 -1.9192152 -2.0403328 -2.1702673][-3.4429786 -3.1262805 -2.9451451 -2.9789004 -2.9242163 -2.6778188 -2.3085775 -1.9777291 -1.7966957 -1.7347634 -1.7547865 -1.8540804 -2.0562439 -2.2841234 -2.5769417][-3.3580916 -3.0337734 -2.9093785 -3.0336938 -2.9730282 -2.6630368 -2.2287431 -1.8241355 -1.6327605 -1.6867182 -1.8754797 -2.1037731 -2.3796673 -2.7060952 -3.08115][-3.5215445 -3.0785756 -2.8542061 -2.9037366 -2.8020458 -2.4776731 -2.0584085 -1.6663821 -1.5317783 -1.7263565 -2.068459 -2.3815119 -2.6781402 -3.0344627 -3.4137392][-3.7250135 -3.1818168 -2.8272521 -2.7553303 -2.6313972 -2.3798103 -2.0865808 -1.8136814 -1.7591379 -2.0097094 -2.3957014 -2.7132411 -2.9487638 -3.218205 -3.5088701][-3.933356 -3.3801246 -2.9468148 -2.7568069 -2.5254107 -2.2531385 -2.0523028 -1.9716177 -2.0145423 -2.2037189 -2.4691336 -2.6900148 -2.8254883 -2.9960852 -3.203732][-4.0535579 -3.5654163 -3.0945628 -2.8452451 -2.6275363 -2.3865318 -2.2642605 -2.2873333 -2.3088088 -2.300019 -2.3097215 -2.3352003 -2.3097055 -2.3555937 -2.4873161][-3.9294615 -3.5875854 -3.1274886 -2.8560162 -2.7427917 -2.6493125 -2.6484094 -2.7384329 -2.7235017 -2.5468521 -2.3714871 -2.2733414 -2.1593561 -2.1332824 -2.2039135][-3.3970819 -3.1687827 -2.7625275 -2.543581 -2.5706873 -2.6619058 -2.7960098 -2.9749603 -3.0332029 -2.8724933 -2.6951313 -2.5878797 -2.4645705 -2.4150524 -2.4490075]]...]
INFO - root - 2017-12-07 07:20:28.247420: step 17310, loss = 0.86, batch loss = 0.79 (6.2 examples/sec; 1.300 sec/batch; 113h:48m:53s remains)
INFO - root - 2017-12-07 07:20:41.651307: step 17320, loss = 0.72, batch loss = 0.65 (5.9 examples/sec; 1.347 sec/batch; 117h:54m:30s remains)
INFO - root - 2017-12-07 07:20:54.739209: step 17330, loss = 0.77, batch loss = 0.70 (6.0 examples/sec; 1.330 sec/batch; 116h:24m:37s remains)
INFO - root - 2017-12-07 07:21:08.067001: step 17340, loss = 0.68, batch loss = 0.61 (6.0 examples/sec; 1.324 sec/batch; 115h:54m:30s remains)
INFO - root - 2017-12-07 07:21:21.038057: step 17350, loss = 0.78, batch loss = 0.70 (6.0 examples/sec; 1.340 sec/batch; 117h:20m:43s remains)
INFO - root - 2017-12-07 07:21:34.306811: step 17360, loss = 0.89, batch loss = 0.81 (5.8 examples/sec; 1.384 sec/batch; 121h:10m:50s remains)
INFO - root - 2017-12-07 07:21:47.443288: step 17370, loss = 0.67, batch loss = 0.60 (6.1 examples/sec; 1.310 sec/batch; 114h:41m:27s remains)
INFO - root - 2017-12-07 07:22:00.627783: step 17380, loss = 0.92, batch loss = 0.85 (6.1 examples/sec; 1.305 sec/batch; 114h:12m:24s remains)
INFO - root - 2017-12-07 07:22:13.716457: step 17390, loss = 0.79, batch loss = 0.72 (6.1 examples/sec; 1.313 sec/batch; 114h:57m:38s remains)
INFO - root - 2017-12-07 07:22:26.924584: step 17400, loss = 0.96, batch loss = 0.88 (6.2 examples/sec; 1.293 sec/batch; 113h:10m:08s remains)
2017-12-07 07:22:27.919252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9750419 -2.2875562 -2.357837 -2.5588105 -2.9369516 -3.0413284 -2.6621542 -2.4942727 -2.7932353 -3.3000298 -3.8483093 -4.0471778 -3.8890305 -3.8765173 -4.0899892][-1.5078468 -1.4393113 -1.2444239 -1.2999306 -1.6377456 -1.9155178 -1.9487801 -2.1601007 -2.6251178 -3.1608865 -3.5427933 -3.575973 -3.4148331 -3.5323541 -3.9028478][-2.4022141 -2.0020707 -1.607511 -1.4631128 -1.6896689 -2.1231849 -2.5815 -3.0670843 -3.5309956 -3.9617236 -4.0715709 -3.936157 -3.7896073 -3.9379203 -4.2438397][-2.6570709 -2.278424 -2.0384121 -1.9674115 -2.1282477 -2.569788 -3.2364502 -3.838423 -4.3191843 -4.7680845 -4.7973928 -4.6134281 -4.5357995 -4.7102361 -4.8196926][-2.4981513 -2.2238936 -2.2070742 -2.2122328 -2.1497102 -2.2993455 -2.7183089 -3.0798714 -3.4943907 -4.12236 -4.4570107 -4.3951211 -4.2564154 -4.344142 -4.4137349][-2.073771 -1.9066472 -2.1721723 -2.2928724 -1.9659514 -1.5825398 -1.2941141 -1.2021294 -1.8071356 -3.0170393 -3.9463382 -3.9776225 -3.5757658 -3.4306493 -3.5308816][-1.5038114 -1.4435966 -1.933989 -2.1557064 -1.6203668 -0.61854959 0.69920778 1.2414799 -0.048915386 -2.2019265 -3.6509933 -3.5462875 -2.7659359 -2.3564126 -2.5507102][-1.4678595 -1.4116182 -1.9213672 -2.0449824 -1.2620628 0.34551859 2.6101489 3.4698758 1.546443 -1.1773827 -2.7398357 -2.4185762 -1.4262705 -0.98672771 -1.4713202][-1.940244 -1.829246 -2.2362688 -2.2777658 -1.4352694 0.37285614 2.8257627 3.4727755 1.4223566 -1.0947716 -2.3816459 -1.9384639 -0.97927165 -0.49887347 -1.0362599][-2.1656103 -2.0507028 -2.3596861 -2.5044236 -2.0419614 -0.65513062 1.2229724 1.4425273 -0.18421412 -1.9791226 -2.8438179 -2.403162 -1.4960246 -0.84478331 -1.1154845][-2.0101514 -1.8679957 -2.0665445 -2.2906449 -2.2462914 -1.4302638 -0.21914387 -0.14830017 -1.0832052 -2.1690457 -2.7983103 -2.533021 -1.678077 -0.87485003 -0.8580637][-1.9943528 -1.7501855 -1.726146 -1.7615111 -1.8175902 -1.3840017 -0.71886945 -0.70578384 -1.0926626 -1.6961918 -2.2449415 -2.2547495 -1.6794496 -0.98878 -0.81182241][-1.8558757 -1.5983973 -1.4195313 -1.2315843 -1.2662249 -1.2275715 -1.1276209 -1.2208011 -1.3487732 -1.7385435 -2.2466252 -2.4047728 -2.1211243 -1.7547758 -1.608232][-1.5224323 -1.4696574 -1.3520322 -1.1252501 -1.1271083 -1.4197092 -1.8346913 -2.1147647 -2.130018 -2.3477752 -2.7912331 -3.0142503 -2.9332395 -2.8210669 -2.68064][-2.016242 -2.0613568 -1.9688308 -1.7512999 -1.6608343 -1.9548929 -2.5051982 -2.8006098 -2.6859293 -2.6916273 -3.0507305 -3.3312001 -3.4657457 -3.5506604 -3.3468828]]...]
INFO - root - 2017-12-07 07:22:41.184420: step 17410, loss = 0.74, batch loss = 0.66 (6.2 examples/sec; 1.299 sec/batch; 113h:41m:11s remains)
INFO - root - 2017-12-07 07:22:54.386180: step 17420, loss = 0.81, batch loss = 0.74 (6.0 examples/sec; 1.336 sec/batch; 116h:57m:08s remains)
INFO - root - 2017-12-07 07:23:07.543211: step 17430, loss = 0.88, batch loss = 0.81 (6.1 examples/sec; 1.308 sec/batch; 114h:27m:01s remains)
INFO - root - 2017-12-07 07:23:20.760336: step 17440, loss = 0.81, batch loss = 0.74 (6.2 examples/sec; 1.298 sec/batch; 113h:35m:17s remains)
INFO - root - 2017-12-07 07:23:34.184680: step 17450, loss = 0.73, batch loss = 0.66 (6.0 examples/sec; 1.338 sec/batch; 117h:06m:48s remains)
INFO - root - 2017-12-07 07:23:47.536484: step 17460, loss = 0.81, batch loss = 0.74 (6.1 examples/sec; 1.319 sec/batch; 115h:25m:39s remains)
INFO - root - 2017-12-07 07:24:00.882136: step 17470, loss = 0.76, batch loss = 0.69 (6.2 examples/sec; 1.296 sec/batch; 113h:26m:45s remains)
INFO - root - 2017-12-07 07:24:14.170138: step 17480, loss = 0.81, batch loss = 0.74 (6.2 examples/sec; 1.281 sec/batch; 112h:03m:31s remains)
INFO - root - 2017-12-07 07:24:27.425580: step 17490, loss = 0.82, batch loss = 0.75 (6.2 examples/sec; 1.298 sec/batch; 113h:32m:14s remains)
INFO - root - 2017-12-07 07:24:40.566879: step 17500, loss = 0.81, batch loss = 0.74 (6.3 examples/sec; 1.276 sec/batch; 111h:40m:50s remains)
2017-12-07 07:24:41.557080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4155676 -2.2849422 -2.2064836 -2.2408342 -2.3127415 -2.400979 -2.5522757 -2.7293537 -2.8581662 -2.9755569 -3.0632162 -3.0885048 -3.1076579 -3.1507754 -3.1709061][-1.9317701 -1.7113833 -1.5780065 -1.6523576 -1.7613716 -1.8349681 -1.9966214 -2.2002058 -2.358048 -2.5231175 -2.6734433 -2.7584097 -2.8393037 -2.932714 -2.9703431][-1.6923273 -1.3928902 -1.2116876 -1.3417556 -1.5313752 -1.634783 -1.7863979 -1.9150536 -1.9962542 -2.1087134 -2.2104354 -2.2928538 -2.447202 -2.6215804 -2.6914415][-1.3984523 -1.0862288 -0.92290711 -1.1183035 -1.3789699 -1.4974618 -1.6207798 -1.6739779 -1.7295997 -1.8480794 -1.9009645 -1.9607024 -2.1909151 -2.4419117 -2.50144][-0.84240913 -0.70733619 -0.70479965 -1.0066643 -1.2635117 -1.2775686 -1.2267025 -1.1538854 -1.230377 -1.432883 -1.4822619 -1.561749 -1.8853905 -2.1552732 -2.100673][-0.43579125 -0.58289742 -0.77596259 -1.1470699 -1.2939441 -1.0220895 -0.59224868 -0.31396389 -0.44054317 -0.77944779 -0.88321829 -0.99890447 -1.3789468 -1.57915 -1.3188086][-0.55696416 -0.86255336 -1.0954852 -1.4137874 -1.4072974 -0.88039732 -0.12223053 0.29132891 0.053449154 -0.38681173 -0.4646554 -0.54354811 -0.92204571 -1.0642428 -0.69694328][-1.1767516 -1.4406698 -1.5580153 -1.734591 -1.647203 -1.1111794 -0.35051823 0.036299706 -0.26676989 -0.67107344 -0.609534 -0.57528162 -0.92996407 -1.1068189 -0.81981993][-1.9164956 -1.9800293 -1.9705338 -2.0737419 -2.0182478 -1.6499276 -1.1116321 -0.86456466 -1.15786 -1.4500091 -1.2776287 -1.1730363 -1.4775035 -1.6699417 -1.4781189][-2.417058 -2.3069577 -2.2565258 -2.3872726 -2.434392 -2.2650266 -1.9587898 -1.8397787 -2.0983641 -2.2897067 -2.0996687 -2.0104985 -2.2587538 -2.4204061 -2.2850475][-2.3661311 -2.2005582 -2.1823928 -2.3547473 -2.4817967 -2.4625344 -2.3391063 -2.331743 -2.5567575 -2.6726658 -2.5210419 -2.4652991 -2.6434958 -2.7573228 -2.670222][-2.0062253 -1.8440495 -1.8396506 -2.0029769 -2.1628451 -2.2509153 -2.2716126 -2.3447189 -2.5163934 -2.5722144 -2.4542165 -2.3975291 -2.4812725 -2.5537157 -2.5206409][-2.1200235 -1.9734776 -1.9680276 -2.0873775 -2.21619 -2.3200574 -2.4004915 -2.4951458 -2.6060271 -2.6322124 -2.562175 -2.5027654 -2.5087969 -2.5336957 -2.5224082][-2.837429 -2.7617402 -2.7697291 -2.8472085 -2.9182417 -2.9761457 -3.03742 -3.1023722 -3.1649871 -3.1846664 -3.1538277 -3.1136494 -3.0879068 -3.0823975 -3.0788574][-3.3757138 -3.3581903 -3.3741021 -3.4169807 -3.4438796 -3.4642463 -3.48715 -3.5111403 -3.5413902 -3.5536535 -3.5369442 -3.5081415 -3.4789469 -3.464416 -3.4564362]]...]
INFO - root - 2017-12-07 07:24:54.892080: step 17510, loss = 0.83, batch loss = 0.76 (6.1 examples/sec; 1.303 sec/batch; 114h:01m:44s remains)
INFO - root - 2017-12-07 07:25:08.322754: step 17520, loss = 0.70, batch loss = 0.63 (5.9 examples/sec; 1.349 sec/batch; 117h:59m:42s remains)
INFO - root - 2017-12-07 07:25:21.527080: step 17530, loss = 0.84, batch loss = 0.77 (5.8 examples/sec; 1.375 sec/batch; 120h:18m:23s remains)
INFO - root - 2017-12-07 07:25:34.875728: step 17540, loss = 0.71, batch loss = 0.64 (6.1 examples/sec; 1.320 sec/batch; 115h:29m:10s remains)
INFO - root - 2017-12-07 07:25:48.199193: step 17550, loss = 0.68, batch loss = 0.61 (5.9 examples/sec; 1.363 sec/batch; 119h:12m:02s remains)
INFO - root - 2017-12-07 07:26:01.357974: step 17560, loss = 0.89, batch loss = 0.82 (6.1 examples/sec; 1.322 sec/batch; 115h:38m:47s remains)
INFO - root - 2017-12-07 07:26:14.434657: step 17570, loss = 1.05, batch loss = 0.98 (6.1 examples/sec; 1.302 sec/batch; 113h:54m:52s remains)
INFO - root - 2017-12-07 07:26:27.844590: step 17580, loss = 0.82, batch loss = 0.74 (6.1 examples/sec; 1.320 sec/batch; 115h:30m:02s remains)
INFO - root - 2017-12-07 07:26:41.233256: step 17590, loss = 0.73, batch loss = 0.65 (5.8 examples/sec; 1.388 sec/batch; 121h:23m:40s remains)
INFO - root - 2017-12-07 07:26:54.590530: step 17600, loss = 1.03, batch loss = 0.96 (6.1 examples/sec; 1.318 sec/batch; 115h:19m:30s remains)
2017-12-07 07:26:55.667679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1294532 -3.1535931 -3.1675034 -3.1399679 -3.1184862 -3.1190779 -3.1341119 -3.1665373 -3.1335146 -3.0266271 -2.9947183 -3.0086257 -3.0024552 -3.0256939 -3.0022149][-2.1138315 -2.1603367 -2.2317822 -2.3055811 -2.383198 -2.4617758 -2.5498977 -2.660531 -2.6944294 -2.619288 -2.536761 -2.4368606 -2.3171914 -2.2602086 -2.2266095][-2.1261737 -2.2167029 -2.3389821 -2.476757 -2.6096973 -2.7253306 -2.8353603 -2.9560485 -2.9864902 -2.9033213 -2.7928009 -2.6135929 -2.3518157 -2.1264422 -1.9981017][-2.4909039 -2.4855947 -2.501534 -2.5569944 -2.6237788 -2.6746268 -2.7248454 -2.8084393 -2.8512487 -2.8278236 -2.8434713 -2.7940207 -2.5738926 -2.293185 -2.1175766][-3.0687382 -3.1311626 -3.1885936 -3.2220743 -3.2259083 -3.1862674 -3.1283588 -3.1349201 -3.185101 -3.2554398 -3.4564738 -3.6280746 -3.5429831 -3.3184018 -3.2280874][-3.5924435 -3.7854323 -3.9596384 -4.038343 -3.9779215 -3.7993102 -3.5572743 -3.4387274 -3.5218685 -3.7263677 -4.0985932 -4.3952017 -4.323565 -4.0812483 -4.0349631][-3.2912982 -3.4144669 -3.5144536 -3.4783933 -3.1616011 -2.6543221 -2.1228442 -1.8799868 -2.0769475 -2.4974375 -3.0469189 -3.4656703 -3.491853 -3.3556166 -3.4540071][-2.754931 -2.7900629 -2.762202 -2.5751729 -2.0072908 -1.2125504 -0.48307562 -0.20961332 -0.55466413 -1.1332173 -1.7103283 -2.1085534 -2.1731389 -2.1349251 -2.4172144][-2.7187929 -2.8797936 -2.9681654 -2.8904073 -2.4068658 -1.7052507 -1.1350403 -0.9696908 -1.2525158 -1.6530676 -1.928448 -1.9995003 -1.8555124 -1.7083237 -1.9307115][-2.7407403 -3.0315838 -3.2394619 -3.2705598 -2.948684 -2.5019906 -2.199666 -2.1530623 -2.33525 -2.5449615 -2.6125226 -2.5077558 -2.2803392 -2.0726051 -2.149699][-2.9104333 -3.1860061 -3.3623111 -3.3782539 -3.1634445 -2.9416852 -2.8550687 -2.8991933 -3.0140989 -3.1046488 -3.1010911 -3.0133982 -2.9151895 -2.8533115 -2.9462533][-3.6989119 -3.8463233 -3.9046786 -3.8830976 -3.7951493 -3.7712097 -3.8492825 -3.9541945 -4.0406742 -4.0779104 -4.0466285 -4.0075827 -4.0012236 -4.0111003 -4.0670218][-4.6034431 -4.5584841 -4.4603057 -4.3624578 -4.300581 -4.3268147 -4.4489832 -4.5963721 -4.7435341 -4.8604345 -4.9172349 -4.9547477 -4.971796 -4.9629216 -4.9379597][-4.6749992 -4.574904 -4.4357834 -4.3227715 -4.2718439 -4.3032932 -4.4113526 -4.5602093 -4.7291222 -4.8825994 -4.9916887 -5.0688057 -5.1127291 -5.1197305 -5.0985327][-3.7372377 -3.7152026 -3.6799288 -3.6714106 -3.6964417 -3.7521257 -3.8273549 -3.9136128 -4.0055265 -4.0825219 -4.1286454 -4.1447759 -4.1402678 -4.1227012 -4.089304]]...]
INFO - root - 2017-12-07 07:27:09.041424: step 17610, loss = 1.01, batch loss = 0.94 (6.0 examples/sec; 1.322 sec/batch; 115h:40m:18s remains)
INFO - root - 2017-12-07 07:27:22.340914: step 17620, loss = 0.87, batch loss = 0.80 (6.0 examples/sec; 1.338 sec/batch; 117h:00m:27s remains)
INFO - root - 2017-12-07 07:27:35.502374: step 17630, loss = 0.81, batch loss = 0.74 (6.2 examples/sec; 1.286 sec/batch; 112h:28m:09s remains)
INFO - root - 2017-12-07 07:27:48.807668: step 17640, loss = 0.72, batch loss = 0.65 (5.9 examples/sec; 1.349 sec/batch; 117h:57m:38s remains)
INFO - root - 2017-12-07 07:28:02.220448: step 17650, loss = 0.77, batch loss = 0.69 (5.9 examples/sec; 1.350 sec/batch; 118h:02m:42s remains)
INFO - root - 2017-12-07 07:28:15.571034: step 17660, loss = 0.95, batch loss = 0.87 (6.1 examples/sec; 1.315 sec/batch; 114h:59m:16s remains)
INFO - root - 2017-12-07 07:28:28.830558: step 17670, loss = 0.75, batch loss = 0.68 (6.0 examples/sec; 1.324 sec/batch; 115h:48m:16s remains)
INFO - root - 2017-12-07 07:28:42.178352: step 17680, loss = 0.79, batch loss = 0.71 (6.0 examples/sec; 1.344 sec/batch; 117h:31m:34s remains)
INFO - root - 2017-12-07 07:28:55.501045: step 17690, loss = 0.89, batch loss = 0.82 (6.0 examples/sec; 1.327 sec/batch; 116h:01m:15s remains)
INFO - root - 2017-12-07 07:29:08.659043: step 17700, loss = 1.10, batch loss = 1.02 (6.1 examples/sec; 1.317 sec/batch; 115h:08m:19s remains)
2017-12-07 07:29:09.605779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0998755 -3.149852 -2.9484043 -2.7899594 -2.8226705 -2.9930885 -3.0098548 -2.7844181 -2.4779291 -2.1908958 -1.939759 -1.9409924 -2.3016756 -2.7103829 -2.5928][-2.3973591 -2.3657382 -2.1748054 -2.1764109 -2.3810453 -2.6005208 -2.4961085 -2.0769339 -1.59971 -1.2254734 -1.0037389 -1.1173861 -1.6428151 -2.285975 -2.3954234][-1.6848152 -1.5891061 -1.4215231 -1.5600348 -1.8756976 -2.0132513 -1.7194519 -1.187506 -0.65497875 -0.308568 -0.20003748 -0.49322057 -1.1964731 -1.9498472 -2.2181811][-1.2749517 -1.2234519 -1.144279 -1.370429 -1.6451256 -1.5812619 -1.0963988 -0.53257918 -0.051665306 0.20072556 0.0982852 -0.44269347 -1.2979758 -1.9709291 -2.198689][-1.1472564 -1.1718893 -1.2527795 -1.4296794 -1.4069958 -1.1268513 -0.70419455 -0.30316925 0.045015812 0.24124241 -0.014199257 -0.722383 -1.6159022 -2.2021725 -2.4095812][-1.0200648 -1.1094503 -1.3077903 -1.3780885 -1.0564992 -0.72560406 -0.60996008 -0.5334456 -0.3573761 -0.18316603 -0.43606019 -1.0564148 -1.7459383 -2.2205739 -2.4875796][-0.78612137 -0.943902 -1.1494756 -1.0770195 -0.63107252 -0.4829495 -0.731951 -0.84516525 -0.68981791 -0.55893946 -0.83532 -1.312053 -1.7654943 -2.1380932 -2.3946209][-0.63677907 -0.82904553 -0.97093034 -0.76553059 -0.33034897 -0.40295124 -0.77608228 -0.75554371 -0.43408227 -0.31718445 -0.57378364 -0.96528935 -1.4346616 -1.9242709 -2.2126055][-0.42718029 -0.56497121 -0.63378787 -0.47932744 -0.30282068 -0.54584789 -0.747087 -0.42249775 -0.045797348 -0.057614803 -0.26701593 -0.54269886 -0.97878075 -1.5340176 -1.8084211][-0.13339853 -0.21972036 -0.24659586 -0.20117569 -0.331419 -0.687593 -0.68594956 -0.22751474 0.016912937 -0.15103292 -0.305377 -0.4958303 -0.8137269 -1.1749582 -1.2418199][0.17056751 0.059601307 0.0020642281 0.0019402504 -0.26651573 -0.58776784 -0.4842279 -0.16002369 -0.1325736 -0.32310104 -0.34073019 -0.443578 -0.627198 -0.7130878 -0.58470082][0.27152634 0.12128544 0.09994173 0.15205956 -0.15029526 -0.42533588 -0.35871077 -0.25797033 -0.31577158 -0.35699844 -0.24264479 -0.26168633 -0.29530191 -0.18352652 -0.030773163][0.13286638 -0.056752205 0.036574364 0.2291069 -0.0093541145 -0.30233812 -0.39360571 -0.43066955 -0.39569473 -0.21286678 -0.034899235 -0.0044169426 0.072571278 0.30206919 0.41277838][-0.062598228 -0.20087671 -0.00046110153 0.26433134 0.095729351 -0.18827915 -0.34205008 -0.36243296 -0.23496962 -0.0022125244 0.06261301 0.0099415779 0.056824684 0.26224995 0.34406137][-0.4351716 -0.37489843 -0.0409503 0.28800011 0.25138283 0.032073021 -0.065351963 0.025365353 0.19232035 0.2712841 0.010728836 -0.33462572 -0.46896887 -0.38141251 -0.27095079]]...]
INFO - root - 2017-12-07 07:29:22.797699: step 17710, loss = 0.73, batch loss = 0.65 (6.0 examples/sec; 1.328 sec/batch; 116h:07m:21s remains)
INFO - root - 2017-12-07 07:29:36.046344: step 17720, loss = 0.89, batch loss = 0.82 (5.9 examples/sec; 1.356 sec/batch; 118h:34m:59s remains)
INFO - root - 2017-12-07 07:29:49.249238: step 17730, loss = 0.85, batch loss = 0.77 (6.1 examples/sec; 1.319 sec/batch; 115h:22m:16s remains)
INFO - root - 2017-12-07 07:30:02.535376: step 17740, loss = 0.86, batch loss = 0.78 (5.9 examples/sec; 1.365 sec/batch; 119h:19m:29s remains)
INFO - root - 2017-12-07 07:30:15.713981: step 17750, loss = 0.91, batch loss = 0.84 (6.0 examples/sec; 1.333 sec/batch; 116h:33m:30s remains)
INFO - root - 2017-12-07 07:30:29.256961: step 17760, loss = 1.02, batch loss = 0.94 (5.9 examples/sec; 1.347 sec/batch; 117h:48m:01s remains)
INFO - root - 2017-12-07 07:30:42.542502: step 17770, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.344 sec/batch; 117h:31m:25s remains)
INFO - root - 2017-12-07 07:30:55.985757: step 17780, loss = 0.69, batch loss = 0.62 (5.8 examples/sec; 1.374 sec/batch; 120h:08m:00s remains)
INFO - root - 2017-12-07 07:31:09.180058: step 17790, loss = 0.92, batch loss = 0.85 (6.1 examples/sec; 1.312 sec/batch; 114h:39m:50s remains)
INFO - root - 2017-12-07 07:31:22.549129: step 17800, loss = 0.75, batch loss = 0.67 (6.1 examples/sec; 1.305 sec/batch; 114h:03m:44s remains)
2017-12-07 07:31:23.527071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1579356 -3.0710919 -2.9427691 -2.9180713 -2.9795911 -3.043601 -3.0183327 -2.9481149 -2.9771767 -3.0587966 -3.1021671 -3.0893292 -3.0183473 -2.95784 -2.9532313][-3.1363316 -2.956255 -2.7799191 -2.8069677 -2.9954181 -3.1875725 -3.2175822 -3.1510253 -3.1739626 -3.22584 -3.2242525 -3.1353598 -2.9724352 -2.8596632 -2.8566904][-3.01502 -2.7609255 -2.595715 -2.7288833 -3.0590644 -3.331089 -3.3324275 -3.1841908 -3.129745 -3.1284893 -3.1063905 -2.9758139 -2.773454 -2.6545143 -2.6699281][-2.8587148 -2.6237578 -2.5618937 -2.8087459 -3.1963515 -3.4090657 -3.250865 -2.9388905 -2.7935214 -2.7935495 -2.8197429 -2.7063446 -2.5280066 -2.4524755 -2.5025988][-2.7435656 -2.6169097 -2.7040272 -3.0031044 -3.3168845 -3.3428633 -2.9561071 -2.4582872 -2.265511 -2.367497 -2.5505269 -2.5370553 -2.4226718 -2.3647571 -2.3932364][-2.731235 -2.7595415 -2.9945655 -3.2904747 -3.4280009 -3.1891603 -2.5581279 -1.9264925 -1.7688336 -2.0546517 -2.4406524 -2.540642 -2.444581 -2.33485 -2.3099039][-2.8087111 -2.9739261 -3.2995226 -3.5131474 -3.3906045 -2.8546317 -2.0537987 -1.4240823 -1.3907738 -1.855257 -2.3894942 -2.5663421 -2.4523425 -2.2963886 -2.2723677][-2.7956681 -3.0452585 -3.3914423 -3.470015 -3.0950823 -2.3687518 -1.5898304 -1.1449687 -1.2967188 -1.8531547 -2.4071383 -2.5933981 -2.4446695 -2.2735174 -2.321337][-2.7079229 -3.0133522 -3.3558989 -3.327302 -2.8445492 -2.1600626 -1.639061 -1.5066395 -1.7567058 -2.2094691 -2.6119514 -2.6874566 -2.4583266 -2.2694108 -2.3916597][-2.7705598 -3.0671062 -3.3181581 -3.1743774 -2.676122 -2.1643629 -1.9592218 -2.0675721 -2.3004818 -2.5689526 -2.7701485 -2.7256832 -2.4560828 -2.2867565 -2.4659381][-3.0569329 -3.2886744 -3.3404167 -3.0335829 -2.5383458 -2.216109 -2.2574797 -2.4953277 -2.684145 -2.8139856 -2.8720763 -2.7517452 -2.4833477 -2.347075 -2.5470829][-3.3969338 -3.5613585 -3.3727241 -2.8957493 -2.4391096 -2.3220894 -2.5493565 -2.8352165 -2.9733729 -3.0047221 -2.9579802 -2.7869978 -2.5375233 -2.4087243 -2.56032][-3.5910256 -3.6729217 -3.2655911 -2.6432853 -2.2062719 -2.2257988 -2.5417089 -2.8260398 -2.9417028 -2.9404666 -2.8624637 -2.7079566 -2.5237083 -2.413985 -2.50777][-3.6501009 -3.6475356 -3.1128607 -2.4332573 -2.0232465 -2.0849831 -2.3774047 -2.6274083 -2.7539196 -2.7777653 -2.7207813 -2.6082163 -2.4897351 -2.402739 -2.4547858][-3.5898068 -3.5234096 -3.0032678 -2.4551172 -2.1896565 -2.2736669 -2.4622803 -2.6163392 -2.7228727 -2.7618084 -2.7251246 -2.63978 -2.5627379 -2.4960504 -2.5092354]]...]
INFO - root - 2017-12-07 07:31:36.698673: step 17810, loss = 0.88, batch loss = 0.81 (6.0 examples/sec; 1.334 sec/batch; 116h:35m:26s remains)
INFO - root - 2017-12-07 07:31:49.869490: step 17820, loss = 0.68, batch loss = 0.61 (5.8 examples/sec; 1.374 sec/batch; 120h:05m:25s remains)
INFO - root - 2017-12-07 07:32:03.018622: step 17830, loss = 0.75, batch loss = 0.68 (6.4 examples/sec; 1.253 sec/batch; 109h:29m:52s remains)
INFO - root - 2017-12-07 07:32:16.240416: step 17840, loss = 0.89, batch loss = 0.82 (6.0 examples/sec; 1.337 sec/batch; 116h:49m:06s remains)
INFO - root - 2017-12-07 07:32:29.327716: step 17850, loss = 0.68, batch loss = 0.60 (6.2 examples/sec; 1.284 sec/batch; 112h:14m:46s remains)
INFO - root - 2017-12-07 07:32:42.593867: step 17860, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.344 sec/batch; 117h:28m:57s remains)
INFO - root - 2017-12-07 07:32:55.936912: step 17870, loss = 0.81, batch loss = 0.73 (6.0 examples/sec; 1.325 sec/batch; 115h:47m:51s remains)
INFO - root - 2017-12-07 07:33:09.254840: step 17880, loss = 0.66, batch loss = 0.58 (6.2 examples/sec; 1.295 sec/batch; 113h:10m:35s remains)
INFO - root - 2017-12-07 07:33:22.456113: step 17890, loss = 0.79, batch loss = 0.72 (5.9 examples/sec; 1.345 sec/batch; 117h:30m:19s remains)
INFO - root - 2017-12-07 07:33:35.765278: step 17900, loss = 0.92, batch loss = 0.85 (6.0 examples/sec; 1.341 sec/batch; 117h:10m:48s remains)
2017-12-07 07:33:36.803481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0703301 -3.1352489 -3.2718306 -3.5697291 -3.8774085 -3.7838564 -3.505362 -3.6557426 -3.6948688 -3.1752949 -2.7344208 -2.3202448 -1.7309833 -1.4991548 -1.8153651][-2.9314289 -3.0683413 -3.1937084 -3.6368828 -4.1281681 -4.0356588 -3.6184742 -3.5537968 -3.4140763 -2.8292222 -2.3994248 -2.1174939 -1.7954559 -1.8281713 -2.2692573][-2.6001306 -2.7942317 -2.8793836 -3.3087339 -3.8810749 -3.8451805 -3.4471033 -3.2478228 -2.9467635 -2.3802316 -2.08124 -2.0700793 -2.1144204 -2.3154397 -2.6855636][-2.4018486 -2.5744019 -2.5883367 -2.948144 -3.5795887 -3.6609766 -3.3535419 -3.0590065 -2.5840926 -2.0154815 -1.8599715 -2.14513 -2.5290904 -2.8180242 -3.0323956][-2.6062393 -2.7439876 -2.676744 -2.9527967 -3.5524371 -3.6942532 -3.3848445 -2.9283886 -2.2989085 -1.7166107 -1.6396499 -2.1094179 -2.7106934 -3.0342317 -3.1433232][-3.0922103 -3.2767413 -3.2505734 -3.3977671 -3.7261767 -3.6731515 -3.2198696 -2.6046059 -1.9582453 -1.4822221 -1.479881 -2.006779 -2.6085024 -2.8576632 -2.956511][-3.4784079 -3.646244 -3.6626055 -3.6902392 -3.7700086 -3.5211749 -2.8817492 -2.1318841 -1.5796278 -1.3064499 -1.3919246 -1.891407 -2.368763 -2.5373 -2.694092][-3.6125219 -3.6302359 -3.5529766 -3.4242249 -3.3194549 -2.9489 -2.2506683 -1.5014915 -1.0489025 -0.87293816 -0.95255613 -1.347513 -1.7340186 -1.9788494 -2.2742107][-3.4732375 -3.4302444 -3.2568839 -2.9698477 -2.7346859 -2.3359737 -1.7367852 -1.2008569 -0.86830449 -0.6073947 -0.47649646 -0.65671945 -1.0169864 -1.4604681 -1.9267943][-3.1851325 -3.2199936 -3.0531387 -2.6912405 -2.4390464 -2.1345322 -1.6747775 -1.3671794 -1.1522827 -0.79923487 -0.48723364 -0.50821853 -0.88056612 -1.459826 -1.9892309][-3.1809301 -3.2673411 -3.0961833 -2.6985641 -2.4921329 -2.2711384 -1.9281261 -1.78883 -1.6751516 -1.3226008 -0.99014235 -1.0020773 -1.419925 -2.0282221 -2.5213437][-3.572372 -3.6317818 -3.4371319 -3.0456102 -2.853631 -2.66514 -2.4129736 -2.3829298 -2.321671 -2.052165 -1.8269763 -1.8789368 -2.2687385 -2.8093905 -3.2257347][-3.7468483 -3.7856026 -3.6217442 -3.2519479 -3.0272911 -2.8668742 -2.7298977 -2.7768703 -2.7705922 -2.6287122 -2.5138793 -2.5497856 -2.8285797 -3.2267175 -3.535392][-3.6606808 -3.7004008 -3.602556 -3.3141947 -3.1093833 -3.0073521 -2.9438105 -2.9920344 -3.0264966 -3.0004487 -2.9462523 -2.9337015 -3.0771675 -3.3037324 -3.486413][-3.5693002 -3.5746632 -3.5099022 -3.3238974 -3.197783 -3.1597404 -3.1351061 -3.1600471 -3.2136569 -3.2485185 -3.2189004 -3.1793251 -3.2306256 -3.3303757 -3.4265356]]...]
INFO - root - 2017-12-07 07:33:49.916607: step 17910, loss = 0.87, batch loss = 0.80 (6.1 examples/sec; 1.315 sec/batch; 114h:53m:21s remains)
INFO - root - 2017-12-07 07:34:03.340805: step 17920, loss = 0.96, batch loss = 0.89 (6.0 examples/sec; 1.344 sec/batch; 117h:27m:58s remains)
INFO - root - 2017-12-07 07:34:16.307217: step 17930, loss = 0.82, batch loss = 0.75 (6.1 examples/sec; 1.312 sec/batch; 114h:37m:33s remains)
INFO - root - 2017-12-07 07:34:29.643011: step 17940, loss = 0.80, batch loss = 0.73 (6.2 examples/sec; 1.283 sec/batch; 112h:05m:49s remains)
INFO - root - 2017-12-07 07:34:42.968315: step 17950, loss = 0.77, batch loss = 0.70 (6.1 examples/sec; 1.317 sec/batch; 115h:03m:10s remains)
INFO - root - 2017-12-07 07:34:56.196849: step 17960, loss = 0.68, batch loss = 0.61 (6.1 examples/sec; 1.302 sec/batch; 113h:45m:48s remains)
INFO - root - 2017-12-07 07:35:09.488805: step 17970, loss = 0.79, batch loss = 0.71 (6.0 examples/sec; 1.336 sec/batch; 116h:44m:53s remains)
INFO - root - 2017-12-07 07:35:22.768076: step 17980, loss = 0.70, batch loss = 0.62 (5.9 examples/sec; 1.346 sec/batch; 117h:36m:00s remains)
INFO - root - 2017-12-07 07:35:35.945687: step 17990, loss = 0.95, batch loss = 0.88 (6.1 examples/sec; 1.306 sec/batch; 114h:06m:36s remains)
INFO - root - 2017-12-07 07:35:49.189515: step 18000, loss = 0.68, batch loss = 0.61 (6.0 examples/sec; 1.325 sec/batch; 115h:43m:17s remains)
2017-12-07 07:35:50.202270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6665592 -2.6611385 -2.5555553 -2.4879894 -2.6028643 -2.7689333 -2.8329787 -2.8817403 -2.9915795 -3.1042559 -2.9990764 -2.5909619 -2.1144495 -2.1759977 -2.8891521][-2.2564509 -2.1507628 -1.972558 -1.8976374 -2.0784469 -2.305676 -2.4137626 -2.4707479 -2.5607398 -2.6895213 -2.629354 -2.1427188 -1.4459777 -1.442585 -2.4128816][-2.19751 -1.9717791 -1.724237 -1.6134865 -1.7605443 -1.9212065 -1.9474862 -1.8556216 -1.8332245 -2.03793 -2.2109754 -1.7582457 -0.82318664 -0.6093967 -1.6620083][-2.1983311 -1.9796681 -1.8154252 -1.7661059 -1.8779714 -1.9292791 -1.734134 -1.2928829 -1.015085 -1.2595086 -1.7178648 -1.4340994 -0.42029381 -0.024638653 -1.0003245][-2.5053611 -2.2583685 -2.1692908 -2.1930223 -2.2234402 -2.082912 -1.5779669 -0.70448804 -0.081437588 -0.3790493 -1.1576257 -1.1632757 -0.23808622 0.20558548 -0.67839265][-3.2026649 -2.9070039 -2.7989268 -2.8006225 -2.6426561 -2.1438107 -1.1730905 0.20050764 1.1125116 0.62196922 -0.56880903 -0.95042467 -0.24420214 0.17019939 -0.61070633][-3.5719757 -3.2241604 -3.0282016 -2.9630628 -2.5754614 -1.6629207 -0.16035461 1.7351236 2.7555022 1.8795528 0.25054598 -0.51123476 -0.10505533 0.25947332 -0.40032053][-3.2356768 -2.8947496 -2.7178218 -2.6872706 -2.1678855 -0.93347192 0.99103689 3.1969824 4.0515137 2.7146006 0.823586 -0.14536333 -0.01047945 0.34746885 -0.11401463][-2.706748 -2.4305816 -2.4742215 -2.6883597 -2.2128508 -0.87003779 1.1349707 3.1988378 3.7520151 2.2848778 0.49331617 -0.43147659 -0.41190434 0.010466576 -0.15912485][-1.9607289 -1.8671458 -2.3060873 -2.92621 -2.6224029 -1.3402247 0.4924159 2.151165 2.5176787 1.3018913 -0.20594692 -1.039499 -1.091682 -0.65509629 -0.60039711][-1.0744472 -1.1798341 -2.0691533 -3.0948896 -3.022577 -1.8293719 -0.17198563 1.1472392 1.38445 0.40801859 -0.8855114 -1.6358464 -1.7590404 -1.4008138 -1.2427955][-0.66121769 -0.9033618 -2.0308206 -3.1818986 -3.152566 -1.9977837 -0.5385735 0.44842863 0.50058985 -0.35297441 -1.4901228 -2.1649554 -2.3445849 -2.1186619 -1.9557219][-0.72094965 -1.0641334 -2.110266 -2.9658475 -2.7057619 -1.6128581 -0.52647233 -0.03249836 -0.25107336 -0.96685123 -1.8956363 -2.4509037 -2.7352178 -2.7400248 -2.638129][-1.0748131 -1.459343 -2.2382166 -2.668463 -2.1750174 -1.2162597 -0.52313256 -0.45146585 -0.81523895 -1.3236389 -2.0529909 -2.5068889 -2.9173393 -3.1792877 -3.1446753][-1.7971034 -2.1062627 -2.6299567 -2.8224602 -2.3493621 -1.5520861 -0.99534392 -0.97840405 -1.2422869 -1.5719311 -2.2013454 -2.5914969 -3.0390129 -3.435739 -3.4289489]]...]
INFO - root - 2017-12-07 07:36:03.542457: step 18010, loss = 0.93, batch loss = 0.85 (6.1 examples/sec; 1.318 sec/batch; 115h:09m:07s remains)
INFO - root - 2017-12-07 07:36:16.925671: step 18020, loss = 0.86, batch loss = 0.79 (5.9 examples/sec; 1.350 sec/batch; 117h:55m:13s remains)
INFO - root - 2017-12-07 07:36:29.983324: step 18030, loss = 0.76, batch loss = 0.69 (6.3 examples/sec; 1.263 sec/batch; 110h:19m:48s remains)
INFO - root - 2017-12-07 07:36:43.338993: step 18040, loss = 1.05, batch loss = 0.98 (6.0 examples/sec; 1.330 sec/batch; 116h:10m:15s remains)
INFO - root - 2017-12-07 07:36:56.631760: step 18050, loss = 0.76, batch loss = 0.69 (6.1 examples/sec; 1.308 sec/batch; 114h:16m:15s remains)
INFO - root - 2017-12-07 07:37:09.781719: step 18060, loss = 0.91, batch loss = 0.84 (6.1 examples/sec; 1.312 sec/batch; 114h:37m:15s remains)
INFO - root - 2017-12-07 07:37:23.155341: step 18070, loss = 0.93, batch loss = 0.86 (6.1 examples/sec; 1.317 sec/batch; 115h:02m:18s remains)
INFO - root - 2017-12-07 07:37:36.387006: step 18080, loss = 0.82, batch loss = 0.74 (6.1 examples/sec; 1.316 sec/batch; 114h:54m:22s remains)
INFO - root - 2017-12-07 07:37:49.798367: step 18090, loss = 0.75, batch loss = 0.68 (6.1 examples/sec; 1.315 sec/batch; 114h:52m:39s remains)
INFO - root - 2017-12-07 07:38:03.236767: step 18100, loss = 0.84, batch loss = 0.77 (5.8 examples/sec; 1.373 sec/batch; 119h:56m:04s remains)
2017-12-07 07:38:04.279837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2108879 -3.2353749 -3.1946616 -3.0262134 -2.9452579 -3.0577807 -3.1510477 -3.1228976 -3.0736203 -3.0749965 -3.1480184 -3.2039104 -3.0661459 -2.9189961 -2.8866963][-2.6148093 -2.591785 -2.5585806 -2.375716 -2.2861736 -2.4086857 -2.4903836 -2.4503963 -2.3751493 -2.3698933 -2.4635987 -2.5344348 -2.3979006 -2.2558215 -2.2524276][-2.0981407 -2.0570002 -2.0790193 -1.8959587 -1.7483327 -1.82184 -1.8993344 -1.9090178 -1.8373103 -1.7704194 -1.8160429 -1.8895552 -1.830934 -1.7502086 -1.7648277][-1.5066705 -1.6185129 -1.8164937 -1.67043 -1.4363835 -1.4364946 -1.5407982 -1.6942811 -1.7016473 -1.5860169 -1.5367434 -1.5997007 -1.666889 -1.7209127 -1.7651546][-1.2266867 -1.5267382 -1.8773463 -1.7713797 -1.4312868 -1.3358254 -1.4950545 -1.8471911 -2.0073743 -1.8705471 -1.6848078 -1.6664884 -1.7839336 -1.9342487 -2.0189312][-1.5731714 -1.9448221 -2.2997806 -2.1989079 -1.7973142 -1.6396091 -1.8642805 -2.3408914 -2.6152325 -2.5086939 -2.249872 -2.1325295 -2.1201634 -2.1487834 -2.1001894][-1.6157157 -1.9494789 -2.2220721 -2.1251566 -1.6981652 -1.4013822 -1.5414283 -2.0155241 -2.4110894 -2.4398053 -2.215637 -2.0544953 -1.841449 -1.6357224 -1.3829842][-1.3135915 -1.6584122 -1.9083645 -1.8448064 -1.3606782 -0.79011679 -0.66385436 -0.99806428 -1.5077512 -1.7605081 -1.7241192 -1.6109471 -1.257278 -0.886729 -0.473711][-0.87633634 -1.3223393 -1.6595805 -1.7376194 -1.3189113 -0.68068218 -0.48493004 -0.83086848 -1.4932449 -1.9837027 -2.0960841 -1.9795032 -1.5775878 -1.1260636 -0.59599686][-0.71906877 -1.082557 -1.4327779 -1.6638196 -1.3760009 -0.79300547 -0.58179903 -0.89309144 -1.6062691 -2.2360935 -2.4397588 -2.3587644 -2.0502405 -1.6875818 -1.1651552][-1.7302935 -1.970161 -2.2870817 -2.6326785 -2.5339229 -2.0715492 -1.782989 -1.854274 -2.2794893 -2.7125793 -2.8175259 -2.6947222 -2.4842539 -2.2612734 -1.8708601][-2.2986963 -2.4096975 -2.6367064 -2.9674783 -2.95357 -2.6200504 -2.3723025 -2.3772025 -2.6237202 -2.8549151 -2.8588634 -2.720943 -2.5982413 -2.5218935 -2.2707376][-2.1921315 -2.2129095 -2.336118 -2.5519102 -2.4923587 -2.1774659 -1.9613733 -1.9682484 -2.1445994 -2.2893381 -2.3180056 -2.2999642 -2.2914076 -2.2820036 -2.0911126][-1.7377522 -1.7642517 -1.8677487 -2.0042078 -1.940954 -1.7137125 -1.6311228 -1.6761062 -1.7451267 -1.7598302 -1.7599709 -1.828927 -1.8840702 -1.8552651 -1.6540737][-1.4086008 -1.4035079 -1.3907101 -1.3738999 -1.2996926 -1.2299685 -1.3908935 -1.5856934 -1.6522706 -1.5969474 -1.5398171 -1.6059666 -1.6589019 -1.5577576 -1.2786024]]...]
INFO - root - 2017-12-07 07:38:17.612643: step 18110, loss = 0.68, batch loss = 0.61 (6.0 examples/sec; 1.344 sec/batch; 117h:20m:31s remains)
INFO - root - 2017-12-07 07:38:30.857702: step 18120, loss = 1.03, batch loss = 0.96 (6.0 examples/sec; 1.339 sec/batch; 116h:53m:47s remains)
INFO - root - 2017-12-07 07:38:44.106124: step 18130, loss = 0.69, batch loss = 0.61 (6.0 examples/sec; 1.339 sec/batch; 116h:56m:41s remains)
INFO - root - 2017-12-07 07:38:57.377967: step 18140, loss = 0.85, batch loss = 0.78 (6.0 examples/sec; 1.342 sec/batch; 117h:08m:58s remains)
INFO - root - 2017-12-07 07:39:10.628226: step 18150, loss = 0.69, batch loss = 0.61 (5.9 examples/sec; 1.346 sec/batch; 117h:29m:28s remains)
INFO - root - 2017-12-07 07:39:24.106888: step 18160, loss = 0.87, batch loss = 0.79 (5.8 examples/sec; 1.385 sec/batch; 120h:54m:59s remains)
INFO - root - 2017-12-07 07:39:37.492507: step 18170, loss = 0.79, batch loss = 0.72 (5.9 examples/sec; 1.365 sec/batch; 119h:13m:36s remains)
INFO - root - 2017-12-07 07:39:50.800890: step 18180, loss = 0.74, batch loss = 0.67 (6.0 examples/sec; 1.329 sec/batch; 116h:00m:24s remains)
INFO - root - 2017-12-07 07:40:04.012449: step 18190, loss = 0.77, batch loss = 0.70 (6.1 examples/sec; 1.303 sec/batch; 113h:46m:10s remains)
INFO - root - 2017-12-07 07:40:17.303340: step 18200, loss = 0.91, batch loss = 0.84 (6.1 examples/sec; 1.314 sec/batch; 114h:40m:58s remains)
2017-12-07 07:40:18.311935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6141646 -3.4858108 -3.5563383 -3.7818255 -4.0647192 -4.2676678 -4.3316994 -4.3130565 -4.1960621 -3.9104042 -3.6983452 -3.5655761 -3.3955123 -3.24268 -3.2609692][-3.555542 -3.3075213 -3.2928042 -3.4399559 -3.7117798 -4.0290308 -4.2414412 -4.36485 -4.2877283 -3.8674307 -3.5405507 -3.4373515 -3.307826 -3.1328583 -3.107903][-3.4102981 -2.9834518 -2.8333888 -2.838129 -3.0173473 -3.4088106 -3.7624454 -4.0649261 -4.1389647 -3.7176938 -3.3339093 -3.3234477 -3.341435 -3.2482827 -3.1856685][-3.1877441 -2.4641652 -2.1687427 -2.0875325 -2.1934752 -2.6210332 -3.0135055 -3.3662028 -3.5676405 -3.2302756 -2.8280978 -2.9342179 -3.1961966 -3.3282285 -3.3462222][-2.823441 -1.7435091 -1.3264472 -1.3750041 -1.6181316 -2.1336696 -2.4122851 -2.5111837 -2.6089764 -2.3429432 -2.0245686 -2.3403392 -2.9307027 -3.4083591 -3.6157281][-2.4810653 -1.1143425 -0.59332609 -0.9142065 -1.4724348 -2.110085 -2.2022018 -1.8126423 -1.5510905 -1.2177374 -0.99244905 -1.5572517 -2.5356302 -3.4168134 -3.9112809][-2.4106877 -0.83361387 -0.20010376 -0.75097966 -1.6482565 -2.3539846 -2.1903298 -1.2383528 -0.50882268 -0.08342886 0.007414341 -0.8099699 -2.1058664 -3.236939 -3.9374166][-2.5744815 -0.93005753 -0.20868397 -0.8715353 -2.0192313 -2.7268181 -2.2297158 -0.71358824 0.4978466 1.0194473 0.954514 -0.044458866 -1.5094128 -2.7772393 -3.6384625][-2.7662811 -1.2263188 -0.46681118 -1.0708292 -2.3130178 -3.0736277 -2.3780248 -0.46090245 1.1293964 1.7657018 1.5998664 0.5332284 -0.92319775 -2.1819913 -3.0994551][-2.9278708 -1.6016076 -0.80283 -1.1047585 -2.1974294 -3.0441318 -2.4388626 -0.48104215 1.303792 2.0076981 1.7256384 0.63773441 -0.62050462 -1.624495 -2.3994753][-3.0848818 -2.0312819 -1.2627597 -1.2466595 -2.0044167 -2.7880921 -2.36084 -0.667048 1.0248985 1.6785431 1.2473006 0.2692275 -0.61929822 -1.2622948 -1.7938147][-3.1964741 -2.3610678 -1.6517041 -1.4623435 -1.9450984 -2.6253152 -2.4433198 -1.1819489 0.26179028 0.87336922 0.46640015 -0.26358271 -0.75993156 -1.0938621 -1.3888872][-3.2401252 -2.5548944 -1.8972607 -1.599026 -1.8898518 -2.4706278 -2.5299006 -1.7428503 -0.66110539 -0.19890881 -0.57525468 -1.0649691 -1.1977415 -1.1884804 -1.2248406][-3.327971 -2.8197665 -2.3041387 -2.0296855 -2.2192008 -2.6440227 -2.7220819 -2.2431505 -1.5651801 -1.3929517 -1.8280838 -2.1623123 -1.9968109 -1.6428812 -1.4497411][-3.4802873 -3.10675 -2.76223 -2.589252 -2.760536 -3.056963 -3.0776639 -2.7952428 -2.4756007 -2.5500898 -2.9807422 -3.1718044 -2.8082404 -2.2095144 -1.8202059]]...]
INFO - root - 2017-12-07 07:40:31.370006: step 18210, loss = 0.77, batch loss = 0.70 (6.2 examples/sec; 1.292 sec/batch; 112h:49m:56s remains)
INFO - root - 2017-12-07 07:40:44.829943: step 18220, loss = 0.77, batch loss = 0.70 (5.9 examples/sec; 1.352 sec/batch; 118h:00m:15s remains)
INFO - root - 2017-12-07 07:40:58.063753: step 18230, loss = 0.69, batch loss = 0.62 (6.4 examples/sec; 1.245 sec/batch; 108h:38m:57s remains)
INFO - root - 2017-12-07 07:41:11.246451: step 18240, loss = 0.81, batch loss = 0.74 (6.3 examples/sec; 1.273 sec/batch; 111h:08m:06s remains)
INFO - root - 2017-12-07 07:41:24.428823: step 18250, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.344 sec/batch; 117h:19m:22s remains)
INFO - root - 2017-12-07 07:41:37.815223: step 18260, loss = 0.87, batch loss = 0.80 (6.1 examples/sec; 1.308 sec/batch; 114h:08m:41s remains)
INFO - root - 2017-12-07 07:41:51.183537: step 18270, loss = 0.81, batch loss = 0.74 (5.8 examples/sec; 1.370 sec/batch; 119h:32m:52s remains)
INFO - root - 2017-12-07 07:42:04.703363: step 18280, loss = 0.67, batch loss = 0.59 (5.8 examples/sec; 1.371 sec/batch; 119h:38m:19s remains)
INFO - root - 2017-12-07 07:42:17.976185: step 18290, loss = 0.84, batch loss = 0.77 (6.0 examples/sec; 1.338 sec/batch; 116h:47m:10s remains)
INFO - root - 2017-12-07 07:42:31.358651: step 18300, loss = 1.03, batch loss = 0.96 (6.0 examples/sec; 1.334 sec/batch; 116h:26m:13s remains)
2017-12-07 07:42:32.352318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9508533 -3.9644794 -3.867002 -3.6694548 -3.4508662 -3.3515139 -3.4704022 -3.719295 -3.9521668 -4.1164665 -4.1657238 -4.13834 -4.0749273 -4.0035539 -3.928524][-3.8137155 -3.6967607 -3.389349 -2.9865751 -2.5856972 -2.3883069 -2.5219955 -2.8518009 -3.2811282 -3.7178364 -4.0087276 -4.1431365 -4.1466231 -4.0648441 -3.9598703][-3.6014891 -3.353292 -2.9274774 -2.4922411 -2.050956 -1.7624021 -1.7474184 -1.9278569 -2.3715892 -2.9600477 -3.4265769 -3.7853897 -4.0320764 -4.0937819 -4.0307884][-3.5001843 -3.1985955 -2.8407185 -2.5356064 -2.17928 -1.8507464 -1.6699421 -1.5829861 -1.7816346 -2.1664703 -2.4704533 -2.8595872 -3.3533974 -3.6755474 -3.7973702][-3.3552752 -2.9969311 -2.72683 -2.52397 -2.1703489 -1.7820568 -1.4899724 -1.2673278 -1.271112 -1.4668181 -1.6143286 -1.9399645 -2.5150347 -2.9950366 -3.3122697][-3.2657671 -2.8535404 -2.5778995 -2.3113215 -1.8340764 -1.254993 -0.72972345 -0.33572483 -0.27927208 -0.57549429 -0.8911705 -1.3118849 -1.9568956 -2.5299888 -2.9641142][-3.2692494 -2.8761876 -2.5394349 -2.1229842 -1.5220549 -0.731488 0.131114 0.8426199 1.0048633 0.54840708 -0.0099906921 -0.58969831 -1.361275 -2.0995767 -2.6763802][-3.3372045 -3.0395792 -2.6688702 -2.146261 -1.5545514 -0.77081537 0.20987368 1.0949636 1.3740792 0.98973608 0.48798418 -0.096274376 -0.9206984 -1.7788181 -2.45523][-3.6894329 -3.5537939 -3.2554855 -2.7970324 -2.3462167 -1.77403 -1.0603206 -0.44947195 -0.22851038 -0.31848669 -0.51153588 -0.89410734 -1.4870224 -2.1333816 -2.6092591][-4.2265944 -4.28412 -4.158165 -3.908246 -3.6463563 -3.2735922 -2.8307581 -2.5111856 -2.362551 -2.2805252 -2.2660949 -2.4109237 -2.6560969 -2.9153211 -3.0463762][-4.4344916 -4.6090727 -4.6271315 -4.5869436 -4.5060334 -4.2965765 -4.0356779 -3.8773036 -3.7653725 -3.6250877 -3.5391154 -3.5132942 -3.5088236 -3.5136805 -3.4293623][-4.2595673 -4.386054 -4.4033866 -4.3978372 -4.3626089 -4.2101312 -4.0239773 -3.9354987 -3.8712876 -3.7831206 -3.7397904 -3.693727 -3.6369789 -3.612246 -3.5305111][-4.1259356 -4.2351017 -4.2507839 -4.2306247 -4.1827221 -4.0623589 -3.9224494 -3.8537836 -3.8156106 -3.7758293 -3.7726974 -3.7263572 -3.6559217 -3.6271179 -3.5525951][-3.9536569 -4.112998 -4.2166576 -4.29289 -4.3434157 -4.339828 -4.2902856 -4.2365947 -4.1837544 -4.12228 -4.0783825 -3.9671013 -3.8167551 -3.7075999 -3.5807815][-3.5601537 -3.6241727 -3.6896691 -3.7632957 -3.8396235 -3.8895159 -3.9048898 -3.8941762 -3.8690963 -3.8375034 -3.809674 -3.7380304 -3.6341414 -3.546695 -3.4569664]]...]
INFO - root - 2017-12-07 07:42:45.619369: step 18310, loss = 0.77, batch loss = 0.70 (6.0 examples/sec; 1.341 sec/batch; 117h:02m:42s remains)
INFO - root - 2017-12-07 07:42:58.945549: step 18320, loss = 1.02, batch loss = 0.95 (5.9 examples/sec; 1.347 sec/batch; 117h:32m:39s remains)
INFO - root - 2017-12-07 07:43:12.070620: step 18330, loss = 0.71, batch loss = 0.64 (6.5 examples/sec; 1.231 sec/batch; 107h:23m:08s remains)
INFO - root - 2017-12-07 07:43:25.334279: step 18340, loss = 0.85, batch loss = 0.78 (6.1 examples/sec; 1.321 sec/batch; 115h:18m:29s remains)
INFO - root - 2017-12-07 07:43:38.689893: step 18350, loss = 0.83, batch loss = 0.75 (6.1 examples/sec; 1.308 sec/batch; 114h:08m:48s remains)
INFO - root - 2017-12-07 07:43:51.963562: step 18360, loss = 0.75, batch loss = 0.68 (6.0 examples/sec; 1.327 sec/batch; 115h:47m:25s remains)
INFO - root - 2017-12-07 07:44:05.267342: step 18370, loss = 0.90, batch loss = 0.83 (6.0 examples/sec; 1.336 sec/batch; 116h:33m:09s remains)
INFO - root - 2017-12-07 07:44:18.660456: step 18380, loss = 0.85, batch loss = 0.77 (6.0 examples/sec; 1.336 sec/batch; 116h:36m:41s remains)
INFO - root - 2017-12-07 07:44:32.080412: step 18390, loss = 0.90, batch loss = 0.83 (5.9 examples/sec; 1.356 sec/batch; 118h:16m:42s remains)
INFO - root - 2017-12-07 07:44:45.351901: step 18400, loss = 0.95, batch loss = 0.88 (6.0 examples/sec; 1.336 sec/batch; 116h:33m:37s remains)
2017-12-07 07:44:46.389029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8666716 -3.924974 -3.9768779 -4.0305605 -4.0628977 -4.0954132 -4.1155148 -4.1192269 -4.1141758 -4.1039391 -4.091969 -4.0299282 -3.9529595 -3.9022353 -3.8320625][-3.7282138 -3.7331591 -3.7555892 -3.8421309 -3.9384952 -4.0345769 -4.104414 -4.1336117 -4.1194477 -4.0706544 -4.0206423 -3.917532 -3.8013349 -3.7626693 -3.7331905][-3.2602389 -3.1415715 -3.0961671 -3.2372496 -3.4476969 -3.6342568 -3.7644577 -3.8322558 -3.8139353 -3.7109528 -3.6116037 -3.4959929 -3.3732615 -3.3904903 -3.4732089][-2.5777102 -2.4260404 -2.4306686 -2.7391257 -3.135639 -3.4308944 -3.6372092 -3.7690666 -3.7719326 -3.6210046 -3.461071 -3.3472433 -3.2077875 -3.2374773 -3.3861969][-2.3300683 -2.28303 -2.4075885 -2.822207 -3.253962 -3.4788711 -3.6005411 -3.651772 -3.6218903 -3.430387 -3.2595863 -3.2338076 -3.1618981 -3.2299533 -3.4198256][-2.4356441 -2.4373252 -2.5747664 -2.8801627 -3.1113324 -3.1175656 -3.0181577 -2.8601003 -2.7526665 -2.5733767 -2.4781613 -2.6645076 -2.8130717 -3.0249686 -3.3288808][-2.3761547 -2.2797213 -2.3101885 -2.3273957 -2.1675744 -1.8719139 -1.4993222 -1.146698 -1.1037722 -1.0662668 -1.1183898 -1.5588381 -2.0247991 -2.4705539 -2.9484715][-2.3238206 -2.0081496 -1.8040731 -1.4184697 -0.76081133 -0.10292578 0.52360058 0.92401171 0.74156618 0.55830574 0.35965633 -0.28171444 -1.0552008 -1.7660329 -2.4188652][-2.7395818 -2.2610755 -1.8888657 -1.4148719 -0.78215337 -0.28493452 0.16641188 0.31180048 -0.010018826 -0.17430592 -0.22413111 -0.6473527 -1.3147976 -1.9563205 -2.4793839][-3.3916965 -2.9857342 -2.637464 -2.3984218 -2.2424867 -2.2178831 -2.1401579 -2.226799 -2.5060763 -2.5180671 -2.3512423 -2.3961828 -2.6634636 -2.9434083 -3.09594][-3.9936173 -3.805232 -3.6468368 -3.646102 -3.7554677 -3.851985 -3.8233585 -3.8411198 -3.9179392 -3.7643242 -3.5285807 -3.4391503 -3.4898274 -3.5617859 -3.5137293][-4.5896721 -4.5643921 -4.5183139 -4.5737123 -4.662868 -4.681181 -4.6126537 -4.5479617 -4.419517 -4.1102662 -3.8125198 -3.6699228 -3.65705 -3.6399298 -3.5279779][-4.6471438 -4.6031666 -4.5021863 -4.4434109 -4.4070687 -4.3674889 -4.3436594 -4.3451056 -4.2555561 -4.0405993 -3.8506951 -3.7762203 -3.7643704 -3.6728992 -3.4937487][-4.407125 -4.4002233 -4.3479075 -4.2766585 -4.2047057 -4.1525717 -4.1513848 -4.1957884 -4.2027225 -4.1592045 -4.1287918 -4.1320262 -4.0873661 -3.8734186 -3.5891593][-3.9619083 -3.9607649 -3.9317665 -3.8614128 -3.7769291 -3.7224236 -3.7135983 -3.761714 -3.8397851 -3.9429839 -4.0412645 -4.1024909 -4.0551233 -3.8341339 -3.572485]]...]
INFO - root - 2017-12-07 07:44:59.467566: step 18410, loss = 0.63, batch loss = 0.55 (6.2 examples/sec; 1.291 sec/batch; 112h:37m:17s remains)
INFO - root - 2017-12-07 07:45:12.770014: step 18420, loss = 0.78, batch loss = 0.71 (5.9 examples/sec; 1.345 sec/batch; 117h:21m:23s remains)
INFO - root - 2017-12-07 07:45:26.019894: step 18430, loss = 0.87, batch loss = 0.79 (6.2 examples/sec; 1.290 sec/batch; 112h:32m:19s remains)
INFO - root - 2017-12-07 07:45:39.157629: step 18440, loss = 0.81, batch loss = 0.74 (6.0 examples/sec; 1.335 sec/batch; 116h:27m:16s remains)
INFO - root - 2017-12-07 07:45:52.516696: step 18450, loss = 0.77, batch loss = 0.70 (5.9 examples/sec; 1.352 sec/batch; 117h:58m:38s remains)
INFO - root - 2017-12-07 07:46:05.882548: step 18460, loss = 0.79, batch loss = 0.71 (6.2 examples/sec; 1.296 sec/batch; 113h:03m:34s remains)
INFO - root - 2017-12-07 07:46:19.112720: step 18470, loss = 0.88, batch loss = 0.81 (6.0 examples/sec; 1.330 sec/batch; 116h:00m:21s remains)
INFO - root - 2017-12-07 07:46:32.420279: step 18480, loss = 0.95, batch loss = 0.88 (6.1 examples/sec; 1.310 sec/batch; 114h:17m:14s remains)
INFO - root - 2017-12-07 07:46:45.750645: step 18490, loss = 0.88, batch loss = 0.81 (5.9 examples/sec; 1.354 sec/batch; 118h:04m:45s remains)
INFO - root - 2017-12-07 07:46:59.216796: step 18500, loss = 0.60, batch loss = 0.53 (6.1 examples/sec; 1.313 sec/batch; 114h:31m:49s remains)
2017-12-07 07:47:00.203017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7852874 -2.7955818 -2.8056378 -2.8176522 -2.8283887 -2.8322432 -2.8282537 -2.8207462 -2.8127642 -2.8066959 -2.8020864 -2.7973452 -2.8409894 -2.9667256 -3.1540742][-2.8088703 -2.8409944 -2.871537 -2.8989897 -2.917073 -2.9180238 -2.9028902 -2.880657 -2.8572526 -2.8366964 -2.8183756 -2.802393 -2.8376598 -2.9591489 -3.1454265][-2.8583131 -2.9195714 -2.9720705 -3.0120823 -3.0327814 -3.0262685 -2.9963617 -2.9566605 -2.9194059 -2.8859339 -2.8511238 -2.8168893 -2.8367696 -2.9487393 -3.1300216][-2.9153888 -3.005054 -3.0748911 -3.1223178 -3.139492 -3.1246989 -3.0835943 -3.0295267 -2.9836905 -2.9428668 -2.8905911 -2.8304641 -2.8278747 -2.9273996 -3.1023078][-2.9586756 -3.0748079 -3.1640577 -3.2236583 -3.2393606 -3.218082 -3.1648102 -3.0921421 -3.032433 -2.9859073 -2.9171991 -2.8295727 -2.8066926 -2.8989537 -3.071413][-2.9789381 -3.1218562 -3.2359657 -3.3093238 -3.3215389 -3.2863173 -3.2090998 -3.104018 -3.0235889 -2.9790936 -2.9071393 -2.8051274 -2.7744684 -2.8706717 -3.0469303][-2.9827886 -3.1477461 -3.284904 -3.36764 -3.3765433 -3.3278339 -3.2236695 -3.0752604 -2.9704819 -2.9334488 -2.8658617 -2.7582471 -2.7302575 -2.8424411 -3.0302229][-2.9806213 -3.1491456 -3.2916858 -3.3716617 -3.3796253 -3.3323762 -3.2160354 -3.0319505 -2.905534 -2.8759427 -2.8174143 -2.7103615 -2.692143 -2.8262749 -3.0310397][-2.9766064 -3.1373737 -3.2729445 -3.3420038 -3.3502097 -3.3185489 -3.212554 -3.0129471 -2.8666062 -2.8351121 -2.7853436 -2.6841524 -2.677731 -2.8334155 -3.0568409][-2.9336817 -3.0783715 -3.1999989 -3.2578349 -3.2747426 -3.2742274 -3.2074955 -3.0304725 -2.8766093 -2.8307991 -2.7778492 -2.6806087 -2.6829479 -2.8549457 -3.0931978][-2.8532038 -2.971612 -3.0737796 -3.1220646 -3.1504016 -3.1796374 -3.1586528 -3.0248816 -2.8797002 -2.8138943 -2.7513771 -2.664258 -2.6810255 -2.8723438 -3.1235757][-2.7863469 -2.86754 -2.9424436 -2.9786034 -3.009985 -3.0527027 -3.0608935 -2.968214 -2.8435354 -2.7643447 -2.697408 -2.6293144 -2.6723504 -2.8910065 -3.1587439][-2.7766953 -2.8146591 -2.8528094 -2.8680334 -2.8853273 -2.916954 -2.9275827 -2.8627043 -2.7628293 -2.6846559 -2.6272476 -2.5895565 -2.6652484 -2.9094357 -3.1886616][-2.8041036 -2.8047171 -2.8074534 -2.8003316 -2.7920961 -2.7918873 -2.7812891 -2.7251463 -2.6457367 -2.5810328 -2.5491884 -2.5535307 -2.6625314 -2.9206131 -3.2027793][-2.8423548 -2.8102765 -2.7793784 -2.7538466 -2.72465 -2.6918118 -2.6540542 -2.5983777 -2.5350952 -2.4866915 -2.4828329 -2.5344026 -2.6747975 -2.9356794 -3.2169688]]...]
INFO - root - 2017-12-07 07:47:13.384985: step 18510, loss = 0.74, batch loss = 0.67 (6.3 examples/sec; 1.277 sec/batch; 111h:22m:42s remains)
INFO - root - 2017-12-07 07:47:26.593863: step 18520, loss = 0.77, batch loss = 0.69 (6.1 examples/sec; 1.316 sec/batch; 114h:46m:34s remains)
INFO - root - 2017-12-07 07:47:39.897854: step 18530, loss = 0.78, batch loss = 0.71 (6.2 examples/sec; 1.293 sec/batch; 112h:46m:35s remains)
INFO - root - 2017-12-07 07:47:53.158852: step 18540, loss = 0.97, batch loss = 0.90 (6.2 examples/sec; 1.294 sec/batch; 112h:53m:05s remains)
INFO - root - 2017-12-07 07:48:06.304879: step 18550, loss = 0.67, batch loss = 0.60 (6.0 examples/sec; 1.338 sec/batch; 116h:42m:55s remains)
INFO - root - 2017-12-07 07:48:19.640799: step 18560, loss = 0.76, batch loss = 0.69 (6.1 examples/sec; 1.314 sec/batch; 114h:33m:35s remains)
INFO - root - 2017-12-07 07:48:33.001510: step 18570, loss = 0.79, batch loss = 0.72 (5.9 examples/sec; 1.351 sec/batch; 117h:49m:42s remains)
INFO - root - 2017-12-07 07:48:46.227676: step 18580, loss = 0.68, batch loss = 0.61 (5.9 examples/sec; 1.346 sec/batch; 117h:19m:44s remains)
INFO - root - 2017-12-07 07:48:59.513302: step 18590, loss = 0.80, batch loss = 0.73 (6.0 examples/sec; 1.326 sec/batch; 115h:37m:00s remains)
INFO - root - 2017-12-07 07:49:12.880268: step 18600, loss = 0.70, batch loss = 0.63 (6.0 examples/sec; 1.331 sec/batch; 116h:01m:07s remains)
2017-12-07 07:49:13.915649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5603313 -3.5648947 -3.38558 -3.0576987 -2.9771214 -3.2846458 -3.4757013 -3.4388053 -3.1789451 -2.3472114 -1.6840622 -1.3209679 -0.94343281 -0.97982049 -1.7731519][-3.7265234 -3.5284696 -3.2484164 -2.9372537 -2.9390426 -3.217083 -3.2983809 -3.2263417 -2.847991 -2.1276751 -1.9145393 -1.7671175 -1.3205633 -1.2217927 -1.767415][-3.6582093 -3.3964009 -3.0843291 -2.9151587 -3.0853209 -3.2406585 -3.0861921 -2.8789577 -2.473314 -2.0819733 -2.3625991 -2.3058345 -1.732585 -1.4506354 -1.6928158][-3.4268222 -3.3194094 -3.143 -3.3075461 -3.6881671 -3.54649 -2.8999705 -2.2963719 -1.8700736 -2.005801 -2.7722082 -2.6639688 -1.8589172 -1.3570311 -1.4065511][-3.1604171 -3.3441935 -3.4869585 -4.0521193 -4.4361925 -3.7929049 -2.6179652 -1.4949074 -0.97209978 -1.6766655 -2.8186588 -2.6450357 -1.707109 -1.0926971 -1.1197135][-3.1415372 -3.5735312 -3.9823031 -4.7515783 -4.7694607 -3.541069 -2.0049562 -0.46388578 0.15480137 -1.0851088 -2.5146518 -2.4347122 -1.6461618 -1.0041862 -0.96108723][-3.2109969 -3.752223 -4.243844 -5.0151749 -4.6029058 -3.0228548 -1.3247044 0.50593805 1.1428909 -0.61416125 -2.3442369 -2.566581 -2.0700536 -1.3163731 -1.051759][-3.26026 -3.7807243 -4.2275267 -4.9545455 -4.2842426 -2.6103249 -0.99502373 0.79263592 1.2081108 -0.70408177 -2.3850913 -2.878063 -2.7361238 -1.9523106 -1.5194204][-3.3878937 -3.7484937 -4.1029673 -4.7798948 -4.0500212 -2.491631 -1.1940968 0.16617441 0.16673422 -1.4775724 -2.6908643 -3.1185074 -3.0697746 -2.3458533 -2.0340712][-3.4441755 -3.520956 -3.8254144 -4.4526711 -3.8036501 -2.4943585 -1.5297248 -0.5855844 -0.92343712 -2.3074956 -3.0597007 -3.257875 -3.1069512 -2.4699917 -2.3329711][-3.492331 -3.2924223 -3.6588027 -4.2540655 -3.7480702 -2.6865025 -1.8388629 -1.1413352 -1.712045 -2.9619794 -3.3747039 -3.3946214 -3.2690563 -2.8208742 -2.7977424][-3.4934523 -3.1101217 -3.5441682 -4.0944991 -3.7819953 -2.9694896 -2.1289284 -1.5271413 -2.1041689 -3.1171827 -3.2946444 -3.3378568 -3.4728863 -3.3610983 -3.4657953][-3.2191448 -2.8551641 -3.2801824 -3.6959536 -3.5642967 -2.9843383 -2.2508993 -1.7386281 -2.141305 -2.7548695 -2.7363853 -2.8713226 -3.2850413 -3.4689617 -3.6850266][-2.9449761 -2.6748068 -2.9151177 -3.1191072 -3.1368394 -2.7527623 -2.2167356 -1.8310549 -2.0564954 -2.3398972 -2.2016311 -2.3456049 -2.8026495 -3.0629258 -3.3301318][-2.7977238 -2.6293302 -2.6575294 -2.6882148 -2.7799368 -2.5413055 -2.220525 -1.9945209 -2.0794303 -2.1565163 -1.9455285 -2.0768361 -2.4703498 -2.7142713 -3.0172114]]...]
INFO - root - 2017-12-07 07:49:27.202910: step 18610, loss = 0.90, batch loss = 0.83 (6.1 examples/sec; 1.313 sec/batch; 114h:28m:45s remains)
INFO - root - 2017-12-07 07:49:40.558820: step 18620, loss = 0.80, batch loss = 0.73 (6.2 examples/sec; 1.296 sec/batch; 113h:02m:13s remains)
INFO - root - 2017-12-07 07:49:54.031733: step 18630, loss = 0.83, batch loss = 0.76 (5.9 examples/sec; 1.352 sec/batch; 117h:50m:13s remains)
INFO - root - 2017-12-07 07:50:07.088353: step 18640, loss = 0.80, batch loss = 0.72 (6.0 examples/sec; 1.341 sec/batch; 116h:56m:33s remains)
INFO - root - 2017-12-07 07:50:20.370110: step 18650, loss = 0.93, batch loss = 0.86 (6.4 examples/sec; 1.241 sec/batch; 108h:09m:40s remains)
INFO - root - 2017-12-07 07:50:33.693801: step 18660, loss = 0.81, batch loss = 0.73 (6.0 examples/sec; 1.337 sec/batch; 116h:34m:36s remains)
INFO - root - 2017-12-07 07:50:47.108327: step 18670, loss = 0.90, batch loss = 0.82 (5.8 examples/sec; 1.389 sec/batch; 121h:05m:37s remains)
INFO - root - 2017-12-07 07:51:00.419971: step 18680, loss = 0.60, batch loss = 0.53 (6.0 examples/sec; 1.337 sec/batch; 116h:30m:49s remains)
INFO - root - 2017-12-07 07:51:13.790130: step 18690, loss = 0.65, batch loss = 0.58 (6.1 examples/sec; 1.319 sec/batch; 114h:56m:46s remains)
INFO - root - 2017-12-07 07:51:27.029201: step 18700, loss = 0.70, batch loss = 0.63 (5.9 examples/sec; 1.353 sec/batch; 117h:56m:09s remains)
2017-12-07 07:51:28.039704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4186108 -2.7997856 -3.2055216 -3.7911911 -3.9472992 -3.5735464 -3.1418085 -2.6230352 -2.2232141 -1.9326873 -1.731775 -1.8004162 -2.2919514 -3.1108994 -3.6004725][-2.1155579 -2.5499728 -3.020318 -3.5450363 -3.5808063 -3.142765 -2.8205791 -2.54218 -2.2938173 -2.1383104 -2.0337908 -1.980448 -2.1937783 -2.7216198 -3.0101037][-1.7618725 -2.2762258 -2.8104334 -3.2784095 -3.261281 -2.9367268 -2.7525816 -2.6112494 -2.4978871 -2.5538986 -2.6684403 -2.5842838 -2.4690108 -2.5531991 -2.5157263][-1.7867191 -2.3524582 -2.8545816 -3.2165523 -3.2058029 -3.0819905 -2.9142804 -2.6479983 -2.5533111 -2.8761053 -3.304419 -3.2557273 -2.8192832 -2.4327226 -2.1069763][-1.983366 -2.5653644 -3.02354 -3.304213 -3.40049 -3.4692135 -3.159946 -2.4867873 -2.2314637 -2.7534311 -3.48571 -3.5319068 -2.9088483 -2.1678205 -1.6760511][-2.0671256 -2.5946422 -3.0407324 -3.3133779 -3.498795 -3.6289904 -3.0463033 -1.8891253 -1.4668958 -2.2133245 -3.2844522 -3.5566053 -2.905791 -1.9296427 -1.2828045][-1.9833806 -2.4579926 -2.9581161 -3.2770245 -3.4282923 -3.3126307 -2.2397029 -0.615566 -0.14681482 -1.2088575 -2.660799 -3.255754 -2.7593093 -1.7217145 -0.91400886][-1.9337258 -2.4020586 -3.0276284 -3.5079994 -3.5503764 -2.9373674 -1.2600019 0.74237919 1.1842957 -0.17866421 -1.9941618 -2.960758 -2.7489223 -1.7556086 -0.77330971][-2.0485609 -2.5280485 -3.194438 -3.7363696 -3.5958402 -2.5796182 -0.62936974 1.394063 1.7668047 0.3113122 -1.6827474 -2.9569993 -3.0419874 -2.1339035 -1.0020654][-2.3818603 -2.7767003 -3.3354616 -3.7792473 -3.4645822 -2.346947 -0.60436225 1.0438299 1.3366337 0.06108284 -1.8080273 -3.186363 -3.4680152 -2.6995587 -1.5901768][-2.5838361 -2.6988702 -3.0870349 -3.5083878 -3.2882967 -2.4075718 -1.1410892 0.060622692 0.31492472 -0.62471461 -2.1355281 -3.3541141 -3.6590474 -3.101419 -2.2538574][-2.511775 -2.2449892 -2.4257977 -2.8869176 -2.9297986 -2.4739065 -1.7043843 -0.8820672 -0.65053916 -1.2548177 -2.3480432 -3.2121096 -3.3595624 -3.0017323 -2.5612297][-2.159658 -1.6176355 -1.6229069 -2.0883298 -2.3574777 -2.2765133 -1.8952477 -1.3853815 -1.2027516 -1.5591824 -2.2523685 -2.7394862 -2.7455468 -2.590405 -2.5268893][-1.8721175 -1.2728889 -1.1295569 -1.4552171 -1.7853885 -1.9379456 -1.865839 -1.6438937 -1.5771623 -1.7930467 -2.1801586 -2.4024076 -2.3476844 -2.3299015 -2.4735694][-2.0371892 -1.61222 -1.4029891 -1.4762654 -1.6541648 -1.8598804 -1.9727516 -1.9732575 -1.9941068 -2.0984769 -2.2798843 -2.3853006 -2.3696773 -2.4161222 -2.5800562]]...]
INFO - root - 2017-12-07 07:51:41.393992: step 18710, loss = 0.97, batch loss = 0.89 (6.1 examples/sec; 1.313 sec/batch; 114h:27m:39s remains)
INFO - root - 2017-12-07 07:51:54.849774: step 18720, loss = 0.58, batch loss = 0.51 (5.8 examples/sec; 1.375 sec/batch; 119h:50m:13s remains)
INFO - root - 2017-12-07 07:52:08.180347: step 18730, loss = 0.64, batch loss = 0.56 (6.0 examples/sec; 1.324 sec/batch; 115h:24m:35s remains)
INFO - root - 2017-12-07 07:52:21.319743: step 18740, loss = 0.69, batch loss = 0.62 (6.2 examples/sec; 1.297 sec/batch; 113h:02m:17s remains)
INFO - root - 2017-12-07 07:52:34.474256: step 18750, loss = 0.89, batch loss = 0.81 (6.1 examples/sec; 1.320 sec/batch; 115h:02m:55s remains)
INFO - root - 2017-12-07 07:52:47.695709: step 18760, loss = 0.71, batch loss = 0.64 (6.1 examples/sec; 1.302 sec/batch; 113h:29m:01s remains)
INFO - root - 2017-12-07 07:53:01.034110: step 18770, loss = 0.87, batch loss = 0.79 (6.0 examples/sec; 1.344 sec/batch; 117h:09m:00s remains)
INFO - root - 2017-12-07 07:53:14.400065: step 18780, loss = 1.01, batch loss = 0.94 (5.9 examples/sec; 1.364 sec/batch; 118h:53m:19s remains)
INFO - root - 2017-12-07 07:53:27.661731: step 18790, loss = 0.80, batch loss = 0.73 (6.2 examples/sec; 1.300 sec/batch; 113h:16m:55s remains)
INFO - root - 2017-12-07 07:53:40.984133: step 18800, loss = 0.78, batch loss = 0.71 (6.1 examples/sec; 1.314 sec/batch; 114h:30m:55s remains)
2017-12-07 07:53:41.959426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8296075 -3.0729356 -3.2112665 -3.26724 -3.2396293 -3.157794 -3.0109265 -2.8203511 -2.4774907 -2.057626 -1.7795441 -1.636091 -1.7174675 -1.9142883 -2.0317459][-2.5236661 -2.8154869 -2.9638505 -3.0240998 -2.9888597 -2.9142156 -2.7928581 -2.6344767 -2.3206973 -1.855428 -1.5626223 -1.4003358 -1.4109251 -1.5746584 -1.666434][-2.0386753 -2.4016192 -2.5852702 -2.6422958 -2.5725472 -2.4885783 -2.4103174 -2.316119 -2.1247129 -1.7556143 -1.5586214 -1.4346807 -1.3126135 -1.320014 -1.3299448][-1.4404774 -1.8196988 -2.0563285 -2.1392455 -2.0761971 -2.0038881 -1.939394 -1.8370504 -1.7135952 -1.471132 -1.4223142 -1.4352424 -1.2774909 -1.1969721 -1.1817162][-1.1083038 -1.3496628 -1.5357463 -1.6117868 -1.5524416 -1.4674108 -1.3625867 -1.2036748 -1.1652472 -1.1044314 -1.229049 -1.3953602 -1.280777 -1.1871691 -1.2034748][-1.1021101 -1.1375675 -1.2130437 -1.2257118 -1.074353 -0.82598805 -0.55354953 -0.32186174 -0.5182693 -0.83506417 -1.2219548 -1.5493264 -1.5392337 -1.4396582 -1.3983917][-1.0735695 -0.98167253 -1.002744 -0.95192742 -0.64147854 -0.15058708 0.33880568 0.64660549 0.15625095 -0.53989315 -1.1226907 -1.574651 -1.7438397 -1.7204857 -1.6211405][-0.89064622 -0.75328183 -0.74017882 -0.62608123 -0.23542881 0.31858683 0.82329369 1.1113825 0.46100187 -0.35231924 -0.92511463 -1.3767824 -1.6676364 -1.727056 -1.6264675][-0.85354996 -0.75378084 -0.66898465 -0.47079659 -0.11103344 0.24987936 0.47707462 0.57536507 0.12646866 -0.45318103 -0.837163 -1.1545489 -1.4154699 -1.4551718 -1.3539836][-1.2018101 -1.2067995 -1.0947547 -0.87065578 -0.5988493 -0.44654846 -0.4430275 -0.37413073 -0.46875644 -0.67059684 -0.80781841 -0.93666577 -1.0634968 -1.0326746 -0.94926691][-1.6244359 -1.7264667 -1.6626947 -1.5249259 -1.3831232 -1.3179758 -1.3105443 -1.0932734 -0.794822 -0.63155389 -0.55642343 -0.516953 -0.52427578 -0.50111651 -0.49338365][-1.8211932 -1.9642355 -1.9627986 -1.9482691 -1.9065666 -1.8441734 -1.723227 -1.3478525 -0.797801 -0.43283319 -0.26537848 -0.14853859 -0.11587143 -0.18828297 -0.29672718][-1.9891167 -2.205225 -2.2641878 -2.294903 -2.2455637 -2.1047871 -1.8836238 -1.4452796 -0.84341168 -0.47018766 -0.35640192 -0.30209017 -0.33634806 -0.50984669 -0.69076204][-2.0573184 -2.361455 -2.4778733 -2.5096531 -2.4207122 -2.2336273 -1.9855483 -1.6062908 -1.1300755 -0.89860964 -0.93847466 -1.0320666 -1.16153 -1.3706665 -1.5388][-1.9559476 -2.2515817 -2.3654025 -2.38709 -2.3375056 -2.2061915 -2.0506449 -1.8215594 -1.5382297 -1.4698219 -1.631392 -1.8319886 -2.0314271 -2.2366793 -2.3445511]]...]
INFO - root - 2017-12-07 07:53:55.125337: step 18810, loss = 0.87, batch loss = 0.80 (6.2 examples/sec; 1.294 sec/batch; 112h:44m:39s remains)
INFO - root - 2017-12-07 07:54:08.478696: step 18820, loss = 1.02, batch loss = 0.95 (6.1 examples/sec; 1.322 sec/batch; 115h:09m:27s remains)
INFO - root - 2017-12-07 07:54:21.882531: step 18830, loss = 0.78, batch loss = 0.71 (6.1 examples/sec; 1.320 sec/batch; 115h:02m:43s remains)
INFO - root - 2017-12-07 07:54:35.069506: step 18840, loss = 0.91, batch loss = 0.84 (6.0 examples/sec; 1.334 sec/batch; 116h:15m:46s remains)
INFO - root - 2017-12-07 07:54:48.341566: step 18850, loss = 0.76, batch loss = 0.69 (6.0 examples/sec; 1.326 sec/batch; 115h:30m:02s remains)
INFO - root - 2017-12-07 07:55:01.778782: step 18860, loss = 0.72, batch loss = 0.65 (6.0 examples/sec; 1.324 sec/batch; 115h:21m:33s remains)
INFO - root - 2017-12-07 07:55:14.955971: step 18870, loss = 0.92, batch loss = 0.85 (5.9 examples/sec; 1.351 sec/batch; 117h:39m:37s remains)
INFO - root - 2017-12-07 07:55:28.326713: step 18880, loss = 0.74, batch loss = 0.67 (5.8 examples/sec; 1.374 sec/batch; 119h:43m:54s remains)
INFO - root - 2017-12-07 07:55:41.702512: step 18890, loss = 0.66, batch loss = 0.59 (6.1 examples/sec; 1.303 sec/batch; 113h:31m:02s remains)
INFO - root - 2017-12-07 07:55:54.989476: step 18900, loss = 0.82, batch loss = 0.75 (6.0 examples/sec; 1.344 sec/batch; 117h:04m:27s remains)
2017-12-07 07:55:56.046543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1994343 -4.1257143 -4.0776763 -4.1530042 -4.1649919 -4.3173213 -4.401639 -3.9852662 -3.417624 -3.2967629 -3.3561912 -3.4930062 -3.8213198 -4.184516 -4.5155382][-3.9630938 -3.9660501 -4.0838356 -4.3265181 -4.4364443 -4.5141306 -4.3159251 -3.5217278 -2.9052403 -3.0069795 -3.2532053 -3.5082421 -3.9186521 -4.3104768 -4.6284618][-2.7465739 -2.8134604 -3.1908402 -3.7815723 -4.2321954 -4.3902903 -3.9745269 -2.8965962 -2.2951887 -2.6546748 -3.0635953 -3.4637098 -4.0568504 -4.5514436 -4.8275032][-1.1815319 -1.1719782 -1.6662064 -2.4879074 -3.186182 -3.4958661 -3.1682668 -2.2248845 -1.7828791 -2.1980383 -2.394376 -2.7754912 -3.6345534 -4.4739866 -5.011447][-0.47149539 -0.12586308 -0.53759336 -1.4402242 -2.1278689 -2.2081313 -1.6968369 -0.98418093 -1.0682445 -1.7724333 -1.8980711 -2.2398226 -3.1039014 -3.9685683 -4.6704087][-1.0877142 -0.29403543 -0.26340818 -0.78645396 -0.95170832 -0.38614845 0.61338854 1.0388145 0.059188366 -1.4002054 -2.0311668 -2.6545577 -3.4059527 -3.9004078 -4.2883663][-2.3718641 -1.5609796 -1.146661 -0.96551037 -0.10645247 1.4478874 3.0157776 3.0090852 1.0865984 -0.9426887 -2.0352373 -3.020618 -3.7698708 -4.0161104 -4.0456462][-3.1465387 -2.6815495 -2.4228683 -2.0246768 -0.64606214 1.3471322 2.9431643 2.5886202 0.56205368 -1.1328523 -1.9368429 -2.7649894 -3.4187047 -3.7340465 -3.7295041][-3.2073493 -2.9839706 -3.0482647 -2.8705902 -1.7425845 -0.21473122 0.73605061 0.1438427 -1.3011198 -2.0220673 -2.0621111 -2.378479 -2.8623343 -3.3576586 -3.4713564][-2.6434035 -2.691174 -3.0839338 -3.1760831 -2.4290316 -1.3949549 -0.95797491 -1.614706 -2.5792484 -2.6181779 -2.1520662 -2.1454268 -2.6458282 -3.3870492 -3.6287396][-1.9155135 -2.2194805 -2.93678 -3.2839198 -2.8345625 -2.0570521 -1.7858534 -2.2888041 -2.8906248 -2.6896205 -2.0793834 -1.9570031 -2.578505 -3.5655835 -3.9513414][-1.7243402 -2.1664743 -2.9347262 -3.2465119 -2.9029522 -2.4093444 -2.504669 -3.0345798 -3.3774095 -3.0062017 -2.2506318 -1.9310739 -2.5206842 -3.5763211 -4.0049057][-2.1318944 -2.6269169 -3.2883 -3.3884645 -2.9195366 -2.5603535 -2.9727197 -3.66499 -3.9531727 -3.5717161 -2.773169 -2.3550935 -2.8699949 -3.8210614 -4.1288042][-2.6953821 -3.0643475 -3.5051203 -3.4540944 -2.9584785 -2.715615 -3.1805034 -3.8312898 -4.0732665 -3.7839739 -3.1429827 -2.8451214 -3.3056469 -4.0431995 -4.1797576][-3.2255392 -3.3172629 -3.382885 -3.2312808 -2.9530332 -2.9525123 -3.3510487 -3.7374952 -3.8085964 -3.6278262 -3.2818983 -3.1706858 -3.4397371 -3.7422423 -3.6072035]]...]
INFO - root - 2017-12-07 07:56:09.471188: step 18910, loss = 0.87, batch loss = 0.80 (5.9 examples/sec; 1.365 sec/batch; 118h:55m:53s remains)
INFO - root - 2017-12-07 07:56:22.793285: step 18920, loss = 0.93, batch loss = 0.85 (6.1 examples/sec; 1.309 sec/batch; 114h:01m:06s remains)
INFO - root - 2017-12-07 07:56:36.016279: step 18930, loss = 0.88, batch loss = 0.80 (5.9 examples/sec; 1.354 sec/batch; 117h:56m:39s remains)
INFO - root - 2017-12-07 07:56:49.235747: step 18940, loss = 0.97, batch loss = 0.90 (6.0 examples/sec; 1.327 sec/batch; 115h:33m:09s remains)
INFO - root - 2017-12-07 07:57:02.505397: step 18950, loss = 0.55, batch loss = 0.48 (5.9 examples/sec; 1.366 sec/batch; 118h:57m:05s remains)
INFO - root - 2017-12-07 07:57:15.799614: step 18960, loss = 0.84, batch loss = 0.77 (6.0 examples/sec; 1.326 sec/batch; 115h:30m:51s remains)
INFO - root - 2017-12-07 07:57:29.136979: step 18970, loss = 0.73, batch loss = 0.66 (5.9 examples/sec; 1.348 sec/batch; 117h:22m:07s remains)
INFO - root - 2017-12-07 07:57:42.423281: step 18980, loss = 0.81, batch loss = 0.73 (6.0 examples/sec; 1.332 sec/batch; 116h:01m:55s remains)
INFO - root - 2017-12-07 07:57:55.648935: step 18990, loss = 0.68, batch loss = 0.61 (6.2 examples/sec; 1.295 sec/batch; 112h:47m:24s remains)
INFO - root - 2017-12-07 07:58:08.964843: step 19000, loss = 0.91, batch loss = 0.84 (6.0 examples/sec; 1.328 sec/batch; 115h:37m:53s remains)
2017-12-07 07:58:09.979258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.478548 -2.5336685 -2.5571213 -2.5268068 -2.4528854 -2.3572428 -2.1815057 -2.2015378 -2.5281734 -2.7422457 -2.8492746 -2.7441502 -2.3036146 -1.8841131 -1.7341468][-2.3045583 -2.2501547 -2.1749523 -2.1371975 -2.1472347 -2.1595566 -2.0536625 -2.1136909 -2.4563518 -2.6399741 -2.6889536 -2.5483408 -2.1215222 -1.738668 -1.6186638][-2.1876452 -2.0088918 -1.8313043 -1.7898102 -1.8984787 -2.0232 -1.9895031 -2.062638 -2.3692544 -2.4899011 -2.46306 -2.3480055 -2.0357149 -1.7399335 -1.6305382][-2.2938447 -1.9923348 -1.7238016 -1.6720016 -1.8519166 -2.042031 -2.0492249 -2.1098843 -2.34514 -2.3803709 -2.2612498 -2.2065029 -2.07251 -1.8835673 -1.7617023][-2.5069332 -2.1497209 -1.845793 -1.7968144 -2.0027394 -2.1351874 -2.0941081 -2.1422355 -2.319268 -2.2865219 -2.0744863 -2.0944071 -2.1722093 -2.0993896 -1.9528382][-2.712821 -2.3423371 -2.0525091 -2.0099759 -2.1569676 -2.1021571 -1.9045997 -1.9157131 -2.087069 -2.0663104 -1.8454049 -1.9858181 -2.2741935 -2.2940822 -2.1224983][-2.9295459 -2.5333433 -2.24481 -2.1795778 -2.2227464 -1.9720006 -1.6040618 -1.5950892 -1.8144357 -1.8613241 -1.6946843 -1.9487503 -2.3798852 -2.4410472 -2.2459846][-3.019361 -2.6293507 -2.3886819 -2.3255448 -2.2927241 -1.9725292 -1.5916348 -1.6534553 -1.9492762 -2.0122321 -1.8385837 -2.0887525 -2.513413 -2.5456176 -2.329371][-2.8895235 -2.5199144 -2.4017889 -2.4571071 -2.4866393 -2.2662673 -1.9893601 -2.1158986 -2.4092572 -2.4048183 -2.1799676 -2.32777 -2.6321626 -2.5834141 -2.3514535][-2.7046099 -2.4121795 -2.45716 -2.6682103 -2.7890239 -2.6924803 -2.5127437 -2.6464128 -2.8773208 -2.7546313 -2.4773545 -2.4959016 -2.6272578 -2.5031219 -2.2803216][-2.5829403 -2.4254935 -2.5967095 -2.8754725 -3.0337539 -3.0403261 -2.9480875 -3.0659533 -3.2080994 -2.9715095 -2.6760654 -2.5978255 -2.5665708 -2.3737235 -2.1650469][-2.554832 -2.5495338 -2.7675548 -3.0020723 -3.1198008 -3.1822062 -3.1618581 -3.2668443 -3.3591559 -3.07906 -2.7889771 -2.6570172 -2.5033112 -2.2656674 -2.0599813][-2.6574841 -2.7528145 -2.9251583 -3.0537949 -3.1135364 -3.2012608 -3.2269511 -3.32838 -3.4268386 -3.1835232 -2.921577 -2.7537718 -2.5195241 -2.2259026 -1.9797678][-2.681046 -2.8546085 -2.9766877 -3.0140026 -3.0058699 -3.0588751 -3.1059964 -3.2254457 -3.3720562 -3.2143655 -2.9691768 -2.7881489 -2.5497789 -2.240555 -1.9550424][-2.5643396 -2.7344263 -2.8002722 -2.778728 -2.7256799 -2.7417164 -2.8126297 -2.9553328 -3.1243265 -3.0319767 -2.8093371 -2.6575441 -2.4864652 -2.2262802 -1.9608688]]...]
INFO - root - 2017-12-07 07:58:23.261948: step 19010, loss = 0.71, batch loss = 0.64 (6.1 examples/sec; 1.310 sec/batch; 114h:03m:02s remains)
INFO - root - 2017-12-07 07:58:36.463082: step 19020, loss = 0.73, batch loss = 0.66 (6.1 examples/sec; 1.302 sec/batch; 113h:20m:50s remains)
INFO - root - 2017-12-07 07:58:49.725279: step 19030, loss = 0.80, batch loss = 0.73 (6.1 examples/sec; 1.318 sec/batch; 114h:43m:22s remains)
INFO - root - 2017-12-07 07:59:02.659051: step 19040, loss = 0.91, batch loss = 0.83 (6.1 examples/sec; 1.319 sec/batch; 114h:50m:01s remains)
INFO - root - 2017-12-07 07:59:15.970842: step 19050, loss = 0.87, batch loss = 0.79 (6.1 examples/sec; 1.310 sec/batch; 114h:01m:03s remains)
INFO - root - 2017-12-07 07:59:29.225446: step 19060, loss = 0.92, batch loss = 0.85 (6.0 examples/sec; 1.330 sec/batch; 115h:47m:57s remains)
INFO - root - 2017-12-07 07:59:42.545512: step 19070, loss = 0.80, batch loss = 0.73 (5.9 examples/sec; 1.358 sec/batch; 118h:15m:43s remains)
INFO - root - 2017-12-07 07:59:55.893306: step 19080, loss = 0.71, batch loss = 0.64 (6.0 examples/sec; 1.338 sec/batch; 116h:29m:12s remains)
INFO - root - 2017-12-07 08:00:09.210159: step 19090, loss = 0.81, batch loss = 0.74 (6.0 examples/sec; 1.341 sec/batch; 116h:46m:12s remains)
INFO - root - 2017-12-07 08:00:22.558181: step 19100, loss = 0.85, batch loss = 0.78 (5.9 examples/sec; 1.348 sec/batch; 117h:19m:51s remains)
2017-12-07 08:00:23.539629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6270561 -3.77875 -3.9025362 -3.9174473 -3.9075491 -3.9240701 -3.9233356 -3.8038082 -3.6160898 -3.5176454 -3.7042089 -4.0097742 -4.1075268 -3.9950469 -3.809963][-3.6037874 -3.8795333 -4.0499983 -4.0728 -4.081687 -4.0879459 -4.0380116 -3.7871943 -3.4191239 -3.2505088 -3.6233115 -4.2221045 -4.45361 -4.2644682 -3.9266334][-3.6223125 -4.0328836 -4.2165003 -4.2059016 -4.2241373 -4.1950846 -4.05205 -3.6620836 -3.1185579 -2.905549 -3.502816 -4.4132991 -4.791328 -4.5335693 -4.0515366][-3.5161231 -4.1304379 -4.3526258 -4.3152227 -4.3358674 -4.218236 -3.9010918 -3.3749352 -2.7448759 -2.5781231 -3.4205601 -4.5593948 -5.0052042 -4.6549954 -4.0731387][-3.3132634 -4.1341043 -4.3647819 -4.2391286 -4.1469789 -3.798254 -3.2961745 -2.774718 -2.276567 -2.3075089 -3.3469539 -4.5587029 -4.9515758 -4.4826527 -3.8755953][-2.987206 -3.9002719 -4.0297856 -3.7575631 -3.4415843 -2.7724128 -2.1410017 -1.8219461 -1.7383342 -2.1368904 -3.3177855 -4.4774537 -4.7450285 -4.1392627 -3.5468307][-2.5931358 -3.4986458 -3.4978609 -3.0944653 -2.596025 -1.7786574 -1.244118 -1.2651336 -1.6259205 -2.3251164 -3.5215766 -4.5517797 -4.6594939 -3.882431 -3.2984333][-2.5650196 -3.36174 -3.1831384 -2.6206689 -2.0222061 -1.3409984 -1.1781843 -1.5956335 -2.2450535 -2.9755073 -3.9256697 -4.6783071 -4.5761786 -3.6970499 -3.1678956][-2.7987189 -3.4805007 -3.165628 -2.4905443 -1.8759832 -1.4315598 -1.6011722 -2.2396221 -2.9558034 -3.5252926 -4.0969028 -4.5248632 -4.2956915 -3.5352886 -3.1799221][-3.0023203 -3.5639789 -3.2420301 -2.6101174 -2.1275222 -1.9308333 -2.2306795 -2.824259 -3.4184012 -3.7646232 -3.9751468 -4.1234317 -3.9092689 -3.4334435 -3.3238702][-3.1830192 -3.509619 -3.1624551 -2.640306 -2.3318167 -2.3284538 -2.6191187 -3.0084918 -3.4228613 -3.5989895 -3.5879917 -3.5949669 -3.4747982 -3.3173113 -3.4380121][-3.1448364 -3.2291365 -2.8926353 -2.5571136 -2.4390182 -2.5343137 -2.7540569 -2.9298344 -3.1625276 -3.2672338 -3.2393856 -3.283071 -3.2907958 -3.3285875 -3.5415049][-3.118166 -3.0525026 -2.7780814 -2.6222897 -2.6491933 -2.8070238 -2.9874678 -3.0650139 -3.1957307 -3.2788076 -3.2913694 -3.3854589 -3.4584515 -3.5487623 -3.7057137][-3.3788095 -3.3198781 -3.1654832 -3.1373408 -3.2223868 -3.3829627 -3.5170043 -3.5347776 -3.5688844 -3.5946608 -3.5750091 -3.61993 -3.6750369 -3.7415161 -3.8225443][-3.6357965 -3.6244416 -3.5707095 -3.5979433 -3.672574 -3.7686348 -3.8172688 -3.776731 -3.7358694 -3.7013814 -3.6432385 -3.6371188 -3.6770797 -3.7383704 -3.797374]]...]
INFO - root - 2017-12-07 08:00:36.831525: step 19110, loss = 0.72, batch loss = 0.65 (6.1 examples/sec; 1.305 sec/batch; 113h:36m:53s remains)
INFO - root - 2017-12-07 08:00:49.972785: step 19120, loss = 0.86, batch loss = 0.79 (6.0 examples/sec; 1.340 sec/batch; 116h:38m:31s remains)
INFO - root - 2017-12-07 08:01:03.030113: step 19130, loss = 0.74, batch loss = 0.67 (6.1 examples/sec; 1.305 sec/batch; 113h:35m:19s remains)
INFO - root - 2017-12-07 08:01:16.243466: step 19140, loss = 0.90, batch loss = 0.82 (6.2 examples/sec; 1.293 sec/batch; 112h:31m:42s remains)
INFO - root - 2017-12-07 08:01:29.686150: step 19150, loss = 0.84, batch loss = 0.76 (5.9 examples/sec; 1.355 sec/batch; 117h:54m:35s remains)
INFO - root - 2017-12-07 08:01:42.955167: step 19160, loss = 0.63, batch loss = 0.56 (6.0 examples/sec; 1.329 sec/batch; 115h:40m:18s remains)
INFO - root - 2017-12-07 08:01:56.177709: step 19170, loss = 0.60, batch loss = 0.53 (6.1 examples/sec; 1.317 sec/batch; 114h:39m:20s remains)
INFO - root - 2017-12-07 08:02:09.557305: step 19180, loss = 0.90, batch loss = 0.82 (5.9 examples/sec; 1.352 sec/batch; 117h:38m:13s remains)
INFO - root - 2017-12-07 08:02:22.798830: step 19190, loss = 0.80, batch loss = 0.72 (6.1 examples/sec; 1.318 sec/batch; 114h:40m:13s remains)
INFO - root - 2017-12-07 08:02:36.176234: step 19200, loss = 0.70, batch loss = 0.63 (6.1 examples/sec; 1.310 sec/batch; 113h:59m:28s remains)
2017-12-07 08:02:37.136109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3586736 -2.4054193 -2.4921589 -2.5084982 -2.48938 -2.4661591 -2.4279673 -2.419209 -2.4814739 -2.6255417 -2.7775826 -2.8958254 -2.9681044 -2.9870276 -2.9780667][-2.5076752 -2.7353497 -2.9903059 -3.0780418 -3.0867758 -3.0939975 -3.0656881 -3.0316195 -3.0354958 -3.1127887 -3.2212906 -3.3126819 -3.3449345 -3.3155556 -3.2360635][-2.8755002 -3.2517786 -3.6630261 -3.8122768 -3.8120246 -3.7897286 -3.70376 -3.562953 -3.4130793 -3.3543804 -3.4223127 -3.5723143 -3.6622458 -3.6306548 -3.4760718][-2.8412085 -3.228344 -3.7175624 -3.8986442 -3.8853793 -3.8178954 -3.6706147 -3.4473021 -3.1810758 -2.9798481 -3.0176954 -3.2904582 -3.5585382 -3.6331758 -3.5082927][-2.2900057 -2.4743054 -2.8753443 -3.0226259 -2.9630265 -2.8370945 -2.6836495 -2.5463219 -2.4066038 -2.3014677 -2.453054 -2.8624864 -3.2618809 -3.4202838 -3.337141][-1.6344035 -1.6229286 -1.9106746 -1.9728515 -1.77351 -1.5263512 -1.3937309 -1.4445155 -1.5357015 -1.6294196 -1.9622631 -2.5088634 -3.0273404 -3.2538829 -3.1815925][-1.0539281 -0.85957217 -0.99256372 -0.94944358 -0.70619082 -0.47683954 -0.4774797 -0.76178837 -1.0860169 -1.2983351 -1.6829965 -2.3225493 -2.9892683 -3.3470104 -3.3279319][-1.0021007 -0.76217604 -0.84260273 -0.794796 -0.59561491 -0.3962245 -0.40133429 -0.70237446 -1.0542042 -1.2403874 -1.5162303 -2.1047664 -2.8564415 -3.3526587 -3.4505882][-1.7317429 -1.6651869 -1.8876505 -2.0410075 -1.9897184 -1.7694798 -1.5898111 -1.6452925 -1.7859569 -1.802624 -1.8430979 -2.242496 -2.9195602 -3.3983014 -3.4880869][-2.5109892 -2.6204379 -2.966064 -3.2437284 -3.2600071 -2.9905844 -2.6473641 -2.4420433 -2.3678365 -2.3306928 -2.3304772 -2.6259258 -3.1906896 -3.599117 -3.5705504][-2.9507728 -3.1200476 -3.449554 -3.6897519 -3.6434884 -3.2813661 -2.854795 -2.5731359 -2.4885581 -2.5460396 -2.6187136 -2.816874 -3.248158 -3.6109006 -3.5381961][-2.9626288 -3.0870166 -3.3202877 -3.4913273 -3.4166775 -3.0955644 -2.7756991 -2.6017032 -2.6215053 -2.7578821 -2.8482034 -2.9249687 -3.1671362 -3.4280889 -3.3865051][-2.9380357 -3.0302391 -3.1833925 -3.279741 -3.2025118 -2.996726 -2.8479614 -2.8013535 -2.8621581 -2.9633856 -2.9998918 -3.0190535 -3.1531355 -3.3097818 -3.2780428][-3.1499965 -3.256218 -3.3723419 -3.4270701 -3.3688409 -3.2722416 -3.245419 -3.2613912 -3.2955356 -3.3061905 -3.2826524 -3.2751002 -3.3293395 -3.3703923 -3.2787871][-3.228765 -3.3320103 -3.4256811 -3.4687982 -3.4497046 -3.4311693 -3.4616079 -3.5029037 -3.5399792 -3.5343981 -3.5006719 -3.4667778 -3.437181 -3.3747935 -3.2359095]]...]
INFO - root - 2017-12-07 08:02:50.411757: step 19210, loss = 0.76, batch loss = 0.68 (6.0 examples/sec; 1.341 sec/batch; 116h:41m:35s remains)
INFO - root - 2017-12-07 08:03:03.574436: step 19220, loss = 0.75, batch loss = 0.68 (6.0 examples/sec; 1.332 sec/batch; 115h:52m:45s remains)
INFO - root - 2017-12-07 08:03:16.793321: step 19230, loss = 0.69, batch loss = 0.62 (5.9 examples/sec; 1.353 sec/batch; 117h:45m:32s remains)
INFO - root - 2017-12-07 08:03:30.003717: step 19240, loss = 0.93, batch loss = 0.86 (6.1 examples/sec; 1.319 sec/batch; 114h:44m:44s remains)
INFO - root - 2017-12-07 08:03:43.212163: step 19250, loss = 0.93, batch loss = 0.86 (6.0 examples/sec; 1.334 sec/batch; 116h:06m:25s remains)
INFO - root - 2017-12-07 08:03:56.588184: step 19260, loss = 0.73, batch loss = 0.65 (5.9 examples/sec; 1.358 sec/batch; 118h:10m:03s remains)
INFO - root - 2017-12-07 08:04:09.903413: step 19270, loss = 0.76, batch loss = 0.69 (6.3 examples/sec; 1.264 sec/batch; 110h:00m:20s remains)
INFO - root - 2017-12-07 08:04:23.307904: step 19280, loss = 0.83, batch loss = 0.76 (5.9 examples/sec; 1.361 sec/batch; 118h:23m:43s remains)
INFO - root - 2017-12-07 08:04:36.590767: step 19290, loss = 0.70, batch loss = 0.62 (6.1 examples/sec; 1.307 sec/batch; 113h:43m:18s remains)
INFO - root - 2017-12-07 08:04:49.878090: step 19300, loss = 0.76, batch loss = 0.69 (5.9 examples/sec; 1.346 sec/batch; 117h:08m:31s remains)
2017-12-07 08:04:50.916699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0074172 -3.0713966 -3.1112385 -3.1277442 -3.149384 -3.1850128 -3.1986554 -3.1921022 -3.1796434 -3.1534028 -3.1080637 -3.0544405 -2.9923739 -2.9270868 -2.8799217][-3.0952232 -3.1993327 -3.2674325 -3.2766602 -3.2817545 -3.3298414 -3.3663623 -3.3748863 -3.3694851 -3.3471127 -3.2991426 -3.2314253 -3.1483259 -3.0698986 -3.0088894][-3.4954498 -3.5886314 -3.6454134 -3.6121318 -3.5766406 -3.6302812 -3.6893268 -3.7191284 -3.7242777 -3.7158833 -3.6900477 -3.6313171 -3.5546312 -3.5071282 -3.4804015][-4.0648365 -4.1808577 -4.2620597 -4.2202649 -4.1493859 -4.1633687 -4.1938891 -4.2250452 -4.2397442 -4.2457585 -4.2638206 -4.2529373 -4.1992316 -4.1677437 -4.1598797][-3.4832129 -3.616447 -3.7058601 -3.6565442 -3.5591717 -3.4939198 -3.4472208 -3.4703441 -3.5243416 -3.557524 -3.6190217 -3.6766484 -3.6577511 -3.6272364 -3.6196673][-3.0013731 -3.087945 -3.1087134 -3.0075574 -2.8667588 -2.7242143 -2.5986083 -2.6201358 -2.7532659 -2.8432839 -2.9536271 -3.0876024 -3.1191554 -3.1024098 -3.0955641][-3.2212725 -3.2803416 -3.2505283 -3.1036243 -2.9186554 -2.7337844 -2.5722408 -2.5909433 -2.7769856 -2.897006 -3.0422096 -3.245167 -3.3256626 -3.3193502 -3.287545][-3.0279918 -3.092453 -3.0610576 -2.9053979 -2.6907241 -2.5018721 -2.3523853 -2.3755057 -2.567224 -2.6694956 -2.8111224 -3.0512204 -3.1790991 -3.2023072 -3.1673946][-2.7883809 -2.825088 -2.8224144 -2.7177038 -2.5369468 -2.4260528 -2.3796992 -2.4493566 -2.6315877 -2.6910305 -2.7745361 -2.9731932 -3.0725203 -3.1046526 -3.1114283][-2.8672214 -2.8904681 -2.907413 -2.8383236 -2.6775618 -2.6287928 -2.6732922 -2.7642431 -2.9032478 -2.9116738 -2.9452586 -3.0883646 -3.1439507 -3.1655378 -3.2069273][-2.9822762 -2.9987688 -3.0188928 -2.9516129 -2.7663615 -2.69293 -2.7056253 -2.7246063 -2.7853322 -2.7819214 -2.8356683 -2.9787569 -3.0481696 -3.0691278 -3.1118326][-2.9628134 -3.001081 -3.0646822 -3.0531185 -2.9131331 -2.8379676 -2.7986383 -2.7450459 -2.7425113 -2.7552018 -2.8401775 -2.9892282 -3.0687037 -3.0848644 -3.1053915][-2.8843069 -2.9311442 -3.0315402 -3.0938163 -3.04952 -3.0273492 -2.9935236 -2.9204209 -2.8801222 -2.8800914 -2.9256239 -2.9848042 -2.9909723 -2.9627175 -2.9632854][-3.0228319 -3.0348175 -3.1121879 -3.1762667 -3.1774716 -3.1920137 -3.1867752 -3.1355445 -3.0776336 -3.0523987 -3.031795 -2.9939723 -2.9386945 -2.88091 -2.880827][-3.3295217 -3.2932181 -3.3286488 -3.3533592 -3.3311105 -3.3293333 -3.3364806 -3.3126836 -3.2672892 -3.254775 -3.220263 -3.1610365 -3.1057448 -3.0527487 -3.0555043]]...]
INFO - root - 2017-12-07 08:05:04.067248: step 19310, loss = 0.74, batch loss = 0.67 (6.5 examples/sec; 1.237 sec/batch; 107h:34m:40s remains)
INFO - root - 2017-12-07 08:05:17.260047: step 19320, loss = 0.79, batch loss = 0.72 (6.1 examples/sec; 1.316 sec/batch; 114h:28m:06s remains)
INFO - root - 2017-12-07 08:05:30.605818: step 19330, loss = 0.92, batch loss = 0.85 (5.9 examples/sec; 1.365 sec/batch; 118h:42m:04s remains)
INFO - root - 2017-12-07 08:05:43.796457: step 19340, loss = 0.83, batch loss = 0.76 (6.3 examples/sec; 1.268 sec/batch; 110h:20m:07s remains)
INFO - root - 2017-12-07 08:05:56.969387: step 19350, loss = 0.91, batch loss = 0.84 (5.9 examples/sec; 1.351 sec/batch; 117h:30m:18s remains)
INFO - root - 2017-12-07 08:06:10.340806: step 19360, loss = 0.80, batch loss = 0.73 (5.8 examples/sec; 1.374 sec/batch; 119h:29m:54s remains)
INFO - root - 2017-12-07 08:06:23.633934: step 19370, loss = 0.76, batch loss = 0.69 (6.0 examples/sec; 1.330 sec/batch; 115h:40m:37s remains)
INFO - root - 2017-12-07 08:06:36.950252: step 19380, loss = 0.94, batch loss = 0.87 (5.9 examples/sec; 1.359 sec/batch; 118h:11m:50s remains)
INFO - root - 2017-12-07 08:06:50.395240: step 19390, loss = 0.86, batch loss = 0.79 (6.0 examples/sec; 1.334 sec/batch; 116h:03m:10s remains)
INFO - root - 2017-12-07 08:07:03.714772: step 19400, loss = 0.73, batch loss = 0.66 (6.1 examples/sec; 1.303 sec/batch; 113h:17m:21s remains)
2017-12-07 08:07:04.732873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1532705 -1.0272818 -1.4158332 -1.8106213 -1.7647984 -1.5664048 -1.514513 -1.4699118 -1.2145808 -1.104955 -1.1583436 -1.1886325 -1.3196228 -1.430687 -1.4684396][-0.99143791 -0.71720576 -0.99703479 -1.4021189 -1.5551574 -1.6197596 -1.6416717 -1.4608788 -1.0575027 -0.84018326 -0.756016 -0.7327466 -0.94571757 -1.1919441 -1.2785337][-1.0108428 -0.75513053 -0.98178363 -1.3757498 -1.6373432 -1.8283479 -1.8670213 -1.5986443 -1.1286345 -0.8165586 -0.62633324 -0.58485007 -0.81683731 -1.0642207 -1.0983596][-0.90880704 -0.84907436 -1.1372092 -1.5898926 -1.9276345 -2.1313512 -2.1337328 -1.8347771 -1.3387644 -0.91173291 -0.64434481 -0.67915249 -1.0211887 -1.2874267 -1.2241528][-0.98837781 -1.0664706 -1.3823364 -1.8048649 -2.062407 -2.1622312 -2.1653514 -2.03072 -1.6716447 -1.1324391 -0.69675207 -0.76771 -1.2474279 -1.5929632 -1.5321078][-1.5314951 -1.6276813 -1.9275022 -2.2400589 -2.2586522 -2.0458102 -1.8386502 -1.7988539 -1.6833298 -1.2831633 -0.88826346 -1.0559561 -1.5925391 -1.9225428 -1.8486266][-2.1118226 -2.1182199 -2.4133363 -2.7860088 -2.7882266 -2.3419797 -1.7309182 -1.3970778 -1.2713492 -1.0567455 -0.93466806 -1.3496015 -1.9916532 -2.3047528 -2.2274497][-2.591393 -2.4244578 -2.6293409 -3.06581 -3.1913991 -2.7259805 -1.8384759 -1.146328 -0.88173461 -0.78566074 -0.90056276 -1.5066442 -2.23122 -2.5914633 -2.612587][-3.0553665 -2.7791908 -2.8371663 -3.1771913 -3.2981749 -2.8553863 -1.8477209 -0.91286993 -0.55732846 -0.63987589 -1.0365522 -1.7641971 -2.4441824 -2.7652521 -2.8327856][-3.2979255 -3.0563588 -3.0422895 -3.2833152 -3.4409792 -3.1662645 -2.2617528 -1.2372308 -0.72223449 -0.790113 -1.2979496 -2.04198 -2.6419008 -2.9233828 -3.0277653][-3.4894078 -3.3406415 -3.3169761 -3.4773309 -3.6808915 -3.6403196 -3.0537124 -2.2304099 -1.7040091 -1.6585324 -2.0174828 -2.5379372 -2.9369874 -3.1408415 -3.2554176][-3.8804617 -3.8478858 -3.8464997 -3.9044225 -3.9946074 -3.9583976 -3.5538576 -2.9561515 -2.5600109 -2.5235176 -2.7910357 -3.1213658 -3.3571463 -3.4951806 -3.615489][-4.11597 -4.1764016 -4.2424259 -4.2740874 -4.2769675 -4.1928005 -3.8738129 -3.438005 -3.1686702 -3.1620936 -3.3616672 -3.5854218 -3.7596016 -3.8923395 -4.0149984][-4.0923886 -4.1655421 -4.253521 -4.3038611 -4.3103991 -4.2596965 -4.0599666 -3.7996142 -3.6560273 -3.6713483 -3.7883835 -3.8964615 -3.9721708 -4.0319142 -4.0884552][-3.7903855 -3.8694589 -3.9765687 -4.0579834 -4.101686 -4.0888238 -3.9649756 -3.8015716 -3.7041926 -3.6887622 -3.7141747 -3.7263093 -3.726476 -3.7300897 -3.7380621]]...]
INFO - root - 2017-12-07 08:07:17.946505: step 19410, loss = 0.85, batch loss = 0.78 (6.1 examples/sec; 1.302 sec/batch; 113h:15m:26s remains)
INFO - root - 2017-12-07 08:07:31.151256: step 19420, loss = 0.79, batch loss = 0.72 (6.1 examples/sec; 1.320 sec/batch; 114h:49m:53s remains)
INFO - root - 2017-12-07 08:07:44.415611: step 19430, loss = 0.89, batch loss = 0.82 (6.1 examples/sec; 1.309 sec/batch; 113h:51m:50s remains)
INFO - root - 2017-12-07 08:07:57.536552: step 19440, loss = 0.81, batch loss = 0.73 (6.0 examples/sec; 1.326 sec/batch; 115h:18m:26s remains)
INFO - root - 2017-12-07 08:08:10.864831: step 19450, loss = 0.86, batch loss = 0.78 (6.0 examples/sec; 1.326 sec/batch; 115h:17m:56s remains)
INFO - root - 2017-12-07 08:08:24.145087: step 19460, loss = 0.65, batch loss = 0.58 (6.0 examples/sec; 1.323 sec/batch; 115h:02m:16s remains)
INFO - root - 2017-12-07 08:08:37.486306: step 19470, loss = 0.82, batch loss = 0.74 (5.8 examples/sec; 1.373 sec/batch; 119h:24m:01s remains)
INFO - root - 2017-12-07 08:08:50.875420: step 19480, loss = 0.76, batch loss = 0.69 (6.0 examples/sec; 1.342 sec/batch; 116h:42m:09s remains)
INFO - root - 2017-12-07 08:09:04.167334: step 19490, loss = 0.68, batch loss = 0.61 (6.0 examples/sec; 1.336 sec/batch; 116h:07m:22s remains)
INFO - root - 2017-12-07 08:09:15.828762: step 19500, loss = 0.93, batch loss = 0.86 (7.8 examples/sec; 1.020 sec/batch; 88h:40m:21s remains)
2017-12-07 08:09:16.709637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8275404 -1.8482802 -1.8798647 -1.9207609 -1.953244 -1.9683499 -1.9574339 -1.9351959 -1.9022753 -1.8605592 -1.830174 -1.8135352 -1.8057439 -1.8044066 -1.8037226][-2.0135775 -2.0865433 -2.1592529 -2.207108 -2.2291443 -2.2170458 -2.1810193 -2.1379287 -2.0906327 -2.0361509 -1.9827976 -1.9372885 -1.8879991 -1.8459444 -1.8169327][-2.3116007 -2.4446411 -2.5634289 -2.6069558 -2.5900221 -2.4998546 -2.3611534 -2.2255569 -2.1361277 -2.0826628 -2.049408 -2.0319021 -1.9980168 -1.9530904 -1.9041052][-2.67775 -2.8502574 -3.0058966 -3.0705788 -3.0693448 -2.9577515 -2.7267518 -2.4553981 -2.2292297 -2.05043 -1.9307566 -1.8785679 -1.8607693 -1.8601506 -1.8615005][-2.8132706 -2.9869239 -3.1550303 -3.253593 -3.3175063 -3.2990499 -3.1694379 -2.987855 -2.7750211 -2.517153 -2.2721786 -2.0629401 -1.8943017 -1.7851043 -1.7464108][-2.889 -2.9837713 -3.0904822 -3.1264648 -3.126822 -3.1294413 -3.1580293 -3.2493925 -3.2787654 -3.1977525 -3.0804832 -2.8855031 -2.6208076 -2.3366172 -2.1147697][-3.1406169 -3.1979032 -3.20839 -3.0369527 -2.7502847 -2.4958982 -2.417737 -2.5709493 -2.737637 -2.8713078 -3.0244153 -3.0675421 -2.9612849 -2.740819 -2.4987698][-2.8953404 -2.9872298 -2.9582772 -2.6542597 -2.1287422 -1.5630395 -1.2293591 -1.2432618 -1.3518672 -1.5119119 -1.7786829 -2.0208733 -2.1672106 -2.2181759 -2.2048738][-1.9751608 -2.1718683 -2.238632 -2.0495062 -1.5973303 -0.99476337 -0.54780722 -0.39236927 -0.31553173 -0.30141163 -0.44601297 -0.69636631 -0.97488 -1.2554877 -1.503545][-1.2302685 -1.4661286 -1.6400588 -1.6731172 -1.4871461 -1.140193 -0.88548708 -0.77270532 -0.59755969 -0.39276934 -0.31825686 -0.39955664 -0.57083988 -0.82372475 -1.112087][-1.3970404 -1.5314832 -1.6882727 -1.8089213 -1.7945764 -1.7064357 -1.7318592 -1.8561487 -1.8465219 -1.7075031 -1.5640998 -1.4580793 -1.3849368 -1.387423 -1.4537268][-2.1226032 -2.120492 -2.1721771 -2.230547 -2.2097137 -2.1797824 -2.2974539 -2.5114136 -2.5949538 -2.5326576 -2.4105353 -2.2512839 -2.0832858 -1.9619925 -1.87784][-2.422775 -2.3985376 -2.402389 -2.4040425 -2.3426418 -2.3001935 -2.4059563 -2.5919874 -2.668761 -2.6213021 -2.5210235 -2.3814704 -2.2325501 -2.1236815 -2.0302424][-2.3015535 -2.3430407 -2.3735256 -2.3650486 -2.2907319 -2.2196388 -2.2515821 -2.3366261 -2.3676858 -2.3424137 -2.3023627 -2.2454598 -2.1768134 -2.122967 -2.0596774][-2.1678941 -2.2441921 -2.3046939 -2.3194919 -2.2701705 -2.1964731 -2.1551714 -2.1287327 -2.0974872 -2.0759881 -2.0756695 -2.0805471 -2.0751882 -2.0578303 -2.0165808]]...]
INFO - root - 2017-12-07 08:09:27.100157: step 19510, loss = 0.98, batch loss = 0.90 (7.8 examples/sec; 1.025 sec/batch; 89h:04m:57s remains)
INFO - root - 2017-12-07 08:09:37.418465: step 19520, loss = 0.89, batch loss = 0.82 (7.7 examples/sec; 1.039 sec/batch; 90h:21m:36s remains)
INFO - root - 2017-12-07 08:09:47.724953: step 19530, loss = 0.77, batch loss = 0.70 (7.9 examples/sec; 1.019 sec/batch; 88h:35m:20s remains)
INFO - root - 2017-12-07 08:09:57.768266: step 19540, loss = 0.66, batch loss = 0.58 (7.7 examples/sec; 1.036 sec/batch; 90h:06m:21s remains)
INFO - root - 2017-12-07 08:10:08.245784: step 19550, loss = 0.74, batch loss = 0.67 (7.7 examples/sec; 1.036 sec/batch; 90h:02m:27s remains)
INFO - root - 2017-12-07 08:10:18.753612: step 19560, loss = 0.69, batch loss = 0.62 (7.6 examples/sec; 1.056 sec/batch; 91h:45m:59s remains)
INFO - root - 2017-12-07 08:10:29.196111: step 19570, loss = 0.84, batch loss = 0.77 (7.6 examples/sec; 1.057 sec/batch; 91h:51m:34s remains)
INFO - root - 2017-12-07 08:10:39.657973: step 19580, loss = 0.98, batch loss = 0.91 (7.7 examples/sec; 1.033 sec/batch; 89h:47m:10s remains)
INFO - root - 2017-12-07 08:10:49.983930: step 19590, loss = 0.69, batch loss = 0.62 (7.8 examples/sec; 1.031 sec/batch; 89h:38m:47s remains)
INFO - root - 2017-12-07 08:11:00.407211: step 19600, loss = 1.02, batch loss = 0.95 (7.7 examples/sec; 1.033 sec/batch; 89h:46m:09s remains)
2017-12-07 08:11:01.241938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6925235 -3.7450967 -3.6146307 -3.3779202 -3.2216115 -3.0409274 -2.8406515 -2.8573287 -3.0127902 -3.1039982 -3.2433047 -3.3937349 -3.394242 -3.4339461 -3.5554082][-3.78032 -3.7666748 -3.5393867 -3.2056994 -2.9333911 -2.6091843 -2.3571942 -2.3613777 -2.5113571 -2.6539092 -2.8944931 -3.1071978 -3.1015229 -3.173346 -3.3285701][-3.7401309 -3.6100707 -3.2873292 -2.9124436 -2.566783 -2.1578054 -1.9372156 -1.9459746 -2.0085976 -2.1588597 -2.5016003 -2.7126012 -2.6468959 -2.7387843 -2.9184744][-3.7028468 -3.4783487 -3.0875754 -2.7489405 -2.3974416 -2.0022178 -1.8554015 -1.7757707 -1.6024616 -1.7868898 -2.2569373 -2.4023728 -2.2554753 -2.3624191 -2.5939496][-3.6874974 -3.426497 -3.0353787 -2.787504 -2.458025 -2.1140652 -2.0296297 -1.7296925 -1.2386239 -1.4974277 -2.064173 -2.068476 -1.8500206 -2.0422053 -2.4044297][-3.6331966 -3.4048805 -3.0450048 -2.8259895 -2.4521351 -2.1153245 -2.0418453 -1.3973656 -0.58803058 -0.9919951 -1.5882056 -1.4421916 -1.299711 -1.6773491 -2.1849675][-3.683157 -3.5399215 -3.151217 -2.8400104 -2.3601317 -1.9257867 -1.6794274 -0.56430769 0.53731918 -0.13032579 -0.829093 -0.70557141 -0.78433967 -1.3469582 -1.8990092][-3.8895068 -3.7745411 -3.3518534 -2.9959676 -2.4637654 -1.9220252 -1.451735 0.044727325 1.211463 0.23164177 -0.55491638 -0.51300526 -0.7790041 -1.3545866 -1.7215996][-4.061378 -3.9845185 -3.6432092 -3.3184357 -2.7683034 -2.1841719 -1.6256688 -0.22519732 0.57045746 -0.51368046 -1.1213958 -1.0009887 -1.2885466 -1.7583239 -1.9078879][-4.0995984 -4.113256 -3.9086967 -3.5920024 -2.9422717 -2.2964935 -1.7504749 -0.74078894 -0.41912889 -1.3738863 -1.6116829 -1.3527117 -1.576134 -1.9607556 -2.079005][-4.0211344 -4.0784163 -3.9315689 -3.5806057 -2.9085722 -2.2804701 -1.8408611 -1.2818987 -1.2600622 -1.869199 -1.7601955 -1.5002458 -1.6692853 -1.9982264 -2.2420087][-3.9457355 -4.0690565 -3.9906232 -3.643918 -3.0143971 -2.4065025 -2.0919185 -1.9064674 -1.9711766 -2.1577578 -1.8922603 -1.7685604 -1.9184859 -2.2514102 -2.6613989][-3.9847488 -4.2316651 -4.3188705 -4.0684786 -3.5224526 -2.9259253 -2.6565804 -2.6085644 -2.5510921 -2.3946962 -2.1474068 -2.2218044 -2.3954592 -2.720685 -3.1667829][-4.0999117 -4.4494343 -4.6353431 -4.4164953 -3.9173443 -3.371696 -3.1976285 -3.2408752 -3.0939906 -2.7394705 -2.5196881 -2.674751 -2.8274698 -3.093024 -3.415303][-4.0572028 -4.4148288 -4.5992513 -4.3720474 -3.892714 -3.4605913 -3.4214184 -3.5699604 -3.4186506 -3.032167 -2.848191 -3.0129094 -3.1355929 -3.3063288 -3.4337988]]...]
INFO - root - 2017-12-07 08:11:11.601626: step 19610, loss = 0.83, batch loss = 0.76 (7.7 examples/sec; 1.042 sec/batch; 90h:34m:56s remains)
INFO - root - 2017-12-07 08:11:21.982612: step 19620, loss = 0.89, batch loss = 0.82 (7.7 examples/sec; 1.036 sec/batch; 90h:02m:18s remains)
INFO - root - 2017-12-07 08:11:32.276250: step 19630, loss = 0.73, batch loss = 0.66 (7.8 examples/sec; 1.026 sec/batch; 89h:10m:44s remains)
INFO - root - 2017-12-07 08:11:42.545599: step 19640, loss = 0.74, batch loss = 0.67 (7.9 examples/sec; 1.011 sec/batch; 87h:49m:42s remains)
INFO - root - 2017-12-07 08:11:52.962511: step 19650, loss = 0.59, batch loss = 0.52 (7.4 examples/sec; 1.077 sec/batch; 93h:35m:00s remains)
INFO - root - 2017-12-07 08:12:03.444519: step 19660, loss = 0.71, batch loss = 0.64 (7.8 examples/sec; 1.028 sec/batch; 89h:19m:01s remains)
INFO - root - 2017-12-07 08:12:13.956424: step 19670, loss = 0.84, batch loss = 0.77 (7.6 examples/sec; 1.053 sec/batch; 91h:31m:22s remains)
INFO - root - 2017-12-07 08:12:24.253768: step 19680, loss = 0.71, batch loss = 0.64 (7.5 examples/sec; 1.064 sec/batch; 92h:24m:55s remains)
INFO - root - 2017-12-07 08:12:34.579736: step 19690, loss = 0.68, batch loss = 0.60 (7.7 examples/sec; 1.034 sec/batch; 89h:52m:40s remains)
INFO - root - 2017-12-07 08:12:44.921508: step 19700, loss = 0.88, batch loss = 0.81 (7.8 examples/sec; 1.029 sec/batch; 89h:22m:55s remains)
2017-12-07 08:12:45.828104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6699567 -2.896771 -3.0120025 -3.0554171 -3.0153208 -2.9169168 -2.8636203 -2.6328211 -2.5276642 -2.7859097 -2.6746616 -2.3750837 -2.1982076 -1.8919504 -1.7359645][-2.7378583 -2.8680086 -2.8995831 -2.9124496 -2.8803511 -2.8525157 -2.8089242 -2.612031 -2.5884149 -2.7369134 -2.4897795 -2.1189325 -1.9281306 -1.7876291 -1.7361336][-2.1357987 -2.2067852 -2.2387748 -2.3503284 -2.504375 -2.6671886 -2.6730886 -2.5637865 -2.5886538 -2.6057897 -2.2764759 -1.8935955 -1.845247 -1.9741805 -2.0719006][-1.3700426 -1.3948684 -1.4138644 -1.6295104 -2.0494125 -2.4483967 -2.5410659 -2.5362754 -2.6210637 -2.5746446 -2.2043574 -1.8085852 -1.8933895 -2.1928418 -2.2977822][-0.89716387 -0.92445612 -0.94544244 -1.1764543 -1.7436788 -2.2676399 -2.4305704 -2.5158091 -2.6379266 -2.5433898 -2.0868282 -1.6386194 -1.7536428 -1.9954052 -1.9312258][-0.81277847 -0.88225675 -0.91181517 -0.98691964 -1.4286776 -1.8848877 -2.0948019 -2.3009524 -2.5141511 -2.4207592 -1.9058144 -1.3881788 -1.4194381 -1.4460464 -1.1265187][-1.4246833 -1.4341195 -1.2897489 -0.95374084 -0.8861506 -0.91100693 -0.87383771 -1.1575701 -1.5868089 -1.7061002 -1.3891678 -1.0702281 -1.1875618 -1.0872033 -0.55162024][-2.2495596 -2.0906956 -1.6860025 -0.98547626 -0.38673115 0.26773214 0.90818834 0.8253932 0.29508591 -0.043636322 -0.078407288 -0.28034878 -0.87496662 -0.97378278 -0.49160886][-2.70539 -2.3957992 -1.9078627 -1.2287157 -0.5659337 0.45323372 1.5276718 1.7495294 1.3975234 1.1744952 1.1154389 0.51712322 -0.58237314 -1.0229857 -0.90658283][-3.0534654 -2.7585206 -2.4024019 -2.0545061 -1.6210907 -0.695189 0.22827435 0.5367384 0.47866678 0.533391 0.74034595 0.14810467 -1.0474212 -1.5749178 -1.7545662][-3.41681 -3.306366 -3.2076573 -3.2112582 -3.0229287 -2.3444028 -1.7184353 -1.4614727 -1.3554859 -1.1216366 -0.75021434 -1.1748893 -2.1508236 -2.5125725 -2.6555192][-3.397733 -3.4564602 -3.5930996 -3.8399355 -3.8069367 -3.3401878 -2.9515338 -2.8208697 -2.7163157 -2.4557076 -2.1006596 -2.3741477 -3.0415618 -3.1603479 -3.1019273][-3.0670438 -3.2761269 -3.5519066 -3.8999033 -3.9552782 -3.6695237 -3.4794326 -3.5170975 -3.5104787 -3.3012204 -3.0164948 -3.1164136 -3.4414606 -3.3597295 -3.1123157][-2.7524548 -3.0569949 -3.4126315 -3.7860591 -3.902132 -3.7885482 -3.7585874 -3.8766503 -3.9153228 -3.7568822 -3.5035224 -3.4409089 -3.4778304 -3.2715373 -2.9442654][-2.7236271 -3.0365963 -3.378684 -3.7111893 -3.8566668 -3.8638206 -3.8881176 -3.9617586 -3.9528813 -3.8171978 -3.6039891 -3.4344645 -3.3264546 -3.113821 -2.8124087]]...]
INFO - root - 2017-12-07 08:12:56.243834: step 19710, loss = 0.74, batch loss = 0.67 (7.8 examples/sec; 1.029 sec/batch; 89h:22m:39s remains)
INFO - root - 2017-12-07 08:13:06.600340: step 19720, loss = 0.87, batch loss = 0.79 (7.5 examples/sec; 1.074 sec/batch; 93h:16m:53s remains)
INFO - root - 2017-12-07 08:13:17.185434: step 19730, loss = 0.65, batch loss = 0.58 (7.4 examples/sec; 1.074 sec/batch; 93h:18m:27s remains)
INFO - root - 2017-12-07 08:13:27.374707: step 19740, loss = 0.81, batch loss = 0.74 (8.2 examples/sec; 0.975 sec/batch; 84h:41m:17s remains)
INFO - root - 2017-12-07 08:13:37.693295: step 19750, loss = 0.90, batch loss = 0.83 (7.7 examples/sec; 1.039 sec/batch; 90h:13m:20s remains)
INFO - root - 2017-12-07 08:13:48.145002: step 19760, loss = 0.71, batch loss = 0.64 (7.8 examples/sec; 1.028 sec/batch; 89h:18m:31s remains)
INFO - root - 2017-12-07 08:13:58.630570: step 19770, loss = 0.74, batch loss = 0.67 (7.6 examples/sec; 1.057 sec/batch; 91h:49m:32s remains)
INFO - root - 2017-12-07 08:14:09.101436: step 19780, loss = 1.01, batch loss = 0.94 (7.5 examples/sec; 1.066 sec/batch; 92h:35m:08s remains)
INFO - root - 2017-12-07 08:14:19.500491: step 19790, loss = 0.62, batch loss = 0.55 (7.5 examples/sec; 1.061 sec/batch; 92h:11m:21s remains)
INFO - root - 2017-12-07 08:14:29.848339: step 19800, loss = 1.00, batch loss = 0.93 (7.6 examples/sec; 1.052 sec/batch; 91h:20m:57s remains)
2017-12-07 08:14:30.637982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4817569 -2.7233138 -1.3203292 -0.086434841 1.0121241 1.3766704 0.83000374 -0.21473551 -0.99992657 -1.5923383 -2.8351579 -4.0640125 -4.2937684 -4.0207953 -3.9728694][-3.8587914 -3.0252094 -1.3880582 0.22969389 1.688961 2.2219968 1.6987276 0.77506638 0.029955864 -0.76516819 -2.4048774 -3.9691224 -4.2851329 -3.9484735 -3.8441613][-3.8196921 -3.0290346 -1.291697 0.63706446 2.3653851 3.0845551 2.7928028 2.2056513 1.5161505 0.51812792 -1.419102 -3.2811151 -3.9305832 -3.7805924 -3.6988611][-3.7384307 -3.0434456 -1.4014845 0.69397974 2.6680479 3.6941423 3.8890686 3.8073988 3.2566981 2.0413737 -0.16081858 -2.3258345 -3.4518108 -3.6631143 -3.5955441][-3.8432689 -3.2626913 -1.8448517 0.30639029 2.4870119 3.7957029 4.5341625 4.972374 4.6584454 3.3047404 0.92777014 -1.4680552 -3.0478959 -3.5971248 -3.5064569][-3.8160677 -3.2853599 -2.0275209 0.091677189 2.2349215 3.5314651 4.4714584 5.1165047 4.8715515 3.483901 1.1802096 -1.1137545 -2.8305459 -3.5578814 -3.4805822][-3.5593131 -3.0121412 -1.788682 0.23105955 2.1527982 3.2613888 4.0739431 4.5021858 4.0405836 2.7017059 0.74287271 -1.1920033 -2.8078852 -3.559413 -3.5184472][-3.3068988 -2.6627443 -1.4427168 0.37157202 1.942421 2.79074 3.3441534 3.4239211 2.7704854 1.6700301 0.24063444 -1.2557828 -2.6832137 -3.3853226 -3.411056][-3.3234768 -2.6337805 -1.5077801 0.073163986 1.3583016 1.9723463 2.1780238 1.8455162 1.0756407 0.24456739 -0.65673447 -1.7279069 -2.8454752 -3.3306823 -3.355212][-3.370522 -2.818018 -1.9405503 -0.61327672 0.50766277 0.94227409 0.8246417 0.13771296 -0.77800512 -1.4533234 -2.0053217 -2.7393432 -3.4879959 -3.6365118 -3.5780187][-3.2192206 -2.9239311 -2.3393688 -1.2663538 -0.32441187 -0.086440563 -0.49957132 -1.4690602 -2.4963965 -3.0260053 -3.34919 -3.8071985 -4.1950207 -4.0504856 -3.9507558][-2.999332 -3.0275204 -2.7245293 -1.8717282 -1.1600444 -1.135082 -1.7675498 -2.8417993 -3.8142631 -4.2147474 -4.4082375 -4.6047468 -4.6501012 -4.2915511 -4.1552286][-3.0466805 -3.3718016 -3.3296595 -2.7180929 -2.2066126 -2.225744 -2.7669671 -3.5883861 -4.2822194 -4.53216 -4.6426859 -4.693769 -4.6353765 -4.3101707 -4.2334571][-3.5154376 -3.9890192 -4.0928655 -3.6505179 -3.2277107 -3.1410828 -3.3585472 -3.74993 -4.084455 -4.1779337 -4.2412319 -4.295413 -4.3495684 -4.250917 -4.32006][-3.7344112 -4.151001 -4.2954607 -3.9931366 -3.6517534 -3.5130959 -3.5369267 -3.6512771 -3.7497287 -3.770788 -3.8278639 -3.9362926 -4.0970736 -4.1703348 -4.3087606]]...]
INFO - root - 2017-12-07 08:14:41.085552: step 19810, loss = 0.82, batch loss = 0.75 (7.7 examples/sec; 1.039 sec/batch; 90h:16m:19s remains)
INFO - root - 2017-12-07 08:14:51.454074: step 19820, loss = 0.71, batch loss = 0.63 (7.7 examples/sec; 1.037 sec/batch; 90h:05m:57s remains)
INFO - root - 2017-12-07 08:15:01.867859: step 19830, loss = 0.73, batch loss = 0.66 (7.4 examples/sec; 1.079 sec/batch; 93h:40m:48s remains)
INFO - root - 2017-12-07 08:15:11.965223: step 19840, loss = 0.84, batch loss = 0.77 (7.7 examples/sec; 1.037 sec/batch; 90h:02m:30s remains)
INFO - root - 2017-12-07 08:15:22.455920: step 19850, loss = 0.88, batch loss = 0.81 (7.8 examples/sec; 1.029 sec/batch; 89h:23m:41s remains)
INFO - root - 2017-12-07 08:15:32.870585: step 19860, loss = 0.89, batch loss = 0.82 (7.6 examples/sec; 1.047 sec/batch; 90h:53m:02s remains)
INFO - root - 2017-12-07 08:15:43.169662: step 19870, loss = 0.88, batch loss = 0.81 (7.7 examples/sec; 1.044 sec/batch; 90h:40m:09s remains)
INFO - root - 2017-12-07 08:15:53.614438: step 19880, loss = 0.79, batch loss = 0.72 (7.8 examples/sec; 1.025 sec/batch; 89h:01m:46s remains)
INFO - root - 2017-12-07 08:16:03.932429: step 19890, loss = 0.90, batch loss = 0.83 (7.8 examples/sec; 1.031 sec/batch; 89h:30m:46s remains)
INFO - root - 2017-12-07 08:16:14.410908: step 19900, loss = 0.94, batch loss = 0.87 (7.5 examples/sec; 1.064 sec/batch; 92h:25m:30s remains)
2017-12-07 08:16:15.188149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2823637 -1.8827484 -1.5088625 -1.5585454 -1.7108479 -1.8174956 -1.997721 -2.1316171 -2.1540627 -1.9453828 -1.7436337 -1.7433796 -1.6823878 -1.4780097 -1.2425277][-2.1143053 -1.8447649 -1.6545203 -1.7406785 -1.8532248 -1.9099395 -2.0619051 -2.2154613 -2.2353671 -2.0217214 -1.8209641 -1.7728209 -1.6399815 -1.4293997 -1.1810389][-1.9617679 -1.8487306 -1.7541957 -1.7842634 -1.8888631 -2.005028 -2.1656957 -2.289052 -2.2774007 -2.1246233 -1.9910367 -1.8894944 -1.6949949 -1.5086875 -1.3726616][-2.1097381 -2.1221018 -2.0501695 -1.982718 -2.0122747 -2.0652857 -2.0981247 -2.1416316 -2.1705158 -2.1700449 -2.1212761 -1.9643507 -1.7407546 -1.5928557 -1.5721538][-2.3818498 -2.4427819 -2.4281435 -2.3463717 -2.2977898 -2.1419992 -1.9819267 -1.9757414 -2.0978954 -2.2128823 -2.1439559 -1.856827 -1.5824497 -1.442323 -1.4859896][-2.4107111 -2.492517 -2.5881238 -2.5832815 -2.4733491 -2.1425974 -1.89013 -1.9127865 -2.0976164 -2.1990185 -2.0153606 -1.5862195 -1.2566707 -1.0866954 -1.1040008][-2.2791107 -2.4571607 -2.6456676 -2.6619141 -2.4619884 -2.014051 -1.6715178 -1.6581125 -1.7863827 -1.803411 -1.575155 -1.2011375 -0.98816085 -0.862365 -0.80702543][-2.30575 -2.6339874 -2.8432298 -2.7663183 -2.4492092 -1.9219849 -1.4436994 -1.2690809 -1.2834816 -1.2501702 -1.0970769 -0.95628023 -1.0070388 -1.0277159 -0.9487586][-2.468328 -2.8628263 -3.0244374 -2.8496437 -2.4969282 -1.9782069 -1.4007902 -1.0977833 -1.0496199 -1.0521536 -1.0373099 -1.0413966 -1.2037845 -1.3281956 -1.2602251][-2.3881133 -2.7090127 -2.8169007 -2.612597 -2.3227544 -1.9320331 -1.445194 -1.2122204 -1.1650863 -1.170048 -1.1821995 -1.1357276 -1.2224519 -1.3161936 -1.2394824][-2.0661676 -2.2484536 -2.3341978 -2.1612833 -1.9508591 -1.7024889 -1.3730145 -1.2837574 -1.2374732 -1.1682281 -1.0855081 -0.93936038 -0.91972351 -0.93211985 -0.83534074][-1.891135 -1.935936 -1.9816766 -1.84007 -1.6959324 -1.5279009 -1.2990289 -1.2918377 -1.2305846 -1.1020691 -0.96018171 -0.79581475 -0.72326279 -0.68647456 -0.6168642][-2.1878924 -2.1906774 -2.1796336 -2.0534096 -1.959739 -1.8538487 -1.6926298 -1.6914613 -1.6178484 -1.4621732 -1.3101947 -1.1453586 -1.0255358 -0.96709085 -0.927021][-2.7266889 -2.7289691 -2.6470594 -2.5076277 -2.4525924 -2.4386389 -2.3754196 -2.37193 -2.3122928 -2.1687839 -2.0241849 -1.8617036 -1.706979 -1.6138992 -1.5760231][-3.06009 -3.008122 -2.8400812 -2.6527014 -2.5984871 -2.637773 -2.6530986 -2.67597 -2.6645737 -2.5697751 -2.45418 -2.3487337 -2.2431505 -2.1564543 -2.1227238]]...]
INFO - root - 2017-12-07 08:16:25.706869: step 19910, loss = 0.84, batch loss = 0.77 (7.8 examples/sec; 1.030 sec/batch; 89h:26m:17s remains)
INFO - root - 2017-12-07 08:16:36.152860: step 19920, loss = 0.73, batch loss = 0.66 (7.5 examples/sec; 1.068 sec/batch; 92h:43m:13s remains)
INFO - root - 2017-12-07 08:16:46.674129: step 19930, loss = 0.86, batch loss = 0.79 (7.3 examples/sec; 1.090 sec/batch; 94h:36m:15s remains)
INFO - root - 2017-12-07 08:16:56.952505: step 19940, loss = 0.92, batch loss = 0.84 (7.7 examples/sec; 1.041 sec/batch; 90h:25m:28s remains)
INFO - root - 2017-12-07 08:17:07.277811: step 19950, loss = 0.66, batch loss = 0.59 (7.6 examples/sec; 1.051 sec/batch; 91h:16m:54s remains)
INFO - root - 2017-12-07 08:17:17.763166: step 19960, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.061 sec/batch; 92h:09m:05s remains)
INFO - root - 2017-12-07 08:17:28.145744: step 19970, loss = 0.85, batch loss = 0.78 (7.6 examples/sec; 1.048 sec/batch; 90h:58m:09s remains)
INFO - root - 2017-12-07 08:17:38.537204: step 19980, loss = 0.65, batch loss = 0.58 (7.7 examples/sec; 1.033 sec/batch; 89h:41m:02s remains)
INFO - root - 2017-12-07 08:17:49.000174: step 19990, loss = 0.78, batch loss = 0.71 (7.7 examples/sec; 1.041 sec/batch; 90h:22m:04s remains)
INFO - root - 2017-12-07 08:17:59.404299: step 20000, loss = 0.54, batch loss = 0.47 (7.5 examples/sec; 1.066 sec/batch; 92h:31m:25s remains)
2017-12-07 08:18:00.240993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3587484 -3.3453636 -3.3828821 -3.3709586 -3.4259496 -3.5222878 -3.5485942 -3.5827363 -3.6441607 -3.6280251 -3.6078544 -3.631809 -3.7567377 -3.8504009 -3.7421913][-3.0444112 -3.1483679 -3.3311799 -3.3842063 -3.4971714 -3.6526372 -3.6655872 -3.6683841 -3.661025 -3.497478 -3.3176064 -3.3006978 -3.5144863 -3.7694077 -3.7207222][-2.8911247 -3.1020546 -3.3432469 -3.3152375 -3.3244386 -3.3536053 -3.2325945 -3.1611142 -3.1156411 -2.8557243 -2.5683093 -2.5675492 -2.9843671 -3.558825 -3.7071545][-2.8241525 -2.8983634 -2.9427619 -2.6384461 -2.5063558 -2.4580503 -2.3189173 -2.3484304 -2.5190282 -2.4466455 -2.2178245 -2.2706807 -2.8355386 -3.6090951 -3.8663056][-2.7299733 -2.4416113 -2.189043 -1.6387248 -1.4734383 -1.4755387 -1.3994131 -1.5491521 -2.017179 -2.2950039 -2.2399774 -2.3805311 -3.0341141 -3.8630314 -4.1308994][-2.4736238 -1.9086225 -1.5351415 -0.87038326 -0.6907649 -0.6474731 -0.32744122 -0.27557611 -0.93446255 -1.5900106 -1.7924418 -2.1369183 -3.0020261 -3.8879006 -4.1604662][-2.0709007 -1.5938919 -1.3158348 -0.51292109 -0.064547539 0.37000275 1.3479819 1.8756127 1.0050073 -0.1541791 -0.71492863 -1.4005938 -2.6599274 -3.6864505 -3.9741266][-2.0455189 -1.9955564 -1.8361046 -0.70355225 0.26597738 1.2632041 2.9099579 3.8396358 2.627656 0.96840715 0.12752867 -0.77286124 -2.3189235 -3.399411 -3.6459188][-2.3252816 -2.6549225 -2.4978704 -1.0939078 0.21824694 1.4441137 3.1848125 4.0374823 2.5904799 0.88251972 0.12922144 -0.61041856 -1.9563079 -2.7994809 -2.9842181][-2.4938073 -2.9390407 -2.8110065 -1.6502943 -0.68932247 0.054510117 1.1467743 1.5602365 0.43150902 -0.76398516 -1.1492941 -1.4358127 -2.1521993 -2.5162978 -2.5623317][-2.5884573 -2.8364618 -2.6786366 -1.9796541 -1.6584754 -1.5945852 -1.2409515 -1.2036829 -1.9416385 -2.6290655 -2.7495434 -2.680676 -2.8077023 -2.7670867 -2.6463861][-3.0383906 -3.0711236 -2.8319283 -2.4017944 -2.4466989 -2.7394133 -2.8079712 -2.9505603 -3.4022942 -3.811738 -3.9068022 -3.7713885 -3.6585782 -3.4059787 -3.1105027][-3.5434995 -3.5631664 -3.3881216 -3.1535459 -3.3132195 -3.6596284 -3.8400352 -3.9130549 -4.0431294 -4.1995807 -4.3038039 -4.2958331 -4.2681041 -4.077661 -3.7014923][-3.6648002 -3.7798359 -3.8430228 -3.857023 -4.0539231 -4.3431511 -4.4762931 -4.3841362 -4.2100172 -4.1482816 -4.1971006 -4.2807918 -4.3913684 -4.3340397 -3.9756017][-3.6953335 -3.9227693 -4.2066531 -4.3995624 -4.5612621 -4.7123489 -4.7398219 -4.5444641 -4.2328849 -4.0398188 -4.01531 -4.0968714 -4.2315216 -4.2211909 -3.9243839]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 08:18:10.970899: step 20010, loss = 0.75, batch loss = 0.68 (7.9 examples/sec; 1.016 sec/batch; 88h:10m:49s remains)
INFO - root - 2017-12-07 08:18:21.348809: step 20020, loss = 0.60, batch loss = 0.52 (7.6 examples/sec; 1.047 sec/batch; 90h:50m:54s remains)
INFO - root - 2017-12-07 08:18:31.774802: step 20030, loss = 0.71, batch loss = 0.64 (7.5 examples/sec; 1.073 sec/batch; 93h:08m:00s remains)
INFO - root - 2017-12-07 08:18:42.189807: step 20040, loss = 0.91, batch loss = 0.84 (7.3 examples/sec; 1.099 sec/batch; 95h:20m:39s remains)
INFO - root - 2017-12-07 08:18:52.659530: step 20050, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.067 sec/batch; 92h:38m:56s remains)
INFO - root - 2017-12-07 08:19:02.962081: step 20060, loss = 0.84, batch loss = 0.77 (7.8 examples/sec; 1.026 sec/batch; 89h:02m:35s remains)
INFO - root - 2017-12-07 08:19:13.407177: step 20070, loss = 1.02, batch loss = 0.94 (7.9 examples/sec; 1.009 sec/batch; 87h:33m:51s remains)
INFO - root - 2017-12-07 08:19:23.781092: step 20080, loss = 0.79, batch loss = 0.72 (7.6 examples/sec; 1.047 sec/batch; 90h:52m:06s remains)
INFO - root - 2017-12-07 08:19:34.204097: step 20090, loss = 0.74, batch loss = 0.67 (7.8 examples/sec; 1.031 sec/batch; 89h:26m:14s remains)
INFO - root - 2017-12-07 08:19:44.640252: step 20100, loss = 0.99, batch loss = 0.91 (7.7 examples/sec; 1.042 sec/batch; 90h:27m:41s remains)
2017-12-07 08:19:45.450716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7383556 -3.5498137 -3.7200911 -3.0479636 -2.3977647 -1.8669255 -0.87006474 -0.017797947 0.13808107 -0.10208654 -0.4320159 -0.68906069 -0.873929 -1.1595399 -1.8160765][-3.0243263 -3.8777494 -3.9552789 -3.0881109 -2.313201 -1.6466191 -0.5172112 0.30138636 0.25105858 -0.22732449 -0.64461637 -0.69782758 -0.55345154 -0.54481411 -1.0838523][-3.3339317 -4.0557432 -3.9905334 -3.0371761 -2.2325172 -1.4460728 -0.26525307 0.4225235 0.17629862 -0.44505668 -0.88012981 -0.75236416 -0.28800535 0.033383846 -0.27706766][-3.7813973 -4.2918859 -3.9989629 -2.9958072 -2.1669734 -1.1867244 0.073776722 0.65261984 0.23954535 -0.56267309 -1.0637376 -0.84643865 -0.17065239 0.30149984 0.13079929][-4.051959 -4.3399382 -3.8902242 -2.9831116 -2.2354593 -1.1049747 0.26801586 0.94889355 0.62049055 -0.25265789 -0.90014625 -0.829339 -0.20284271 0.18795824 0.049609184][-3.9715323 -4.0958409 -3.6066117 -2.9062362 -2.3408158 -1.2701378 -0.0021209717 0.8385148 0.82036924 0.098761559 -0.59681487 -0.80389857 -0.50135612 -0.38388443 -0.53040004][-3.8162026 -3.7552247 -3.2321467 -2.6636236 -2.2045631 -1.3502607 -0.4508698 0.30689812 0.53577852 0.12419128 -0.4160285 -0.81266332 -0.9433322 -1.199049 -1.3952501][-3.816982 -3.5707352 -3.0020278 -2.484261 -2.0184989 -1.3963573 -0.91689873 -0.32881594 0.06178093 -0.0076828003 -0.34084368 -0.86442351 -1.3515897 -1.8606822 -2.054028][-3.9648912 -3.5943375 -3.0125661 -2.5027604 -1.9347157 -1.4574075 -1.2809868 -0.84545112 -0.38323164 -0.2261467 -0.46845222 -1.121418 -1.7759485 -2.3067942 -2.4187493][-4.0590348 -3.6597471 -3.1212049 -2.6276076 -1.9377322 -1.4889679 -1.4347651 -1.1262302 -0.74243236 -0.57613444 -0.88200974 -1.6024489 -2.252459 -2.6454329 -2.6104434][-4.03483 -3.6765523 -3.1902609 -2.755029 -2.0859737 -1.6590238 -1.5869453 -1.3776357 -1.1424997 -1.1313567 -1.58868 -2.312161 -2.8412352 -3.0243106 -2.7914696][-3.9325273 -3.6551161 -3.2363675 -2.9205937 -2.437983 -2.07786 -1.9723291 -1.8361096 -1.7524509 -1.9797733 -2.5728838 -3.1987724 -3.5467618 -3.4882922 -3.0692554][-3.8255854 -3.5996397 -3.2605934 -3.1214027 -2.9392433 -2.7584696 -2.726634 -2.6272964 -2.556061 -2.8588045 -3.3962476 -3.8515065 -4.0653534 -3.8550172 -3.3636503][-3.7858071 -3.6103749 -3.29125 -3.1911573 -3.2154012 -3.2946973 -3.5087848 -3.5277894 -3.4324718 -3.6421022 -3.9327352 -4.1563578 -4.2560983 -3.9541011 -3.4782071][-3.7109802 -3.5756159 -3.2568426 -3.1254745 -3.2308903 -3.5305963 -4.0069332 -4.1928272 -4.1177664 -4.1688156 -4.1979766 -4.2225423 -4.1825061 -3.76448 -3.2835457]]...]
INFO - root - 2017-12-07 08:19:55.919085: step 20110, loss = 0.86, batch loss = 0.79 (7.7 examples/sec; 1.037 sec/batch; 90h:00m:47s remains)
INFO - root - 2017-12-07 08:20:06.337674: step 20120, loss = 0.79, batch loss = 0.72 (7.8 examples/sec; 1.027 sec/batch; 89h:04m:24s remains)
INFO - root - 2017-12-07 08:20:14.691444: step 20130, loss = 0.95, batch loss = 0.88 (10.7 examples/sec; 0.750 sec/batch; 65h:07m:07s remains)
INFO - root - 2017-12-07 08:20:22.331346: step 20140, loss = 0.99, batch loss = 0.92 (10.6 examples/sec; 0.758 sec/batch; 65h:47m:30s remains)
INFO - root - 2017-12-07 08:20:30.143686: step 20150, loss = 0.98, batch loss = 0.90 (10.2 examples/sec; 0.785 sec/batch; 68h:04m:13s remains)
INFO - root - 2017-12-07 08:20:37.903636: step 20160, loss = 0.81, batch loss = 0.74 (9.8 examples/sec; 0.814 sec/batch; 70h:35m:11s remains)
INFO - root - 2017-12-07 08:20:45.664493: step 20170, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.750 sec/batch; 65h:05m:40s remains)
INFO - root - 2017-12-07 08:20:53.276015: step 20180, loss = 0.88, batch loss = 0.80 (10.9 examples/sec; 0.733 sec/batch; 63h:35m:56s remains)
INFO - root - 2017-12-07 08:21:00.884611: step 20190, loss = 0.87, batch loss = 0.80 (10.7 examples/sec; 0.748 sec/batch; 64h:55m:07s remains)
INFO - root - 2017-12-07 08:21:08.415601: step 20200, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.753 sec/batch; 65h:17m:18s remains)
2017-12-07 08:21:09.001500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6307971 -3.6483655 -3.6659629 -3.7117887 -3.7258704 -3.711875 -3.692121 -3.6098385 -3.55446 -3.6100278 -3.7283559 -3.8125696 -3.8589978 -3.9391761 -3.9892266][-3.6862597 -3.6773076 -3.6712112 -3.7253141 -3.7674112 -3.7572763 -3.7278674 -3.6227791 -3.5194068 -3.5012813 -3.5699339 -3.6389551 -3.691628 -3.8081439 -3.9143677][-3.7894883 -3.7847812 -3.7793612 -3.8504126 -3.9251323 -3.9141133 -3.853579 -3.7172441 -3.5618651 -3.4332554 -3.3604207 -3.3021455 -3.2865241 -3.4135804 -3.5542092][-3.9436769 -3.9747219 -4.0122609 -4.1357946 -4.2652459 -4.2570171 -4.13006 -3.9258547 -3.7084649 -3.4558833 -3.2002733 -2.9783025 -2.8881776 -3.0175452 -3.1716762][-4.0391121 -4.0885005 -4.1715546 -4.3394241 -4.5029969 -4.4647746 -4.2169814 -3.885808 -3.5579247 -3.194412 -2.8411741 -2.5900083 -2.5436256 -2.7138948 -2.910964][-3.9625516 -3.96844 -4.0385056 -4.1699791 -4.2499185 -4.0898757 -3.7125559 -3.3301387 -3.0288503 -2.7442093 -2.5129197 -2.4279816 -2.4948997 -2.6180553 -2.7230268][-3.7083433 -3.5783887 -3.5196681 -3.5022871 -3.4082778 -3.1135006 -2.6881709 -2.4201384 -2.3773344 -2.4188504 -2.5072961 -2.7000432 -2.9012666 -2.9247179 -2.8399477][-3.2384911 -2.8573813 -2.5132225 -2.2147171 -1.910799 -1.5595136 -1.2396452 -1.2368352 -1.5753396 -2.0009871 -2.3810925 -2.7667246 -3.0474236 -3.0214887 -2.8692112][-2.8089986 -2.2829237 -1.7852321 -1.3364615 -0.98518348 -0.74602461 -0.6357379 -0.80965424 -1.2640738 -1.734236 -2.0744729 -2.388171 -2.5926449 -2.5252805 -2.4078112][-2.6034214 -2.2254634 -1.9126287 -1.6878829 -1.6145096 -1.6625345 -1.7503486 -1.8630497 -1.9867966 -2.0263348 -1.9739707 -2.009666 -2.0637786 -1.9653196 -1.9043527][-2.7239456 -2.6155396 -2.5215592 -2.4797583 -2.5372479 -2.6191621 -2.6577723 -2.619226 -2.4995933 -2.3136332 -2.1141486 -2.0574055 -2.0900137 -2.0064216 -1.9645441][-3.372241 -3.430949 -3.393877 -3.3038421 -3.2089643 -3.0201964 -2.7566085 -2.4996362 -2.2910564 -2.161551 -2.1105711 -2.2026668 -2.3956497 -2.4056218 -2.3704481][-3.9986351 -4.0577121 -3.9972162 -3.849402 -3.6693778 -3.3844094 -3.0294464 -2.735364 -2.538348 -2.4549279 -2.4663908 -2.5918787 -2.8113167 -2.8493342 -2.808326][-4.177022 -4.1866717 -4.1229706 -4.0091758 -3.9007235 -3.7650623 -3.5961485 -3.4573598 -3.3397551 -3.2194657 -3.108223 -3.0503273 -3.0575695 -2.9702392 -2.8585324][-4.0671582 -4.0954146 -4.0935144 -4.0746951 -4.0739985 -4.0811343 -4.064374 -4.0226541 -3.9289346 -3.7301364 -3.4896028 -3.2982521 -3.1432178 -2.9663894 -2.795064]]...]
INFO - root - 2017-12-07 08:21:16.572810: step 20210, loss = 0.85, batch loss = 0.78 (10.8 examples/sec; 0.738 sec/batch; 64h:00m:33s remains)
INFO - root - 2017-12-07 08:21:24.227201: step 20220, loss = 0.83, batch loss = 0.75 (10.3 examples/sec; 0.779 sec/batch; 67h:36m:24s remains)
INFO - root - 2017-12-07 08:21:31.928632: step 20230, loss = 0.65, batch loss = 0.58 (10.9 examples/sec; 0.736 sec/batch; 63h:48m:44s remains)
INFO - root - 2017-12-07 08:21:39.359207: step 20240, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.785 sec/batch; 68h:06m:53s remains)
INFO - root - 2017-12-07 08:21:47.094706: step 20250, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.786 sec/batch; 68h:09m:57s remains)
INFO - root - 2017-12-07 08:21:54.821428: step 20260, loss = 0.91, batch loss = 0.83 (10.5 examples/sec; 0.763 sec/batch; 66h:08m:24s remains)
INFO - root - 2017-12-07 08:22:02.481404: step 20270, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.759 sec/batch; 65h:48m:55s remains)
INFO - root - 2017-12-07 08:22:10.226551: step 20280, loss = 0.90, batch loss = 0.83 (9.9 examples/sec; 0.807 sec/batch; 69h:58m:35s remains)
INFO - root - 2017-12-07 08:22:18.047040: step 20290, loss = 0.71, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 67h:37m:42s remains)
INFO - root - 2017-12-07 08:22:25.793718: step 20300, loss = 1.13, batch loss = 1.05 (10.6 examples/sec; 0.757 sec/batch; 65h:38m:11s remains)
2017-12-07 08:22:26.384832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3389964 -3.3294373 -3.3079011 -3.306541 -3.4042187 -3.5804651 -3.5810823 -3.4663153 -3.475492 -3.5880489 -3.7777035 -3.9394271 -3.8613319 -3.6362796 -3.4824739][-3.5310578 -3.5123463 -3.4247184 -3.3002069 -3.392087 -3.6807227 -3.7157984 -3.5470948 -3.3941579 -3.2648134 -3.2841 -3.4283268 -3.4485548 -3.3036239 -3.2308669][-3.6936111 -3.6615307 -3.4640336 -3.1378362 -3.1067758 -3.3955243 -3.5376558 -3.5386703 -3.4758108 -3.2299211 -3.007767 -2.931381 -2.9043965 -2.7726226 -2.7406163][-3.6972187 -3.6832895 -3.384347 -2.872613 -2.6724834 -2.868412 -3.0500677 -3.2103865 -3.3159018 -3.1473527 -2.9177241 -2.792531 -2.7291777 -2.5662937 -2.4414537][-3.3357704 -3.3871825 -3.1301634 -2.5467196 -2.166548 -2.2148821 -2.4368896 -2.8259134 -3.2348809 -3.3096275 -3.1672678 -2.9439855 -2.6403713 -2.289319 -1.9657013][-3.1381769 -3.1772313 -2.9827366 -2.4493637 -1.9256244 -1.7028849 -1.8028042 -2.2519057 -2.8963783 -3.3865428 -3.5755937 -3.4470792 -2.9912167 -2.4623003 -1.9124103][-3.2779479 -3.3213551 -3.1557045 -2.6758742 -1.9560289 -1.3466411 -1.1062262 -1.2662807 -1.8961434 -2.839869 -3.5698905 -3.8489337 -3.6379414 -3.2635186 -2.7288046][-3.1990101 -3.2078295 -3.0648179 -2.6407676 -1.7967269 -0.880013 -0.20569563 0.22021723 -0.11569309 -1.3037403 -2.5005651 -3.2959051 -3.6315279 -3.6919918 -3.4458392][-3.227901 -3.2480989 -3.1767192 -2.8396649 -2.02932 -1.0856464 -0.14788485 0.82347918 1.0638838 0.21426153 -0.96878386 -2.0940118 -3.0080757 -3.5332687 -3.5632184][-3.1708946 -3.4047341 -3.58837 -3.4829559 -2.9599841 -2.3014491 -1.4703479 -0.35580063 0.40858173 0.41677237 -0.14754009 -1.1566062 -2.3430789 -3.2262444 -3.5510087][-2.6517224 -3.0324039 -3.5237284 -3.7491496 -3.6313362 -3.3296123 -2.71306 -1.7621663 -0.87444758 -0.18175507 0.049893379 -0.42636395 -1.4217505 -2.474822 -3.2510276][-2.388145 -2.5561543 -2.9745393 -3.3871241 -3.6718361 -3.760396 -3.4761825 -2.880446 -2.2169483 -1.4195106 -0.7558949 -0.62150645 -0.96957254 -1.6882622 -2.6047091][-2.6426592 -2.4951878 -2.4820466 -2.6753087 -3.04774 -3.3514986 -3.4238043 -3.3362796 -3.17344 -2.7503014 -2.2552247 -1.9599431 -1.8326435 -1.9573109 -2.4726753][-2.8738496 -2.6946063 -2.4410379 -2.3396342 -2.470618 -2.5963874 -2.6984987 -2.9049497 -3.2034798 -3.3449285 -3.3059874 -3.2094064 -2.9998097 -2.7583528 -2.805172][-2.7929535 -2.7400658 -2.5186682 -2.4143608 -2.5227094 -2.5194993 -2.4533014 -2.5629272 -2.8814569 -3.2263913 -3.4572423 -3.5740232 -3.4907231 -3.2076564 -2.9928436]]...]
INFO - root - 2017-12-07 08:22:34.102166: step 20310, loss = 0.91, batch loss = 0.84 (10.3 examples/sec; 0.776 sec/batch; 67h:19m:17s remains)
INFO - root - 2017-12-07 08:22:41.817912: step 20320, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.772 sec/batch; 66h:55m:56s remains)
INFO - root - 2017-12-07 08:22:49.463846: step 20330, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.777 sec/batch; 67h:24m:49s remains)
INFO - root - 2017-12-07 08:22:57.012187: step 20340, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.764 sec/batch; 66h:14m:41s remains)
INFO - root - 2017-12-07 08:23:04.686712: step 20350, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.774 sec/batch; 67h:06m:16s remains)
INFO - root - 2017-12-07 08:23:12.383135: step 20360, loss = 0.83, batch loss = 0.75 (10.4 examples/sec; 0.770 sec/batch; 66h:44m:24s remains)
INFO - root - 2017-12-07 08:23:19.945371: step 20370, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.760 sec/batch; 65h:52m:54s remains)
INFO - root - 2017-12-07 08:23:27.718165: step 20380, loss = 0.81, batch loss = 0.73 (10.8 examples/sec; 0.740 sec/batch; 64h:10m:16s remains)
INFO - root - 2017-12-07 08:23:35.268529: step 20390, loss = 0.66, batch loss = 0.59 (10.8 examples/sec; 0.741 sec/batch; 64h:16m:49s remains)
INFO - root - 2017-12-07 08:23:42.877046: step 20400, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.758 sec/batch; 65h:42m:28s remains)
2017-12-07 08:23:43.475334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6446121 -1.7789962 -2.0575082 -2.4828792 -3.0798488 -3.729207 -4.2289577 -4.4430285 -4.4827709 -4.4566011 -4.2936053 -4.1099486 -3.968703 -3.7807713 -3.6709223][-1.7393909 -1.8890319 -2.1122775 -2.4614463 -3.0131426 -3.6210415 -4.0885634 -4.3089371 -4.3856053 -4.4077239 -4.2935228 -4.1173182 -3.9349065 -3.6751757 -3.4774904][-1.76684 -1.9239225 -2.1194329 -2.42188 -2.9232693 -3.4694209 -3.8741336 -4.0768843 -4.1764946 -4.2263241 -4.1413307 -3.9649451 -3.7473483 -3.4272933 -3.1562729][-1.8435144 -1.9822388 -2.1526165 -2.4184749 -2.838932 -3.2780788 -3.5911093 -3.7640338 -3.8656943 -3.8927348 -3.8055048 -3.6528859 -3.475188 -3.2007833 -2.9526734][-2.1668108 -2.3092668 -2.4336534 -2.6136603 -2.8858995 -3.1642728 -3.3531613 -3.4700229 -3.5551009 -3.5294652 -3.4109008 -3.2867165 -3.1707823 -2.9995761 -2.8361709][-2.3934548 -2.5505638 -2.6516974 -2.783493 -2.9466317 -3.0917706 -3.1471481 -3.1625571 -3.1858668 -3.116112 -3.0172513 -2.9676232 -2.9318931 -2.8548791 -2.7489972][-2.2220521 -2.3633077 -2.4693422 -2.6244943 -2.7760968 -2.8678446 -2.8546743 -2.8067517 -2.7770369 -2.6903331 -2.645792 -2.6938846 -2.7278924 -2.6945493 -2.5916827][-1.8269222 -1.9244175 -2.0343869 -2.2196231 -2.411324 -2.5538888 -2.6196103 -2.6503866 -2.6659374 -2.5992384 -2.5611825 -2.5888333 -2.5608912 -2.4533725 -2.2831335][-1.520627 -1.5839016 -1.6873662 -1.8570251 -2.0345466 -2.1983011 -2.3480589 -2.4837308 -2.5779743 -2.5618832 -2.5237286 -2.4936652 -2.3628352 -2.1532631 -1.93187][-1.311595 -1.3280802 -1.4053748 -1.5216691 -1.6197796 -1.7315173 -1.8871589 -2.0565183 -2.19457 -2.2383294 -2.2463217 -2.2256374 -2.0717337 -1.8366468 -1.6158957][-1.173156 -1.1196477 -1.1520901 -1.2149734 -1.2457311 -1.300992 -1.4207292 -1.5536356 -1.666045 -1.7280653 -1.7793865 -1.7923944 -1.6707742 -1.4724829 -1.2906544][-1.1131587 -0.99991322 -0.9776845 -0.98112774 -0.9562068 -0.9605701 -1.0329778 -1.1113837 -1.1742632 -1.2199373 -1.2786798 -1.3150697 -1.2367158 -1.091928 -0.95725513][-1.0349989 -0.90858722 -0.85058808 -0.8094461 -0.75595284 -0.73911309 -0.77996945 -0.81727934 -0.84488845 -0.87437344 -0.92387629 -0.95432711 -0.89346862 -0.78384209 -0.68661237][-0.96035194 -0.84647083 -0.77050829 -0.70590591 -0.64311743 -0.61460829 -0.61515212 -0.605953 -0.59639955 -0.59925628 -0.63128686 -0.65273166 -0.60373616 -0.518497 -0.4578948][-0.92920017 -0.83133316 -0.742779 -0.65668535 -0.57812858 -0.52658868 -0.47633553 -0.41214037 -0.359375 -0.33476925 -0.35337162 -0.38045979 -0.35732079 -0.30054379 -0.27651215]]...]
INFO - root - 2017-12-07 08:23:51.212431: step 20410, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 67h:29m:20s remains)
INFO - root - 2017-12-07 08:23:58.825313: step 20420, loss = 1.01, batch loss = 0.94 (10.2 examples/sec; 0.788 sec/batch; 68h:18m:04s remains)
INFO - root - 2017-12-07 08:24:06.705218: step 20430, loss = 0.81, batch loss = 0.74 (10.3 examples/sec; 0.779 sec/batch; 67h:31m:37s remains)
INFO - root - 2017-12-07 08:24:14.164893: step 20440, loss = 0.64, batch loss = 0.57 (10.4 examples/sec; 0.767 sec/batch; 66h:29m:20s remains)
INFO - root - 2017-12-07 08:24:21.748259: step 20450, loss = 0.61, batch loss = 0.54 (10.7 examples/sec; 0.746 sec/batch; 64h:40m:06s remains)
INFO - root - 2017-12-07 08:24:29.441589: step 20460, loss = 0.69, batch loss = 0.62 (11.0 examples/sec; 0.730 sec/batch; 63h:16m:52s remains)
INFO - root - 2017-12-07 08:24:37.048031: step 20470, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.755 sec/batch; 65h:28m:32s remains)
INFO - root - 2017-12-07 08:24:44.726595: step 20480, loss = 0.98, batch loss = 0.91 (10.5 examples/sec; 0.761 sec/batch; 65h:57m:19s remains)
INFO - root - 2017-12-07 08:24:52.453832: step 20490, loss = 0.99, batch loss = 0.92 (10.0 examples/sec; 0.797 sec/batch; 69h:06m:37s remains)
INFO - root - 2017-12-07 08:25:00.200898: step 20500, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.753 sec/batch; 65h:15m:54s remains)
2017-12-07 08:25:00.808386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.58705688 -0.977762 -1.1300445 -1.5661373 -1.4546733 -0.875649 -0.47614384 -0.14173841 -0.39069271 -0.90337396 -1.2976644 -1.3989742 -1.5107918 -1.4605589 -1.0440955][-0.881799 -1.1305599 -1.3348498 -1.79562 -1.8042827 -1.3153386 -0.89802313 -0.40082645 -0.50368237 -0.94945455 -1.2428567 -1.3593104 -1.5332386 -1.7431471 -1.5773318][-0.92293286 -1.0194089 -1.2412827 -1.6591461 -1.7260203 -1.3965065 -1.1882043 -0.7721858 -0.80686641 -1.0876937 -1.1708868 -1.258426 -1.4531658 -1.8881013 -1.9314954][-1.0361605 -1.1081846 -1.2734091 -1.4117858 -1.2657526 -0.95584893 -0.9642663 -0.7579 -0.78692007 -0.85928917 -0.69191432 -0.69431591 -0.901371 -1.53016 -1.7907412][-1.4891167 -1.5654891 -1.531713 -1.2207794 -0.68873405 -0.28505468 -0.48196054 -0.64836574 -0.78568077 -0.64549255 -0.164258 0.063111305 -0.14475679 -0.93462086 -1.4064834][-1.8463664 -1.9957364 -1.8335354 -1.2307014 -0.40048552 0.19664192 0.033698082 -0.51161957 -0.982718 -0.92071605 -0.33804178 0.10969162 -0.047849178 -0.84962964 -1.4351945][-1.8027909 -2.1220169 -2.0368509 -1.3687413 -0.38106108 0.525249 0.76908255 0.064834595 -0.88257 -1.309041 -0.98332214 -0.50987029 -0.57479119 -1.2033532 -1.7358406][-1.9800153 -2.218137 -2.0711215 -1.351846 -0.36025953 0.81344891 1.6861887 1.0958128 -0.24204254 -1.2783084 -1.347301 -0.88925076 -0.74870133 -1.0732381 -1.501704][-2.2602444 -2.1962488 -1.8673942 -1.1390042 -0.44017935 0.58924866 1.7780089 1.5690498 0.32093048 -0.99147582 -1.2972028 -0.86767387 -0.54883003 -0.676548 -1.1966929][-2.2013166 -2.0366228 -1.6170814 -1.0313926 -0.79512811 -0.32012892 0.61505127 0.87141275 0.13756752 -1.0089772 -1.3197114 -0.9871223 -0.62793136 -0.72223306 -1.431576][-1.7490098 -1.7398837 -1.3846345 -1.0021226 -1.0986881 -1.0945055 -0.54635215 -0.11390591 -0.53218937 -1.4837756 -1.8010209 -1.6039901 -1.1422734 -1.0362437 -1.61661][-1.18009 -1.2886546 -1.1024406 -0.98340011 -1.2523923 -1.436697 -1.141449 -0.7178309 -1.0092194 -1.7884459 -2.1702566 -2.1027284 -1.532985 -1.1707067 -1.4425764][-0.58710122 -0.73914337 -0.79781651 -0.955549 -1.2850163 -1.4786613 -1.272444 -0.89368224 -1.1197047 -1.6529882 -1.9756205 -1.9726999 -1.4689915 -1.1382616 -1.3240702][-0.22126341 -0.42556477 -0.72450614 -1.0606515 -1.2966843 -1.3754482 -1.2519269 -1.058697 -1.3577235 -1.7149725 -1.8711672 -1.7923102 -1.3359144 -1.0755522 -1.2232556][-0.47956324 -0.4247756 -0.6428299 -1.0013678 -1.1890817 -1.268692 -1.2876449 -1.300885 -1.6548605 -1.9255667 -1.9374914 -1.8045459 -1.460289 -1.2807987 -1.3236771]]...]
INFO - root - 2017-12-07 08:25:08.485634: step 20510, loss = 0.98, batch loss = 0.91 (10.7 examples/sec; 0.745 sec/batch; 64h:36m:25s remains)
INFO - root - 2017-12-07 08:25:16.218623: step 20520, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.788 sec/batch; 68h:17m:28s remains)
INFO - root - 2017-12-07 08:25:24.086847: step 20530, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.784 sec/batch; 67h:54m:00s remains)
INFO - root - 2017-12-07 08:25:31.564368: step 20540, loss = 0.79, batch loss = 0.72 (11.2 examples/sec; 0.717 sec/batch; 62h:09m:53s remains)
INFO - root - 2017-12-07 08:25:39.287055: step 20550, loss = 0.65, batch loss = 0.57 (10.1 examples/sec; 0.789 sec/batch; 68h:19m:45s remains)
INFO - root - 2017-12-07 08:25:46.945255: step 20560, loss = 0.86, batch loss = 0.79 (10.7 examples/sec; 0.751 sec/batch; 65h:03m:45s remains)
INFO - root - 2017-12-07 08:25:54.599819: step 20570, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.773 sec/batch; 66h:59m:44s remains)
INFO - root - 2017-12-07 08:26:02.320095: step 20580, loss = 0.93, batch loss = 0.85 (10.3 examples/sec; 0.779 sec/batch; 67h:30m:35s remains)
INFO - root - 2017-12-07 08:26:09.983111: step 20590, loss = 0.82, batch loss = 0.75 (10.8 examples/sec; 0.738 sec/batch; 63h:58m:49s remains)
INFO - root - 2017-12-07 08:26:17.677004: step 20600, loss = 0.96, batch loss = 0.89 (10.5 examples/sec; 0.765 sec/batch; 66h:15m:38s remains)
2017-12-07 08:26:18.293679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5691123 -3.8696284 -4.0490341 -4.0458541 -3.9493103 -3.8062983 -3.7947292 -3.9443986 -4.0630555 -4.1513467 -4.1782656 -4.0642304 -3.7879658 -3.65885 -3.6807065][-4.3192563 -4.455565 -4.3929768 -4.1910777 -3.9959984 -3.8343558 -3.8105719 -3.927999 -4.0006108 -4.0497684 -4.068182 -3.9980419 -3.8648274 -3.823349 -3.8830757][-4.6991949 -4.7141414 -4.5769181 -4.4201841 -4.3858705 -4.4139814 -4.4031749 -4.378119 -4.28813 -4.1949492 -4.1292853 -4.061799 -4.0078177 -3.963249 -3.9829166][-4.5184269 -4.4424648 -4.3420091 -4.3830614 -4.6566219 -4.969326 -4.9956908 -4.8328333 -4.6372027 -4.4730268 -4.37323 -4.3187881 -4.2581844 -4.0931921 -3.9842489][-3.6578772 -3.530329 -3.5449328 -3.8326702 -4.39048 -4.9176788 -4.9836159 -4.7423754 -4.5452528 -4.4250956 -4.3987937 -4.4011078 -4.308424 -4.0305567 -3.8328431][-2.5197754 -2.3896072 -2.5130143 -2.9395745 -3.5603876 -4.0471416 -4.0777082 -3.8446088 -3.7466922 -3.8115058 -3.9747853 -4.0652823 -3.9720001 -3.6823988 -3.5128422][-1.632077 -1.6478097 -1.9426928 -2.4142146 -2.8688784 -3.0348454 -2.8516507 -2.5617733 -2.5127296 -2.7778089 -3.1967466 -3.4449334 -3.448724 -3.2817588 -3.2366068][-1.5420029 -1.8234401 -2.262516 -2.624321 -2.7732835 -2.5784945 -2.2067604 -1.8988478 -1.8669455 -2.2173424 -2.7308683 -3.0344489 -3.0995092 -3.0517492 -3.1006818][-2.1665442 -2.538835 -2.9189487 -3.0761957 -3.0189393 -2.7393868 -2.4247921 -2.2168827 -2.132988 -2.3197536 -2.6449363 -2.8532329 -2.9354875 -2.9618032 -3.0080528][-2.6202741 -2.9109998 -3.1333284 -3.149498 -3.0915661 -3.0060685 -2.937602 -2.8598232 -2.6941195 -2.6344285 -2.6656661 -2.7105172 -2.7759988 -2.8281631 -2.8403716][-2.4445915 -2.619802 -2.709306 -2.6747375 -2.74233 -2.9242525 -3.1029639 -3.1548979 -2.9891241 -2.8054051 -2.6614633 -2.5549159 -2.5240521 -2.5222726 -2.5026455][-2.0099618 -2.1071467 -2.1266868 -2.0952048 -2.2435551 -2.5424066 -2.8341198 -2.9265852 -2.7795677 -2.5838718 -2.4297633 -2.2820032 -2.1604013 -2.0947709 -2.0563469][-1.773021 -1.8638885 -1.8819702 -1.8618171 -2.0125613 -2.2547274 -2.4808598 -2.4807873 -2.2971346 -2.12797 -2.0271907 -1.9406252 -1.8515708 -1.8194959 -1.8128586][-1.6561086 -1.7587721 -1.8021262 -1.8108182 -1.9271266 -2.0729251 -2.2006834 -2.1049037 -1.8847694 -1.7558241 -1.72874 -1.7628191 -1.7846019 -1.8550086 -1.9180908][-1.6331673 -1.7055819 -1.7280951 -1.7388783 -1.8087897 -1.8961232 -1.9667988 -1.8629704 -1.6909661 -1.6203663 -1.65295 -1.768343 -1.8696008 -2.0320184 -2.1518507]]...]
INFO - root - 2017-12-07 08:26:25.841952: step 20610, loss = 0.93, batch loss = 0.86 (10.7 examples/sec; 0.748 sec/batch; 64h:47m:14s remains)
INFO - root - 2017-12-07 08:26:33.555519: step 20620, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.766 sec/batch; 66h:19m:14s remains)
INFO - root - 2017-12-07 08:26:41.397006: step 20630, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.764 sec/batch; 66h:08m:58s remains)
INFO - root - 2017-12-07 08:26:48.779344: step 20640, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.780 sec/batch; 67h:33m:07s remains)
INFO - root - 2017-12-07 08:26:56.358379: step 20650, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.766 sec/batch; 66h:22m:12s remains)
INFO - root - 2017-12-07 08:27:03.915070: step 20660, loss = 1.03, batch loss = 0.96 (10.6 examples/sec; 0.753 sec/batch; 65h:14m:29s remains)
INFO - root - 2017-12-07 08:27:11.475861: step 20670, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.757 sec/batch; 65h:36m:22s remains)
INFO - root - 2017-12-07 08:27:19.133382: step 20680, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.768 sec/batch; 66h:29m:34s remains)
INFO - root - 2017-12-07 08:27:26.806589: step 20690, loss = 0.70, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 65h:54m:14s remains)
INFO - root - 2017-12-07 08:27:34.339102: step 20700, loss = 0.75, batch loss = 0.68 (10.9 examples/sec; 0.733 sec/batch; 63h:27m:40s remains)
2017-12-07 08:27:34.949587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7805314 -2.7421985 -2.7501693 -2.7535007 -2.9305263 -3.1676178 -3.3919358 -3.6159062 -3.6975939 -3.6369689 -3.5700369 -3.4930818 -3.4667549 -3.56174 -3.6156042][-2.9409652 -2.8321905 -2.8005962 -2.7519755 -2.8710835 -3.1375856 -3.4087591 -3.6500025 -3.7698717 -3.7514296 -3.7423944 -3.7073984 -3.6225147 -3.6111317 -3.6273341][-3.160347 -2.9857221 -2.8578784 -2.6542649 -2.6041503 -2.79947 -3.0351846 -3.2502913 -3.4128854 -3.4817336 -3.6059868 -3.6780238 -3.5881472 -3.4959219 -3.4469891][-3.2472596 -2.9443779 -2.6147966 -2.1349773 -1.8737392 -2.0329437 -2.2680476 -2.4573541 -2.6653636 -2.842752 -3.169209 -3.4188347 -3.4055057 -3.3169279 -3.244103][-3.2002335 -2.6881862 -2.0251825 -1.1805825 -0.73910952 -0.923445 -1.2361453 -1.4888802 -1.8145447 -2.1772351 -2.7579436 -3.1901045 -3.290287 -3.290153 -3.2285962][-3.1697397 -2.508405 -1.5713065 -0.42849922 0.17176342 -0.0034065247 -0.3633585 -0.75042605 -1.2639265 -1.8693841 -2.7025366 -3.252964 -3.4233789 -3.5164213 -3.4592366][-3.2364497 -2.6222823 -1.6552019 -0.46989274 0.14371061 0.0450778 -0.24296951 -0.69680214 -1.2674654 -1.9404666 -2.8388 -3.3727512 -3.5451865 -3.7030373 -3.6528602][-3.3444953 -2.9099736 -2.1314015 -1.1503084 -0.70151329 -0.78352284 -1.0004659 -1.4022956 -1.7697539 -2.1831338 -2.8644094 -3.2422342 -3.3597436 -3.5580168 -3.5680704][-3.4184761 -3.1512868 -2.6279454 -1.933424 -1.6968117 -1.8557277 -2.1114235 -2.4363055 -2.5229456 -2.5933318 -2.9276552 -3.0623345 -3.074101 -3.2545505 -3.3182187][-3.5070305 -3.3513155 -3.0293174 -2.559761 -2.4548886 -2.671093 -2.9569018 -3.2131262 -3.1506696 -3.0543947 -3.1427333 -3.1128061 -3.061482 -3.2010121 -3.2755351][-3.6414149 -3.572516 -3.4162769 -3.0891283 -2.995223 -3.1577997 -3.3490491 -3.5239952 -3.4975734 -3.454989 -3.4620228 -3.3796544 -3.3152843 -3.3998122 -3.4430654][-3.7201657 -3.7420111 -3.7678051 -3.5835545 -3.4722738 -3.5356958 -3.5722773 -3.6701055 -3.7588222 -3.8182626 -3.820796 -3.7547436 -3.6890724 -3.6547837 -3.5705612][-3.6904955 -3.7515733 -3.9239724 -3.8777146 -3.7682481 -3.7575843 -3.6798186 -3.7264285 -3.9077871 -4.0353055 -4.0602331 -4.0426846 -3.9865544 -3.8649845 -3.6880665][-3.5574849 -3.583755 -3.8142977 -3.8570163 -3.7630649 -3.7418442 -3.61267 -3.609827 -3.8010879 -3.9113293 -3.9117122 -3.8972988 -3.8380365 -3.6957762 -3.5190904][-3.4027944 -3.3607554 -3.5930252 -3.6850657 -3.6139143 -3.6178451 -3.47562 -3.4120924 -3.5459089 -3.5894372 -3.5271726 -3.460947 -3.3423376 -3.1715651 -3.0036683]]...]
INFO - root - 2017-12-07 08:27:42.663656: step 20710, loss = 0.98, batch loss = 0.90 (10.6 examples/sec; 0.756 sec/batch; 65h:27m:22s remains)
INFO - root - 2017-12-07 08:27:50.246943: step 20720, loss = 0.66, batch loss = 0.58 (10.6 examples/sec; 0.753 sec/batch; 65h:11m:38s remains)
INFO - root - 2017-12-07 08:27:57.892309: step 20730, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.768 sec/batch; 66h:29m:53s remains)
INFO - root - 2017-12-07 08:28:05.414765: step 20740, loss = 0.90, batch loss = 0.83 (10.1 examples/sec; 0.794 sec/batch; 68h:47m:50s remains)
INFO - root - 2017-12-07 08:28:13.114164: step 20750, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.777 sec/batch; 67h:16m:23s remains)
INFO - root - 2017-12-07 08:28:20.818290: step 20760, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 66h:07m:09s remains)
INFO - root - 2017-12-07 08:28:28.434829: step 20770, loss = 0.81, batch loss = 0.73 (10.6 examples/sec; 0.757 sec/batch; 65h:32m:52s remains)
INFO - root - 2017-12-07 08:28:36.065650: step 20780, loss = 0.92, batch loss = 0.85 (10.9 examples/sec; 0.735 sec/batch; 63h:38m:07s remains)
INFO - root - 2017-12-07 08:28:43.669692: step 20790, loss = 0.96, batch loss = 0.89 (10.6 examples/sec; 0.754 sec/batch; 65h:16m:21s remains)
INFO - root - 2017-12-07 08:28:51.337674: step 20800, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.758 sec/batch; 65h:39m:23s remains)
2017-12-07 08:28:52.000632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7555194 -3.5833886 -3.3818169 -3.2213483 -3.1591146 -3.2540107 -3.5017381 -3.6448903 -3.6145349 -3.67809 -3.866236 -3.9749639 -4.02896 -4.0725546 -4.0669565][-3.7848909 -3.618001 -3.4439173 -3.3370376 -3.33177 -3.5113821 -3.8152046 -3.9937637 -3.9950795 -4.0069351 -4.1025066 -4.1914673 -4.2622256 -4.2595291 -4.168118][-3.3331456 -3.1907308 -3.0895815 -3.0341156 -2.9849966 -3.132988 -3.5011168 -3.8600173 -4.0481539 -4.1314464 -4.1950588 -4.2676387 -4.3370113 -4.3335795 -4.2560153][-2.8769557 -2.7772634 -2.7502422 -2.6765146 -2.4465771 -2.3805757 -2.6874108 -3.1982012 -3.6161678 -3.9018214 -4.0998716 -4.2350059 -4.3125906 -4.3225484 -4.315516][-2.9752746 -2.8840656 -2.8606067 -2.7462859 -2.3740208 -2.0939953 -2.2365911 -2.7157381 -3.2095025 -3.711437 -4.1353555 -4.3310442 -4.3617482 -4.2896314 -4.2269392][-3.1077309 -2.9985595 -2.8892097 -2.6084766 -2.0455937 -1.5087893 -1.3326292 -1.5030456 -1.8609281 -2.6118498 -3.4298344 -3.8755224 -4.08421 -4.1496468 -4.1539927][-3.0667708 -3.0258212 -2.8880343 -2.4538198 -1.6975474 -0.89781404 -0.33986855 -0.064052105 -0.16199398 -1.0480657 -2.1416085 -2.7461417 -3.1489129 -3.4038982 -3.4951477][-2.9341261 -3.0068731 -3.0101981 -2.6007934 -1.8716068 -1.0732536 -0.40252924 0.14202547 0.28484631 -0.37434721 -1.1937068 -1.6254022 -2.0802007 -2.439167 -2.5531707][-2.6846519 -2.7821484 -2.9294443 -2.6752186 -2.2012441 -1.7133608 -1.2476351 -0.717628 -0.3843379 -0.53607059 -0.79167128 -0.95916748 -1.4789422 -1.9681714 -2.1645069][-2.756278 -2.7426243 -2.8185978 -2.6404321 -2.4289258 -2.3701148 -2.2501912 -1.8656983 -1.4428368 -1.2033575 -0.96165872 -0.78052425 -1.048007 -1.280257 -1.2812536][-3.0128717 -2.6817083 -2.3901148 -2.0668263 -2.0099754 -2.3015516 -2.5661211 -2.4945223 -2.2113025 -1.8859704 -1.4618289 -1.0871944 -1.0746653 -1.0070393 -0.79479527][-3.1077371 -2.4730492 -1.8704579 -1.4508748 -1.5309203 -2.058008 -2.6074333 -2.8471954 -2.8083663 -2.5905733 -2.2483904 -1.9308469 -1.8552589 -1.7183919 -1.496465][-3.2859292 -2.5308867 -1.7671683 -1.3187635 -1.4749026 -2.0473657 -2.6608877 -3.0795174 -3.2943597 -3.2946455 -3.1087213 -2.8102622 -2.558486 -2.2102625 -1.8732691][-3.3484778 -2.7316098 -2.0368962 -1.5826335 -1.6184034 -1.9729035 -2.3933837 -2.8001523 -3.141993 -3.3373098 -3.3340492 -3.1075044 -2.7498908 -2.253875 -1.7721541][-3.3606355 -2.9824076 -2.3943353 -1.8969257 -1.7308342 -1.7965641 -1.979522 -2.2772942 -2.5980546 -2.7670448 -2.7985687 -2.6490674 -2.3021598 -1.8126392 -1.325031]]...]
INFO - root - 2017-12-07 08:28:59.788053: step 20810, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.755 sec/batch; 65h:19m:55s remains)
INFO - root - 2017-12-07 08:29:07.546107: step 20820, loss = 0.65, batch loss = 0.58 (10.2 examples/sec; 0.787 sec/batch; 68h:07m:39s remains)
INFO - root - 2017-12-07 08:29:15.409116: step 20830, loss = 0.59, batch loss = 0.52 (9.8 examples/sec; 0.816 sec/batch; 70h:37m:58s remains)
INFO - root - 2017-12-07 08:29:22.857155: step 20840, loss = 0.70, batch loss = 0.62 (10.4 examples/sec; 0.766 sec/batch; 66h:18m:34s remains)
INFO - root - 2017-12-07 08:29:30.621872: step 20850, loss = 0.62, batch loss = 0.54 (10.3 examples/sec; 0.777 sec/batch; 67h:14m:02s remains)
INFO - root - 2017-12-07 08:29:38.215142: step 20860, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.758 sec/batch; 65h:38m:34s remains)
INFO - root - 2017-12-07 08:29:45.912262: step 20870, loss = 0.57, batch loss = 0.49 (10.8 examples/sec; 0.738 sec/batch; 63h:54m:00s remains)
INFO - root - 2017-12-07 08:29:53.598594: step 20880, loss = 0.58, batch loss = 0.51 (10.5 examples/sec; 0.761 sec/batch; 65h:54m:13s remains)
INFO - root - 2017-12-07 08:30:01.404906: step 20890, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.772 sec/batch; 66h:47m:18s remains)
INFO - root - 2017-12-07 08:30:09.047210: step 20900, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.777 sec/batch; 67h:12m:38s remains)
2017-12-07 08:30:09.667336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.20262718 -0.97479963 -1.6314087 -1.9281895 -1.9362991 -1.8511097 -1.6682706 -1.4837019 -1.3754005 -1.2677279 -1.1491833 -1.0285194 -0.97443676 -0.95401478 -0.91679168][-0.041509151 -0.6891818 -1.2680182 -1.4987214 -1.5161414 -1.5204897 -1.4272738 -1.2914865 -1.2015207 -1.0816648 -0.88996625 -0.68613529 -0.56633592 -0.49416232 -0.43228865][-0.59025979 -0.9594326 -1.2852237 -1.3358634 -1.2611961 -1.2565463 -1.206805 -1.1295328 -1.089736 -0.99416542 -0.78854585 -0.55998993 -0.38647175 -0.22493172 -0.060823441][-1.1400955 -1.2530684 -1.3377151 -1.2685223 -1.1797786 -1.2496262 -1.3400328 -1.3914499 -1.418967 -1.3186991 -1.0657039 -0.79648304 -0.57401133 -0.32805586 -0.043435574][-1.4141352 -1.266644 -1.0935867 -0.88490987 -0.76829815 -0.90731597 -1.1727092 -1.426451 -1.6280921 -1.6366951 -1.4899168 -1.3363109 -1.18736 -0.92998958 -0.54424191][-1.4245834 -1.0610275 -0.64858055 -0.2400527 -0.0040392876 -0.12628555 -0.47936082 -0.85220623 -1.1662765 -1.2848599 -1.3301139 -1.4410818 -1.5235274 -1.4121578 -1.1195407][-1.3419256 -0.84860039 -0.23859882 0.43426847 0.92992115 0.96511793 0.65533495 0.19956493 -0.22112608 -0.424819 -0.64041281 -1.0404739 -1.3803635 -1.423279 -1.2531457][-1.4722116 -0.986686 -0.28842306 0.61395788 1.4204054 1.7371521 1.6290426 1.2026033 0.6939249 0.39017582 0.027604103 -0.57848597 -1.0867722 -1.1935344 -1.0426798][-1.7844915 -1.451226 -0.8796134 -0.044010162 0.77754164 1.2213917 1.3260403 1.1125088 0.71410418 0.46387863 0.11961269 -0.49865532 -1.0224655 -1.0917795 -0.8831389][-2.1107483 -1.9565349 -1.5969386 -1.0087664 -0.39617062 0.005718708 0.22026157 0.21239424 -0.010290146 -0.18035173 -0.43284774 -0.88685822 -1.2499743 -1.2159002 -0.9473269][-2.3234317 -2.3112843 -2.1582904 -1.8437903 -1.4857502 -1.1803582 -0.92623806 -0.79222083 -0.8587873 -0.97120428 -1.1618476 -1.4629519 -1.6810389 -1.6138859 -1.3813734][-2.3819864 -2.4219816 -2.3830776 -2.2597532 -2.0942926 -1.8965979 -1.6814704 -1.5245695 -1.5315886 -1.6248012 -1.7756047 -1.955735 -2.0702088 -2.0262268 -1.8995492][-2.3689828 -2.402462 -2.3999119 -2.3653452 -2.2981117 -2.1795948 -2.028183 -1.9034309 -1.8858206 -1.9596848 -2.0805569 -2.1904581 -2.2496426 -2.2285745 -2.1848745][-2.3392441 -2.3676262 -2.3848741 -2.3900728 -2.3708832 -2.3065898 -2.2147768 -2.1320739 -2.1054308 -2.1423142 -2.2125065 -2.2697537 -2.2931027 -2.2796042 -2.2800472][-2.2926605 -2.3032033 -2.3247612 -2.3568935 -2.3769717 -2.3635657 -2.3287683 -2.2876668 -2.2640057 -2.2721074 -2.2984378 -2.3150651 -2.3080471 -2.2854555 -2.2955897]]...]
INFO - root - 2017-12-07 08:30:17.380979: step 20910, loss = 0.63, batch loss = 0.56 (10.5 examples/sec; 0.765 sec/batch; 66h:12m:51s remains)
INFO - root - 2017-12-07 08:30:25.129365: step 20920, loss = 1.01, batch loss = 0.94 (10.2 examples/sec; 0.784 sec/batch; 67h:51m:10s remains)
INFO - root - 2017-12-07 08:30:32.822014: step 20930, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.759 sec/batch; 65h:41m:01s remains)
INFO - root - 2017-12-07 08:30:40.260374: step 20940, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.779 sec/batch; 67h:24m:44s remains)
INFO - root - 2017-12-07 08:30:47.913222: step 20950, loss = 0.83, batch loss = 0.76 (10.7 examples/sec; 0.751 sec/batch; 65h:00m:00s remains)
INFO - root - 2017-12-07 08:30:55.592018: step 20960, loss = 1.04, batch loss = 0.97 (10.2 examples/sec; 0.787 sec/batch; 68h:04m:59s remains)
INFO - root - 2017-12-07 08:31:03.277324: step 20970, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.782 sec/batch; 67h:41m:25s remains)
INFO - root - 2017-12-07 08:31:10.961241: step 20980, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.794 sec/batch; 68h:40m:38s remains)
INFO - root - 2017-12-07 08:31:18.689107: step 20990, loss = 0.95, batch loss = 0.88 (10.6 examples/sec; 0.757 sec/batch; 65h:30m:32s remains)
INFO - root - 2017-12-07 08:31:26.417125: step 21000, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.773 sec/batch; 66h:55m:20s remains)
2017-12-07 08:31:27.084029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9759285 -2.7021759 -2.1443827 -2.0075183 -2.315346 -2.4700391 -2.6750236 -2.8128493 -2.7228475 -2.8244596 -2.9645152 -3.0748665 -3.3477042 -3.3825796 -3.2620127][-2.8897309 -2.6296964 -2.1226766 -2.0120785 -2.2940788 -2.3969524 -2.5409231 -2.6262074 -2.4625378 -2.4529772 -2.589406 -2.7883205 -3.1085198 -3.203526 -3.1887164][-2.8229053 -2.5935359 -2.1703582 -2.0865812 -2.3110466 -2.3316498 -2.3749659 -2.3952014 -2.1626427 -2.0431862 -2.2218957 -2.5706825 -2.9560251 -3.1108258 -3.1668911][-2.7902732 -2.614558 -2.3115847 -2.2775123 -2.46847 -2.4286482 -2.3636675 -2.3085711 -1.9999592 -1.7644351 -1.9993396 -2.509654 -2.9631581 -3.1769557 -3.2623384][-2.7982826 -2.6833282 -2.5024095 -2.5277517 -2.707284 -2.6331959 -2.4748683 -2.333755 -1.9554174 -1.6316452 -1.9245439 -2.5628171 -3.0561988 -3.314466 -3.4134264][-2.8394985 -2.7768192 -2.6901894 -2.7574515 -2.9306726 -2.8362513 -2.5996408 -2.3795838 -1.9594946 -1.5920184 -1.9000871 -2.5665803 -3.0785015 -3.3889573 -3.507555][-2.8847759 -2.8486316 -2.8103509 -2.8913205 -3.063587 -2.9727664 -2.6796894 -2.3713157 -1.9035339 -1.5232708 -1.8200803 -2.4636564 -3.0218015 -3.4030447 -3.5311697][-2.9136939 -2.8661723 -2.8255906 -2.9073248 -3.1235929 -3.1022933 -2.8126707 -2.4380438 -1.9371071 -1.5904653 -1.8588724 -2.4302597 -3.0087285 -3.4201069 -3.5089521][-2.913599 -2.8257198 -2.7444024 -2.830864 -3.1508763 -3.2683573 -3.0656111 -2.7017341 -2.2290595 -1.9507389 -2.1572497 -2.5824559 -3.0922785 -3.4665794 -3.4866867][-2.8996937 -2.7620027 -2.6178341 -2.7080441 -3.1279221 -3.3834829 -3.3103852 -3.0194559 -2.618367 -2.4192209 -2.5593464 -2.8137648 -3.1828361 -3.4677808 -3.4390252][-2.8811026 -2.6931481 -2.4757214 -2.5549679 -3.0308497 -3.3746004 -3.3990037 -3.1683655 -2.8328526 -2.7181005 -2.8414154 -2.9800262 -3.2124467 -3.4188929 -3.4016798][-2.8846965 -2.6566658 -2.3803847 -2.4318976 -2.886786 -3.2447662 -3.3185098 -3.123055 -2.8507013 -2.8368552 -2.9915204 -3.085217 -3.2273626 -3.3775644 -3.3947015][-2.906754 -2.6519594 -2.3416111 -2.3508 -2.7201281 -3.0401201 -3.1410656 -2.9839468 -2.8003764 -2.8999228 -3.0784497 -3.1394796 -3.2102525 -3.3059821 -3.3372006][-2.9566689 -2.6986613 -2.3846409 -2.3527567 -2.6125231 -2.8755422 -2.9870603 -2.8708556 -2.7895107 -2.9793911 -3.151165 -3.1845441 -3.1945 -3.2272577 -3.2551031][-3.0211542 -2.7785282 -2.4827905 -2.419054 -2.594243 -2.8155303 -2.9353337 -2.8571 -2.8678057 -3.0839562 -3.208024 -3.2237811 -3.2084455 -3.2093997 -3.2548108]]...]
INFO - root - 2017-12-07 08:31:34.650472: step 21010, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.757 sec/batch; 65h:29m:53s remains)
INFO - root - 2017-12-07 08:31:42.270367: step 21020, loss = 1.00, batch loss = 0.92 (10.7 examples/sec; 0.749 sec/batch; 64h:49m:22s remains)
INFO - root - 2017-12-07 08:31:49.899390: step 21030, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.764 sec/batch; 66h:03m:32s remains)
INFO - root - 2017-12-07 08:31:57.414622: step 21040, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.781 sec/batch; 67h:33m:01s remains)
INFO - root - 2017-12-07 08:32:05.284033: step 21050, loss = 0.84, batch loss = 0.77 (10.1 examples/sec; 0.788 sec/batch; 68h:12m:24s remains)
INFO - root - 2017-12-07 08:32:12.934270: step 21060, loss = 0.97, batch loss = 0.90 (10.7 examples/sec; 0.750 sec/batch; 64h:52m:19s remains)
INFO - root - 2017-12-07 08:32:20.769074: step 21070, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 68h:22m:04s remains)
INFO - root - 2017-12-07 08:32:28.427503: step 21080, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.759 sec/batch; 65h:41m:54s remains)
INFO - root - 2017-12-07 08:32:36.115976: step 21090, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.781 sec/batch; 67h:35m:47s remains)
INFO - root - 2017-12-07 08:32:43.844730: step 21100, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.762 sec/batch; 65h:52m:41s remains)
2017-12-07 08:32:44.448143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7086418 -3.6741805 -3.6103747 -3.5501888 -3.49686 -3.4592822 -3.4340627 -3.415765 -3.4344957 -3.496304 -3.5582342 -3.5891511 -3.5760841 -3.5477781 -3.5764678][-3.7585924 -3.693121 -3.5585082 -3.4000354 -3.273726 -3.2575283 -3.3240137 -3.3458741 -3.2852969 -3.2367153 -3.267621 -3.3726282 -3.4601016 -3.469393 -3.4984679][-3.8867497 -3.7859125 -3.6206157 -3.424376 -3.2449427 -3.2381573 -3.3937478 -3.4548926 -3.2974615 -3.0880411 -3.034482 -3.1783342 -3.3438745 -3.373024 -3.4055543][-4.0038157 -3.8606565 -3.6617353 -3.4023135 -3.1014161 -3.016716 -3.2327175 -3.3830831 -3.2269125 -2.9218264 -2.7925568 -2.9446621 -3.1507611 -3.1784136 -3.2358446][-3.9423301 -3.7716827 -3.5951769 -3.2789774 -2.7783127 -2.4427128 -2.524838 -2.6845222 -2.5956361 -2.3765178 -2.3600476 -2.6434391 -2.9021604 -2.9085894 -2.9873271][-3.6440296 -3.3912156 -3.1890614 -2.7741177 -2.0450153 -1.4193211 -1.3073907 -1.4982717 -1.5941527 -1.5999212 -1.7848313 -2.1749978 -2.4120896 -2.3733881 -2.5246761][-3.2003922 -2.7854495 -2.4172251 -1.7732279 -0.77630782 0.0916028 0.30377245 -0.037609577 -0.43881321 -0.75394654 -1.1697602 -1.6675563 -1.860348 -1.7192972 -1.9063272][-2.8101931 -2.1902361 -1.635843 -0.84384012 0.12002563 0.88686705 1.0576143 0.72956419 0.33757973 -0.063251495 -0.6479907 -1.2941008 -1.4920089 -1.2534704 -1.3830812][-2.6985867 -1.9959385 -1.458262 -0.84486723 -0.23885489 0.094128609 0.014414787 -0.23129082 -0.31099224 -0.380404 -0.75464344 -1.3207898 -1.5425441 -1.3658798 -1.4913571][-2.8551865 -2.2861986 -1.9506116 -1.6403337 -1.3073494 -1.1974633 -1.4132571 -1.6303318 -1.5003254 -1.24179 -1.3001134 -1.7169869 -1.9944735 -2.0213294 -2.2097683][-3.0781045 -2.6561308 -2.4587913 -2.3154745 -2.0621641 -1.8889267 -1.9502697 -2.0821238 -1.9268742 -1.6365402 -1.6020951 -1.9259496 -2.195406 -2.3487034 -2.6120265][-3.3439221 -2.9774747 -2.7717502 -2.5967026 -2.2552547 -1.8931062 -1.6963899 -1.727802 -1.686007 -1.5526285 -1.5307834 -1.6784983 -1.834759 -2.086863 -2.4908185][-3.5590379 -3.2637849 -3.0288646 -2.7598777 -2.2968817 -1.7681253 -1.372025 -1.3038383 -1.3529172 -1.3591223 -1.3282199 -1.3056066 -1.4267304 -1.8905275 -2.4682965][-3.6152868 -3.3949561 -3.1892071 -2.9370968 -2.5152249 -1.9767334 -1.4646697 -1.227447 -1.212007 -1.2678084 -1.2414443 -1.1333778 -1.2509744 -1.8284578 -2.4687443][-3.5372791 -3.3394003 -3.1794193 -3.0440884 -2.8389385 -2.4615216 -1.9426067 -1.5553548 -1.4647429 -1.5865202 -1.6161072 -1.5120764 -1.6208742 -2.1359324 -2.6339929]]...]
INFO - root - 2017-12-07 08:32:52.136360: step 21110, loss = 0.60, batch loss = 0.53 (10.6 examples/sec; 0.751 sec/batch; 64h:59m:09s remains)
INFO - root - 2017-12-07 08:32:59.888903: step 21120, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.766 sec/batch; 66h:16m:33s remains)
INFO - root - 2017-12-07 08:33:07.623901: step 21130, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.790 sec/batch; 68h:19m:40s remains)
INFO - root - 2017-12-07 08:33:15.098199: step 21140, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.755 sec/batch; 65h:16m:12s remains)
INFO - root - 2017-12-07 08:33:22.689260: step 21150, loss = 0.78, batch loss = 0.71 (10.7 examples/sec; 0.748 sec/batch; 64h:41m:27s remains)
INFO - root - 2017-12-07 08:33:30.315273: step 21160, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.757 sec/batch; 65h:29m:26s remains)
INFO - root - 2017-12-07 08:33:37.929314: step 21170, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.772 sec/batch; 66h:45m:10s remains)
INFO - root - 2017-12-07 08:33:45.634479: step 21180, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.771 sec/batch; 66h:40m:38s remains)
INFO - root - 2017-12-07 08:33:53.324892: step 21190, loss = 0.89, batch loss = 0.82 (10.0 examples/sec; 0.796 sec/batch; 68h:51m:40s remains)
INFO - root - 2017-12-07 08:34:01.016988: step 21200, loss = 0.77, batch loss = 0.70 (10.8 examples/sec; 0.743 sec/batch; 64h:15m:50s remains)
2017-12-07 08:34:01.629570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2470999 -2.1578033 -2.0622232 -2.09245 -2.2960491 -2.4069166 -2.3680706 -2.3903165 -2.4093866 -2.3908551 -2.6234605 -3.093945 -3.4544516 -3.7028732 -3.8676884][-1.3382332 -1.2781053 -1.2787607 -1.3479471 -1.5341017 -1.7252128 -1.863368 -2.0047259 -2.059505 -2.0340886 -2.2602341 -2.6504874 -2.9083514 -3.1395428 -3.3443127][-0.64257479 -0.71717453 -0.93110967 -1.0743649 -1.2265041 -1.477885 -1.764677 -1.9761193 -1.993422 -1.9298353 -2.1065915 -2.3406169 -2.4346421 -2.5117097 -2.6158957][-0.31821442 -0.63950109 -1.040906 -1.1609106 -1.2114923 -1.5034759 -1.9263978 -2.214808 -2.2236152 -2.1726103 -2.3054812 -2.377562 -2.33503 -2.2494035 -2.243722][-0.47362208 -1.0886402 -1.5652249 -1.4840202 -1.2711012 -1.469696 -1.9459295 -2.3135247 -2.4030197 -2.4614882 -2.5870743 -2.4855556 -2.2704396 -2.062531 -2.0813389][-1.0781567 -1.8174517 -2.1434541 -1.7455287 -1.2020373 -1.1651897 -1.52741 -1.890887 -2.1312385 -2.3896956 -2.6075416 -2.4249079 -2.082408 -1.834687 -1.93171][-1.8743517 -2.5433762 -2.527535 -1.790837 -0.97778153 -0.63783884 -0.69395947 -0.97925663 -1.4262731 -1.9226155 -2.2522166 -2.062804 -1.7043426 -1.5521684 -1.7724664][-2.6942072 -3.1641445 -2.801007 -1.8431351 -0.9149704 -0.33586597 -0.068231583 -0.249259 -0.80571127 -1.3737087 -1.6326015 -1.372906 -1.0750656 -1.141741 -1.5580022][-3.4028282 -3.6442883 -3.0966396 -2.1112447 -1.2140729 -0.52641416 -0.126585 -0.37243319 -0.9778347 -1.4442344 -1.442965 -1.0593872 -0.82062006 -1.0490451 -1.5905628][-3.684907 -3.8118687 -3.2991111 -2.4378645 -1.6225266 -0.91017056 -0.5959456 -1.0806952 -1.7560823 -2.02811 -1.6833875 -1.1367855 -0.89604497 -1.1317604 -1.6428456][-3.5992322 -3.7153392 -3.3087893 -2.5842717 -1.8403885 -1.1568229 -1.0050251 -1.7346778 -2.472472 -2.5339255 -1.9096398 -1.2275422 -0.90309596 -1.0006962 -1.3859892][-3.4430439 -3.5108604 -3.161561 -2.5655887 -1.9576471 -1.406131 -1.3843801 -2.1755269 -2.8645134 -2.7508705 -2.0228167 -1.2977457 -0.90795159 -0.84919262 -1.1402035][-3.3169408 -3.343539 -3.0840254 -2.6589329 -2.2373459 -1.8535452 -1.8918991 -2.5907454 -3.1632257 -2.9598804 -2.2512889 -1.5485585 -1.0968781 -0.92152643 -1.17503][-3.3219724 -3.3539267 -3.2091997 -2.9579144 -2.7011054 -2.4258142 -2.4573793 -3.0015335 -3.4958715 -3.3471217 -2.7208309 -1.9933903 -1.3743675 -1.0074208 -1.1565471][-3.3529987 -3.4153972 -3.3474689 -3.2113957 -3.0817912 -2.8803275 -2.8505664 -3.1927013 -3.6160722 -3.5581989 -3.027154 -2.255343 -1.4356594 -0.86336136 -0.88848019]]...]
INFO - root - 2017-12-07 08:34:09.345803: step 21210, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 66h:07m:06s remains)
INFO - root - 2017-12-07 08:34:17.110011: step 21220, loss = 0.68, batch loss = 0.60 (10.3 examples/sec; 0.778 sec/batch; 67h:15m:56s remains)
INFO - root - 2017-12-07 08:34:24.701225: step 21230, loss = 0.72, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 65h:47m:00s remains)
INFO - root - 2017-12-07 08:34:32.094388: step 21240, loss = 0.85, batch loss = 0.77 (10.2 examples/sec; 0.781 sec/batch; 67h:29m:09s remains)
INFO - root - 2017-12-07 08:34:39.880953: step 21250, loss = 0.93, batch loss = 0.85 (10.4 examples/sec; 0.767 sec/batch; 66h:21m:00s remains)
INFO - root - 2017-12-07 08:34:47.708795: step 21260, loss = 0.78, batch loss = 0.71 (10.0 examples/sec; 0.796 sec/batch; 68h:50m:51s remains)
INFO - root - 2017-12-07 08:34:55.365387: step 21270, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.755 sec/batch; 65h:15m:47s remains)
INFO - root - 2017-12-07 08:35:03.014280: step 21280, loss = 0.85, batch loss = 0.77 (10.2 examples/sec; 0.784 sec/batch; 67h:46m:05s remains)
INFO - root - 2017-12-07 08:35:10.744555: step 21290, loss = 0.80, batch loss = 0.73 (10.1 examples/sec; 0.792 sec/batch; 68h:28m:18s remains)
INFO - root - 2017-12-07 08:35:18.480410: step 21300, loss = 0.65, batch loss = 0.57 (10.5 examples/sec; 0.760 sec/batch; 65h:42m:01s remains)
2017-12-07 08:35:19.100899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2688804 -3.095628 -3.0616932 -3.3769097 -3.6167676 -3.5391793 -3.0462813 -2.7989039 -2.8138933 -2.3679838 -1.5643554 -1.0722263 -1.3817344 -2.0942421 -2.5014181][-2.5737009 -2.4360931 -2.6018205 -3.1175616 -3.3988023 -3.1572347 -2.36058 -2.0133314 -2.2529924 -2.1298311 -1.5969367 -1.2716248 -1.5552769 -2.1789894 -2.5263243][-1.7806287 -1.7847557 -2.1592014 -2.7632155 -2.9936411 -2.5982425 -1.5778961 -1.1974056 -1.6730709 -1.9038014 -1.701359 -1.5722659 -1.8227746 -2.3130496 -2.5655711][-1.4509923 -1.6163909 -2.0522487 -2.5835257 -2.6823668 -2.1708038 -1.0075688 -0.5599215 -1.1382322 -1.638983 -1.7380252 -1.7846966 -2.0303617 -2.4218016 -2.6008065][-1.774013 -2.0189939 -2.4250119 -2.8462129 -2.7998791 -2.1620011 -0.85893393 -0.22837114 -0.7416532 -1.4036419 -1.7437873 -1.9229307 -2.1736224 -2.5057466 -2.6376805][-2.1372116 -2.3510239 -2.6327777 -2.9696741 -2.8931236 -2.2645645 -1.0222614 -0.29976463 -0.68759894 -1.3636158 -1.7569394 -1.9582136 -2.2142727 -2.5393045 -2.6662362][-1.9304907 -2.0968904 -2.2855883 -2.63999 -2.7086263 -2.3266008 -1.3655047 -0.65839386 -0.85523558 -1.3482916 -1.5981238 -1.7785354 -2.1108048 -2.5043631 -2.6758444][-1.4584477 -1.6828656 -1.883544 -2.282388 -2.5079489 -2.5066261 -1.9424844 -1.3205402 -1.2941394 -1.422384 -1.382972 -1.5045559 -1.9300711 -2.4243014 -2.665719][-1.2143631 -1.6171408 -1.8838611 -2.19891 -2.4082353 -2.7307792 -2.5932965 -2.1240566 -1.9274914 -1.6279476 -1.2480454 -1.2791831 -1.7491965 -2.329777 -2.6410255][-1.09831 -1.7499192 -2.1797292 -2.3024645 -2.3380764 -2.7796483 -2.9816566 -2.7254024 -2.4475989 -1.7978053 -1.1547644 -1.1228888 -1.6257257 -2.26813 -2.6241164][-1.1212122 -1.8800418 -2.3152683 -2.1579826 -1.9748044 -2.5091777 -3.0439076 -3.0810747 -2.812566 -1.9771199 -1.2014704 -1.1202493 -1.6107197 -2.2582629 -2.6130517][-1.3251545 -1.9284947 -2.1926 -1.8119221 -1.6001911 -2.2972386 -3.0911121 -3.329143 -3.0670753 -2.2281818 -1.4503312 -1.2918544 -1.697974 -2.2859476 -2.6027093][-1.3746657 -1.7278004 -1.7799866 -1.3342552 -1.2588093 -2.0764444 -2.9747443 -3.3059196 -3.0906155 -2.3775373 -1.6839919 -1.4494863 -1.7690041 -2.3082371 -2.5905464][-1.5565681 -1.6855407 -1.6002252 -1.1993084 -1.1801794 -1.8892922 -2.724144 -3.1490517 -3.0726719 -2.5224912 -1.8666751 -1.5056028 -1.7614381 -2.3007262 -2.5787878][-2.0184755 -2.0295763 -1.8863921 -1.5740354 -1.5073678 -1.9472196 -2.6673307 -3.2422714 -3.3217516 -2.8510489 -2.1147068 -1.5832269 -1.7690735 -2.3066385 -2.5784392]]...]
INFO - root - 2017-12-07 08:35:26.773095: step 21310, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 67h:13m:34s remains)
INFO - root - 2017-12-07 08:35:34.395704: step 21320, loss = 0.90, batch loss = 0.83 (10.9 examples/sec; 0.735 sec/batch; 63h:30m:24s remains)
INFO - root - 2017-12-07 08:35:41.997110: step 21330, loss = 0.57, batch loss = 0.49 (10.6 examples/sec; 0.755 sec/batch; 65h:13m:44s remains)
INFO - root - 2017-12-07 08:35:49.336359: step 21340, loss = 0.78, batch loss = 0.71 (10.9 examples/sec; 0.733 sec/batch; 63h:18m:46s remains)
INFO - root - 2017-12-07 08:35:56.926728: step 21350, loss = 0.75, batch loss = 0.68 (10.7 examples/sec; 0.749 sec/batch; 64h:45m:09s remains)
INFO - root - 2017-12-07 08:36:04.455626: step 21360, loss = 0.81, batch loss = 0.73 (10.7 examples/sec; 0.746 sec/batch; 64h:26m:17s remains)
INFO - root - 2017-12-07 08:36:12.073191: step 21370, loss = 0.69, batch loss = 0.62 (11.0 examples/sec; 0.726 sec/batch; 62h:46m:50s remains)
INFO - root - 2017-12-07 08:36:19.611532: step 21380, loss = 0.94, batch loss = 0.87 (10.9 examples/sec; 0.732 sec/batch; 63h:18m:09s remains)
INFO - root - 2017-12-07 08:36:27.294563: step 21390, loss = 1.00, batch loss = 0.93 (10.0 examples/sec; 0.798 sec/batch; 68h:58m:02s remains)
INFO - root - 2017-12-07 08:36:34.983584: step 21400, loss = 1.16, batch loss = 1.09 (10.2 examples/sec; 0.786 sec/batch; 67h:54m:50s remains)
2017-12-07 08:36:35.703096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.054184 -4.2119927 -4.1134882 -3.9485226 -3.788115 -3.7446218 -3.8138371 -3.8290286 -3.7585073 -3.649225 -3.5533023 -3.5857403 -3.8018048 -4.1081357 -4.3023214][-4.5339622 -4.6660314 -4.4714503 -4.3216248 -4.2557 -4.2954879 -4.3610716 -4.283926 -4.141119 -4.0256147 -3.9282293 -3.9677472 -4.2133117 -4.5320516 -4.6711698][-4.8337941 -4.8428068 -4.4966059 -4.3356147 -4.3654833 -4.5010762 -4.5369473 -4.3556185 -4.178349 -4.1556067 -4.1833062 -4.295083 -4.5086069 -4.6881652 -4.6474991][-4.7165275 -4.532743 -4.07401 -3.9508488 -4.0731583 -4.3072314 -4.3659577 -4.15733 -3.9803674 -4.0506692 -4.2157974 -4.3903308 -4.5170856 -4.4550428 -4.156157][-4.1923041 -3.8339429 -3.3829279 -3.3800864 -3.6102803 -3.8754838 -3.8672357 -3.5977511 -3.3781428 -3.4697826 -3.7230086 -3.97095 -4.0826588 -3.8546646 -3.3642278][-3.59059 -3.1599364 -2.8347178 -2.9963849 -3.2901118 -3.42263 -3.1474249 -2.6732965 -2.3431532 -2.462486 -2.8574538 -3.2847557 -3.5544209 -3.3450112 -2.830097][-3.3406765 -2.9508047 -2.7401683 -2.87386 -3.0132713 -2.8545518 -2.1821287 -1.3771026 -0.88567924 -1.1270416 -1.855058 -2.6796055 -3.2699287 -3.2345495 -2.8394289][-3.5761561 -3.2896197 -3.0442283 -2.86829 -2.630239 -2.168242 -1.2625511 -0.2680316 0.31383038 -0.074008942 -1.1979265 -2.4713664 -3.3422279 -3.4369845 -3.126755][-3.8502288 -3.6326156 -3.2985535 -2.808322 -2.3265061 -1.8673933 -1.1779733 -0.40966034 0.048491955 -0.37276888 -1.5352814 -2.8634367 -3.6987383 -3.744312 -3.4022374][-3.9763105 -3.7707896 -3.3791089 -2.7454953 -2.2674534 -2.0938287 -1.9077981 -1.5874193 -1.3346536 -1.633837 -2.4185469 -3.36709 -3.9330964 -3.8821669 -3.5336149][-3.9128203 -3.6632457 -3.2402053 -2.5811462 -2.1679878 -2.2351263 -2.4616208 -2.5248129 -2.42569 -2.5287707 -2.8812985 -3.3938053 -3.7062702 -3.6099303 -3.325701][-3.4329865 -3.1252146 -2.7269702 -2.1849802 -1.923635 -2.1460478 -2.5896142 -2.8571875 -2.8277907 -2.7523973 -2.818903 -3.0445294 -3.1534181 -2.9796941 -2.7306871][-2.7397668 -2.3922637 -2.0593717 -1.7479377 -1.7619729 -2.1370163 -2.6213415 -2.9082718 -2.8859532 -2.729285 -2.6845942 -2.7667866 -2.6999915 -2.3839917 -2.0925422][-2.3158946 -2.0164697 -1.8123436 -1.7647038 -2.0135486 -2.4344516 -2.7960088 -2.9737883 -2.9583287 -2.8418202 -2.7996058 -2.8061848 -2.6243997 -2.2302585 -1.9131243][-2.5569482 -2.4032662 -2.3303471 -2.3916368 -2.6307266 -2.9040565 -3.0737829 -3.1316195 -3.1249123 -3.0849848 -3.0899315 -3.0833752 -2.913697 -2.6051779 -2.3784931]]...]
INFO - root - 2017-12-07 08:36:43.398510: step 21410, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.771 sec/batch; 66h:36m:04s remains)
INFO - root - 2017-12-07 08:36:51.058721: step 21420, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.768 sec/batch; 66h:22m:18s remains)
INFO - root - 2017-12-07 08:36:58.645690: step 21430, loss = 0.72, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 67h:05m:06s remains)
INFO - root - 2017-12-07 08:37:06.073628: step 21440, loss = 0.93, batch loss = 0.86 (10.5 examples/sec; 0.764 sec/batch; 66h:00m:12s remains)
INFO - root - 2017-12-07 08:37:13.638377: step 21450, loss = 0.83, batch loss = 0.76 (10.8 examples/sec; 0.741 sec/batch; 64h:00m:05s remains)
INFO - root - 2017-12-07 08:37:21.269616: step 21460, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.776 sec/batch; 67h:01m:25s remains)
INFO - root - 2017-12-07 08:37:29.089385: step 21470, loss = 1.02, batch loss = 0.95 (10.0 examples/sec; 0.803 sec/batch; 69h:23m:00s remains)
INFO - root - 2017-12-07 08:37:36.774583: step 21480, loss = 0.85, batch loss = 0.77 (10.4 examples/sec; 0.768 sec/batch; 66h:23m:09s remains)
INFO - root - 2017-12-07 08:37:44.509244: step 21490, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 65h:15m:58s remains)
INFO - root - 2017-12-07 08:37:52.117970: step 21500, loss = 0.99, batch loss = 0.92 (10.5 examples/sec; 0.760 sec/batch; 65h:37m:38s remains)
2017-12-07 08:37:52.740477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7639732 -2.91466 -3.1637173 -3.5270274 -3.8521817 -3.9959841 -4.0390453 -4.0059948 -3.8658295 -3.5668442 -3.296505 -3.141464 -3.1702943 -3.3995345 -3.6550741][-2.9162521 -2.8374116 -2.8997042 -3.2047372 -3.4752407 -3.5814018 -3.547658 -3.4548054 -3.33539 -3.1136804 -2.9348912 -2.8048801 -2.7584023 -2.9213576 -3.1746058][-3.346312 -3.1201243 -3.0593948 -3.2723167 -3.3556077 -3.2512784 -3.0556128 -2.9448218 -2.9718451 -2.9775593 -2.9067597 -2.6511297 -2.2767334 -2.1305325 -2.2826557][-3.42507 -3.1863465 -3.1687398 -3.4748778 -3.585042 -3.4601765 -3.2627542 -3.1827459 -3.2963452 -3.3928506 -3.229073 -2.7064822 -2.0201981 -1.6907907 -1.8977599][-3.526957 -3.2737823 -3.2232828 -3.4959128 -3.5670662 -3.3917525 -3.1605105 -3.05655 -3.1883698 -3.3206491 -3.1373649 -2.5798461 -1.9089363 -1.6121387 -1.9568563][-3.4772162 -3.1130943 -2.9599152 -3.1608577 -3.1809559 -2.9322047 -2.60973 -2.4834993 -2.7078793 -2.9734311 -2.9248872 -2.5125051 -1.9236732 -1.5695672 -1.8359807][-2.8830967 -2.3118322 -1.9976432 -2.1011784 -2.0570602 -1.7643094 -1.4498439 -1.4638958 -1.9589398 -2.4267724 -2.5152242 -2.2338758 -1.7324152 -1.3845508 -1.5429392][-1.9841957 -1.3604696 -0.99312758 -0.97328448 -0.71053314 -0.20694447 0.21507215 0.10211134 -0.72689223 -1.573324 -2.0118537 -2.0338895 -1.7167542 -1.4075007 -1.3539076][-1.6662486 -1.2062159 -0.88311744 -0.63668633 0.015945911 0.80834961 1.3070607 1.1269522 0.066248417 -1.1182277 -1.9328096 -2.3009255 -2.2034676 -1.9090884 -1.5556977][-2.1075335 -1.8983529 -1.5953887 -1.073379 -0.10050106 0.78778028 1.1633172 0.9067626 -0.13329363 -1.3219926 -2.2066047 -2.6747127 -2.7097366 -2.4683328 -1.9077163][-2.379787 -2.3419797 -2.0490499 -1.4367707 -0.4863801 0.12545967 0.23292875 -0.04709959 -0.89305043 -1.8715901 -2.585201 -2.9503965 -3.0276628 -2.8585186 -2.1933086][-2.2083929 -2.4778135 -2.4549713 -2.0528738 -1.4048014 -1.133765 -1.1900306 -1.3951242 -1.9488499 -2.568785 -2.9774268 -3.1949544 -3.2777863 -3.2038984 -2.5890594][-2.1298368 -2.8330574 -3.1843004 -3.0823398 -2.7726614 -2.7195382 -2.8068166 -2.8402996 -3.0227666 -3.2126019 -3.2786641 -3.3590403 -3.4265289 -3.4387541 -3.028965][-2.2961314 -3.1354494 -3.62336 -3.6844623 -3.6063011 -3.6497693 -3.6899686 -3.6022153 -3.553957 -3.4843895 -3.4132073 -3.4815044 -3.5779812 -3.670927 -3.4718337][-2.4980841 -3.2418652 -3.7407618 -3.89605 -3.9133534 -3.9008923 -3.8177927 -3.6848261 -3.5805264 -3.4640474 -3.3983724 -3.4898229 -3.6420555 -3.8294926 -3.7872071]]...]
INFO - root - 2017-12-07 08:38:00.579069: step 21510, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.783 sec/batch; 67h:36m:20s remains)
INFO - root - 2017-12-07 08:38:08.347447: step 21520, loss = 0.90, batch loss = 0.82 (10.1 examples/sec; 0.795 sec/batch; 68h:41m:06s remains)
INFO - root - 2017-12-07 08:38:16.095766: step 21530, loss = 0.73, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 68h:16m:43s remains)
INFO - root - 2017-12-07 08:38:23.511709: step 21540, loss = 0.55, batch loss = 0.48 (10.5 examples/sec; 0.762 sec/batch; 65h:49m:59s remains)
INFO - root - 2017-12-07 08:38:31.180328: step 21550, loss = 0.97, batch loss = 0.89 (10.3 examples/sec; 0.780 sec/batch; 67h:23m:39s remains)
INFO - root - 2017-12-07 08:38:38.786227: step 21560, loss = 1.01, batch loss = 0.93 (10.3 examples/sec; 0.773 sec/batch; 66h:46m:25s remains)
INFO - root - 2017-12-07 08:38:46.448424: step 21570, loss = 1.07, batch loss = 1.00 (10.2 examples/sec; 0.782 sec/batch; 67h:33m:35s remains)
INFO - root - 2017-12-07 08:38:54.159808: step 21580, loss = 0.68, batch loss = 0.61 (10.6 examples/sec; 0.758 sec/batch; 65h:25m:51s remains)
INFO - root - 2017-12-07 08:39:01.908108: step 21590, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.759 sec/batch; 65h:34m:42s remains)
INFO - root - 2017-12-07 08:39:09.512369: step 21600, loss = 0.93, batch loss = 0.85 (10.6 examples/sec; 0.754 sec/batch; 65h:05m:23s remains)
2017-12-07 08:39:10.128676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2429714 -3.253875 -3.2253256 -3.1882858 -3.2107997 -3.2079697 -3.1977456 -3.1190453 -2.9182565 -2.7979155 -2.8260124 -2.9162841 -2.8819766 -2.8265352 -2.927382][-2.8702722 -2.8720889 -2.8344886 -2.8117337 -2.8428564 -2.8222628 -2.8116999 -2.7428071 -2.5626884 -2.472877 -2.5543041 -2.7351813 -2.7691822 -2.7435606 -2.8568821][-2.5019498 -2.4867063 -2.434617 -2.4173255 -2.4582489 -2.433701 -2.4514184 -2.4387677 -2.3146403 -2.2540812 -2.3642538 -2.6097288 -2.7015667 -2.6971755 -2.8071165][-2.2069392 -2.1975877 -2.137543 -2.1163094 -2.1626515 -2.1363218 -2.1908121 -2.2480974 -2.1854658 -2.1378005 -2.2337997 -2.4875212 -2.6170778 -2.6483169 -2.7707596][-1.9915171 -2.0250981 -1.9868674 -1.9731314 -2.0208192 -1.9763675 -2.0303712 -2.1105051 -2.0744903 -2.0417693 -2.1475184 -2.4036655 -2.5597715 -2.6277452 -2.7650681][-1.9389246 -1.9881411 -1.9711974 -1.9747198 -2.0186691 -1.9526458 -1.9935486 -2.0657165 -2.0228298 -1.9910593 -2.1143246 -2.3695359 -2.5341489 -2.6238413 -2.7743366][-1.9121184 -1.9246027 -1.9119282 -1.9257073 -1.949116 -1.8612785 -1.8915105 -1.9660292 -1.9307983 -1.9071794 -2.0477953 -2.2920418 -2.44911 -2.5574338 -2.745208][-1.7355185 -1.7238984 -1.7234993 -1.7518253 -1.7643309 -1.6692722 -1.7010374 -1.8028226 -1.8191004 -1.8369827 -2.0084538 -2.2485778 -2.3883772 -2.4970627 -2.7124295][-1.6092384 -1.591907 -1.5939276 -1.6241839 -1.6496146 -1.5903599 -1.6552172 -1.8075578 -1.8796878 -1.9221835 -2.1073976 -2.3347669 -2.4430735 -2.5212204 -2.7242901][-1.8242166 -1.7976043 -1.7751875 -1.7804382 -1.7985466 -1.7651443 -1.8402827 -2.0159485 -2.1004159 -2.1229029 -2.2822137 -2.4782529 -2.5594516 -2.607553 -2.778126][-2.2694688 -2.2763913 -2.2597649 -2.235348 -2.2107654 -2.1471388 -2.1669323 -2.2974319 -2.3485696 -2.3392708 -2.4706731 -2.6372771 -2.7018712 -2.7248902 -2.8507566][-2.6144269 -2.6650097 -2.6797137 -2.6530197 -2.6124721 -2.5455041 -2.5260494 -2.5914116 -2.590013 -2.55752 -2.6630938 -2.7931685 -2.8421834 -2.8435454 -2.922195][-2.835011 -2.8902035 -2.9093394 -2.8804095 -2.8404868 -2.7943504 -2.7697568 -2.7940736 -2.7681172 -2.7360983 -2.8216825 -2.9148059 -2.9457307 -2.9350185 -2.9839263][-2.9458337 -2.98884 -3.0092473 -2.9780338 -2.9273233 -2.8974328 -2.8982353 -2.9233453 -2.8981552 -2.8732698 -2.9422181 -3.0162342 -3.0332241 -3.0137448 -3.0400937][-2.9183724 -2.9521399 -2.982626 -2.9654784 -2.9170361 -2.9003656 -2.9299097 -2.9690981 -2.949029 -2.929224 -2.9944086 -3.0634346 -3.070349 -3.0475187 -3.0627379]]...]
INFO - root - 2017-12-07 08:39:17.795016: step 21610, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.751 sec/batch; 64h:51m:32s remains)
INFO - root - 2017-12-07 08:39:25.458967: step 21620, loss = 0.96, batch loss = 0.89 (10.0 examples/sec; 0.798 sec/batch; 68h:53m:47s remains)
INFO - root - 2017-12-07 08:39:33.201137: step 21630, loss = 0.90, batch loss = 0.83 (10.5 examples/sec; 0.764 sec/batch; 65h:59m:44s remains)
INFO - root - 2017-12-07 08:39:40.680189: step 21640, loss = 0.88, batch loss = 0.81 (10.2 examples/sec; 0.783 sec/batch; 67h:37m:51s remains)
INFO - root - 2017-12-07 08:39:48.386700: step 21650, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 66h:24m:55s remains)
INFO - root - 2017-12-07 08:39:56.126289: step 21660, loss = 0.73, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 66h:28m:14s remains)
INFO - root - 2017-12-07 08:40:03.857518: step 21670, loss = 0.64, batch loss = 0.57 (10.4 examples/sec; 0.770 sec/batch; 66h:26m:55s remains)
INFO - root - 2017-12-07 08:40:11.557509: step 21680, loss = 0.67, batch loss = 0.60 (10.7 examples/sec; 0.749 sec/batch; 64h:41m:09s remains)
INFO - root - 2017-12-07 08:40:19.176443: step 21690, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.760 sec/batch; 65h:36m:18s remains)
INFO - root - 2017-12-07 08:40:26.778423: step 21700, loss = 0.92, batch loss = 0.85 (10.3 examples/sec; 0.778 sec/batch; 67h:11m:23s remains)
2017-12-07 08:40:27.396520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.70818019 -1.5675862 -2.453814 -3.1696048 -3.2255359 -2.7313471 -2.1320872 -1.3064446 -0.86334014 -1.2697012 -1.9060433 -2.4505734 -2.6367869 -2.434093 -2.2393117][-0.86014915 -1.8434389 -2.4715073 -2.8635831 -2.7261167 -2.1512568 -1.636167 -0.95504189 -0.46841908 -0.87158704 -1.5449388 -2.0261848 -2.125602 -1.8875747 -1.6485307][-1.1484129 -2.2907307 -2.8241515 -2.9499011 -2.5233636 -1.8085253 -1.3874393 -0.8243742 -0.24064589 -0.62103963 -1.5375311 -2.2477312 -2.3193567 -1.8747053 -1.3277962][-0.92731285 -2.2291429 -3.019038 -3.2060418 -2.6870761 -1.8977048 -1.3970647 -0.59958458 0.28930473 -0.164464 -1.6676114 -2.8786714 -3.021112 -2.3629863 -1.4320824][-0.68364358 -2.0138347 -2.9975758 -3.2459219 -2.8155293 -2.2439156 -1.640089 -0.28005886 1.0880485 0.46035147 -1.6228909 -3.2223589 -3.4781666 -2.750577 -1.5607035][-0.56295371 -1.715749 -2.5936146 -2.7723222 -2.5448503 -2.3549209 -1.6748602 0.24379587 1.9351511 1.0381827 -1.3915226 -3.1071792 -3.385314 -2.5655112 -1.0920444][-0.70754862 -1.6096971 -2.1055679 -2.1287196 -2.1197226 -2.1757474 -1.39833 0.71586657 2.2861671 1.1019197 -1.3578906 -2.9582825 -3.1474051 -2.1063094 -0.34366369][-1.0886018 -1.930408 -2.1134391 -1.8972821 -1.7905903 -1.6712232 -0.78142834 0.98923826 1.9149432 0.56641006 -1.676944 -3.0501318 -3.1207733 -1.8900847 -0.080639839][-1.5870085 -2.6801107 -2.9433236 -2.6153588 -2.148561 -1.4287496 -0.28147173 0.98982763 1.2179518 -0.22597361 -2.1729655 -3.3493066 -3.3506227 -2.0854609 -0.483402][-1.7312627 -3.3345656 -4.01619 -3.7581818 -2.9377029 -1.6751218 -0.37542009 0.48775482 0.28005266 -1.0782733 -2.6701679 -3.6764989 -3.6092248 -2.459033 -1.2397394][-1.3328238 -3.2837644 -4.4585958 -4.511456 -3.5938349 -2.16747 -0.97314596 -0.39825392 -0.75945759 -1.9023035 -3.1435676 -3.9832559 -3.8977122 -2.9401441 -2.0082686][-1.0550478 -2.9986076 -4.5049 -4.9334135 -4.118854 -2.8336024 -1.8939893 -1.4480929 -1.734488 -2.5843534 -3.54739 -4.30536 -4.2901521 -3.5806158 -2.8632016][-1.0112839 -2.7940736 -4.4498549 -5.1530347 -4.5432091 -3.55826 -2.9218509 -2.5109777 -2.5738778 -3.0968506 -3.7982292 -4.4700737 -4.57192 -4.1345034 -3.6341567][-1.0719247 -2.732888 -4.3866754 -5.176177 -4.7665696 -4.096529 -3.7390068 -3.3663034 -3.2760444 -3.5528898 -4.0052519 -4.4789414 -4.5733471 -4.3141108 -4.0203748][-1.0729311 -2.7555661 -4.3408313 -5.0479531 -4.7292919 -4.2765737 -4.1384711 -3.877943 -3.7741594 -3.9263895 -4.1693935 -4.39963 -4.4206905 -4.2835636 -4.1643991]]...]
INFO - root - 2017-12-07 08:40:35.176218: step 21710, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 66h:19m:55s remains)
INFO - root - 2017-12-07 08:40:42.947718: step 21720, loss = 0.88, batch loss = 0.80 (10.8 examples/sec; 0.738 sec/batch; 63h:42m:44s remains)
INFO - root - 2017-12-07 08:40:50.617813: step 21730, loss = 0.85, batch loss = 0.78 (10.2 examples/sec; 0.787 sec/batch; 67h:55m:25s remains)
INFO - root - 2017-12-07 08:40:58.131699: step 21740, loss = 1.05, batch loss = 0.98 (10.1 examples/sec; 0.794 sec/batch; 68h:34m:57s remains)
INFO - root - 2017-12-07 08:41:05.821714: step 21750, loss = 0.83, batch loss = 0.76 (10.1 examples/sec; 0.793 sec/batch; 68h:25m:05s remains)
INFO - root - 2017-12-07 08:41:13.524362: step 21760, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.757 sec/batch; 65h:18m:06s remains)
INFO - root - 2017-12-07 08:41:21.225725: step 21770, loss = 0.77, batch loss = 0.70 (10.8 examples/sec; 0.741 sec/batch; 63h:59m:25s remains)
INFO - root - 2017-12-07 08:41:28.832731: step 21780, loss = 0.66, batch loss = 0.59 (10.3 examples/sec; 0.775 sec/batch; 66h:51m:22s remains)
INFO - root - 2017-12-07 08:41:36.462016: step 21790, loss = 1.15, batch loss = 1.08 (10.2 examples/sec; 0.782 sec/batch; 67h:28m:17s remains)
INFO - root - 2017-12-07 08:41:44.096136: step 21800, loss = 0.72, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 67h:10m:14s remains)
2017-12-07 08:41:44.726723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.83243132 -1.1791697 -1.6217179 -1.7995226 -1.2786491 -0.69053578 -0.59065175 -0.77397585 -1.0244353 -1.3208559 -1.6832623 -1.8935246 -2.0138392 -2.1557221 -2.2736242][-0.64580727 -0.89388537 -1.2863142 -1.3521593 -0.81089139 -0.43331003 -0.51147676 -0.78072619 -1.1316788 -1.4984879 -1.8994226 -2.1222048 -2.2897224 -2.4753051 -2.5723414][-0.6951468 -0.83318114 -1.1032126 -1.0507829 -0.61149168 -0.45716476 -0.60966372 -0.94612432 -1.4484246 -1.9061508 -2.2729335 -2.4373221 -2.60211 -2.7878458 -2.843442][-0.6621139 -0.66302824 -0.8427434 -0.79588008 -0.59154367 -0.62423706 -0.776011 -1.1453533 -1.7737651 -2.3279183 -2.66913 -2.7886176 -2.9362631 -3.0390668 -2.9852083][-0.60292649 -0.52532578 -0.66344357 -0.65525222 -0.605124 -0.68724227 -0.82131863 -1.2515061 -1.9972801 -2.6607246 -3.0354776 -3.164484 -3.2724085 -3.2207041 -3.0272923][-0.65214133 -0.64806604 -0.80959058 -0.80483747 -0.6978209 -0.64259791 -0.7311461 -1.2297776 -2.0856261 -2.8510261 -3.2985148 -3.4792483 -3.5274138 -3.3293877 -3.0293412][-0.87680197 -0.98503327 -1.1803074 -1.1325696 -0.86717343 -0.62999368 -0.66865993 -1.2040875 -2.0941789 -2.8636336 -3.3324108 -3.5429361 -3.5349808 -3.2645962 -2.9513421][-1.233418 -1.3553727 -1.5116096 -1.3771477 -0.98397923 -0.63792372 -0.655149 -1.1801314 -2.000881 -2.6624503 -3.1001816 -3.316638 -3.2464252 -2.9679785 -2.749649][-1.4877355 -1.4775403 -1.4987087 -1.2745502 -0.85576153 -0.55631971 -0.61081314 -1.1147058 -1.8110452 -2.3605907 -2.782475 -3.0065718 -2.9003336 -2.6488872 -2.5481167][-1.5994787 -1.3474026 -1.146924 -0.846025 -0.51740837 -0.398551 -0.55575323 -1.0218637 -1.5839913 -2.0648487 -2.5000648 -2.7500863 -2.6734054 -2.4862731 -2.460382][-1.6307201 -1.2350833 -0.87183738 -0.51945758 -0.30754757 -0.39364052 -0.66061115 -1.0584834 -1.5023677 -1.9647925 -2.3889048 -2.6343303 -2.6030152 -2.4791594 -2.4732089][-1.5353968 -1.2225466 -0.90655255 -0.57946897 -0.43780351 -0.62790442 -0.93206406 -1.2388155 -1.6033177 -2.0812268 -2.5074763 -2.7274311 -2.6989775 -2.592236 -2.5623906][-1.2976534 -1.1752849 -1.0561976 -0.88003612 -0.82622838 -1.0463831 -1.3479552 -1.5885072 -1.8936794 -2.3563859 -2.7601552 -2.9353285 -2.8932953 -2.789772 -2.7376251][-1.1607184 -1.1741726 -1.2255325 -1.2097082 -1.2790325 -1.5466287 -1.8670931 -2.0934086 -2.3364074 -2.6998148 -3.0082371 -3.112252 -3.0505614 -2.9542592 -2.8969181][-1.2596285 -1.2965226 -1.4098854 -1.5196354 -1.7299502 -2.0483861 -2.3471966 -2.5346427 -2.699754 -2.9238341 -3.1094577 -3.1456022 -3.069838 -2.9776802 -2.9235477]]...]
INFO - root - 2017-12-07 08:41:52.421464: step 21810, loss = 0.99, batch loss = 0.92 (10.5 examples/sec; 0.761 sec/batch; 65h:41m:29s remains)
INFO - root - 2017-12-07 08:42:00.142879: step 21820, loss = 0.98, batch loss = 0.90 (10.1 examples/sec; 0.791 sec/batch; 68h:13m:23s remains)
INFO - root - 2017-12-07 08:42:07.820748: step 21830, loss = 0.87, batch loss = 0.80 (10.0 examples/sec; 0.798 sec/batch; 68h:53m:42s remains)
INFO - root - 2017-12-07 08:42:15.167217: step 21840, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.780 sec/batch; 67h:18m:50s remains)
INFO - root - 2017-12-07 08:42:22.829265: step 21850, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.755 sec/batch; 65h:10m:25s remains)
INFO - root - 2017-12-07 08:42:30.422846: step 21860, loss = 0.88, batch loss = 0.80 (10.7 examples/sec; 0.751 sec/batch; 64h:47m:24s remains)
INFO - root - 2017-12-07 08:42:38.089954: step 21870, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.760 sec/batch; 65h:36m:56s remains)
INFO - root - 2017-12-07 08:42:45.772438: step 21880, loss = 1.01, batch loss = 0.94 (10.3 examples/sec; 0.780 sec/batch; 67h:18m:14s remains)
INFO - root - 2017-12-07 08:42:53.526500: step 21890, loss = 0.71, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 68h:30m:43s remains)
INFO - root - 2017-12-07 08:43:01.355289: step 21900, loss = 0.83, batch loss = 0.75 (10.2 examples/sec; 0.783 sec/batch; 67h:33m:36s remains)
2017-12-07 08:43:01.956034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3914988 -3.2713509 -3.3063464 -3.3828254 -3.34087 -3.1397738 -2.9264712 -2.7326965 -2.577208 -2.2345877 -1.8016715 -1.7694998 -2.0425885 -2.247179 -2.0255892][-2.3805976 -2.4002335 -2.4823203 -2.4698014 -2.2722528 -1.9355071 -1.7016222 -1.6534071 -1.7121012 -1.5731399 -1.2252538 -1.187906 -1.4660449 -1.7027855 -1.5769403][-1.8866093 -1.9422715 -1.9152205 -1.7756798 -1.5585837 -1.3547883 -1.2889264 -1.3604608 -1.4206829 -1.3147016 -0.99181366 -0.89437985 -1.1032615 -1.3025403 -1.1820581][-2.4721134 -2.5153902 -2.3732378 -2.1334796 -1.9264343 -1.7982776 -1.7638977 -1.7622771 -1.6299822 -1.3904147 -1.0325511 -0.83257341 -0.91713333 -0.99652457 -0.7789607][-3.0398037 -3.0225859 -2.7647851 -2.4759717 -2.3761351 -2.2940907 -2.2216296 -2.1892362 -2.0179534 -1.7486472 -1.3962216 -1.1426365 -1.1200516 -1.0474977 -0.73428082][-3.1185431 -2.9891472 -2.5967035 -2.3597195 -2.4929013 -2.5738969 -2.5365281 -2.4887753 -2.2816296 -1.9173865 -1.5615623 -1.3962858 -1.457149 -1.4137306 -1.0916233][-2.8307238 -2.5269308 -1.9549651 -1.7190442 -1.9725673 -2.2022583 -2.2323828 -2.149719 -1.8330603 -1.3045695 -0.93482542 -0.949352 -1.2284799 -1.337827 -1.0710688][-2.0597804 -1.5322716 -0.87196064 -0.74742246 -1.1515996 -1.5170867 -1.6435974 -1.631752 -1.3710964 -0.87216306 -0.58268905 -0.67944312 -0.96805429 -1.0608013 -0.77535486][-1.6362352 -1.0520689 -0.52018619 -0.62222576 -1.1615286 -1.5988677 -1.700103 -1.6691921 -1.4648361 -1.1003964 -0.95680976 -1.0771697 -1.202225 -1.0782607 -0.70094728][-1.766613 -1.219748 -0.83679366 -0.97208619 -1.445755 -1.8622923 -1.9306765 -1.7754343 -1.4822712 -1.1975904 -1.2319672 -1.4625072 -1.5460851 -1.3173645 -0.90576243][-1.436846 -0.92704797 -0.6332016 -0.69118142 -0.97775888 -1.3271472 -1.4439673 -1.3112595 -1.0497835 -0.90499949 -1.0805871 -1.3796425 -1.4930964 -1.3297288 -1.0069616][-0.86685109 -0.50709629 -0.36266232 -0.40834713 -0.5410316 -0.71742105 -0.69412637 -0.52446795 -0.35448933 -0.3954339 -0.66410303 -0.93507695 -1.0512986 -0.95973516 -0.6994164][-0.63845682 -0.36023569 -0.21978617 -0.19088602 -0.21895409 -0.22725773 -0.04217577 0.14629602 0.17219591 -0.064085007 -0.40290165 -0.63429809 -0.71509361 -0.61507535 -0.28392506][-0.52522612 -0.27665281 -0.12942982 -0.11905193 -0.20680857 -0.24183512 -0.077524662 0.04578495 -0.034226894 -0.30731964 -0.57485771 -0.69613051 -0.69126105 -0.56274033 -0.1642065][-0.86559629 -0.68161058 -0.587239 -0.6230979 -0.73835373 -0.74676824 -0.56912947 -0.48891759 -0.594085 -0.77630591 -0.92791224 -0.97705388 -0.94786549 -0.86626482 -0.53200459]]...]
INFO - root - 2017-12-07 08:43:09.599947: step 21910, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.756 sec/batch; 65h:12m:25s remains)
INFO - root - 2017-12-07 08:43:17.305253: step 21920, loss = 0.63, batch loss = 0.56 (10.4 examples/sec; 0.767 sec/batch; 66h:11m:59s remains)
INFO - root - 2017-12-07 08:43:24.968403: step 21930, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.770 sec/batch; 66h:25m:28s remains)
INFO - root - 2017-12-07 08:43:32.459935: step 21940, loss = 0.70, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 64h:50m:18s remains)
INFO - root - 2017-12-07 08:43:40.139506: step 21950, loss = 0.80, batch loss = 0.72 (10.4 examples/sec; 0.768 sec/batch; 66h:13m:29s remains)
INFO - root - 2017-12-07 08:43:47.948629: step 21960, loss = 0.96, batch loss = 0.88 (10.3 examples/sec; 0.778 sec/batch; 67h:05m:55s remains)
INFO - root - 2017-12-07 08:43:55.695443: step 21970, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.768 sec/batch; 66h:13m:53s remains)
INFO - root - 2017-12-07 08:44:03.235326: step 21980, loss = 0.88, batch loss = 0.81 (11.1 examples/sec; 0.723 sec/batch; 62h:20m:09s remains)
INFO - root - 2017-12-07 08:44:10.787289: step 21990, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.752 sec/batch; 64h:52m:39s remains)
INFO - root - 2017-12-07 08:44:18.480525: step 22000, loss = 0.66, batch loss = 0.59 (10.4 examples/sec; 0.768 sec/batch; 66h:14m:48s remains)
2017-12-07 08:44:19.094171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9409351 -2.6535826 -2.5423272 -2.6271133 -2.7229562 -2.7284133 -2.7922277 -2.832772 -2.9007254 -3.1244211 -3.4193468 -3.5766928 -3.3736482 -2.8752146 -2.4320672][-2.9043984 -2.5004356 -2.3778806 -2.4992194 -2.5434465 -2.4099042 -2.329067 -2.2001171 -2.1717381 -2.469384 -3.0106544 -3.4365251 -3.3904648 -2.9663582 -2.518297][-2.7467365 -2.1981685 -2.066133 -2.2409158 -2.2266574 -1.9750652 -1.725347 -1.3811438 -1.1658688 -1.4514976 -2.2286706 -2.9621387 -3.153975 -2.8636429 -2.4336171][-2.5191517 -1.8877482 -1.8509047 -2.1748283 -2.1490672 -1.7233539 -1.2225895 -0.6813817 -0.35488367 -0.67158604 -1.6463201 -2.5706968 -2.9078922 -2.664566 -2.2230496][-2.3324256 -1.7059338 -1.8170407 -2.2962618 -2.2063959 -1.426095 -0.48556209 0.19370651 0.33369017 -0.28913355 -1.5070369 -2.5303118 -2.8762207 -2.5549479 -2.059638][-2.2185264 -1.5888636 -1.7079275 -2.1305106 -1.8341167 -0.63346291 0.71613073 1.3790913 1.1149998 0.074280262 -1.2573893 -2.2497478 -2.5964675 -2.26284 -1.8314376][-2.1854658 -1.5962565 -1.6836793 -1.9910212 -1.5517855 -0.18110704 1.2805672 1.7806549 1.1588693 -0.030067444 -1.0730605 -1.718528 -1.9530959 -1.707237 -1.5104778][-2.2670989 -1.7872455 -1.9054632 -2.2201669 -1.8267484 -0.65368223 0.56896687 0.87436533 0.075871944 -1.0679669 -1.7292192 -1.9531999 -1.9818223 -1.7650263 -1.7481453][-2.4379759 -2.0630481 -2.1862855 -2.490345 -2.1865511 -1.2693913 -0.34273911 -0.20272875 -1.0277693 -2.0429592 -2.4450707 -2.4274752 -2.4052627 -2.3087492 -2.466475][-2.6498337 -2.3892667 -2.5117416 -2.8014863 -2.5837109 -1.9062767 -1.2159393 -1.1616344 -1.911598 -2.7200031 -2.8487468 -2.6249864 -2.5866513 -2.6646419 -2.9407661][-2.785744 -2.5937958 -2.712357 -2.9770479 -2.8277276 -2.3458772 -1.7944739 -1.7276545 -2.3223827 -2.9212229 -2.8966317 -2.5803852 -2.5334415 -2.7171178 -3.0083048][-2.8273365 -2.6859741 -2.8295856 -3.0932188 -3.0561712 -2.7711217 -2.3417342 -2.2327003 -2.5747683 -2.9161763 -2.8364682 -2.6318107 -2.6712208 -2.9410176 -3.2548962][-2.7743547 -2.7323618 -2.934113 -3.1873236 -3.2183628 -3.0951145 -2.8131528 -2.736218 -2.9586363 -3.1355524 -3.0789633 -3.0280221 -3.1527319 -3.3917575 -3.6496124][-2.6281896 -2.6642942 -2.8929896 -3.12159 -3.2003391 -3.1981711 -3.05445 -3.0482531 -3.2522676 -3.3652611 -3.3440886 -3.3165197 -3.3583002 -3.416183 -3.4706798][-2.5482569 -2.5721056 -2.7146039 -2.8434229 -2.9183989 -2.9986014 -2.9929585 -3.0402987 -3.2084241 -3.2932465 -3.2882557 -3.2149687 -3.1127582 -2.9953341 -2.8823438]]...]
INFO - root - 2017-12-07 08:44:26.731218: step 22010, loss = 0.78, batch loss = 0.71 (10.9 examples/sec; 0.732 sec/batch; 63h:08m:54s remains)
INFO - root - 2017-12-07 08:44:34.371037: step 22020, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.776 sec/batch; 66h:54m:35s remains)
INFO - root - 2017-12-07 08:44:42.029442: step 22030, loss = 0.64, batch loss = 0.57 (10.6 examples/sec; 0.753 sec/batch; 64h:57m:47s remains)
INFO - root - 2017-12-07 08:44:49.471866: step 22040, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.773 sec/batch; 66h:37m:37s remains)
INFO - root - 2017-12-07 08:44:57.101170: step 22050, loss = 0.70, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 65h:51m:07s remains)
INFO - root - 2017-12-07 08:45:04.756185: step 22060, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.753 sec/batch; 64h:57m:05s remains)
INFO - root - 2017-12-07 08:45:12.269851: step 22070, loss = 0.89, batch loss = 0.82 (10.8 examples/sec; 0.743 sec/batch; 64h:01m:36s remains)
INFO - root - 2017-12-07 08:45:20.005660: step 22080, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.768 sec/batch; 66h:14m:01s remains)
INFO - root - 2017-12-07 08:45:27.730957: step 22090, loss = 0.66, batch loss = 0.59 (10.6 examples/sec; 0.757 sec/batch; 65h:15m:48s remains)
INFO - root - 2017-12-07 08:45:36.767999: step 22100, loss = 1.13, batch loss = 1.06 (7.6 examples/sec; 1.054 sec/batch; 90h:53m:35s remains)
2017-12-07 08:45:37.565758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5102842 -2.8367469 -3.0955153 -3.1909943 -3.2003748 -3.0981927 -2.9786263 -2.9248829 -2.9092278 -2.9184227 -2.93982 -2.9895697 -2.9533663 -2.7798104 -2.5777972][-2.3261728 -2.6951408 -3.0041785 -3.2124825 -3.3253636 -3.2547994 -3.1587579 -3.1176143 -3.0777876 -3.0388956 -2.9879918 -2.9550505 -2.8644602 -2.6816854 -2.5169554][-2.0627537 -2.3522089 -2.5627904 -2.7646849 -2.8750734 -2.8005345 -2.7266769 -2.7337542 -2.8271263 -2.9954557 -3.0777111 -3.0772848 -2.988987 -2.7789755 -2.5859141][-1.8877928 -2.02024 -1.95808 -1.8889527 -1.7262073 -1.5237005 -1.5149124 -1.6317451 -1.9089732 -2.2761312 -2.5092556 -2.6316547 -2.6378746 -2.4912422 -2.3340917][-1.7371433 -1.6246159 -1.1674089 -0.70743442 -0.24493313 -0.043316364 -0.26976013 -0.69332647 -1.2188332 -1.5785036 -1.677222 -1.5962629 -1.3906975 -1.1646702 -1.068927][-1.5437729 -1.1974821 -0.49149871 0.13738441 0.63381863 0.62675571 0.077867985 -0.75949287 -1.5706363 -1.9429975 -1.9082379 -1.5061252 -0.8408258 -0.3106041 -0.0771389][-1.187187 -0.71081185 -0.14054823 0.12476683 0.18003035 -0.073483944 -0.63278341 -1.35605 -1.8855648 -1.9674838 -1.7568283 -1.2086403 -0.48035908 -0.025019646 0.17395973][-0.77698255 -0.38143587 -0.31862116 -0.8132143 -1.4264095 -1.9301741 -2.3897986 -2.6660552 -2.5386095 -2.0705457 -1.6016891 -1.0945208 -0.68384147 -0.56771708 -0.54960251][-0.59306836 -0.4674468 -0.97350073 -2.0101719 -2.9391727 -3.4724774 -3.76257 -3.7402434 -3.2962356 -2.6810772 -2.2727094 -2.0410578 -1.9753697 -1.9544446 -1.847034][-0.78367996 -1.0526009 -1.9998918 -3.2303042 -4.0871463 -4.38748 -4.4060383 -4.2460871 -3.8546081 -3.4795775 -3.4243188 -3.634274 -3.8255954 -3.7919858 -3.5584645][-1.0739346 -1.4913802 -2.4714856 -3.5586827 -4.1984558 -4.3316154 -4.2165527 -4.0154204 -3.7393525 -3.6390696 -3.9451137 -4.4877815 -4.8407612 -4.86152 -4.6438627][-1.4316759 -1.7543845 -2.4574423 -3.1518831 -3.4139144 -3.3299909 -3.1179583 -2.8867154 -2.6658487 -2.6323307 -2.9319663 -3.3643365 -3.6053722 -3.5974576 -3.4452941][-1.8931675 -2.2056479 -2.7516937 -3.1853251 -3.1783056 -2.932055 -2.6212726 -2.3676133 -2.1982775 -2.1497147 -2.258393 -2.3810322 -2.3910854 -2.3268445 -2.2464664][-2.2636051 -2.73646 -3.3794706 -3.8453679 -3.8886275 -3.7533116 -3.5952764 -3.4766879 -3.41737 -3.4109263 -3.4163046 -3.3776231 -3.257946 -3.1274719 -3.0328493][-2.3791387 -2.9749904 -3.7291217 -4.3234506 -4.5403814 -4.5534458 -4.507607 -4.4516864 -4.4343834 -4.5034614 -4.5798607 -4.6077027 -4.563077 -4.4873953 -4.4224529]]...]
INFO - root - 2017-12-07 08:45:48.234907: step 22110, loss = 0.78, batch loss = 0.71 (7.5 examples/sec; 1.062 sec/batch; 91h:31m:39s remains)
INFO - root - 2017-12-07 08:45:58.928021: step 22120, loss = 0.78, batch loss = 0.71 (7.6 examples/sec; 1.059 sec/batch; 91h:18m:41s remains)
INFO - root - 2017-12-07 08:46:09.621238: step 22130, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.063 sec/batch; 91h:40m:54s remains)
INFO - root - 2017-12-07 08:46:20.167492: step 22140, loss = 0.80, batch loss = 0.72 (7.5 examples/sec; 1.067 sec/batch; 91h:58m:19s remains)
INFO - root - 2017-12-07 08:46:30.778903: step 22150, loss = 0.72, batch loss = 0.65 (7.7 examples/sec; 1.038 sec/batch; 89h:26m:41s remains)
INFO - root - 2017-12-07 08:46:41.443379: step 22160, loss = 0.74, batch loss = 0.67 (7.6 examples/sec; 1.051 sec/batch; 90h:36m:54s remains)
INFO - root - 2017-12-07 08:46:52.229760: step 22170, loss = 0.65, batch loss = 0.58 (7.4 examples/sec; 1.079 sec/batch; 93h:02m:03s remains)
INFO - root - 2017-12-07 08:47:02.813379: step 22180, loss = 0.86, batch loss = 0.79 (7.6 examples/sec; 1.054 sec/batch; 90h:51m:54s remains)
INFO - root - 2017-12-07 08:47:13.357693: step 22190, loss = 0.83, batch loss = 0.75 (7.5 examples/sec; 1.061 sec/batch; 91h:29m:23s remains)
INFO - root - 2017-12-07 08:47:23.899807: step 22200, loss = 0.78, batch loss = 0.71 (7.2 examples/sec; 1.107 sec/batch; 95h:26m:21s remains)
2017-12-07 08:47:24.743292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1068792 -2.71276 -3.1067638 -2.9094973 -2.6495447 -2.7911141 -3.0289564 -3.3676052 -3.5905602 -3.6155281 -3.54242 -3.1551752 -2.3772786 -1.9212992 -2.1656063][-1.7744145 -2.2968822 -2.5839605 -2.4412177 -2.3320632 -2.5197539 -2.738672 -2.9600163 -3.0176802 -2.895524 -2.7991548 -2.521554 -1.8423166 -1.4079354 -1.7246926][-1.8201954 -2.1952121 -2.3093412 -2.128619 -2.0115764 -2.0161736 -2.0217493 -2.0479157 -1.9819596 -1.8906097 -2.0104051 -2.1011119 -1.7063234 -1.3092341 -1.5707004][-2.0822098 -2.3676808 -2.5336633 -2.5937829 -2.659049 -2.6406875 -2.4551826 -2.2011361 -1.8533556 -1.6764088 -1.9745374 -2.4148917 -2.2434938 -1.7381246 -1.7901468][-1.5422103 -1.8925476 -2.3122351 -2.7192602 -3.0515203 -3.2156496 -3.0753369 -2.7582061 -2.2300572 -1.8386834 -2.0779445 -2.6257424 -2.6035085 -1.9839149 -1.7892699][-0.87977028 -1.1443408 -1.5263157 -1.8467534 -2.0654659 -2.2536995 -2.2414293 -2.0905881 -1.6291018 -1.1184537 -1.2673712 -1.8865242 -2.1346555 -1.6758327 -1.3898191][-0.80788469 -0.75020289 -0.77191615 -0.61242437 -0.33447647 -0.21246243 -0.18440628 -0.28199387 -0.14066601 0.21434355 0.02177906 -0.66836429 -1.164093 -0.93310738 -0.56226087][-0.47796988 -0.31818724 -0.2957468 -0.089128494 0.34367466 0.64961195 0.70744848 0.44668674 0.26331234 0.30121326 0.038922787 -0.620029 -1.1425977 -0.87762809 -0.18360376][-0.41040182 -0.43576479 -0.69139671 -0.89971614 -0.88169837 -0.84469509 -0.83081222 -0.97069883 -1.0483024 -0.93665504 -0.97037125 -1.3911715 -1.833657 -1.5593126 -0.69371748][-0.60614753 -0.68759394 -1.0424573 -1.5226042 -1.8283501 -1.9540739 -1.9438722 -1.9735537 -1.9860935 -1.8584085 -1.7630954 -2.0544348 -2.471034 -2.3066046 -1.5957301][-0.90952373 -0.85397959 -1.0749626 -1.5587149 -1.8846502 -1.8672829 -1.6190374 -1.4184349 -1.4159539 -1.4676213 -1.5452533 -1.9394293 -2.4407539 -2.4778552 -2.076237][-1.6359806 -1.37393 -1.3434479 -1.6123309 -1.7828245 -1.617348 -1.2320876 -0.88540387 -0.81877041 -0.9331 -1.0938895 -1.5291164 -2.0335953 -2.1525817 -1.964664][-2.4625678 -2.1820056 -2.1029127 -2.3092906 -2.4320192 -2.2952859 -2.0128107 -1.782598 -1.7873449 -1.953722 -2.0717552 -2.3081207 -2.5345752 -2.503057 -2.3529437][-3.0068645 -2.884459 -2.9474978 -3.2245135 -3.3921454 -3.3329418 -3.1453493 -2.9621255 -2.9505939 -3.1099446 -3.2117157 -3.3082728 -3.3254304 -3.1609535 -3.0360081][-3.282625 -3.2262836 -3.2576761 -3.445555 -3.587182 -3.6301739 -3.5887382 -3.4954381 -3.4681835 -3.5736222 -3.6644669 -3.7421815 -3.7441776 -3.6309607 -3.6117647]]...]
INFO - root - 2017-12-07 08:47:35.326899: step 22210, loss = 0.88, batch loss = 0.81 (7.7 examples/sec; 1.040 sec/batch; 89h:39m:50s remains)
INFO - root - 2017-12-07 08:47:45.881410: step 22220, loss = 0.80, batch loss = 0.73 (7.5 examples/sec; 1.063 sec/batch; 91h:35m:30s remains)
INFO - root - 2017-12-07 08:47:56.560382: step 22230, loss = 0.96, batch loss = 0.88 (7.6 examples/sec; 1.056 sec/batch; 90h:59m:21s remains)
INFO - root - 2017-12-07 08:48:07.201903: step 22240, loss = 0.80, batch loss = 0.73 (7.5 examples/sec; 1.066 sec/batch; 91h:52m:10s remains)
INFO - root - 2017-12-07 08:48:17.943511: step 22250, loss = 0.84, batch loss = 0.77 (7.2 examples/sec; 1.107 sec/batch; 95h:23m:12s remains)
INFO - root - 2017-12-07 08:48:28.691105: step 22260, loss = 0.69, batch loss = 0.62 (7.5 examples/sec; 1.066 sec/batch; 91h:50m:33s remains)
INFO - root - 2017-12-07 08:48:39.364895: step 22270, loss = 0.67, batch loss = 0.59 (7.4 examples/sec; 1.078 sec/batch; 92h:54m:06s remains)
INFO - root - 2017-12-07 08:48:50.079076: step 22280, loss = 0.76, batch loss = 0.68 (7.8 examples/sec; 1.029 sec/batch; 88h:41m:59s remains)
INFO - root - 2017-12-07 08:49:00.542254: step 22290, loss = 0.70, batch loss = 0.63 (8.1 examples/sec; 0.984 sec/batch; 84h:49m:39s remains)
INFO - root - 2017-12-07 08:49:11.253490: step 22300, loss = 0.74, batch loss = 0.67 (7.2 examples/sec; 1.113 sec/batch; 95h:55m:21s remains)
2017-12-07 08:49:12.061374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.010685 -4.0605278 -4.17975 -4.2526803 -4.1385617 -3.6986709 -3.1143188 -2.6931956 -2.6404672 -2.9899492 -3.5604646 -4.0212975 -4.1709704 -4.0533319 -3.8832846][-4.4820967 -4.6117406 -4.7727642 -4.78148 -4.4771047 -3.7119937 -2.7857685 -2.1123581 -1.9627805 -2.491323 -3.3430481 -3.9864619 -4.1643353 -3.9584043 -3.7152553][-4.9924374 -5.2301178 -5.4503 -5.4082747 -4.9041538 -3.8014398 -2.4714103 -1.4812768 -1.2302792 -1.9643636 -3.0610731 -3.8321586 -4.0289865 -3.76543 -3.48704][-5.157043 -5.4206762 -5.6661353 -5.6254325 -5.0403438 -3.7544274 -2.0853374 -0.77137542 -0.45156193 -1.4097552 -2.7022014 -3.6077254 -3.854064 -3.5508108 -3.2563941][-4.8379259 -4.9649878 -5.1237698 -5.0425987 -4.4071546 -3.0503836 -1.1958942 0.23705292 0.34532022 -0.93317747 -2.3591516 -3.3580296 -3.6261098 -3.269496 -2.9733706][-4.3605437 -4.3236742 -4.3791094 -4.19859 -3.3963368 -1.8425539 0.21758366 1.5835724 1.1315727 -0.61175919 -2.1629212 -3.2062945 -3.4696856 -3.0573764 -2.7512183][-4.08134 -3.9492955 -4.00834 -3.778667 -2.7054665 -0.75040793 1.6262741 2.8592238 1.6801472 -0.50026512 -2.1125002 -3.1316817 -3.3916557 -3.0013692 -2.7018566][-4.1037197 -3.9720414 -4.0988612 -3.8706753 -2.5269568 -0.13193989 2.5529828 3.6178761 1.8280644 -0.61416173 -2.2450461 -3.1897302 -3.3864756 -3.0105333 -2.7130089][-4.1925244 -4.1390738 -4.3588085 -4.19425 -2.7446122 -0.15551805 2.4861312 3.2840428 1.4947343 -0.79185939 -2.3243234 -3.1831079 -3.3007915 -2.9275293 -2.6144938][-4.1397 -4.184433 -4.4403481 -4.3611274 -2.9945426 -0.58212519 1.5838432 2.0992846 0.79837179 -0.97743654 -2.2643933 -3.040453 -3.1530123 -2.8261547 -2.4679456][-3.8663781 -3.9870665 -4.2465787 -4.2764473 -3.1318712 -1.0530012 0.62809086 0.96829128 0.12062311 -1.1890168 -2.2070534 -2.8571687 -2.995465 -2.7373056 -2.3626308][-3.4282961 -3.5663946 -3.8211706 -3.9786341 -3.1548927 -1.4518683 -0.14727545 0.078016281 -0.57012033 -1.5772545 -2.356842 -2.8652048 -2.9816773 -2.7740886 -2.4696751][-3.119504 -3.1796312 -3.3351989 -3.4995024 -2.9818704 -1.7045977 -0.69154978 -0.51459146 -1.0929368 -2.003057 -2.6984596 -3.1476674 -3.1932402 -2.9934669 -2.7986712][-2.9176278 -2.8522944 -2.8701334 -2.9822946 -2.7288642 -1.8978255 -1.1593397 -1.0280287 -1.5343385 -2.3708661 -3.0272841 -3.4455097 -3.4320209 -3.228339 -3.0982141][-2.7356324 -2.5886977 -2.532372 -2.5780501 -2.4496188 -1.9618742 -1.4919264 -1.4547145 -1.8966403 -2.6133227 -3.1957631 -3.5645671 -3.5322404 -3.338629 -3.2202344]]...]
INFO - root - 2017-12-07 08:49:22.678876: step 22310, loss = 0.86, batch loss = 0.79 (7.7 examples/sec; 1.041 sec/batch; 89h:42m:48s remains)
INFO - root - 2017-12-07 08:49:33.467135: step 22320, loss = 0.76, batch loss = 0.68 (7.3 examples/sec; 1.094 sec/batch; 94h:15m:53s remains)
INFO - root - 2017-12-07 08:49:44.329292: step 22330, loss = 0.91, batch loss = 0.84 (7.1 examples/sec; 1.122 sec/batch; 96h:37m:42s remains)
INFO - root - 2017-12-07 08:49:54.893855: step 22340, loss = 0.84, batch loss = 0.77 (7.4 examples/sec; 1.080 sec/batch; 93h:00m:20s remains)
INFO - root - 2017-12-07 08:50:05.569341: step 22350, loss = 0.97, batch loss = 0.89 (7.5 examples/sec; 1.066 sec/batch; 91h:48m:55s remains)
INFO - root - 2017-12-07 08:50:16.132102: step 22360, loss = 0.75, batch loss = 0.68 (7.7 examples/sec; 1.045 sec/batch; 90h:01m:32s remains)
INFO - root - 2017-12-07 08:50:26.965271: step 22370, loss = 0.85, batch loss = 0.78 (7.4 examples/sec; 1.077 sec/batch; 92h:47m:25s remains)
INFO - root - 2017-12-07 08:50:37.501121: step 22380, loss = 0.69, batch loss = 0.61 (7.6 examples/sec; 1.059 sec/batch; 91h:11m:45s remains)
INFO - root - 2017-12-07 08:50:48.340556: step 22390, loss = 0.66, batch loss = 0.59 (7.3 examples/sec; 1.102 sec/batch; 94h:56m:23s remains)
INFO - root - 2017-12-07 08:50:59.131531: step 22400, loss = 0.64, batch loss = 0.57 (7.6 examples/sec; 1.057 sec/batch; 91h:04m:31s remains)
2017-12-07 08:50:59.994757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2987669 -3.490644 -3.6932261 -3.811202 -3.7867825 -3.6811819 -3.593132 -3.5446713 -3.5110784 -3.4718642 -3.4235497 -3.3826885 -3.3588796 -3.3615141 -3.3820543][-3.3534594 -3.5865088 -3.822053 -3.9378226 -3.8774908 -3.725338 -3.6259005 -3.5972855 -3.5839751 -3.5455947 -3.4883375 -3.4248717 -3.369307 -3.3528175 -3.3769979][-3.3504167 -3.5816145 -3.7882428 -3.8470831 -3.7206764 -3.5196166 -3.4151428 -3.4205661 -3.4475443 -3.4160147 -3.336864 -3.2400267 -3.1536536 -3.1344666 -3.189966][-3.2759204 -3.4742079 -3.6177552 -3.6053424 -3.4400222 -3.2435851 -3.1775889 -3.2545772 -3.3375 -3.3107195 -3.1861033 -3.0375829 -2.9091468 -2.8805227 -2.9813979][-3.143043 -3.2931757 -3.3685527 -3.3078451 -3.1651816 -3.0456386 -3.0606065 -3.2012982 -3.304868 -3.250282 -3.0689151 -2.869633 -2.6847553 -2.6138606 -2.7362247][-3.0244429 -3.1410441 -3.1747103 -3.0880103 -2.9711027 -2.9083996 -2.9725471 -3.1327693 -3.2225552 -3.1194196 -2.8938191 -2.6658449 -2.4445977 -2.3452878 -2.4859326][-2.9312243 -3.0572681 -3.1050534 -3.0354924 -2.9411609 -2.8885698 -2.954885 -3.1282632 -3.23344 -3.096535 -2.8175149 -2.5364089 -2.2578738 -2.1564772 -2.3444085][-2.8646941 -3.0052822 -3.0807552 -3.0310521 -2.907721 -2.7904415 -2.8049736 -3.0082588 -3.2161193 -3.1985302 -3.0074744 -2.7334833 -2.3850925 -2.2195904 -2.4014573][-2.8025482 -2.9229198 -3.0017262 -2.9584422 -2.7893219 -2.5994482 -2.5665243 -2.8044429 -3.1408815 -3.323349 -3.2997599 -3.0832331 -2.7011886 -2.4493034 -2.550997][-2.7390552 -2.8155377 -2.9030786 -2.9052844 -2.752146 -2.5557332 -2.4832087 -2.6388268 -2.9431863 -3.1948733 -3.2802293 -3.1317225 -2.8036251 -2.5742395 -2.6690626][-2.734736 -2.7880392 -2.9121008 -2.9858184 -2.8767672 -2.6694443 -2.4960175 -2.4554431 -2.5899277 -2.8005362 -2.9539914 -2.9199109 -2.738544 -2.632267 -2.767735][-2.7577372 -2.808578 -2.9609945 -3.0841832 -3.0213206 -2.8272474 -2.6099637 -2.4379497 -2.4104602 -2.5197635 -2.671803 -2.7455282 -2.7459583 -2.7822719 -2.9376915][-2.7660732 -2.7906747 -2.9245515 -3.0436344 -3.0112672 -2.8749847 -2.7246995 -2.6019292 -2.5796413 -2.6581221 -2.7701831 -2.8421092 -2.9047852 -2.9640346 -3.0497527][-2.7917209 -2.7735758 -2.8481648 -2.9304447 -2.9218435 -2.8517997 -2.7937722 -2.775 -2.8407707 -2.9594512 -3.0265083 -3.0088792 -2.96994 -2.9130745 -2.8605814][-2.8521872 -2.8194425 -2.8492255 -2.8933058 -2.897645 -2.8759944 -2.8818119 -2.9115243 -2.9871423 -3.0643692 -3.0440392 -2.927628 -2.7962816 -2.6615894 -2.5384951]]...]
INFO - root - 2017-12-07 08:51:10.608143: step 22410, loss = 0.74, batch loss = 0.67 (7.4 examples/sec; 1.081 sec/batch; 93h:04m:54s remains)
INFO - root - 2017-12-07 08:51:21.323522: step 22420, loss = 0.93, batch loss = 0.85 (7.3 examples/sec; 1.091 sec/batch; 93h:58m:41s remains)
INFO - root - 2017-12-07 08:51:32.056603: step 22430, loss = 0.88, batch loss = 0.81 (7.6 examples/sec; 1.059 sec/batch; 91h:12m:28s remains)
INFO - root - 2017-12-07 08:51:42.599431: step 22440, loss = 0.82, batch loss = 0.75 (7.4 examples/sec; 1.077 sec/batch; 92h:45m:00s remains)
INFO - root - 2017-12-07 08:51:53.294973: step 22450, loss = 0.65, batch loss = 0.58 (7.5 examples/sec; 1.072 sec/batch; 92h:18m:43s remains)
INFO - root - 2017-12-07 08:52:04.119034: step 22460, loss = 0.76, batch loss = 0.68 (7.4 examples/sec; 1.075 sec/batch; 92h:34m:10s remains)
INFO - root - 2017-12-07 08:52:14.770799: step 22470, loss = 0.67, batch loss = 0.60 (7.6 examples/sec; 1.051 sec/batch; 90h:28m:07s remains)
INFO - root - 2017-12-07 08:52:25.521263: step 22480, loss = 1.20, batch loss = 1.13 (7.4 examples/sec; 1.077 sec/batch; 92h:43m:47s remains)
INFO - root - 2017-12-07 08:52:36.260874: step 22490, loss = 0.80, batch loss = 0.72 (7.4 examples/sec; 1.079 sec/batch; 92h:53m:08s remains)
INFO - root - 2017-12-07 08:52:46.929956: step 22500, loss = 0.73, batch loss = 0.66 (7.3 examples/sec; 1.097 sec/batch; 94h:25m:31s remains)
2017-12-07 08:52:47.794028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.85066676 -0.81298351 -0.87183619 -1.0438299 -1.3076658 -1.626142 -1.9662278 -2.0918 -1.9201326 -1.4748309 -0.83419609 -0.33143282 -0.25800705 -0.47453403 -0.71215749][-0.76699376 -0.656693 -0.64377809 -0.77123952 -1.0228224 -1.3960819 -1.9039214 -2.2811329 -2.3702137 -2.0722227 -1.3883109 -0.74391294 -0.55578232 -0.71091843 -0.89340925][-0.75939274 -0.6338079 -0.59098506 -0.65231228 -0.75757241 -0.96983624 -1.3782721 -1.8156843 -2.1269381 -2.0828834 -1.5646412 -0.97024536 -0.788044 -0.91933465 -1.0358179][-0.79719186 -0.70009565 -0.69536352 -0.76823068 -0.78105187 -0.782922 -0.87875962 -1.0888445 -1.3750126 -1.4749236 -1.1813323 -0.77249289 -0.70584726 -0.88911009 -0.98675466][-0.79948068 -0.7183516 -0.77867556 -0.94878721 -0.98456335 -0.88890982 -0.72509933 -0.66183233 -0.78495407 -0.88282681 -0.72535968 -0.48227119 -0.52140975 -0.753397 -0.85958076][-0.79520941 -0.73018622 -0.83459783 -1.0748131 -1.1740954 -1.1382918 -0.94901705 -0.81553388 -0.84706616 -0.870116 -0.70815682 -0.47942734 -0.51274824 -0.72201085 -0.81662464][-0.78210926 -0.69518042 -0.76280427 -0.95610285 -1.1003458 -1.2150185 -1.2039826 -1.2142303 -1.2914138 -1.28825 -1.0816467 -0.79978061 -0.74898553 -0.8631227 -0.89678621][-0.77949786 -0.662091 -0.6668911 -0.78487515 -0.96454525 -1.1536458 -1.2379041 -1.3548758 -1.4838898 -1.5144253 -1.3513093 -1.1060374 -1.0227258 -1.0583718 -1.0201523][-0.81084895 -0.70999432 -0.70854068 -0.81832337 -1.0457752 -1.2104177 -1.1923647 -1.2199018 -1.2739038 -1.3181853 -1.2649133 -1.1275961 -1.0805902 -1.1040027 -1.0432072][-0.83886576 -0.777236 -0.80848217 -0.94845247 -1.2340264 -1.3551004 -1.2187443 -1.0918682 -0.98595214 -0.986758 -1.0099261 -0.94610763 -0.92995954 -0.9902842 -0.96949649][-0.83139014 -0.79381561 -0.84382129 -0.98919964 -1.2868068 -1.3473661 -1.151628 -0.94840264 -0.75286841 -0.74215484 -0.81953096 -0.77896357 -0.77307773 -0.87744713 -0.91420317][-0.79310346 -0.73806667 -0.74975657 -0.83622074 -1.070559 -1.057713 -0.8628118 -0.70394492 -0.55635977 -0.63566756 -0.79074574 -0.77316236 -0.78271985 -0.90689969 -0.96421123][-0.74972725 -0.64818144 -0.57485271 -0.56008267 -0.69686651 -0.6325233 -0.49884629 -0.45046067 -0.42589378 -0.63245988 -0.85403085 -0.864182 -0.88679218 -0.99790049 -1.0436585][-0.7354455 -0.59953475 -0.45787811 -0.38515997 -0.45759535 -0.37710333 -0.3139987 -0.35221577 -0.42498875 -0.69183588 -0.91620922 -0.94690418 -0.97608542 -1.0466633 -1.0669942][-0.7583766 -0.624583 -0.47284484 -0.42000771 -0.49310279 -0.44129944 -0.43964624 -0.52209496 -0.62788177 -0.86059904 -1.0172625 -1.0374713 -1.0494628 -1.0612643 -1.052536]]...]
INFO - root - 2017-12-07 08:52:58.520291: step 22510, loss = 0.90, batch loss = 0.83 (7.4 examples/sec; 1.075 sec/batch; 92h:35m:08s remains)
INFO - root - 2017-12-07 08:53:09.239339: step 22520, loss = 1.04, batch loss = 0.97 (7.6 examples/sec; 1.053 sec/batch; 90h:39m:38s remains)
INFO - root - 2017-12-07 08:53:20.015138: step 22530, loss = 0.77, batch loss = 0.70 (7.4 examples/sec; 1.076 sec/batch; 92h:40m:48s remains)
INFO - root - 2017-12-07 08:53:30.649504: step 22540, loss = 0.92, batch loss = 0.84 (7.4 examples/sec; 1.087 sec/batch; 93h:36m:05s remains)
INFO - root - 2017-12-07 08:53:41.226715: step 22550, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.064 sec/batch; 91h:38m:46s remains)
INFO - root - 2017-12-07 08:53:52.015024: step 22560, loss = 1.05, batch loss = 0.98 (7.6 examples/sec; 1.060 sec/batch; 91h:13m:22s remains)
INFO - root - 2017-12-07 08:54:02.768087: step 22570, loss = 0.68, batch loss = 0.60 (7.5 examples/sec; 1.062 sec/batch; 91h:25m:17s remains)
INFO - root - 2017-12-07 08:54:13.422505: step 22580, loss = 0.94, batch loss = 0.87 (7.5 examples/sec; 1.064 sec/batch; 91h:36m:37s remains)
INFO - root - 2017-12-07 08:54:24.148818: step 22590, loss = 0.92, batch loss = 0.85 (7.5 examples/sec; 1.062 sec/batch; 91h:26m:30s remains)
INFO - root - 2017-12-07 08:54:34.802225: step 22600, loss = 1.10, batch loss = 1.03 (7.6 examples/sec; 1.054 sec/batch; 90h:46m:06s remains)
2017-12-07 08:54:35.663230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1832209 -4.1689224 -4.171946 -4.2543592 -4.4256015 -4.5934315 -4.6922226 -4.7463317 -4.7843938 -4.7470098 -4.705461 -4.6559739 -4.5723014 -4.5015397 -4.4384007][-4.3223982 -4.2834415 -4.2797589 -4.3987527 -4.6709204 -4.9205112 -5.0823884 -5.1556191 -5.2203646 -5.2091928 -5.2091904 -5.2045889 -5.0655084 -4.9248085 -4.8154211][-4.5767632 -4.3782315 -4.189683 -4.1589808 -4.3803411 -4.6088333 -4.806201 -4.9328713 -5.0552707 -5.0976548 -5.1798844 -5.2445278 -5.035522 -4.8369546 -4.7972612][-4.6137624 -4.1152086 -3.6254587 -3.3317106 -3.4259784 -3.5557516 -3.7113004 -3.8534615 -3.9755707 -4.0866852 -4.2722969 -4.3889875 -4.1466532 -3.959089 -4.1463575][-4.226624 -3.3529148 -2.5885425 -2.0928085 -2.0594461 -2.0972805 -2.12235 -2.0762763 -2.0429 -2.0955598 -2.2665739 -2.3346586 -2.1455381 -2.2089641 -2.8537862][-3.8071582 -2.7184231 -1.7884634 -1.208178 -1.0159135 -0.92187119 -0.63671231 -0.22245073 -0.091522217 -0.038440704 -0.0978713 -0.016895771 0.038329124 -0.40337753 -1.5415542][-3.4570832 -2.2338612 -1.0862637 -0.29707146 0.12611103 0.51722717 1.3000669 2.0862765 1.9280548 1.6520758 1.5683503 1.7158303 1.6075559 0.87142849 -0.57610869][-3.16765 -1.8361604 -0.46679258 0.48079586 1.0016189 1.665328 2.9068894 3.9344215 3.236095 2.4206786 2.1402693 2.1712685 1.9510036 1.2033691 -0.22817993][-3.0869589 -1.8239739 -0.56670523 0.15132904 0.42569113 0.9668622 1.9751358 2.6745548 1.9783978 1.207118 0.92783165 0.85682106 0.72635317 0.27932835 -0.84329605][-3.1512508 -2.1859763 -1.3292131 -0.87674856 -0.74691272 -0.36938381 0.17178869 0.49040651 0.044949055 -0.41775131 -0.52367306 -0.51652479 -0.49047089 -0.68328 -1.556906][-3.2863941 -2.6488369 -2.1652486 -1.8116906 -1.5428607 -1.1360588 -0.78663206 -0.67752528 -0.9823873 -1.3380845 -1.4235919 -1.3659401 -1.3000996 -1.4740977 -2.1866882][-3.5854964 -3.2314177 -3.015861 -2.742557 -2.4351702 -2.1076593 -1.9045072 -1.8855309 -2.0797753 -2.3589818 -2.4359293 -2.3269064 -2.2832487 -2.4785326 -2.9887118][-3.890141 -3.7295184 -3.6922905 -3.556838 -3.362468 -3.1836083 -3.0741987 -3.0696428 -3.1518383 -3.2911022 -3.2936275 -3.1560924 -3.1651065 -3.3765635 -3.6828878][-4.0745749 -4.01548 -4.0531359 -4.0037913 -3.8839116 -3.7689102 -3.7109926 -3.735399 -3.7988675 -3.8856604 -3.8615122 -3.7431071 -3.7693408 -3.9264421 -4.04439][-4.2286053 -4.2438445 -4.2960739 -4.2960105 -4.2441735 -4.179841 -4.1583481 -4.2001176 -4.2727585 -4.3427672 -4.3177342 -4.2223825 -4.2157249 -4.2697859 -4.2501478]]...]
INFO - root - 2017-12-07 08:54:46.397328: step 22610, loss = 0.86, batch loss = 0.79 (7.5 examples/sec; 1.074 sec/batch; 92h:25m:58s remains)
INFO - root - 2017-12-07 08:54:57.243634: step 22620, loss = 0.91, batch loss = 0.83 (7.4 examples/sec; 1.079 sec/batch; 92h:51m:05s remains)
INFO - root - 2017-12-07 08:55:07.874068: step 22630, loss = 0.81, batch loss = 0.74 (7.5 examples/sec; 1.071 sec/batch; 92h:12m:01s remains)
INFO - root - 2017-12-07 08:55:18.343798: step 22640, loss = 0.82, batch loss = 0.74 (7.4 examples/sec; 1.081 sec/batch; 93h:03m:19s remains)
INFO - root - 2017-12-07 08:55:28.949708: step 22650, loss = 0.59, batch loss = 0.51 (7.5 examples/sec; 1.067 sec/batch; 91h:50m:22s remains)
INFO - root - 2017-12-07 08:55:39.612529: step 22660, loss = 0.88, batch loss = 0.81 (7.3 examples/sec; 1.098 sec/batch; 94h:32m:07s remains)
INFO - root - 2017-12-07 08:55:50.303453: step 22670, loss = 0.69, batch loss = 0.61 (7.6 examples/sec; 1.057 sec/batch; 91h:00m:32s remains)
INFO - root - 2017-12-07 08:56:01.080006: step 22680, loss = 0.88, batch loss = 0.81 (7.4 examples/sec; 1.088 sec/batch; 93h:37m:05s remains)
INFO - root - 2017-12-07 08:56:11.849178: step 22690, loss = 0.94, batch loss = 0.86 (7.1 examples/sec; 1.134 sec/batch; 97h:34m:18s remains)
INFO - root - 2017-12-07 08:56:22.626047: step 22700, loss = 0.85, batch loss = 0.78 (7.5 examples/sec; 1.062 sec/batch; 91h:21m:43s remains)
2017-12-07 08:56:23.421667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7732227 -3.8525848 -3.9484119 -4.0478129 -4.0949244 -4.0696583 -4.0193424 -3.9263639 -3.8849356 -3.9163969 -3.7983158 -3.6350527 -3.4837604 -3.4642103 -3.6171827][-3.8022752 -3.8704298 -3.9238374 -3.9497252 -3.8843462 -3.7496922 -3.5818522 -3.3899446 -3.3580916 -3.4148641 -3.2241721 -2.9439356 -2.7063432 -2.7037058 -2.9381237][-3.7930148 -3.8056722 -3.7661126 -3.6760948 -3.4950109 -3.3000743 -3.1158147 -2.923182 -2.9178536 -2.9511371 -2.6836371 -2.3183329 -2.0768845 -2.1686301 -2.4717484][-3.7574863 -3.7001517 -3.5829754 -3.43975 -3.2200282 -3.038775 -2.8999348 -2.723743 -2.648632 -2.5714071 -2.2544181 -1.8896198 -1.8069813 -2.0640211 -2.3302941][-3.7003646 -3.5311284 -3.314595 -3.1237264 -2.8273835 -2.5441608 -2.3389041 -2.1151094 -1.9509673 -1.9194798 -1.7755859 -1.5901289 -1.8053758 -2.2436228 -2.3377836][-3.6703222 -3.3659697 -3.0494924 -2.8079085 -2.3688288 -1.9014428 -1.597662 -1.3359056 -1.1927907 -1.4696183 -1.684675 -1.6722145 -2.1209514 -2.6461785 -2.4858623][-3.7177498 -3.3051739 -2.9275489 -2.5964494 -1.9731476 -1.3185432 -0.87839985 -0.42697525 -0.3156085 -1.0087359 -1.5689893 -1.5936193 -2.0523257 -2.4888844 -2.083899][-3.8159146 -3.3658342 -2.9333282 -2.415837 -1.5689754 -0.76376629 -0.1364007 0.58185339 0.67830706 -0.3414402 -1.0829043 -1.1328154 -1.5458536 -1.7874305 -1.2567811][-3.950711 -3.600045 -3.1598282 -2.4265406 -1.3939247 -0.53615022 0.17006111 0.99489355 1.0802321 0.042340755 -0.6287539 -0.7159884 -1.0376887 -1.0209935 -0.50000215][-4.0885453 -3.9434009 -3.5579259 -2.6880786 -1.6130695 -0.80688167 -0.12106562 0.560771 0.626235 -0.0093398094 -0.41888428 -0.59034991 -0.83280826 -0.59101152 -0.16000843][-4.1834006 -4.2398319 -3.9579978 -3.137207 -2.1737216 -1.4228652 -0.79922414 -0.36481762 -0.30980206 -0.49676895 -0.6740787 -0.8846674 -1.0159149 -0.68235493 -0.42512274][-4.1998391 -4.3708768 -4.2244935 -3.6429534 -2.9242697 -2.2454824 -1.7143285 -1.489012 -1.4338572 -1.3810878 -1.4614553 -1.6057923 -1.5528665 -1.2426741 -1.1790838][-4.07056 -4.2169762 -4.1390038 -3.8224616 -3.3962059 -2.9142585 -2.5927634 -2.5311093 -2.5014234 -2.4299159 -2.5000439 -2.5340283 -2.3829265 -2.1867945 -2.2464123][-3.823405 -3.840992 -3.7406979 -3.5893173 -3.4173727 -3.2059255 -3.1181965 -3.1705067 -3.1788185 -3.1736398 -3.2383652 -3.2025452 -3.0474372 -2.9758806 -3.0853062][-3.6311066 -3.5731792 -3.483614 -3.4340129 -3.4314809 -3.4293933 -3.4736378 -3.5260186 -3.5158138 -3.5227172 -3.5550063 -3.5230727 -3.4563036 -3.452033 -3.5127213]]...]
INFO - root - 2017-12-07 08:56:34.199641: step 22710, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.079 sec/batch; 92h:49m:04s remains)
INFO - root - 2017-12-07 08:56:44.907350: step 22720, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.075 sec/batch; 92h:27m:50s remains)
INFO - root - 2017-12-07 08:56:55.590899: step 22730, loss = 0.82, batch loss = 0.74 (7.7 examples/sec; 1.034 sec/batch; 89h:00m:26s remains)
INFO - root - 2017-12-07 08:57:06.352171: step 22740, loss = 1.02, batch loss = 0.95 (7.2 examples/sec; 1.109 sec/batch; 95h:26m:16s remains)
INFO - root - 2017-12-07 08:57:16.994805: step 22750, loss = 0.92, batch loss = 0.84 (7.2 examples/sec; 1.103 sec/batch; 94h:56m:47s remains)
INFO - root - 2017-12-07 08:57:27.699099: step 22760, loss = 0.79, batch loss = 0.72 (7.5 examples/sec; 1.066 sec/batch; 91h:43m:44s remains)
INFO - root - 2017-12-07 08:57:38.443211: step 22770, loss = 1.02, batch loss = 0.95 (7.2 examples/sec; 1.107 sec/batch; 95h:14m:57s remains)
INFO - root - 2017-12-07 08:57:49.255400: step 22780, loss = 0.86, batch loss = 0.79 (7.4 examples/sec; 1.076 sec/batch; 92h:31m:46s remains)
INFO - root - 2017-12-07 08:57:59.909884: step 22790, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.064 sec/batch; 91h:33m:42s remains)
INFO - root - 2017-12-07 08:58:10.361722: step 22800, loss = 1.07, batch loss = 0.99 (7.4 examples/sec; 1.087 sec/batch; 93h:29m:11s remains)
2017-12-07 08:58:11.242235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.383383 -1.4990344 -1.3871169 -1.3534477 -1.4667752 -1.3126242 -1.1134748 -1.2543123 -1.5215046 -1.4531472 -1.1236067 -1.0680499 -1.189007 -1.2703245 -1.4094362][-1.4792693 -1.6075034 -1.481158 -1.4054079 -1.488821 -1.3775239 -1.1966588 -1.2794421 -1.467833 -1.3352044 -1.0063386 -0.97009492 -1.1572142 -1.3247504 -1.5031853][-1.6513503 -1.7994156 -1.6650379 -1.5155933 -1.5910668 -1.5753133 -1.3977282 -1.3679032 -1.5043402 -1.412406 -1.1240919 -1.0416105 -1.1930249 -1.3757341 -1.5525947][-1.7999594 -1.989048 -1.8114212 -1.5350728 -1.6037557 -1.7056849 -1.4725318 -1.3339086 -1.5319993 -1.6180692 -1.4008079 -1.2398148 -1.3428006 -1.5031686 -1.6379166][-1.8610704 -2.0800724 -1.7860022 -1.3775237 -1.4781413 -1.6659689 -1.3259909 -1.1129088 -1.4206781 -1.6939845 -1.528758 -1.2974441 -1.4104636 -1.5553927 -1.6377857][-1.9301119 -2.1209574 -1.6810632 -1.2034788 -1.3586881 -1.4679523 -0.95474935 -0.77311063 -1.2606754 -1.6029258 -1.3633177 -1.0622768 -1.1962721 -1.3019996 -1.3433607][-1.9345517 -2.0858924 -1.5556679 -1.128072 -1.311893 -1.1611981 -0.44440603 -0.38431215 -1.0497053 -1.3698781 -1.0411258 -0.72810864 -0.82976484 -0.85109925 -0.90457106][-1.8837631 -1.99563 -1.4446452 -1.1358006 -1.3077557 -0.86800909 -0.076754093 -0.23734999 -0.99286413 -1.218091 -0.93035245 -0.74972343 -0.71426678 -0.52049541 -0.53493142][-1.769176 -1.8395765 -1.3673699 -1.2333565 -1.3821862 -0.85626364 -0.24604082 -0.56280041 -1.1133552 -1.140435 -1.0208812 -1.0714607 -0.88816929 -0.47390676 -0.38004875][-1.6041262 -1.7164936 -1.4093571 -1.4248874 -1.5480914 -1.1494393 -0.855134 -1.1744363 -1.3412375 -1.1899524 -1.280793 -1.4742975 -1.1681066 -0.66835165 -0.50606084][-1.4265895 -1.6356025 -1.4782507 -1.5445704 -1.6804328 -1.5435138 -1.5273471 -1.6665251 -1.5008016 -1.3430066 -1.6400957 -1.8104837 -1.3836696 -0.93078923 -0.78776646][-1.2716234 -1.5522654 -1.4655452 -1.5361643 -1.6769426 -1.7088797 -1.8253422 -1.8305149 -1.5862563 -1.5499959 -1.9284847 -1.9632573 -1.464967 -1.1603551 -1.0856867][-1.1888177 -1.5089538 -1.4827926 -1.5803854 -1.6917741 -1.73403 -1.8521111 -1.8457761 -1.7486284 -1.8124874 -2.1026921 -1.9845035 -1.5039823 -1.3542194 -1.2828209][-1.1524785 -1.4728103 -1.5139232 -1.5953689 -1.5869653 -1.563117 -1.6787748 -1.7621517 -1.8503063 -1.966639 -2.1561408 -1.9599919 -1.5258651 -1.4336083 -1.3222375][-1.1451437 -1.4027431 -1.4373548 -1.423068 -1.279851 -1.2513371 -1.4495525 -1.6957288 -1.87496 -1.9353602 -2.0321393 -1.8225622 -1.4498751 -1.3696859 -1.2942033]]...]
INFO - root - 2017-12-07 08:58:21.963725: step 22810, loss = 0.77, batch loss = 0.70 (7.4 examples/sec; 1.080 sec/batch; 92h:53m:21s remains)
INFO - root - 2017-12-07 08:58:32.632248: step 22820, loss = 0.82, batch loss = 0.75 (7.4 examples/sec; 1.084 sec/batch; 93h:16m:09s remains)
INFO - root - 2017-12-07 08:58:43.341870: step 22830, loss = 1.13, batch loss = 1.06 (7.4 examples/sec; 1.079 sec/batch; 92h:48m:58s remains)
INFO - root - 2017-12-07 08:58:53.816945: step 22840, loss = 0.63, batch loss = 0.56 (7.4 examples/sec; 1.085 sec/batch; 93h:18m:58s remains)
INFO - root - 2017-12-07 08:59:04.606231: step 22850, loss = 0.71, batch loss = 0.64 (7.4 examples/sec; 1.083 sec/batch; 93h:11m:33s remains)
INFO - root - 2017-12-07 08:59:15.219496: step 22860, loss = 1.03, batch loss = 0.95 (7.5 examples/sec; 1.070 sec/batch; 91h:59m:31s remains)
INFO - root - 2017-12-07 08:59:25.910703: step 22870, loss = 0.66, batch loss = 0.59 (7.6 examples/sec; 1.056 sec/batch; 90h:48m:00s remains)
INFO - root - 2017-12-07 08:59:36.625888: step 22880, loss = 0.94, batch loss = 0.87 (7.6 examples/sec; 1.057 sec/batch; 90h:52m:50s remains)
INFO - root - 2017-12-07 08:59:47.268725: step 22890, loss = 1.16, batch loss = 1.09 (7.3 examples/sec; 1.100 sec/batch; 94h:35m:00s remains)
INFO - root - 2017-12-07 08:59:58.000243: step 22900, loss = 0.63, batch loss = 0.56 (7.4 examples/sec; 1.084 sec/batch; 93h:11m:23s remains)
2017-12-07 08:59:58.847412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0833085 -3.1288004 -3.1600337 -3.1618156 -3.1491833 -3.1370058 -3.1323891 -3.1155806 -3.0895154 -3.0849602 -3.0716503 -3.0161803 -2.9463065 -2.8922157 -2.84167][-3.2439642 -3.26729 -3.2449408 -3.1766887 -3.1094069 -3.0645485 -3.0546775 -3.0455406 -3.022006 -3.0280085 -3.0208349 -2.9445896 -2.8407197 -2.7719703 -2.7308974][-3.3611948 -3.3579516 -3.2631307 -3.1111479 -2.9901927 -2.9127541 -2.8913226 -2.903827 -2.8901861 -2.8862619 -2.8710625 -2.7798195 -2.6452088 -2.5633235 -2.5525916][-3.4770365 -3.4672685 -3.2914457 -3.0522819 -2.8737626 -2.7336559 -2.6589904 -2.7028074 -2.7348313 -2.7214937 -2.6847677 -2.5897646 -2.4295979 -2.3296905 -2.3586977][-3.5925758 -3.5987239 -3.3290706 -3.0072064 -2.7740898 -2.5318718 -2.3278425 -2.3766878 -2.5046039 -2.5240455 -2.4705248 -2.3877909 -2.2314858 -2.1283898 -2.1957104][-3.6623459 -3.674258 -3.2963462 -2.8921332 -2.6065228 -2.2346227 -1.845259 -1.8547742 -2.1174755 -2.238812 -2.2005496 -2.1527429 -2.040132 -1.9584022 -2.0569749][-3.8030405 -3.8282075 -3.3633924 -2.8950362 -2.5586019 -2.0354834 -1.4099946 -1.293735 -1.6768029 -1.9519134 -1.9841685 -1.9952538 -1.951941 -1.9077127 -2.01497][-3.94785 -4.0013514 -3.5181911 -3.0496864 -2.7121172 -2.0738943 -1.2371023 -0.95310378 -1.4002073 -1.8205476 -1.9406123 -2.0004728 -2.0166597 -2.008899 -2.09408][-3.8900013 -3.9377565 -3.4634056 -3.0462151 -2.7607441 -2.1372471 -1.2663972 -0.91861653 -1.3564739 -1.8399525 -2.0058489 -2.0771224 -2.1221738 -2.1465075 -2.2109995][-3.5964892 -3.614671 -3.1830606 -2.8456192 -2.6640053 -2.1670418 -1.4184568 -1.091702 -1.4679849 -1.9286628 -2.0986116 -2.1570692 -2.2122087 -2.2661521 -2.3242414][-3.3929069 -3.3739986 -2.9902182 -2.7040472 -2.5856175 -2.227957 -1.6648114 -1.4056461 -1.6961467 -2.0765402 -2.2254989 -2.261198 -2.3132737 -2.381784 -2.4372067][-3.4740169 -3.4443951 -3.1140108 -2.8362579 -2.6998949 -2.4190774 -2.0129929 -1.8217378 -2.0195062 -2.2950237 -2.4084556 -2.4288812 -2.4696808 -2.5311513 -2.573915][-3.4972358 -3.4764194 -3.2267303 -3.001193 -2.8730855 -2.6624303 -2.3975394 -2.2692702 -2.3848374 -2.5574956 -2.6243525 -2.6279707 -2.6462917 -2.6810856 -2.7011132][-3.3943279 -3.369844 -3.1980019 -3.0459428 -2.9517879 -2.8142281 -2.6697907 -2.6036682 -2.6636045 -2.749115 -2.7713542 -2.7586951 -2.7574739 -2.7683821 -2.7732563][-3.3408365 -3.3087416 -3.1859014 -3.0723085 -2.9890757 -2.8936532 -2.8191576 -2.7886667 -2.8100352 -2.8407474 -2.840523 -2.8250332 -2.8161085 -2.8123865 -2.8077757]]...]
INFO - root - 2017-12-07 09:00:09.577095: step 22910, loss = 0.94, batch loss = 0.87 (7.6 examples/sec; 1.046 sec/batch; 89h:59m:15s remains)
INFO - root - 2017-12-07 09:00:20.266275: step 22920, loss = 0.66, batch loss = 0.58 (7.5 examples/sec; 1.073 sec/batch; 92h:17m:20s remains)
INFO - root - 2017-12-07 09:00:31.034232: step 22930, loss = 0.73, batch loss = 0.65 (7.5 examples/sec; 1.064 sec/batch; 91h:29m:42s remains)
INFO - root - 2017-12-07 09:00:41.771316: step 22940, loss = 0.91, batch loss = 0.83 (7.5 examples/sec; 1.063 sec/batch; 91h:23m:16s remains)
INFO - root - 2017-12-07 09:00:52.503690: step 22950, loss = 0.69, batch loss = 0.62 (7.5 examples/sec; 1.073 sec/batch; 92h:13m:55s remains)
INFO - root - 2017-12-07 09:01:03.208156: step 22960, loss = 0.71, batch loss = 0.63 (7.7 examples/sec; 1.037 sec/batch; 89h:11m:04s remains)
INFO - root - 2017-12-07 09:01:13.933439: step 22970, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.074 sec/batch; 92h:18m:35s remains)
INFO - root - 2017-12-07 09:01:24.696545: step 22980, loss = 0.87, batch loss = 0.80 (7.6 examples/sec; 1.057 sec/batch; 90h:52m:36s remains)
INFO - root - 2017-12-07 09:01:35.430287: step 22990, loss = 1.16, batch loss = 1.08 (7.5 examples/sec; 1.064 sec/batch; 91h:30m:31s remains)
INFO - root - 2017-12-07 09:01:46.161875: step 23000, loss = 0.63, batch loss = 0.55 (7.4 examples/sec; 1.080 sec/batch; 92h:50m:45s remains)
2017-12-07 09:01:46.958554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1232879 -2.2871907 -2.390269 -2.128988 -1.8159554 -1.7275212 -1.664988 -1.610528 -1.6834674 -1.7890203 -1.8961859 -1.8635705 -1.6891239 -1.4983299 -1.0181561][-1.6790671 -1.8049359 -1.7158945 -1.3326302 -1.033262 -0.94154215 -0.98106956 -1.3242993 -1.910681 -2.2885783 -2.2856174 -2.0163856 -1.6621535 -1.4722347 -1.1411843][-1.9581707 -1.9992547 -1.7308359 -1.3327639 -1.1289768 -1.0832248 -1.2312641 -1.7524045 -2.5144541 -2.9497671 -2.904732 -2.6425617 -2.326637 -2.2594855 -2.2189705][-1.9719353 -1.9728918 -1.6797576 -1.3901966 -1.2877457 -1.2860618 -1.4851639 -1.9442742 -2.6263642 -3.1150856 -3.258862 -3.1671727 -2.7880919 -2.4734139 -2.4252584][-0.95653057 -0.9387331 -0.857718 -0.91745687 -1.0953968 -1.2059636 -1.3134086 -1.431812 -1.7971747 -2.4165049 -3.0400295 -3.4081678 -3.1223114 -2.5312543 -2.2418792][-0.39953947 -0.49133086 -0.68536687 -1.0345526 -1.396275 -1.5412786 -1.4288571 -1.0456834 -0.83090043 -1.2281277 -2.0414131 -2.8360977 -2.9807382 -2.6188097 -2.2865095][-0.53475642 -0.79733086 -1.2111804 -1.6751735 -2.0333741 -2.0833139 -1.7731018 -1.1362419 -0.55712032 -0.47001004 -0.89973545 -1.5976884 -2.1004043 -2.2873085 -2.259882][-1.0051172 -1.2176402 -1.62202 -2.0634847 -2.3825119 -2.4454284 -2.2085948 -1.7648709 -1.3128827 -0.94339418 -0.7705667 -0.90516615 -1.360007 -2.0029619 -2.50742][-1.7194989 -1.7920032 -2.04065 -2.2750647 -2.4343786 -2.5057368 -2.4753187 -2.453372 -2.3952389 -2.1737533 -1.7661784 -1.4843385 -1.6684647 -2.2788088 -2.9082649][-1.9127352 -1.9881587 -2.0836279 -2.0897512 -2.0539777 -2.0641732 -2.2124789 -2.4877548 -2.7382059 -2.8618538 -2.7431333 -2.5889237 -2.6613452 -2.9476347 -3.2404637][-1.8061512 -2.0296192 -2.0835049 -1.9163399 -1.6776321 -1.5088041 -1.6074736 -1.8256426 -2.0672946 -2.4033594 -2.6743989 -2.9276166 -3.2235432 -3.4117966 -3.4816976][-1.8289592 -2.1999753 -2.312089 -2.1210954 -1.8145537 -1.483851 -1.3117404 -1.2274616 -1.2449524 -1.5446062 -1.9825335 -2.5377245 -3.0657454 -3.35785 -3.4251375][-1.5175674 -1.8650746 -1.9776967 -1.871213 -1.758785 -1.5428436 -1.3302462 -1.131681 -1.0790398 -1.323971 -1.7192628 -2.2650959 -2.6880302 -2.9202094 -3.0328968][-1.3360565 -1.42606 -1.3970187 -1.3249381 -1.3959191 -1.3396368 -1.2097561 -1.1204958 -1.2544925 -1.7124104 -2.228261 -2.690599 -2.8629107 -2.8243113 -2.8121009][-1.468509 -1.0074787 -0.68263197 -0.69357347 -0.98584986 -1.0626581 -1.0018668 -0.96348357 -1.253022 -1.8788869 -2.4025896 -2.7225056 -2.6944275 -2.5103247 -2.5162973]]...]
INFO - root - 2017-12-07 09:01:57.581861: step 23010, loss = 0.70, batch loss = 0.62 (7.5 examples/sec; 1.068 sec/batch; 91h:47m:36s remains)
INFO - root - 2017-12-07 09:02:08.409723: step 23020, loss = 0.79, batch loss = 0.72 (7.4 examples/sec; 1.080 sec/batch; 92h:50m:36s remains)
INFO - root - 2017-12-07 09:02:19.247956: step 23030, loss = 1.07, batch loss = 1.00 (7.4 examples/sec; 1.086 sec/batch; 93h:21m:37s remains)
INFO - root - 2017-12-07 09:02:29.896607: step 23040, loss = 0.64, batch loss = 0.57 (7.3 examples/sec; 1.092 sec/batch; 93h:50m:14s remains)
INFO - root - 2017-12-07 09:02:40.514252: step 23050, loss = 0.99, batch loss = 0.91 (7.9 examples/sec; 1.013 sec/batch; 87h:05m:10s remains)
INFO - root - 2017-12-07 09:02:51.368576: step 23060, loss = 0.73, batch loss = 0.66 (7.5 examples/sec; 1.063 sec/batch; 91h:21m:55s remains)
INFO - root - 2017-12-07 09:03:02.139778: step 23070, loss = 0.84, batch loss = 0.77 (7.7 examples/sec; 1.036 sec/batch; 89h:01m:47s remains)
INFO - root - 2017-12-07 09:03:12.922705: step 23080, loss = 0.79, batch loss = 0.72 (7.5 examples/sec; 1.072 sec/batch; 92h:10m:14s remains)
INFO - root - 2017-12-07 09:03:23.707858: step 23090, loss = 0.87, batch loss = 0.80 (7.5 examples/sec; 1.071 sec/batch; 92h:00m:39s remains)
INFO - root - 2017-12-07 09:03:34.597021: step 23100, loss = 0.83, batch loss = 0.75 (7.4 examples/sec; 1.083 sec/batch; 93h:03m:43s remains)
2017-12-07 09:03:35.430755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5520854 -2.5008984 -2.3865426 -2.3630061 -2.3745856 -2.3437726 -2.3729148 -2.4484768 -2.503581 -2.6095839 -2.7596974 -2.9480658 -2.9606621 -2.6773579 -2.3885787][-2.5813282 -2.520519 -2.4186873 -2.3928256 -2.4020278 -2.3223264 -2.2861209 -2.3272445 -2.3352029 -2.426903 -2.6378212 -2.9082322 -2.9902437 -2.6643727 -2.2724931][-2.95716 -2.8529224 -2.68789 -2.55225 -2.4429324 -2.2214553 -2.0618396 -2.0415075 -2.0356991 -2.1904657 -2.5246854 -2.8881269 -3.0233536 -2.6470256 -2.1592305][-3.2500157 -3.1596813 -2.9785085 -2.7758105 -2.5887542 -2.3222249 -2.1458635 -2.1093812 -2.083168 -2.2166946 -2.5462937 -2.8858993 -2.9883521 -2.5755384 -2.0879269][-3.3300276 -3.3337173 -3.2016087 -3.058569 -2.9612446 -2.8167279 -2.706677 -2.6151676 -2.4464037 -2.387754 -2.5598919 -2.80126 -2.8677492 -2.5102673 -2.1504526][-3.373419 -3.4253135 -3.2606671 -3.1214101 -3.1102843 -3.1211519 -3.0834281 -2.8785765 -2.5166593 -2.2827938 -2.4140558 -2.7055454 -2.8482721 -2.6320641 -2.3909402][-3.1267753 -3.1685424 -2.9209666 -2.7634025 -2.7899237 -2.852025 -2.8017049 -2.443876 -1.9263091 -1.7174287 -2.0624144 -2.6161451 -2.988004 -2.9229035 -2.7100515][-2.6172409 -2.6564536 -2.3942516 -2.2748888 -2.300468 -2.3064265 -2.1631327 -1.6431384 -1.0656364 -1.0717621 -1.7444587 -2.6057177 -3.2164359 -3.2741075 -3.0376732][-2.1695378 -2.2012141 -2.0103841 -2.006382 -2.013725 -1.8815222 -1.6650739 -1.1568806 -0.703567 -0.94215488 -1.8074152 -2.7757506 -3.4731612 -3.5622144 -3.2981255][-2.0942144 -2.1232772 -2.0190477 -2.0663333 -2.0446053 -1.8495743 -1.712616 -1.4528186 -1.2454066 -1.6183488 -2.4892669 -3.3732786 -3.9427166 -3.9255638 -3.5857859][-2.6437562 -2.6964593 -2.6576142 -2.6900833 -2.6778939 -2.5743933 -2.6004333 -2.5903575 -2.5382476 -2.86691 -3.5662041 -4.1781263 -4.4464388 -4.2272611 -3.7654397][-3.6034524 -3.6670728 -3.6555343 -3.6420586 -3.655827 -3.6710799 -3.7469883 -3.8005764 -3.7802765 -3.9965148 -4.4461627 -4.743865 -4.7068491 -4.3131189 -3.7793083][-4.2517381 -4.3204522 -4.3495989 -4.3246045 -4.3456798 -4.3999825 -4.4260187 -4.4321322 -4.4283247 -4.5808749 -4.8404589 -4.899992 -4.6714153 -4.2042451 -3.6642442][-4.2958488 -4.3785176 -4.4521132 -4.4239211 -4.4223084 -4.4398737 -4.3806305 -4.3162742 -4.3012161 -4.385551 -4.4918156 -4.4264383 -4.1718535 -3.7778139 -3.3445125][-3.8299973 -3.903265 -3.9898386 -3.9831557 -3.9678562 -3.9564555 -3.8695507 -3.7867186 -3.7537904 -3.7719553 -3.7752411 -3.6704216 -3.4665904 -3.1924381 -2.9070847]]...]
INFO - root - 2017-12-07 09:03:46.105486: step 23110, loss = 0.82, batch loss = 0.75 (7.5 examples/sec; 1.067 sec/batch; 91h:39m:26s remains)
INFO - root - 2017-12-07 09:03:56.796654: step 23120, loss = 0.65, batch loss = 0.58 (7.4 examples/sec; 1.088 sec/batch; 93h:28m:17s remains)
INFO - root - 2017-12-07 09:04:07.548673: step 23130, loss = 1.02, batch loss = 0.95 (7.5 examples/sec; 1.063 sec/batch; 91h:22m:29s remains)
INFO - root - 2017-12-07 09:04:18.181395: step 23140, loss = 0.93, batch loss = 0.85 (7.6 examples/sec; 1.053 sec/batch; 90h:31m:49s remains)
INFO - root - 2017-12-07 09:04:28.944524: step 23150, loss = 0.80, batch loss = 0.73 (7.4 examples/sec; 1.082 sec/batch; 93h:01m:05s remains)
INFO - root - 2017-12-07 09:04:39.691912: step 23160, loss = 0.89, batch loss = 0.82 (7.5 examples/sec; 1.066 sec/batch; 91h:35m:02s remains)
INFO - root - 2017-12-07 09:04:50.320335: step 23170, loss = 0.67, batch loss = 0.60 (7.6 examples/sec; 1.057 sec/batch; 90h:50m:43s remains)
INFO - root - 2017-12-07 09:05:00.978536: step 23180, loss = 0.79, batch loss = 0.72 (7.5 examples/sec; 1.073 sec/batch; 92h:13m:25s remains)
INFO - root - 2017-12-07 09:05:11.790925: step 23190, loss = 0.73, batch loss = 0.66 (7.3 examples/sec; 1.090 sec/batch; 93h:40m:32s remains)
INFO - root - 2017-12-07 09:05:22.387959: step 23200, loss = 0.63, batch loss = 0.56 (7.5 examples/sec; 1.069 sec/batch; 91h:53m:11s remains)
2017-12-07 09:05:23.240024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0402169 -3.1780887 -3.209939 -3.1086175 -3.0323761 -3.0896258 -3.1985269 -3.1685812 -3.1223154 -3.2312274 -3.357275 -3.2935419 -3.0932307 -2.9329352 -2.856709][-3.1534657 -3.1961339 -3.2126763 -3.2031422 -3.2354627 -3.3754706 -3.6014442 -3.7032595 -3.7095649 -3.806664 -3.9351625 -3.8661726 -3.5871756 -3.2934008 -3.096427][-2.9244819 -2.8360844 -2.9400816 -3.2090228 -3.5499072 -3.8548279 -4.0948272 -4.1425352 -4.0754113 -4.1358275 -4.2804728 -4.2349515 -3.9060123 -3.4956112 -3.21741][-2.6526628 -2.5779257 -2.8981657 -3.44552 -3.9607315 -4.2229524 -4.2145247 -3.9536958 -3.6594074 -3.6697814 -3.9410419 -4.0545874 -3.7626529 -3.2971008 -3.0307355][-2.6249776 -2.6983478 -3.1946895 -3.7847443 -4.1503534 -4.146771 -3.7773662 -3.1249194 -2.5096769 -2.3708396 -2.7388315 -3.0744734 -2.9480929 -2.5783186 -2.5009298][-2.3586044 -2.3364811 -2.7267971 -3.1056361 -3.1653194 -2.9127626 -2.3181913 -1.415972 -0.58176947 -0.37740135 -0.89045835 -1.5046954 -1.682142 -1.6059358 -1.8604395][-1.4190803 -1.2371497 -1.52036 -1.7639961 -1.6613081 -1.3440943 -0.71485376 0.26657534 1.0959296 1.1093903 0.33840227 -0.507622 -0.90326262 -1.0485308 -1.5149629][-0.27814198 -0.36040306 -0.90014124 -1.2928753 -1.2985973 -1.1500227 -0.73643947 0.11166096 0.82147217 0.66693735 -0.20025492 -1.0058982 -1.3027589 -1.3679674 -1.7397478][0.12268162 -0.41469765 -1.2675345 -1.8287928 -1.9622488 -2.0176013 -1.9262531 -1.4195464 -0.95000148 -1.1975889 -1.9441106 -2.4637465 -2.4511981 -2.2401974 -2.3208573][-0.59983659 -1.2575943 -2.0994377 -2.6295853 -2.74685 -2.8451238 -2.9578485 -2.8121552 -2.6208286 -2.9232869 -3.519424 -3.7642405 -3.4933367 -3.0522594 -2.8760815][-2.3439229 -2.8701882 -3.4986942 -3.8747797 -3.8768215 -3.8482828 -3.9360819 -3.8922863 -3.760674 -3.9452806 -4.2984276 -4.2921162 -3.859664 -3.3596759 -3.1037214][-3.8075538 -4.122138 -4.443336 -4.5522552 -4.39246 -4.2481365 -4.2794733 -4.2219543 -4.0399075 -4.0491395 -4.1763692 -4.0089679 -3.5556877 -3.1689713 -3.0043874][-4.3509865 -4.498394 -4.5098977 -4.3006382 -3.9946158 -3.8623314 -3.9420881 -3.958946 -3.8636165 -3.883182 -3.9338131 -3.7076056 -3.291574 -3.0146403 -2.9260302][-4.451869 -4.5892825 -4.5087376 -4.1883154 -3.8527102 -3.7433817 -3.8273954 -3.8624022 -3.8503213 -3.9276967 -3.9322047 -3.63947 -3.2198043 -2.9694655 -2.8990397][-4.2787671 -4.45402 -4.3984523 -4.11803 -3.8510511 -3.7939289 -3.8640354 -3.8435137 -3.8327587 -3.9192231 -3.8654118 -3.4975586 -3.0533135 -2.8087969 -2.7625453]]...]
INFO - root - 2017-12-07 09:05:33.941689: step 23210, loss = 0.78, batch loss = 0.71 (7.3 examples/sec; 1.098 sec/batch; 94h:21m:22s remains)
INFO - root - 2017-12-07 09:05:44.699955: step 23220, loss = 0.72, batch loss = 0.65 (7.4 examples/sec; 1.082 sec/batch; 92h:59m:09s remains)
INFO - root - 2017-12-07 09:05:55.335494: step 23230, loss = 0.98, batch loss = 0.91 (7.7 examples/sec; 1.040 sec/batch; 89h:21m:58s remains)
INFO - root - 2017-12-07 09:06:05.803609: step 23240, loss = 0.71, batch loss = 0.64 (7.6 examples/sec; 1.050 sec/batch; 90h:14m:18s remains)
INFO - root - 2017-12-07 09:06:16.645327: step 23250, loss = 0.80, batch loss = 0.73 (7.5 examples/sec; 1.066 sec/batch; 91h:34m:50s remains)
INFO - root - 2017-12-07 09:06:27.342617: step 23260, loss = 0.78, batch loss = 0.71 (7.6 examples/sec; 1.059 sec/batch; 90h:59m:18s remains)
INFO - root - 2017-12-07 09:06:38.139999: step 23270, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.083 sec/batch; 93h:01m:08s remains)
INFO - root - 2017-12-07 09:06:48.746939: step 23280, loss = 0.91, batch loss = 0.84 (7.6 examples/sec; 1.051 sec/batch; 90h:18m:43s remains)
INFO - root - 2017-12-07 09:06:59.292210: step 23290, loss = 0.82, batch loss = 0.75 (7.5 examples/sec; 1.067 sec/batch; 91h:37m:42s remains)
INFO - root - 2017-12-07 09:07:10.041311: step 23300, loss = 0.85, batch loss = 0.78 (7.4 examples/sec; 1.084 sec/batch; 93h:05m:23s remains)
2017-12-07 09:07:10.861513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4069948 -4.3571057 -4.6088967 -4.7813406 -4.5598121 -4.3497691 -4.5585141 -5.0195713 -5.1707463 -4.8373342 -4.1113114 -3.53134 -3.7156076 -4.6932182 -5.4947577][-3.6280138 -3.6478484 -3.9678872 -4.2008495 -4.0683446 -3.922946 -4.0845413 -4.4995022 -4.6516013 -4.3383942 -3.6902623 -3.1990366 -3.4971027 -4.5165887 -5.3079343][-3.152247 -3.4159088 -3.9136136 -4.2642822 -4.2579865 -4.1308866 -4.20786 -4.5084081 -4.6105289 -4.2982259 -3.6483579 -3.1058626 -3.3428702 -4.2716479 -4.9847465][-3.4449408 -3.9506054 -4.5146832 -4.8658485 -4.8533592 -4.6247172 -4.63542 -4.8915334 -5.0100565 -4.7938862 -4.1756687 -3.5445733 -3.5933521 -4.3300753 -4.92989][-4.1894512 -4.7123027 -5.0582266 -5.1335216 -4.8896971 -4.4869618 -4.5061431 -4.8578167 -5.1074705 -5.1120553 -4.6363444 -4.0361705 -3.95449 -4.5657339 -5.15133][-4.4477587 -4.6881666 -4.5385513 -4.1474867 -3.5306559 -2.9487479 -3.0273619 -3.516587 -3.9396076 -4.1665831 -3.8792939 -3.4433911 -3.4091644 -4.1335564 -4.9441633][-3.9368734 -3.6923516 -2.9214716 -1.9621649 -0.84293294 -0.0074338913 -0.083612442 -0.69001365 -1.339474 -1.8198841 -1.8494534 -1.8120925 -2.1332064 -3.2271752 -4.4183612][-3.5667238 -2.9271173 -1.7197318 -0.38022947 1.0851398 2.0792289 2.0316248 1.3580189 0.59078789 -0.0032072067 -0.27688503 -0.59940314 -1.3099873 -2.7359357 -4.1737051][-4.0321054 -3.3897491 -2.2788565 -1.0893002 0.19983864 0.96758223 0.88623142 0.3315134 -0.34203434 -0.81844091 -1.066669 -1.4269538 -2.1623049 -3.4785452 -4.7196684][-4.4121051 -4.0504279 -3.42202 -2.7425947 -1.9808924 -1.6257689 -1.7745147 -2.1968939 -2.7139406 -2.9601865 -2.959856 -3.0212867 -3.4266872 -4.316206 -5.0980744][-4.0837836 -3.9020224 -3.6405878 -3.3703513 -3.0610042 -3.0674846 -3.3455822 -3.7559903 -4.1941009 -4.276823 -3.9915125 -3.6686177 -3.7149315 -4.2587543 -4.7319736][-3.5170233 -3.407238 -3.3349204 -3.2991738 -3.2668686 -3.4862289 -3.8623419 -4.2776 -4.657167 -4.66565 -4.224556 -3.6775434 -3.5403419 -3.9534047 -4.3375387][-3.5254169 -3.5644519 -3.6940861 -3.8707922 -4.007781 -4.2818093 -4.5909352 -4.8347349 -4.9590874 -4.7442989 -4.1344118 -3.5096574 -3.3925223 -3.8815212 -4.3546686][-3.9185727 -4.162703 -4.4932485 -4.8178735 -5.0082903 -5.1564941 -5.1945205 -5.0893283 -4.8647408 -4.4262533 -3.8120742 -3.3927488 -3.5292239 -4.2026095 -4.7812824][-4.3869319 -4.6893396 -5.0013213 -5.2189794 -5.2162576 -5.1089134 -4.8887053 -4.5865636 -4.283504 -3.9247332 -3.5538449 -3.4632583 -3.84161 -4.5798011 -5.1326237]]...]
INFO - root - 2017-12-07 09:07:21.668394: step 23310, loss = 0.71, batch loss = 0.63 (7.5 examples/sec; 1.065 sec/batch; 91h:27m:10s remains)
INFO - root - 2017-12-07 09:07:32.556496: step 23320, loss = 0.61, batch loss = 0.54 (7.4 examples/sec; 1.079 sec/batch; 92h:39m:02s remains)
INFO - root - 2017-12-07 09:07:43.259357: step 23330, loss = 0.79, batch loss = 0.72 (7.8 examples/sec; 1.031 sec/batch; 88h:31m:43s remains)
INFO - root - 2017-12-07 09:07:53.855919: step 23340, loss = 0.60, batch loss = 0.53 (7.4 examples/sec; 1.081 sec/batch; 92h:51m:19s remains)
INFO - root - 2017-12-07 09:08:04.503003: step 23350, loss = 0.74, batch loss = 0.67 (7.6 examples/sec; 1.049 sec/batch; 90h:03m:32s remains)
INFO - root - 2017-12-07 09:08:15.112533: step 23360, loss = 0.79, batch loss = 0.72 (7.3 examples/sec; 1.092 sec/batch; 93h:45m:15s remains)
INFO - root - 2017-12-07 09:08:25.813009: step 23370, loss = 0.81, batch loss = 0.73 (7.1 examples/sec; 1.127 sec/batch; 96h:44m:05s remains)
INFO - root - 2017-12-07 09:08:36.500444: step 23380, loss = 0.88, batch loss = 0.81 (7.3 examples/sec; 1.094 sec/batch; 93h:57m:17s remains)
INFO - root - 2017-12-07 09:08:47.197295: step 23390, loss = 0.87, batch loss = 0.79 (7.6 examples/sec; 1.050 sec/batch; 90h:07m:58s remains)
INFO - root - 2017-12-07 09:08:57.853993: step 23400, loss = 0.88, batch loss = 0.80 (7.7 examples/sec; 1.043 sec/batch; 89h:32m:40s remains)
2017-12-07 09:08:58.689268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8445706 -1.9044592 -1.6696105 -1.5439441 -1.8227086 -1.9752884 -1.5987816 -1.2702079 -1.1225944 -1.1501462 -1.5224993 -1.7342014 -1.5208647 -1.1612794 -1.2660291][-1.7798171 -1.7274055 -1.4828825 -1.4293244 -1.7778668 -1.9048898 -1.432163 -1.0423405 -0.96007872 -1.0012431 -1.4211383 -1.7441139 -1.5794675 -1.2918568 -1.4305558][-1.5166874 -1.5323248 -1.4187088 -1.495688 -1.8829203 -1.959965 -1.4257209 -0.98416567 -0.90949225 -0.93150711 -1.3313024 -1.7417815 -1.7109127 -1.5704217 -1.7138007][-1.0748289 -1.2567761 -1.3249388 -1.5882146 -2.0388415 -2.0668552 -1.5002902 -1.0631862 -1.0146339 -1.0211222 -1.3372874 -1.7784526 -1.843154 -1.8018954 -1.9009554][-0.71011353 -0.97784567 -1.1287508 -1.5334868 -2.0412245 -2.0482142 -1.5477576 -1.1960716 -1.1834552 -1.227752 -1.5282092 -2.0101311 -2.1312356 -2.0977962 -2.1117649][-0.59843779 -0.88885903 -1.0431359 -1.4903758 -1.9774125 -1.9521952 -1.5355651 -1.2407703 -1.2133465 -1.2787039 -1.6047962 -2.1430612 -2.3568568 -2.3501444 -2.3089559][-0.60268974 -0.903568 -1.0507252 -1.46082 -1.8674588 -1.8240471 -1.4673953 -1.1411898 -1.040036 -1.1213634 -1.4785666 -2.0665565 -2.3981879 -2.4636428 -2.4279733][-0.83345962 -1.1104634 -1.2262976 -1.5127101 -1.7723358 -1.7092693 -1.3750761 -1.0072482 -0.85097146 -0.91426635 -1.2634602 -1.8926413 -2.3460248 -2.5309513 -2.5421009][-1.1895418 -1.4231532 -1.5141163 -1.652467 -1.7370999 -1.6137395 -1.2492151 -0.88086009 -0.71779752 -0.71114421 -1.0106082 -1.6956415 -2.2743232 -2.5935254 -2.6601682][-1.4787688 -1.5860975 -1.626179 -1.6494749 -1.6179154 -1.4902956 -1.1636984 -0.87791085 -0.71943235 -0.62982106 -0.84884739 -1.4983606 -2.1014767 -2.4962511 -2.6253934][-1.8102982 -1.7723274 -1.6987996 -1.5410867 -1.3626423 -1.2464035 -0.99113846 -0.80600429 -0.68983555 -0.57910824 -0.77511311 -1.3463781 -1.870743 -2.2553239 -2.4600878][-2.1752083 -2.0464151 -1.865387 -1.5324192 -1.2127752 -1.0949578 -0.88427424 -0.73404264 -0.6248548 -0.53913 -0.77842546 -1.3109846 -1.7179391 -2.0504749 -2.3246858][-2.5603328 -2.3967354 -2.1461022 -1.7525573 -1.4106932 -1.2689281 -1.0228591 -0.82392192 -0.6916852 -0.55475688 -0.76288557 -1.2587521 -1.5837665 -1.8874779 -2.2211652][-2.9705403 -2.7675576 -2.4952717 -2.1678329 -1.861913 -1.6850631 -1.3577323 -1.0817776 -0.90625525 -0.69098306 -0.792274 -1.1643605 -1.3765028 -1.6349099 -2.0214508][-3.2958565 -3.0702157 -2.8300085 -2.6246634 -2.3869452 -2.1651721 -1.7869673 -1.4724674 -1.2587926 -0.9746902 -0.92531013 -1.0639811 -1.0977335 -1.2919827 -1.71397]]...]
INFO - root - 2017-12-07 09:09:09.348114: step 23410, loss = 0.73, batch loss = 0.66 (7.3 examples/sec; 1.098 sec/batch; 94h:16m:25s remains)
INFO - root - 2017-12-07 09:09:20.083585: step 23420, loss = 0.82, batch loss = 0.75 (7.6 examples/sec; 1.058 sec/batch; 90h:47m:45s remains)
INFO - root - 2017-12-07 09:09:30.871445: step 23430, loss = 0.98, batch loss = 0.91 (7.7 examples/sec; 1.039 sec/batch; 89h:14m:26s remains)
INFO - root - 2017-12-07 09:09:41.436924: step 23440, loss = 0.89, batch loss = 0.82 (7.4 examples/sec; 1.083 sec/batch; 92h:57m:11s remains)
INFO - root - 2017-12-07 09:09:52.107689: step 23450, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.086 sec/batch; 93h:15m:40s remains)
INFO - root - 2017-12-07 09:10:02.785174: step 23460, loss = 0.86, batch loss = 0.79 (7.6 examples/sec; 1.059 sec/batch; 90h:56m:34s remains)
INFO - root - 2017-12-07 09:10:13.614366: step 23470, loss = 0.77, batch loss = 0.70 (7.4 examples/sec; 1.081 sec/batch; 92h:50m:02s remains)
INFO - root - 2017-12-07 09:10:24.339564: step 23480, loss = 0.94, batch loss = 0.87 (7.5 examples/sec; 1.070 sec/batch; 91h:48m:45s remains)
INFO - root - 2017-12-07 09:10:35.135238: step 23490, loss = 0.76, batch loss = 0.69 (7.3 examples/sec; 1.093 sec/batch; 93h:49m:50s remains)
INFO - root - 2017-12-07 09:10:45.830230: step 23500, loss = 0.72, batch loss = 0.65 (7.4 examples/sec; 1.076 sec/batch; 92h:21m:17s remains)
2017-12-07 09:10:46.751288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4128842 -3.6091533 -3.4218707 -2.9776182 -2.5235658 -2.4353585 -2.8838053 -3.2384005 -3.1030471 -2.767025 -2.6582327 -3.0250506 -3.5722802 -3.2270977 -2.3172774][-3.6744905 -3.9027157 -3.6556988 -3.1145887 -2.5626581 -2.5059185 -3.0741639 -3.4035268 -3.153013 -2.9155436 -2.9319377 -3.3877196 -3.9703732 -3.3620849 -2.0391052][-3.7000129 -3.7597687 -3.3553646 -2.8220618 -2.39072 -2.5347321 -3.1910014 -3.4098692 -3.0978572 -3.1425624 -3.3401217 -3.7573905 -4.1823773 -3.27625 -1.6485744][-3.3729587 -3.1496887 -2.6063774 -2.2034683 -2.0725002 -2.4500399 -3.0979753 -3.1414266 -2.8751435 -3.3384738 -3.7235737 -4.0018086 -4.1638818 -3.009757 -1.2681806][-3.1483417 -2.7151928 -2.1697307 -1.9142828 -1.9767849 -2.3192546 -2.6614218 -2.3791966 -2.2030959 -3.1844392 -3.8402646 -3.9910932 -3.9554098 -2.6857781 -0.97767806][-3.3549929 -2.9573438 -2.5589998 -2.4214175 -2.4225364 -2.3220222 -1.9651375 -1.121454 -1.051326 -2.67785 -3.7590072 -3.9175923 -3.7987764 -2.501358 -0.83192968][-3.4950657 -3.3028424 -3.149591 -3.1757557 -3.0735297 -2.4399123 -1.2326534 0.35669613 0.39936686 -1.929328 -3.6154196 -3.9958191 -3.9466033 -2.6309793 -0.89796662][-3.2606654 -3.3513162 -3.4950984 -3.7275333 -3.5894084 -2.6842513 -0.88826966 1.4137783 1.6502256 -1.1036124 -3.2950125 -4.0230794 -4.162015 -2.9102247 -1.1128833][-2.7313347 -3.1527643 -3.6352975 -4.0971966 -4.0468335 -3.2272463 -1.4023154 1.1560688 1.7606487 -0.75889134 -2.8871431 -3.7704368 -4.0965228 -3.0517726 -1.3456452][-2.0734997 -2.7186933 -3.4377189 -4.0642986 -4.1654658 -3.6938758 -2.3164587 -0.074858665 0.71819305 -1.1240547 -2.6674774 -3.4011683 -3.8265228 -3.1154318 -1.7170086][-1.6253576 -2.2663519 -2.9794588 -3.5798402 -3.7803044 -3.7066832 -2.9578459 -1.3345454 -0.61328888 -1.7994549 -2.6277676 -3.0711865 -3.5213206 -3.2123475 -2.2826307][-1.7376184 -2.2365613 -2.7300868 -3.1129305 -3.2927363 -3.4810605 -3.2218366 -2.215158 -1.7214699 -2.4625697 -2.7885075 -2.9542243 -3.3141837 -3.2843037 -2.8157687][-2.4083939 -2.7128634 -2.9436262 -3.1355753 -3.26054 -3.494457 -3.4315605 -2.83447 -2.540421 -2.9984732 -3.0375454 -2.9757953 -3.1288307 -3.1375494 -2.9533277][-2.9902687 -3.1068621 -3.1477971 -3.2811272 -3.3821507 -3.5079439 -3.4316907 -3.0832677 -2.952456 -3.2202303 -3.1223974 -2.9655104 -2.9736762 -2.9161611 -2.8263545][-3.0716221 -3.0828674 -3.0678225 -3.2047274 -3.2960017 -3.2898486 -3.1438637 -2.9454024 -2.9443147 -3.1373167 -3.0460839 -2.93518 -2.9416187 -2.8768711 -2.8060641]]...]
INFO - root - 2017-12-07 09:10:57.451416: step 23510, loss = 0.83, batch loss = 0.76 (7.3 examples/sec; 1.103 sec/batch; 94h:38m:42s remains)
INFO - root - 2017-12-07 09:11:08.141071: step 23520, loss = 0.89, batch loss = 0.82 (7.2 examples/sec; 1.113 sec/batch; 95h:32m:43s remains)
INFO - root - 2017-12-07 09:11:19.048630: step 23530, loss = 0.69, batch loss = 0.62 (7.2 examples/sec; 1.107 sec/batch; 95h:01m:22s remains)
INFO - root - 2017-12-07 09:11:29.564000: step 23540, loss = 0.78, batch loss = 0.70 (7.5 examples/sec; 1.069 sec/batch; 91h:44m:00s remains)
INFO - root - 2017-12-07 09:11:40.257487: step 23550, loss = 0.67, batch loss = 0.60 (7.5 examples/sec; 1.073 sec/batch; 92h:02m:58s remains)
INFO - root - 2017-12-07 09:11:50.991998: step 23560, loss = 0.74, batch loss = 0.67 (7.5 examples/sec; 1.062 sec/batch; 91h:10m:35s remains)
INFO - root - 2017-12-07 09:12:01.665768: step 23570, loss = 0.79, batch loss = 0.72 (7.5 examples/sec; 1.074 sec/batch; 92h:08m:24s remains)
INFO - root - 2017-12-07 09:12:12.403486: step 23580, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.064 sec/batch; 91h:20m:38s remains)
INFO - root - 2017-12-07 09:12:23.127041: step 23590, loss = 0.80, batch loss = 0.73 (7.6 examples/sec; 1.059 sec/batch; 90h:50m:12s remains)
INFO - root - 2017-12-07 09:12:33.822377: step 23600, loss = 0.79, batch loss = 0.71 (8.1 examples/sec; 0.988 sec/batch; 84h:47m:26s remains)
2017-12-07 09:12:34.700303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1107743 -2.154762 -2.2632418 -2.4027421 -2.594208 -2.6336098 -2.5128579 -2.2541027 -1.8591995 -1.4246764 -1.0695748 -1.0200148 -1.2833278 -1.5863719 -1.7965505][-2.0516682 -2.0721598 -2.1513727 -2.3039021 -2.5706575 -2.6956658 -2.746357 -2.6150117 -2.1859245 -1.6107337 -1.0597999 -0.89116979 -1.2137923 -1.6135929 -1.8611588][-1.9860384 -1.9692366 -1.9731879 -2.0836551 -2.3430238 -2.4959693 -2.7418408 -2.8227651 -2.4710727 -1.8732967 -1.1960609 -0.89003253 -1.2270348 -1.7092326 -1.9609666][-1.976073 -1.9299712 -1.8260863 -1.8462968 -2.0172675 -2.0878611 -2.3900695 -2.5752602 -2.3309865 -1.8623323 -1.2394514 -0.89875174 -1.2542698 -1.7699151 -1.9718697][-1.9701016 -1.8989012 -1.7445598 -1.7519925 -1.8715441 -1.790333 -1.9396925 -1.9897757 -1.7730289 -1.518894 -1.1291144 -0.91026783 -1.2963603 -1.7746503 -1.8801119][-1.9834664 -1.9267924 -1.80936 -1.8683655 -1.9268899 -1.6671953 -1.558373 -1.3842988 -1.1700003 -1.127176 -1.0182705 -0.98861623 -1.3918831 -1.781168 -1.789341][-1.9991882 -1.9576352 -1.8657508 -1.9018848 -1.8527961 -1.5234137 -1.2923503 -1.0476897 -0.91378665 -1.0055823 -1.0532718 -1.1103308 -1.4816606 -1.7993653 -1.7781312][-1.9931612 -1.9181042 -1.7890389 -1.7264245 -1.5980296 -1.3715234 -1.1798482 -1.0185182 -1.000567 -1.1303732 -1.218924 -1.2546022 -1.5500727 -1.8376865 -1.8385684][-2.0012636 -1.8907211 -1.7352974 -1.5854478 -1.4221575 -1.2857592 -1.0533438 -0.92181563 -0.979321 -1.1350868 -1.2802403 -1.3169367 -1.5522258 -1.8367753 -1.8688254][-2.0268991 -1.9189899 -1.8068554 -1.6246176 -1.4224305 -1.2393541 -0.82354259 -0.61484885 -0.68742108 -0.88410068 -1.124697 -1.201952 -1.4150262 -1.7259603 -1.7985342][-2.048085 -1.9772215 -1.9560115 -1.7661815 -1.5324616 -1.2720129 -0.6734817 -0.36330128 -0.39658642 -0.5908699 -0.87968493 -0.9801321 -1.2027309 -1.5574558 -1.674026][-2.0540261 -2.0342078 -2.0859051 -1.8706598 -1.6028831 -1.266715 -0.544466 -0.1881032 -0.22090626 -0.40115929 -0.6900413 -0.80540657 -1.0798831 -1.4827943 -1.6216729][-2.0370905 -2.0499535 -2.1182704 -1.8635857 -1.5701981 -1.1714249 -0.39058447 -0.064726353 -0.18418264 -0.41068077 -0.71972489 -0.86513591 -1.1679358 -1.5561202 -1.6684148][-2.0180233 -2.0304136 -2.0551164 -1.7614417 -1.4564259 -1.0283055 -0.28380823 -0.066565514 -0.29302645 -0.583158 -0.9318161 -1.1145773 -1.3874373 -1.6832054 -1.741153][-2.0156393 -1.9991531 -1.9420149 -1.6057014 -1.2908521 -0.88419104 -0.27534914 -0.19559288 -0.48289919 -0.80055547 -1.171597 -1.3755305 -1.5970461 -1.7919676 -1.8122077]]...]
INFO - root - 2017-12-07 09:12:45.500857: step 23610, loss = 0.84, batch loss = 0.77 (7.4 examples/sec; 1.078 sec/batch; 92h:27m:35s remains)
INFO - root - 2017-12-07 09:12:56.306027: step 23620, loss = 0.76, batch loss = 0.69 (7.6 examples/sec; 1.053 sec/batch; 90h:19m:10s remains)
INFO - root - 2017-12-07 09:13:06.988880: step 23630, loss = 0.71, batch loss = 0.64 (7.5 examples/sec; 1.063 sec/batch; 91h:12m:07s remains)
INFO - root - 2017-12-07 09:13:17.681329: step 23640, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 1.136 sec/batch; 97h:28m:48s remains)
INFO - root - 2017-12-07 09:13:28.316686: step 23650, loss = 0.94, batch loss = 0.87 (7.5 examples/sec; 1.073 sec/batch; 92h:04m:38s remains)
INFO - root - 2017-12-07 09:13:38.907931: step 23660, loss = 0.75, batch loss = 0.67 (7.6 examples/sec; 1.048 sec/batch; 89h:55m:48s remains)
INFO - root - 2017-12-07 09:13:49.647837: step 23670, loss = 0.93, batch loss = 0.85 (7.3 examples/sec; 1.090 sec/batch; 93h:29m:28s remains)
INFO - root - 2017-12-07 09:14:00.373352: step 23680, loss = 0.62, batch loss = 0.55 (7.5 examples/sec; 1.068 sec/batch; 91h:38m:52s remains)
INFO - root - 2017-12-07 09:14:10.964607: step 23690, loss = 0.81, batch loss = 0.74 (7.4 examples/sec; 1.082 sec/batch; 92h:49m:32s remains)
INFO - root - 2017-12-07 09:14:21.651312: step 23700, loss = 0.87, batch loss = 0.80 (7.0 examples/sec; 1.138 sec/batch; 97h:37m:23s remains)
2017-12-07 09:14:22.517818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1769481 -0.75088525 -0.8666687 -1.2828944 -1.6324573 -1.9401555 -2.5234513 -3.229943 -3.733032 -3.8321791 -3.556118 -2.8919415 -2.1821623 -1.9556618 -2.4871533][-1.0687263 -0.61437821 -0.61711407 -0.98333478 -1.3773909 -1.7299104 -2.2150304 -2.6955605 -2.985595 -3.0400348 -2.9036436 -2.5663338 -2.218574 -2.1087246 -2.434063][-1.1449904 -0.60437322 -0.36676264 -0.52975726 -0.88701534 -1.3624094 -1.9161248 -2.3302934 -2.5206325 -2.578341 -2.5767465 -2.5756636 -2.6020589 -2.6620283 -2.8896279][-1.3792555 -0.71917295 -0.20429897 -0.10289478 -0.38663292 -0.98588824 -1.6491966 -2.068954 -2.2108872 -2.2495384 -2.3233562 -2.5757413 -2.9684367 -3.2788746 -3.5191176][-1.511492 -0.67163396 0.15961313 0.55519009 0.36877632 -0.32654953 -1.0789268 -1.4777386 -1.5212004 -1.4910691 -1.610225 -2.0624604 -2.79876 -3.3946373 -3.7181761][-1.5002668 -0.431242 0.72258997 1.4076686 1.3517971 0.64752626 -0.10720539 -0.46212053 -0.41031456 -0.36030769 -0.63132095 -1.3175933 -2.3269377 -3.164309 -3.6013391][-1.3158102 -0.11146688 1.1984029 1.9971004 2.0486708 1.5450554 1.0347424 0.89940405 1.0778818 1.0015526 0.32574558 -0.72503781 -1.9494469 -2.931531 -3.4966028][-1.1846461 0.090934277 1.4388819 2.238327 2.3769903 2.1718488 2.0557351 2.2684684 2.5322733 2.1021686 0.82436132 -0.61962867 -1.9562511 -2.9371469 -3.5009718][-1.3757284 -0.074052811 1.2547293 2.0044842 2.1527085 2.1179543 2.289979 2.7333093 2.9105344 2.0693593 0.36187983 -1.2499089 -2.4442344 -3.2211449 -3.6948161][-1.8299348 -0.57001281 0.6386323 1.2384324 1.3150134 1.369812 1.7131996 2.2682776 2.316226 1.1987472 -0.69745564 -2.2359555 -3.0691223 -3.4358342 -3.6656334][-2.2124169 -1.0670042 -0.046234131 0.40810013 0.44381714 0.57317781 1.0711632 1.7272482 1.7020674 0.39865112 -1.5959876 -3.0433364 -3.5354292 -3.4331284 -3.2715168][-2.1161091 -1.1554363 -0.36413193 -0.031227589 0.022188663 0.26200581 0.936996 1.7045097 1.6307707 0.17776537 -1.8942015 -3.3227415 -3.7169 -3.3745804 -2.9161592][-1.8267043 -1.0605116 -0.47520852 -0.20756483 -0.087260723 0.27983618 1.1231284 1.9980879 1.9725165 0.59287882 -1.3436999 -2.6887035 -3.0805383 -2.6940589 -2.1225376][-1.7906063 -1.1643291 -0.72281623 -0.50875378 -0.37534475 0.058739185 0.99164057 1.9567409 2.1190929 1.1194754 -0.39187193 -1.5164657 -1.8875637 -1.5828595 -1.0696125][-1.6613753 -1.0694129 -0.68318033 -0.51769352 -0.43278313 -0.060316563 0.77747011 1.7014842 2.0258408 1.4765105 0.44816875 -0.43752909 -0.79081297 -0.61906767 -0.26839161]]...]
INFO - root - 2017-12-07 09:14:33.207423: step 23710, loss = 0.66, batch loss = 0.58 (7.5 examples/sec; 1.062 sec/batch; 91h:04m:53s remains)
INFO - root - 2017-12-07 09:14:43.819333: step 23720, loss = 0.74, batch loss = 0.67 (7.6 examples/sec; 1.048 sec/batch; 89h:51m:28s remains)
INFO - root - 2017-12-07 09:14:54.530489: step 23730, loss = 0.76, batch loss = 0.68 (7.5 examples/sec; 1.062 sec/batch; 91h:03m:13s remains)
INFO - root - 2017-12-07 09:15:04.996955: step 23740, loss = 0.80, batch loss = 0.73 (7.9 examples/sec; 1.014 sec/batch; 86h:58m:02s remains)
INFO - root - 2017-12-07 09:15:15.676736: step 23750, loss = 0.79, batch loss = 0.72 (7.3 examples/sec; 1.090 sec/batch; 93h:30m:10s remains)
INFO - root - 2017-12-07 09:15:26.378617: step 23760, loss = 0.83, batch loss = 0.75 (7.5 examples/sec; 1.060 sec/batch; 90h:54m:05s remains)
INFO - root - 2017-12-07 09:15:37.054788: step 23770, loss = 0.63, batch loss = 0.55 (7.5 examples/sec; 1.062 sec/batch; 91h:06m:06s remains)
INFO - root - 2017-12-07 09:15:47.853065: step 23780, loss = 1.15, batch loss = 1.08 (7.2 examples/sec; 1.105 sec/batch; 94h:47m:31s remains)
INFO - root - 2017-12-07 09:15:58.707100: step 23790, loss = 0.81, batch loss = 0.73 (7.3 examples/sec; 1.090 sec/batch; 93h:28m:24s remains)
INFO - root - 2017-12-07 09:16:09.447810: step 23800, loss = 0.85, batch loss = 0.78 (7.4 examples/sec; 1.083 sec/batch; 92h:51m:39s remains)
2017-12-07 09:16:10.302182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8347371 -1.9911692 -2.1066916 -2.1245735 -2.0984538 -2.0610137 -2.0414016 -1.943377 -1.7968132 -1.7206731 -1.7451165 -1.9087429 -2.0241821 -2.0454359 -2.1647017][-2.2291639 -2.3862715 -2.4873614 -2.4956563 -2.4352903 -2.3338416 -2.2428124 -2.1345804 -2.0127361 -1.9515667 -1.975826 -2.1174583 -2.2407603 -2.3053136 -2.4308105][-2.4493151 -2.53419 -2.5809546 -2.5830522 -2.521843 -2.431304 -2.3667536 -2.3227189 -2.2586031 -2.1885889 -2.1296971 -2.14426 -2.1994307 -2.2667372 -2.4116669][-2.3259051 -2.334944 -2.2975686 -2.2506881 -2.1849647 -2.1408689 -2.1274538 -2.114099 -2.0657103 -1.9959314 -1.9230428 -1.951438 -2.08332 -2.2528484 -2.4759417][-1.7286038 -1.8168752 -1.837635 -1.8046019 -1.7506461 -1.7544613 -1.7908611 -1.7856259 -1.7037055 -1.5701165 -1.4242454 -1.426657 -1.604481 -1.851505 -2.1712987][-0.72810936 -0.87727308 -1.0666981 -1.1788249 -1.1927238 -1.1992171 -1.1695893 -1.0433967 -0.8552711 -0.67457581 -0.52309108 -0.5374279 -0.74129176 -1.0186069 -1.4351037][-0.18433094 -0.28364944 -0.49461198 -0.615824 -0.62593865 -0.57838106 -0.37919092 -0.015793324 0.32051659 0.47099495 0.47919035 0.31276846 -0.0044431686 -0.32161808 -0.74193859][-0.16852713 -0.21512222 -0.3804307 -0.46292448 -0.46911 -0.41590881 -0.18132162 0.16158342 0.35745764 0.269516 0.080845356 -0.13826942 -0.35073233 -0.45326447 -0.6150167][-0.96631289 -0.986918 -1.0804753 -1.1227121 -1.1485281 -1.0929928 -0.84876347 -0.54627681 -0.42057943 -0.58549929 -0.83014774 -1.04843 -1.1478567 -1.0395789 -0.95856023][-1.4457304 -1.6343391 -1.7796507 -1.8338888 -1.8600998 -1.7647803 -1.4935112 -1.1634851 -0.94433212 -0.96076727 -1.0988958 -1.2930558 -1.3896742 -1.2775197 -1.2043846][-1.060427 -1.358319 -1.5727026 -1.7111316 -1.8203034 -1.7926564 -1.6210935 -1.3996551 -1.2286325 -1.222472 -1.3203537 -1.4954388 -1.6129727 -1.5790529 -1.6260338][-1.3113897 -1.4687023 -1.6105578 -1.7686865 -1.9511313 -2.0313997 -2.0299788 -1.9959464 -1.9273088 -1.9099298 -1.902643 -1.946667 -1.9883714 -1.9682224 -2.0732958][-2.0327773 -2.0529153 -2.1313179 -2.2838171 -2.4831591 -2.6040604 -2.6629269 -2.67843 -2.6172833 -2.5434256 -2.45521 -2.4315963 -2.458467 -2.4761212 -2.5842719][-2.5120282 -2.5190344 -2.5805542 -2.6857698 -2.8147612 -2.8794699 -2.8816268 -2.8263612 -2.7170415 -2.6023312 -2.5066657 -2.5103936 -2.5818715 -2.6458135 -2.7245517][-2.5238156 -2.5525045 -2.5959086 -2.6097293 -2.5980456 -2.566381 -2.5234463 -2.4792874 -2.4357464 -2.3700542 -2.3027945 -2.3043587 -2.3409474 -2.3704896 -2.3900297]]...]
INFO - root - 2017-12-07 09:16:21.072814: step 23810, loss = 0.69, batch loss = 0.62 (7.4 examples/sec; 1.087 sec/batch; 93h:14m:44s remains)
INFO - root - 2017-12-07 09:16:31.768110: step 23820, loss = 0.98, batch loss = 0.91 (7.4 examples/sec; 1.079 sec/batch; 92h:33m:32s remains)
INFO - root - 2017-12-07 09:16:42.491793: step 23830, loss = 0.82, batch loss = 0.75 (7.7 examples/sec; 1.037 sec/batch; 88h:55m:22s remains)
INFO - root - 2017-12-07 09:16:53.116257: step 23840, loss = 0.75, batch loss = 0.68 (7.5 examples/sec; 1.062 sec/batch; 91h:01m:20s remains)
INFO - root - 2017-12-07 09:17:03.854710: step 23850, loss = 0.67, batch loss = 0.60 (7.4 examples/sec; 1.079 sec/batch; 92h:29m:56s remains)
INFO - root - 2017-12-07 09:17:14.523467: step 23860, loss = 0.82, batch loss = 0.75 (7.3 examples/sec; 1.090 sec/batch; 93h:24m:51s remains)
INFO - root - 2017-12-07 09:17:25.232085: step 23870, loss = 0.96, batch loss = 0.88 (7.2 examples/sec; 1.116 sec/batch; 95h:38m:01s remains)
INFO - root - 2017-12-07 09:17:35.910395: step 23880, loss = 0.66, batch loss = 0.59 (7.5 examples/sec; 1.068 sec/batch; 91h:32m:01s remains)
INFO - root - 2017-12-07 09:17:46.565309: step 23890, loss = 0.81, batch loss = 0.74 (7.6 examples/sec; 1.047 sec/batch; 89h:46m:55s remains)
INFO - root - 2017-12-07 09:17:57.376462: step 23900, loss = 1.04, batch loss = 0.97 (7.6 examples/sec; 1.047 sec/batch; 89h:46m:09s remains)
2017-12-07 09:17:58.248209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8438144 -2.7893524 -2.8003144 -2.870388 -2.9100575 -2.8586709 -2.7743983 -2.7074442 -2.8211298 -3.0836792 -3.2565598 -3.2272463 -3.1395459 -3.2343369 -3.4448419][-2.4166813 -2.3161137 -2.3068764 -2.4247994 -2.501431 -2.380393 -2.2078478 -2.0945256 -2.2051058 -2.5042067 -2.7411909 -2.8121631 -2.8497033 -3.0590625 -3.3712063][-1.5158126 -1.4325986 -1.493387 -1.7335904 -1.9293244 -1.8240833 -1.6045859 -1.4415836 -1.5435474 -1.8876989 -2.1517346 -2.2322814 -2.2952938 -2.5705028 -3.0121412][-1.0736134 -1.1001041 -1.1769028 -1.3013084 -1.2762899 -0.90952325 -0.48187518 -0.23103046 -0.44148397 -1.0222902 -1.5117512 -1.7524357 -1.8913882 -2.2026715 -2.7248611][-1.1737878 -1.3941214 -1.6257486 -1.7698674 -1.5894718 -0.96650577 -0.23696995 0.27919436 0.2124815 -0.28806067 -0.75946903 -1.0537009 -1.2916911 -1.7468872 -2.47509][-0.97387862 -1.1446517 -1.4870765 -1.81745 -1.8040226 -1.3178387 -0.67049551 -0.21772957 -0.3216691 -0.6868484 -0.89760804 -0.8874135 -0.8668611 -1.2582884 -2.1436188][-0.63183427 -0.63082266 -0.86111593 -1.0880175 -0.96860456 -0.38624382 0.3222723 0.65284109 0.26565695 -0.38961458 -0.84912419 -0.91428375 -0.78595757 -1.0608068 -1.9549332][-0.90082788 -0.80332303 -0.86456776 -0.80256033 -0.33895063 0.59005833 1.6352625 2.1369991 1.7742901 0.97957516 0.18062782 -0.23075008 -0.36889696 -0.812068 -1.8195853][-1.5444729 -1.3875008 -1.3534679 -1.2547336 -0.81452608 0.0033926964 0.86909771 1.2772303 1.0843482 0.5762887 0.061311245 -0.086525917 -0.14036322 -0.60697961 -1.6846628][-1.8120492 -1.6428285 -1.7235515 -1.9350772 -1.8791311 -1.4151723 -0.89635849 -0.70104074 -0.85309887 -1.1496999 -1.3058395 -1.0125878 -0.6798327 -0.86341572 -1.795119][-1.7030153 -1.4678595 -1.5876405 -2.0013416 -2.1649969 -1.8243191 -1.3163552 -1.0628114 -1.1766322 -1.5409451 -1.8171818 -1.65539 -1.3088694 -1.3642969 -2.1636484][-1.6018903 -1.4179149 -1.5807834 -2.0069129 -2.1731021 -1.8986905 -1.4150248 -1.0608156 -1.0197592 -1.3187487 -1.6914392 -1.7614417 -1.5754683 -1.7070353 -2.5055976][-1.6219246 -1.519454 -1.7428057 -2.1559665 -2.3758409 -2.3013513 -2.097306 -1.9424989 -1.9447901 -2.1350305 -2.3845549 -2.3661287 -2.1227853 -2.2153378 -2.8898196][-2.0074387 -1.8851395 -1.9591951 -2.1294739 -2.2110639 -2.1547952 -2.0694256 -2.0561159 -2.190577 -2.5028567 -2.8408275 -2.8874207 -2.7304487 -2.8317387 -3.3052044][-2.6210594 -2.5631657 -2.5876486 -2.6234865 -2.6160784 -2.5585856 -2.4919529 -2.4125996 -2.4070749 -2.5844851 -2.8421416 -2.9132679 -2.8988512 -3.0924745 -3.4690156]]...]
INFO - root - 2017-12-07 09:18:09.005535: step 23910, loss = 0.83, batch loss = 0.75 (7.4 examples/sec; 1.075 sec/batch; 92h:09m:43s remains)
INFO - root - 2017-12-07 09:18:19.790530: step 23920, loss = 0.69, batch loss = 0.62 (7.3 examples/sec; 1.098 sec/batch; 94h:07m:23s remains)
INFO - root - 2017-12-07 09:18:30.486101: step 23930, loss = 0.69, batch loss = 0.62 (7.6 examples/sec; 1.049 sec/batch; 89h:56m:14s remains)
INFO - root - 2017-12-07 09:18:40.953292: step 23940, loss = 0.63, batch loss = 0.56 (7.7 examples/sec; 1.045 sec/batch; 89h:33m:12s remains)
INFO - root - 2017-12-07 09:18:51.675642: step 23950, loss = 0.99, batch loss = 0.92 (7.5 examples/sec; 1.062 sec/batch; 90h:59m:39s remains)
INFO - root - 2017-12-07 09:19:02.382832: step 23960, loss = 0.87, batch loss = 0.80 (7.5 examples/sec; 1.060 sec/batch; 90h:52m:08s remains)
INFO - root - 2017-12-07 09:19:13.181424: step 23970, loss = 0.85, batch loss = 0.78 (7.5 examples/sec; 1.070 sec/batch; 91h:40m:25s remains)
INFO - root - 2017-12-07 09:19:23.804366: step 23980, loss = 0.73, batch loss = 0.66 (7.5 examples/sec; 1.062 sec/batch; 91h:02m:16s remains)
INFO - root - 2017-12-07 09:19:34.367026: step 23990, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.055 sec/batch; 90h:26m:54s remains)
INFO - root - 2017-12-07 09:19:45.131957: step 24000, loss = 0.96, batch loss = 0.89 (7.0 examples/sec; 1.148 sec/batch; 98h:22m:38s remains)
2017-12-07 09:19:46.004595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.634367 -1.5998042 -1.7823815 -1.9736078 -1.9735744 -2.2652514 -2.4263146 -2.4412816 -2.7054377 -2.6592522 -2.5470214 -3.2779226 -4.2080216 -3.8939075 -3.1207027][-1.9848979 -2.1360283 -2.2915902 -2.3606136 -2.2416525 -2.4089432 -2.5039511 -2.3115649 -2.1552975 -1.8648815 -1.9253163 -2.923945 -4.053225 -3.8773065 -3.1321402][-2.3411303 -2.5573864 -2.667578 -2.7213044 -2.6982021 -2.9145215 -3.0266528 -2.7204869 -2.236536 -1.6981468 -1.7457671 -2.7331667 -3.7765741 -3.6061974 -2.9451642][-2.4907284 -2.6163044 -2.7159562 -2.8839827 -3.0453742 -3.3227861 -3.4743953 -3.2440076 -2.7736964 -2.2812274 -2.3591049 -3.1461892 -3.777437 -3.3579841 -2.6650414][-2.1250331 -2.1392269 -2.2660947 -2.5872669 -2.9229999 -3.2203083 -3.400691 -3.340363 -3.1082067 -2.9048462 -3.1639643 -3.820147 -4.0392852 -3.3137195 -2.5263972][-1.4960964 -1.4271014 -1.6301501 -2.0550759 -2.3686657 -2.5015342 -2.5452509 -2.5372076 -2.56991 -2.8064578 -3.474431 -4.2921224 -4.4103761 -3.5321188 -2.6654871][-1.0158622 -1.0157614 -1.3481858 -1.752461 -1.8648806 -1.7260494 -1.521369 -1.3910286 -1.5376513 -2.1193509 -3.1595521 -4.3293142 -4.6607323 -3.87124 -3.0370777][-0.96204662 -1.0135908 -1.3274255 -1.5527713 -1.4093926 -1.1203706 -0.89807248 -0.72045946 -0.78554463 -1.3123527 -2.3267286 -3.6817567 -4.3716946 -3.8977075 -3.2401152][-1.4174392 -1.3800764 -1.5290627 -1.5422225 -1.2749467 -1.0752561 -1.1373396 -1.0987079 -1.0028057 -1.1580417 -1.764993 -2.9498887 -3.7736771 -3.4740946 -2.9689415][-2.2109394 -2.0685275 -2.0016241 -1.8178232 -1.5164521 -1.4635661 -1.8473914 -2.0795617 -1.9940789 -1.9099753 -2.1388359 -2.9789543 -3.581625 -3.0997548 -2.5146885][-3.0498905 -2.8674378 -2.6512089 -2.3473437 -2.0317771 -2.0340085 -2.5107992 -2.860724 -2.8323383 -2.6934834 -2.7669654 -3.3510914 -3.6620309 -2.9343793 -2.2321992][-3.7086997 -3.5999537 -3.4134245 -3.1340761 -2.8223686 -2.740551 -3.0184975 -3.2268045 -3.1872163 -3.0702515 -3.1484718 -3.6750884 -3.867703 -3.0783186 -2.3497322][-3.5664334 -3.5477924 -3.4776378 -3.336266 -3.1212182 -3.0047276 -3.0808673 -3.1187172 -3.0139713 -2.8538346 -2.9684963 -3.6055112 -3.9303269 -3.3405881 -2.7318578][-2.62879 -2.6522496 -2.6601441 -2.6414523 -2.5702159 -2.51869 -2.5206022 -2.4472179 -2.2058258 -1.8783097 -1.9719479 -2.8109202 -3.484252 -3.325845 -2.9866192][-2.0622513 -2.0375838 -2.0106704 -1.9842212 -1.9543679 -1.937222 -1.9192159 -1.8356369 -1.5755291 -1.1579607 -1.2382526 -2.250339 -3.208672 -3.35706 -3.1689286]]...]
INFO - root - 2017-12-07 09:19:56.733191: step 24010, loss = 0.72, batch loss = 0.65 (7.4 examples/sec; 1.087 sec/batch; 93h:07m:12s remains)
INFO - root - 2017-12-07 09:20:07.412644: step 24020, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.066 sec/batch; 91h:22m:24s remains)
INFO - root - 2017-12-07 09:20:18.145850: step 24030, loss = 0.77, batch loss = 0.69 (7.6 examples/sec; 1.053 sec/batch; 90h:11m:38s remains)
INFO - root - 2017-12-07 09:20:28.646954: step 24040, loss = 0.81, batch loss = 0.74 (7.7 examples/sec; 1.044 sec/batch; 89h:28m:47s remains)
INFO - root - 2017-12-07 09:20:39.355218: step 24050, loss = 0.63, batch loss = 0.56 (7.4 examples/sec; 1.080 sec/batch; 92h:30m:19s remains)
INFO - root - 2017-12-07 09:20:50.163478: step 24060, loss = 0.94, batch loss = 0.87 (7.4 examples/sec; 1.077 sec/batch; 92h:14m:25s remains)
INFO - root - 2017-12-07 09:21:00.833110: step 24070, loss = 0.60, batch loss = 0.53 (7.5 examples/sec; 1.066 sec/batch; 91h:19m:16s remains)
INFO - root - 2017-12-07 09:21:11.645259: step 24080, loss = 0.70, batch loss = 0.63 (7.5 examples/sec; 1.068 sec/batch; 91h:28m:02s remains)
INFO - root - 2017-12-07 09:21:22.509704: step 24090, loss = 0.69, batch loss = 0.61 (7.1 examples/sec; 1.120 sec/batch; 95h:56m:31s remains)
INFO - root - 2017-12-07 09:21:33.232010: step 24100, loss = 0.82, batch loss = 0.75 (7.6 examples/sec; 1.048 sec/batch; 89h:48m:03s remains)
2017-12-07 09:21:34.095699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7353296 -3.700325 -3.653687 -3.5953658 -3.5158224 -3.4893827 -3.5261197 -3.605588 -3.7381496 -3.9065604 -4.0308537 -4.0619483 -4.0190988 -3.9269223 -3.8210485][-4.3458304 -4.2519755 -4.1275492 -3.97753 -3.8049893 -3.7223854 -3.726809 -3.7607267 -3.8766646 -4.0849385 -4.2621312 -4.3437734 -4.3848825 -4.3462996 -4.2107248][-4.56258 -4.4729404 -4.374742 -4.242897 -4.0346141 -3.8553751 -3.7475805 -3.6406054 -3.6002424 -3.6386991 -3.6546068 -3.6145444 -3.6029451 -3.5367119 -3.4138799][-4.2630844 -4.2598329 -4.29637 -4.3125582 -4.1941967 -3.9681456 -3.743351 -3.5292888 -3.3296659 -3.2234974 -3.1637897 -3.1174555 -3.1221743 -3.1294649 -3.1363819][-3.4743073 -3.5249853 -3.662508 -3.8042006 -3.8511999 -3.7567916 -3.6309702 -3.498333 -3.3286052 -3.2261937 -3.197186 -3.2238188 -3.3487804 -3.6083758 -3.9359577][-2.8667905 -2.7819433 -2.6966848 -2.5553031 -2.4831033 -2.5257602 -2.7165523 -2.9468203 -3.0922246 -3.1744957 -3.1861949 -3.0698485 -2.9502411 -3.0171633 -3.2483368][-2.5111403 -2.0796816 -1.4574227 -0.70057821 -0.26441431 -0.39773893 -0.96908808 -1.6697297 -2.2307644 -2.5872397 -2.611551 -2.2807825 -1.7666988 -1.3500633 -1.1792889][-2.2472517 -1.5911732 -0.70565939 0.25783491 0.812243 0.72508812 0.18532133 -0.48217297 -1.0287552 -1.4060602 -1.4486256 -1.1385951 -0.57513 -0.026618481 0.26092863][-2.3583899 -1.9868908 -1.5654435 -1.1681623 -0.92316771 -0.90517688 -1.0919158 -1.3218224 -1.4772081 -1.5691292 -1.505821 -1.2169704 -0.73931241 -0.23269653 0.10539341][-3.12893 -3.2323198 -3.3622513 -3.4643221 -3.4393992 -3.3167195 -3.2849088 -3.335217 -3.369087 -3.3532226 -3.2608886 -3.0427141 -2.6937039 -2.2220037 -1.7652414][-4.2805595 -4.5475092 -4.7307339 -4.7579603 -4.6358147 -4.4790411 -4.4851913 -4.6018553 -4.7141991 -4.7559724 -4.7345753 -4.66797 -4.5124145 -4.1908565 -3.7689931][-4.8045244 -4.8657122 -4.7452097 -4.4539413 -4.1888752 -4.0741897 -4.1338811 -4.2552791 -4.3690352 -4.4328008 -4.4353666 -4.4641294 -4.52089 -4.4983449 -4.402863][-4.1530266 -3.8960607 -3.4997287 -3.05926 -2.7551823 -2.6398456 -2.645731 -2.6926825 -2.7411237 -2.778419 -2.7550244 -2.7640629 -2.87702 -3.0323396 -3.237988][-3.5846562 -3.3828416 -3.1644092 -2.9993131 -2.933567 -2.9602361 -3.0091596 -3.0462155 -3.05689 -3.0388846 -2.9502141 -2.8558478 -2.8503785 -2.9328222 -3.1093078][-3.3374195 -3.3368099 -3.4108005 -3.5718358 -3.7607231 -3.9297619 -4.046207 -4.1169996 -4.1409326 -4.1236639 -4.0503755 -3.9496682 -3.8816166 -3.8672714 -3.8992085]]...]
INFO - root - 2017-12-07 09:21:44.850940: step 24110, loss = 0.92, batch loss = 0.85 (7.5 examples/sec; 1.062 sec/batch; 90h:57m:08s remains)
INFO - root - 2017-12-07 09:21:55.630055: step 24120, loss = 0.67, batch loss = 0.60 (7.7 examples/sec; 1.038 sec/batch; 88h:54m:49s remains)
INFO - root - 2017-12-07 09:22:06.342279: step 24130, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 1.124 sec/batch; 96h:14m:34s remains)
INFO - root - 2017-12-07 09:22:16.767546: step 24140, loss = 0.80, batch loss = 0.73 (7.8 examples/sec; 1.031 sec/batch; 88h:18m:28s remains)
INFO - root - 2017-12-07 09:22:27.320377: step 24150, loss = 0.75, batch loss = 0.68 (7.3 examples/sec; 1.095 sec/batch; 93h:49m:18s remains)
INFO - root - 2017-12-07 09:22:37.980571: step 24160, loss = 0.90, batch loss = 0.83 (7.6 examples/sec; 1.048 sec/batch; 89h:44m:54s remains)
INFO - root - 2017-12-07 09:22:48.594388: step 24170, loss = 0.66, batch loss = 0.59 (7.4 examples/sec; 1.079 sec/batch; 92h:24m:36s remains)
INFO - root - 2017-12-07 09:22:59.188939: step 24180, loss = 0.74, batch loss = 0.67 (7.8 examples/sec; 1.022 sec/batch; 87h:31m:55s remains)
INFO - root - 2017-12-07 09:23:09.773351: step 24190, loss = 0.71, batch loss = 0.64 (7.8 examples/sec; 1.028 sec/batch; 88h:00m:15s remains)
INFO - root - 2017-12-07 09:23:20.440914: step 24200, loss = 0.76, batch loss = 0.68 (7.4 examples/sec; 1.075 sec/batch; 92h:05m:58s remains)
2017-12-07 09:23:21.336168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8410954 -2.6821153 -2.2943916 -2.3446448 -2.7799885 -2.9573894 -2.7907658 -2.7374854 -2.9329548 -3.0519457 -3.0018716 -2.8760104 -2.7745996 -2.6737704 -2.6537986][-2.0460365 -1.6705534 -1.0316167 -1.1436489 -1.7681706 -1.934551 -1.7522538 -1.8381307 -2.2782502 -2.5952587 -2.5576353 -2.2730687 -2.0228634 -1.8634217 -1.7855628][-1.6552804 -1.0687308 -0.25798368 -0.42776585 -1.0092897 -0.92553234 -0.72319365 -0.9448595 -1.5266004 -1.9728005 -1.960007 -1.6563532 -1.5736697 -1.6449254 -1.5981417][-1.594341 -0.79801917 -0.024859428 -0.2555213 -0.63054752 -0.35675859 -0.27536821 -0.55068564 -1.0794566 -1.6338973 -1.7244265 -1.5945432 -1.8323205 -2.0351181 -1.7635667][-1.3771322 -0.49044824 -0.042249203 -0.46228981 -0.67700982 -0.3026185 -0.24214745 -0.2534976 -0.51066732 -1.0980773 -1.2519279 -1.2389045 -1.588279 -1.6522737 -1.099401][-1.147191 -0.26275063 -0.11670113 -0.59784794 -0.71086144 -0.31432962 -0.0312953 0.48366928 0.47065115 -0.21282673 -0.4209075 -0.40256929 -0.64353633 -0.52288413 -0.015616417][-0.87807155 0.023734093 0.15592194 -0.078510761 -0.021622181 0.36142778 0.9748559 1.8242602 1.6509032 0.68056488 0.43794203 0.58658838 0.53215837 0.63257551 0.71126509][-0.72801638 0.14937782 0.43621731 0.58949423 0.74459314 1.1072774 2.0263271 2.9802508 2.434505 1.1428623 0.68115139 0.68195391 0.57829 0.5070653 0.24562263][-0.95927262 -0.10815239 0.34429216 0.70700312 0.74275064 0.87787008 1.6738644 2.361464 1.7956386 0.76863575 0.33933306 0.20436096 -0.061600685 -0.3102169 -0.63074231][-1.3206367 -0.49575639 0.053775311 0.4450388 0.26966286 0.12565231 0.5402565 0.898294 0.63172626 0.3162303 0.26355743 0.15673733 -0.26235676 -0.6873126 -0.97214413][-1.6597028 -1.0599508 -0.63834643 -0.30424404 -0.54087877 -0.85645247 -0.71820879 -0.51829696 -0.49943829 -0.35897875 -0.2551856 -0.35182428 -0.72885919 -1.0957901 -1.2772224][-2.0366869 -1.858721 -1.756072 -1.4738026 -1.5471358 -1.7754939 -1.6663363 -1.4556246 -1.3182254 -1.1465638 -1.1822386 -1.2972517 -1.4142611 -1.5057516 -1.6177266][-2.4697604 -2.6140649 -2.7410135 -2.4712212 -2.3279495 -2.3660657 -2.163372 -1.9366136 -1.8461981 -1.82887 -1.9991047 -2.0366712 -1.9057844 -1.8489246 -2.0311317][-2.8074296 -2.8996539 -2.8911185 -2.5620155 -2.3433161 -2.3472166 -2.1810465 -2.0267391 -2.0143311 -2.083317 -2.2417634 -2.1931674 -1.9881408 -1.9606457 -2.2314095][-2.786159 -2.645298 -2.4207118 -2.1259937 -2.043251 -2.1699486 -2.160038 -2.1070964 -2.108712 -2.1375086 -2.2129543 -2.1523108 -2.01563 -2.0564919 -2.3286402]]...]
INFO - root - 2017-12-07 09:23:31.869617: step 24210, loss = 0.85, batch loss = 0.78 (7.7 examples/sec; 1.042 sec/batch; 89h:15m:09s remains)
INFO - root - 2017-12-07 09:23:42.636157: step 24220, loss = 0.90, batch loss = 0.83 (7.4 examples/sec; 1.080 sec/batch; 92h:27m:59s remains)
INFO - root - 2017-12-07 09:23:53.396626: step 24230, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.072 sec/batch; 91h:47m:31s remains)
INFO - root - 2017-12-07 09:24:03.904675: step 24240, loss = 0.92, batch loss = 0.85 (7.6 examples/sec; 1.053 sec/batch; 90h:08m:53s remains)
INFO - root - 2017-12-07 09:24:14.676846: step 24250, loss = 0.70, batch loss = 0.63 (7.5 examples/sec; 1.072 sec/batch; 91h:49m:57s remains)
INFO - root - 2017-12-07 09:24:25.429838: step 24260, loss = 0.90, batch loss = 0.82 (7.6 examples/sec; 1.059 sec/batch; 90h:39m:24s remains)
INFO - root - 2017-12-07 09:24:36.174922: step 24270, loss = 0.90, batch loss = 0.83 (7.5 examples/sec; 1.062 sec/batch; 90h:58m:07s remains)
INFO - root - 2017-12-07 09:24:46.872648: step 24280, loss = 0.86, batch loss = 0.79 (7.4 examples/sec; 1.084 sec/batch; 92h:48m:02s remains)
INFO - root - 2017-12-07 09:24:57.669827: step 24290, loss = 0.81, batch loss = 0.74 (7.4 examples/sec; 1.077 sec/batch; 92h:12m:06s remains)
INFO - root - 2017-12-07 09:25:08.367837: step 24300, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.047 sec/batch; 89h:36m:57s remains)
2017-12-07 09:25:09.197000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3784924 -2.3281596 -2.3882155 -1.9646556 -1.123822 -0.50745487 -0.35031605 -0.65730977 -1.4269588 -2.1531565 -2.416424 -2.2538767 -1.8549871 -1.5336316 -1.7025051][-2.2741854 -2.2298262 -2.407655 -2.1787682 -1.6111207 -1.1869495 -1.0105584 -1.150821 -1.6600149 -2.200742 -2.4663467 -2.5109558 -2.2442579 -1.8551028 -1.7246985][-2.3002973 -2.3718357 -2.65668 -2.5808308 -2.2860625 -2.1164861 -2.0394881 -2.0374894 -2.1225226 -2.2098484 -2.3150704 -2.5589747 -2.6005039 -2.3232942 -1.9997771][-2.3072407 -2.50984 -2.8324871 -2.8116674 -2.6583476 -2.6640503 -2.6964591 -2.6228547 -2.3570051 -1.9910653 -1.791723 -2.0355833 -2.3897827 -2.540153 -2.478457][-2.0594945 -2.1541233 -2.3289258 -2.3227732 -2.3094034 -2.4725742 -2.7027397 -2.7315512 -2.3728793 -1.7353821 -1.1748376 -1.1609538 -1.5276973 -1.9313817 -2.2217195][-1.708391 -1.5554593 -1.479574 -1.4185979 -1.4682262 -1.7417929 -2.2523282 -2.6004868 -2.450983 -1.8526106 -1.1550739 -0.88675904 -1.0041194 -1.2368767 -1.4714031][-1.7273464 -1.5126057 -1.3812368 -1.2688689 -1.169735 -1.2402432 -1.7310398 -2.2699745 -2.4365277 -2.1710365 -1.6703219 -1.3067391 -1.0333948 -0.77037954 -0.60983109][-2.0939929 -1.9512284 -1.8714032 -1.6060622 -1.146167 -0.81383204 -1.0053961 -1.3993585 -1.6342752 -1.5970731 -1.3583305 -1.0997629 -0.7383728 -0.25107145 0.17219019][-2.3143156 -2.1315165 -2.0289607 -1.5992341 -0.8506043 -0.1913352 -0.040203571 -0.23386049 -0.55418921 -0.76241326 -0.75639129 -0.66257834 -0.49198222 -0.19817305 0.1150651][-2.2425241 -2.0275857 -1.9630451 -1.5904348 -0.90550661 -0.25058365 0.10033751 0.14503717 -0.14049339 -0.52318335 -0.80782795 -0.97108126 -1.0498106 -0.98349619 -0.83126473][-2.1401908 -2.0377352 -2.19158 -2.1000481 -1.735919 -1.3056018 -0.944674 -0.75691724 -0.87537217 -1.1320081 -1.3556042 -1.5225291 -1.6773899 -1.7093058 -1.6315694][-2.1842458 -2.1496096 -2.4466214 -2.5825415 -2.4734535 -2.1625979 -1.7711947 -1.511848 -1.5246708 -1.6373444 -1.6840427 -1.7041254 -1.7590213 -1.7614095 -1.7314107][-2.4227619 -2.3493962 -2.5717478 -2.7522998 -2.7235498 -2.4062769 -1.9567385 -1.6733651 -1.6490564 -1.733613 -1.7585363 -1.8030989 -1.8916912 -1.9630053 -2.0220053][-2.8039079 -2.7367244 -2.9060106 -3.1197991 -3.1431937 -2.8445513 -2.4086814 -2.14546 -2.1126585 -2.1848488 -2.2335286 -2.306324 -2.4307756 -2.5596056 -2.6741073][-2.9809299 -2.9908166 -3.2135258 -3.4882259 -3.5625615 -3.3448312 -3.0188148 -2.8175516 -2.7717431 -2.8014078 -2.8154945 -2.8483338 -2.9055676 -2.9634166 -3.0124307]]...]
INFO - root - 2017-12-07 09:25:19.894758: step 24310, loss = 0.79, batch loss = 0.72 (7.4 examples/sec; 1.075 sec/batch; 92h:03m:07s remains)
INFO - root - 2017-12-07 09:25:30.658414: step 24320, loss = 0.91, batch loss = 0.84 (7.4 examples/sec; 1.083 sec/batch; 92h:40m:25s remains)
INFO - root - 2017-12-07 09:25:41.395552: step 24330, loss = 0.81, batch loss = 0.73 (7.4 examples/sec; 1.087 sec/batch; 93h:01m:47s remains)
INFO - root - 2017-12-07 09:25:51.939362: step 24340, loss = 0.87, batch loss = 0.80 (7.4 examples/sec; 1.082 sec/batch; 92h:39m:17s remains)
INFO - root - 2017-12-07 09:26:02.658555: step 24350, loss = 0.78, batch loss = 0.71 (7.3 examples/sec; 1.100 sec/batch; 94h:11m:14s remains)
INFO - root - 2017-12-07 09:26:13.318647: step 24360, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.067 sec/batch; 91h:20m:03s remains)
INFO - root - 2017-12-07 09:26:23.947443: step 24370, loss = 0.67, batch loss = 0.60 (7.3 examples/sec; 1.099 sec/batch; 94h:04m:13s remains)
INFO - root - 2017-12-07 09:26:34.657478: step 24380, loss = 0.88, batch loss = 0.81 (7.5 examples/sec; 1.072 sec/batch; 91h:45m:01s remains)
INFO - root - 2017-12-07 09:26:45.350699: step 24390, loss = 0.95, batch loss = 0.88 (7.3 examples/sec; 1.101 sec/batch; 94h:13m:04s remains)
INFO - root - 2017-12-07 09:26:56.030444: step 24400, loss = 0.85, batch loss = 0.78 (7.5 examples/sec; 1.061 sec/batch; 90h:49m:17s remains)
2017-12-07 09:26:56.875318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3761582 -4.5313125 -4.6279492 -4.5246391 -4.353519 -4.0785961 -3.7607875 -3.8062258 -3.9657385 -4.0066853 -3.9787145 -3.8605785 -3.8209481 -3.9204822 -4.1562862][-3.6092303 -3.9064531 -4.1521244 -4.0887737 -3.9032586 -3.5574806 -3.105545 -3.1704512 -3.4143219 -3.4614348 -3.3751178 -3.1690538 -3.056756 -3.0634742 -3.2526336][-2.7280517 -3.1524956 -3.539 -3.5345988 -3.3542504 -2.9653406 -2.4166493 -2.5242388 -2.9061794 -3.0300884 -2.9684269 -2.7193184 -2.4957151 -2.3158915 -2.3787889][-2.3254523 -2.8202362 -3.3023705 -3.4308574 -3.4370956 -3.2419605 -2.82026 -3.0420365 -3.5007443 -3.6153409 -3.506566 -3.1816502 -2.7710991 -2.3181744 -2.15146][-2.178606 -2.7220292 -3.2986712 -3.5502672 -3.687067 -3.6194339 -3.2393208 -3.4389944 -3.8688874 -3.983393 -3.8567479 -3.4978514 -3.0116825 -2.4935782 -2.3385694][-2.2166734 -2.5953252 -2.9948139 -3.1085215 -3.2179337 -3.1683187 -2.7117605 -2.7946587 -3.2446146 -3.5152187 -3.534235 -3.2992077 -2.9202108 -2.5297723 -2.5491192][-2.4368196 -2.7357895 -2.9370115 -2.8836884 -2.990958 -2.9817367 -2.4566994 -2.437696 -2.9113808 -3.2606633 -3.3009343 -3.1285219 -2.8477862 -2.5263057 -2.6070752][-2.0377278 -2.2427933 -2.310786 -2.1704547 -2.1842554 -1.945014 -1.1779373 -1.1243963 -1.7796133 -2.2525227 -2.3684289 -2.3761854 -2.3164093 -2.1385119 -2.3174219][-1.8490138 -1.9712753 -1.9859293 -1.8269761 -1.7135217 -1.1418993 -0.14276266 -0.18644142 -1.0394607 -1.5790119 -1.751909 -1.903105 -1.944217 -1.8177655 -2.0117655][-2.4233131 -2.6271315 -2.6794472 -2.5645342 -2.5065577 -2.0239432 -1.2716908 -1.4530246 -2.1561801 -2.4566851 -2.4756539 -2.5124216 -2.4009778 -2.1069803 -2.1439533][-2.7543736 -3.1184866 -3.2613778 -3.1911283 -3.167439 -2.8302817 -2.3522542 -2.6468399 -3.2191677 -3.3842182 -3.3574991 -3.2645292 -2.9556718 -2.4624043 -2.2871647][-2.8247433 -3.3478444 -3.5858874 -3.5975447 -3.6023691 -3.3553309 -3.0332618 -3.3517513 -3.827275 -3.9317398 -3.8678875 -3.6231644 -3.1304061 -2.4924865 -2.1308959][-2.4333425 -2.9518704 -3.2015591 -3.3092079 -3.4091444 -3.3116322 -3.1543305 -3.4533913 -3.8195958 -3.8729632 -3.7784095 -3.4917617 -2.9731712 -2.3643122 -1.968261][-2.1824806 -2.5871625 -2.7822568 -2.9252894 -3.0781736 -3.057797 -2.9547799 -3.1504951 -3.3795035 -3.3831553 -3.2607508 -3.0160065 -2.6049447 -2.1525536 -1.8987679][-2.5331118 -2.8020864 -2.930418 -3.0527244 -3.173306 -3.1537437 -3.0505831 -3.1443782 -3.2821414 -3.2604318 -3.1276619 -2.9209738 -2.6287858 -2.3302336 -2.2313957]]...]
INFO - root - 2017-12-07 09:27:07.655247: step 24410, loss = 0.59, batch loss = 0.52 (7.3 examples/sec; 1.094 sec/batch; 93h:36m:14s remains)
INFO - root - 2017-12-07 09:27:18.454745: step 24420, loss = 0.92, batch loss = 0.85 (7.3 examples/sec; 1.099 sec/batch; 94h:02m:44s remains)
INFO - root - 2017-12-07 09:27:29.236269: step 24430, loss = 0.92, batch loss = 0.84 (7.5 examples/sec; 1.066 sec/batch; 91h:12m:29s remains)
INFO - root - 2017-12-07 09:27:39.692505: step 24440, loss = 0.90, batch loss = 0.83 (7.6 examples/sec; 1.046 sec/batch; 89h:30m:45s remains)
INFO - root - 2017-12-07 09:27:50.352882: step 24450, loss = 0.80, batch loss = 0.73 (7.2 examples/sec; 1.114 sec/batch; 95h:18m:24s remains)
INFO - root - 2017-12-07 09:28:01.120787: step 24460, loss = 0.80, batch loss = 0.73 (7.4 examples/sec; 1.079 sec/batch; 92h:17m:14s remains)
INFO - root - 2017-12-07 09:28:11.858456: step 24470, loss = 0.83, batch loss = 0.76 (7.5 examples/sec; 1.060 sec/batch; 90h:41m:11s remains)
INFO - root - 2017-12-07 09:28:22.555635: step 24480, loss = 0.88, batch loss = 0.80 (7.4 examples/sec; 1.076 sec/batch; 92h:02m:59s remains)
INFO - root - 2017-12-07 09:28:33.240862: step 24490, loss = 0.81, batch loss = 0.74 (7.6 examples/sec; 1.055 sec/batch; 90h:13m:34s remains)
INFO - root - 2017-12-07 09:28:43.904719: step 24500, loss = 0.98, batch loss = 0.91 (7.6 examples/sec; 1.053 sec/batch; 90h:03m:00s remains)
2017-12-07 09:28:44.772862: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2343714 -1.8399522 -1.3751252 -1.0908098 -0.95957303 -0.93378568 -0.85242891 -0.74634433 -0.72037387 -0.76226425 -0.94557 -1.0936387 -0.91984296 -0.53438258 -0.50821018][-2.133698 -1.7247844 -1.3651786 -1.1606665 -1.0251248 -1.0043235 -1.0419207 -1.0499582 -1.0086515 -0.94081807 -1.0580709 -1.225986 -1.0807014 -0.64865851 -0.46785545][-2.1245778 -1.8252184 -1.7103689 -1.7465749 -1.7738552 -1.8872912 -2.13435 -2.3674722 -2.4724774 -2.4493656 -2.5659866 -2.70922 -2.5613475 -2.1353488 -1.8383753][-2.51811 -2.2922204 -2.311094 -2.5302453 -2.7221179 -2.9148903 -3.226685 -3.5116985 -3.6307461 -3.5728545 -3.6338406 -3.7995689 -3.7902572 -3.5332375 -3.1976435][-2.9050734 -2.6741023 -2.8035598 -3.2261696 -3.5939145 -3.7758503 -3.9888837 -4.2302947 -4.2837677 -4.0434771 -3.9451604 -4.2369113 -4.6540604 -4.8744583 -4.7314987][-2.7015243 -2.2052739 -2.1715069 -2.5352657 -2.8841076 -2.9307356 -2.9413671 -3.0939851 -3.1557817 -2.8694277 -2.7205191 -3.2544138 -4.2424822 -5.0939174 -5.3318939][-2.3082385 -1.4201691 -1.0002439 -1.0346775 -1.1745782 -1.0163753 -0.82986331 -0.9037528 -0.95008278 -0.64700866 -0.52429509 -1.307029 -2.8081653 -4.27794 -5.0026979][-2.125551 -0.87796855 0.0087490082 0.37364817 0.3792305 0.44053316 0.44980574 0.21494007 0.13486958 0.52877903 0.8079958 0.00067472458 -1.7034056 -3.4697804 -4.5032411][-2.4300613 -1.0131805 0.26527166 1.0106139 1.1472745 0.99748278 0.59083891 -0.10329008 -0.44285083 0.036171913 0.67127705 0.17403603 -1.3245304 -3.0652723 -4.2319889][-3.1006408 -1.8844817 -0.6950047 0.05453968 0.18840885 -0.084114075 -0.69838595 -1.5804853 -1.9497099 -1.2493777 -0.12338209 -0.17093372 -1.3645017 -3.0784152 -4.4201045][-3.8231404 -2.9448757 -2.1019793 -1.6522727 -1.7165637 -2.154757 -2.8453407 -3.5490859 -3.541337 -2.3693192 -0.72047877 -0.34594774 -1.3316803 -3.0525656 -4.580555][-4.2062173 -3.6400068 -3.044126 -2.7523689 -2.8836124 -3.3500452 -3.9972878 -4.5112104 -4.3070035 -3.0258083 -1.3016915 -0.70892286 -1.4584634 -2.9593043 -4.3967667][-4.4866357 -4.2629671 -3.9122784 -3.7094781 -3.7965083 -4.1097436 -4.5236487 -4.7624478 -4.4371982 -3.305912 -1.7994978 -1.0738032 -1.4822013 -2.6506014 -3.9205344][-4.294549 -4.25541 -4.0757823 -3.962992 -4.050025 -4.3196464 -4.6319566 -4.7511749 -4.4463854 -3.6197155 -2.484499 -1.7114999 -1.7357676 -2.5073962 -3.5681143][-3.8372455 -3.7876368 -3.686625 -3.6696227 -3.8033535 -4.031323 -4.2578158 -4.3431239 -4.1637254 -3.7174544 -3.0710206 -2.5315161 -2.419652 -2.8938022 -3.6843033]]...]
INFO - root - 2017-12-07 09:28:55.332285: step 24510, loss = 0.79, batch loss = 0.71 (7.5 examples/sec; 1.063 sec/batch; 90h:57m:08s remains)
INFO - root - 2017-12-07 09:29:06.002534: step 24520, loss = 0.82, batch loss = 0.75 (7.6 examples/sec; 1.057 sec/batch; 90h:24m:06s remains)
INFO - root - 2017-12-07 09:29:16.802537: step 24530, loss = 0.89, batch loss = 0.82 (7.5 examples/sec; 1.064 sec/batch; 90h:59m:56s remains)
INFO - root - 2017-12-07 09:29:27.423906: step 24540, loss = 0.66, batch loss = 0.59 (7.4 examples/sec; 1.087 sec/batch; 93h:01m:30s remains)
INFO - root - 2017-12-07 09:29:38.260852: step 24550, loss = 0.85, batch loss = 0.78 (7.5 examples/sec; 1.069 sec/batch; 91h:25m:54s remains)
INFO - root - 2017-12-07 09:29:49.017722: step 24560, loss = 0.67, batch loss = 0.60 (7.3 examples/sec; 1.102 sec/batch; 94h:13m:56s remains)
INFO - root - 2017-12-07 09:29:59.812840: step 24570, loss = 0.83, batch loss = 0.75 (7.4 examples/sec; 1.075 sec/batch; 91h:58m:02s remains)
INFO - root - 2017-12-07 09:30:10.559032: step 24580, loss = 0.67, batch loss = 0.60 (7.4 examples/sec; 1.085 sec/batch; 92h:50m:33s remains)
INFO - root - 2017-12-07 09:30:21.324181: step 24590, loss = 0.96, batch loss = 0.89 (7.7 examples/sec; 1.042 sec/batch; 89h:05m:07s remains)
INFO - root - 2017-12-07 09:30:32.075518: step 24600, loss = 0.84, batch loss = 0.77 (7.5 examples/sec; 1.071 sec/batch; 91h:37m:19s remains)
2017-12-07 09:30:32.972994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.07603 -3.2387886 -3.3141956 -3.3952947 -3.6195593 -3.8737967 -4.0084147 -4.144722 -4.3023252 -4.302074 -4.0814829 -3.7100554 -3.306812 -3.0558431 -3.0201898][-3.1064208 -3.2806239 -3.2263093 -3.1602874 -3.4234505 -3.819263 -4.0732226 -4.4152746 -4.8247366 -4.9718328 -4.789794 -4.3594871 -3.771966 -3.2841425 -3.0716724][-3.1368442 -3.1925447 -2.832073 -2.4906764 -2.7224512 -3.1553307 -3.3738828 -3.8084273 -4.4469657 -4.8301086 -4.9098253 -4.7314262 -4.2546048 -3.6605792 -3.2246714][-3.0515423 -2.8219852 -2.0848048 -1.5684125 -1.9249785 -2.4477353 -2.55676 -2.9186411 -3.555367 -3.9793181 -4.2269392 -4.3344173 -4.1307073 -3.6368451 -3.1712813][-2.82698 -2.2632837 -1.2494156 -0.82011294 -1.4943287 -2.1055195 -2.0526919 -2.2261903 -2.7200012 -3.0109713 -3.1923788 -3.4016991 -3.4928954 -3.30156 -3.1448922][-2.5986657 -1.8202643 -0.7915175 -0.68438339 -1.560843 -1.9731069 -1.5496516 -1.4365609 -1.8502972 -2.0729434 -2.1135569 -2.255115 -2.5511184 -2.7042055 -3.0013976][-2.4312947 -1.6074293 -0.82647777 -1.0456331 -1.8045306 -1.6944237 -0.80325174 -0.47673392 -0.95274258 -1.2566273 -1.1820879 -1.2243207 -1.7092562 -2.1024232 -2.5744231][-2.2403057 -1.4394708 -0.96106219 -1.37078 -1.8661911 -1.3147793 -0.19955635 0.11669493 -0.45852613 -0.78706026 -0.55119467 -0.51264954 -1.1981213 -1.7442338 -2.1329823][-2.0542617 -1.3289523 -1.0996518 -1.5454006 -1.8379602 -1.2262146 -0.32311583 -0.20212889 -0.73056364 -0.90179205 -0.44793105 -0.33612871 -1.0546963 -1.5732224 -1.8039167][-2.0448482 -1.484755 -1.4279606 -1.7599087 -1.9031193 -1.4896243 -1.0050299 -1.0706441 -1.4411979 -1.4468224 -0.930002 -0.85038495 -1.4524193 -1.7188098 -1.7016737][-2.1985917 -1.8086123 -1.7960935 -1.9082336 -1.9169798 -1.718713 -1.5665803 -1.7232468 -1.9772966 -1.9207351 -1.519882 -1.5806711 -2.0684297 -2.0990252 -1.8994346][-2.3788702 -2.0349212 -1.9015183 -1.7843792 -1.7230959 -1.6952825 -1.7090256 -1.8836129 -2.0796497 -2.0123603 -1.7158244 -1.8389568 -2.1635544 -2.0843322 -1.8964851][-2.5429471 -2.1550817 -1.8227024 -1.5391517 -1.4837809 -1.5528319 -1.6040442 -1.7477038 -1.9115307 -1.8144515 -1.5393698 -1.6040828 -1.744169 -1.6560876 -1.6094699][-2.6742258 -2.2722 -1.840956 -1.5614035 -1.5934544 -1.6838193 -1.6791482 -1.7726839 -1.9439807 -1.8610377 -1.633575 -1.647718 -1.6848967 -1.6533062 -1.6877401][-2.6914132 -2.3172252 -1.9220045 -1.7836185 -1.9441247 -2.032702 -1.967514 -2.0336235 -2.2279577 -2.2097282 -2.0826926 -2.1210189 -2.1546092 -2.1571805 -2.1431193]]...]
INFO - root - 2017-12-07 09:30:43.466717: step 24610, loss = 0.89, batch loss = 0.82 (7.6 examples/sec; 1.056 sec/batch; 90h:19m:14s remains)
INFO - root - 2017-12-07 09:30:54.219398: step 24620, loss = 0.81, batch loss = 0.73 (7.4 examples/sec; 1.077 sec/batch; 92h:05m:24s remains)
INFO - root - 2017-12-07 09:31:04.786532: step 24630, loss = 0.60, batch loss = 0.53 (7.6 examples/sec; 1.058 sec/batch; 90h:27m:18s remains)
INFO - root - 2017-12-07 09:31:15.191606: step 24640, loss = 0.85, batch loss = 0.78 (7.3 examples/sec; 1.093 sec/batch; 93h:26m:33s remains)
INFO - root - 2017-12-07 09:31:25.911491: step 24650, loss = 0.93, batch loss = 0.86 (7.4 examples/sec; 1.078 sec/batch; 92h:08m:51s remains)
INFO - root - 2017-12-07 09:31:36.728534: step 24660, loss = 0.73, batch loss = 0.65 (7.5 examples/sec; 1.073 sec/batch; 91h:47m:34s remains)
INFO - root - 2017-12-07 09:31:47.553261: step 24670, loss = 0.73, batch loss = 0.65 (7.2 examples/sec; 1.106 sec/batch; 94h:36m:09s remains)
INFO - root - 2017-12-07 09:31:58.268088: step 24680, loss = 0.94, batch loss = 0.87 (7.4 examples/sec; 1.075 sec/batch; 91h:55m:46s remains)
INFO - root - 2017-12-07 09:32:08.987507: step 24690, loss = 0.61, batch loss = 0.54 (7.3 examples/sec; 1.097 sec/batch; 93h:48m:45s remains)
INFO - root - 2017-12-07 09:32:19.774110: step 24700, loss = 0.76, batch loss = 0.69 (7.3 examples/sec; 1.099 sec/batch; 93h:56m:32s remains)
2017-12-07 09:32:20.603227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3925605 -4.4390674 -4.4985428 -4.5525203 -4.5899034 -4.6338692 -4.6742668 -4.7103925 -4.7415824 -4.7566628 -4.7575231 -4.7541137 -4.7302742 -4.6957822 -4.6746492][-4.3436227 -4.394506 -4.4650846 -4.5373631 -4.594151 -4.6552553 -4.7052169 -4.7568426 -4.8045745 -4.8165989 -4.8059688 -4.8023276 -4.777657 -4.7346182 -4.6975684][-4.1949191 -4.2437348 -4.324492 -4.4162583 -4.4928222 -4.5625591 -4.6000171 -4.6238489 -4.6361041 -4.6143546 -4.5992103 -4.6148739 -4.6175494 -4.5972481 -4.572588][-3.9257758 -3.959415 -4.0386682 -4.1359029 -4.20838 -4.2461295 -4.2045078 -4.1038461 -3.9954572 -3.907882 -3.8997095 -3.9519291 -3.996928 -4.0093093 -3.9889677][-3.5565836 -3.5476441 -3.5783024 -3.6184397 -3.6243639 -3.5913591 -3.4457777 -3.1867537 -2.9384885 -2.8128638 -2.8448548 -2.9475141 -3.0398059 -3.0939956 -3.0854487][-3.0195384 -2.9171946 -2.8070962 -2.6723447 -2.5130258 -2.364114 -2.1347175 -1.7902019 -1.547363 -1.5467284 -1.7402146 -1.9686909 -2.114902 -2.154773 -2.0575359][-2.4706783 -2.243597 -1.9330235 -1.5625687 -1.216903 -0.99021792 -0.78639293 -0.5440743 -0.50165272 -0.73331308 -1.1199398 -1.4442112 -1.5824323 -1.5208316 -1.2869146][-2.2603421 -1.9724815 -1.5815575 -1.143512 -0.80079985 -0.66982317 -0.63353038 -0.62971592 -0.82433653 -1.1859944 -1.6381013 -1.9949534 -2.1216297 -2.018492 -1.7897704][-2.6271648 -2.4285588 -2.1681705 -1.8725004 -1.670742 -1.6508868 -1.6951015 -1.7808826 -1.9894257 -2.2856355 -2.6667109 -3.0044122 -3.140481 -3.0583968 -2.9153941][-3.3252017 -3.3100233 -3.2621381 -3.1548049 -3.0805552 -3.0744083 -3.0769813 -3.1260481 -3.2562819 -3.4500251 -3.7166154 -3.9983997 -4.1435604 -4.1187496 -4.0479727][-3.8806949 -4.05815 -4.2156849 -4.287725 -4.3322082 -4.3294373 -4.2954206 -4.2918348 -4.3399682 -4.4293838 -4.5415092 -4.6986051 -4.8188491 -4.8645158 -4.8729963][-4.1049976 -4.3932862 -4.6443739 -4.8051496 -4.911272 -4.9221935 -4.8759789 -4.8295836 -4.829268 -4.8700328 -4.8947062 -4.9295473 -4.9566851 -4.9628544 -4.9525924][-4.0542293 -4.3428311 -4.5822387 -4.7704105 -4.9234748 -5.008523 -5.0260773 -5.006834 -5.0073833 -5.0339217 -5.0164003 -4.9444542 -4.8490362 -4.7470584 -4.6467085][-3.8716285 -4.0965662 -4.2940335 -4.4910431 -4.6797619 -4.8240809 -4.897017 -4.9077768 -4.8974829 -4.8823524 -4.8026738 -4.6618319 -4.5052962 -4.3580837 -4.2138524][-3.685442 -3.8164361 -3.9326675 -4.0577674 -4.1780872 -4.270853 -4.3227868 -4.3334374 -4.3265986 -4.3060446 -4.23165 -4.1143494 -4.0010195 -3.910295 -3.8174813]]...]
INFO - root - 2017-12-07 09:32:31.406116: step 24710, loss = 0.82, batch loss = 0.75 (7.8 examples/sec; 1.026 sec/batch; 87h:44m:51s remains)
INFO - root - 2017-12-07 09:32:42.100342: step 24720, loss = 0.75, batch loss = 0.68 (7.4 examples/sec; 1.077 sec/batch; 92h:05m:39s remains)
INFO - root - 2017-12-07 09:32:52.935180: step 24730, loss = 0.64, batch loss = 0.56 (7.1 examples/sec; 1.120 sec/batch; 95h:43m:45s remains)
INFO - root - 2017-12-07 09:33:03.471304: step 24740, loss = 0.78, batch loss = 0.71 (7.5 examples/sec; 1.063 sec/batch; 90h:53m:58s remains)
INFO - root - 2017-12-07 09:33:14.086987: step 24750, loss = 0.70, batch loss = 0.63 (7.6 examples/sec; 1.049 sec/batch; 89h:41m:48s remains)
INFO - root - 2017-12-07 09:33:24.687009: step 24760, loss = 1.24, batch loss = 1.16 (7.5 examples/sec; 1.067 sec/batch; 91h:13m:13s remains)
INFO - root - 2017-12-07 09:33:35.249789: step 24770, loss = 0.79, batch loss = 0.72 (7.6 examples/sec; 1.052 sec/batch; 89h:54m:59s remains)
INFO - root - 2017-12-07 09:33:45.994579: step 24780, loss = 0.78, batch loss = 0.71 (7.3 examples/sec; 1.098 sec/batch; 93h:52m:01s remains)
INFO - root - 2017-12-07 09:33:56.619264: step 24790, loss = 0.88, batch loss = 0.81 (7.4 examples/sec; 1.075 sec/batch; 91h:53m:06s remains)
INFO - root - 2017-12-07 09:34:07.302265: step 24800, loss = 0.81, batch loss = 0.73 (7.3 examples/sec; 1.095 sec/batch; 93h:36m:46s remains)
2017-12-07 09:34:08.219292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3890803 -2.0478172 -1.7544093 -1.6948938 -1.9795012 -2.39923 -2.6512129 -2.8023796 -2.8399236 -2.7293887 -2.6274095 -2.5583434 -2.4764066 -2.5409369 -2.5618687][-2.1786056 -1.85093 -1.6357441 -1.696609 -2.06279 -2.4561059 -2.6397786 -2.764761 -2.8283224 -2.7860758 -2.692198 -2.5822806 -2.4784498 -2.54944 -2.6749597][-2.0996084 -1.8973882 -1.8498318 -2.0060463 -2.33783 -2.6041431 -2.655158 -2.6676896 -2.7174449 -2.7603092 -2.7606254 -2.6910472 -2.6044431 -2.6988311 -2.8930857][-2.0136931 -1.9877713 -2.1246307 -2.300194 -2.4924736 -2.5691895 -2.4794083 -2.4241679 -2.5512555 -2.7756462 -2.9176404 -2.86204 -2.6839437 -2.7192788 -2.8776436][-1.8318737 -1.9416227 -2.2539196 -2.4013178 -2.44866 -2.3408611 -2.1677639 -2.2018471 -2.5092657 -2.9124608 -3.1148632 -2.958005 -2.6356916 -2.5723495 -2.6643634][-1.7202861 -1.9206939 -2.2828743 -2.2730019 -2.1293876 -1.908114 -1.7928779 -2.0513196 -2.523293 -2.974005 -3.1574471 -2.9321809 -2.6181483 -2.5712767 -2.6772087][-1.852026 -2.1006417 -2.3707707 -2.1278605 -1.8238635 -1.6069608 -1.6420531 -2.0840182 -2.5895634 -2.9235213 -2.9794183 -2.7244468 -2.5689034 -2.710264 -2.942049][-2.0984156 -2.3304207 -2.4715166 -2.122988 -1.8218861 -1.7052982 -1.8400974 -2.3000569 -2.7032404 -2.830106 -2.7108526 -2.4920526 -2.5587349 -2.937192 -3.2831278][-2.3804915 -2.5500107 -2.6146545 -2.3036835 -2.0790687 -2.0293286 -2.1675391 -2.5410421 -2.7912529 -2.7256589 -2.519177 -2.4673662 -2.7510824 -3.2117753 -3.5304425][-2.6474938 -2.7627597 -2.8141775 -2.6289935 -2.5012391 -2.4832056 -2.5492291 -2.7104807 -2.7645581 -2.6018703 -2.4780817 -2.6621594 -3.0187593 -3.3046536 -3.512229][-2.9043043 -2.9564886 -3.0062079 -2.9166789 -2.8377869 -2.858562 -2.8496263 -2.7631574 -2.6845152 -2.5471182 -2.5633497 -2.889986 -3.1818149 -3.2462459 -3.4081511][-3.0240362 -2.958612 -2.9866724 -2.9277387 -2.9032297 -3.0059929 -2.99304 -2.8324094 -2.7443035 -2.6244345 -2.6532636 -2.9864569 -3.2453003 -3.2828989 -3.5078006][-2.9715261 -2.8737888 -2.8745255 -2.7640123 -2.7779744 -3.0191028 -3.1230531 -3.0630572 -2.9837885 -2.6860344 -2.4790726 -2.7458091 -3.111805 -3.3403509 -3.6466072][-2.8980699 -2.9141932 -2.88593 -2.6571531 -2.65514 -2.9896133 -3.2092795 -3.2640553 -3.13117 -2.5888963 -2.152087 -2.4127634 -2.9246264 -3.2744331 -3.5048163][-2.9485493 -3.079061 -3.016392 -2.6735721 -2.6029243 -2.9360991 -3.2018566 -3.2820897 -3.0557685 -2.384773 -1.9475949 -2.2863252 -2.8887458 -3.1875937 -3.2436481]]...]
INFO - root - 2017-12-07 09:34:18.882785: step 24810, loss = 0.71, batch loss = 0.64 (7.4 examples/sec; 1.075 sec/batch; 91h:51m:41s remains)
INFO - root - 2017-12-07 09:34:29.488855: step 24820, loss = 0.73, batch loss = 0.66 (7.6 examples/sec; 1.053 sec/batch; 89h:57m:34s remains)
INFO - root - 2017-12-07 09:34:40.107480: step 24830, loss = 0.65, batch loss = 0.58 (7.2 examples/sec; 1.109 sec/batch; 94h:45m:52s remains)
INFO - root - 2017-12-07 09:34:50.692359: step 24840, loss = 0.59, batch loss = 0.52 (7.5 examples/sec; 1.061 sec/batch; 90h:39m:06s remains)
INFO - root - 2017-12-07 09:35:01.401182: step 24850, loss = 0.82, batch loss = 0.74 (7.6 examples/sec; 1.050 sec/batch; 89h:44m:59s remains)
INFO - root - 2017-12-07 09:35:12.124430: step 24860, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.063 sec/batch; 90h:52m:34s remains)
INFO - root - 2017-12-07 09:35:22.805658: step 24870, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.076 sec/batch; 91h:54m:39s remains)
INFO - root - 2017-12-07 09:35:33.430713: step 24880, loss = 0.74, batch loss = 0.67 (7.5 examples/sec; 1.061 sec/batch; 90h:41m:51s remains)
INFO - root - 2017-12-07 09:35:44.216268: step 24890, loss = 0.78, batch loss = 0.71 (7.5 examples/sec; 1.070 sec/batch; 91h:23m:44s remains)
INFO - root - 2017-12-07 09:35:54.925363: step 24900, loss = 0.77, batch loss = 0.70 (7.7 examples/sec; 1.037 sec/batch; 88h:37m:04s remains)
2017-12-07 09:35:55.751442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0109694 -1.3380799 -1.2518053 -0.95770884 -0.80061865 -0.86796308 -1.076864 -1.2905004 -1.6613734 -1.991133 -1.9119046 -1.4959664 -0.93713927 -0.41902351 -0.1173296][-0.80081964 -1.1235778 -1.1605012 -1.0675452 -1.082449 -1.2088432 -1.3415301 -1.3888488 -1.5074236 -1.5654457 -1.3484664 -0.9255662 -0.45044804 -0.053541183 0.093928337][-0.51433182 -0.87202907 -1.1493738 -1.3611355 -1.5479949 -1.6304133 -1.5540416 -1.3689055 -1.2785115 -1.1901183 -1.0016356 -0.741384 -0.46494055 -0.2381916 -0.20191193][-0.32339287 -0.74986672 -1.3084431 -1.8244157 -2.0498822 -1.8854475 -1.4087489 -0.871681 -0.62579274 -0.60099912 -0.65263057 -0.67146063 -0.59761047 -0.51410556 -0.50775409][-0.33498192 -0.83364868 -1.633745 -2.3882463 -2.5650017 -2.0675209 -1.080296 -0.066929817 0.29308605 0.075098991 -0.32179451 -0.58139968 -0.58350253 -0.49527597 -0.41057158][-0.26481581 -0.74602151 -1.6127713 -2.4072883 -2.4441323 -1.6151891 -0.15327549 1.2605457 1.5231352 0.74187469 -0.15817595 -0.5696063 -0.43175888 -0.077112675 0.22558451][-0.099575043 -0.54274869 -1.3502293 -2.0094709 -1.8333421 -0.72692323 1.0282106 2.6167617 2.6758533 1.3610544 0.020232677 -0.50352168 -0.19041014 0.50324965 1.0392284][-0.13824606 -0.57997441 -1.3010855 -1.7764385 -1.4864156 -0.44086576 1.0604997 2.26411 2.1028914 0.78981733 -0.40861177 -0.74310613 -0.28479004 0.55475283 1.1124802][-0.38949919 -0.83969855 -1.4023855 -1.6318214 -1.2748404 -0.4467237 0.52247858 1.0569234 0.63767481 -0.34689283 -0.99815655 -0.93255448 -0.40295315 0.29103518 0.66890621][-0.55896187 -1.0006757 -1.3993015 -1.453737 -1.131299 -0.60508418 -0.15544844 -0.15098047 -0.68848991 -1.2570515 -1.3558435 -0.970371 -0.488796 -0.095974922 0.042895317][-0.51415348 -0.84603882 -1.052634 -0.95771217 -0.70140696 -0.48248458 -0.44896436 -0.72265863 -1.1878538 -1.4174652 -1.205513 -0.75163174 -0.44217181 -0.36790419 -0.44373393][-0.39963484 -0.5690372 -0.57393265 -0.34767675 -0.076355457 0.028048038 -0.09869957 -0.38624477 -0.66869783 -0.69769168 -0.45351553 -0.17882824 -0.12687159 -0.28981543 -0.48703289][-0.38839722 -0.46128345 -0.38923693 -0.16436481 0.06984663 0.15139818 0.051775455 -0.096473217 -0.17863035 -0.12181711 0.031664848 0.091471672 -0.028306484 -0.26024294 -0.43935823][-0.46735406 -0.51743889 -0.45678377 -0.271729 -0.044380665 0.09805584 0.10698223 0.083076954 0.093215466 0.14621162 0.20406961 0.14605284 -0.0221982 -0.22702456 -0.3711195][-0.32834053 -0.4416306 -0.46173024 -0.34138346 -0.14808893 0.0042214394 0.041390896 0.023265839 0.013655186 0.023361206 0.031990051 -0.026453972 -0.14777946 -0.2907362 -0.41303921]]...]
INFO - root - 2017-12-07 09:36:06.460218: step 24910, loss = 0.92, batch loss = 0.85 (7.6 examples/sec; 1.052 sec/batch; 89h:54m:28s remains)
INFO - root - 2017-12-07 09:36:17.178908: step 24920, loss = 1.00, batch loss = 0.93 (7.4 examples/sec; 1.079 sec/batch; 92h:13m:31s remains)
INFO - root - 2017-12-07 09:36:27.880229: step 24930, loss = 0.92, batch loss = 0.85 (7.3 examples/sec; 1.098 sec/batch; 93h:49m:01s remains)
INFO - root - 2017-12-07 09:36:38.423585: step 24940, loss = 0.86, batch loss = 0.79 (7.4 examples/sec; 1.077 sec/batch; 92h:02m:25s remains)
INFO - root - 2017-12-07 09:36:49.051990: step 24950, loss = 0.85, batch loss = 0.78 (7.4 examples/sec; 1.079 sec/batch; 92h:09m:26s remains)
INFO - root - 2017-12-07 09:36:59.753601: step 24960, loss = 0.69, batch loss = 0.62 (7.3 examples/sec; 1.088 sec/batch; 92h:59m:16s remains)
INFO - root - 2017-12-07 09:37:10.598135: step 24970, loss = 0.85, batch loss = 0.77 (7.3 examples/sec; 1.102 sec/batch; 94h:09m:30s remains)
INFO - root - 2017-12-07 09:37:21.353034: step 24980, loss = 0.70, batch loss = 0.63 (7.3 examples/sec; 1.091 sec/batch; 93h:12m:01s remains)
INFO - root - 2017-12-07 09:37:32.247628: step 24990, loss = 0.80, batch loss = 0.73 (7.5 examples/sec; 1.060 sec/batch; 90h:33m:50s remains)
INFO - root - 2017-12-07 09:37:43.096130: step 25000, loss = 0.92, batch loss = 0.84 (7.5 examples/sec; 1.069 sec/batch; 91h:17m:42s remains)
2017-12-07 09:37:43.937835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8281617 -1.0881457 -0.62801433 -0.60316133 -1.1908486 -2.3035991 -3.1240742 -3.5101795 -3.5408173 -3.1514015 -2.4638987 -1.9090197 -1.7535114 -2.0409148 -2.7921143][-1.8968909 -1.0636985 -0.55318189 -0.486624 -1.0779479 -2.1804354 -2.894228 -3.0798013 -2.9395258 -2.4030056 -1.5643933 -1.0089524 -1.0074356 -1.4930766 -2.400645][-2.5423996 -1.7594481 -1.245014 -1.0912619 -1.5168478 -2.4260409 -2.920054 -2.8100331 -2.4587834 -1.8245227 -0.90511942 -0.38000011 -0.52407479 -1.1451242 -2.1536427][-2.6444683 -1.9728942 -1.5314593 -1.3981397 -1.6956742 -2.4393883 -2.8490007 -2.663919 -2.3800213 -1.9348347 -1.152689 -0.70264769 -0.87819815 -1.48651 -2.425514][-2.09545 -1.6397755 -1.4454694 -1.4278846 -1.5862732 -2.1128817 -2.4683671 -2.3763826 -2.3467619 -2.2562666 -1.7210081 -1.3433585 -1.5224192 -2.0870249 -2.9304705][-1.5861077 -1.5032725 -1.5717957 -1.5385289 -1.4489503 -1.6364186 -1.7610188 -1.5896988 -1.6353734 -1.8227048 -1.599762 -1.4491541 -1.7813148 -2.434104 -3.2893748][-1.0845599 -1.4315555 -1.6552238 -1.4892991 -1.0989397 -0.88102078 -0.5804739 -0.038383484 0.081318855 -0.34472656 -0.62923217 -1.0203292 -1.7658069 -2.6795201 -3.5913343][-1.030072 -1.6790481 -1.8793502 -1.5175228 -0.863116 -0.25384712 0.50164461 1.4019852 1.6201901 0.85065365 0.05559063 -0.75724673 -1.8028302 -2.89009 -3.7801523][-1.5461109 -2.2251453 -2.2538028 -1.8093359 -1.1567843 -0.45679116 0.42090416 1.2799573 1.2616258 0.40146875 -0.39013338 -1.1053481 -2.0482845 -3.0592234 -3.8041115][-1.8770058 -2.4397893 -2.3992782 -2.0148849 -1.5321009 -0.98297095 -0.33481169 0.15385199 -0.073468685 -0.755281 -1.2824779 -1.7749431 -2.5481043 -3.372798 -3.8920791][-1.767519 -2.2020528 -2.1484158 -1.7660623 -1.377028 -0.99016142 -0.64148712 -0.48870873 -0.77254581 -1.2748415 -1.6871786 -2.172199 -2.9108262 -3.5863855 -3.9541581][-1.3574748 -1.6148865 -1.5182457 -1.0907545 -0.76241565 -0.50302672 -0.35385895 -0.39829397 -0.64690995 -1.0446789 -1.518759 -2.1511502 -2.9360957 -3.5373135 -3.8837657][-1.2837961 -1.3847175 -1.1728592 -0.70945239 -0.45204544 -0.28881931 -0.27452564 -0.42912149 -0.64972138 -1.0200841 -1.5764935 -2.3036015 -3.0672932 -3.5815358 -3.8746731][-1.7873068 -1.7678063 -1.4687707 -1.071177 -0.9427793 -0.93279004 -1.0605941 -1.2729118 -1.4530571 -1.7579186 -2.21445 -2.8128028 -3.3805127 -3.7158408 -3.8860674][-2.6128538 -2.5166917 -2.24193 -1.9963524 -1.956239 -2.0400696 -2.2103968 -2.3597982 -2.4676452 -2.6927395 -2.9714394 -3.3189464 -3.6429782 -3.7977457 -3.8413179]]...]
INFO - root - 2017-12-07 09:37:54.605442: step 25010, loss = 0.77, batch loss = 0.70 (7.4 examples/sec; 1.082 sec/batch; 92h:26m:29s remains)
INFO - root - 2017-12-07 09:38:05.230607: step 25020, loss = 0.82, batch loss = 0.75 (7.5 examples/sec; 1.067 sec/batch; 91h:06m:25s remains)
INFO - root - 2017-12-07 09:38:15.904662: step 25030, loss = 1.00, batch loss = 0.93 (7.6 examples/sec; 1.050 sec/batch; 89h:39m:14s remains)
INFO - root - 2017-12-07 09:38:26.488362: step 25040, loss = 0.91, batch loss = 0.84 (7.5 examples/sec; 1.060 sec/batch; 90h:32m:28s remains)
INFO - root - 2017-12-07 09:38:37.179452: step 25050, loss = 0.85, batch loss = 0.77 (7.4 examples/sec; 1.082 sec/batch; 92h:23m:01s remains)
INFO - root - 2017-12-07 09:38:47.882925: step 25060, loss = 0.75, batch loss = 0.68 (7.5 examples/sec; 1.064 sec/batch; 90h:53m:42s remains)
INFO - root - 2017-12-07 09:38:58.646490: step 25070, loss = 0.61, batch loss = 0.53 (7.5 examples/sec; 1.063 sec/batch; 90h:44m:11s remains)
INFO - root - 2017-12-07 09:39:09.428072: step 25080, loss = 0.66, batch loss = 0.59 (7.2 examples/sec; 1.111 sec/batch; 94h:53m:59s remains)
INFO - root - 2017-12-07 09:39:20.233490: step 25090, loss = 0.67, batch loss = 0.60 (7.5 examples/sec; 1.072 sec/batch; 91h:32m:36s remains)
INFO - root - 2017-12-07 09:39:30.861369: step 25100, loss = 0.98, batch loss = 0.91 (7.5 examples/sec; 1.064 sec/batch; 90h:48m:46s remains)
2017-12-07 09:39:31.710810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0625324 -3.009388 -2.9430871 -2.8610582 -2.8300648 -2.8466859 -2.8533249 -2.8479605 -2.8781586 -2.9588857 -3.048856 -3.055398 -3.0514519 -3.0859075 -3.1312041][-3.2508268 -3.1721158 -3.0628369 -2.9142747 -2.8184493 -2.8073931 -2.791615 -2.8153214 -2.8647504 -2.961235 -3.125268 -3.1765237 -3.130362 -3.0446067 -2.9844544][-3.6898 -3.6003752 -3.4906492 -3.3435602 -3.2807682 -3.3318133 -3.3702052 -3.4401882 -3.4005861 -3.3333716 -3.3992064 -3.4271555 -3.3223505 -3.091502 -2.941051][-3.9200571 -3.8001933 -3.6545405 -3.4723816 -3.4140224 -3.551254 -3.7684388 -4.0618653 -4.1336203 -4.0354223 -3.9417267 -3.7669332 -3.4850738 -3.0872989 -2.889708][-3.6202698 -3.5971465 -3.5910916 -3.4781382 -3.3246913 -3.2566047 -3.3071446 -3.6148138 -3.876883 -4.0732622 -4.1779704 -3.9721663 -3.5763643 -3.0176802 -2.7089877][-2.7954378 -2.8555317 -2.9750433 -2.9161654 -2.6593037 -2.3822398 -2.2061062 -2.4294395 -2.9415302 -3.648051 -4.1928115 -4.0895605 -3.5659769 -2.8248677 -2.3910162][-2.2095838 -2.3058567 -2.3774197 -2.1656325 -1.6265688 -0.95081663 -0.33627605 -0.41004562 -1.3158488 -2.7092533 -3.8168294 -3.9630206 -3.514864 -2.7746379 -2.308744][-2.4151957 -2.5601861 -2.5277209 -2.1076958 -1.2621198 -0.15660667 0.92796755 1.0335879 -0.11008692 -1.8144753 -3.1681671 -3.5862081 -3.4511409 -2.9186778 -2.4828618][-3.0173407 -3.1270657 -2.9226072 -2.3656228 -1.5829489 -0.69653392 0.072311878 0.10624409 -0.79285789 -2.0135453 -2.9548829 -3.3194053 -3.3568053 -2.9882755 -2.6112447][-3.49832 -3.6647172 -3.4694991 -3.0252881 -2.556849 -2.0731413 -1.6841903 -1.6871228 -2.1784773 -2.7854998 -3.2504191 -3.4635437 -3.4827518 -3.1712492 -2.8415666][-3.341893 -3.5676548 -3.5582421 -3.4109275 -3.3229475 -3.1990869 -3.0470734 -2.9841142 -3.0933681 -3.2359657 -3.4334326 -3.6615357 -3.7436111 -3.5438418 -3.2596495][-2.8911381 -3.0190649 -3.0579982 -3.0886815 -3.2606571 -3.4204192 -3.4520185 -3.3484097 -3.2099071 -3.105216 -3.2537422 -3.5971394 -3.7827768 -3.6981273 -3.4693108][-2.7886815 -2.8176703 -2.821568 -2.8559053 -2.9900451 -3.0892158 -3.0261633 -2.8293271 -2.6663167 -2.6205714 -2.8508615 -3.2454939 -3.4492066 -3.4342766 -3.2941918][-3.0038028 -2.9260936 -2.8276668 -2.7717614 -2.8031192 -2.8564105 -2.8150766 -2.6733189 -2.56252 -2.5348794 -2.6770935 -2.9316039 -3.0823236 -3.1216109 -3.0803647][-3.2948108 -3.19141 -3.0719247 -3.0298185 -3.08994 -3.2053638 -3.255322 -3.191762 -3.0860057 -2.9926147 -2.9830184 -3.0742664 -3.1498532 -3.1801813 -3.1519032]]...]
INFO - root - 2017-12-07 09:39:42.425324: step 25110, loss = 0.95, batch loss = 0.88 (7.5 examples/sec; 1.069 sec/batch; 91h:15m:51s remains)
INFO - root - 2017-12-07 09:39:53.070203: step 25120, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.062 sec/batch; 90h:39m:09s remains)
INFO - root - 2017-12-07 09:40:03.803643: step 25130, loss = 0.73, batch loss = 0.66 (7.5 examples/sec; 1.073 sec/batch; 91h:38m:06s remains)
INFO - root - 2017-12-07 09:40:14.208984: step 25140, loss = 0.59, batch loss = 0.52 (7.6 examples/sec; 1.055 sec/batch; 90h:05m:14s remains)
INFO - root - 2017-12-07 09:40:24.782891: step 25150, loss = 0.86, batch loss = 0.79 (7.5 examples/sec; 1.064 sec/batch; 90h:48m:40s remains)
INFO - root - 2017-12-07 09:40:35.551364: step 25160, loss = 0.92, batch loss = 0.84 (7.3 examples/sec; 1.093 sec/batch; 93h:17m:25s remains)
INFO - root - 2017-12-07 09:40:46.205634: step 25170, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.064 sec/batch; 90h:48m:22s remains)
INFO - root - 2017-12-07 09:40:56.977127: step 25180, loss = 0.70, batch loss = 0.62 (7.6 examples/sec; 1.059 sec/batch; 90h:23m:14s remains)
INFO - root - 2017-12-07 09:41:07.744277: step 25190, loss = 0.83, batch loss = 0.76 (7.4 examples/sec; 1.086 sec/batch; 92h:42m:41s remains)
INFO - root - 2017-12-07 09:41:18.410743: step 25200, loss = 0.77, batch loss = 0.70 (7.3 examples/sec; 1.096 sec/batch; 93h:31m:56s remains)
2017-12-07 09:41:19.255371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8542061 -3.1346745 -3.2108254 -2.7486651 -2.0796139 -1.818182 -2.036854 -2.4141836 -2.7591703 -3.1174989 -3.3186145 -3.1755896 -2.8130043 -2.4841771 -2.1907527][-2.7910013 -3.0151095 -2.9313817 -2.3072131 -1.5797677 -1.4025793 -1.841574 -2.3656018 -2.7811794 -3.1944361 -3.387589 -3.0872195 -2.5415707 -2.2423089 -2.1884432][-2.6582015 -2.6354265 -2.3033175 -1.582567 -0.94667029 -0.89638138 -1.39028 -1.8405132 -2.1276135 -2.4487824 -2.5803032 -2.2992868 -1.8299031 -1.7346776 -1.9447443][-2.4401574 -2.2195826 -1.7621257 -1.0910158 -0.63612366 -0.70499587 -1.100049 -1.3067236 -1.4420147 -1.8060708 -2.0665443 -1.9801323 -1.6711867 -1.6509342 -1.9020894][-2.3057168 -2.0479856 -1.5963206 -1.0520623 -0.75338387 -0.78549933 -0.85501933 -0.69382477 -0.76338124 -1.4764071 -2.1467605 -2.3190236 -2.1219022 -2.0490224 -2.1858964][-2.3557627 -2.1824911 -1.7489123 -1.2245662 -0.87857008 -0.56771946 -0.042204857 0.59338379 0.42841578 -0.84083962 -2.0711093 -2.5780246 -2.5368786 -2.4515953 -2.5403728][-2.5558364 -2.4020045 -1.8492913 -1.1676121 -0.55479217 0.27639818 1.3563399 2.2031507 1.7197332 0.041601658 -1.4855764 -2.230746 -2.3740394 -2.3509505 -2.4542205][-2.815156 -2.5697305 -1.8223445 -0.96091962 -0.16169167 0.85113478 1.8972077 2.4317794 1.7074871 0.30345392 -0.84493232 -1.503757 -1.7095199 -1.6376214 -1.5712821][-3.0168531 -2.6647959 -1.8431754 -0.98697138 -0.262681 0.47412682 0.95546484 0.92188978 0.33945894 -0.23231888 -0.54514861 -0.87428856 -1.0268092 -0.84173059 -0.53909612][-3.0905125 -2.723978 -1.9581265 -1.1867895 -0.61216855 -0.20468855 -0.24930668 -0.62630773 -0.95116425 -0.82186747 -0.47214174 -0.56309938 -0.7475667 -0.68933964 -0.36900234][-3.0342135 -2.7517815 -2.1142094 -1.3462322 -0.74375772 -0.4289341 -0.65122986 -1.0341151 -1.1573195 -0.83290458 -0.38518906 -0.46978188 -0.76640487 -0.95269084 -0.81340885][-2.9462597 -2.8512082 -2.3424501 -1.4673185 -0.677546 -0.24891615 -0.390131 -0.6524806 -0.7188518 -0.57267928 -0.39201784 -0.57345772 -0.85437369 -1.1049509 -1.117089][-2.9223452 -3.0501504 -2.6510212 -1.6280231 -0.59051824 -0.00048160553 -0.091629028 -0.34248495 -0.44289541 -0.5183928 -0.58503366 -0.7693243 -0.81050229 -0.84095907 -0.84226251][-2.9572382 -3.2324309 -2.8897161 -1.7787254 -0.64993048 -0.061586857 -0.21925116 -0.50300169 -0.56325316 -0.62993526 -0.74854374 -0.84357786 -0.63638616 -0.3811388 -0.26749706][-3.0091608 -3.3355889 -3.0377021 -1.9610379 -0.90111041 -0.4598546 -0.70241761 -0.94358182 -0.88729405 -0.81605625 -0.876364 -0.92673755 -0.72588611 -0.4529748 -0.25629759]]...]
INFO - root - 2017-12-07 09:41:29.899976: step 25210, loss = 0.71, batch loss = 0.64 (7.5 examples/sec; 1.071 sec/batch; 91h:23m:12s remains)
INFO - root - 2017-12-07 09:41:40.622350: step 25220, loss = 0.73, batch loss = 0.66 (7.6 examples/sec; 1.050 sec/batch; 89h:38m:05s remains)
INFO - root - 2017-12-07 09:41:51.368486: step 25230, loss = 0.73, batch loss = 0.66 (7.3 examples/sec; 1.096 sec/batch; 93h:34m:30s remains)
INFO - root - 2017-12-07 09:42:02.039982: step 25240, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.068 sec/batch; 91h:07m:31s remains)
INFO - root - 2017-12-07 09:42:12.731139: step 25250, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 1.154 sec/batch; 98h:29m:15s remains)
INFO - root - 2017-12-07 09:42:23.562804: step 25260, loss = 0.71, batch loss = 0.64 (7.6 examples/sec; 1.058 sec/batch; 90h:19m:24s remains)
INFO - root - 2017-12-07 09:42:34.137792: step 25270, loss = 0.80, batch loss = 0.72 (7.6 examples/sec; 1.047 sec/batch; 89h:21m:57s remains)
INFO - root - 2017-12-07 09:42:44.749455: step 25280, loss = 0.64, batch loss = 0.57 (7.4 examples/sec; 1.074 sec/batch; 91h:38m:59s remains)
INFO - root - 2017-12-07 09:42:55.440264: step 25290, loss = 0.66, batch loss = 0.59 (7.6 examples/sec; 1.048 sec/batch; 89h:26m:46s remains)
INFO - root - 2017-12-07 09:43:06.074434: step 25300, loss = 0.76, batch loss = 0.69 (7.4 examples/sec; 1.083 sec/batch; 92h:26m:25s remains)
2017-12-07 09:43:06.935151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0209179 -3.9301956 -3.7557526 -3.5506773 -3.4439085 -3.4125228 -3.4700036 -3.5979993 -3.6754646 -3.6486182 -3.4943624 -3.3331575 -3.2340193 -3.1547728 -3.1389232][-3.6339622 -3.4298978 -3.2588522 -3.1520765 -3.1988139 -3.35587 -3.551775 -3.7051468 -3.6839404 -3.4993932 -3.2150226 -2.9867167 -2.8977346 -2.8835542 -2.962923][-3.3780913 -3.046155 -2.8395829 -2.7817879 -2.9119682 -3.1749113 -3.4357884 -3.5588789 -3.4334464 -3.1296206 -2.7748876 -2.5348053 -2.4945002 -2.5730371 -2.7639837][-3.2066379 -2.8443174 -2.6381431 -2.5829532 -2.6622891 -2.8321512 -2.9541268 -2.9364243 -2.7397561 -2.4497805 -2.1674435 -2.0023196 -2.0454283 -2.2173114 -2.5138593][-3.0430069 -2.7511218 -2.5593333 -2.4677758 -2.4403524 -2.4297323 -2.3253357 -2.1333447 -1.9221141 -1.737045 -1.5809772 -1.5000238 -1.6166961 -1.8635204 -2.2386906][-3.0269897 -2.7172384 -2.3857083 -2.1167204 -1.8997273 -1.716454 -1.4687121 -1.2130861 -1.1037517 -1.1051295 -1.1096556 -1.1305292 -1.324291 -1.6449752 -2.0764992][-2.9265218 -2.5224659 -2.0018134 -1.5227063 -1.1033533 -0.76483512 -0.43443131 -0.19357157 -0.23156023 -0.44158173 -0.64303803 -0.82184744 -1.1407292 -1.5672 -2.0616159][-2.7070384 -2.2775691 -1.7406406 -1.2701895 -0.85041237 -0.49814916 -0.15300417 0.067692757 -0.031602859 -0.32488871 -0.61558962 -0.86626792 -1.2370188 -1.6918123 -2.1740093][-2.6662848 -2.3195148 -1.9075468 -1.5709207 -1.2730968 -1.0219603 -0.77247334 -0.63771462 -0.76755285 -1.0378532 -1.2724044 -1.4407384 -1.7068925 -2.0436401 -2.4073567][-2.7105622 -2.4758377 -2.2281311 -2.0531888 -1.9098389 -1.7987754 -1.7061312 -1.6924071 -1.8416765 -2.040966 -2.1473572 -2.1782119 -2.2902608 -2.4672778 -2.6847787][-2.738132 -2.6135714 -2.5145688 -2.4754453 -2.4639611 -2.4757249 -2.5084541 -2.5693014 -2.6945498 -2.8026042 -2.8001857 -2.7361503 -2.7266448 -2.7688766 -2.8699625][-2.9411671 -2.89404 -2.8841398 -2.9177811 -2.9615641 -3.003099 -3.0455275 -3.0925479 -3.1637568 -3.2051165 -3.1531816 -3.0512664 -2.97477 -2.9363809 -2.9656034][-3.2749434 -3.294404 -3.3278582 -3.3810062 -3.4202075 -3.4398053 -3.4409347 -3.4390769 -3.4540517 -3.4488697 -3.3702788 -3.2525299 -3.1524863 -3.0831342 -3.0695252][-3.4313114 -3.4769962 -3.5196495 -3.5617673 -3.5826151 -3.5852079 -3.5674725 -3.5369632 -3.5154884 -3.4847722 -3.412508 -3.3247013 -3.2544007 -3.2014153 -3.1720409][-3.3771684 -3.4082046 -3.4286666 -3.4466681 -3.4531198 -3.4497695 -3.4348717 -3.4087248 -3.3927717 -3.3795071 -3.3483844 -3.3089104 -3.2759852 -3.2457633 -3.2168031]]...]
INFO - root - 2017-12-07 09:43:17.758045: step 25310, loss = 0.94, batch loss = 0.87 (7.6 examples/sec; 1.050 sec/batch; 89h:35m:13s remains)
INFO - root - 2017-12-07 09:43:28.506919: step 25320, loss = 0.81, batch loss = 0.74 (7.2 examples/sec; 1.112 sec/batch; 94h:50m:48s remains)
INFO - root - 2017-12-07 09:43:39.120490: step 25330, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.071 sec/batch; 91h:23m:17s remains)
INFO - root - 2017-12-07 09:43:49.676008: step 25340, loss = 1.01, batch loss = 0.94 (7.2 examples/sec; 1.109 sec/batch; 94h:37m:12s remains)
INFO - root - 2017-12-07 09:44:00.351582: step 25350, loss = 0.75, batch loss = 0.68 (7.6 examples/sec; 1.051 sec/batch; 89h:38m:38s remains)
INFO - root - 2017-12-07 09:44:11.060058: step 25360, loss = 0.48, batch loss = 0.41 (7.3 examples/sec; 1.098 sec/batch; 93h:41m:04s remains)
INFO - root - 2017-12-07 09:44:21.796143: step 25370, loss = 0.86, batch loss = 0.79 (7.4 examples/sec; 1.088 sec/batch; 92h:48m:52s remains)
INFO - root - 2017-12-07 09:44:32.582169: step 25380, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.066 sec/batch; 90h:55m:24s remains)
INFO - root - 2017-12-07 09:44:43.465040: step 25390, loss = 0.97, batch loss = 0.90 (7.5 examples/sec; 1.071 sec/batch; 91h:20m:31s remains)
INFO - root - 2017-12-07 09:44:54.216207: step 25400, loss = 0.97, batch loss = 0.90 (7.5 examples/sec; 1.070 sec/batch; 91h:15m:11s remains)
2017-12-07 09:44:55.048613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.08796 -3.9117343 -3.4979591 -3.4151855 -3.4247215 -3.3017762 -3.2213323 -3.1931481 -3.0566463 -3.0439506 -3.1774125 -3.2663851 -3.1911869 -3.1196954 -3.1825776][-3.9442282 -3.686023 -3.0805087 -2.9651914 -2.9520311 -2.7450495 -2.5672309 -2.4294167 -2.1852372 -2.2406664 -2.60112 -2.9145079 -2.9381018 -2.9505391 -3.1605563][-3.5985947 -3.2522397 -2.5473428 -2.4059632 -2.3113563 -2.0362589 -1.7750638 -1.4028006 -0.93145251 -1.0331938 -1.6530256 -2.2678328 -2.4782853 -2.6599169 -3.0741179][-2.8974621 -2.4632654 -1.7371976 -1.5076394 -1.2674997 -0.98594189 -0.73276734 -0.11094856 0.58534527 0.40333176 -0.50272775 -1.4386218 -1.9005468 -2.3155222 -2.9354482][-2.1253703 -1.6445563 -0.96604562 -0.62651396 -0.22047472 0.021311283 0.27973366 1.1819005 2.0805335 1.7612901 0.560287 -0.67115736 -1.419867 -2.0640905 -2.8030462][-1.5773304 -1.2642787 -0.69736266 -0.20109081 0.38188076 0.64839125 1.0351181 2.2833209 3.3300977 2.7904696 1.3660922 -0.073099613 -1.1230774 -1.9279823 -2.6754918][-1.0890322 -1.07676 -0.66636992 -0.046426296 0.66296291 1.0062304 1.5784101 3.160233 4.2883863 3.4440451 1.8783526 0.33576298 -1.0082273 -1.8817518 -2.558404][-0.65703154 -0.837219 -0.49924874 0.17884016 0.88073635 1.2105937 1.8453455 3.6317949 4.8022308 3.7644472 2.228991 0.65433693 -0.93478346 -1.8160746 -2.4133062][-0.5272119 -0.78915787 -0.38933992 0.34784365 0.94996405 1.1699324 1.7132802 3.4648194 4.6016407 3.666893 2.3330674 0.74620152 -0.93658614 -1.7360828 -2.286351][-1.0213869 -1.2056434 -0.65400887 0.12459946 0.64844608 0.81761742 1.3119373 2.8906779 3.8921671 3.2754765 2.2022271 0.545722 -1.0821588 -1.7334795 -2.2652802][-1.531786 -1.5070276 -0.833277 -0.16134787 0.15911627 0.26284742 0.69648933 1.9882889 2.8099666 2.5287375 1.7037997 0.013308525 -1.3917434 -1.8351331 -2.3683059][-1.8609846 -1.6837411 -1.0323167 -0.59286094 -0.52504015 -0.52414346 -0.20272064 0.7933054 1.4891133 1.4645705 0.8026576 -0.78353095 -1.8539696 -2.0959311 -2.6023695][-2.3050964 -2.0791357 -1.5588307 -1.3629382 -1.4718122 -1.5105319 -1.2792466 -0.498832 0.18612099 0.36600828 -0.2203269 -1.6108019 -2.3876734 -2.4859796 -2.892725][-2.6275837 -2.4419808 -2.1099241 -2.1026664 -2.3299983 -2.42022 -2.3422804 -1.7952616 -1.121233 -0.86392832 -1.3764386 -2.4702437 -2.9821072 -2.9609451 -3.1786075][-2.6219969 -2.5181851 -2.3655505 -2.4493937 -2.7029288 -2.8439174 -2.9250765 -2.6625834 -2.1901495 -2.0535116 -2.470433 -3.1801281 -3.4420857 -3.3232009 -3.3510094]]...]
INFO - root - 2017-12-07 09:45:05.865902: step 25410, loss = 0.71, batch loss = 0.64 (7.3 examples/sec; 1.093 sec/batch; 93h:13m:12s remains)
INFO - root - 2017-12-07 09:45:14.061509: step 25420, loss = 0.76, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:07m:09s remains)
INFO - root - 2017-12-07 09:45:21.689905: step 25430, loss = 0.63, batch loss = 0.56 (10.3 examples/sec; 0.774 sec/batch; 65h:58m:54s remains)
INFO - root - 2017-12-07 09:45:29.156575: step 25440, loss = 0.94, batch loss = 0.87 (10.6 examples/sec; 0.756 sec/batch; 64h:31m:07s remains)
INFO - root - 2017-12-07 09:45:36.776794: step 25450, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 65h:08m:08s remains)
INFO - root - 2017-12-07 09:45:44.499915: step 25460, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.766 sec/batch; 65h:18m:24s remains)
INFO - root - 2017-12-07 09:45:52.093802: step 25470, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.758 sec/batch; 64h:38m:01s remains)
INFO - root - 2017-12-07 09:45:59.789162: step 25480, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.775 sec/batch; 66h:05m:53s remains)
INFO - root - 2017-12-07 09:46:07.389672: step 25490, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 66h:05m:49s remains)
INFO - root - 2017-12-07 09:46:14.961330: step 25500, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.757 sec/batch; 64h:35m:23s remains)
2017-12-07 09:46:15.640400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4442432 -3.45313 -3.3348961 -3.1581912 -3.0232935 -2.7959738 -2.5514174 -2.4210749 -2.4020619 -2.4311113 -2.4622042 -2.5223613 -2.5339029 -2.4504166 -2.3883824][-4.2377982 -4.2196693 -4.0625834 -3.8381209 -3.6842806 -3.4083028 -3.1183267 -2.9643435 -2.9348111 -2.9625616 -2.9745562 -2.982981 -2.8387551 -2.5241513 -2.2612898][-4.6791463 -4.53182 -4.3474588 -4.1358309 -4.0050044 -3.7889066 -3.6198027 -3.6762655 -3.8769994 -4.0659933 -4.1326289 -4.0674357 -3.6904447 -3.0309095 -2.4263127][-4.6819797 -4.4571638 -4.2960072 -4.1144919 -3.9245715 -3.6224399 -3.4165983 -3.5806649 -4.006783 -4.3877625 -4.6006079 -4.5576572 -4.068265 -3.2491155 -2.4537821][-4.2826643 -4.0936546 -4.0217705 -3.887749 -3.63341 -3.184535 -2.8096023 -2.8864617 -3.3743391 -3.8714345 -4.2100692 -4.209589 -3.7112117 -2.9030533 -2.0847082][-3.6585917 -3.4763207 -3.4218526 -3.2928462 -2.974771 -2.4502373 -1.9556444 -1.9715195 -2.5410035 -3.185647 -3.6697884 -3.7200103 -3.2791626 -2.5611255 -1.759881][-3.0283318 -2.738667 -2.6003957 -2.4251084 -2.004283 -1.3706849 -0.75237036 -0.685302 -1.2648947 -2.0204442 -2.6101494 -2.7114735 -2.3455567 -1.7703753 -1.1279807][-2.6185505 -2.1064613 -1.7801688 -1.5001056 -0.95697689 -0.18401861 0.58172941 0.768445 0.25458002 -0.52400041 -1.1406803 -1.2881439 -1.018261 -0.60082483 -0.20781088][-2.8934941 -2.3741546 -2.0874431 -1.9250307 -1.5452433 -0.92937183 -0.24991512 -0.018311977 -0.41290951 -1.0709786 -1.5366213 -1.5582521 -1.1891725 -0.62235737 -0.14640522][-3.471765 -3.0562835 -2.8783052 -2.8382983 -2.7042346 -2.405427 -1.9979372 -1.8069596 -2.0387232 -2.4621773 -2.6949365 -2.6039023 -2.1681521 -1.4566317 -0.87531018][-3.7818372 -3.3940721 -3.2047734 -3.1183403 -3.0277376 -2.8699312 -2.644243 -2.5310621 -2.6855907 -2.9388165 -3.0259242 -2.9317431 -2.5738611 -1.9175794 -1.3770466][-4.024725 -3.7325163 -3.6325686 -3.6184025 -3.5880179 -3.5156562 -3.4077964 -3.3652053 -3.4794903 -3.6275558 -3.6464987 -3.5632932 -3.2378013 -2.5925589 -2.0379305][-3.9084892 -3.600003 -3.5069036 -3.5583925 -3.5805833 -3.5539024 -3.514183 -3.5221622 -3.6128418 -3.7280474 -3.8037372 -3.8197191 -3.6012032 -3.081181 -2.6335807][-3.640842 -3.2776477 -3.1076119 -3.1030045 -3.0911775 -3.0524755 -3.0184646 -3.0308695 -3.1053584 -3.225908 -3.3686543 -3.4752214 -3.3614335 -3.0081813 -2.7391906][-3.6192946 -3.3680377 -3.2507291 -3.2527938 -3.2622905 -3.2567959 -3.245234 -3.2510309 -3.2847445 -3.353457 -3.4603474 -3.5376339 -3.4317279 -3.1623516 -2.9747915]]...]
INFO - root - 2017-12-07 09:46:23.468871: step 25510, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 64h:26m:43s remains)
INFO - root - 2017-12-07 09:46:31.212003: step 25520, loss = 1.00, batch loss = 0.93 (10.6 examples/sec; 0.757 sec/batch; 64h:33m:00s remains)
INFO - root - 2017-12-07 09:46:38.867851: step 25530, loss = 0.76, batch loss = 0.69 (10.2 examples/sec; 0.781 sec/batch; 66h:37m:31s remains)
INFO - root - 2017-12-07 09:46:46.361916: step 25540, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.773 sec/batch; 65h:53m:50s remains)
INFO - root - 2017-12-07 09:46:54.072920: step 25550, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.748 sec/batch; 63h:48m:06s remains)
INFO - root - 2017-12-07 09:47:01.667692: step 25560, loss = 0.99, batch loss = 0.92 (10.8 examples/sec; 0.744 sec/batch; 63h:25m:18s remains)
INFO - root - 2017-12-07 09:47:09.284133: step 25570, loss = 0.86, batch loss = 0.79 (10.1 examples/sec; 0.793 sec/batch; 67h:37m:37s remains)
INFO - root - 2017-12-07 09:47:16.855377: step 25580, loss = 0.81, batch loss = 0.73 (10.5 examples/sec; 0.763 sec/batch; 65h:02m:44s remains)
INFO - root - 2017-12-07 09:47:24.671094: step 25590, loss = 0.99, batch loss = 0.91 (10.4 examples/sec; 0.770 sec/batch; 65h:36m:45s remains)
INFO - root - 2017-12-07 09:47:32.421073: step 25600, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 65h:09m:54s remains)
2017-12-07 09:47:33.054418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7095046 -2.7561722 -2.7463808 -2.7272758 -2.7073116 -2.5632348 -2.0301876 -1.587604 -1.9510932 -2.5976827 -3.0342057 -3.2114997 -2.7693419 -2.0890634 -1.732825][-2.905261 -2.875247 -2.7938275 -2.6763382 -2.620986 -2.4876251 -1.9460447 -1.5097196 -1.9557414 -2.6742501 -3.0895743 -3.2270968 -2.7918534 -2.1351264 -1.785013][-2.9369261 -2.8984561 -2.7996156 -2.6403713 -2.5898838 -2.4409246 -1.8906262 -1.4959707 -2.0102537 -2.7654438 -3.1451366 -3.2193766 -2.7807107 -2.1505327 -1.8131897][-2.9783874 -2.9426026 -2.8142629 -2.6222479 -2.5558479 -2.332288 -1.7621219 -1.4501753 -2.0292714 -2.7963083 -3.1554003 -3.1708326 -2.7201447 -2.1244457 -1.8148625][-3.0634389 -3.0476942 -2.8844388 -2.6392262 -2.5136549 -2.1849523 -1.5974741 -1.3985195 -2.0643508 -2.8470387 -3.1903009 -3.1401834 -2.6666255 -2.0962126 -1.8092351][-3.0135062 -3.0413058 -2.8871248 -2.6286163 -2.4584432 -2.0416155 -1.4467771 -1.3419907 -2.0731642 -2.8837907 -3.2154956 -3.1143751 -2.6274297 -2.0789785 -1.8054547][-2.92451 -2.9766321 -2.8665226 -2.6521535 -2.4736052 -2.0031033 -1.3957145 -1.310091 -2.0384049 -2.8620625 -3.1972387 -3.0690036 -2.5794616 -2.0535929 -1.7973492][-2.8408666 -2.918839 -2.8549938 -2.70523 -2.5287008 -2.0200765 -1.3909159 -1.2771018 -1.9679139 -2.8038096 -3.1555569 -3.0181644 -2.5308487 -2.0202951 -1.7813671][-2.8146658 -2.9070704 -2.8950286 -2.8171129 -2.6351261 -2.103842 -1.4789803 -1.3280916 -1.9599183 -2.7933836 -3.1464207 -2.9911423 -2.4889684 -1.9786637 -1.7568128][-2.8506513 -2.9198003 -2.9415689 -2.924871 -2.7607737 -2.2701445 -1.707449 -1.5378866 -2.1123841 -2.9203343 -3.2258046 -3.0206888 -2.4822369 -1.9556961 -1.736378][-2.7910371 -2.8184192 -2.8700843 -2.926755 -2.8117089 -2.4070115 -1.9436588 -1.765147 -2.2782526 -3.0462861 -3.3008475 -3.0625448 -2.4974647 -1.9526823 -1.7255816][-2.7195077 -2.7060418 -2.7663717 -2.8799062 -2.8050046 -2.4798779 -2.0940564 -1.8915491 -2.3352971 -3.0531602 -3.2897263 -3.0585771 -2.5023088 -1.9523542 -1.7198684][-2.6917491 -2.6324296 -2.6621127 -2.8002307 -2.7487731 -2.4877448 -2.1437459 -1.9082954 -2.3028181 -2.9720311 -3.213975 -3.0167649 -2.4880571 -1.949173 -1.7203972][-2.7109647 -2.6122394 -2.5958829 -2.7357407 -2.7121522 -2.52594 -2.197571 -1.9095123 -2.2589173 -2.8792174 -3.1194499 -2.9596858 -2.4572873 -1.9399967 -1.7244153][-2.7588439 -2.629782 -2.5597467 -2.6758969 -2.6963396 -2.614203 -2.3097332 -1.9877155 -2.3047674 -2.8614426 -3.0697541 -2.9266343 -2.4419503 -1.9424028 -1.7357996]]...]
INFO - root - 2017-12-07 09:47:40.774649: step 25610, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.774 sec/batch; 66h:01m:25s remains)
INFO - root - 2017-12-07 09:47:48.357212: step 25620, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 64h:26m:32s remains)
INFO - root - 2017-12-07 09:47:55.971275: step 25630, loss = 1.01, batch loss = 0.94 (10.5 examples/sec; 0.760 sec/batch; 64h:48m:07s remains)
INFO - root - 2017-12-07 09:48:03.362823: step 25640, loss = 0.85, batch loss = 0.78 (10.7 examples/sec; 0.748 sec/batch; 63h:44m:57s remains)
INFO - root - 2017-12-07 09:48:11.098610: step 25650, loss = 0.76, batch loss = 0.69 (10.1 examples/sec; 0.796 sec/batch; 67h:48m:41s remains)
INFO - root - 2017-12-07 09:48:18.793414: step 25660, loss = 1.07, batch loss = 0.99 (10.3 examples/sec; 0.777 sec/batch; 66h:12m:26s remains)
INFO - root - 2017-12-07 09:48:26.485651: step 25670, loss = 1.11, batch loss = 1.03 (10.3 examples/sec; 0.776 sec/batch; 66h:08m:07s remains)
INFO - root - 2017-12-07 09:48:34.115453: step 25680, loss = 0.71, batch loss = 0.64 (10.8 examples/sec; 0.742 sec/batch; 63h:12m:37s remains)
INFO - root - 2017-12-07 09:48:41.774584: step 25690, loss = 0.73, batch loss = 0.66 (10.7 examples/sec; 0.745 sec/batch; 63h:30m:28s remains)
INFO - root - 2017-12-07 09:48:49.354364: step 25700, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 64h:35m:59s remains)
2017-12-07 09:48:49.956236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2383139 -3.4532475 -3.4740465 -3.3805292 -3.3244205 -3.4210641 -3.4657686 -3.2671022 -3.074789 -2.9505358 -2.906599 -2.9337192 -2.904649 -2.9761584 -3.1518369][-2.9435422 -3.1098661 -3.0997295 -3.0307183 -3.049283 -3.239866 -3.3587866 -3.1734507 -3.00168 -2.9363911 -2.8952174 -2.8841834 -2.8428721 -2.8954797 -3.0345471][-2.738838 -2.8323197 -2.790267 -2.73106 -2.7958138 -3.0390944 -3.1976228 -3.0265558 -2.9177132 -2.978786 -3.0005255 -2.9710882 -2.8851352 -2.921083 -3.0651355][-2.5181563 -2.6201575 -2.6766586 -2.6738486 -2.7327256 -2.9118466 -2.993197 -2.7883019 -2.7274172 -2.9366131 -3.0952935 -3.09372 -2.9644914 -2.9867392 -3.1461155][-2.4273818 -2.5252137 -2.6904054 -2.7527514 -2.8153648 -2.922363 -2.8844938 -2.6097975 -2.5670457 -2.8973985 -3.2076383 -3.2272191 -3.0244613 -3.0009637 -3.123035][-2.533587 -2.578383 -2.7472019 -2.7933288 -2.8446748 -2.9030519 -2.7497342 -2.3773992 -2.3329418 -2.7681377 -3.2272151 -3.2801995 -3.0221658 -2.9495487 -2.9837766][-2.5510824 -2.5581722 -2.6942086 -2.6946933 -2.7555723 -2.83351 -2.6277246 -2.1980252 -2.1354215 -2.6112955 -3.1497016 -3.241045 -2.9752846 -2.8936894 -2.8521903][-2.5318766 -2.5076313 -2.5985994 -2.5846272 -2.6810632 -2.7945297 -2.6258693 -2.2593534 -2.2082703 -2.6419182 -3.1607511 -3.2565622 -2.9823184 -2.8752232 -2.7731996][-2.68693 -2.6406946 -2.6685348 -2.6501293 -2.7591321 -2.8648987 -2.6934118 -2.3519933 -2.2981131 -2.698379 -3.2210648 -3.345911 -3.0862207 -2.96656 -2.8454432][-2.9366374 -2.9121065 -2.8788381 -2.8323693 -2.9348385 -3.0493569 -2.8878987 -2.510922 -2.3753331 -2.6867905 -3.1705375 -3.3193212 -3.1074893 -3.0166616 -2.9399548][-3.0545063 -3.102716 -3.0617619 -2.9782624 -3.0296235 -3.1412103 -3.0338249 -2.7083929 -2.5639329 -2.7611551 -3.080843 -3.1756146 -3.0184345 -2.9739571 -2.9340019][-2.9610682 -3.0506406 -3.0604348 -3.0186951 -3.0538976 -3.1340077 -3.0576413 -2.8459992 -2.8022416 -2.933073 -3.0771623 -3.0717716 -2.923718 -2.8985355 -2.8735306][-2.7096798 -2.7764773 -2.8250675 -2.8544526 -2.9087276 -2.9809053 -2.9375935 -2.8418627 -2.9071462 -2.9979901 -3.0138278 -2.9330325 -2.7802994 -2.764576 -2.7699363][-2.5280316 -2.5638556 -2.6088245 -2.6453452 -2.6589284 -2.6854415 -2.6387391 -2.5914874 -2.713551 -2.8043716 -2.7947602 -2.71238 -2.5928226 -2.6098597 -2.6630154][-2.5350676 -2.5665915 -2.6020448 -2.6173685 -2.5761447 -2.53958 -2.4486079 -2.3557012 -2.4346228 -2.5115235 -2.5074365 -2.4561181 -2.3892787 -2.4482727 -2.5413127]]...]
INFO - root - 2017-12-07 09:48:57.621011: step 25710, loss = 0.60, batch loss = 0.53 (10.4 examples/sec; 0.767 sec/batch; 65h:19m:38s remains)
INFO - root - 2017-12-07 09:49:05.244644: step 25720, loss = 0.93, batch loss = 0.86 (10.7 examples/sec; 0.746 sec/batch; 63h:35m:19s remains)
INFO - root - 2017-12-07 09:49:12.937132: step 25730, loss = 0.88, batch loss = 0.80 (9.8 examples/sec; 0.814 sec/batch; 69h:19m:57s remains)
INFO - root - 2017-12-07 09:49:20.422328: step 25740, loss = 0.95, batch loss = 0.88 (10.7 examples/sec; 0.748 sec/batch; 63h:44m:35s remains)
INFO - root - 2017-12-07 09:49:28.009327: step 25750, loss = 0.77, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 64h:08m:33s remains)
INFO - root - 2017-12-07 09:49:35.698747: step 25760, loss = 0.96, batch loss = 0.89 (10.5 examples/sec; 0.763 sec/batch; 65h:03m:06s remains)
INFO - root - 2017-12-07 09:49:43.396461: step 25770, loss = 0.93, batch loss = 0.86 (10.8 examples/sec; 0.744 sec/batch; 63h:21m:31s remains)
INFO - root - 2017-12-07 09:49:51.122834: step 25780, loss = 0.92, batch loss = 0.85 (10.3 examples/sec; 0.779 sec/batch; 66h:24m:41s remains)
INFO - root - 2017-12-07 09:49:58.711651: step 25790, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.783 sec/batch; 66h:40m:08s remains)
INFO - root - 2017-12-07 09:50:06.346804: step 25800, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.754 sec/batch; 64h:12m:22s remains)
2017-12-07 09:50:06.950318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1672385 -2.4259913 -3.3230338 -3.4260302 -2.7143798 -1.7560198 -1.2000251 -1.3121378 -1.7558615 -2.1620965 -2.4009211 -2.5042415 -2.5674992 -2.5420871 -2.302954][-0.95502114 -2.1457856 -3.162075 -3.5011308 -3.0620508 -2.3348262 -1.9088058 -2.0599334 -2.4280686 -2.7322216 -3.0383034 -3.2357073 -3.3118253 -3.3215692 -3.1937494][-0.11157656 -1.3544712 -2.627085 -3.2444837 -2.9982581 -2.4209189 -2.0480773 -2.1283882 -2.417484 -2.695715 -3.0791068 -3.31347 -3.3561831 -3.4360881 -3.5366039][0.92534828 -0.424994 -1.9931295 -2.8386412 -2.6479249 -2.0924437 -1.6592767 -1.6278715 -1.9524505 -2.4097054 -2.937067 -3.2455893 -3.325218 -3.4986634 -3.7319524][1.3315964 -0.082856178 -1.8165421 -2.7512665 -2.5293093 -1.8751545 -1.2943742 -1.2130773 -1.7227852 -2.4367361 -3.0724216 -3.4727426 -3.7027657 -4.0256329 -4.2627807][0.70919228 -0.6941545 -2.2906876 -3.0599339 -2.5868134 -1.5459495 -0.60316777 -0.52330875 -1.3352582 -2.2762876 -2.9527249 -3.3989148 -3.7886639 -4.2535872 -4.4261422][-0.39504719 -1.6831465 -2.8531294 -3.1337984 -2.2044725 -0.61814475 0.7871747 0.78149271 -0.55924916 -1.9196706 -2.7055197 -3.1073458 -3.5122988 -4.0160322 -4.1619496][-1.4348578 -2.4557557 -3.0491037 -2.7462039 -1.305501 0.85992575 2.7954822 2.7255049 0.72260857 -1.3146975 -2.5283604 -3.0921006 -3.5648129 -4.1214547 -4.3208938][-2.5797038 -3.2026396 -3.325788 -2.6905782 -1.0256491 1.4200854 3.5996847 3.5190277 1.2457519 -1.1313024 -2.7219963 -3.5880761 -4.1865244 -4.711709 -4.86623][-3.3462837 -3.6461422 -3.6329079 -3.1123533 -1.7292082 0.32961035 2.0175338 1.9118929 0.17621279 -1.739145 -3.1217909 -3.9343262 -4.4139881 -4.7645764 -4.8100815][-3.3692021 -3.5186906 -3.5559618 -3.3154297 -2.3691883 -0.89610434 0.21969032 0.18470097 -0.84059191 -2.0556667 -2.9490004 -3.4260302 -3.6294494 -3.8020985 -3.8516438][-3.2909746 -3.4719234 -3.5898252 -3.4870393 -2.7764649 -1.6589468 -0.80773664 -0.70195532 -1.2350717 -2.0171928 -2.6657996 -2.9762876 -3.1240704 -3.3497524 -3.6120586][-3.5233042 -3.6802998 -3.7714667 -3.7033949 -3.1657052 -2.3105552 -1.5850315 -1.3180046 -1.5730629 -2.1865292 -2.7870626 -3.1183617 -3.3340774 -3.6811452 -4.1111994][-3.6847842 -3.6509435 -3.6944122 -3.7820086 -3.558248 -3.0424984 -2.4491343 -2.0111911 -2.0478137 -2.5587907 -3.0898223 -3.2964036 -3.3996086 -3.7287714 -4.2163615][-3.5303292 -3.4729629 -3.5669179 -3.8312037 -3.9115829 -3.6896756 -3.1971273 -2.6426034 -2.5578427 -2.9218524 -3.2822168 -3.3281832 -3.324084 -3.5835576 -4.0736604]]...]
INFO - root - 2017-12-07 09:50:14.730562: step 25810, loss = 1.04, batch loss = 0.97 (10.4 examples/sec; 0.766 sec/batch; 65h:15m:33s remains)
INFO - root - 2017-12-07 09:50:22.548088: step 25820, loss = 0.99, batch loss = 0.92 (9.6 examples/sec; 0.831 sec/batch; 70h:45m:52s remains)
INFO - root - 2017-12-07 09:50:30.278034: step 25830, loss = 0.79, batch loss = 0.72 (10.1 examples/sec; 0.789 sec/batch; 67h:14m:00s remains)
INFO - root - 2017-12-07 09:50:37.669984: step 25840, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.767 sec/batch; 65h:18m:37s remains)
INFO - root - 2017-12-07 09:50:45.391410: step 25850, loss = 0.65, batch loss = 0.58 (10.1 examples/sec; 0.795 sec/batch; 67h:42m:26s remains)
INFO - root - 2017-12-07 09:50:53.025551: step 25860, loss = 1.06, batch loss = 0.99 (10.6 examples/sec; 0.755 sec/batch; 64h:20m:50s remains)
INFO - root - 2017-12-07 09:51:00.686446: step 25870, loss = 0.91, batch loss = 0.83 (10.3 examples/sec; 0.778 sec/batch; 66h:14m:39s remains)
INFO - root - 2017-12-07 09:51:08.414178: step 25880, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 67h:52m:09s remains)
INFO - root - 2017-12-07 09:51:16.093725: step 25890, loss = 0.89, batch loss = 0.82 (10.3 examples/sec; 0.780 sec/batch; 66h:25m:15s remains)
INFO - root - 2017-12-07 09:51:23.787471: step 25900, loss = 0.81, batch loss = 0.73 (9.9 examples/sec; 0.811 sec/batch; 69h:02m:47s remains)
2017-12-07 09:51:24.396127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4574893 -3.5345674 -3.7689285 -3.893688 -3.7391782 -3.5732028 -3.5735703 -3.6077073 -3.5986259 -3.656688 -3.7020888 -3.7473149 -3.8454723 -3.8615384 -3.6873186][-3.2212358 -3.155596 -3.2950821 -3.2551508 -2.884522 -2.6221187 -2.7271147 -2.96937 -3.1666453 -3.458416 -3.6883173 -3.8064437 -3.9264538 -3.909528 -3.6459429][-3.0131223 -2.8536177 -2.9051874 -2.6849246 -2.1312928 -1.8461728 -2.1034548 -2.5352964 -2.8701711 -3.286376 -3.612654 -3.7453537 -3.8424735 -3.780149 -3.4604375][-3.0195918 -2.8770912 -2.8799322 -2.5182214 -1.8726699 -1.6214573 -1.9869065 -2.4532986 -2.7299719 -3.032572 -3.2942674 -3.3896198 -3.4726784 -3.4102459 -3.1318412][-3.3402717 -3.3478143 -3.3915339 -3.039134 -2.4921875 -2.3135557 -2.6091137 -2.8428674 -2.8179154 -2.8302231 -2.9106984 -2.9024844 -2.9144812 -2.8524423 -2.6861339][-3.5295751 -3.5997627 -3.6148367 -3.350781 -2.9770186 -2.8085828 -2.9090471 -2.8724866 -2.6334825 -2.5137081 -2.5465081 -2.4863935 -2.3893404 -2.2796109 -2.2258875][-3.3102741 -3.2194302 -3.0619893 -2.8764405 -2.6918654 -2.5506535 -2.5462103 -2.459374 -2.2662406 -2.2003279 -2.2654679 -2.1786771 -1.9841113 -1.8540256 -1.9231839][-2.6954784 -2.3451316 -2.0319152 -1.975369 -1.9912167 -1.9182484 -1.9212487 -1.9411321 -1.8688593 -1.8277273 -1.8408971 -1.6978321 -1.4642239 -1.4338889 -1.7220187][-2.0105917 -1.3809671 -0.95885611 -0.97412419 -1.0677464 -0.99647045 -1.0122819 -1.1692612 -1.2586319 -1.2980735 -1.3476024 -1.2663503 -1.1148913 -1.2251654 -1.6786726][-1.9786942 -1.2897644 -0.92252994 -0.98423457 -1.0514348 -0.93774247 -0.9177804 -1.07411 -1.205795 -1.2650459 -1.3647568 -1.3607297 -1.2578511 -1.4007618 -1.8682487][-2.5323782 -2.0028837 -1.7514913 -1.7685094 -1.7272232 -1.5801358 -1.5291271 -1.6385529 -1.7480211 -1.7769213 -1.8744738 -1.8910952 -1.7962434 -1.8805134 -2.2383854][-3.2461705 -2.9413152 -2.7670155 -2.6811008 -2.5284348 -2.3780327 -2.3291817 -2.420821 -2.5210249 -2.5588822 -2.654459 -2.6762252 -2.5963292 -2.6201305 -2.8273745][-3.7061086 -3.6325343 -3.5961175 -3.5553119 -3.4557557 -3.3868098 -3.354229 -3.40442 -3.4425492 -3.4387095 -3.4632888 -3.434453 -3.3473744 -3.3097093 -3.3610008][-3.745208 -3.7803555 -3.8224287 -3.8517802 -3.8522766 -3.8690534 -3.8773952 -3.9152431 -3.9232607 -3.900651 -3.8825336 -3.8207524 -3.7395577 -3.6751814 -3.6270509][-3.6620047 -3.7234032 -3.7807038 -3.8293946 -3.8639846 -3.899169 -3.9251952 -3.9560785 -3.9632139 -3.9426646 -3.9136586 -3.855623 -3.7853377 -3.7128181 -3.6330173]]...]
INFO - root - 2017-12-07 09:51:32.031219: step 25910, loss = 0.79, batch loss = 0.71 (10.9 examples/sec; 0.734 sec/batch; 62h:32m:58s remains)
INFO - root - 2017-12-07 09:51:39.701596: step 25920, loss = 0.69, batch loss = 0.61 (10.3 examples/sec; 0.780 sec/batch; 66h:23m:48s remains)
INFO - root - 2017-12-07 09:51:47.435474: step 25930, loss = 0.70, batch loss = 0.63 (10.7 examples/sec; 0.749 sec/batch; 63h:46m:58s remains)
INFO - root - 2017-12-07 09:51:54.833282: step 25940, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.772 sec/batch; 65h:46m:46s remains)
INFO - root - 2017-12-07 09:52:02.572353: step 25950, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.785 sec/batch; 66h:51m:57s remains)
INFO - root - 2017-12-07 09:52:10.293885: step 25960, loss = 0.92, batch loss = 0.85 (10.5 examples/sec; 0.761 sec/batch; 64h:46m:33s remains)
INFO - root - 2017-12-07 09:52:17.898540: step 25970, loss = 0.82, batch loss = 0.74 (10.6 examples/sec; 0.752 sec/batch; 64h:04m:00s remains)
INFO - root - 2017-12-07 09:52:25.496113: step 25980, loss = 0.62, batch loss = 0.54 (10.1 examples/sec; 0.794 sec/batch; 67h:38m:10s remains)
INFO - root - 2017-12-07 09:52:33.110258: step 25990, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.754 sec/batch; 64h:11m:02s remains)
INFO - root - 2017-12-07 09:52:40.925851: step 26000, loss = 0.82, batch loss = 0.75 (10.2 examples/sec; 0.783 sec/batch; 66h:40m:30s remains)
2017-12-07 09:52:41.576443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0427322 -2.2145543 -1.1673698 -0.60842943 -0.39725542 -0.6114428 -1.6385446 -2.2195344 -2.4269466 -2.6904354 -2.6261749 -2.350049 -1.9186058 -1.9128044 -2.5214436][-2.6941767 -1.9903367 -1.0657306 -0.5433569 -0.1677227 -0.17496872 -1.2435045 -1.8819139 -2.1562545 -2.6876216 -2.8028786 -2.5299275 -2.0876522 -2.0392356 -2.4765358][-2.3940635 -1.8121538 -0.996053 -0.55058312 -0.1288929 -0.16313028 -1.3041477 -1.8545325 -1.9684381 -2.532362 -2.8290691 -2.749249 -2.4675941 -2.4201591 -2.5345833][-2.3941643 -1.8745422 -1.1263266 -0.71933079 -0.28962994 -0.4958446 -1.6257689 -1.931782 -1.8535666 -2.3294568 -2.7136104 -2.8158872 -2.6968544 -2.5780442 -2.3420208][-2.6786089 -2.1442163 -1.3903108 -0.95457292 -0.4650774 -0.78445292 -1.6756711 -1.6453469 -1.6067624 -2.2255266 -2.7706318 -2.9817078 -2.8688645 -2.5678606 -2.0094283][-3.0345855 -2.4640574 -1.7293072 -1.332304 -0.80441833 -1.0350552 -1.4276724 -0.95080948 -1.0795879 -2.0445685 -2.8898044 -3.2510934 -3.1186972 -2.6906705 -1.8316498][-3.2427528 -2.6507711 -1.9520857 -1.6110749 -1.0486641 -1.0652566 -0.97488689 -0.0896163 -0.34528923 -1.62834 -2.7503924 -3.276104 -3.2122293 -2.8242192 -1.8727341][-3.3393421 -2.7578039 -2.0309613 -1.650089 -1.0254362 -0.8771441 -0.51268864 0.6392107 0.36309767 -1.1111705 -2.407239 -3.0869379 -3.129436 -2.825736 -1.8878751][-3.5158625 -3.0621209 -2.318222 -1.7870266 -1.0192213 -0.70103 -0.22086143 1.016427 0.74295282 -0.80416036 -2.1236558 -2.8655915 -3.0174961 -2.7999225 -1.9277396][-3.6680219 -3.4341495 -2.7826149 -2.1493807 -1.2585835 -0.772408 -0.29713726 0.78152561 0.52688313 -0.92793846 -2.1416731 -2.8527637 -3.0675342 -2.9125333 -2.1649115][-3.6245134 -3.5564294 -3.077394 -2.5164313 -1.6867614 -1.1682923 -0.82640219 0.012776375 -0.12990427 -1.3452458 -2.4074197 -3.0713775 -3.3474846 -3.1898131 -2.4258842][-3.50968 -3.5228481 -3.2099853 -2.8010383 -2.1303 -1.7139995 -1.5642927 -0.98503828 -1.002888 -1.9146624 -2.7943501 -3.3925042 -3.6980405 -3.5134957 -2.728507][-3.4528241 -3.4965186 -3.320112 -3.0513415 -2.565156 -2.2774203 -2.2320743 -1.8536427 -1.8049567 -2.4645174 -3.136405 -3.5864611 -3.8588867 -3.6984308 -2.9921315][-3.4139419 -3.4781017 -3.412055 -3.2887557 -3.0085092 -2.8361125 -2.77778 -2.46564 -2.3561969 -2.7926173 -3.2521791 -3.5782592 -3.8491359 -3.7614012 -3.1773825][-3.3175411 -3.4104013 -3.4393675 -3.4552853 -3.3722103 -3.2944064 -3.2007265 -2.9421427 -2.7912335 -3.0005031 -3.2216048 -3.4043107 -3.6596212 -3.6888378 -3.3401494]]...]
INFO - root - 2017-12-07 09:52:49.252669: step 26010, loss = 0.92, batch loss = 0.85 (10.5 examples/sec; 0.765 sec/batch; 65h:07m:35s remains)
INFO - root - 2017-12-07 09:52:56.985450: step 26020, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.771 sec/batch; 65h:38m:07s remains)
INFO - root - 2017-12-07 09:53:04.704433: step 26030, loss = 0.76, batch loss = 0.68 (10.3 examples/sec; 0.774 sec/batch; 65h:52m:41s remains)
INFO - root - 2017-12-07 09:53:12.148409: step 26040, loss = 0.90, batch loss = 0.83 (10.8 examples/sec; 0.738 sec/batch; 62h:50m:08s remains)
INFO - root - 2017-12-07 09:53:19.713418: step 26050, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.768 sec/batch; 65h:23m:28s remains)
INFO - root - 2017-12-07 09:53:27.519027: step 26060, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.772 sec/batch; 65h:44m:06s remains)
INFO - root - 2017-12-07 09:53:35.182480: step 26070, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.760 sec/batch; 64h:40m:50s remains)
INFO - root - 2017-12-07 09:53:42.745379: step 26080, loss = 0.66, batch loss = 0.58 (11.0 examples/sec; 0.730 sec/batch; 62h:10m:03s remains)
INFO - root - 2017-12-07 09:53:50.335476: step 26090, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.751 sec/batch; 63h:56m:28s remains)
INFO - root - 2017-12-07 09:53:57.986143: step 26100, loss = 1.00, batch loss = 0.92 (10.6 examples/sec; 0.758 sec/batch; 64h:28m:52s remains)
2017-12-07 09:53:58.606426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.80287 -3.1255867 -3.1987667 -2.8687749 -2.5620785 -2.381948 -2.10732 -2.0008593 -2.1687894 -1.975996 -1.65661 -1.7472405 -2.0578973 -2.08763 -2.0581212][-2.6512113 -3.0676918 -3.2192681 -3.0228143 -2.6676302 -2.2856231 -1.8897455 -1.766928 -2.031333 -2.0663047 -2.012558 -2.2112939 -2.3696771 -2.13913 -2.0185041][-2.9994097 -3.2441134 -3.2456746 -3.0764613 -2.8096118 -2.4121242 -1.9851408 -1.950948 -2.4080148 -2.7193027 -2.9321032 -3.1904602 -3.1999602 -2.7943273 -2.5352783][-3.5140686 -3.5285196 -3.344389 -3.1813505 -3.0406199 -2.6925316 -2.3478978 -2.5493107 -3.2167988 -3.6933422 -3.9924641 -4.1300211 -3.9402885 -3.3773394 -2.9774396][-3.3821282 -3.3151526 -3.0615444 -2.851918 -2.6726532 -2.2751966 -2.0170105 -2.5278199 -3.5233552 -4.290647 -4.7588634 -4.7954073 -4.3571067 -3.4740093 -2.78059][-2.4203725 -2.2967927 -1.9859724 -1.742517 -1.5212643 -1.1168387 -1.0043209 -1.8406072 -3.109971 -4.1028647 -4.7645245 -4.8440986 -4.3039584 -3.2074118 -2.2971354][-1.7557259 -1.5976927 -1.2084289 -0.85666633 -0.4231 0.069115162 -0.020051003 -1.1449041 -2.5626564 -3.6288996 -4.2647362 -4.25205 -3.6290574 -2.5115538 -1.6421571][-1.8651853 -1.5923295 -1.0382872 -0.49561024 0.34398222 1.0997529 0.83737469 -0.56777906 -2.2060308 -3.4000072 -3.9159336 -3.7192216 -3.010695 -1.934166 -1.2077353][-2.3672276 -2.0202153 -1.2994576 -0.54022336 0.70465612 1.6787176 1.2319727 -0.39883852 -2.2000668 -3.51897 -3.8946176 -3.4922147 -2.728054 -1.7151663 -1.0497482][-2.6591458 -2.3531656 -1.6158807 -0.7424469 0.77463007 1.9302726 1.3449473 -0.45054841 -2.3860402 -3.7054813 -3.888046 -3.2956376 -2.482954 -1.5050321 -0.90875077][-2.6745584 -2.428741 -1.8100326 -0.98381329 0.5620842 1.6968164 0.98401976 -0.89973664 -2.832706 -4.00743 -4.048389 -3.3443217 -2.4713387 -1.4369419 -0.83145905][-2.5475252 -2.3584242 -1.8983963 -1.206553 0.1365633 1.000916 0.19278336 -1.5863855 -3.2869692 -4.2095971 -4.1920233 -3.5246649 -2.5712724 -1.339282 -0.54289079][-2.3814192 -2.2025318 -1.8284261 -1.2647452 -0.23034525 0.29490948 -0.50515771 -2.0070715 -3.3465657 -4.0529532 -4.1274376 -3.6596706 -2.7060771 -1.3172374 -0.37761784][-2.143868 -1.9623246 -1.5962462 -1.1046526 -0.36697865 -0.13539028 -0.84056497 -1.9829688 -2.935818 -3.4643221 -3.6477866 -3.4483731 -2.6788669 -1.416882 -0.54006267][-2.0142941 -1.8841355 -1.5972815 -1.2383776 -0.76270032 -0.66232634 -1.1411564 -1.8503616 -2.4126079 -2.7568173 -2.968987 -2.9241581 -2.3509591 -1.3552439 -0.73065829]]...]
INFO - root - 2017-12-07 09:54:06.251776: step 26110, loss = 0.82, batch loss = 0.75 (10.8 examples/sec; 0.741 sec/batch; 63h:04m:47s remains)
INFO - root - 2017-12-07 09:54:13.992104: step 26120, loss = 0.74, batch loss = 0.66 (10.3 examples/sec; 0.777 sec/batch; 66h:06m:57s remains)
INFO - root - 2017-12-07 09:54:21.643497: step 26130, loss = 0.61, batch loss = 0.54 (10.6 examples/sec; 0.758 sec/batch; 64h:31m:21s remains)
INFO - root - 2017-12-07 09:54:29.094801: step 26140, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.785 sec/batch; 66h:47m:21s remains)
INFO - root - 2017-12-07 09:54:36.651193: step 26150, loss = 0.77, batch loss = 0.70 (10.7 examples/sec; 0.746 sec/batch; 63h:29m:21s remains)
INFO - root - 2017-12-07 09:54:44.295695: step 26160, loss = 0.75, batch loss = 0.67 (10.6 examples/sec; 0.755 sec/batch; 64h:14m:21s remains)
INFO - root - 2017-12-07 09:54:51.942104: step 26170, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.763 sec/batch; 64h:54m:03s remains)
INFO - root - 2017-12-07 09:54:59.616466: step 26180, loss = 0.76, batch loss = 0.68 (10.3 examples/sec; 0.778 sec/batch; 66h:12m:31s remains)
INFO - root - 2017-12-07 09:55:07.450981: step 26190, loss = 0.69, batch loss = 0.62 (10.6 examples/sec; 0.757 sec/batch; 64h:22m:26s remains)
INFO - root - 2017-12-07 09:55:15.133183: step 26200, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.778 sec/batch; 66h:11m:20s remains)
2017-12-07 09:55:15.776190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6411541 -2.6926246 -2.7447476 -2.8349037 -2.9504223 -3.0216169 -3.0870712 -3.1613131 -3.1964798 -3.1689644 -3.1383786 -3.1193361 -3.1309934 -3.1365075 -3.0620215][-2.6774416 -2.7585597 -2.8425279 -2.9685559 -3.1353641 -3.2406116 -3.3401442 -3.4834855 -3.5537667 -3.4962544 -3.4394948 -3.3856492 -3.3702049 -3.3781393 -3.3159869][-2.6384754 -2.6676965 -2.6831779 -2.7617776 -2.961153 -3.1127481 -3.2644331 -3.5295489 -3.7288444 -3.7367692 -3.7520416 -3.7293842 -3.7064314 -3.6942964 -3.6154566][-2.3061266 -2.1699698 -2.061218 -2.0766516 -2.3230238 -2.6010339 -2.8793278 -3.2770004 -3.6028826 -3.704747 -3.7923017 -3.8240063 -3.8431654 -3.880872 -3.8594403][-1.8175485 -1.4645462 -1.2223752 -1.1390834 -1.3462303 -1.6690593 -2.0173237 -2.4689612 -2.8585632 -3.0522943 -3.223439 -3.3838646 -3.5708559 -3.7600296 -3.8679082][-1.5084271 -1.044219 -0.69814658 -0.46118426 -0.45795918 -0.6051178 -0.84588432 -1.1798658 -1.5242503 -1.7778664 -2.0359466 -2.4442949 -3.0060604 -3.483418 -3.7537768][-1.4273148 -1.0285568 -0.714267 -0.41536903 -0.16220045 0.028020382 0.069508076 0.0091862679 -0.25939846 -0.58493614 -0.91183591 -1.5158899 -2.406275 -3.1036024 -3.4824998][-1.5423589 -1.175909 -0.88823366 -0.61888409 -0.23366117 0.18508959 0.53564262 0.79771185 0.5982399 0.1255064 -0.31728935 -0.99194789 -1.9421756 -2.6352077 -3.0365791][-1.7827981 -1.352587 -0.99063849 -0.75721836 -0.42589808 -0.059243202 0.3156023 0.70714331 0.62150574 0.18784142 -0.20452833 -0.74935961 -1.5152931 -2.0444446 -2.4466445][-2.1506295 -1.6206584 -1.1156831 -0.863698 -0.59694719 -0.39708805 -0.2568078 -0.081536293 -0.21802568 -0.532393 -0.75500989 -1.0343649 -1.4336889 -1.6998529 -2.035985][-2.6129911 -2.1376657 -1.6305032 -1.3564041 -1.0646842 -0.90310359 -0.94816613 -1.0603392 -1.35512 -1.6468241 -1.8016126 -1.8524799 -1.8681145 -1.8532288 -2.0334189][-2.9648578 -2.729285 -2.4557948 -2.2964187 -2.0514734 -1.8766146 -1.8918307 -2.0292625 -2.2889588 -2.5199754 -2.6518059 -2.5678697 -2.350744 -2.1917787 -2.2534351][-3.1834543 -3.2064056 -3.1880398 -3.1275582 -2.9177985 -2.6879969 -2.5792849 -2.6145496 -2.7686281 -2.9589636 -3.1196165 -3.0277019 -2.7421169 -2.5510268 -2.5580502][-3.2204671 -3.3792329 -3.5152843 -3.569844 -3.4676881 -3.2871933 -3.1352835 -3.0645761 -3.0672548 -3.1640968 -3.3144622 -3.2719479 -3.0219536 -2.834326 -2.7912557][-3.105103 -3.2710614 -3.4373896 -3.57166 -3.6135402 -3.5551949 -3.4496682 -3.3468075 -3.2420783 -3.2021379 -3.2513809 -3.2120447 -3.0288696 -2.8626847 -2.7931733]]...]
INFO - root - 2017-12-07 09:55:23.487569: step 26210, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.756 sec/batch; 64h:17m:29s remains)
INFO - root - 2017-12-07 09:55:31.154974: step 26220, loss = 0.75, batch loss = 0.67 (10.5 examples/sec; 0.763 sec/batch; 64h:55m:29s remains)
INFO - root - 2017-12-07 09:55:38.841781: step 26230, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.757 sec/batch; 64h:22m:18s remains)
INFO - root - 2017-12-07 09:55:46.217069: step 26240, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.761 sec/batch; 64h:45m:12s remains)
INFO - root - 2017-12-07 09:55:53.826003: step 26250, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.749 sec/batch; 63h:41m:47s remains)
INFO - root - 2017-12-07 09:56:01.487907: step 26260, loss = 0.99, batch loss = 0.92 (10.3 examples/sec; 0.777 sec/batch; 66h:05m:32s remains)
INFO - root - 2017-12-07 09:56:09.134455: step 26270, loss = 0.98, batch loss = 0.90 (10.6 examples/sec; 0.751 sec/batch; 63h:54m:53s remains)
INFO - root - 2017-12-07 09:56:16.758078: step 26280, loss = 0.70, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 65h:57m:29s remains)
INFO - root - 2017-12-07 09:56:24.333842: step 26290, loss = 1.00, batch loss = 0.93 (10.6 examples/sec; 0.752 sec/batch; 63h:56m:30s remains)
INFO - root - 2017-12-07 09:56:32.006165: step 26300, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.749 sec/batch; 63h:41m:09s remains)
2017-12-07 09:56:32.664789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8701763 -2.929111 -2.3075094 -1.9259675 -2.3286507 -2.652045 -2.216697 -1.6437628 -1.7763126 -2.20799 -2.0630276 -1.2013595 -0.36243153 -0.56440878 -1.802191][-2.0593851 -2.4731126 -2.2589846 -2.2547889 -2.8322954 -2.9999468 -2.2228303 -1.4217019 -1.4944029 -1.774951 -1.3460569 -0.23796654 0.62024164 0.37216568 -0.883615][-1.8432648 -2.4347839 -2.5272458 -2.8322382 -3.6888237 -3.9693387 -3.1432295 -2.3452146 -2.4070296 -2.4832425 -1.7908854 -0.69036245 -0.16529703 -0.52625608 -1.4565172][-1.6559603 -2.0191586 -2.0108731 -2.374054 -3.491365 -4.0851412 -3.5271544 -3.0436707 -3.4322228 -3.620579 -2.8101273 -1.8001208 -1.7220609 -2.3529093 -2.9831309][-1.8042657 -1.7544308 -1.4010351 -1.5496373 -2.60748 -3.2354889 -2.7766852 -2.57763 -3.4941521 -4.1021209 -3.3519351 -2.3577087 -2.5098381 -3.4055765 -3.9860861][-2.286428 -1.7588787 -0.93167758 -0.59960771 -1.2189038 -1.4118254 -0.60717416 -0.48743272 -1.9725292 -3.1835654 -2.7912173 -2.0450637 -2.439254 -3.5133162 -4.0595846][-3.0856018 -2.173281 -0.95899415 -0.11784363 -0.071395874 0.44133186 1.882875 2.2088418 0.36111021 -1.2876117 -1.3201787 -1.0423563 -1.7184639 -2.8807757 -3.3755221][-3.9291735 -2.9749045 -1.7990479 -0.86386943 -0.39145756 0.61603403 2.5673456 3.3198047 1.6859365 0.14643812 -0.065536976 -0.15522385 -0.99385452 -1.9997799 -2.3469162][-4.208746 -3.3984947 -2.4661324 -1.7278881 -1.2083845 -0.28966236 1.371491 2.197124 1.1540976 0.093232155 -0.0577178 -0.37839413 -1.3115828 -2.1200535 -2.320828][-4.3803062 -3.6932764 -3.0073776 -2.5148454 -1.9860396 -1.2092378 -0.063250065 0.49270868 -0.26434851 -1.0686731 -1.2429602 -1.7469783 -2.6965837 -3.3165712 -3.3665733][-4.4742813 -3.8920567 -3.4428771 -3.2183948 -2.7484243 -2.1466398 -1.5113623 -1.2811706 -1.8862567 -2.5007339 -2.6882429 -3.2336636 -4.0256968 -4.4371719 -4.3734975][-4.2048626 -3.7643344 -3.5435228 -3.5374227 -3.1972604 -2.8344383 -2.6099803 -2.5663733 -2.936202 -3.2315531 -3.2375188 -3.5473604 -4.0495644 -4.3709793 -4.4631805][-3.7732983 -3.5228338 -3.50495 -3.6447213 -3.4741037 -3.2853153 -3.1734426 -3.0579522 -3.0840843 -2.9707041 -2.6863642 -2.6958356 -2.9312599 -3.2319312 -3.5791142][-3.3046961 -3.2334583 -3.3261523 -3.4744425 -3.4361908 -3.4232588 -3.3622804 -3.1932812 -3.0427592 -2.7207208 -2.3015881 -2.1210589 -2.1658554 -2.3816831 -2.7635422][-3.0544531 -3.1024456 -3.2191002 -3.292985 -3.2914031 -3.3560023 -3.3156133 -3.1635137 -3.0249076 -2.77426 -2.4559126 -2.2441511 -2.1684263 -2.2368574 -2.4549179]]...]
INFO - root - 2017-12-07 09:56:40.347693: step 26310, loss = 0.98, batch loss = 0.91 (10.6 examples/sec; 0.758 sec/batch; 64h:27m:01s remains)
INFO - root - 2017-12-07 09:56:48.009395: step 26320, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.764 sec/batch; 64h:58m:27s remains)
INFO - root - 2017-12-07 09:56:55.637755: step 26330, loss = 0.84, batch loss = 0.76 (10.7 examples/sec; 0.750 sec/batch; 63h:48m:42s remains)
INFO - root - 2017-12-07 09:57:03.036994: step 26340, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 64h:54m:56s remains)
INFO - root - 2017-12-07 09:57:10.607578: step 26350, loss = 0.67, batch loss = 0.60 (10.5 examples/sec; 0.763 sec/batch; 64h:54m:48s remains)
INFO - root - 2017-12-07 09:57:18.309174: step 26360, loss = 0.89, batch loss = 0.82 (10.3 examples/sec; 0.779 sec/batch; 66h:14m:21s remains)
INFO - root - 2017-12-07 09:57:25.920598: step 26370, loss = 0.77, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 64h:01m:18s remains)
INFO - root - 2017-12-07 09:57:33.485436: step 26380, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.763 sec/batch; 64h:50m:16s remains)
INFO - root - 2017-12-07 09:57:41.087166: step 26390, loss = 0.86, batch loss = 0.78 (10.1 examples/sec; 0.792 sec/batch; 67h:18m:45s remains)
INFO - root - 2017-12-07 09:57:48.755512: step 26400, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 64h:47m:17s remains)
2017-12-07 09:57:49.384883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1047654 -4.2702942 -4.4304442 -4.5070882 -4.5189137 -4.5069623 -4.4961228 -4.5094519 -4.5753856 -4.6736951 -4.6929488 -4.60286 -4.403173 -4.2000451 -4.0047913][-4.2939076 -4.50905 -4.7057805 -4.7503591 -4.686193 -4.5791283 -4.4704185 -4.404038 -4.4282179 -4.5929637 -4.7424994 -4.709012 -4.4387307 -4.1259847 -3.8262842][-4.1869478 -4.3388095 -4.4438453 -4.3402534 -4.10904 -3.8280046 -3.5739734 -3.4516926 -3.5422215 -3.9252317 -4.3434916 -4.4509616 -4.1383085 -3.724633 -3.3689818][-3.854867 -3.8575454 -3.8213592 -3.572078 -3.1740661 -2.7163274 -2.3160164 -2.2140043 -2.5058498 -3.1933498 -3.893527 -4.1426268 -3.8092377 -3.2934811 -2.8832283][-3.5129411 -3.4526639 -3.4126565 -3.2285237 -2.8497071 -2.3133252 -1.7593162 -1.664129 -2.1217332 -2.9670386 -3.791985 -4.1543403 -3.8364244 -3.2048931 -2.6949396][-3.0078487 -2.8588772 -2.8242357 -2.722466 -2.3659329 -1.7197726 -1.0000913 -0.99547482 -1.6962223 -2.6522932 -3.48952 -3.9395051 -3.7239347 -3.0891597 -2.5666275][-2.6006424 -2.2844055 -2.0770154 -1.7802439 -1.1313901 -0.042864323 0.96004915 0.71810722 -0.44881797 -1.6739163 -2.6163371 -3.2147584 -3.1700349 -2.662133 -2.2467048][-2.4999304 -2.1190104 -1.7719495 -1.1814601 -0.059571266 1.6293116 2.9126954 2.3402863 0.73493338 -0.767647 -1.8406332 -2.569665 -2.7031448 -2.3574686 -2.0677254][-2.6141605 -2.300894 -1.9742761 -1.27512 0.0064921379 1.7387738 2.7439208 1.950942 0.40455008 -0.92906404 -1.8584373 -2.5308003 -2.7115366 -2.4440289 -2.1934769][-2.8878405 -2.7350512 -2.5154338 -1.8681912 -0.77451205 0.46290302 0.87858677 0.010546207 -1.1544523 -2.0575595 -2.6734576 -3.130012 -3.2065513 -2.8776097 -2.538012][-3.0988612 -3.1239843 -3.0493128 -2.533257 -1.7395744 -1.0480249 -1.041132 -1.7790036 -2.5710063 -3.1609714 -3.583703 -3.8583515 -3.7803202 -3.3358288 -2.91417][-3.1615658 -3.2164454 -3.2076049 -2.8421533 -2.3187654 -2.0154011 -2.2011762 -2.7696812 -3.3045635 -3.7173305 -4.0135326 -4.1362786 -3.959446 -3.5071094 -3.172612][-3.30233 -3.2854004 -3.2376752 -2.9890561 -2.6796322 -2.5997081 -2.8234682 -3.2184553 -3.5759025 -3.8385913 -3.99845 -3.9948008 -3.7894318 -3.466028 -3.3386836][-3.6157777 -3.5895262 -3.5647361 -3.465759 -3.3448114 -3.3747544 -3.566283 -3.8152997 -4.0009131 -4.0944123 -4.0931826 -3.9978726 -3.8120451 -3.6309495 -3.6268601][-3.900372 -3.9258072 -3.9521232 -3.9690275 -3.9771624 -4.0424414 -4.1840773 -4.3452978 -4.4532218 -4.473906 -4.4036856 -4.2612019 -4.069499 -3.9278231 -3.9151888]]...]
INFO - root - 2017-12-07 09:57:56.950730: step 26410, loss = 0.76, batch loss = 0.69 (10.9 examples/sec; 0.735 sec/batch; 62h:30m:45s remains)
INFO - root - 2017-12-07 09:58:04.616563: step 26420, loss = 0.80, batch loss = 0.73 (10.0 examples/sec; 0.802 sec/batch; 68h:12m:42s remains)
INFO - root - 2017-12-07 09:58:12.226839: step 26430, loss = 1.12, batch loss = 1.04 (10.8 examples/sec; 0.741 sec/batch; 62h:58m:55s remains)
INFO - root - 2017-12-07 09:58:19.655216: step 26440, loss = 0.77, batch loss = 0.70 (10.8 examples/sec; 0.743 sec/batch; 63h:09m:09s remains)
INFO - root - 2017-12-07 09:58:27.372082: step 26450, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.772 sec/batch; 65h:39m:42s remains)
INFO - root - 2017-12-07 09:58:35.044704: step 26460, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.780 sec/batch; 66h:19m:18s remains)
INFO - root - 2017-12-07 09:58:42.624996: step 26470, loss = 0.64, batch loss = 0.56 (10.5 examples/sec; 0.760 sec/batch; 64h:38m:55s remains)
INFO - root - 2017-12-07 09:58:50.257014: step 26480, loss = 0.95, batch loss = 0.88 (10.6 examples/sec; 0.752 sec/batch; 63h:55m:09s remains)
INFO - root - 2017-12-07 09:58:57.993878: step 26490, loss = 0.66, batch loss = 0.59 (10.1 examples/sec; 0.796 sec/batch; 67h:38m:58s remains)
INFO - root - 2017-12-07 09:59:05.742790: step 26500, loss = 0.62, batch loss = 0.54 (10.2 examples/sec; 0.787 sec/batch; 66h:53m:59s remains)
2017-12-07 09:59:06.541557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2159514 -3.0769517 -2.8738766 -2.5694723 -2.5738933 -2.8338704 -3.0716815 -3.1405189 -2.957783 -2.6864281 -2.5527306 -2.6045392 -2.8678255 -3.1088672 -3.3052771][-3.6930404 -3.3891971 -2.941844 -2.5267408 -2.5206237 -2.7453947 -2.9803505 -3.1023049 -2.907093 -2.6942525 -2.7177296 -2.8882568 -3.2063279 -3.4450662 -3.5972285][-4.0008183 -3.764946 -3.3549056 -2.99082 -2.9348795 -3.0003314 -3.1099644 -3.1725693 -2.9587135 -2.8315823 -3.0373924 -3.3560753 -3.7272086 -3.915966 -3.8811064][-4.0383925 -3.829746 -3.4249763 -3.021771 -2.8380635 -2.7879643 -2.8147857 -2.837822 -2.6323843 -2.5963678 -2.9519687 -3.3909109 -3.8309805 -4.0312562 -3.9179273][-3.5449362 -3.371341 -3.0265551 -2.7244861 -2.6621628 -2.6970119 -2.7564068 -2.7835467 -2.6356921 -2.673533 -3.0513 -3.5011148 -3.8624501 -3.9313445 -3.7032652][-2.9123564 -2.848578 -2.6647358 -2.5690379 -2.6980667 -2.7593579 -2.6164641 -2.3585327 -2.0907958 -2.0862536 -2.3284469 -2.5550678 -2.5981345 -2.3771083 -2.0012677][-2.6660135 -2.5422163 -2.2444668 -2.0417702 -2.0387957 -1.8456857 -1.298573 -0.70599627 -0.40139198 -0.50745487 -0.7000792 -0.69235086 -0.45285869 -0.11456013 0.090637207][-2.248121 -2.0730398 -1.6882498 -1.3869739 -1.2122357 -0.80996513 -0.066094875 0.5084362 0.45712519 -0.13518763 -0.70977521 -0.97804165 -1.0070815 -1.0193124 -1.2382164][-1.600013 -1.6381891 -1.6212068 -1.7006927 -1.8051496 -1.6853311 -1.3288498 -1.1807523 -1.4698384 -2.0844543 -2.6416354 -2.9588199 -3.124316 -3.2750454 -3.5194952][-2.2683153 -2.468606 -2.7253318 -3.0354767 -3.3028626 -3.3865025 -3.3523953 -3.4366043 -3.6513267 -3.9401031 -4.14785 -4.2531996 -4.3249125 -4.352725 -4.360971][-3.2907305 -3.5251708 -3.7954667 -4.0538893 -4.2656016 -4.3466511 -4.3288736 -4.3678818 -4.4453678 -4.5234985 -4.5398235 -4.5472827 -4.6223745 -4.6436205 -4.5388074][-3.32387 -3.5485137 -3.7350583 -3.8987558 -4.1007729 -4.2299075 -4.2448511 -4.319726 -4.4594965 -4.5370278 -4.4392395 -4.2841964 -4.2491374 -4.2427225 -4.1330705][-2.7780156 -3.0035014 -3.1102405 -3.1577 -3.3124464 -3.4640126 -3.5529151 -3.7504318 -4.0821867 -4.3817091 -4.4176397 -4.2510662 -4.1425786 -4.1271348 -4.1017895][-2.6801922 -2.8850741 -2.9553466 -2.9405141 -3.0559392 -3.2271013 -3.3558884 -3.5375676 -3.8439302 -4.2190204 -4.3980236 -4.33034 -4.2453413 -4.2542925 -4.2962456][-3.2194147 -3.2943707 -3.2844033 -3.225841 -3.2860541 -3.4034796 -3.482358 -3.5326989 -3.6386952 -3.8749301 -4.0527191 -4.04235 -3.9655368 -3.9198124 -3.9299474]]...]
INFO - root - 2017-12-07 09:59:14.241518: step 26510, loss = 0.69, batch loss = 0.62 (9.9 examples/sec; 0.805 sec/batch; 68h:27m:27s remains)
INFO - root - 2017-12-07 09:59:21.873008: step 26520, loss = 0.69, batch loss = 0.61 (10.6 examples/sec; 0.755 sec/batch; 64h:09m:00s remains)
INFO - root - 2017-12-07 09:59:29.474717: step 26530, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.755 sec/batch; 64h:12m:11s remains)
INFO - root - 2017-12-07 09:59:36.907648: step 26540, loss = 0.72, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 64h:31m:26s remains)
INFO - root - 2017-12-07 09:59:44.680905: step 26550, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.781 sec/batch; 66h:23m:07s remains)
INFO - root - 2017-12-07 09:59:52.470105: step 26560, loss = 0.75, batch loss = 0.67 (10.4 examples/sec; 0.769 sec/batch; 65h:18m:42s remains)
INFO - root - 2017-12-07 10:00:00.045297: step 26570, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.757 sec/batch; 64h:21m:57s remains)
INFO - root - 2017-12-07 10:00:07.804106: step 26580, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.794 sec/batch; 67h:29m:59s remains)
INFO - root - 2017-12-07 10:00:15.599193: step 26590, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.770 sec/batch; 65h:25m:41s remains)
INFO - root - 2017-12-07 10:00:23.458432: step 26600, loss = 0.89, batch loss = 0.82 (9.8 examples/sec; 0.819 sec/batch; 69h:37m:37s remains)
2017-12-07 10:00:24.060766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7574258 -3.8264627 -3.903491 -3.9652805 -4.0117598 -4.008141 -3.9345815 -3.8260422 -3.7465229 -3.7354779 -3.7475107 -3.6978846 -3.6246684 -3.5569808 -3.5191531][-3.6027493 -3.6329937 -3.6701789 -3.7028232 -3.733222 -3.7184205 -3.6270778 -3.4914629 -3.4075937 -3.4188848 -3.4623811 -3.4310551 -3.3709426 -3.3457894 -3.3851233][-3.3091969 -3.2740462 -3.26442 -3.2674093 -3.2913642 -3.3178067 -3.2644308 -3.1048813 -2.9984536 -3.010046 -3.0751324 -3.0912189 -3.083461 -3.0922973 -3.1787391][-3.0538836 -2.9628665 -2.9044709 -2.8427746 -2.8081229 -2.8828607 -2.9032903 -2.7607098 -2.6608558 -2.6845746 -2.771255 -2.8695209 -2.9381027 -2.9551907 -3.0721278][-2.8176656 -2.7001595 -2.6569972 -2.5593472 -2.41513 -2.4248586 -2.4243474 -2.311923 -2.3261018 -2.4079194 -2.4705763 -2.5773935 -2.6649592 -2.7046256 -2.908844][-2.6017222 -2.4617996 -2.4547095 -2.3899236 -2.1892614 -2.0349469 -1.773844 -1.5262599 -1.6792874 -1.9445519 -2.0440576 -2.0443807 -1.9408779 -1.8612003 -2.0791347][-2.5134997 -2.3529062 -2.3314505 -2.3192601 -2.1975162 -1.9854784 -1.4836009 -1.0244839 -1.2189133 -1.6641052 -1.9135573 -1.9138577 -1.6552918 -1.4038796 -1.4815152][-2.4718029 -2.3602903 -2.3384788 -2.3554184 -2.3818657 -2.314199 -1.9314094 -1.5503602 -1.6763103 -2.0408146 -2.3096225 -2.3851359 -2.2109458 -1.9885681 -1.9108088][-2.3438332 -2.3972051 -2.5091131 -2.6236336 -2.776063 -2.8348994 -2.6571078 -2.48581 -2.5808043 -2.7450733 -2.8608027 -2.8598561 -2.6897721 -2.494159 -2.3718278][-2.0653834 -2.3804727 -2.7073636 -2.9748909 -3.2260842 -3.3594904 -3.3329144 -3.3275468 -3.4177914 -3.4712477 -3.469435 -3.4158335 -3.2763915 -3.1436086 -3.0736878][-1.8799484 -2.3850555 -2.8121567 -3.1559627 -3.4569032 -3.6514065 -3.7328868 -3.8078079 -3.8713663 -3.8720968 -3.8340936 -3.7968335 -3.7527418 -3.7456751 -3.7713504][-2.1958454 -2.7217379 -3.0717757 -3.3397255 -3.5776329 -3.7320487 -3.8090463 -3.847764 -3.8589246 -3.8373356 -3.7965002 -3.7508597 -3.7169003 -3.7326772 -3.8241372][-2.7104044 -3.1100264 -3.2798257 -3.3919032 -3.5198889 -3.6130815 -3.6691966 -3.6952715 -3.7054775 -3.706212 -3.7035775 -3.7052791 -3.7331395 -3.7907619 -3.89858][-3.0923076 -3.2589965 -3.2338972 -3.2113059 -3.2617073 -3.3245323 -3.3802905 -3.4186072 -3.4461141 -3.4771609 -3.5240512 -3.5921116 -3.6955631 -3.8066592 -3.9055209][-3.4717081 -3.4514308 -3.3194933 -3.2270913 -3.2101049 -3.2255182 -3.2528958 -3.2759614 -3.2932684 -3.3133693 -3.3424649 -3.391747 -3.4755247 -3.5724723 -3.6566656]]...]
INFO - root - 2017-12-07 10:00:31.736263: step 26610, loss = 0.57, batch loss = 0.50 (10.8 examples/sec; 0.743 sec/batch; 63h:08m:15s remains)
INFO - root - 2017-12-07 10:00:39.527304: step 26620, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 65h:14m:38s remains)
INFO - root - 2017-12-07 10:00:47.111854: step 26630, loss = 0.79, batch loss = 0.71 (10.5 examples/sec; 0.761 sec/batch; 64h:40m:23s remains)
INFO - root - 2017-12-07 10:00:54.466800: step 26640, loss = 0.66, batch loss = 0.59 (10.3 examples/sec; 0.776 sec/batch; 65h:54m:25s remains)
INFO - root - 2017-12-07 10:01:02.235089: step 26650, loss = 0.89, batch loss = 0.81 (9.6 examples/sec; 0.830 sec/batch; 70h:29m:37s remains)
INFO - root - 2017-12-07 10:01:09.975613: step 26660, loss = 0.81, batch loss = 0.74 (10.8 examples/sec; 0.743 sec/batch; 63h:06m:06s remains)
INFO - root - 2017-12-07 10:01:17.622525: step 26670, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.774 sec/batch; 65h:44m:31s remains)
INFO - root - 2017-12-07 10:01:25.238427: step 26680, loss = 0.88, batch loss = 0.81 (10.3 examples/sec; 0.775 sec/batch; 65h:51m:05s remains)
INFO - root - 2017-12-07 10:01:32.832015: step 26690, loss = 1.09, batch loss = 1.02 (10.9 examples/sec; 0.733 sec/batch; 62h:17m:37s remains)
INFO - root - 2017-12-07 10:01:40.465390: step 26700, loss = 0.76, batch loss = 0.68 (10.5 examples/sec; 0.762 sec/batch; 64h:43m:32s remains)
2017-12-07 10:01:41.124867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1391115 -2.1165159 -2.1818025 -2.199522 -2.1194847 -2.1083093 -2.1091418 -2.06031 -2.0202231 -1.9798572 -2.0002694 -2.1431549 -2.2042615 -2.1678562 -2.1749103][-2.0821357 -2.1217239 -2.2666981 -2.3489997 -2.2604787 -2.256654 -2.2848916 -2.2618074 -2.2502365 -2.2157102 -2.2603548 -2.4457097 -2.4773345 -2.3638148 -2.2784169][-2.0912986 -2.1952043 -2.4512014 -2.643904 -2.570796 -2.5116963 -2.4738216 -2.4315939 -2.4924436 -2.5431337 -2.6715269 -2.9246907 -2.9354806 -2.7313533 -2.4896479][-2.0113139 -2.1070969 -2.4227324 -2.7515466 -2.765939 -2.6873431 -2.5519071 -2.4003637 -2.4440308 -2.5484283 -2.7106452 -2.9761872 -2.9778266 -2.7479639 -2.3987222][-1.9477596 -1.9650247 -2.25319 -2.639379 -2.6795373 -2.5619786 -2.3177369 -2.0237834 -2.0462379 -2.2430673 -2.4586833 -2.7064919 -2.6993928 -2.497386 -2.1005335][-1.899812 -1.854125 -2.0994782 -2.4875951 -2.4700751 -2.2358627 -1.823179 -1.2971017 -1.3363473 -1.7610846 -2.152678 -2.4388561 -2.4810815 -2.3529365 -1.9559822][-1.7546718 -1.6251862 -1.7970233 -2.1316862 -2.0496452 -1.7345796 -1.17291 -0.42545128 -0.50503778 -1.21965 -1.8575237 -2.2743301 -2.4174504 -2.3223569 -1.9352946][-1.5938144 -1.3593946 -1.4213352 -1.6647882 -1.5465446 -1.2373061 -0.63407826 0.21030235 0.10147619 -0.78490543 -1.5770915 -2.0990064 -2.3023319 -2.2037861 -1.8496032][-1.528228 -1.2529523 -1.29157 -1.5681472 -1.5555129 -1.3876147 -0.90190887 -0.18668842 -0.25239658 -0.99668527 -1.6576717 -2.089927 -2.2099841 -2.0392902 -1.7471142][-1.6834986 -1.4440281 -1.5485978 -1.9372509 -2.0533841 -2.0320985 -1.6899745 -1.1197119 -1.1353111 -1.6963184 -2.2206438 -2.5718031 -2.5770488 -2.2820978 -1.9465551][-2.0947983 -1.9073067 -2.0782235 -2.4949028 -2.6342773 -2.6445723 -2.3948092 -1.9193199 -1.8969114 -2.3039107 -2.7463999 -3.1067147 -3.1405902 -2.8246188 -2.4260252][-2.6021035 -2.4516666 -2.6326141 -2.980267 -3.0553346 -3.0477848 -2.8758101 -2.4927711 -2.4244123 -2.6743174 -2.9785931 -3.3070598 -3.3878298 -3.164865 -2.8121114][-2.9468617 -2.7938271 -2.9325957 -3.1853957 -3.2024095 -3.19957 -3.1201255 -2.8535657 -2.740016 -2.8231325 -2.9605 -3.2116373 -3.306499 -3.1962833 -2.9693646][-3.0236816 -2.8709426 -2.9540768 -3.1346064 -3.1578283 -3.199173 -3.2220669 -3.0826652 -2.9482896 -2.885848 -2.871892 -3.0152318 -3.1134732 -3.082469 -2.9646072][-2.8636737 -2.698622 -2.7090445 -2.8031859 -2.8341877 -2.9063935 -2.9857655 -2.9675376 -2.883215 -2.7947659 -2.752058 -2.8361554 -2.9234078 -2.9195342 -2.85755]]...]
INFO - root - 2017-12-07 10:01:48.765785: step 26710, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.761 sec/batch; 64h:35m:53s remains)
INFO - root - 2017-12-07 10:01:56.539691: step 26720, loss = 0.65, batch loss = 0.58 (10.1 examples/sec; 0.796 sec/batch; 67h:36m:23s remains)
INFO - root - 2017-12-07 10:02:04.186440: step 26730, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.773 sec/batch; 65h:41m:31s remains)
INFO - root - 2017-12-07 10:02:11.688256: step 26740, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.760 sec/batch; 64h:31m:00s remains)
INFO - root - 2017-12-07 10:02:19.391641: step 26750, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.772 sec/batch; 65h:36m:18s remains)
INFO - root - 2017-12-07 10:02:27.039141: step 26760, loss = 0.99, batch loss = 0.92 (10.5 examples/sec; 0.761 sec/batch; 64h:35m:23s remains)
INFO - root - 2017-12-07 10:02:34.740811: step 26770, loss = 0.78, batch loss = 0.70 (10.6 examples/sec; 0.752 sec/batch; 63h:51m:48s remains)
INFO - root - 2017-12-07 10:02:42.524021: step 26780, loss = 0.98, batch loss = 0.91 (10.2 examples/sec; 0.783 sec/batch; 66h:29m:46s remains)
INFO - root - 2017-12-07 10:02:50.113595: step 26790, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.761 sec/batch; 64h:37m:05s remains)
INFO - root - 2017-12-07 10:02:57.820226: step 26800, loss = 0.98, batch loss = 0.90 (10.6 examples/sec; 0.751 sec/batch; 63h:48m:04s remains)
2017-12-07 10:02:58.486073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8647597 -3.4586451 -3.2246718 -3.0994422 -2.9184213 -2.7984657 -2.7566342 -2.7846174 -2.9513493 -3.2163754 -3.37904 -3.3911271 -3.341444 -3.2563326 -2.9563155][-3.8296027 -3.4129014 -3.189482 -3.0926085 -2.9358866 -2.7441521 -2.6405299 -2.6411653 -2.74484 -2.8846383 -2.9621091 -3.0164795 -3.0523791 -3.0267076 -2.7330663][-3.8109653 -3.4135306 -3.1950216 -3.0136304 -2.7663357 -2.5485132 -2.5195985 -2.5966864 -2.659935 -2.5567858 -2.4135637 -2.4982574 -2.6611986 -2.7483182 -2.5385509][-3.8130493 -3.4335933 -3.1952488 -2.8485482 -2.4186726 -2.2100015 -2.242661 -2.2877147 -2.26027 -1.9340305 -1.579401 -1.6975691 -1.9609745 -2.1666508 -2.1742663][-3.8202095 -3.433392 -3.1410565 -2.6768808 -2.1459739 -1.9780974 -1.8674841 -1.5842688 -1.4212654 -1.0904386 -0.72220111 -0.86676717 -1.0812433 -1.2462943 -1.4433458][-3.8129225 -3.4422054 -3.1401472 -2.6489811 -2.0140085 -1.6565289 -1.1001067 -0.36513948 -0.34101057 -0.52145386 -0.56853676 -0.81596947 -0.89626741 -0.86320972 -1.0616891][-3.7556438 -3.4006362 -3.133734 -2.7328224 -2.0022895 -1.2596424 -0.17252636 0.95692873 0.65854883 -0.27464676 -0.84506655 -1.1869011 -1.2503083 -1.1890154 -1.3805187][-3.7009616 -3.3274481 -3.0365524 -2.6526237 -1.8560832 -0.92928243 0.10058403 0.99624205 0.45031023 -0.60616088 -1.0822814 -1.2835248 -1.4082384 -1.5120027 -1.8002713][-3.681932 -3.2884009 -2.9414811 -2.4875674 -1.6501791 -0.76867867 -0.28936052 -0.10474825 -0.61422586 -1.2013242 -1.2562459 -1.2890048 -1.564966 -1.8867633 -2.2442603][-3.6704926 -3.2270627 -2.83773 -2.3794007 -1.7146628 -1.1331992 -1.1248889 -1.2519753 -1.4041 -1.4768887 -1.3143795 -1.4566112 -1.9881234 -2.4716792 -2.7931194][-3.64438 -3.1493974 -2.7553949 -2.3915336 -1.9851556 -1.7189949 -1.8845143 -1.9903674 -1.8201067 -1.6658103 -1.6719866 -2.0516405 -2.6422315 -3.0788236 -3.2948933][-3.6400924 -3.1333337 -2.7979085 -2.6189418 -2.3734522 -2.1279714 -2.1130087 -2.0791855 -1.8796546 -1.8430793 -2.1438575 -2.6293631 -3.0781808 -3.318048 -3.3532391][-3.6595702 -3.2043591 -2.9605992 -2.9264219 -2.7084017 -2.2816155 -1.9685862 -1.8148549 -1.7527845 -1.8192036 -2.1170146 -2.4340878 -2.62047 -2.6886964 -2.6486096][-3.7235177 -3.3311541 -3.1014898 -3.0402765 -2.7136393 -2.090549 -1.6227624 -1.5268166 -1.6810741 -1.820529 -2.000392 -2.1069007 -2.0601614 -2.00869 -1.9232087][-3.7838769 -3.4386935 -3.1763394 -3.0111842 -2.5542784 -1.8322957 -1.3695002 -1.3572423 -1.6394553 -1.8241086 -1.9719577 -2.0269344 -1.8903191 -1.7363157 -1.5626185]]...]
INFO - root - 2017-12-07 10:03:06.114568: step 26810, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.777 sec/batch; 65h:58m:42s remains)
INFO - root - 2017-12-07 10:03:13.865091: step 26820, loss = 0.92, batch loss = 0.85 (10.2 examples/sec; 0.783 sec/batch; 66h:31m:04s remains)
INFO - root - 2017-12-07 10:03:21.444188: step 26830, loss = 0.78, batch loss = 0.70 (10.3 examples/sec; 0.776 sec/batch; 65h:54m:42s remains)
INFO - root - 2017-12-07 10:03:28.801515: step 26840, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 65h:28m:53s remains)
INFO - root - 2017-12-07 10:03:36.446224: step 26850, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.758 sec/batch; 64h:22m:30s remains)
INFO - root - 2017-12-07 10:03:44.092294: step 26860, loss = 0.83, batch loss = 0.75 (10.9 examples/sec; 0.735 sec/batch; 62h:26m:37s remains)
INFO - root - 2017-12-07 10:03:51.835503: step 26870, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 65h:32m:40s remains)
INFO - root - 2017-12-07 10:03:59.565862: step 26880, loss = 0.98, batch loss = 0.90 (10.7 examples/sec; 0.746 sec/batch; 63h:20m:16s remains)
INFO - root - 2017-12-07 10:04:07.190146: step 26890, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.778 sec/batch; 66h:01m:47s remains)
INFO - root - 2017-12-07 10:04:14.916775: step 26900, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.759 sec/batch; 64h:27m:05s remains)
2017-12-07 10:04:15.522291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1800518 -4.2190881 -4.2416458 -4.2618923 -4.2486267 -4.2083869 -4.1396337 -4.1092906 -4.129015 -4.1516657 -4.1671014 -4.16162 -4.0848126 -3.9871998 -3.9111176][-4.4437227 -4.4422412 -4.4076319 -4.3810692 -4.331388 -4.2609487 -4.1266418 -4.0405154 -4.0389509 -4.0693316 -4.1022749 -4.1522708 -4.1335011 -4.1014433 -4.11681][-4.0463824 -4.0497808 -4.0116878 -3.9382854 -3.8144038 -3.6716881 -3.4634953 -3.332659 -3.3275986 -3.3569655 -3.3976264 -3.48122 -3.4970949 -3.5108812 -3.62036][-3.5174913 -3.6812572 -3.8061442 -3.82901 -3.7031026 -3.5202851 -3.3059711 -3.1828089 -3.2209158 -3.2533069 -3.2955031 -3.386703 -3.3254089 -3.2259381 -3.2701149][-3.3014045 -3.5605867 -3.7734227 -3.8505869 -3.7155845 -3.4834049 -3.270328 -3.2024195 -3.3360004 -3.4420068 -3.5769446 -3.75843 -3.6267242 -3.3651938 -3.2782845][-3.5197794 -3.8426614 -4.0430951 -4.0831375 -3.8908873 -3.5311384 -3.2103367 -3.21306 -3.5329866 -3.7873678 -3.9952075 -4.1462512 -3.7811389 -3.1488256 -2.7486362][-3.1641989 -3.5563262 -3.7061939 -3.5656991 -3.1077695 -2.4012566 -1.7016492 -1.663367 -2.3000245 -2.9182355 -3.3452322 -3.5546994 -3.0859749 -2.2610593 -1.7084923][-2.3978612 -2.7125864 -2.7375669 -2.3421628 -1.5157957 -0.43525171 0.66066694 0.82617283 -0.12606573 -1.1596832 -1.9187238 -2.3771322 -2.1743875 -1.6228697 -1.3290985][-2.4724231 -2.6887941 -2.6549077 -2.206095 -1.3716857 -0.41766834 0.49834681 0.61904526 -0.35036707 -1.3618419 -2.0086653 -2.3995147 -2.2557125 -1.8351712 -1.6862721][-3.0368819 -3.2157459 -3.2733674 -3.0398335 -2.5452919 -2.0763688 -1.6690285 -1.6989064 -2.4329829 -3.0918322 -3.2960362 -3.3001194 -2.9176989 -2.3222103 -2.0587065][-3.4844768 -3.6383944 -3.7991276 -3.8365798 -3.6771035 -3.5683789 -3.4506283 -3.4687331 -3.8763714 -4.2154918 -4.1400261 -3.896204 -3.4045799 -2.7281582 -2.361264][-3.4975231 -3.5608091 -3.6701384 -3.7605011 -3.7246685 -3.737247 -3.7055757 -3.6148479 -3.7656753 -3.9540768 -3.8288083 -3.542011 -3.1180801 -2.5579977 -2.2344067][-2.9960351 -2.8865702 -2.8155618 -2.7935157 -2.7527175 -2.7615266 -2.765748 -2.6621971 -2.7020855 -2.8704257 -2.7976859 -2.5856121 -2.3613107 -2.0603664 -1.9264843][-2.3591309 -2.1533823 -1.9771922 -1.9124269 -1.8921185 -1.8718781 -1.8829632 -1.8235266 -1.8351531 -1.9781842 -1.9762733 -1.9163685 -1.9664221 -1.9982395 -2.1073811][-2.03915 -1.8965428 -1.7445171 -1.7376659 -1.8264329 -1.8885169 -1.9580884 -1.9750013 -2.0232732 -2.1512167 -2.1766675 -2.179425 -2.3482618 -2.5179935 -2.6792216]]...]
INFO - root - 2017-12-07 10:04:23.230324: step 26910, loss = 0.67, batch loss = 0.59 (10.3 examples/sec; 0.776 sec/batch; 65h:53m:48s remains)
INFO - root - 2017-12-07 10:04:30.931546: step 26920, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.759 sec/batch; 64h:26m:13s remains)
INFO - root - 2017-12-07 10:04:38.537579: step 26930, loss = 0.98, batch loss = 0.91 (10.2 examples/sec; 0.786 sec/batch; 66h:41m:07s remains)
INFO - root - 2017-12-07 10:04:45.921425: step 26940, loss = 0.96, batch loss = 0.89 (10.0 examples/sec; 0.797 sec/batch; 67h:40m:19s remains)
INFO - root - 2017-12-07 10:04:53.603082: step 26950, loss = 0.89, batch loss = 0.81 (10.4 examples/sec; 0.767 sec/batch; 65h:04m:32s remains)
INFO - root - 2017-12-07 10:05:01.382227: step 26960, loss = 1.01, batch loss = 0.93 (10.1 examples/sec; 0.791 sec/batch; 67h:09m:23s remains)
INFO - root - 2017-12-07 10:05:09.055239: step 26970, loss = 0.88, batch loss = 0.81 (10.1 examples/sec; 0.794 sec/batch; 67h:22m:08s remains)
INFO - root - 2017-12-07 10:05:16.612212: step 26980, loss = 0.71, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 64h:50m:09s remains)
INFO - root - 2017-12-07 10:05:24.245277: step 26990, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.772 sec/batch; 65h:31m:05s remains)
INFO - root - 2017-12-07 10:05:31.887193: step 27000, loss = 0.92, batch loss = 0.85 (10.6 examples/sec; 0.753 sec/batch; 63h:51m:38s remains)
2017-12-07 10:05:32.512332: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0111449 -0.86276007 -0.84169221 -0.7375288 -0.621845 -0.73309755 -1.1390765 -1.6185184 -1.8396213 -1.9462206 -1.9676731 -1.5332561 -1.5027356 -2.3706028 -2.9198918][-1.1412916 -1.1146619 -1.1489177 -1.0305519 -0.9232409 -0.96243525 -1.2260745 -1.5495379 -1.729964 -1.9111955 -1.8958571 -1.2983582 -1.1898625 -2.0982788 -2.6954112][-1.1120071 -1.1730056 -1.3342376 -1.292784 -1.3164096 -1.4229162 -1.5898738 -1.7173531 -1.7705891 -1.9867783 -1.9390988 -1.2382889 -1.1178815 -2.0417683 -2.6073427][-1.0159607 -1.1846662 -1.476306 -1.4881413 -1.6280148 -1.8906677 -2.0662024 -2.0510318 -1.9861717 -2.1937888 -2.0754149 -1.2758889 -1.1751623 -2.0985653 -2.5857892][-0.87005758 -1.1439378 -1.53563 -1.6038778 -1.8003743 -2.0963461 -2.163532 -1.9596632 -1.80373 -2.0385914 -1.9456296 -1.1972117 -1.1930883 -2.1180172 -2.5218315][-0.66246915 -0.98811746 -1.33793 -1.4073675 -1.5771902 -1.8031003 -1.6513538 -1.2188308 -1.0424912 -1.4002013 -1.4754403 -0.91663766 -1.1022961 -2.0756497 -2.4330153][-0.43721318 -0.83173847 -1.0589771 -0.97744942 -0.94057775 -0.96517754 -0.58446121 0.0003118515 0.020226955 -0.64743137 -0.970922 -0.58699393 -0.91651106 -1.9255815 -2.262584][-0.40448236 -0.8102355 -0.88009858 -0.60581994 -0.33043957 -0.18173933 0.34336376 0.98799849 0.79594326 -0.15607595 -0.600307 -0.30405474 -0.7108376 -1.6515975 -1.9442813][-0.70432425 -1.0088992 -0.86818385 -0.41256618 -0.079995155 -0.034622192 0.31280756 0.71922445 0.39501762 -0.42860317 -0.64748216 -0.28826237 -0.640671 -1.432888 -1.6987016][-1.1506031 -1.3190784 -1.0724089 -0.61192751 -0.39413548 -0.54343271 -0.42901278 -0.28927469 -0.65482879 -1.1420979 -1.0115876 -0.56357145 -0.78963637 -1.3960872 -1.6969929][-1.6827345 -1.7237775 -1.5203538 -1.2471287 -1.2838328 -1.5634856 -1.5038278 -1.415242 -1.6572657 -1.7297416 -1.3166199 -0.92681623 -1.1101832 -1.5772195 -1.9545999][-2.461566 -2.4542847 -2.2999027 -2.1505079 -2.2825663 -2.5067384 -2.3638313 -2.2434802 -2.3571296 -2.183259 -1.7087193 -1.4855807 -1.6585388 -1.9915009 -2.3744068][-3.3001637 -3.2974329 -3.18643 -3.087873 -3.1889496 -3.2713053 -3.0234885 -2.8411837 -2.8901134 -2.7022142 -2.39925 -2.4069164 -2.546088 -2.6800933 -2.936229][-3.9691241 -3.9666359 -3.8894362 -3.8366129 -3.9020779 -3.9158335 -3.6668029 -3.5096974 -3.5692911 -3.4517717 -3.3139706 -3.4255857 -3.4464238 -3.3586748 -3.4423621][-4.2412524 -4.2357683 -4.1923823 -4.1721392 -4.1959672 -4.1844559 -4.0159483 -3.9821811 -4.1575608 -4.1407566 -4.0801983 -4.1249852 -3.9427435 -3.6668005 -3.6571798]]...]
INFO - root - 2017-12-07 10:05:40.132776: step 27010, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 67h:44m:59s remains)
INFO - root - 2017-12-07 10:05:47.713157: step 27020, loss = 0.94, batch loss = 0.87 (10.6 examples/sec; 0.756 sec/batch; 64h:10m:54s remains)
INFO - root - 2017-12-07 10:05:55.383048: step 27030, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.763 sec/batch; 64h:43m:55s remains)
INFO - root - 2017-12-07 10:06:02.755963: step 27040, loss = 0.97, batch loss = 0.90 (10.8 examples/sec; 0.744 sec/batch; 63h:07m:17s remains)
INFO - root - 2017-12-07 10:06:10.505694: step 27050, loss = 0.61, batch loss = 0.54 (10.2 examples/sec; 0.781 sec/batch; 66h:13m:35s remains)
INFO - root - 2017-12-07 10:06:18.149391: step 27060, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.804 sec/batch; 68h:11m:59s remains)
INFO - root - 2017-12-07 10:06:25.744073: step 27070, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.762 sec/batch; 64h:37m:37s remains)
INFO - root - 2017-12-07 10:06:33.323749: step 27080, loss = 1.06, batch loss = 0.99 (10.4 examples/sec; 0.767 sec/batch; 65h:06m:05s remains)
INFO - root - 2017-12-07 10:06:40.850956: step 27090, loss = 0.82, batch loss = 0.74 (10.6 examples/sec; 0.757 sec/batch; 64h:11m:15s remains)
INFO - root - 2017-12-07 10:06:48.464765: step 27100, loss = 0.81, batch loss = 0.73 (10.5 examples/sec; 0.760 sec/batch; 64h:26m:10s remains)
2017-12-07 10:06:49.085421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1253541 -3.0712321 -2.9304953 -2.8288026 -2.7738233 -2.772753 -2.8368709 -2.8698318 -2.82572 -2.7692513 -2.7505322 -2.7659893 -2.7999053 -2.8161001 -2.832427][-3.0527506 -3.0242016 -2.8538766 -2.704098 -2.5974274 -2.5749655 -2.6874008 -2.7832887 -2.728169 -2.6262426 -2.6081634 -2.6900973 -2.8219345 -2.9006627 -2.9247265][-3.2696066 -3.2820892 -3.093339 -2.8986781 -2.7421169 -2.7266245 -2.9487481 -3.1230948 -3.023953 -2.8022037 -2.7223611 -2.8610113 -3.1204703 -3.3296113 -3.4400403][-3.660635 -3.6585431 -3.3918304 -3.08852 -2.8172708 -2.7749622 -3.0968692 -3.2862973 -3.068989 -2.6947124 -2.5409646 -2.7383513 -3.1197062 -3.4940217 -3.8160124][-3.8336105 -3.8981631 -3.6482654 -3.2750301 -2.8260031 -2.6731198 -3.0533767 -3.2756953 -3.0386028 -2.6537485 -2.472647 -2.6337943 -2.9419789 -3.2975113 -3.7809753][-3.9948134 -4.1113949 -3.9075284 -3.4575031 -2.774704 -2.3657269 -2.6670771 -2.9567008 -2.8974583 -2.6794741 -2.5117111 -2.51534 -2.5664411 -2.7503021 -3.2991986][-4.2545762 -4.3873835 -4.2206612 -3.664278 -2.715106 -1.9724779 -2.0427742 -2.3461637 -2.5531709 -2.6422238 -2.6373971 -2.6211314 -2.4762497 -2.4654617 -2.9512668][-4.028791 -4.1183529 -4.0364532 -3.553304 -2.5615458 -1.6622982 -1.5215459 -1.8025506 -2.2554774 -2.6019807 -2.8137245 -2.9298139 -2.7068887 -2.5293927 -2.831645][-3.622829 -3.5889406 -3.5169971 -3.1617846 -2.3336244 -1.5216985 -1.330816 -1.6529982 -2.2572224 -2.6880794 -3.0335479 -3.2853837 -3.019906 -2.6840312 -2.7388196][-3.454839 -3.335362 -3.2011893 -2.854619 -2.1876791 -1.5439436 -1.400213 -1.7594395 -2.34314 -2.6752117 -3.0800121 -3.4522722 -3.2330666 -2.9014027 -2.8519959][-3.7653518 -3.5798404 -3.2480373 -2.6842728 -2.0182722 -1.52865 -1.5028329 -1.8783684 -2.2591438 -2.3754489 -2.7771945 -3.2339783 -3.1713867 -3.0142536 -3.0451632][-3.9489138 -3.8433456 -3.537447 -2.8759985 -2.1635916 -1.6956146 -1.6909671 -1.9606159 -2.0616746 -2.0347459 -2.4651551 -3.0002599 -3.1187205 -3.1218491 -3.1876204][-3.7715633 -3.8302848 -3.8321493 -3.4114039 -2.7447908 -2.181284 -2.0012429 -2.0068524 -1.81781 -1.7295287 -2.1648078 -2.7252097 -2.9981968 -3.119297 -3.1909032][-3.4314399 -3.6066296 -3.9408102 -3.8417571 -3.2154884 -2.5005445 -2.1476965 -2.0543468 -1.8656948 -1.8617516 -2.2059603 -2.5778451 -2.7447076 -2.780077 -2.8560028][-3.0656726 -3.230176 -3.7542224 -3.9284749 -3.418654 -2.6463628 -2.2916248 -2.3564198 -2.4034119 -2.5259633 -2.6232405 -2.6029439 -2.4389632 -2.2482936 -2.293195]]...]
INFO - root - 2017-12-07 10:06:56.673007: step 27110, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.775 sec/batch; 65h:45m:55s remains)
INFO - root - 2017-12-07 10:07:04.352848: step 27120, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.776 sec/batch; 65h:51m:04s remains)
INFO - root - 2017-12-07 10:07:12.063622: step 27130, loss = 1.11, batch loss = 1.03 (10.7 examples/sec; 0.749 sec/batch; 63h:33m:32s remains)
INFO - root - 2017-12-07 10:07:19.341547: step 27140, loss = 0.70, batch loss = 0.63 (10.9 examples/sec; 0.734 sec/batch; 62h:16m:47s remains)
INFO - root - 2017-12-07 10:07:26.757538: step 27150, loss = 0.82, batch loss = 0.74 (11.0 examples/sec; 0.727 sec/batch; 61h:38m:39s remains)
INFO - root - 2017-12-07 10:07:34.448188: step 27160, loss = 0.66, batch loss = 0.58 (10.1 examples/sec; 0.794 sec/batch; 67h:21m:20s remains)
INFO - root - 2017-12-07 10:07:42.130251: step 27170, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.771 sec/batch; 65h:24m:53s remains)
INFO - root - 2017-12-07 10:07:49.859782: step 27180, loss = 0.70, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 67h:16m:20s remains)
INFO - root - 2017-12-07 10:07:57.525081: step 27190, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.753 sec/batch; 63h:51m:36s remains)
INFO - root - 2017-12-07 10:08:05.117764: step 27200, loss = 0.72, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 65h:03m:59s remains)
2017-12-07 10:08:05.816798: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6544046 -2.6191015 -2.6547484 -2.7288818 -2.8254862 -2.9650121 -3.0453634 -3.0816808 -3.2045312 -3.3282099 -3.3333423 -3.2702327 -3.247735 -3.25351 -3.2530456][-2.6980402 -2.6722155 -2.7136416 -2.7544727 -2.7999449 -2.9477878 -3.0615745 -3.1129661 -3.2887216 -3.4881134 -3.5148935 -3.4385781 -3.3665195 -3.3174195 -3.3060274][-2.7349432 -2.7468257 -2.7982478 -2.7676425 -2.6959133 -2.7605455 -2.8139634 -2.7986591 -2.9745412 -3.2735682 -3.3857946 -3.3426776 -3.2360172 -3.1043925 -3.0676889][-2.9477935 -2.9873912 -3.0077047 -2.8584003 -2.6142426 -2.5152469 -2.3801005 -2.1787717 -2.3302786 -2.8122506 -3.1638184 -3.2934594 -3.2332211 -3.0247097 -2.9203086][-3.2154722 -3.2513158 -3.2035813 -2.9248309 -2.5016627 -2.1806679 -1.7286344 -1.1985631 -1.2629011 -1.9638305 -2.6780879 -3.1479247 -3.2733469 -3.0568111 -2.889822][-3.302083 -3.3327947 -3.2660623 -2.9488902 -2.4065866 -1.8388188 -0.98148561 0.0016112328 0.15780067 -0.69826412 -1.7999783 -2.679882 -3.0837321 -2.9550896 -2.7861338][-3.2430325 -3.2786005 -3.2132473 -2.9022694 -2.2888026 -1.5186844 -0.33834696 0.99127531 1.3220196 0.37197256 -0.98597813 -2.1529746 -2.7769918 -2.7667127 -2.6257415][-3.2046938 -3.2648759 -3.2179844 -2.9879084 -2.43083 -1.6444707 -0.44613886 0.91650057 1.2880182 0.36912727 -0.91715693 -2.036649 -2.6513638 -2.6385207 -2.4358373][-3.2098448 -3.2768459 -3.1930788 -3.0115235 -2.6163945 -2.1159911 -1.3441706 -0.39111423 -0.13547182 -0.77783656 -1.6189773 -2.3637624 -2.7794023 -2.6437511 -2.2572191][-3.1665463 -3.27255 -3.1351793 -2.9146757 -2.6013167 -2.4227388 -2.2160859 -1.7861712 -1.6465075 -1.8844435 -2.1293633 -2.4117761 -2.6200004 -2.3847649 -1.859756][-2.9702125 -3.2505977 -3.1980209 -2.9907598 -2.7236533 -2.7280254 -2.879725 -2.7688141 -2.6445045 -2.5481458 -2.3417292 -2.282207 -2.3050148 -1.9475718 -1.2866418][-2.8558273 -3.3652568 -3.4599152 -3.2751544 -3.0263166 -3.1140218 -3.4333162 -3.440891 -3.2680178 -2.963872 -2.5443754 -2.3013978 -2.1579256 -1.6638503 -0.87383962][-3.0510998 -3.682611 -3.83826 -3.6169748 -3.3026361 -3.3515794 -3.6630614 -3.6613269 -3.3969691 -2.9263434 -2.4335022 -2.1549847 -1.9959981 -1.5987184 -0.89857793][-3.1401896 -3.6660061 -3.7845972 -3.56559 -3.2726293 -3.3200378 -3.5649154 -3.4674628 -3.1008625 -2.6101966 -2.2775197 -2.1435907 -2.0710268 -1.8518875 -1.2672644][-2.9801764 -3.3335376 -3.4220886 -3.285429 -3.1038694 -3.1998129 -3.39404 -3.2256799 -2.8443816 -2.5036514 -2.4815283 -2.5877929 -2.5796461 -2.4262774 -1.8677697]]...]
INFO - root - 2017-12-07 10:08:13.452747: step 27210, loss = 0.77, batch loss = 0.70 (10.1 examples/sec; 0.793 sec/batch; 67h:12m:55s remains)
INFO - root - 2017-12-07 10:08:21.003787: step 27220, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.752 sec/batch; 63h:44m:49s remains)
INFO - root - 2017-12-07 10:08:28.695824: step 27230, loss = 0.71, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 67h:45m:06s remains)
INFO - root - 2017-12-07 10:08:36.045166: step 27240, loss = 1.07, batch loss = 0.99 (10.7 examples/sec; 0.748 sec/batch; 63h:26m:34s remains)
