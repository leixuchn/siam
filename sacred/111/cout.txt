INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "111"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1-lr0.01
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 09:58:33.851652: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 09:58:33.851689: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 09:58:33.851695: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 09:58:33.851699: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 09:58:33.851704: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/def1/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/def1/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/def1/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/def1/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/def1/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/def1/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-06 09:58:39.121298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 8.75GiB
2017-12-06 09:58:39.121342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 09:58:39.121348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 09:58:39.121366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-20000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-20000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset1/biases:0' shape=(72,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-06 09:58:53.315131: step 0, loss = 2.06, batch loss = 2.00 (0.8 examples/sec; 9.414 sec/batch; 869h:26m:38s remains)
2017-12-06 09:58:54.239842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.330164 -4.330328 -4.3304672 -4.3272629 -4.3219442 -4.3137784 -4.2993793 -4.2810016 -4.270647 -4.2739453 -4.2874813 -4.3055577 -4.3223548 -4.3313513 -4.3335576][-4.3284941 -4.3286695 -4.3268294 -4.3189392 -4.3062534 -4.288218 -4.262527 -4.2366257 -4.2258587 -4.2344203 -4.2569094 -4.2854252 -4.3102255 -4.3242116 -4.3296523][-4.3266306 -4.3254318 -4.3193178 -4.301054 -4.2733145 -4.23766 -4.1919994 -4.1515164 -4.1396027 -4.1602373 -4.1997237 -4.247282 -4.2884851 -4.3141856 -4.3269062][-4.3184061 -4.3161178 -4.3065577 -4.2781343 -4.2368751 -4.181479 -4.1099563 -4.0450191 -4.0308456 -4.06885 -4.1294065 -4.2017937 -4.26469 -4.303761 -4.3245797][-4.3030677 -4.3012652 -4.2916036 -4.2573071 -4.2052178 -4.129231 -4.0241728 -3.9242902 -3.9114921 -3.97661 -4.0617094 -4.16128 -4.2448606 -4.2964268 -4.3237824][-4.2821045 -4.2815385 -4.2705183 -4.2281685 -4.1625562 -4.0579805 -3.9096689 -3.7743175 -3.7814736 -3.8904495 -4.0061107 -4.1299238 -4.2284174 -4.28969 -4.3212738][-4.2597542 -4.2606888 -4.246676 -4.1970587 -4.118485 -3.9897194 -3.8060503 -3.6551101 -3.7105131 -3.8621402 -3.9978282 -4.1295333 -4.2276258 -4.2891216 -4.3205867][-4.2446527 -4.2455711 -4.2284694 -4.1800561 -4.1048369 -3.9814053 -3.8129129 -3.7061696 -3.7982008 -3.9408922 -4.057591 -4.165328 -4.2452555 -4.2969904 -4.324265][-4.2319951 -4.2324424 -4.2180772 -4.1833735 -4.1285815 -4.0362906 -3.9237833 -3.8736088 -3.9561799 -4.0565562 -4.1341367 -4.20616 -4.2644405 -4.3049445 -4.327559][-4.2215238 -4.2206206 -4.2114739 -4.1912584 -4.1568956 -4.1001911 -4.0364027 -4.0181618 -4.0746541 -4.1380148 -4.1827826 -4.2277293 -4.273046 -4.3080611 -4.328743][-4.2171407 -4.2155662 -4.2096462 -4.1974788 -4.1769133 -4.1447506 -4.1094766 -4.1062913 -4.1433115 -4.1821046 -4.2076278 -4.2405677 -4.2811742 -4.3146763 -4.3326492][-4.2248492 -4.2247295 -4.22112 -4.2135773 -4.2043943 -4.1920652 -4.1750631 -4.1777492 -4.1977072 -4.2165685 -4.2290516 -4.2552876 -4.2931643 -4.3239164 -4.3375349][-4.2446337 -4.2487073 -4.2494812 -4.2468028 -4.2465634 -4.2447414 -4.2328057 -4.23037 -4.2318673 -4.2322297 -4.235074 -4.2586374 -4.2968493 -4.3269992 -4.3388591][-4.2656198 -4.2739859 -4.2789984 -4.2787132 -4.2800632 -4.275795 -4.2582765 -4.2447848 -4.2317147 -4.222765 -4.2245541 -4.2512922 -4.2927008 -4.3245025 -4.3371267][-4.2723293 -4.2837768 -4.2911673 -4.2920041 -4.2909737 -4.2790051 -4.2514968 -4.2279887 -4.2107453 -4.2038832 -4.2111497 -4.2425137 -4.2870045 -4.3204675 -4.3352804]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1-lr0.01/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1-lr0.01/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 09:59:00.710617: step 10, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.430 sec/batch; 39h:40m:11s remains)
INFO - root - 2017-12-06 09:59:05.188937: step 20, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 42h:12m:10s remains)
INFO - root - 2017-12-06 09:59:09.428514: step 30, loss = 2.08, batch loss = 2.02 (20.4 examples/sec; 0.393 sec/batch; 36h:15m:38s remains)
INFO - root - 2017-12-06 09:59:13.649263: step 40, loss = 2.09, batch loss = 2.03 (19.2 examples/sec; 0.417 sec/batch; 38h:29m:59s remains)
INFO - root - 2017-12-06 09:59:17.889593: step 50, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.422 sec/batch; 38h:56m:28s remains)
INFO - root - 2017-12-06 09:59:22.201718: step 60, loss = 2.09, batch loss = 2.04 (18.3 examples/sec; 0.437 sec/batch; 40h:18m:58s remains)
INFO - root - 2017-12-06 09:59:26.532068: step 70, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 39h:20m:25s remains)
INFO - root - 2017-12-06 09:59:30.713841: step 80, loss = 2.07, batch loss = 2.02 (18.7 examples/sec; 0.428 sec/batch; 39h:32m:37s remains)
INFO - root - 2017-12-06 09:59:34.819691: step 90, loss = 2.10, batch loss = 2.04 (18.3 examples/sec; 0.438 sec/batch; 40h:27m:46s remains)
INFO - root - 2017-12-06 09:59:39.120507: step 100, loss = 2.09, batch loss = 2.03 (17.8 examples/sec; 0.451 sec/batch; 41h:36m:16s remains)
2017-12-06 09:59:39.635804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2423496 -4.2056966 -4.1720366 -4.1441274 -4.1414976 -4.1632628 -4.1776371 -4.1879687 -4.2006083 -4.2039957 -4.212101 -4.2218757 -4.211947 -4.1862087 -4.1710291][-4.2418594 -4.2006116 -4.1627731 -4.1330276 -4.1298528 -4.1478367 -4.1597314 -4.1684465 -4.1811748 -4.1902475 -4.1996255 -4.2057118 -4.1960106 -4.1749177 -4.1643367][-4.2403159 -4.1991105 -4.1608386 -4.1337681 -4.12884 -4.1399803 -4.1509891 -4.1615829 -4.1784496 -4.1920629 -4.1991062 -4.1984839 -4.1873174 -4.1698122 -4.1606445][-4.23746 -4.1974559 -4.1614347 -4.1353841 -4.1254129 -4.1295247 -4.1377068 -4.1459546 -4.1633673 -4.1810703 -4.1895866 -4.1874061 -4.178256 -4.1662297 -4.1614981][-4.2344294 -4.1959834 -4.1606345 -4.1329956 -4.1166997 -4.11454 -4.1152892 -4.1106229 -4.119978 -4.1414709 -4.1584482 -4.1641464 -4.163599 -4.1607647 -4.1636491][-4.2325006 -4.1949568 -4.1585321 -4.1277695 -4.1067553 -4.0994778 -4.0909657 -4.0685081 -4.0623622 -4.0874553 -4.1169658 -4.1350412 -4.145205 -4.1531682 -4.1646352][-4.2297745 -4.1907482 -4.151217 -4.1180792 -4.0973282 -4.0925069 -4.0833449 -4.0551105 -4.0415249 -4.0663781 -4.0971279 -4.1145473 -4.127 -4.1415243 -4.1585484][-4.2253757 -4.1846108 -4.1424856 -4.1090603 -4.09267 -4.0952554 -4.0921488 -4.07187 -4.063652 -4.0850019 -4.1056294 -4.1113553 -4.1169424 -4.1313348 -4.1478095][-4.2242594 -4.183805 -4.1408477 -4.1079741 -4.0947218 -4.0975537 -4.0958266 -4.0836716 -4.0858741 -4.1080537 -4.1203337 -4.115406 -4.1138587 -4.123199 -4.1351552][-4.2278018 -4.1880841 -4.1465707 -4.1158252 -4.1046247 -4.1054277 -4.1017613 -4.0935626 -4.1010494 -4.1249256 -4.1350617 -4.1268692 -4.12239 -4.1281576 -4.1350751][-4.2369866 -4.1989837 -4.161562 -4.1355467 -4.128068 -4.1312394 -4.1291804 -4.1237292 -4.1289196 -4.1473351 -4.1554866 -4.1497874 -4.1469955 -4.1514091 -4.1549692][-4.2484593 -4.2123528 -4.1780872 -4.1548481 -4.1494803 -4.1561036 -4.1617861 -4.16531 -4.1720185 -4.1821041 -4.1841145 -4.1786857 -4.1755042 -4.1784062 -4.1805987][-4.269001 -4.2387791 -4.2101297 -4.1883163 -4.1810203 -4.1890049 -4.2016764 -4.21271 -4.2204523 -4.2244878 -4.2218022 -4.2155552 -4.2112527 -4.2118931 -4.2141833][-4.2969308 -4.2767215 -4.2573185 -4.2408948 -4.2327776 -4.2380114 -4.2494435 -4.2593803 -4.264585 -4.2659516 -4.2628579 -4.2578759 -4.2536917 -4.2536159 -4.2565908][-4.3193979 -4.3076153 -4.2967525 -4.2877169 -4.2826405 -4.2848282 -4.2915349 -4.2966957 -4.2988248 -4.2990212 -4.2971625 -4.294476 -4.2915626 -4.2915649 -4.2945795]]...]
INFO - root - 2017-12-06 09:59:43.982242: step 110, loss = 2.08, batch loss = 2.03 (19.1 examples/sec; 0.418 sec/batch; 38h:36m:20s remains)
INFO - root - 2017-12-06 09:59:48.285661: step 120, loss = 2.08, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 39h:52m:46s remains)
INFO - root - 2017-12-06 09:59:52.648579: step 130, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.431 sec/batch; 39h:49m:03s remains)
INFO - root - 2017-12-06 09:59:56.944681: step 140, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.423 sec/batch; 39h:04m:42s remains)
INFO - root - 2017-12-06 10:00:01.182994: step 150, loss = 2.11, batch loss = 2.05 (18.2 examples/sec; 0.440 sec/batch; 40h:37m:51s remains)
INFO - root - 2017-12-06 10:00:05.431907: step 160, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.440 sec/batch; 40h:36m:08s remains)
INFO - root - 2017-12-06 10:00:09.719508: step 170, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.423 sec/batch; 39h:00m:44s remains)
INFO - root - 2017-12-06 10:00:13.935423: step 180, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.420 sec/batch; 38h:48m:46s remains)
INFO - root - 2017-12-06 10:00:17.875759: step 190, loss = 2.10, batch loss = 2.04 (19.8 examples/sec; 0.405 sec/batch; 37h:21m:46s remains)
INFO - root - 2017-12-06 10:00:22.025004: step 200, loss = 2.07, batch loss = 2.01 (19.6 examples/sec; 0.408 sec/batch; 37h:40m:11s remains)
2017-12-06 10:00:22.494693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.318306 -4.3139763 -4.3101997 -4.3115134 -4.3177056 -4.3244486 -4.3305969 -4.3352947 -4.3398876 -4.3446207 -4.34996 -4.3531947 -4.3532715 -4.3495321 -4.3433523][-4.3020816 -4.2937741 -4.2912135 -4.300056 -4.3110032 -4.3157926 -4.3166318 -4.3162575 -4.317903 -4.3225288 -4.3332372 -4.3447461 -4.350596 -4.3480191 -4.3412123][-4.2835703 -4.2715588 -4.2733626 -4.2892489 -4.3009129 -4.2976623 -4.2855797 -4.272584 -4.2670956 -4.2753596 -4.2977281 -4.3213782 -4.3368587 -4.3388228 -4.3344646][-4.2749109 -4.2634468 -4.2654696 -4.278522 -4.2817955 -4.2626472 -4.2315927 -4.202642 -4.1943903 -4.2126079 -4.2491426 -4.2845969 -4.3115783 -4.321382 -4.3221836][-4.2646332 -4.2499804 -4.2471256 -4.2500038 -4.2381587 -4.2010336 -4.1530185 -4.1170621 -4.1211319 -4.1591673 -4.2089362 -4.2524567 -4.2886925 -4.3060927 -4.31175][-4.2338319 -4.2115893 -4.1995273 -4.1889405 -4.160296 -4.099381 -4.0347643 -4.0046463 -4.0441856 -4.1137857 -4.17644 -4.2257972 -4.2679729 -4.2933459 -4.3045697][-4.1930804 -4.161201 -4.1341205 -4.1033955 -4.0512691 -3.9645262 -3.8820131 -3.8719468 -3.965842 -4.0696397 -4.1451187 -4.1983809 -4.2463865 -4.2788825 -4.295207][-4.1534266 -4.1150336 -4.0761843 -4.0290842 -3.966203 -3.8725326 -3.7864108 -3.8001142 -3.9248052 -4.044096 -4.1236191 -4.1808071 -4.2345543 -4.2712975 -4.2900639][-4.1075473 -4.0684361 -4.0352116 -4.0021629 -3.9643831 -3.9051304 -3.8555031 -3.8808475 -3.9811625 -4.075109 -4.1397872 -4.1905723 -4.2415891 -4.2758894 -4.2927818][-4.089304 -4.05606 -4.0386848 -4.0331254 -4.0332456 -4.0124035 -3.9898942 -4.0098238 -4.0718513 -4.1318269 -4.1770906 -4.2178125 -4.2592521 -4.2865763 -4.2999492][-4.1106892 -4.0891914 -4.0903111 -4.1024675 -4.121954 -4.118886 -4.1050649 -4.1154356 -4.1496458 -4.1874242 -4.2193236 -4.2503972 -4.2811661 -4.3019528 -4.3124352][-4.1617174 -4.1513839 -4.16391 -4.1813908 -4.2006216 -4.1991706 -4.1883378 -4.1932068 -4.2116 -4.2349586 -4.2567625 -4.2799664 -4.3040304 -4.3195004 -4.3263416][-4.2239695 -4.2203298 -4.2324429 -4.2455053 -4.2551718 -4.2500658 -4.2398796 -4.2414751 -4.2519608 -4.2660151 -4.2815156 -4.3003869 -4.3188276 -4.3300505 -4.333745][-4.2758861 -4.274797 -4.2821965 -4.2893267 -4.2935724 -4.2887726 -4.2813735 -4.279963 -4.2833347 -4.2899618 -4.3000932 -4.3147383 -4.3268032 -4.3325081 -4.3333192][-4.30687 -4.3038197 -4.30605 -4.3095593 -4.3120136 -4.3100028 -4.3062749 -4.3049679 -4.30561 -4.308434 -4.314981 -4.3239069 -4.3296366 -4.330802 -4.32936]]...]
INFO - root - 2017-12-06 10:00:26.742624: step 210, loss = 2.07, batch loss = 2.01 (19.2 examples/sec; 0.416 sec/batch; 38h:22m:23s remains)
INFO - root - 2017-12-06 10:00:31.100650: step 220, loss = 2.10, batch loss = 2.05 (18.2 examples/sec; 0.440 sec/batch; 40h:37m:54s remains)
INFO - root - 2017-12-06 10:00:35.393412: step 230, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.427 sec/batch; 39h:22m:34s remains)
INFO - root - 2017-12-06 10:00:39.733184: step 240, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.453 sec/batch; 41h:48m:28s remains)
INFO - root - 2017-12-06 10:00:44.011196: step 250, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.424 sec/batch; 39h:07m:18s remains)
INFO - root - 2017-12-06 10:00:48.417714: step 260, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 40h:28m:36s remains)
INFO - root - 2017-12-06 10:00:52.856561: step 270, loss = 2.09, batch loss = 2.04 (18.5 examples/sec; 0.432 sec/batch; 39h:54m:33s remains)
INFO - root - 2017-12-06 10:00:57.165997: step 280, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.426 sec/batch; 39h:21m:09s remains)
INFO - root - 2017-12-06 10:01:01.190062: step 290, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 39h:51m:28s remains)
INFO - root - 2017-12-06 10:01:05.501795: step 300, loss = 2.10, batch loss = 2.04 (18.3 examples/sec; 0.437 sec/batch; 40h:17m:42s remains)
2017-12-06 10:01:06.010314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1107659 -4.1286421 -4.15458 -4.1844459 -4.2094135 -4.2220807 -4.2199364 -4.2038774 -4.1764431 -4.1551008 -4.1553679 -4.1689167 -4.1732025 -4.1728535 -4.1810646][-4.1725368 -4.169683 -4.1745129 -4.1936665 -4.2218285 -4.2423687 -4.247056 -4.2355375 -4.2168326 -4.2036724 -4.2061825 -4.2133932 -4.2096763 -4.1983085 -4.1978049][-4.2339988 -4.2161722 -4.1987977 -4.2020688 -4.2282429 -4.2570724 -4.2712455 -4.26782 -4.2623048 -4.2564445 -4.2568326 -4.252481 -4.2356129 -4.2085533 -4.1965594][-4.2664542 -4.2384386 -4.2053971 -4.1956158 -4.2160521 -4.2503614 -4.272254 -4.2786813 -4.28659 -4.2861481 -4.2795448 -4.2645063 -4.2361679 -4.1970668 -4.1791182][-4.2705612 -4.2375 -4.1983852 -4.1794777 -4.1912107 -4.2217803 -4.2443786 -4.2594953 -4.2788076 -4.2813993 -4.2669225 -4.2400827 -4.2018294 -4.1560736 -4.140933][-4.2510333 -4.2188754 -4.1822481 -4.1643844 -4.1694932 -4.19009 -4.2095847 -4.229424 -4.251545 -4.2518148 -4.2309012 -4.1923542 -4.14532 -4.1024914 -4.0986171][-4.2188334 -4.1926537 -4.1673713 -4.1599894 -4.1630325 -4.173892 -4.1866994 -4.2016773 -4.2154527 -4.2112522 -4.1850548 -4.1382403 -4.0871572 -4.0547781 -4.0678782][-4.1949716 -4.1737761 -4.1594667 -4.1569014 -4.1573005 -4.1606426 -4.1680818 -4.177454 -4.1771607 -4.1650381 -4.1343393 -4.0844712 -4.0334234 -4.013422 -4.0458632][-4.1880774 -4.172595 -4.1647844 -4.1626244 -4.1581912 -4.154058 -4.15596 -4.1615095 -4.1519251 -4.1397772 -4.1139717 -4.0693035 -4.0226846 -4.0122261 -4.0574856][-4.202136 -4.2027245 -4.2058492 -4.2048335 -4.1982903 -4.18463 -4.1761236 -4.1731172 -4.1579461 -4.147871 -4.1356907 -4.1063905 -4.07135 -4.0658078 -4.1065869][-4.2210011 -4.2376504 -4.2484188 -4.2520757 -4.2474041 -4.2298603 -4.2151523 -4.2033758 -4.1846604 -4.1763897 -4.1807094 -4.1714945 -4.1518869 -4.1493368 -4.1756868][-4.2263184 -4.2542505 -4.2726879 -4.2838778 -4.28273 -4.2639112 -4.2474232 -4.2311549 -4.2083454 -4.2002411 -4.2182412 -4.227335 -4.2205954 -4.2230721 -4.2381859][-4.2085123 -4.2445216 -4.2707314 -4.2918758 -4.2945676 -4.2783885 -4.2637806 -4.2450857 -4.2186089 -4.2083073 -4.2315254 -4.2494111 -4.2520914 -4.2589846 -4.2707448][-4.1941566 -4.2271104 -4.2536025 -4.2781143 -4.2841272 -4.2752666 -4.2658272 -4.2471132 -4.2220254 -4.2118516 -4.2364416 -4.257288 -4.2622609 -4.26875 -4.2773395][-4.2036104 -4.2233849 -4.2422194 -4.263731 -4.2730427 -4.271142 -4.2644095 -4.2453647 -4.22243 -4.2148428 -4.23722 -4.2572217 -4.261848 -4.2644053 -4.2683225]]...]
INFO - root - 2017-12-06 10:01:10.267033: step 310, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 39h:08m:45s remains)
INFO - root - 2017-12-06 10:01:14.628000: step 320, loss = 2.05, batch loss = 2.00 (18.1 examples/sec; 0.443 sec/batch; 40h:51m:14s remains)
INFO - root - 2017-12-06 10:01:19.057121: step 330, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.427 sec/batch; 39h:23m:03s remains)
INFO - root - 2017-12-06 10:01:23.335181: step 340, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.429 sec/batch; 39h:33m:48s remains)
INFO - root - 2017-12-06 10:01:27.727123: step 350, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.430 sec/batch; 39h:41m:58s remains)
INFO - root - 2017-12-06 10:01:32.004147: step 360, loss = 2.10, batch loss = 2.04 (18.4 examples/sec; 0.435 sec/batch; 40h:09m:28s remains)
INFO - root - 2017-12-06 10:01:36.458086: step 370, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.433 sec/batch; 39h:58m:02s remains)
INFO - root - 2017-12-06 10:01:40.866409: step 380, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 39h:10m:29s remains)
INFO - root - 2017-12-06 10:01:44.917735: step 390, loss = 2.06, batch loss = 2.01 (19.1 examples/sec; 0.420 sec/batch; 38h:42m:56s remains)
INFO - root - 2017-12-06 10:01:49.291139: step 400, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.464 sec/batch; 42h:49m:10s remains)
2017-12-06 10:01:49.772856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2479744 -4.239357 -4.2370467 -4.2378817 -4.23964 -4.2421184 -4.2433805 -4.2449365 -4.2417617 -4.2351251 -4.2313442 -4.2324839 -4.2372675 -4.2430563 -4.2489643][-4.2766085 -4.2706852 -4.2662034 -4.2637749 -4.2646942 -4.2710981 -4.2792263 -4.2886448 -4.2913156 -4.2854791 -4.276773 -4.26503 -4.2519064 -4.2435966 -4.2397456][-4.2910156 -4.2887111 -4.2819915 -4.2744942 -4.2711153 -4.276979 -4.2906179 -4.3108211 -4.3258152 -4.3309588 -4.3257065 -4.3068495 -4.2777452 -4.2484655 -4.2258673][-4.2817163 -4.2802038 -4.269609 -4.2572064 -4.249752 -4.2514958 -4.2656116 -4.2919436 -4.31968 -4.3395815 -4.3458519 -4.3314228 -4.2969851 -4.2526612 -4.21039][-4.2698865 -4.2598495 -4.2373228 -4.2178674 -4.208425 -4.2069483 -4.214798 -4.2398329 -4.2750897 -4.3085756 -4.3291163 -4.32723 -4.2996292 -4.2503514 -4.1935525][-4.2602739 -4.2341943 -4.1950164 -4.1656351 -4.1532989 -4.1481009 -4.1489868 -4.1708388 -4.2123122 -4.2579679 -4.29539 -4.3105059 -4.29298 -4.2422428 -4.1737728][-4.2506704 -4.2105012 -4.1548057 -4.1126194 -4.0914044 -4.0781832 -4.0676293 -4.0828314 -4.1328797 -4.1951485 -4.250073 -4.2815223 -4.2800283 -4.2402906 -4.1721797][-4.2478118 -4.2013564 -4.1369658 -4.08075 -4.0403876 -4.00185 -3.9625115 -3.9626184 -4.0239916 -4.1102667 -4.1874404 -4.2363181 -4.257524 -4.2442522 -4.1946673][-4.2520561 -4.2110796 -4.1513119 -4.0878515 -4.0277796 -3.9574087 -3.8760376 -3.8419762 -3.9059639 -4.0161934 -4.1142178 -4.1825356 -4.2256346 -4.2427874 -4.2255254][-4.26242 -4.2352638 -4.1940994 -4.1405711 -4.0807662 -4.0042543 -3.9063802 -3.8427923 -3.87749 -3.9677618 -4.0573349 -4.1309619 -4.1883826 -4.2308717 -4.2455673][-4.277914 -4.2647672 -4.2446404 -4.2114 -4.1716514 -4.1160603 -4.0403109 -3.9820993 -3.9852161 -4.0265431 -4.0766425 -4.1289325 -4.1794167 -4.2251058 -4.2573757][-4.2913671 -4.2873082 -4.2817287 -4.2681274 -4.2496419 -4.2219563 -4.1807232 -4.1445355 -4.13533 -4.1443763 -4.1637259 -4.1934943 -4.2236538 -4.2518048 -4.2757125][-4.3039279 -4.3040957 -4.3034616 -4.3001022 -4.295577 -4.2890248 -4.2764416 -4.2613864 -4.2534552 -4.2509265 -4.2563863 -4.2716918 -4.2842093 -4.2940149 -4.3013887][-4.3174968 -4.319757 -4.318635 -4.3166151 -4.3161235 -4.3177919 -4.3198948 -4.3187275 -4.3165159 -4.3139057 -4.3146577 -4.3192806 -4.3214865 -4.3219233 -4.3195391][-4.3273792 -4.3306437 -4.3297639 -4.32622 -4.3246269 -4.3259535 -4.3289714 -4.3316388 -4.3330975 -4.3335972 -4.3347645 -4.3363123 -4.3353548 -4.3327746 -4.3280468]]...]
INFO - root - 2017-12-06 10:01:53.996086: step 410, loss = 2.09, batch loss = 2.04 (18.5 examples/sec; 0.433 sec/batch; 39h:59m:14s remains)
INFO - root - 2017-12-06 10:01:58.187039: step 420, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.427 sec/batch; 39h:21m:34s remains)
INFO - root - 2017-12-06 10:02:02.528471: step 430, loss = 2.10, batch loss = 2.04 (18.5 examples/sec; 0.432 sec/batch; 39h:52m:35s remains)
INFO - root - 2017-12-06 10:02:06.821925: step 440, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.427 sec/batch; 39h:25m:48s remains)
INFO - root - 2017-12-06 10:02:11.169439: step 450, loss = 2.08, batch loss = 2.03 (18.3 examples/sec; 0.437 sec/batch; 40h:20m:45s remains)
INFO - root - 2017-12-06 10:02:15.468614: step 460, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.431 sec/batch; 39h:42m:46s remains)
INFO - root - 2017-12-06 10:02:19.757395: step 470, loss = 2.08, batch loss = 2.02 (19.3 examples/sec; 0.414 sec/batch; 38h:11m:33s remains)
INFO - root - 2017-12-06 10:02:23.969613: step 480, loss = 2.07, batch loss = 2.02 (19.6 examples/sec; 0.409 sec/batch; 37h:43m:21s remains)
INFO - root - 2017-12-06 10:02:27.897595: step 490, loss = 2.08, batch loss = 2.02 (19.2 examples/sec; 0.417 sec/batch; 38h:25m:42s remains)
INFO - root - 2017-12-06 10:02:32.151885: step 500, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.424 sec/batch; 39h:07m:49s remains)
2017-12-06 10:02:32.631667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2399092 -4.2358336 -4.2397656 -4.2489729 -4.2435818 -4.2297926 -4.2182593 -4.2107906 -4.2079916 -4.20929 -4.2041955 -4.1932821 -4.1754351 -4.1488714 -4.1250324][-4.2030306 -4.198875 -4.2020164 -4.2120748 -4.2038198 -4.1867142 -4.1711092 -4.16798 -4.1790276 -4.1934171 -4.1951337 -4.1820536 -4.1568666 -4.127882 -4.10679][-4.1798544 -4.1723547 -4.1714869 -4.1764646 -4.1628532 -4.1410728 -4.1315889 -4.1439 -4.1714587 -4.1919932 -4.1943035 -4.171679 -4.136734 -4.1037641 -4.0876622][-4.1634927 -4.1545296 -4.1463809 -4.1368585 -4.115828 -4.0935378 -4.0910935 -4.1195521 -4.1590323 -4.1847878 -4.1831188 -4.1502819 -4.1050663 -4.0731606 -4.0723143][-4.1456413 -4.138092 -4.1240358 -4.1024542 -4.0770521 -4.0581193 -4.0570192 -4.0833464 -4.1255174 -4.1534405 -4.1480169 -4.111227 -4.0668011 -4.0522628 -4.0739641][-4.1341906 -4.1302295 -4.1154823 -4.0862951 -4.0504041 -4.0224209 -4.0077028 -4.013618 -4.0469875 -4.0794239 -4.0785007 -4.0473261 -4.0174737 -4.0257831 -4.0606732][-4.1293554 -4.1348767 -4.1264243 -4.0958495 -4.0495868 -3.9989176 -3.9462543 -3.9088163 -3.9268436 -3.9774139 -4.0039177 -3.9948611 -3.9861188 -4.0062256 -4.0422387][-4.1285839 -4.1370921 -4.1295023 -4.1029139 -4.057971 -3.9983532 -3.9164569 -3.8367457 -3.8340085 -3.9043789 -3.9563646 -3.97862 -3.9932773 -4.0158072 -4.0387807][-4.1326623 -4.1286459 -4.111505 -4.0862842 -4.05407 -4.011457 -3.9392231 -3.8657529 -3.8599942 -3.9194698 -3.9677415 -4.0047703 -4.0366664 -4.0562706 -4.0639639][-4.1260171 -4.1157455 -4.0987816 -4.0828395 -4.0678167 -4.0444865 -3.9988258 -3.9522853 -3.9518769 -3.98733 -4.0131655 -4.0457764 -4.0841537 -4.1039596 -4.1036553][-4.1179962 -4.1164036 -4.1137114 -4.1126957 -4.1098318 -4.1003404 -4.073267 -4.04345 -4.0416636 -4.0519176 -4.0526218 -4.0676422 -4.1004119 -4.1223445 -4.1226563][-4.129971 -4.1402507 -4.1485586 -4.1512194 -4.1490531 -4.1429873 -4.122716 -4.0986 -4.0903506 -4.084249 -4.0669684 -4.0651612 -4.0896826 -4.1136222 -4.1224747][-4.1603069 -4.1690888 -4.1778669 -4.178257 -4.17186 -4.1647882 -4.1482425 -4.1288843 -4.115694 -4.1016779 -4.0832763 -4.0777583 -4.0946975 -4.1163006 -4.1356454][-4.1800008 -4.1856737 -4.19126 -4.1887856 -4.1801305 -4.1722007 -4.1586394 -4.1454854 -4.1385822 -4.1304045 -4.1202631 -4.1182137 -4.1265268 -4.1405706 -4.1618929][-4.1798768 -4.1830134 -4.1913247 -4.1921644 -4.1827879 -4.1724439 -4.1581974 -4.1496205 -4.1544147 -4.1603994 -4.1619163 -4.1608524 -4.15693 -4.1582885 -4.1720934]]...]
INFO - root - 2017-12-06 10:02:36.992347: step 510, loss = 2.10, batch loss = 2.04 (18.0 examples/sec; 0.444 sec/batch; 40h:59m:22s remains)
INFO - root - 2017-12-06 10:02:41.340972: step 520, loss = 2.09, batch loss = 2.03 (19.6 examples/sec; 0.409 sec/batch; 37h:40m:57s remains)
INFO - root - 2017-12-06 10:02:45.759528: step 530, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.447 sec/batch; 41h:12m:44s remains)
INFO - root - 2017-12-06 10:02:50.128285: step 540, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.436 sec/batch; 40h:11m:18s remains)
INFO - root - 2017-12-06 10:02:54.530362: step 550, loss = 2.08, batch loss = 2.02 (19.5 examples/sec; 0.410 sec/batch; 37h:47m:55s remains)
INFO - root - 2017-12-06 10:02:58.811080: step 560, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.427 sec/batch; 39h:22m:12s remains)
INFO - root - 2017-12-06 10:03:03.136671: step 570, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.432 sec/batch; 39h:47m:14s remains)
INFO - root - 2017-12-06 10:03:07.577633: step 580, loss = 2.08, batch loss = 2.03 (17.7 examples/sec; 0.453 sec/batch; 41h:44m:06s remains)
INFO - root - 2017-12-06 10:03:11.780612: step 590, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 40h:13m:09s remains)
INFO - root - 2017-12-06 10:03:16.078621: step 600, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 38h:59m:26s remains)
2017-12-06 10:03:16.589450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.10092 -4.0709896 -4.0807686 -4.1312885 -4.1776223 -4.1977606 -4.17526 -4.1432042 -4.1470137 -4.1893792 -4.2445574 -4.2885776 -4.32111 -4.3401828 -4.3402433][-4.0799909 -4.0546966 -4.0718627 -4.1317348 -4.1854024 -4.2078147 -4.1868606 -4.1566372 -4.1583934 -4.198607 -4.250639 -4.2905006 -4.32108 -4.3335958 -4.3255754][-4.0780883 -4.0588331 -4.0843496 -4.1451426 -4.1929274 -4.2086043 -4.1960135 -4.1820207 -4.1895428 -4.2250018 -4.2642536 -4.2889948 -4.3061881 -4.3073516 -4.2905951][-4.0961437 -4.0778155 -4.099411 -4.1474853 -4.1860037 -4.1978893 -4.1939611 -4.1945515 -4.2068396 -4.2356963 -4.2637453 -4.27711 -4.2828088 -4.2762952 -4.25744][-4.1001449 -4.0708857 -4.0762453 -4.1054077 -4.1360259 -4.1547236 -4.1646628 -4.1765194 -4.19294 -4.2143683 -4.2355771 -4.2467728 -4.2519541 -4.247663 -4.23577][-4.0798616 -4.0224962 -3.9963336 -4.0035038 -4.0293446 -4.0589247 -4.0838881 -4.1116362 -4.1434283 -4.1736393 -4.198482 -4.2147937 -4.2255535 -4.227396 -4.2246141][-4.0610251 -3.9779663 -3.9179308 -3.8993251 -3.920702 -3.9572992 -3.9956155 -4.0414071 -4.089684 -4.1332064 -4.168252 -4.193327 -4.2091036 -4.2161732 -4.2213612][-4.0720272 -3.9971395 -3.9322519 -3.9048958 -3.9203911 -3.9537516 -3.9898968 -4.0353785 -4.0857472 -4.1318889 -4.1702714 -4.1988945 -4.2164855 -4.2262807 -4.235][-4.117559 -4.0721288 -4.0315909 -4.0155263 -4.027535 -4.0497079 -4.0734458 -4.1025758 -4.1379714 -4.1724191 -4.2053909 -4.2310977 -4.2462583 -4.2546315 -4.2620211][-4.1604719 -4.1339445 -4.1163316 -4.11439 -4.1317453 -4.1561842 -4.1762052 -4.1925921 -4.2106972 -4.2301106 -4.25299 -4.2707396 -4.2790537 -4.2813358 -4.2836137][-4.1813622 -4.1607389 -4.1545172 -4.1620445 -4.1862025 -4.2173576 -4.2415757 -4.2547235 -4.2623248 -4.2698421 -4.2820325 -4.2922206 -4.296175 -4.2953262 -4.2938395][-4.1985679 -4.1776757 -4.1725068 -4.1803784 -4.2033682 -4.23305 -4.2549238 -4.2645521 -4.2668953 -4.2677808 -4.2746897 -4.2851992 -4.2935314 -4.2965035 -4.2963891][-4.2220635 -4.202888 -4.1947341 -4.1970563 -4.2105875 -4.2292175 -4.2413487 -4.2439036 -4.2435575 -4.2437105 -4.2521272 -4.2679152 -4.2834363 -4.2919083 -4.2945042][-4.2451987 -4.2310667 -4.2245488 -4.2250347 -4.2310972 -4.2414784 -4.2464428 -4.2442107 -4.2405624 -4.2398324 -4.2496138 -4.2671247 -4.2834663 -4.2922754 -4.2948217][-4.2550712 -4.2492414 -4.2474942 -4.2492881 -4.2532539 -4.2612858 -4.265449 -4.2630811 -4.2585406 -4.2578988 -4.2670813 -4.281847 -4.2944255 -4.3003769 -4.3003817]]...]
INFO - root - 2017-12-06 10:03:21.027295: step 610, loss = 2.11, batch loss = 2.05 (17.2 examples/sec; 0.464 sec/batch; 42h:45m:35s remains)
INFO - root - 2017-12-06 10:03:25.444434: step 620, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.425 sec/batch; 39h:11m:42s remains)
INFO - root - 2017-12-06 10:03:29.822153: step 630, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.448 sec/batch; 41h:20m:36s remains)
INFO - root - 2017-12-06 10:03:34.255550: step 640, loss = 2.09, batch loss = 2.03 (17.4 examples/sec; 0.460 sec/batch; 42h:23m:42s remains)
INFO - root - 2017-12-06 10:03:38.634703: step 650, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.421 sec/batch; 38h:47m:20s remains)
INFO - root - 2017-12-06 10:03:43.006215: step 660, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.445 sec/batch; 41h:00m:57s remains)
INFO - root - 2017-12-06 10:03:47.373802: step 670, loss = 2.11, batch loss = 2.05 (18.8 examples/sec; 0.426 sec/batch; 39h:17m:12s remains)
INFO - root - 2017-12-06 10:03:51.741702: step 680, loss = 2.10, batch loss = 2.04 (17.5 examples/sec; 0.457 sec/batch; 42h:10m:06s remains)
INFO - root - 2017-12-06 10:03:55.842917: step 690, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.435 sec/batch; 40h:04m:10s remains)
INFO - root - 2017-12-06 10:04:00.136138: step 700, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 39h:14m:36s remains)
2017-12-06 10:04:00.604816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21256 -4.2059469 -4.2011805 -4.1964684 -4.1940289 -4.1921377 -4.1899233 -4.1887369 -4.1899371 -4.1913218 -4.1883745 -4.1758308 -4.159596 -4.14555 -4.1484451][-4.22936 -4.2230496 -4.2183981 -4.2132206 -4.2102494 -4.2063541 -4.2011652 -4.1961842 -4.1942058 -4.192626 -4.1867933 -4.1721687 -4.1556988 -4.1415076 -4.1440024][-4.2437091 -4.2393732 -4.2367873 -4.2348981 -4.23508 -4.2324691 -4.2264767 -4.2208972 -4.2190647 -4.217979 -4.21246 -4.1981974 -4.1826434 -4.1666508 -4.1656022][-4.2341228 -4.2290635 -4.2279615 -4.2309036 -4.2352452 -4.2347889 -4.2299242 -4.2277923 -4.2324848 -4.2394891 -4.2395945 -4.2290492 -4.2149086 -4.1960568 -4.19064][-4.2106996 -4.1994357 -4.19658 -4.20303 -4.2085147 -4.205658 -4.1984129 -4.2010708 -4.2175312 -4.2383757 -4.2507467 -4.2489457 -4.2396574 -4.22066 -4.2120156][-4.1834846 -4.1660495 -4.1601911 -4.1659384 -4.1680779 -4.1576753 -4.1438704 -4.1485376 -4.1735854 -4.2064147 -4.2314329 -4.2425184 -4.2435527 -4.230752 -4.223918][-4.1710854 -4.1551666 -4.1491756 -4.1521311 -4.1469593 -4.1254878 -4.1016841 -4.10323 -4.13012 -4.1672606 -4.1994171 -4.2199044 -4.2300844 -4.2253532 -4.2237248][-4.1736922 -4.1663241 -4.1662984 -4.1713552 -4.1658263 -4.1434021 -4.1168752 -4.1124253 -4.1310706 -4.1599851 -4.1877332 -4.2065125 -4.2168684 -4.215261 -4.218574][-4.1876688 -4.1853485 -4.1886754 -4.1963286 -4.1946979 -4.1799893 -4.161911 -4.1605654 -4.1741376 -4.1928687 -4.2098241 -4.2192616 -4.2208538 -4.214561 -4.21754][-4.213294 -4.2096815 -4.2083693 -4.2101049 -4.2062612 -4.1958995 -4.186904 -4.1916761 -4.2063389 -4.2226038 -4.2368112 -4.2415085 -4.2357206 -4.2231622 -4.2214031][-4.2397161 -4.2322326 -4.2244811 -4.2188673 -4.2097135 -4.1983867 -4.192112 -4.1985083 -4.2132363 -4.2293148 -4.2434959 -4.2490759 -4.2430778 -4.2290049 -4.2252254][-4.2497225 -4.2409272 -4.2322545 -4.2258458 -4.2175546 -4.2076006 -4.2019792 -4.2052894 -4.2164779 -4.229383 -4.2407246 -4.2447209 -4.2386742 -4.2260108 -4.2243719][-4.2475462 -4.2404938 -4.2345605 -4.2304859 -4.2259188 -4.2211342 -4.2190027 -4.2208843 -4.2280974 -4.2355285 -4.2417593 -4.241981 -4.2341785 -4.2224059 -4.222713][-4.2373519 -4.2312469 -4.2260904 -4.2232223 -4.2211766 -4.2213244 -4.2233996 -4.22553 -4.2297134 -4.233119 -4.2359095 -4.2345963 -4.226891 -4.2170439 -4.2191596][-4.2163849 -4.2098885 -4.2051897 -4.2036872 -4.2044749 -4.207346 -4.2110605 -4.2131042 -4.2153506 -4.2165246 -4.2172565 -4.2153149 -4.2087073 -4.20218 -4.2057939]]...]
INFO - root - 2017-12-06 10:04:04.889726: step 710, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 40h:10m:55s remains)
INFO - root - 2017-12-06 10:04:09.345378: step 720, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.448 sec/batch; 41h:15m:02s remains)
INFO - root - 2017-12-06 10:04:13.748173: step 730, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.435 sec/batch; 40h:03m:42s remains)
INFO - root - 2017-12-06 10:04:18.150909: step 740, loss = 2.10, batch loss = 2.04 (16.9 examples/sec; 0.474 sec/batch; 43h:38m:58s remains)
INFO - root - 2017-12-06 10:04:22.539958: step 750, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.458 sec/batch; 42h:10m:30s remains)
INFO - root - 2017-12-06 10:04:26.913157: step 760, loss = 2.11, batch loss = 2.05 (18.5 examples/sec; 0.434 sec/batch; 39h:57m:03s remains)
INFO - root - 2017-12-06 10:04:31.217551: step 770, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.421 sec/batch; 38h:46m:10s remains)
INFO - root - 2017-12-06 10:04:35.650179: step 780, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.452 sec/batch; 41h:40m:46s remains)
INFO - root - 2017-12-06 10:04:39.855656: step 790, loss = 2.07, batch loss = 2.02 (18.7 examples/sec; 0.427 sec/batch; 39h:20m:35s remains)
INFO - root - 2017-12-06 10:04:44.272862: step 800, loss = 2.10, batch loss = 2.04 (17.2 examples/sec; 0.466 sec/batch; 42h:54m:38s remains)
2017-12-06 10:04:44.754767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2683425 -4.27014 -4.2752485 -4.2694426 -4.2515278 -4.2370019 -4.2333121 -4.2374573 -4.2418895 -4.2453651 -4.2478056 -4.2497544 -4.2444057 -4.2285709 -4.22143][-4.2876787 -4.2875381 -4.2917595 -4.2858219 -4.2682147 -4.2494521 -4.2353554 -4.2280307 -4.2261415 -4.2273874 -4.2303371 -4.2330136 -4.2297459 -4.2162776 -4.2134809][-4.294508 -4.2928796 -4.2963352 -4.2909946 -4.2733493 -4.2522578 -4.232409 -4.2175574 -4.2097063 -4.2077179 -4.210381 -4.2137027 -4.2127109 -4.2028856 -4.2016072][-4.2780762 -4.273901 -4.2754197 -4.268589 -4.2497134 -4.2284946 -4.2084303 -4.192584 -4.1856856 -4.1846008 -4.1883283 -4.1930838 -4.1946363 -4.1908336 -4.1926012][-4.2419047 -4.2346354 -4.2332344 -4.2246513 -4.2067018 -4.1878529 -4.1700354 -4.1543927 -4.1511884 -4.1576023 -4.1676717 -4.1748657 -4.18091 -4.1845121 -4.1889811][-4.2005892 -4.1923485 -4.1888824 -4.179019 -4.1628084 -4.1459017 -4.1287408 -4.1143489 -4.1173463 -4.1351519 -4.1529751 -4.1619706 -4.1702375 -4.179142 -4.1868019][-4.1617146 -4.1499791 -4.1395774 -4.1241779 -4.1042604 -4.0831876 -4.0597315 -4.0449734 -4.0615172 -4.1004639 -4.132884 -4.1473532 -4.1564307 -4.1649971 -4.171752][-4.137547 -4.1240959 -4.1061664 -4.0812988 -4.0493283 -4.0124154 -3.9698048 -3.9489856 -3.9801581 -4.0422459 -4.0903206 -4.1119308 -4.11797 -4.1183376 -4.1181221][-4.1350517 -4.1258693 -4.10561 -4.0755854 -4.0368857 -3.9912622 -3.937917 -3.9113865 -3.9455042 -4.0121145 -4.0610557 -4.0788984 -4.0792432 -4.0719457 -4.0651817][-4.1528835 -4.1482286 -4.1302967 -4.1050262 -4.0761127 -4.0440221 -4.0040736 -3.9810278 -4.0025749 -4.0504661 -4.0844741 -4.0923681 -4.0872588 -4.0751104 -4.0660105][-4.1854739 -4.1817708 -4.1668434 -4.1482506 -4.1301279 -4.1113372 -4.0842738 -4.0660119 -4.0773458 -4.1061707 -4.1268854 -4.1273708 -4.1171975 -4.10053 -4.0934644][-4.2325897 -4.2287879 -4.2167587 -4.2026143 -4.1895185 -4.1760798 -4.1553431 -4.1394463 -4.1427536 -4.1574044 -4.1702175 -4.1689391 -4.1584916 -4.1406479 -4.1339316][-4.2748628 -4.2703557 -4.261507 -4.2518349 -4.2423573 -4.2334847 -4.2194281 -4.2083964 -4.2097583 -4.217134 -4.2233748 -4.2202249 -4.2104864 -4.195581 -4.1886077][-4.2976828 -4.29513 -4.2897668 -4.2833734 -4.2775545 -4.2737823 -4.267498 -4.26408 -4.2667289 -4.2700744 -4.271286 -4.2674065 -4.2614217 -4.2519236 -4.2469835][-4.3060942 -4.304925 -4.3010445 -4.2953539 -4.2908916 -4.2897353 -4.2892027 -4.2907662 -4.293921 -4.295011 -4.2941866 -4.2917695 -4.2901516 -4.2857885 -4.2830729]]...]
INFO - root - 2017-12-06 10:04:49.053051: step 810, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.431 sec/batch; 39h:40m:20s remains)
INFO - root - 2017-12-06 10:04:53.297226: step 820, loss = 2.04, batch loss = 1.98 (19.1 examples/sec; 0.420 sec/batch; 38h:41m:11s remains)
INFO - root - 2017-12-06 10:04:57.630884: step 830, loss = 2.10, batch loss = 2.04 (18.6 examples/sec; 0.430 sec/batch; 39h:36m:52s remains)
INFO - root - 2017-12-06 10:05:02.015876: step 840, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.436 sec/batch; 40h:11m:13s remains)
INFO - root - 2017-12-06 10:05:06.332406: step 850, loss = 2.07, batch loss = 2.02 (18.5 examples/sec; 0.431 sec/batch; 39h:44m:07s remains)
INFO - root - 2017-12-06 10:05:10.784840: step 860, loss = 2.10, batch loss = 2.04 (18.4 examples/sec; 0.435 sec/batch; 40h:03m:09s remains)
INFO - root - 2017-12-06 10:05:15.127208: step 870, loss = 2.07, batch loss = 2.01 (19.1 examples/sec; 0.419 sec/batch; 38h:33m:43s remains)
INFO - root - 2017-12-06 10:05:19.545572: step 880, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.443 sec/batch; 40h:50m:50s remains)
INFO - root - 2017-12-06 10:05:23.676307: step 890, loss = 2.10, batch loss = 2.04 (18.9 examples/sec; 0.424 sec/batch; 39h:02m:59s remains)
INFO - root - 2017-12-06 10:05:27.873559: step 900, loss = 2.06, batch loss = 2.01 (18.7 examples/sec; 0.427 sec/batch; 39h:20m:17s remains)
2017-12-06 10:05:28.464827: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31703 -4.3090529 -4.3061976 -4.3094864 -4.3148613 -4.3210473 -4.3275385 -4.3320317 -4.3328233 -4.3302321 -4.3253732 -4.320713 -4.3184233 -4.3182015 -4.318706][-4.3065457 -4.292429 -4.28246 -4.2802777 -4.283535 -4.2914476 -4.3033104 -4.3156343 -4.3254185 -4.3310089 -4.3305058 -4.3265624 -4.3229818 -4.3207169 -4.3186469][-4.2911386 -4.2718225 -4.2558732 -4.2471623 -4.2456169 -4.2531919 -4.2674265 -4.2834654 -4.3012619 -4.3187943 -4.328095 -4.3311782 -4.331924 -4.3302112 -4.3249378][-4.2713985 -4.2463927 -4.2213459 -4.2040567 -4.1968527 -4.2025971 -4.2182217 -4.2361321 -4.2600355 -4.28627 -4.3058381 -4.3213925 -4.3323183 -4.3365602 -4.33105][-4.2473469 -4.217948 -4.1800795 -4.1488428 -4.1335015 -4.1340733 -4.150269 -4.171216 -4.2041683 -4.2399468 -4.26678 -4.2932181 -4.3174634 -4.3325911 -4.3328505][-4.2261844 -4.1909132 -4.1422567 -4.0956521 -4.0657554 -4.0519075 -4.0594769 -4.0818887 -4.1273122 -4.1776419 -4.2172956 -4.2543821 -4.2897086 -4.3157392 -4.3254671][-4.211555 -4.1729074 -4.1185727 -4.0614791 -4.01521 -3.9789395 -3.960278 -3.9698246 -4.0246224 -4.0956869 -4.1531544 -4.2041855 -4.2507048 -4.2869191 -4.3069124][-4.2061357 -4.1701546 -4.1206264 -4.066247 -4.0140147 -3.9560401 -3.9000554 -3.8699224 -3.9134293 -3.9990079 -4.074945 -4.1402783 -4.2013988 -4.2497821 -4.2810407][-4.2091837 -4.1787677 -4.1399641 -4.0951042 -4.0468092 -3.9828205 -3.9081197 -3.8394012 -3.8406823 -3.9136982 -3.9974704 -4.0728111 -4.1493778 -4.2136626 -4.2576623][-4.2196488 -4.1961455 -4.1704249 -4.1392651 -4.0973225 -4.0442567 -3.9809713 -3.9097111 -3.8762727 -3.9060316 -3.9703648 -4.0398755 -4.1200147 -4.1952915 -4.2478504][-4.239151 -4.22197 -4.208456 -4.1899252 -4.1575069 -4.1184039 -4.0767336 -4.0285583 -3.9954019 -3.9956546 -4.0226727 -4.0659952 -4.1323309 -4.2035661 -4.2538528][-4.2619123 -4.2477927 -4.2400713 -4.2294946 -4.2074471 -4.18147 -4.1592875 -4.1377912 -4.1228561 -4.119617 -4.1247783 -4.1422873 -4.1836696 -4.2361355 -4.2741337][-4.2871175 -4.2751026 -4.2693367 -4.2639437 -4.2529936 -4.2382584 -4.2272787 -4.2209759 -4.2182846 -4.2185316 -4.2175469 -4.2233853 -4.2452369 -4.2760825 -4.2991257][-4.3080373 -4.2993455 -4.2941189 -4.2904406 -4.286087 -4.2806783 -4.2765255 -4.2754498 -4.2759523 -4.2779493 -4.2785535 -4.281045 -4.2917633 -4.3074794 -4.3188319][-4.3205805 -4.3156624 -4.3129439 -4.3100905 -4.3074574 -4.3055754 -4.3052506 -4.3052125 -4.3057632 -4.3074136 -4.3086557 -4.3095574 -4.3136292 -4.3205061 -4.3255858]]...]
INFO - root - 2017-12-06 10:05:32.758649: step 910, loss = 2.10, batch loss = 2.05 (19.2 examples/sec; 0.416 sec/batch; 38h:20m:56s remains)
INFO - root - 2017-12-06 10:05:37.049097: step 920, loss = 2.10, batch loss = 2.05 (18.2 examples/sec; 0.440 sec/batch; 40h:30m:41s remains)
INFO - root - 2017-12-06 10:05:41.342546: step 930, loss = 2.10, batch loss = 2.04 (18.1 examples/sec; 0.442 sec/batch; 40h:43m:58s remains)
INFO - root - 2017-12-06 10:05:45.646996: step 940, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 40h:14m:11s remains)
INFO - root - 2017-12-06 10:05:49.937754: step 950, loss = 2.11, batch loss = 2.05 (18.6 examples/sec; 0.430 sec/batch; 39h:34m:01s remains)
INFO - root - 2017-12-06 10:05:54.120960: step 960, loss = 2.09, batch loss = 2.03 (19.2 examples/sec; 0.416 sec/batch; 38h:18m:12s remains)
INFO - root - 2017-12-06 10:05:58.557426: step 970, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 0.530 sec/batch; 48h:48m:01s remains)
INFO - root - 2017-12-06 10:06:02.950459: step 980, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.444 sec/batch; 40h:53m:01s remains)
INFO - root - 2017-12-06 10:06:07.148567: step 990, loss = 2.05, batch loss = 1.99 (18.8 examples/sec; 0.426 sec/batch; 39h:13m:09s remains)
INFO - root - 2017-12-06 10:06:11.432153: step 1000, loss = 2.05, batch loss = 1.99 (18.8 examples/sec; 0.425 sec/batch; 39h:06m:30s remains)
2017-12-06 10:06:11.910597: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1554575 -4.1605015 -4.174037 -4.1999407 -4.2155762 -4.2105851 -4.1906037 -4.17124 -4.1761918 -4.187314 -4.1980257 -4.2186842 -4.2439418 -4.2621 -4.2844748][-4.1483927 -4.15642 -4.1713257 -4.1959004 -4.2099934 -4.2059283 -4.1819148 -4.1580324 -4.1644855 -4.1819205 -4.2001019 -4.2237773 -4.2483578 -4.2660604 -4.2847729][-4.1394496 -4.1493034 -4.1642828 -4.1860065 -4.1986985 -4.1934371 -4.165194 -4.1371326 -4.1431308 -4.1658154 -4.1908889 -4.2170815 -4.2423763 -4.261107 -4.2809048][-4.13923 -4.1494365 -4.1581192 -4.1715236 -4.1777129 -4.1667075 -4.1330056 -4.1025648 -4.1127305 -4.145236 -4.175859 -4.2034507 -4.2323089 -4.2559714 -4.2788506][-4.1569195 -4.1623259 -4.1624417 -4.162797 -4.1544809 -4.13 -4.0854993 -4.050765 -4.0713711 -4.1202741 -4.1608372 -4.1945887 -4.2286673 -4.2564158 -4.2814031][-4.1937923 -4.1884723 -4.1767206 -4.1643686 -4.140799 -4.099987 -4.0371947 -3.9902327 -4.0237045 -4.0961461 -4.1527672 -4.1951332 -4.2333469 -4.2627177 -4.2873955][-4.2126493 -4.1992793 -4.1808457 -4.1621237 -4.1306233 -4.0797338 -4.00197 -3.9452422 -3.9917507 -4.0850549 -4.156013 -4.2058287 -4.2452803 -4.2735658 -4.2964654][-4.2220941 -4.2059221 -4.1880608 -4.1713104 -4.1445761 -4.1001039 -4.0294333 -3.9783766 -4.0234818 -4.1093168 -4.1740084 -4.2211323 -4.2571516 -4.2822986 -4.30343][-4.2211232 -4.20505 -4.1910224 -4.1825356 -4.1697717 -4.1445274 -4.0971341 -4.0600605 -4.0882277 -4.1449432 -4.1901026 -4.2280974 -4.2592726 -4.2821441 -4.3046117][-4.2137618 -4.1991138 -4.1885242 -4.1877589 -4.1868906 -4.1778383 -4.1497121 -4.121645 -4.1312842 -4.1642218 -4.1952891 -4.2266269 -4.2567878 -4.2806573 -4.3054132][-4.2055244 -4.1911254 -4.1808243 -4.1838021 -4.1910548 -4.1946096 -4.1810551 -4.1582184 -4.157444 -4.1771193 -4.1998439 -4.2281308 -4.2602687 -4.2861118 -4.3105016][-4.205337 -4.1928606 -4.1832509 -4.1882272 -4.1998649 -4.2094693 -4.2018032 -4.1807394 -4.1759534 -4.1891251 -4.2077961 -4.2366152 -4.2714891 -4.2977204 -4.3194203][-4.2227468 -4.2124615 -4.2055655 -4.2111053 -4.2214832 -4.2292132 -4.2200103 -4.1977062 -4.1931915 -4.2068 -4.2269483 -4.2565708 -4.289856 -4.312685 -4.3301015][-4.2501559 -4.2425408 -4.2380686 -4.2419395 -4.2477908 -4.2495308 -4.237711 -4.2171659 -4.2148452 -4.2298784 -4.2512093 -4.2783175 -4.3052516 -4.3231964 -4.3364086][-4.2691364 -4.2638483 -4.261713 -4.2645125 -4.2677126 -4.2674084 -4.2567134 -4.2400503 -4.2376914 -4.2497473 -4.2682662 -4.290205 -4.3107295 -4.325388 -4.336657]]...]
INFO - root - 2017-12-06 10:06:16.315981: step 1010, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.454 sec/batch; 41h:50m:28s remains)
INFO - root - 2017-12-06 10:06:20.675397: step 1020, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.430 sec/batch; 39h:35m:07s remains)
INFO - root - 2017-12-06 10:06:25.028662: step 1030, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.441 sec/batch; 40h:35m:51s remains)
INFO - root - 2017-12-06 10:06:29.354995: step 1040, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.430 sec/batch; 39h:35m:22s remains)
INFO - root - 2017-12-06 10:06:33.699955: step 1050, loss = 2.09, batch loss = 2.03 (19.1 examples/sec; 0.420 sec/batch; 38h:37m:42s remains)
INFO - root - 2017-12-06 10:06:38.062182: step 1060, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.444 sec/batch; 40h:50m:51s remains)
INFO - root - 2017-12-06 10:06:42.546188: step 1070, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.451 sec/batch; 41h:30m:53s remains)
INFO - root - 2017-12-06 10:06:46.908365: step 1080, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 40h:25m:15s remains)
INFO - root - 2017-12-06 10:06:50.908740: step 1090, loss = 2.08, batch loss = 2.02 (19.4 examples/sec; 0.413 sec/batch; 37h:59m:29s remains)
INFO - root - 2017-12-06 10:06:55.164446: step 1100, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.424 sec/batch; 39h:00m:05s remains)
2017-12-06 10:06:55.674213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2023945 -4.1773925 -4.149045 -4.130827 -4.1244469 -4.1324453 -4.1703467 -4.213511 -4.2441068 -4.2663178 -4.2836995 -4.2925773 -4.3071356 -4.3189921 -4.3219714][-4.2012515 -4.1772871 -4.140121 -4.0979714 -4.0590682 -4.05009 -4.1022677 -4.16995 -4.2177095 -4.2494564 -4.2700028 -4.2803588 -4.2986488 -4.3172617 -4.3264885][-4.2092328 -4.1916409 -4.1562362 -4.1060972 -4.0527806 -4.0306892 -4.0783706 -4.1507111 -4.204742 -4.2408905 -4.2617 -4.2714682 -4.2920718 -4.315124 -4.3267136][-4.2182527 -4.2037058 -4.1762185 -4.1368866 -4.0905838 -4.0651021 -4.0952415 -4.153728 -4.2024007 -4.2340484 -4.2539048 -4.2628021 -4.2852778 -4.3109317 -4.3252683][-4.2097578 -4.1948466 -4.1694617 -4.1411166 -4.107295 -4.0829186 -4.0946531 -4.1307397 -4.1734147 -4.206769 -4.2310877 -4.246654 -4.2726007 -4.3019238 -4.3201118][-4.1873126 -4.1737165 -4.1507111 -4.1280966 -4.0988317 -4.0704055 -4.0595403 -4.06619 -4.1018624 -4.1480393 -4.1872196 -4.2170124 -4.2505722 -4.2866125 -4.3113809][-4.1770468 -4.1628428 -4.1426578 -4.1266975 -4.0994115 -4.0626769 -4.0347805 -4.0179939 -4.0417914 -4.0984559 -4.1500196 -4.1908727 -4.2292094 -4.2680521 -4.2998161][-4.1977143 -4.1803384 -4.1598759 -4.1454225 -4.1215887 -4.0829692 -4.0517974 -4.0323253 -4.0461712 -4.0952396 -4.1448812 -4.1853418 -4.2231393 -4.2602277 -4.2930994][-4.227252 -4.2113161 -4.1909642 -4.1716771 -4.145071 -4.1085353 -4.0843687 -4.0728397 -4.0798821 -4.1120276 -4.1533904 -4.1920671 -4.2301846 -4.2649302 -4.2966027][-4.2345872 -4.22593 -4.2128325 -4.1922107 -4.1599402 -4.1207323 -4.09814 -4.0934362 -4.097517 -4.1167111 -4.1528931 -4.1947389 -4.2338586 -4.2689042 -4.2998123][-4.219142 -4.2215991 -4.2228341 -4.2100229 -4.1800737 -4.1391439 -4.1110253 -4.1032786 -4.1017294 -4.111814 -4.1448612 -4.1875067 -4.2283378 -4.2666016 -4.2984767][-4.2138495 -4.221911 -4.2321754 -4.2289147 -4.20885 -4.1730056 -4.1429181 -4.1286907 -4.119617 -4.1192985 -4.1429243 -4.1822338 -4.2228613 -4.2619748 -4.2925143][-4.231214 -4.2369556 -4.2443309 -4.2440839 -4.2347689 -4.2137547 -4.191083 -4.1726971 -4.1550436 -4.145205 -4.1567125 -4.1861138 -4.223628 -4.2611661 -4.2893124][-4.2449503 -4.2460594 -4.2471371 -4.2463555 -4.2451582 -4.2379193 -4.2281561 -4.2110147 -4.1879916 -4.17269 -4.1759181 -4.1965151 -4.228333 -4.2627487 -4.28708][-4.2438431 -4.2405515 -4.2377429 -4.2368259 -4.2400546 -4.2422872 -4.2408481 -4.2257662 -4.2009258 -4.1833992 -4.1831393 -4.1978359 -4.2257824 -4.2575893 -4.279139]]...]
INFO - root - 2017-12-06 10:07:00.035083: step 1110, loss = 2.06, batch loss = 2.00 (18.3 examples/sec; 0.438 sec/batch; 40h:17m:00s remains)
INFO - root - 2017-12-06 10:07:04.426156: step 1120, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.453 sec/batch; 41h:41m:54s remains)
INFO - root - 2017-12-06 10:07:08.724705: step 1130, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.420 sec/batch; 38h:41m:00s remains)
INFO - root - 2017-12-06 10:07:12.999852: step 1140, loss = 2.09, batch loss = 2.03 (19.4 examples/sec; 0.413 sec/batch; 38h:00m:25s remains)
INFO - root - 2017-12-06 10:07:17.167960: step 1150, loss = 2.08, batch loss = 2.03 (18.9 examples/sec; 0.422 sec/batch; 38h:52m:52s remains)
INFO - root - 2017-12-06 10:07:21.396854: step 1160, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.430 sec/batch; 39h:33m:10s remains)
INFO - root - 2017-12-06 10:07:25.655691: step 1170, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.424 sec/batch; 38h:59m:34s remains)
INFO - root - 2017-12-06 10:07:29.914535: step 1180, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 39h:01m:29s remains)
INFO - root - 2017-12-06 10:07:33.959609: step 1190, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.427 sec/batch; 39h:18m:05s remains)
INFO - root - 2017-12-06 10:07:38.230952: step 1200, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.425 sec/batch; 39h:08m:56s remains)
2017-12-06 10:07:38.797090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2627969 -4.2666883 -4.2811794 -4.2947521 -4.2994676 -4.2994461 -4.3012528 -4.3066621 -4.3117962 -4.3157773 -4.3147521 -4.3122554 -4.3094893 -4.3063822 -4.3019633][-4.2354393 -4.2323017 -4.2512431 -4.2752113 -4.2900405 -4.29774 -4.30263 -4.3129382 -4.3238573 -4.3329492 -4.3292222 -4.3165221 -4.3031268 -4.2946296 -4.2854891][-4.2020755 -4.1896996 -4.2050571 -4.2403445 -4.2665772 -4.2794275 -4.285274 -4.2981548 -4.3154154 -4.3307238 -4.3322067 -4.3180132 -4.2997775 -4.2881131 -4.2726712][-4.1835122 -4.1708951 -4.1839218 -4.2251992 -4.2539945 -4.2611337 -4.2604775 -4.2700377 -4.2895203 -4.3093343 -4.3188753 -4.313055 -4.3001947 -4.2891159 -4.2697453][-4.187572 -4.1788573 -4.1906619 -4.2276478 -4.249227 -4.2411427 -4.222044 -4.22363 -4.2493038 -4.2761559 -4.2965693 -4.3058081 -4.3080873 -4.3004823 -4.2783189][-4.1926217 -4.190701 -4.2021794 -4.233005 -4.2414641 -4.207438 -4.1571851 -4.1349869 -4.171742 -4.2204695 -4.2587714 -4.2854714 -4.3043008 -4.3082404 -4.2907219][-4.1905336 -4.1937304 -4.204896 -4.2241945 -4.2164979 -4.1591063 -4.0693 -4.0012422 -4.0461187 -4.1378679 -4.20913 -4.2571254 -4.291975 -4.3100896 -4.3008838][-4.1930189 -4.1980562 -4.2081127 -4.2105002 -4.1845064 -4.1103024 -3.9887466 -3.8733766 -3.9243622 -4.0618863 -4.1651278 -4.230824 -4.2751117 -4.3025012 -4.3045659][-4.1951017 -4.2007289 -4.2096391 -4.2019 -4.1733508 -4.1043854 -3.9994247 -3.8976064 -3.9336591 -4.0581484 -4.1539874 -4.2174635 -4.26087 -4.2890935 -4.3009977][-4.1945481 -4.2019682 -4.2112546 -4.20873 -4.194025 -4.1522393 -4.0968351 -4.0444732 -4.058054 -4.1251993 -4.1834331 -4.2251859 -4.2554517 -4.278686 -4.2933035][-4.2060781 -4.2139831 -4.2255692 -4.23202 -4.2310715 -4.2146859 -4.1973162 -4.1823688 -4.1851888 -4.2090416 -4.2328568 -4.2498717 -4.2600946 -4.2717595 -4.2842855][-4.2366457 -4.2390881 -4.2476907 -4.2546782 -4.2576556 -4.257092 -4.2649641 -4.2747951 -4.2781606 -4.2781405 -4.2770014 -4.2758756 -4.2687221 -4.2668538 -4.2752633][-4.279367 -4.2764969 -4.2781944 -4.2769818 -4.2756166 -4.2800369 -4.3012877 -4.3202891 -4.3199472 -4.3057137 -4.2900524 -4.28113 -4.2692237 -4.2618012 -4.2664366][-4.3014641 -4.2940145 -4.2924433 -4.2885361 -4.2834678 -4.2862 -4.3040309 -4.319077 -4.3139138 -4.2932134 -4.2741494 -4.2650013 -4.2558327 -4.2535481 -4.2602863][-4.3000088 -4.2944479 -4.2955923 -4.2950873 -4.2911477 -4.2919254 -4.2990141 -4.3031945 -4.2948608 -4.2745194 -4.258903 -4.2502179 -4.242063 -4.2452908 -4.2612286]]...]
INFO - root - 2017-12-06 10:07:42.943861: step 1210, loss = 2.08, batch loss = 2.03 (19.2 examples/sec; 0.416 sec/batch; 38h:15m:48s remains)
INFO - root - 2017-12-06 10:07:47.185663: step 1220, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.427 sec/batch; 39h:16m:37s remains)
INFO - root - 2017-12-06 10:07:51.520194: step 1230, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.454 sec/batch; 41h:44m:05s remains)
INFO - root - 2017-12-06 10:07:55.806973: step 1240, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.438 sec/batch; 40h:19m:15s remains)
INFO - root - 2017-12-06 10:08:00.170332: step 1250, loss = 2.11, batch loss = 2.05 (18.4 examples/sec; 0.434 sec/batch; 39h:55m:47s remains)
INFO - root - 2017-12-06 10:08:04.436889: step 1260, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 40h:26m:18s remains)
INFO - root - 2017-12-06 10:08:08.840301: step 1270, loss = 2.09, batch loss = 2.03 (18.1 examples/sec; 0.443 sec/batch; 40h:44m:34s remains)
INFO - root - 2017-12-06 10:08:13.230701: step 1280, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.452 sec/batch; 41h:36m:23s remains)
INFO - root - 2017-12-06 10:08:18.211728: step 1290, loss = 2.10, batch loss = 2.05 (16.3 examples/sec; 0.490 sec/batch; 45h:04m:07s remains)
INFO - root - 2017-12-06 10:08:23.311813: step 1300, loss = 2.08, batch loss = 2.03 (15.9 examples/sec; 0.502 sec/batch; 46h:10m:07s remains)
2017-12-06 10:08:23.835133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2882209 -4.2843962 -4.2566676 -4.2236886 -4.186533 -4.155457 -4.1486449 -4.1758223 -4.2171178 -4.2439208 -4.2548161 -4.2531633 -4.2539473 -4.2594 -4.2707005][-4.2703776 -4.2636213 -4.2339282 -4.193202 -4.1433558 -4.1017523 -4.0916982 -4.1183348 -4.1646786 -4.2022076 -4.2234139 -4.2262592 -4.2280941 -4.2288442 -4.2342][-4.2319322 -4.2194362 -4.1910172 -4.1497483 -4.09835 -4.0564957 -4.0478859 -4.0761986 -4.123405 -4.16407 -4.1884255 -4.1916118 -4.1892538 -4.1855178 -4.1858211][-4.1837754 -4.1698203 -4.1469569 -4.1132193 -4.06936 -4.0298996 -4.0180988 -4.0479937 -4.0957837 -4.1355786 -4.1621795 -4.1667871 -4.1608114 -4.1539655 -4.1518769][-4.1486697 -4.1392 -4.12448 -4.09774 -4.0550122 -4.0039196 -3.97461 -4.0076795 -4.0683546 -4.1203561 -4.1566682 -4.1672378 -4.161417 -4.1547685 -4.151371][-4.1389694 -4.1311383 -4.1208644 -4.0913181 -4.0403647 -3.9704833 -3.915108 -3.9546845 -4.0410023 -4.1196952 -4.1752429 -4.194313 -4.1897497 -4.18548 -4.1799927][-4.152566 -4.1443624 -4.1342697 -4.0986505 -4.0427079 -3.9732668 -3.914398 -3.954766 -4.0465326 -4.1355891 -4.1995091 -4.2235026 -4.2252049 -4.2281876 -4.2247686][-4.1599054 -4.1520567 -4.1450381 -4.1140571 -4.0731044 -4.0304585 -3.997396 -4.0264487 -4.0923352 -4.1605821 -4.214437 -4.2365522 -4.2466283 -4.2584996 -4.257236][-4.1478629 -4.1409559 -4.1458664 -4.1331077 -4.1110244 -4.0962992 -4.0864525 -4.1043611 -4.1380763 -4.1783786 -4.2177958 -4.2367487 -4.2511983 -4.2658825 -4.2657895][-4.1399488 -4.1425886 -4.1611576 -4.1628361 -4.1480842 -4.1418104 -4.1413803 -4.1480312 -4.1630278 -4.1861033 -4.2146459 -4.2322388 -4.2481351 -4.257103 -4.2515774][-4.1537147 -4.1645441 -4.1871185 -4.1931429 -4.1802168 -4.175127 -4.174005 -4.1752748 -4.186059 -4.2014236 -4.2201781 -4.2331829 -4.2445779 -4.2397361 -4.2202439][-4.1808009 -4.1928186 -4.2108526 -4.2161646 -4.2064691 -4.2032142 -4.198998 -4.1997118 -4.2094574 -4.2179832 -4.2271142 -4.235734 -4.2422729 -4.2277122 -4.1971121][-4.20568 -4.2199349 -4.2373271 -4.2422795 -4.2355561 -4.2324514 -4.2279172 -4.22653 -4.22731 -4.2258368 -4.225564 -4.2266712 -4.227766 -4.2161946 -4.1920924][-4.2164793 -4.2344394 -4.2583771 -4.2676468 -4.264009 -4.2615347 -4.2583437 -4.2537723 -4.243784 -4.2335362 -4.2218904 -4.2124429 -4.211525 -4.2123771 -4.2068439][-4.22679 -4.2428055 -4.2696185 -4.2859979 -4.2903061 -4.2908573 -4.2882938 -4.2804489 -4.2640781 -4.2492552 -4.2321935 -4.2141647 -4.2096863 -4.2187114 -4.2245431]]...]
INFO - root - 2017-12-06 10:08:28.762126: step 1310, loss = 2.09, batch loss = 2.04 (15.8 examples/sec; 0.506 sec/batch; 46h:32m:37s remains)
INFO - root - 2017-12-06 10:08:33.168277: step 1320, loss = 2.08, batch loss = 2.02 (19.5 examples/sec; 0.409 sec/batch; 37h:39m:57s remains)
INFO - root - 2017-12-06 10:08:37.393255: step 1330, loss = 2.10, batch loss = 2.04 (19.1 examples/sec; 0.419 sec/batch; 38h:33m:47s remains)
INFO - root - 2017-12-06 10:08:41.708991: step 1340, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 39h:05m:28s remains)
INFO - root - 2017-12-06 10:08:45.928526: step 1350, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.425 sec/batch; 39h:03m:34s remains)
INFO - root - 2017-12-06 10:08:50.159876: step 1360, loss = 2.07, batch loss = 2.02 (18.9 examples/sec; 0.424 sec/batch; 39h:00m:51s remains)
INFO - root - 2017-12-06 10:08:54.444624: step 1370, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.414 sec/batch; 38h:03m:02s remains)
INFO - root - 2017-12-06 10:08:58.816165: step 1380, loss = 2.06, batch loss = 2.01 (19.0 examples/sec; 0.421 sec/batch; 38h:40m:47s remains)
INFO - root - 2017-12-06 10:09:03.005893: step 1390, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.425 sec/batch; 39h:04m:49s remains)
INFO - root - 2017-12-06 10:09:07.211544: step 1400, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.429 sec/batch; 39h:29m:15s remains)
2017-12-06 10:09:07.725478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2081513 -4.2104549 -4.2059708 -4.1984668 -4.1945662 -4.1946826 -4.2002811 -4.2030134 -4.200542 -4.1952004 -4.1926823 -4.1914525 -4.1874704 -4.1858134 -4.1946349][-4.1934481 -4.199399 -4.1987557 -4.1940551 -4.1919961 -4.1936984 -4.1992984 -4.201797 -4.2007952 -4.1971192 -4.1962013 -4.1952562 -4.1911144 -4.1859331 -4.187736][-4.189692 -4.1945505 -4.1947117 -4.1910119 -4.1896167 -4.192637 -4.1993041 -4.2013788 -4.1994104 -4.19675 -4.1989322 -4.2001781 -4.1962051 -4.1883354 -4.184854][-4.1796088 -4.1815848 -4.182611 -4.1827612 -4.1826782 -4.1847949 -4.1920605 -4.1959376 -4.1967263 -4.1976023 -4.2045755 -4.2098107 -4.204051 -4.1909952 -4.1819429][-4.16161 -4.1588945 -4.1618457 -4.1658559 -4.164577 -4.16176 -4.1643124 -4.1646962 -4.1670437 -4.1717958 -4.1801815 -4.1890578 -4.186594 -4.1741514 -4.16351][-4.1473417 -4.135149 -4.1331611 -4.1315556 -4.1224213 -4.1095433 -4.1019821 -4.0971437 -4.1053915 -4.1225677 -4.1388092 -4.1539488 -4.1592174 -4.1541362 -4.1451511][-4.165678 -4.1375971 -4.116991 -4.0983 -4.0717134 -4.0400772 -4.0201941 -4.0146284 -4.0351334 -4.07292 -4.1082296 -4.1363029 -4.1525655 -4.157598 -4.1534877][-4.1890178 -4.1499739 -4.1148853 -4.0835047 -4.0431743 -3.9926984 -3.9586272 -3.9510524 -3.9782012 -4.0367808 -4.0948782 -4.1448407 -4.18122 -4.2019157 -4.2060571][-4.2091041 -4.1715322 -4.1384583 -4.109529 -4.0706148 -4.018219 -3.9834611 -3.9713292 -3.9884963 -4.0463929 -4.109055 -4.1667037 -4.2118626 -4.2415805 -4.25345][-4.2251015 -4.1942339 -4.1723289 -4.1538477 -4.1293807 -4.0948868 -4.072958 -4.0642152 -4.0737281 -4.1183481 -4.1633191 -4.2024832 -4.2318721 -4.2498951 -4.2588987][-4.2426319 -4.2142439 -4.1939883 -4.1793256 -4.1670184 -4.1502895 -4.1410871 -4.1390338 -4.1506381 -4.1847053 -4.2114615 -4.2275853 -4.2344003 -4.2313495 -4.2281523][-4.262424 -4.2389951 -4.2191095 -4.2048988 -4.1974354 -4.1883388 -4.180027 -4.1770978 -4.1837811 -4.203917 -4.21637 -4.2174163 -4.2111826 -4.2000651 -4.1957712][-4.2786469 -4.2645845 -4.2484789 -4.2361116 -4.2288766 -4.2186623 -4.2047038 -4.1932435 -4.1876717 -4.1909914 -4.1899033 -4.1890988 -4.1896667 -4.1903634 -4.1979427][-4.2847319 -4.2821584 -4.2711983 -4.260561 -4.2529373 -4.240509 -4.2225428 -4.2036419 -4.1904764 -4.1833453 -4.177268 -4.1818581 -4.1946607 -4.2108631 -4.2335811][-4.2761726 -4.2857471 -4.2803836 -4.2716722 -4.2625518 -4.250917 -4.2341385 -4.2166409 -4.2042465 -4.1950436 -4.1883807 -4.1966848 -4.2171378 -4.2421927 -4.2726159]]...]
INFO - root - 2017-12-06 10:09:12.047364: step 1410, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.427 sec/batch; 39h:14m:48s remains)
INFO - root - 2017-12-06 10:09:16.340295: step 1420, loss = 2.08, batch loss = 2.03 (18.4 examples/sec; 0.435 sec/batch; 40h:00m:10s remains)
INFO - root - 2017-12-06 10:09:20.618231: step 1430, loss = 2.10, batch loss = 2.04 (19.2 examples/sec; 0.416 sec/batch; 38h:16m:43s remains)
INFO - root - 2017-12-06 10:09:24.918469: step 1440, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.445 sec/batch; 40h:56m:06s remains)
INFO - root - 2017-12-06 10:09:29.233622: step 1450, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.423 sec/batch; 38h:53m:51s remains)
INFO - root - 2017-12-06 10:09:33.575726: step 1460, loss = 2.10, batch loss = 2.04 (18.3 examples/sec; 0.437 sec/batch; 40h:11m:28s remains)
INFO - root - 2017-12-06 10:09:37.916235: step 1470, loss = 2.10, batch loss = 2.04 (17.2 examples/sec; 0.465 sec/batch; 42h:47m:19s remains)
INFO - root - 2017-12-06 10:09:42.234127: step 1480, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 39h:41m:16s remains)
INFO - root - 2017-12-06 10:09:46.232664: step 1490, loss = 2.09, batch loss = 2.04 (18.1 examples/sec; 0.441 sec/batch; 40h:33m:25s remains)
INFO - root - 2017-12-06 10:09:50.531858: step 1500, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.426 sec/batch; 39h:08m:22s remains)
2017-12-06 10:09:51.031749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2642474 -4.2465219 -4.23739 -4.2477536 -4.2596512 -4.2659707 -4.2762804 -4.2948389 -4.3040686 -4.291429 -4.26045 -4.2500315 -4.2761645 -4.2930169 -4.2932696][-4.19595 -4.1771469 -4.178648 -4.20732 -4.2272458 -4.2342067 -4.2452397 -4.2662849 -4.2731338 -4.2568579 -4.2286797 -4.2236485 -4.2556028 -4.2768917 -4.2813425][-4.1498775 -4.1339197 -4.1461425 -4.1846151 -4.2046838 -4.2083259 -4.2173085 -4.2382855 -4.24587 -4.2343326 -4.2179089 -4.21948 -4.2520018 -4.2732286 -4.2793541][-4.13477 -4.1186657 -4.1367416 -4.1754961 -4.1906338 -4.1905918 -4.1932678 -4.20968 -4.2191186 -4.2159138 -4.21212 -4.2222195 -4.2572155 -4.2791166 -4.2866445][-4.134553 -4.11496 -4.132575 -4.1646533 -4.1754212 -4.1728377 -4.1734872 -4.1868739 -4.1987748 -4.2005181 -4.1993656 -4.2160196 -4.25733 -4.2852893 -4.295311][-4.147872 -4.1286397 -4.1389165 -4.1599703 -4.1642518 -4.1542468 -4.15016 -4.1622052 -4.176373 -4.1812263 -4.1778259 -4.1962633 -4.239563 -4.274878 -4.29065][-4.1561646 -4.1415782 -4.1468496 -4.155117 -4.1462193 -4.1222038 -4.1104612 -4.1194587 -4.1367 -4.1428142 -4.1377363 -4.1535072 -4.198266 -4.24372 -4.2707405][-4.139297 -4.1337681 -4.1410561 -4.1403594 -4.1207871 -4.0842023 -4.0617981 -4.0647383 -4.0834165 -4.0924664 -4.0819349 -4.0903878 -4.1368861 -4.1977882 -4.2398214][-4.1140513 -4.1170526 -4.127902 -4.122818 -4.097259 -4.0551548 -4.0220728 -4.0173492 -4.036706 -4.0495224 -4.0319672 -4.0308056 -4.0752888 -4.143599 -4.1964893][-4.1014938 -4.10417 -4.1151643 -4.1115532 -4.090837 -4.054441 -4.0202684 -4.0109854 -4.0258541 -4.0375209 -4.0161057 -4.0067334 -4.0435085 -4.1052823 -4.1576343][-4.0900536 -4.0914507 -4.1027851 -4.1037803 -4.0964932 -4.0778675 -4.0532093 -4.0411139 -4.0459332 -4.0508795 -4.0272532 -4.0106516 -4.0371156 -4.0879736 -4.1328769][-4.1064816 -4.1087537 -4.1197977 -4.1229019 -4.1248145 -4.1205688 -4.1059813 -4.0908537 -4.0865793 -4.087 -4.0702085 -4.0546761 -4.071815 -4.1103325 -4.1474571][-4.1487565 -4.1523037 -4.1639214 -4.1691408 -4.1738553 -4.1766744 -4.1709752 -4.1587472 -4.1495433 -4.1457181 -4.1348414 -4.1219039 -4.1289458 -4.1538382 -4.1835842][-4.2014346 -4.2054119 -4.2173104 -4.2244315 -4.228972 -4.2318759 -4.231287 -4.2251139 -4.21758 -4.2118416 -4.20515 -4.1971989 -4.1993942 -4.2127891 -4.2324762][-4.2518296 -4.254653 -4.2637291 -4.2704215 -4.2750888 -4.2775979 -4.2789278 -4.2783604 -4.2755094 -4.2715688 -4.2662148 -4.2619877 -4.2646027 -4.2724886 -4.2834983]]...]
INFO - root - 2017-12-06 10:09:55.380129: step 1510, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.433 sec/batch; 39h:46m:41s remains)
INFO - root - 2017-12-06 10:10:00.329477: step 1520, loss = 2.07, batch loss = 2.01 (16.0 examples/sec; 0.499 sec/batch; 45h:52m:17s remains)
INFO - root - 2017-12-06 10:10:05.373749: step 1530, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 0.516 sec/batch; 47h:27m:55s remains)
INFO - root - 2017-12-06 10:10:10.403796: step 1540, loss = 2.09, batch loss = 2.03 (15.9 examples/sec; 0.503 sec/batch; 46h:13m:11s remains)
INFO - root - 2017-12-06 10:10:15.468730: step 1550, loss = 2.07, batch loss = 2.02 (15.6 examples/sec; 0.512 sec/batch; 47h:04m:36s remains)
INFO - root - 2017-12-06 10:10:20.620580: step 1560, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 0.506 sec/batch; 46h:30m:33s remains)
INFO - root - 2017-12-06 10:10:25.732304: step 1570, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 0.516 sec/batch; 47h:27m:13s remains)
INFO - root - 2017-12-06 10:10:30.870998: step 1580, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 0.528 sec/batch; 48h:34m:25s remains)
INFO - root - 2017-12-06 10:10:35.861785: step 1590, loss = 2.09, batch loss = 2.03 (19.5 examples/sec; 0.411 sec/batch; 37h:46m:44s remains)
INFO - root - 2017-12-06 10:10:41.047444: step 1600, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 0.528 sec/batch; 48h:32m:17s remains)
2017-12-06 10:10:41.585072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1946139 -4.2032652 -4.2014213 -4.1956248 -4.1896634 -4.1851668 -4.175118 -4.1617007 -4.1592717 -4.1734281 -4.1894121 -4.1902208 -4.1837945 -4.1871185 -4.1875968][-4.1814828 -4.190527 -4.1857862 -4.1778917 -4.1679711 -4.1614223 -4.1504831 -4.1353555 -4.1333508 -4.1501532 -4.1696167 -4.1709962 -4.1679239 -4.171545 -4.1630383][-4.1720438 -4.1826491 -4.1778159 -4.1730571 -4.164577 -4.1590891 -4.1478925 -4.1309395 -4.12964 -4.1434455 -4.1543918 -4.1564164 -4.1625071 -4.1747074 -4.1659908][-4.1486859 -4.1644835 -4.1676035 -4.1713185 -4.1660724 -4.1613312 -4.1485987 -4.1302276 -4.124135 -4.1271944 -4.1253109 -4.1280823 -4.1497369 -4.1721177 -4.1644506][-4.1191607 -4.139194 -4.1526747 -4.1670322 -4.1694922 -4.1652803 -4.1552496 -4.1421213 -4.1344028 -4.1243238 -4.1111641 -4.1169353 -4.1500645 -4.1707144 -4.1581154][-4.0931492 -4.1146622 -4.1297188 -4.1448603 -4.1459875 -4.1392193 -4.1349463 -4.1304588 -4.1219039 -4.1043172 -4.0942593 -4.1107087 -4.1492491 -4.1648626 -4.1465535][-4.0656242 -4.0873766 -4.1004577 -4.1107531 -4.1019945 -4.0861263 -4.0796118 -4.0728345 -4.0528331 -4.0251331 -4.0247965 -4.0606384 -4.1085653 -4.1260505 -4.1092973][-4.0609126 -4.0826941 -4.089828 -4.0906582 -4.0726185 -4.0496445 -4.0342922 -4.0106258 -3.9570813 -3.8995986 -3.9116147 -3.980881 -4.049005 -4.0795379 -4.0796008][-4.0893893 -4.113636 -4.1123319 -4.1012154 -4.077754 -4.0596476 -4.0511675 -4.0260391 -3.9576578 -3.8881817 -3.9063315 -3.98878 -4.0550857 -4.0873318 -4.1010537][-4.1317158 -4.1553879 -4.14968 -4.1317458 -4.1098104 -4.105206 -4.1145334 -4.1098804 -4.0685487 -4.0260715 -4.0420871 -4.096139 -4.1346431 -4.1537008 -4.1673703][-4.18577 -4.2049365 -4.1962938 -4.1771445 -4.1581969 -4.1582904 -4.1737022 -4.1778946 -4.1599565 -4.1468182 -4.1651158 -4.1953526 -4.2113266 -4.220098 -4.2335849][-4.2403107 -4.2571797 -4.25023 -4.2343745 -4.2194309 -4.2196794 -4.2307415 -4.2321544 -4.2215009 -4.2213607 -4.244205 -4.2700925 -4.2782292 -4.2793012 -4.2876835][-4.2721567 -4.2824874 -4.2781029 -4.2710228 -4.2650132 -4.2683406 -4.2765126 -4.276947 -4.2673073 -4.2681842 -4.288866 -4.312778 -4.3188457 -4.3165178 -4.3174052][-4.2904325 -4.2914014 -4.2864795 -4.2865605 -4.2880058 -4.2938814 -4.3002052 -4.3024707 -4.2981482 -4.2982922 -4.3085032 -4.320766 -4.3225212 -4.319191 -4.3176332][-4.2922411 -4.2849731 -4.2787194 -4.2836127 -4.2898722 -4.2970366 -4.3022037 -4.3040671 -4.3029585 -4.3012667 -4.3012061 -4.2989573 -4.2965183 -4.2954535 -4.2979684]]...]
INFO - root - 2017-12-06 10:10:46.770773: step 1610, loss = 2.10, batch loss = 2.04 (15.7 examples/sec; 0.510 sec/batch; 46h:53m:25s remains)
INFO - root - 2017-12-06 10:10:52.015855: step 1620, loss = 2.05, batch loss = 1.99 (15.1 examples/sec; 0.531 sec/batch; 48h:48m:31s remains)
INFO - root - 2017-12-06 10:10:57.174761: step 1630, loss = 2.08, batch loss = 2.02 (15.9 examples/sec; 0.502 sec/batch; 46h:07m:01s remains)
INFO - root - 2017-12-06 10:11:02.260515: step 1640, loss = 2.07, batch loss = 2.02 (16.0 examples/sec; 0.501 sec/batch; 46h:01m:34s remains)
INFO - root - 2017-12-06 10:11:07.323304: step 1650, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 41h:59m:50s remains)
INFO - root - 2017-12-06 10:11:12.445349: step 1660, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 0.496 sec/batch; 45h:36m:20s remains)
INFO - root - 2017-12-06 10:11:17.669710: step 1670, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 0.512 sec/batch; 47h:03m:41s remains)
INFO - root - 2017-12-06 10:11:22.877381: step 1680, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 0.516 sec/batch; 47h:26m:35s remains)
INFO - root - 2017-12-06 10:11:27.991860: step 1690, loss = 2.10, batch loss = 2.04 (16.2 examples/sec; 0.494 sec/batch; 45h:22m:51s remains)
INFO - root - 2017-12-06 10:11:32.798735: step 1700, loss = 2.09, batch loss = 2.03 (16.2 examples/sec; 0.493 sec/batch; 45h:15m:48s remains)
2017-12-06 10:11:33.463405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0953484 -4.0644884 -4.1043077 -4.1893711 -4.26987 -4.3192887 -4.3338614 -4.3211312 -4.3102732 -4.3050942 -4.312324 -4.3152037 -4.2999468 -4.2699804 -4.2499833][-4.0355978 -3.9967823 -4.0520959 -4.1627803 -4.2542109 -4.29917 -4.3065343 -4.296279 -4.29966 -4.305346 -4.3189898 -4.3256216 -4.3071523 -4.2704077 -4.245779][-4.0324535 -3.9986627 -4.0617685 -4.1767797 -4.2585597 -4.2816849 -4.268189 -4.2553525 -4.2713952 -4.2919168 -4.320941 -4.3400264 -4.3218184 -4.2789407 -4.2474842][-4.0990939 -4.0845113 -4.1366711 -4.2252841 -4.2786117 -4.2699409 -4.2224545 -4.2006664 -4.2254982 -4.2642169 -4.3133907 -4.3458166 -4.3319468 -4.2889338 -4.2546568][-4.1737275 -4.1815228 -4.2150249 -4.2574039 -4.2650261 -4.2141781 -4.1309791 -4.1036325 -4.1543894 -4.2268963 -4.2968326 -4.3371453 -4.3262877 -4.2860341 -4.2540755][-4.2310548 -4.2427244 -4.2508373 -4.2467823 -4.1999874 -4.0905638 -3.9654152 -3.9479933 -4.0491505 -4.1685815 -4.261652 -4.3050833 -4.2938051 -4.2590475 -4.2350135][-4.2642117 -4.2729506 -4.2615933 -4.2233834 -4.1281481 -3.9574971 -3.7872066 -3.7933707 -3.951014 -4.1083684 -4.2165546 -4.2615032 -4.2523541 -4.2220016 -4.2081604][-4.292573 -4.2950258 -4.2711558 -4.2177453 -4.1149187 -3.9492195 -3.8066418 -3.8361325 -3.9858122 -4.1247144 -4.2143993 -4.2441058 -4.2244582 -4.1929421 -4.1921725][-4.3063602 -4.30616 -4.2802482 -4.2335281 -4.1584063 -4.0518274 -3.9826655 -4.0170445 -4.117487 -4.2094612 -4.2607365 -4.2595153 -4.2230654 -4.1912627 -4.1995025][-4.30715 -4.3054662 -4.2836924 -4.2517166 -4.2068191 -4.1518278 -4.1247244 -4.155282 -4.2229934 -4.2824125 -4.3065791 -4.2842731 -4.2411876 -4.21146 -4.2201242][-4.3085604 -4.3049054 -4.2878923 -4.2703266 -4.2468328 -4.2158966 -4.2033119 -4.227644 -4.2756453 -4.3162208 -4.3270755 -4.3033705 -4.2679658 -4.2415142 -4.2446928][-4.3172655 -4.3110228 -4.301959 -4.2952552 -4.28424 -4.2652936 -4.2571354 -4.2734241 -4.3049197 -4.3275862 -4.3291478 -4.3089814 -4.284821 -4.2651963 -4.2650318][-4.3271394 -4.3190136 -4.3125806 -4.3106146 -4.3070111 -4.2996407 -4.2982774 -4.3079848 -4.3218145 -4.3270693 -4.320755 -4.30573 -4.2920809 -4.2809596 -4.2824788][-4.328618 -4.3210177 -4.3167629 -4.3171105 -4.3192348 -4.3199711 -4.32201 -4.3252592 -4.3263893 -4.321166 -4.3134379 -4.304781 -4.2988396 -4.2930284 -4.2942991][-4.3227663 -4.317 -4.3151 -4.3161259 -4.3182507 -4.320178 -4.3227477 -4.3226967 -4.318428 -4.3116488 -4.3072844 -4.3050604 -4.3047404 -4.3017921 -4.3018851]]...]
INFO - root - 2017-12-06 10:11:38.735260: step 1710, loss = 2.11, batch loss = 2.05 (15.2 examples/sec; 0.528 sec/batch; 48h:30m:52s remains)
INFO - root - 2017-12-06 10:11:43.947152: step 1720, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 0.506 sec/batch; 46h:29m:21s remains)
INFO - root - 2017-12-06 10:11:49.007506: step 1730, loss = 2.07, batch loss = 2.01 (15.8 examples/sec; 0.505 sec/batch; 46h:23m:02s remains)
INFO - root - 2017-12-06 10:11:54.102925: step 1740, loss = 2.09, batch loss = 2.03 (16.5 examples/sec; 0.485 sec/batch; 44h:33m:22s remains)
INFO - root - 2017-12-06 10:11:59.199387: step 1750, loss = 2.10, batch loss = 2.04 (15.5 examples/sec; 0.517 sec/batch; 47h:30m:07s remains)
INFO - root - 2017-12-06 10:12:04.280529: step 1760, loss = 2.11, batch loss = 2.05 (15.4 examples/sec; 0.518 sec/batch; 47h:36m:09s remains)
INFO - root - 2017-12-06 10:12:09.379809: step 1770, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 0.532 sec/batch; 48h:52m:31s remains)
INFO - root - 2017-12-06 10:12:14.447030: step 1780, loss = 2.11, batch loss = 2.05 (16.3 examples/sec; 0.491 sec/batch; 45h:03m:40s remains)
INFO - root - 2017-12-06 10:12:19.566033: step 1790, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 0.520 sec/batch; 47h:44m:26s remains)
INFO - root - 2017-12-06 10:12:24.479225: step 1800, loss = 2.09, batch loss = 2.03 (15.8 examples/sec; 0.507 sec/batch; 46h:33m:04s remains)
2017-12-06 10:12:25.063960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1993484 -4.1905389 -4.187161 -4.1787686 -4.1788726 -4.1886654 -4.2055106 -4.2222543 -4.2304087 -4.235929 -4.2386909 -4.2430382 -4.2452278 -4.2402892 -4.2316775][-4.1730852 -4.1606684 -4.1571245 -4.1521306 -4.1580544 -4.1767449 -4.1952481 -4.2103848 -4.2163072 -4.2203503 -4.216176 -4.2150474 -4.2152328 -4.2124629 -4.2054896][-4.1448612 -4.1222496 -4.115037 -4.114243 -4.1255431 -4.1531086 -4.175354 -4.1919336 -4.1974449 -4.1981187 -4.1889291 -4.1868052 -4.188292 -4.1889668 -4.1875238][-4.1384931 -4.1086264 -4.0937409 -4.0868349 -4.0965977 -4.1258392 -4.1529632 -4.1758404 -4.1887174 -4.187932 -4.1766253 -4.17686 -4.1805954 -4.18515 -4.18727][-4.1600823 -4.1414886 -4.1255016 -4.1066236 -4.1007676 -4.1163454 -4.1424856 -4.1769805 -4.2016673 -4.1955781 -4.1750989 -4.1735268 -4.1751103 -4.1781621 -4.179635][-4.1808662 -4.180336 -4.1727381 -4.1441388 -4.1102934 -4.09458 -4.1139169 -4.1613684 -4.2041621 -4.2006879 -4.1733489 -4.1655359 -4.1595116 -4.1534581 -4.1489024][-4.1822653 -4.1980619 -4.1987238 -4.1658673 -4.1068897 -4.060389 -4.0638595 -4.1201067 -4.1825881 -4.193213 -4.1678429 -4.1543841 -4.1389828 -4.1169138 -4.0976744][-4.1706023 -4.1969924 -4.2050538 -4.1753497 -4.1078653 -4.0353613 -4.0147543 -4.0669022 -4.1442361 -4.1811728 -4.1670241 -4.1465373 -4.1192112 -4.0738754 -4.0303869][-4.1438146 -4.1800232 -4.1925683 -4.1691022 -4.1028876 -4.0042119 -3.9456987 -3.9853165 -4.0816 -4.1467938 -4.1493235 -4.1331663 -4.1032009 -4.0399818 -3.9704885][-4.1115279 -4.1474075 -4.1605272 -4.1456089 -4.0870976 -3.975503 -3.8799968 -3.9058762 -4.0194569 -4.1032934 -4.1246214 -4.1257734 -4.1075044 -4.0472827 -3.9718454][-4.0898881 -4.1252756 -4.1408858 -4.1333156 -4.0923285 -4.0036292 -3.9098158 -3.9100416 -4.0041265 -4.0849738 -4.1162529 -4.1347308 -4.140461 -4.1071086 -4.0510406][-4.0866542 -4.116065 -4.1376843 -4.1420574 -4.1339521 -4.0980487 -4.0433164 -4.0243998 -4.0693903 -4.1176276 -4.1392221 -4.1599245 -4.18135 -4.17139 -4.1375761][-4.1142869 -4.1276708 -4.1479545 -4.1576471 -4.1710815 -4.1816354 -4.1685529 -4.153441 -4.163816 -4.1782541 -4.1838193 -4.1951447 -4.2135453 -4.2149067 -4.1989117][-4.1767912 -4.1737695 -4.1821718 -4.1910634 -4.2089729 -4.2358913 -4.2438469 -4.2406435 -4.2405663 -4.2380247 -4.2350731 -4.23864 -4.249764 -4.2573586 -4.2522583][-4.2548985 -4.2443919 -4.2422471 -4.2456288 -4.2591887 -4.2846875 -4.2991095 -4.3054953 -4.3028822 -4.294116 -4.292397 -4.2955775 -4.3026056 -4.30918 -4.3052034]]...]
INFO - root - 2017-12-06 10:12:30.127089: step 1810, loss = 2.06, batch loss = 2.00 (15.9 examples/sec; 0.503 sec/batch; 46h:11m:00s remains)
INFO - root - 2017-12-06 10:12:35.299730: step 1820, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 0.538 sec/batch; 49h:24m:24s remains)
INFO - root - 2017-12-06 10:12:40.359098: step 1830, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.482 sec/batch; 44h:17m:24s remains)
INFO - root - 2017-12-06 10:12:45.398132: step 1840, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 0.489 sec/batch; 44h:54m:42s remains)
INFO - root - 2017-12-06 10:12:50.477741: step 1850, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 0.533 sec/batch; 48h:55m:21s remains)
INFO - root - 2017-12-06 10:12:55.635371: step 1860, loss = 2.04, batch loss = 1.98 (15.6 examples/sec; 0.514 sec/batch; 47h:12m:25s remains)
INFO - root - 2017-12-06 10:13:00.963192: step 1870, loss = 2.09, batch loss = 2.03 (15.8 examples/sec; 0.506 sec/batch; 46h:26m:03s remains)
INFO - root - 2017-12-06 10:13:05.971121: step 1880, loss = 2.10, batch loss = 2.04 (15.4 examples/sec; 0.518 sec/batch; 47h:33m:48s remains)
INFO - root - 2017-12-06 10:13:11.063675: step 1890, loss = 2.10, batch loss = 2.04 (15.4 examples/sec; 0.518 sec/batch; 47h:36m:18s remains)
INFO - root - 2017-12-06 10:13:15.959887: step 1900, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 0.516 sec/batch; 47h:22m:39s remains)
2017-12-06 10:13:16.500833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2205882 -4.2211771 -4.2244077 -4.2298269 -4.2348609 -4.2453103 -4.2586865 -4.2698607 -4.2711649 -4.2616749 -4.2509861 -4.24462 -4.2422676 -4.2362223 -4.2254033][-4.2347383 -4.2359347 -4.241281 -4.251821 -4.2651672 -4.2773132 -4.2886171 -4.2970386 -4.2977133 -4.2888408 -4.2793202 -4.2780714 -4.2811885 -4.2770791 -4.2661448][-4.2599378 -4.2617326 -4.26908 -4.28118 -4.294323 -4.3057675 -4.3163214 -4.3249865 -4.3282547 -4.3242364 -4.3224764 -4.3295231 -4.3351388 -4.328362 -4.315074][-4.2895393 -4.2892604 -4.2928553 -4.2970304 -4.3036251 -4.3095613 -4.3170786 -4.3251958 -4.33194 -4.3339763 -4.3387218 -4.3477926 -4.3510771 -4.3442059 -4.3353539][-4.3024549 -4.2972231 -4.28966 -4.281714 -4.2751851 -4.26577 -4.2588143 -4.2607327 -4.2705436 -4.2826438 -4.2958193 -4.3064489 -4.30875 -4.305778 -4.3081636][-4.2909331 -4.2806931 -4.2582412 -4.22877 -4.1989923 -4.1684546 -4.139595 -4.1282654 -4.141902 -4.1671038 -4.1931024 -4.2111845 -4.2181807 -4.2251019 -4.2408776][-4.2438016 -4.2240949 -4.1848097 -4.1330404 -4.0784159 -4.0226293 -3.9667525 -3.9382188 -3.9590249 -4.0070024 -4.0552716 -4.091507 -4.112905 -4.1321936 -4.1607332][-4.19309 -4.1706719 -4.1266379 -4.0669832 -4.0042114 -3.9423153 -3.8784671 -3.8526285 -3.8906734 -3.9599268 -4.0246186 -4.0720596 -4.0971942 -4.1139994 -4.1346893][-4.1947818 -4.1839285 -4.157865 -4.1225467 -4.0903559 -4.0626526 -4.0279531 -4.0179982 -4.0483632 -4.0946579 -4.1374168 -4.166265 -4.176055 -4.1786728 -4.18206][-4.2158918 -4.2145424 -4.2067037 -4.1949978 -4.1895833 -4.1867456 -4.1741843 -4.1715474 -4.1892147 -4.2119241 -4.2309117 -4.2423835 -4.2413435 -4.2365546 -4.2292881][-4.2360244 -4.2394238 -4.245337 -4.2507753 -4.2595263 -4.2638121 -4.2610164 -4.2626681 -4.2735715 -4.2825694 -4.2820849 -4.2794032 -4.2722068 -4.2604156 -4.2472253][-4.2651262 -4.2708836 -4.2839775 -4.298368 -4.3097668 -4.314692 -4.3155975 -4.3188334 -4.3240376 -4.3233862 -4.3101492 -4.29814 -4.2897782 -4.2771149 -4.2654495][-4.28504 -4.2899361 -4.3028769 -4.3147736 -4.3192925 -4.3163967 -4.3125181 -4.309319 -4.3065414 -4.2993345 -4.2856336 -4.281004 -4.2814827 -4.2759142 -4.2704649][-4.2806439 -4.2838707 -4.2923503 -4.2969947 -4.2943106 -4.2885866 -4.2874022 -4.2857747 -4.2808728 -4.271605 -4.26124 -4.2611837 -4.2660847 -4.2644811 -4.2585177][-4.2519341 -4.2489519 -4.2491841 -4.2450762 -4.2392139 -4.2415447 -4.2532206 -4.2641945 -4.2652564 -4.260798 -4.2551012 -4.2566543 -4.2621503 -4.2608604 -4.2528138]]...]
INFO - root - 2017-12-06 10:13:21.762278: step 1910, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 0.529 sec/batch; 48h:36m:00s remains)
INFO - root - 2017-12-06 10:13:26.885081: step 1920, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 0.506 sec/batch; 46h:27m:36s remains)
INFO - root - 2017-12-06 10:13:32.127402: step 1930, loss = 2.10, batch loss = 2.04 (15.9 examples/sec; 0.504 sec/batch; 46h:19m:07s remains)
INFO - root - 2017-12-06 10:13:37.240967: step 1940, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 0.544 sec/batch; 49h:55m:27s remains)
INFO - root - 2017-12-06 10:13:42.299265: step 1950, loss = 2.10, batch loss = 2.04 (16.0 examples/sec; 0.499 sec/batch; 45h:51m:05s remains)
INFO - root - 2017-12-06 10:13:47.438433: step 1960, loss = 2.08, batch loss = 2.02 (15.7 examples/sec; 0.511 sec/batch; 46h:53m:01s remains)
INFO - root - 2017-12-06 10:13:52.556746: step 1970, loss = 2.11, batch loss = 2.05 (14.5 examples/sec; 0.552 sec/batch; 50h:42m:32s remains)
INFO - root - 2017-12-06 10:13:57.698477: step 1980, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 42h:59m:35s remains)
INFO - root - 2017-12-06 10:14:02.859781: step 1990, loss = 2.09, batch loss = 2.03 (15.5 examples/sec; 0.516 sec/batch; 47h:21m:53s remains)
INFO - root - 2017-12-06 10:14:07.691539: step 2000, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 0.489 sec/batch; 44h:52m:00s remains)
2017-12-06 10:14:08.333080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3380265 -4.3366127 -4.3366957 -4.33863 -4.341104 -4.34524 -4.3515368 -4.353229 -4.351162 -4.3466606 -4.3457284 -4.34838 -4.351192 -4.3533492 -4.3507938][-4.3315849 -4.3261738 -4.3237534 -4.3239012 -4.3252606 -4.3293934 -4.3377929 -4.33682 -4.328023 -4.3167338 -4.312458 -4.3162923 -4.3240428 -4.3330336 -4.336946][-4.3279028 -4.3173385 -4.3106213 -4.3070006 -4.3050003 -4.3060637 -4.3100829 -4.3002567 -4.2795949 -4.25747 -4.2470703 -4.2506166 -4.2677217 -4.2901278 -4.3065095][-4.3260012 -4.3109918 -4.2993374 -4.2881451 -4.2794037 -4.2744174 -4.2724319 -4.2515197 -4.215054 -4.1770468 -4.15285 -4.1541619 -4.18316 -4.2254672 -4.2609162][-4.3167715 -4.2956929 -4.2794867 -4.2597966 -4.2461677 -4.23761 -4.2296529 -4.1953731 -4.1436844 -4.0929551 -4.05871 -4.0650787 -4.109302 -4.1715212 -4.2250257][-4.2920384 -4.2641692 -4.2402868 -4.2118134 -4.1941247 -4.1831369 -4.1712289 -4.1264267 -4.0689006 -4.0245323 -4.0003681 -4.0203424 -4.0762558 -4.1494932 -4.21139][-4.2551107 -4.2182612 -4.1826353 -4.1370835 -4.111598 -4.1028252 -4.0960617 -4.0544147 -4.0089459 -3.9959853 -3.997947 -4.02915 -4.0887346 -4.160584 -4.2205577][-4.2125969 -4.1619191 -4.10434 -4.0303035 -4.0040264 -4.0148811 -4.0323381 -4.0065293 -3.9872837 -4.0041766 -4.02782 -4.0679631 -4.1264868 -4.1918893 -4.2433276][-4.1791067 -4.1162963 -4.0368481 -3.9401619 -3.9272408 -3.9665546 -4.0172544 -4.0196524 -4.0222645 -4.0510736 -4.0851531 -4.1285734 -4.1801624 -4.23224 -4.2696877][-4.177525 -4.1187472 -4.0533981 -3.9775054 -3.9806037 -4.0313315 -4.0894728 -4.1074457 -4.1141028 -4.1386757 -4.1682577 -4.2036662 -4.2404962 -4.2740045 -4.2943435][-4.214479 -4.1731596 -4.139226 -4.1034946 -4.113205 -4.1511507 -4.1987906 -4.2164822 -4.2184081 -4.2296729 -4.244957 -4.2640157 -4.2838683 -4.302228 -4.3113432][-4.2664452 -4.2418346 -4.2301083 -4.2214117 -4.2331996 -4.2577305 -4.2872353 -4.2941084 -4.2897367 -4.2897148 -4.2938452 -4.3016353 -4.3118005 -4.3214879 -4.3239818][-4.3015428 -4.2875514 -4.2834854 -4.2848272 -4.293745 -4.3090453 -4.3240571 -4.3242488 -4.3196607 -4.3165522 -4.3159151 -4.3186121 -4.3240972 -4.3293419 -4.3300195][-4.3183694 -4.3102884 -4.3078513 -4.3105435 -4.3167048 -4.3254166 -4.3311629 -4.3294878 -4.3268571 -4.3251739 -4.3245635 -4.3255343 -4.32874 -4.331975 -4.3322096][-4.3239751 -4.3197932 -4.3183584 -4.3201146 -4.3238292 -4.328289 -4.32981 -4.3283057 -4.327703 -4.3274436 -4.3269415 -4.3273573 -4.3290663 -4.3308072 -4.3309741]]...]
INFO - root - 2017-12-06 10:14:13.521309: step 2010, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 0.547 sec/batch; 50h:11m:39s remains)
INFO - root - 2017-12-06 10:14:18.828894: step 2020, loss = 2.10, batch loss = 2.04 (14.9 examples/sec; 0.536 sec/batch; 49h:11m:51s remains)
INFO - root - 2017-12-06 10:14:23.878653: step 2030, loss = 2.07, batch loss = 2.01 (15.9 examples/sec; 0.502 sec/batch; 46h:03m:53s remains)
INFO - root - 2017-12-06 10:14:28.815800: step 2040, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.427 sec/batch; 39h:09m:37s remains)
INFO - root - 2017-12-06 10:14:33.065382: step 2050, loss = 2.10, batch loss = 2.04 (19.3 examples/sec; 0.414 sec/batch; 38h:00m:54s remains)
INFO - root - 2017-12-06 10:14:37.389396: step 2060, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 42h:26m:47s remains)
INFO - root - 2017-12-06 10:14:41.888054: step 2070, loss = 2.10, batch loss = 2.04 (17.6 examples/sec; 0.454 sec/batch; 41h:39m:03s remains)
INFO - root - 2017-12-06 10:14:46.258834: step 2080, loss = 2.08, batch loss = 2.03 (18.0 examples/sec; 0.444 sec/batch; 40h:47m:33s remains)
INFO - root - 2017-12-06 10:14:50.615387: step 2090, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 38h:54m:56s remains)
INFO - root - 2017-12-06 10:14:54.708079: step 2100, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.430 sec/batch; 39h:27m:33s remains)
2017-12-06 10:14:55.178258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1566772 -4.1573935 -4.1623826 -4.1811218 -4.2068396 -4.2195039 -4.2279644 -4.2396612 -4.2475481 -4.2531319 -4.2672586 -4.2836814 -4.299046 -4.3061366 -4.3017693][-4.1596932 -4.1671295 -4.1806159 -4.2037745 -4.2294459 -4.2401414 -4.2470751 -4.2578006 -4.2740436 -4.2935367 -4.3184233 -4.33257 -4.3383808 -4.3364258 -4.3277488][-4.1715512 -4.1708484 -4.1856418 -4.2057896 -4.2288051 -4.2373304 -4.2408195 -4.2486053 -4.2682314 -4.2978668 -4.3319168 -4.3508048 -4.3585892 -4.35769 -4.3470283][-4.1887956 -4.1725655 -4.1827159 -4.2034392 -4.2253423 -4.2272553 -4.21851 -4.2170134 -4.2349238 -4.2745452 -4.3194261 -4.3452663 -4.3577385 -4.3614516 -4.3486071][-4.2013597 -4.1797204 -4.1844444 -4.2015257 -4.2173147 -4.2041893 -4.175384 -4.1591463 -4.1719317 -4.2231588 -4.2799053 -4.3182087 -4.34047 -4.3516417 -4.3410149][-4.2209735 -4.20433 -4.2043309 -4.2112117 -4.2121363 -4.1736135 -4.1116381 -4.0759726 -4.086669 -4.1499128 -4.224721 -4.2804594 -4.3159747 -4.3407669 -4.3389115][-4.2366071 -4.235517 -4.2390823 -4.239181 -4.22383 -4.1628208 -4.0648794 -3.9964566 -4.0039086 -4.0809493 -4.1728268 -4.247674 -4.2986245 -4.3321724 -4.3373055][-4.2504983 -4.2641287 -4.2740111 -4.2715664 -4.2476406 -4.178484 -4.0645113 -3.9689927 -3.9660385 -4.0432081 -4.1396375 -4.2253656 -4.2890525 -4.3260422 -4.3310614][-4.2496819 -4.2710609 -4.281198 -4.2787766 -4.2532949 -4.1884856 -4.0818424 -3.9830134 -3.9698391 -4.0354953 -4.1276088 -4.2139673 -4.2823219 -4.3174667 -4.3186889][-4.2347193 -4.2587538 -4.2677393 -4.2638097 -4.2409825 -4.1892786 -4.1027641 -4.0148778 -3.9945021 -4.0400672 -4.1218195 -4.2052093 -4.2737465 -4.3065243 -4.3037419][-4.2257237 -4.2518735 -4.260664 -4.2552447 -4.23446 -4.1933789 -4.1239676 -4.049089 -4.030551 -4.0652938 -4.1322503 -4.2032967 -4.2628736 -4.2925363 -4.2903919][-4.2397947 -4.2612381 -4.2637787 -4.2547035 -4.2374277 -4.2085595 -4.1563854 -4.0927935 -4.0737915 -4.1048126 -4.159112 -4.2120361 -4.2538419 -4.2745447 -4.275353][-4.2676558 -4.2792454 -4.2733288 -4.2597923 -4.2446189 -4.2245946 -4.1866136 -4.1396446 -4.1228929 -4.150321 -4.1917353 -4.22445 -4.2426019 -4.2450738 -4.2418509][-4.292223 -4.3004804 -4.2936182 -4.2809863 -4.267302 -4.250751 -4.222393 -4.1924157 -4.1827111 -4.2043114 -4.2334929 -4.2486916 -4.241221 -4.2142458 -4.1936908][-4.2887964 -4.2979445 -4.2980857 -4.2977414 -4.2952871 -4.2889524 -4.2735376 -4.2561407 -4.2519264 -4.26561 -4.2802367 -4.280529 -4.2527409 -4.2007246 -4.1609068]]...]
INFO - root - 2017-12-06 10:14:59.498574: step 2110, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 39h:39m:32s remains)
INFO - root - 2017-12-06 10:15:03.943454: step 2120, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.457 sec/batch; 41h:58m:05s remains)
INFO - root - 2017-12-06 10:15:08.302241: step 2130, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 41h:15m:11s remains)
INFO - root - 2017-12-06 10:15:12.714186: step 2140, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.431 sec/batch; 39h:35m:03s remains)
INFO - root - 2017-12-06 10:15:17.042924: step 2150, loss = 2.09, batch loss = 2.04 (19.0 examples/sec; 0.421 sec/batch; 38h:37m:06s remains)
INFO - root - 2017-12-06 10:15:21.387289: step 2160, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.465 sec/batch; 42h:38m:26s remains)
INFO - root - 2017-12-06 10:15:25.681009: step 2170, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 38h:43m:11s remains)
INFO - root - 2017-12-06 10:15:29.997391: step 2180, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.435 sec/batch; 39h:54m:29s remains)
INFO - root - 2017-12-06 10:15:34.338738: step 2190, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.444 sec/batch; 40h:42m:40s remains)
INFO - root - 2017-12-06 10:15:38.422708: step 2200, loss = 2.06, batch loss = 2.01 (18.4 examples/sec; 0.435 sec/batch; 39h:55m:13s remains)
2017-12-06 10:15:38.944088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2866073 -4.2794304 -4.2699804 -4.2539883 -4.23332 -4.1992016 -4.1639872 -4.1571212 -4.1926832 -4.2247324 -4.2306886 -4.2480049 -4.2672925 -4.2798524 -4.2700024][-4.2782383 -4.2750349 -4.26613 -4.2474546 -4.2247214 -4.1910238 -4.1593881 -4.1584854 -4.2003794 -4.2405953 -4.2535152 -4.2708273 -4.2935739 -4.308742 -4.2961497][-4.2720704 -4.273663 -4.2668343 -4.2425656 -4.2124214 -4.1716971 -4.1366396 -4.1424279 -4.1912169 -4.2440739 -4.2730675 -4.2955308 -4.3181176 -4.3315063 -4.3132005][-4.2757382 -4.2811689 -4.2743516 -4.2431684 -4.1996117 -4.1425548 -4.0924468 -4.0985518 -4.1568179 -4.2273479 -4.2774038 -4.3116755 -4.3361783 -4.3456397 -4.3214331][-4.2798071 -4.2868257 -4.2784586 -4.2396379 -4.1799078 -4.102253 -4.03484 -4.0427675 -4.1181917 -4.2091646 -4.2792706 -4.3260713 -4.3507547 -4.3528 -4.3202667][-4.282548 -4.2921166 -4.2797041 -4.2323008 -4.1554852 -4.0549941 -3.9700801 -3.9798808 -4.0774946 -4.1922879 -4.2804408 -4.3344235 -4.3544416 -4.3464146 -4.3068929][-4.2894268 -4.2986474 -4.2833338 -4.2312365 -4.1410408 -4.0175056 -3.9067469 -3.9069877 -4.0183468 -4.1541104 -4.2624855 -4.3269506 -4.3470173 -4.3338141 -4.2900438][-4.3043489 -4.3086019 -4.2885513 -4.2345505 -4.142065 -4.008791 -3.8764465 -3.8578517 -3.9698384 -4.114275 -4.2359838 -4.3103127 -4.3362651 -4.3226008 -4.27633][-4.3235083 -4.3216958 -4.2973504 -4.242805 -4.1564121 -4.0307174 -3.896841 -3.8633287 -3.9632511 -4.099751 -4.2214465 -4.2986717 -4.3271871 -4.3163314 -4.27091][-4.3409367 -4.3359032 -4.3121266 -4.2602777 -4.1862283 -4.0785012 -3.9581048 -3.915951 -3.9930942 -4.110661 -4.2209187 -4.2911768 -4.3176017 -4.3098817 -4.2714767][-4.3507676 -4.3440533 -4.3237958 -4.2794971 -4.2222738 -4.1393204 -4.0396996 -3.9961188 -4.0471697 -4.1384478 -4.2295485 -4.2865181 -4.3066349 -4.3011189 -4.273982][-4.3484797 -4.3397589 -4.3235731 -4.2903829 -4.2486877 -4.1891346 -4.11723 -4.0837803 -4.1160312 -4.1827431 -4.2512612 -4.2922864 -4.3047023 -4.2988744 -4.278563][-4.3387237 -4.328279 -4.314919 -4.2899728 -4.2564354 -4.2144661 -4.16855 -4.1522694 -4.1782842 -4.2308655 -4.2801681 -4.3065605 -4.3116727 -4.3053532 -4.2897191][-4.3267312 -4.3144569 -4.303575 -4.2846994 -4.2570133 -4.2262349 -4.1996379 -4.1948566 -4.2187877 -4.2600489 -4.2951632 -4.312479 -4.3149419 -4.311666 -4.300456][-4.3180637 -4.3039002 -4.2936244 -4.27986 -4.2598934 -4.2381034 -4.2215261 -4.2195148 -4.2367864 -4.2655663 -4.2902365 -4.3027687 -4.3062797 -4.3075767 -4.3025064]]...]
INFO - root - 2017-12-06 10:15:43.383141: step 2210, loss = 2.09, batch loss = 2.03 (17.9 examples/sec; 0.448 sec/batch; 41h:05m:18s remains)
INFO - root - 2017-12-06 10:15:47.863601: step 2220, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 42h:22m:23s remains)
INFO - root - 2017-12-06 10:15:52.208150: step 2230, loss = 2.09, batch loss = 2.03 (19.3 examples/sec; 0.414 sec/batch; 37h:56m:51s remains)
INFO - root - 2017-12-06 10:15:56.692577: step 2240, loss = 2.09, batch loss = 2.03 (16.8 examples/sec; 0.477 sec/batch; 43h:45m:20s remains)
INFO - root - 2017-12-06 10:16:00.990315: step 2250, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.435 sec/batch; 39h:56m:03s remains)
INFO - root - 2017-12-06 10:16:05.260243: step 2260, loss = 2.06, batch loss = 2.00 (19.0 examples/sec; 0.422 sec/batch; 38h:41m:46s remains)
INFO - root - 2017-12-06 10:16:09.543754: step 2270, loss = 2.10, batch loss = 2.04 (18.0 examples/sec; 0.443 sec/batch; 40h:40m:04s remains)
INFO - root - 2017-12-06 10:16:13.843516: step 2280, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.431 sec/batch; 39h:31m:27s remains)
INFO - root - 2017-12-06 10:16:18.097919: step 2290, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.433 sec/batch; 39h:40m:50s remains)
INFO - root - 2017-12-06 10:16:22.206561: step 2300, loss = 2.09, batch loss = 2.03 (19.1 examples/sec; 0.418 sec/batch; 38h:19m:47s remains)
2017-12-06 10:16:22.733395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2744169 -4.2707024 -4.2615495 -4.2424569 -4.2194571 -4.2038751 -4.1934829 -4.1846266 -4.1760244 -4.1614742 -4.1388578 -4.117445 -4.1088133 -4.1349683 -4.1842313][-4.2684722 -4.2565374 -4.2402925 -4.2190518 -4.1939826 -4.1708879 -4.1504335 -4.1305728 -4.1080346 -4.0865436 -4.0832996 -4.0991731 -4.1243424 -4.1648741 -4.2168708][-4.261169 -4.2372618 -4.2106261 -4.1839113 -4.1597481 -4.1364107 -4.1149421 -4.0889335 -4.0534697 -4.0314026 -4.05515 -4.09838 -4.1409621 -4.18779 -4.2402945][-4.2509336 -4.2230721 -4.1927047 -4.1631379 -4.1413031 -4.1210165 -4.1039209 -4.0793953 -4.0503621 -4.046803 -4.0846338 -4.1267653 -4.1646605 -4.203433 -4.2502131][-4.2503238 -4.2241211 -4.1966987 -4.1633272 -4.1324816 -4.1073074 -4.0844126 -4.0589261 -4.0517011 -4.0837107 -4.1300273 -4.1678948 -4.2022057 -4.2346272 -4.2711277][-4.2478552 -4.2285213 -4.2074022 -4.1669235 -4.1169548 -4.0673037 -4.0191946 -3.974057 -3.9968376 -4.0823197 -4.15043 -4.1959267 -4.2364616 -4.2682471 -4.2926712][-4.2365637 -4.2297683 -4.2140422 -4.1662836 -4.0924459 -4.0116005 -3.9237444 -3.8626728 -3.9260817 -4.0617223 -4.150197 -4.2073846 -4.2597861 -4.2938371 -4.3081117][-4.2114716 -4.2004023 -4.1874876 -4.1475735 -4.0755334 -4.0016785 -3.9295785 -3.9004831 -3.9778018 -4.098752 -4.1718054 -4.224052 -4.2709861 -4.3008227 -4.3089252][-4.1595979 -4.1514635 -4.1533051 -4.1407008 -4.1074944 -4.0805054 -4.0537171 -4.0573025 -4.1160955 -4.1839547 -4.2233753 -4.2534962 -4.279387 -4.2976017 -4.3028326][-4.1175656 -4.1331139 -4.1582136 -4.1690168 -4.1597934 -4.150351 -4.1429892 -4.1618128 -4.2064466 -4.2437973 -4.2663374 -4.2824192 -4.2949142 -4.3000512 -4.3028221][-4.1283817 -4.1596828 -4.1880403 -4.1960549 -4.1874328 -4.1785975 -4.1740961 -4.1985011 -4.2373962 -4.26374 -4.2836032 -4.3004322 -4.3069181 -4.3056555 -4.30734][-4.164567 -4.1911287 -4.2114868 -4.21299 -4.2025404 -4.1894078 -4.1828423 -4.203722 -4.235301 -4.2596507 -4.2822695 -4.3008723 -4.3080225 -4.3085618 -4.3118124][-4.1956077 -4.2134233 -4.2239966 -4.2215757 -4.2104487 -4.1983604 -4.1911497 -4.2047014 -4.2300611 -4.2550616 -4.2776241 -4.2980456 -4.3085608 -4.3119035 -4.3142195][-4.1921048 -4.2032914 -4.211133 -4.2080569 -4.20574 -4.2053237 -4.2043257 -4.2150006 -4.2367268 -4.2593484 -4.2823062 -4.30271 -4.3130784 -4.3152547 -4.31498][-4.1756735 -4.1906776 -4.1996365 -4.1980228 -4.2008462 -4.211278 -4.2243748 -4.23805 -4.2576833 -4.2760358 -4.2947922 -4.3086023 -4.3129253 -4.3130841 -4.3119879]]...]
INFO - root - 2017-12-06 10:16:27.028072: step 2310, loss = 2.10, batch loss = 2.04 (18.6 examples/sec; 0.431 sec/batch; 39h:32m:58s remains)
INFO - root - 2017-12-06 10:16:31.408703: step 2320, loss = 2.07, batch loss = 2.02 (18.0 examples/sec; 0.444 sec/batch; 40h:41m:53s remains)
INFO - root - 2017-12-06 10:16:35.804174: step 2330, loss = 2.08, batch loss = 2.03 (18.2 examples/sec; 0.439 sec/batch; 40h:13m:47s remains)
INFO - root - 2017-12-06 10:16:40.164211: step 2340, loss = 2.10, batch loss = 2.04 (18.3 examples/sec; 0.437 sec/batch; 40h:03m:05s remains)
INFO - root - 2017-12-06 10:16:44.431678: step 2350, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.424 sec/batch; 38h:55m:30s remains)
INFO - root - 2017-12-06 10:16:48.843696: step 2360, loss = 2.07, batch loss = 2.01 (19.1 examples/sec; 0.420 sec/batch; 38h:28m:15s remains)
INFO - root - 2017-12-06 10:16:53.291891: step 2370, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.449 sec/batch; 41h:11m:52s remains)
INFO - root - 2017-12-06 10:16:57.662674: step 2380, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.422 sec/batch; 38h:43m:44s remains)
INFO - root - 2017-12-06 10:17:02.037009: step 2390, loss = 2.09, batch loss = 2.03 (19.3 examples/sec; 0.415 sec/batch; 38h:05m:34s remains)
INFO - root - 2017-12-06 10:17:06.128057: step 2400, loss = 2.06, batch loss = 2.01 (28.9 examples/sec; 0.277 sec/batch; 25h:24m:13s remains)
2017-12-06 10:17:06.590465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2211146 -4.2478085 -4.2691336 -4.281 -4.2935762 -4.3008814 -4.2886357 -4.2631354 -4.2289443 -4.2208433 -4.2320938 -4.2557783 -4.2862515 -4.3073516 -4.320107][-4.2046165 -4.2433157 -4.2785125 -4.2977533 -4.3054328 -4.3016782 -4.2789774 -4.2467794 -4.2046204 -4.1934648 -4.206521 -4.2369089 -4.27257 -4.29706 -4.3118782][-4.215323 -4.252717 -4.2905684 -4.3074379 -4.3064113 -4.2948942 -4.2665443 -4.2286057 -4.1862316 -4.1792941 -4.1988034 -4.2317061 -4.2663016 -4.289712 -4.3060255][-4.2476459 -4.2737827 -4.2982478 -4.2992473 -4.2841539 -4.2650003 -4.236146 -4.2012563 -4.1675982 -4.171896 -4.201417 -4.2374344 -4.2672453 -4.2874765 -4.304657][-4.2614374 -4.2738104 -4.279489 -4.2625084 -4.2331271 -4.20313 -4.1710839 -4.1384983 -4.1158953 -4.1407309 -4.1887193 -4.2331357 -4.2636442 -4.2870893 -4.3084311][-4.2460747 -4.2447066 -4.2354879 -4.2081141 -4.1695013 -4.1277819 -4.082346 -4.0414486 -4.0229788 -4.0704017 -4.1405921 -4.1979671 -4.2411871 -4.2784519 -4.3095646][-4.2043138 -4.198894 -4.1843505 -4.1572657 -4.1222677 -4.0790496 -4.0205579 -3.9615903 -3.9319017 -3.9856977 -4.0620961 -4.13129 -4.1965313 -4.251864 -4.2970476][-4.1711793 -4.169745 -4.1618962 -4.1431937 -4.1233406 -4.0947881 -4.0409441 -3.9772272 -3.9336023 -3.9664576 -4.0257745 -4.0923352 -4.1644692 -4.2247 -4.2767019][-4.16596 -4.169858 -4.1753883 -4.1716018 -4.164865 -4.1513567 -4.1150546 -4.0679169 -4.0273662 -4.0363135 -4.069447 -4.11777 -4.17344 -4.2204189 -4.2663779][-4.190208 -4.1950216 -4.2091031 -4.2131948 -4.2114286 -4.2045922 -4.1841526 -4.1553135 -4.12335 -4.120461 -4.1378951 -4.1678267 -4.2018929 -4.2332969 -4.2694054][-4.2345138 -4.2384319 -4.2473063 -4.2484927 -4.2475305 -4.2464709 -4.2391028 -4.2256861 -4.2056584 -4.2010503 -4.2092967 -4.2242146 -4.2401586 -4.2568545 -4.2797976][-4.2760906 -4.2781115 -4.2801871 -4.2790661 -4.2797308 -4.2830095 -4.2841277 -4.2818923 -4.2729397 -4.2696233 -4.2704778 -4.2744994 -4.2796574 -4.2860169 -4.2962589][-4.3017931 -4.3025632 -4.3027053 -4.3015394 -4.3037333 -4.3099647 -4.31498 -4.3168473 -4.312325 -4.3086286 -4.3069572 -4.3096581 -4.313446 -4.3146067 -4.3170633][-4.3169284 -4.3182116 -4.3179855 -4.3167496 -4.3194857 -4.3252993 -4.3296928 -4.3308411 -4.3282619 -4.3251 -4.32403 -4.3274727 -4.3319755 -4.3327732 -4.3314185][-4.3205867 -4.3211966 -4.3207316 -4.3188405 -4.3191571 -4.3220325 -4.3240552 -4.3238282 -4.3219748 -4.3212662 -4.3222294 -4.3254185 -4.3298883 -4.332294 -4.3323913]]...]
INFO - root - 2017-12-06 10:17:10.861238: step 2410, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.421 sec/batch; 38h:35m:10s remains)
INFO - root - 2017-12-06 10:17:15.039439: step 2420, loss = 2.09, batch loss = 2.04 (18.8 examples/sec; 0.427 sec/batch; 39h:06m:26s remains)
INFO - root - 2017-12-06 10:17:19.362445: step 2430, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.448 sec/batch; 41h:02m:06s remains)
INFO - root - 2017-12-06 10:17:23.710376: step 2440, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.460 sec/batch; 42h:09m:57s remains)
INFO - root - 2017-12-06 10:17:28.143033: step 2450, loss = 2.04, batch loss = 1.98 (17.8 examples/sec; 0.450 sec/batch; 41h:15m:36s remains)
INFO - root - 2017-12-06 10:17:32.425376: step 2460, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.427 sec/batch; 39h:07m:21s remains)
INFO - root - 2017-12-06 10:17:36.872352: step 2470, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 41h:13m:56s remains)
INFO - root - 2017-12-06 10:17:41.215951: step 2480, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.443 sec/batch; 40h:38m:14s remains)
INFO - root - 2017-12-06 10:17:45.586429: step 2490, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.421 sec/batch; 38h:33m:40s remains)
INFO - root - 2017-12-06 10:17:49.834964: step 2500, loss = 2.07, batch loss = 2.01 (31.0 examples/sec; 0.258 sec/batch; 23h:41m:38s remains)
2017-12-06 10:17:50.346770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3040161 -4.3083148 -4.30823 -4.3061562 -4.3052473 -4.3063741 -4.3078918 -4.3119073 -4.3194871 -4.3259177 -4.3252339 -4.3169818 -4.3079276 -4.3058581 -4.3093934][-4.2980838 -4.2992768 -4.2962885 -4.2922282 -4.2885332 -4.2861009 -4.2859416 -4.2935324 -4.3090959 -4.3228917 -4.3254561 -4.3157134 -4.3014569 -4.29396 -4.2978497][-4.2899408 -4.2858663 -4.2748885 -4.2633743 -4.2541232 -4.24795 -4.2446933 -4.2569389 -4.28623 -4.3107023 -4.3183389 -4.3098559 -4.2934847 -4.2824163 -4.2851067][-4.2632933 -4.2500257 -4.2307081 -4.2138276 -4.2053132 -4.2001772 -4.1916785 -4.2049842 -4.2439508 -4.2780027 -4.2938666 -4.2935405 -4.2805762 -4.2672615 -4.2670155][-4.2379103 -4.2157145 -4.1871352 -4.1660032 -4.1574311 -4.1481867 -4.1339455 -4.1442528 -4.1910057 -4.2358894 -4.2577977 -4.2628336 -4.2545238 -4.2424726 -4.2418704][-4.2246895 -4.1993628 -4.1649261 -4.1359415 -4.1187406 -4.0993743 -4.0778923 -4.0789018 -4.1272497 -4.1852236 -4.2196445 -4.2318492 -4.2306046 -4.2284384 -4.2349238][-4.2097578 -4.1852112 -4.1517119 -4.1144853 -4.0785561 -4.0404944 -3.9985752 -3.9784958 -4.0204077 -4.1043167 -4.1653581 -4.1959243 -4.20897 -4.21968 -4.2338052][-4.2028813 -4.1780653 -4.1490626 -4.1038866 -4.0480266 -3.9871762 -3.9217486 -3.8794694 -3.9107723 -4.0188322 -4.1087418 -4.1574125 -4.1812878 -4.200768 -4.21983][-4.2099476 -4.1864619 -4.1599073 -4.1152983 -4.0528326 -3.995327 -3.9377334 -3.9014354 -3.9334569 -4.022666 -4.098959 -4.1392121 -4.1615558 -4.1810584 -4.2038016][-4.2187023 -4.2032 -4.1810203 -4.1393433 -4.0870514 -4.0433216 -4.0028367 -3.9863217 -4.0179887 -4.0692735 -4.1106539 -4.1378636 -4.153511 -4.1650467 -4.18723][-4.2060475 -4.1968584 -4.183147 -4.1539159 -4.1219897 -4.0915995 -4.0582905 -4.0477796 -4.0667653 -4.0850368 -4.0960932 -4.1185713 -4.1355357 -4.1443977 -4.1664872][-4.1884913 -4.1807113 -4.1775565 -4.1681323 -4.156724 -4.1384578 -4.1123109 -4.1078119 -4.1179824 -4.1127524 -4.1000271 -4.112093 -4.1335664 -4.1473103 -4.1686783][-4.2011843 -4.1956143 -4.2012391 -4.2048631 -4.2022705 -4.1890693 -4.1702704 -4.1681814 -4.1725969 -4.1604795 -4.1430864 -4.1499376 -4.1695657 -4.1860003 -4.204031][-4.2222247 -4.2179437 -4.2278748 -4.238358 -4.2388582 -4.2310967 -4.2226772 -4.2225642 -4.2229376 -4.21461 -4.2000527 -4.2040691 -4.2191424 -4.2304015 -4.2435994][-4.2552328 -4.2531567 -4.264452 -4.2771692 -4.2815337 -4.2804866 -4.2783742 -4.2784681 -4.2814107 -4.2808189 -4.2712092 -4.2684908 -4.2747474 -4.2788143 -4.2841406]]...]
INFO - root - 2017-12-06 10:17:54.687516: step 2510, loss = 2.06, batch loss = 2.00 (18.1 examples/sec; 0.442 sec/batch; 40h:29m:45s remains)
INFO - root - 2017-12-06 10:17:59.180576: step 2520, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 41h:39m:10s remains)
INFO - root - 2017-12-06 10:18:03.556219: step 2530, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.440 sec/batch; 40h:20m:31s remains)
INFO - root - 2017-12-06 10:18:07.907042: step 2540, loss = 2.07, batch loss = 2.02 (18.5 examples/sec; 0.432 sec/batch; 39h:33m:06s remains)
INFO - root - 2017-12-06 10:18:12.295610: step 2550, loss = 2.07, batch loss = 2.02 (19.3 examples/sec; 0.415 sec/batch; 38h:04m:46s remains)
INFO - root - 2017-12-06 10:18:16.818084: step 2560, loss = 2.09, batch loss = 2.03 (16.7 examples/sec; 0.479 sec/batch; 43h:52m:40s remains)
INFO - root - 2017-12-06 10:18:22.096439: step 2570, loss = 2.06, batch loss = 2.01 (15.8 examples/sec; 0.507 sec/batch; 46h:30m:14s remains)
INFO - root - 2017-12-06 10:18:27.113791: step 2580, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 0.501 sec/batch; 45h:53m:55s remains)
INFO - root - 2017-12-06 10:18:32.166521: step 2590, loss = 2.08, batch loss = 2.03 (15.7 examples/sec; 0.511 sec/batch; 46h:49m:37s remains)
INFO - root - 2017-12-06 10:18:37.246077: step 2600, loss = 2.10, batch loss = 2.04 (15.8 examples/sec; 0.507 sec/batch; 46h:26m:06s remains)
2017-12-06 10:18:37.784531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2427287 -4.2378011 -4.2334533 -4.2299805 -4.2276816 -4.2249002 -4.2262959 -4.229537 -4.2345939 -4.2470746 -4.262145 -4.2724676 -4.2849469 -4.29068 -4.2911448][-4.2086926 -4.1957769 -4.1846151 -4.175879 -4.1672072 -4.1594481 -4.160553 -4.1650243 -4.1721697 -4.1882925 -4.2121649 -4.2293711 -4.2494683 -4.2595911 -4.2551975][-4.1731148 -4.1581211 -4.1481266 -4.1414742 -4.1316385 -4.11847 -4.1145349 -4.1180334 -4.1249681 -4.1411948 -4.1701279 -4.189292 -4.2092347 -4.219739 -4.2064204][-4.1483612 -4.1424961 -4.1443257 -4.1493235 -4.1467819 -4.1304188 -4.1190052 -4.1213861 -4.1274352 -4.1404943 -4.1656866 -4.1745257 -4.1835914 -4.1898212 -4.1657753][-4.133656 -4.1365604 -4.146 -4.1537547 -4.1516209 -4.1322131 -4.1129866 -4.1217585 -4.1373234 -4.1527963 -4.1738076 -4.1707592 -4.1666636 -4.1681 -4.1392217][-4.1264472 -4.1280508 -4.1336913 -4.1287766 -4.1109238 -4.0767303 -4.0426917 -4.0617723 -4.0989041 -4.1307855 -4.1619172 -4.161828 -4.1561141 -4.162662 -4.1405778][-4.117126 -4.1124072 -4.1074567 -4.0822425 -4.0420723 -3.9902136 -3.9345455 -3.9663231 -4.0325756 -4.0866833 -4.1347113 -4.1520033 -4.1570644 -4.1730409 -4.1628094][-4.1129351 -4.1098142 -4.1067467 -4.079618 -4.0387893 -3.993309 -3.9407248 -3.9690135 -4.0349512 -4.0868444 -4.1350431 -4.1640172 -4.1767268 -4.1941533 -4.1902332][-4.1239309 -4.1287894 -4.1372976 -4.1275458 -4.1097646 -4.0937243 -4.0673585 -4.0826612 -4.1179581 -4.1414557 -4.1670656 -4.1856976 -4.1910863 -4.2000256 -4.197526][-4.1535339 -4.1590447 -4.170403 -4.1713762 -4.1691432 -4.168201 -4.1598444 -4.168427 -4.1843572 -4.1861973 -4.1895747 -4.1939287 -4.1895 -4.1891913 -4.1862459][-4.1915064 -4.1882105 -4.1885786 -4.1852336 -4.1819735 -4.1817665 -4.1799731 -4.1866817 -4.1977677 -4.1933866 -4.1877012 -4.1845794 -4.1752334 -4.1678519 -4.1634][-4.2084427 -4.1961923 -4.1839709 -4.1707468 -4.1595531 -4.152297 -4.1537809 -4.1644945 -4.1807804 -4.1800838 -4.1749678 -4.1730433 -4.1653619 -4.1557741 -4.1510825][-4.2037454 -4.1898212 -4.1767745 -4.1622782 -4.1477137 -4.131341 -4.1299758 -4.1423149 -4.1634946 -4.1739087 -4.1792364 -4.1849751 -4.18279 -4.17555 -4.1721187][-4.18796 -4.1760626 -4.1677771 -4.1599326 -4.1517873 -4.1326761 -4.1298113 -4.1412506 -4.1618962 -4.178484 -4.1948109 -4.208241 -4.2120442 -4.2092805 -4.2084041][-4.158751 -4.1469216 -4.1425018 -4.1433225 -4.1459846 -4.131526 -4.1307797 -4.1424646 -4.1618495 -4.1824689 -4.2079496 -4.2271667 -4.2337847 -4.2346735 -4.2367029]]...]
INFO - root - 2017-12-06 10:18:42.885428: step 2610, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 0.521 sec/batch; 47h:45m:27s remains)
INFO - root - 2017-12-06 10:18:48.032732: step 2620, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 0.510 sec/batch; 46h:44m:35s remains)
INFO - root - 2017-12-06 10:18:53.091938: step 2630, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 0.526 sec/batch; 48h:10m:09s remains)
INFO - root - 2017-12-06 10:18:58.229229: step 2640, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 0.521 sec/batch; 47h:44m:27s remains)
INFO - root - 2017-12-06 10:19:03.351927: step 2650, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 0.518 sec/batch; 47h:29m:09s remains)
INFO - root - 2017-12-06 10:19:08.525853: step 2660, loss = 2.10, batch loss = 2.04 (16.7 examples/sec; 0.479 sec/batch; 43h:50m:48s remains)
INFO - root - 2017-12-06 10:19:13.616933: step 2670, loss = 2.11, batch loss = 2.05 (15.7 examples/sec; 0.510 sec/batch; 46h:44m:29s remains)
INFO - root - 2017-12-06 10:19:18.607940: step 2680, loss = 2.10, batch loss = 2.04 (16.2 examples/sec; 0.494 sec/batch; 45h:14m:39s remains)
INFO - root - 2017-12-06 10:19:23.789527: step 2690, loss = 2.09, batch loss = 2.03 (16.7 examples/sec; 0.480 sec/batch; 43h:58m:46s remains)
INFO - root - 2017-12-06 10:19:28.964370: step 2700, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 0.545 sec/batch; 49h:57m:13s remains)
2017-12-06 10:19:29.600976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2247639 -4.2340012 -4.2489276 -4.2678261 -4.2813892 -4.2830606 -4.2800884 -4.2805724 -4.2828369 -4.2821078 -4.278502 -4.2737074 -4.2697768 -4.2677636 -4.2653937][-4.2590523 -4.264318 -4.2709851 -4.2790146 -4.2834268 -4.2785268 -4.2699389 -4.2688608 -4.2720075 -4.2740679 -4.2749681 -4.2756386 -4.2778358 -4.279902 -4.2793689][-4.2936597 -4.295393 -4.2913995 -4.2864275 -4.2786303 -4.2642465 -4.2482171 -4.2441435 -4.2500319 -4.2579103 -4.2662444 -4.2755256 -4.2852974 -4.2927604 -4.2941718][-4.29521 -4.2952037 -4.2865782 -4.2722831 -4.2540288 -4.2328224 -4.213232 -4.2083626 -4.2173905 -4.2302866 -4.2446842 -4.2604136 -4.2767429 -4.2891555 -4.2893944][-4.2667894 -4.2634077 -4.2525005 -4.2319913 -4.2080078 -4.1852808 -4.1674905 -4.1659484 -4.1799746 -4.1996732 -4.2188158 -4.2351727 -4.2518539 -4.2657342 -4.2625732][-4.2214389 -4.2148519 -4.1978831 -4.1717744 -4.14711 -4.1244893 -4.1077714 -4.1076665 -4.125607 -4.1587644 -4.1926627 -4.2166557 -4.2344117 -4.2465463 -4.2375336][-4.1601038 -4.1477242 -4.1232882 -4.090313 -4.0627022 -4.037149 -4.0170431 -4.010952 -4.03027 -4.0782142 -4.1296024 -4.16399 -4.1859231 -4.1996088 -4.1916795][-4.1211014 -4.1014791 -4.0729089 -4.0387893 -4.0102878 -3.9813654 -3.9536214 -3.9358842 -3.9502461 -4.0063047 -4.0713344 -4.1151943 -4.1423893 -4.1588478 -4.1517582][-4.1330833 -4.1142454 -4.0923624 -4.0717173 -4.0580497 -4.0425682 -4.0230494 -4.0081515 -4.0195951 -4.0663514 -4.121099 -4.1556578 -4.17387 -4.1807437 -4.1656737][-4.1868615 -4.1733823 -4.1590939 -4.1484075 -4.1431103 -4.1362295 -4.1273251 -4.12211 -4.1334996 -4.166604 -4.2044625 -4.22585 -4.2328787 -4.2314792 -4.2155585][-4.2410464 -4.2333403 -4.2211847 -4.2105756 -4.20255 -4.196279 -4.1945953 -4.1956716 -4.20298 -4.2227712 -4.2476988 -4.2573705 -4.2558141 -4.2517619 -4.2400851][-4.2432089 -4.2419786 -4.2318182 -4.218689 -4.2066507 -4.2006874 -4.203073 -4.2070713 -4.20936 -4.2206378 -4.2371292 -4.2390804 -4.2347832 -4.2335792 -4.2303324][-4.2281318 -4.2267661 -4.2145891 -4.1959672 -4.1796689 -4.1742373 -4.1807671 -4.1854086 -4.1823449 -4.1874647 -4.1991973 -4.2022934 -4.204658 -4.2126179 -4.2218947][-4.22766 -4.2209845 -4.2032857 -4.182755 -4.167686 -4.1645279 -4.1714778 -4.1748266 -4.1676931 -4.1688809 -4.1787734 -4.1855059 -4.1941032 -4.2091546 -4.2266526][-4.2221022 -4.2115636 -4.1942029 -4.1761384 -4.1646404 -4.1653113 -4.1747422 -4.1793871 -4.1727867 -4.1748991 -4.1861529 -4.1963763 -4.2087412 -4.2259736 -4.2431622]]...]
INFO - root - 2017-12-06 10:19:34.427251: step 2710, loss = 2.12, batch loss = 2.06 (15.6 examples/sec; 0.512 sec/batch; 46h:53m:02s remains)
INFO - root - 2017-12-06 10:19:39.513520: step 2720, loss = 2.09, batch loss = 2.03 (15.6 examples/sec; 0.512 sec/batch; 46h:54m:08s remains)
INFO - root - 2017-12-06 10:19:44.587115: step 2730, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 0.513 sec/batch; 46h:57m:37s remains)
INFO - root - 2017-12-06 10:19:49.607925: step 2740, loss = 2.11, batch loss = 2.05 (16.2 examples/sec; 0.494 sec/batch; 45h:16m:09s remains)
INFO - root - 2017-12-06 10:19:54.674476: step 2750, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 0.517 sec/batch; 47h:19m:48s remains)
INFO - root - 2017-12-06 10:19:59.827800: step 2760, loss = 2.09, batch loss = 2.03 (15.7 examples/sec; 0.509 sec/batch; 46h:38m:20s remains)
INFO - root - 2017-12-06 10:20:04.903377: step 2770, loss = 2.06, batch loss = 2.01 (15.9 examples/sec; 0.503 sec/batch; 46h:06m:13s remains)
INFO - root - 2017-12-06 10:20:09.858154: step 2780, loss = 2.06, batch loss = 2.00 (14.7 examples/sec; 0.546 sec/batch; 49h:59m:39s remains)
INFO - root - 2017-12-06 10:20:14.915008: step 2790, loss = 2.09, batch loss = 2.04 (15.9 examples/sec; 0.502 sec/batch; 45h:59m:55s remains)
INFO - root - 2017-12-06 10:20:20.090826: step 2800, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 0.536 sec/batch; 49h:03m:52s remains)
2017-12-06 10:20:20.692514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1906233 -4.1986237 -4.2059259 -4.2198963 -4.2338381 -4.2350049 -4.2029939 -4.1482019 -4.1174517 -4.144608 -4.2017865 -4.2610621 -4.3043022 -4.3307004 -4.3371434][-4.1750178 -4.1665764 -4.1608486 -4.1683545 -4.1852875 -4.192009 -4.1663332 -4.1153326 -4.0929317 -4.1294394 -4.1928983 -4.2554674 -4.2985644 -4.3218894 -4.3272223][-4.1976948 -4.1733375 -4.1494818 -4.1399369 -4.1462197 -4.1488519 -4.1295404 -4.088717 -4.0738115 -4.1196208 -4.1923046 -4.2551317 -4.2946262 -4.3152251 -4.3200579][-4.2367387 -4.2093511 -4.1763015 -4.1500559 -4.1340451 -4.11875 -4.0974727 -4.0663481 -4.0587029 -4.1142154 -4.1993713 -4.2608347 -4.2952237 -4.3150935 -4.3203726][-4.2596807 -4.2412014 -4.21159 -4.1768112 -4.1427641 -4.1018357 -4.06398 -4.0288405 -4.0212803 -4.0862393 -4.1859965 -4.253602 -4.2898779 -4.3136549 -4.3207388][-4.2663836 -4.2532921 -4.2298884 -4.1883931 -4.1333852 -4.0678515 -4.016902 -3.976218 -3.9679689 -4.041894 -4.1531482 -4.2312126 -4.2746344 -4.3039179 -4.3163347][-4.2604017 -4.2514863 -4.2258053 -4.1737967 -4.0980573 -4.0155807 -3.967783 -3.9323273 -3.9258182 -4.00756 -4.1254139 -4.2132187 -4.2634764 -4.2957845 -4.3127713][-4.2467942 -4.2387843 -4.2070656 -4.1457033 -4.059083 -3.9744024 -3.9403489 -3.9150379 -3.9161668 -4.0039215 -4.1198153 -4.2105703 -4.2660184 -4.2976389 -4.3129649][-4.2382894 -4.2251472 -4.1901746 -4.1345792 -4.0703692 -4.0120344 -3.9900594 -3.9658852 -3.9606688 -4.0341325 -4.1360726 -4.2223883 -4.2779264 -4.3056774 -4.3157516][-4.2411227 -4.223115 -4.186708 -4.1496568 -4.1301265 -4.1091261 -4.0931373 -4.0608168 -4.0412917 -4.0837817 -4.1607533 -4.235528 -4.2846446 -4.30765 -4.3154993][-4.2296891 -4.20157 -4.1688604 -4.1480975 -4.1568112 -4.1583281 -4.1446123 -4.1135421 -4.0940223 -4.1226172 -4.1820345 -4.2430682 -4.2831035 -4.3056183 -4.3145385][-4.2157936 -4.18268 -4.1545229 -4.1451864 -4.1602225 -4.1633029 -4.1472559 -4.1207561 -4.1138811 -4.1468277 -4.2001457 -4.2480412 -4.2779994 -4.3029604 -4.31425][-4.21285 -4.1789246 -4.1559606 -4.1558237 -4.1702371 -4.1653385 -4.1408553 -4.1167088 -4.1205573 -4.1611247 -4.2150888 -4.2545815 -4.2781062 -4.3033309 -4.3175735][-4.2006965 -4.1717238 -4.1529255 -4.1593981 -4.1797428 -4.1673584 -4.1389456 -4.1218228 -4.1362219 -4.1811523 -4.2324028 -4.2629471 -4.2802339 -4.3026586 -4.3174143][-4.1772022 -4.1565967 -4.1357889 -4.1377363 -4.1580024 -4.1602449 -4.1431565 -4.1375027 -4.1596756 -4.2036972 -4.2495365 -4.2772312 -4.2913537 -4.3064775 -4.3174639]]...]
INFO - root - 2017-12-06 10:20:24.056298: step 2810, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 38h:58m:18s remains)
INFO - root - 2017-12-06 10:20:28.330487: step 2820, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 39h:14m:06s remains)
INFO - root - 2017-12-06 10:20:32.543938: step 2830, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.428 sec/batch; 39h:10m:54s remains)
INFO - root - 2017-12-06 10:20:36.819142: step 2840, loss = 2.05, batch loss = 2.00 (17.4 examples/sec; 0.460 sec/batch; 42h:07m:46s remains)
INFO - root - 2017-12-06 10:20:41.119169: step 2850, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.446 sec/batch; 40h:47m:59s remains)
INFO - root - 2017-12-06 10:20:45.407497: step 2860, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.435 sec/batch; 39h:50m:43s remains)
INFO - root - 2017-12-06 10:20:49.812149: step 2870, loss = 2.10, batch loss = 2.04 (18.0 examples/sec; 0.444 sec/batch; 40h:41m:32s remains)
INFO - root - 2017-12-06 10:20:54.174737: step 2880, loss = 2.08, batch loss = 2.02 (16.3 examples/sec; 0.490 sec/batch; 44h:53m:28s remains)
INFO - root - 2017-12-06 10:20:58.584608: step 2890, loss = 2.06, batch loss = 2.00 (18.5 examples/sec; 0.432 sec/batch; 39h:32m:53s remains)
INFO - root - 2017-12-06 10:21:03.002405: step 2900, loss = 2.10, batch loss = 2.04 (18.3 examples/sec; 0.438 sec/batch; 40h:03m:36s remains)
2017-12-06 10:21:03.481165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2747936 -4.2668743 -4.2481041 -4.2303333 -4.2255754 -4.22953 -4.2314534 -4.2353125 -4.2509546 -4.2676606 -4.2769384 -4.2855253 -4.2799439 -4.2672148 -4.2694478][-4.2326069 -4.2223911 -4.193995 -4.1667609 -4.1623144 -4.1714664 -4.1836662 -4.1967888 -4.2223463 -4.2405229 -4.2452459 -4.2476573 -4.2380724 -4.2196603 -4.2219667][-4.1736579 -4.1669507 -4.1353149 -4.1006985 -4.0988517 -4.1154933 -4.1408682 -4.1676283 -4.200325 -4.2100954 -4.2002716 -4.1954818 -4.1878705 -4.1686072 -4.1719322][-4.1177936 -4.12243 -4.0952611 -4.0565467 -4.0534248 -4.0692916 -4.0999179 -4.1356544 -4.1685066 -4.1701488 -4.1438341 -4.1355529 -4.1409917 -4.125823 -4.1241651][-4.0975442 -4.1147084 -4.1019034 -4.0641427 -4.0533586 -4.056849 -4.0786428 -4.1116643 -4.1355367 -4.1301055 -4.0969529 -4.0886555 -4.1076441 -4.0989652 -4.0872078][-4.1233687 -4.1447263 -4.14654 -4.1183319 -4.0970311 -4.0814548 -4.0853662 -4.1051149 -4.1183167 -4.1111708 -4.0826607 -4.0780177 -4.1014652 -4.10575 -4.0932221][-4.1727023 -4.1879625 -4.1956229 -4.1787119 -4.1576824 -4.1348653 -4.124423 -4.1244473 -4.1236081 -4.1143708 -4.08998 -4.0873661 -4.1158686 -4.1387272 -4.132585][-4.2294722 -4.2367983 -4.2381968 -4.2240319 -4.2002387 -4.1769767 -4.1572962 -4.1392293 -4.1226296 -4.1004219 -4.0751839 -4.0857038 -4.1259389 -4.1589379 -4.157629][-4.2650986 -4.2696934 -4.26238 -4.2419171 -4.2111034 -4.185698 -4.167469 -4.1461525 -4.1171541 -4.0833755 -4.059298 -4.0825806 -4.1266546 -4.1545196 -4.1513066][-4.2580204 -4.2614956 -4.2489815 -4.2232723 -4.1895962 -4.1624174 -4.150877 -4.1415205 -4.1157818 -4.083034 -4.0668783 -4.0922427 -4.1292028 -4.1444478 -4.1394806][-4.2311063 -4.2305584 -4.2183404 -4.1916637 -4.1553473 -4.1200409 -4.1132326 -4.1220627 -4.1071229 -4.0863509 -4.0852108 -4.1091313 -4.1359487 -4.140635 -4.1376743][-4.2032976 -4.1957355 -4.1839442 -4.1607523 -4.1218004 -4.07687 -4.0703025 -4.093534 -4.0864758 -4.0798106 -4.09886 -4.1274977 -4.1427307 -4.1394076 -4.1371861][-4.1679773 -4.1559076 -4.1491852 -4.1352358 -4.1015434 -4.0559821 -4.0500937 -4.0811219 -4.081605 -4.081316 -4.1103063 -4.1399975 -4.1418815 -4.1316228 -4.1319308][-4.1245732 -4.1158271 -4.1214738 -4.1220126 -4.0992618 -4.064137 -4.066391 -4.0984554 -4.1018558 -4.0993919 -4.1265621 -4.1474743 -4.1309094 -4.1149063 -4.121048][-4.0884733 -4.0920534 -4.107635 -4.1170139 -4.1058207 -4.0854468 -4.0971045 -4.1287251 -4.1317987 -4.1205082 -4.135941 -4.1447244 -4.1110554 -4.0901694 -4.0994325]]...]
INFO - root - 2017-12-06 10:21:07.625112: step 2910, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.448 sec/batch; 41h:02m:12s remains)
INFO - root - 2017-12-06 10:21:11.965219: step 2920, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.425 sec/batch; 38h:55m:54s remains)
INFO - root - 2017-12-06 10:21:16.385407: step 2930, loss = 2.11, batch loss = 2.05 (17.8 examples/sec; 0.451 sec/batch; 41h:14m:32s remains)
INFO - root - 2017-12-06 10:21:20.879825: step 2940, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.447 sec/batch; 40h:55m:16s remains)
INFO - root - 2017-12-06 10:21:25.325390: step 2950, loss = 2.12, batch loss = 2.06 (18.5 examples/sec; 0.434 sec/batch; 39h:41m:22s remains)
INFO - root - 2017-12-06 10:21:29.644800: step 2960, loss = 2.12, batch loss = 2.06 (18.2 examples/sec; 0.439 sec/batch; 40h:13m:19s remains)
INFO - root - 2017-12-06 10:21:34.124895: step 2970, loss = 2.08, batch loss = 2.03 (17.3 examples/sec; 0.463 sec/batch; 42h:23m:56s remains)
INFO - root - 2017-12-06 10:21:38.485392: step 2980, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 39h:39m:49s remains)
INFO - root - 2017-12-06 10:21:42.811308: step 2990, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.440 sec/batch; 40h:18m:59s remains)
INFO - root - 2017-12-06 10:21:47.174615: step 3000, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.434 sec/batch; 39h:45m:05s remains)
2017-12-06 10:21:47.706125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.334147 -4.3328495 -4.3325658 -4.3296461 -4.3201313 -4.3012943 -4.2705355 -4.2238283 -4.1750345 -4.1169004 -4.0439558 -3.9599743 -3.8968048 -3.9006984 -3.9576564][-4.3333364 -4.3342829 -4.3371396 -4.338 -4.3330841 -4.3205142 -4.2962322 -4.2567635 -4.2144752 -4.164609 -4.1015091 -4.0293956 -3.9766645 -3.9711823 -4.0067449][-4.3358836 -4.34022 -4.3467827 -4.351325 -4.3508549 -4.3425069 -4.3233895 -4.2935157 -4.2631111 -4.2335072 -4.1977258 -4.15296 -4.1168928 -4.1064539 -4.1226354][-4.33853 -4.3444095 -4.3520007 -4.3568587 -4.3562074 -4.3474455 -4.3297997 -4.3049884 -4.2816262 -4.2710218 -4.26575 -4.2547889 -4.2418981 -4.2360229 -4.2416387][-4.3383355 -4.3422408 -4.345479 -4.345046 -4.3386245 -4.32247 -4.2978334 -4.2703042 -4.2501445 -4.2556105 -4.28072 -4.3032041 -4.3139277 -4.3163147 -4.3183489][-4.3355227 -4.3356829 -4.3317337 -4.3196778 -4.2984619 -4.2631969 -4.2186732 -4.1772838 -4.1545568 -4.177115 -4.2323012 -4.2851448 -4.320231 -4.3374624 -4.3438797][-4.3337359 -4.3310833 -4.3204365 -4.2950764 -4.2563524 -4.197093 -4.1213202 -4.0482097 -4.0070543 -4.0448976 -4.1342478 -4.220295 -4.282002 -4.3177261 -4.3334637][-4.338367 -4.3360596 -4.3224239 -4.2889233 -4.2393785 -4.1655493 -4.0673165 -3.9647837 -3.8972168 -3.9427586 -4.0586295 -4.1698012 -4.2516966 -4.2998018 -4.3222394][-4.351892 -4.3512397 -4.3390722 -4.3077765 -4.2632871 -4.1980104 -4.1119871 -4.0227332 -3.9630878 -4.001317 -4.1020894 -4.1994643 -4.2697153 -4.3074493 -4.3243165][-4.3625627 -4.361197 -4.3513656 -4.3271055 -4.2959747 -4.2550359 -4.203341 -4.1510859 -4.1180897 -4.1416688 -4.2045684 -4.2665091 -4.3100853 -4.3296032 -4.335546][-4.3480172 -4.3409638 -4.3309627 -4.3145142 -4.2983813 -4.2817059 -4.2617316 -4.2394018 -4.2240024 -4.2349768 -4.2687855 -4.3043556 -4.330071 -4.3423085 -4.3450255][-4.3100471 -4.2971649 -4.2877436 -4.2801518 -4.2778115 -4.2807531 -4.2815313 -4.2756219 -4.2676611 -4.2683344 -4.2832565 -4.3045216 -4.3227768 -4.337194 -4.3452444][-4.2624383 -4.2432976 -4.2310085 -4.22789 -4.2363925 -4.25448 -4.2663088 -4.2631993 -4.2534842 -4.2441249 -4.2488346 -4.2656446 -4.284544 -4.3064842 -4.3259144][-4.2097511 -4.1872854 -4.171802 -4.1672583 -4.1777806 -4.2018323 -4.2171206 -4.209362 -4.1923838 -4.175395 -4.1764641 -4.1936483 -4.2183814 -4.2502103 -4.2827096][-4.1763606 -4.1615505 -4.1515355 -4.1465659 -4.1504173 -4.1660571 -4.1736007 -4.1614852 -4.1435981 -4.1243787 -4.1225152 -4.1396852 -4.1673746 -4.20404 -4.2399626]]...]
INFO - root - 2017-12-06 10:21:51.750303: step 3010, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.428 sec/batch; 39h:09m:34s remains)
INFO - root - 2017-12-06 10:21:56.061012: step 3020, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.422 sec/batch; 38h:38m:24s remains)
INFO - root - 2017-12-06 10:22:00.350101: step 3030, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.440 sec/batch; 40h:13m:31s remains)
INFO - root - 2017-12-06 10:22:04.764027: step 3040, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.432 sec/batch; 39h:31m:18s remains)
INFO - root - 2017-12-06 10:22:09.176648: step 3050, loss = 2.07, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 39h:51m:22s remains)
INFO - root - 2017-12-06 10:22:13.619944: step 3060, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.439 sec/batch; 40h:13m:00s remains)
INFO - root - 2017-12-06 10:22:17.934909: step 3070, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.444 sec/batch; 40h:40m:21s remains)
INFO - root - 2017-12-06 10:22:22.266139: step 3080, loss = 2.07, batch loss = 2.02 (18.5 examples/sec; 0.432 sec/batch; 39h:30m:00s remains)
INFO - root - 2017-12-06 10:22:26.587262: step 3090, loss = 2.10, batch loss = 2.05 (17.3 examples/sec; 0.462 sec/batch; 42h:19m:11s remains)
INFO - root - 2017-12-06 10:22:30.973441: step 3100, loss = 2.08, batch loss = 2.03 (18.2 examples/sec; 0.440 sec/batch; 40h:17m:13s remains)
2017-12-06 10:22:31.448215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3540258 -4.3577023 -4.3596888 -4.3606114 -4.3617854 -4.3618703 -4.3634734 -4.3658018 -4.3646545 -4.3611989 -4.3571839 -4.3524814 -4.34523 -4.3392534 -4.3349848][-4.366384 -4.3711629 -4.3730989 -4.374011 -4.374577 -4.3712225 -4.3694234 -4.368041 -4.361136 -4.3522444 -4.3416753 -4.3294291 -4.3123622 -4.2978649 -4.290473][-4.3622231 -4.3638935 -4.3615804 -4.3571362 -4.3537631 -4.3458538 -4.3395462 -4.3355174 -4.3263254 -4.3167057 -4.3032417 -4.2869616 -4.2613063 -4.2370129 -4.2272992][-4.3321271 -4.3289204 -4.3215041 -4.3089185 -4.2963753 -4.282475 -4.2698541 -4.2639503 -4.2606888 -4.2599049 -4.2472067 -4.2261276 -4.19649 -4.168932 -4.1579366][-4.2881107 -4.2813711 -4.2703214 -4.2522635 -4.2322736 -4.2115216 -4.1897106 -4.1811404 -4.1891551 -4.1984587 -4.1851497 -4.1549664 -4.1228318 -4.0973525 -4.0836234][-4.2441483 -4.233501 -4.2171936 -4.195888 -4.1702476 -4.1375041 -4.0960803 -4.0799484 -4.1063986 -4.1292191 -4.1138306 -4.0813127 -4.0584393 -4.0471315 -4.0371161][-4.2081761 -4.1878357 -4.159 -4.1280909 -4.0908623 -4.03833 -3.9651475 -3.9377489 -3.9936364 -4.0425024 -4.0375485 -4.0167093 -4.0148525 -4.0317659 -4.0410576][-4.1962442 -4.1661325 -4.121645 -4.07726 -4.0294852 -3.9594021 -3.8531983 -3.8116922 -3.9017837 -3.9818463 -3.9978411 -4.001461 -4.0262065 -4.0662475 -4.092185][-4.2007189 -4.1667304 -4.116549 -4.0628686 -4.0109391 -3.9437017 -3.8378186 -3.7967629 -3.891562 -3.9814973 -4.0156817 -4.0387297 -4.07771 -4.1269879 -4.1637263][-4.2220855 -4.1926308 -4.1497083 -4.1006694 -4.0527015 -4.0028672 -3.9327598 -3.9077325 -3.9685462 -4.0361233 -4.0721483 -4.098578 -4.135716 -4.1835632 -4.2212672][-4.25471 -4.2317023 -4.2015281 -4.1639576 -4.1235423 -4.0867605 -4.0480137 -4.0369372 -4.0680261 -4.1105919 -4.1415582 -4.1677461 -4.1983633 -4.235826 -4.265357][-4.2845888 -4.2671971 -4.24779 -4.2235117 -4.19582 -4.1718378 -4.1516294 -4.147634 -4.1640491 -4.18845 -4.2107043 -4.232286 -4.252738 -4.2758336 -4.2937703][-4.302321 -4.2912049 -4.2805295 -4.2674704 -4.2534714 -4.2423081 -4.234592 -4.233717 -4.2439151 -4.2565336 -4.2686596 -4.2799478 -4.2892661 -4.2987714 -4.3071065][-4.3075728 -4.301332 -4.2955589 -4.2900634 -4.2855725 -4.2830906 -4.2830095 -4.2854924 -4.29296 -4.3004074 -4.3057933 -4.3082075 -4.308692 -4.3102369 -4.3137012][-4.3035641 -4.3011556 -4.3003573 -4.3004713 -4.3011336 -4.3039541 -4.30702 -4.309978 -4.3148322 -4.3200426 -4.3230734 -4.3222427 -4.3202515 -4.3191919 -4.319634]]...]
INFO - root - 2017-12-06 10:22:35.441023: step 3110, loss = 2.05, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 39h:10m:55s remains)
INFO - root - 2017-12-06 10:22:39.703601: step 3120, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.437 sec/batch; 40h:01m:01s remains)
INFO - root - 2017-12-06 10:22:44.098182: step 3130, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.453 sec/batch; 41h:24m:29s remains)
INFO - root - 2017-12-06 10:22:48.429733: step 3140, loss = 2.09, batch loss = 2.04 (18.7 examples/sec; 0.428 sec/batch; 39h:09m:10s remains)
INFO - root - 2017-12-06 10:22:52.839967: step 3150, loss = 2.05, batch loss = 1.99 (18.9 examples/sec; 0.424 sec/batch; 38h:45m:58s remains)
INFO - root - 2017-12-06 10:22:57.294793: step 3160, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.461 sec/batch; 42h:07m:47s remains)
INFO - root - 2017-12-06 10:23:01.649328: step 3170, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.432 sec/batch; 39h:31m:53s remains)
INFO - root - 2017-12-06 10:23:06.011912: step 3180, loss = 2.10, batch loss = 2.04 (18.2 examples/sec; 0.440 sec/batch; 40h:12m:43s remains)
INFO - root - 2017-12-06 10:23:10.317614: step 3190, loss = 2.10, batch loss = 2.04 (19.2 examples/sec; 0.416 sec/batch; 38h:05m:03s remains)
INFO - root - 2017-12-06 10:23:14.704195: step 3200, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.433 sec/batch; 39h:35m:09s remains)
2017-12-06 10:23:15.199390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2774644 -4.294136 -4.3026066 -4.2966022 -4.2883492 -4.2754836 -4.2532043 -4.2311196 -4.20106 -4.1650329 -4.1450148 -4.1457472 -4.1565657 -4.1736617 -4.1928163][-4.2777138 -4.2921658 -4.3018661 -4.2967706 -4.2869029 -4.2746263 -4.2563505 -4.2400861 -4.219348 -4.1896834 -4.167573 -4.1643867 -4.1759548 -4.1962137 -4.2188768][-4.2859993 -4.30359 -4.3166604 -4.3115196 -4.3025436 -4.2948852 -4.2812691 -4.2639327 -4.2450662 -4.2233467 -4.2048798 -4.1995869 -4.2108388 -4.2303219 -4.25173][-4.2963624 -4.3199511 -4.3350377 -4.3317013 -4.3226337 -4.3139334 -4.2972932 -4.2757359 -4.2562137 -4.2429447 -4.2342124 -4.2352085 -4.2458863 -4.2603669 -4.2785006][-4.3027 -4.3283534 -4.3411713 -4.3347182 -4.3197918 -4.3032746 -4.2757034 -4.2459087 -4.2314377 -4.2314043 -4.2384214 -4.2534113 -4.2660365 -4.2735596 -4.2868195][-4.3022642 -4.3234444 -4.3287039 -4.3152618 -4.2891045 -4.259654 -4.2146878 -4.1709275 -4.1645751 -4.1868763 -4.2159324 -4.2501016 -4.2715883 -4.2782993 -4.2878385][-4.3016095 -4.312345 -4.3055615 -4.2819686 -4.2423954 -4.1954269 -4.1273212 -4.0651159 -4.0745482 -4.1318789 -4.1901841 -4.2412133 -4.2703824 -4.2761321 -4.2801833][-4.3130326 -4.3073211 -4.2863941 -4.2531681 -4.2044883 -4.1474762 -4.07044 -4.0150423 -4.0493793 -4.1344652 -4.2059622 -4.25591 -4.2756524 -4.2708149 -4.2624464][-4.3229995 -4.3057227 -4.2767806 -4.2414126 -4.1995 -4.1590562 -4.1109648 -4.086566 -4.1218753 -4.189424 -4.24025 -4.2687783 -4.2723989 -4.251153 -4.2282119][-4.3240547 -4.3056316 -4.281333 -4.2588181 -4.2345104 -4.2176113 -4.1996188 -4.1940832 -4.2072453 -4.2349362 -4.2502069 -4.2507405 -4.2390461 -4.2060485 -4.1686058][-4.3218079 -4.3090382 -4.29458 -4.2825575 -4.268074 -4.2606988 -4.2544165 -4.2502341 -4.2419872 -4.2336988 -4.2164803 -4.1939936 -4.1749077 -4.1402607 -4.0954647][-4.3139119 -4.3072453 -4.3000293 -4.2921085 -4.282115 -4.2773194 -4.2731133 -4.2640939 -4.2400832 -4.2088776 -4.1685328 -4.1295013 -4.109108 -4.0836411 -4.0467281][-4.3034534 -4.3002281 -4.2973723 -4.2915673 -4.2850747 -4.2826233 -4.278604 -4.2662964 -4.237906 -4.1990318 -4.1516743 -4.113862 -4.0985994 -4.0806336 -4.0573583][-4.2846136 -4.2878 -4.2945027 -4.29674 -4.2947688 -4.2927227 -4.2861643 -4.2726221 -4.2483487 -4.21149 -4.1687336 -4.1401243 -4.1294355 -4.1181464 -4.1121292][-4.2638521 -4.2743158 -4.2885427 -4.2979345 -4.3029633 -4.3036475 -4.2948833 -4.2830458 -4.263917 -4.2287579 -4.1923413 -4.1731381 -4.1691546 -4.1689153 -4.1786861]]...]
INFO - root - 2017-12-06 10:23:19.234193: step 3210, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.444 sec/batch; 40h:39m:12s remains)
INFO - root - 2017-12-06 10:23:23.710364: step 3220, loss = 2.07, batch loss = 2.02 (18.4 examples/sec; 0.434 sec/batch; 39h:44m:10s remains)
INFO - root - 2017-12-06 10:23:28.105609: step 3230, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 40h:26m:02s remains)
INFO - root - 2017-12-06 10:23:32.460850: step 3240, loss = 2.09, batch loss = 2.03 (19.2 examples/sec; 0.416 sec/batch; 38h:05m:07s remains)
INFO - root - 2017-12-06 10:23:36.820312: step 3250, loss = 2.10, batch loss = 2.04 (18.5 examples/sec; 0.433 sec/batch; 39h:35m:18s remains)
INFO - root - 2017-12-06 10:23:41.211897: step 3260, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.427 sec/batch; 39h:05m:39s remains)
INFO - root - 2017-12-06 10:23:45.621022: step 3270, loss = 2.10, batch loss = 2.05 (17.0 examples/sec; 0.472 sec/batch; 43h:08m:27s remains)
INFO - root - 2017-12-06 10:23:49.971389: step 3280, loss = 2.06, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 39h:56m:41s remains)
INFO - root - 2017-12-06 10:23:54.373363: step 3290, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.449 sec/batch; 41h:04m:09s remains)
INFO - root - 2017-12-06 10:23:58.834545: step 3300, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.458 sec/batch; 41h:53m:40s remains)
2017-12-06 10:23:59.335325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2908287 -4.2813816 -4.277637 -4.2808466 -4.2878404 -4.293901 -4.303381 -4.3100839 -4.3073168 -4.2991 -4.2912393 -4.2932062 -4.310246 -4.3254533 -4.3253803][-4.2685747 -4.2576456 -4.2555833 -4.2592263 -4.2647791 -4.2689686 -4.2790985 -4.2888842 -4.2910128 -4.2819128 -4.2675509 -4.2638259 -4.2837605 -4.30437 -4.3030686][-4.2566314 -4.2471523 -4.2455535 -4.2420244 -4.2377753 -4.2328086 -4.2386179 -4.2508388 -4.2622814 -4.2569737 -4.2395906 -4.231504 -4.2480907 -4.267344 -4.2651124][-4.2559142 -4.2453241 -4.2394981 -4.2241759 -4.2057838 -4.1894875 -4.182754 -4.1910739 -4.2114329 -4.2169127 -4.2074151 -4.2011814 -4.2112451 -4.2198496 -4.2143083][-4.2587352 -4.2416468 -4.2309818 -4.2067118 -4.1738729 -4.1436963 -4.1130934 -4.0986357 -4.1176329 -4.1449113 -4.16505 -4.1762924 -4.1875615 -4.1843591 -4.1706223][-4.2510405 -4.2225976 -4.2045712 -4.1783218 -4.139852 -4.0929003 -4.0246024 -3.9628477 -3.9745991 -4.0434742 -4.1077771 -4.1472845 -4.1658111 -4.1546116 -4.1344128][-4.2390904 -4.1967726 -4.1685276 -4.1414289 -4.0999613 -4.02979 -3.9064035 -3.7710118 -3.7623858 -3.8954968 -4.0229068 -4.0982041 -4.1314487 -4.1238604 -4.1069865][-4.2302141 -4.180007 -4.1422949 -4.1070075 -4.0627322 -3.9848251 -3.838295 -3.65592 -3.619503 -3.7954891 -3.9636075 -4.0646033 -4.1124554 -4.1095605 -4.0948334][-4.2230611 -4.1762872 -4.1293612 -4.08435 -4.0417738 -3.9835794 -3.8942134 -3.7921805 -3.7731369 -3.8834229 -4.0057311 -4.0891376 -4.1320739 -4.1260509 -4.1074843][-4.2226744 -4.1829524 -4.1347494 -4.0843391 -4.0428023 -4.0047169 -3.9800081 -3.9675283 -3.9751949 -4.0202217 -4.0811839 -4.1317854 -4.1614766 -4.1536131 -4.1342521][-4.2217069 -4.19011 -4.1478567 -4.1011982 -4.0658016 -4.0492349 -4.0602026 -4.0856891 -4.1046443 -4.1236877 -4.1470346 -4.1692739 -4.1839066 -4.1705728 -4.1490073][-4.2365346 -4.2164693 -4.1872873 -4.1520772 -4.1279173 -4.1247091 -4.1461034 -4.1803904 -4.2017908 -4.2095046 -4.2092152 -4.2085652 -4.2069039 -4.1916189 -4.1724563][-4.2572093 -4.2458105 -4.2322674 -4.2138624 -4.2056932 -4.2120676 -4.2304988 -4.2548566 -4.2701888 -4.2734737 -4.2653036 -4.2553191 -4.2479548 -4.2354078 -4.2208424][-4.2771463 -4.2669287 -4.26446 -4.2638412 -4.2682157 -4.2770381 -4.2876577 -4.2990603 -4.3060012 -4.3045845 -4.296277 -4.2878337 -4.2828255 -4.2739377 -4.2641969][-4.2982416 -4.2879434 -4.287745 -4.2940083 -4.3031373 -4.3098869 -4.3121681 -4.3144679 -4.3163562 -4.3139844 -4.308579 -4.3060489 -4.3052244 -4.3013496 -4.2980986]]...]
INFO - root - 2017-12-06 10:24:03.332169: step 3310, loss = 2.08, batch loss = 2.02 (19.1 examples/sec; 0.418 sec/batch; 38h:15m:58s remains)
INFO - root - 2017-12-06 10:24:07.610693: step 3320, loss = 2.13, batch loss = 2.07 (18.6 examples/sec; 0.431 sec/batch; 39h:24m:08s remains)
INFO - root - 2017-12-06 10:24:11.872855: step 3330, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.435 sec/batch; 39h:44m:20s remains)
INFO - root - 2017-12-06 10:24:16.148645: step 3340, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.448 sec/batch; 41h:00m:08s remains)
INFO - root - 2017-12-06 10:24:20.676818: step 3350, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.426 sec/batch; 38h:59m:37s remains)
INFO - root - 2017-12-06 10:24:24.954533: step 3360, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.444 sec/batch; 40h:35m:51s remains)
INFO - root - 2017-12-06 10:24:29.270987: step 3370, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.426 sec/batch; 38h:56m:14s remains)
INFO - root - 2017-12-06 10:24:33.638367: step 3380, loss = 2.06, batch loss = 2.01 (19.0 examples/sec; 0.420 sec/batch; 38h:26m:24s remains)
INFO - root - 2017-12-06 10:24:37.963793: step 3390, loss = 2.10, batch loss = 2.04 (18.0 examples/sec; 0.444 sec/batch; 40h:33m:19s remains)
INFO - root - 2017-12-06 10:24:42.386316: step 3400, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 39h:06m:24s remains)
2017-12-06 10:24:42.855789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1597271 -4.1879644 -4.2114921 -4.2201834 -4.2244859 -4.2245584 -4.2237406 -4.2257414 -4.231545 -4.2369318 -4.2412438 -4.2419448 -4.2425604 -4.245965 -4.2516985][-4.1258492 -4.1681018 -4.202383 -4.2175941 -4.227242 -4.230937 -4.2311683 -4.227911 -4.2284641 -4.2310925 -4.2333841 -4.2327876 -4.23115 -4.2289524 -4.2256818][-4.1265283 -4.1797495 -4.222332 -4.2439661 -4.2530117 -4.2590556 -4.2564096 -4.2431669 -4.2347541 -4.231369 -4.2314529 -4.2330642 -4.2297349 -4.2175641 -4.2040606][-4.1562543 -4.21197 -4.2515259 -4.2705355 -4.2754335 -4.2797637 -4.2723732 -4.2518206 -4.2384977 -4.2363353 -4.23896 -4.2437644 -4.2368364 -4.2128 -4.1929774][-4.19853 -4.2455668 -4.2736626 -4.2830291 -4.2818551 -4.2780795 -4.2616959 -4.2372761 -4.2299328 -4.2356124 -4.2426734 -4.25106 -4.2451634 -4.2198453 -4.201406][-4.2279062 -4.262362 -4.2791486 -4.2802997 -4.2695746 -4.2520947 -4.2211876 -4.188467 -4.1878858 -4.2046714 -4.218071 -4.2302318 -4.2285657 -4.2169948 -4.2137904][-4.2264447 -4.2523994 -4.2578092 -4.2513523 -4.2303815 -4.1946659 -4.1417117 -4.0944548 -4.0988107 -4.1366224 -4.1693063 -4.1945715 -4.2093244 -4.2191567 -4.2312689][-4.1982183 -4.2113829 -4.2063694 -4.1936488 -4.1633368 -4.1100411 -4.033534 -3.9701548 -3.9949691 -4.0795379 -4.1476517 -4.1977663 -4.2301116 -4.2520137 -4.265614][-4.1882844 -4.1823 -4.1730194 -4.1596589 -4.1252389 -4.0621634 -3.9783089 -3.9242704 -3.9843812 -4.0981016 -4.1825285 -4.2408009 -4.2715869 -4.2873383 -4.2899671][-4.1946292 -4.1805515 -4.1751866 -4.16983 -4.1462674 -4.1071777 -4.063086 -4.0477705 -4.096714 -4.1794538 -4.2373605 -4.2714005 -4.2813 -4.2848153 -4.2793703][-4.2037697 -4.1898236 -4.1941857 -4.2037039 -4.1989789 -4.1894894 -4.1785536 -4.1790285 -4.2055144 -4.2488008 -4.2734785 -4.276186 -4.2638645 -4.2570806 -4.2476969][-4.2075267 -4.1948781 -4.2065582 -4.2279549 -4.2351379 -4.2365794 -4.2348447 -4.2353106 -4.25043 -4.2706146 -4.2717466 -4.2514729 -4.223146 -4.2111826 -4.2078862][-4.2007408 -4.185008 -4.1950932 -4.2199726 -4.2329149 -4.2382655 -4.2403283 -4.2429376 -4.2566643 -4.2667832 -4.2548232 -4.2227664 -4.187428 -4.1709495 -4.17544][-4.1873412 -4.1691089 -4.1759744 -4.1964278 -4.207015 -4.2131429 -4.2193508 -4.2252021 -4.2360482 -4.244092 -4.2309165 -4.1966052 -4.163753 -4.1520071 -4.16503][-4.19385 -4.1787796 -4.17968 -4.1873913 -4.1868925 -4.1845126 -4.1878395 -4.1948338 -4.2101736 -4.2233944 -4.2150755 -4.1878948 -4.1599894 -4.1557956 -4.17151]]...]
INFO - root - 2017-12-06 10:24:46.916433: step 3410, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 39h:10m:06s remains)
INFO - root - 2017-12-06 10:24:51.252988: step 3420, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.429 sec/batch; 39h:14m:49s remains)
INFO - root - 2017-12-06 10:24:55.590713: step 3430, loss = 2.10, batch loss = 2.04 (18.7 examples/sec; 0.428 sec/batch; 39h:09m:10s remains)
INFO - root - 2017-12-06 10:24:59.942031: step 3440, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 41h:05m:25s remains)
INFO - root - 2017-12-06 10:25:04.302002: step 3450, loss = 2.10, batch loss = 2.04 (19.0 examples/sec; 0.421 sec/batch; 38h:29m:40s remains)
INFO - root - 2017-12-06 10:25:08.602104: step 3460, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.425 sec/batch; 38h:49m:45s remains)
INFO - root - 2017-12-06 10:25:12.846081: step 3470, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.431 sec/batch; 39h:23m:46s remains)
INFO - root - 2017-12-06 10:25:17.239133: step 3480, loss = 2.08, batch loss = 2.03 (18.5 examples/sec; 0.433 sec/batch; 39h:34m:43s remains)
INFO - root - 2017-12-06 10:25:21.520437: step 3490, loss = 2.07, batch loss = 2.01 (19.6 examples/sec; 0.409 sec/batch; 37h:22m:55s remains)
INFO - root - 2017-12-06 10:25:25.802067: step 3500, loss = 2.05, batch loss = 1.99 (18.5 examples/sec; 0.433 sec/batch; 39h:34m:03s remains)
2017-12-06 10:25:26.343456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2183018 -4.221941 -4.2257738 -4.229187 -4.23725 -4.2463927 -4.2484808 -4.247025 -4.2372985 -4.2354627 -4.2384081 -4.2310672 -4.2178588 -4.1927552 -4.1648645][-4.1892595 -4.1886086 -4.1868582 -4.1895885 -4.2017756 -4.2112045 -4.2137995 -4.2150021 -4.2082024 -4.2103014 -4.2156477 -4.2067308 -4.1863785 -4.1527977 -4.1196175][-4.1365242 -4.1352892 -4.136323 -4.1448927 -4.1623716 -4.17054 -4.1706491 -4.1750851 -4.1794348 -4.1926913 -4.2032385 -4.1949487 -4.1665258 -4.1241217 -4.0831971][-4.1057587 -4.10606 -4.1144185 -4.1294317 -4.1492243 -4.1506314 -4.1419158 -4.1464806 -4.1613984 -4.1813936 -4.194942 -4.1861176 -4.1501093 -4.1016941 -4.0557652][-4.1072512 -4.106606 -4.1184845 -4.1355257 -4.1516504 -4.145493 -4.1292744 -4.133924 -4.1545439 -4.1762972 -4.1902456 -4.1837893 -4.149291 -4.0990295 -4.053947][-4.1367421 -4.12763 -4.13329 -4.1458869 -4.1555953 -4.1433563 -4.1236081 -4.1327791 -4.1624036 -4.1908274 -4.2061534 -4.2027879 -4.174252 -4.1264544 -4.0834141][-4.1546555 -4.1390457 -4.1334472 -4.1399093 -4.1406283 -4.123589 -4.1041832 -4.1168756 -4.15264 -4.1864185 -4.2042866 -4.2063866 -4.1848412 -4.1387792 -4.0936294][-4.1617656 -4.1407814 -4.1245313 -4.1247377 -4.1223297 -4.1029315 -4.0841784 -4.0956178 -4.1304746 -4.1618862 -4.1760368 -4.1775694 -4.1579323 -4.1162672 -4.0763574][-4.1733208 -4.1525164 -4.1357064 -4.1340117 -4.1363826 -4.1258912 -4.1163177 -4.1250939 -4.1473045 -4.1628184 -4.1662235 -4.1622787 -4.1432562 -4.1111126 -4.0816503][-4.1841345 -4.1717992 -4.1632123 -4.1666317 -4.1773567 -4.1789837 -4.178997 -4.1851726 -4.193399 -4.19596 -4.190042 -4.1825838 -4.1672797 -4.1464229 -4.1285462][-4.1989822 -4.1946883 -4.1923466 -4.1991434 -4.2152443 -4.2249079 -4.2307515 -4.2367487 -4.2380648 -4.2346616 -4.224926 -4.2131424 -4.199615 -4.1867633 -4.1806412][-4.2090158 -4.2102957 -4.2168627 -4.22915 -4.2475576 -4.2618093 -4.27293 -4.2813315 -4.2825003 -4.2781811 -4.2697115 -4.2589555 -4.2463717 -4.2346745 -4.2304454][-4.2292681 -4.2344055 -4.2451677 -4.2596393 -4.2771249 -4.2916384 -4.3044581 -4.3126926 -4.3136587 -4.3093572 -4.3033824 -4.2985024 -4.2907176 -4.2832527 -4.2802958][-4.231379 -4.2359047 -4.2478805 -4.2636652 -4.2802496 -4.2921181 -4.3002253 -4.3048477 -4.3055177 -4.3033276 -4.3023911 -4.3056936 -4.3079505 -4.3071852 -4.307579][-4.2241144 -4.2274256 -4.2411413 -4.2621956 -4.2830124 -4.2943664 -4.3002434 -4.303524 -4.3036542 -4.3036022 -4.3059325 -4.3137512 -4.3205066 -4.3228574 -4.3237834]]...]
INFO - root - 2017-12-06 10:25:30.404558: step 3510, loss = 2.10, batch loss = 2.04 (19.2 examples/sec; 0.417 sec/batch; 38h:05m:20s remains)
INFO - root - 2017-12-06 10:25:34.661004: step 3520, loss = 2.08, batch loss = 2.03 (19.3 examples/sec; 0.414 sec/batch; 37h:50m:02s remains)
INFO - root - 2017-12-06 10:25:38.977294: step 3530, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.427 sec/batch; 39h:01m:10s remains)
INFO - root - 2017-12-06 10:25:43.250669: step 3540, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.426 sec/batch; 38h:53m:08s remains)
INFO - root - 2017-12-06 10:25:47.629902: step 3550, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.447 sec/batch; 40h:50m:30s remains)
INFO - root - 2017-12-06 10:25:51.950110: step 3560, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.426 sec/batch; 38h:54m:30s remains)
INFO - root - 2017-12-06 10:25:56.275622: step 3570, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.439 sec/batch; 40h:07m:02s remains)
INFO - root - 2017-12-06 10:26:00.676403: step 3580, loss = 2.07, batch loss = 2.02 (19.1 examples/sec; 0.420 sec/batch; 38h:21m:19s remains)
INFO - root - 2017-12-06 10:26:04.990243: step 3590, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:52m:22s remains)
INFO - root - 2017-12-06 10:26:09.364455: step 3600, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 39h:47m:43s remains)
2017-12-06 10:26:09.932289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2655816 -4.235908 -4.192564 -4.1430826 -4.1171436 -4.1227722 -4.14675 -4.1693764 -4.1947665 -4.2324367 -4.2665491 -4.2687607 -4.2378368 -4.1994309 -4.1708236][-4.2644844 -4.2442894 -4.2098341 -4.1559372 -4.1105351 -4.1015205 -4.1247406 -4.1562953 -4.1904655 -4.2330961 -4.268743 -4.2671151 -4.23226 -4.1906157 -4.1577597][-4.2626381 -4.2547746 -4.2361817 -4.1882882 -4.1320834 -4.1074991 -4.1253848 -4.1645117 -4.2051597 -4.2479873 -4.2822971 -4.2825217 -4.2519307 -4.2135425 -4.1795878][-4.2581444 -4.2600489 -4.2559628 -4.2195482 -4.1603317 -4.124258 -4.1346488 -4.1777925 -4.2253518 -4.2694077 -4.3029656 -4.3065972 -4.2815614 -4.2462811 -4.2117052][-4.2517138 -4.2587 -4.2622433 -4.2357645 -4.1785727 -4.1355743 -4.1391249 -4.1811619 -4.2337217 -4.2796354 -4.3141341 -4.3233809 -4.3047376 -4.2725363 -4.2363029][-4.237555 -4.2469821 -4.2521343 -4.2311034 -4.178586 -4.1333618 -4.1330023 -4.1741734 -4.2265368 -4.2721863 -4.3075838 -4.3222561 -4.3091917 -4.2818971 -4.2461171][-4.2101851 -4.2200418 -4.2257957 -4.2088351 -4.16624 -4.1289024 -4.1304536 -4.1688905 -4.2155142 -4.254878 -4.2862091 -4.3040748 -4.2971292 -4.274353 -4.2402048][-4.1754575 -4.1879296 -4.2016129 -4.1939931 -4.1616845 -4.1353369 -4.1381087 -4.1705551 -4.2077875 -4.2359819 -4.259304 -4.277842 -4.2756968 -4.2558475 -4.2250781][-4.1567793 -4.1694427 -4.193069 -4.1960373 -4.1723385 -4.1541233 -4.159256 -4.1840677 -4.20803 -4.2221804 -4.2344241 -4.2472157 -4.2482185 -4.2369332 -4.2168822][-4.1662936 -4.1719713 -4.1962996 -4.2058845 -4.191483 -4.1821365 -4.1907549 -4.208828 -4.2197971 -4.2222233 -4.223597 -4.2258024 -4.2252274 -4.2250872 -4.2203293][-4.1924062 -4.1886415 -4.2040644 -4.2141962 -4.2068267 -4.2040429 -4.2133918 -4.2238092 -4.227468 -4.2239351 -4.2227674 -4.2192111 -4.215456 -4.2211604 -4.2269616][-4.2061863 -4.1961617 -4.2016473 -4.2092876 -4.2062626 -4.2051668 -4.2098684 -4.2116284 -4.2107425 -4.2104378 -4.2145047 -4.212369 -4.2076106 -4.2138267 -4.223865][-4.2023807 -4.1893177 -4.190465 -4.1970372 -4.1968288 -4.1940918 -4.191668 -4.18507 -4.180069 -4.1840334 -4.1935296 -4.1965065 -4.1940446 -4.1994061 -4.2100644][-4.205883 -4.1930976 -4.1923409 -4.1985393 -4.2004361 -4.1956091 -4.1886296 -4.1786747 -4.1718521 -4.1753039 -4.1856742 -4.1913395 -4.1914287 -4.1965609 -4.2068062][-4.2217693 -4.2134566 -4.2154231 -4.2228584 -4.2260337 -4.221364 -4.2121119 -4.1998491 -4.1909323 -4.1913862 -4.198905 -4.2056375 -4.2076488 -4.2117033 -4.2203565]]...]
INFO - root - 2017-12-06 10:26:13.863491: step 3610, loss = 2.10, batch loss = 2.04 (18.5 examples/sec; 0.433 sec/batch; 39h:35m:18s remains)
INFO - root - 2017-12-06 10:26:18.170621: step 3620, loss = 2.06, batch loss = 2.01 (18.0 examples/sec; 0.443 sec/batch; 40h:30m:11s remains)
INFO - root - 2017-12-06 10:26:22.418079: step 3630, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.420 sec/batch; 38h:23m:32s remains)
INFO - root - 2017-12-06 10:26:26.629827: step 3640, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 38h:44m:39s remains)
INFO - root - 2017-12-06 10:26:30.827892: step 3650, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.427 sec/batch; 39h:00m:42s remains)
INFO - root - 2017-12-06 10:26:35.226496: step 3660, loss = 2.11, batch loss = 2.05 (18.6 examples/sec; 0.429 sec/batch; 39h:11m:46s remains)
INFO - root - 2017-12-06 10:26:39.463795: step 3670, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.415 sec/batch; 37h:52m:23s remains)
INFO - root - 2017-12-06 10:26:43.721463: step 3680, loss = 2.07, batch loss = 2.02 (18.6 examples/sec; 0.431 sec/batch; 39h:21m:36s remains)
INFO - root - 2017-12-06 10:26:48.018163: step 3690, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.428 sec/batch; 39h:04m:47s remains)
INFO - root - 2017-12-06 10:26:52.348782: step 3700, loss = 2.06, batch loss = 2.01 (18.6 examples/sec; 0.430 sec/batch; 39h:14m:15s remains)
2017-12-06 10:26:52.806728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2682858 -4.2823124 -4.2887993 -4.287313 -4.2848248 -4.2824936 -4.2832675 -4.2887177 -4.2942805 -4.2977633 -4.3002353 -4.303884 -4.3068113 -4.3083959 -4.30898][-4.2869925 -4.2979479 -4.2988305 -4.2937107 -4.2898188 -4.2883964 -4.2912221 -4.2975788 -4.300983 -4.3005719 -4.2986774 -4.2987828 -4.3006859 -4.3049431 -4.3093472][-4.3145323 -4.323566 -4.3192649 -4.3075809 -4.2955694 -4.2892971 -4.2893305 -4.2936578 -4.29401 -4.2886224 -4.2829785 -4.280118 -4.2845879 -4.2962489 -4.3067794][-4.3366237 -4.3441024 -4.3339453 -4.3118367 -4.2882109 -4.2739549 -4.2696638 -4.2702503 -4.2659378 -4.2552767 -4.2460361 -4.2431693 -4.2558217 -4.2797089 -4.299531][-4.3431716 -4.3479033 -4.3309126 -4.2974505 -4.2628307 -4.2409263 -4.2323875 -4.2275848 -4.2143211 -4.1946282 -4.1789732 -4.1762743 -4.2013254 -4.2433619 -4.2785554][-4.3321314 -4.3353367 -4.3131595 -4.2696691 -4.2243404 -4.1971364 -4.184773 -4.1742797 -4.1521149 -4.1204967 -4.0979142 -4.1006312 -4.144083 -4.2096353 -4.2625051][-4.3091941 -4.3081169 -4.2812023 -4.2304955 -4.1790919 -4.1509762 -4.1380224 -4.1242132 -4.095264 -4.0569229 -4.0350676 -4.0541978 -4.1214504 -4.2028346 -4.2614551][-4.2890716 -4.2850542 -4.2578378 -4.209229 -4.1602149 -4.1365652 -4.1279593 -4.1149673 -4.0864849 -4.0500646 -4.0380745 -4.0726786 -4.1476622 -4.2241106 -4.2724929][-4.2790256 -4.275363 -4.2519336 -4.211854 -4.1739721 -4.1576958 -4.1522923 -4.1425204 -4.1236057 -4.1011715 -4.1042738 -4.1424327 -4.2029958 -4.2567754 -4.2849631][-4.2834263 -4.279973 -4.2625751 -4.2358627 -4.2131338 -4.20727 -4.2070637 -4.2006292 -4.1899824 -4.1814094 -4.1927314 -4.2211919 -4.25845 -4.2880597 -4.2952147][-4.2941008 -4.2928581 -4.2828059 -4.2686834 -4.260313 -4.2620163 -4.26395 -4.2600532 -4.2544761 -4.2516756 -4.2597818 -4.2757258 -4.2936897 -4.3043766 -4.2985268][-4.3046875 -4.3051724 -4.3009844 -4.2957325 -4.2935948 -4.2950521 -4.2966309 -4.2952409 -4.2922139 -4.2906737 -4.2943368 -4.3010535 -4.3063064 -4.3060718 -4.2966256][-4.3176184 -4.3180447 -4.3161216 -4.3134665 -4.3106828 -4.3098783 -4.310432 -4.3095241 -4.3073716 -4.3057132 -4.3070478 -4.309227 -4.3081455 -4.3036203 -4.2966747][-4.327395 -4.3284259 -4.3276129 -4.3255234 -4.322751 -4.3215365 -4.3221383 -4.3219972 -4.320394 -4.3194461 -4.3195987 -4.3184438 -4.313849 -4.3080287 -4.30391][-4.3311682 -4.3322349 -4.3310509 -4.3285556 -4.325798 -4.3251452 -4.3263721 -4.3279028 -4.3279247 -4.3267174 -4.3252697 -4.3227406 -4.3180203 -4.3134704 -4.3124981]]...]
INFO - root - 2017-12-06 10:26:56.861940: step 3710, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.420 sec/batch; 38h:21m:55s remains)
INFO - root - 2017-12-06 10:27:01.094127: step 3720, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.441 sec/batch; 40h:17m:17s remains)
INFO - root - 2017-12-06 10:27:05.382593: step 3730, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 39h:03m:22s remains)
INFO - root - 2017-12-06 10:27:09.726960: step 3740, loss = 2.09, batch loss = 2.04 (18.6 examples/sec; 0.429 sec/batch; 39h:10m:54s remains)
INFO - root - 2017-12-06 10:27:14.096612: step 3750, loss = 2.11, batch loss = 2.06 (17.6 examples/sec; 0.455 sec/batch; 41h:32m:14s remains)
INFO - root - 2017-12-06 10:27:18.402895: step 3760, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 39h:26m:13s remains)
INFO - root - 2017-12-06 10:27:22.768847: step 3770, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.435 sec/batch; 39h:43m:57s remains)
INFO - root - 2017-12-06 10:27:27.087785: step 3780, loss = 2.07, batch loss = 2.01 (19.1 examples/sec; 0.419 sec/batch; 38h:15m:16s remains)
INFO - root - 2017-12-06 10:27:31.402473: step 3790, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.438 sec/batch; 39h:57m:16s remains)
INFO - root - 2017-12-06 10:27:35.704561: step 3800, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.448 sec/batch; 40h:52m:37s remains)
2017-12-06 10:27:36.235364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3078265 -4.3064857 -4.3102455 -4.31643 -4.3199081 -4.3236213 -4.3274164 -4.3276892 -4.3228011 -4.3173614 -4.315 -4.3154669 -4.3192115 -4.3240018 -4.3217344][-4.2930632 -4.2879868 -4.290174 -4.2971559 -4.3028955 -4.308835 -4.3151383 -4.3153453 -4.3089991 -4.3025913 -4.3019071 -4.3028412 -4.3070455 -4.31458 -4.312861][-4.2699127 -4.2597089 -4.2595458 -4.2680783 -4.2783823 -4.2879963 -4.2990556 -4.3021255 -4.2984662 -4.2949448 -4.2944503 -4.2946105 -4.2985168 -4.3071861 -4.304635][-4.239903 -4.2191491 -4.2116847 -4.2215195 -4.2401013 -4.2587638 -4.2779388 -4.2849879 -4.2884712 -4.2931566 -4.2952495 -4.29544 -4.298439 -4.3064013 -4.3014016][-4.2077541 -4.1724863 -4.1527462 -4.1534405 -4.1701484 -4.1925778 -4.2178125 -4.2356005 -4.2569356 -4.2767072 -4.2839932 -4.2882624 -4.2933993 -4.3017068 -4.2995291][-4.1947079 -4.1485038 -4.1112247 -4.0876379 -4.0798836 -4.0848351 -4.1010795 -4.1309123 -4.1801395 -4.2222 -4.24169 -4.2590928 -4.2740779 -4.2897744 -4.2960773][-4.2023382 -4.1526427 -4.1017838 -4.0542607 -4.0103068 -3.9691567 -3.9420819 -3.9692688 -4.0556555 -4.1309896 -4.17136 -4.2081413 -4.2378078 -4.2664456 -4.2838488][-4.2197351 -4.1754804 -4.1256742 -4.0721426 -4.006391 -3.9143229 -3.8168266 -3.8147392 -3.9287391 -4.0376329 -4.1020622 -4.1575251 -4.1969757 -4.2326946 -4.260406][-4.249352 -4.219183 -4.1819005 -4.1417384 -4.0871997 -3.9966741 -3.8880711 -3.851372 -3.9263933 -4.0181489 -4.0855646 -4.1475792 -4.1902504 -4.224124 -4.2528772][-4.2712984 -4.2563949 -4.2363429 -4.2148895 -4.1832647 -4.1253772 -4.0558891 -4.023304 -4.0483279 -4.0942464 -4.1363468 -4.180892 -4.2125969 -4.2379122 -4.2608385][-4.2786756 -4.2723293 -4.2659578 -4.2601914 -4.2466683 -4.2172461 -4.1868548 -4.1717916 -4.1749763 -4.1919031 -4.2093439 -4.231091 -4.2493339 -4.2675486 -4.2846289][-4.2830667 -4.2793808 -4.2807961 -4.2864828 -4.2841177 -4.2690282 -4.2610445 -4.2633095 -4.265985 -4.2752166 -4.28072 -4.2872605 -4.2939391 -4.3056464 -4.3161631][-4.2873955 -4.2877145 -4.2938123 -4.3040762 -4.3111424 -4.3072505 -4.30986 -4.3188329 -4.3219881 -4.3281088 -4.330163 -4.3306093 -4.3298836 -4.3339319 -4.3370805][-4.2928209 -4.2958665 -4.3051286 -4.31619 -4.3260469 -4.3303757 -4.3367767 -4.3444171 -4.3458643 -4.3492484 -4.3503323 -4.34935 -4.3459396 -4.34532 -4.34389][-4.3018947 -4.3037453 -4.3107138 -4.3184738 -4.325994 -4.3314333 -4.3363724 -4.3406034 -4.3413482 -4.3436913 -4.3461242 -4.3454738 -4.3426123 -4.3409357 -4.3384423]]...]
INFO - root - 2017-12-06 10:27:40.206348: step 3810, loss = 2.06, batch loss = 2.01 (18.8 examples/sec; 0.425 sec/batch; 38h:48m:46s remains)
INFO - root - 2017-12-06 10:27:44.570034: step 3820, loss = 2.07, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 39h:33m:05s remains)
INFO - root - 2017-12-06 10:27:48.976220: step 3830, loss = 2.07, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 41h:06m:07s remains)
INFO - root - 2017-12-06 10:27:53.338092: step 3840, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.432 sec/batch; 39h:25m:03s remains)
INFO - root - 2017-12-06 10:27:57.692873: step 3850, loss = 2.10, batch loss = 2.04 (19.1 examples/sec; 0.419 sec/batch; 38h:16m:10s remains)
INFO - root - 2017-12-06 10:28:01.928771: step 3860, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.429 sec/batch; 39h:10m:42s remains)
INFO - root - 2017-12-06 10:28:06.104582: step 3870, loss = 2.08, batch loss = 2.03 (18.7 examples/sec; 0.428 sec/batch; 39h:05m:27s remains)
INFO - root - 2017-12-06 10:28:10.328193: step 3880, loss = 2.09, batch loss = 2.04 (19.0 examples/sec; 0.421 sec/batch; 38h:25m:52s remains)
INFO - root - 2017-12-06 10:28:14.544703: step 3890, loss = 2.08, batch loss = 2.02 (19.2 examples/sec; 0.417 sec/batch; 38h:03m:28s remains)
INFO - root - 2017-12-06 10:28:18.777099: step 3900, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.414 sec/batch; 37h:49m:45s remains)
2017-12-06 10:28:19.287010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2947865 -4.29362 -4.2962193 -4.2998929 -4.3041306 -4.309051 -4.3142624 -4.3149729 -4.3068676 -4.2898808 -4.2700834 -4.2580371 -4.2622709 -4.28202 -4.305274][-4.2995534 -4.3011084 -4.3066587 -4.3134089 -4.3205924 -4.3265829 -4.3316636 -4.3324041 -4.326324 -4.3164945 -4.3060708 -4.29856 -4.29923 -4.3105884 -4.3241973][-4.3109808 -4.3148336 -4.3212881 -4.3267789 -4.3304186 -4.3314285 -4.331614 -4.3289371 -4.3231492 -4.3184452 -4.3148007 -4.3114634 -4.3116355 -4.3198342 -4.3301387][-4.3236175 -4.3267312 -4.3306031 -4.33072 -4.3257794 -4.3157415 -4.3063126 -4.2978 -4.2905965 -4.2883224 -4.2880511 -4.2868729 -4.2890911 -4.3009276 -4.3159928][-4.331984 -4.3330383 -4.3313236 -4.3219719 -4.3049369 -4.2799735 -4.2569118 -4.2408857 -4.2336149 -4.2316346 -4.2302284 -4.2283263 -4.2345729 -4.2562113 -4.2839003][-4.3289175 -4.3284025 -4.3222637 -4.3032751 -4.2720428 -4.2301707 -4.1928639 -4.17105 -4.1625919 -4.1575961 -4.1509233 -4.1472716 -4.1627669 -4.2022347 -4.2490168][-4.3076458 -4.3086138 -4.301549 -4.2758694 -4.2327032 -4.1747222 -4.1238008 -4.094871 -4.08723 -4.0840278 -4.07644 -4.0798569 -4.1141829 -4.1749372 -4.237103][-4.2646403 -4.2704716 -4.26574 -4.2369022 -4.183876 -4.1119404 -4.0452819 -4.0090036 -4.0070024 -4.0183072 -4.0322442 -4.0616903 -4.1193285 -4.1929097 -4.2554064][-4.2142744 -4.2296605 -4.2322335 -4.2065392 -4.1511474 -4.072926 -3.9961963 -3.9589314 -3.96804 -4.0020041 -4.0468445 -4.1031289 -4.1732111 -4.2419043 -4.2902][-4.1853108 -4.2133174 -4.2252727 -4.2061067 -4.1570492 -4.0881042 -4.0228004 -3.9967308 -4.0168619 -4.0648623 -4.12193 -4.18318 -4.2442741 -4.2927833 -4.3215752][-4.1923695 -4.2298727 -4.2498713 -4.2406054 -4.2058511 -4.1557126 -4.1089458 -4.0925379 -4.1138887 -4.158206 -4.2078381 -4.2566104 -4.29714 -4.3239131 -4.3370743][-4.2304287 -4.2668538 -4.2875938 -4.2874904 -4.2682891 -4.2376237 -4.2086778 -4.1979504 -4.21317 -4.2441897 -4.2783093 -4.30817 -4.32851 -4.3382974 -4.3400273][-4.2816906 -4.307785 -4.3231821 -4.3257113 -4.3178859 -4.3028822 -4.28639 -4.2789593 -4.287199 -4.3053527 -4.3231616 -4.3345838 -4.3386445 -4.3366289 -4.3309445][-4.3153672 -4.3301749 -4.3399959 -4.3432112 -4.3412724 -4.3333707 -4.3241777 -4.3181758 -4.3205342 -4.3291512 -4.3362079 -4.3374 -4.3330259 -4.3259473 -4.3181806][-4.3277664 -4.33394 -4.338716 -4.3398509 -4.3375888 -4.3317671 -4.3251514 -4.3201041 -4.3208628 -4.32675 -4.3311572 -4.3306766 -4.324976 -4.317616 -4.3117027]]...]
INFO - root - 2017-12-06 10:28:23.209507: step 3910, loss = 2.11, batch loss = 2.06 (31.5 examples/sec; 0.254 sec/batch; 23h:13m:00s remains)
INFO - root - 2017-12-06 10:28:27.568949: step 3920, loss = 2.09, batch loss = 2.04 (17.8 examples/sec; 0.450 sec/batch; 41h:06m:16s remains)
INFO - root - 2017-12-06 10:28:31.845821: step 3930, loss = 2.11, batch loss = 2.05 (18.7 examples/sec; 0.428 sec/batch; 39h:02m:36s remains)
INFO - root - 2017-12-06 10:28:36.154472: step 3940, loss = 2.08, batch loss = 2.03 (19.0 examples/sec; 0.421 sec/batch; 38h:25m:06s remains)
INFO - root - 2017-12-06 10:28:40.407435: step 3950, loss = 2.11, batch loss = 2.05 (19.7 examples/sec; 0.407 sec/batch; 37h:07m:20s remains)
INFO - root - 2017-12-06 10:28:44.737313: step 3960, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.440 sec/batch; 40h:09m:03s remains)
INFO - root - 2017-12-06 10:28:49.091098: step 3970, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.446 sec/batch; 40h:43m:17s remains)
INFO - root - 2017-12-06 10:28:53.503290: step 3980, loss = 2.11, batch loss = 2.05 (17.7 examples/sec; 0.452 sec/batch; 41h:15m:43s remains)
INFO - root - 2017-12-06 10:28:57.859641: step 3990, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.429 sec/batch; 39h:10m:46s remains)
INFO - root - 2017-12-06 10:29:02.371697: step 4000, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.424 sec/batch; 38h:40m:48s remains)
2017-12-06 10:29:02.947971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2406168 -4.25237 -4.2699132 -4.2788906 -4.2856207 -4.2981887 -4.3125143 -4.3204041 -4.324255 -4.3269825 -4.3230877 -4.3128057 -4.3026171 -4.302032 -4.3151078][-4.2373738 -4.2503438 -4.2680225 -4.2775621 -4.2822561 -4.292027 -4.305531 -4.3167729 -4.3234081 -4.3231483 -4.3178506 -4.3082333 -4.2986126 -4.2962613 -4.3066459][-4.2225504 -4.23045 -4.2455821 -4.2551179 -4.2585254 -4.2627811 -4.2750587 -4.2923732 -4.3091731 -4.316277 -4.3151298 -4.3072891 -4.2957716 -4.2876282 -4.2908006][-4.1941686 -4.194809 -4.2098956 -4.2243309 -4.2341485 -4.2373657 -4.2465296 -4.2649088 -4.2879333 -4.3024497 -4.3053803 -4.3009195 -4.2899942 -4.2764573 -4.2727423][-4.1766219 -4.1729574 -4.1893387 -4.2076774 -4.2187181 -4.2164888 -4.221005 -4.2403569 -4.2657642 -4.2825265 -4.2890987 -4.28794 -4.2793016 -4.2650628 -4.2585735][-4.1820669 -4.1767659 -4.1899481 -4.1992831 -4.1986666 -4.1809535 -4.1733737 -4.1937528 -4.229382 -4.2575207 -4.272686 -4.2774382 -4.2721024 -4.2598577 -4.2497559][-4.205368 -4.2004256 -4.2074914 -4.2019014 -4.1793151 -4.1340752 -4.0999947 -4.1195731 -4.1795669 -4.2319994 -4.2638726 -4.2770605 -4.2739635 -4.2629433 -4.2499089][-4.2295747 -4.2262049 -4.2260656 -4.2118521 -4.1773062 -4.1139135 -4.0537515 -4.0651269 -4.1444283 -4.2181597 -4.26542 -4.2860007 -4.283288 -4.2723646 -4.258399][-4.242012 -4.2386446 -4.23452 -4.2220531 -4.1959095 -4.1418047 -4.0825129 -4.0805964 -4.1480021 -4.217165 -4.26494 -4.2874804 -4.285687 -4.2762346 -4.2675109][-4.246882 -4.2435904 -4.2396631 -4.2344165 -4.2220693 -4.1860642 -4.1401691 -4.1268716 -4.1670661 -4.2182455 -4.2583861 -4.2796407 -4.283287 -4.2788706 -4.27714][-4.2556496 -4.2534456 -4.2534528 -4.2577691 -4.2552676 -4.2303629 -4.1942534 -4.174284 -4.1938572 -4.2292991 -4.2619934 -4.2818379 -4.290575 -4.2903528 -4.2901807][-4.267468 -4.2675672 -4.2708225 -4.278347 -4.2815304 -4.265224 -4.2397919 -4.2206326 -4.2295165 -4.253715 -4.2758164 -4.2909493 -4.2997 -4.3001184 -4.3001876][-4.2795157 -4.2804456 -4.2822208 -4.283721 -4.2849178 -4.2764692 -4.2647381 -4.2546654 -4.2605681 -4.2762752 -4.2876334 -4.2957153 -4.3000135 -4.2985516 -4.3001213][-4.278018 -4.2792296 -4.2776127 -4.2689633 -4.2629728 -4.2583013 -4.2591915 -4.2600293 -4.2660413 -4.2789764 -4.288518 -4.2956505 -4.2978454 -4.2953944 -4.2976704][-4.27039 -4.2711573 -4.2649069 -4.2480092 -4.23696 -4.23491 -4.2414589 -4.2464914 -4.2520971 -4.2670226 -4.2830873 -4.2978096 -4.3027558 -4.2989497 -4.2978897]]...]
INFO - root - 2017-12-06 10:29:06.910056: step 4010, loss = 2.07, batch loss = 2.01 (23.5 examples/sec; 0.340 sec/batch; 31h:02m:32s remains)
INFO - root - 2017-12-06 10:29:11.154084: step 4020, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 38h:35m:13s remains)
INFO - root - 2017-12-06 10:29:15.432438: step 4030, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.431 sec/batch; 39h:19m:33s remains)
INFO - root - 2017-12-06 10:29:19.691236: step 4040, loss = 2.05, batch loss = 2.00 (18.8 examples/sec; 0.425 sec/batch; 38h:46m:26s remains)
INFO - root - 2017-12-06 10:29:23.976910: step 4050, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.431 sec/batch; 39h:21m:55s remains)
INFO - root - 2017-12-06 10:29:28.306625: step 4060, loss = 2.06, batch loss = 2.00 (19.1 examples/sec; 0.420 sec/batch; 38h:17m:45s remains)
INFO - root - 2017-12-06 10:29:32.599098: step 4070, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.427 sec/batch; 38h:58m:13s remains)
INFO - root - 2017-12-06 10:29:36.899562: step 4080, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.432 sec/batch; 39h:22m:33s remains)
INFO - root - 2017-12-06 10:29:41.346059: step 4090, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.427 sec/batch; 38h:57m:08s remains)
INFO - root - 2017-12-06 10:29:45.631990: step 4100, loss = 2.09, batch loss = 2.04 (18.3 examples/sec; 0.438 sec/batch; 39h:56m:59s remains)
2017-12-06 10:29:46.133419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3107934 -4.3187528 -4.3177161 -4.3127809 -4.3150382 -4.3173981 -4.3132215 -4.311604 -4.3178272 -4.3207345 -4.3209343 -4.321528 -4.3218074 -4.3205395 -4.31975][-4.3155894 -4.3202124 -4.3146815 -4.3114948 -4.3192019 -4.32351 -4.3175564 -4.3178778 -4.3286853 -4.3329358 -4.3313236 -4.328825 -4.3261056 -4.3243809 -4.3258381][-4.2995739 -4.2969518 -4.2883525 -4.2929349 -4.3066006 -4.3134089 -4.308075 -4.3094606 -4.3212142 -4.32428 -4.3212481 -4.3148007 -4.3093152 -4.3099675 -4.3155665][-4.2596412 -4.2543073 -4.250948 -4.2657285 -4.2838135 -4.2909293 -4.2836614 -4.2830029 -4.2930756 -4.2944756 -4.2897768 -4.2835655 -4.2814608 -4.2888317 -4.2993307][-4.2265477 -4.2272844 -4.2320094 -4.2493286 -4.2643971 -4.2647095 -4.247797 -4.2406783 -4.2489662 -4.250679 -4.2464724 -4.2474694 -4.2554026 -4.2744226 -4.2875338][-4.2233586 -4.2302485 -4.2398515 -4.2549567 -4.2610941 -4.2437739 -4.2083864 -4.1900525 -4.1968327 -4.2048492 -4.2139907 -4.2321863 -4.2516856 -4.272635 -4.281311][-4.2364264 -4.2401876 -4.2440939 -4.2470288 -4.2342191 -4.19425 -4.1359978 -4.1061726 -4.1252165 -4.1586113 -4.1983709 -4.2345071 -4.2581124 -4.270226 -4.2671032][-4.2486081 -4.2419782 -4.2319312 -4.2182865 -4.1829762 -4.1135249 -4.026176 -3.9925303 -4.0533543 -4.132082 -4.1989703 -4.2371922 -4.2527847 -4.2523503 -4.2397676][-4.2507658 -4.2371464 -4.2232618 -4.200242 -4.1516986 -4.0665555 -3.9679713 -3.9472785 -4.0465961 -4.1527672 -4.2211761 -4.2424507 -4.2410831 -4.2297635 -4.2154121][-4.2465463 -4.2353754 -4.2257314 -4.2045636 -4.1656656 -4.1058397 -4.0492835 -4.0547118 -4.133832 -4.2093229 -4.2480893 -4.2415247 -4.2213 -4.2040753 -4.1962614][-4.2358594 -4.2312703 -4.2267451 -4.2122211 -4.1909051 -4.1631975 -4.1474237 -4.1698031 -4.2188053 -4.2528958 -4.2595406 -4.2292604 -4.1944718 -4.182807 -4.1931081][-4.2257628 -4.2274513 -4.2265587 -4.2187471 -4.2096014 -4.1969028 -4.1997252 -4.2254858 -4.2515979 -4.2595854 -4.2459168 -4.202632 -4.1699839 -4.1769657 -4.210403][-4.2270494 -4.2341657 -4.2346849 -4.2285728 -4.2227855 -4.2164974 -4.2266712 -4.24778 -4.2601423 -4.252974 -4.2255373 -4.181119 -4.1619482 -4.1873384 -4.2323442][-4.2118568 -4.2183657 -4.21924 -4.2173834 -4.214169 -4.2123537 -4.2267518 -4.2412395 -4.2442966 -4.23336 -4.2061496 -4.1736245 -4.1674862 -4.1974082 -4.242929][-4.163743 -4.16809 -4.1725168 -4.1773715 -4.1796417 -4.1841741 -4.202178 -4.2121139 -4.2115307 -4.2018876 -4.1853514 -4.1704955 -4.1711044 -4.1912823 -4.2271981]]...]
INFO - root - 2017-12-06 10:29:50.287365: step 4110, loss = 2.07, batch loss = 2.01 (23.0 examples/sec; 0.348 sec/batch; 31h:43m:05s remains)
INFO - root - 2017-12-06 10:29:54.483569: step 4120, loss = 2.08, batch loss = 2.03 (19.2 examples/sec; 0.416 sec/batch; 37h:58m:38s remains)
INFO - root - 2017-12-06 10:29:58.792654: step 4130, loss = 2.08, batch loss = 2.03 (19.0 examples/sec; 0.421 sec/batch; 38h:23m:25s remains)
INFO - root - 2017-12-06 10:30:03.111113: step 4140, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.466 sec/batch; 42h:29m:08s remains)
INFO - root - 2017-12-06 10:30:07.364989: step 4150, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.427 sec/batch; 38h:58m:53s remains)
INFO - root - 2017-12-06 10:30:11.693657: step 4160, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.425 sec/batch; 38h:45m:37s remains)
INFO - root - 2017-12-06 10:30:16.010186: step 4170, loss = 2.11, batch loss = 2.05 (18.2 examples/sec; 0.440 sec/batch; 40h:08m:45s remains)
INFO - root - 2017-12-06 10:30:20.344436: step 4180, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.453 sec/batch; 41h:19m:07s remains)
INFO - root - 2017-12-06 10:30:24.660674: step 4190, loss = 2.08, batch loss = 2.02 (19.3 examples/sec; 0.414 sec/batch; 37h:43m:51s remains)
INFO - root - 2017-12-06 10:30:28.927968: step 4200, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.429 sec/batch; 39h:07m:48s remains)
2017-12-06 10:30:29.434911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2777514 -4.2679248 -4.2683525 -4.276083 -4.28299 -4.2820749 -4.2755651 -4.2621579 -4.2435808 -4.2214632 -4.1983867 -4.179152 -4.1695142 -4.1803508 -4.2074895][-4.2601948 -4.255949 -4.2571621 -4.2587709 -4.2538624 -4.237361 -4.2139387 -4.1867213 -4.1586657 -4.128191 -4.09627 -4.0692511 -4.0567422 -4.0729809 -4.1143117][-4.2590976 -4.2615538 -4.259841 -4.2497745 -4.2300892 -4.1967506 -4.1576185 -4.1195736 -4.0888228 -4.0587754 -4.0262628 -3.9994578 -3.9910927 -4.0148611 -4.0626626][-4.249815 -4.2577448 -4.2533779 -4.2341914 -4.204041 -4.1603765 -4.11268 -4.0734453 -4.0493522 -4.0325241 -4.01576 -4.0050836 -4.0127811 -4.0415158 -4.0816875][-4.21188 -4.2257209 -4.224072 -4.2053404 -4.173933 -4.1311259 -4.0874581 -4.0598345 -4.0524511 -4.0553188 -4.0596051 -4.0678163 -4.0853906 -4.1092172 -4.1350212][-4.1482906 -4.171145 -4.1773372 -4.1626415 -4.1312022 -4.0931206 -4.0606408 -4.0507641 -4.060977 -4.0798759 -4.099987 -4.1212039 -4.14354 -4.1637306 -4.1795177][-4.0854406 -4.1168494 -4.1281843 -4.1145039 -4.0851049 -4.054038 -4.0307846 -4.031549 -4.0545349 -4.0851893 -4.1130214 -4.1384454 -4.1616316 -4.1836228 -4.2010827][-4.0543661 -4.0914006 -4.1056366 -4.0957646 -4.0764565 -4.0618358 -4.0535221 -4.0600228 -4.0840073 -4.1104946 -4.1310558 -4.1488829 -4.1689806 -4.1942029 -4.2184615][-4.065043 -4.1049433 -4.1262708 -4.128963 -4.1280146 -4.1297708 -4.1309175 -4.1354051 -4.1457438 -4.1554885 -4.1618509 -4.1688571 -4.1851377 -4.2121677 -4.2415576][-4.1103783 -4.1459184 -4.1723824 -4.1865911 -4.1967559 -4.2049251 -4.208519 -4.2089186 -4.2043343 -4.19438 -4.182735 -4.1756916 -4.1842074 -4.2104492 -4.2443132][-4.14806 -4.1746497 -4.200707 -4.2194543 -4.2360892 -4.2511067 -4.2614756 -4.2645535 -4.2545991 -4.2288418 -4.1980028 -4.1723409 -4.1653051 -4.1839523 -4.2208309][-4.1723242 -4.1908627 -4.2133245 -4.2333226 -4.2548027 -4.27756 -4.2954931 -4.3027143 -4.2913985 -4.2562122 -4.2098069 -4.1665325 -4.1446896 -4.1549878 -4.1932011][-4.2105665 -4.2214127 -4.2380619 -4.2559929 -4.277348 -4.2999997 -4.3177586 -4.3253884 -4.3150449 -4.281877 -4.2365317 -4.1935987 -4.170558 -4.177433 -4.2097259][-4.2599473 -4.2647829 -4.2747555 -4.2870197 -4.3017759 -4.316885 -4.3285933 -4.3340936 -4.3286219 -4.3096385 -4.2829175 -4.2558436 -4.2395916 -4.2410903 -4.2557917][-4.3091412 -4.3106542 -4.3146658 -4.3196907 -4.3246474 -4.3296924 -4.3339534 -4.3363571 -4.3342004 -4.3258195 -4.3144779 -4.3016887 -4.2896323 -4.2817478 -4.2755504]]...]
INFO - root - 2017-12-06 10:30:33.452289: step 4210, loss = 2.09, batch loss = 2.03 (35.1 examples/sec; 0.228 sec/batch; 20h:47m:54s remains)
INFO - root - 2017-12-06 10:30:37.579319: step 4220, loss = 2.12, batch loss = 2.06 (19.0 examples/sec; 0.422 sec/batch; 38h:29m:34s remains)
INFO - root - 2017-12-06 10:30:41.779444: step 4230, loss = 2.06, batch loss = 2.00 (19.2 examples/sec; 0.417 sec/batch; 37h:59m:22s remains)
INFO - root - 2017-12-06 10:30:46.089374: step 4240, loss = 2.07, batch loss = 2.02 (18.6 examples/sec; 0.430 sec/batch; 39h:14m:16s remains)
INFO - root - 2017-12-06 10:30:50.402049: step 4250, loss = 2.06, batch loss = 2.00 (19.0 examples/sec; 0.421 sec/batch; 38h:21m:20s remains)
INFO - root - 2017-12-06 10:30:54.682909: step 4260, loss = 2.10, batch loss = 2.04 (19.3 examples/sec; 0.415 sec/batch; 37h:52m:24s remains)
INFO - root - 2017-12-06 10:30:59.017777: step 4270, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.452 sec/batch; 41h:11m:24s remains)
INFO - root - 2017-12-06 10:31:03.339105: step 4280, loss = 2.06, batch loss = 2.00 (18.5 examples/sec; 0.433 sec/batch; 39h:29m:16s remains)
INFO - root - 2017-12-06 10:31:07.637705: step 4290, loss = 2.10, batch loss = 2.04 (19.3 examples/sec; 0.415 sec/batch; 37h:49m:56s remains)
INFO - root - 2017-12-06 10:31:12.029859: step 4300, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 38h:41m:12s remains)
2017-12-06 10:31:12.492793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2051911 -4.2055597 -4.2029676 -4.1892972 -4.1655984 -4.1411405 -4.1332397 -4.14796 -4.1718397 -4.1936607 -4.2085962 -4.2134485 -4.2083097 -4.1979766 -4.1869617][-4.19626 -4.1986008 -4.1937976 -4.170846 -4.133822 -4.0997133 -4.0944324 -4.1201425 -4.1535311 -4.1813264 -4.2013903 -4.2113724 -4.2079291 -4.1953411 -4.1811805][-4.1899533 -4.1913581 -4.1797276 -4.1448278 -4.0951958 -4.0560341 -4.0565748 -4.0934916 -4.1368656 -4.1716528 -4.1965532 -4.2094312 -4.2062845 -4.1905379 -4.1715016][-4.1876845 -4.18471 -4.1633949 -4.1167316 -4.0589581 -4.0197692 -4.0271606 -4.0745282 -4.1271062 -4.1689 -4.197916 -4.2106276 -4.2053838 -4.1840873 -4.1566443][-4.1883192 -4.1771331 -4.1456285 -4.0889578 -4.0279408 -3.9919899 -4.0062428 -4.0644827 -4.1266952 -4.1754942 -4.2083626 -4.2206078 -4.2108574 -4.1797 -4.1397738][-4.1920328 -4.170558 -4.1300688 -4.06744 -4.0060725 -3.9723222 -3.9934654 -4.0621328 -4.1330347 -4.18871 -4.2253919 -4.2365217 -4.21961 -4.1743608 -4.1188483][-4.1918139 -4.162251 -4.1164265 -4.050303 -3.9866881 -3.9493275 -3.9760196 -4.0561061 -4.1366258 -4.1987557 -4.2391567 -4.2507477 -4.2282057 -4.1731782 -4.1066942][-4.184545 -4.1544809 -4.1104522 -4.0444818 -3.9755492 -3.931325 -3.9612646 -4.050077 -4.1382513 -4.2042813 -4.248 -4.2619877 -4.2392459 -4.1820607 -4.1101203][-4.168786 -4.145865 -4.1142459 -4.0613446 -3.9971457 -3.9519694 -3.9788399 -4.0653033 -4.1531029 -4.2164936 -4.2586718 -4.2728548 -4.2526684 -4.2010932 -4.1310158][-4.150239 -4.1374168 -4.1235495 -4.0909605 -4.0430055 -4.0076423 -4.0331521 -4.1093073 -4.1864824 -4.23814 -4.2694359 -4.2783313 -4.2624207 -4.2203479 -4.1580567][-4.1362386 -4.1354942 -4.1400151 -4.129221 -4.101078 -4.0789409 -4.10357 -4.1642423 -4.2221613 -4.2582126 -4.2771163 -4.2805352 -4.2676787 -4.232903 -4.1770806][-4.1277814 -4.141686 -4.1630492 -4.1696582 -4.1566796 -4.14443 -4.1645389 -4.2082119 -4.2490649 -4.274034 -4.284555 -4.2839904 -4.2708135 -4.2364278 -4.1807785][-4.1284781 -4.1538086 -4.1860571 -4.2033849 -4.2014627 -4.196444 -4.2089744 -4.236001 -4.264843 -4.2848582 -4.2926168 -4.2892857 -4.2719893 -4.2335787 -4.1758432][-4.1347246 -4.16403 -4.1986303 -4.2194171 -4.2227459 -4.2216926 -4.2274046 -4.2441339 -4.2684159 -4.2878675 -4.296391 -4.2923656 -4.2715898 -4.232646 -4.1789732][-4.1462321 -4.1755548 -4.208014 -4.2256961 -4.2271137 -4.2254734 -4.2223687 -4.2289805 -4.2522511 -4.2740331 -4.2864056 -4.2853761 -4.2645121 -4.2292743 -4.1862645]]...]
INFO - root - 2017-12-06 10:31:16.661063: step 4310, loss = 2.11, batch loss = 2.05 (22.0 examples/sec; 0.363 sec/batch; 33h:08m:02s remains)
INFO - root - 2017-12-06 10:31:20.808330: step 4320, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.430 sec/batch; 39h:09m:14s remains)
INFO - root - 2017-12-06 10:31:25.110241: step 4330, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 39h:24m:45s remains)
INFO - root - 2017-12-06 10:31:29.416828: step 4340, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.439 sec/batch; 40h:00m:30s remains)
INFO - root - 2017-12-06 10:31:33.820192: step 4350, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 39h:20m:57s remains)
INFO - root - 2017-12-06 10:31:38.065676: step 4360, loss = 2.10, batch loss = 2.04 (18.4 examples/sec; 0.436 sec/batch; 39h:43m:27s remains)
INFO - root - 2017-12-06 10:31:42.354950: step 4370, loss = 2.08, batch loss = 2.03 (18.6 examples/sec; 0.429 sec/batch; 39h:06m:01s remains)
INFO - root - 2017-12-06 10:31:46.698833: step 4380, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.452 sec/batch; 41h:09m:38s remains)
INFO - root - 2017-12-06 10:31:51.013992: step 4390, loss = 2.06, batch loss = 2.01 (19.1 examples/sec; 0.419 sec/batch; 38h:11m:30s remains)
INFO - root - 2017-12-06 10:31:55.320763: step 4400, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.431 sec/batch; 39h:14m:26s remains)
2017-12-06 10:31:55.816853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2201447 -4.2360191 -4.2516232 -4.249311 -4.2372913 -4.2201729 -4.2102122 -4.2075882 -4.2065606 -4.2016897 -4.2034674 -4.2073622 -4.2246304 -4.2463117 -4.2458568][-4.2194786 -4.2374759 -4.2532659 -4.2458024 -4.2329793 -4.2253113 -4.224607 -4.2266984 -4.2301245 -4.2269106 -4.2279248 -4.2331858 -4.2411275 -4.2441287 -4.2322297][-4.1904263 -4.2081337 -4.2222466 -4.2163215 -4.2080979 -4.204442 -4.2103758 -4.2201962 -4.2279387 -4.2288694 -4.2279191 -4.2324076 -4.229517 -4.2154555 -4.200388][-4.127079 -4.1456094 -4.1595583 -4.1567397 -4.1485839 -4.1449614 -4.1522918 -4.1652703 -4.1771116 -4.1839185 -4.1837807 -4.1823573 -4.1684446 -4.1481953 -4.1373496][-4.0411787 -4.0604491 -4.0788689 -4.0865417 -4.088881 -4.0937886 -4.1020412 -4.1090178 -4.1154623 -4.1196513 -4.1149888 -4.1094165 -4.0957065 -4.0794892 -4.0725484][-3.9841375 -4.0074286 -4.0311151 -4.0449567 -4.0496836 -4.0540156 -4.056953 -4.0520058 -4.0473013 -4.0547771 -4.0514326 -4.0431795 -4.0343 -4.0198011 -4.01823][-3.9573002 -3.983259 -4.0042763 -4.0066581 -3.9990332 -3.9913602 -3.972898 -3.9368591 -3.9162462 -3.947125 -3.9611945 -3.9616203 -3.9658279 -3.9575045 -3.9661996][-3.9682817 -3.9782753 -3.9810951 -3.9598708 -3.9299319 -3.9002318 -3.8558848 -3.7909541 -3.76097 -3.8311391 -3.8797333 -3.8996584 -3.9264777 -3.9395447 -3.9702179][-4.0295992 -4.0192437 -4.0044851 -3.9701939 -3.9348941 -3.8993988 -3.8509214 -3.7924814 -3.7763429 -3.8518336 -3.9034791 -3.9299164 -3.97102 -4.0019789 -4.0452981][-4.0959029 -4.076355 -4.0569935 -4.0293813 -4.0066795 -3.9842949 -3.9587107 -3.9318614 -3.9304006 -3.9797354 -4.0111117 -4.0307817 -4.0687556 -4.098659 -4.1362238][-4.1482182 -4.1332054 -4.1209846 -4.103992 -4.0896297 -4.0793366 -4.07283 -4.0667238 -4.0678177 -4.0915184 -4.1066008 -4.1206059 -4.1494431 -4.1681929 -4.1930218][-4.1936393 -4.1837616 -4.1785059 -4.1699648 -4.1615615 -4.1597457 -4.1622334 -4.1615896 -4.1546073 -4.1599631 -4.1661897 -4.1761737 -4.1916971 -4.1982684 -4.2097764][-4.2375011 -4.2283669 -4.22376 -4.2182775 -4.2141986 -4.2174983 -4.2229171 -4.2203951 -4.2092919 -4.2047949 -4.2039237 -4.2053876 -4.2054043 -4.1958184 -4.1917171][-4.2709136 -4.2637205 -4.2602944 -4.2571168 -4.2559175 -4.25948 -4.2630076 -4.26077 -4.25291 -4.2474027 -4.2430911 -4.2356482 -4.2208915 -4.1936455 -4.169301][-4.2975922 -4.2934208 -4.2908921 -4.2881479 -4.2866421 -4.2878852 -4.2889071 -4.287838 -4.2844648 -4.2819037 -4.2779241 -4.2668238 -4.2461443 -4.21113 -4.1709642]]...]
INFO - root - 2017-12-06 10:31:59.984972: step 4410, loss = 2.10, batch loss = 2.04 (27.4 examples/sec; 0.292 sec/batch; 26h:37m:45s remains)
INFO - root - 2017-12-06 10:32:04.101888: step 4420, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 38h:47m:31s remains)
INFO - root - 2017-12-06 10:32:08.325015: step 4430, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.414 sec/batch; 37h:44m:42s remains)
INFO - root - 2017-12-06 10:32:12.536167: step 4440, loss = 2.08, batch loss = 2.03 (20.4 examples/sec; 0.392 sec/batch; 35h:45m:46s remains)
INFO - root - 2017-12-06 10:32:16.736531: step 4450, loss = 2.08, batch loss = 2.03 (18.8 examples/sec; 0.424 sec/batch; 38h:40m:25s remains)
INFO - root - 2017-12-06 10:32:20.984630: step 4460, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.421 sec/batch; 38h:20m:43s remains)
INFO - root - 2017-12-06 10:32:25.243810: step 4470, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 40h:58m:40s remains)
INFO - root - 2017-12-06 10:32:29.564169: step 4480, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 39h:24m:30s remains)
INFO - root - 2017-12-06 10:32:33.831733: step 4490, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 39h:43m:03s remains)
INFO - root - 2017-12-06 10:32:38.108503: step 4500, loss = 2.05, batch loss = 2.00 (19.0 examples/sec; 0.422 sec/batch; 38h:25m:43s remains)
2017-12-06 10:32:38.566902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2760525 -4.273725 -4.2741294 -4.2750444 -4.2681804 -4.2510481 -4.223824 -4.2087812 -4.223753 -4.245173 -4.2602739 -4.2685452 -4.2786684 -4.2850704 -4.2889981][-4.2607179 -4.2524319 -4.2562594 -4.2679577 -4.2746015 -4.2660208 -4.2420893 -4.218914 -4.22865 -4.2507887 -4.2632327 -4.2654963 -4.2681708 -4.2671876 -4.2660904][-4.243731 -4.226027 -4.2277393 -4.2432837 -4.2512035 -4.2394567 -4.2194576 -4.1991806 -4.215179 -4.2455754 -4.2584949 -4.255537 -4.2484808 -4.23294 -4.2199411][-4.2209783 -4.190743 -4.1848693 -4.1926236 -4.191977 -4.1741118 -4.157239 -4.1462116 -4.1759491 -4.2197781 -4.2348013 -4.222753 -4.206039 -4.1784139 -4.1607819][-4.1982188 -4.1541977 -4.1367955 -4.128583 -4.12018 -4.1009593 -4.0873089 -4.082016 -4.1210866 -4.1778092 -4.199532 -4.1857805 -4.1667519 -4.1414242 -4.1330161][-4.1801782 -4.1250477 -4.0927496 -4.0688133 -4.0552344 -4.0353923 -4.0185585 -4.007205 -4.0497746 -4.1219749 -4.1577187 -4.1586723 -4.1556964 -4.1507349 -4.1561613][-4.1671114 -4.1117954 -4.0692525 -4.0286918 -4.0019512 -3.9701254 -3.9390426 -3.9077542 -3.9483635 -4.0459557 -4.1127968 -4.1430383 -4.1634336 -4.1801381 -4.1927485][-4.1761737 -4.1342363 -4.0905333 -4.0334406 -3.9885764 -3.9419107 -3.8918369 -3.8336303 -3.8610525 -3.9754398 -4.073369 -4.1285214 -4.1632829 -4.191483 -4.2088947][-4.1965909 -4.1734524 -4.143393 -4.0911551 -4.0491295 -4.0070739 -3.9579232 -3.8965 -3.9078798 -4.0034337 -4.0948362 -4.1449656 -4.1768265 -4.2036161 -4.220808][-4.22493 -4.2192473 -4.2035975 -4.167109 -4.1416659 -4.1128216 -4.0739236 -4.025032 -4.0266685 -4.0882778 -4.1492577 -4.1760716 -4.1914291 -4.2089386 -4.2217579][-4.2443542 -4.2514014 -4.2467351 -4.2227635 -4.2078538 -4.1886663 -4.162406 -4.132494 -4.1313457 -4.1631775 -4.1963773 -4.2044764 -4.2030292 -4.2060232 -4.2132988][-4.2551217 -4.2671452 -4.26823 -4.2550292 -4.2467527 -4.2340345 -4.2181363 -4.2008643 -4.1999269 -4.2131739 -4.2319221 -4.2310944 -4.2183204 -4.2092209 -4.2074018][-4.2619104 -4.2746358 -4.2784338 -4.2749715 -4.2752318 -4.2712569 -4.2639284 -4.2539239 -4.25249 -4.2562709 -4.2650275 -4.2598615 -4.2416487 -4.2262645 -4.21642][-4.2649159 -4.2766352 -4.283123 -4.2878156 -4.2945862 -4.2972188 -4.2960582 -4.2911868 -4.289639 -4.2886887 -4.2902884 -4.2850661 -4.2690959 -4.25123 -4.2361488][-4.2640605 -4.2756314 -4.2831678 -4.2902584 -4.2978296 -4.3031363 -4.305284 -4.3041897 -4.3034596 -4.30046 -4.3005557 -4.2976813 -4.2855916 -4.2683239 -4.249939]]...]
INFO - root - 2017-12-06 10:32:42.797018: step 4510, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 38h:30m:07s remains)
INFO - root - 2017-12-06 10:32:46.811655: step 4520, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 38h:26m:25s remains)
INFO - root - 2017-12-06 10:32:51.080247: step 4530, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.429 sec/batch; 39h:07m:26s remains)
INFO - root - 2017-12-06 10:32:55.330143: step 4540, loss = 2.07, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:45m:33s remains)
INFO - root - 2017-12-06 10:32:59.551586: step 4550, loss = 2.08, batch loss = 2.02 (19.4 examples/sec; 0.413 sec/batch; 37h:39m:45s remains)
INFO - root - 2017-12-06 10:33:03.882745: step 4560, loss = 2.10, batch loss = 2.04 (17.9 examples/sec; 0.447 sec/batch; 40h:41m:32s remains)
INFO - root - 2017-12-06 10:33:08.162451: step 4570, loss = 2.07, batch loss = 2.01 (19.4 examples/sec; 0.412 sec/batch; 37h:33m:08s remains)
INFO - root - 2017-12-06 10:33:12.432636: step 4580, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.426 sec/batch; 38h:48m:40s remains)
INFO - root - 2017-12-06 10:33:16.777697: step 4590, loss = 2.10, batch loss = 2.04 (18.3 examples/sec; 0.437 sec/batch; 39h:50m:00s remains)
