INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "188"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr10-lastlr15-clip10000
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>]
2017-12-10 05:13:03.861973: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:13:03.862008: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:13:03.862014: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:13:03.862018: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:13:03.862022: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:13:04.238085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 05:13:04.238124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 05:13:04.238131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 05:13:04.238138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 05:13:07.577978: step 0, loss = 2.28, batch loss = 2.23 (3.3 examples/sec; 2.432 sec/batch; 224h:35m:27s remains)
2017-12-10 05:13:07.960476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290175 -4.4290104 -4.4290061 -4.4289927 -4.4289727 -4.4289494 -4.4289222 -4.42889 -4.428864 -4.4288564 -4.4288697 -4.4288974 -4.42893 -4.4289656 -4.4289942][-4.429018 -4.4290071 -4.4289918 -4.428966 -4.4289317 -4.428894 -4.4288478 -4.4288 -4.4287729 -4.4287763 -4.4287987 -4.4288392 -4.4288874 -4.428937 -4.4289775][-4.4290147 -4.4290004 -4.4289751 -4.4289365 -4.428884 -4.4288216 -4.4287505 -4.4286823 -4.4286442 -4.4286561 -4.4286933 -4.4287567 -4.4288321 -4.4289 -4.4289546][-4.4290061 -4.4289947 -4.428968 -4.4289179 -4.4288421 -4.4287505 -4.4286489 -4.428544 -4.4284825 -4.4285007 -4.4285622 -4.4286604 -4.4287691 -4.4288588 -4.4289289][-4.4289889 -4.4289837 -4.4289632 -4.4288983 -4.4287958 -4.428669 -4.4285283 -4.4283705 -4.4282751 -4.4283061 -4.4284086 -4.4285622 -4.4287143 -4.4288306 -4.4289131][-4.4289646 -4.4289651 -4.4289465 -4.4288659 -4.4287376 -4.42857 -4.4283729 -4.4281406 -4.4280195 -4.4280968 -4.4282713 -4.4284873 -4.4286804 -4.4288182 -4.42891][-4.4289379 -4.4289374 -4.4289169 -4.4288216 -4.4286695 -4.4284625 -4.4281974 -4.4278851 -4.42777 -4.4279461 -4.4282074 -4.4284673 -4.4286757 -4.4288192 -4.4289126][-4.4289107 -4.4288974 -4.4288607 -4.4287481 -4.4285865 -4.4283652 -4.4280643 -4.4277248 -4.4276853 -4.4279671 -4.4282694 -4.4285207 -4.4287062 -4.428834 -4.4289203][-4.4288807 -4.4288468 -4.4287953 -4.4286857 -4.428544 -4.4283595 -4.4281092 -4.427866 -4.4279284 -4.4282012 -4.4284482 -4.4286342 -4.4287696 -4.4288678 -4.4289384][-4.4288516 -4.4288015 -4.4287491 -4.4286594 -4.42856 -4.428441 -4.428288 -4.4281688 -4.428268 -4.428462 -4.4286213 -4.4287333 -4.4288192 -4.4288931 -4.4289532][-4.428822 -4.4287663 -4.4287167 -4.4286513 -4.428586 -4.4285231 -4.4284458 -4.4284039 -4.4284949 -4.428618 -4.4287152 -4.42878 -4.4288406 -4.4289055 -4.42896][-4.428793 -4.4287443 -4.4287038 -4.428659 -4.4286203 -4.4285946 -4.4285693 -4.4285707 -4.4286437 -4.4287148 -4.4287677 -4.4288092 -4.4288588 -4.4289193 -4.428968][-4.4287844 -4.4287515 -4.4287257 -4.4287057 -4.4286885 -4.4286895 -4.4286933 -4.4287062 -4.42875 -4.4287767 -4.4287992 -4.4288282 -4.4288745 -4.4289336 -4.4289756][-4.4288092 -4.4287953 -4.4287877 -4.4287863 -4.42878 -4.4287853 -4.4287858 -4.4287825 -4.4287925 -4.4287868 -4.4287944 -4.4288225 -4.4288764 -4.4289374 -4.4289751][-4.4288349 -4.4288321 -4.4288383 -4.4288483 -4.4288454 -4.4288387 -4.4288173 -4.4287882 -4.4287739 -4.4287562 -4.4287629 -4.4287987 -4.4288626 -4.4289279 -4.428966]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr10-lastlr15-clip10000/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr10-lastlr15-clip10000/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 05:13:10.908899: step 10, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 26h:40m:32s remains)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-10 05:13:13.518937: step 20, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:11m:24s remains)
INFO - root - 2017-12-10 05:13:16.112112: step 30, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:30m:12s remains)
INFO - root - 2017-12-10 05:13:18.718142: step 40, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:42m:15s remains)
INFO - root - 2017-12-10 05:13:21.323190: step 50, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:14m:03s remains)
INFO - root - 2017-12-10 05:13:23.917265: step 60, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:57m:00s remains)
INFO - root - 2017-12-10 05:13:26.516544: step 70, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:56m:05s remains)
INFO - root - 2017-12-10 05:13:29.121716: step 80, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:20m:24s remains)
INFO - root - 2017-12-10 05:13:31.673840: step 90, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:17m:27s remains)
INFO - root - 2017-12-10 05:13:34.237986: step 100, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 23h:02m:04s remains)
2017-12-10 05:13:34.563055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42886 -4.4287925 -4.4287081 -4.4286447 -4.4286327 -4.4286666 -4.4287028 -4.4287596 -4.4288292 -4.428865 -4.4288673 -4.4288597 -4.428833 -4.4287791 -4.4287415][-4.4288664 -4.42879 -4.4286895 -4.4286046 -4.4285784 -4.4286089 -4.428648 -4.4287019 -4.4287667 -4.4288068 -4.4288187 -4.4288187 -4.428803 -4.4287653 -4.4287419][-4.4288645 -4.428782 -4.428679 -4.4285927 -4.4285603 -4.42858 -4.4286165 -4.4286656 -4.4287286 -4.4287753 -4.4287925 -4.4287934 -4.4287844 -4.4287567 -4.4287386][-4.4288592 -4.4287744 -4.4286757 -4.4285927 -4.4285545 -4.4285617 -4.4285893 -4.4286304 -4.4286938 -4.428751 -4.4287758 -4.4287763 -4.4287696 -4.4287519 -4.4287353][-4.4288516 -4.4287667 -4.4286728 -4.4285936 -4.4285488 -4.4285412 -4.428555 -4.4285812 -4.4286389 -4.4287062 -4.4287419 -4.4287481 -4.4287486 -4.4287434 -4.4287324][-4.4288468 -4.4287639 -4.4286742 -4.4285922 -4.4285345 -4.4285126 -4.42851 -4.428514 -4.4285555 -4.4286304 -4.4286795 -4.4286947 -4.4287062 -4.4287157 -4.4287171][-4.428844 -4.4287605 -4.428669 -4.4285822 -4.4285173 -4.4284863 -4.428473 -4.4284558 -4.4284773 -4.4285564 -4.428616 -4.4286442 -4.428668 -4.428688 -4.4286995][-4.4288454 -4.4287653 -4.4286757 -4.4285922 -4.4285345 -4.4285088 -4.4284968 -4.428473 -4.4284806 -4.4285512 -4.428607 -4.4286342 -4.42866 -4.4286795 -4.42869][-4.4288459 -4.4287672 -4.4286795 -4.4286013 -4.4285522 -4.4285345 -4.4285297 -4.4285178 -4.4285316 -4.4285913 -4.4286313 -4.4286442 -4.428659 -4.428668 -4.4286714][-4.4288478 -4.4287696 -4.4286823 -4.428606 -4.42856 -4.4285436 -4.428545 -4.4285483 -4.428575 -4.4286261 -4.4286547 -4.4286571 -4.428658 -4.4286556 -4.4286551][-4.4288607 -4.4287853 -4.4287033 -4.4286327 -4.4285893 -4.4285727 -4.4285784 -4.4285913 -4.4286189 -4.42866 -4.4286814 -4.4286809 -4.4286771 -4.4286714 -4.4286709][-4.4288821 -4.4288139 -4.4287419 -4.4286814 -4.4286427 -4.42863 -4.4286427 -4.4286633 -4.428689 -4.4287171 -4.4287305 -4.4287286 -4.4287248 -4.4287195 -4.4287167][-4.4289136 -4.4288588 -4.4288015 -4.4287491 -4.4287138 -4.4287066 -4.4287281 -4.4287591 -4.4287848 -4.4288 -4.4288044 -4.4287987 -4.4287906 -4.4287829 -4.4287786][-4.4289527 -4.4289165 -4.4288797 -4.428843 -4.4288139 -4.4288087 -4.4288297 -4.4288592 -4.4288788 -4.4288855 -4.4288845 -4.4288769 -4.4288673 -4.42886 -4.4288583][-4.4289827 -4.4289627 -4.4289446 -4.4289255 -4.4289074 -4.4289031 -4.4289155 -4.4289341 -4.4289451 -4.4289474 -4.4289451 -4.4289389 -4.4289317 -4.4289279 -4.4289289]]...]
INFO - root - 2017-12-10 05:13:37.139261: step 110, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:15m:10s remains)
INFO - root - 2017-12-10 05:13:39.751854: step 120, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:43m:58s remains)
INFO - root - 2017-12-10 05:13:42.395939: step 130, loss = 2.28, batch loss = 2.23 (27.6 examples/sec; 0.290 sec/batch; 26h:47m:14s remains)
INFO - root - 2017-12-10 05:13:44.948047: step 140, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:46m:04s remains)
INFO - root - 2017-12-10 05:13:47.504821: step 150, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:11m:03s remains)
INFO - root - 2017-12-10 05:13:50.059147: step 160, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:10m:11s remains)
INFO - root - 2017-12-10 05:13:52.724960: step 170, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:55m:38s remains)
INFO - root - 2017-12-10 05:13:55.332238: step 180, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 24h:02m:54s remains)
INFO - root - 2017-12-10 05:13:57.902287: step 190, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:44m:33s remains)
INFO - root - 2017-12-10 05:14:00.481656: step 200, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:31m:39s remains)
2017-12-10 05:14:00.816324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288158 -4.42879 -4.4287744 -4.4287791 -4.4287758 -4.4287624 -4.4287534 -4.4287491 -4.4287424 -4.4287496 -4.4287615 -4.4287777 -4.428792 -4.4288063 -4.4288125][-4.4288092 -4.4287744 -4.4287543 -4.4287524 -4.4287448 -4.4287448 -4.4287405 -4.4287281 -4.4287105 -4.4287152 -4.4287319 -4.4287505 -4.4287667 -4.4287796 -4.4287853][-4.4288006 -4.428762 -4.4287519 -4.4287481 -4.4287405 -4.4287481 -4.4287443 -4.4287281 -4.4287157 -4.4287243 -4.4287395 -4.4287515 -4.4287643 -4.4287672 -4.428761][-4.4287925 -4.4287553 -4.4287586 -4.4287591 -4.4287457 -4.4287429 -4.4287319 -4.4287181 -4.4287267 -4.4287515 -4.4287696 -4.4287691 -4.4287672 -4.4287534 -4.4287333][-4.4288006 -4.4287634 -4.4287658 -4.4287653 -4.4287438 -4.4287248 -4.4286976 -4.4286714 -4.4287057 -4.4287634 -4.4287968 -4.4287977 -4.4287796 -4.42875 -4.4287271][-4.42879 -4.4287472 -4.4287405 -4.4287467 -4.4287224 -4.4286814 -4.4286027 -4.4285293 -4.4285951 -4.4287143 -4.4287777 -4.4287887 -4.4287663 -4.4287329 -4.4287214][-4.4287419 -4.4286876 -4.4286795 -4.4287052 -4.4286847 -4.4286056 -4.428443 -4.428288 -4.4283938 -4.4286122 -4.4287257 -4.4287457 -4.4287086 -4.4286685 -4.4286723][-4.4286847 -4.4286203 -4.4286146 -4.4286594 -4.42865 -4.4285417 -4.428319 -4.4281 -4.4282351 -4.428525 -4.4286866 -4.4287181 -4.4286733 -4.428627 -4.4286346][-4.4286642 -4.4286141 -4.4286146 -4.4286709 -4.4286695 -4.4285846 -4.4284258 -4.4282708 -4.4283562 -4.4285789 -4.4287252 -4.428772 -4.4287434 -4.4287 -4.42869][-4.428699 -4.4286642 -4.428659 -4.428709 -4.4287167 -4.4286695 -4.4285922 -4.4285107 -4.4285374 -4.4286623 -4.4287634 -4.4288135 -4.4288054 -4.4287715 -4.4287515][-4.4287539 -4.4287167 -4.4287 -4.4287357 -4.4287534 -4.4287386 -4.4287019 -4.4286566 -4.4286571 -4.4287205 -4.4287829 -4.4288225 -4.4288211 -4.4287972 -4.4287786][-4.4287734 -4.4287357 -4.4287271 -4.4287629 -4.428782 -4.4287796 -4.4287562 -4.4287238 -4.4287162 -4.4287515 -4.428793 -4.4288259 -4.4288254 -4.4288106 -4.4287968][-4.4287891 -4.4287553 -4.4287581 -4.4287906 -4.4288006 -4.4288025 -4.4287844 -4.4287634 -4.42876 -4.4287763 -4.428802 -4.4288287 -4.428834 -4.428834 -4.4288306][-4.4287829 -4.4287624 -4.4287748 -4.4288034 -4.4288082 -4.428813 -4.4288077 -4.4287949 -4.428793 -4.4287953 -4.4288054 -4.4288259 -4.4288383 -4.4288459 -4.4288473][-4.4287882 -4.4287767 -4.428792 -4.4288173 -4.4288254 -4.4288311 -4.4288311 -4.4288206 -4.4288144 -4.4288111 -4.4288168 -4.4288344 -4.4288487 -4.4288554 -4.4288588]]...]
INFO - root - 2017-12-10 05:14:03.475427: step 210, loss = 2.28, batch loss = 2.23 (27.8 examples/sec; 0.288 sec/batch; 26h:34m:40s remains)
INFO - root - 2017-12-10 05:14:06.083833: step 220, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:27m:13s remains)
INFO - root - 2017-12-10 05:14:08.710549: step 230, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:21m:31s remains)
INFO - root - 2017-12-10 05:14:11.325653: step 240, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:38m:47s remains)
INFO - root - 2017-12-10 05:14:13.945388: step 250, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:12m:50s remains)
INFO - root - 2017-12-10 05:14:16.546546: step 260, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:30m:53s remains)
INFO - root - 2017-12-10 05:14:19.172082: step 270, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:27m:45s remains)
INFO - root - 2017-12-10 05:14:21.811734: step 280, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:54m:44s remains)
INFO - root - 2017-12-10 05:14:24.458575: step 290, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:22m:28s remains)
INFO - root - 2017-12-10 05:14:27.091609: step 300, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:11m:12s remains)
2017-12-10 05:14:27.365269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286156 -4.4286013 -4.4286122 -4.4286551 -4.4287176 -4.4287868 -4.4288282 -4.4288268 -4.4287834 -4.4287305 -4.4286909 -4.4286947 -4.4287076 -4.4287071 -4.428699][-4.4287395 -4.4287148 -4.4286885 -4.42868 -4.4287076 -4.4287705 -4.4288249 -4.4288387 -4.4288158 -4.4287848 -4.4287648 -4.4287763 -4.4287782 -4.4287577 -4.4287286][-4.4288321 -4.4288063 -4.4287534 -4.4286971 -4.4286833 -4.4287267 -4.42879 -4.4288311 -4.4288421 -4.4288387 -4.4288297 -4.4288344 -4.4288235 -4.4287829 -4.4287329][-4.4288735 -4.4288464 -4.4287744 -4.4286852 -4.4286361 -4.428659 -4.4287205 -4.4287796 -4.4288187 -4.4288397 -4.42884 -4.428834 -4.4288077 -4.4287562 -4.4287014][-4.4288726 -4.4288449 -4.4287663 -4.4286609 -4.4285893 -4.4285903 -4.428638 -4.4287 -4.4287539 -4.42879 -4.4287939 -4.4287744 -4.42873 -4.4286757 -4.4286265][-4.4288545 -4.4288282 -4.4287548 -4.4286604 -4.4285903 -4.4285736 -4.4286 -4.4286509 -4.4287038 -4.4287376 -4.4287372 -4.4287033 -4.4286485 -4.4285903 -4.4285445][-4.4288435 -4.4288168 -4.42875 -4.4286747 -4.42862 -4.4286013 -4.4286075 -4.4286413 -4.4286861 -4.4287066 -4.42869 -4.4286418 -4.4285817 -4.4285197 -4.4284749][-4.4288368 -4.4288077 -4.42875 -4.4286885 -4.4286451 -4.428627 -4.4286156 -4.4286337 -4.4286723 -4.4286857 -4.4286609 -4.4286065 -4.4285464 -4.4284759 -4.4284282][-4.4288378 -4.42881 -4.4287682 -4.4287224 -4.4286876 -4.4286647 -4.4286356 -4.42864 -4.4286695 -4.4286828 -4.4286685 -4.4286294 -4.4285736 -4.4285026 -4.4284611][-4.4288349 -4.42882 -4.4288011 -4.4287758 -4.4287534 -4.4287372 -4.4287019 -4.428689 -4.4287024 -4.428709 -4.4287014 -4.4286833 -4.4286413 -4.42858 -4.4285488][-4.4288015 -4.4288087 -4.4288092 -4.4288 -4.428792 -4.4287839 -4.428751 -4.4287314 -4.4287357 -4.4287319 -4.4287229 -4.4287252 -4.4287086 -4.428668 -4.42865][-4.4287462 -4.4287777 -4.428803 -4.4288158 -4.428823 -4.4288206 -4.4287896 -4.4287677 -4.4287639 -4.4287429 -4.4287243 -4.4287419 -4.4287529 -4.4287405 -4.4287357][-4.42868 -4.4287243 -4.4287658 -4.4287972 -4.42882 -4.4288278 -4.4288111 -4.4287958 -4.4287848 -4.42875 -4.4287224 -4.4287429 -4.4287634 -4.4287596 -4.4287605][-4.4286504 -4.428688 -4.4287233 -4.4287591 -4.4287934 -4.4288158 -4.42882 -4.4288177 -4.4288011 -4.4287562 -4.4287195 -4.4287353 -4.4287572 -4.4287596 -4.4287639][-4.4286771 -4.4286952 -4.4287152 -4.4287419 -4.4287782 -4.428812 -4.4288354 -4.4288454 -4.4288244 -4.4287715 -4.4287252 -4.4287281 -4.4287467 -4.4287543 -4.4287624]]...]
INFO - root - 2017-12-10 05:14:30.016482: step 310, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:50m:25s remains)
INFO - root - 2017-12-10 05:14:32.727885: step 320, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 24h:24m:11s remains)
INFO - root - 2017-12-10 05:14:35.356387: step 330, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:06m:39s remains)
INFO - root - 2017-12-10 05:14:38.054748: step 340, loss = 2.28, batch loss = 2.23 (27.4 examples/sec; 0.292 sec/batch; 26h:57m:24s remains)
INFO - root - 2017-12-10 05:14:40.693516: step 350, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:53m:35s remains)
INFO - root - 2017-12-10 05:14:43.330111: step 360, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:19m:47s remains)
INFO - root - 2017-12-10 05:14:45.940046: step 370, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:36m:38s remains)
INFO - root - 2017-12-10 05:14:48.539051: step 380, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 25h:07m:35s remains)
INFO - root - 2017-12-10 05:14:51.142774: step 390, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:37m:29s remains)
INFO - root - 2017-12-10 05:14:53.786306: step 400, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:12m:53s remains)
2017-12-10 05:14:54.049971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288716 -4.4288549 -4.4288535 -4.4288578 -4.4288874 -4.4288979 -4.4288931 -4.42889 -4.4288917 -4.428896 -4.4288988 -4.4289036 -4.4289031 -4.4288898 -4.4288683][-4.4288454 -4.4288278 -4.4288325 -4.4288468 -4.4288864 -4.4288974 -4.4288826 -4.4288678 -4.4288626 -4.428864 -4.42886 -4.4288559 -4.4288492 -4.4288344 -4.4288116][-4.4288411 -4.4288177 -4.4288216 -4.428843 -4.4288874 -4.428895 -4.4288726 -4.4288478 -4.4288373 -4.4288335 -4.428822 -4.428813 -4.4287992 -4.4287844 -4.4287634][-4.4288268 -4.4287958 -4.428793 -4.4288158 -4.428863 -4.4288678 -4.4288344 -4.4288011 -4.4287906 -4.4287887 -4.4287739 -4.4287596 -4.4287453 -4.4287386 -4.4287271][-4.4287872 -4.4287343 -4.4287124 -4.4287324 -4.4287848 -4.4287958 -4.4287629 -4.4287291 -4.4287214 -4.4287286 -4.42872 -4.4287086 -4.4287 -4.4287062 -4.428709][-4.4287739 -4.4286933 -4.428638 -4.4286356 -4.4286771 -4.4286952 -4.4286761 -4.4286528 -4.42866 -4.4286819 -4.4286809 -4.4286761 -4.4286771 -4.4286895 -4.4287024][-4.4287939 -4.4287062 -4.42862 -4.42858 -4.4285951 -4.428607 -4.4285946 -4.4285803 -4.4286041 -4.4286427 -4.4286551 -4.428658 -4.428669 -4.4286895 -4.4287105][-4.428813 -4.4287434 -4.42866 -4.42861 -4.4286156 -4.4286184 -4.4285836 -4.4285522 -4.4285731 -4.4286103 -4.4286261 -4.42864 -4.4286633 -4.4286966 -4.4287276][-4.4287782 -4.4287238 -4.4286675 -4.4286456 -4.4286828 -4.4287062 -4.4286604 -4.4286079 -4.4286032 -4.4286184 -4.4286246 -4.4286404 -4.4286714 -4.4287124 -4.4287539][-4.42869 -4.4286346 -4.4285994 -4.42861 -4.428689 -4.4287558 -4.4287405 -4.4286971 -4.4286742 -4.4286675 -4.4286628 -4.4286718 -4.4286995 -4.4287381 -4.4287825][-4.4286323 -4.42858 -4.4285603 -4.428586 -4.4286766 -4.428762 -4.4287791 -4.4287658 -4.4287519 -4.4287415 -4.4287329 -4.4287357 -4.4287553 -4.4287844 -4.42882][-4.4286551 -4.4286 -4.4285827 -4.4286113 -4.428699 -4.4287806 -4.4288058 -4.4288044 -4.4288011 -4.4287992 -4.4288006 -4.4288082 -4.4288244 -4.4288406 -4.4288626][-4.4287362 -4.4286828 -4.4286585 -4.4286752 -4.4287419 -4.4288073 -4.4288311 -4.4288311 -4.428833 -4.4288397 -4.4288516 -4.4288688 -4.4288864 -4.428895 -4.4289055][-4.4288354 -4.4287972 -4.4287705 -4.4287691 -4.4288054 -4.4288454 -4.428864 -4.4288669 -4.42887 -4.4288783 -4.4288898 -4.4289069 -4.4289222 -4.42893 -4.4289374][-4.4289074 -4.4288855 -4.428865 -4.4288573 -4.4288731 -4.4288921 -4.4289026 -4.4289055 -4.4289088 -4.4289131 -4.4289179 -4.4289274 -4.4289384 -4.4289474 -4.4289565]]...]
INFO - root - 2017-12-10 05:14:56.671029: step 410, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:41m:15s remains)
INFO - root - 2017-12-10 05:14:59.285458: step 420, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:32m:24s remains)
INFO - root - 2017-12-10 05:15:01.889651: step 430, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:20m:28s remains)
INFO - root - 2017-12-10 05:15:04.558941: step 440, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 25h:04m:23s remains)
INFO - root - 2017-12-10 05:15:07.156698: step 450, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:30m:35s remains)
INFO - root - 2017-12-10 05:15:09.855533: step 460, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:55m:04s remains)
INFO - root - 2017-12-10 05:15:12.540284: step 470, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:39m:25s remains)
INFO - root - 2017-12-10 05:15:15.166268: step 480, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:51m:45s remains)
INFO - root - 2017-12-10 05:15:17.799796: step 490, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 24h:23m:33s remains)
INFO - root - 2017-12-10 05:15:20.466592: step 500, loss = 2.28, batch loss = 2.23 (25.0 examples/sec; 0.319 sec/batch; 29h:27m:34s remains)
2017-12-10 05:15:20.765658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287138 -4.4287715 -4.4288349 -4.4288888 -4.4289455 -4.4289813 -4.4289703 -4.428937 -4.4288888 -4.4288497 -4.4288435 -4.4288583 -4.4288797 -4.4288907 -4.4288688][-4.4286551 -4.4287386 -4.4288349 -4.428916 -4.4289813 -4.4290137 -4.428988 -4.4289317 -4.4288626 -4.4288154 -4.4288106 -4.4288321 -4.4288583 -4.4288721 -4.4288564][-4.428628 -4.4287314 -4.4288545 -4.4289532 -4.42901 -4.429028 -4.428988 -4.4289107 -4.428823 -4.4287677 -4.4287682 -4.4288058 -4.4288435 -4.4288688 -4.4288692][-4.42865 -4.4287577 -4.4288797 -4.4289737 -4.4290147 -4.4290147 -4.428956 -4.4288573 -4.4287543 -4.4286976 -4.4287143 -4.4287791 -4.4288359 -4.4288778 -4.4288969][-4.428719 -4.4288092 -4.4288988 -4.4289656 -4.4289918 -4.4289703 -4.4288859 -4.4287572 -4.4286418 -4.4286051 -4.4286604 -4.4287577 -4.4288292 -4.428885 -4.4289169][-4.4288144 -4.42887 -4.4289179 -4.4289484 -4.42895 -4.4288983 -4.4287734 -4.4286013 -4.4284892 -4.4285049 -4.4286127 -4.4287348 -4.4288111 -4.4288669 -4.4289007][-4.4288869 -4.4289088 -4.4289212 -4.4289217 -4.4288888 -4.4287944 -4.42861 -4.4283795 -4.4282956 -4.4283934 -4.4285612 -4.4287028 -4.4287858 -4.4288392 -4.428865][-4.4288912 -4.4288926 -4.4288912 -4.4288731 -4.4288068 -4.4286737 -4.4284368 -4.4281721 -4.4281483 -4.4283257 -4.42853 -4.4286771 -4.4287663 -4.4288182 -4.4288354][-4.4288435 -4.4288359 -4.4288349 -4.4288149 -4.4287357 -4.4285908 -4.4283557 -4.4281363 -4.4281707 -4.4283652 -4.4285555 -4.4286804 -4.4287615 -4.4288082 -4.4288182][-4.4287949 -4.4287786 -4.4287806 -4.4287615 -4.42868 -4.4285479 -4.4283729 -4.4282527 -4.4283175 -4.4284763 -4.4286246 -4.42872 -4.4287767 -4.42881 -4.4288139][-4.4287486 -4.4287281 -4.4287214 -4.4287033 -4.4286337 -4.4285393 -4.4284387 -4.4283953 -4.4284625 -4.4285765 -4.4286838 -4.4287553 -4.4287958 -4.4288249 -4.4288292][-4.4287014 -4.4286757 -4.4286633 -4.4286537 -4.4286146 -4.4285679 -4.4285245 -4.4285192 -4.4285731 -4.42865 -4.4287262 -4.4287806 -4.4288168 -4.4288454 -4.4288535][-4.428678 -4.4286542 -4.4286475 -4.4286504 -4.4286427 -4.4286337 -4.4286246 -4.4286304 -4.4286623 -4.4287128 -4.4287672 -4.4288077 -4.4288387 -4.4288659 -4.4288769][-4.4287205 -4.4287038 -4.4287047 -4.4287157 -4.4287262 -4.4287357 -4.4287391 -4.4287443 -4.4287615 -4.4287949 -4.4288325 -4.4288578 -4.4288793 -4.4289007 -4.4289117][-4.4288192 -4.4288049 -4.4288096 -4.4288225 -4.4288306 -4.4288392 -4.428843 -4.4288459 -4.4288564 -4.4288774 -4.4289012 -4.4289155 -4.4289265 -4.4289384 -4.4289465]]...]
INFO - root - 2017-12-10 05:15:23.452835: step 510, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 25h:14m:03s remains)
INFO - root - 2017-12-10 05:15:26.054267: step 520, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 23h:11m:05s remains)
INFO - root - 2017-12-10 05:15:28.702795: step 530, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:15m:44s remains)
INFO - root - 2017-12-10 05:15:31.339494: step 540, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:47m:20s remains)
INFO - root - 2017-12-10 05:15:33.963493: step 550, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:42m:21s remains)
INFO - root - 2017-12-10 05:15:36.556523: step 560, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:55m:19s remains)
INFO - root - 2017-12-10 05:15:39.239084: step 570, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 24h:00m:15s remains)
INFO - root - 2017-12-10 05:15:41.845656: step 580, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 26h:14m:57s remains)
INFO - root - 2017-12-10 05:15:44.478467: step 590, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:28m:16s remains)
INFO - root - 2017-12-10 05:15:47.120666: step 600, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:41m:48s remains)
2017-12-10 05:15:47.413308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288158 -4.4287944 -4.4287791 -4.4287672 -4.4287591 -4.4287634 -4.4288158 -4.4288788 -4.4289207 -4.4289308 -4.4289322 -4.4289136 -4.4288774 -4.4288726 -4.4288979][-4.4287782 -4.4287643 -4.4287596 -4.4287472 -4.4287376 -4.4287438 -4.4287963 -4.4288573 -4.4288936 -4.428894 -4.428885 -4.4288697 -4.4288549 -4.4288745 -4.4289222][-4.4287305 -4.4287271 -4.4287424 -4.4287367 -4.4287224 -4.4287243 -4.4287729 -4.4288249 -4.4288464 -4.4288397 -4.4288311 -4.4288192 -4.42881 -4.4288363 -4.4288979][-4.4286928 -4.4286981 -4.428721 -4.4287128 -4.4286828 -4.4286718 -4.428709 -4.4287529 -4.4287753 -4.4287815 -4.4287872 -4.4287763 -4.4287658 -4.428781 -4.428833][-4.4286776 -4.4286914 -4.4287047 -4.4286709 -4.4286175 -4.4285932 -4.4286203 -4.4286647 -4.4287062 -4.4287357 -4.4287534 -4.4287386 -4.4287148 -4.428709 -4.4287438][-4.4286814 -4.4286909 -4.4286747 -4.4286017 -4.4285192 -4.4284797 -4.4285026 -4.4285684 -4.4286366 -4.4286785 -4.4286938 -4.4286718 -4.4286308 -4.4286022 -4.4286213][-4.428679 -4.4286714 -4.4286203 -4.4285159 -4.4284091 -4.4283595 -4.4283834 -4.4284635 -4.4285364 -4.4285765 -4.4285884 -4.4285607 -4.428504 -4.4284711 -4.4285026][-4.4286613 -4.42864 -4.4285765 -4.4284668 -4.4283614 -4.4283142 -4.4283333 -4.428401 -4.4284472 -4.4284625 -4.4284596 -4.4284296 -4.4283733 -4.42837 -4.428442][-4.4286747 -4.4286489 -4.4285955 -4.4285083 -4.4284272 -4.4283876 -4.4283872 -4.4284153 -4.4284186 -4.4284053 -4.4283872 -4.4283552 -4.4283156 -4.4283438 -4.4284463][-4.4287186 -4.4287019 -4.4286709 -4.4286184 -4.4285727 -4.4285507 -4.4285412 -4.42854 -4.4285164 -4.428483 -4.4284496 -4.42841 -4.42838 -4.4284177 -4.428515][-4.428771 -4.4287639 -4.4287543 -4.4287333 -4.4287171 -4.4287162 -4.42871 -4.4286981 -4.4286737 -4.4286404 -4.4286017 -4.4285607 -4.4285364 -4.428565 -4.4286361][-4.4288039 -4.4287968 -4.4287958 -4.4287949 -4.4288006 -4.4288173 -4.4288239 -4.4288168 -4.428802 -4.4287786 -4.428751 -4.4287233 -4.4287047 -4.4287205 -4.4287639][-4.42882 -4.4288058 -4.4288 -4.4288063 -4.4288278 -4.4288526 -4.4288645 -4.4288664 -4.4288621 -4.42885 -4.4288335 -4.4288192 -4.4288073 -4.4288168 -4.4288425][-4.4288239 -4.4288054 -4.4288 -4.4288135 -4.4288425 -4.4288678 -4.4288764 -4.4288759 -4.4288692 -4.4288583 -4.4288464 -4.4288363 -4.4288249 -4.4288325 -4.4288516][-4.4288225 -4.4288139 -4.4288163 -4.4288363 -4.4288621 -4.4288788 -4.4288807 -4.4288712 -4.4288526 -4.42883 -4.4288139 -4.4288044 -4.4287953 -4.4288068 -4.4288297]]...]
INFO - root - 2017-12-10 05:15:50.053740: step 610, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:27m:17s remains)
INFO - root - 2017-12-10 05:15:52.667705: step 620, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 24h:22m:56s remains)
INFO - root - 2017-12-10 05:15:55.317009: step 630, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:59m:45s remains)
INFO - root - 2017-12-10 05:15:57.895237: step 640, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 26h:20m:27s remains)
INFO - root - 2017-12-10 05:16:00.516464: step 650, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:53m:37s remains)
INFO - root - 2017-12-10 05:16:03.142159: step 660, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:41m:33s remains)
INFO - root - 2017-12-10 05:16:05.813934: step 670, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:16m:07s remains)
INFO - root - 2017-12-10 05:16:08.441432: step 680, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:34m:53s remains)
INFO - root - 2017-12-10 05:16:11.088096: step 690, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:28m:49s remains)
INFO - root - 2017-12-10 05:16:13.698517: step 700, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:23m:29s remains)
2017-12-10 05:16:14.004361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287672 -4.4287591 -4.4287596 -4.4287648 -4.4287696 -4.4287724 -4.4287734 -4.4287734 -4.4287763 -4.4287791 -4.4287806 -4.4287748 -4.4287624 -4.4287477 -4.4287472][-4.4287748 -4.4287586 -4.4287534 -4.4287529 -4.4287529 -4.4287486 -4.428741 -4.4287333 -4.4287314 -4.4287324 -4.4287324 -4.4287276 -4.4287171 -4.4287043 -4.4287109][-4.428823 -4.4288077 -4.4288044 -4.4288044 -4.4288015 -4.4287906 -4.4287739 -4.4287558 -4.4287443 -4.4287395 -4.4287357 -4.4287319 -4.4287267 -4.4287195 -4.4287314][-4.4288554 -4.4288421 -4.428843 -4.4288507 -4.428854 -4.428844 -4.4288259 -4.4288054 -4.4287906 -4.4287806 -4.4287715 -4.4287667 -4.4287624 -4.4287572 -4.4287686][-4.4288449 -4.4288273 -4.4288273 -4.42884 -4.4288526 -4.4288487 -4.4288344 -4.4288192 -4.4288144 -4.428813 -4.4288087 -4.4288049 -4.4288 -4.42879 -4.4287949][-4.428802 -4.4287744 -4.4287677 -4.428782 -4.4287992 -4.4287977 -4.4287844 -4.4287777 -4.428792 -4.4288116 -4.4288254 -4.4288321 -4.4288306 -4.4288173 -4.428813][-4.4287553 -4.4287195 -4.4287047 -4.4287148 -4.4287238 -4.42871 -4.4286861 -4.4286809 -4.4287119 -4.4287562 -4.4287963 -4.4288268 -4.42884 -4.4288321 -4.4288249][-4.4287381 -4.4287038 -4.4286876 -4.4286895 -4.4286814 -4.4286466 -4.4286051 -4.4285913 -4.4286275 -4.4286876 -4.4287467 -4.4287977 -4.4288268 -4.42883 -4.4288282][-4.4287453 -4.4287267 -4.4287195 -4.428721 -4.4287071 -4.4286666 -4.428617 -4.4285908 -4.4286141 -4.4286642 -4.4287186 -4.428772 -4.4288063 -4.4288163 -4.4288216][-4.428771 -4.4287639 -4.4287643 -4.4287663 -4.4287567 -4.4287291 -4.4286919 -4.42867 -4.4286795 -4.4287095 -4.4287434 -4.4287791 -4.4288 -4.4288039 -4.4288077][-4.4288106 -4.4288044 -4.4288006 -4.4287953 -4.4287848 -4.4287667 -4.4287443 -4.4287338 -4.4287419 -4.4287629 -4.4287848 -4.4288049 -4.4288096 -4.4288006 -4.4287977][-4.4288421 -4.4288278 -4.4288154 -4.4288025 -4.4287877 -4.4287729 -4.4287586 -4.4287558 -4.4287648 -4.428781 -4.4287972 -4.4288082 -4.4288039 -4.4287882 -4.4287815][-4.428843 -4.4288244 -4.4288092 -4.4287977 -4.4287872 -4.4287777 -4.4287667 -4.4287629 -4.4287667 -4.4287758 -4.4287868 -4.4287934 -4.4287844 -4.4287663 -4.428761][-4.4288249 -4.4288058 -4.4287944 -4.428791 -4.42879 -4.42879 -4.4287853 -4.428782 -4.4287796 -4.4287791 -4.4287834 -4.4287834 -4.4287696 -4.428751 -4.4287486][-4.4287992 -4.4287825 -4.4287724 -4.4287739 -4.4287786 -4.4287839 -4.4287863 -4.4287858 -4.428782 -4.4287777 -4.4287777 -4.4287739 -4.4287586 -4.4287391 -4.4287362]]...]
INFO - root - 2017-12-10 05:16:16.615764: step 710, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:29m:47s remains)
INFO - root - 2017-12-10 05:16:19.261061: step 720, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:52m:06s remains)
INFO - root - 2017-12-10 05:16:21.912654: step 730, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:42m:24s remains)
INFO - root - 2017-12-10 05:16:24.646815: step 740, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:48m:25s remains)
INFO - root - 2017-12-10 05:16:27.289784: step 750, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.276 sec/batch; 25h:28m:37s remains)
INFO - root - 2017-12-10 05:16:29.850857: step 760, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 24h:06m:00s remains)
INFO - root - 2017-12-10 05:16:32.480999: step 770, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:32m:05s remains)
INFO - root - 2017-12-10 05:16:35.106795: step 780, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:33m:50s remains)
INFO - root - 2017-12-10 05:16:37.755477: step 790, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:32m:01s remains)
INFO - root - 2017-12-10 05:16:40.329953: step 800, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:03m:16s remains)
2017-12-10 05:16:40.628456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288254 -4.4288177 -4.4288063 -4.4287663 -4.4287286 -4.4287157 -4.4287171 -4.4287362 -4.4287596 -4.4287763 -4.4287858 -4.4287705 -4.4287314 -4.4286847 -4.4286618][-4.4287872 -4.42877 -4.4287553 -4.4287128 -4.4286733 -4.428658 -4.4286609 -4.4286833 -4.4287086 -4.4287372 -4.4287539 -4.4287496 -4.4287214 -4.4286909 -4.4286871][-4.4287515 -4.4287357 -4.428721 -4.4286838 -4.4286432 -4.4286222 -4.4286284 -4.4286523 -4.4286761 -4.4287081 -4.4287257 -4.4287357 -4.4287257 -4.4287133 -4.4287224][-4.4287329 -4.4287148 -4.4286966 -4.4286704 -4.428637 -4.4286208 -4.42863 -4.428647 -4.4286613 -4.4286871 -4.4287033 -4.42872 -4.4287214 -4.4287176 -4.4287271][-4.42872 -4.4286919 -4.42867 -4.4286633 -4.4286504 -4.4286385 -4.4286351 -4.428637 -4.4286437 -4.42867 -4.4286857 -4.428699 -4.4286976 -4.4287019 -4.4287148][-4.4287186 -4.4286838 -4.4286633 -4.4286718 -4.4286766 -4.4286695 -4.4286442 -4.4286227 -4.4286208 -4.42864 -4.4286537 -4.4286575 -4.4286485 -4.4286642 -4.4286776][-4.4287362 -4.4286914 -4.428668 -4.4286695 -4.42867 -4.4286509 -4.4285941 -4.4285307 -4.4285054 -4.4285288 -4.4285583 -4.4285712 -4.428566 -4.4285917 -4.4286151][-4.4287319 -4.4286771 -4.4286461 -4.4286284 -4.4286127 -4.4285636 -4.4284506 -4.4283085 -4.428236 -4.4282818 -4.4283571 -4.428412 -4.428453 -4.4285221 -4.4285789][-4.4287052 -4.4286609 -4.4286313 -4.4285936 -4.4285502 -4.4284739 -4.4283257 -4.4281311 -4.4280396 -4.4281325 -4.4282603 -4.4283619 -4.428452 -4.4285474 -4.4286132][-4.4286618 -4.4286494 -4.428637 -4.4286084 -4.4285822 -4.4285445 -4.4284582 -4.4283376 -4.4282813 -4.4283404 -4.4284248 -4.4285049 -4.4285827 -4.4286551 -4.4286947][-4.4285746 -4.4285984 -4.4286242 -4.4286342 -4.4286528 -4.4286671 -4.4286451 -4.4285874 -4.4285517 -4.4285741 -4.4286065 -4.4286485 -4.4287019 -4.4287472 -4.4287667][-4.4284797 -4.4285011 -4.4285502 -4.4286027 -4.4286661 -4.4287257 -4.4287553 -4.4287467 -4.4287348 -4.4287457 -4.4287539 -4.4287672 -4.4287982 -4.4288206 -4.4288239][-4.428422 -4.4284239 -4.4284763 -4.4285574 -4.4286566 -4.4287448 -4.4288011 -4.42882 -4.4288259 -4.4288321 -4.4288273 -4.4288254 -4.4288397 -4.4288483 -4.4288483][-4.4284697 -4.4284697 -4.4285126 -4.4285874 -4.428679 -4.428762 -4.4288182 -4.4288459 -4.4288568 -4.4288616 -4.4288549 -4.4288454 -4.428854 -4.42886 -4.4288654][-4.4286108 -4.4286203 -4.4286485 -4.4286933 -4.4287448 -4.4287972 -4.4288373 -4.4288592 -4.4288721 -4.4288764 -4.4288683 -4.4288564 -4.4288578 -4.4288597 -4.4288654]]...]
INFO - root - 2017-12-10 05:16:43.311914: step 810, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.282 sec/batch; 25h:57m:50s remains)
INFO - root - 2017-12-10 05:16:45.937026: step 820, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:48m:06s remains)
INFO - root - 2017-12-10 05:16:48.582135: step 830, loss = 2.28, batch loss = 2.23 (27.4 examples/sec; 0.292 sec/batch; 26h:56m:30s remains)
INFO - root - 2017-12-10 05:16:51.238118: step 840, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:51m:08s remains)
INFO - root - 2017-12-10 05:16:53.874481: step 850, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:27m:44s remains)
INFO - root - 2017-12-10 05:16:56.503300: step 860, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:59m:05s remains)
INFO - root - 2017-12-10 05:16:59.170634: step 870, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:46m:44s remains)
INFO - root - 2017-12-10 05:17:01.814464: step 880, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:28m:54s remains)
INFO - root - 2017-12-10 05:17:04.466967: step 890, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:48m:30s remains)
INFO - root - 2017-12-10 05:17:07.096266: step 900, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 25h:13m:54s remains)
2017-12-10 05:17:07.427388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428751 -4.4287004 -4.4286404 -4.4285879 -4.4285655 -4.4285574 -4.4285932 -4.4286504 -4.4286551 -4.4286637 -4.4286723 -4.4286757 -4.4286613 -4.4286203 -4.4285827][-4.4286919 -4.4286556 -4.4286222 -4.4285774 -4.4285526 -4.4285321 -4.4285622 -4.4286213 -4.4286361 -4.4286652 -4.4286771 -4.4286685 -4.4286475 -4.4286094 -4.4285803][-4.4286189 -4.4286242 -4.4286418 -4.4286294 -4.4285908 -4.4285369 -4.4285421 -4.4285955 -4.4286227 -4.4286628 -4.4286776 -4.4286585 -4.4286203 -4.4285827 -4.4285603][-4.428493 -4.4285522 -4.4286427 -4.4286747 -4.4286342 -4.4285622 -4.428544 -4.4285865 -4.4286175 -4.4286542 -4.4286757 -4.4286528 -4.428606 -4.4285645 -4.428534][-4.4283814 -4.4284754 -4.4286184 -4.4286942 -4.4286733 -4.4286022 -4.428555 -4.4285712 -4.4285889 -4.4286208 -4.4286485 -4.4286366 -4.4286127 -4.4285722 -4.4285383][-4.4284797 -4.428544 -4.4286551 -4.42873 -4.4287176 -4.4286332 -4.4285502 -4.4285164 -4.4285154 -4.4285603 -4.428607 -4.428617 -4.4286256 -4.4285889 -4.428555][-4.4286342 -4.4286575 -4.4287066 -4.4287415 -4.4287114 -4.4286017 -4.4284945 -4.428421 -4.4284 -4.428484 -4.4285693 -4.4286103 -4.4286323 -4.4285989 -4.4285617][-4.4287515 -4.4287553 -4.4287543 -4.4287457 -4.4286776 -4.428524 -4.4283872 -4.4282846 -4.4282551 -4.4284053 -4.4285312 -4.4285865 -4.4286051 -4.4285727 -4.4285235][-4.4288206 -4.4288015 -4.428761 -4.4287167 -4.428618 -4.428441 -4.4282804 -4.4281564 -4.428144 -4.4283633 -4.4285259 -4.4285855 -4.4285994 -4.4285531 -4.4284811][-4.4288378 -4.4288058 -4.4287553 -4.4286995 -4.4286084 -4.4284678 -4.4283552 -4.4282775 -4.4282942 -4.4284863 -4.4286118 -4.4286537 -4.4286623 -4.4286003 -4.4285021][-4.4288378 -4.4288182 -4.4287939 -4.4287529 -4.4286871 -4.4286013 -4.4285426 -4.4285 -4.4285197 -4.4286442 -4.4287057 -4.4287066 -4.4287109 -4.4286709 -4.4285874][-4.428833 -4.4288316 -4.4288349 -4.428823 -4.4287882 -4.4287405 -4.4287114 -4.4286866 -4.4286919 -4.428751 -4.4287686 -4.4287434 -4.4287419 -4.4287157 -4.4286437][-4.4288292 -4.4288268 -4.4288359 -4.4288454 -4.428834 -4.4288111 -4.428802 -4.428793 -4.4287949 -4.4288173 -4.42881 -4.4287691 -4.4287558 -4.4287286 -4.4286575][-4.4288168 -4.4288063 -4.4288068 -4.4288206 -4.4288239 -4.4288182 -4.4288235 -4.4288292 -4.428834 -4.4288492 -4.4288378 -4.4287944 -4.4287744 -4.4287539 -4.4286923][-4.4288192 -4.4288034 -4.4287982 -4.4288049 -4.428812 -4.4288177 -4.4288325 -4.4288464 -4.4288616 -4.4288769 -4.4288692 -4.4288311 -4.4288106 -4.4287925 -4.4287338]]...]
INFO - root - 2017-12-10 05:17:10.077043: step 910, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:43m:24s remains)
INFO - root - 2017-12-10 05:17:12.727051: step 920, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:48m:10s remains)
INFO - root - 2017-12-10 05:17:15.389465: step 930, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:02m:41s remains)
INFO - root - 2017-12-10 05:17:18.048638: step 940, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:34m:09s remains)
INFO - root - 2017-12-10 05:17:20.708653: step 950, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 26h:12m:26s remains)
INFO - root - 2017-12-10 05:17:23.351914: step 960, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 25h:25m:31s remains)
INFO - root - 2017-12-10 05:17:26.016869: step 970, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 26h:38m:25s remains)
INFO - root - 2017-12-10 05:17:28.669630: step 980, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:18m:46s remains)
INFO - root - 2017-12-10 05:17:31.266028: step 990, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:40m:13s remains)
INFO - root - 2017-12-10 05:17:33.869280: step 1000, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:25m:34s remains)
2017-12-10 05:17:34.161266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286284 -4.4286294 -4.4286375 -4.4286585 -4.4286947 -4.428721 -4.4287128 -4.4286823 -4.4286785 -4.4287009 -4.4287214 -4.4287472 -4.4287825 -4.4288287 -4.4288707][-4.428616 -4.428627 -4.4286437 -4.4286666 -4.4287 -4.4287195 -4.4287057 -4.4286633 -4.4286551 -4.428689 -4.4287252 -4.4287572 -4.428793 -4.4288387 -4.4288759][-4.4286184 -4.4286389 -4.428668 -4.4286942 -4.428719 -4.42872 -4.42869 -4.4286366 -4.4286242 -4.4286642 -4.42871 -4.4287486 -4.4287848 -4.4288316 -4.4288712][-4.4286361 -4.4286628 -4.4286938 -4.4287181 -4.4287267 -4.4286995 -4.4286475 -4.4285822 -4.4285727 -4.428627 -4.4286857 -4.4287305 -4.42877 -4.4288197 -4.4288659][-4.4287038 -4.4287224 -4.4287391 -4.4287467 -4.42873 -4.4286766 -4.4285965 -4.4285088 -4.4285011 -4.4285808 -4.4286652 -4.4287219 -4.4287696 -4.428822 -4.4288716][-4.4287653 -4.4287686 -4.42877 -4.4287615 -4.4287224 -4.4286451 -4.428535 -4.4284182 -4.4284124 -4.4285293 -4.4286475 -4.4287219 -4.4287786 -4.4288344 -4.4288836][-4.4287777 -4.4287639 -4.4287562 -4.42874 -4.4286895 -4.4285984 -4.4284687 -4.4283314 -4.4283366 -4.4284897 -4.4286394 -4.4287305 -4.4287963 -4.4288521 -4.4288988][-4.4287758 -4.4287534 -4.428741 -4.4287248 -4.4286776 -4.428597 -4.4284825 -4.4283628 -4.4283781 -4.4285264 -4.428668 -4.4287543 -4.4288173 -4.4288664 -4.42891][-4.4287705 -4.4287434 -4.4287262 -4.4287124 -4.4286828 -4.4286356 -4.4285674 -4.428493 -4.428504 -4.4286036 -4.4287052 -4.4287705 -4.4288206 -4.428865 -4.4289107][-4.4287539 -4.4287243 -4.4287019 -4.4286909 -4.4286823 -4.428668 -4.4286427 -4.4286036 -4.4286032 -4.428658 -4.4287224 -4.4287686 -4.4288092 -4.428854 -4.428906][-4.4287429 -4.4287105 -4.4286861 -4.4286819 -4.4286895 -4.4286976 -4.4286995 -4.4286809 -4.4286714 -4.428699 -4.4287424 -4.4287763 -4.4288139 -4.42886 -4.4289122][-4.4287372 -4.4287057 -4.4286847 -4.4286852 -4.4287004 -4.4287171 -4.42873 -4.4287205 -4.4287095 -4.4287286 -4.4287653 -4.4287963 -4.4288359 -4.42888 -4.4289274][-4.4287519 -4.4287267 -4.4287095 -4.4287114 -4.4287271 -4.4287419 -4.4287519 -4.4287391 -4.4287286 -4.4287529 -4.4287939 -4.4288282 -4.4288692 -4.4289088 -4.4289474][-4.4287896 -4.4287739 -4.4287629 -4.4287653 -4.4287767 -4.4287834 -4.4287829 -4.4287643 -4.4287558 -4.4287872 -4.4288335 -4.4288712 -4.4289079 -4.4289374 -4.4289651][-4.4288359 -4.4288278 -4.4288225 -4.4288244 -4.4288297 -4.42883 -4.4288259 -4.4288106 -4.4288063 -4.4288354 -4.4288778 -4.4289122 -4.4289384 -4.4289565 -4.4289756]]...]
INFO - root - 2017-12-10 05:17:36.809249: step 1010, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:30m:08s remains)
INFO - root - 2017-12-10 05:17:39.481049: step 1020, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:26m:39s remains)
INFO - root - 2017-12-10 05:17:42.135210: step 1030, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:35m:28s remains)
INFO - root - 2017-12-10 05:17:44.799399: step 1040, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:34m:19s remains)
INFO - root - 2017-12-10 05:17:47.461707: step 1050, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:04m:29s remains)
INFO - root - 2017-12-10 05:17:50.027967: step 1060, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:48m:32s remains)
INFO - root - 2017-12-10 05:17:52.661992: step 1070, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:52m:34s remains)
INFO - root - 2017-12-10 05:17:55.262567: step 1080, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:21m:19s remains)
INFO - root - 2017-12-10 05:17:57.886987: step 1090, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:59m:34s remains)
INFO - root - 2017-12-10 05:18:00.541172: step 1100, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:31m:38s remains)
2017-12-10 05:18:00.834166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287186 -4.4286284 -4.4285479 -4.4285355 -4.4285674 -4.4286256 -4.4286885 -4.4287515 -4.4287972 -4.4288311 -4.4288497 -4.428864 -4.4288855 -4.4289007 -4.4289093][-4.4287543 -4.428679 -4.4285955 -4.4285398 -4.4285064 -4.4285159 -4.4285755 -4.4286652 -4.4287443 -4.4288049 -4.4288416 -4.4288592 -4.4288788 -4.4288974 -4.4289107][-4.4287782 -4.4287109 -4.428627 -4.428534 -4.4284396 -4.4284024 -4.4284678 -4.428587 -4.4286976 -4.4287825 -4.4288387 -4.4288607 -4.428874 -4.4288883 -4.4289026][-4.4287868 -4.4287343 -4.4286661 -4.4285736 -4.4284639 -4.4283977 -4.428442 -4.4285588 -4.42868 -4.4287753 -4.4288383 -4.428863 -4.4288731 -4.4288812 -4.4288936][-4.4287834 -4.4287491 -4.4286933 -4.4286203 -4.4285307 -4.4284654 -4.4284863 -4.4285688 -4.4286723 -4.428761 -4.4288206 -4.428853 -4.4288669 -4.4288759 -4.4288878][-4.4287562 -4.4287348 -4.4286842 -4.4286232 -4.4285512 -4.4284892 -4.428484 -4.4285283 -4.4286141 -4.4286995 -4.4287677 -4.4288225 -4.4288545 -4.4288735 -4.4288912][-4.4287362 -4.4287176 -4.4286675 -4.428607 -4.4285412 -4.428473 -4.4284339 -4.4284339 -4.4284997 -4.4285922 -4.4286795 -4.4287648 -4.4288254 -4.428864 -4.4288917][-4.4287634 -4.4287434 -4.4286957 -4.4286337 -4.4285684 -4.428494 -4.4284315 -4.4283991 -4.42844 -4.4285326 -4.4286265 -4.4287243 -4.4288044 -4.4288616 -4.4289007][-4.4288225 -4.4288044 -4.428762 -4.4287004 -4.4286304 -4.428555 -4.4284925 -4.4284563 -4.4284806 -4.4285612 -4.4286475 -4.4287386 -4.4288197 -4.4288831 -4.4289279][-4.428863 -4.4288521 -4.42882 -4.4287643 -4.428688 -4.4286075 -4.4285507 -4.4285216 -4.4285359 -4.4286 -4.4286795 -4.4287663 -4.428844 -4.4289055 -4.4289494][-4.4288592 -4.42886 -4.4288473 -4.4288063 -4.4287348 -4.4286556 -4.4285975 -4.4285679 -4.4285684 -4.428607 -4.428678 -4.4287667 -4.4288449 -4.428906 -4.4289465][-4.4288483 -4.4288588 -4.4288635 -4.4288416 -4.4287896 -4.4287248 -4.4286675 -4.4286313 -4.4286122 -4.4286227 -4.4286747 -4.4287548 -4.4288316 -4.4288921 -4.4289327][-4.42887 -4.4288788 -4.4288874 -4.4288788 -4.42885 -4.4288077 -4.428761 -4.4287252 -4.4286942 -4.4286833 -4.4287105 -4.4287682 -4.428833 -4.4288878 -4.4289255][-4.4289041 -4.4289026 -4.4289026 -4.4288945 -4.42888 -4.4288616 -4.428834 -4.4288063 -4.4287724 -4.4287467 -4.4287524 -4.4287887 -4.42884 -4.4288845 -4.4289184][-4.4289155 -4.4289026 -4.4288921 -4.42888 -4.428874 -4.4288731 -4.4288626 -4.428844 -4.4288111 -4.4287786 -4.42877 -4.4287934 -4.4288363 -4.4288721 -4.4289017]]...]
INFO - root - 2017-12-10 05:18:03.470813: step 1110, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:18m:04s remains)
INFO - root - 2017-12-10 05:18:06.111746: step 1120, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:57m:39s remains)
INFO - root - 2017-12-10 05:18:08.772881: step 1130, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:01m:59s remains)
INFO - root - 2017-12-10 05:18:11.417026: step 1140, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:16m:59s remains)
INFO - root - 2017-12-10 05:18:14.113323: step 1150, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:34m:31s remains)
INFO - root - 2017-12-10 05:18:16.743805: step 1160, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:14m:42s remains)
INFO - root - 2017-12-10 05:18:19.392236: step 1170, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:06m:15s remains)
INFO - root - 2017-12-10 05:18:22.128333: step 1180, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:27m:33s remains)
INFO - root - 2017-12-10 05:18:24.839049: step 1190, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:34m:36s remains)
INFO - root - 2017-12-10 05:18:27.495475: step 1200, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:17m:04s remains)
2017-12-10 05:18:27.783665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288797 -4.4288812 -4.4288874 -4.428905 -4.42891 -4.4289036 -4.4289069 -4.4289179 -4.4289455 -4.4289646 -4.4289718 -4.4289808 -4.4289865 -4.4289851 -4.428977][-4.4288254 -4.4288197 -4.428823 -4.4288568 -4.4288859 -4.4288917 -4.4289021 -4.42892 -4.42896 -4.428978 -4.4289737 -4.4289727 -4.4289732 -4.428967 -4.4289541][-4.4287605 -4.4287496 -4.4287515 -4.4288054 -4.4288611 -4.428885 -4.4288917 -4.4289021 -4.4289408 -4.4289613 -4.4289522 -4.4289451 -4.4289484 -4.4289465 -4.4289341][-4.4287148 -4.4287033 -4.4287071 -4.4287677 -4.4288292 -4.4288497 -4.42884 -4.42884 -4.4288783 -4.4289122 -4.4289241 -4.4289303 -4.4289427 -4.4289455 -4.4289284][-4.4286952 -4.428688 -4.4286933 -4.4287472 -4.4287953 -4.42879 -4.4287505 -4.42873 -4.4287724 -4.4288335 -4.4288859 -4.4289188 -4.4289465 -4.428956 -4.4289384][-4.42869 -4.4286876 -4.4286947 -4.4287281 -4.4287457 -4.4287033 -4.4286208 -4.4285536 -4.4285941 -4.4287071 -4.4288116 -4.428875 -4.4289231 -4.42895 -4.4289427][-4.42869 -4.4286976 -4.42871 -4.4287143 -4.4286866 -4.4286036 -4.4284649 -4.4283142 -4.4283328 -4.4285221 -4.4286966 -4.428802 -4.4288788 -4.4289193 -4.4289269][-4.4286995 -4.4287086 -4.4287148 -4.428699 -4.4286413 -4.4285269 -4.4283509 -4.4281278 -4.4281154 -4.4283772 -4.4286184 -4.4287634 -4.4288568 -4.4289007 -4.4289088][-4.4287033 -4.4287066 -4.4287081 -4.4286895 -4.4286342 -4.4285336 -4.4284005 -4.4282303 -4.4282155 -4.42843 -4.4286418 -4.4287782 -4.428864 -4.4288969 -4.4289041][-4.4287167 -4.4287176 -4.4287267 -4.4287229 -4.4286923 -4.4286332 -4.4285789 -4.428504 -4.4284992 -4.4286108 -4.4287362 -4.4288282 -4.428884 -4.4289064 -4.4289141][-4.4287624 -4.4287591 -4.4287763 -4.4287863 -4.4287782 -4.4287534 -4.4287462 -4.4287357 -4.4287462 -4.4287944 -4.4288507 -4.4288979 -4.4289227 -4.4289289 -4.428937][-4.4288392 -4.4288292 -4.4288397 -4.4288454 -4.4288421 -4.4288373 -4.4288559 -4.4288774 -4.428895 -4.4289107 -4.4289279 -4.4289465 -4.4289474 -4.4289393 -4.4289422][-4.4289036 -4.4288845 -4.4288845 -4.428885 -4.4288812 -4.4288783 -4.4288969 -4.4289207 -4.4289274 -4.42892 -4.4289179 -4.4289231 -4.4289169 -4.4289088 -4.4289103][-4.4289274 -4.4288993 -4.428894 -4.428896 -4.4288921 -4.4288855 -4.4288912 -4.4289017 -4.4288983 -4.4288783 -4.4288659 -4.4288659 -4.4288616 -4.428865 -4.428875][-4.4289274 -4.4288988 -4.4288931 -4.4288955 -4.4288936 -4.4288888 -4.4288836 -4.4288831 -4.4288759 -4.4288549 -4.4288368 -4.4288321 -4.4288278 -4.4288406 -4.4288616]]...]
INFO - root - 2017-12-10 05:18:30.378572: step 1210, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:37m:16s remains)
INFO - root - 2017-12-10 05:18:33.082352: step 1220, loss = 2.28, batch loss = 2.23 (26.9 examples/sec; 0.297 sec/batch; 27h:21m:30s remains)
INFO - root - 2017-12-10 05:18:35.722752: step 1230, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:17m:59s remains)
INFO - root - 2017-12-10 05:18:38.448042: step 1240, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 25h:00m:07s remains)
INFO - root - 2017-12-10 05:18:41.104876: step 1250, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:12m:09s remains)
INFO - root - 2017-12-10 05:18:43.752451: step 1260, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:27m:26s remains)
INFO - root - 2017-12-10 05:18:46.402246: step 1270, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:46m:37s remains)
INFO - root - 2017-12-10 05:18:49.076135: step 1280, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:26m:48s remains)
INFO - root - 2017-12-10 05:18:51.725723: step 1290, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:07m:12s remains)
INFO - root - 2017-12-10 05:18:54.355520: step 1300, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:34m:49s remains)
2017-12-10 05:18:54.637939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287643 -4.4287786 -4.4287939 -4.4287996 -4.4287934 -4.4287705 -4.4286871 -4.4285564 -4.4285121 -4.4285684 -4.4286466 -4.4287257 -4.428802 -4.428863 -4.4288921][-4.4287109 -4.4287229 -4.4287372 -4.4287438 -4.428751 -4.4287438 -4.4286685 -4.4285388 -4.4284973 -4.4285545 -4.4286294 -4.4287086 -4.4287877 -4.4288549 -4.4288864][-4.4286652 -4.428669 -4.428678 -4.4286876 -4.4287128 -4.4287257 -4.4286604 -4.4285355 -4.4285 -4.4285631 -4.4286356 -4.4287109 -4.4287877 -4.4288545 -4.4288845][-4.4286318 -4.4286246 -4.4286265 -4.4286361 -4.4286771 -4.4287066 -4.428647 -4.428524 -4.4284983 -4.4285741 -4.4286494 -4.4287224 -4.4287968 -4.4288611 -4.4288869][-4.42861 -4.4285917 -4.428587 -4.4285927 -4.4286375 -4.4286733 -4.4286103 -4.4284821 -4.4284692 -4.428566 -4.4286523 -4.42873 -4.4288077 -4.4288716 -4.4288936][-4.4286156 -4.4285836 -4.4285655 -4.4285541 -4.4285855 -4.4286141 -4.4285364 -4.4283981 -4.4284043 -4.4285326 -4.4286385 -4.42873 -4.4288182 -4.428884 -4.4289036][-4.4286685 -4.4286261 -4.4285927 -4.4285569 -4.4285631 -4.4285703 -4.4284649 -4.428309 -4.4283342 -4.4284911 -4.4286146 -4.4287219 -4.428822 -4.4288926 -4.4289117][-4.4287372 -4.4286995 -4.4286618 -4.4286122 -4.4286 -4.4285851 -4.4284563 -4.4282889 -4.4283218 -4.4284849 -4.4286075 -4.4287176 -4.4288239 -4.4288964 -4.4289155][-4.4288058 -4.4287758 -4.4287372 -4.4286857 -4.4286675 -4.4286404 -4.4285111 -4.4283586 -4.4283938 -4.4285369 -4.4286389 -4.4287357 -4.4288335 -4.4289007 -4.4289169][-4.4288511 -4.4288282 -4.4287925 -4.4287491 -4.4287329 -4.4287014 -4.4285851 -4.4284592 -4.4284954 -4.4286156 -4.4286976 -4.4287767 -4.4288568 -4.4289122 -4.4289212][-4.4288735 -4.4288592 -4.4288311 -4.4287968 -4.4287844 -4.4287534 -4.4286551 -4.428555 -4.4285908 -4.4286923 -4.428761 -4.4288273 -4.4288898 -4.4289308 -4.4289303][-4.4288816 -4.4288735 -4.4288511 -4.4288287 -4.428822 -4.4287939 -4.4287138 -4.4286342 -4.4286685 -4.4287577 -4.4288173 -4.4288731 -4.42892 -4.4289489 -4.4289408][-4.4288874 -4.4288816 -4.4288645 -4.428853 -4.4288487 -4.4288216 -4.42876 -4.4287014 -4.4287367 -4.4288154 -4.4288683 -4.428915 -4.4289484 -4.4289656 -4.4289503][-4.4288883 -4.4288836 -4.4288731 -4.4288692 -4.4288635 -4.4288397 -4.4287982 -4.4287581 -4.428791 -4.4288592 -4.4289069 -4.428946 -4.4289703 -4.4289775 -4.4289575][-4.4288936 -4.4288878 -4.4288821 -4.4288812 -4.4288774 -4.4288621 -4.4288368 -4.4288077 -4.4288311 -4.4288864 -4.4289265 -4.4289603 -4.42898 -4.4289818 -4.42896]]...]
INFO - root - 2017-12-10 05:18:57.290238: step 1310, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:07m:49s remains)
INFO - root - 2017-12-10 05:18:59.924597: step 1320, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:39m:59s remains)
INFO - root - 2017-12-10 05:19:02.599904: step 1330, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 25h:01m:14s remains)
INFO - root - 2017-12-10 05:19:05.257778: step 1340, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:20m:56s remains)
INFO - root - 2017-12-10 05:19:07.912185: step 1350, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:51m:31s remains)
INFO - root - 2017-12-10 05:19:10.620119: step 1360, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:21m:06s remains)
INFO - root - 2017-12-10 05:19:13.277413: step 1370, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:59m:48s remains)
INFO - root - 2017-12-10 05:19:15.913249: step 1380, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:57m:34s remains)
INFO - root - 2017-12-10 05:19:18.519019: step 1390, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:23m:52s remains)
INFO - root - 2017-12-10 05:19:21.177861: step 1400, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:52m:24s remains)
2017-12-10 05:19:21.454997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288416 -4.428833 -4.4288282 -4.4288225 -4.428822 -4.428822 -4.4288096 -4.42878 -4.4287496 -4.4287329 -4.4287295 -4.42874 -4.4287534 -4.4287748 -4.4287992][-4.4288287 -4.4288282 -4.4288273 -4.4288225 -4.428823 -4.4288259 -4.4288144 -4.4287834 -4.4287515 -4.4287333 -4.4287271 -4.4287319 -4.428741 -4.4287643 -4.4287949][-4.4288216 -4.4288268 -4.4288287 -4.4288249 -4.4288263 -4.4288335 -4.4288287 -4.4288082 -4.4287858 -4.4287744 -4.4287677 -4.4287672 -4.4287696 -4.4287863 -4.42881][-4.4288173 -4.4288197 -4.4288206 -4.4288182 -4.4288192 -4.4288273 -4.42883 -4.4288235 -4.4288154 -4.428813 -4.4288044 -4.4287934 -4.4287882 -4.4287992 -4.4288182][-4.4288025 -4.4287972 -4.4287887 -4.4287782 -4.428771 -4.4287744 -4.4287796 -4.4287868 -4.428803 -4.4288211 -4.4288239 -4.4288096 -4.428793 -4.428793 -4.4288077][-4.4287691 -4.4287515 -4.4287314 -4.4287066 -4.4286819 -4.4286656 -4.4286656 -4.4286838 -4.4287276 -4.4287763 -4.4288073 -4.4288111 -4.4287972 -4.428792 -4.4287977][-4.4287271 -4.4286914 -4.4286575 -4.42861 -4.4285569 -4.4285178 -4.4285135 -4.4285436 -4.428616 -4.4286976 -4.4287586 -4.4287863 -4.4287863 -4.4287834 -4.4287834][-4.4287205 -4.4286604 -4.4286051 -4.4285288 -4.4284339 -4.4283643 -4.4283576 -4.4284096 -4.4285216 -4.4286442 -4.4287338 -4.4287806 -4.4287939 -4.4287915 -4.4287839][-4.4287443 -4.4286666 -4.4285946 -4.4285 -4.4283767 -4.42828 -4.4282737 -4.4283504 -4.4284997 -4.4286532 -4.4287653 -4.4288263 -4.4288487 -4.4288468 -4.428833][-4.4287891 -4.4287109 -4.4286404 -4.4285502 -4.4284306 -4.4283314 -4.4283276 -4.4284062 -4.4285593 -4.4287052 -4.42881 -4.4288688 -4.4288945 -4.4288988 -4.4288874][-4.4288511 -4.4287863 -4.4287252 -4.4286509 -4.4285583 -4.428484 -4.4284892 -4.4285526 -4.428669 -4.4287691 -4.4288321 -4.4288669 -4.4288831 -4.4288898 -4.4288883][-4.4289074 -4.4288592 -4.4288073 -4.428751 -4.4286904 -4.4286394 -4.4286389 -4.4286704 -4.4287348 -4.4287834 -4.428813 -4.4288297 -4.4288406 -4.42885 -4.4288592][-4.4289346 -4.4289103 -4.4288754 -4.4288373 -4.4287972 -4.4287567 -4.4287381 -4.4287314 -4.4287472 -4.4287572 -4.4287663 -4.4287825 -4.4288044 -4.4288239 -4.4288425][-4.4289246 -4.4289274 -4.4289169 -4.4288964 -4.4288692 -4.4288278 -4.4287882 -4.4287562 -4.42874 -4.4287333 -4.42874 -4.4287653 -4.4288058 -4.4288425 -4.4288654][-4.428885 -4.4289074 -4.4289203 -4.4289222 -4.4289079 -4.4288707 -4.4288297 -4.4287949 -4.4287667 -4.4287481 -4.4287548 -4.4287858 -4.4288349 -4.4288807 -4.4289021]]...]
INFO - root - 2017-12-10 05:19:24.121186: step 1410, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:39m:35s remains)
INFO - root - 2017-12-10 05:19:26.790795: step 1420, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:45m:31s remains)
INFO - root - 2017-12-10 05:19:29.459798: step 1430, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:14m:33s remains)
INFO - root - 2017-12-10 05:19:32.055206: step 1440, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:07m:46s remains)
INFO - root - 2017-12-10 05:19:34.697619: step 1450, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:32m:11s remains)
INFO - root - 2017-12-10 05:19:37.331482: step 1460, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:18m:00s remains)
INFO - root - 2017-12-10 05:19:39.977818: step 1470, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:45m:00s remains)
INFO - root - 2017-12-10 05:19:42.623611: step 1480, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:28m:59s remains)
INFO - root - 2017-12-10 05:19:45.285154: step 1490, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:35m:02s remains)
INFO - root - 2017-12-10 05:19:47.978343: step 1500, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:42m:47s remains)
2017-12-10 05:19:48.324211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42895 -4.4289551 -4.4289446 -4.4289222 -4.4289017 -4.4288692 -4.42883 -4.4287939 -4.4287744 -4.42877 -4.4287782 -4.4287891 -4.4287896 -4.4287767 -4.4287872][-4.4289432 -4.4289546 -4.4289432 -4.4289141 -4.4288926 -4.428863 -4.4288349 -4.42881 -4.4287987 -4.428792 -4.4287939 -4.4287958 -4.4287853 -4.4287658 -4.4287767][-4.4289284 -4.4289379 -4.4289279 -4.4289 -4.4288869 -4.4288707 -4.4288573 -4.4288416 -4.4288316 -4.4288173 -4.4288087 -4.4288011 -4.4287786 -4.4287572 -4.4287767][-4.4289136 -4.4289188 -4.4289136 -4.4288917 -4.428885 -4.4288778 -4.4288731 -4.4288545 -4.428833 -4.4288092 -4.4288034 -4.4287944 -4.4287643 -4.4287491 -4.4287806][-4.4289246 -4.4289236 -4.4289136 -4.4288907 -4.4288807 -4.4288654 -4.4288468 -4.4288039 -4.4287558 -4.42873 -4.4287539 -4.4287615 -4.4287391 -4.4287391 -4.4287796][-4.428956 -4.4289474 -4.4289279 -4.428896 -4.4288697 -4.42883 -4.4287796 -4.4286919 -4.4285989 -4.4285769 -4.4286504 -4.4286971 -4.4287043 -4.428731 -4.4287796][-4.4289551 -4.4289412 -4.4289274 -4.4289007 -4.4288645 -4.4288044 -4.4287214 -4.4285836 -4.4284391 -4.428421 -4.428546 -4.4286366 -4.4286866 -4.4287467 -4.4287963][-4.4289045 -4.4289 -4.4289131 -4.4289107 -4.4288836 -4.4288249 -4.4287343 -4.4285765 -4.4284134 -4.4284048 -4.4285502 -4.4286518 -4.4287186 -4.42879 -4.4288244][-4.4288607 -4.4288735 -4.428916 -4.4289351 -4.42892 -4.428875 -4.4288054 -4.428669 -4.4285336 -4.4285388 -4.4286618 -4.4287348 -4.4287853 -4.4288416 -4.4288507][-4.4288535 -4.4288778 -4.4289331 -4.4289565 -4.4289432 -4.4289083 -4.42886 -4.4287553 -4.4286537 -4.4286675 -4.4287581 -4.4288034 -4.42884 -4.4288797 -4.4288707][-4.4288778 -4.4288974 -4.4289465 -4.4289627 -4.4289436 -4.4289136 -4.4288812 -4.4288063 -4.4287381 -4.4287515 -4.4288 -4.4288206 -4.4288516 -4.4288812 -4.4288678][-4.4289145 -4.4289174 -4.4289503 -4.4289589 -4.4289393 -4.4289169 -4.4288969 -4.4288478 -4.4288015 -4.4288068 -4.4288144 -4.4288125 -4.4288344 -4.4288535 -4.4288459][-4.4289432 -4.4289331 -4.4289494 -4.4289517 -4.428937 -4.4289222 -4.4289117 -4.4288793 -4.4288464 -4.4288421 -4.428823 -4.42881 -4.4288244 -4.4288349 -4.4288354][-4.4289479 -4.4289293 -4.4289289 -4.4289227 -4.4289112 -4.428905 -4.4289041 -4.4288878 -4.4288669 -4.4288578 -4.4288306 -4.4288259 -4.4288468 -4.4288564 -4.4288611][-4.4289374 -4.4289174 -4.4289021 -4.4288893 -4.4288821 -4.4288845 -4.4288936 -4.4288921 -4.4288797 -4.4288692 -4.4288473 -4.4288645 -4.4289 -4.4289122 -4.4289193]]...]
INFO - root - 2017-12-10 05:19:50.944956: step 1510, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:23m:01s remains)
INFO - root - 2017-12-10 05:19:53.565061: step 1520, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.253 sec/batch; 23h:13m:55s remains)
INFO - root - 2017-12-10 05:19:56.187877: step 1530, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:05m:31s remains)
INFO - root - 2017-12-10 05:19:58.899851: step 1540, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:25m:46s remains)
INFO - root - 2017-12-10 05:20:01.533695: step 1550, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:24m:52s remains)
INFO - root - 2017-12-10 05:20:04.212157: step 1560, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:21m:01s remains)
INFO - root - 2017-12-10 05:20:06.863759: step 1570, loss = 2.28, batch loss = 2.23 (28.5 examples/sec; 0.280 sec/batch; 25h:45m:52s remains)
INFO - root - 2017-12-10 05:20:09.519739: step 1580, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:06m:49s remains)
INFO - root - 2017-12-10 05:20:12.171856: step 1590, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:28m:44s remains)
INFO - root - 2017-12-10 05:20:14.822418: step 1600, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:50m:02s remains)
2017-12-10 05:20:15.112945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286823 -4.4286714 -4.4286361 -4.4285994 -4.428576 -4.4285717 -4.4285731 -4.4285789 -4.4285979 -4.428637 -4.4286709 -4.4286761 -4.4286618 -4.4286537 -4.428628][-4.4286737 -4.4286609 -4.4286203 -4.42858 -4.428545 -4.4285231 -4.4285131 -4.4285154 -4.4285407 -4.4285955 -4.4286456 -4.428658 -4.4286451 -4.4286318 -4.4285965][-4.4286423 -4.4286432 -4.4286141 -4.4285812 -4.4285383 -4.4284964 -4.4284668 -4.4284558 -4.4284763 -4.4285288 -4.4285741 -4.4286036 -4.4286156 -4.4286103 -4.4285803][-4.4285808 -4.4286056 -4.4286008 -4.4285884 -4.4285569 -4.4285197 -4.4284816 -4.4284534 -4.4284558 -4.4284811 -4.4285021 -4.4285369 -4.4285746 -4.4285884 -4.4285669][-4.4285283 -4.4285636 -4.4285836 -4.4285932 -4.428575 -4.4285479 -4.4285192 -4.4284892 -4.428473 -4.4284716 -4.4284678 -4.4284983 -4.4285479 -4.4285703 -4.4285541][-4.4285078 -4.4285445 -4.4285679 -4.4285827 -4.4285626 -4.4285312 -4.4285021 -4.4284816 -4.428462 -4.4284525 -4.4284506 -4.4284873 -4.4285393 -4.4285579 -4.4285407][-4.4285145 -4.4285545 -4.4285774 -4.4285851 -4.428555 -4.4285131 -4.4284673 -4.4284229 -4.428371 -4.4283438 -4.4283676 -4.4284358 -4.4284983 -4.4285216 -4.4285107][-4.4285522 -4.4285913 -4.4286084 -4.428606 -4.4285684 -4.4285226 -4.428462 -4.42838 -4.4282765 -4.4282117 -4.4282613 -4.4283748 -4.4284577 -4.4284925 -4.4284954][-4.4285841 -4.4286284 -4.4286418 -4.4286284 -4.4285936 -4.4285669 -4.4285312 -4.4284616 -4.4283504 -4.428268 -4.4283061 -4.4284134 -4.4284954 -4.4285374 -4.42855][-4.4286132 -4.4286661 -4.4286914 -4.4286852 -4.4286604 -4.4286547 -4.428647 -4.4286132 -4.4285426 -4.4284863 -4.428503 -4.428566 -4.4286175 -4.42865 -4.4286704][-4.4286723 -4.4287252 -4.4287558 -4.4287591 -4.4287424 -4.4287424 -4.4287453 -4.4287319 -4.4286971 -4.4286728 -4.428688 -4.4287262 -4.4287524 -4.4287691 -4.4287863][-4.4287443 -4.4287872 -4.4288139 -4.4288259 -4.4288158 -4.4288144 -4.4288163 -4.4288082 -4.42879 -4.4287825 -4.4287963 -4.4288235 -4.4288392 -4.42885 -4.4288616][-4.4288082 -4.4288373 -4.4288559 -4.4288721 -4.4288754 -4.4288778 -4.4288807 -4.4288774 -4.4288678 -4.4288635 -4.4288654 -4.4288778 -4.42889 -4.4289002 -4.4289012][-4.4288483 -4.4288669 -4.42888 -4.4288983 -4.4289093 -4.4289174 -4.4289236 -4.428926 -4.4289231 -4.4289141 -4.4289026 -4.4288926 -4.4288864 -4.4288855 -4.4288816][-4.428844 -4.428843 -4.4288425 -4.4288521 -4.4288635 -4.4288778 -4.428894 -4.4289045 -4.4289045 -4.4288912 -4.4288621 -4.4288321 -4.428813 -4.4288054 -4.4288063]]...]
INFO - root - 2017-12-10 05:20:17.735215: step 1610, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:29m:23s remains)
INFO - root - 2017-12-10 05:20:20.374177: step 1620, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:28m:52s remains)
INFO - root - 2017-12-10 05:20:23.056681: step 1630, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:28m:28s remains)
INFO - root - 2017-12-10 05:20:25.730818: step 1640, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:27m:54s remains)
INFO - root - 2017-12-10 05:20:28.372278: step 1650, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:29m:25s remains)
INFO - root - 2017-12-10 05:20:31.008098: step 1660, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 26h:43m:28s remains)
INFO - root - 2017-12-10 05:20:33.633373: step 1670, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:41m:54s remains)
INFO - root - 2017-12-10 05:20:36.257527: step 1680, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:09m:15s remains)
INFO - root - 2017-12-10 05:20:38.916356: step 1690, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:13m:57s remains)
INFO - root - 2017-12-10 05:20:41.559126: step 1700, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:15m:55s remains)
2017-12-10 05:20:41.854501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285188 -4.4283595 -4.4283519 -4.4284935 -4.4286637 -4.4287882 -4.4288559 -4.428875 -4.4288697 -4.4288487 -4.4288459 -4.428865 -4.4288821 -4.4288621 -4.4288087][-4.4284415 -4.4282632 -4.4282742 -4.4284515 -4.4286447 -4.4287744 -4.4288335 -4.4288397 -4.428823 -4.4288077 -4.428813 -4.4288416 -4.4288712 -4.4288535 -4.4287939][-4.4284759 -4.4283094 -4.4283314 -4.4285045 -4.4286838 -4.4287839 -4.4287982 -4.4287663 -4.4287353 -4.4287429 -4.4287748 -4.4288268 -4.4288688 -4.4288535 -4.4287906][-4.4285827 -4.4284782 -4.428515 -4.428648 -4.4287739 -4.4288158 -4.4287558 -4.4286523 -4.4286036 -4.4286466 -4.4287167 -4.4287963 -4.4288521 -4.42884 -4.4287763][-4.4286985 -4.4286623 -4.4287019 -4.4287715 -4.4288211 -4.42879 -4.4286366 -4.4284511 -4.4283938 -4.4284925 -4.4286194 -4.4287333 -4.4288077 -4.4287972 -4.4287381][-4.4287896 -4.4287848 -4.4288 -4.428803 -4.4287758 -4.4286556 -4.4283943 -4.4281216 -4.4280815 -4.4282775 -4.4284825 -4.4286351 -4.4287295 -4.4287276 -4.4286833][-4.428843 -4.4288411 -4.4288306 -4.4287934 -4.42871 -4.4285159 -4.4281669 -4.4278378 -4.427846 -4.4281254 -4.4283838 -4.4285593 -4.4286623 -4.4286728 -4.4286456][-4.42888 -4.4288697 -4.4288397 -4.4287844 -4.4286895 -4.4284992 -4.4281878 -4.4279327 -4.4279737 -4.4282074 -4.4284205 -4.428565 -4.4286509 -4.4286571 -4.4286313][-4.4288983 -4.4288764 -4.4288392 -4.4287906 -4.4287138 -4.4285731 -4.4283686 -4.4282303 -4.42827 -4.4284134 -4.4285522 -4.4286447 -4.4286876 -4.4286733 -4.4286461][-4.4288988 -4.4288778 -4.428844 -4.4288073 -4.4287553 -4.4286628 -4.4285421 -4.4284735 -4.4285088 -4.4285979 -4.4286895 -4.428741 -4.4287505 -4.4287224 -4.4286966][-4.4288955 -4.4288836 -4.4288611 -4.4288354 -4.4288077 -4.4287567 -4.4286885 -4.4286571 -4.4286871 -4.4287405 -4.4287944 -4.4288206 -4.4288206 -4.4287915 -4.4287653][-4.4289002 -4.4288936 -4.4288788 -4.4288654 -4.4288559 -4.4288363 -4.4288049 -4.4287958 -4.4288177 -4.4288425 -4.4288664 -4.428875 -4.4288678 -4.4288411 -4.4288154][-4.4289083 -4.4288993 -4.428896 -4.4288983 -4.4289012 -4.4288979 -4.4288869 -4.4288845 -4.428895 -4.4289017 -4.4289026 -4.4288964 -4.4288878 -4.4288697 -4.4288492][-4.4289246 -4.4289103 -4.4289122 -4.4289222 -4.4289331 -4.4289379 -4.428937 -4.4289341 -4.42893 -4.4289246 -4.4289169 -4.4289079 -4.4289041 -4.428895 -4.4288816][-4.428936 -4.4289241 -4.4289284 -4.4289384 -4.4289484 -4.4289532 -4.4289527 -4.4289484 -4.4289393 -4.4289308 -4.4289241 -4.4289203 -4.4289212 -4.4289193 -4.4289136]]...]
INFO - root - 2017-12-10 05:20:44.511898: step 1710, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:39m:49s remains)
INFO - root - 2017-12-10 05:20:47.192451: step 1720, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:58m:42s remains)
INFO - root - 2017-12-10 05:20:49.825335: step 1730, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:04m:29s remains)
INFO - root - 2017-12-10 05:20:52.498960: step 1740, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:38m:30s remains)
INFO - root - 2017-12-10 05:20:55.132062: step 1750, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:41m:26s remains)
INFO - root - 2017-12-10 05:20:57.800158: step 1760, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:26m:35s remains)
INFO - root - 2017-12-10 05:21:00.418326: step 1770, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:39m:32s remains)
INFO - root - 2017-12-10 05:21:03.052266: step 1780, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:59m:50s remains)
INFO - root - 2017-12-10 05:21:05.671359: step 1790, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:40m:40s remains)
INFO - root - 2017-12-10 05:21:08.331130: step 1800, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:05m:50s remains)
2017-12-10 05:21:08.630634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289336 -4.4289055 -4.4288459 -4.4287534 -4.4286823 -4.4286427 -4.4286318 -4.4286618 -4.4287138 -4.4287338 -4.4286871 -4.4286313 -4.428647 -4.4287128 -4.42878][-4.4289389 -4.4289136 -4.428854 -4.4287634 -4.4286895 -4.4286361 -4.4286137 -4.4286461 -4.4287109 -4.4287367 -4.4286861 -4.4286251 -4.4286413 -4.4287171 -4.4287944][-4.4289403 -4.4289145 -4.428854 -4.4287663 -4.4286857 -4.4286203 -4.4285917 -4.4286385 -4.4287181 -4.4287448 -4.4286838 -4.4286137 -4.4286408 -4.4287281 -4.4288125][-4.4289365 -4.4289074 -4.4288464 -4.4287591 -4.42867 -4.4285951 -4.4285688 -4.4286261 -4.4287167 -4.4287291 -4.428658 -4.4285994 -4.4286547 -4.4287515 -4.4288268][-4.4289351 -4.4289069 -4.4288478 -4.4287562 -4.428658 -4.4285789 -4.4285517 -4.4286122 -4.4286971 -4.4286866 -4.4286075 -4.4285936 -4.4286885 -4.4287906 -4.4288454][-4.4289384 -4.4289155 -4.4288573 -4.4287558 -4.4286475 -4.4285626 -4.4285293 -4.4285841 -4.4286556 -4.4286151 -4.4285474 -4.4285975 -4.4287205 -4.4288149 -4.4288597][-4.4289422 -4.42892 -4.428864 -4.4287558 -4.4286408 -4.4285522 -4.4285145 -4.4285569 -4.4285975 -4.4285192 -4.4284835 -4.4286036 -4.4287424 -4.4288249 -4.4288645][-4.428946 -4.4289193 -4.428864 -4.428761 -4.4286489 -4.4285588 -4.42852 -4.4285469 -4.4285564 -4.4284706 -4.4284921 -4.4286523 -4.4287872 -4.4288526 -4.42888][-4.4289446 -4.4289122 -4.4288592 -4.4287682 -4.4286671 -4.4285808 -4.4285445 -4.4285631 -4.4285579 -4.4285049 -4.4285707 -4.4287291 -4.4288464 -4.4288936 -4.4289017][-4.4289393 -4.4289026 -4.4288511 -4.4287729 -4.4286871 -4.4286146 -4.4285827 -4.4285884 -4.4285693 -4.4285631 -4.42866 -4.4288068 -4.4289017 -4.4289351 -4.428925][-4.4289308 -4.4288926 -4.4288459 -4.4287844 -4.4287143 -4.4286556 -4.4286189 -4.4286 -4.4285769 -4.428617 -4.4287353 -4.42887 -4.4289503 -4.4289742 -4.4289484][-4.4289265 -4.4288869 -4.4288445 -4.4287958 -4.4287395 -4.4286828 -4.4286304 -4.4285936 -4.4285922 -4.4286709 -4.4287949 -4.4289141 -4.428978 -4.4289851 -4.4289455][-4.4289284 -4.4288926 -4.428853 -4.4288087 -4.428751 -4.4286771 -4.428607 -4.4285831 -4.4286284 -4.4287333 -4.4288507 -4.4289474 -4.428987 -4.42897 -4.4289212][-4.4289355 -4.4289074 -4.4288673 -4.428813 -4.4287343 -4.4286289 -4.4285522 -4.4285655 -4.4286594 -4.4287848 -4.4288936 -4.4289703 -4.4289823 -4.4289427 -4.4288893][-4.4289441 -4.4289246 -4.4288807 -4.4288034 -4.42869 -4.4285526 -4.4284878 -4.4285531 -4.4286919 -4.4288259 -4.4289165 -4.4289703 -4.4289579 -4.4289088 -4.4288583]]...]
INFO - root - 2017-12-10 05:21:11.279585: step 1810, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:47m:42s remains)
INFO - root - 2017-12-10 05:21:13.917175: step 1820, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:41m:37s remains)
INFO - root - 2017-12-10 05:21:16.540563: step 1830, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:19m:37s remains)
INFO - root - 2017-12-10 05:21:19.207466: step 1840, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:08m:41s remains)
INFO - root - 2017-12-10 05:21:21.860775: step 1850, loss = 2.28, batch loss = 2.23 (28.2 examples/sec; 0.284 sec/batch; 26h:04m:17s remains)
INFO - root - 2017-12-10 05:21:24.481698: step 1860, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:04m:27s remains)
INFO - root - 2017-12-10 05:21:27.121558: step 1870, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 25h:19m:25s remains)
INFO - root - 2017-12-10 05:21:29.769281: step 1880, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:28m:19s remains)
INFO - root - 2017-12-10 05:21:32.358741: step 1890, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:29m:19s remains)
INFO - root - 2017-12-10 05:21:35.002049: step 1900, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:25m:31s remains)
2017-12-10 05:21:35.287225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287424 -4.4287114 -4.4287295 -4.4287724 -4.4288135 -4.4288445 -4.4288421 -4.4288459 -4.4288554 -4.4288445 -4.4288383 -4.428833 -4.4288111 -4.4288063 -4.4288239][-4.4287248 -4.4286566 -4.428647 -4.4286957 -4.4287682 -4.428813 -4.4287963 -4.4287906 -4.42881 -4.4288049 -4.4287949 -4.4287915 -4.4287763 -4.4287705 -4.4287839][-4.4287047 -4.4286051 -4.42857 -4.4286222 -4.4287219 -4.4287791 -4.4287567 -4.4287429 -4.4287624 -4.428762 -4.4287577 -4.42877 -4.4287677 -4.4287639 -4.4287624][-4.4286842 -4.4285793 -4.4285274 -4.4285626 -4.4286618 -4.4287291 -4.4287205 -4.4287057 -4.4287205 -4.4287114 -4.4287157 -4.428751 -4.4287696 -4.4287677 -4.4287562][-4.42866 -4.4285755 -4.428525 -4.4285355 -4.4286084 -4.4286718 -4.4286847 -4.4286866 -4.4287138 -4.4287066 -4.4287152 -4.428761 -4.4287844 -4.4287782 -4.4287562][-4.4286408 -4.4285836 -4.4285431 -4.4285312 -4.428575 -4.4286175 -4.4286408 -4.4286728 -4.4287229 -4.4287267 -4.4287367 -4.4287763 -4.4287877 -4.4287772 -4.4287548][-4.4286346 -4.4285975 -4.4285622 -4.4285245 -4.4285293 -4.4285412 -4.428566 -4.4286242 -4.4286971 -4.42871 -4.4287138 -4.4287376 -4.4287391 -4.4287357 -4.4287219][-4.4286337 -4.4286051 -4.4285731 -4.4285245 -4.4284983 -4.4284787 -4.4284887 -4.428545 -4.4286289 -4.4286537 -4.4286609 -4.4286819 -4.4286895 -4.4286995 -4.428699][-4.4286394 -4.4286056 -4.428575 -4.4285355 -4.4285011 -4.4284639 -4.4284468 -4.4284744 -4.4285421 -4.4285779 -4.4285994 -4.4286361 -4.4286575 -4.4286776 -4.4286838][-4.4286318 -4.4285932 -4.4285607 -4.4285312 -4.4285069 -4.4284706 -4.4284415 -4.4284477 -4.4284959 -4.4285345 -4.42857 -4.4286265 -4.4286604 -4.428678 -4.4286766][-4.4286323 -4.4285941 -4.4285631 -4.4285388 -4.4285226 -4.4284973 -4.4284706 -4.4284678 -4.4285016 -4.4285336 -4.4285669 -4.4286366 -4.4286833 -4.4287024 -4.4286962][-4.4286895 -4.4286594 -4.4286327 -4.4286103 -4.428596 -4.4285808 -4.4285631 -4.4285569 -4.4285693 -4.4285793 -4.4285975 -4.4286637 -4.42872 -4.4287434 -4.4287329][-4.428772 -4.4287539 -4.4287333 -4.4287138 -4.4287014 -4.4286938 -4.4286833 -4.4286757 -4.4286752 -4.428668 -4.4286609 -4.4287 -4.4287429 -4.4287667 -4.4287624][-4.4288688 -4.4288583 -4.4288445 -4.42883 -4.4288197 -4.428813 -4.4288073 -4.428802 -4.4287949 -4.428782 -4.4287629 -4.4287763 -4.4287987 -4.4288144 -4.4288168][-4.4289479 -4.42894 -4.4289317 -4.4289227 -4.4289141 -4.4289064 -4.4289 -4.428894 -4.4288878 -4.4288769 -4.4288588 -4.4288588 -4.4288669 -4.4288745 -4.4288774]]...]
INFO - root - 2017-12-10 05:21:37.999978: step 1910, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:38m:18s remains)
INFO - root - 2017-12-10 05:21:40.655682: step 1920, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:58m:16s remains)
INFO - root - 2017-12-10 05:21:43.305628: step 1930, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:45m:03s remains)
INFO - root - 2017-12-10 05:21:45.939493: step 1940, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:48m:50s remains)
INFO - root - 2017-12-10 05:21:48.590735: step 1950, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:39m:53s remains)
INFO - root - 2017-12-10 05:21:51.209378: step 1960, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:04m:31s remains)
INFO - root - 2017-12-10 05:21:53.879462: step 1970, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:55m:50s remains)
INFO - root - 2017-12-10 05:21:56.492715: step 1980, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:38m:58s remains)
INFO - root - 2017-12-10 05:21:59.135479: step 1990, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:48m:25s remains)
INFO - root - 2017-12-10 05:22:01.732663: step 2000, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:35m:53s remains)
2017-12-10 05:22:02.053404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290047 -4.4289985 -4.4289947 -4.4289927 -4.4289927 -4.4289966 -4.4290004 -4.429 -4.428998 -4.428998 -4.4289989 -4.4290023 -4.4290075 -4.4290109 -4.429009][-4.4290009 -4.4289875 -4.4289775 -4.4289689 -4.4289665 -4.4289703 -4.4289727 -4.4289684 -4.4289641 -4.4289637 -4.4289675 -4.4289789 -4.4289932 -4.4290037 -4.4290028][-4.4289832 -4.428956 -4.4289336 -4.4289145 -4.4289064 -4.4289117 -4.428915 -4.4289088 -4.4289021 -4.4288945 -4.4288907 -4.4289021 -4.4289322 -4.4289641 -4.4289784][-4.4289508 -4.4289093 -4.4288759 -4.428844 -4.42883 -4.4288325 -4.428834 -4.428823 -4.4288054 -4.4287796 -4.4287562 -4.4287663 -4.4288177 -4.4288821 -4.428926][-4.4289122 -4.4288626 -4.4288211 -4.4287806 -4.4287581 -4.4287519 -4.4287457 -4.4287171 -4.428678 -4.4286294 -4.4285927 -4.428607 -4.4286838 -4.4287829 -4.4288588][-4.4288764 -4.4288182 -4.4287634 -4.42871 -4.4286804 -4.428669 -4.4286475 -4.4285951 -4.4285326 -4.4284678 -4.4284358 -4.4284711 -4.4285741 -4.4287014 -4.4288044][-4.4288292 -4.4287543 -4.4286723 -4.4285994 -4.4285684 -4.4285555 -4.4285188 -4.4284573 -4.4284167 -4.4283819 -4.4283824 -4.4284439 -4.4285579 -4.4286876 -4.4287944][-4.4287682 -4.4286633 -4.4285336 -4.4284339 -4.4284067 -4.42841 -4.4283781 -4.4283319 -4.4283485 -4.4283714 -4.4284105 -4.4284873 -4.4285979 -4.4287176 -4.4288149][-4.4286761 -4.4285369 -4.4283624 -4.4282546 -4.4282575 -4.4283066 -4.4283071 -4.4282966 -4.4283595 -4.4284205 -4.4284825 -4.4285631 -4.4286656 -4.42877 -4.42885][-4.4286275 -4.4284711 -4.4282861 -4.4281907 -4.4282308 -4.4283261 -4.4283714 -4.4283967 -4.4284773 -4.4285479 -4.4286103 -4.4286785 -4.4287581 -4.4288335 -4.4288888][-4.4286795 -4.4285641 -4.428443 -4.4283814 -4.4284172 -4.4285035 -4.4285607 -4.4285984 -4.4286661 -4.4287219 -4.4287648 -4.4288092 -4.4288559 -4.4288964 -4.4289236][-4.4288011 -4.4287367 -4.4286833 -4.4286537 -4.4286628 -4.4287167 -4.4287653 -4.4287992 -4.428843 -4.42887 -4.4288869 -4.4289036 -4.4289236 -4.42894 -4.4289494][-4.4289184 -4.428884 -4.4288626 -4.4288497 -4.4288473 -4.4288726 -4.4289031 -4.4289265 -4.4289474 -4.4289541 -4.4289565 -4.4289584 -4.4289641 -4.4289689 -4.4289703][-4.4289918 -4.42898 -4.4289727 -4.4289618 -4.4289508 -4.4289546 -4.4289694 -4.4289808 -4.4289865 -4.4289885 -4.4289885 -4.4289861 -4.4289861 -4.4289856 -4.4289842][-4.4290104 -4.4290094 -4.4290056 -4.4289961 -4.4289846 -4.4289808 -4.4289856 -4.42899 -4.4289923 -4.4289942 -4.4289956 -4.4289951 -4.4289942 -4.4289923 -4.4289904]]...]
INFO - root - 2017-12-10 05:22:04.715680: step 2010, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:26m:50s remains)
INFO - root - 2017-12-10 05:22:07.389700: step 2020, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:39m:55s remains)
INFO - root - 2017-12-10 05:22:10.044339: step 2030, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:40m:04s remains)
INFO - root - 2017-12-10 05:22:12.690452: step 2040, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:02m:41s remains)
INFO - root - 2017-12-10 05:22:15.345611: step 2050, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:38m:55s remains)
INFO - root - 2017-12-10 05:22:18.023445: step 2060, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:38m:35s remains)
INFO - root - 2017-12-10 05:22:20.674067: step 2070, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:14m:00s remains)
INFO - root - 2017-12-10 05:22:23.359751: step 2080, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:37m:37s remains)
INFO - root - 2017-12-10 05:22:26.029479: step 2090, loss = 2.28, batch loss = 2.23 (27.9 examples/sec; 0.287 sec/batch; 26h:18m:54s remains)
INFO - root - 2017-12-10 05:22:28.695068: step 2100, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 24h:01m:13s remains)
2017-12-10 05:22:28.990392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286947 -4.4286833 -4.4287028 -4.4287333 -4.4287677 -4.4287744 -4.4287648 -4.4287648 -4.4287848 -4.4288096 -4.4288373 -4.4288611 -4.4288669 -4.4288764 -4.4288917][-4.4287243 -4.4287071 -4.4287324 -4.4287643 -4.4287877 -4.4287863 -4.4287748 -4.4287796 -4.4288054 -4.4288359 -4.4288578 -4.42887 -4.4288664 -4.4288745 -4.4288874][-4.4287739 -4.4287343 -4.4287448 -4.428771 -4.4287839 -4.428771 -4.4287548 -4.42876 -4.428782 -4.4288182 -4.4288435 -4.4288578 -4.4288588 -4.4288669 -4.4288731][-4.4287982 -4.4287467 -4.4287391 -4.4287496 -4.4287486 -4.4287195 -4.4286909 -4.4286933 -4.4287114 -4.4287567 -4.4288025 -4.4288287 -4.4288425 -4.4288559 -4.4288645][-4.4287968 -4.4287496 -4.4287376 -4.4287338 -4.4287143 -4.4286566 -4.4285908 -4.4285655 -4.4285808 -4.4286489 -4.4287295 -4.428791 -4.4288311 -4.4288607 -4.4288793][-4.4288049 -4.4287777 -4.4287729 -4.4287591 -4.4287205 -4.4286361 -4.4285188 -4.4284348 -4.4284368 -4.4285288 -4.428647 -4.4287491 -4.4288278 -4.4288797 -4.4289083][-4.4288306 -4.4288278 -4.4288263 -4.4288106 -4.4287739 -4.4286847 -4.42854 -4.4284048 -4.428369 -4.4284587 -4.42859 -4.42871 -4.4288116 -4.4288797 -4.4289165][-4.4288344 -4.4288507 -4.4288478 -4.4288244 -4.4287972 -4.4287262 -4.4285913 -4.4284439 -4.4283876 -4.4284606 -4.42858 -4.4286933 -4.4287934 -4.4288621 -4.4288974][-4.4288135 -4.4288344 -4.4288306 -4.4287958 -4.4287715 -4.428719 -4.4286141 -4.428483 -4.4284291 -4.4284897 -4.4285941 -4.4286947 -4.4287806 -4.4288397 -4.4288664][-4.4287992 -4.4288149 -4.4288039 -4.4287648 -4.4287405 -4.4286995 -4.4286175 -4.4285116 -4.4284735 -4.4285321 -4.4286213 -4.4287009 -4.4287634 -4.4288158 -4.4288416][-4.4288077 -4.4288144 -4.4287882 -4.4287477 -4.428731 -4.4287066 -4.4286475 -4.4285665 -4.4285378 -4.4285941 -4.4286695 -4.4287329 -4.4287682 -4.4288044 -4.4288292][-4.4288383 -4.4288392 -4.4288044 -4.4287639 -4.4287496 -4.4287386 -4.4287052 -4.4286518 -4.4286313 -4.4286761 -4.4287353 -4.4287729 -4.4287715 -4.4287715 -4.4287815][-4.4288588 -4.4288654 -4.4288397 -4.4288073 -4.4287915 -4.4287839 -4.4287658 -4.4287353 -4.4287252 -4.4287581 -4.428803 -4.4288163 -4.4287744 -4.4287214 -4.4287][-4.4288511 -4.4288521 -4.4288359 -4.4288263 -4.4288206 -4.4288197 -4.4288173 -4.4288077 -4.4288077 -4.4288311 -4.4288588 -4.4288468 -4.4287825 -4.4286938 -4.4286385][-4.4288254 -4.4288187 -4.4288068 -4.4288106 -4.4288168 -4.4288244 -4.4288321 -4.4288411 -4.4288583 -4.4288855 -4.4289002 -4.4288726 -4.4287934 -4.428688 -4.428617]]...]
INFO - root - 2017-12-10 05:22:31.605630: step 2110, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:12m:30s remains)
INFO - root - 2017-12-10 05:22:34.231019: step 2120, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:20m:35s remains)
INFO - root - 2017-12-10 05:22:36.886620: step 2130, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 25h:15m:48s remains)
INFO - root - 2017-12-10 05:22:39.487650: step 2140, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:12m:56s remains)
INFO - root - 2017-12-10 05:22:42.133043: step 2150, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:37m:41s remains)
INFO - root - 2017-12-10 05:22:44.753911: step 2160, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:42m:27s remains)
INFO - root - 2017-12-10 05:22:47.393366: step 2170, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 24h:00m:39s remains)
INFO - root - 2017-12-10 05:22:50.030396: step 2180, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:39m:11s remains)
INFO - root - 2017-12-10 05:22:52.629261: step 2190, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:19m:40s remains)
INFO - root - 2017-12-10 05:22:55.267747: step 2200, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:59m:28s remains)
2017-12-10 05:22:55.543741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289074 -4.4289141 -4.4289403 -4.428947 -4.4289088 -4.4288239 -4.4287047 -4.4286232 -4.4286094 -4.4286714 -4.4287343 -4.4287825 -4.428822 -4.4288459 -4.4288716][-4.4289026 -4.4289055 -4.42893 -4.4289379 -4.4288936 -4.4288025 -4.4286833 -4.4286017 -4.4285984 -4.4286633 -4.4287386 -4.4288006 -4.4288521 -4.4288845 -4.4289083][-4.4288945 -4.4289074 -4.4289336 -4.4289403 -4.4288845 -4.4287829 -4.4286542 -4.4285674 -4.4285765 -4.4286509 -4.4287467 -4.4288268 -4.4288869 -4.4289279 -4.4289522][-4.4288883 -4.4289126 -4.4289403 -4.4289384 -4.42887 -4.4287519 -4.428597 -4.4284935 -4.4285154 -4.428616 -4.4287462 -4.4288449 -4.4289122 -4.4289556 -4.4289727][-4.4288898 -4.4289227 -4.4289494 -4.4289336 -4.4288497 -4.4287086 -4.428524 -4.4284034 -4.4284396 -4.4285812 -4.4287462 -4.4288626 -4.428936 -4.4289746 -4.4289789][-4.4288926 -4.4289346 -4.4289613 -4.4289293 -4.4288273 -4.428658 -4.428443 -4.4283032 -4.4283552 -4.4285483 -4.4287477 -4.4288807 -4.4289594 -4.4289908 -4.4289832][-4.4288945 -4.4289412 -4.42897 -4.4289331 -4.4288211 -4.42863 -4.4283819 -4.4282045 -4.4282513 -4.4284844 -4.4287171 -4.428875 -4.4289694 -4.4289985 -4.4289827][-4.4289188 -4.4289608 -4.4289837 -4.4289441 -4.428834 -4.4286413 -4.4283824 -4.4281659 -4.42818 -4.4284191 -4.4286761 -4.4288588 -4.4289684 -4.4289994 -4.4289818][-4.4289632 -4.4289961 -4.4290061 -4.4289637 -4.428863 -4.4286904 -4.4284468 -4.4282184 -4.4281945 -4.4284024 -4.4286561 -4.4288497 -4.4289651 -4.4289975 -4.4289823][-4.428997 -4.4290228 -4.429028 -4.4289923 -4.4289093 -4.4287667 -4.4285564 -4.428339 -4.4282761 -4.4284296 -4.4286585 -4.4288445 -4.4289575 -4.4289918 -4.4289823][-4.4290066 -4.4290276 -4.4290342 -4.4290109 -4.42895 -4.428843 -4.4286828 -4.4284978 -4.4284058 -4.4284987 -4.4286857 -4.4288478 -4.42895 -4.4289846 -4.4289823][-4.4290032 -4.4290142 -4.42902 -4.4290071 -4.4289651 -4.4288917 -4.4287829 -4.4286432 -4.4285493 -4.428597 -4.4287362 -4.428863 -4.428946 -4.4289804 -4.4289837][-4.4290032 -4.4290004 -4.428998 -4.4289889 -4.4289589 -4.428906 -4.4288335 -4.4287415 -4.4286718 -4.4287004 -4.4287982 -4.4288912 -4.4289522 -4.4289832 -4.4289889][-4.4290028 -4.4289861 -4.4289751 -4.428966 -4.4289441 -4.4289041 -4.4288526 -4.4287953 -4.4287543 -4.4287791 -4.4288492 -4.4289122 -4.4289551 -4.4289808 -4.4289889][-4.4289985 -4.4289756 -4.4289579 -4.4289474 -4.4289303 -4.4289002 -4.4288659 -4.428834 -4.428813 -4.4288344 -4.4288836 -4.4289246 -4.4289536 -4.4289713 -4.4289794]]...]
INFO - root - 2017-12-10 05:22:58.228335: step 2210, loss = 2.28, batch loss = 2.23 (27.6 examples/sec; 0.290 sec/batch; 26h:37m:55s remains)
INFO - root - 2017-12-10 05:23:00.873245: step 2220, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:25m:44s remains)
INFO - root - 2017-12-10 05:23:03.524601: step 2230, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:13m:39s remains)
INFO - root - 2017-12-10 05:23:06.201987: step 2240, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:30m:28s remains)
INFO - root - 2017-12-10 05:23:08.843566: step 2250, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:40m:11s remains)
INFO - root - 2017-12-10 05:23:11.470286: step 2260, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:24m:43s remains)
INFO - root - 2017-12-10 05:23:14.085673: step 2270, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:42m:05s remains)
INFO - root - 2017-12-10 05:23:16.681014: step 2280, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:43m:59s remains)
INFO - root - 2017-12-10 05:23:19.318170: step 2290, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:27m:47s remains)
INFO - root - 2017-12-10 05:23:21.967429: step 2300, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:17m:39s remains)
2017-12-10 05:23:22.262462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288535 -4.4288378 -4.4288087 -4.4287658 -4.4287257 -4.4286866 -4.4286461 -4.42862 -4.4286103 -4.4286051 -4.4285831 -4.4285555 -4.4285789 -4.4286513 -4.4287286][-4.4288378 -4.428812 -4.4287772 -4.4287348 -4.4286876 -4.4286313 -4.4285765 -4.4285326 -4.4285021 -4.4285069 -4.4285336 -4.4285703 -4.4286284 -4.4287167 -4.428793][-4.428812 -4.4287705 -4.4287281 -4.4286857 -4.4286361 -4.4285717 -4.4285097 -4.4284544 -4.4284139 -4.4284439 -4.428524 -4.4286089 -4.42869 -4.4287839 -4.428853][-4.428791 -4.4287472 -4.4287004 -4.428658 -4.4286079 -4.4285359 -4.428462 -4.4284115 -4.4283957 -4.4284534 -4.4285583 -4.4286623 -4.4287462 -4.4288254 -4.4288721][-4.4287753 -4.4287324 -4.4286823 -4.4286351 -4.4285774 -4.4284954 -4.4284058 -4.4283667 -4.428411 -4.4285021 -4.4286118 -4.4287157 -4.4287982 -4.4288564 -4.4288816][-4.4287567 -4.4287243 -4.4286733 -4.4286113 -4.4285312 -4.42842 -4.4282851 -4.4282584 -4.4283929 -4.4285407 -4.4286566 -4.4287624 -4.4288425 -4.4288821 -4.4288878][-4.4287333 -4.4287181 -4.4286666 -4.4285808 -4.4284678 -4.4283161 -4.4281507 -4.4281635 -4.4283814 -4.4285731 -4.4286976 -4.4288015 -4.4288721 -4.4288955 -4.4288936][-4.4286871 -4.4286747 -4.4286313 -4.4285541 -4.4284539 -4.4283366 -4.4282365 -4.4282894 -4.42849 -4.4286532 -4.4287496 -4.4288273 -4.42888 -4.428896 -4.4288955][-4.4286017 -4.4286127 -4.4286189 -4.4285965 -4.428555 -4.42851 -4.4284863 -4.4285393 -4.4286547 -4.4287453 -4.428802 -4.4288464 -4.4288735 -4.4288855 -4.4288964][-4.4285674 -4.4286127 -4.4286523 -4.4286685 -4.4286671 -4.4286566 -4.4286628 -4.4286976 -4.4287519 -4.4288011 -4.4288387 -4.4288635 -4.4288745 -4.4288883 -4.4289083][-4.4285989 -4.4286475 -4.4286962 -4.4287252 -4.4287329 -4.4287257 -4.4287329 -4.4287577 -4.4287853 -4.4288168 -4.428853 -4.4288797 -4.4288931 -4.4289103 -4.4289331][-4.428659 -4.4286938 -4.4287291 -4.4287491 -4.4287481 -4.4287353 -4.4287443 -4.4287748 -4.4288006 -4.4288263 -4.4288597 -4.4288898 -4.4289117 -4.4289355 -4.428957][-4.4286995 -4.4287105 -4.4287219 -4.4287305 -4.428731 -4.4287314 -4.4287524 -4.4287896 -4.4288173 -4.4288387 -4.4288669 -4.4288974 -4.4289303 -4.4289551 -4.4289718][-4.4287043 -4.4286981 -4.4286957 -4.4287038 -4.4287229 -4.4287481 -4.4287815 -4.428813 -4.4288378 -4.4288583 -4.4288864 -4.428916 -4.4289451 -4.4289656 -4.4289761][-4.4287128 -4.4287124 -4.4287114 -4.4287276 -4.4287586 -4.4287944 -4.4288297 -4.4288554 -4.428875 -4.4288964 -4.4289184 -4.4289389 -4.4289608 -4.4289756 -4.4289808]]...]
INFO - root - 2017-12-10 05:23:24.896733: step 2310, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:35m:50s remains)
INFO - root - 2017-12-10 05:23:27.539209: step 2320, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.271 sec/batch; 24h:48m:48s remains)
INFO - root - 2017-12-10 05:23:30.208549: step 2330, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 26h:29m:47s remains)
INFO - root - 2017-12-10 05:23:32.838067: step 2340, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:25m:48s remains)
INFO - root - 2017-12-10 05:23:35.449938: step 2350, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:23m:11s remains)
INFO - root - 2017-12-10 05:23:38.122951: step 2360, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:27m:38s remains)
INFO - root - 2017-12-10 05:23:40.738549: step 2370, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:31m:41s remains)
INFO - root - 2017-12-10 05:23:43.329125: step 2380, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:06m:16s remains)
INFO - root - 2017-12-10 05:23:46.009855: step 2390, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:25m:29s remains)
INFO - root - 2017-12-10 05:23:48.652439: step 2400, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:07m:35s remains)
2017-12-10 05:23:48.961210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287 -4.4286928 -4.4287195 -4.4287415 -4.428751 -4.4287472 -4.4287415 -4.4287562 -4.4287758 -4.42879 -4.4287925 -4.428791 -4.4287796 -4.42878 -4.4288034][-4.4286427 -4.4286337 -4.42867 -4.4287033 -4.4287262 -4.4287376 -4.4287457 -4.4287691 -4.4287815 -4.4287868 -4.4287815 -4.4287791 -4.4287758 -4.4287858 -4.4288125][-4.4286656 -4.4286523 -4.4286838 -4.4287138 -4.4287438 -4.4287696 -4.4287944 -4.4288206 -4.4288206 -4.4288077 -4.4287858 -4.4287791 -4.4287786 -4.4287896 -4.4288092][-4.4287248 -4.4287105 -4.4287329 -4.4287586 -4.4287848 -4.4288092 -4.4288316 -4.428844 -4.4288306 -4.42881 -4.4287868 -4.4287839 -4.4287796 -4.4287791 -4.4287834][-4.42877 -4.4287648 -4.4287858 -4.4288063 -4.4288168 -4.4288177 -4.4288111 -4.4287934 -4.42877 -4.4287624 -4.4287677 -4.4287858 -4.4287844 -4.4287705 -4.4287496][-4.42877 -4.4287777 -4.4288011 -4.4288111 -4.4287972 -4.4287605 -4.4287081 -4.4286523 -4.4286318 -4.4286623 -4.428721 -4.4287777 -4.4287968 -4.4287815 -4.4287405][-4.4287186 -4.4287281 -4.4287543 -4.4287591 -4.4287186 -4.4286289 -4.4285154 -4.4284296 -4.4284434 -4.4285421 -4.4286685 -4.4287729 -4.428822 -4.4288163 -4.4287658][-4.4286656 -4.4286671 -4.428688 -4.4286852 -4.4286237 -4.4284916 -4.4283338 -4.4282455 -4.4283128 -4.4284682 -4.4286361 -4.428772 -4.4288449 -4.4288526 -4.4288073][-4.4286766 -4.4286728 -4.4286866 -4.42868 -4.4286184 -4.4284968 -4.4283657 -4.4283161 -4.4283919 -4.4285264 -4.4286652 -4.4287872 -4.428864 -4.428884 -4.4288521][-4.4287477 -4.4287386 -4.4287462 -4.4287424 -4.4287033 -4.4286327 -4.4285722 -4.4285636 -4.4286118 -4.4286766 -4.4287438 -4.4288187 -4.4288821 -4.4289069 -4.4288864][-4.4288177 -4.4288068 -4.4288073 -4.428803 -4.4287906 -4.428771 -4.4287658 -4.4287782 -4.4287944 -4.4288011 -4.4288092 -4.4288435 -4.4288878 -4.4289093 -4.4288979][-4.4288492 -4.428844 -4.4288445 -4.4288459 -4.4288507 -4.42886 -4.42888 -4.4288945 -4.4288855 -4.4288597 -4.4288359 -4.4288435 -4.4288688 -4.4288836 -4.4288807][-4.428844 -4.4288483 -4.42886 -4.4288745 -4.4288883 -4.4289041 -4.4289222 -4.4289203 -4.4288878 -4.428843 -4.4287996 -4.4287848 -4.4287891 -4.4288 -4.428812][-4.4288111 -4.4288263 -4.4288564 -4.4288831 -4.4288974 -4.4289045 -4.4289093 -4.4288898 -4.4288411 -4.4287844 -4.4287271 -4.4286923 -4.4286838 -4.4286995 -4.42873][-4.4287505 -4.4287696 -4.4288158 -4.4288516 -4.4288659 -4.428865 -4.4288573 -4.4288292 -4.428781 -4.4287343 -4.4286838 -4.4286456 -4.4286385 -4.4286633 -4.4287133]]...]
INFO - root - 2017-12-10 05:23:51.577898: step 2410, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:02m:49s remains)
INFO - root - 2017-12-10 05:23:54.279214: step 2420, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.275 sec/batch; 25h:15m:27s remains)
INFO - root - 2017-12-10 05:23:56.900526: step 2430, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:24m:04s remains)
INFO - root - 2017-12-10 05:23:59.554356: step 2440, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:31m:41s remains)
INFO - root - 2017-12-10 05:24:02.235337: step 2450, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:22m:45s remains)
INFO - root - 2017-12-10 05:24:04.838834: step 2460, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:28m:28s remains)
INFO - root - 2017-12-10 05:24:07.457485: step 2470, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:46m:40s remains)
INFO - root - 2017-12-10 05:24:10.113903: step 2480, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:46m:47s remains)
INFO - root - 2017-12-10 05:24:12.776437: step 2490, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:14m:25s remains)
INFO - root - 2017-12-10 05:24:15.423521: step 2500, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:57m:32s remains)
2017-12-10 05:24:15.697914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289188 -4.4288969 -4.4288797 -4.4288721 -4.4288774 -4.4288812 -4.4288783 -4.4288783 -4.428884 -4.4288869 -4.4288845 -4.4288778 -4.4288735 -4.4288807 -4.428895][-4.4289026 -4.4288697 -4.428844 -4.4288278 -4.4288297 -4.4288316 -4.4288235 -4.4288321 -4.428854 -4.4288716 -4.4288721 -4.4288588 -4.428844 -4.42884 -4.4288535][-4.4288621 -4.4288116 -4.4287739 -4.4287515 -4.4287515 -4.4287472 -4.4287343 -4.4287519 -4.4287906 -4.4288244 -4.4288335 -4.4288211 -4.4287987 -4.428782 -4.4287939][-4.428812 -4.4287434 -4.4286914 -4.428669 -4.4286613 -4.4286404 -4.4286156 -4.4286394 -4.4287038 -4.4287519 -4.42877 -4.4287658 -4.4287434 -4.428719 -4.4287262][-4.4287825 -4.4287043 -4.4286323 -4.4285984 -4.4285746 -4.4285321 -4.4284854 -4.4285059 -4.428597 -4.4286652 -4.4286947 -4.428699 -4.4286838 -4.4286666 -4.4286723][-4.428761 -4.428688 -4.4286 -4.428535 -4.4284873 -4.4284186 -4.428328 -4.4283113 -4.4284272 -4.4285412 -4.4286032 -4.4286346 -4.4286466 -4.4286509 -4.4286637][-4.4287558 -4.4286957 -4.4286084 -4.4285064 -4.4284239 -4.4283271 -4.42818 -4.4281015 -4.4282322 -4.428411 -4.4285197 -4.4285827 -4.42862 -4.4286394 -4.4286661][-4.428771 -4.4287229 -4.4286504 -4.4285364 -4.4284415 -4.4283524 -4.4281931 -4.4280791 -4.4281864 -4.4283724 -4.428484 -4.4285483 -4.4285903 -4.428617 -4.4286618][-4.4287953 -4.4287615 -4.4287062 -4.4286 -4.4285054 -4.4284344 -4.4283109 -4.4282231 -4.4282918 -4.4284215 -4.4284987 -4.4285393 -4.4285688 -4.4285951 -4.4286494][-4.4288077 -4.4287834 -4.4287424 -4.4286647 -4.4285774 -4.4285159 -4.4284277 -4.4283676 -4.4283986 -4.4284625 -4.4285021 -4.4285288 -4.428545 -4.4285612 -4.4286208][-4.4287853 -4.4287586 -4.4287267 -4.4286842 -4.4286218 -4.4285703 -4.4285131 -4.4284792 -4.4284863 -4.4285016 -4.428504 -4.4285197 -4.4285393 -4.4285612 -4.428616][-4.4287882 -4.4287562 -4.428721 -4.4286981 -4.4286633 -4.4286318 -4.4285994 -4.428587 -4.4285941 -4.4285917 -4.428575 -4.4285784 -4.4285994 -4.4286227 -4.4286623][-4.4288292 -4.4288 -4.4287639 -4.4287419 -4.4287214 -4.4287028 -4.4286857 -4.4286847 -4.4286971 -4.4286919 -4.4286752 -4.4286771 -4.4286995 -4.428721 -4.4287481][-4.4288707 -4.4288535 -4.4288263 -4.428802 -4.4287891 -4.4287863 -4.428782 -4.4287896 -4.4288111 -4.4288111 -4.4287963 -4.4287958 -4.428813 -4.4288259 -4.4288416][-4.4289041 -4.4288983 -4.4288836 -4.428865 -4.428863 -4.4288659 -4.4288664 -4.4288759 -4.4288969 -4.4289012 -4.42889 -4.4288878 -4.4288979 -4.4289041 -4.4289136]]...]
INFO - root - 2017-12-10 05:24:18.383307: step 2510, loss = 2.28, batch loss = 2.23 (28.3 examples/sec; 0.282 sec/batch; 25h:52m:08s remains)
INFO - root - 2017-12-10 05:24:21.062145: step 2520, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:14m:49s remains)
INFO - root - 2017-12-10 05:24:23.753995: step 2530, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:00m:35s remains)
INFO - root - 2017-12-10 05:24:26.413192: step 2540, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:25m:39s remains)
INFO - root - 2017-12-10 05:24:29.109087: step 2550, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:56m:27s remains)
INFO - root - 2017-12-10 05:24:31.825962: step 2560, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.281 sec/batch; 25h:47m:08s remains)
INFO - root - 2017-12-10 05:24:34.473749: step 2570, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:50m:50s remains)
INFO - root - 2017-12-10 05:24:37.152656: step 2580, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:24m:11s remains)
INFO - root - 2017-12-10 05:24:39.821399: step 2590, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:40m:40s remains)
INFO - root - 2017-12-10 05:24:42.412396: step 2600, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:14m:23s remains)
2017-12-10 05:24:42.706686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289837 -4.4289865 -4.4289947 -4.4289994 -4.4290023 -4.4290028 -4.4290037 -4.4290042 -4.4290042 -4.4290042 -4.4290042 -4.4290042 -4.4290042 -4.4290042 -4.4290032][-4.4289222 -4.4289336 -4.4289536 -4.4289694 -4.42898 -4.4289832 -4.4289846 -4.4289851 -4.4289842 -4.4289818 -4.428978 -4.4289737 -4.4289684 -4.4289651 -4.4289622][-4.428812 -4.4288282 -4.4288626 -4.4288979 -4.4289246 -4.428937 -4.4289412 -4.4289446 -4.4289455 -4.4289441 -4.42894 -4.4289341 -4.4289231 -4.4289141 -4.4289069][-4.4286747 -4.4286871 -4.4287367 -4.4287968 -4.4288468 -4.4288754 -4.4288888 -4.4288964 -4.4289021 -4.4289055 -4.4289036 -4.4288964 -4.4288831 -4.4288692 -4.4288588][-4.4285426 -4.4285374 -4.42859 -4.4286661 -4.4287324 -4.4287715 -4.42879 -4.4288054 -4.4288244 -4.4288406 -4.4288507 -4.4288507 -4.4288416 -4.42883 -4.4288244][-4.4284654 -4.4284282 -4.4284616 -4.4285321 -4.4285975 -4.4286323 -4.4286456 -4.4286666 -4.4287043 -4.4287405 -4.4287682 -4.4287782 -4.4287767 -4.4287767 -4.4287863][-4.4284911 -4.4284253 -4.4284229 -4.4284582 -4.42849 -4.4284921 -4.42848 -4.4284949 -4.4285469 -4.4286079 -4.4286618 -4.4286914 -4.4287033 -4.428719 -4.4287486][-4.4286132 -4.4285536 -4.4285345 -4.428535 -4.4285231 -4.428484 -4.4284339 -4.4284205 -4.42846 -4.4285221 -4.4285865 -4.4286265 -4.4286447 -4.4286661 -4.4287052][-4.4287267 -4.4286938 -4.428689 -4.4286909 -4.428678 -4.42864 -4.428586 -4.428555 -4.428566 -4.4285984 -4.428638 -4.4286613 -4.4286647 -4.4286718 -4.4287028][-4.4287772 -4.4287715 -4.4287915 -4.4288135 -4.4288225 -4.4288087 -4.4287758 -4.4287481 -4.4287443 -4.4287562 -4.4287744 -4.42878 -4.4287691 -4.4287605 -4.428771][-4.4287591 -4.42877 -4.4288144 -4.4288645 -4.4289007 -4.4289155 -4.4289069 -4.4288921 -4.4288888 -4.4288955 -4.428905 -4.4289012 -4.42888 -4.4288583 -4.4288492][-4.4286923 -4.4287028 -4.4287543 -4.4288225 -4.4288788 -4.4289145 -4.428926 -4.4289274 -4.4289346 -4.4289465 -4.4289556 -4.428946 -4.4289117 -4.428875 -4.428853][-4.4285712 -4.4285665 -4.4286146 -4.4286928 -4.4287634 -4.4288116 -4.428834 -4.4288464 -4.428864 -4.4288836 -4.4288936 -4.4288783 -4.4288292 -4.42878 -4.4287558][-4.4284511 -4.4284225 -4.4284563 -4.4285297 -4.4286 -4.428648 -4.4286709 -4.4286847 -4.4287062 -4.4287281 -4.4287424 -4.4287262 -4.4286666 -4.4286075 -4.428587][-4.4284492 -4.4284077 -4.4284244 -4.428484 -4.4285421 -4.4285808 -4.4285955 -4.4286027 -4.4286194 -4.428638 -4.4286513 -4.4286389 -4.4285855 -4.4285288 -4.4285111]]...]
INFO - root - 2017-12-10 05:24:45.339769: step 2610, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:36m:00s remains)
INFO - root - 2017-12-10 05:24:48.012519: step 2620, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:30m:27s remains)
INFO - root - 2017-12-10 05:24:50.707010: step 2630, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:56m:44s remains)
INFO - root - 2017-12-10 05:24:53.353743: step 2640, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:06m:19s remains)
INFO - root - 2017-12-10 05:24:55.988739: step 2650, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 25h:13m:52s remains)
INFO - root - 2017-12-10 05:24:58.630574: step 2660, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 24h:04m:02s remains)
INFO - root - 2017-12-10 05:25:01.240304: step 2670, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:38m:19s remains)
INFO - root - 2017-12-10 05:25:03.866078: step 2680, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:36m:11s remains)
INFO - root - 2017-12-10 05:25:06.523314: step 2690, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:48m:27s remains)
INFO - root - 2017-12-10 05:25:09.216385: step 2700, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:17m:13s remains)
2017-12-10 05:25:09.516386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428822 -4.4288206 -4.4288297 -4.4288511 -4.4288721 -4.4288836 -4.4288859 -4.4288893 -4.4288955 -4.4288969 -4.4288931 -4.4288898 -4.4288907 -4.4288926 -4.4288926][-4.4287896 -4.428791 -4.4288073 -4.4288363 -4.4288669 -4.4288845 -4.4288936 -4.4289 -4.4289041 -4.4289007 -4.4288931 -4.4288869 -4.428885 -4.4288855 -4.4288864][-4.42882 -4.428822 -4.4288287 -4.4288435 -4.4288597 -4.4288697 -4.428874 -4.4288778 -4.4288836 -4.4288816 -4.4288754 -4.4288692 -4.4288683 -4.4288716 -4.4288769][-4.4288621 -4.4288645 -4.4288664 -4.4288683 -4.4288683 -4.4288616 -4.4288559 -4.4288597 -4.4288721 -4.4288797 -4.4288764 -4.4288712 -4.4288731 -4.4288778 -4.4288816][-4.4288869 -4.4288917 -4.4288945 -4.4288869 -4.4288735 -4.428853 -4.4288392 -4.4288449 -4.4288664 -4.4288845 -4.4288869 -4.4288855 -4.4288878 -4.4288874 -4.4288821][-4.4288917 -4.4288945 -4.428894 -4.4288716 -4.4288416 -4.4288087 -4.4287925 -4.4288044 -4.4288387 -4.4288726 -4.4288874 -4.4288931 -4.4288979 -4.4288921 -4.428875][-4.4288163 -4.4288139 -4.4288106 -4.4287782 -4.4287338 -4.42869 -4.428669 -4.4286795 -4.4287233 -4.4287829 -4.4288259 -4.4288564 -4.4288774 -4.4288769 -4.4288545][-4.4286828 -4.4286766 -4.4286613 -4.4286137 -4.4285593 -4.4285088 -4.4284754 -4.4284697 -4.4285192 -4.4286108 -4.4286871 -4.4287429 -4.4287825 -4.428792 -4.4287758][-4.4286337 -4.4286156 -4.4285822 -4.428525 -4.4284692 -4.4284163 -4.4283662 -4.4283385 -4.4283834 -4.4284921 -4.4285889 -4.42866 -4.4287043 -4.4287162 -4.4287024][-4.4287114 -4.4286985 -4.42867 -4.4286304 -4.4285979 -4.4285674 -4.4285321 -4.4285088 -4.4285388 -4.4286156 -4.428688 -4.4287376 -4.4287596 -4.4287543 -4.4287329][-4.4288182 -4.4288116 -4.428791 -4.4287629 -4.4287448 -4.4287348 -4.4287233 -4.4287167 -4.4287338 -4.4287758 -4.4288187 -4.4288473 -4.4288583 -4.4288554 -4.4288383][-4.4288597 -4.4288535 -4.428833 -4.4288058 -4.4287877 -4.4287848 -4.4287882 -4.4287968 -4.4288092 -4.4288349 -4.4288621 -4.4288754 -4.4288816 -4.4288864 -4.4288731][-4.4288478 -4.42884 -4.4288149 -4.42878 -4.4287548 -4.4287534 -4.4287653 -4.4287791 -4.4287887 -4.4288116 -4.4288325 -4.4288387 -4.4288449 -4.4288573 -4.428854][-4.4288316 -4.4288211 -4.4287925 -4.4287519 -4.4287224 -4.4287214 -4.4287343 -4.4287424 -4.4287424 -4.4287596 -4.4287791 -4.4287882 -4.4288082 -4.4288368 -4.4288526][-4.4288473 -4.4288344 -4.4288082 -4.42877 -4.4287434 -4.4287438 -4.4287524 -4.4287553 -4.4287481 -4.4287572 -4.4287744 -4.4287882 -4.4288154 -4.4288526 -4.4288788]]...]
INFO - root - 2017-12-10 05:25:12.111966: step 2710, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:43m:47s remains)
INFO - root - 2017-12-10 05:25:14.780484: step 2720, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:48m:09s remains)
INFO - root - 2017-12-10 05:25:17.443898: step 2730, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:21m:59s remains)
INFO - root - 2017-12-10 05:25:20.091210: step 2740, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:17m:29s remains)
INFO - root - 2017-12-10 05:25:22.765929: step 2750, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:25m:07s remains)
INFO - root - 2017-12-10 05:25:25.454280: step 2760, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:29m:32s remains)
INFO - root - 2017-12-10 05:25:28.103408: step 2770, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:16m:42s remains)
INFO - root - 2017-12-10 05:25:30.808421: step 2780, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:33m:11s remains)
INFO - root - 2017-12-10 05:25:33.456085: step 2790, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:57m:00s remains)
INFO - root - 2017-12-10 05:25:36.105535: step 2800, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:04m:44s remains)
2017-12-10 05:25:36.386802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289193 -4.428895 -4.4288859 -4.4288869 -4.4288921 -4.4289012 -4.4288931 -4.4288554 -4.4288082 -4.4287806 -4.4287653 -4.4287581 -4.4287558 -4.428772 -4.4287992][-4.4288774 -4.4288559 -4.4288559 -4.4288664 -4.4288778 -4.4288836 -4.4288731 -4.4288349 -4.4287934 -4.4287672 -4.428751 -4.428741 -4.4287395 -4.4287658 -4.4288039][-4.4288206 -4.42881 -4.4288263 -4.428854 -4.428875 -4.42888 -4.4288626 -4.4288225 -4.4287925 -4.4287796 -4.4287777 -4.4287758 -4.4287772 -4.4287972 -4.428833][-4.428792 -4.4287972 -4.4288287 -4.4288592 -4.4288793 -4.4288764 -4.4288511 -4.4288116 -4.4287887 -4.428792 -4.4288054 -4.428812 -4.4288182 -4.4288292 -4.42885][-4.4287872 -4.4287949 -4.4288216 -4.4288459 -4.4288578 -4.4288425 -4.428803 -4.428751 -4.4287291 -4.4287448 -4.4287715 -4.4287839 -4.4287939 -4.4288063 -4.428823][-4.428761 -4.4287524 -4.428761 -4.4287767 -4.428782 -4.4287543 -4.4286914 -4.4286075 -4.4285793 -4.4286175 -4.428668 -4.4286933 -4.4287176 -4.4287539 -4.4287806][-4.42871 -4.4286923 -4.4286909 -4.4286981 -4.4286947 -4.4286509 -4.4285569 -4.4284382 -4.4284 -4.428463 -4.428546 -4.4285975 -4.428647 -4.4287066 -4.4287524][-4.4286876 -4.4286757 -4.4286757 -4.4286823 -4.4286909 -4.4286652 -4.4285817 -4.4284768 -4.4284372 -4.4284887 -4.4285617 -4.428617 -4.4286757 -4.4287329 -4.4287786][-4.42871 -4.4287086 -4.4287219 -4.4287376 -4.4287591 -4.42876 -4.4287291 -4.4286761 -4.4286394 -4.42865 -4.4286852 -4.4287219 -4.4287639 -4.4288044 -4.428834][-4.428761 -4.4287658 -4.4287896 -4.4288087 -4.42882 -4.4288163 -4.4287987 -4.4287663 -4.4287333 -4.4287319 -4.4287524 -4.4287829 -4.4288163 -4.4288459 -4.4288626][-4.4287949 -4.4287915 -4.428813 -4.4288278 -4.4288259 -4.4288125 -4.4287934 -4.4287663 -4.42874 -4.428741 -4.428761 -4.428792 -4.4288211 -4.4288449 -4.4288588][-4.4287934 -4.4287767 -4.4287877 -4.4288058 -4.4288096 -4.4288006 -4.4287782 -4.4287548 -4.4287353 -4.4287391 -4.4287553 -4.4287815 -4.4288087 -4.4288297 -4.4288425][-4.4287682 -4.4287453 -4.4287581 -4.4287949 -4.4288197 -4.4288163 -4.4287925 -4.4287677 -4.4287457 -4.4287434 -4.4287515 -4.4287667 -4.4287829 -4.4287958 -4.42881][-4.4287658 -4.4287572 -4.428781 -4.4288278 -4.4288564 -4.4288468 -4.4288106 -4.4287772 -4.4287505 -4.4287419 -4.4287457 -4.4287548 -4.4287639 -4.428771 -4.4287834][-4.4287791 -4.4287877 -4.4288125 -4.42885 -4.4288712 -4.4288554 -4.4288116 -4.4287753 -4.4287519 -4.4287434 -4.4287453 -4.4287529 -4.42876 -4.42876 -4.4287663]]...]
INFO - root - 2017-12-10 05:25:39.064968: step 2810, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.277 sec/batch; 25h:24m:48s remains)
INFO - root - 2017-12-10 05:25:41.700692: step 2820, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:14m:36s remains)
INFO - root - 2017-12-10 05:25:44.325554: step 2830, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 25h:26m:45s remains)
INFO - root - 2017-12-10 05:25:47.011735: step 2840, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:48m:52s remains)
INFO - root - 2017-12-10 05:25:49.661301: step 2850, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:18m:18s remains)
INFO - root - 2017-12-10 05:25:52.327985: step 2860, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:34m:14s remains)
INFO - root - 2017-12-10 05:25:54.907117: step 2870, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:34m:05s remains)
INFO - root - 2017-12-10 05:25:57.618954: step 2880, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:22m:34s remains)
INFO - root - 2017-12-10 05:26:00.267339: step 2890, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:58m:24s remains)
INFO - root - 2017-12-10 05:26:02.920776: step 2900, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:15m:02s remains)
2017-12-10 05:26:03.216395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42885 -4.428865 -4.4288821 -4.4289045 -4.4289093 -4.4288793 -4.4288354 -4.4287953 -4.4287663 -4.4287715 -4.4288068 -4.4288535 -4.4288826 -4.4288716 -4.4288182][-4.4288435 -4.4288487 -4.4288516 -4.428863 -4.42887 -4.4288583 -4.4288292 -4.4287958 -4.428761 -4.4287639 -4.4288044 -4.4288554 -4.4288874 -4.4288855 -4.4288425][-4.4288464 -4.4288378 -4.4288244 -4.4288297 -4.4288487 -4.4288626 -4.4288516 -4.4288254 -4.4287848 -4.4287729 -4.4288063 -4.428854 -4.4288855 -4.4288874 -4.4288583][-4.4288468 -4.4288316 -4.4288149 -4.4288225 -4.4288521 -4.4288797 -4.4288831 -4.4288616 -4.4288154 -4.4287925 -4.4288163 -4.4288549 -4.4288769 -4.428875 -4.4288516][-4.4288378 -4.4288287 -4.4288192 -4.4288349 -4.42887 -4.4288993 -4.4289041 -4.4288807 -4.428833 -4.4288054 -4.4288197 -4.4288454 -4.428853 -4.4288406 -4.4288168][-4.4288297 -4.4288349 -4.4288411 -4.4288664 -4.4289026 -4.4289246 -4.4289227 -4.4288969 -4.4288535 -4.4288292 -4.4288349 -4.4288435 -4.428833 -4.4288058 -4.4287758][-4.4288273 -4.4288478 -4.4288683 -4.4288964 -4.428926 -4.4289379 -4.4289293 -4.428906 -4.4288759 -4.4288659 -4.4288745 -4.4288716 -4.4288445 -4.428803 -4.4287691][-4.4288254 -4.42886 -4.4288878 -4.4289079 -4.4289207 -4.4289212 -4.4289088 -4.4288945 -4.4288807 -4.4288855 -4.4289012 -4.4288988 -4.4288716 -4.428834 -4.4288073][-4.4288197 -4.4288692 -4.4288974 -4.4289012 -4.4288907 -4.4288793 -4.428865 -4.4288549 -4.4288507 -4.4288635 -4.4288864 -4.4288917 -4.4288745 -4.428854 -4.4288468][-4.428803 -4.428863 -4.4288893 -4.4288793 -4.4288511 -4.4288325 -4.4288182 -4.4288092 -4.4288073 -4.4288244 -4.4288492 -4.4288559 -4.4288468 -4.4288421 -4.428853][-4.4287891 -4.4288549 -4.4288797 -4.4288654 -4.428833 -4.4288125 -4.4288011 -4.4287953 -4.4287906 -4.4287982 -4.4288063 -4.4287992 -4.4287877 -4.4287925 -4.4288187][-4.428802 -4.4288635 -4.4288864 -4.4288731 -4.4288445 -4.4288239 -4.4288158 -4.4288096 -4.4287977 -4.4287944 -4.4287848 -4.428761 -4.4287381 -4.4287438 -4.4287758][-4.4288383 -4.4288864 -4.4289045 -4.4288931 -4.4288688 -4.4288473 -4.4288392 -4.428833 -4.428823 -4.4288206 -4.4288054 -4.4287691 -4.4287329 -4.4287281 -4.4287472][-4.4288731 -4.4289 -4.4289093 -4.4289007 -4.4288836 -4.4288659 -4.4288568 -4.42885 -4.4288492 -4.4288597 -4.4288521 -4.42881 -4.4287634 -4.4287448 -4.4287434][-4.4288912 -4.4289 -4.4289031 -4.4289 -4.4288921 -4.4288788 -4.4288654 -4.4288578 -4.4288669 -4.4288936 -4.4288974 -4.4288583 -4.4288106 -4.4287806 -4.4287577]]...]
INFO - root - 2017-12-10 05:26:05.811671: step 2910, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:55m:35s remains)
INFO - root - 2017-12-10 05:26:08.498105: step 2920, loss = 2.28, batch loss = 2.23 (27.3 examples/sec; 0.293 sec/batch; 26h:48m:05s remains)
INFO - root - 2017-12-10 05:26:11.148590: step 2930, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:17m:54s remains)
INFO - root - 2017-12-10 05:26:13.726680: step 2940, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:14m:16s remains)
INFO - root - 2017-12-10 05:26:16.395672: step 2950, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:16m:19s remains)
INFO - root - 2017-12-10 05:26:19.057441: step 2960, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:22m:01s remains)
INFO - root - 2017-12-10 05:26:21.748636: step 2970, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 26h:03m:32s remains)
INFO - root - 2017-12-10 05:26:24.431860: step 2980, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:25m:25s remains)
INFO - root - 2017-12-10 05:26:27.069520: step 2990, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 25h:03m:33s remains)
INFO - root - 2017-12-10 05:26:29.747938: step 3000, loss = 2.28, batch loss = 2.23 (28.2 examples/sec; 0.284 sec/batch; 25h:57m:43s remains)
2017-12-10 05:26:30.032486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286447 -4.4286633 -4.4287119 -4.4287767 -4.4288135 -4.4287777 -4.4287105 -4.4286408 -4.4286146 -4.428637 -4.4286747 -4.4287095 -4.4287534 -4.4287653 -4.4287739][-4.42864 -4.4286547 -4.4286947 -4.4287586 -4.4288006 -4.4287724 -4.4287071 -4.4286342 -4.42861 -4.428637 -4.4286728 -4.4287 -4.4287372 -4.4287486 -4.4287629][-4.4286342 -4.428637 -4.4286633 -4.4287257 -4.4287786 -4.4287691 -4.428721 -4.4286723 -4.4286747 -4.4287119 -4.428751 -4.4287767 -4.4287987 -4.428793 -4.4288011][-4.4286437 -4.4286342 -4.428637 -4.4286804 -4.4287229 -4.4287162 -4.4286795 -4.4286575 -4.4286861 -4.4287477 -4.4288044 -4.4288464 -4.4288688 -4.4288549 -4.4288597][-4.4286451 -4.4286351 -4.4286108 -4.428618 -4.42864 -4.4286227 -4.4285746 -4.4285469 -4.4285884 -4.4286814 -4.428762 -4.4288211 -4.428853 -4.4288521 -4.4288654][-4.428616 -4.4285975 -4.428545 -4.4285121 -4.4285169 -4.4284849 -4.4283934 -4.4283166 -4.4283586 -4.4284978 -4.4286118 -4.42869 -4.4287324 -4.4287596 -4.4287891][-4.428618 -4.4285827 -4.4284887 -4.4284019 -4.428369 -4.4283071 -4.4281492 -4.4279857 -4.4280314 -4.4282455 -4.4284143 -4.4285088 -4.4285593 -4.428618 -4.4286633][-4.4286857 -4.4286451 -4.4285464 -4.4284434 -4.4283857 -4.4283037 -4.4281282 -4.4279375 -4.4279761 -4.4282007 -4.4283738 -4.4284554 -4.428483 -4.428524 -4.4285564][-4.4287767 -4.4287434 -4.4286728 -4.4285927 -4.4285369 -4.4284778 -4.4283719 -4.4282727 -4.4283128 -4.4284544 -4.4285641 -4.4286027 -4.4285827 -4.4285717 -4.4285645][-4.42886 -4.4288378 -4.4287977 -4.4287429 -4.4286962 -4.4286575 -4.4286103 -4.4285722 -4.4285994 -4.4286675 -4.4287176 -4.428719 -4.428679 -4.4286542 -4.4286366][-4.4289007 -4.4288855 -4.4288673 -4.4288383 -4.4288116 -4.4287934 -4.4287758 -4.4287615 -4.4287758 -4.4288015 -4.428803 -4.428762 -4.4287009 -4.4286618 -4.4286251][-4.4289126 -4.4288983 -4.4288869 -4.4288721 -4.4288607 -4.4288559 -4.4288487 -4.4288344 -4.4288306 -4.4288306 -4.4288015 -4.4287357 -4.4286623 -4.4286122 -4.4285526][-4.4289255 -4.4289074 -4.4288936 -4.4288826 -4.4288774 -4.428875 -4.4288678 -4.4288516 -4.4288387 -4.4288278 -4.4287863 -4.4287124 -4.4286294 -4.4285679 -4.4284959][-4.4289508 -4.4289284 -4.4289117 -4.4289002 -4.4288921 -4.4288864 -4.4288793 -4.4288678 -4.4288597 -4.4288487 -4.4288125 -4.4287472 -4.4286718 -4.4286089 -4.4285412][-4.4289808 -4.4289622 -4.4289479 -4.4289374 -4.4289289 -4.4289193 -4.4289088 -4.4289002 -4.4288945 -4.4288788 -4.4288468 -4.4288 -4.4287443 -4.4286952 -4.4286475]]...]
INFO - root - 2017-12-10 05:26:32.672880: step 3010, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:23m:42s remains)
INFO - root - 2017-12-10 05:26:35.332757: step 3020, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:39m:21s remains)
INFO - root - 2017-12-10 05:26:37.994816: step 3030, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:11m:26s remains)
INFO - root - 2017-12-10 05:26:40.638401: step 3040, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 25h:06m:16s remains)
INFO - root - 2017-12-10 05:26:43.332896: step 3050, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:24m:44s remains)
INFO - root - 2017-12-10 05:26:46.045549: step 3060, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:21m:06s remains)
INFO - root - 2017-12-10 05:26:48.714727: step 3070, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:42m:31s remains)
INFO - root - 2017-12-10 05:26:51.342008: step 3080, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:30m:20s remains)
INFO - root - 2017-12-10 05:26:54.029211: step 3090, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:19m:07s remains)
INFO - root - 2017-12-10 05:26:56.659929: step 3100, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:43m:33s remains)
2017-12-10 05:26:56.954003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428936 -4.4289336 -4.4289317 -4.4289241 -4.4289184 -4.4289188 -4.4289188 -4.4289184 -4.42892 -4.4289236 -4.4289269 -4.4289246 -4.4289207 -4.4289136 -4.4289107][-4.4289365 -4.4289379 -4.4289341 -4.4289231 -4.4289155 -4.4289117 -4.4289069 -4.428905 -4.428906 -4.4289107 -4.4289165 -4.4289122 -4.4289074 -4.428894 -4.4288883][-4.428925 -4.4289303 -4.4289222 -4.428905 -4.42889 -4.4288774 -4.428863 -4.4288607 -4.428863 -4.4288707 -4.4288831 -4.4288778 -4.4288716 -4.42885 -4.428833][-4.4289002 -4.42891 -4.428896 -4.4288716 -4.4288468 -4.428823 -4.4287944 -4.4287815 -4.4287868 -4.4288068 -4.4288249 -4.4288077 -4.4287958 -4.4287648 -4.4287271][-4.4288616 -4.4288759 -4.4288588 -4.42883 -4.4287934 -4.4287496 -4.4286962 -4.4286652 -4.428679 -4.428719 -4.4287381 -4.4287043 -4.4286823 -4.4286451 -4.4285922][-4.4288139 -4.4288254 -4.4287958 -4.4287496 -4.4286914 -4.4286132 -4.4285207 -4.4284644 -4.4285049 -4.4285822 -4.4286146 -4.4285822 -4.4285703 -4.428556 -4.428514][-4.4287949 -4.4287925 -4.4287305 -4.4286432 -4.4285464 -4.4284239 -4.42828 -4.428184 -4.4282656 -4.4284043 -4.4284749 -4.4284744 -4.4285016 -4.4285417 -4.4285517][-4.4288149 -4.4287953 -4.4287033 -4.4285722 -4.4284377 -4.4282823 -4.4280925 -4.4279552 -4.42808 -4.4282794 -4.4283929 -4.4284372 -4.4285049 -4.4285979 -4.4286628][-4.4288592 -4.4288325 -4.4287391 -4.4285946 -4.428443 -4.428288 -4.4281163 -4.4279938 -4.428112 -4.4283071 -4.42843 -4.4284968 -4.4285784 -4.4286861 -4.4287682][-4.4289064 -4.4288845 -4.4288116 -4.4286847 -4.4285479 -4.4284191 -4.428308 -4.4282436 -4.4283166 -4.4284444 -4.4285374 -4.4286022 -4.4286733 -4.4287663 -4.428843][-4.4289551 -4.4289441 -4.4288974 -4.4288044 -4.4287043 -4.4286122 -4.4285507 -4.4285264 -4.4285631 -4.428627 -4.4286819 -4.4287271 -4.4287772 -4.4288425 -4.4288983][-4.4289861 -4.4289837 -4.4289608 -4.4289083 -4.4288521 -4.4287972 -4.4287653 -4.4287605 -4.4287791 -4.4288073 -4.4288311 -4.4288526 -4.428874 -4.4289055 -4.428936][-4.4289908 -4.4289932 -4.4289865 -4.4289637 -4.4289393 -4.4289141 -4.4289031 -4.4289088 -4.4289222 -4.428936 -4.4289446 -4.4289484 -4.428947 -4.4289532 -4.4289646][-4.4289827 -4.4289861 -4.428987 -4.4289804 -4.4289751 -4.4289675 -4.4289684 -4.4289789 -4.4289932 -4.4290047 -4.4290056 -4.4290018 -4.4289908 -4.4289846 -4.4289875][-4.4289718 -4.4289751 -4.4289794 -4.42898 -4.4289846 -4.4289885 -4.4289927 -4.4290028 -4.4290156 -4.4290247 -4.4290242 -4.429019 -4.4290094 -4.4290051 -4.4290094]]...]
INFO - root - 2017-12-10 05:26:59.569235: step 3110, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:10m:01s remains)
INFO - root - 2017-12-10 05:27:02.218140: step 3120, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:58m:57s remains)
INFO - root - 2017-12-10 05:27:04.868079: step 3130, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:38m:59s remains)
INFO - root - 2017-12-10 05:27:07.535299: step 3140, loss = 2.28, batch loss = 2.23 (27.9 examples/sec; 0.286 sec/batch; 26h:11m:42s remains)
INFO - root - 2017-12-10 05:27:10.171616: step 3150, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:39m:50s remains)
INFO - root - 2017-12-10 05:27:12.817109: step 3160, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:32m:15s remains)
INFO - root - 2017-12-10 05:27:15.390400: step 3170, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:20m:42s remains)
INFO - root - 2017-12-10 05:27:18.042123: step 3180, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:08m:07s remains)
INFO - root - 2017-12-10 05:27:20.692384: step 3190, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:26m:59s remains)
INFO - root - 2017-12-10 05:27:23.364202: step 3200, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:44m:13s remains)
2017-12-10 05:27:23.647485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290156 -4.4290028 -4.4289923 -4.4289765 -4.4289608 -4.4289503 -4.4289484 -4.4289465 -4.4289408 -4.4289327 -4.4289274 -4.4289279 -4.4289331 -4.4289384 -4.4289422][-4.428988 -4.4289742 -4.4289632 -4.4289441 -4.4289217 -4.428905 -4.4289012 -4.428896 -4.428884 -4.4288716 -4.4288659 -4.4288683 -4.4288778 -4.4288831 -4.4288821][-4.4289575 -4.4289427 -4.4289308 -4.4289088 -4.42888 -4.42886 -4.4288497 -4.4288383 -4.4288173 -4.4287944 -4.4287829 -4.4287839 -4.4287934 -4.4287953 -4.4287896][-4.4289303 -4.428916 -4.4289012 -4.4288721 -4.4288344 -4.4288039 -4.4287877 -4.4287782 -4.4287577 -4.42873 -4.4287148 -4.4287128 -4.4287195 -4.4287186 -4.4287062][-4.4289122 -4.4289021 -4.4288831 -4.4288373 -4.4287767 -4.4287162 -4.4286852 -4.428679 -4.4286723 -4.4286509 -4.4286351 -4.4286289 -4.4286408 -4.4286575 -4.428659][-4.4289021 -4.428894 -4.4288635 -4.4288 -4.4287148 -4.4286137 -4.4285464 -4.4285321 -4.4285345 -4.428524 -4.4285083 -4.4285 -4.4285297 -4.4285865 -4.4286265][-4.428896 -4.4288812 -4.4288392 -4.4287658 -4.4286623 -4.4285173 -4.4283957 -4.4283562 -4.4283633 -4.4283748 -4.4283819 -4.4284034 -4.4284687 -4.4285617 -4.4286304][-4.4288955 -4.4288721 -4.4288239 -4.4287515 -4.4286342 -4.4284449 -4.4282594 -4.42818 -4.428206 -4.4282613 -4.4283156 -4.42839 -4.4284935 -4.4285908 -4.4286532][-4.4288974 -4.4288707 -4.4288254 -4.4287605 -4.4286366 -4.4284267 -4.4282169 -4.4281354 -4.4282012 -4.4283023 -4.4283786 -4.4284654 -4.4285674 -4.428638 -4.428659][-4.428894 -4.4288592 -4.4288168 -4.428761 -4.4286451 -4.4284573 -4.4282784 -4.4282193 -4.4283166 -4.4284396 -4.4285131 -4.4285831 -4.428648 -4.4286742 -4.4286518][-4.4288936 -4.4288507 -4.4288096 -4.4287648 -4.4286733 -4.4285355 -4.4284086 -4.428359 -4.42844 -4.4285355 -4.4285836 -4.4286346 -4.4286804 -4.428688 -4.4286423][-4.4289055 -4.4288607 -4.4288273 -4.4287944 -4.4287372 -4.4286442 -4.428546 -4.42849 -4.4285345 -4.4285846 -4.4286022 -4.4286361 -4.4286771 -4.4286761 -4.4286242][-4.4289236 -4.4288816 -4.4288616 -4.4288445 -4.4288154 -4.4287505 -4.4286704 -4.4286094 -4.4286218 -4.4286361 -4.42862 -4.4286256 -4.4286551 -4.4286485 -4.4286][-4.4289436 -4.4289031 -4.4288907 -4.4288883 -4.4288745 -4.4288325 -4.4287806 -4.4287319 -4.428731 -4.4287305 -4.4286995 -4.4286842 -4.4286928 -4.4286704 -4.4286203][-4.4289651 -4.4289279 -4.4289165 -4.4289174 -4.4289141 -4.4288936 -4.4288721 -4.4288473 -4.4288483 -4.4288492 -4.4288249 -4.4288044 -4.4287958 -4.4287677 -4.4287252]]...]
INFO - root - 2017-12-10 05:27:26.356189: step 3210, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:35m:36s remains)
INFO - root - 2017-12-10 05:27:29.013432: step 3220, loss = 2.28, batch loss = 2.23 (27.1 examples/sec; 0.296 sec/batch; 27h:03m:01s remains)
INFO - root - 2017-12-10 05:27:31.678992: step 3230, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:14m:48s remains)
INFO - root - 2017-12-10 05:27:34.357209: step 3240, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:48m:16s remains)
INFO - root - 2017-12-10 05:27:36.993197: step 3250, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:13m:32s remains)
INFO - root - 2017-12-10 05:27:39.588161: step 3260, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:13m:16s remains)
INFO - root - 2017-12-10 05:27:42.236502: step 3270, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 25h:15m:54s remains)
INFO - root - 2017-12-10 05:27:44.870645: step 3280, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:04m:43s remains)
INFO - root - 2017-12-10 05:27:47.543792: step 3290, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:20m:24s remains)
INFO - root - 2017-12-10 05:27:50.226412: step 3300, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:42m:58s remains)
2017-12-10 05:27:50.508686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288487 -4.4289064 -4.4289451 -4.4289703 -4.4289823 -4.4289875 -4.4289761 -4.4289346 -4.4288845 -4.4288511 -4.428812 -4.4287724 -4.4287696 -4.4287963 -4.4288192][-4.4288077 -4.428875 -4.4289255 -4.4289546 -4.4289694 -4.4289818 -4.4289861 -4.4289703 -4.4289479 -4.4289351 -4.4289145 -4.4288831 -4.4288568 -4.4288416 -4.4288187][-4.4287558 -4.4288239 -4.428874 -4.4289002 -4.428915 -4.4289312 -4.4289393 -4.428937 -4.4289346 -4.4289384 -4.4289346 -4.4289174 -4.4288931 -4.4288678 -4.4288239][-4.4287572 -4.4288015 -4.4288297 -4.4288363 -4.4288363 -4.4288392 -4.4288349 -4.4288349 -4.4288511 -4.4288769 -4.4288974 -4.4289031 -4.4288964 -4.428885 -4.4288545][-4.4288006 -4.4288116 -4.4288044 -4.4287758 -4.4287376 -4.4286985 -4.4286609 -4.4286575 -4.4286966 -4.4287558 -4.4288149 -4.4288621 -4.4288907 -4.428906 -4.4288988][-4.4288483 -4.4288335 -4.428793 -4.4287224 -4.4286342 -4.4285383 -4.4284549 -4.4284463 -4.4285159 -4.428618 -4.4287181 -4.4288058 -4.4288621 -4.4288893 -4.4288907][-4.4288983 -4.4288664 -4.4287992 -4.4286885 -4.4285483 -4.4283881 -4.4282484 -4.4282389 -4.4283504 -4.4285064 -4.4286532 -4.42877 -4.4288373 -4.4288592 -4.4288549][-4.42896 -4.4289174 -4.4288321 -4.4286952 -4.4285212 -4.4283123 -4.428124 -4.4281139 -4.42826 -4.4284544 -4.4286327 -4.4287629 -4.4288273 -4.4288387 -4.4288316][-4.4290142 -4.4289694 -4.428884 -4.4287505 -4.4285831 -4.42838 -4.4281983 -4.428195 -4.4283347 -4.4285145 -4.42868 -4.4287982 -4.428853 -4.4288573 -4.4288487][-4.4290333 -4.428987 -4.4289069 -4.4287958 -4.4286723 -4.4285331 -4.4284172 -4.4284325 -4.4285369 -4.4286575 -4.4287715 -4.4288559 -4.4289002 -4.4289074 -4.4288993][-4.4290109 -4.4289646 -4.4288983 -4.4288192 -4.4287496 -4.4286847 -4.42864 -4.4286718 -4.4287386 -4.4287982 -4.4288559 -4.4289021 -4.4289351 -4.4289513 -4.4289494][-4.4289641 -4.4289207 -4.428875 -4.4288335 -4.4288116 -4.4288054 -4.4288106 -4.428844 -4.428874 -4.4288859 -4.4288969 -4.4289093 -4.4289303 -4.4289551 -4.4289646][-4.4289122 -4.4288659 -4.4288349 -4.4288259 -4.4288397 -4.4288726 -4.4289079 -4.4289365 -4.4289393 -4.4289193 -4.4288974 -4.4288821 -4.4288926 -4.4289274 -4.4289541][-4.4288664 -4.4288168 -4.4287949 -4.4288077 -4.428843 -4.4288921 -4.4289384 -4.4289641 -4.428957 -4.4289222 -4.4288783 -4.4288397 -4.4288397 -4.4288821 -4.4289279][-4.4288497 -4.4288077 -4.4287949 -4.4288173 -4.4288573 -4.4289036 -4.4289441 -4.4289651 -4.4289556 -4.4289184 -4.4288635 -4.4288116 -4.428803 -4.4288449 -4.4289002]]...]
INFO - root - 2017-12-10 05:27:53.184537: step 3310, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:21m:02s remains)
INFO - root - 2017-12-10 05:27:55.819349: step 3320, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:56m:23s remains)
INFO - root - 2017-12-10 05:27:58.510800: step 3330, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:35m:45s remains)
INFO - root - 2017-12-10 05:28:01.103433: step 3340, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:10m:19s remains)
INFO - root - 2017-12-10 05:28:03.757328: step 3350, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:02m:08s remains)
INFO - root - 2017-12-10 05:28:06.431309: step 3360, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:22m:44s remains)
INFO - root - 2017-12-10 05:28:09.085201: step 3370, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:22m:10s remains)
INFO - root - 2017-12-10 05:28:11.776334: step 3380, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:20m:45s remains)
INFO - root - 2017-12-10 05:28:14.468620: step 3390, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:38m:51s remains)
INFO - root - 2017-12-10 05:28:17.109102: step 3400, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:43m:57s remains)
2017-12-10 05:28:17.386576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4284706 -4.4285178 -4.4286075 -4.4287024 -4.4287596 -4.4287643 -4.4287472 -4.4287319 -4.4287219 -4.4287295 -4.4287438 -4.4287553 -4.4287562 -4.4287376 -4.4287148][-4.4284892 -4.4285574 -4.4286594 -4.4287605 -4.4288125 -4.4288111 -4.42879 -4.4287629 -4.4287419 -4.4287381 -4.4287453 -4.4287586 -4.428761 -4.4287348 -4.4286885][-4.4285994 -4.4286752 -4.428771 -4.4288511 -4.4288831 -4.4288716 -4.4288411 -4.4287987 -4.4287572 -4.4287438 -4.4287539 -4.428772 -4.4287829 -4.4287562 -4.4286985][-4.4287138 -4.4287705 -4.4288411 -4.4288936 -4.4289064 -4.4288845 -4.4288368 -4.428772 -4.4287109 -4.4286933 -4.42871 -4.4287415 -4.4287705 -4.4287553 -4.4287066][-4.4287663 -4.4288025 -4.4288435 -4.42887 -4.4288688 -4.4288273 -4.4287472 -4.4286418 -4.4285545 -4.4285455 -4.4285965 -4.428669 -4.4287338 -4.4287448 -4.4287205][-4.4287481 -4.4287634 -4.4287767 -4.4287858 -4.4287715 -4.4287062 -4.4285879 -4.4284344 -4.4283137 -4.4283357 -4.4284563 -4.4285865 -4.4286909 -4.4287357 -4.4287424][-4.4287152 -4.4287014 -4.4286919 -4.4286838 -4.4286571 -4.4285731 -4.4284244 -4.4282341 -4.428103 -4.4281926 -4.4283972 -4.4285731 -4.4287028 -4.4287691 -4.4287939][-4.4286866 -4.428658 -4.4286408 -4.4286284 -4.4286027 -4.428524 -4.4283957 -4.4282541 -4.4281983 -4.4283228 -4.4285207 -4.42867 -4.4287734 -4.4288287 -4.4288487][-4.4286919 -4.4286623 -4.4286532 -4.4286451 -4.428627 -4.4285731 -4.42851 -4.4284654 -4.4284749 -4.4285736 -4.4286985 -4.4287753 -4.42882 -4.428843 -4.4288511][-4.4287267 -4.4286957 -4.4286885 -4.4286904 -4.4286909 -4.4286609 -4.4286489 -4.4286604 -4.4286842 -4.4287415 -4.4287992 -4.4288135 -4.428802 -4.42879 -4.4287815][-4.428751 -4.4287109 -4.4286919 -4.4287009 -4.4287167 -4.4287076 -4.4287181 -4.4287477 -4.428771 -4.4288011 -4.4288177 -4.4288006 -4.4287596 -4.4287143 -4.4286871][-4.4287539 -4.4287095 -4.4286814 -4.428689 -4.4287105 -4.4287114 -4.4287214 -4.4287434 -4.4287629 -4.4287858 -4.4287992 -4.4287744 -4.4287224 -4.4286585 -4.4286227][-4.4287329 -4.4286852 -4.4286613 -4.428678 -4.4287004 -4.4286928 -4.4286866 -4.4286962 -4.4287171 -4.4287524 -4.4287791 -4.4287653 -4.4287076 -4.4286361 -4.4286036][-4.428721 -4.4286819 -4.4286671 -4.4286814 -4.4286928 -4.4286752 -4.4286532 -4.428647 -4.42867 -4.4287205 -4.4287586 -4.4287567 -4.4287062 -4.4286432 -4.42862][-4.4287353 -4.4287114 -4.4287095 -4.428719 -4.4287162 -4.4286876 -4.42865 -4.42863 -4.4286456 -4.4286981 -4.4287434 -4.42875 -4.4287157 -4.4286728 -4.4286618]]...]
INFO - root - 2017-12-10 05:28:20.070934: step 3410, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 26h:09m:35s remains)
INFO - root - 2017-12-10 05:28:22.723829: step 3420, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:07m:27s remains)
INFO - root - 2017-12-10 05:28:25.380456: step 3430, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:44m:08s remains)
INFO - root - 2017-12-10 05:28:28.049687: step 3440, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.285 sec/batch; 26h:04m:15s remains)
INFO - root - 2017-12-10 05:28:30.732979: step 3450, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:23m:49s remains)
INFO - root - 2017-12-10 05:28:33.380877: step 3460, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:19m:36s remains)
INFO - root - 2017-12-10 05:28:36.008386: step 3470, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:18m:48s remains)
INFO - root - 2017-12-10 05:28:38.699802: step 3480, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:15m:33s remains)
INFO - root - 2017-12-10 05:28:41.337065: step 3490, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:41m:08s remains)
INFO - root - 2017-12-10 05:28:44.032861: step 3500, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:30m:00s remains)
2017-12-10 05:28:44.325138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288583 -4.4288726 -4.4288864 -4.4288869 -4.4288707 -4.4288559 -4.4288554 -4.4288521 -4.4288392 -4.42885 -4.4288878 -4.4289355 -4.4289656 -4.4289737 -4.4289684][-4.4287906 -4.4287987 -4.4288096 -4.4288111 -4.4287968 -4.4287863 -4.4287925 -4.4287848 -4.42876 -4.4287643 -4.4288092 -4.4288731 -4.4289188 -4.4289379 -4.4289408][-4.4287353 -4.4287329 -4.4287419 -4.4287481 -4.4287329 -4.4287148 -4.4287114 -4.4286962 -4.4286733 -4.4286876 -4.4287457 -4.4288216 -4.4288731 -4.4288974 -4.428906][-4.4286823 -4.428679 -4.4286938 -4.4287062 -4.4286847 -4.4286442 -4.4286084 -4.4285784 -4.4285746 -4.428627 -4.4287152 -4.4288054 -4.4288573 -4.4288778 -4.4288821][-4.4286227 -4.4286218 -4.4286437 -4.4286718 -4.4286437 -4.428565 -4.4284763 -4.4284196 -4.4284544 -4.4285722 -4.4287062 -4.4288158 -4.4288683 -4.4288821 -4.4288778][-4.428503 -4.4285135 -4.4285474 -4.4285932 -4.4285622 -4.4284444 -4.4282722 -4.4281545 -4.4282389 -4.4284611 -4.4286695 -4.4288116 -4.4288769 -4.4288898 -4.4288769][-4.4283867 -4.4284148 -4.4284492 -4.4284945 -4.4284635 -4.4283204 -4.4280777 -4.4278703 -4.4279742 -4.4282913 -4.4285765 -4.4287596 -4.4288487 -4.428875 -4.4288626][-4.4284205 -4.4284606 -4.4284892 -4.4285154 -4.4284868 -4.4283628 -4.4281468 -4.4279213 -4.4279618 -4.4282446 -4.4285293 -4.4287138 -4.4288096 -4.4288468 -4.4288425][-4.42855 -4.4285793 -4.4286056 -4.4286232 -4.4286051 -4.4285254 -4.4283919 -4.4282255 -4.4281907 -4.4283504 -4.4285679 -4.4287214 -4.4288054 -4.4288368 -4.4288354][-4.4286981 -4.4287109 -4.4287319 -4.4287529 -4.4287472 -4.4287004 -4.4286251 -4.428514 -4.4284487 -4.4285097 -4.4286485 -4.4287643 -4.4288325 -4.4288526 -4.4288492][-4.4288373 -4.4288344 -4.4288449 -4.4288621 -4.428865 -4.4288416 -4.428803 -4.4287381 -4.428679 -4.4286847 -4.428762 -4.4288454 -4.4288926 -4.4288974 -4.428885][-4.428936 -4.428926 -4.4289265 -4.4289355 -4.42894 -4.42893 -4.4289131 -4.4288812 -4.4288487 -4.42884 -4.4288821 -4.42894 -4.428966 -4.4289565 -4.4289331][-4.4289722 -4.4289618 -4.428956 -4.4289589 -4.4289618 -4.428956 -4.428947 -4.4289351 -4.4289231 -4.4289212 -4.4289465 -4.4289837 -4.429 -4.428987 -4.4289637][-4.4289584 -4.4289503 -4.4289441 -4.4289446 -4.4289441 -4.4289393 -4.4289341 -4.42893 -4.4289284 -4.4289331 -4.4289513 -4.4289761 -4.42899 -4.4289842 -4.4289718][-4.4289436 -4.4289412 -4.4289389 -4.4289384 -4.428936 -4.4289322 -4.4289284 -4.4289246 -4.4289246 -4.42893 -4.42894 -4.4289551 -4.428968 -4.4289718 -4.4289718]]...]
INFO - root - 2017-12-10 05:28:47.036419: step 3510, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:32m:50s remains)
INFO - root - 2017-12-10 05:28:49.678313: step 3520, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:36m:14s remains)
INFO - root - 2017-12-10 05:28:52.324010: step 3530, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:00m:33s remains)
INFO - root - 2017-12-10 05:28:54.960309: step 3540, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:56m:26s remains)
INFO - root - 2017-12-10 05:28:57.645474: step 3550, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 25h:08m:34s remains)
INFO - root - 2017-12-10 05:29:00.299450: step 3560, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:23m:55s remains)
INFO - root - 2017-12-10 05:29:02.919815: step 3570, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:51m:32s remains)
INFO - root - 2017-12-10 05:29:05.639754: step 3580, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.276 sec/batch; 25h:15m:21s remains)
INFO - root - 2017-12-10 05:29:08.294391: step 3590, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:41m:38s remains)
INFO - root - 2017-12-10 05:29:10.942354: step 3600, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:18m:05s remains)
2017-12-10 05:29:11.234256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289174 -4.4288974 -4.4288497 -4.4287686 -4.42869 -4.4286504 -4.4286642 -4.4287128 -4.4287705 -4.4288244 -4.4288559 -4.4288511 -4.428812 -4.4287772 -4.4287438][-4.4289079 -4.4288888 -4.4288511 -4.42877 -4.4286695 -4.4286065 -4.4286156 -4.4286747 -4.4287434 -4.4288077 -4.4288454 -4.4288387 -4.4287896 -4.4287496 -4.4287124][-4.4288878 -4.4288845 -4.4288669 -4.4287977 -4.428689 -4.4286089 -4.4286089 -4.4286675 -4.4287381 -4.4288034 -4.4288425 -4.4288406 -4.4287992 -4.4287605 -4.4287167][-4.428863 -4.4288797 -4.4288821 -4.4288349 -4.4287376 -4.4286513 -4.428638 -4.4286885 -4.4287572 -4.4288249 -4.4288716 -4.4288807 -4.4288521 -4.4288135 -4.4287596][-4.428823 -4.4288573 -4.4288769 -4.4288492 -4.4287677 -4.4286838 -4.4286628 -4.4287052 -4.4287686 -4.4288368 -4.4288912 -4.4289122 -4.4288955 -4.4288564 -4.4287992][-4.4287734 -4.4288158 -4.4288445 -4.4288321 -4.4287653 -4.4286895 -4.4286709 -4.4287143 -4.42877 -4.4288306 -4.4288888 -4.4289203 -4.4289131 -4.4288788 -4.4288235][-4.4287381 -4.42877 -4.4288011 -4.428802 -4.4287505 -4.4286838 -4.428668 -4.4287124 -4.4287596 -4.4288087 -4.428865 -4.4289074 -4.4289122 -4.4288855 -4.4288354][-4.4287171 -4.4287214 -4.4287515 -4.4287667 -4.4287357 -4.4286838 -4.4286685 -4.4287033 -4.4287372 -4.4287729 -4.4288168 -4.428865 -4.4288864 -4.428875 -4.4288421][-4.4287348 -4.4287071 -4.4287257 -4.4287529 -4.4287481 -4.4287205 -4.4287081 -4.4287271 -4.4287395 -4.4287562 -4.4287815 -4.4288259 -4.4288564 -4.4288645 -4.428854][-4.4287691 -4.4287262 -4.4287295 -4.4287562 -4.4287663 -4.4287558 -4.4287467 -4.4287515 -4.4287457 -4.4287448 -4.42876 -4.4287958 -4.4288273 -4.4288487 -4.4288545][-4.428802 -4.4287596 -4.4287505 -4.4287663 -4.4287744 -4.4287629 -4.4287505 -4.4287448 -4.4287286 -4.428721 -4.4287395 -4.4287739 -4.4288015 -4.4288259 -4.4288406][-4.4288235 -4.4287896 -4.4287777 -4.4287858 -4.42879 -4.4287763 -4.4287567 -4.4287443 -4.4287248 -4.4287171 -4.428741 -4.4287748 -4.4287992 -4.4288182 -4.4288321][-4.4288311 -4.4288054 -4.4287953 -4.4288 -4.428802 -4.4287877 -4.4287653 -4.4287481 -4.4287262 -4.4287186 -4.4287448 -4.4287796 -4.4288039 -4.42882 -4.4288282][-4.428863 -4.4288483 -4.4288445 -4.4288511 -4.428853 -4.4288387 -4.4288158 -4.4287949 -4.4287724 -4.4287639 -4.4287858 -4.4288192 -4.4288454 -4.4288607 -4.4288659][-4.4289021 -4.4288955 -4.4288964 -4.4289074 -4.428915 -4.4289093 -4.4288912 -4.4288716 -4.428853 -4.4288445 -4.4288588 -4.4288836 -4.4289012 -4.4289088 -4.42891]]...]
INFO - root - 2017-12-10 05:29:13.861135: step 3610, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:49m:03s remains)
INFO - root - 2017-12-10 05:29:16.489881: step 3620, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:57m:48s remains)
INFO - root - 2017-12-10 05:29:19.169687: step 3630, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:11m:04s remains)
INFO - root - 2017-12-10 05:29:21.838607: step 3640, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 25h:05m:10s remains)
INFO - root - 2017-12-10 05:29:24.495433: step 3650, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:59m:23s remains)
INFO - root - 2017-12-10 05:29:27.202127: step 3660, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:17m:39s remains)
INFO - root - 2017-12-10 05:29:29.827705: step 3670, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:24m:29s remains)
INFO - root - 2017-12-10 05:29:32.475635: step 3680, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:31m:06s remains)
INFO - root - 2017-12-10 05:29:35.182289: step 3690, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:15m:18s remains)
INFO - root - 2017-12-10 05:29:37.849545: step 3700, loss = 2.28, batch loss = 2.23 (27.6 examples/sec; 0.290 sec/batch; 26h:26m:57s remains)
2017-12-10 05:29:38.148715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287772 -4.4287815 -4.4287567 -4.4287157 -4.4286828 -4.4286714 -4.4286737 -4.4286637 -4.4286342 -4.4285989 -4.4286008 -4.42865 -4.4286923 -4.4287128 -4.4287081][-4.4287305 -4.4287333 -4.4287157 -4.4286785 -4.4286566 -4.4286628 -4.4286737 -4.4286571 -4.4286137 -4.4285674 -4.4285669 -4.428617 -4.4286685 -4.4286981 -4.4287052][-4.428668 -4.4286661 -4.4286537 -4.4286265 -4.4286232 -4.4286551 -4.4286876 -4.4286847 -4.4286489 -4.4286122 -4.4286242 -4.4286709 -4.4287081 -4.4287186 -4.4287119][-4.4286265 -4.4286246 -4.4286132 -4.428597 -4.4286108 -4.4286594 -4.4287052 -4.428721 -4.4287024 -4.42868 -4.4286962 -4.4287329 -4.4287534 -4.4287395 -4.4287176][-4.428607 -4.42859 -4.4285674 -4.4285479 -4.4285574 -4.4285951 -4.4286265 -4.4286485 -4.4286585 -4.4286613 -4.4286904 -4.4287233 -4.4287372 -4.4287148 -4.428688][-4.4285779 -4.4285259 -4.4284763 -4.4284363 -4.4284177 -4.4284182 -4.428412 -4.4284315 -4.4284806 -4.4285355 -4.4285965 -4.4286442 -4.4286642 -4.4286504 -4.42864][-4.4285045 -4.4284096 -4.428319 -4.428247 -4.4282002 -4.428164 -4.4281249 -4.428154 -4.4282575 -4.4283719 -4.4284735 -4.4285393 -4.4285741 -4.4285846 -4.4286017][-4.4284797 -4.4283695 -4.4282584 -4.4281697 -4.4281178 -4.4280734 -4.4280281 -4.4280667 -4.4281864 -4.4283185 -4.4284263 -4.4284906 -4.4285307 -4.4285617 -4.4286003][-4.4286165 -4.4285383 -4.42846 -4.4283962 -4.4283681 -4.4283524 -4.4283352 -4.4283533 -4.4284182 -4.4285007 -4.4285688 -4.4286041 -4.4286213 -4.428638 -4.428668][-4.4287629 -4.4287128 -4.4286594 -4.4286141 -4.4285965 -4.4285979 -4.4286041 -4.428618 -4.4286475 -4.4286923 -4.4287329 -4.4287515 -4.4287524 -4.4287438 -4.4287505][-4.4288526 -4.4288177 -4.4287782 -4.4287448 -4.4287338 -4.4287448 -4.4287624 -4.4287734 -4.4287868 -4.42881 -4.4288335 -4.428844 -4.4288406 -4.4288273 -4.428823][-4.42891 -4.42888 -4.4288492 -4.4288259 -4.4288187 -4.4288278 -4.4288425 -4.42885 -4.4288559 -4.4288673 -4.428884 -4.428895 -4.428896 -4.4288921 -4.4288931][-4.4289341 -4.4289107 -4.4288888 -4.4288726 -4.4288654 -4.4288688 -4.4288783 -4.4288864 -4.4288926 -4.4288979 -4.4289088 -4.4289217 -4.4289284 -4.4289308 -4.4289312][-4.4289594 -4.4289393 -4.4289241 -4.4289136 -4.4289074 -4.4289074 -4.4289107 -4.4289141 -4.4289145 -4.4289122 -4.428915 -4.4289231 -4.4289317 -4.4289355 -4.4289365][-4.4289751 -4.4289603 -4.4289532 -4.4289503 -4.4289508 -4.4289546 -4.4289608 -4.4289627 -4.4289579 -4.4289494 -4.4289436 -4.4289427 -4.4289446 -4.4289455 -4.4289441]]...]
INFO - root - 2017-12-10 05:29:40.763271: step 3710, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:13m:40s remains)
INFO - root - 2017-12-10 05:29:43.384444: step 3720, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:13m:17s remains)
INFO - root - 2017-12-10 05:29:46.043570: step 3730, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:33m:03s remains)
INFO - root - 2017-12-10 05:29:48.719661: step 3740, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:21m:09s remains)
INFO - root - 2017-12-10 05:29:51.392290: step 3750, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:58m:38s remains)
INFO - root - 2017-12-10 05:29:54.063642: step 3760, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:15m:48s remains)
INFO - root - 2017-12-10 05:29:56.739761: step 3770, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:19m:20s remains)
INFO - root - 2017-12-10 05:29:59.376791: step 3780, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:14m:19s remains)
INFO - root - 2017-12-10 05:30:02.049963: step 3790, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:30m:28s remains)
INFO - root - 2017-12-10 05:30:04.717301: step 3800, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:13m:00s remains)
2017-12-10 05:30:05.013313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289589 -4.4289408 -4.4289241 -4.4289093 -4.4288926 -4.4288864 -4.4288898 -4.428896 -4.4288945 -4.42889 -4.4288974 -4.428906 -4.4289131 -4.4289184 -4.4289217][-4.428947 -4.4289231 -4.4288993 -4.4288778 -4.4288535 -4.4288416 -4.42884 -4.4288483 -4.4288549 -4.4288545 -4.4288611 -4.428875 -4.42889 -4.4288988 -4.4289012][-4.4289255 -4.4288869 -4.4288497 -4.4288211 -4.4288006 -4.4287953 -4.4287953 -4.4288039 -4.4288168 -4.4288216 -4.4288273 -4.4288421 -4.4288659 -4.4288774 -4.4288759][-4.4288988 -4.4288373 -4.4287739 -4.4287233 -4.4287009 -4.4287028 -4.4287076 -4.4287267 -4.4287648 -4.4287858 -4.4287934 -4.4288111 -4.4288454 -4.4288578 -4.428853][-4.4288793 -4.4287934 -4.4287024 -4.4286141 -4.4285574 -4.4285355 -4.4285316 -4.428576 -4.42867 -4.4287229 -4.4287467 -4.4287815 -4.4288282 -4.4288445 -4.4288397][-4.4288864 -4.4287872 -4.4286747 -4.4285464 -4.42843 -4.4283309 -4.4282632 -4.4283171 -4.4284859 -4.4285975 -4.4286513 -4.4287148 -4.4287829 -4.4288139 -4.4288254][-4.4289031 -4.4288106 -4.4286933 -4.4285421 -4.4283686 -4.4281697 -4.4279795 -4.4279914 -4.4282188 -4.4283991 -4.42851 -4.4286146 -4.4287124 -4.4287648 -4.4288063][-4.42892 -4.4288449 -4.4287496 -4.4286222 -4.4284592 -4.4282451 -4.4280014 -4.4279313 -4.4281058 -4.4282866 -4.4284296 -4.428566 -4.42868 -4.4287448 -4.4288073][-4.4289327 -4.4288826 -4.4288187 -4.4287353 -4.4286251 -4.4284806 -4.428319 -4.4282503 -4.4283156 -4.4284172 -4.4285264 -4.4286356 -4.42872 -4.42877 -4.4288287][-4.4289412 -4.4289131 -4.4288721 -4.4288244 -4.4287629 -4.4286828 -4.428607 -4.4285722 -4.4285851 -4.4286313 -4.4286985 -4.428761 -4.4288011 -4.4288254 -4.4288664][-4.4289503 -4.4289317 -4.4289031 -4.4288769 -4.4288521 -4.4288187 -4.428793 -4.4287872 -4.4287848 -4.4288049 -4.4288416 -4.4288716 -4.4288883 -4.4288945 -4.428915][-4.4289551 -4.4289494 -4.4289317 -4.4289141 -4.4289088 -4.4289074 -4.4289107 -4.4289193 -4.4289126 -4.4289227 -4.4289427 -4.4289546 -4.4289541 -4.4289432 -4.4289479][-4.428956 -4.4289551 -4.4289465 -4.428937 -4.4289412 -4.4289532 -4.4289651 -4.4289742 -4.428968 -4.4289732 -4.4289851 -4.4289885 -4.4289784 -4.4289618 -4.4289589][-4.4289804 -4.4289775 -4.4289732 -4.42897 -4.4289742 -4.4289827 -4.42899 -4.4289918 -4.4289875 -4.4289894 -4.4289966 -4.4289966 -4.4289861 -4.4289746 -4.4289718][-4.4290004 -4.4289985 -4.4289961 -4.4289937 -4.4289937 -4.4289966 -4.428998 -4.4289956 -4.4289937 -4.4289951 -4.4289966 -4.4289932 -4.4289861 -4.4289813 -4.4289818]]...]
INFO - root - 2017-12-10 05:30:07.664217: step 3810, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.273 sec/batch; 24h:53m:11s remains)
INFO - root - 2017-12-10 05:30:10.341240: step 3820, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:46m:53s remains)
INFO - root - 2017-12-10 05:30:12.995032: step 3830, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:27m:58s remains)
INFO - root - 2017-12-10 05:30:15.673866: step 3840, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:34m:26s remains)
INFO - root - 2017-12-10 05:30:18.310420: step 3850, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:17m:43s remains)
INFO - root - 2017-12-10 05:30:20.904459: step 3860, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:25m:11s remains)
INFO - root - 2017-12-10 05:30:23.601013: step 3870, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:11m:19s remains)
INFO - root - 2017-12-10 05:30:26.275392: step 3880, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 25h:13m:15s remains)
INFO - root - 2017-12-10 05:30:28.933717: step 3890, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:38m:59s remains)
INFO - root - 2017-12-10 05:30:31.569784: step 3900, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:43m:22s remains)
2017-12-10 05:30:31.863767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287758 -4.4286103 -4.4284654 -4.4284039 -4.428442 -4.4285412 -4.4286413 -4.4287128 -4.4287376 -4.4287062 -4.42867 -4.4286504 -4.428648 -4.4286494 -4.4286518][-4.4287624 -4.42858 -4.42842 -4.4283538 -4.4284129 -4.428535 -4.4286442 -4.4287214 -4.4287462 -4.4287109 -4.4286695 -4.4286504 -4.4286475 -4.428647 -4.4286451][-4.4287558 -4.4285655 -4.4284005 -4.4283357 -4.4284115 -4.4285512 -4.4286656 -4.4287386 -4.4287624 -4.4287229 -4.4286733 -4.4286518 -4.4286475 -4.4286447 -4.4286451][-4.428761 -4.4285655 -4.4283934 -4.4283252 -4.4284077 -4.4285617 -4.4286814 -4.4287515 -4.4287744 -4.4287415 -4.4286857 -4.4286537 -4.4286489 -4.4286513 -4.4286656][-4.4287763 -4.428586 -4.4284096 -4.4283328 -4.428411 -4.4285731 -4.4286962 -4.4287624 -4.4287834 -4.42876 -4.4287081 -4.4286757 -4.42867 -4.4286771 -4.4287019][-4.428792 -4.4286151 -4.4284463 -4.4283671 -4.4284315 -4.4285841 -4.4287004 -4.428761 -4.4287863 -4.4287744 -4.4287348 -4.4287057 -4.4286966 -4.4287071 -4.4287357][-4.4288006 -4.42864 -4.4284878 -4.4284177 -4.4284716 -4.42861 -4.4287195 -4.4287724 -4.428802 -4.4287972 -4.4287605 -4.4287286 -4.42872 -4.4287314 -4.42876][-4.4288092 -4.4286633 -4.4285278 -4.428472 -4.4285192 -4.4286423 -4.4287472 -4.4287915 -4.4288216 -4.428823 -4.4287906 -4.4287567 -4.4287438 -4.4287481 -4.4287753][-4.4288249 -4.4286909 -4.4285665 -4.4285169 -4.4285603 -4.4286675 -4.4287672 -4.4288063 -4.428834 -4.4288445 -4.42882 -4.42879 -4.4287806 -4.4287772 -4.428793][-4.4288349 -4.4287124 -4.4285951 -4.4285464 -4.4285831 -4.428679 -4.4287753 -4.4288168 -4.4288392 -4.4288497 -4.4288321 -4.4288092 -4.4288058 -4.4288068 -4.4288225][-4.4288411 -4.4287319 -4.4286246 -4.4285769 -4.428607 -4.4286919 -4.4287834 -4.4288263 -4.4288483 -4.4288583 -4.4288435 -4.428822 -4.4288211 -4.4288278 -4.4288516][-4.4288521 -4.4287596 -4.428669 -4.4286246 -4.428648 -4.4287162 -4.4287958 -4.4288354 -4.4288549 -4.4288664 -4.428854 -4.428834 -4.428833 -4.4288449 -4.4288764][-4.4288678 -4.4287925 -4.4287181 -4.4286809 -4.4286957 -4.4287443 -4.4288063 -4.4288392 -4.428853 -4.4288583 -4.428843 -4.4288259 -4.4288278 -4.4288435 -4.4288769][-4.4288926 -4.4288344 -4.4287767 -4.4287462 -4.4287558 -4.428792 -4.4288378 -4.42886 -4.4288621 -4.4288549 -4.4288344 -4.4288187 -4.4288235 -4.4288411 -4.4288731][-4.42893 -4.428885 -4.4288425 -4.4288192 -4.4288263 -4.4288526 -4.428884 -4.4288969 -4.4288917 -4.4288745 -4.4288497 -4.4288344 -4.4288406 -4.4288592 -4.4288845]]...]
INFO - root - 2017-12-10 05:30:34.486478: step 3910, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:59m:37s remains)
INFO - root - 2017-12-10 05:30:37.154767: step 3920, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:21m:52s remains)
INFO - root - 2017-12-10 05:30:39.832724: step 3930, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:25m:03s remains)
INFO - root - 2017-12-10 05:30:42.494182: step 3940, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:26m:25s remains)
INFO - root - 2017-12-10 05:30:45.116762: step 3950, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:39m:47s remains)
INFO - root - 2017-12-10 05:30:47.773235: step 3960, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:55m:57s remains)
INFO - root - 2017-12-10 05:30:50.487687: step 3970, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.288 sec/batch; 26h:19m:21s remains)
INFO - root - 2017-12-10 05:30:53.164642: step 3980, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:39m:28s remains)
INFO - root - 2017-12-10 05:30:55.863636: step 3990, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:31m:46s remains)
INFO - root - 2017-12-10 05:30:58.507747: step 4000, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:26m:14s remains)
2017-12-10 05:30:58.803990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289708 -4.4289503 -4.4289322 -4.4289117 -4.4288864 -4.4288568 -4.42884 -4.4288521 -4.4288731 -4.4288836 -4.428885 -4.4289041 -4.4289303 -4.4289451 -4.428936][-4.4289465 -4.4289303 -4.4289117 -4.4288797 -4.4288373 -4.4287877 -4.4287486 -4.4287505 -4.4287744 -4.4287939 -4.4288015 -4.4288263 -4.428874 -4.4289155 -4.4289236][-4.4289031 -4.4288988 -4.4288874 -4.4288349 -4.4287605 -4.4286757 -4.4286089 -4.4285994 -4.4286313 -4.4286661 -4.4286842 -4.4287152 -4.4287772 -4.4288478 -4.4288836][-4.4288483 -4.4288626 -4.4288607 -4.4287848 -4.4286704 -4.4285398 -4.4284339 -4.42842 -4.4284787 -4.428546 -4.4285893 -4.4286308 -4.4286957 -4.4287834 -4.42884][-4.4287782 -4.4288144 -4.428822 -4.4287319 -4.428575 -4.428381 -4.4282112 -4.4282012 -4.4283295 -4.4284706 -4.4285579 -4.4286118 -4.4286666 -4.428751 -4.4288096][-4.4286675 -4.4287224 -4.4287553 -4.4286766 -4.4284945 -4.4282131 -4.4279361 -4.4279385 -4.428185 -4.4284363 -4.4285889 -4.4286604 -4.4286957 -4.4287553 -4.4288015][-4.4285464 -4.428607 -4.4286714 -4.428628 -4.4284487 -4.4280987 -4.4276962 -4.4277077 -4.4280944 -4.4284463 -4.4286442 -4.4287262 -4.4287453 -4.428772 -4.428802][-4.4284539 -4.4285054 -4.4286003 -4.4286137 -4.428484 -4.428174 -4.4277835 -4.427762 -4.4281397 -4.4284792 -4.4286819 -4.4287696 -4.4287844 -4.4287872 -4.4287963][-4.4284439 -4.4284663 -4.4285703 -4.4286385 -4.428596 -4.4284277 -4.4281979 -4.4281526 -4.4283414 -4.4285502 -4.4286985 -4.4287648 -4.4287748 -4.42877 -4.4287691][-4.4284968 -4.4284925 -4.4285865 -4.4286747 -4.4286923 -4.4286413 -4.4285517 -4.4285092 -4.4285417 -4.4286175 -4.4287047 -4.4287524 -4.4287486 -4.4287376 -4.4287291][-4.4285483 -4.428534 -4.4286137 -4.4286909 -4.4287233 -4.42872 -4.4286971 -4.4286561 -4.428607 -4.4286075 -4.4286737 -4.4287338 -4.42874 -4.42873 -4.4287133][-4.4285855 -4.4285717 -4.4286332 -4.4286742 -4.4286866 -4.4286981 -4.4287071 -4.4286742 -4.428607 -4.4285803 -4.4286375 -4.4287162 -4.4287567 -4.4287677 -4.428762][-4.4286327 -4.4286103 -4.4286494 -4.4286489 -4.4286318 -4.4286451 -4.4286714 -4.4286666 -4.4286327 -4.428618 -4.4286661 -4.4287467 -4.4287958 -4.4288216 -4.4288239][-4.4287152 -4.4286966 -4.42872 -4.4286904 -4.4286532 -4.4286551 -4.4286819 -4.4286985 -4.4287014 -4.4287133 -4.4287491 -4.4288111 -4.42885 -4.4288716 -4.4288688][-4.4288149 -4.4288025 -4.4288177 -4.4287744 -4.4287238 -4.4287114 -4.4287229 -4.4287391 -4.4287553 -4.42878 -4.4288077 -4.4288549 -4.4288912 -4.4289069 -4.4288926]]...]
INFO - root - 2017-12-10 05:31:01.454828: step 4010, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:22m:05s remains)
INFO - root - 2017-12-10 05:31:04.127951: step 4020, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:47m:17s remains)
INFO - root - 2017-12-10 05:31:06.793068: step 4030, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.284 sec/batch; 25h:56m:31s remains)
INFO - root - 2017-12-10 05:31:09.451482: step 4040, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:37m:09s remains)
INFO - root - 2017-12-10 05:31:12.110803: step 4050, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:45m:13s remains)
INFO - root - 2017-12-10 05:31:14.806109: step 4060, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:09m:34s remains)
INFO - root - 2017-12-10 05:31:17.452990: step 4070, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:38m:54s remains)
INFO - root - 2017-12-10 05:31:20.100997: step 4080, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:21m:25s remains)
INFO - root - 2017-12-10 05:31:22.779866: step 4090, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:30m:11s remains)
INFO - root - 2017-12-10 05:31:25.404394: step 4100, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:26m:06s remains)
2017-12-10 05:31:25.736003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289546 -4.428947 -4.4289451 -4.4289446 -4.4289412 -4.4289393 -4.4289351 -4.428936 -4.4289389 -4.4289384 -4.428937 -4.428936 -4.4289331 -4.4289317 -4.4289265][-4.4289355 -4.4289207 -4.4289188 -4.4289217 -4.4289241 -4.4289284 -4.4289265 -4.4289279 -4.4289317 -4.4289265 -4.4289184 -4.4289131 -4.4289083 -4.4289112 -4.4289136][-4.4288988 -4.4288807 -4.4288845 -4.4288964 -4.4289083 -4.4289203 -4.4289203 -4.4289222 -4.4289255 -4.4289112 -4.4288936 -4.42889 -4.42889 -4.4289 -4.428905][-4.4288483 -4.4288368 -4.428854 -4.4288783 -4.4288993 -4.4289117 -4.4289064 -4.4288993 -4.4288931 -4.4288673 -4.4288535 -4.4288645 -4.4288754 -4.4288898 -4.428894][-4.4287896 -4.428792 -4.4288254 -4.4288597 -4.4288812 -4.4288845 -4.4288583 -4.4288244 -4.4288049 -4.4287844 -4.4288 -4.4288387 -4.4288616 -4.4288735 -4.4288664][-4.4287634 -4.4287748 -4.4288054 -4.4288335 -4.428844 -4.4288249 -4.4287605 -4.4286933 -4.4286866 -4.4287162 -4.4287872 -4.4288487 -4.4288726 -4.4288673 -4.42884][-4.4287629 -4.4287648 -4.4287758 -4.4287829 -4.4287672 -4.4286995 -4.4285784 -4.4284973 -4.4285603 -4.4286809 -4.4288039 -4.4288673 -4.4288735 -4.4288507 -4.4288111][-4.428771 -4.4287539 -4.4287472 -4.4287376 -4.4286952 -4.4285917 -4.4284434 -4.4283876 -4.4285245 -4.4286966 -4.428823 -4.4288621 -4.428843 -4.4288 -4.4287539][-4.4287906 -4.42876 -4.4287529 -4.4287353 -4.4286861 -4.4285984 -4.4285078 -4.428515 -4.4286489 -4.4287782 -4.4288492 -4.4288421 -4.4287992 -4.4287477 -4.4287109][-4.4288168 -4.4287868 -4.4287872 -4.428771 -4.4287257 -4.4286761 -4.4286485 -4.4286871 -4.4287734 -4.4288359 -4.4288487 -4.4288096 -4.4287553 -4.4287176 -4.4287143][-4.4288416 -4.4288259 -4.4288325 -4.4288173 -4.4287858 -4.42876 -4.4287577 -4.4287868 -4.4288225 -4.428833 -4.4288082 -4.4287596 -4.4287181 -4.4287148 -4.4287462][-4.428844 -4.4288445 -4.4288516 -4.4288349 -4.4288173 -4.428813 -4.4288206 -4.4288278 -4.4288263 -4.4288073 -4.4287624 -4.4287162 -4.4287038 -4.4287367 -4.4287906][-4.4288011 -4.4288077 -4.4288168 -4.4288054 -4.4288015 -4.4288125 -4.4288225 -4.4288187 -4.4288011 -4.4287667 -4.42872 -4.42869 -4.4287081 -4.4287581 -4.42881][-4.4287424 -4.42874 -4.4287481 -4.4287443 -4.4287524 -4.4287724 -4.4287891 -4.4287868 -4.4287672 -4.428731 -4.4286962 -4.428688 -4.428719 -4.4287672 -4.4288111][-4.4287066 -4.4287 -4.4287066 -4.4287057 -4.4287086 -4.4287238 -4.4287477 -4.4287567 -4.4287529 -4.4287376 -4.4287162 -4.4287076 -4.4287281 -4.4287682 -4.4288082]]...]
INFO - root - 2017-12-10 05:31:28.368229: step 4110, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:04m:25s remains)
INFO - root - 2017-12-10 05:31:30.991444: step 4120, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:16m:51s remains)
INFO - root - 2017-12-10 05:31:33.643496: step 4130, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:14m:12s remains)
INFO - root - 2017-12-10 05:31:36.290168: step 4140, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:47m:07s remains)
INFO - root - 2017-12-10 05:31:38.957701: step 4150, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:14m:03s remains)
INFO - root - 2017-12-10 05:31:41.566660: step 4160, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:49m:58s remains)
INFO - root - 2017-12-10 05:31:44.259782: step 4170, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 26h:22m:20s remains)
INFO - root - 2017-12-10 05:31:46.889282: step 4180, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:18m:35s remains)
INFO - root - 2017-12-10 05:31:49.568244: step 4190, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 26h:18m:44s remains)
INFO - root - 2017-12-10 05:31:52.198228: step 4200, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:46m:41s remains)
2017-12-10 05:31:52.498731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287477 -4.4287558 -4.4287877 -4.4288211 -4.4288287 -4.428802 -4.4287362 -4.4286613 -4.4286432 -4.4286842 -4.4286885 -4.4286575 -4.4286618 -4.4287128 -4.4287839][-4.4287577 -4.4287744 -4.4288049 -4.4288259 -4.4288239 -4.4287996 -4.4287462 -4.4286885 -4.4286747 -4.428699 -4.4286761 -4.4286218 -4.4286103 -4.4286556 -4.4287333][-4.4287939 -4.4288158 -4.4288411 -4.4288487 -4.4288311 -4.4287872 -4.4287238 -4.4286766 -4.4286747 -4.4287038 -4.4286833 -4.4286332 -4.4286242 -4.428658 -4.4287205][-4.4288425 -4.4288573 -4.4288678 -4.42886 -4.4288244 -4.42875 -4.4286604 -4.4286089 -4.4286265 -4.4286795 -4.4286866 -4.4286623 -4.4286642 -4.4286928 -4.4287391][-4.4288831 -4.4288907 -4.4288917 -4.4288726 -4.4288158 -4.4287047 -4.4285746 -4.4284973 -4.4285245 -4.428616 -4.428668 -4.4286842 -4.4287071 -4.42874 -4.4287739][-4.4289107 -4.428915 -4.4289136 -4.42889 -4.428812 -4.4286709 -4.4285011 -4.4283876 -4.4284077 -4.4285207 -4.428617 -4.4286833 -4.4287372 -4.42878 -4.4288068][-4.4289222 -4.4289308 -4.4289346 -4.4289107 -4.4288235 -4.4286747 -4.4284954 -4.4283538 -4.4283361 -4.4284286 -4.428546 -4.4286551 -4.4287391 -4.4287968 -4.4288278][-4.428925 -4.428937 -4.4289417 -4.4289155 -4.4288344 -4.428709 -4.4285603 -4.4284239 -4.4283681 -4.4284072 -4.4285092 -4.428638 -4.4287419 -4.4288092 -4.4288435][-4.4289327 -4.4289446 -4.4289508 -4.4289308 -4.4288669 -4.4287672 -4.4286475 -4.4285264 -4.4284506 -4.4284472 -4.4285226 -4.4286437 -4.4287529 -4.4288211 -4.4288573][-4.4289384 -4.4289465 -4.4289527 -4.4289432 -4.4289026 -4.4288316 -4.4287348 -4.4286289 -4.4285507 -4.4285254 -4.4285755 -4.4286795 -4.4287815 -4.4288516 -4.4288898][-4.4289513 -4.4289551 -4.4289594 -4.4289594 -4.4289422 -4.4289017 -4.4288325 -4.4287467 -4.4286752 -4.4286413 -4.4286675 -4.4287448 -4.4288316 -4.4288983 -4.4289336][-4.428968 -4.4289684 -4.4289722 -4.4289789 -4.4289765 -4.4289565 -4.42891 -4.4288464 -4.4287896 -4.4287572 -4.4287682 -4.4288177 -4.4288812 -4.4289336 -4.428966][-4.4289846 -4.4289827 -4.4289846 -4.4289908 -4.428997 -4.42899 -4.4289618 -4.4289193 -4.4288778 -4.428854 -4.4288592 -4.4288878 -4.4289265 -4.4289613 -4.428987][-4.4289856 -4.4289846 -4.4289856 -4.4289927 -4.4290009 -4.4290009 -4.4289875 -4.4289618 -4.4289365 -4.4289212 -4.4289241 -4.4289408 -4.4289603 -4.4289775 -4.4289932][-4.4289775 -4.4289756 -4.4289756 -4.4289808 -4.4289885 -4.4289923 -4.4289885 -4.4289751 -4.42896 -4.4289517 -4.428957 -4.4289689 -4.428977 -4.4289818 -4.428988]]...]
INFO - root - 2017-12-10 05:31:55.166133: step 4210, loss = 2.28, batch loss = 2.23 (26.9 examples/sec; 0.298 sec/batch; 27h:09m:56s remains)
INFO - root - 2017-12-10 05:31:57.866633: step 4220, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:16m:20s remains)
INFO - root - 2017-12-10 05:32:00.541423: step 4230, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:12m:47s remains)
INFO - root - 2017-12-10 05:32:03.235288: step 4240, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:31m:35s remains)
INFO - root - 2017-12-10 05:32:05.932602: step 4250, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:33m:59s remains)
INFO - root - 2017-12-10 05:32:08.574032: step 4260, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 25h:04m:01s remains)
INFO - root - 2017-12-10 05:32:11.183922: step 4270, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:09m:01s remains)
INFO - root - 2017-12-10 05:32:13.878331: step 4280, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 25h:18m:14s remains)
INFO - root - 2017-12-10 05:32:16.549260: step 4290, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:54m:34s remains)
INFO - root - 2017-12-10 05:32:19.213730: step 4300, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:55m:59s remains)
2017-12-10 05:32:19.524327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288058 -4.4288249 -4.4288316 -4.428823 -4.4287953 -4.4287548 -4.4287186 -4.4287071 -4.4287238 -4.4287539 -4.4287815 -4.4288 -4.4288077 -4.4288034 -4.4287949][-4.4287748 -4.4288068 -4.4288197 -4.4288063 -4.4287596 -4.4286919 -4.4286361 -4.4286261 -4.4286566 -4.4287024 -4.4287467 -4.4287796 -4.4287949 -4.428791 -4.4287791][-4.4287577 -4.4287977 -4.4288111 -4.4287858 -4.4287162 -4.4286213 -4.4285493 -4.4285455 -4.4285946 -4.428658 -4.4287171 -4.4287629 -4.4287839 -4.4287763 -4.4287586][-4.4287634 -4.4288025 -4.4288077 -4.4287639 -4.428669 -4.4285502 -4.4284697 -4.4284749 -4.4285431 -4.4286251 -4.428699 -4.4287543 -4.4287758 -4.42876 -4.4287291][-4.42879 -4.4288173 -4.4288058 -4.4287391 -4.4286232 -4.4284916 -4.4284086 -4.4284234 -4.4285088 -4.4286056 -4.4286928 -4.4287581 -4.4287782 -4.4287491 -4.4286962][-4.42883 -4.4288373 -4.4288039 -4.4287148 -4.4285831 -4.4284425 -4.428359 -4.4283834 -4.4284849 -4.428597 -4.4286981 -4.4287715 -4.4287872 -4.4287443 -4.4286637][-4.4288669 -4.4288554 -4.4288025 -4.4286962 -4.4285517 -4.4283981 -4.4283023 -4.4283328 -4.4284568 -4.4285927 -4.4287105 -4.42879 -4.4288015 -4.4287472 -4.4286432][-4.4288774 -4.4288545 -4.4287958 -4.42869 -4.4285431 -4.4283767 -4.4282613 -4.4282951 -4.4284444 -4.4286051 -4.4287319 -4.4288139 -4.4288259 -4.428772 -4.4286561][-4.4288559 -4.4288263 -4.428781 -4.4287014 -4.4285793 -4.4284258 -4.42831 -4.4283385 -4.4284883 -4.4286461 -4.4287634 -4.4288397 -4.4288545 -4.4288111 -4.4287062][-4.428822 -4.4287891 -4.4287629 -4.4287238 -4.4286466 -4.4285331 -4.4284434 -4.4284697 -4.4285936 -4.4287181 -4.4288044 -4.4288621 -4.4288769 -4.4288468 -4.4287634][-4.42878 -4.4287481 -4.4287462 -4.4287467 -4.4287162 -4.4286489 -4.42859 -4.4286108 -4.4287 -4.4287829 -4.4288387 -4.4288793 -4.4288931 -4.428874 -4.4288054][-4.4287286 -4.4287047 -4.4287286 -4.4287653 -4.4287739 -4.4287443 -4.4287095 -4.4287219 -4.4287777 -4.4288316 -4.4288664 -4.4288926 -4.4289017 -4.4288831 -4.4288154][-4.4286819 -4.42867 -4.4287105 -4.4287653 -4.4287949 -4.42879 -4.4287767 -4.4287877 -4.4288211 -4.4288583 -4.4288864 -4.4289041 -4.4289055 -4.4288783 -4.4288073][-4.4286475 -4.4286518 -4.4286981 -4.4287534 -4.4287868 -4.4287877 -4.4287815 -4.4287877 -4.4288058 -4.4288416 -4.4288783 -4.4289007 -4.4288993 -4.4288683 -4.428803][-4.4286318 -4.4286532 -4.4287 -4.4287472 -4.4287705 -4.428762 -4.4287491 -4.4287419 -4.4287486 -4.428791 -4.4288421 -4.428875 -4.4288774 -4.4288521 -4.4288039]]...]
INFO - root - 2017-12-10 05:32:22.191052: step 4310, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:38m:58s remains)
INFO - root - 2017-12-10 05:32:24.824113: step 4320, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:47m:39s remains)
INFO - root - 2017-12-10 05:32:27.511285: step 4330, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:24m:38s remains)
INFO - root - 2017-12-10 05:32:30.152915: step 4340, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:24m:01s remains)
INFO - root - 2017-12-10 05:32:32.805371: step 4350, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:10m:05s remains)
INFO - root - 2017-12-10 05:32:35.497260: step 4360, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 26h:32m:19s remains)
INFO - root - 2017-12-10 05:32:38.148907: step 4370, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:52m:12s remains)
INFO - root - 2017-12-10 05:32:40.772735: step 4380, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:39m:17s remains)
INFO - root - 2017-12-10 05:32:43.447946: step 4390, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:43m:49s remains)
INFO - root - 2017-12-10 05:32:46.111415: step 4400, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:44m:59s remains)
2017-12-10 05:32:46.403007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287348 -4.4287324 -4.428709 -4.4286966 -4.4287076 -4.4287291 -4.4287205 -4.4286761 -4.4286413 -4.4286375 -4.4286561 -4.4287038 -4.4287443 -4.42874 -4.4286976][-4.4287004 -4.4286942 -4.428668 -4.428679 -4.4287076 -4.4287238 -4.4287076 -4.42866 -4.4286227 -4.4286141 -4.428637 -4.4286804 -4.428709 -4.4286904 -4.4286509][-4.4286489 -4.4286532 -4.428628 -4.4286361 -4.4286575 -4.4286718 -4.428669 -4.4286423 -4.4286103 -4.4285941 -4.4286075 -4.4286237 -4.4286313 -4.428617 -4.4285979][-4.42855 -4.4285755 -4.428555 -4.4285517 -4.4285593 -4.4285703 -4.4285822 -4.4285827 -4.4285593 -4.428544 -4.4285522 -4.4285445 -4.4285369 -4.4285245 -4.42852][-4.4284678 -4.4285021 -4.4284811 -4.4284625 -4.4284687 -4.4284778 -4.42849 -4.4284954 -4.4284735 -4.4284663 -4.4284821 -4.4284668 -4.4284568 -4.4284472 -4.4284439][-4.4284511 -4.4284787 -4.4284506 -4.4284272 -4.4284229 -4.42841 -4.4283948 -4.4283919 -4.4283895 -4.4283962 -4.4284096 -4.4283834 -4.4283705 -4.4283748 -4.4283795][-4.42847 -4.4284754 -4.4284348 -4.428391 -4.4283414 -4.4282613 -4.428185 -4.4281926 -4.4282532 -4.4282885 -4.4283085 -4.4282861 -4.4282713 -4.4282923 -4.4283147][-4.4285078 -4.4284792 -4.4284139 -4.4283419 -4.4282374 -4.4280806 -4.4279451 -4.4279838 -4.4281125 -4.4281788 -4.428216 -4.4282193 -4.4282284 -4.428288 -4.4283495][-4.4285922 -4.4285421 -4.4284716 -4.4283919 -4.4282737 -4.4281125 -4.4279923 -4.4280519 -4.4281816 -4.4282384 -4.4282756 -4.4283085 -4.428349 -4.4284344 -4.4285221][-4.4286828 -4.4286308 -4.428575 -4.4285178 -4.4284325 -4.4283304 -4.4282651 -4.4283109 -4.42839 -4.428421 -4.4284406 -4.4284768 -4.4285192 -4.4285932 -4.4286728][-4.4287572 -4.4287162 -4.4286714 -4.4286313 -4.4285817 -4.4285355 -4.4285107 -4.4285388 -4.4285803 -4.4285951 -4.4286 -4.4286227 -4.4286461 -4.4286914 -4.4287462][-4.4288225 -4.428791 -4.4287562 -4.4287276 -4.4287019 -4.4286838 -4.4286752 -4.4286871 -4.4287047 -4.4287133 -4.4287157 -4.4287262 -4.4287324 -4.4287543 -4.4287863][-4.4288683 -4.4288397 -4.4288163 -4.4288058 -4.4288 -4.4287906 -4.4287791 -4.4287739 -4.4287767 -4.4287829 -4.4287782 -4.4287748 -4.4287643 -4.4287672 -4.4287829][-4.4289045 -4.4288821 -4.4288673 -4.4288664 -4.4288688 -4.4288626 -4.4288526 -4.428844 -4.4288406 -4.42884 -4.4288259 -4.4288068 -4.4287767 -4.4287615 -4.4287639][-4.4289393 -4.4289246 -4.428916 -4.4289155 -4.4289169 -4.4289122 -4.4289079 -4.4289045 -4.4289045 -4.4289036 -4.42889 -4.4288673 -4.4288278 -4.4287972 -4.4287829]]...]
INFO - root - 2017-12-10 05:32:49.065639: step 4410, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:22m:11s remains)
INFO - root - 2017-12-10 05:32:51.702892: step 4420, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:58m:17s remains)
INFO - root - 2017-12-10 05:32:54.383497: step 4430, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:26m:10s remains)
INFO - root - 2017-12-10 05:32:57.019918: step 4440, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:17m:12s remains)
INFO - root - 2017-12-10 05:32:59.660233: step 4450, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:17m:25s remains)
INFO - root - 2017-12-10 05:33:02.309177: step 4460, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:13m:06s remains)
INFO - root - 2017-12-10 05:33:04.979987: step 4470, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:53m:33s remains)
INFO - root - 2017-12-10 05:33:07.648066: step 4480, loss = 2.28, batch loss = 2.23 (27.2 examples/sec; 0.294 sec/batch; 26h:45m:25s remains)
INFO - root - 2017-12-10 05:33:10.212224: step 4490, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.253 sec/batch; 23h:00m:57s remains)
INFO - root - 2017-12-10 05:33:12.854816: step 4500, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:31m:58s remains)
2017-12-10 05:33:13.133528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288626 -4.4288826 -4.4289064 -4.4289331 -4.4289403 -4.4289289 -4.4288893 -4.4288507 -4.4288464 -4.4288721 -4.428894 -4.4289126 -4.42893 -4.4289494 -4.428956][-4.428853 -4.42886 -4.4288774 -4.4289131 -4.428937 -4.4289351 -4.4289036 -4.4288645 -4.4288511 -4.4288683 -4.4288821 -4.4288931 -4.4289007 -4.4289055 -4.4289007][-4.4288321 -4.4288235 -4.4288259 -4.4288564 -4.4288788 -4.4288759 -4.4288507 -4.4288216 -4.428823 -4.4288478 -4.4288573 -4.4288583 -4.428853 -4.4288368 -4.428813][-4.428812 -4.4287853 -4.428761 -4.4287677 -4.4287734 -4.4287615 -4.4287395 -4.4287205 -4.4287386 -4.4287848 -4.4288044 -4.4288058 -4.4287915 -4.4287548 -4.4287181][-4.4287963 -4.4287477 -4.4286919 -4.4286661 -4.4286442 -4.428618 -4.4285946 -4.4285746 -4.4286032 -4.4286838 -4.4287395 -4.4287615 -4.4287553 -4.428719 -4.4286838][-4.42878 -4.4287081 -4.4286227 -4.4285631 -4.4285116 -4.4284782 -4.428453 -4.4284158 -4.4284458 -4.4285722 -4.42868 -4.4287381 -4.4287591 -4.4287448 -4.4287252][-4.4287634 -4.428689 -4.4285984 -4.42851 -4.428431 -4.4283819 -4.4283357 -4.4282608 -4.4282775 -4.4284525 -4.428616 -4.4287128 -4.4287629 -4.4287763 -4.4287677][-4.4287624 -4.4287119 -4.4286413 -4.4285398 -4.4284472 -4.4283895 -4.4283209 -4.4282055 -4.4281974 -4.4283948 -4.4285851 -4.4286919 -4.4287519 -4.4287915 -4.4287982][-4.4287786 -4.4287639 -4.4287224 -4.4286404 -4.4285593 -4.4285097 -4.4284468 -4.4283452 -4.4283342 -4.4284916 -4.4286418 -4.4287157 -4.428761 -4.4288049 -4.4288192][-4.4288006 -4.4288149 -4.428803 -4.4287524 -4.4286952 -4.4286571 -4.428606 -4.4285336 -4.4285197 -4.4286165 -4.4287095 -4.4287491 -4.4287653 -4.4287944 -4.4288158][-4.4288225 -4.4288516 -4.4288621 -4.4288344 -4.4287891 -4.4287519 -4.4287105 -4.4286594 -4.4286451 -4.4286985 -4.4287519 -4.4287748 -4.4287753 -4.4287882 -4.42881][-4.4288292 -4.4288588 -4.4288826 -4.4288764 -4.4288511 -4.4288244 -4.4287944 -4.4287562 -4.4287429 -4.42877 -4.428802 -4.4288177 -4.42881 -4.42881 -4.4288192][-4.4288325 -4.4288554 -4.4288859 -4.4288979 -4.4288898 -4.4288726 -4.4288549 -4.4288344 -4.4288268 -4.4288416 -4.428863 -4.4288745 -4.428864 -4.428854 -4.4288497][-4.4288416 -4.428863 -4.4288983 -4.4289217 -4.4289269 -4.4289193 -4.4289093 -4.4289021 -4.4289021 -4.4289126 -4.4289255 -4.4289331 -4.4289188 -4.4289026 -4.4288807][-4.4288559 -4.4288797 -4.4289136 -4.4289403 -4.4289527 -4.4289522 -4.42895 -4.4289527 -4.4289556 -4.4289603 -4.4289651 -4.4289694 -4.4289551 -4.4289351 -4.4289031]]...]
INFO - root - 2017-12-10 05:33:15.819987: step 4510, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:31m:03s remains)
INFO - root - 2017-12-10 05:33:18.490502: step 4520, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:43m:16s remains)
INFO - root - 2017-12-10 05:33:21.236092: step 4530, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 25h:19m:05s remains)
INFO - root - 2017-12-10 05:33:23.886347: step 4540, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:42m:36s remains)
INFO - root - 2017-12-10 05:33:26.518420: step 4550, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:49m:17s remains)
INFO - root - 2017-12-10 05:33:29.145027: step 4560, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:01m:23s remains)
INFO - root - 2017-12-10 05:33:31.774996: step 4570, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:36m:37s remains)
INFO - root - 2017-12-10 05:33:34.430778: step 4580, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:14m:37s remains)
INFO - root - 2017-12-10 05:33:37.098797: step 4590, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:36m:08s remains)
INFO - root - 2017-12-10 05:33:39.784224: step 4600, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:31m:30s remains)
2017-12-10 05:33:40.056767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288821 -4.42888 -4.4288821 -4.428885 -4.4288883 -4.4288888 -4.4288869 -4.4288855 -4.4288788 -4.4288712 -4.4288697 -4.428865 -4.4288597 -4.428854 -4.4288445][-4.4288883 -4.4288845 -4.4288883 -4.428894 -4.4288993 -4.4288993 -4.428896 -4.4288926 -4.4288793 -4.428863 -4.4288511 -4.4288397 -4.4288316 -4.4288287 -4.428823][-4.4288812 -4.4288754 -4.4288783 -4.4288859 -4.4288936 -4.428895 -4.4288907 -4.4288888 -4.42888 -4.428864 -4.4288473 -4.4288349 -4.4288311 -4.4288344 -4.4288349][-4.4288626 -4.4288497 -4.4288497 -4.4288564 -4.428865 -4.4288692 -4.4288707 -4.4288769 -4.4288788 -4.4288692 -4.4288535 -4.428843 -4.4288425 -4.42885 -4.4288549][-4.42886 -4.4288335 -4.4288206 -4.4288168 -4.4288149 -4.4288154 -4.4288211 -4.42884 -4.4288621 -4.4288697 -4.4288673 -4.4288635 -4.428864 -4.4288683 -4.4288692][-4.4288096 -4.4287691 -4.4287424 -4.4287257 -4.42871 -4.4286938 -4.4286923 -4.42872 -4.4287624 -4.4287934 -4.4288144 -4.4288325 -4.4288473 -4.4288592 -4.428863][-4.4287095 -4.4286523 -4.4286094 -4.428575 -4.4285369 -4.4284921 -4.42847 -4.4284964 -4.4285545 -4.4286132 -4.4286666 -4.4287143 -4.4287519 -4.428781 -4.4288011][-4.4287019 -4.428638 -4.4285851 -4.4285345 -4.4284716 -4.4283929 -4.4283342 -4.4283319 -4.4283781 -4.4284458 -4.4285235 -4.4285955 -4.4286528 -4.4286952 -4.4287286][-4.4287691 -4.4287181 -4.4286771 -4.42864 -4.4285874 -4.4285188 -4.4284577 -4.4284315 -4.4284406 -4.4284773 -4.4285355 -4.428597 -4.4286475 -4.4286871 -4.428719][-4.4288359 -4.4287982 -4.4287672 -4.4287453 -4.4287167 -4.428679 -4.42864 -4.4286137 -4.4286094 -4.4286284 -4.4286666 -4.4287076 -4.4287381 -4.4287577 -4.4287724][-4.4288774 -4.4288545 -4.4288368 -4.4288268 -4.4288163 -4.428802 -4.4287829 -4.4287639 -4.4287577 -4.4287705 -4.4287953 -4.4288187 -4.4288335 -4.4288368 -4.4288335][-4.4289112 -4.4288983 -4.4288926 -4.4288926 -4.4288945 -4.4288917 -4.428885 -4.428875 -4.4288697 -4.4288778 -4.4288931 -4.4289017 -4.4289036 -4.4288955 -4.4288797][-4.4289212 -4.4289145 -4.4289174 -4.4289255 -4.4289355 -4.4289412 -4.4289441 -4.4289451 -4.4289384 -4.4289346 -4.4289341 -4.4289289 -4.4289236 -4.4289117 -4.4288936][-4.4288936 -4.4288921 -4.4289002 -4.4289126 -4.428925 -4.4289327 -4.4289389 -4.4289455 -4.4289346 -4.4289155 -4.4289007 -4.4288859 -4.4288821 -4.4288783 -4.4288678][-4.4288521 -4.4288521 -4.4288611 -4.4288735 -4.4288836 -4.42889 -4.4288955 -4.4289 -4.4288793 -4.4288468 -4.428822 -4.4288096 -4.4288173 -4.4288316 -4.4288349]]...]
INFO - root - 2017-12-10 05:33:42.705914: step 4610, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.285 sec/batch; 25h:59m:13s remains)
INFO - root - 2017-12-10 05:33:45.337145: step 4620, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:39m:08s remains)
INFO - root - 2017-12-10 05:33:47.992912: step 4630, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:17m:36s remains)
INFO - root - 2017-12-10 05:33:50.685020: step 4640, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:45m:15s remains)
INFO - root - 2017-12-10 05:33:53.346465: step 4650, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:33m:32s remains)
INFO - root - 2017-12-10 05:33:56.064870: step 4660, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:06m:05s remains)
INFO - root - 2017-12-10 05:33:58.684099: step 4670, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:12m:36s remains)
INFO - root - 2017-12-10 05:34:01.309481: step 4680, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:25m:50s remains)
INFO - root - 2017-12-10 05:34:03.907002: step 4690, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:33m:42s remains)
INFO - root - 2017-12-10 05:34:06.560907: step 4700, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:36m:25s remains)
2017-12-10 05:34:06.880503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428947 -4.4289527 -4.4289536 -4.4289551 -4.4289541 -4.4289436 -4.4289274 -4.4289141 -4.4289 -4.4288912 -4.4288883 -4.4288845 -4.4288726 -4.4288511 -4.4288254][-4.42896 -4.4289651 -4.4289665 -4.4289656 -4.4289594 -4.4289412 -4.4289217 -4.4289103 -4.4289002 -4.4288893 -4.4288793 -4.4288692 -4.4288554 -4.4288363 -4.4288154][-4.4289455 -4.4289584 -4.4289665 -4.4289651 -4.4289556 -4.4289322 -4.4289107 -4.4289012 -4.4288945 -4.4288826 -4.4288654 -4.428843 -4.4288239 -4.4288082 -4.4287963][-4.4289236 -4.4289465 -4.4289618 -4.4289584 -4.4289412 -4.4289126 -4.4288912 -4.4288807 -4.428875 -4.4288664 -4.4288497 -4.42882 -4.428793 -4.4287744 -4.4287643][-4.428894 -4.4289222 -4.4289374 -4.4289231 -4.4288931 -4.4288611 -4.428844 -4.4288397 -4.4288373 -4.4288349 -4.4288211 -4.4287868 -4.4287529 -4.4287348 -4.4287186][-4.4288449 -4.4288683 -4.4288754 -4.4288478 -4.4288034 -4.4287639 -4.42876 -4.42878 -4.4287944 -4.4287972 -4.428782 -4.4287486 -4.4287181 -4.42871 -4.4286947][-4.4287786 -4.4287858 -4.42878 -4.4287381 -4.4286695 -4.4286084 -4.4286203 -4.428678 -4.4287167 -4.4287248 -4.4287162 -4.4286962 -4.428688 -4.4287057 -4.4287138][-4.4287362 -4.4287205 -4.4286938 -4.4286337 -4.428534 -4.4284382 -4.428453 -4.4285483 -4.4286222 -4.4286509 -4.4286566 -4.428648 -4.4286513 -4.4286923 -4.4287372][-4.4287467 -4.4287195 -4.4286842 -4.4286232 -4.4285154 -4.4283991 -4.4283986 -4.4284964 -4.4285851 -4.428638 -4.4286609 -4.4286513 -4.4286442 -4.4286814 -4.4287415][-4.4287825 -4.4287586 -4.4287314 -4.4286952 -4.4286208 -4.42853 -4.4285188 -4.4285817 -4.4286551 -4.4287109 -4.4287376 -4.428721 -4.4286928 -4.4286976 -4.4287353][-4.4288106 -4.4288006 -4.4287891 -4.4287763 -4.4287419 -4.4286909 -4.4286838 -4.4287167 -4.4287624 -4.4288049 -4.4288235 -4.4288025 -4.42876 -4.42873 -4.4287271][-4.4288125 -4.4288211 -4.4288321 -4.4288411 -4.4288335 -4.4288111 -4.4288149 -4.428834 -4.4288559 -4.4288778 -4.4288826 -4.4288592 -4.4288111 -4.4287634 -4.4287291][-4.4288077 -4.4288282 -4.42886 -4.428884 -4.4288859 -4.4288669 -4.4288664 -4.4288778 -4.4288888 -4.4289002 -4.4289017 -4.4288836 -4.4288445 -4.4287958 -4.4287477][-4.428833 -4.4288573 -4.4288969 -4.4289179 -4.4289103 -4.4288783 -4.4288664 -4.4288764 -4.4288893 -4.4289045 -4.4289083 -4.428894 -4.4288659 -4.4288273 -4.4287791][-4.4288907 -4.4289107 -4.4289408 -4.4289474 -4.4289231 -4.4288712 -4.4288406 -4.4288416 -4.4288559 -4.4288831 -4.4289012 -4.4288974 -4.4288807 -4.4288545 -4.4288139]]...]
INFO - root - 2017-12-10 05:34:09.551267: step 4710, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:25m:22s remains)
INFO - root - 2017-12-10 05:34:12.155704: step 4720, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:13m:39s remains)
INFO - root - 2017-12-10 05:34:14.794212: step 4730, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:49m:59s remains)
INFO - root - 2017-12-10 05:34:17.460813: step 4740, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:11m:44s remains)
INFO - root - 2017-12-10 05:34:20.123681: step 4750, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:31m:33s remains)
INFO - root - 2017-12-10 05:34:22.756021: step 4760, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:10m:56s remains)
INFO - root - 2017-12-10 05:34:25.389780: step 4770, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:22m:15s remains)
INFO - root - 2017-12-10 05:34:28.051272: step 4780, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:31m:07s remains)
INFO - root - 2017-12-10 05:34:30.681416: step 4790, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:57m:07s remains)
INFO - root - 2017-12-10 05:34:33.312824: step 4800, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:54m:28s remains)
2017-12-10 05:34:33.604621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288912 -4.428885 -4.428874 -4.4288664 -4.4288688 -4.4288797 -4.4288974 -4.4289184 -4.4289427 -4.4289646 -4.42897 -4.428957 -4.4289384 -4.4289346 -4.4289517][-4.4288135 -4.42882 -4.4288216 -4.4288197 -4.4288206 -4.4288278 -4.4288411 -4.4288616 -4.4288936 -4.4289274 -4.4289446 -4.4289408 -4.4289265 -4.4289231 -4.4289403][-4.4287348 -4.4287572 -4.4287744 -4.4287791 -4.4287744 -4.4287715 -4.4287734 -4.4287882 -4.4288254 -4.4288707 -4.4289036 -4.4289131 -4.4289079 -4.4289088 -4.4289274][-4.4286785 -4.4287128 -4.42874 -4.4287467 -4.4287314 -4.4287119 -4.4286995 -4.4287028 -4.4287419 -4.4287972 -4.4288464 -4.4288731 -4.4288812 -4.4288888 -4.42891][-4.4286542 -4.42869 -4.4287133 -4.4287148 -4.4286895 -4.4286532 -4.4286208 -4.4286094 -4.4286523 -4.4287205 -4.428781 -4.4288225 -4.4288449 -4.42886 -4.4288826][-4.4286766 -4.4287024 -4.4287057 -4.4286838 -4.4286389 -4.4285784 -4.4285111 -4.4284768 -4.4285359 -4.4286346 -4.4287152 -4.4287682 -4.4288006 -4.4288197 -4.4288406][-4.4287338 -4.4287415 -4.4287176 -4.4286628 -4.4285903 -4.4284997 -4.4283867 -4.428319 -4.4283948 -4.4285412 -4.4286537 -4.4287162 -4.4287491 -4.4287648 -4.4287815][-4.4288015 -4.4287987 -4.4287591 -4.4286857 -4.4285917 -4.4284759 -4.4283218 -4.4282079 -4.4282718 -4.4284477 -4.4285908 -4.4286628 -4.4286938 -4.4287062 -4.4287229][-4.4288611 -4.4288573 -4.4288158 -4.4287434 -4.4286456 -4.4285226 -4.42836 -4.4282227 -4.42825 -4.4284034 -4.4285421 -4.4286108 -4.4286385 -4.4286556 -4.4286804][-4.4289041 -4.428905 -4.4288692 -4.4288063 -4.4287238 -4.428618 -4.4284797 -4.4283552 -4.4283462 -4.4284439 -4.4285445 -4.4285941 -4.4286146 -4.4286327 -4.4286637][-4.4289269 -4.4289317 -4.42891 -4.428864 -4.428803 -4.4287229 -4.4286218 -4.4285331 -4.4285097 -4.4285574 -4.428616 -4.4286442 -4.4286532 -4.4286618 -4.428679][-4.4289346 -4.4289474 -4.4289403 -4.42891 -4.428864 -4.4288073 -4.428741 -4.4286904 -4.4286752 -4.4286957 -4.4287281 -4.4287443 -4.4287443 -4.4287429 -4.4287395][-4.4289207 -4.4289417 -4.4289465 -4.4289308 -4.4289007 -4.4288664 -4.4288316 -4.4288135 -4.4288144 -4.4288225 -4.4288373 -4.4288425 -4.4288373 -4.4288349 -4.428823][-4.4288826 -4.4289064 -4.4289193 -4.4289136 -4.4288974 -4.4288859 -4.4288836 -4.428894 -4.4289074 -4.4289122 -4.4289145 -4.4289088 -4.4288993 -4.4289 -4.4288926][-4.4288397 -4.4288568 -4.4288745 -4.4288764 -4.4288712 -4.4288745 -4.428894 -4.4289231 -4.4289441 -4.4289517 -4.4289489 -4.4289379 -4.4289289 -4.4289322 -4.4289331]]...]
INFO - root - 2017-12-10 05:34:36.226775: step 4810, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:28m:09s remains)
INFO - root - 2017-12-10 05:34:38.892976: step 4820, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:04m:10s remains)
INFO - root - 2017-12-10 05:34:41.583248: step 4830, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:46m:41s remains)
INFO - root - 2017-12-10 05:34:44.157244: step 4840, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:49m:59s remains)
INFO - root - 2017-12-10 05:34:46.826945: step 4850, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 26h:02m:09s remains)
INFO - root - 2017-12-10 05:34:49.423209: step 4860, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:09m:09s remains)
INFO - root - 2017-12-10 05:34:52.069837: step 4870, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:27m:57s remains)
INFO - root - 2017-12-10 05:34:54.697971: step 4880, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:37m:03s remains)
INFO - root - 2017-12-10 05:34:57.353374: step 4890, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:45m:03s remains)
INFO - root - 2017-12-10 05:34:59.990207: step 4900, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:00m:37s remains)
2017-12-10 05:35:00.304914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288898 -4.4288816 -4.4288712 -4.4288592 -4.4288349 -4.4288106 -4.4287934 -4.428792 -4.42881 -4.428834 -4.4288564 -4.4288688 -4.4288726 -4.4288759 -4.4288788][-4.4288406 -4.4288278 -4.4288244 -4.4288211 -4.4288006 -4.4287758 -4.4287615 -4.4287643 -4.428782 -4.4288 -4.4288187 -4.4288297 -4.4288349 -4.4288445 -4.4288507][-4.4288158 -4.4288049 -4.4288106 -4.4288144 -4.4287944 -4.4287615 -4.4287367 -4.4287329 -4.4287448 -4.4287553 -4.4287691 -4.4287791 -4.4287896 -4.4288082 -4.4288206][-4.4288149 -4.4288068 -4.4288154 -4.4288182 -4.4287949 -4.4287539 -4.4287105 -4.4286914 -4.4286985 -4.4287143 -4.4287324 -4.4287472 -4.4287634 -4.4287896 -4.4288039][-4.4288044 -4.4287891 -4.4287939 -4.4287977 -4.4287772 -4.4287286 -4.4286704 -4.4286385 -4.4286456 -4.4286752 -4.4287109 -4.4287419 -4.4287682 -4.4287958 -4.4288015][-4.4287682 -4.4287376 -4.4287381 -4.4287472 -4.4287372 -4.428688 -4.4286208 -4.4285774 -4.4285865 -4.4286327 -4.4286842 -4.4287252 -4.4287615 -4.4287887 -4.4287882][-4.4287262 -4.4286819 -4.4286733 -4.4286895 -4.4286952 -4.4286604 -4.4286017 -4.4285555 -4.428565 -4.4286103 -4.4286551 -4.4286985 -4.4287419 -4.4287691 -4.4287682][-4.4287529 -4.4287043 -4.4286842 -4.4286947 -4.4287114 -4.4287 -4.4286604 -4.428618 -4.4286132 -4.4286342 -4.4286551 -4.4286857 -4.4287286 -4.4287596 -4.4287629][-4.4288092 -4.4287653 -4.4287395 -4.42874 -4.4287581 -4.428761 -4.4287386 -4.4286971 -4.4286795 -4.4286823 -4.4286904 -4.4287047 -4.4287329 -4.4287548 -4.4287615][-4.4288669 -4.4288373 -4.4288149 -4.428812 -4.428822 -4.4288292 -4.4288192 -4.4287891 -4.4287648 -4.4287605 -4.42876 -4.4287596 -4.4287591 -4.4287591 -4.4287567][-4.4289289 -4.4289131 -4.4288974 -4.4288926 -4.4288921 -4.4288926 -4.4288855 -4.4288688 -4.428853 -4.4288464 -4.4288421 -4.4288321 -4.4288135 -4.4287958 -4.4287772][-4.4289536 -4.4289432 -4.4289312 -4.4289279 -4.4289274 -4.4289255 -4.4289203 -4.4289145 -4.4289069 -4.4289012 -4.4288898 -4.4288726 -4.428853 -4.42883 -4.4287996][-4.4289508 -4.4289312 -4.4289136 -4.4289126 -4.4289155 -4.4289112 -4.4289107 -4.4289193 -4.4289222 -4.4289188 -4.4289079 -4.428894 -4.4288793 -4.4288621 -4.428834][-4.4289165 -4.4288979 -4.4288831 -4.4288845 -4.4288888 -4.4288826 -4.4288788 -4.4288893 -4.4289031 -4.4289083 -4.4289045 -4.4289026 -4.428895 -4.4288797 -4.4288516][-4.4288898 -4.428875 -4.4288683 -4.4288764 -4.4288845 -4.4288783 -4.4288683 -4.4288692 -4.428884 -4.4288974 -4.428905 -4.4289074 -4.4289002 -4.4288893 -4.4288626]]...]
INFO - root - 2017-12-10 05:35:02.949371: step 4910, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:53m:48s remains)
INFO - root - 2017-12-10 05:35:05.609095: step 4920, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:44m:07s remains)
INFO - root - 2017-12-10 05:35:08.284567: step 4930, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:09m:33s remains)
INFO - root - 2017-12-10 05:35:10.961450: step 4940, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:26m:12s remains)
INFO - root - 2017-12-10 05:35:13.602457: step 4950, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:11m:41s remains)
INFO - root - 2017-12-10 05:35:16.274753: step 4960, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:35m:08s remains)
INFO - root - 2017-12-10 05:35:18.989900: step 4970, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:17m:00s remains)
INFO - root - 2017-12-10 05:35:21.717353: step 4980, loss = 2.28, batch loss = 2.23 (28.3 examples/sec; 0.282 sec/batch; 25h:41m:42s remains)
INFO - root - 2017-12-10 05:35:24.395006: step 4990, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:00m:44s remains)
INFO - root - 2017-12-10 05:35:27.055617: step 5000, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:47m:53s remains)
2017-12-10 05:35:27.357266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289489 -4.4288707 -4.4287925 -4.4287391 -4.4287205 -4.4287143 -4.4287114 -4.4287262 -4.428741 -4.4287338 -4.4287376 -4.4287333 -4.428731 -4.4287367 -4.4287314][-4.4289489 -4.42888 -4.4288082 -4.4287639 -4.4287434 -4.4287243 -4.4286871 -4.4286709 -4.4286714 -4.4286718 -4.4287004 -4.4287214 -4.4287248 -4.4287233 -4.4287][-4.4289408 -4.4288759 -4.4288139 -4.42878 -4.4287653 -4.4287405 -4.4286823 -4.4286366 -4.4286203 -4.4286318 -4.4286823 -4.4287176 -4.4287148 -4.4287047 -4.428669][-4.4289293 -4.4288578 -4.4287968 -4.4287677 -4.4287596 -4.4287395 -4.428678 -4.4286203 -4.4286017 -4.4286227 -4.4286809 -4.4287162 -4.4286962 -4.4286766 -4.4286494][-4.428916 -4.4288306 -4.4287577 -4.4287214 -4.4287171 -4.4287119 -4.428678 -4.4286447 -4.4286385 -4.4286666 -4.4287181 -4.4287376 -4.4287019 -4.4286742 -4.4286633][-4.4289026 -4.4287992 -4.4287009 -4.4286366 -4.4286242 -4.4286327 -4.4286404 -4.42865 -4.42867 -4.4287081 -4.4287515 -4.4287605 -4.4287233 -4.4286971 -4.4287028][-4.4288993 -4.428781 -4.42865 -4.428545 -4.4285073 -4.4285173 -4.4285583 -4.4286203 -4.4286752 -4.4287291 -4.4287748 -4.4287791 -4.4287491 -4.428731 -4.4287481][-4.4289074 -4.4287939 -4.4286523 -4.4285212 -4.4284463 -4.4284277 -4.4284792 -4.4285727 -4.4286561 -4.4287219 -4.4287677 -4.4287758 -4.4287539 -4.4287372 -4.4287567][-4.4289169 -4.4288259 -4.4287143 -4.42861 -4.4285316 -4.4284911 -4.4285235 -4.428606 -4.4286814 -4.4287324 -4.4287696 -4.4287791 -4.4287596 -4.4287424 -4.4287567][-4.4289241 -4.4288573 -4.4287872 -4.4287286 -4.428679 -4.42864 -4.4286556 -4.4287143 -4.4287648 -4.4287891 -4.4288077 -4.428812 -4.4287896 -4.4287624 -4.4287667][-4.42893 -4.4288845 -4.4288445 -4.4288187 -4.428793 -4.4287686 -4.4287767 -4.4288139 -4.4288435 -4.4288454 -4.4288464 -4.4288497 -4.4288306 -4.4288054 -4.4288058][-4.4289322 -4.4288964 -4.4288673 -4.428854 -4.42884 -4.4288254 -4.4288287 -4.4288549 -4.4288764 -4.4288721 -4.4288678 -4.4288669 -4.4288487 -4.4288292 -4.4288316][-4.4289236 -4.4288812 -4.4288468 -4.4288297 -4.4288144 -4.428803 -4.4287992 -4.4288149 -4.4288373 -4.4288411 -4.4288359 -4.4288321 -4.4288197 -4.4288125 -4.4288187][-4.4289126 -4.4288507 -4.4287963 -4.428762 -4.4287343 -4.4287171 -4.4287081 -4.4287162 -4.4287381 -4.428751 -4.4287534 -4.4287524 -4.428751 -4.4287581 -4.42877][-4.4289069 -4.4288297 -4.4287496 -4.4286866 -4.42864 -4.4286165 -4.4286075 -4.4286137 -4.4286308 -4.4286461 -4.428659 -4.4286666 -4.4286776 -4.4286966 -4.4287128]]...]
INFO - root - 2017-12-10 05:35:30.028661: step 5010, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:13m:42s remains)
INFO - root - 2017-12-10 05:35:32.659045: step 5020, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:29m:52s remains)
INFO - root - 2017-12-10 05:35:35.307817: step 5030, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:01m:36s remains)
INFO - root - 2017-12-10 05:35:37.953643: step 5040, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:09m:33s remains)
INFO - root - 2017-12-10 05:35:40.620371: step 5050, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.271 sec/batch; 24h:36m:26s remains)
INFO - root - 2017-12-10 05:35:43.264982: step 5060, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:56m:40s remains)
INFO - root - 2017-12-10 05:35:45.940782: step 5070, loss = 2.28, batch loss = 2.23 (28.2 examples/sec; 0.284 sec/batch; 25h:49m:15s remains)
INFO - root - 2017-12-10 05:35:48.619307: step 5080, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 25h:02m:35s remains)
INFO - root - 2017-12-10 05:35:51.271226: step 5090, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:29m:31s remains)
INFO - root - 2017-12-10 05:35:53.917466: step 5100, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:23m:17s remains)
2017-12-10 05:35:54.218282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286661 -4.4286952 -4.4287024 -4.4286866 -4.4287009 -4.4287395 -4.4287572 -4.4287515 -4.4287424 -4.4287496 -4.4287872 -4.428834 -4.4288397 -4.4287848 -4.4287276][-4.4286437 -4.4286613 -4.4286776 -4.4286828 -4.4287009 -4.4287295 -4.4287419 -4.4287338 -4.4287248 -4.4287343 -4.4287825 -4.428843 -4.4288573 -4.4287992 -4.4287348][-4.4287295 -4.4287353 -4.4287443 -4.428751 -4.4287562 -4.4287553 -4.4287405 -4.4287186 -4.4287109 -4.4287276 -4.4287882 -4.4288607 -4.428885 -4.4288392 -4.4287839][-4.4287982 -4.428793 -4.4287877 -4.4287853 -4.4287748 -4.42875 -4.4287128 -4.4286857 -4.4286914 -4.4287214 -4.4287891 -4.4288611 -4.4288898 -4.4288645 -4.4288297][-4.4288211 -4.4288163 -4.4287982 -4.4287729 -4.4287324 -4.428679 -4.4286222 -4.4285955 -4.4286232 -4.42867 -4.4287391 -4.428813 -4.4288483 -4.4288487 -4.4288397][-4.4287953 -4.4287977 -4.428771 -4.4287186 -4.4286389 -4.4285431 -4.42845 -4.4284158 -4.4284749 -4.4285545 -4.4286366 -4.4287214 -4.4287753 -4.428802 -4.4288211][-4.4287448 -4.4287434 -4.4287062 -4.428618 -4.42849 -4.4283381 -4.4281969 -4.42815 -4.4282608 -4.4284015 -4.4285169 -4.4286189 -4.4286976 -4.4287534 -4.4287949][-4.42871 -4.4287062 -4.4286671 -4.428565 -4.4284196 -4.4282494 -4.4280963 -4.42806 -4.4282064 -4.4283805 -4.4285097 -4.4286089 -4.4286885 -4.4287438 -4.4287853][-4.4287252 -4.428721 -4.4286919 -4.428607 -4.4284906 -4.4283619 -4.4282627 -4.4282675 -4.4283996 -4.4285407 -4.4286346 -4.4286966 -4.4287419 -4.4287639 -4.4287853][-4.428761 -4.4287558 -4.4287386 -4.4286795 -4.4286003 -4.4285188 -4.4284654 -4.4284878 -4.4285841 -4.4286823 -4.4287376 -4.4287629 -4.4287748 -4.4287686 -4.4287715][-4.4287891 -4.428792 -4.4287848 -4.4287448 -4.4286904 -4.4286413 -4.4286222 -4.4286518 -4.4287152 -4.4287767 -4.4288025 -4.428803 -4.4287896 -4.4287639 -4.4287534][-4.4288092 -4.4288173 -4.4288206 -4.4288 -4.4287672 -4.4287443 -4.4287472 -4.4287724 -4.4288082 -4.4288359 -4.4288383 -4.4288211 -4.4287968 -4.4287653 -4.4287486][-4.4288583 -4.428865 -4.4288697 -4.428863 -4.4288454 -4.428834 -4.42884 -4.4288535 -4.4288659 -4.4288659 -4.4288535 -4.42883 -4.4288049 -4.4287753 -4.4287586][-4.4289317 -4.4289365 -4.42894 -4.4289346 -4.4289222 -4.4289122 -4.4289117 -4.4289126 -4.4289103 -4.4289 -4.428884 -4.428864 -4.4288454 -4.4288216 -4.4288049][-4.4289737 -4.4289794 -4.4289823 -4.4289794 -4.4289737 -4.4289689 -4.4289684 -4.428967 -4.4289613 -4.4289513 -4.4289393 -4.4289274 -4.4289169 -4.4289036 -4.4288917]]...]
INFO - root - 2017-12-10 05:35:56.824484: step 5110, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:25m:43s remains)
INFO - root - 2017-12-10 05:35:59.543353: step 5120, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:10m:49s remains)
INFO - root - 2017-12-10 05:36:02.252405: step 5130, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:12m:32s remains)
INFO - root - 2017-12-10 05:36:04.863979: step 5140, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:08m:15s remains)
INFO - root - 2017-12-10 05:36:07.567572: step 5150, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:01m:40s remains)
INFO - root - 2017-12-10 05:36:10.243935: step 5160, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:19m:11s remains)
INFO - root - 2017-12-10 05:36:12.866273: step 5170, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:31m:38s remains)
INFO - root - 2017-12-10 05:36:15.518493: step 5180, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 25h:15m:04s remains)
INFO - root - 2017-12-10 05:36:18.167645: step 5190, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:43m:18s remains)
INFO - root - 2017-12-10 05:36:20.807559: step 5200, loss = 2.28, batch loss = 2.23 (28.3 examples/sec; 0.283 sec/batch; 25h:43m:22s remains)
2017-12-10 05:36:21.134363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289489 -4.4289494 -4.4289403 -4.4289255 -4.4289145 -4.4289117 -4.4289188 -4.4289312 -4.4289422 -4.4289489 -4.4289527 -4.4289541 -4.4289532 -4.4289479 -4.4289432][-4.4289179 -4.4289312 -4.428925 -4.4289021 -4.4288783 -4.4288678 -4.4288797 -4.428905 -4.4289303 -4.4289479 -4.428956 -4.4289556 -4.4289536 -4.42895 -4.4289474][-4.4288497 -4.4288864 -4.4288816 -4.4288445 -4.428802 -4.4287839 -4.4288063 -4.4288549 -4.4289041 -4.4289403 -4.4289584 -4.4289613 -4.4289589 -4.4289551 -4.4289532][-4.4287381 -4.4288187 -4.4288211 -4.4287577 -4.4286752 -4.4286332 -4.4286661 -4.4287491 -4.4288406 -4.4289117 -4.4289503 -4.4289622 -4.4289646 -4.4289627 -4.4289603][-4.4286232 -4.4287543 -4.4287734 -4.4286823 -4.4285378 -4.4284344 -4.4284554 -4.4285779 -4.4287257 -4.4288507 -4.4289246 -4.4289517 -4.4289613 -4.4289641 -4.4289651][-4.4285593 -4.4287286 -4.428771 -4.4286709 -4.4284749 -4.4282846 -4.4282475 -4.4283872 -4.4285841 -4.428762 -4.4288807 -4.4289327 -4.4289494 -4.4289541 -4.42896][-4.4286017 -4.4287558 -4.4288087 -4.4287257 -4.428535 -4.4283094 -4.4281912 -4.4282842 -4.4284849 -4.4286866 -4.4288354 -4.4289107 -4.4289346 -4.4289389 -4.428946][-4.4287205 -4.4288182 -4.4288521 -4.428793 -4.4286537 -4.4284863 -4.4283714 -4.4283853 -4.4285088 -4.4286718 -4.4288116 -4.428895 -4.4289274 -4.4289308 -4.4289351][-4.428844 -4.428885 -4.4288816 -4.4288282 -4.4287405 -4.4286513 -4.4285884 -4.4285851 -4.4286413 -4.4287357 -4.4288306 -4.4288988 -4.4289346 -4.4289417 -4.42894][-4.4289212 -4.4289236 -4.4288945 -4.4288435 -4.428793 -4.4287572 -4.4287357 -4.4287353 -4.4287577 -4.4288 -4.4288559 -4.4289079 -4.4289427 -4.4289546 -4.4289522][-4.4289403 -4.4289165 -4.428875 -4.4288297 -4.4288049 -4.4287963 -4.4287915 -4.428782 -4.4287729 -4.4287839 -4.4288235 -4.4288788 -4.4289279 -4.4289513 -4.4289532][-4.4289217 -4.4288793 -4.4288335 -4.4287977 -4.4287925 -4.4287934 -4.4287758 -4.4287324 -4.4286752 -4.4286566 -4.4287004 -4.4287825 -4.4288697 -4.4289255 -4.4289403][-4.4288845 -4.4288354 -4.4287949 -4.4287739 -4.4287882 -4.428792 -4.428751 -4.4286561 -4.4285288 -4.4284635 -4.4285164 -4.4286389 -4.4287744 -4.4288754 -4.4289174][-4.4288454 -4.4287987 -4.4287715 -4.4287715 -4.4288044 -4.4288168 -4.4287682 -4.4286504 -4.428484 -4.4283509 -4.4283705 -4.4285092 -4.4286761 -4.4288154 -4.4288883][-4.4288154 -4.4287748 -4.4287658 -4.4287848 -4.4288287 -4.4288511 -4.4288139 -4.4287119 -4.4285746 -4.4284463 -4.428412 -4.428494 -4.4286351 -4.4287758 -4.428864]]...]
INFO - root - 2017-12-10 05:36:23.760863: step 5210, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:28m:02s remains)
INFO - root - 2017-12-10 05:36:26.438990: step 5220, loss = 2.28, batch loss = 2.23 (27.3 examples/sec; 0.293 sec/batch; 26h:39m:55s remains)
INFO - root - 2017-12-10 05:36:29.099742: step 5230, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:14m:47s remains)
INFO - root - 2017-12-10 05:36:31.757927: step 5240, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:46m:07s remains)
INFO - root - 2017-12-10 05:36:34.413093: step 5250, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:58m:28s remains)
INFO - root - 2017-12-10 05:36:37.010942: step 5260, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:48m:51s remains)
INFO - root - 2017-12-10 05:36:39.647389: step 5270, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:18m:43s remains)
INFO - root - 2017-12-10 05:36:42.351782: step 5280, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:39m:12s remains)
INFO - root - 2017-12-10 05:36:44.964632: step 5290, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:10m:37s remains)
INFO - root - 2017-12-10 05:36:47.594881: step 5300, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:04m:05s remains)
2017-12-10 05:36:47.900058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288664 -4.4288745 -4.4288917 -4.4289026 -4.4288926 -4.4288788 -4.4288659 -4.4288559 -4.4288511 -4.42885 -4.4288478 -4.4288592 -4.4288807 -4.428905 -4.4289246][-4.4288259 -4.4288449 -4.4288726 -4.42889 -4.4288778 -4.4288559 -4.4288321 -4.4288139 -4.4288049 -4.4287968 -4.4287891 -4.4288025 -4.428833 -4.4288688 -4.428896][-4.4287739 -4.4288006 -4.4288359 -4.4288578 -4.428843 -4.428812 -4.4287748 -4.4287519 -4.4287467 -4.4287391 -4.4287267 -4.4287357 -4.4287705 -4.4288125 -4.4288425][-4.4287152 -4.4287457 -4.4287872 -4.4288092 -4.4287868 -4.4287443 -4.4286985 -4.4286766 -4.4286838 -4.4286847 -4.4286685 -4.4286695 -4.4287028 -4.4287448 -4.428772][-4.4286661 -4.4286947 -4.4287376 -4.4287572 -4.4287233 -4.4286671 -4.4286132 -4.4285975 -4.4286251 -4.4286442 -4.4286342 -4.4286351 -4.4286666 -4.4287081 -4.4287319][-4.4286265 -4.42865 -4.4286909 -4.4287033 -4.4286566 -4.4285827 -4.4285188 -4.4285054 -4.4285493 -4.4285955 -4.4286141 -4.4286289 -4.4286613 -4.4286985 -4.4287167][-4.428617 -4.4286432 -4.42868 -4.428678 -4.4286122 -4.4285111 -4.4284272 -4.428411 -4.4284706 -4.42855 -4.4286036 -4.428637 -4.4286714 -4.4287038 -4.4287186][-4.4286332 -4.4286675 -4.4286985 -4.4286823 -4.4286051 -4.4284892 -4.4283986 -4.4283857 -4.428452 -4.4285488 -4.4286227 -4.4286637 -4.4286957 -4.4287243 -4.428741][-4.4286442 -4.4286895 -4.4287148 -4.4286904 -4.4286156 -4.4285126 -4.4284415 -4.4284387 -4.4284978 -4.4285784 -4.4286408 -4.4286737 -4.4286981 -4.4287252 -4.4287534][-4.4286542 -4.4287019 -4.4287248 -4.4287004 -4.4286342 -4.4285526 -4.4285026 -4.428503 -4.4285469 -4.4286003 -4.428638 -4.4286571 -4.4286819 -4.4287195 -4.4287624][-4.4286656 -4.4287071 -4.4287267 -4.428709 -4.4286547 -4.4285941 -4.4285593 -4.4285607 -4.4285913 -4.4286242 -4.4286442 -4.42866 -4.428689 -4.4287338 -4.4287853][-4.428668 -4.4287028 -4.4287219 -4.4287114 -4.4286718 -4.4286289 -4.4286084 -4.4286151 -4.42864 -4.4286633 -4.4286747 -4.4286876 -4.4287195 -4.4287691 -4.4288177][-4.4286752 -4.4287024 -4.4287171 -4.42871 -4.4286804 -4.4286494 -4.4286408 -4.4286594 -4.4286885 -4.4287105 -4.4287152 -4.4287248 -4.4287553 -4.428802 -4.4288416][-4.4286838 -4.4287004 -4.4287057 -4.4287 -4.4286823 -4.4286666 -4.4286695 -4.4286995 -4.4287324 -4.428751 -4.4287505 -4.4287562 -4.4287815 -4.4288206 -4.4288521][-4.4286895 -4.4286995 -4.4286971 -4.4286928 -4.4286861 -4.4286852 -4.4286971 -4.4287295 -4.4287591 -4.4287734 -4.428771 -4.4287782 -4.4288011 -4.4288287 -4.4288454]]...]
INFO - root - 2017-12-10 05:36:50.540913: step 5310, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:12m:07s remains)
INFO - root - 2017-12-10 05:36:53.223565: step 5320, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:18m:14s remains)
INFO - root - 2017-12-10 05:36:55.866652: step 5330, loss = 2.28, batch loss = 2.23 (28.5 examples/sec; 0.281 sec/batch; 25h:32m:51s remains)
INFO - root - 2017-12-10 05:36:58.466149: step 5340, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:46m:56s remains)
INFO - root - 2017-12-10 05:37:01.079104: step 5350, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:45m:14s remains)
INFO - root - 2017-12-10 05:37:03.686028: step 5360, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:15m:45s remains)
INFO - root - 2017-12-10 05:37:06.314691: step 5370, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:01m:46s remains)
INFO - root - 2017-12-10 05:37:08.954467: step 5380, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:27m:39s remains)
INFO - root - 2017-12-10 05:37:11.597638: step 5390, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:40m:36s remains)
INFO - root - 2017-12-10 05:37:14.229467: step 5400, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:24m:03s remains)
2017-12-10 05:37:14.519122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289479 -4.4289284 -4.4289145 -4.4288888 -4.4288507 -4.4287896 -4.4287481 -4.4287348 -4.4287291 -4.42871 -4.4286933 -4.4286861 -4.4286861 -4.4286852 -4.4286752][-4.428936 -4.4289036 -4.4288759 -4.4288363 -4.4287934 -4.4287248 -4.4286785 -4.4286747 -4.4286819 -4.4286704 -4.4286594 -4.42866 -4.428648 -4.4286313 -4.4286103][-4.4289217 -4.4288812 -4.4288487 -4.42881 -4.4287686 -4.4286942 -4.4286366 -4.42864 -4.4286604 -4.4286551 -4.4286485 -4.4286461 -4.4286251 -4.4286003 -4.4285741][-4.4289246 -4.4288898 -4.4288635 -4.4288268 -4.4287829 -4.4286914 -4.4286065 -4.4285917 -4.4286127 -4.428616 -4.4286218 -4.42863 -4.428617 -4.4285908 -4.4285617][-4.4289265 -4.4288917 -4.4288621 -4.428822 -4.4287686 -4.42866 -4.4285502 -4.4285173 -4.4285297 -4.4285245 -4.428546 -4.428587 -4.4286103 -4.4285893 -4.4285502][-4.4289131 -4.4288697 -4.4288316 -4.4287796 -4.4287119 -4.4285851 -4.428463 -4.4284263 -4.4284182 -4.4283824 -4.4284034 -4.4284849 -4.4285607 -4.4285717 -4.4285517][-4.4288983 -4.428843 -4.4287829 -4.4287019 -4.4286022 -4.4284472 -4.4283152 -4.4282827 -4.4282684 -4.4282088 -4.4282403 -4.4283633 -4.4285007 -4.4285655 -4.4285817][-4.4288888 -4.4288216 -4.4287305 -4.4286118 -4.4284792 -4.42831 -4.4281883 -4.4281855 -4.4282045 -4.4281754 -4.4282351 -4.4283729 -4.4285316 -4.4286261 -4.4286585][-4.4288635 -4.4287848 -4.4286804 -4.4285583 -4.42843 -4.42828 -4.4281859 -4.4282284 -4.4283061 -4.4283381 -4.4284058 -4.4285126 -4.428637 -4.4287124 -4.4287381][-4.4288316 -4.4287591 -4.4286828 -4.42861 -4.4285312 -4.4284286 -4.42837 -4.4284272 -4.4285192 -4.4285712 -4.4286203 -4.4286761 -4.4287448 -4.4287868 -4.4287963][-4.428834 -4.4287858 -4.4287524 -4.4287271 -4.4286942 -4.4286385 -4.4286084 -4.4286461 -4.4287028 -4.4287429 -4.4287705 -4.4287915 -4.4288182 -4.4288397 -4.4288487][-4.428864 -4.4288378 -4.4288287 -4.4288239 -4.4288106 -4.42878 -4.4287581 -4.4287686 -4.4287972 -4.4288239 -4.4288473 -4.4288645 -4.428874 -4.4288845 -4.4288836][-4.4288855 -4.4288688 -4.4288626 -4.4288535 -4.428843 -4.428822 -4.428803 -4.4288044 -4.4288254 -4.428854 -4.4288778 -4.4288945 -4.4288936 -4.4288778 -4.4288354][-4.4288831 -4.4288583 -4.42884 -4.428823 -4.4288144 -4.4287953 -4.4287868 -4.428802 -4.4288321 -4.4288673 -4.4288883 -4.428884 -4.428853 -4.4287963 -4.4287095][-4.4288712 -4.4288368 -4.4288139 -4.4287953 -4.4287848 -4.428762 -4.4287591 -4.4287872 -4.4288282 -4.4288621 -4.4288688 -4.4288406 -4.4287853 -4.4287009 -4.4285855]]...]
INFO - root - 2017-12-10 05:37:17.211140: step 5410, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:17m:03s remains)
INFO - root - 2017-12-10 05:37:19.855881: step 5420, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:21m:47s remains)
INFO - root - 2017-12-10 05:37:22.501167: step 5430, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:16m:38s remains)
INFO - root - 2017-12-10 05:37:25.149474: step 5440, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:58m:01s remains)
INFO - root - 2017-12-10 05:37:27.783944: step 5450, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:56m:08s remains)
INFO - root - 2017-12-10 05:37:30.429128: step 5460, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:05m:52s remains)
INFO - root - 2017-12-10 05:37:33.070139: step 5470, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:13m:45s remains)
INFO - root - 2017-12-10 05:37:35.704665: step 5480, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:34m:00s remains)
INFO - root - 2017-12-10 05:37:38.344112: step 5490, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:54m:56s remains)
INFO - root - 2017-12-10 05:37:40.963715: step 5500, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:06m:55s remains)
2017-12-10 05:37:41.264424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287524 -4.4287634 -4.4287744 -4.4287491 -4.4287357 -4.4287415 -4.4287577 -4.428782 -4.4288106 -4.4288454 -4.4288492 -4.4288206 -4.428782 -4.4287615 -4.4287148][-4.4287148 -4.428731 -4.4287457 -4.4287224 -4.4287171 -4.4287286 -4.4287405 -4.4287577 -4.4287839 -4.4288187 -4.4288254 -4.4287958 -4.4287491 -4.428731 -4.4286928][-4.4286561 -4.42867 -4.4286914 -4.428679 -4.4286852 -4.428709 -4.428731 -4.4287515 -4.4287763 -4.4288068 -4.42881 -4.428772 -4.4287176 -4.4287047 -4.4286852][-4.4286304 -4.4286366 -4.4286537 -4.4286509 -4.4286618 -4.4286847 -4.4287081 -4.4287291 -4.4287496 -4.428771 -4.4287682 -4.4287229 -4.4286613 -4.4286566 -4.4286637][-4.4286308 -4.4286261 -4.4286294 -4.4286256 -4.428637 -4.4286427 -4.4286513 -4.4286623 -4.4286757 -4.4286938 -4.4286947 -4.4286566 -4.4285989 -4.428596 -4.4286118][-4.4285889 -4.4285769 -4.4285688 -4.4285679 -4.4285779 -4.4285569 -4.4285293 -4.4285259 -4.4285636 -4.428606 -4.4286342 -4.428618 -4.4285641 -4.4285502 -4.4285555][-4.4285359 -4.4285064 -4.4284797 -4.42848 -4.42849 -4.4284272 -4.4283195 -4.4282832 -4.4283895 -4.4285073 -4.42858 -4.428596 -4.428546 -4.4285135 -4.4285126][-4.4284983 -4.4284458 -4.4283938 -4.4283781 -4.4283767 -4.428257 -4.4280238 -4.4279046 -4.4281039 -4.4283266 -4.4284616 -4.4285116 -4.4284806 -4.428462 -4.428483][-4.4284787 -4.4284143 -4.4283533 -4.4283433 -4.428349 -4.4282265 -4.4279442 -4.4277267 -4.4279389 -4.4282193 -4.42839 -4.4284596 -4.4284487 -4.4284639 -4.4285135][-4.4284596 -4.4284329 -4.4284091 -4.4284215 -4.4284549 -4.4284043 -4.4282355 -4.4280543 -4.4281478 -4.4283438 -4.4284749 -4.4285254 -4.4285035 -4.4285154 -4.4285641][-4.4284482 -4.42846 -4.4284782 -4.4285054 -4.4285507 -4.4285488 -4.4284778 -4.4283643 -4.4283848 -4.4284825 -4.4285612 -4.4285846 -4.4285412 -4.4285502 -4.4285913][-4.4285131 -4.4285479 -4.42859 -4.4286113 -4.4286494 -4.4286642 -4.42864 -4.4285779 -4.4285679 -4.428596 -4.4286385 -4.428628 -4.4285665 -4.42858 -4.4286289][-4.4285927 -4.428638 -4.4286838 -4.4287038 -4.4287262 -4.4287415 -4.4287343 -4.4286947 -4.4286675 -4.4286523 -4.4286561 -4.4286041 -4.428514 -4.4285221 -4.4285955][-4.4286513 -4.4286962 -4.4287348 -4.4287519 -4.4287715 -4.4287896 -4.42879 -4.4287639 -4.4287262 -4.4286833 -4.4286504 -4.4285641 -4.4284534 -4.4284568 -4.4285445][-4.4286904 -4.4287281 -4.4287577 -4.4287691 -4.4287825 -4.4287987 -4.428803 -4.4287868 -4.4287553 -4.4287133 -4.4286718 -4.4285946 -4.4285159 -4.4285307 -4.4286079]]...]
INFO - root - 2017-12-10 05:37:43.911305: step 5510, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:27m:27s remains)
INFO - root - 2017-12-10 05:37:46.568775: step 5520, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:27m:48s remains)
INFO - root - 2017-12-10 05:37:49.195938: step 5530, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:41m:54s remains)
INFO - root - 2017-12-10 05:37:51.863950: step 5540, loss = 2.28, batch loss = 2.23 (28.2 examples/sec; 0.283 sec/batch; 25h:43m:10s remains)
INFO - root - 2017-12-10 05:37:54.531964: step 5550, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:50m:11s remains)
INFO - root - 2017-12-10 05:37:57.197965: step 5560, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:32m:01s remains)
INFO - root - 2017-12-10 05:37:59.868109: step 5570, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:00m:12s remains)
INFO - root - 2017-12-10 05:38:02.551070: step 5580, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:30m:18s remains)
INFO - root - 2017-12-10 05:38:05.177935: step 5590, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:52m:43s remains)
INFO - root - 2017-12-10 05:38:07.831569: step 5600, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:21m:14s remains)
2017-12-10 05:38:08.130823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289474 -4.4289279 -4.4289117 -4.428896 -4.42888 -4.4288697 -4.4288716 -4.4288807 -4.4288926 -4.4289064 -4.4289212 -4.4289317 -4.4289408 -4.4289484 -4.428957][-4.428915 -4.4288888 -4.4288673 -4.428844 -4.4288177 -4.4287982 -4.4287992 -4.4288092 -4.4288225 -4.4288392 -4.4288645 -4.4288883 -4.4289079 -4.4289207 -4.4289317][-4.4288907 -4.4288578 -4.4288316 -4.4288044 -4.4287729 -4.4287429 -4.4287305 -4.4287252 -4.4287314 -4.4287519 -4.4287953 -4.4288425 -4.4288783 -4.4288955 -4.4289036][-4.4288759 -4.4288354 -4.4288063 -4.42878 -4.4287419 -4.4286962 -4.4286566 -4.4286222 -4.428618 -4.4286461 -4.4287148 -4.4287915 -4.4288449 -4.4288645 -4.4288678][-4.4288697 -4.428822 -4.4287934 -4.4287705 -4.4287286 -4.4286671 -4.4285932 -4.4285188 -4.4284897 -4.4285178 -4.4286103 -4.4287252 -4.4288 -4.4288244 -4.4288287][-4.4288588 -4.4288077 -4.4287825 -4.4287643 -4.4287195 -4.4286451 -4.4285388 -4.4284182 -4.4283457 -4.4283648 -4.4284873 -4.4286432 -4.4287438 -4.428781 -4.4287944][-4.4288278 -4.4287739 -4.4287577 -4.4287496 -4.4287162 -4.4286451 -4.428524 -4.4283638 -4.4282312 -4.4282117 -4.4283528 -4.4285512 -4.4286823 -4.4287376 -4.4287634][-4.4287853 -4.4287271 -4.4287195 -4.4287257 -4.4287152 -4.4286623 -4.4285374 -4.4283371 -4.428123 -4.4280386 -4.4282041 -4.42845 -4.4286108 -4.4286809 -4.42872][-4.4287639 -4.4286995 -4.4286928 -4.4287105 -4.4287205 -4.428689 -4.4285793 -4.4283724 -4.4281197 -4.428009 -4.4281907 -4.4284391 -4.42859 -4.4286518 -4.4286861][-4.4287767 -4.4287105 -4.4286995 -4.4287286 -4.4287596 -4.4287477 -4.4286661 -4.4284949 -4.4282823 -4.4281845 -4.4283218 -4.428503 -4.4286079 -4.4286461 -4.4286704][-4.4288211 -4.4287605 -4.4287438 -4.4287696 -4.4288025 -4.4287953 -4.4287372 -4.4286156 -4.42846 -4.4283791 -4.4284544 -4.4285636 -4.4286337 -4.4286604 -4.4286842][-4.4288845 -4.4288354 -4.4288158 -4.4288268 -4.4288454 -4.4288387 -4.428802 -4.4287267 -4.4286218 -4.428555 -4.4285841 -4.4286447 -4.4286938 -4.42872 -4.42875][-4.4289422 -4.4289055 -4.428885 -4.4288836 -4.4288912 -4.428885 -4.42886 -4.4288077 -4.4287338 -4.4286828 -4.4286952 -4.4287324 -4.4287744 -4.42881 -4.4288421][-4.4289832 -4.42896 -4.4289436 -4.4289374 -4.4289412 -4.4289403 -4.4289241 -4.428884 -4.4288354 -4.4288073 -4.4288163 -4.4288373 -4.4288688 -4.4289036 -4.4289284][-4.4290028 -4.4289894 -4.4289765 -4.42897 -4.4289708 -4.4289718 -4.4289632 -4.428936 -4.42891 -4.4288993 -4.4289074 -4.42892 -4.4289417 -4.4289689 -4.4289827]]...]
INFO - root - 2017-12-10 05:38:10.760225: step 5610, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:21m:04s remains)
INFO - root - 2017-12-10 05:38:13.408797: step 5620, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:01m:45s remains)
INFO - root - 2017-12-10 05:38:16.018547: step 5630, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:51m:12s remains)
INFO - root - 2017-12-10 05:38:18.677215: step 5640, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:09m:19s remains)
INFO - root - 2017-12-10 05:38:21.410378: step 5650, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:13m:01s remains)
INFO - root - 2017-12-10 05:38:24.032407: step 5660, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:09m:25s remains)
INFO - root - 2017-12-10 05:38:26.676911: step 5670, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:01m:32s remains)
INFO - root - 2017-12-10 05:38:29.382354: step 5680, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:20m:02s remains)
INFO - root - 2017-12-10 05:38:31.997437: step 5690, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:23m:33s remains)
INFO - root - 2017-12-10 05:38:34.577338: step 5700, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:05m:24s remains)
2017-12-10 05:38:34.897063: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289055 -4.4288898 -4.4288707 -4.4288282 -4.4288015 -4.4288082 -4.4287858 -4.4287667 -4.4287515 -4.4287395 -4.428721 -4.4287167 -4.4287124 -4.4286914 -4.42868][-4.4288983 -4.4288688 -4.4288344 -4.4287729 -4.4287229 -4.4287186 -4.4286923 -4.4286618 -4.428637 -4.4286175 -4.4286 -4.4285951 -4.428586 -4.4285593 -4.4285474][-4.4288945 -4.42885 -4.4287987 -4.4287181 -4.4286485 -4.4286346 -4.4286094 -4.4285779 -4.428546 -4.4285221 -4.4285078 -4.4285021 -4.4284892 -4.4284663 -4.4284606][-4.4288974 -4.4288449 -4.4287863 -4.4287 -4.4286256 -4.4286084 -4.4285903 -4.4285641 -4.4285345 -4.4285154 -4.4285026 -4.4284887 -4.4284682 -4.4284482 -4.4284434][-4.4289041 -4.4288568 -4.42881 -4.4287415 -4.4286804 -4.4286652 -4.4286556 -4.42864 -4.4286213 -4.4286132 -4.4286008 -4.4285765 -4.4285469 -4.4285235 -4.4285121][-4.4289055 -4.4288721 -4.4288511 -4.4288125 -4.4287734 -4.4287677 -4.42877 -4.4287663 -4.4287596 -4.4287581 -4.4287434 -4.4287133 -4.4286785 -4.4286528 -4.428638][-4.4289036 -4.4288807 -4.4288793 -4.4288607 -4.4288335 -4.4288363 -4.42885 -4.4288597 -4.428863 -4.4288621 -4.4288459 -4.4288187 -4.4287891 -4.4287677 -4.4287572][-4.4288993 -4.4288783 -4.4288821 -4.4288673 -4.4288416 -4.42885 -4.4288697 -4.4288874 -4.4288926 -4.4288878 -4.4288731 -4.4288578 -4.4288435 -4.4288349 -4.4288359][-4.428894 -4.4288669 -4.4288635 -4.4288425 -4.4288125 -4.428823 -4.428844 -4.4288588 -4.4288578 -4.4288483 -4.4288383 -4.4288387 -4.428844 -4.4288535 -4.4288678][-4.42889 -4.4288549 -4.4288416 -4.4288135 -4.4287791 -4.4287877 -4.4288034 -4.4288063 -4.4287968 -4.4287858 -4.4287806 -4.4287949 -4.4288168 -4.4288392 -4.428864][-4.4288859 -4.4288468 -4.4288287 -4.4287987 -4.4287629 -4.4287686 -4.4287758 -4.4287658 -4.428751 -4.4287424 -4.4287415 -4.4287624 -4.4287939 -4.4288244 -4.4288526][-4.428885 -4.4288483 -4.4288311 -4.4288034 -4.4287682 -4.4287677 -4.4287682 -4.4287519 -4.4287395 -4.4287343 -4.4287353 -4.4287586 -4.4287934 -4.4288244 -4.4288516][-4.4288931 -4.4288607 -4.4288478 -4.4288259 -4.428792 -4.4287853 -4.4287839 -4.428771 -4.4287629 -4.4287586 -4.42876 -4.4287834 -4.4288149 -4.42884 -4.4288611][-4.4289083 -4.4288807 -4.4288707 -4.4288549 -4.428823 -4.4288125 -4.4288125 -4.4288044 -4.4288011 -4.4287982 -4.4287996 -4.42882 -4.4288445 -4.4288626 -4.4288759][-4.4289265 -4.4289017 -4.428895 -4.4288855 -4.4288568 -4.4288459 -4.4288473 -4.4288445 -4.4288468 -4.4288464 -4.4288468 -4.42886 -4.4288769 -4.4288874 -4.428894]]...]
INFO - root - 2017-12-10 05:38:37.530561: step 5710, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:46m:40s remains)
INFO - root - 2017-12-10 05:38:40.189671: step 5720, loss = 2.28, batch loss = 2.23 (28.5 examples/sec; 0.281 sec/batch; 25h:31m:25s remains)
INFO - root - 2017-12-10 05:38:42.865484: step 5730, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:10m:30s remains)
INFO - root - 2017-12-10 05:38:45.524978: step 5740, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:37m:08s remains)
INFO - root - 2017-12-10 05:38:48.165292: step 5750, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.281 sec/batch; 25h:32m:58s remains)
INFO - root - 2017-12-10 05:38:50.821147: step 5760, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:24m:42s remains)
INFO - root - 2017-12-10 05:38:53.445464: step 5770, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:51m:26s remains)
INFO - root - 2017-12-10 05:38:56.079335: step 5780, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:47m:05s remains)
INFO - root - 2017-12-10 05:38:58.761560: step 5790, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:22m:00s remains)
INFO - root - 2017-12-10 05:39:01.402000: step 5800, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:06m:05s remains)
2017-12-10 05:39:01.699125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287624 -4.4287624 -4.4287524 -4.4287534 -4.4287677 -4.4287977 -4.428833 -4.4288549 -4.4288487 -4.4288058 -4.4287572 -4.4287338 -4.4287195 -4.4287086 -4.4286914][-4.4287004 -4.42872 -4.4287338 -4.4287519 -4.4287696 -4.4287825 -4.4287915 -4.4287906 -4.428782 -4.4287524 -4.428719 -4.4287009 -4.4286966 -4.4286985 -4.4286957][-4.4286728 -4.4287 -4.4287314 -4.4287524 -4.4287572 -4.4287477 -4.4287415 -4.4287338 -4.4287333 -4.4287286 -4.4287162 -4.4287095 -4.4287119 -4.4287114 -4.4287076][-4.42865 -4.4286704 -4.4286947 -4.4286885 -4.4286523 -4.4286194 -4.4286127 -4.428628 -4.4286542 -4.4286795 -4.4286942 -4.4287043 -4.4287114 -4.4287004 -4.4286838][-4.4286103 -4.428616 -4.42862 -4.4285789 -4.42848 -4.4284129 -4.4284263 -4.4284773 -4.4285312 -4.4285893 -4.4286447 -4.4286785 -4.4286928 -4.4286771 -4.4286489][-4.428627 -4.4285884 -4.4285412 -4.4284396 -4.4282565 -4.4281096 -4.4281249 -4.42824 -4.4283466 -4.4284468 -4.4285655 -4.4286561 -4.4286976 -4.4286909 -4.4286656][-4.4286618 -4.4285827 -4.42848 -4.4283328 -4.4281039 -4.4278727 -4.4278345 -4.4279904 -4.4281559 -4.428297 -4.4284744 -4.428616 -4.4286828 -4.4287024 -4.4286962][-4.4287305 -4.428647 -4.4285469 -4.4284372 -4.4282813 -4.4280963 -4.4280181 -4.4281116 -4.4282336 -4.4283338 -4.4284849 -4.4286122 -4.428669 -4.4286919 -4.4287071][-4.4288144 -4.4287529 -4.4286847 -4.4286203 -4.42854 -4.4284406 -4.4283776 -4.4284139 -4.4284797 -4.4285293 -4.42862 -4.4286938 -4.4287138 -4.4287181 -4.4287291][-4.4288549 -4.4288144 -4.4287782 -4.4287457 -4.4287119 -4.4286776 -4.42865 -4.4286666 -4.4286985 -4.4287162 -4.4287667 -4.4288068 -4.4288135 -4.4288015 -4.4287925][-4.4288821 -4.4288535 -4.4288373 -4.4288263 -4.4288154 -4.4288173 -4.4288111 -4.4288173 -4.4288249 -4.4288306 -4.4288597 -4.4288859 -4.4288955 -4.4288807 -4.4288578][-4.4289064 -4.4288836 -4.4288735 -4.4288716 -4.4288707 -4.4288793 -4.4288836 -4.4288812 -4.4288764 -4.4288793 -4.4288993 -4.4289169 -4.4289289 -4.4289188 -4.4288988][-4.4289217 -4.4289041 -4.4288964 -4.4288969 -4.4289002 -4.4289079 -4.4289155 -4.4289117 -4.4289064 -4.4289074 -4.428916 -4.4289222 -4.4289293 -4.4289269 -4.4289188][-4.4289303 -4.4289155 -4.4289103 -4.42891 -4.4289117 -4.4289188 -4.4289255 -4.4289241 -4.4289207 -4.4289188 -4.4289207 -4.4289207 -4.4289246 -4.4289284 -4.4289269][-4.4289379 -4.4289241 -4.4289174 -4.4289131 -4.4289103 -4.4289112 -4.4289141 -4.4289141 -4.4289122 -4.4289112 -4.4289136 -4.4289165 -4.4289217 -4.428926 -4.4289284]]...]
INFO - root - 2017-12-10 05:39:04.322055: step 5810, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:39m:50s remains)
INFO - root - 2017-12-10 05:39:06.923223: step 5820, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:05m:37s remains)
INFO - root - 2017-12-10 05:39:09.588082: step 5830, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:42m:55s remains)
INFO - root - 2017-12-10 05:39:12.271697: step 5840, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:46m:28s remains)
INFO - root - 2017-12-10 05:39:14.925037: step 5850, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:01m:29s remains)
INFO - root - 2017-12-10 05:39:17.605290: step 5860, loss = 2.28, batch loss = 2.23 (28.3 examples/sec; 0.282 sec/batch; 25h:36m:49s remains)
INFO - root - 2017-12-10 05:39:20.249792: step 5870, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:05m:57s remains)
INFO - root - 2017-12-10 05:39:22.870923: step 5880, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:21m:41s remains)
INFO - root - 2017-12-10 05:39:25.492712: step 5890, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 25h:56m:54s remains)
INFO - root - 2017-12-10 05:39:28.122488: step 5900, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:00m:02s remains)
2017-12-10 05:39:28.405107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288592 -4.4288282 -4.4287891 -4.4287329 -4.4286575 -4.4285855 -4.4285169 -4.4284396 -4.4283619 -4.4283037 -4.42832 -4.4284539 -4.4286337 -4.4287934 -4.4289055][-4.428822 -4.4287953 -4.4287553 -4.4286971 -4.428628 -4.4285645 -4.4285045 -4.4284377 -4.4283814 -4.4283509 -4.4283924 -4.4285436 -4.4287257 -4.42887 -4.42896][-4.4287724 -4.4287682 -4.4287415 -4.4286976 -4.4286447 -4.4285936 -4.4285436 -4.4284921 -4.4284616 -4.4284596 -4.4285097 -4.42865 -4.4288125 -4.4289336 -4.4290009][-4.4287109 -4.4287386 -4.4287372 -4.4287095 -4.4286757 -4.4286389 -4.4286017 -4.42857 -4.4285707 -4.42859 -4.4286304 -4.428741 -4.4288692 -4.428966 -4.4290185][-4.4286776 -4.4287286 -4.4287477 -4.4287324 -4.4287066 -4.4286709 -4.4286318 -4.4286189 -4.4286556 -4.4286981 -4.4287338 -4.428813 -4.4289031 -4.4289784 -4.4290218][-4.428689 -4.4287419 -4.4287658 -4.4287486 -4.4287171 -4.4286609 -4.4286137 -4.4286251 -4.4286985 -4.42876 -4.4287977 -4.4288516 -4.4289145 -4.428978 -4.429018][-4.4287162 -4.4287572 -4.428772 -4.4287486 -4.4286971 -4.4286184 -4.4285784 -4.4286227 -4.4287262 -4.4288025 -4.4288359 -4.4288673 -4.4289126 -4.4289713 -4.4290113][-4.4287362 -4.4287615 -4.4287744 -4.4287491 -4.4286819 -4.4285913 -4.4285688 -4.4286456 -4.4287686 -4.4288459 -4.4288688 -4.4288831 -4.42892 -4.4289742 -4.4290094][-4.4287214 -4.4287405 -4.4287663 -4.4287448 -4.4286747 -4.4285917 -4.4285975 -4.4286976 -4.4288177 -4.4288859 -4.428895 -4.4288988 -4.4289303 -4.4289789 -4.429009][-4.4286871 -4.4287186 -4.4287629 -4.4287491 -4.4286857 -4.4286265 -4.4286556 -4.4287572 -4.4288611 -4.4289136 -4.4289126 -4.4289122 -4.4289417 -4.4289842 -4.4290085][-4.428648 -4.4287086 -4.4287715 -4.4287629 -4.4287157 -4.4286866 -4.4287291 -4.4288125 -4.4288917 -4.4289279 -4.4289174 -4.428916 -4.4289446 -4.4289823 -4.4290037][-4.4286294 -4.4287171 -4.4287958 -4.428792 -4.4287572 -4.4287519 -4.4287963 -4.4288554 -4.4289112 -4.4289341 -4.4289165 -4.4289103 -4.4289336 -4.4289665 -4.42899][-4.4286304 -4.4287391 -4.428823 -4.4288192 -4.4287906 -4.4288 -4.4288373 -4.42888 -4.4289222 -4.4289341 -4.4289079 -4.4288926 -4.4289088 -4.4289417 -4.4289713][-4.4286504 -4.4287748 -4.4288516 -4.4288445 -4.4288173 -4.4288268 -4.4288549 -4.4288921 -4.428926 -4.428925 -4.4288878 -4.428864 -4.4288778 -4.4289145 -4.4289513][-4.4286981 -4.4288273 -4.4288893 -4.4288774 -4.4288492 -4.4288468 -4.4288654 -4.4288974 -4.4289222 -4.4289055 -4.4288559 -4.428833 -4.4288526 -4.4288955 -4.42894]]...]
INFO - root - 2017-12-10 05:39:31.031285: step 5910, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:02m:11s remains)
INFO - root - 2017-12-10 05:39:33.717681: step 5920, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:52m:42s remains)
INFO - root - 2017-12-10 05:39:36.373033: step 5930, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 26h:21m:52s remains)
INFO - root - 2017-12-10 05:39:39.069347: step 5940, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:57m:28s remains)
INFO - root - 2017-12-10 05:39:41.734271: step 5950, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:20m:50s remains)
INFO - root - 2017-12-10 05:39:44.354497: step 5960, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:00m:11s remains)
INFO - root - 2017-12-10 05:39:47.028806: step 5970, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:30m:37s remains)
INFO - root - 2017-12-10 05:39:49.645304: step 5980, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:28m:53s remains)
INFO - root - 2017-12-10 05:39:52.298925: step 5990, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:39m:12s remains)
INFO - root - 2017-12-10 05:39:55.009599: step 6000, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:25m:30s remains)
2017-12-10 05:39:55.292428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286442 -4.4286542 -4.42869 -4.4287462 -4.4287939 -4.4288244 -4.4288468 -4.4288607 -4.42887 -4.4288754 -4.4288492 -4.4288 -4.42873 -4.4286685 -4.428638][-4.428575 -4.4285965 -4.4286532 -4.428721 -4.4287663 -4.4287949 -4.4288106 -4.4288163 -4.4288263 -4.428834 -4.4288106 -4.4287581 -4.4286847 -4.4286251 -4.4286141][-4.4285626 -4.4285946 -4.42866 -4.4287257 -4.4287558 -4.4287677 -4.4287753 -4.4287806 -4.4287963 -4.4288106 -4.428792 -4.4287424 -4.428669 -4.4286137 -4.4286127][-4.4286013 -4.4286351 -4.4286895 -4.4287343 -4.4287395 -4.4287305 -4.4287348 -4.4287472 -4.4287729 -4.4287949 -4.4287853 -4.4287429 -4.4286757 -4.428628 -4.4286337][-4.4286504 -4.4286833 -4.4287214 -4.428741 -4.4287133 -4.4286761 -4.4286814 -4.4286985 -4.4287214 -4.4287472 -4.4287577 -4.4287453 -4.4287047 -4.4286695 -4.4286804][-4.42869 -4.4287162 -4.4287376 -4.4287329 -4.4286847 -4.4286265 -4.4286242 -4.4286442 -4.4286561 -4.4286714 -4.4287 -4.4287357 -4.4287438 -4.4287324 -4.4287472][-4.4287047 -4.4287252 -4.4287443 -4.42874 -4.4286952 -4.4286304 -4.428607 -4.4286036 -4.428586 -4.4285665 -4.4286036 -4.4286904 -4.4287586 -4.4287939 -4.4288282][-4.4286723 -4.428699 -4.4287267 -4.4287481 -4.4287291 -4.4286747 -4.4286308 -4.4285841 -4.4285092 -4.4284382 -4.4284687 -4.428596 -4.4287214 -4.4288077 -4.4288683][-4.4286056 -4.4286532 -4.4287128 -4.42877 -4.4287829 -4.4287486 -4.42869 -4.4285994 -4.42846 -4.4283371 -4.4283547 -4.4285073 -4.428669 -4.4287796 -4.4288568][-4.4285226 -4.4286151 -4.4287171 -4.428812 -4.428853 -4.4288321 -4.4287558 -4.4286318 -4.4284568 -4.4283085 -4.4283214 -4.4284849 -4.4286566 -4.4287696 -4.4288454][-4.4284549 -4.4286008 -4.4287405 -4.4288449 -4.4288945 -4.4288774 -4.4287858 -4.4286509 -4.4284868 -4.4283509 -4.4283624 -4.4285111 -4.4286618 -4.42876 -4.4288239][-4.4284096 -4.4285908 -4.4287343 -4.4288311 -4.4288816 -4.428864 -4.4287734 -4.4286585 -4.4285369 -4.4284415 -4.4284525 -4.4285612 -4.4286742 -4.4287453 -4.4287906][-4.4283891 -4.4285645 -4.4286804 -4.4287643 -4.4288206 -4.42881 -4.428741 -4.4286637 -4.4285932 -4.4285474 -4.4285612 -4.4286227 -4.4286852 -4.4287262 -4.4287548][-4.4284148 -4.4285507 -4.4286265 -4.4286971 -4.4287548 -4.42875 -4.4287081 -4.428669 -4.4286432 -4.4286346 -4.4286513 -4.428678 -4.4286995 -4.4287171 -4.4287391][-4.4284854 -4.4285746 -4.428618 -4.428679 -4.4287224 -4.4287138 -4.4286866 -4.4286671 -4.4286637 -4.42868 -4.4287071 -4.4287229 -4.4287329 -4.4287477 -4.4287691]]...]
INFO - root - 2017-12-10 05:39:57.918801: step 6010, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:20m:55s remains)
INFO - root - 2017-12-10 05:40:00.567007: step 6020, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:09m:31s remains)
INFO - root - 2017-12-10 05:40:03.184875: step 6030, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:19m:22s remains)
INFO - root - 2017-12-10 05:40:05.830307: step 6040, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:01m:55s remains)
INFO - root - 2017-12-10 05:40:08.502831: step 6050, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:15m:26s remains)
INFO - root - 2017-12-10 05:40:11.113309: step 6060, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:27m:36s remains)
INFO - root - 2017-12-10 05:40:13.711961: step 6070, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:16m:36s remains)
INFO - root - 2017-12-10 05:40:16.353776: step 6080, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:19m:58s remains)
INFO - root - 2017-12-10 05:40:19.004235: step 6090, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:38m:56s remains)
INFO - root - 2017-12-10 05:40:21.617846: step 6100, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:00m:37s remains)
2017-12-10 05:40:21.927414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289737 -4.4289722 -4.4289737 -4.4289718 -4.4289613 -4.428946 -4.4289389 -4.4289417 -4.4289546 -4.4289708 -4.4289794 -4.4289789 -4.4289722 -4.428967 -4.4289608][-4.4289665 -4.4289637 -4.4289632 -4.4289589 -4.4289455 -4.428925 -4.4289174 -4.4289184 -4.4289303 -4.4289479 -4.4289613 -4.4289646 -4.4289584 -4.4289551 -4.4289541][-4.4289556 -4.4289494 -4.428947 -4.428937 -4.4289122 -4.4288797 -4.4288678 -4.4288673 -4.4288759 -4.428896 -4.4289188 -4.4289274 -4.4289246 -4.428926 -4.4289327][-4.4289374 -4.4289255 -4.4289207 -4.4289036 -4.428865 -4.4288216 -4.4288049 -4.4288025 -4.4288082 -4.4288321 -4.4288692 -4.4288898 -4.42889 -4.4288926 -4.4289045][-4.4289155 -4.4288979 -4.4288874 -4.4288645 -4.4288216 -4.4287777 -4.4287596 -4.4287615 -4.4287748 -4.4287996 -4.4288378 -4.4288621 -4.4288607 -4.4288592 -4.4288664][-4.4289012 -4.4288793 -4.4288669 -4.4288445 -4.4288058 -4.4287624 -4.4287438 -4.4287539 -4.4287777 -4.4287987 -4.4288263 -4.4288411 -4.4288311 -4.4288249 -4.4288254][-4.4288926 -4.4288673 -4.4288516 -4.428834 -4.4287992 -4.4287519 -4.42873 -4.42875 -4.4287877 -4.4288125 -4.428833 -4.4288292 -4.4287968 -4.4287763 -4.4287672][-4.4288888 -4.4288564 -4.4288316 -4.4288082 -4.4287629 -4.4287095 -4.4286823 -4.4287138 -4.4287739 -4.4288273 -4.4288588 -4.4288421 -4.4287915 -4.4287562 -4.4287329][-4.4288936 -4.4288588 -4.4288306 -4.4287996 -4.4287429 -4.4286757 -4.4286413 -4.4286733 -4.4287462 -4.4288263 -4.4288764 -4.4288669 -4.428822 -4.4287868 -4.4287567][-4.4289093 -4.4288788 -4.428853 -4.4288192 -4.4287581 -4.4286823 -4.4286394 -4.4286613 -4.4287276 -4.428812 -4.4288692 -4.4288659 -4.4288368 -4.4288144 -4.4287858][-4.42893 -4.4289088 -4.4288883 -4.4288549 -4.4287953 -4.4287171 -4.4286652 -4.428669 -4.4287219 -4.428793 -4.4288406 -4.428844 -4.4288306 -4.4288197 -4.4287963][-4.4289427 -4.42893 -4.4289145 -4.4288816 -4.4288239 -4.4287519 -4.4286995 -4.4287 -4.4287448 -4.4288011 -4.4288363 -4.4288454 -4.4288454 -4.4288421 -4.4288282][-4.4289494 -4.4289379 -4.4289231 -4.4288931 -4.4288468 -4.4287953 -4.4287562 -4.428761 -4.4287987 -4.4288378 -4.42886 -4.4288678 -4.4288716 -4.4288735 -4.4288683][-4.4289517 -4.4289436 -4.4289322 -4.4289136 -4.4288898 -4.4288669 -4.4288483 -4.4288554 -4.4288807 -4.4289036 -4.4289131 -4.4289141 -4.4289145 -4.4289179 -4.4289179][-4.42895 -4.428946 -4.4289408 -4.4289331 -4.4289255 -4.4289203 -4.4289174 -4.428926 -4.4289417 -4.4289536 -4.4289556 -4.4289517 -4.42895 -4.4289513 -4.4289513]]...]
INFO - root - 2017-12-10 05:40:24.613901: step 6110, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:18m:10s remains)
INFO - root - 2017-12-10 05:40:27.274255: step 6120, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:36m:33s remains)
INFO - root - 2017-12-10 05:40:29.890464: step 6130, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:00m:08s remains)
INFO - root - 2017-12-10 05:40:32.492274: step 6140, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:08m:50s remains)
INFO - root - 2017-12-10 05:40:35.123395: step 6150, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:07m:29s remains)
INFO - root - 2017-12-10 05:40:37.790390: step 6160, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:22m:58s remains)
INFO - root - 2017-12-10 05:40:40.427897: step 6170, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:59m:25s remains)
INFO - root - 2017-12-10 05:40:43.099006: step 6180, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:04m:57s remains)
INFO - root - 2017-12-10 05:40:45.795253: step 6190, loss = 2.28, batch loss = 2.23 (27.4 examples/sec; 0.292 sec/batch; 26h:27m:41s remains)
INFO - root - 2017-12-10 05:40:48.463843: step 6200, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:40m:53s remains)
2017-12-10 05:40:48.742331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288039 -4.4288077 -4.428812 -4.4288073 -4.428792 -4.4287858 -4.4287829 -4.4287696 -4.4287381 -4.4287558 -4.4287896 -4.4288197 -4.4288659 -4.4289045 -4.4289236][-4.4287386 -4.4287524 -4.4287477 -4.4287271 -4.4287009 -4.4286938 -4.4286976 -4.4286876 -4.4286637 -4.4286966 -4.4287496 -4.4287915 -4.4288416 -4.4288826 -4.42891][-4.4287 -4.4287224 -4.4287066 -4.4286685 -4.428628 -4.4286165 -4.4286194 -4.42861 -4.4286017 -4.4286537 -4.4287243 -4.4287748 -4.42882 -4.4288564 -4.4288931][-4.4286885 -4.4287181 -4.4287009 -4.4286466 -4.4285884 -4.4285612 -4.42855 -4.4285378 -4.4285426 -4.4286079 -4.4286866 -4.4287491 -4.4288025 -4.428844 -4.4288912][-4.4286857 -4.4287124 -4.428688 -4.4286175 -4.428546 -4.4285064 -4.4284754 -4.4284511 -4.4284549 -4.4285336 -4.4286342 -4.42872 -4.4287882 -4.4288406 -4.4288955][-4.4286594 -4.4286819 -4.4286604 -4.4285836 -4.4285131 -4.428473 -4.428431 -4.4283853 -4.4283586 -4.4284325 -4.4285493 -4.4286575 -4.4287424 -4.428813 -4.4288807][-4.428596 -4.4286151 -4.4286079 -4.428535 -4.4284854 -4.4284568 -4.4284196 -4.4283509 -4.4282837 -4.4283314 -4.4284391 -4.4285502 -4.4286423 -4.4287362 -4.4288282][-4.4284687 -4.4284806 -4.4284859 -4.42844 -4.4284372 -4.4284387 -4.428422 -4.4283485 -4.428256 -4.4282804 -4.4283581 -4.4284453 -4.4285383 -4.4286518 -4.4287648][-4.4284039 -4.4284053 -4.4284129 -4.4284024 -4.42843 -4.4284492 -4.4284482 -4.4283943 -4.4283166 -4.4283352 -4.4283872 -4.4284434 -4.4285235 -4.4286308 -4.4287395][-4.4284763 -4.4284687 -4.4284682 -4.4284749 -4.4285083 -4.4285293 -4.4285331 -4.428504 -4.4284577 -4.4284749 -4.4285107 -4.42855 -4.4286051 -4.4286795 -4.4287629][-4.428607 -4.4285965 -4.4285913 -4.4286103 -4.4286437 -4.4286594 -4.4286585 -4.4286432 -4.4286242 -4.42864 -4.4286642 -4.4286957 -4.4287252 -4.4287653 -4.4288244][-4.4287467 -4.4287467 -4.4287472 -4.4287715 -4.4288039 -4.4288163 -4.4288116 -4.4288011 -4.4287968 -4.4288096 -4.4288249 -4.4288449 -4.4288559 -4.4288731 -4.4289079][-4.4288645 -4.4288712 -4.4288807 -4.4288979 -4.4289203 -4.4289274 -4.4289207 -4.4289126 -4.4289165 -4.4289265 -4.4289336 -4.4289436 -4.428946 -4.4289517 -4.428968][-4.4289408 -4.4289432 -4.4289494 -4.4289579 -4.42897 -4.4289703 -4.4289622 -4.4289527 -4.428956 -4.428966 -4.4289732 -4.4289851 -4.4289908 -4.4289927 -4.4289994][-4.4289885 -4.4289851 -4.4289865 -4.4289865 -4.428988 -4.4289846 -4.4289775 -4.4289746 -4.4289789 -4.4289894 -4.429 -4.4290147 -4.4290228 -4.4290237 -4.4290247]]...]
INFO - root - 2017-12-10 05:40:51.328008: step 6210, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:22m:40s remains)
INFO - root - 2017-12-10 05:40:54.080962: step 6220, loss = 2.28, batch loss = 2.23 (28.5 examples/sec; 0.281 sec/batch; 25h:25m:24s remains)
INFO - root - 2017-12-10 05:40:56.740884: step 6230, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:34m:11s remains)
INFO - root - 2017-12-10 05:40:59.406427: step 6240, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:49m:30s remains)
INFO - root - 2017-12-10 05:41:02.086515: step 6250, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 26h:22m:33s remains)
INFO - root - 2017-12-10 05:41:04.702312: step 6260, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:00m:03s remains)
INFO - root - 2017-12-10 05:41:07.382604: step 6270, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:06m:16s remains)
INFO - root - 2017-12-10 05:41:10.051532: step 6280, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:14m:30s remains)
INFO - root - 2017-12-10 05:41:12.712808: step 6290, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:42m:04s remains)
INFO - root - 2017-12-10 05:41:15.321010: step 6300, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:23m:03s remains)
2017-12-10 05:41:15.641722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287343 -4.4287672 -4.4287744 -4.4287791 -4.4287872 -4.428802 -4.428823 -4.4288435 -4.4288468 -4.428823 -4.4287968 -4.4287896 -4.4287806 -4.4287634 -4.4287562][-4.4287872 -4.4288216 -4.42883 -4.4288354 -4.4288373 -4.4288421 -4.42885 -4.4288588 -4.4288597 -4.4288378 -4.4288063 -4.4287858 -4.4287639 -4.4287443 -4.4287496][-4.4288368 -4.4288621 -4.4288669 -4.4288721 -4.4288745 -4.4288735 -4.4288645 -4.4288564 -4.4288578 -4.428843 -4.428813 -4.4287791 -4.4287438 -4.4287257 -4.42874][-4.4288344 -4.4288516 -4.4288545 -4.4288583 -4.4288545 -4.428833 -4.428791 -4.42876 -4.4287691 -4.4287763 -4.4287596 -4.4287252 -4.4286857 -4.4286757 -4.4287019][-4.428792 -4.4288039 -4.4288058 -4.4288092 -4.4287949 -4.4287372 -4.4286418 -4.4285741 -4.4286036 -4.4286566 -4.4286728 -4.4286537 -4.4286232 -4.4286275 -4.4286671][-4.4287405 -4.4287405 -4.4287362 -4.4287391 -4.4287119 -4.42861 -4.4284391 -4.4283161 -4.428381 -4.42851 -4.4285789 -4.4285874 -4.4285784 -4.4286003 -4.4286504][-4.4286919 -4.4286728 -4.4286528 -4.4286494 -4.4286175 -4.4284859 -4.428246 -4.4280624 -4.4281759 -4.4283929 -4.4285116 -4.4285431 -4.4285502 -4.4285779 -4.4286337][-4.4286633 -4.428628 -4.428597 -4.4285913 -4.4285693 -4.4284554 -4.4282207 -4.4280372 -4.4281626 -4.4283957 -4.428524 -4.4285574 -4.4285612 -4.4285817 -4.4286313][-4.4286742 -4.4286346 -4.4286046 -4.4286032 -4.4286056 -4.4285407 -4.4283733 -4.4282451 -4.4283266 -4.4284921 -4.4285927 -4.4286165 -4.4286137 -4.4286261 -4.4286618][-4.4287324 -4.4286985 -4.4286747 -4.4286714 -4.4286876 -4.4286647 -4.4285655 -4.4284806 -4.4285145 -4.4286065 -4.4286752 -4.4286966 -4.4286938 -4.428699 -4.4287233][-4.4288039 -4.4287839 -4.4287653 -4.4287553 -4.4287648 -4.428762 -4.4287105 -4.4286542 -4.4286561 -4.4286976 -4.4287424 -4.4287639 -4.4287658 -4.4287696 -4.4287887][-4.428853 -4.4288464 -4.428833 -4.4288173 -4.4288163 -4.428822 -4.4288058 -4.4287724 -4.4287562 -4.4287648 -4.428793 -4.428812 -4.4288158 -4.4288163 -4.4288292][-4.4288907 -4.4288907 -4.4288836 -4.42887 -4.4288645 -4.4288759 -4.42888 -4.42886 -4.4288368 -4.42883 -4.4288464 -4.4288583 -4.428854 -4.4288445 -4.4288449][-4.4289193 -4.428926 -4.428925 -4.4289141 -4.428905 -4.428915 -4.4289246 -4.4289136 -4.4288926 -4.4288788 -4.4288821 -4.42888 -4.4288607 -4.4288397 -4.4288278][-4.428925 -4.4289331 -4.4289308 -4.4289207 -4.4289122 -4.4289179 -4.428925 -4.4289222 -4.4289083 -4.4288936 -4.4288888 -4.42887 -4.4288325 -4.4288011 -4.4287772]]...]
INFO - root - 2017-12-10 05:41:18.255768: step 6310, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:34m:56s remains)
INFO - root - 2017-12-10 05:41:20.916418: step 6320, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:57m:10s remains)
INFO - root - 2017-12-10 05:41:23.560981: step 6330, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:28m:52s remains)
INFO - root - 2017-12-10 05:41:26.178695: step 6340, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:05m:33s remains)
INFO - root - 2017-12-10 05:41:28.805865: step 6350, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:41m:43s remains)
INFO - root - 2017-12-10 05:41:31.511800: step 6360, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:22m:25s remains)
INFO - root - 2017-12-10 05:41:34.149206: step 6370, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:56m:03s remains)
INFO - root - 2017-12-10 05:41:36.803870: step 6380, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:51m:37s remains)
INFO - root - 2017-12-10 05:41:39.474687: step 6390, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:37m:36s remains)
INFO - root - 2017-12-10 05:41:42.089013: step 6400, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:48m:14s remains)
2017-12-10 05:41:42.361216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288149 -4.4288363 -4.4288645 -4.4288812 -4.4288859 -4.4288797 -4.4288669 -4.4288487 -4.4288168 -4.4287882 -4.42877 -4.4287553 -4.4287472 -4.4287167 -4.4286733][-4.4288239 -4.42884 -4.4288559 -4.4288645 -4.4288664 -4.4288564 -4.4288459 -4.428833 -4.4288073 -4.428781 -4.4287634 -4.4287534 -4.4287453 -4.4287186 -4.4286795][-4.4288344 -4.4288349 -4.4288359 -4.4288363 -4.4288297 -4.4288111 -4.4287982 -4.4287934 -4.4287858 -4.4287772 -4.4287653 -4.4287543 -4.4287424 -4.428719 -4.4286876][-4.4288483 -4.4288325 -4.4288278 -4.428823 -4.4287972 -4.4287543 -4.4287243 -4.4287271 -4.428751 -4.4287705 -4.4287634 -4.4287457 -4.4287319 -4.4287133 -4.4286876][-4.4288549 -4.4288354 -4.4288349 -4.4288278 -4.4287796 -4.4287019 -4.4286437 -4.4286451 -4.4286947 -4.4287391 -4.4287391 -4.4287143 -4.4286952 -4.4286861 -4.4286766][-4.428854 -4.428843 -4.4288516 -4.4288416 -4.4287791 -4.428678 -4.4285917 -4.4285822 -4.4286423 -4.4286976 -4.4287014 -4.4286718 -4.4286528 -4.4286528 -4.4286656][-4.428844 -4.4288359 -4.4288459 -4.4288335 -4.4287767 -4.428679 -4.4285817 -4.4285555 -4.4286089 -4.4286685 -4.4286804 -4.4286633 -4.4286566 -4.4286752 -4.4287119][-4.4288297 -4.4288163 -4.4288192 -4.4288111 -4.4287729 -4.4286928 -4.4285941 -4.4285488 -4.4285893 -4.4286571 -4.4286833 -4.4286885 -4.4287076 -4.4287524 -4.4288][-4.4288321 -4.4288177 -4.4288139 -4.4288063 -4.4287829 -4.4287233 -4.4286318 -4.428575 -4.4286 -4.4286666 -4.4287009 -4.4287233 -4.4287667 -4.4288311 -4.4288769][-4.42885 -4.4288368 -4.4288325 -4.4288287 -4.4288163 -4.4287767 -4.4287052 -4.4286461 -4.4286489 -4.4286981 -4.4287338 -4.4287663 -4.4288177 -4.4288778 -4.4289088][-4.4288621 -4.4288464 -4.4288378 -4.4288354 -4.4288373 -4.4288244 -4.4287825 -4.4287372 -4.4287276 -4.4287567 -4.42879 -4.4288187 -4.4288492 -4.4288821 -4.4288926][-4.4288764 -4.4288597 -4.4288397 -4.4288321 -4.4288416 -4.428853 -4.4288411 -4.4288163 -4.4288087 -4.4288297 -4.4288521 -4.4288635 -4.4288611 -4.4288607 -4.4288507][-4.4288993 -4.4288826 -4.4288549 -4.4288392 -4.4288468 -4.4288707 -4.4288774 -4.4288683 -4.4288731 -4.4288955 -4.4289064 -4.4288921 -4.428854 -4.4288268 -4.4288034][-4.4289069 -4.4288859 -4.4288573 -4.4288387 -4.4288406 -4.4288654 -4.428884 -4.4288878 -4.4289007 -4.4289222 -4.4289293 -4.4289031 -4.4288511 -4.4288158 -4.4287944][-4.4289012 -4.4288793 -4.4288545 -4.4288373 -4.4288368 -4.4288578 -4.4288793 -4.4288907 -4.4289041 -4.4289188 -4.4289231 -4.4289002 -4.4288597 -4.4288344 -4.4288235]]...]
INFO - root - 2017-12-10 05:41:44.979238: step 6410, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.271 sec/batch; 24h:31m:12s remains)
INFO - root - 2017-12-10 05:41:47.623972: step 6420, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:57m:48s remains)
INFO - root - 2017-12-10 05:41:50.292678: step 6430, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:32m:46s remains)
INFO - root - 2017-12-10 05:41:52.918872: step 6440, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:52m:29s remains)
INFO - root - 2017-12-10 05:41:55.539775: step 6450, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:13m:49s remains)
INFO - root - 2017-12-10 05:41:58.132465: step 6460, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:06m:38s remains)
INFO - root - 2017-12-10 05:42:00.800622: step 6470, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:13m:41s remains)
INFO - root - 2017-12-10 05:42:03.504250: step 6480, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:27m:08s remains)
INFO - root - 2017-12-10 05:42:06.129588: step 6490, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:32m:32s remains)
INFO - root - 2017-12-10 05:42:08.777420: step 6500, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:27m:37s remains)
2017-12-10 05:42:09.053394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289732 -4.4289408 -4.4288921 -4.4288268 -4.4287643 -4.4286947 -4.4286585 -4.4286757 -4.4287114 -4.4287577 -4.4288015 -4.428813 -4.4287925 -4.4287648 -4.4287395][-4.4289594 -4.4289169 -4.4288554 -4.4287786 -4.4287066 -4.42863 -4.4285941 -4.4286189 -4.4286666 -4.4287271 -4.4287682 -4.428762 -4.4287248 -4.4286904 -4.4286637][-4.4289474 -4.4288926 -4.4288154 -4.4287224 -4.4286346 -4.4285531 -4.4285254 -4.4285674 -4.4286251 -4.4286914 -4.4287262 -4.4287057 -4.4286609 -4.4286251 -4.4285965][-4.4289365 -4.4288683 -4.4287767 -4.4286714 -4.4285741 -4.4284921 -4.4284744 -4.4285288 -4.4285913 -4.42866 -4.428689 -4.4286633 -4.4286137 -4.4285722 -4.4285393][-4.4289203 -4.4288416 -4.4287438 -4.4286408 -4.4285445 -4.4284725 -4.4284663 -4.4285207 -4.4285808 -4.4286394 -4.4286566 -4.4286304 -4.4285855 -4.4285388 -4.4285026][-4.4289026 -4.4288158 -4.4287143 -4.4286122 -4.4285183 -4.4284577 -4.4284468 -4.4284835 -4.4285312 -4.428575 -4.42859 -4.4285746 -4.4285445 -4.4285069 -4.4284749][-4.4288912 -4.4287972 -4.4286923 -4.4285812 -4.4284749 -4.4284143 -4.4283853 -4.4283957 -4.4284387 -4.4284797 -4.4285092 -4.42852 -4.4285131 -4.4284973 -4.4284706][-4.4288797 -4.4287782 -4.4286666 -4.4285417 -4.4284234 -4.4283504 -4.4283075 -4.4283109 -4.4283648 -4.4284124 -4.4284496 -4.4284773 -4.4284883 -4.4284868 -4.4284582][-4.4288564 -4.428741 -4.4286184 -4.4284873 -4.4283729 -4.4282889 -4.428236 -4.4282417 -4.4283166 -4.4283814 -4.4284215 -4.4284573 -4.4284778 -4.4284744 -4.42844][-4.428834 -4.4287095 -4.428586 -4.4284706 -4.4283724 -4.4282866 -4.4282289 -4.4282455 -4.4283342 -4.4284077 -4.428453 -4.4284921 -4.4285107 -4.4284959 -4.4284549][-4.4288263 -4.4287076 -4.4285994 -4.4285111 -4.4284372 -4.428359 -4.4283061 -4.4283333 -4.4284234 -4.4284997 -4.428555 -4.4285946 -4.4286 -4.4285703 -4.428524][-4.428834 -4.4287333 -4.4286485 -4.4285855 -4.4285283 -4.4284654 -4.4284325 -4.4284744 -4.4285541 -4.4286208 -4.4286742 -4.4287081 -4.4287024 -4.4286661 -4.4286275][-4.4288483 -4.4287705 -4.4287128 -4.4286747 -4.4286342 -4.4285903 -4.4285793 -4.4286261 -4.4286823 -4.4287276 -4.4287643 -4.4287825 -4.428772 -4.4287415 -4.4287152][-4.4288778 -4.4288225 -4.4287868 -4.4287658 -4.4287405 -4.4287124 -4.4287086 -4.4287457 -4.4287806 -4.4288049 -4.4288216 -4.4288287 -4.42882 -4.428803 -4.4287887][-4.42892 -4.4288855 -4.4288626 -4.4288483 -4.4288297 -4.4288116 -4.4288096 -4.428834 -4.428854 -4.4288664 -4.4288707 -4.4288683 -4.4288616 -4.428853 -4.4288459]]...]
INFO - root - 2017-12-10 05:42:11.755096: step 6510, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:31m:42s remains)
INFO - root - 2017-12-10 05:42:14.389169: step 6520, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:35m:53s remains)
INFO - root - 2017-12-10 05:42:17.068436: step 6530, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:32m:32s remains)
INFO - root - 2017-12-10 05:42:19.732887: step 6540, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:05m:28s remains)
INFO - root - 2017-12-10 05:42:22.331218: step 6550, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:43m:49s remains)
INFO - root - 2017-12-10 05:42:25.009733: step 6560, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:59m:10s remains)
INFO - root - 2017-12-10 05:42:27.643854: step 6570, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:50m:54s remains)
INFO - root - 2017-12-10 05:42:30.295893: step 6580, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:12m:53s remains)
INFO - root - 2017-12-10 05:42:32.938840: step 6590, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:37m:25s remains)
INFO - root - 2017-12-10 05:42:35.564860: step 6600, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:06m:37s remains)
2017-12-10 05:42:35.857192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289064 -4.428853 -4.4288 -4.42876 -4.4287205 -4.4286842 -4.4286556 -4.4286284 -4.4286208 -4.4286132 -4.4285922 -4.428575 -4.4285655 -4.4285583 -4.4285383][-4.428905 -4.42884 -4.42877 -4.4287205 -4.4286904 -4.428668 -4.4286504 -4.4286232 -4.4286079 -4.4285884 -4.4285541 -4.4285474 -4.4285622 -4.4285622 -4.4285345][-4.42892 -4.4288464 -4.4287682 -4.4287167 -4.42869 -4.428679 -4.4286733 -4.428659 -4.4286375 -4.42861 -4.4285831 -4.4285908 -4.4286108 -4.4285979 -4.42856][-4.42894 -4.4288654 -4.428782 -4.4287224 -4.4286828 -4.428669 -4.4286695 -4.4286709 -4.4286647 -4.4286528 -4.4286389 -4.4286427 -4.4286437 -4.4286242 -4.4285879][-4.42895 -4.4288759 -4.4287891 -4.4287243 -4.428679 -4.42866 -4.4286509 -4.4286551 -4.4286647 -4.42868 -4.4286823 -4.4286752 -4.4286628 -4.4286456 -4.4286213][-4.4289441 -4.4288707 -4.4287872 -4.42872 -4.4286785 -4.4286532 -4.428618 -4.428606 -4.4286289 -4.4286633 -4.42868 -4.42867 -4.4286604 -4.4286656 -4.4286656][-4.4289346 -4.4288511 -4.4287696 -4.4287004 -4.4286532 -4.428606 -4.4285378 -4.4285111 -4.4285522 -4.4286089 -4.42864 -4.4286304 -4.4286146 -4.428628 -4.4286513][-4.4289293 -4.4288321 -4.4287395 -4.4286571 -4.4285936 -4.4285145 -4.4284163 -4.4283829 -4.4284339 -4.4284997 -4.4285455 -4.4285445 -4.4285278 -4.4285393 -4.4285688][-4.4289303 -4.4288287 -4.4287276 -4.4286389 -4.4285612 -4.428463 -4.4283433 -4.428299 -4.4283419 -4.4284039 -4.4284625 -4.4284782 -4.4284739 -4.428484 -4.4285107][-4.4289346 -4.428844 -4.4287519 -4.4286704 -4.4285946 -4.4284964 -4.4283733 -4.4283209 -4.4283509 -4.4284058 -4.4284658 -4.428494 -4.4284854 -4.428473 -4.4284792][-4.4289341 -4.4288607 -4.4287887 -4.428721 -4.4286532 -4.4285645 -4.4284549 -4.428401 -4.428421 -4.4284716 -4.4285297 -4.4285541 -4.4285278 -4.428484 -4.4284625][-4.428925 -4.4288616 -4.428803 -4.4287419 -4.4286704 -4.4285936 -4.4285192 -4.4284787 -4.4284868 -4.42853 -4.4285769 -4.4285979 -4.4285631 -4.4285073 -4.4284778][-4.4289145 -4.4288568 -4.4288011 -4.4287434 -4.4286809 -4.4286256 -4.4285808 -4.4285536 -4.4285569 -4.4285932 -4.4286194 -4.4286304 -4.4285989 -4.4285426 -4.4285131][-4.4289184 -4.4288673 -4.428813 -4.4287581 -4.4287128 -4.4286804 -4.4286556 -4.4286346 -4.4286337 -4.4286537 -4.428659 -4.4286594 -4.4286346 -4.428587 -4.4285583][-4.4289384 -4.4288926 -4.4288411 -4.4287925 -4.4287572 -4.4287395 -4.4287281 -4.4287133 -4.4287105 -4.42872 -4.4287167 -4.4287138 -4.4286995 -4.4286594 -4.4286275]]...]
INFO - root - 2017-12-10 05:42:38.452424: step 6610, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:20m:40s remains)
INFO - root - 2017-12-10 05:42:41.044294: step 6620, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:05m:52s remains)
INFO - root - 2017-12-10 05:42:43.657350: step 6630, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:19m:03s remains)
INFO - root - 2017-12-10 05:42:46.334772: step 6640, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:53m:32s remains)
INFO - root - 2017-12-10 05:42:49.043351: step 6650, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:28m:38s remains)
INFO - root - 2017-12-10 05:42:51.753616: step 6660, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 26h:19m:32s remains)
INFO - root - 2017-12-10 05:42:54.422043: step 6670, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:12m:28s remains)
INFO - root - 2017-12-10 05:42:57.063846: step 6680, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:14m:30s remains)
INFO - root - 2017-12-10 05:42:59.733841: step 6690, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:16m:06s remains)
INFO - root - 2017-12-10 05:43:02.415120: step 6700, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 24h:57m:21s remains)
2017-12-10 05:43:02.721877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287782 -4.428802 -4.42881 -4.4287806 -4.4287295 -4.4286594 -4.4285636 -4.4284987 -4.4285274 -4.4286227 -4.4287248 -4.4287939 -4.428823 -4.4288211 -4.4288082][-4.4287844 -4.4287992 -4.4288015 -4.4287853 -4.4287491 -4.428688 -4.428607 -4.4285455 -4.4285569 -4.428617 -4.4286771 -4.4287276 -4.4287515 -4.4287591 -4.4287696][-4.4287958 -4.428822 -4.4288278 -4.4288158 -4.4287853 -4.4287343 -4.4286666 -4.4286146 -4.4286079 -4.4286284 -4.4286485 -4.4286618 -4.4286637 -4.4286833 -4.4287233][-4.4287858 -4.4288321 -4.4288507 -4.4288406 -4.4288139 -4.4287691 -4.4287133 -4.4286695 -4.4286566 -4.4286566 -4.4286442 -4.4286275 -4.4286084 -4.4286323 -4.42869][-4.4287453 -4.4287996 -4.4288316 -4.4288344 -4.4288173 -4.4287767 -4.4287124 -4.4286647 -4.4286566 -4.4286637 -4.428659 -4.4286437 -4.4286194 -4.4286304 -4.428678][-4.42868 -4.4287267 -4.4287806 -4.4287996 -4.4287853 -4.4287372 -4.4286432 -4.4285684 -4.4285593 -4.4286056 -4.4286494 -4.4286671 -4.4286618 -4.4286685 -4.4287024][-4.4286175 -4.4286542 -4.428731 -4.4287658 -4.4287505 -4.4286876 -4.4285603 -4.42844 -4.4284172 -4.4285059 -4.4286141 -4.4286819 -4.4287128 -4.4287319 -4.4287586][-4.4285817 -4.4286 -4.4286718 -4.4287095 -4.4286914 -4.4286294 -4.428503 -4.4283767 -4.4283538 -4.4284506 -4.42858 -4.4286771 -4.4287367 -4.4287648 -4.428791][-4.4285941 -4.4285874 -4.4286327 -4.4286604 -4.4286408 -4.428597 -4.4285097 -4.4284286 -4.4284244 -4.4284935 -4.4285803 -4.4286652 -4.4287214 -4.4287505 -4.4287734][-4.4286103 -4.4286051 -4.4286447 -4.4286623 -4.4286332 -4.428607 -4.4285655 -4.4285355 -4.4285603 -4.42861 -4.4286532 -4.4287066 -4.4287372 -4.4287472 -4.4287548][-4.4286132 -4.428637 -4.4286919 -4.4287062 -4.4286652 -4.4286375 -4.4286132 -4.4286175 -4.4286671 -4.428719 -4.4287443 -4.4287748 -4.4287863 -4.4287791 -4.4287691][-4.428638 -4.4286866 -4.4287391 -4.4287491 -4.428709 -4.4286833 -4.42867 -4.4286876 -4.4287462 -4.4287992 -4.428822 -4.4288445 -4.4288459 -4.42883 -4.4288077][-4.4287033 -4.4287415 -4.4287753 -4.4287853 -4.428762 -4.4287486 -4.4287453 -4.4287634 -4.4288125 -4.4288545 -4.4288759 -4.4288974 -4.4289021 -4.4288893 -4.4288664][-4.4287844 -4.428802 -4.428822 -4.4288411 -4.4288445 -4.428844 -4.4288406 -4.4288454 -4.4288707 -4.4288983 -4.4289207 -4.4289389 -4.4289436 -4.4289355 -4.4289193][-4.4288588 -4.4288664 -4.4288836 -4.4289055 -4.4289184 -4.4289217 -4.4289184 -4.4289141 -4.4289207 -4.4289355 -4.4289536 -4.428967 -4.4289727 -4.4289684 -4.4289575]]...]
INFO - root - 2017-12-10 05:43:05.384076: step 6710, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:05m:38s remains)
INFO - root - 2017-12-10 05:43:08.061898: step 6720, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:44m:02s remains)
INFO - root - 2017-12-10 05:43:10.786999: step 6730, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.281 sec/batch; 25h:27m:54s remains)
INFO - root - 2017-12-10 05:43:13.446654: step 6740, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:45m:58s remains)
INFO - root - 2017-12-10 05:43:16.102583: step 6750, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:58m:59s remains)
INFO - root - 2017-12-10 05:43:18.739856: step 6760, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:17m:56s remains)
INFO - root - 2017-12-10 05:43:21.414519: step 6770, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:13m:01s remains)
INFO - root - 2017-12-10 05:43:24.103055: step 6780, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:07m:44s remains)
INFO - root - 2017-12-10 05:43:26.780372: step 6790, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 25h:53m:05s remains)
INFO - root - 2017-12-10 05:43:29.474481: step 6800, loss = 2.28, batch loss = 2.23 (28.2 examples/sec; 0.283 sec/batch; 25h:38m:31s remains)
2017-12-10 05:43:29.783508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288611 -4.4287744 -4.4287057 -4.4286594 -4.4286332 -4.4286485 -4.4286642 -4.4286551 -4.4286547 -4.4286871 -4.4287229 -4.4287262 -4.4287252 -4.4287233 -4.4287271][-4.42886 -4.4287777 -4.42871 -4.4286618 -4.4286289 -4.4286289 -4.4286251 -4.4286003 -4.4285827 -4.4286103 -4.4286728 -4.4286904 -4.4286714 -4.4286561 -4.4286585][-4.4288611 -4.4287853 -4.4287233 -4.4286771 -4.4286337 -4.4286122 -4.4285979 -4.4285693 -4.428545 -4.4285641 -4.4286408 -4.4286675 -4.4286275 -4.4285984 -4.4285975][-4.4288654 -4.4287939 -4.4287319 -4.4286852 -4.4286323 -4.428597 -4.4285822 -4.4285622 -4.4285407 -4.4285612 -4.428638 -4.4286675 -4.4286304 -4.4285994 -4.4285927][-4.4288697 -4.4287963 -4.4287286 -4.4286752 -4.428617 -4.4285793 -4.4285784 -4.4285769 -4.4285722 -4.4285946 -4.428658 -4.428679 -4.4286604 -4.4286542 -4.4286485][-4.4288735 -4.4287949 -4.4287186 -4.42866 -4.4285994 -4.428565 -4.4285622 -4.42857 -4.4285817 -4.428607 -4.4286566 -4.4286752 -4.4286809 -4.428699 -4.4287014][-4.428865 -4.4287705 -4.4286776 -4.4286113 -4.4285555 -4.4285307 -4.4285164 -4.4285135 -4.4285231 -4.428544 -4.4285917 -4.4286232 -4.4286485 -4.4286823 -4.4286928][-4.4288516 -4.4287386 -4.4286256 -4.4285445 -4.4284911 -4.4284682 -4.4284382 -4.4284182 -4.4284191 -4.4284334 -4.4284821 -4.428525 -4.4285684 -4.4286189 -4.4286494][-4.4288378 -4.4287162 -4.4285936 -4.4284992 -4.4284439 -4.42842 -4.428391 -4.4283643 -4.4283547 -4.4283614 -4.4284019 -4.4284472 -4.4285088 -4.4285746 -4.4286294][-4.4288373 -4.428719 -4.4286075 -4.4285131 -4.4284472 -4.4284244 -4.4284139 -4.4284058 -4.4284034 -4.4284105 -4.4284368 -4.428472 -4.4285297 -4.4285989 -4.4286637][-4.4288616 -4.4287605 -4.42867 -4.4285903 -4.4285278 -4.4285064 -4.4285088 -4.4285207 -4.4285326 -4.428544 -4.4285603 -4.4285836 -4.4286275 -4.4286852 -4.4287448][-4.4289074 -4.4288335 -4.4287677 -4.4287148 -4.428669 -4.4286485 -4.4286523 -4.4286709 -4.4286904 -4.4287019 -4.4287133 -4.4287295 -4.4287577 -4.4287939 -4.428833][-4.4289517 -4.4289031 -4.4288564 -4.4288244 -4.4287992 -4.4287834 -4.4287872 -4.4288073 -4.4288235 -4.4288292 -4.4288335 -4.4288406 -4.4288526 -4.4288678 -4.4288855][-4.4289808 -4.42895 -4.4289165 -4.4288974 -4.4288859 -4.4288774 -4.4288797 -4.4288931 -4.4289017 -4.4289045 -4.428906 -4.4289069 -4.4289103 -4.428916 -4.4289246][-4.4289994 -4.4289856 -4.428967 -4.4289575 -4.4289541 -4.4289503 -4.4289513 -4.4289575 -4.4289622 -4.4289641 -4.4289646 -4.4289637 -4.4289632 -4.4289637 -4.4289656]]...]
INFO - root - 2017-12-10 05:43:32.445622: step 6810, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:06m:31s remains)
INFO - root - 2017-12-10 05:43:35.079240: step 6820, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:38m:05s remains)
INFO - root - 2017-12-10 05:43:37.731246: step 6830, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:07m:26s remains)
INFO - root - 2017-12-10 05:43:40.403641: step 6840, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:12m:59s remains)
INFO - root - 2017-12-10 05:43:43.091930: step 6850, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.285 sec/batch; 25h:48m:16s remains)
INFO - root - 2017-12-10 05:43:45.763268: step 6860, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:22m:45s remains)
INFO - root - 2017-12-10 05:43:48.400106: step 6870, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:55m:07s remains)
INFO - root - 2017-12-10 05:43:51.059080: step 6880, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:52m:04s remains)
INFO - root - 2017-12-10 05:43:53.717691: step 6890, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:57m:30s remains)
INFO - root - 2017-12-10 05:43:56.397974: step 6900, loss = 2.28, batch loss = 2.23 (27.8 examples/sec; 0.287 sec/batch; 25h:58m:52s remains)
2017-12-10 05:43:56.718840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288383 -4.4287839 -4.4287519 -4.4287214 -4.4286757 -4.42865 -4.4286456 -4.4286819 -4.4287271 -4.4287577 -4.42878 -4.4288011 -4.4288006 -4.4287672 -4.4287214][-4.4288116 -4.4287519 -4.4287257 -4.4287086 -4.4286642 -4.4286284 -4.4286132 -4.4286547 -4.4287148 -4.4287515 -4.4287734 -4.4287891 -4.428781 -4.4287362 -4.4286819][-4.4287992 -4.4287348 -4.4287133 -4.4287071 -4.4286723 -4.4286342 -4.4286056 -4.4286466 -4.4287176 -4.42876 -4.428782 -4.4287944 -4.4287772 -4.4287133 -4.4286385][-4.4287906 -4.428721 -4.4286919 -4.4286847 -4.4286623 -4.4286208 -4.4285722 -4.4286022 -4.4286823 -4.4287348 -4.428771 -4.4287848 -4.428762 -4.4286942 -4.4286084][-4.4287934 -4.4287181 -4.428678 -4.4286609 -4.4286375 -4.4285812 -4.4284968 -4.4285135 -4.4286165 -4.4286952 -4.4287596 -4.4287848 -4.4287639 -4.4286971 -4.4286137][-4.4288144 -4.4287338 -4.4286695 -4.4286208 -4.4285669 -4.4284616 -4.4283195 -4.4283371 -4.428514 -4.4286628 -4.428762 -4.4288039 -4.4287944 -4.4287295 -4.4286518][-4.4288378 -4.4287539 -4.4286666 -4.428566 -4.4284487 -4.4282455 -4.4280071 -4.42808 -4.4283881 -4.428627 -4.4287653 -4.4288335 -4.4288383 -4.428793 -4.428731][-4.4288554 -4.4287729 -4.4286871 -4.4285731 -4.4284167 -4.4281335 -4.4278374 -4.4279633 -4.4283347 -4.4286 -4.42876 -4.4288459 -4.4288616 -4.4288416 -4.4287992][-4.42885 -4.4287686 -4.4287014 -4.4286194 -4.4284925 -4.4282613 -4.4280639 -4.4281745 -4.4284439 -4.428628 -4.4287524 -4.4288235 -4.4288425 -4.428844 -4.42882][-4.4288549 -4.4287739 -4.4287186 -4.4286633 -4.4285789 -4.428431 -4.4283276 -4.4284058 -4.4285655 -4.4286728 -4.4287472 -4.4287939 -4.4288087 -4.4288197 -4.428802][-4.4288883 -4.4288158 -4.4287691 -4.4287238 -4.42866 -4.428555 -4.4284844 -4.4285345 -4.4286261 -4.4286895 -4.4287419 -4.4287777 -4.428781 -4.4287844 -4.4287696][-4.4289227 -4.4288573 -4.4288149 -4.4287705 -4.4287105 -4.4286194 -4.4285603 -4.4286056 -4.428668 -4.4286971 -4.4287319 -4.4287639 -4.4287648 -4.4287648 -4.428751][-4.4289436 -4.4288878 -4.4288492 -4.428803 -4.4287429 -4.428659 -4.4286075 -4.4286504 -4.4286966 -4.428709 -4.4287353 -4.4287624 -4.4287682 -4.4287686 -4.4287572][-4.4289622 -4.4289231 -4.4288907 -4.4288445 -4.4287877 -4.4287133 -4.4286652 -4.4286928 -4.428719 -4.4287276 -4.4287529 -4.4287839 -4.4287953 -4.4287968 -4.4287853][-4.4289684 -4.4289436 -4.428915 -4.4288764 -4.42883 -4.4287724 -4.4287329 -4.4287434 -4.42876 -4.4287729 -4.428802 -4.4288344 -4.4288483 -4.4288459 -4.4288316]]...]
INFO - root - 2017-12-10 05:43:59.367674: step 6910, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:44m:19s remains)
INFO - root - 2017-12-10 05:44:02.006250: step 6920, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 26h:08m:47s remains)
INFO - root - 2017-12-10 05:44:04.601096: step 6930, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:03m:47s remains)
INFO - root - 2017-12-10 05:44:07.240541: step 6940, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:14m:40s remains)
INFO - root - 2017-12-10 05:44:09.935015: step 6950, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:23m:52s remains)
INFO - root - 2017-12-10 05:44:12.522865: step 6960, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:58m:53s remains)
INFO - root - 2017-12-10 05:44:15.162440: step 6970, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:46m:11s remains)
INFO - root - 2017-12-10 05:44:17.816030: step 6980, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:33m:36s remains)
INFO - root - 2017-12-10 05:44:20.500835: step 6990, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:53m:25s remains)
INFO - root - 2017-12-10 05:44:23.145242: step 7000, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:27m:35s remains)
2017-12-10 05:44:23.440564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286709 -4.4286928 -4.4286923 -4.4286628 -4.4286194 -4.4285879 -4.42857 -4.4285622 -4.4285622 -4.4285398 -4.4285269 -4.4285507 -4.4285836 -4.4286056 -4.4286203][-4.4286408 -4.4286604 -4.4286647 -4.4286475 -4.428617 -4.4285893 -4.4285707 -4.4285593 -4.4285512 -4.4285312 -4.4285192 -4.42854 -4.4285679 -4.4285841 -4.4285908][-4.4286141 -4.4286127 -4.4286103 -4.4286008 -4.4285774 -4.4285493 -4.4285326 -4.4285331 -4.4285383 -4.4285469 -4.428544 -4.4285464 -4.4285502 -4.4285531 -4.4285522][-4.428597 -4.4285855 -4.4285879 -4.428575 -4.4285421 -4.4285054 -4.428494 -4.428503 -4.4285212 -4.42856 -4.4285722 -4.4285579 -4.4285369 -4.4285345 -4.4285378][-4.4285979 -4.4285903 -4.4285989 -4.4285841 -4.4285464 -4.4284964 -4.4284639 -4.4284563 -4.4284854 -4.4285531 -4.4285965 -4.4285812 -4.4285445 -4.42854 -4.4285526][-4.4285951 -4.4285936 -4.428606 -4.4286 -4.4285693 -4.4285049 -4.4284363 -4.4283986 -4.4284477 -4.428555 -4.4286323 -4.4286356 -4.4286094 -4.4286094 -4.4286151][-4.428566 -4.4285817 -4.4286084 -4.4286118 -4.4285789 -4.4284968 -4.4283805 -4.4283066 -4.4283919 -4.4285455 -4.4286537 -4.4286842 -4.428679 -4.428678 -4.428668][-4.4284811 -4.4285216 -4.4285669 -4.4285774 -4.4285331 -4.4284253 -4.4282541 -4.4281564 -4.428309 -4.4285164 -4.4286375 -4.4286809 -4.4286914 -4.4286923 -4.4286842][-4.4284043 -4.4284606 -4.4285188 -4.4285283 -4.42848 -4.4283652 -4.4281912 -4.4281244 -4.4283042 -4.4284954 -4.4285951 -4.4286351 -4.4286509 -4.4286613 -4.4286737][-4.4283943 -4.42846 -4.4285145 -4.4285245 -4.42849 -4.4284096 -4.428297 -4.4282789 -4.4283938 -4.4285049 -4.4285569 -4.4285779 -4.4285913 -4.428607 -4.4286289][-4.4284463 -4.4285183 -4.4285636 -4.4285684 -4.4285488 -4.4285011 -4.4284372 -4.4284315 -4.4284863 -4.42853 -4.4285421 -4.4285512 -4.4285536 -4.4285583 -4.4285722][-4.428524 -4.4285994 -4.4286394 -4.4286346 -4.4286079 -4.4285684 -4.4285192 -4.4285073 -4.4285231 -4.4285359 -4.4285421 -4.4285593 -4.4285579 -4.4285426 -4.4285412][-4.4286194 -4.4286847 -4.4287109 -4.4286947 -4.4286594 -4.4286113 -4.4285541 -4.4285345 -4.4285398 -4.4285522 -4.4285603 -4.4285822 -4.4285789 -4.4285545 -4.4285517][-4.4286671 -4.4287157 -4.428731 -4.42871 -4.4286785 -4.428628 -4.4285731 -4.4285645 -4.4285817 -4.4285927 -4.4285936 -4.4286046 -4.4285994 -4.4285784 -4.4285836][-4.4286561 -4.4286885 -4.4286952 -4.4286766 -4.4286609 -4.42863 -4.4285879 -4.428587 -4.4286113 -4.4286175 -4.4286103 -4.4286118 -4.4286056 -4.4285932 -4.4286013]]...]
INFO - root - 2017-12-10 05:44:26.085352: step 7010, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:25m:44s remains)
INFO - root - 2017-12-10 05:44:28.739743: step 7020, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:38m:01s remains)
INFO - root - 2017-12-10 05:44:31.372568: step 7030, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:04m:19s remains)
INFO - root - 2017-12-10 05:44:34.041152: step 7040, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:45m:41s remains)
INFO - root - 2017-12-10 05:44:36.677499: step 7050, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:06m:38s remains)
INFO - root - 2017-12-10 05:44:39.314343: step 7060, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:08m:06s remains)
INFO - root - 2017-12-10 05:44:41.975194: step 7070, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:20m:20s remains)
INFO - root - 2017-12-10 05:44:44.694962: step 7080, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:17m:50s remains)
INFO - root - 2017-12-10 05:44:47.352664: step 7090, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:26m:35s remains)
INFO - root - 2017-12-10 05:44:50.047720: step 7100, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:17m:12s remains)
2017-12-10 05:44:50.371075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288187 -4.4287887 -4.4287696 -4.4287424 -4.4287024 -4.4286766 -4.4286675 -4.4286857 -4.4286981 -4.4286923 -4.4287062 -4.4287548 -4.4287939 -4.428771 -4.4287314][-4.4287825 -4.4287524 -4.42874 -4.4287205 -4.4286895 -4.4286613 -4.4286556 -4.4286923 -4.4287276 -4.4287276 -4.4287267 -4.4287605 -4.428792 -4.4287591 -4.4287109][-4.428731 -4.4287109 -4.4287109 -4.4287009 -4.4286819 -4.4286623 -4.4286637 -4.4287176 -4.4287758 -4.4287891 -4.4287848 -4.428793 -4.4288015 -4.4287648 -4.4287162][-4.4286761 -4.4286633 -4.4286737 -4.4286761 -4.428668 -4.4286547 -4.4286585 -4.4287138 -4.4287777 -4.4288068 -4.42882 -4.4288254 -4.4288197 -4.4287853 -4.4287419][-4.4286518 -4.4286385 -4.4286528 -4.4286628 -4.4286628 -4.4286466 -4.4286308 -4.4286537 -4.4286885 -4.4287281 -4.4287782 -4.4288096 -4.4288149 -4.4287934 -4.4287624][-4.4286866 -4.428669 -4.4286737 -4.4286828 -4.42868 -4.428647 -4.4285927 -4.4285426 -4.4285078 -4.4285431 -4.42865 -4.4287395 -4.4287786 -4.42878 -4.4287705][-4.4287572 -4.428751 -4.42875 -4.4287491 -4.4287348 -4.4286742 -4.428566 -4.4284215 -4.4282851 -4.4282875 -4.42845 -4.42862 -4.4287114 -4.4287462 -4.428771][-4.4288054 -4.4288259 -4.4288387 -4.4288373 -4.428812 -4.4287391 -4.4285975 -4.4284005 -4.4281988 -4.4281521 -4.4283133 -4.428515 -4.4286351 -4.4287009 -4.4287624][-4.4287934 -4.4288449 -4.4288759 -4.4288831 -4.428865 -4.4288054 -4.4286852 -4.4285145 -4.4283462 -4.4282837 -4.4283648 -4.4284916 -4.4285779 -4.4286427 -4.4287224][-4.4287343 -4.428802 -4.4288445 -4.4288564 -4.4288554 -4.42883 -4.4287605 -4.4286537 -4.4285483 -4.4284978 -4.4285107 -4.4285412 -4.4285531 -4.4285831 -4.4286451][-4.4286604 -4.4287195 -4.4287615 -4.4287796 -4.428792 -4.4287953 -4.4287772 -4.4287353 -4.428688 -4.4286656 -4.4286571 -4.4286346 -4.4285917 -4.4285684 -4.4285917][-4.4286079 -4.4286337 -4.428668 -4.4286976 -4.4287181 -4.4287286 -4.4287305 -4.4287276 -4.4287238 -4.42874 -4.4287496 -4.4287233 -4.42867 -4.4286242 -4.428618][-4.4286208 -4.4286051 -4.428628 -4.4286737 -4.4287014 -4.4287024 -4.4286895 -4.4286819 -4.42869 -4.42873 -4.4287663 -4.428761 -4.4287291 -4.4286966 -4.4286871][-4.4286795 -4.4286542 -4.4286742 -4.4287243 -4.4287572 -4.428751 -4.4287148 -4.4286733 -4.4286613 -4.4286962 -4.4287348 -4.4287462 -4.4287367 -4.4287305 -4.4287333][-4.4287477 -4.4287314 -4.4287453 -4.4287839 -4.4288149 -4.4288187 -4.4287882 -4.42873 -4.4286885 -4.4286795 -4.4286861 -4.4286952 -4.4287019 -4.428719 -4.4287357]]...]
INFO - root - 2017-12-10 05:44:53.021028: step 7110, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:34m:44s remains)
INFO - root - 2017-12-10 05:44:55.715194: step 7120, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:48m:01s remains)
INFO - root - 2017-12-10 05:44:58.385454: step 7130, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:18m:11s remains)
INFO - root - 2017-12-10 05:45:01.079074: step 7140, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 24h:55m:36s remains)
INFO - root - 2017-12-10 05:45:03.720528: step 7150, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:53m:23s remains)
INFO - root - 2017-12-10 05:45:06.359540: step 7160, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:29m:09s remains)
INFO - root - 2017-12-10 05:45:08.983429: step 7170, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:46m:42s remains)
INFO - root - 2017-12-10 05:45:11.608479: step 7180, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:38m:14s remains)
INFO - root - 2017-12-10 05:45:14.288704: step 7190, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:37m:52s remains)
INFO - root - 2017-12-10 05:45:16.937649: step 7200, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:59m:40s remains)
2017-12-10 05:45:17.239050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428843 -4.428854 -4.4288616 -4.428863 -4.4288797 -4.4288983 -4.4288917 -4.4288774 -4.4288692 -4.4288554 -4.4288411 -4.428834 -4.4288354 -4.4288487 -4.4288592][-4.4288664 -4.4288697 -4.4288692 -4.4288692 -4.4288831 -4.4288974 -4.4288836 -4.428865 -4.4288554 -4.4288464 -4.4288387 -4.42884 -4.4288478 -4.4288607 -4.4288669][-4.4289017 -4.4288898 -4.4288788 -4.4288721 -4.4288793 -4.4288874 -4.4288692 -4.4288478 -4.4288373 -4.4288311 -4.4288239 -4.4288235 -4.4288259 -4.4288287 -4.4288282][-4.4289269 -4.4289126 -4.4288931 -4.4288778 -4.4288774 -4.4288774 -4.4288535 -4.4288311 -4.4288306 -4.4288411 -4.4288459 -4.428844 -4.428833 -4.4288192 -4.4288068][-4.4289021 -4.4288912 -4.42887 -4.4288497 -4.4288349 -4.4288077 -4.4287653 -4.42874 -4.4287629 -4.4288163 -4.4288568 -4.42887 -4.4288592 -4.4288368 -4.4288225][-4.4288831 -4.4288745 -4.4288449 -4.4287982 -4.4287467 -4.4286795 -4.4285994 -4.4285612 -4.42862 -4.4287333 -4.4288287 -4.428884 -4.4288988 -4.4288859 -4.4288764][-4.4288969 -4.4288888 -4.4288421 -4.4287524 -4.4286485 -4.4285254 -4.4283776 -4.428299 -4.4283948 -4.4285879 -4.4287519 -4.4288678 -4.4289327 -4.4289422 -4.4289279][-4.4289207 -4.4289193 -4.4288554 -4.4287281 -4.4285731 -4.4283819 -4.4281511 -4.4280062 -4.4281268 -4.4284062 -4.428638 -4.4288096 -4.4289231 -4.4289565 -4.42894][-4.4289289 -4.4289351 -4.4288716 -4.4287391 -4.4285703 -4.4283576 -4.4280953 -4.4279113 -4.4280248 -4.4283237 -4.4285746 -4.4287648 -4.428896 -4.4289389 -4.4289188][-4.428916 -4.4289365 -4.4288988 -4.4288006 -4.428659 -4.428483 -4.4282775 -4.4281268 -4.428185 -4.4283972 -4.4285989 -4.4287572 -4.4288626 -4.4288912 -4.4288697][-4.4289188 -4.4289494 -4.4289384 -4.428875 -4.4287705 -4.4286509 -4.4285259 -4.4284358 -4.4284525 -4.4285655 -4.4286938 -4.42879 -4.428844 -4.4288435 -4.4288087][-4.4289517 -4.42898 -4.4289784 -4.4289284 -4.4288487 -4.4287763 -4.4287124 -4.4286695 -4.4286747 -4.4287343 -4.4288077 -4.4288521 -4.428863 -4.4288435 -4.4287953][-4.428966 -4.4289851 -4.4289813 -4.4289427 -4.4288883 -4.4288511 -4.4288306 -4.4288182 -4.4288239 -4.42886 -4.4289031 -4.4289122 -4.4288974 -4.4288697 -4.4288187][-4.428956 -4.4289622 -4.4289541 -4.4289317 -4.428905 -4.4288912 -4.4288931 -4.4288955 -4.4289017 -4.428925 -4.4289527 -4.4289503 -4.4289269 -4.4288979 -4.4288583][-4.4289441 -4.4289427 -4.4289336 -4.4289227 -4.4289131 -4.4289074 -4.4289145 -4.4289231 -4.4289322 -4.428946 -4.4289656 -4.4289651 -4.4289455 -4.4289165 -4.4288831]]...]
INFO - root - 2017-12-10 05:45:19.883099: step 7210, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:21m:16s remains)
INFO - root - 2017-12-10 05:45:22.551580: step 7220, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:30m:29s remains)
INFO - root - 2017-12-10 05:45:25.186308: step 7230, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:19m:49s remains)
INFO - root - 2017-12-10 05:45:27.856781: step 7240, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:28m:23s remains)
INFO - root - 2017-12-10 05:45:30.501140: step 7250, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:13m:28s remains)
INFO - root - 2017-12-10 05:45:33.177369: step 7260, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:03m:26s remains)
INFO - root - 2017-12-10 05:45:35.844650: step 7270, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:55m:48s remains)
INFO - root - 2017-12-10 05:45:38.553113: step 7280, loss = 2.28, batch loss = 2.23 (27.4 examples/sec; 0.292 sec/batch; 26h:23m:03s remains)
INFO - root - 2017-12-10 05:45:41.164597: step 7290, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:29m:39s remains)
INFO - root - 2017-12-10 05:45:43.790556: step 7300, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:13m:31s remains)
2017-12-10 05:45:44.066882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287667 -4.4287472 -4.4287515 -4.4287758 -4.4288006 -4.4288039 -4.4287944 -4.4287992 -4.4288206 -4.4288325 -4.4288187 -4.4287829 -4.4287543 -4.4287486 -4.4287658][-4.4286928 -4.4286451 -4.428638 -4.4286733 -4.4287119 -4.4287238 -4.4287205 -4.4287271 -4.4287534 -4.4287629 -4.4287472 -4.4287152 -4.4286871 -4.4286842 -4.4287066][-4.4286575 -4.4285836 -4.4285607 -4.4285951 -4.428647 -4.4286709 -4.4286819 -4.428699 -4.4287305 -4.4287362 -4.4287257 -4.4287105 -4.4286966 -4.4286985 -4.4287171][-4.4286561 -4.4285741 -4.4285388 -4.4285622 -4.4286127 -4.4286394 -4.428658 -4.4286785 -4.4287114 -4.4287276 -4.4287372 -4.4287448 -4.4287477 -4.4287467 -4.4287562][-4.4286852 -4.428616 -4.4285836 -4.4285879 -4.4286137 -4.4286242 -4.42862 -4.4286208 -4.4286509 -4.4287009 -4.4287534 -4.4287891 -4.4287963 -4.4287815 -4.4287763][-4.42872 -4.4286766 -4.4286566 -4.42865 -4.4286394 -4.4286132 -4.4285488 -4.4284868 -4.4285011 -4.4286094 -4.4287281 -4.4288025 -4.4288211 -4.4288015 -4.4287791][-4.4287491 -4.4287271 -4.4287186 -4.4287081 -4.4286723 -4.4285989 -4.4284639 -4.4283061 -4.4282737 -4.4284606 -4.42866 -4.4287739 -4.4288154 -4.4288077 -4.4287872][-4.4287548 -4.4287443 -4.4287467 -4.4287515 -4.4287224 -4.4286327 -4.4284687 -4.4282589 -4.4281688 -4.4283724 -4.4286075 -4.4287372 -4.4287887 -4.4288006 -4.4287982][-4.4287467 -4.4287581 -4.4287829 -4.4288106 -4.4288058 -4.4287438 -4.4286151 -4.4284577 -4.4283814 -4.4284811 -4.4286413 -4.4287405 -4.428782 -4.428803 -4.4288044][-4.4287338 -4.4287519 -4.4287968 -4.428843 -4.4288559 -4.4288363 -4.4287672 -4.4286795 -4.4286337 -4.4286594 -4.4287248 -4.428761 -4.4287744 -4.4287944 -4.4287944][-4.42872 -4.4287324 -4.4287863 -4.4288378 -4.4288554 -4.4288597 -4.4288454 -4.4288063 -4.4287877 -4.4287982 -4.4288154 -4.4288144 -4.4288087 -4.4288168 -4.4288058][-4.42873 -4.4287314 -4.4287772 -4.4288177 -4.4288306 -4.4288421 -4.4288568 -4.4288526 -4.4288526 -4.428865 -4.4288731 -4.4288688 -4.4288607 -4.42886 -4.4288445][-4.42876 -4.4287477 -4.4287829 -4.4288092 -4.4288082 -4.4288125 -4.4288306 -4.4288411 -4.4288516 -4.4288793 -4.4288974 -4.4288974 -4.4288936 -4.4288898 -4.4288731][-4.4288015 -4.4287891 -4.4288106 -4.4288254 -4.4288154 -4.4288087 -4.4288149 -4.428823 -4.4288368 -4.4288697 -4.428896 -4.4289007 -4.4289 -4.4288964 -4.428885][-4.4288368 -4.42883 -4.4288445 -4.4288507 -4.4288363 -4.4288287 -4.4288297 -4.4288287 -4.42884 -4.4288673 -4.4288917 -4.4288988 -4.4289017 -4.4289017 -4.4288964]]...]
INFO - root - 2017-12-10 05:45:46.715844: step 7310, loss = 2.28, batch loss = 2.23 (27.2 examples/sec; 0.294 sec/batch; 26h:32m:44s remains)
INFO - root - 2017-12-10 05:45:49.361355: step 7320, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:09m:08s remains)
INFO - root - 2017-12-10 05:45:52.038912: step 7330, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:14m:56s remains)
INFO - root - 2017-12-10 05:45:54.717049: step 7340, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:32m:10s remains)
INFO - root - 2017-12-10 05:45:57.395129: step 7350, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:44m:06s remains)
INFO - root - 2017-12-10 05:46:00.014055: step 7360, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:40m:15s remains)
INFO - root - 2017-12-10 05:46:02.684671: step 7370, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:29m:15s remains)
INFO - root - 2017-12-10 05:46:05.372287: step 7380, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:20m:45s remains)
INFO - root - 2017-12-10 05:46:08.099758: step 7390, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:11m:01s remains)
INFO - root - 2017-12-10 05:46:10.794249: step 7400, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:28m:48s remains)
2017-12-10 05:46:11.090874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428812 -4.4288526 -4.4288507 -4.4288278 -4.4287939 -4.4287634 -4.4287024 -4.42862 -4.4285612 -4.4285693 -4.4286265 -4.4286995 -4.4287615 -4.42882 -4.42886][-4.4288592 -4.4288788 -4.428864 -4.4288363 -4.4288106 -4.4287815 -4.4287233 -4.4286332 -4.4285722 -4.4285836 -4.4286356 -4.4287038 -4.4287629 -4.4288197 -4.428864][-4.4288745 -4.4288859 -4.4288697 -4.4288459 -4.4288259 -4.428802 -4.4287438 -4.4286528 -4.428596 -4.428616 -4.4286747 -4.4287467 -4.4288 -4.42885 -4.4288898][-4.4289112 -4.4289122 -4.4288917 -4.428865 -4.4288459 -4.4288168 -4.42875 -4.4286485 -4.4285917 -4.4286251 -4.4287086 -4.4287906 -4.4288406 -4.4288921 -4.4289346][-4.4289522 -4.428946 -4.4289193 -4.4288883 -4.4288573 -4.4288044 -4.4287014 -4.4285655 -4.4285021 -4.4285655 -4.4286833 -4.4287825 -4.4288421 -4.4289026 -4.4289527][-4.4289865 -4.4289756 -4.4289479 -4.4289131 -4.4288526 -4.4287457 -4.4285679 -4.4283628 -4.4282951 -4.4284282 -4.4286065 -4.4287362 -4.4288154 -4.4288878 -4.4289417][-4.42899 -4.4289756 -4.4289517 -4.4289112 -4.4288182 -4.428647 -4.4283757 -4.4280772 -4.4280233 -4.4282651 -4.4285188 -4.4286857 -4.4287939 -4.4288845 -4.4289484][-4.4289842 -4.4289689 -4.4289489 -4.4289017 -4.4287882 -4.428586 -4.4282761 -4.427938 -4.4278965 -4.428196 -4.4284849 -4.4286714 -4.4287934 -4.4288912 -4.4289641][-4.428987 -4.4289737 -4.4289513 -4.4288974 -4.428793 -4.4286308 -4.4283957 -4.4281378 -4.4280791 -4.4282846 -4.4285126 -4.4286847 -4.4288063 -4.4289017 -4.4289732][-4.428997 -4.428987 -4.4289641 -4.4289117 -4.4288259 -4.4287167 -4.428575 -4.4284048 -4.4283328 -4.4284282 -4.42857 -4.4287114 -4.4288278 -4.4289184 -4.4289846][-4.4290028 -4.4289947 -4.4289751 -4.428926 -4.4288535 -4.4287739 -4.4286852 -4.428576 -4.4285135 -4.428556 -4.4286442 -4.4287577 -4.4288616 -4.4289422 -4.4290013][-4.429009 -4.4290042 -4.4289851 -4.4289393 -4.428874 -4.4288034 -4.4287324 -4.4286518 -4.4286108 -4.4286456 -4.4287195 -4.4288125 -4.4288993 -4.4289646 -4.4290113][-4.4290094 -4.4290047 -4.42899 -4.4289508 -4.4288945 -4.42883 -4.4287639 -4.4286985 -4.4286718 -4.4287057 -4.4287772 -4.4288583 -4.428926 -4.4289746 -4.4290094][-4.4289975 -4.4289947 -4.4289851 -4.4289579 -4.428916 -4.4288616 -4.4288015 -4.4287472 -4.4287262 -4.4287534 -4.4288154 -4.4288807 -4.4289336 -4.4289703 -4.428997][-4.4289732 -4.4289794 -4.4289794 -4.4289637 -4.4289341 -4.4288931 -4.4288459 -4.4288049 -4.4287891 -4.428812 -4.4288588 -4.428905 -4.4289408 -4.428967 -4.4289856]]...]
INFO - root - 2017-12-10 05:46:13.716920: step 7410, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:39m:52s remains)
INFO - root - 2017-12-10 05:46:16.390161: step 7420, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:30m:32s remains)
INFO - root - 2017-12-10 05:46:19.023572: step 7430, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:21m:06s remains)
INFO - root - 2017-12-10 05:46:21.697208: step 7440, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:28m:54s remains)
INFO - root - 2017-12-10 05:46:24.358661: step 7450, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:54m:07s remains)
INFO - root - 2017-12-10 05:46:26.944971: step 7460, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:01m:58s remains)
INFO - root - 2017-12-10 05:46:29.589943: step 7470, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:18m:22s remains)
INFO - root - 2017-12-10 05:46:32.220242: step 7480, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:27m:29s remains)
INFO - root - 2017-12-10 05:46:34.869870: step 7490, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:15m:06s remains)
INFO - root - 2017-12-10 05:46:37.540365: step 7500, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:17m:47s remains)
2017-12-10 05:46:37.825553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288836 -4.4288898 -4.4288845 -4.4288735 -4.4288497 -4.4288206 -4.42879 -4.4287629 -4.42877 -4.4288034 -4.4288378 -4.428864 -4.4288855 -4.4289002 -4.4289155][-4.4288754 -4.4288855 -4.4288826 -4.42887 -4.4288292 -4.428772 -4.4287081 -4.4286647 -4.4286828 -4.4287419 -4.4287982 -4.4288445 -4.4288735 -4.4288836 -4.4288931][-4.4288659 -4.4288721 -4.4288621 -4.4288354 -4.4287753 -4.4286833 -4.4285846 -4.4285369 -4.4285769 -4.428668 -4.4287562 -4.4288254 -4.4288592 -4.4288568 -4.428853][-4.4288564 -4.428854 -4.428833 -4.4287887 -4.4287057 -4.428575 -4.4284434 -4.4283972 -4.428473 -4.4286036 -4.4287238 -4.42881 -4.4288473 -4.4288387 -4.428823][-4.4288239 -4.428813 -4.428793 -4.42874 -4.42863 -4.4284649 -4.4283051 -4.4282675 -4.4283919 -4.4285703 -4.4287086 -4.4287977 -4.428834 -4.4288259 -4.4288049][-4.4287829 -4.4287748 -4.4287629 -4.4287066 -4.4285793 -4.428391 -4.4281955 -4.4281592 -4.4283428 -4.4285703 -4.4287224 -4.4288135 -4.4288454 -4.4288306 -4.4288039][-4.42877 -4.4287648 -4.4287524 -4.4286823 -4.4285364 -4.4283233 -4.4280763 -4.4280224 -4.4282813 -4.4285669 -4.428751 -4.4288516 -4.428875 -4.4288507 -4.4288125][-4.4287586 -4.4287596 -4.4287539 -4.4286809 -4.4285254 -4.4283061 -4.4280281 -4.427958 -4.4282651 -4.4285893 -4.4287944 -4.428894 -4.4289012 -4.42886 -4.4288073][-4.4287419 -4.4287529 -4.4287734 -4.4287329 -4.4285975 -4.4284039 -4.4281611 -4.4281096 -4.4283795 -4.4286571 -4.4288268 -4.4288983 -4.4288826 -4.428823 -4.4287548][-4.4287233 -4.4287391 -4.4287729 -4.4287615 -4.4286709 -4.4285359 -4.4283748 -4.4283504 -4.4285288 -4.4287133 -4.4288259 -4.4288611 -4.4288306 -4.4287663 -4.4286833][-4.4287081 -4.4287224 -4.4287605 -4.4287724 -4.4287305 -4.428658 -4.4285736 -4.428566 -4.4286623 -4.4287558 -4.4288092 -4.4288158 -4.4287786 -4.4287138 -4.428628][-4.4287195 -4.4287372 -4.4287639 -4.4287786 -4.4287629 -4.4287319 -4.4286933 -4.4286947 -4.4287429 -4.428793 -4.428813 -4.4288006 -4.4287648 -4.4287066 -4.4286289][-4.4287486 -4.4287558 -4.4287596 -4.4287581 -4.4287391 -4.4287066 -4.42868 -4.4286909 -4.4287386 -4.4287896 -4.4288087 -4.4287977 -4.4287815 -4.4287453 -4.4286909][-4.4287739 -4.4287648 -4.4287424 -4.4287186 -4.4286852 -4.4286423 -4.4286237 -4.4286466 -4.4287028 -4.4287739 -4.4288125 -4.4288149 -4.428813 -4.4288025 -4.4287691][-4.4288082 -4.4287834 -4.4287271 -4.4286766 -4.4286342 -4.4286041 -4.4286094 -4.4286489 -4.4287076 -4.4287844 -4.4288311 -4.4288325 -4.4288325 -4.4288316 -4.42881]]...]
INFO - root - 2017-12-10 05:46:40.483523: step 7510, loss = 2.28, batch loss = 2.23 (26.8 examples/sec; 0.298 sec/batch; 26h:54m:38s remains)
INFO - root - 2017-12-10 05:46:43.134156: step 7520, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:08m:07s remains)
INFO - root - 2017-12-10 05:46:45.800080: step 7530, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:34m:49s remains)
INFO - root - 2017-12-10 05:46:48.481746: step 7540, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:03m:03s remains)
INFO - root - 2017-12-10 05:46:51.133757: step 7550, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 23h:57m:25s remains)
INFO - root - 2017-12-10 05:46:53.766905: step 7560, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:24m:02s remains)
INFO - root - 2017-12-10 05:46:56.390761: step 7570, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 23h:46m:59s remains)
INFO - root - 2017-12-10 05:46:59.016038: step 7580, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:53m:50s remains)
INFO - root - 2017-12-10 05:47:01.679377: step 7590, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:44m:38s remains)
INFO - root - 2017-12-10 05:47:04.311609: step 7600, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:51m:15s remains)
2017-12-10 05:47:04.615880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289656 -4.4289336 -4.4288974 -4.428874 -4.4288607 -4.4288478 -4.4288406 -4.4288516 -4.4288759 -4.4288898 -4.4288912 -4.4289002 -4.4289246 -4.4289403 -4.42894][-4.4289246 -4.4288788 -4.4288244 -4.4287872 -4.4287624 -4.4287376 -4.4287233 -4.4287438 -4.4287829 -4.428813 -4.4288235 -4.4288349 -4.4288616 -4.4288788 -4.4288788][-4.4288898 -4.428834 -4.4287672 -4.4287143 -4.4286766 -4.4286366 -4.4286118 -4.4286408 -4.428689 -4.428731 -4.4287586 -4.42878 -4.4288116 -4.4288273 -4.4288259][-4.428864 -4.4288011 -4.4287286 -4.4286604 -4.428607 -4.4285517 -4.42852 -4.4285564 -4.4286022 -4.4286423 -4.4286771 -4.4287066 -4.4287457 -4.4287715 -4.4287753][-4.4288478 -4.4287782 -4.4286947 -4.4286079 -4.4285412 -4.4284754 -4.42844 -4.4284873 -4.4285288 -4.4285569 -4.428587 -4.4286122 -4.4286551 -4.4286971 -4.4287171][-4.4288435 -4.4287682 -4.4286733 -4.42857 -4.4284792 -4.4283876 -4.4283519 -4.428432 -4.4284983 -4.428524 -4.4285345 -4.4285412 -4.4285841 -4.4286432 -4.4286852][-4.4288425 -4.4287677 -4.4286723 -4.4285636 -4.4284453 -4.428299 -4.4282494 -4.4283791 -4.428493 -4.4285355 -4.4285321 -4.4285216 -4.4285631 -4.42863 -4.4286833][-4.4288487 -4.4287858 -4.4287062 -4.4286118 -4.4284921 -4.428318 -4.4282379 -4.4283733 -4.4285088 -4.4285665 -4.4285645 -4.4285545 -4.4285879 -4.4286375 -4.4286823][-4.4288616 -4.4288158 -4.4287572 -4.4286923 -4.4286113 -4.4284787 -4.4283905 -4.4284692 -4.428566 -4.4286156 -4.4286308 -4.4286423 -4.428668 -4.4286819 -4.4286909][-4.428874 -4.4288411 -4.4287963 -4.4287539 -4.428709 -4.4286313 -4.4285603 -4.4285922 -4.4286442 -4.4286761 -4.4287024 -4.428731 -4.42875 -4.4287362 -4.4287119][-4.4288783 -4.4288478 -4.42881 -4.428782 -4.42876 -4.4287219 -4.4286771 -4.4286942 -4.4287219 -4.4287395 -4.4287677 -4.4287963 -4.4288073 -4.4287868 -4.4287548][-4.4288759 -4.4288387 -4.4287992 -4.4287782 -4.42877 -4.4287596 -4.4287453 -4.4287677 -4.4287896 -4.4287982 -4.4288177 -4.4288359 -4.42884 -4.4288206 -4.4287968][-4.4288764 -4.4288254 -4.4287739 -4.4287515 -4.4287481 -4.428751 -4.4287581 -4.4287968 -4.4288259 -4.4288344 -4.4288435 -4.4288507 -4.4288526 -4.4288359 -4.4288187][-4.4288821 -4.4288206 -4.4287548 -4.42872 -4.4287071 -4.4287109 -4.4287276 -4.42878 -4.428823 -4.4288359 -4.4288363 -4.4288363 -4.42884 -4.4288321 -4.428823][-4.4288931 -4.4288349 -4.4287696 -4.4287271 -4.428699 -4.4286942 -4.4287095 -4.4287639 -4.4288111 -4.4288297 -4.42883 -4.4288311 -4.4288392 -4.4288397 -4.4288378]]...]
INFO - root - 2017-12-10 05:47:07.222721: step 7610, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:28m:11s remains)
INFO - root - 2017-12-10 05:47:09.906577: step 7620, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 25h:42m:13s remains)
INFO - root - 2017-12-10 05:47:12.553153: step 7630, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:52m:32s remains)
INFO - root - 2017-12-10 05:47:15.267850: step 7640, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:32m:19s remains)
INFO - root - 2017-12-10 05:47:17.893496: step 7650, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:36m:54s remains)
INFO - root - 2017-12-10 05:47:20.526036: step 7660, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:26m:08s remains)
INFO - root - 2017-12-10 05:47:23.149569: step 7670, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:38m:36s remains)
INFO - root - 2017-12-10 05:47:25.788442: step 7680, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:23m:31s remains)
INFO - root - 2017-12-10 05:47:28.515623: step 7690, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:02m:05s remains)
INFO - root - 2017-12-10 05:47:31.191495: step 7700, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:04m:33s remains)
2017-12-10 05:47:31.493708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289455 -4.4289517 -4.4289589 -4.4289565 -4.4289465 -4.4289403 -4.4289346 -4.4289351 -4.4289236 -4.4288912 -4.428843 -4.4288139 -4.4288206 -4.4288573 -4.4289117][-4.4288883 -4.428906 -4.4289212 -4.4289217 -4.4289103 -4.4289021 -4.42889 -4.4288917 -4.4288836 -4.4288516 -4.4287968 -4.4287539 -4.4287524 -4.4287915 -4.4288578][-4.4287982 -4.4288316 -4.4288597 -4.4288669 -4.4288497 -4.4288278 -4.4287996 -4.42879 -4.4287848 -4.4287677 -4.4287262 -4.4286842 -4.428679 -4.4287248 -4.4287987][-4.4287319 -4.4287777 -4.4288106 -4.4288158 -4.4287839 -4.4287329 -4.4286714 -4.4286385 -4.4286332 -4.4286447 -4.4286437 -4.42863 -4.4286385 -4.4286909 -4.4287658][-4.4287014 -4.428762 -4.4287977 -4.428791 -4.42873 -4.4286313 -4.42852 -4.4284506 -4.4284453 -4.4284983 -4.4285569 -4.4286 -4.4286475 -4.4287152 -4.4287877][-4.4286919 -4.4287562 -4.4287877 -4.4287615 -4.4286628 -4.4285107 -4.4283381 -4.4282227 -4.4282222 -4.4283309 -4.428473 -4.42859 -4.4286866 -4.428771 -4.4288387][-4.4287028 -4.4287529 -4.4287763 -4.4287419 -4.4286213 -4.4284205 -4.428194 -4.4280677 -4.4281058 -4.4282746 -4.428472 -4.4286251 -4.4287353 -4.4288225 -4.4288826][-4.4287019 -4.4287305 -4.4287581 -4.4287405 -4.4286342 -4.4284425 -4.4282374 -4.4281678 -4.4282508 -4.4284067 -4.4285655 -4.42868 -4.4287658 -4.4288435 -4.4289002][-4.4286819 -4.4286947 -4.4287405 -4.4287577 -4.4286981 -4.42856 -4.4284229 -4.4284086 -4.4284868 -4.42858 -4.4286571 -4.42871 -4.4287705 -4.4288411 -4.4288988][-4.4286666 -4.4286709 -4.4287348 -4.4287872 -4.4287682 -4.4286804 -4.4285979 -4.428606 -4.4286637 -4.4287043 -4.4287233 -4.4287362 -4.4287844 -4.4288483 -4.4289002][-4.4286861 -4.4286785 -4.4287496 -4.4288144 -4.4288063 -4.4287386 -4.4286838 -4.4286962 -4.4287457 -4.4287667 -4.4287677 -4.4287753 -4.4288173 -4.4288716 -4.4289083][-4.4287157 -4.4287014 -4.4287639 -4.4288216 -4.4288082 -4.42875 -4.4287047 -4.4287243 -4.42878 -4.4288125 -4.4288225 -4.4288325 -4.428864 -4.4289017 -4.4289184][-4.428731 -4.4287124 -4.4287634 -4.4288063 -4.428792 -4.4287448 -4.4287152 -4.4287462 -4.42881 -4.4288545 -4.4288745 -4.4288859 -4.4289055 -4.4289227 -4.42892][-4.4287372 -4.4287219 -4.4287672 -4.4288039 -4.4287968 -4.4287667 -4.4287519 -4.4287877 -4.4288425 -4.4288812 -4.4288974 -4.428906 -4.4289155 -4.4289193 -4.4289112][-4.4287229 -4.4287138 -4.4287596 -4.4288034 -4.4288077 -4.4287958 -4.4287925 -4.4288297 -4.4288735 -4.4288993 -4.4289021 -4.4288983 -4.4288974 -4.4288974 -4.4288993]]...]
INFO - root - 2017-12-10 05:47:34.144250: step 7710, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 22h:43m:58s remains)
INFO - root - 2017-12-10 05:47:36.704152: step 7720, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:57m:41s remains)
INFO - root - 2017-12-10 05:47:39.333185: step 7730, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:14m:30s remains)
INFO - root - 2017-12-10 05:47:41.981661: step 7740, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:39m:27s remains)
INFO - root - 2017-12-10 05:47:44.619523: step 7750, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:15m:57s remains)
INFO - root - 2017-12-10 05:47:47.216807: step 7760, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:09m:59s remains)
INFO - root - 2017-12-10 05:47:49.860467: step 7770, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:47m:13s remains)
INFO - root - 2017-12-10 05:47:52.512100: step 7780, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:37m:29s remains)
INFO - root - 2017-12-10 05:47:55.147606: step 7790, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:16m:27s remains)
INFO - root - 2017-12-10 05:47:57.777759: step 7800, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:33m:57s remains)
2017-12-10 05:47:58.077786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288254 -4.4288177 -4.4288063 -4.42879 -4.428781 -4.4287829 -4.4287896 -4.4287982 -4.4288011 -4.4288034 -4.4288187 -4.4288483 -4.4288754 -4.4288878 -4.428885][-4.4287543 -4.4287519 -4.4287524 -4.4287386 -4.4287248 -4.4287224 -4.4287319 -4.4287457 -4.4287539 -4.4287581 -4.4287715 -4.4287982 -4.4288273 -4.4288483 -4.428853][-4.428688 -4.4286876 -4.4286981 -4.4286876 -4.428669 -4.4286551 -4.4286513 -4.4286666 -4.4286885 -4.4287043 -4.4287243 -4.4287462 -4.4287691 -4.4287958 -4.4288092][-4.4286871 -4.4286776 -4.4286814 -4.4286661 -4.4286332 -4.4285975 -4.4285746 -4.428597 -4.4286394 -4.4286704 -4.4287019 -4.4287214 -4.4287353 -4.4287581 -4.4287758][-4.4287224 -4.4287014 -4.4287033 -4.4286871 -4.4286404 -4.4285755 -4.428525 -4.428555 -4.4286246 -4.4286714 -4.4287109 -4.4287324 -4.4287429 -4.4287624 -4.4287734][-4.4287243 -4.4286919 -4.4286838 -4.428658 -4.4286003 -4.4284997 -4.4284019 -4.4284325 -4.4285493 -4.4286313 -4.4286966 -4.4287305 -4.4287467 -4.4287753 -4.4287863][-4.428699 -4.4286571 -4.4286337 -4.42859 -4.4285 -4.4283414 -4.42815 -4.4281707 -4.4283795 -4.4285278 -4.4286289 -4.4286833 -4.4287157 -4.4287572 -4.4287739][-4.4287105 -4.4286666 -4.4286208 -4.42855 -4.42843 -4.4282265 -4.427958 -4.42797 -4.4282594 -4.4284682 -4.4285884 -4.4286523 -4.4286957 -4.4287376 -4.4287558][-4.4287772 -4.42875 -4.4287081 -4.4286523 -4.4285555 -4.4284115 -4.4282231 -4.4282146 -4.4284229 -4.4285932 -4.4286885 -4.428741 -4.428771 -4.428792 -4.4287987][-4.4288383 -4.4288263 -4.4287996 -4.4287667 -4.4286971 -4.4286003 -4.4284859 -4.4284692 -4.4285994 -4.4287224 -4.4287939 -4.4288263 -4.4288387 -4.4288487 -4.4288511][-4.4288793 -4.4288745 -4.4288549 -4.4288325 -4.4287691 -4.4286838 -4.4286046 -4.4285855 -4.4286656 -4.4287663 -4.428834 -4.428864 -4.4288716 -4.4288816 -4.4288855][-4.4288898 -4.4288988 -4.42889 -4.4288635 -4.4288025 -4.4287291 -4.4286737 -4.4286633 -4.4287152 -4.42879 -4.4288425 -4.4288659 -4.42887 -4.4288769 -4.4288788][-4.4288726 -4.428894 -4.4289002 -4.4288807 -4.4288316 -4.4287815 -4.4287524 -4.4287567 -4.42879 -4.4288335 -4.4288635 -4.428874 -4.428874 -4.4288735 -4.4288712][-4.4288545 -4.4288845 -4.4289079 -4.4289007 -4.4288707 -4.4288421 -4.4288344 -4.4288468 -4.428864 -4.428884 -4.4288988 -4.4289041 -4.4289036 -4.4289021 -4.4288988][-4.4288535 -4.4288874 -4.4289203 -4.4289231 -4.4289064 -4.4288931 -4.428895 -4.4289055 -4.428915 -4.4289241 -4.4289303 -4.4289317 -4.4289293 -4.4289265 -4.4289241]]...]
INFO - root - 2017-12-10 05:48:00.712860: step 7810, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:52m:39s remains)
INFO - root - 2017-12-10 05:48:03.359797: step 7820, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:04m:26s remains)
INFO - root - 2017-12-10 05:48:06.057837: step 7830, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:17m:55s remains)
INFO - root - 2017-12-10 05:48:08.705874: step 7840, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:38m:23s remains)
INFO - root - 2017-12-10 05:48:11.344995: step 7850, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:02m:29s remains)
INFO - root - 2017-12-10 05:48:13.994154: step 7860, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 23h:56m:12s remains)
INFO - root - 2017-12-10 05:48:16.673018: step 7870, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:05m:46s remains)
INFO - root - 2017-12-10 05:48:19.307489: step 7880, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:23m:53s remains)
INFO - root - 2017-12-10 05:48:21.977769: step 7890, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:27m:38s remains)
INFO - root - 2017-12-10 05:48:24.579903: step 7900, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:22m:22s remains)
2017-12-10 05:48:24.869137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289017 -4.428885 -4.4288788 -4.428884 -4.4288936 -4.4288983 -4.4288979 -4.4288912 -4.4288874 -4.4288874 -4.4288955 -4.4289107 -4.4289269 -4.4289331 -4.4289351][-4.4288507 -4.4288216 -4.42881 -4.4288225 -4.428844 -4.4288521 -4.4288478 -4.4288406 -4.4288397 -4.4288435 -4.4288545 -4.428875 -4.428895 -4.4289045 -4.4289074][-4.4288092 -4.4287682 -4.4287524 -4.4287724 -4.4288025 -4.4288106 -4.4288049 -4.4287958 -4.4287944 -4.4287992 -4.428812 -4.4288392 -4.428864 -4.4288745 -4.4288774][-4.4287734 -4.4287252 -4.4287109 -4.4287395 -4.4287767 -4.4287844 -4.42878 -4.4287691 -4.4287639 -4.4287696 -4.4287848 -4.4288163 -4.4288449 -4.4288559 -4.4288583][-4.4287591 -4.4287105 -4.4287038 -4.4287348 -4.4287682 -4.428771 -4.4287643 -4.428751 -4.4287448 -4.4287529 -4.4287682 -4.4287977 -4.4288263 -4.4288378 -4.4288421][-4.428761 -4.4287214 -4.4287229 -4.428751 -4.4287729 -4.4287653 -4.4287505 -4.4287367 -4.4287295 -4.4287338 -4.4287457 -4.4287744 -4.428802 -4.4288158 -4.4288249][-4.42877 -4.4287386 -4.4287453 -4.4287663 -4.4287748 -4.428762 -4.4287443 -4.4287286 -4.4287186 -4.4287219 -4.4287362 -4.4287648 -4.4287925 -4.4288058 -4.4288163][-4.4287734 -4.4287429 -4.4287486 -4.428761 -4.4287596 -4.4287491 -4.428731 -4.4287152 -4.4287081 -4.4287143 -4.4287343 -4.4287682 -4.4287958 -4.4288096 -4.4288192][-4.428762 -4.4287257 -4.4287305 -4.4287443 -4.4287434 -4.4287405 -4.4287343 -4.4287286 -4.4287267 -4.4287243 -4.4287391 -4.4287696 -4.4287934 -4.4287996 -4.428802][-4.4287543 -4.4287124 -4.4287143 -4.4287338 -4.4287415 -4.42875 -4.4287548 -4.4287572 -4.4287553 -4.4287429 -4.4287472 -4.4287658 -4.4287777 -4.4287658 -4.4287581][-4.4287448 -4.4287004 -4.4287014 -4.4287314 -4.4287519 -4.4287682 -4.4287796 -4.4287853 -4.428781 -4.42876 -4.428751 -4.4287524 -4.4287448 -4.4287143 -4.4287009][-4.4287314 -4.428689 -4.4286947 -4.4287343 -4.4287643 -4.4287887 -4.4288039 -4.4288058 -4.4287934 -4.4287663 -4.4287448 -4.4287329 -4.4287152 -4.4286757 -4.4286609][-4.4287114 -4.4286785 -4.4286942 -4.42874 -4.4287772 -4.4288054 -4.428822 -4.4288158 -4.428792 -4.4287562 -4.4287257 -4.4287133 -4.4287024 -4.4286728 -4.4286642][-4.4286942 -4.4286747 -4.4286985 -4.4287448 -4.4287848 -4.4288177 -4.4288392 -4.4288316 -4.428803 -4.4287663 -4.4287348 -4.4287291 -4.4287305 -4.4287148 -4.4287095][-4.4287014 -4.4286971 -4.4287262 -4.4287667 -4.4288015 -4.4288349 -4.4288545 -4.4288483 -4.4288239 -4.428793 -4.4287677 -4.4287658 -4.4287705 -4.4287653 -4.4287648]]...]
INFO - root - 2017-12-10 05:48:27.514851: step 7910, loss = 2.28, batch loss = 2.23 (27.9 examples/sec; 0.286 sec/batch; 25h:48m:30s remains)
INFO - root - 2017-12-10 05:48:30.189908: step 7920, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:31m:14s remains)
INFO - root - 2017-12-10 05:48:32.798651: step 7930, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:36m:19s remains)
INFO - root - 2017-12-10 05:48:35.374654: step 7940, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:29m:15s remains)
INFO - root - 2017-12-10 05:48:38.028449: step 7950, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:49m:16s remains)
INFO - root - 2017-12-10 05:48:40.727732: step 7960, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:38m:21s remains)
INFO - root - 2017-12-10 05:48:43.406162: step 7970, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:49m:08s remains)
INFO - root - 2017-12-10 05:48:46.058664: step 7980, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:24m:27s remains)
INFO - root - 2017-12-10 05:48:48.679719: step 7990, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:03m:08s remains)
INFO - root - 2017-12-10 05:48:51.348814: step 8000, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:42m:46s remains)
2017-12-10 05:48:51.668563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287791 -4.4287605 -4.4287672 -4.4287853 -4.4287925 -4.4287872 -4.428781 -4.4287806 -4.4287944 -4.428823 -4.4288464 -4.4288554 -4.428853 -4.4288464 -4.4288406][-4.4287515 -4.4287271 -4.4287357 -4.428751 -4.4287567 -4.4287543 -4.4287567 -4.4287639 -4.428782 -4.4288106 -4.4288387 -4.428854 -4.4288526 -4.4288383 -4.4288216][-4.4287338 -4.4287009 -4.428699 -4.4287 -4.4286861 -4.4286714 -4.4286733 -4.42869 -4.4287238 -4.4287658 -4.4288111 -4.4288425 -4.4288521 -4.4288387 -4.4288192][-4.4287128 -4.4286737 -4.4286571 -4.4286394 -4.4286046 -4.4285703 -4.42856 -4.4285817 -4.4286394 -4.4287095 -4.4287744 -4.4288197 -4.4288445 -4.428844 -4.4288354][-4.4286995 -4.4286628 -4.4286356 -4.428606 -4.4285588 -4.4284978 -4.4284635 -4.4284887 -4.4285712 -4.4286671 -4.4287376 -4.4287858 -4.4288206 -4.4288349 -4.42884][-4.4286947 -4.4286647 -4.4286294 -4.4285889 -4.42853 -4.4284353 -4.42836 -4.428381 -4.4284983 -4.4286256 -4.4287004 -4.4287462 -4.4287891 -4.4288144 -4.4288225][-4.428699 -4.4286647 -4.4286113 -4.4285474 -4.4284635 -4.4283209 -4.4281912 -4.4282088 -4.4283705 -4.4285374 -4.4286304 -4.4286942 -4.4287591 -4.4287963 -4.4288082][-4.4287038 -4.4286571 -4.4285817 -4.4284935 -4.4283862 -4.42822 -4.428061 -4.4280806 -4.4282651 -4.428443 -4.4285445 -4.42863 -4.428719 -4.4287715 -4.4287963][-4.4287062 -4.4286523 -4.4285722 -4.4284811 -4.4283881 -4.428266 -4.4281545 -4.4281688 -4.4282961 -4.4284205 -4.4284983 -4.4285822 -4.4286723 -4.4287291 -4.4287667][-4.4287076 -4.4286532 -4.4285784 -4.4284997 -4.4284396 -4.4283814 -4.4283266 -4.4283319 -4.4283905 -4.4284511 -4.428493 -4.4285526 -4.4286242 -4.4286728 -4.4287152][-4.4286957 -4.4286361 -4.4285555 -4.4284816 -4.4284463 -4.428442 -4.4284377 -4.4284463 -4.4284663 -4.4284916 -4.4285135 -4.4285507 -4.4285994 -4.4286427 -4.4286852][-4.4286857 -4.4286165 -4.4285269 -4.4284573 -4.4284468 -4.42849 -4.4285231 -4.4285359 -4.4285307 -4.4285336 -4.4285417 -4.4285583 -4.4285913 -4.428628 -4.4286642][-4.4287086 -4.4286385 -4.4285526 -4.4284968 -4.4285116 -4.4285779 -4.4286141 -4.4286127 -4.4285879 -4.4285755 -4.42858 -4.4285917 -4.4286175 -4.4286418 -4.4286618][-4.4287639 -4.4287038 -4.4286308 -4.42859 -4.428618 -4.4286838 -4.4287038 -4.4286814 -4.4286423 -4.4286194 -4.42862 -4.4286423 -4.4286771 -4.4286923 -4.4286952][-4.428822 -4.4287825 -4.4287305 -4.4287057 -4.4287381 -4.4287891 -4.4287848 -4.4287462 -4.4287 -4.4286695 -4.4286747 -4.4287119 -4.4287558 -4.4287581 -4.4287448]]...]
INFO - root - 2017-12-10 05:48:54.332386: step 8010, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:15m:38s remains)
INFO - root - 2017-12-10 05:48:56.952731: step 8020, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:26m:34s remains)
INFO - root - 2017-12-10 05:48:59.573140: step 8030, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:23m:56s remains)
INFO - root - 2017-12-10 05:49:02.237601: step 8040, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:39m:22s remains)
INFO - root - 2017-12-10 05:49:04.901629: step 8050, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:49m:58s remains)
INFO - root - 2017-12-10 05:49:07.525035: step 8060, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:09m:04s remains)
INFO - root - 2017-12-10 05:49:10.157067: step 8070, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:38m:35s remains)
INFO - root - 2017-12-10 05:49:12.800585: step 8080, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:35m:30s remains)
INFO - root - 2017-12-10 05:49:15.437206: step 8090, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:47m:08s remains)
INFO - root - 2017-12-10 05:49:18.069229: step 8100, loss = 2.28, batch loss = 2.23 (27.4 examples/sec; 0.292 sec/batch; 26h:19m:41s remains)
2017-12-10 05:49:18.372310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287529 -4.4287081 -4.4286957 -4.4287114 -4.428741 -4.4287505 -4.42874 -4.4287415 -4.428761 -4.4287658 -4.4287696 -4.4287925 -4.4288154 -4.4288311 -4.4288359][-4.4287434 -4.4287019 -4.428679 -4.4286828 -4.4287057 -4.4287047 -4.4286933 -4.4287138 -4.4287562 -4.4287744 -4.4287753 -4.4287853 -4.4287958 -4.4288073 -4.4288139][-4.4287338 -4.4287004 -4.4286714 -4.4286628 -4.4286795 -4.4286771 -4.4286737 -4.4287181 -4.4287782 -4.4288054 -4.4288082 -4.4288039 -4.4287963 -4.4287992 -4.4288049][-4.4287276 -4.4287047 -4.4286723 -4.4286485 -4.4286518 -4.4286432 -4.4286485 -4.4287171 -4.42879 -4.4288287 -4.4288406 -4.4288297 -4.4288068 -4.4288011 -4.4288039][-4.4287271 -4.4287124 -4.4286785 -4.4286356 -4.4286151 -4.4285865 -4.42859 -4.4286804 -4.4287677 -4.42882 -4.4288449 -4.4288354 -4.4288058 -4.4287982 -4.4288054][-4.4287205 -4.42871 -4.4286795 -4.4286265 -4.4285822 -4.4285164 -4.4284954 -4.4286022 -4.4287176 -4.4287891 -4.4288235 -4.4288173 -4.4287925 -4.4287915 -4.4288049][-4.4287267 -4.4287128 -4.4286842 -4.4286318 -4.4285665 -4.4284606 -4.4284029 -4.428515 -4.4286618 -4.4287534 -4.4287944 -4.4287939 -4.4287834 -4.4287953 -4.4288149][-4.4287553 -4.4287233 -4.4286923 -4.4286485 -4.4285865 -4.4284778 -4.4284043 -4.4285 -4.4286485 -4.4287429 -4.4287806 -4.4287853 -4.428791 -4.4288182 -4.4288387][-4.4287958 -4.4287429 -4.4287052 -4.4286766 -4.4286451 -4.4285774 -4.4285221 -4.4285831 -4.4286952 -4.4287682 -4.4287944 -4.4288011 -4.4288163 -4.4288511 -4.4288673][-4.4288378 -4.4287744 -4.4287314 -4.4287138 -4.4287157 -4.4286914 -4.4286666 -4.4287033 -4.4287691 -4.4288106 -4.4288239 -4.4288311 -4.4288483 -4.4288816 -4.4288907][-4.428865 -4.4288039 -4.428761 -4.4287496 -4.4287739 -4.4287834 -4.4287844 -4.4288063 -4.4288344 -4.4288468 -4.4288497 -4.4288535 -4.4288673 -4.4288979 -4.4289017][-4.4288783 -4.4288287 -4.4287896 -4.4287815 -4.4288187 -4.4288473 -4.4288597 -4.4288735 -4.4288788 -4.4288735 -4.4288692 -4.4288707 -4.4288821 -4.4289064 -4.4289041][-4.4288869 -4.4288578 -4.4288268 -4.4288187 -4.428853 -4.4288874 -4.4289002 -4.4289074 -4.4289017 -4.4288921 -4.4288893 -4.4288912 -4.4289017 -4.4289169 -4.428906][-4.4288964 -4.4288931 -4.4288721 -4.4288573 -4.4288793 -4.42891 -4.42892 -4.42892 -4.4289107 -4.4289007 -4.4289012 -4.4289074 -4.4289193 -4.4289274 -4.428906][-4.4289126 -4.4289269 -4.4289126 -4.4288907 -4.4289 -4.4289222 -4.42893 -4.428926 -4.4289188 -4.4289093 -4.42891 -4.428915 -4.4289231 -4.4289269 -4.4289017]]...]
INFO - root - 2017-12-10 05:49:20.982698: step 8110, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:10m:23s remains)
INFO - root - 2017-12-10 05:49:23.662439: step 8120, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:28m:00s remains)
INFO - root - 2017-12-10 05:49:26.336218: step 8130, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:36m:59s remains)
INFO - root - 2017-12-10 05:49:29.005489: step 8140, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:32m:50s remains)
INFO - root - 2017-12-10 05:49:31.685647: step 8150, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:23m:25s remains)
INFO - root - 2017-12-10 05:49:34.354875: step 8160, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:32m:20s remains)
INFO - root - 2017-12-10 05:49:37.014842: step 8170, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:23m:14s remains)
INFO - root - 2017-12-10 05:49:39.663402: step 8180, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:24m:34s remains)
INFO - root - 2017-12-10 05:49:42.280747: step 8190, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:08m:17s remains)
INFO - root - 2017-12-10 05:49:44.960465: step 8200, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:23m:35s remains)
2017-12-10 05:49:45.249715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290147 -4.4290133 -4.4289827 -4.4289122 -4.4288354 -4.4287682 -4.4287014 -4.4286566 -4.4286871 -4.4287076 -4.4286904 -4.4286847 -4.428709 -4.4287591 -4.4288111][-4.429028 -4.42903 -4.4289923 -4.4289112 -4.4288225 -4.4287453 -4.4286647 -4.42862 -4.4286618 -4.4286952 -4.4286909 -4.4286909 -4.4287162 -4.428762 -4.4288063][-4.4290309 -4.4290276 -4.4289808 -4.4288945 -4.4287987 -4.4287143 -4.4286284 -4.4285951 -4.4286551 -4.4287086 -4.4287286 -4.4287419 -4.4287686 -4.4288034 -4.4288311][-4.4290218 -4.42901 -4.4289532 -4.4288664 -4.4287715 -4.4286866 -4.4286065 -4.4285846 -4.4286575 -4.4287271 -4.4287663 -4.428791 -4.4288192 -4.4288445 -4.4288611][-4.4290161 -4.4290023 -4.4289436 -4.428854 -4.4287539 -4.4286594 -4.428566 -4.4285316 -4.4286094 -4.428699 -4.4287577 -4.4287968 -4.4288349 -4.4288654 -4.4288836][-4.4290171 -4.4290104 -4.4289584 -4.4288688 -4.4287577 -4.4286385 -4.4285007 -4.4284139 -4.4284782 -4.4285865 -4.4286747 -4.4287438 -4.4288073 -4.4288568 -4.428885][-4.4290218 -4.4290261 -4.4289851 -4.428896 -4.4287715 -4.4286141 -4.4284034 -4.4282289 -4.4282584 -4.4283819 -4.4285159 -4.4286308 -4.42873 -4.4288011 -4.4288435][-4.4290233 -4.4290361 -4.4290051 -4.4289174 -4.4287872 -4.4286013 -4.428329 -4.4280849 -4.4280925 -4.4282241 -4.4283848 -4.4285288 -4.42865 -4.4287295 -4.4287786][-4.4290195 -4.4290357 -4.4290113 -4.4289336 -4.4288173 -4.428637 -4.4283633 -4.4281225 -4.4281373 -4.4282637 -4.4284067 -4.4285336 -4.4286413 -4.4287043 -4.4287424][-4.4290133 -4.4290257 -4.4290032 -4.4289393 -4.4288497 -4.4287086 -4.4284825 -4.4282956 -4.4283257 -4.42843 -4.4285345 -4.4286213 -4.4286962 -4.4287324 -4.4287515][-4.4290028 -4.429009 -4.4289861 -4.4289293 -4.428864 -4.4287672 -4.4286051 -4.4284749 -4.4285135 -4.4285955 -4.4286623 -4.4287124 -4.4287591 -4.4287777 -4.4287863][-4.4289927 -4.4289942 -4.4289746 -4.4289246 -4.4288759 -4.4288111 -4.4287009 -4.4286151 -4.4286518 -4.4287119 -4.4287505 -4.4287763 -4.4288011 -4.42881 -4.4288168][-4.428988 -4.4289837 -4.428966 -4.4289212 -4.4288816 -4.4288378 -4.4287686 -4.4287162 -4.4287548 -4.428803 -4.4288239 -4.4288306 -4.4288359 -4.4288368 -4.42884][-4.42899 -4.4289832 -4.428968 -4.4289255 -4.4288855 -4.4288449 -4.4287953 -4.4287596 -4.4288015 -4.428853 -4.428874 -4.4288774 -4.428875 -4.4288697 -4.4288673][-4.4289966 -4.4289918 -4.428978 -4.4289336 -4.428884 -4.4288282 -4.4287705 -4.4287248 -4.4287724 -4.4288397 -4.4288788 -4.428896 -4.4288993 -4.4288979 -4.4288936]]...]
INFO - root - 2017-12-10 05:49:47.890927: step 8210, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:42m:52s remains)
INFO - root - 2017-12-10 05:49:50.551076: step 8220, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:40m:30s remains)
INFO - root - 2017-12-10 05:49:53.213321: step 8230, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:49m:24s remains)
INFO - root - 2017-12-10 05:49:55.852507: step 8240, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:01m:03s remains)
INFO - root - 2017-12-10 05:49:58.505572: step 8250, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:57m:23s remains)
INFO - root - 2017-12-10 05:50:01.177991: step 8260, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:35m:41s remains)
INFO - root - 2017-12-10 05:50:03.871786: step 8270, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:23m:40s remains)
INFO - root - 2017-12-10 05:50:06.550926: step 8280, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:05m:22s remains)
INFO - root - 2017-12-10 05:50:09.182016: step 8290, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:34m:31s remains)
INFO - root - 2017-12-10 05:50:11.881264: step 8300, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:34m:14s remains)
2017-12-10 05:50:12.170527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288077 -4.4287939 -4.4288058 -4.4288406 -4.4288883 -4.42892 -4.4289265 -4.4289222 -4.4289122 -4.4289036 -4.4288926 -4.4288764 -4.4288588 -4.4288497 -4.4288425][-4.4287734 -4.4287524 -4.4287605 -4.4287939 -4.4288425 -4.4288783 -4.4288907 -4.4288931 -4.4288869 -4.4288859 -4.4288845 -4.4288783 -4.42887 -4.4288683 -4.4288645][-4.4287252 -4.4287024 -4.4287066 -4.4287271 -4.428761 -4.4287863 -4.4287972 -4.4288044 -4.4288063 -4.4288144 -4.4288235 -4.4288306 -4.4288406 -4.4288549 -4.428865][-4.4286985 -4.4286866 -4.4286933 -4.4287038 -4.4287186 -4.4287238 -4.4287186 -4.4287219 -4.4287319 -4.4287443 -4.4287596 -4.4287772 -4.4287996 -4.4288259 -4.4288468][-4.4286909 -4.42869 -4.4287047 -4.4287133 -4.4287176 -4.4287043 -4.4286771 -4.428668 -4.4286785 -4.4286933 -4.4287119 -4.4287391 -4.42877 -4.4288025 -4.4288316][-4.4286718 -4.4286838 -4.42871 -4.4287195 -4.4287181 -4.4286895 -4.4286413 -4.428617 -4.4286213 -4.4286375 -4.4286666 -4.4287081 -4.4287524 -4.4287949 -4.42883][-4.428658 -4.4286823 -4.4287243 -4.42874 -4.4287367 -4.4287028 -4.4286389 -4.4285927 -4.4285812 -4.42859 -4.4286265 -4.4286847 -4.4287429 -4.4287944 -4.4288292][-4.4286618 -4.4286914 -4.42874 -4.4287581 -4.4287529 -4.4287133 -4.4286389 -4.4285669 -4.4285293 -4.428524 -4.428565 -4.4286427 -4.4287219 -4.4287848 -4.4288216][-4.4286828 -4.4287152 -4.4287634 -4.4287863 -4.4287872 -4.4287567 -4.4286866 -4.4286017 -4.4285331 -4.4285021 -4.4285364 -4.4286184 -4.4287043 -4.4287748 -4.4288163][-4.4287248 -4.4287534 -4.428792 -4.4288192 -4.428834 -4.428823 -4.4287748 -4.4287057 -4.4286256 -4.4285812 -4.4285932 -4.4286489 -4.4287148 -4.4287715 -4.4288087][-4.4287643 -4.4287939 -4.4288216 -4.4288435 -4.4288616 -4.4288578 -4.4288249 -4.4287724 -4.4287066 -4.4286714 -4.4286675 -4.4286909 -4.4287229 -4.4287539 -4.4287786][-4.428793 -4.4288211 -4.4288464 -4.4288611 -4.4288774 -4.4288797 -4.4288578 -4.4288225 -4.4287796 -4.4287539 -4.4287395 -4.4287333 -4.4287333 -4.4287419 -4.4287567][-4.4288177 -4.4288425 -4.428863 -4.4288754 -4.4288917 -4.4289017 -4.4288907 -4.4288726 -4.4288473 -4.4288239 -4.428803 -4.4287882 -4.4287753 -4.4287715 -4.4287763][-4.4288096 -4.4288292 -4.4288468 -4.4288611 -4.4288831 -4.4289017 -4.4289017 -4.4288907 -4.4288721 -4.4288511 -4.4288297 -4.4288144 -4.4287972 -4.4287882 -4.4287939][-4.4287519 -4.4287596 -4.4287715 -4.42879 -4.4288259 -4.4288497 -4.4288521 -4.4288416 -4.4288244 -4.4288058 -4.4287972 -4.4287944 -4.42878 -4.4287739 -4.4287848]]...]
INFO - root - 2017-12-10 05:50:14.822732: step 8310, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:08m:00s remains)
INFO - root - 2017-12-10 05:50:17.466407: step 8320, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 24h:50m:10s remains)
INFO - root - 2017-12-10 05:50:20.104247: step 8330, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:53m:31s remains)
INFO - root - 2017-12-10 05:50:22.780364: step 8340, loss = 2.28, batch loss = 2.23 (27.1 examples/sec; 0.296 sec/batch; 26h:36m:53s remains)
INFO - root - 2017-12-10 05:50:25.391871: step 8350, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:34m:59s remains)
INFO - root - 2017-12-10 05:50:28.076897: step 8360, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:18m:10s remains)
INFO - root - 2017-12-10 05:50:30.693897: step 8370, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:36m:10s remains)
INFO - root - 2017-12-10 05:50:33.393227: step 8380, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 23h:43m:32s remains)
INFO - root - 2017-12-10 05:50:36.028986: step 8390, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:37m:11s remains)
INFO - root - 2017-12-10 05:50:38.728025: step 8400, loss = 2.28, batch loss = 2.23 (27.3 examples/sec; 0.293 sec/batch; 26h:21m:54s remains)
2017-12-10 05:50:39.042482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289045 -4.42893 -4.4289556 -4.4289536 -4.428937 -4.4289131 -4.4288983 -4.4289021 -4.4289055 -4.428894 -4.428865 -4.4288383 -4.42884 -4.428844 -4.4288144][-4.4288907 -4.4289246 -4.4289575 -4.4289618 -4.428947 -4.4289126 -4.4288812 -4.428884 -4.4288974 -4.4288936 -4.4288664 -4.4288278 -4.42882 -4.4288192 -4.4287863][-4.4289012 -4.4289355 -4.4289684 -4.4289742 -4.4289579 -4.4289193 -4.4288793 -4.4288778 -4.4288921 -4.4288898 -4.4288592 -4.4288092 -4.4287877 -4.4287763 -4.4287419][-4.4288888 -4.4289184 -4.4289494 -4.4289517 -4.4289346 -4.4289021 -4.4288659 -4.4288559 -4.4288545 -4.4288411 -4.4288049 -4.42875 -4.4287233 -4.4287033 -4.4286642][-4.4288435 -4.428863 -4.4288888 -4.4288883 -4.4288688 -4.4288354 -4.4287949 -4.4287658 -4.4287391 -4.4287119 -4.4286728 -4.428627 -4.4286141 -4.4285989 -4.4285536][-4.4287934 -4.4288006 -4.4288106 -4.4287958 -4.4287648 -4.4287148 -4.4286494 -4.428587 -4.4285345 -4.4285045 -4.428472 -4.4284463 -4.4284625 -4.4284639 -4.42842][-4.428741 -4.4287338 -4.4287229 -4.4286833 -4.4286308 -4.4285531 -4.4284439 -4.4283376 -4.4282703 -4.42827 -4.4282837 -4.428309 -4.4283676 -4.4283905 -4.4283552][-4.4287252 -4.42871 -4.4286852 -4.4286308 -4.4285665 -4.4284678 -4.4283319 -4.4282007 -4.4281406 -4.4281917 -4.4282751 -4.4283667 -4.4284611 -4.4285016 -4.4284744][-4.428761 -4.4287481 -4.4287281 -4.428688 -4.4286427 -4.428566 -4.4284506 -4.4283333 -4.4282866 -4.42835 -4.4284511 -4.4285526 -4.4286437 -4.4286904 -4.4286833][-4.428823 -4.4288211 -4.4288206 -4.428813 -4.4288039 -4.4287667 -4.4286957 -4.4286151 -4.4285769 -4.4286156 -4.4286828 -4.4287534 -4.4288192 -4.4288554 -4.428863][-4.4288716 -4.428874 -4.4288855 -4.4288974 -4.4289136 -4.4289088 -4.4288783 -4.4288349 -4.4288054 -4.4288173 -4.42885 -4.4288917 -4.4289312 -4.4289536 -4.4289618][-4.4289017 -4.4289021 -4.4289141 -4.4289293 -4.4289508 -4.4289579 -4.428947 -4.4289246 -4.4289036 -4.4288988 -4.4289079 -4.4289289 -4.4289517 -4.4289641 -4.4289708][-4.4289389 -4.4289351 -4.4289412 -4.4289503 -4.4289651 -4.4289718 -4.428966 -4.4289513 -4.4289374 -4.4289303 -4.4289303 -4.42894 -4.4289522 -4.4289584 -4.4289637][-4.4289722 -4.4289656 -4.4289651 -4.4289675 -4.4289742 -4.428977 -4.4289737 -4.4289656 -4.4289594 -4.4289551 -4.4289527 -4.428956 -4.4289618 -4.4289665 -4.4289727][-4.4289918 -4.4289856 -4.4289827 -4.4289818 -4.4289827 -4.4289827 -4.4289804 -4.4289761 -4.4289732 -4.4289708 -4.4289703 -4.4289718 -4.4289751 -4.4289789 -4.4289832]]...]
INFO - root - 2017-12-10 05:50:41.733817: step 8410, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:58m:23s remains)
INFO - root - 2017-12-10 05:50:44.369319: step 8420, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:27m:22s remains)
INFO - root - 2017-12-10 05:50:46.982658: step 8430, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:49m:51s remains)
INFO - root - 2017-12-10 05:50:49.580657: step 8440, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:05m:29s remains)
INFO - root - 2017-12-10 05:50:52.254862: step 8450, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:00m:01s remains)
INFO - root - 2017-12-10 05:50:54.923933: step 8460, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:53m:43s remains)
INFO - root - 2017-12-10 05:50:57.564051: step 8470, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:57m:29s remains)
INFO - root - 2017-12-10 05:51:00.221599: step 8480, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:13m:53s remains)
INFO - root - 2017-12-10 05:51:02.861447: step 8490, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:15m:32s remains)
INFO - root - 2017-12-10 05:51:05.477507: step 8500, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:50m:18s remains)
2017-12-10 05:51:05.773942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289484 -4.4289446 -4.4289412 -4.4289408 -4.4289432 -4.4289455 -4.428946 -4.4289441 -4.4289422 -4.4289408 -4.4289403 -4.4289403 -4.4289422 -4.4289436 -4.4289422][-4.4289513 -4.4289489 -4.4289451 -4.4289432 -4.4289432 -4.4289408 -4.4289365 -4.4289331 -4.4289303 -4.4289289 -4.4289289 -4.4289293 -4.4289327 -4.4289351 -4.4289331][-4.4289489 -4.4289436 -4.428936 -4.4289308 -4.4289284 -4.4289231 -4.4289179 -4.4289165 -4.4289179 -4.4289222 -4.4289269 -4.4289317 -4.428937 -4.4289384 -4.4289351][-4.4289365 -4.4289227 -4.4289055 -4.4288921 -4.4288826 -4.4288712 -4.4288607 -4.4288607 -4.4288697 -4.4288874 -4.4289041 -4.4289169 -4.4289241 -4.4289231 -4.4289155][-4.4289174 -4.4288945 -4.428864 -4.4288383 -4.4288173 -4.428793 -4.4287686 -4.4287629 -4.42878 -4.4288149 -4.4288483 -4.4288712 -4.4288807 -4.4288754 -4.4288621][-4.4288921 -4.4288592 -4.4288096 -4.4287615 -4.4287186 -4.4286714 -4.4286251 -4.4286094 -4.4286337 -4.4286909 -4.4287486 -4.4287858 -4.4287992 -4.428791 -4.4287758][-4.4288893 -4.4288554 -4.4287915 -4.4287214 -4.4286542 -4.4285779 -4.4285021 -4.42847 -4.4284935 -4.4285607 -4.4286318 -4.4286761 -4.42869 -4.4286819 -4.4286666][-4.4288754 -4.4288487 -4.4287872 -4.4287186 -4.4286561 -4.42859 -4.428524 -4.4284959 -4.4285135 -4.4285669 -4.4286251 -4.4286594 -4.428669 -4.4286618 -4.4286456][-4.428844 -4.4288244 -4.4287748 -4.4287233 -4.4286847 -4.4286528 -4.4286256 -4.4286213 -4.4286432 -4.4286823 -4.4287233 -4.4287477 -4.4287562 -4.4287539 -4.4287429][-4.4288669 -4.4288545 -4.4288192 -4.428782 -4.4287586 -4.4287472 -4.4287434 -4.4287519 -4.4287739 -4.4288025 -4.4288287 -4.4288459 -4.4288535 -4.4288545 -4.4288483][-4.4289064 -4.4288993 -4.4288812 -4.4288588 -4.4288473 -4.4288468 -4.4288511 -4.4288607 -4.4288759 -4.4288926 -4.4289074 -4.4289193 -4.4289274 -4.4289303 -4.4289269][-4.4289417 -4.428936 -4.4289255 -4.4289126 -4.428906 -4.4289064 -4.4289107 -4.4289165 -4.4289246 -4.4289327 -4.4289389 -4.4289465 -4.4289532 -4.428957 -4.428956][-4.4289622 -4.4289541 -4.4289451 -4.4289346 -4.4289269 -4.4289246 -4.428925 -4.4289274 -4.4289312 -4.4289341 -4.4289346 -4.428937 -4.4289393 -4.4289417 -4.4289427][-4.4289546 -4.4289479 -4.4289427 -4.4289365 -4.4289322 -4.42893 -4.4289293 -4.4289303 -4.4289312 -4.4289303 -4.4289255 -4.4289217 -4.42892 -4.42892 -4.4289207][-4.4289293 -4.4289241 -4.4289222 -4.4289222 -4.4289236 -4.4289269 -4.4289312 -4.4289351 -4.4289374 -4.428937 -4.4289312 -4.4289246 -4.4289212 -4.4289193 -4.4289188]]...]
INFO - root - 2017-12-10 05:51:08.454022: step 8510, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:36m:00s remains)
INFO - root - 2017-12-10 05:51:11.114846: step 8520, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:52m:51s remains)
INFO - root - 2017-12-10 05:51:13.788683: step 8530, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:01m:25s remains)
INFO - root - 2017-12-10 05:51:16.506131: step 8540, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:37m:13s remains)
INFO - root - 2017-12-10 05:51:19.158056: step 8550, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:46m:30s remains)
INFO - root - 2017-12-10 05:51:21.877725: step 8560, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:30m:25s remains)
INFO - root - 2017-12-10 05:51:24.530634: step 8570, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:28m:28s remains)
INFO - root - 2017-12-10 05:51:27.168389: step 8580, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:43m:29s remains)
INFO - root - 2017-12-10 05:51:29.821875: step 8590, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:10m:08s remains)
INFO - root - 2017-12-10 05:51:32.491263: step 8600, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:00m:25s remains)
2017-12-10 05:51:32.765481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289985 -4.42899 -4.4289846 -4.4289765 -4.4289632 -4.4289522 -4.4289494 -4.42895 -4.4289474 -4.4289393 -4.4289236 -4.4289021 -4.4288669 -4.4288354 -4.4288216][-4.4289889 -4.4289732 -4.4289622 -4.428947 -4.4289231 -4.4289041 -4.4288955 -4.4288936 -4.428895 -4.4289007 -4.428894 -4.42888 -4.4288659 -4.4288592 -4.428863][-4.4289761 -4.4289556 -4.4289384 -4.4289217 -4.4288974 -4.4288688 -4.4288454 -4.42883 -4.4288282 -4.4288373 -4.4288292 -4.428812 -4.42881 -4.4288349 -4.4288697][-4.4289641 -4.4289355 -4.4289107 -4.4288921 -4.4288707 -4.4288235 -4.428772 -4.4287376 -4.4287405 -4.4287553 -4.4287353 -4.4287052 -4.4287076 -4.428761 -4.4288282][-4.42895 -4.4289079 -4.4288707 -4.4288397 -4.4288087 -4.4287291 -4.4286308 -4.4285693 -4.4285927 -4.4286385 -4.4286213 -4.4285822 -4.4285846 -4.4286637 -4.4287658][-4.4289331 -4.4288783 -4.4288211 -4.4287748 -4.4287186 -4.4285903 -4.4284267 -4.42833 -4.4283929 -4.4284916 -4.4284911 -4.4284558 -4.4284739 -4.428587 -4.4287243][-4.428916 -4.4288526 -4.4287868 -4.4287314 -4.4286556 -4.4284935 -4.4282823 -4.4281616 -4.4282475 -4.4283791 -4.4284053 -4.4283991 -4.42845 -4.4285851 -4.4287353][-4.4288964 -4.4288287 -4.4287562 -4.4286962 -4.4286342 -4.4285145 -4.4283614 -4.4282708 -4.4283218 -4.4284229 -4.4284635 -4.4284844 -4.4285479 -4.4286637 -4.4287906][-4.4288754 -4.4288058 -4.42873 -4.4286723 -4.4286447 -4.4286017 -4.4285378 -4.4284945 -4.4285164 -4.4285736 -4.4286151 -4.4286451 -4.4286966 -4.4287715 -4.4288564][-4.42888 -4.4288192 -4.4287524 -4.4287014 -4.4286923 -4.4286895 -4.428679 -4.4286742 -4.4286904 -4.4287238 -4.4287586 -4.428792 -4.42883 -4.4288721 -4.4289193][-4.4289117 -4.428864 -4.428812 -4.4287748 -4.4287729 -4.4287772 -4.4287877 -4.4288058 -4.4288197 -4.4288378 -4.4288611 -4.4288936 -4.4289222 -4.4289451 -4.428967][-4.4289508 -4.4289179 -4.42888 -4.4288578 -4.4288592 -4.4288621 -4.4288774 -4.4288969 -4.4289 -4.4289064 -4.4289217 -4.428947 -4.4289689 -4.4289837 -4.4289918][-4.4289837 -4.4289641 -4.4289441 -4.4289341 -4.4289341 -4.4289289 -4.4289355 -4.4289465 -4.4289436 -4.4289465 -4.4289541 -4.4289694 -4.4289846 -4.4289942 -4.4289961][-4.4290071 -4.4289989 -4.4289932 -4.4289923 -4.4289908 -4.4289856 -4.428987 -4.4289885 -4.4289818 -4.42898 -4.4289823 -4.4289875 -4.4289923 -4.4289947 -4.4289956][-4.4290204 -4.4290214 -4.4290233 -4.4290242 -4.4290233 -4.429019 -4.429018 -4.4290156 -4.4290085 -4.4290018 -4.4289985 -4.428998 -4.4289961 -4.4289961 -4.428997]]...]
INFO - root - 2017-12-10 05:51:35.373817: step 8610, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:59m:16s remains)
INFO - root - 2017-12-10 05:51:38.022719: step 8620, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:58m:08s remains)
INFO - root - 2017-12-10 05:51:40.636441: step 8630, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:31m:03s remains)
INFO - root - 2017-12-10 05:51:43.362105: step 8640, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:39m:30s remains)
INFO - root - 2017-12-10 05:51:45.983943: step 8650, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:18m:11s remains)
INFO - root - 2017-12-10 05:51:48.635194: step 8660, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:15m:38s remains)
INFO - root - 2017-12-10 05:51:51.291229: step 8670, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:30m:59s remains)
INFO - root - 2017-12-10 05:51:53.981538: step 8680, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:54m:29s remains)
INFO - root - 2017-12-10 05:51:56.695925: step 8690, loss = 2.28, batch loss = 2.23 (28.2 examples/sec; 0.284 sec/batch; 25h:32m:20s remains)
INFO - root - 2017-12-10 05:51:59.367474: step 8700, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:40m:45s remains)
2017-12-10 05:51:59.667286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289007 -4.4288688 -4.4288583 -4.4288654 -4.4288788 -4.4288926 -4.4289031 -4.428895 -4.42887 -4.4288468 -4.4288244 -4.428823 -4.4288316 -4.4288511 -4.428874][-4.4288788 -4.4288368 -4.4288163 -4.4288239 -4.4288468 -4.4288692 -4.4288845 -4.4288712 -4.4288349 -4.4288073 -4.4287839 -4.4287868 -4.4287963 -4.4288168 -4.428844][-4.4288578 -4.4288082 -4.4287748 -4.4287682 -4.4287925 -4.4288216 -4.4288392 -4.4288168 -4.4287663 -4.4287438 -4.4287271 -4.4287443 -4.4287591 -4.4287834 -4.4288158][-4.428843 -4.4287887 -4.4287405 -4.4287195 -4.42874 -4.4287686 -4.4287763 -4.4287343 -4.4286823 -4.4286785 -4.4286804 -4.4287152 -4.4287443 -4.4287729 -4.428813][-4.4288282 -4.4287729 -4.4287238 -4.4286971 -4.4287014 -4.4286976 -4.4286704 -4.4286151 -4.4285836 -4.4286141 -4.4286485 -4.4287028 -4.4287462 -4.42878 -4.4288278][-4.4288177 -4.4287558 -4.4287066 -4.4286723 -4.4286466 -4.4285932 -4.4285154 -4.4284563 -4.428462 -4.4285374 -4.4286165 -4.4286966 -4.4287515 -4.4287925 -4.42884][-4.4288177 -4.4287386 -4.4286728 -4.4286208 -4.4285517 -4.4284344 -4.4283123 -4.4282737 -4.42833 -4.4284582 -4.4285803 -4.4286857 -4.4287543 -4.4287977 -4.4288368][-4.4288116 -4.4287224 -4.4286351 -4.4285445 -4.4284019 -4.4282045 -4.4280515 -4.4280686 -4.4282184 -4.4283953 -4.4285464 -4.4286671 -4.4287491 -4.428802 -4.4288373][-4.4288135 -4.4287281 -4.4286294 -4.4285021 -4.4283018 -4.4280524 -4.4278903 -4.42798 -4.428216 -4.4284191 -4.4285631 -4.4286709 -4.4287562 -4.4288125 -4.428844][-4.4288149 -4.42873 -4.428637 -4.4285169 -4.4283319 -4.4281139 -4.4279981 -4.4281087 -4.42834 -4.4285159 -4.4286242 -4.4286981 -4.4287634 -4.4288182 -4.4288511][-4.4288278 -4.4287453 -4.4286594 -4.4285645 -4.4284391 -4.4283032 -4.4282522 -4.4283371 -4.4285011 -4.428617 -4.4286795 -4.4287224 -4.4287724 -4.4288278 -4.428863][-4.4288621 -4.42879 -4.4287181 -4.428647 -4.4285827 -4.4285226 -4.4285116 -4.4285603 -4.4286571 -4.4287162 -4.4287467 -4.4287715 -4.4288096 -4.4288621 -4.4288945][-4.4289103 -4.4288497 -4.4287915 -4.4287395 -4.4287195 -4.4287086 -4.4287176 -4.4287448 -4.4287934 -4.4288182 -4.4288273 -4.4288478 -4.4288797 -4.4289203 -4.42894][-4.4289389 -4.428884 -4.4288383 -4.4288135 -4.4288273 -4.4288359 -4.4288454 -4.4288549 -4.428874 -4.4288821 -4.4288883 -4.4289103 -4.4289365 -4.4289665 -4.4289742][-4.4289327 -4.4288769 -4.428843 -4.428844 -4.42888 -4.4288983 -4.4289007 -4.4288983 -4.4289002 -4.4289036 -4.428915 -4.4289346 -4.4289508 -4.42897 -4.4289708]]...]
INFO - root - 2017-12-10 05:52:02.302120: step 8710, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:37m:18s remains)
INFO - root - 2017-12-10 05:52:04.979572: step 8720, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:49m:02s remains)
INFO - root - 2017-12-10 05:52:07.618839: step 8730, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:09m:25s remains)
INFO - root - 2017-12-10 05:52:10.258222: step 8740, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:23m:52s remains)
INFO - root - 2017-12-10 05:52:12.903235: step 8750, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:11m:55s remains)
INFO - root - 2017-12-10 05:52:15.620884: step 8760, loss = 2.28, batch loss = 2.23 (27.4 examples/sec; 0.292 sec/batch; 26h:14m:51s remains)
INFO - root - 2017-12-10 05:52:18.291118: step 8770, loss = 2.28, batch loss = 2.23 (28.5 examples/sec; 0.281 sec/batch; 25h:13m:36s remains)
INFO - root - 2017-12-10 05:52:21.012250: step 8780, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:43m:37s remains)
INFO - root - 2017-12-10 05:52:23.686938: step 8790, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:25m:18s remains)
INFO - root - 2017-12-10 05:52:26.315562: step 8800, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 23h:52m:16s remains)
2017-12-10 05:52:26.615479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287052 -4.4287529 -4.4287944 -4.4288111 -4.42881 -4.4287944 -4.4287577 -4.42872 -4.4287081 -4.42874 -4.4287891 -4.4288359 -4.4288731 -4.4288921 -4.42891][-4.4286661 -4.4287248 -4.4287815 -4.4288096 -4.4288216 -4.428812 -4.4287772 -4.4287372 -4.4287181 -4.4287357 -4.4287739 -4.4288144 -4.4288468 -4.4288635 -4.4288836][-4.428658 -4.4287176 -4.4287796 -4.428823 -4.428844 -4.4288349 -4.4287982 -4.4287548 -4.4287276 -4.4287262 -4.4287434 -4.4287734 -4.4288054 -4.4288373 -4.4288673][-4.4286766 -4.4287329 -4.4287934 -4.4288459 -4.4288683 -4.4288492 -4.4288006 -4.4287438 -4.4287009 -4.4286828 -4.4286885 -4.428719 -4.4287629 -4.4288168 -4.4288597][-4.4287148 -4.4287744 -4.4288368 -4.428884 -4.4288936 -4.428854 -4.4287815 -4.4286947 -4.428616 -4.4285865 -4.4286079 -4.4286647 -4.4287386 -4.4288135 -4.4288568][-4.4287539 -4.4288239 -4.428884 -4.4289136 -4.4289 -4.4288459 -4.4287443 -4.4286065 -4.4284739 -4.4284387 -4.4285135 -4.4286261 -4.4287367 -4.4288182 -4.4288478][-4.428792 -4.428865 -4.4289122 -4.4289203 -4.428895 -4.428843 -4.4287114 -4.4285045 -4.4283004 -4.4282722 -4.4284348 -4.42862 -4.4287577 -4.4288287 -4.4288445][-4.4288235 -4.4288793 -4.4289103 -4.4289107 -4.4288898 -4.4288454 -4.4286914 -4.42843 -4.4281735 -4.4281745 -4.4284177 -4.4286451 -4.4287829 -4.4288378 -4.4288425][-4.4288416 -4.4288688 -4.42889 -4.4289083 -4.4289026 -4.4288626 -4.4287028 -4.4284449 -4.4282174 -4.4282441 -4.4284797 -4.4286819 -4.4287949 -4.4288306 -4.4288259][-4.4288416 -4.4288459 -4.4288678 -4.4289117 -4.4289231 -4.4288893 -4.4287496 -4.4285421 -4.4283853 -4.4284129 -4.4285774 -4.4287148 -4.428791 -4.4288087 -4.4287977][-4.4288292 -4.4288206 -4.4288468 -4.428905 -4.4289331 -4.4289126 -4.4288092 -4.4286609 -4.4285579 -4.4285693 -4.4286652 -4.4287472 -4.428792 -4.4287972 -4.4287853][-4.4288073 -4.4287915 -4.4288206 -4.4288859 -4.4289308 -4.4289322 -4.4288654 -4.428762 -4.4286809 -4.428669 -4.4287171 -4.4287672 -4.4287958 -4.4287972 -4.4287891][-4.4287853 -4.42877 -4.4288044 -4.4288664 -4.4289188 -4.4289389 -4.4289026 -4.4288273 -4.4287529 -4.428721 -4.4287415 -4.4287829 -4.4288073 -4.42881 -4.4288106][-4.4287786 -4.4287724 -4.4288116 -4.4288626 -4.4289093 -4.4289365 -4.4289222 -4.4288669 -4.4287972 -4.4287548 -4.428762 -4.4287972 -4.4288192 -4.4288249 -4.4288254][-4.4288077 -4.4288073 -4.4288425 -4.4288793 -4.4289093 -4.4289303 -4.4289303 -4.4288912 -4.428823 -4.428772 -4.4287696 -4.4287958 -4.4288173 -4.428822 -4.4288168]]...]
INFO - root - 2017-12-10 05:52:29.273999: step 8810, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:35m:54s remains)
INFO - root - 2017-12-10 05:52:31.906649: step 8820, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:12m:38s remains)
INFO - root - 2017-12-10 05:52:34.559636: step 8830, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:06m:09s remains)
INFO - root - 2017-12-10 05:52:37.192159: step 8840, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:14m:47s remains)
INFO - root - 2017-12-10 05:52:39.811045: step 8850, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:36m:44s remains)
INFO - root - 2017-12-10 05:52:42.470877: step 8860, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:46m:04s remains)
INFO - root - 2017-12-10 05:52:45.070397: step 8870, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:30m:05s remains)
INFO - root - 2017-12-10 05:52:47.738140: step 8880, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:13m:46s remains)
INFO - root - 2017-12-10 05:52:50.401311: step 8890, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:12m:59s remains)
INFO - root - 2017-12-10 05:52:53.049814: step 8900, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:58m:00s remains)
2017-12-10 05:52:53.377319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289703 -4.4289603 -4.4289436 -4.4289193 -4.4289 -4.4288912 -4.4288893 -4.428895 -4.4289112 -4.428936 -4.4289589 -4.4289713 -4.428977 -4.4289827 -4.4289837][-4.4289732 -4.4289532 -4.42892 -4.4288774 -4.4288449 -4.4288335 -4.4288373 -4.4288487 -4.4288716 -4.428906 -4.42894 -4.4289589 -4.4289646 -4.4289722 -4.428977][-4.4289761 -4.4289479 -4.4289 -4.4288368 -4.4287839 -4.4287581 -4.42876 -4.4287744 -4.4288044 -4.4288535 -4.428906 -4.428936 -4.428946 -4.4289546 -4.4289627][-4.42898 -4.4289517 -4.4288964 -4.4288173 -4.4287462 -4.4286995 -4.4286704 -4.4286556 -4.4286852 -4.4287577 -4.4288397 -4.4288917 -4.4289174 -4.4289355 -4.42895][-4.4289827 -4.4289618 -4.4289103 -4.4288287 -4.4287438 -4.4286666 -4.4285717 -4.428484 -4.4284973 -4.428607 -4.4287305 -4.4288182 -4.4288745 -4.4289193 -4.428946][-4.428988 -4.42898 -4.4289417 -4.4288659 -4.4287672 -4.428648 -4.4284663 -4.4282651 -4.4282417 -4.4284124 -4.4286089 -4.4287496 -4.4288425 -4.4289107 -4.42895][-4.4289975 -4.4290032 -4.4289794 -4.4289136 -4.4288111 -4.4286604 -4.4284019 -4.4280696 -4.4279695 -4.4282136 -4.4284978 -4.4286885 -4.4288149 -4.428905 -4.4289565][-4.4290009 -4.4290152 -4.4290051 -4.4289575 -4.4288688 -4.4287224 -4.4284558 -4.4280996 -4.427948 -4.4281635 -4.4284439 -4.4286294 -4.42877 -4.42889 -4.4289589][-4.4289851 -4.4290066 -4.4290142 -4.4289918 -4.4289312 -4.428823 -4.4286222 -4.4283557 -4.4282093 -4.4283 -4.4284797 -4.4286122 -4.4287348 -4.4288678 -4.4289527][-4.4289541 -4.4289818 -4.4290023 -4.4290032 -4.4289746 -4.4289122 -4.4287868 -4.4286108 -4.4284849 -4.4284897 -4.4285755 -4.4286523 -4.4287429 -4.428863 -4.4289513][-4.4289055 -4.42895 -4.4289808 -4.428997 -4.4289932 -4.4289641 -4.4288921 -4.4287791 -4.4286852 -4.4286556 -4.4286823 -4.4287252 -4.4287915 -4.4288883 -4.4289637][-4.4288435 -4.428916 -4.4289646 -4.4289937 -4.4290018 -4.4289927 -4.4289527 -4.4288831 -4.4288268 -4.4287972 -4.428793 -4.4288092 -4.4288549 -4.4289279 -4.4289851][-4.4287782 -4.42888 -4.42895 -4.4289956 -4.4290147 -4.4290133 -4.428987 -4.4289451 -4.4289217 -4.4289093 -4.4289 -4.4289017 -4.428926 -4.4289727 -4.4290047][-4.4287357 -4.428854 -4.4289389 -4.4289937 -4.4290257 -4.4290318 -4.4290147 -4.4289908 -4.4289837 -4.4289837 -4.4289765 -4.4289708 -4.4289804 -4.4290056 -4.4290185][-4.4287291 -4.4288464 -4.4289351 -4.4289889 -4.4290223 -4.4290338 -4.4290266 -4.4290175 -4.4290175 -4.4290204 -4.4290133 -4.4290047 -4.4290061 -4.429019 -4.4290242]]...]
INFO - root - 2017-12-10 05:52:55.989308: step 8910, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:53m:16s remains)
INFO - root - 2017-12-10 05:52:58.608873: step 8920, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:56m:09s remains)
INFO - root - 2017-12-10 05:53:01.274456: step 8930, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:03m:55s remains)
INFO - root - 2017-12-10 05:53:03.929620: step 8940, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:34m:56s remains)
INFO - root - 2017-12-10 05:53:06.595206: step 8950, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:04m:09s remains)
INFO - root - 2017-12-10 05:53:09.309425: step 8960, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:53m:45s remains)
INFO - root - 2017-12-10 05:53:11.977838: step 8970, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:13m:14s remains)
INFO - root - 2017-12-10 05:53:14.650668: step 8980, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:44m:23s remains)
INFO - root - 2017-12-10 05:53:17.255956: step 8990, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:23m:04s remains)
INFO - root - 2017-12-10 05:53:19.931149: step 9000, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:17m:01s remains)
2017-12-10 05:53:20.224986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287372 -4.4287534 -4.4287343 -4.4286904 -4.4286633 -4.4287057 -4.4287696 -4.4288082 -4.4288058 -4.4287653 -4.4287243 -4.4287009 -4.4286957 -4.42869 -4.42868][-4.4287648 -4.4287696 -4.4287496 -4.4287133 -4.4286842 -4.4287138 -4.4287734 -4.4288158 -4.428822 -4.4287834 -4.4287348 -4.4286914 -4.4286652 -4.4286575 -4.4286666][-4.4287519 -4.4287505 -4.4287348 -4.4287167 -4.4286919 -4.42871 -4.4287648 -4.4288135 -4.4288282 -4.428792 -4.4287362 -4.4286752 -4.4286385 -4.4286447 -4.428679][-4.4287267 -4.4287071 -4.4286947 -4.4286957 -4.4286857 -4.4286981 -4.4287438 -4.4287939 -4.4288173 -4.4287868 -4.428719 -4.4286375 -4.4286051 -4.4286342 -4.4286942][-4.428688 -4.4286489 -4.4286337 -4.4286432 -4.4286418 -4.4286418 -4.4286737 -4.42873 -4.4287763 -4.4287677 -4.4286971 -4.4286 -4.4285717 -4.4286213 -4.4287014][-4.4286733 -4.4286189 -4.4285946 -4.4285989 -4.4285836 -4.4285479 -4.42855 -4.4286232 -4.4287086 -4.4287405 -4.4286957 -4.4285994 -4.4285717 -4.428627 -4.4287114][-4.42866 -4.4285922 -4.4285541 -4.4285455 -4.4285107 -4.4284306 -4.4283857 -4.4284759 -4.4286113 -4.4286914 -4.42869 -4.4286208 -4.4285908 -4.4286327 -4.4287076][-4.4286652 -4.4285941 -4.4285479 -4.4285269 -4.4284749 -4.4283614 -4.4282522 -4.4283304 -4.428504 -4.4286313 -4.4286747 -4.4286456 -4.4286213 -4.4286447 -4.4287062][-4.4286923 -4.4286237 -4.4285774 -4.4285536 -4.42851 -4.42842 -4.4283128 -4.4283509 -4.4284973 -4.4286284 -4.42869 -4.4286866 -4.42867 -4.4286809 -4.4287276][-4.4287348 -4.4286823 -4.4286427 -4.4286208 -4.4286022 -4.4285622 -4.4284935 -4.4284911 -4.42857 -4.4286709 -4.4287262 -4.4287291 -4.4287114 -4.4287162 -4.42875][-4.4288011 -4.4287653 -4.428731 -4.4287047 -4.4286966 -4.4286776 -4.4286313 -4.4286127 -4.428647 -4.4287143 -4.4287615 -4.4287677 -4.4287505 -4.4287477 -4.428772][-4.42886 -4.4288359 -4.4288111 -4.4287848 -4.4287763 -4.4287663 -4.4287291 -4.4286995 -4.4287071 -4.4287515 -4.4287987 -4.4288068 -4.4287863 -4.4287734 -4.4287887][-4.4288898 -4.4288778 -4.4288659 -4.4288492 -4.4288449 -4.4288411 -4.4288092 -4.428761 -4.4287453 -4.4287753 -4.4288216 -4.4288268 -4.4287972 -4.4287767 -4.4287848][-4.4288883 -4.4288907 -4.428895 -4.4288907 -4.4288898 -4.4288893 -4.428864 -4.4288077 -4.428781 -4.4287977 -4.42883 -4.4288268 -4.4287949 -4.4287724 -4.4287767][-4.4288754 -4.428895 -4.4289141 -4.4289203 -4.4289236 -4.4289284 -4.428915 -4.4288669 -4.428834 -4.4288306 -4.4288416 -4.4288321 -4.4288034 -4.4287791 -4.4287753]]...]
INFO - root - 2017-12-10 05:53:22.918640: step 9010, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:40m:21s remains)
INFO - root - 2017-12-10 05:53:25.597717: step 9020, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:17m:23s remains)
INFO - root - 2017-12-10 05:53:28.256527: step 9030, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:18m:15s remains)
INFO - root - 2017-12-10 05:53:30.928873: step 9040, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:20m:09s remains)
INFO - root - 2017-12-10 05:53:33.581500: step 9050, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:49m:14s remains)
INFO - root - 2017-12-10 05:53:36.215725: step 9060, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:07m:41s remains)
INFO - root - 2017-12-10 05:53:38.871998: step 9070, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:07m:42s remains)
INFO - root - 2017-12-10 05:53:41.566015: step 9080, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 22h:57m:11s remains)
INFO - root - 2017-12-10 05:53:44.273337: step 9090, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:48m:36s remains)
INFO - root - 2017-12-10 05:53:46.909697: step 9100, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:55m:42s remains)
2017-12-10 05:53:47.192003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289675 -4.4289389 -4.4288716 -4.4287939 -4.4287329 -4.4287128 -4.4287362 -4.4287815 -4.4288187 -4.42882 -4.4288063 -4.4288125 -4.4288483 -4.4288788 -4.4288869][-4.4289632 -4.4289312 -4.42886 -4.4287777 -4.4287152 -4.4286938 -4.4287152 -4.4287667 -4.4288163 -4.4288239 -4.4288063 -4.428802 -4.4288287 -4.4288611 -4.4288783][-4.42897 -4.4289393 -4.4288716 -4.4287834 -4.4287043 -4.4286728 -4.4286904 -4.428751 -4.4288182 -4.4288397 -4.4288206 -4.4288077 -4.4288187 -4.4288435 -4.4288607][-4.4289827 -4.4289541 -4.4288888 -4.428791 -4.4286857 -4.4286256 -4.4286256 -4.4286866 -4.4287753 -4.4288278 -4.4288316 -4.4288292 -4.4288278 -4.4288335 -4.4288354][-4.429 -4.4289742 -4.4289064 -4.4288 -4.4286695 -4.428566 -4.4285197 -4.4285555 -4.4286623 -4.4287629 -4.4288182 -4.42885 -4.4288516 -4.4288373 -4.428813][-4.4290104 -4.4289894 -4.4289236 -4.4288149 -4.4286704 -4.4285307 -4.4284167 -4.4283814 -4.4284883 -4.4286518 -4.4287744 -4.4288473 -4.4288673 -4.428853 -4.4288168][-4.4290109 -4.4289923 -4.4289346 -4.4288373 -4.4286985 -4.4285393 -4.428359 -4.4282212 -4.4282923 -4.42851 -4.4287009 -4.42882 -4.4288735 -4.4288778 -4.4288445][-4.429009 -4.42899 -4.4289417 -4.4288645 -4.4287491 -4.4286065 -4.4284286 -4.4282637 -4.4282808 -4.4284573 -4.4286442 -4.4287796 -4.4288559 -4.428884 -4.4288683][-4.4290113 -4.4289923 -4.4289479 -4.4288826 -4.428793 -4.4286871 -4.4285641 -4.4284549 -4.4284477 -4.4285254 -4.4286366 -4.42874 -4.4288187 -4.428865 -4.4288735][-4.4290123 -4.4289966 -4.4289579 -4.428894 -4.4288173 -4.42874 -4.4286642 -4.4286141 -4.4286132 -4.4286304 -4.4286685 -4.4287248 -4.428792 -4.4288468 -4.4288716][-4.4290085 -4.4289956 -4.42896 -4.428895 -4.4288225 -4.4287648 -4.4287219 -4.4287033 -4.4287062 -4.4287028 -4.4287086 -4.4287329 -4.4287777 -4.4288235 -4.4288492][-4.4290004 -4.4289875 -4.42895 -4.4288793 -4.4288082 -4.428762 -4.4287386 -4.4287338 -4.4287372 -4.4287395 -4.4287477 -4.4287605 -4.4287853 -4.4288154 -4.4288316][-4.4289937 -4.4289794 -4.4289365 -4.4288588 -4.4287844 -4.4287405 -4.4287257 -4.4287319 -4.4287481 -4.4287658 -4.42878 -4.4287953 -4.428813 -4.4288254 -4.4288273][-4.4289942 -4.4289808 -4.4289351 -4.4288564 -4.4287844 -4.4287429 -4.4287295 -4.4287453 -4.4287839 -4.4288149 -4.4288249 -4.4288344 -4.4288454 -4.4288421 -4.4288316][-4.4289966 -4.4289875 -4.4289474 -4.4288812 -4.4288239 -4.4287882 -4.4287715 -4.4287872 -4.428833 -4.42887 -4.4288816 -4.4288893 -4.4288974 -4.4288898 -4.4288764]]...]
INFO - root - 2017-12-10 05:53:49.810862: step 9110, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:18m:39s remains)
INFO - root - 2017-12-10 05:53:52.422183: step 9120, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:23m:44s remains)
INFO - root - 2017-12-10 05:53:55.050174: step 9130, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:43m:15s remains)
INFO - root - 2017-12-10 05:53:57.654440: step 9140, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 23h:50m:09s remains)
INFO - root - 2017-12-10 05:54:00.298526: step 9150, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.250 sec/batch; 22h:29m:25s remains)
INFO - root - 2017-12-10 05:54:02.967430: step 9160, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 24h:45m:48s remains)
INFO - root - 2017-12-10 05:54:05.620562: step 9170, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:55m:19s remains)
INFO - root - 2017-12-10 05:54:08.295637: step 9180, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:37m:44s remains)
INFO - root - 2017-12-10 05:54:10.962219: step 9190, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:56m:11s remains)
INFO - root - 2017-12-10 05:54:13.620205: step 9200, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:20m:59s remains)
2017-12-10 05:54:13.926164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287009 -4.42874 -4.42879 -4.4288106 -4.4287977 -4.4287863 -4.4287825 -4.4287825 -4.4287686 -4.4287763 -4.4287715 -4.4287534 -4.4287386 -4.4287419 -4.4287581][-4.4286952 -4.4287152 -4.4287429 -4.42875 -4.4287362 -4.4287391 -4.4287486 -4.4287486 -4.4287333 -4.4287519 -4.428761 -4.4287591 -4.4287591 -4.4287653 -4.42878][-4.4287205 -4.4287133 -4.4287243 -4.4287262 -4.4287186 -4.4287267 -4.4287357 -4.4287319 -4.4287257 -4.428762 -4.4287786 -4.4287882 -4.4288006 -4.4288111 -4.4288239][-4.4287782 -4.4287567 -4.4287515 -4.4287453 -4.428741 -4.4287438 -4.42873 -4.4287095 -4.4287105 -4.4287567 -4.4287796 -4.4287987 -4.4288235 -4.4288397 -4.4288526][-4.42882 -4.4287906 -4.4287753 -4.4287567 -4.4287477 -4.4287386 -4.4286885 -4.42864 -4.4286461 -4.4287167 -4.4287615 -4.4287982 -4.428833 -4.4288616 -4.4288788][-4.42882 -4.428791 -4.4287639 -4.4287267 -4.4286895 -4.4286356 -4.4285388 -4.4284749 -4.4285131 -4.4286246 -4.428709 -4.4287748 -4.4288254 -4.4288716 -4.4288926][-4.4287825 -4.4287524 -4.4287148 -4.4286585 -4.4285822 -4.4284534 -4.4282827 -4.42821 -4.4283113 -4.4285 -4.4286451 -4.4287462 -4.4288116 -4.4288683 -4.428884][-4.4287233 -4.4286923 -4.4286442 -4.4285631 -4.428452 -4.4282684 -4.4280529 -4.4279933 -4.4281683 -4.4284239 -4.4286194 -4.4287443 -4.428813 -4.4288578 -4.4288654][-4.4287019 -4.4286728 -4.4286275 -4.4285436 -4.4284286 -4.4282694 -4.4281197 -4.4281249 -4.4282956 -4.42851 -4.4286709 -4.4287658 -4.4288135 -4.4288349 -4.4288344][-4.428709 -4.4286857 -4.4286571 -4.4285965 -4.4285126 -4.4284225 -4.4283705 -4.428422 -4.4285355 -4.4286566 -4.4287372 -4.4287791 -4.428792 -4.4287953 -4.4287963][-4.42872 -4.4287057 -4.428699 -4.4286613 -4.4286046 -4.4285603 -4.4285603 -4.428616 -4.4286795 -4.4287453 -4.4287791 -4.4287791 -4.4287629 -4.4287581 -4.4287639][-4.4287357 -4.4287314 -4.4287424 -4.4287257 -4.4286919 -4.4286685 -4.4286757 -4.4287148 -4.4287472 -4.4287782 -4.4287791 -4.4287572 -4.4287324 -4.4287372 -4.4287562][-4.4287448 -4.4287586 -4.4287863 -4.4287887 -4.4287691 -4.4287572 -4.42876 -4.4287777 -4.4287868 -4.428792 -4.4287753 -4.4287496 -4.4287329 -4.4287491 -4.4287763][-4.42875 -4.4287796 -4.4288154 -4.4288259 -4.428813 -4.4288039 -4.428802 -4.428803 -4.4287972 -4.4287906 -4.42877 -4.4287457 -4.4287415 -4.4287734 -4.4288068][-4.4287829 -4.4288087 -4.4288392 -4.4288526 -4.4288478 -4.4288421 -4.4288397 -4.4288354 -4.4288235 -4.428813 -4.4287963 -4.4287825 -4.428793 -4.4288273 -4.4288564]]...]
INFO - root - 2017-12-10 05:54:16.617597: step 9210, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:04m:04s remains)
INFO - root - 2017-12-10 05:54:19.271188: step 9220, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:02m:22s remains)
INFO - root - 2017-12-10 05:54:21.894858: step 9230, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:49m:53s remains)
INFO - root - 2017-12-10 05:54:24.573503: step 9240, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:39m:37s remains)
INFO - root - 2017-12-10 05:54:27.203263: step 9250, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:51m:07s remains)
INFO - root - 2017-12-10 05:54:29.826932: step 9260, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:34m:29s remains)
INFO - root - 2017-12-10 05:54:32.487586: step 9270, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:48m:22s remains)
INFO - root - 2017-12-10 05:54:35.132968: step 9280, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:56m:49s remains)
INFO - root - 2017-12-10 05:54:37.819175: step 9290, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:15m:18s remains)
INFO - root - 2017-12-10 05:54:40.461630: step 9300, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:25m:57s remains)
2017-12-10 05:54:40.784114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287996 -4.42878 -4.4287629 -4.4287481 -4.4287448 -4.4287453 -4.4287519 -4.4287853 -4.4288306 -4.42886 -4.42886 -4.4288354 -4.4287825 -4.4287343 -4.4287376][-4.4287839 -4.4287567 -4.4287295 -4.4287038 -4.4286928 -4.4286933 -4.4287052 -4.4287443 -4.4287963 -4.42883 -4.428834 -4.4288163 -4.4287715 -4.4287286 -4.4287362][-4.4287744 -4.4287424 -4.42871 -4.4286695 -4.4286513 -4.42866 -4.4286861 -4.4287305 -4.4287863 -4.4288211 -4.4288244 -4.4288073 -4.4287643 -4.4287252 -4.4287372][-4.4287348 -4.4286981 -4.4286633 -4.4286208 -4.4286089 -4.4286313 -4.4286675 -4.4287081 -4.428762 -4.4287958 -4.4288034 -4.4287882 -4.4287486 -4.4287152 -4.4287329][-4.4287009 -4.4286633 -4.4286289 -4.428597 -4.428597 -4.4286237 -4.4286542 -4.4286842 -4.4287319 -4.4287663 -4.4287806 -4.4287734 -4.4287419 -4.4287148 -4.4287333][-4.4286819 -4.4286461 -4.4286151 -4.4285917 -4.4285965 -4.4286184 -4.4286375 -4.4286542 -4.4286914 -4.4287229 -4.428741 -4.4287424 -4.4287229 -4.4287047 -4.4287267][-4.428648 -4.4286056 -4.4285707 -4.4285507 -4.4285588 -4.4285793 -4.4285927 -4.4286075 -4.4286427 -4.4286771 -4.4287 -4.4287066 -4.4286971 -4.4286866 -4.4287143][-4.4286022 -4.4285488 -4.428504 -4.4284797 -4.4284873 -4.4285116 -4.4285378 -4.4285703 -4.4286132 -4.4286504 -4.4286757 -4.4286857 -4.42868 -4.4286752 -4.4287095][-4.4285789 -4.4285259 -4.4284797 -4.4284525 -4.4284577 -4.4284868 -4.4285278 -4.4285793 -4.4286284 -4.4286628 -4.4286842 -4.4286895 -4.42868 -4.4286771 -4.4287143][-4.4285831 -4.428544 -4.4285073 -4.4284825 -4.4284821 -4.4285073 -4.4285555 -4.4286203 -4.4286761 -4.4287086 -4.4287271 -4.4287248 -4.4287062 -4.4286976 -4.4287291][-4.428596 -4.4285703 -4.428545 -4.4285216 -4.4285145 -4.4285312 -4.4285741 -4.4286408 -4.4287009 -4.42874 -4.4287643 -4.4287605 -4.4287367 -4.4287219 -4.4287457][-4.4286184 -4.4286056 -4.4285908 -4.4285731 -4.4285636 -4.4285707 -4.4285979 -4.4286509 -4.4287066 -4.4287496 -4.42878 -4.4287791 -4.4287534 -4.4287362 -4.4287562][-4.4286304 -4.4286242 -4.4286146 -4.4286036 -4.4285965 -4.4285955 -4.4286056 -4.4286404 -4.4286871 -4.42873 -4.4287648 -4.428771 -4.42875 -4.4287348 -4.4287543][-4.4286265 -4.4286165 -4.4286008 -4.4285913 -4.4285879 -4.4285827 -4.42858 -4.4286003 -4.4286389 -4.4286785 -4.4287167 -4.4287333 -4.4287229 -4.4287148 -4.4287386][-4.4286284 -4.4286137 -4.4285936 -4.4285803 -4.4285784 -4.4285765 -4.4285722 -4.4285831 -4.4286127 -4.428647 -4.4286847 -4.4287062 -4.4287014 -4.4286947 -4.428719]]...]
INFO - root - 2017-12-10 05:54:43.408364: step 9310, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:53m:50s remains)
INFO - root - 2017-12-10 05:54:46.060905: step 9320, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:53m:53s remains)
INFO - root - 2017-12-10 05:54:48.787433: step 9330, loss = 2.28, batch loss = 2.23 (27.6 examples/sec; 0.290 sec/batch; 26h:03m:49s remains)
INFO - root - 2017-12-10 05:54:51.417000: step 9340, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:10m:00s remains)
INFO - root - 2017-12-10 05:54:54.063288: step 9350, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:48m:15s remains)
INFO - root - 2017-12-10 05:54:56.728472: step 9360, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:30m:18s remains)
INFO - root - 2017-12-10 05:54:59.386989: step 9370, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:35m:42s remains)
INFO - root - 2017-12-10 05:55:01.984564: step 9380, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 23h:39m:22s remains)
INFO - root - 2017-12-10 05:55:04.633939: step 9390, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:36m:50s remains)
INFO - root - 2017-12-10 05:55:07.327926: step 9400, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:05m:17s remains)
2017-12-10 05:55:07.643289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290042 -4.4289989 -4.4289985 -4.428998 -4.428998 -4.4289975 -4.4289956 -4.4289923 -4.4289865 -4.4289832 -4.4289856 -4.4289908 -4.4289947 -4.4289937 -4.4289932][-4.4290071 -4.4290032 -4.4290009 -4.4289975 -4.4289937 -4.4289885 -4.4289842 -4.4289813 -4.428978 -4.428977 -4.4289804 -4.4289861 -4.4289894 -4.4289885 -4.4289885][-4.4290042 -4.4290004 -4.4289961 -4.4289889 -4.4289794 -4.4289656 -4.4289546 -4.4289517 -4.4289536 -4.4289613 -4.4289708 -4.4289789 -4.4289842 -4.4289842 -4.4289851][-4.4289966 -4.4289942 -4.4289856 -4.4289722 -4.4289527 -4.4289246 -4.4289017 -4.4288979 -4.4289079 -4.4289289 -4.4289517 -4.428968 -4.4289784 -4.4289813 -4.4289827][-4.4289694 -4.428968 -4.428956 -4.4289374 -4.4289074 -4.4288611 -4.428822 -4.4288154 -4.4288344 -4.428874 -4.4289174 -4.42895 -4.4289694 -4.4289784 -4.4289813][-4.4289141 -4.4289141 -4.428905 -4.4288807 -4.4288316 -4.4287567 -4.42869 -4.4286828 -4.4287257 -4.4287934 -4.4288607 -4.4289141 -4.4289503 -4.4289689 -4.4289742][-4.4288368 -4.4288416 -4.4288354 -4.4287977 -4.428719 -4.4285936 -4.4284806 -4.4284797 -4.4285755 -4.4286933 -4.4287939 -4.4288678 -4.4289207 -4.4289503 -4.42896][-4.4287534 -4.4287548 -4.4287486 -4.4287009 -4.4285989 -4.4284225 -4.4282422 -4.42825 -4.4284167 -4.428597 -4.4287343 -4.4288249 -4.4288874 -4.4289265 -4.4289422][-4.4287138 -4.4287066 -4.428699 -4.4286489 -4.4285407 -4.4283376 -4.4281111 -4.4281116 -4.4283171 -4.4285278 -4.42869 -4.428793 -4.4288621 -4.4289093 -4.4289312][-4.428719 -4.4287028 -4.4286976 -4.4286566 -4.4285722 -4.4284081 -4.4282141 -4.4282064 -4.4283733 -4.428555 -4.4287019 -4.4287972 -4.4288616 -4.4289093 -4.4289336][-4.4287596 -4.4287386 -4.4287333 -4.4287062 -4.4286561 -4.4285564 -4.4284277 -4.4284205 -4.4285293 -4.42866 -4.4287696 -4.4288425 -4.4288921 -4.42893 -4.4289508][-4.4288135 -4.42879 -4.4287872 -4.4287724 -4.4287505 -4.4286957 -4.4286194 -4.428616 -4.4286876 -4.4287791 -4.4288545 -4.4289026 -4.4289365 -4.4289618 -4.428977][-4.428885 -4.4288659 -4.4288664 -4.4288597 -4.4288497 -4.4288177 -4.4287724 -4.4287734 -4.4288216 -4.4288807 -4.4289241 -4.4289532 -4.4289737 -4.428987 -4.4289942][-4.4289446 -4.4289317 -4.4289327 -4.4289331 -4.4289322 -4.4289184 -4.4289002 -4.4289112 -4.4289408 -4.428968 -4.4289808 -4.4289932 -4.4290028 -4.4290042 -4.4290032][-4.428987 -4.42898 -4.4289808 -4.4289837 -4.4289885 -4.4289861 -4.4289846 -4.4289966 -4.429009 -4.4290133 -4.4290104 -4.4290123 -4.4290147 -4.4290094 -4.4290042]]...]
INFO - root - 2017-12-10 05:55:10.313880: step 9410, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:35m:13s remains)
INFO - root - 2017-12-10 05:55:12.939159: step 9420, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:15m:52s remains)
INFO - root - 2017-12-10 05:55:15.551005: step 9430, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:32m:38s remains)
INFO - root - 2017-12-10 05:55:18.229293: step 9440, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:57m:23s remains)
INFO - root - 2017-12-10 05:55:20.904576: step 9450, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:06m:12s remains)
INFO - root - 2017-12-10 05:55:23.597692: step 9460, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:56m:08s remains)
INFO - root - 2017-12-10 05:55:26.247670: step 9470, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:42m:48s remains)
INFO - root - 2017-12-10 05:55:28.877706: step 9480, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 23h:59m:28s remains)
INFO - root - 2017-12-10 05:55:31.506068: step 9490, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:00m:01s remains)
INFO - root - 2017-12-10 05:55:34.185905: step 9500, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:30m:07s remains)
2017-12-10 05:55:34.466280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287782 -4.4288168 -4.4288588 -4.4288325 -4.4287405 -4.4285984 -4.4285021 -4.4285126 -4.4286222 -4.4287524 -4.4288416 -4.4288859 -4.4289093 -4.4289327 -4.4289422][-4.4287868 -4.4288187 -4.428854 -4.4288325 -4.4287491 -4.4286079 -4.4285007 -4.4284873 -4.428587 -4.4287229 -4.42882 -4.4288793 -4.4289165 -4.4289436 -4.4289522][-4.4287887 -4.4288034 -4.4288197 -4.4288034 -4.4287367 -4.4286137 -4.4285011 -4.4284635 -4.4285393 -4.4286709 -4.428771 -4.4288378 -4.428884 -4.4289136 -4.428925][-4.4287815 -4.42878 -4.4287763 -4.42875 -4.4286952 -4.4286013 -4.4284949 -4.4284496 -4.4285011 -4.4286256 -4.4287233 -4.42879 -4.4288383 -4.4288621 -4.4288754][-4.4287486 -4.4287357 -4.4287171 -4.428669 -4.4286027 -4.428525 -4.428443 -4.4284148 -4.428463 -4.4285793 -4.4286742 -4.4287405 -4.4287848 -4.4288082 -4.4288259][-4.4287152 -4.4286923 -4.4286494 -4.4285669 -4.4284868 -4.42842 -4.4283743 -4.4283681 -4.4284124 -4.428515 -4.428617 -4.4286923 -4.4287348 -4.4287596 -4.4287858][-4.4286852 -4.4286633 -4.4286094 -4.4285083 -4.4284296 -4.4283762 -4.4283514 -4.428359 -4.4283981 -4.4284725 -4.428575 -4.4286652 -4.4287138 -4.4287353 -4.4287648][-4.4287009 -4.4286895 -4.4286513 -4.4285564 -4.4284668 -4.4284148 -4.4284039 -4.4284148 -4.4284549 -4.4285083 -4.4286013 -4.4286909 -4.4287386 -4.428751 -4.4287786][-4.428751 -4.4287457 -4.4287214 -4.4286475 -4.428556 -4.4285035 -4.4285035 -4.4285216 -4.4285631 -4.4286027 -4.4286804 -4.4287586 -4.4288011 -4.4288144 -4.4288311][-4.4288082 -4.42881 -4.428802 -4.428751 -4.42868 -4.4286427 -4.4286556 -4.4286737 -4.4286985 -4.4287143 -4.42876 -4.4288216 -4.4288664 -4.4288893 -4.4288964][-4.4288449 -4.4288554 -4.4288621 -4.4288311 -4.4287834 -4.4287667 -4.4287858 -4.4288025 -4.4288135 -4.4288044 -4.4288225 -4.428864 -4.4289026 -4.4289227 -4.4289303][-4.4288449 -4.4288611 -4.4288769 -4.4288564 -4.4288244 -4.4288216 -4.4288445 -4.4288659 -4.4288712 -4.4288568 -4.4288597 -4.4288774 -4.4288993 -4.4289122 -4.4289231][-4.4288497 -4.428863 -4.4288793 -4.428863 -4.4288349 -4.4288344 -4.428853 -4.4288721 -4.4288712 -4.4288583 -4.4288573 -4.428865 -4.4288778 -4.4288921 -4.428905][-4.4288831 -4.4288859 -4.4288936 -4.4288826 -4.4288592 -4.4288521 -4.4288611 -4.4288731 -4.4288731 -4.4288635 -4.4288635 -4.4288692 -4.42888 -4.428895 -4.4289074][-4.4289203 -4.4289188 -4.4289203 -4.4289122 -4.4288955 -4.428885 -4.4288855 -4.4288907 -4.4288921 -4.4288883 -4.42889 -4.4288988 -4.4289088 -4.4289188 -4.4289284]]...]
INFO - root - 2017-12-10 05:55:37.103634: step 9510, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:37m:33s remains)
INFO - root - 2017-12-10 05:55:39.757173: step 9520, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:20m:51s remains)
INFO - root - 2017-12-10 05:55:42.399387: step 9530, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:32m:51s remains)
INFO - root - 2017-12-10 05:55:45.037624: step 9540, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:31m:49s remains)
INFO - root - 2017-12-10 05:55:47.714743: step 9550, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:05m:03s remains)
INFO - root - 2017-12-10 05:55:50.322826: step 9560, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:12m:58s remains)
INFO - root - 2017-12-10 05:55:52.949582: step 9570, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:12m:15s remains)
INFO - root - 2017-12-10 05:55:55.622088: step 9580, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:55m:10s remains)
INFO - root - 2017-12-10 05:55:58.266023: step 9590, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:59m:48s remains)
INFO - root - 2017-12-10 05:56:00.879362: step 9600, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:23m:20s remains)
2017-12-10 05:56:01.184266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289927 -4.42899 -4.4289842 -4.4289584 -4.4289088 -4.4288454 -4.4288034 -4.4287915 -4.4288015 -4.4288058 -4.4287868 -4.4287467 -4.4286671 -4.4285831 -4.4285378][-4.428957 -4.4289536 -4.4289522 -4.42893 -4.428884 -4.4288139 -4.4287729 -4.42877 -4.42879 -4.4288 -4.428793 -4.4287682 -4.4286923 -4.428596 -4.428534][-4.4289184 -4.4289241 -4.4289312 -4.42891 -4.4288683 -4.4288034 -4.4287705 -4.4287715 -4.428791 -4.4287987 -4.4288015 -4.4288034 -4.4287567 -4.4286871 -4.428647][-4.4288554 -4.4288812 -4.4289021 -4.428885 -4.4288516 -4.428802 -4.4287877 -4.4287939 -4.428813 -4.4288149 -4.4288197 -4.4288416 -4.4288225 -4.4287844 -4.4287667][-4.4287868 -4.4288297 -4.4288626 -4.4288564 -4.4288349 -4.4288025 -4.4288039 -4.4288144 -4.4288335 -4.4288354 -4.4288406 -4.4288712 -4.4288692 -4.4288535 -4.4288535][-4.4287033 -4.4287639 -4.4288106 -4.4288182 -4.4288087 -4.4287825 -4.4287853 -4.4287977 -4.4288177 -4.4288278 -4.4288449 -4.4288869 -4.4288979 -4.4288974 -4.4289112][-4.4286761 -4.4287367 -4.4287763 -4.4287853 -4.4287791 -4.4287434 -4.4287276 -4.4287338 -4.4287562 -4.4287829 -4.4288263 -4.4288864 -4.4289126 -4.4289212 -4.42894][-4.4287181 -4.4287524 -4.4287629 -4.4287562 -4.42873 -4.42866 -4.4286222 -4.4286261 -4.428659 -4.428709 -4.4287858 -4.4288731 -4.4289188 -4.4289312 -4.4289508][-4.4287534 -4.4287529 -4.4287229 -4.4286866 -4.4286313 -4.4285316 -4.4284778 -4.4284897 -4.4285464 -4.4286256 -4.4287295 -4.4288321 -4.4288945 -4.4289188 -4.4289393][-4.4287624 -4.4287286 -4.4286661 -4.4286046 -4.428535 -4.42844 -4.4283953 -4.428421 -4.428493 -4.4285884 -4.4286971 -4.428793 -4.428863 -4.4289045 -4.4289231][-4.4287648 -4.4287076 -4.4286346 -4.42858 -4.4285364 -4.4284863 -4.4284725 -4.4284987 -4.428556 -4.4286351 -4.4287233 -4.4287949 -4.4288545 -4.4289007 -4.4289112][-4.4287724 -4.4287148 -4.4286537 -4.42862 -4.4286084 -4.4285975 -4.428607 -4.428628 -4.4286633 -4.4287233 -4.4287963 -4.4288483 -4.4288855 -4.4289184 -4.4289184][-4.4287963 -4.4287505 -4.4287076 -4.4286952 -4.4287066 -4.4287219 -4.4287405 -4.4287529 -4.4287724 -4.4288135 -4.4288688 -4.4289026 -4.4289227 -4.4289389 -4.4289284][-4.4288325 -4.4288044 -4.428781 -4.4287868 -4.4288106 -4.428834 -4.4288535 -4.42886 -4.4288692 -4.4288936 -4.4289312 -4.4289508 -4.4289556 -4.4289551 -4.4289317][-4.4288673 -4.4288559 -4.428853 -4.4288673 -4.42889 -4.4289107 -4.4289236 -4.4289236 -4.4289236 -4.4289384 -4.4289651 -4.4289751 -4.42897 -4.4289546 -4.4289169]]...]
INFO - root - 2017-12-10 05:56:03.832617: step 9610, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:14m:58s remains)
INFO - root - 2017-12-10 05:56:06.476112: step 9620, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:41m:51s remains)
INFO - root - 2017-12-10 05:56:09.228944: step 9630, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:54m:52s remains)
INFO - root - 2017-12-10 05:56:11.887854: step 9640, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:59m:04s remains)
INFO - root - 2017-12-10 05:56:14.574842: step 9650, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:47m:47s remains)
INFO - root - 2017-12-10 05:56:17.203063: step 9660, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:19m:06s remains)
INFO - root - 2017-12-10 05:56:19.847729: step 9670, loss = 2.28, batch loss = 2.23 (27.6 examples/sec; 0.290 sec/batch; 25h:58m:07s remains)
INFO - root - 2017-12-10 05:56:22.479714: step 9680, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:18m:38s remains)
INFO - root - 2017-12-10 05:56:25.138019: step 9690, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:31m:20s remains)
INFO - root - 2017-12-10 05:56:27.740962: step 9700, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:02m:29s remains)
2017-12-10 05:56:28.058438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288783 -4.4288831 -4.428885 -4.4288807 -4.4288788 -4.4288812 -4.4288836 -4.4288888 -4.4288912 -4.4288945 -4.4289045 -4.4289203 -4.4289355 -4.4289441 -4.4289484][-4.4288421 -4.4288526 -4.4288578 -4.4288454 -4.4288321 -4.4288311 -4.4288254 -4.42882 -4.428823 -4.4288368 -4.4288659 -4.4288974 -4.4289222 -4.4289365 -4.4289432][-4.4287968 -4.428813 -4.4288168 -4.428791 -4.4287624 -4.4287505 -4.42873 -4.428709 -4.42872 -4.4287562 -4.428813 -4.428865 -4.4289021 -4.4289255 -4.4289365][-4.4287696 -4.4287796 -4.4287686 -4.4287233 -4.4286804 -4.4286494 -4.4285965 -4.4285479 -4.4285769 -4.4286456 -4.4287324 -4.428812 -4.4288678 -4.4289055 -4.4289236][-4.4287572 -4.4287515 -4.4287171 -4.4286575 -4.4286056 -4.4285502 -4.4284368 -4.4283428 -4.4284005 -4.4285121 -4.4286323 -4.428741 -4.4288249 -4.4288812 -4.4289074][-4.4287477 -4.4287353 -4.4286804 -4.4286041 -4.4285393 -4.4284539 -4.4282594 -4.4281039 -4.4282131 -4.4283938 -4.4285493 -4.4286823 -4.4287882 -4.4288616 -4.4288955][-4.4287634 -4.4287453 -4.4286852 -4.4285946 -4.428503 -4.4283628 -4.4280562 -4.4278378 -4.4280362 -4.4283137 -4.4285088 -4.4286647 -4.428782 -4.4288611 -4.4288931][-4.4287834 -4.4287691 -4.4287143 -4.4286122 -4.4284792 -4.4282632 -4.4278269 -4.4275517 -4.427876 -4.4282594 -4.4284883 -4.4286628 -4.4287891 -4.4288635 -4.4288898][-4.4287877 -4.4287844 -4.4287462 -4.4286485 -4.4284935 -4.428247 -4.4278035 -4.4275651 -4.427918 -4.4283109 -4.4285235 -4.428689 -4.4288092 -4.4288721 -4.4288898][-4.428792 -4.4288006 -4.4287796 -4.4287004 -4.4285564 -4.4283381 -4.4279885 -4.4278264 -4.4280992 -4.4284096 -4.4285789 -4.428719 -4.4288254 -4.4288783 -4.4288912][-4.4287724 -4.4287925 -4.42879 -4.4287367 -4.428617 -4.4284396 -4.4281778 -4.4280705 -4.4282708 -4.4285011 -4.4286275 -4.4287386 -4.4288321 -4.4288783 -4.4288917][-4.4287362 -4.4287734 -4.4287992 -4.4287734 -4.4286885 -4.4285607 -4.4283814 -4.4283109 -4.4284477 -4.4286051 -4.4286866 -4.4287653 -4.4288445 -4.4288855 -4.4288979][-4.4287224 -4.428772 -4.4288211 -4.4288216 -4.4287744 -4.4286938 -4.4285707 -4.428515 -4.4285936 -4.4286966 -4.4287539 -4.4288082 -4.4288635 -4.428895 -4.428906][-4.4287314 -4.4287825 -4.4288449 -4.4288578 -4.4288373 -4.4287863 -4.4287014 -4.4286604 -4.4287057 -4.4287786 -4.4288168 -4.4288416 -4.4288745 -4.4288983 -4.4289088][-4.4287496 -4.4288039 -4.4288673 -4.4288874 -4.4288816 -4.4288454 -4.428781 -4.42875 -4.428781 -4.4288368 -4.4288578 -4.4288549 -4.42887 -4.4288926 -4.4289069]]...]
INFO - root - 2017-12-10 05:56:30.727991: step 9710, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:40m:35s remains)
INFO - root - 2017-12-10 05:56:33.357544: step 9720, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:28m:32s remains)
INFO - root - 2017-12-10 05:56:35.974066: step 9730, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 25h:32m:35s remains)
INFO - root - 2017-12-10 05:56:38.643841: step 9740, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:46m:28s remains)
INFO - root - 2017-12-10 05:56:41.294846: step 9750, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:03m:59s remains)
INFO - root - 2017-12-10 05:56:43.940014: step 9760, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:28m:50s remains)
INFO - root - 2017-12-10 05:56:46.627671: step 9770, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.282 sec/batch; 25h:17m:23s remains)
INFO - root - 2017-12-10 05:56:49.272519: step 9780, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:55m:04s remains)
INFO - root - 2017-12-10 05:56:51.975569: step 9790, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:55m:12s remains)
INFO - root - 2017-12-10 05:56:54.642950: step 9800, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:07m:37s remains)
2017-12-10 05:56:54.927508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289093 -4.4289002 -4.4288936 -4.4288874 -4.428885 -4.4288878 -4.4288936 -4.4288988 -4.4289031 -4.4289017 -4.4288983 -4.4288921 -4.4288726 -4.4288492 -4.4288239][-4.42886 -4.4288487 -4.4288373 -4.4288273 -4.4288239 -4.4288311 -4.4288459 -4.4288621 -4.4288759 -4.42888 -4.4288826 -4.4288754 -4.4288507 -4.4288239 -4.4287972][-4.428834 -4.4288192 -4.4288011 -4.4287839 -4.4287782 -4.4287906 -4.4288168 -4.4288487 -4.42888 -4.4288983 -4.4289122 -4.4289093 -4.42888 -4.4288478 -4.4288096][-4.428813 -4.4287944 -4.4287653 -4.4287343 -4.4287157 -4.4287238 -4.4287553 -4.4287992 -4.4288454 -4.4288774 -4.4289064 -4.4289136 -4.4288864 -4.4288516 -4.4288049][-4.4287663 -4.4287434 -4.4287024 -4.4286532 -4.428616 -4.4286094 -4.4286346 -4.4286795 -4.4287319 -4.4287767 -4.4288287 -4.4288583 -4.4288483 -4.4288211 -4.4287772][-4.4286966 -4.4286742 -4.4286275 -4.4285631 -4.428504 -4.4284773 -4.4284863 -4.4285159 -4.42856 -4.4286103 -4.4286842 -4.4287419 -4.428762 -4.4287572 -4.428731][-4.4286919 -4.4286757 -4.4286327 -4.4285631 -4.42849 -4.428441 -4.4284182 -4.4284105 -4.4284248 -4.4284663 -4.4285574 -4.4286408 -4.428689 -4.4287109 -4.4287081][-4.428813 -4.4288039 -4.4287705 -4.4287133 -4.4286537 -4.4286122 -4.4285803 -4.4285488 -4.4285331 -4.4285455 -4.4286146 -4.4286866 -4.428731 -4.428751 -4.4287519][-4.4288874 -4.4288845 -4.4288645 -4.4288268 -4.428791 -4.4287686 -4.4287467 -4.4287143 -4.428688 -4.4286733 -4.4287028 -4.4287434 -4.4287724 -4.4287815 -4.4287772][-4.4288836 -4.4288893 -4.428884 -4.4288626 -4.4288449 -4.4288354 -4.428822 -4.4287968 -4.4287724 -4.4287438 -4.4287453 -4.4287596 -4.4287772 -4.42878 -4.4287739][-4.4288807 -4.4288878 -4.4288888 -4.428875 -4.4288635 -4.4288583 -4.4288521 -4.4288363 -4.4288254 -4.4288039 -4.4288 -4.4288034 -4.42881 -4.428802 -4.4287877][-4.428906 -4.4289036 -4.4288988 -4.4288826 -4.4288654 -4.4288535 -4.4288487 -4.4288406 -4.4288397 -4.428833 -4.4288378 -4.4288363 -4.4288316 -4.4288154 -4.4287972][-4.428937 -4.4289279 -4.4289169 -4.4288955 -4.4288659 -4.4288392 -4.428823 -4.428812 -4.4288173 -4.4288216 -4.428834 -4.4288287 -4.4288144 -4.4287949 -4.4287786][-4.4289694 -4.42896 -4.428947 -4.4289227 -4.428885 -4.4288449 -4.428813 -4.4287925 -4.4287939 -4.4288015 -4.4288154 -4.4288077 -4.4287877 -4.4287658 -4.4287519][-4.4289894 -4.4289861 -4.428978 -4.4289589 -4.4289246 -4.4288821 -4.4288411 -4.4288092 -4.4287996 -4.4288 -4.4288077 -4.428802 -4.4287825 -4.428761 -4.4287515]]...]
INFO - root - 2017-12-10 05:56:57.579405: step 9810, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:46m:49s remains)
INFO - root - 2017-12-10 05:57:00.256096: step 9820, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:35m:02s remains)
INFO - root - 2017-12-10 05:57:02.918967: step 9830, loss = 2.28, batch loss = 2.23 (27.3 examples/sec; 0.293 sec/batch; 26h:13m:39s remains)
INFO - root - 2017-12-10 05:57:05.557187: step 9840, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:58m:37s remains)
INFO - root - 2017-12-10 05:57:08.205954: step 9850, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:01m:57s remains)
INFO - root - 2017-12-10 05:57:10.853974: step 9860, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:36m:45s remains)
INFO - root - 2017-12-10 05:57:13.568876: step 9870, loss = 2.28, batch loss = 2.23 (28.2 examples/sec; 0.284 sec/batch; 25h:24m:34s remains)
INFO - root - 2017-12-10 05:57:16.307933: step 9880, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:08m:52s remains)
INFO - root - 2017-12-10 05:57:18.979298: step 9890, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:58m:37s remains)
INFO - root - 2017-12-10 05:57:21.622528: step 9900, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 22h:59m:50s remains)
2017-12-10 05:57:21.979804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289637 -4.4289408 -4.4289207 -4.4289117 -4.4289122 -4.4289222 -4.4289227 -4.4289141 -4.4289131 -4.4289203 -4.4289274 -4.4289455 -4.4289622 -4.4289656 -4.4289613][-4.4289336 -4.4289031 -4.42888 -4.4288697 -4.4288721 -4.4288874 -4.4288907 -4.4288774 -4.4288745 -4.4288883 -4.4289055 -4.4289322 -4.4289522 -4.4289541 -4.4289479][-4.4289145 -4.4288778 -4.42885 -4.42883 -4.4288259 -4.4288387 -4.42884 -4.4288244 -4.4288187 -4.4288363 -4.428865 -4.4289021 -4.4289284 -4.4289341 -4.4289308][-4.4288993 -4.4288578 -4.4288239 -4.4287925 -4.4287739 -4.4287777 -4.4287667 -4.4287453 -4.42874 -4.4287586 -4.4287934 -4.4288421 -4.4288783 -4.4288969 -4.428906][-4.4288778 -4.4288316 -4.4287939 -4.4287558 -4.4287257 -4.4287114 -4.4286757 -4.4286432 -4.4286451 -4.4286695 -4.4287133 -4.4287715 -4.4288192 -4.4288492 -4.4288688][-4.4288549 -4.4288006 -4.4287553 -4.4287152 -4.4286809 -4.4286451 -4.4285765 -4.4285321 -4.4285455 -4.42858 -4.4286385 -4.4287095 -4.4287682 -4.4288063 -4.428833][-4.4288487 -4.4287858 -4.4287286 -4.4286795 -4.4286408 -4.4285827 -4.4284763 -4.4284215 -4.4284549 -4.4285131 -4.4285932 -4.4286757 -4.4287395 -4.4287863 -4.4288168][-4.4288669 -4.4287944 -4.428719 -4.4286575 -4.42861 -4.4285216 -4.4283834 -4.4283347 -4.4284115 -4.4285026 -4.4286003 -4.4286857 -4.4287481 -4.4287992 -4.42883][-4.4288969 -4.4288268 -4.4287467 -4.428689 -4.4286418 -4.42854 -4.4283929 -4.4283652 -4.4284644 -4.428566 -4.4286532 -4.4287233 -4.4287839 -4.4288383 -4.428865][-4.4289255 -4.428864 -4.428793 -4.4287505 -4.428719 -4.4286366 -4.4285212 -4.428503 -4.4285793 -4.4286561 -4.4287171 -4.4287667 -4.428822 -4.4288692 -4.4288912][-4.4289355 -4.4288874 -4.4288292 -4.4288025 -4.4287848 -4.4287267 -4.4286475 -4.428628 -4.4286747 -4.4287267 -4.4287696 -4.4288054 -4.4288483 -4.4288788 -4.4288931][-4.4289417 -4.4289103 -4.4288683 -4.42885 -4.4288378 -4.4287949 -4.428741 -4.4287214 -4.4287472 -4.4287844 -4.4288206 -4.4288507 -4.4288735 -4.428884 -4.4288921][-4.428956 -4.4289341 -4.4289017 -4.4288836 -4.4288726 -4.428843 -4.4288116 -4.428802 -4.4288182 -4.42884 -4.4288645 -4.428885 -4.4288974 -4.4288983 -4.4289036][-4.4289689 -4.4289522 -4.4289255 -4.4289074 -4.4288988 -4.4288769 -4.4288583 -4.4288583 -4.4288669 -4.4288712 -4.4288812 -4.4289 -4.428915 -4.4289184 -4.4289231][-4.4289818 -4.4289718 -4.4289536 -4.4289408 -4.4289355 -4.4289184 -4.4289055 -4.428906 -4.4289079 -4.4289012 -4.4289045 -4.4289231 -4.42894 -4.4289455 -4.4289484]]...]
INFO - root - 2017-12-10 05:57:24.652268: step 9910, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:19m:53s remains)
INFO - root - 2017-12-10 05:57:27.327244: step 9920, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:56m:03s remains)
INFO - root - 2017-12-10 05:57:29.977747: step 9930, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:01m:22s remains)
INFO - root - 2017-12-10 05:57:32.616019: step 9940, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:50m:05s remains)
INFO - root - 2017-12-10 05:57:35.283988: step 9950, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 23h:41m:51s remains)
INFO - root - 2017-12-10 05:57:37.969542: step 9960, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 23h:57m:33s remains)
INFO - root - 2017-12-10 05:57:40.639473: step 9970, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.271 sec/batch; 24h:14m:54s remains)
INFO - root - 2017-12-10 05:57:43.288216: step 9980, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:16m:00s remains)
INFO - root - 2017-12-10 05:57:45.999495: step 9990, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:25m:25s remains)
INFO - root - 2017-12-10 05:57:48.655945: step 10000, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:38m:04s remains)
2017-12-10 05:57:48.990629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289622 -4.4289446 -4.4289279 -4.4289126 -4.428895 -4.4288797 -4.4288688 -4.4288673 -4.428874 -4.4288778 -4.4288769 -4.4288812 -4.4288921 -4.4289045 -4.4289165][-4.4289541 -4.4289289 -4.4289064 -4.4288888 -4.4288692 -4.4288464 -4.4288282 -4.4288254 -4.4288383 -4.4288449 -4.4288354 -4.4288363 -4.4288545 -4.4288731 -4.4288907][-4.4289322 -4.4288931 -4.4288568 -4.4288282 -4.4288049 -4.4287734 -4.428741 -4.4287343 -4.4287543 -4.42877 -4.428761 -4.4287667 -4.4287925 -4.42882 -4.4288454][-4.428916 -4.4288654 -4.4288168 -4.4287758 -4.4287443 -4.4287 -4.428647 -4.4286389 -4.428668 -4.428689 -4.4286809 -4.4286885 -4.4287195 -4.42876 -4.4287987][-4.4288988 -4.4288411 -4.428782 -4.4287295 -4.4286771 -4.4286027 -4.4285188 -4.4285083 -4.4285593 -4.4286027 -4.4286084 -4.4286256 -4.428668 -4.4287219 -4.4287729][-4.4288807 -4.4288173 -4.4287486 -4.4286842 -4.4286103 -4.4284959 -4.4283609 -4.4283371 -4.4284253 -4.4285121 -4.4285583 -4.428597 -4.4286489 -4.4287066 -4.4287581][-4.4288731 -4.428812 -4.4287395 -4.428659 -4.428556 -4.4283891 -4.4281716 -4.4281063 -4.4282532 -4.4284072 -4.4285092 -4.4285851 -4.4286461 -4.4287009 -4.4287515][-4.4288793 -4.4288263 -4.4287586 -4.4286666 -4.428525 -4.4283013 -4.427999 -4.4278817 -4.4280882 -4.4283166 -4.4284773 -4.4285841 -4.4286571 -4.4287133 -4.4287634][-4.42889 -4.4288473 -4.4287925 -4.4287167 -4.4285903 -4.4283977 -4.4281497 -4.4280548 -4.4282236 -4.4284225 -4.4285655 -4.4286513 -4.4287076 -4.4287543 -4.428792][-4.4288898 -4.4288497 -4.4288011 -4.4287381 -4.428648 -4.4285207 -4.4283767 -4.4283352 -4.4284477 -4.4285793 -4.4286742 -4.4287281 -4.4287639 -4.4287963 -4.42882][-4.428896 -4.4288588 -4.4288168 -4.4287615 -4.4286942 -4.4286156 -4.4285483 -4.4285502 -4.428637 -4.4287305 -4.428792 -4.4288263 -4.4288416 -4.4288545 -4.4288592][-4.4289231 -4.4289021 -4.42888 -4.4288406 -4.4287968 -4.4287562 -4.4287276 -4.4287434 -4.4288 -4.4288545 -4.4288883 -4.4289117 -4.4289126 -4.4289064 -4.4288969][-4.4289522 -4.4289455 -4.4289379 -4.4289103 -4.4288812 -4.4288597 -4.4288492 -4.4288592 -4.42889 -4.4289155 -4.4289308 -4.4289436 -4.4289384 -4.4289284 -4.4289174][-4.4289732 -4.4289708 -4.4289637 -4.4289441 -4.428925 -4.4289169 -4.428916 -4.4289174 -4.4289289 -4.4289389 -4.4289412 -4.4289484 -4.4289441 -4.4289351 -4.4289289][-4.4289832 -4.4289804 -4.4289722 -4.428956 -4.4289427 -4.428937 -4.428937 -4.4289355 -4.4289403 -4.4289479 -4.4289522 -4.4289565 -4.4289532 -4.428946 -4.4289436]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr10-lastlr15-clip10000/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr10-lastlr15-clip10000/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 05:57:52.130700: step 10010, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:54m:34s remains)
INFO - root - 2017-12-10 05:57:54.786540: step 10020, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.285 sec/batch; 25h:33m:44s remains)
INFO - root - 2017-12-10 05:57:57.418805: step 10030, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:10m:57s remains)
INFO - root - 2017-12-10 05:58:00.052969: step 10040, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:17m:30s remains)
INFO - root - 2017-12-10 05:58:02.676456: step 10050, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:02m:01s remains)
INFO - root - 2017-12-10 05:58:05.356243: step 10060, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:40m:28s remains)
INFO - root - 2017-12-10 05:58:08.063287: step 10070, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.282 sec/batch; 25h:13m:53s remains)
INFO - root - 2017-12-10 05:58:10.737912: step 10080, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:06m:04s remains)
INFO - root - 2017-12-10 05:58:13.398590: step 10090, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 23h:41m:15s remains)
INFO - root - 2017-12-10 05:58:16.064991: step 10100, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 22h:52m:32s remains)
2017-12-10 05:58:16.353415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289031 -4.4288912 -4.4289036 -4.4289436 -4.42899 -4.4290257 -4.4290409 -4.4290366 -4.4290204 -4.4290013 -4.4289827 -4.4289765 -4.4289904 -4.4290075 -4.4290156][-4.4288931 -4.4288712 -4.4288688 -4.4288931 -4.4289308 -4.4289665 -4.4289913 -4.4289994 -4.4289923 -4.4289737 -4.4289513 -4.4289432 -4.4289517 -4.4289627 -4.4289651][-4.4288988 -4.4288664 -4.4288445 -4.4288487 -4.4288716 -4.4288983 -4.4289227 -4.4289384 -4.4289393 -4.4289231 -4.4289041 -4.4288974 -4.4289026 -4.4289064 -4.42891][-4.4289 -4.428853 -4.4288082 -4.428791 -4.4287982 -4.428812 -4.4288273 -4.4288478 -4.4288611 -4.4288564 -4.4288459 -4.4288411 -4.4288435 -4.4288445 -4.4288545][-4.4288974 -4.4288321 -4.4287596 -4.428721 -4.4287071 -4.4286957 -4.4286842 -4.4286942 -4.4287243 -4.4287505 -4.4287686 -4.4287796 -4.428792 -4.4288058 -4.4288292][-4.428896 -4.4288158 -4.428721 -4.4286609 -4.4286189 -4.4285679 -4.428503 -4.42847 -4.4285145 -4.4285975 -4.428679 -4.4287333 -4.4287772 -4.4288149 -4.4288445][-4.4289007 -4.4288168 -4.4287143 -4.428638 -4.42857 -4.4284706 -4.4283266 -4.4282265 -4.4282837 -4.428442 -4.4286008 -4.4287081 -4.4287882 -4.4288473 -4.4288774][-4.4289002 -4.4288216 -4.4287286 -4.4286532 -4.4285727 -4.428442 -4.428247 -4.4280992 -4.4281578 -4.4283538 -4.4285531 -4.4286962 -4.4287992 -4.4288692 -4.4289017][-4.4288707 -4.4288063 -4.4287395 -4.4286923 -4.42864 -4.4285398 -4.4283857 -4.4282613 -4.4282818 -4.4284196 -4.4285808 -4.4287066 -4.4287972 -4.4288568 -4.428884][-4.4288158 -4.4287653 -4.4287276 -4.4287186 -4.4287148 -4.4286804 -4.4286046 -4.4285245 -4.4285164 -4.4285827 -4.4286728 -4.4287443 -4.4287906 -4.4288163 -4.4288216][-4.4287591 -4.4287138 -4.4286962 -4.42872 -4.4287591 -4.4287777 -4.4287615 -4.4287257 -4.428719 -4.4287496 -4.4287858 -4.4288015 -4.4287968 -4.4287786 -4.428751][-4.428741 -4.4286981 -4.4286909 -4.42874 -4.4288049 -4.4288487 -4.428863 -4.4288559 -4.428854 -4.4288597 -4.4288592 -4.4288335 -4.4287934 -4.4287472 -4.4286976][-4.4287815 -4.4287505 -4.4287519 -4.4288116 -4.4288759 -4.4289112 -4.4289165 -4.4289036 -4.4288898 -4.4288716 -4.4288464 -4.4288077 -4.4287667 -4.4287238 -4.4286814][-4.4288626 -4.4288425 -4.4288478 -4.4288988 -4.4289451 -4.4289589 -4.4289427 -4.4289074 -4.428864 -4.4288096 -4.4287605 -4.4287248 -4.4287024 -4.4286819 -4.428669][-4.4289451 -4.4289327 -4.4289355 -4.4289718 -4.4289956 -4.428988 -4.4289575 -4.4289117 -4.42885 -4.428771 -4.4287052 -4.4286714 -4.4286585 -4.4286528 -4.4286661]]...]
INFO - root - 2017-12-10 05:58:18.979543: step 10110, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 23h:35m:55s remains)
INFO - root - 2017-12-10 05:58:21.646240: step 10120, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:04m:37s remains)
INFO - root - 2017-12-10 05:58:24.283041: step 10130, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:14m:12s remains)
INFO - root - 2017-12-10 05:58:26.945364: step 10140, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:04m:08s remains)
INFO - root - 2017-12-10 05:58:29.544195: step 10150, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 23h:56m:33s remains)
INFO - root - 2017-12-10 05:58:32.247518: step 10160, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:36m:21s remains)
INFO - root - 2017-12-10 05:58:34.885091: step 10170, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:16m:00s remains)
INFO - root - 2017-12-10 05:58:37.567973: step 10180, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:01m:03s remains)
INFO - root - 2017-12-10 05:58:40.221952: step 10190, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 24h:49m:23s remains)
INFO - root - 2017-12-10 05:58:42.885634: step 10200, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:09m:34s remains)
2017-12-10 05:58:43.177963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428947 -4.4289451 -4.4289446 -4.4289513 -4.4289689 -4.4289913 -4.4290085 -4.4290166 -4.4290137 -4.4290032 -4.4289966 -4.4290023 -4.4290175 -4.4290314 -4.4290295][-4.4289742 -4.4289761 -4.4289746 -4.4289765 -4.428987 -4.4290013 -4.4290104 -4.4290109 -4.4290075 -4.4290018 -4.429 -4.429008 -4.4290223 -4.4290318 -4.4290271][-4.4289927 -4.4289966 -4.4289956 -4.4289923 -4.4289927 -4.4289956 -4.4289947 -4.428987 -4.4289837 -4.4289818 -4.4289856 -4.4289956 -4.4290061 -4.4290109 -4.4290028][-4.4290013 -4.4290061 -4.4290013 -4.4289894 -4.4289789 -4.428967 -4.4289503 -4.4289293 -4.4289227 -4.4289308 -4.4289465 -4.4289618 -4.4289713 -4.4289694 -4.428957][-4.4289937 -4.4290018 -4.4289947 -4.4289751 -4.4289494 -4.428916 -4.4288735 -4.4288273 -4.4288111 -4.4288373 -4.4288812 -4.4289107 -4.4289241 -4.4289155 -4.4289007][-4.428967 -4.4289746 -4.4289618 -4.42893 -4.4288836 -4.4288225 -4.4287424 -4.4286594 -4.4286366 -4.4286985 -4.428791 -4.4288516 -4.4288774 -4.4288678 -4.428854][-4.4289379 -4.4289384 -4.4289117 -4.4288607 -4.4287872 -4.42869 -4.4285626 -4.4284444 -4.4284344 -4.4285564 -4.4287114 -4.4288111 -4.4288545 -4.42885 -4.428834][-4.4289231 -4.4289131 -4.4288678 -4.4287972 -4.428699 -4.4285741 -4.4284143 -4.4282823 -4.4283061 -4.4284883 -4.42869 -4.4288125 -4.428864 -4.4288616 -4.4288373][-4.4289236 -4.4289083 -4.4288526 -4.4287705 -4.4286666 -4.428544 -4.4283981 -4.428297 -4.4283576 -4.4285526 -4.4287457 -4.4288535 -4.4288917 -4.4288769 -4.4288449][-4.4289532 -4.4289365 -4.4288769 -4.4287977 -4.4287105 -4.4286237 -4.4285307 -4.4284835 -4.4285522 -4.4287043 -4.4288421 -4.42891 -4.4289207 -4.4288936 -4.4288626][-4.428997 -4.4289775 -4.4289222 -4.4288573 -4.428793 -4.4287443 -4.4287033 -4.4286938 -4.4287515 -4.4288507 -4.4289331 -4.4289646 -4.4289517 -4.4289212 -4.4289007][-4.4290218 -4.4290013 -4.428957 -4.428906 -4.4288654 -4.4288464 -4.4288387 -4.4288478 -4.4288898 -4.4289479 -4.4289942 -4.4290061 -4.4289842 -4.428956 -4.4289422][-4.42902 -4.429 -4.4289665 -4.4289322 -4.4289122 -4.4289126 -4.4289227 -4.4289389 -4.428968 -4.4290028 -4.4290309 -4.4290376 -4.4290204 -4.4290018 -4.4289927][-4.4290051 -4.4289832 -4.4289522 -4.4289246 -4.4289141 -4.4289212 -4.4289403 -4.4289618 -4.428988 -4.4290175 -4.4290395 -4.429049 -4.4290409 -4.4290318 -4.4290266][-4.428978 -4.4289422 -4.4289012 -4.4288669 -4.4288487 -4.42885 -4.4288716 -4.4289021 -4.4289403 -4.4289789 -4.4290075 -4.4290252 -4.4290261 -4.4290209 -4.4290171]]...]
INFO - root - 2017-12-10 05:58:45.830686: step 10210, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 25h:33m:53s remains)
INFO - root - 2017-12-10 05:58:48.503830: step 10220, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:54m:17s remains)
INFO - root - 2017-12-10 05:58:51.166507: step 10230, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:52m:37s remains)
INFO - root - 2017-12-10 05:58:53.855078: step 10240, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.290 sec/batch; 26h:00m:06s remains)
INFO - root - 2017-12-10 05:58:56.501324: step 10250, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:04m:21s remains)
INFO - root - 2017-12-10 05:58:59.128932: step 10260, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:59m:47s remains)
INFO - root - 2017-12-10 05:59:01.796094: step 10270, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:02m:15s remains)
INFO - root - 2017-12-10 05:59:04.456614: step 10280, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:54m:23s remains)
INFO - root - 2017-12-10 05:59:07.090062: step 10290, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:13m:57s remains)
INFO - root - 2017-12-10 05:59:09.770113: step 10300, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:09m:46s remains)
2017-12-10 05:59:10.086288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288297 -4.4288244 -4.428823 -4.4288239 -4.4288192 -4.4288158 -4.4288092 -4.4288049 -4.4288039 -4.4288087 -4.4288187 -4.4288359 -4.42886 -4.4288816 -4.4289][-4.4287357 -4.4287376 -4.4287457 -4.4287519 -4.4287453 -4.4287405 -4.4287353 -4.4287295 -4.4287281 -4.4287314 -4.4287443 -4.4287672 -4.4287949 -4.4288135 -4.4288235][-4.4286823 -4.4286966 -4.4287148 -4.4287257 -4.42872 -4.4287152 -4.4287095 -4.4286957 -4.4286885 -4.4286895 -4.4287047 -4.4287286 -4.4287548 -4.4287682 -4.42877][-4.4286942 -4.42871 -4.4287219 -4.4287267 -4.4287181 -4.4287105 -4.4287038 -4.428688 -4.4286771 -4.4286728 -4.4286804 -4.4286938 -4.4287052 -4.4287076 -4.4287052][-4.4287243 -4.4287405 -4.4287429 -4.42874 -4.4287276 -4.4287124 -4.4287009 -4.4286852 -4.428668 -4.4286532 -4.42865 -4.4286532 -4.4286551 -4.4286485 -4.4286447][-4.4287491 -4.4287634 -4.4287596 -4.4287438 -4.4287162 -4.4286823 -4.4286542 -4.4286275 -4.428596 -4.4285712 -4.4285669 -4.4285769 -4.428586 -4.42859 -4.4285965][-4.4287214 -4.4287329 -4.4287267 -4.4287 -4.428658 -4.4286046 -4.4285517 -4.4285016 -4.4284592 -4.4284339 -4.4284339 -4.4284539 -4.4284821 -4.4285097 -4.4285283][-4.4286251 -4.4286265 -4.4286118 -4.4285731 -4.4285278 -4.4284749 -4.428412 -4.4283509 -4.428308 -4.4282889 -4.4282932 -4.4283209 -4.4283681 -4.4284182 -4.4284449][-4.4285779 -4.4285665 -4.4285445 -4.4285035 -4.4284658 -4.4284291 -4.4283848 -4.42834 -4.428308 -4.4282937 -4.4282866 -4.428299 -4.4283433 -4.4283972 -4.4284177][-4.4286547 -4.4286389 -4.428618 -4.4285836 -4.4285541 -4.428534 -4.4285116 -4.4284906 -4.428472 -4.4284568 -4.4284358 -4.4284286 -4.4284525 -4.4284921 -4.4285016][-4.4287667 -4.4287548 -4.428741 -4.4287167 -4.4286971 -4.4286866 -4.4286733 -4.4286623 -4.4286504 -4.428637 -4.4286141 -4.4285951 -4.4286017 -4.4286237 -4.4286246][-4.4288006 -4.4287944 -4.4287872 -4.4287734 -4.4287615 -4.4287558 -4.4287477 -4.4287372 -4.4287252 -4.4287176 -4.4287086 -4.4286962 -4.4286981 -4.4287114 -4.4287076][-4.4287534 -4.4287515 -4.4287515 -4.4287505 -4.4287472 -4.4287438 -4.4287372 -4.4287257 -4.4287133 -4.4287071 -4.4287071 -4.4287038 -4.4287047 -4.4287148 -4.4287124][-4.4287214 -4.4287243 -4.4287329 -4.4287438 -4.4287524 -4.4287572 -4.4287567 -4.4287496 -4.42874 -4.4287348 -4.4287362 -4.4287329 -4.4287314 -4.4287376 -4.4287367][-4.4287658 -4.4287682 -4.4287777 -4.4287934 -4.4288087 -4.4288211 -4.4288268 -4.4288254 -4.4288192 -4.4288144 -4.4288125 -4.4288068 -4.4288044 -4.4288087 -4.4288116]]...]
INFO - root - 2017-12-10 05:59:12.713386: step 10310, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.274 sec/batch; 24h:33m:44s remains)
INFO - root - 2017-12-10 05:59:15.359701: step 10320, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:32m:24s remains)
INFO - root - 2017-12-10 05:59:18.060138: step 10330, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:53m:24s remains)
INFO - root - 2017-12-10 05:59:20.762629: step 10340, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 23h:44m:50s remains)
INFO - root - 2017-12-10 05:59:23.455003: step 10350, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:47m:30s remains)
INFO - root - 2017-12-10 05:59:26.117649: step 10360, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:32m:36s remains)
INFO - root - 2017-12-10 05:59:28.764694: step 10370, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:48m:14s remains)
INFO - root - 2017-12-10 05:59:31.448435: step 10380, loss = 2.28, batch loss = 2.23 (27.3 examples/sec; 0.293 sec/batch; 26h:13m:19s remains)
INFO - root - 2017-12-10 05:59:34.073203: step 10390, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:53m:45s remains)
INFO - root - 2017-12-10 05:59:36.721929: step 10400, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:07m:26s remains)
2017-12-10 05:59:37.034590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289904 -4.4289794 -4.4289551 -4.4289145 -4.4288721 -4.4288435 -4.4288397 -4.4288688 -4.4289207 -4.4289675 -4.4289937 -4.4290009 -4.4289975 -4.4289908 -4.4289851][-4.4289865 -4.4289708 -4.4289384 -4.4288888 -4.4288416 -4.428812 -4.4288149 -4.4288554 -4.4289165 -4.428968 -4.4289966 -4.4290032 -4.4289975 -4.42899 -4.4289846][-4.4289827 -4.4289613 -4.4289193 -4.42886 -4.4288058 -4.4287782 -4.4287891 -4.4288406 -4.4289088 -4.4289641 -4.4289947 -4.4290032 -4.4289985 -4.4289908 -4.4289861][-4.42897 -4.4289403 -4.428884 -4.42881 -4.4287472 -4.4287248 -4.4287486 -4.4288139 -4.4288912 -4.4289546 -4.4289918 -4.4290042 -4.4290009 -4.4289937 -4.4289894][-4.428936 -4.428894 -4.4288182 -4.4287276 -4.4286594 -4.4286437 -4.4286795 -4.4287553 -4.4288392 -4.4289169 -4.4289722 -4.4289975 -4.4290023 -4.4289975 -4.4289947][-4.4288778 -4.4288163 -4.4287195 -4.4286175 -4.4285517 -4.4285436 -4.4285865 -4.428668 -4.4287586 -4.4288573 -4.4289412 -4.4289865 -4.4290004 -4.428998 -4.4289966][-4.4288082 -4.4287243 -4.4286108 -4.4285092 -4.4284606 -4.4284635 -4.4285088 -4.4285865 -4.4286723 -4.4287868 -4.4288988 -4.428967 -4.4289932 -4.4289937 -4.4289942][-4.4287586 -4.42866 -4.428544 -4.4284534 -4.4284205 -4.4284315 -4.4284725 -4.4285345 -4.428607 -4.4287267 -4.4288545 -4.4289412 -4.4289846 -4.4289947 -4.428998][-4.4287457 -4.4286523 -4.4285531 -4.42848 -4.4284534 -4.4284582 -4.428483 -4.4285264 -4.4285865 -4.4286995 -4.4288287 -4.4289255 -4.4289823 -4.4290023 -4.4290094][-4.4287438 -4.4286675 -4.428596 -4.4285436 -4.4285221 -4.42852 -4.4285321 -4.4285617 -4.4286094 -4.4287071 -4.4288259 -4.4289222 -4.4289865 -4.4290142 -4.4290223][-4.4287605 -4.4287057 -4.428659 -4.4286275 -4.4286137 -4.4286089 -4.4286137 -4.4286342 -4.4286709 -4.4287477 -4.4288449 -4.4289289 -4.42899 -4.429019 -4.4290261][-4.4287987 -4.4287663 -4.4287453 -4.4287372 -4.4287362 -4.4287353 -4.4287362 -4.4287462 -4.4287677 -4.42882 -4.42889 -4.4289527 -4.4289989 -4.42902 -4.4290214][-4.428844 -4.4288278 -4.4288263 -4.4288359 -4.4288459 -4.4288507 -4.4288507 -4.428854 -4.4288621 -4.4288926 -4.4289379 -4.4289804 -4.429009 -4.4290185 -4.4290133][-4.4288855 -4.428874 -4.4288783 -4.428894 -4.4289083 -4.4289169 -4.42892 -4.4289203 -4.4289203 -4.4289336 -4.4289622 -4.4289904 -4.4290061 -4.4290075 -4.429][-4.4289284 -4.4289179 -4.4289193 -4.4289293 -4.4289389 -4.428946 -4.4289503 -4.4289494 -4.4289455 -4.4289489 -4.4289651 -4.4289823 -4.4289908 -4.42899 -4.4289837]]...]
INFO - root - 2017-12-10 05:59:39.635827: step 10410, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:45m:05s remains)
INFO - root - 2017-12-10 05:59:42.268714: step 10420, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:47m:18s remains)
INFO - root - 2017-12-10 05:59:44.934157: step 10430, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:31m:06s remains)
INFO - root - 2017-12-10 05:59:47.612688: step 10440, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:04m:56s remains)
INFO - root - 2017-12-10 05:59:50.271405: step 10450, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:20m:41s remains)
INFO - root - 2017-12-10 05:59:52.910469: step 10460, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:09m:23s remains)
INFO - root - 2017-12-10 05:59:55.559273: step 10470, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:48m:13s remains)
INFO - root - 2017-12-10 05:59:58.209036: step 10480, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:05m:37s remains)
INFO - root - 2017-12-10 06:00:00.861670: step 10490, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:57m:03s remains)
INFO - root - 2017-12-10 06:00:03.517975: step 10500, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:35m:47s remains)
2017-12-10 06:00:03.827341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285665 -4.4285622 -4.4286036 -4.4286532 -4.4286718 -4.4286575 -4.428638 -4.4286332 -4.4286594 -4.4286971 -4.4287248 -4.4287343 -4.4287505 -4.42878 -4.4288144][-4.4285641 -4.4285526 -4.4285979 -4.4286489 -4.4286747 -4.4286761 -4.4286637 -4.42865 -4.4286757 -4.4287248 -4.4287581 -4.4287658 -4.4287691 -4.4287853 -4.4288177][-4.4285836 -4.4285636 -4.428607 -4.4286571 -4.4286833 -4.4286914 -4.4286852 -4.4286737 -4.4286942 -4.4287462 -4.4287844 -4.428793 -4.4287896 -4.4287996 -4.4288321][-4.4286094 -4.4285879 -4.4286232 -4.4286642 -4.4286761 -4.4286723 -4.4286656 -4.4286642 -4.4286942 -4.4287472 -4.4287825 -4.4287934 -4.4287934 -4.4288068 -4.4288411][-4.4286504 -4.4286327 -4.4286528 -4.42867 -4.428658 -4.4286356 -4.4286213 -4.4286294 -4.428679 -4.4287333 -4.4287677 -4.428781 -4.4287858 -4.4288063 -4.4288454][-4.4287043 -4.428699 -4.4287038 -4.4286947 -4.4286594 -4.4286208 -4.428596 -4.4286084 -4.4286666 -4.4287124 -4.42874 -4.4287581 -4.4287744 -4.4288044 -4.4288492][-4.4287572 -4.4287624 -4.4287558 -4.4287248 -4.4286733 -4.4286327 -4.4286084 -4.4286242 -4.4286695 -4.4286923 -4.4287086 -4.4287343 -4.4287648 -4.428802 -4.4288497][-4.4287987 -4.4288077 -4.4287944 -4.4287486 -4.4286933 -4.4286671 -4.4286566 -4.428669 -4.4286904 -4.4286871 -4.4286914 -4.4287205 -4.4287577 -4.4287972 -4.4288468][-4.4288311 -4.428843 -4.428823 -4.4287715 -4.4287233 -4.4287081 -4.4287105 -4.4287181 -4.4287257 -4.4287114 -4.4287047 -4.428719 -4.4287457 -4.4287848 -4.4288392][-4.4288392 -4.4288483 -4.4288311 -4.4287896 -4.4287624 -4.4287548 -4.4287562 -4.4287534 -4.4287505 -4.4287357 -4.4287162 -4.4287109 -4.4287291 -4.4287682 -4.4288273][-4.4288073 -4.4288116 -4.4288073 -4.4287944 -4.4287963 -4.4287987 -4.428793 -4.4287758 -4.4287596 -4.4287381 -4.4287076 -4.4286885 -4.4287028 -4.4287467 -4.4288116][-4.4287515 -4.4287519 -4.428761 -4.428781 -4.4288154 -4.4288349 -4.4288259 -4.4287977 -4.4287715 -4.4287391 -4.4286885 -4.4286532 -4.4286675 -4.4287224 -4.4287972][-4.428709 -4.4286923 -4.4286957 -4.4287381 -4.4288015 -4.428844 -4.4288397 -4.4288077 -4.4287767 -4.4287338 -4.4286666 -4.4286222 -4.4286447 -4.4287095 -4.42879][-4.4286866 -4.4286585 -4.4286494 -4.4286981 -4.4287734 -4.4288239 -4.4288216 -4.4287944 -4.4287696 -4.4287257 -4.4286628 -4.4286251 -4.4286537 -4.4287181 -4.4287953][-4.4286976 -4.4286737 -4.4286604 -4.4286976 -4.4287596 -4.428792 -4.4287925 -4.428782 -4.4287658 -4.428731 -4.4286847 -4.4286594 -4.4286833 -4.4287348 -4.4288011]]...]
INFO - root - 2017-12-10 06:00:06.552869: step 10510, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:51m:12s remains)
INFO - root - 2017-12-10 06:00:09.215844: step 10520, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 24h:58m:14s remains)
INFO - root - 2017-12-10 06:00:11.854570: step 10530, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:00m:49s remains)
INFO - root - 2017-12-10 06:00:14.518127: step 10540, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:11m:45s remains)
INFO - root - 2017-12-10 06:00:17.129879: step 10550, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:53m:03s remains)
INFO - root - 2017-12-10 06:00:19.798769: step 10560, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:52m:16s remains)
INFO - root - 2017-12-10 06:00:22.493049: step 10570, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.271 sec/batch; 24h:12m:15s remains)
INFO - root - 2017-12-10 06:00:25.171612: step 10580, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:28m:19s remains)
INFO - root - 2017-12-10 06:00:27.837923: step 10590, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:00m:43s remains)
INFO - root - 2017-12-10 06:00:30.539744: step 10600, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 25h:33m:56s remains)
2017-12-10 06:00:30.831522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289017 -4.4288692 -4.4287715 -4.428617 -4.4284945 -4.428525 -4.4286871 -4.4288015 -4.4288473 -4.428854 -4.4288163 -4.4287386 -4.4287071 -4.4287338 -4.428781][-4.4289 -4.4288712 -4.4287868 -4.428659 -4.4285488 -4.4285793 -4.4287186 -4.4288116 -4.428854 -4.428865 -4.4288282 -4.4287496 -4.4287238 -4.4287529 -4.4287825][-4.4288707 -4.4288521 -4.4287977 -4.4287052 -4.4286137 -4.428628 -4.4287224 -4.4287753 -4.4288077 -4.42885 -4.4288454 -4.4287896 -4.4287724 -4.4287968 -4.4288011][-4.4288154 -4.4288058 -4.4287868 -4.4287252 -4.4286337 -4.4286189 -4.4286575 -4.4286637 -4.4286895 -4.4287767 -4.4288173 -4.4288054 -4.4288116 -4.4288344 -4.4288163][-4.4287295 -4.4287448 -4.4287682 -4.4287333 -4.4286261 -4.4285464 -4.4285059 -4.4284482 -4.4284844 -4.4286275 -4.4287276 -4.4287744 -4.4288054 -4.4288344 -4.4288073][-4.4285855 -4.4286261 -4.4287004 -4.4286933 -4.4285755 -4.4284015 -4.42822 -4.4280596 -4.4281359 -4.4284015 -4.4285965 -4.4287095 -4.428761 -4.4287949 -4.42877][-4.4284177 -4.4284654 -4.428587 -4.4286261 -4.4285178 -4.4282613 -4.4278884 -4.4275727 -4.4276805 -4.4280925 -4.4284196 -4.4286265 -4.4287157 -4.4287615 -4.4287434][-4.4282966 -4.4283381 -4.4284744 -4.4285531 -4.4284906 -4.4282379 -4.427774 -4.4273334 -4.4273868 -4.4278479 -4.4282656 -4.4285493 -4.4286852 -4.4287553 -4.4287567][-4.4283152 -4.4283495 -4.4284577 -4.4285316 -4.4285121 -4.4283476 -4.427979 -4.4276013 -4.4275889 -4.42793 -4.4282794 -4.4285398 -4.4286761 -4.428761 -4.4287853][-4.4283977 -4.4284353 -4.4285126 -4.4285665 -4.4285765 -4.428503 -4.428288 -4.4280367 -4.4280047 -4.4282174 -4.4284415 -4.4286146 -4.42872 -4.4287882 -4.4288244][-4.428463 -4.4284821 -4.428514 -4.4285603 -4.4286103 -4.4286213 -4.4285326 -4.4283814 -4.4283462 -4.4284678 -4.4286027 -4.4287124 -4.4287891 -4.4288368 -4.4288735][-4.428504 -4.4284906 -4.4284692 -4.4284992 -4.4285865 -4.4286728 -4.4286861 -4.4286137 -4.4285874 -4.4286642 -4.4287577 -4.4288244 -4.4288635 -4.42889 -4.428915][-4.4285426 -4.428503 -4.4284263 -4.4284177 -4.4285197 -4.4286432 -4.4287171 -4.4287133 -4.4287262 -4.4287996 -4.4288721 -4.4289088 -4.4289203 -4.4289289 -4.4289336][-4.4286027 -4.4285593 -4.42848 -4.42845 -4.4285212 -4.4286327 -4.42871 -4.4287415 -4.4287872 -4.4288669 -4.4289222 -4.4289379 -4.4289451 -4.4289484 -4.4289417][-4.4286857 -4.42863 -4.4285688 -4.4285579 -4.428607 -4.4286757 -4.4287286 -4.4287643 -4.428822 -4.4289017 -4.428947 -4.42895 -4.4289536 -4.4289551 -4.4289451]]...]
INFO - root - 2017-12-10 06:00:33.478251: step 10610, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.258 sec/batch; 23h:06m:43s remains)
INFO - root - 2017-12-10 06:00:36.108641: step 10620, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:17m:51s remains)
INFO - root - 2017-12-10 06:00:38.764875: step 10630, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:07m:33s remains)
INFO - root - 2017-12-10 06:00:41.395659: step 10640, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 24h:55m:22s remains)
INFO - root - 2017-12-10 06:00:44.074856: step 10650, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:06m:28s remains)
INFO - root - 2017-12-10 06:00:46.750087: step 10660, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:06m:08s remains)
INFO - root - 2017-12-10 06:00:49.436943: step 10670, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:44m:28s remains)
INFO - root - 2017-12-10 06:00:52.085172: step 10680, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:22m:13s remains)
INFO - root - 2017-12-10 06:00:54.748464: step 10690, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:04m:33s remains)
INFO - root - 2017-12-10 06:00:57.377505: step 10700, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:17m:31s remains)
2017-12-10 06:00:57.684813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289522 -4.428947 -4.4289484 -4.4289422 -4.428926 -4.4288945 -4.428885 -4.4288554 -4.4288168 -4.4288197 -4.4288378 -4.428874 -4.4289174 -4.4289446 -4.4289293][-4.4289455 -4.428947 -4.428947 -4.4289227 -4.4288745 -4.4288149 -4.4287791 -4.4287205 -4.42867 -4.4286861 -4.4287276 -4.4287944 -4.4288688 -4.428905 -4.4288769][-4.42895 -4.4289708 -4.4289765 -4.4289341 -4.4288559 -4.42877 -4.4286962 -4.4286013 -4.4285374 -4.4285669 -4.4286432 -4.42875 -4.4288521 -4.4288793 -4.4288239][-4.4289474 -4.4289804 -4.4289837 -4.4289145 -4.4288 -4.4286866 -4.4285831 -4.4284835 -4.428441 -4.4284983 -4.4286175 -4.4287539 -4.42886 -4.4288554 -4.4287553][-4.4289455 -4.4289784 -4.428966 -4.4288659 -4.4287162 -4.4285617 -4.4284186 -4.4283247 -4.4283257 -4.428431 -4.4285908 -4.4287481 -4.4288373 -4.428791 -4.4286404][-4.4289546 -4.4289889 -4.4289618 -4.4288392 -4.4286618 -4.4284739 -4.428299 -4.4282079 -4.4282379 -4.428371 -4.4285393 -4.4287071 -4.428781 -4.428699 -4.4285054][-4.4289637 -4.429 -4.4289594 -4.4288206 -4.4286289 -4.4284277 -4.428237 -4.4281225 -4.4281454 -4.4282923 -4.4284687 -4.4286451 -4.4287205 -4.4286222 -4.4284034][-4.428967 -4.4290004 -4.4289489 -4.428803 -4.428607 -4.4284143 -4.4282212 -4.4280667 -4.4280496 -4.4281926 -4.4283743 -4.4285579 -4.4286475 -4.4285679 -4.4283571][-4.4289618 -4.4289865 -4.4289241 -4.4287758 -4.4285793 -4.428401 -4.4282293 -4.4280677 -4.42801 -4.4281259 -4.4283004 -4.4284773 -4.4285803 -4.4285364 -4.4283576][-4.4289474 -4.4289613 -4.4288921 -4.4287419 -4.4285517 -4.4283829 -4.4282322 -4.428093 -4.4280286 -4.4281139 -4.4282823 -4.4284406 -4.4285321 -4.4285154 -4.4283862][-4.4289265 -4.4289289 -4.428864 -4.4287257 -4.4285536 -4.4284015 -4.4282651 -4.4281492 -4.4280963 -4.4281535 -4.4283104 -4.4284563 -4.4285355 -4.4285254 -4.428432][-4.4289117 -4.4289031 -4.4288492 -4.4287348 -4.4285951 -4.4284754 -4.4283748 -4.428297 -4.4282713 -4.428309 -4.4284248 -4.4285359 -4.4285922 -4.4285836 -4.4285197][-4.4289122 -4.4289002 -4.42886 -4.428771 -4.4286647 -4.4285703 -4.4285121 -4.4284849 -4.4284978 -4.4285345 -4.4286118 -4.4286761 -4.4287009 -4.4286885 -4.4286561][-4.4289255 -4.4289131 -4.4288812 -4.4288158 -4.4287419 -4.4286766 -4.4286537 -4.4286618 -4.4286866 -4.4287128 -4.4287677 -4.4288077 -4.4288182 -4.4288082 -4.4287934][-4.4289474 -4.4289341 -4.428906 -4.4288611 -4.4288158 -4.4287758 -4.4287705 -4.4287887 -4.4288125 -4.4288268 -4.4288607 -4.428885 -4.4288878 -4.42888 -4.4288726]]...]
INFO - root - 2017-12-10 06:01:00.337773: step 10710, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:54m:31s remains)
INFO - root - 2017-12-10 06:01:03.007157: step 10720, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:12m:18s remains)
INFO - root - 2017-12-10 06:01:05.664968: step 10730, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:45m:58s remains)
INFO - root - 2017-12-10 06:01:08.340517: step 10740, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:52m:46s remains)
INFO - root - 2017-12-10 06:01:11.021578: step 10750, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:06m:22s remains)
INFO - root - 2017-12-10 06:01:13.653631: step 10760, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:37m:32s remains)
INFO - root - 2017-12-10 06:01:16.337794: step 10770, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:53m:59s remains)
INFO - root - 2017-12-10 06:01:18.989694: step 10780, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:58m:16s remains)
INFO - root - 2017-12-10 06:01:21.677089: step 10790, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 23h:32m:59s remains)
INFO - root - 2017-12-10 06:01:24.361423: step 10800, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 23h:54m:01s remains)
2017-12-10 06:01:24.653807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42899 -4.4289856 -4.4289775 -4.4289622 -4.4289422 -4.4289188 -4.42889 -4.4288669 -4.428853 -4.4288497 -4.428853 -4.4288578 -4.428844 -4.4288182 -4.4287853][-4.4289727 -4.4289632 -4.4289489 -4.4289241 -4.4288912 -4.4288597 -4.4288287 -4.4288049 -4.4287939 -4.4287977 -4.428812 -4.4288206 -4.4288063 -4.4287763 -4.42873][-4.4289479 -4.4289303 -4.4289079 -4.4288721 -4.4288273 -4.4287949 -4.428772 -4.4287515 -4.4287453 -4.4287577 -4.4287853 -4.428803 -4.4287868 -4.4287553 -4.4287057][-4.4289136 -4.4288936 -4.4288678 -4.4288244 -4.4287753 -4.4287481 -4.4287329 -4.42871 -4.4287038 -4.4287171 -4.4287519 -4.4287825 -4.42878 -4.4287672 -4.4287367][-4.4288888 -4.4288764 -4.42885 -4.4288063 -4.4287615 -4.4287291 -4.4286919 -4.4286456 -4.4286323 -4.4286475 -4.4286852 -4.4287291 -4.4287472 -4.4287591 -4.4287572][-4.42888 -4.4288778 -4.428853 -4.4288054 -4.4287515 -4.428699 -4.4286127 -4.4285154 -4.4284868 -4.4285254 -4.4285774 -4.4286294 -4.4286628 -4.4286985 -4.4287281][-4.4288912 -4.4288936 -4.4288616 -4.4287891 -4.4287081 -4.4286203 -4.428483 -4.4283266 -4.4282923 -4.4283905 -4.4284739 -4.4285283 -4.4285693 -4.4286289 -4.4286962][-4.4289002 -4.4289045 -4.4288664 -4.42877 -4.4286566 -4.4285393 -4.428391 -4.4282346 -4.4282222 -4.4283705 -4.4284644 -4.4285026 -4.428545 -4.4286289 -4.4287252][-4.4288826 -4.4288874 -4.4288549 -4.4287605 -4.4286423 -4.4285364 -4.4284539 -4.4283862 -4.4284005 -4.4285088 -4.4285636 -4.4285765 -4.4286184 -4.4287047 -4.428792][-4.428813 -4.4288139 -4.4288011 -4.4287467 -4.4286695 -4.4286108 -4.4285922 -4.4285893 -4.4286017 -4.4286566 -4.4286718 -4.4286709 -4.4287062 -4.4287758 -4.4288349][-4.4287062 -4.4287195 -4.4287462 -4.4287462 -4.4287195 -4.428699 -4.4287057 -4.4287224 -4.4287295 -4.4287477 -4.4287338 -4.4287205 -4.4287395 -4.4287872 -4.4288297][-4.4286175 -4.4286585 -4.4287224 -4.4287581 -4.4287577 -4.4287496 -4.4287524 -4.4287663 -4.4287696 -4.4287682 -4.4287443 -4.4287262 -4.4287286 -4.4287629 -4.4288025][-4.4285846 -4.4286547 -4.4287205 -4.4287558 -4.4287519 -4.4287357 -4.4287248 -4.4287338 -4.4287472 -4.4287581 -4.4287472 -4.4287381 -4.4287357 -4.4287634 -4.4288034][-4.4286108 -4.4286723 -4.4287171 -4.4287376 -4.428721 -4.4286895 -4.4286766 -4.428699 -4.4287367 -4.428771 -4.4287853 -4.428793 -4.4287982 -4.4288244 -4.4288592][-4.4286613 -4.428688 -4.4287124 -4.4287181 -4.4287019 -4.4286842 -4.4286904 -4.4287314 -4.4287848 -4.4288297 -4.4288526 -4.4288664 -4.4288759 -4.4288955 -4.4289174]]...]
INFO - root - 2017-12-10 06:01:27.278901: step 10810, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:48m:09s remains)
INFO - root - 2017-12-10 06:01:29.938654: step 10820, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:40m:30s remains)
INFO - root - 2017-12-10 06:01:32.620179: step 10830, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:11m:23s remains)
INFO - root - 2017-12-10 06:01:35.237553: step 10840, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:44m:02s remains)
INFO - root - 2017-12-10 06:01:37.896617: step 10850, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 25h:25m:38s remains)
INFO - root - 2017-12-10 06:01:40.527714: step 10860, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:37m:09s remains)
INFO - root - 2017-12-10 06:01:43.204561: step 10870, loss = 2.28, batch loss = 2.23 (27.2 examples/sec; 0.294 sec/batch; 26h:18m:30s remains)
INFO - root - 2017-12-10 06:01:45.846507: step 10880, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:51m:43s remains)
INFO - root - 2017-12-10 06:01:48.521463: step 10890, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:59m:05s remains)
INFO - root - 2017-12-10 06:01:51.157991: step 10900, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 23h:51m:52s remains)
2017-12-10 06:01:51.446821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290085 -4.4289856 -4.42897 -4.4289603 -4.4289584 -4.4289532 -4.4289474 -4.4289489 -4.4289575 -4.4289637 -4.4289694 -4.4289746 -4.4289823 -4.4289989 -4.42902][-4.4290037 -4.4289746 -4.4289603 -4.42895 -4.4289408 -4.4289246 -4.4289126 -4.4289103 -4.428925 -4.4289355 -4.4289436 -4.42895 -4.4289613 -4.4289827 -4.4290118][-4.4289956 -4.4289613 -4.4289427 -4.428925 -4.4288974 -4.4288597 -4.4288378 -4.4288287 -4.4288507 -4.4288688 -4.4288836 -4.4288983 -4.4289188 -4.4289455 -4.4289861][-4.4289818 -4.4289441 -4.4289184 -4.4288878 -4.4288373 -4.4287663 -4.4287205 -4.4287014 -4.4287419 -4.4287786 -4.4288077 -4.4288321 -4.42886 -4.4288955 -4.4289484][-4.4289603 -4.4289203 -4.4288821 -4.4288316 -4.42875 -4.4286385 -4.4285483 -4.428514 -4.4285917 -4.4286737 -4.4287286 -4.4287591 -4.4287891 -4.4288349 -4.4288983][-4.42893 -4.4288783 -4.4288144 -4.42873 -4.4286075 -4.428443 -4.4282832 -4.4282336 -4.4283848 -4.4285488 -4.4286489 -4.4286919 -4.4287333 -4.4287868 -4.4288516][-4.428894 -4.4288254 -4.4287333 -4.4286122 -4.4284539 -4.428236 -4.4280005 -4.427916 -4.4281373 -4.4284005 -4.4285645 -4.4286451 -4.4287128 -4.4287782 -4.4288383][-4.4288707 -4.4287882 -4.42867 -4.4285278 -4.4283576 -4.4281344 -4.4278789 -4.4277782 -4.4280105 -4.4283152 -4.4285212 -4.4286327 -4.4287291 -4.4288077 -4.4288607][-4.4288712 -4.4287767 -4.4286356 -4.4284806 -4.4283223 -4.4281378 -4.4279456 -4.427887 -4.4280753 -4.4283395 -4.4285417 -4.4286618 -4.4287624 -4.4288406 -4.4288821][-4.4288731 -4.4287667 -4.4286151 -4.428453 -4.4282994 -4.4281454 -4.4280181 -4.428009 -4.4281626 -4.4283814 -4.4285712 -4.4286938 -4.4287877 -4.4288568 -4.4288936][-4.4288797 -4.4287658 -4.4286137 -4.4284506 -4.4282923 -4.42815 -4.42806 -4.4280849 -4.428226 -4.4284167 -4.4285913 -4.4287143 -4.4288073 -4.4288712 -4.4289055][-4.4289088 -4.4288058 -4.4286714 -4.4285207 -4.4283719 -4.428247 -4.4281836 -4.4282241 -4.4283514 -4.4285054 -4.4286532 -4.4287634 -4.4288487 -4.428906 -4.428936][-4.4289446 -4.4288707 -4.4287763 -4.4286623 -4.4285493 -4.4284625 -4.4284296 -4.4284744 -4.4285707 -4.4286728 -4.4287734 -4.4288483 -4.4289064 -4.4289489 -4.4289694][-4.4289742 -4.4289284 -4.4288745 -4.4288082 -4.4287457 -4.4287057 -4.4286923 -4.4287238 -4.4287796 -4.4288325 -4.4288864 -4.428926 -4.428957 -4.4289808 -4.4289932][-4.4289913 -4.4289646 -4.4289355 -4.4289045 -4.4288793 -4.4288697 -4.4288673 -4.428885 -4.42891 -4.4289327 -4.4289575 -4.4289742 -4.428988 -4.4289985 -4.4290051]]...]
INFO - root - 2017-12-10 06:01:54.063328: step 10910, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:16m:22s remains)
INFO - root - 2017-12-10 06:01:56.724933: step 10920, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:09m:17s remains)
INFO - root - 2017-12-10 06:01:59.374961: step 10930, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 22h:55m:31s remains)
INFO - root - 2017-12-10 06:02:02.014341: step 10940, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:03m:11s remains)
INFO - root - 2017-12-10 06:02:04.681375: step 10950, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:33m:43s remains)
INFO - root - 2017-12-10 06:02:07.362975: step 10960, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:50m:05s remains)
INFO - root - 2017-12-10 06:02:10.005761: step 10970, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:31m:45s remains)
INFO - root - 2017-12-10 06:02:12.663103: step 10980, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:11m:10s remains)
INFO - root - 2017-12-10 06:02:15.328204: step 10990, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:00m:56s remains)
INFO - root - 2017-12-10 06:02:17.924084: step 11000, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:44m:29s remains)
2017-12-10 06:02:18.246808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288034 -4.4288321 -4.4288516 -4.4288726 -4.4288797 -4.4288573 -4.4288225 -4.428802 -4.428791 -4.4287834 -4.4287748 -4.4287653 -4.4287562 -4.4287591 -4.4287763][-4.4287586 -4.42879 -4.4288158 -4.4288349 -4.4288449 -4.4288316 -4.4288034 -4.4287858 -4.4287734 -4.4287562 -4.4287376 -4.4287248 -4.4287262 -4.4287386 -4.4287548][-4.4287291 -4.4287562 -4.4287853 -4.4288034 -4.4288135 -4.4288039 -4.4287796 -4.4287696 -4.4287653 -4.428751 -4.4287276 -4.4287119 -4.4287186 -4.4287386 -4.4287539][-4.4287052 -4.4287248 -4.4287519 -4.4287696 -4.4287739 -4.4287572 -4.4287248 -4.4287248 -4.4287467 -4.4287648 -4.428761 -4.4287462 -4.4287429 -4.4287553 -4.4287629][-4.4286757 -4.4286866 -4.4287057 -4.4287181 -4.4287081 -4.4286628 -4.42861 -4.4286246 -4.4287004 -4.428771 -4.4288015 -4.4287987 -4.4287858 -4.4287815 -4.4287772][-4.4286504 -4.4286561 -4.4286647 -4.428668 -4.4286323 -4.4285312 -4.4284267 -4.428462 -4.4286132 -4.4287448 -4.4288087 -4.4288197 -4.428812 -4.428803 -4.4287925][-4.4286528 -4.4286594 -4.4286642 -4.4286571 -4.428597 -4.4284358 -4.4282451 -4.4282718 -4.4284906 -4.4286761 -4.4287658 -4.4287896 -4.4287958 -4.4287968 -4.4287915][-4.4286776 -4.4286828 -4.4286995 -4.4287009 -4.42865 -4.4284968 -4.4282856 -4.4282665 -4.4284506 -4.4286265 -4.4287143 -4.4287348 -4.4287386 -4.4287405 -4.4287472][-4.4287143 -4.4287128 -4.4287448 -4.4287615 -4.4287281 -4.4286227 -4.4284739 -4.4284377 -4.4285345 -4.4286475 -4.4287 -4.4286942 -4.4286733 -4.4286575 -4.4286594][-4.4287267 -4.4287181 -4.4287534 -4.4287872 -4.4287734 -4.4287052 -4.4286127 -4.4285789 -4.4286218 -4.4286919 -4.4287219 -4.4287043 -4.4286642 -4.4286256 -4.4286046][-4.4287114 -4.4286971 -4.4287295 -4.4287825 -4.4287934 -4.4287472 -4.428678 -4.4286375 -4.4286423 -4.4286938 -4.4287271 -4.4287295 -4.4287009 -4.428658 -4.4286194][-4.4286985 -4.4286833 -4.4287076 -4.4287653 -4.428782 -4.4287434 -4.4286842 -4.4286385 -4.4286184 -4.4286628 -4.4287095 -4.4287333 -4.4287224 -4.4286866 -4.4286537][-4.4286995 -4.428689 -4.4287076 -4.4287605 -4.428781 -4.4287457 -4.4286995 -4.4286523 -4.4286213 -4.4286518 -4.4286971 -4.4287276 -4.4287329 -4.4287195 -4.4286952][-4.4286757 -4.428669 -4.4286871 -4.4287491 -4.4287829 -4.4287529 -4.4287143 -4.4286733 -4.4286432 -4.4286618 -4.4286904 -4.4287114 -4.4287252 -4.4287267 -4.4287057][-4.4286232 -4.4286275 -4.4286633 -4.4287391 -4.4287872 -4.4287682 -4.4287372 -4.4287 -4.4286718 -4.428679 -4.4286885 -4.4286952 -4.4287052 -4.4287004 -4.4286695]]...]
INFO - root - 2017-12-10 06:02:20.868867: step 11010, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:05m:44s remains)
INFO - root - 2017-12-10 06:02:23.524700: step 11020, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:22m:54s remains)
INFO - root - 2017-12-10 06:02:26.114784: step 11030, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:19m:10s remains)
INFO - root - 2017-12-10 06:02:28.771792: step 11040, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.275 sec/batch; 24h:36m:01s remains)
INFO - root - 2017-12-10 06:02:31.487210: step 11050, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 24h:41m:24s remains)
INFO - root - 2017-12-10 06:02:34.177147: step 11060, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:54m:09s remains)
INFO - root - 2017-12-10 06:02:36.772427: step 11070, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 23h:52m:02s remains)
INFO - root - 2017-12-10 06:02:39.421624: step 11080, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:23m:53s remains)
INFO - root - 2017-12-10 06:02:42.068016: step 11090, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:34m:23s remains)
INFO - root - 2017-12-10 06:02:44.700864: step 11100, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:39m:01s remains)
2017-12-10 06:02:45.004300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287834 -4.4287777 -4.4287748 -4.4287686 -4.4287639 -4.4287605 -4.4287567 -4.4287562 -4.4287653 -4.428782 -4.4287944 -4.4287963 -4.4287996 -4.4288139 -4.4288325][-4.4287958 -4.4287939 -4.4287934 -4.4287868 -4.4287758 -4.4287643 -4.4287534 -4.4287486 -4.4287624 -4.4287915 -4.4288149 -4.4288216 -4.4288239 -4.4288383 -4.4288549][-4.4288311 -4.4288287 -4.4288263 -4.4288139 -4.4287863 -4.4287553 -4.4287343 -4.4287305 -4.4287539 -4.4287987 -4.4288354 -4.4288511 -4.4288535 -4.4288659 -4.4288812][-4.428854 -4.4288464 -4.428833 -4.4288 -4.4287443 -4.4286833 -4.428647 -4.4286513 -4.428699 -4.4287715 -4.4288306 -4.4288616 -4.4288683 -4.4288764 -4.4288888][-4.4288473 -4.428833 -4.4288034 -4.4287415 -4.428647 -4.4285445 -4.4284854 -4.4285054 -4.4285908 -4.4287047 -4.4287992 -4.4288507 -4.4288692 -4.428875 -4.4288774][-4.42884 -4.4288173 -4.42877 -4.4286742 -4.428534 -4.4283776 -4.4282918 -4.4283309 -4.4284577 -4.4286113 -4.4287381 -4.4288135 -4.4288492 -4.4288559 -4.4288459][-4.4288392 -4.428813 -4.4287443 -4.42861 -4.4284239 -4.4282188 -4.428112 -4.4281697 -4.4283342 -4.4285169 -4.4286642 -4.4287615 -4.428812 -4.4288216 -4.4288006][-4.4288578 -4.4288354 -4.4287539 -4.4286079 -4.4284115 -4.4281945 -4.4280758 -4.4281259 -4.4282913 -4.4284768 -4.4286327 -4.42874 -4.4287858 -4.428781 -4.428741][-4.4289117 -4.4288797 -4.4287872 -4.4286475 -4.428483 -4.428309 -4.4282069 -4.4282322 -4.4283633 -4.4285226 -4.4286637 -4.4287553 -4.4287696 -4.4287324 -4.4286695][-4.4289432 -4.4288921 -4.428792 -4.428668 -4.42855 -4.4284515 -4.4284005 -4.4284225 -4.4285135 -4.4286222 -4.4287167 -4.4287715 -4.4287491 -4.4286857 -4.4286122][-4.4289222 -4.42886 -4.4287663 -4.4286628 -4.4285851 -4.42855 -4.4285531 -4.42859 -4.4286523 -4.4287143 -4.4287591 -4.42877 -4.4287248 -4.4286542 -4.4285913][-4.4288654 -4.4288025 -4.4287233 -4.42864 -4.4285951 -4.4286065 -4.428658 -4.4287152 -4.4287581 -4.4287839 -4.4287887 -4.4287667 -4.4287095 -4.4286447 -4.4286094][-4.4288282 -4.4287724 -4.4287047 -4.4286346 -4.4286079 -4.4286394 -4.4287052 -4.4287686 -4.4288106 -4.4288259 -4.4288154 -4.4287763 -4.428719 -4.42867 -4.4286613][-4.4288325 -4.4287853 -4.4287252 -4.4286585 -4.4286332 -4.4286571 -4.4287148 -4.4287791 -4.4288297 -4.4288473 -4.4288378 -4.4288015 -4.4287491 -4.4287114 -4.428719][-4.4288616 -4.42882 -4.4287677 -4.4287076 -4.428678 -4.4286861 -4.4287295 -4.4287906 -4.4288435 -4.4288621 -4.4288487 -4.4288106 -4.4287615 -4.4287319 -4.4287534]]...]
INFO - root - 2017-12-10 06:02:47.679142: step 11110, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 25h:32m:53s remains)
INFO - root - 2017-12-10 06:02:50.327999: step 11120, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:49m:48s remains)
INFO - root - 2017-12-10 06:02:52.967706: step 11130, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:22m:27s remains)
INFO - root - 2017-12-10 06:02:55.598412: step 11140, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:28m:10s remains)
INFO - root - 2017-12-10 06:02:58.253264: step 11150, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:27m:41s remains)
INFO - root - 2017-12-10 06:03:00.891138: step 11160, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:31m:32s remains)
INFO - root - 2017-12-10 06:03:03.522718: step 11170, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.271 sec/batch; 24h:08m:41s remains)
INFO - root - 2017-12-10 06:03:06.124304: step 11180, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:47m:54s remains)
INFO - root - 2017-12-10 06:03:08.820243: step 11190, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:49m:34s remains)
INFO - root - 2017-12-10 06:03:11.428853: step 11200, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:29m:48s remains)
2017-12-10 06:03:11.758969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288907 -4.4289083 -4.428916 -4.4289169 -4.4289131 -4.4289141 -4.4289207 -4.4289303 -4.4289312 -4.4289165 -4.42889 -4.4288678 -4.4288611 -4.4288754 -4.4288993][-4.4288559 -4.4288688 -4.4288654 -4.4288516 -4.4288406 -4.4288473 -4.4288673 -4.4288917 -4.428906 -4.4289 -4.4288778 -4.4288573 -4.4288535 -4.4288697 -4.4288931][-4.4288325 -4.4288268 -4.4287992 -4.4287629 -4.4287429 -4.4287543 -4.4287939 -4.4288406 -4.4288726 -4.4288793 -4.4288712 -4.4288635 -4.4288669 -4.4288826 -4.4289007][-4.4288249 -4.4287996 -4.4287434 -4.4286766 -4.428637 -4.4286427 -4.4286957 -4.4287696 -4.4288311 -4.42886 -4.4288731 -4.4288788 -4.4288898 -4.4289064 -4.4289179][-4.42885 -4.4288187 -4.4287343 -4.4286251 -4.4285407 -4.42851 -4.4285569 -4.4286618 -4.428771 -4.4288383 -4.428874 -4.4288921 -4.42891 -4.4289274 -4.428937][-4.4288917 -4.4288626 -4.4287639 -4.4286141 -4.428472 -4.4283733 -4.4283714 -4.4284987 -4.428669 -4.4287896 -4.4288583 -4.4288936 -4.4289188 -4.4289436 -4.4289541][-4.4289231 -4.4289079 -4.4288168 -4.42865 -4.4284616 -4.4282775 -4.4281754 -4.4282966 -4.4285197 -4.4286957 -4.4288054 -4.4288659 -4.4289007 -4.4289317 -4.4289451][-4.4289417 -4.4289446 -4.4288788 -4.4287276 -4.428535 -4.4283271 -4.42816 -4.428226 -4.4284387 -4.428628 -4.4287539 -4.4288244 -4.4288611 -4.4288926 -4.4289141][-4.4289527 -4.4289713 -4.4289346 -4.4288216 -4.4286685 -4.4285078 -4.4283676 -4.4283762 -4.4285026 -4.4286366 -4.42873 -4.4287682 -4.4287739 -4.4288006 -4.4288507][-4.4289589 -4.4289942 -4.4289842 -4.4289079 -4.4287987 -4.4286942 -4.4286017 -4.4285874 -4.4286366 -4.4286904 -4.4287109 -4.4286709 -4.4286051 -4.428618 -4.4287186][-4.4289589 -4.4290032 -4.4290109 -4.4289613 -4.4288859 -4.4288192 -4.4287658 -4.4287524 -4.428761 -4.4287543 -4.4286981 -4.4285626 -4.4283881 -4.4283695 -4.4285297][-4.4289546 -4.4290004 -4.42902 -4.4289923 -4.4289403 -4.4288983 -4.4288697 -4.4288635 -4.4288616 -4.4288268 -4.4287276 -4.4285479 -4.4283152 -4.4282446 -4.4284105][-4.4289522 -4.429 -4.4290271 -4.4290175 -4.4289846 -4.4289579 -4.4289408 -4.4289346 -4.4289303 -4.4288955 -4.4287987 -4.4286466 -4.4284534 -4.4283652 -4.4284554][-4.4289522 -4.4290009 -4.4290309 -4.4290314 -4.4290109 -4.4289923 -4.4289827 -4.4289756 -4.4289684 -4.4289408 -4.4288697 -4.4287663 -4.4286447 -4.4285755 -4.4285975][-4.4289508 -4.429 -4.4290318 -4.4290385 -4.4290276 -4.4290152 -4.4290104 -4.4290047 -4.4289989 -4.4289742 -4.4289165 -4.4288478 -4.4287806 -4.4287419 -4.4287343]]...]
INFO - root - 2017-12-10 06:03:14.415464: step 11210, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 25h:29m:29s remains)
INFO - root - 2017-12-10 06:03:17.049010: step 11220, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:49m:29s remains)
INFO - root - 2017-12-10 06:03:19.653487: step 11230, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:49m:54s remains)
INFO - root - 2017-12-10 06:03:22.322846: step 11240, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:44m:50s remains)
INFO - root - 2017-12-10 06:03:24.961332: step 11250, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:26m:34s remains)
INFO - root - 2017-12-10 06:03:27.666939: step 11260, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:05m:08s remains)
INFO - root - 2017-12-10 06:03:30.306381: step 11270, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:39m:08s remains)
INFO - root - 2017-12-10 06:03:32.973067: step 11280, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 23h:50m:43s remains)
INFO - root - 2017-12-10 06:03:35.654277: step 11290, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:25m:01s remains)
INFO - root - 2017-12-10 06:03:38.274665: step 11300, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:37m:55s remains)
2017-12-10 06:03:38.580018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287934 -4.428803 -4.4288049 -4.4288063 -4.4287958 -4.4287739 -4.4287519 -4.4287372 -4.4287224 -4.4286914 -4.4286485 -4.4286 -4.4285736 -4.4286146 -4.4286795][-4.428803 -4.4288006 -4.4287906 -4.4287848 -4.428772 -4.4287491 -4.4287305 -4.4287291 -4.4287186 -4.4286838 -4.4286323 -4.4285674 -4.428534 -4.428586 -4.4286551][-4.4287543 -4.4287481 -4.4287386 -4.4287329 -4.4287286 -4.4287162 -4.4287157 -4.428731 -4.4287324 -4.4287028 -4.4286475 -4.4285674 -4.4285355 -4.4286 -4.428669][-4.4286709 -4.42867 -4.4286742 -4.4286804 -4.4286857 -4.4286833 -4.4286985 -4.4287238 -4.4287367 -4.4287205 -4.4286776 -4.4286017 -4.4285817 -4.4286494 -4.4287095][-4.428617 -4.4286308 -4.4286451 -4.4286518 -4.4286575 -4.4286556 -4.4286718 -4.42869 -4.4287033 -4.4287014 -4.4286838 -4.4286385 -4.4286427 -4.4287148 -4.4287658][-4.4286027 -4.4286242 -4.428638 -4.4286404 -4.4286361 -4.4286208 -4.4286218 -4.4286265 -4.4286442 -4.4286609 -4.428668 -4.4286466 -4.4286709 -4.4287419 -4.4287896][-4.4286041 -4.428628 -4.4286532 -4.4286566 -4.4286356 -4.4285979 -4.4285765 -4.4285655 -4.4285808 -4.4286056 -4.4286137 -4.4285903 -4.42862 -4.4286995 -4.4287581][-4.4286165 -4.4286561 -4.4286933 -4.428689 -4.4286509 -4.4285908 -4.4285426 -4.4285192 -4.4285293 -4.4285383 -4.4285159 -4.4284573 -4.4284849 -4.4285893 -4.4286766][-4.4286556 -4.4287086 -4.4287429 -4.4287281 -4.4286776 -4.4286084 -4.4285488 -4.4285235 -4.42853 -4.4285192 -4.4284754 -4.4283862 -4.4283991 -4.428514 -4.428607][-4.4287043 -4.4287505 -4.4287744 -4.4287562 -4.4287086 -4.42865 -4.4285975 -4.428576 -4.4285822 -4.428575 -4.4285474 -4.4284739 -4.4284887 -4.4285831 -4.42866][-4.4287186 -4.4287496 -4.4287724 -4.4287648 -4.4287324 -4.42869 -4.4286537 -4.4286451 -4.4286604 -4.4286604 -4.4286408 -4.428586 -4.4286065 -4.4286814 -4.4287376][-4.4286823 -4.4287171 -4.4287567 -4.4287658 -4.4287529 -4.4287348 -4.4287138 -4.4287071 -4.4287162 -4.4287148 -4.4286985 -4.4286551 -4.4286723 -4.4287338 -4.4287796][-4.4286389 -4.4286814 -4.4287391 -4.4287658 -4.4287753 -4.4287891 -4.4287796 -4.4287586 -4.4287457 -4.4287338 -4.4287214 -4.4286861 -4.4286981 -4.4287462 -4.4287872][-4.4286132 -4.4286637 -4.4287276 -4.42876 -4.4287868 -4.4288249 -4.4288225 -4.4287887 -4.42876 -4.4287472 -4.4287357 -4.4286995 -4.4287014 -4.4287376 -4.428771][-4.4286084 -4.4286556 -4.4287176 -4.4287586 -4.4287982 -4.4288392 -4.4288383 -4.428802 -4.4287767 -4.4287825 -4.428782 -4.4287472 -4.4287348 -4.4287515 -4.428762]]...]
INFO - root - 2017-12-10 06:03:41.174118: step 11310, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:49m:50s remains)
INFO - root - 2017-12-10 06:03:43.860507: step 11320, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:51m:58s remains)
INFO - root - 2017-12-10 06:03:46.474709: step 11330, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:22m:56s remains)
INFO - root - 2017-12-10 06:03:49.082486: step 11340, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:41m:44s remains)
INFO - root - 2017-12-10 06:03:51.752095: step 11350, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:48m:02s remains)
INFO - root - 2017-12-10 06:03:54.424203: step 11360, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:09m:44s remains)
INFO - root - 2017-12-10 06:03:57.089001: step 11370, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:27m:51s remains)
INFO - root - 2017-12-10 06:03:59.785874: step 11380, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:05m:20s remains)
INFO - root - 2017-12-10 06:04:02.456037: step 11390, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:50m:05s remains)
INFO - root - 2017-12-10 06:04:05.076099: step 11400, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 22h:58m:43s remains)
2017-12-10 06:04:05.356292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428936 -4.4289188 -4.4289083 -4.4289079 -4.428915 -4.4289217 -4.4289184 -4.42891 -4.4289041 -4.4288979 -4.4289 -4.4289188 -4.4289389 -4.4289603 -4.4289765][-4.4288921 -4.4288611 -4.4288363 -4.4288321 -4.4288421 -4.4288535 -4.4288521 -4.4288473 -4.4288468 -4.4288421 -4.4288445 -4.4288726 -4.428905 -4.4289403 -4.4289665][-4.4288535 -4.4288092 -4.4287686 -4.4287505 -4.4287572 -4.4287691 -4.4287682 -4.42877 -4.4287815 -4.4287887 -4.4287958 -4.4288254 -4.4288654 -4.428915 -4.4289522][-4.4288077 -4.4287519 -4.4286981 -4.4286637 -4.4286575 -4.4286609 -4.4286475 -4.4286528 -4.428688 -4.4287257 -4.4287477 -4.4287744 -4.4288068 -4.4288635 -4.4289122][-4.4287486 -4.4286866 -4.42862 -4.4285607 -4.42852 -4.4284868 -4.4284425 -4.4284482 -4.4285283 -4.4286218 -4.4286766 -4.4287028 -4.4287252 -4.4287815 -4.4288464][-4.4287143 -4.428648 -4.4285665 -4.4284744 -4.4283719 -4.4282608 -4.4281397 -4.4281249 -4.4282756 -4.4284568 -4.4285636 -4.4286075 -4.4286413 -4.4287105 -4.42879][-4.4286985 -4.4286346 -4.4285593 -4.4284606 -4.4283066 -4.4280891 -4.4278312 -4.42775 -4.4279914 -4.4282875 -4.4284754 -4.4285617 -4.428618 -4.4286947 -4.4287753][-4.4286861 -4.4286284 -4.4285865 -4.4285269 -4.4283829 -4.4281373 -4.4278097 -4.4276624 -4.4279103 -4.4282446 -4.428473 -4.4285927 -4.4286547 -4.4287238 -4.4287939][-4.4286761 -4.4286242 -4.4286203 -4.4286246 -4.4285564 -4.4283891 -4.4281392 -4.4280019 -4.4281435 -4.4283772 -4.4285617 -4.4286709 -4.4287171 -4.4287639 -4.42882][-4.4286523 -4.4286 -4.4286184 -4.4286771 -4.4286938 -4.4286213 -4.4284697 -4.4283686 -4.4284306 -4.4285607 -4.4286761 -4.4287481 -4.4287705 -4.4287949 -4.4288397][-4.4286394 -4.4285793 -4.4286051 -4.4286947 -4.4287724 -4.4287663 -4.4286871 -4.42863 -4.4286556 -4.4287205 -4.4287839 -4.4288139 -4.4288158 -4.4288254 -4.42886][-4.4286828 -4.4286222 -4.4286351 -4.4287181 -4.4288149 -4.4288397 -4.4288011 -4.4287777 -4.4287896 -4.4288197 -4.428853 -4.4288592 -4.4288535 -4.4288573 -4.4288855][-4.4287529 -4.4287019 -4.4287028 -4.4287581 -4.428834 -4.4288607 -4.4288459 -4.428844 -4.4288583 -4.4288774 -4.4288974 -4.428895 -4.4288859 -4.4288883 -4.4289136][-4.428823 -4.4287825 -4.4287806 -4.4288116 -4.428853 -4.4288721 -4.4288697 -4.4288764 -4.4288888 -4.4289026 -4.4289193 -4.4289203 -4.4289141 -4.4289174 -4.42894][-4.4289131 -4.4288855 -4.4288812 -4.4288936 -4.42891 -4.4289207 -4.428926 -4.4289331 -4.4289408 -4.4289494 -4.4289589 -4.4289603 -4.4289551 -4.4289575 -4.4289727]]...]
INFO - root - 2017-12-10 06:04:08.016166: step 11410, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:05m:39s remains)
INFO - root - 2017-12-10 06:04:10.647047: step 11420, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:22m:50s remains)
INFO - root - 2017-12-10 06:04:13.328674: step 11430, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:56m:07s remains)
INFO - root - 2017-12-10 06:04:15.990721: step 11440, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:11m:09s remains)
INFO - root - 2017-12-10 06:04:18.650170: step 11450, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:09m:31s remains)
INFO - root - 2017-12-10 06:04:21.244806: step 11460, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:24m:49s remains)
INFO - root - 2017-12-10 06:04:23.885137: step 11470, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:05m:22s remains)
INFO - root - 2017-12-10 06:04:26.516497: step 11480, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:25m:42s remains)
INFO - root - 2017-12-10 06:04:29.155133: step 11490, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:48m:28s remains)
INFO - root - 2017-12-10 06:04:31.836380: step 11500, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:23m:09s remains)
2017-12-10 06:04:32.132380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290047 -4.4289966 -4.4289804 -4.42896 -4.4289393 -4.4289193 -4.4289088 -4.4289083 -4.4289093 -4.4289017 -4.4288936 -4.4289 -4.4289212 -4.428947 -4.4289665][-4.4289865 -4.4289703 -4.428947 -4.4289184 -4.428894 -4.4288712 -4.4288568 -4.42886 -4.4288707 -4.4288721 -4.4288707 -4.4288831 -4.4289103 -4.42894 -4.4289632][-4.4289737 -4.4289479 -4.428916 -4.428884 -4.4288573 -4.4288311 -4.4288111 -4.4288073 -4.4288211 -4.4288325 -4.4288521 -4.4288812 -4.4289184 -4.4289489 -4.428966][-4.4289665 -4.4289355 -4.4288969 -4.428864 -4.4288321 -4.4287972 -4.4287682 -4.4287515 -4.4287643 -4.4287882 -4.4288311 -4.4288807 -4.4289241 -4.4289522 -4.4289632][-4.4289613 -4.4289308 -4.4288898 -4.4288511 -4.4288073 -4.4287567 -4.4287095 -4.4286847 -4.4287019 -4.4287448 -4.4288092 -4.4288726 -4.4289179 -4.4289412 -4.4289484][-4.4289489 -4.4289155 -4.4288688 -4.4288163 -4.4287548 -4.4286776 -4.4286056 -4.4285793 -4.4286213 -4.4286957 -4.4287829 -4.428853 -4.4288936 -4.4289107 -4.4289141][-4.4289279 -4.4288859 -4.4288254 -4.428751 -4.4286618 -4.428544 -4.4284182 -4.4283805 -4.428473 -4.4285994 -4.4287229 -4.4288077 -4.428854 -4.4288716 -4.4288726][-4.4288955 -4.4288344 -4.4287486 -4.4286461 -4.4285245 -4.4283595 -4.4281707 -4.4281044 -4.4282541 -4.4284511 -4.4286146 -4.4287195 -4.4287844 -4.4288168 -4.4288249][-4.4288521 -4.428772 -4.4286642 -4.4285431 -4.4284139 -4.4282422 -4.4280457 -4.4279823 -4.4281592 -4.4283772 -4.4285488 -4.428658 -4.4287353 -4.4287872 -4.4288149][-4.428833 -4.4287491 -4.4286456 -4.4285345 -4.428431 -4.4283166 -4.4282074 -4.4281869 -4.4282975 -4.4284439 -4.4285755 -4.4286714 -4.4287467 -4.428812 -4.4288588][-4.4288626 -4.4287891 -4.4286985 -4.4286127 -4.4285479 -4.4284911 -4.4284539 -4.4284663 -4.4285216 -4.4285989 -4.4286852 -4.4287567 -4.4288225 -4.4288836 -4.4289265][-4.4289103 -4.4288487 -4.4287734 -4.4287095 -4.4286728 -4.4286537 -4.4286575 -4.4286866 -4.4287171 -4.4287562 -4.4288063 -4.4288497 -4.4289 -4.4289474 -4.4289765][-4.4289389 -4.428894 -4.4288373 -4.4287925 -4.4287758 -4.4287753 -4.4287972 -4.428833 -4.42886 -4.4288836 -4.4289083 -4.4289293 -4.4289579 -4.4289865 -4.4290028][-4.4289589 -4.4289341 -4.4289036 -4.4288807 -4.4288783 -4.4288883 -4.4289117 -4.4289412 -4.4289613 -4.4289722 -4.4289794 -4.4289837 -4.4289927 -4.4290037 -4.429009][-4.4289765 -4.4289684 -4.4289579 -4.4289494 -4.4289517 -4.4289622 -4.4289789 -4.4289966 -4.4290047 -4.4290047 -4.4290018 -4.4289985 -4.4289994 -4.4290037 -4.4290047]]...]
INFO - root - 2017-12-10 06:04:34.781678: step 11510, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:41m:15s remains)
INFO - root - 2017-12-10 06:04:37.444144: step 11520, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:28m:47s remains)
INFO - root - 2017-12-10 06:04:40.044730: step 11530, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 22h:29m:43s remains)
INFO - root - 2017-12-10 06:04:42.694433: step 11540, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:03m:11s remains)
INFO - root - 2017-12-10 06:04:45.334165: step 11550, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:33m:01s remains)
INFO - root - 2017-12-10 06:04:47.978376: step 11560, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:14m:27s remains)
INFO - root - 2017-12-10 06:04:50.634704: step 11570, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:51m:41s remains)
INFO - root - 2017-12-10 06:04:53.313999: step 11580, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:16m:35s remains)
INFO - root - 2017-12-10 06:04:55.981952: step 11590, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:55m:09s remains)
INFO - root - 2017-12-10 06:04:58.644907: step 11600, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 22h:52m:00s remains)
2017-12-10 06:04:58.945395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289989 -4.428977 -4.4289584 -4.4289489 -4.4289513 -4.4289603 -4.4289713 -4.4289742 -4.4289784 -4.4289885 -4.4289918 -4.428987 -4.428978 -4.4289708 -4.4289713][-4.4289365 -4.4289093 -4.4288898 -4.4288788 -4.4288754 -4.4288816 -4.428896 -4.4288969 -4.4288983 -4.4289126 -4.4289227 -4.4289222 -4.4289169 -4.4289136 -4.428916][-4.428864 -4.4288254 -4.4287992 -4.4287891 -4.4287815 -4.4287848 -4.4288054 -4.4288096 -4.4288106 -4.428833 -4.4288516 -4.4288554 -4.4288526 -4.428853 -4.4288616][-4.42879 -4.42873 -4.4286895 -4.4286819 -4.4286852 -4.4286938 -4.4287181 -4.4287252 -4.4287233 -4.4287534 -4.4287806 -4.428792 -4.4287939 -4.4287949 -4.4288054][-4.428721 -4.4286432 -4.4285908 -4.4285836 -4.428597 -4.42861 -4.4286294 -4.4286346 -4.4286284 -4.428668 -4.4287086 -4.4287324 -4.4287438 -4.4287448 -4.4287577][-4.4286609 -4.4285784 -4.4285254 -4.4285169 -4.4285264 -4.4285288 -4.428524 -4.428504 -4.4284887 -4.4285445 -4.4286108 -4.4286571 -4.4286895 -4.4287081 -4.4287367][-4.428628 -4.4285469 -4.4284997 -4.4284854 -4.4284768 -4.4284511 -4.4284 -4.4283352 -4.428308 -4.4283915 -4.428494 -4.4285741 -4.4286437 -4.4286923 -4.4287391][-4.4286385 -4.4285641 -4.4285259 -4.4285116 -4.4284925 -4.4284382 -4.42834 -4.4282217 -4.4281845 -4.4282956 -4.4284248 -4.4285283 -4.4286232 -4.4286923 -4.4287548][-4.428679 -4.4286218 -4.4286022 -4.4285975 -4.4285769 -4.4285121 -4.4284072 -4.4282823 -4.4282479 -4.4283452 -4.4284611 -4.428556 -4.428648 -4.428719 -4.4287806][-4.4287195 -4.4286809 -4.4286776 -4.4286857 -4.4286709 -4.4286122 -4.428524 -4.4284248 -4.4284043 -4.4284711 -4.4285517 -4.4286237 -4.4286985 -4.428761 -4.4288139][-4.4287553 -4.4287286 -4.4287281 -4.4287391 -4.4287281 -4.4286838 -4.4286189 -4.4285469 -4.4285288 -4.4285703 -4.4286289 -4.4286833 -4.4287391 -4.42879 -4.4288368][-4.4288111 -4.4287882 -4.428782 -4.4287848 -4.4287734 -4.4287462 -4.4287066 -4.4286566 -4.4286394 -4.4286637 -4.4287071 -4.4287467 -4.428782 -4.4288158 -4.4288588][-4.4288912 -4.4288726 -4.4288645 -4.4288616 -4.4288521 -4.4288383 -4.4288187 -4.4287858 -4.4287705 -4.4287829 -4.4288082 -4.42883 -4.4288473 -4.4288673 -4.4289017][-4.42897 -4.4289565 -4.4289489 -4.4289455 -4.42894 -4.428937 -4.4289308 -4.4289093 -4.4288955 -4.4289017 -4.4289165 -4.4289236 -4.4289279 -4.4289389 -4.4289608][-4.4290242 -4.4290152 -4.4290085 -4.4290066 -4.4290066 -4.4290085 -4.4290085 -4.428998 -4.4289894 -4.4289927 -4.4289985 -4.428998 -4.428998 -4.4290023 -4.4290142]]...]
INFO - root - 2017-12-10 06:05:01.597828: step 11610, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:37m:50s remains)
INFO - root - 2017-12-10 06:05:04.280096: step 11620, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:21m:40s remains)
INFO - root - 2017-12-10 06:05:06.943383: step 11630, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 24h:51m:41s remains)
INFO - root - 2017-12-10 06:05:09.624843: step 11640, loss = 2.28, batch loss = 2.23 (27.8 examples/sec; 0.288 sec/batch; 25h:38m:05s remains)
INFO - root - 2017-12-10 06:05:12.294510: step 11650, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:35m:25s remains)
INFO - root - 2017-12-10 06:05:14.933633: step 11660, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:02m:17s remains)
INFO - root - 2017-12-10 06:05:17.542154: step 11670, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:02m:41s remains)
INFO - root - 2017-12-10 06:05:20.170138: step 11680, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 23h:49m:02s remains)
INFO - root - 2017-12-10 06:05:22.831883: step 11690, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:50m:25s remains)
INFO - root - 2017-12-10 06:05:25.443110: step 11700, loss = 2.28, batch loss = 2.23 (28.5 examples/sec; 0.281 sec/batch; 25h:00m:08s remains)
2017-12-10 06:05:25.735986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286633 -4.4286284 -4.4286346 -4.4286647 -4.4286876 -4.4286852 -4.4286528 -4.4286294 -4.4286408 -4.4286761 -4.428709 -4.4286747 -4.428628 -4.4286056 -4.4286146][-4.4286604 -4.4286194 -4.4286265 -4.42866 -4.4286866 -4.4286885 -4.4286594 -4.4286447 -4.4286561 -4.4286833 -4.4287162 -4.4286952 -4.4286585 -4.4286313 -4.4286246][-4.4286747 -4.4286313 -4.428638 -4.4286685 -4.4286861 -4.428678 -4.42865 -4.428647 -4.4286714 -4.4286971 -4.4287252 -4.4287114 -4.42869 -4.42867 -4.4286528][-4.4286814 -4.4286394 -4.4286408 -4.4286594 -4.4286594 -4.42863 -4.4285955 -4.428606 -4.4286494 -4.4286852 -4.428709 -4.4286971 -4.4286904 -4.42868 -4.4286656][-4.4286442 -4.4286003 -4.4286022 -4.4286175 -4.4286013 -4.4285383 -4.4284778 -4.4284945 -4.4285736 -4.4286408 -4.4286714 -4.4286604 -4.4286623 -4.4286609 -4.4286542][-4.4285855 -4.428535 -4.4285336 -4.428546 -4.4285078 -4.4283972 -4.4282875 -4.4283152 -4.4284506 -4.4285679 -4.4286242 -4.4286218 -4.4286313 -4.4286323 -4.4286289][-4.4286089 -4.4285579 -4.4285522 -4.4285526 -4.4284883 -4.4283357 -4.4281869 -4.4282169 -4.4283843 -4.4285283 -4.4286051 -4.4286146 -4.4286218 -4.4286137 -4.4286008][-4.4286847 -4.428648 -4.4286513 -4.4286575 -4.4286022 -4.4284735 -4.4283586 -4.428381 -4.4285121 -4.4286213 -4.4286709 -4.4286723 -4.4286637 -4.4286451 -4.42862][-4.4287577 -4.4287424 -4.4287591 -4.428772 -4.4287381 -4.4286494 -4.4285746 -4.428587 -4.4286704 -4.4287343 -4.4287548 -4.4287486 -4.4287329 -4.4287152 -4.4287004][-4.4288082 -4.4288116 -4.4288359 -4.4288507 -4.4288311 -4.4287715 -4.4287181 -4.4287124 -4.4287472 -4.4287744 -4.4287777 -4.4287696 -4.4287605 -4.4287581 -4.4287705][-4.4288316 -4.428843 -4.428865 -4.4288788 -4.4288645 -4.4288206 -4.428781 -4.4287663 -4.4287653 -4.4287667 -4.4287643 -4.4287548 -4.4287515 -4.4287624 -4.4287977][-4.4288759 -4.4288783 -4.4288855 -4.4288907 -4.4288659 -4.4288182 -4.4287758 -4.4287581 -4.4287462 -4.4287457 -4.428751 -4.4287481 -4.4287481 -4.428762 -4.4287992][-4.4288936 -4.4288731 -4.42886 -4.4288597 -4.4288292 -4.4287767 -4.4287324 -4.4287176 -4.4287095 -4.4287205 -4.4287381 -4.4287448 -4.4287519 -4.428762 -4.4287887][-4.4288664 -4.4288287 -4.428802 -4.4288025 -4.4287872 -4.4287457 -4.4287124 -4.4286976 -4.4286804 -4.4286957 -4.4287271 -4.4287434 -4.4287491 -4.4287486 -4.4287529][-4.4288197 -4.4287815 -4.4287615 -4.428772 -4.428782 -4.4287639 -4.4287457 -4.4287248 -4.4286914 -4.4286938 -4.4287167 -4.4287243 -4.4287138 -4.4286976 -4.4286814]]...]
INFO - root - 2017-12-10 06:05:28.380329: step 11710, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:55m:45s remains)
INFO - root - 2017-12-10 06:05:31.068501: step 11720, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:24m:47s remains)
INFO - root - 2017-12-10 06:05:33.736048: step 11730, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:29m:06s remains)
INFO - root - 2017-12-10 06:05:36.385460: step 11740, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:38m:20s remains)
INFO - root - 2017-12-10 06:05:39.083981: step 11750, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:11m:07s remains)
INFO - root - 2017-12-10 06:05:41.697471: step 11760, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:39m:20s remains)
INFO - root - 2017-12-10 06:05:44.370794: step 11770, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:53m:34s remains)
INFO - root - 2017-12-10 06:05:46.996988: step 11780, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:29m:53s remains)
INFO - root - 2017-12-10 06:05:49.595139: step 11790, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:53m:42s remains)
INFO - root - 2017-12-10 06:05:52.274734: step 11800, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:32m:27s remains)
2017-12-10 06:05:52.575909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288416 -4.4288397 -4.4288282 -4.4288015 -4.4287925 -4.4287953 -4.42878 -4.4287434 -4.4287267 -4.4287105 -4.4287157 -4.4287086 -4.4287095 -4.4287391 -4.42878][-4.4288263 -4.4288116 -4.4287877 -4.4287429 -4.4287214 -4.4287372 -4.4287558 -4.4287524 -4.4287481 -4.4287329 -4.4287314 -4.4287171 -4.4287162 -4.4287381 -4.4287772][-4.4287972 -4.42877 -4.4287405 -4.4286895 -4.4286647 -4.4286962 -4.428741 -4.4287567 -4.428762 -4.4287415 -4.4287133 -4.4286981 -4.4287047 -4.4287276 -4.4287663][-4.4287581 -4.4287267 -4.4286952 -4.4286547 -4.42863 -4.4286609 -4.42871 -4.4287281 -4.4287419 -4.4287271 -4.4286966 -4.4286876 -4.4286962 -4.4287128 -4.4287486][-4.428721 -4.4286909 -4.4286609 -4.4286356 -4.4286184 -4.4286504 -4.4286933 -4.4287076 -4.4287143 -4.4287143 -4.4287081 -4.428699 -4.4287028 -4.4287181 -4.4287496][-4.4287043 -4.4286695 -4.4286413 -4.4286294 -4.4286232 -4.4286413 -4.428658 -4.4286633 -4.4286647 -4.4286885 -4.4287152 -4.4287095 -4.4287205 -4.428741 -4.4287672][-4.4286652 -4.4286203 -4.4285946 -4.4285946 -4.428597 -4.4285951 -4.4285836 -4.4285617 -4.4285541 -4.428607 -4.428668 -4.4286733 -4.4286952 -4.4287305 -4.4287691][-4.4286094 -4.4285512 -4.4285278 -4.4285312 -4.42853 -4.42851 -4.428462 -4.4284015 -4.4283919 -4.4284816 -4.4285884 -4.42863 -4.4286742 -4.4287286 -4.4287777][-4.4286094 -4.4285526 -4.4285336 -4.428535 -4.4285245 -4.4284887 -4.4284263 -4.4283614 -4.4283624 -4.4284697 -4.4285989 -4.4286613 -4.4287062 -4.4287596 -4.4288054][-4.4286962 -4.4286427 -4.4286275 -4.4286342 -4.4286251 -4.4285913 -4.42854 -4.4284911 -4.4284887 -4.4285731 -4.4286852 -4.42874 -4.4287682 -4.4288068 -4.428844][-4.428782 -4.4287353 -4.4287305 -4.4287529 -4.4287548 -4.4287291 -4.42869 -4.4286456 -4.428638 -4.4286957 -4.42878 -4.42882 -4.4288306 -4.4288507 -4.4288735][-4.4288411 -4.4288034 -4.4288054 -4.4288354 -4.4288449 -4.4288254 -4.4287906 -4.4287515 -4.4287472 -4.4287853 -4.4288349 -4.4288549 -4.4288526 -4.4288635 -4.4288793][-4.4288669 -4.4288435 -4.4288521 -4.4288783 -4.4288907 -4.4288731 -4.4288406 -4.428803 -4.4287987 -4.4288177 -4.4288397 -4.4288473 -4.4288445 -4.4288583 -4.428874][-4.4288764 -4.4288764 -4.428885 -4.4289017 -4.4289021 -4.4288788 -4.4288468 -4.4288054 -4.4287996 -4.4288187 -4.4288378 -4.4288478 -4.4288478 -4.4288645 -4.4288812][-4.4289002 -4.4289136 -4.4289179 -4.4289203 -4.4289145 -4.4288979 -4.4288673 -4.4288306 -4.4288273 -4.4288454 -4.4288597 -4.4288621 -4.4288578 -4.42887 -4.4288845]]...]
INFO - root - 2017-12-10 06:05:55.247588: step 11810, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:54m:15s remains)
INFO - root - 2017-12-10 06:05:57.882622: step 11820, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:03m:16s remains)
INFO - root - 2017-12-10 06:06:00.522126: step 11830, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:19m:49s remains)
INFO - root - 2017-12-10 06:06:03.176552: step 11840, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:05m:19s remains)
INFO - root - 2017-12-10 06:06:05.816144: step 11850, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:55m:49s remains)
INFO - root - 2017-12-10 06:06:08.486162: step 11860, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 24h:34m:18s remains)
INFO - root - 2017-12-10 06:06:11.129426: step 11870, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:57m:56s remains)
INFO - root - 2017-12-10 06:06:13.799481: step 11880, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:03m:49s remains)
INFO - root - 2017-12-10 06:06:16.441494: step 11890, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:15m:41s remains)
INFO - root - 2017-12-10 06:06:19.137902: step 11900, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.290 sec/batch; 25h:51m:49s remains)
2017-12-10 06:06:19.470656: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288864 -4.4288635 -4.4288349 -4.4288273 -4.4288526 -4.4288745 -4.4288712 -4.4288535 -4.4288092 -4.4287415 -4.4286861 -4.4286909 -4.4287634 -4.4288449 -4.4289007][-4.4288564 -4.428843 -4.4288192 -4.428813 -4.42883 -4.4288435 -4.4288287 -4.4287987 -4.4287558 -4.428699 -4.4286571 -4.4286628 -4.4287291 -4.4288163 -4.4288788][-4.4288292 -4.4288244 -4.4288087 -4.4288077 -4.42882 -4.4288254 -4.4287996 -4.4287577 -4.4287014 -4.4286518 -4.4286323 -4.428648 -4.4287124 -4.4287992 -4.4288597][-4.4288111 -4.4288106 -4.4287987 -4.4288015 -4.4288096 -4.42881 -4.4287724 -4.4287162 -4.4286375 -4.4285874 -4.4285936 -4.428627 -4.42869 -4.42878 -4.4288378][-4.4287996 -4.4287958 -4.4287767 -4.4287705 -4.4287672 -4.4287581 -4.4287057 -4.4286222 -4.42852 -4.4284844 -4.42853 -4.4285936 -4.4286671 -4.4287648 -4.4288192][-4.4287915 -4.428782 -4.4287596 -4.4287462 -4.4287229 -4.4286919 -4.4286036 -4.4284754 -4.4283533 -4.4283628 -4.4284706 -4.4285645 -4.4286537 -4.4287543 -4.4288044][-4.428793 -4.428792 -4.4287834 -4.4287629 -4.4287128 -4.42864 -4.4284964 -4.4283061 -4.4281516 -4.4282184 -4.4284043 -4.4285359 -4.4286432 -4.4287434 -4.4287963][-4.428791 -4.4288054 -4.4288182 -4.428793 -4.4287267 -4.4286275 -4.4284616 -4.428247 -4.4280853 -4.4281917 -4.4284105 -4.4285445 -4.4286413 -4.4287305 -4.42879][-4.4288082 -4.4288368 -4.4288635 -4.428834 -4.42876 -4.4286613 -4.42852 -4.4283376 -4.4282002 -4.4283042 -4.4284863 -4.4285827 -4.4286504 -4.4287271 -4.4287944][-4.4288449 -4.4288807 -4.4289074 -4.4288793 -4.4288116 -4.4287333 -4.4286389 -4.4285088 -4.4283934 -4.4284606 -4.4285793 -4.4286394 -4.4286828 -4.4287486 -4.4288177][-4.4288812 -4.4289117 -4.4289265 -4.4288983 -4.4288497 -4.4288096 -4.4287667 -4.4286819 -4.4285922 -4.4286294 -4.4286995 -4.4287362 -4.4287605 -4.4288054 -4.4288616][-4.4289179 -4.42893 -4.4289241 -4.4288983 -4.4288721 -4.428865 -4.4288516 -4.4287896 -4.4287128 -4.4287386 -4.428791 -4.4288216 -4.4288359 -4.4288678 -4.4289064][-4.4289546 -4.4289455 -4.4289203 -4.4288979 -4.4288917 -4.4289017 -4.4288983 -4.4288478 -4.4287848 -4.4288006 -4.4288425 -4.4288712 -4.4288831 -4.4289107 -4.4289341][-4.4289722 -4.4289522 -4.4289212 -4.4289031 -4.4289069 -4.4289246 -4.42893 -4.4288979 -4.4288545 -4.4288554 -4.4288754 -4.4288921 -4.428906 -4.4289317 -4.4289455][-4.4289761 -4.4289594 -4.4289341 -4.4289212 -4.4289246 -4.4289408 -4.4289484 -4.4289317 -4.4289055 -4.4288921 -4.4288855 -4.4288869 -4.4289017 -4.4289308 -4.4289436]]...]
INFO - root - 2017-12-10 06:06:22.141773: step 11910, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:58m:07s remains)
INFO - root - 2017-12-10 06:06:24.774047: step 11920, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:17m:31s remains)
INFO - root - 2017-12-10 06:06:27.430570: step 11930, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:38m:08s remains)
INFO - root - 2017-12-10 06:06:30.065287: step 11940, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:19m:07s remains)
INFO - root - 2017-12-10 06:06:32.710371: step 11950, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:59m:37s remains)
INFO - root - 2017-12-10 06:06:35.321684: step 11960, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:50m:08s remains)
INFO - root - 2017-12-10 06:06:37.955780: step 11970, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:39m:09s remains)
INFO - root - 2017-12-10 06:06:40.628178: step 11980, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:42m:32s remains)
INFO - root - 2017-12-10 06:06:43.259183: step 11990, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:08m:21s remains)
INFO - root - 2017-12-10 06:06:45.894000: step 12000, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:07m:16s remains)
2017-12-10 06:06:46.203607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287906 -4.4287786 -4.4287629 -4.4287696 -4.4287958 -4.4288259 -4.4288335 -4.428812 -4.4287963 -4.4287996 -4.4288106 -4.4288106 -4.4288034 -4.4287863 -4.4287629][-4.4287806 -4.4287605 -4.4287386 -4.428751 -4.4287863 -4.42882 -4.4288235 -4.4287939 -4.428772 -4.4287796 -4.4287972 -4.4288006 -4.4287872 -4.4287591 -4.4287262][-4.42878 -4.4287562 -4.4287295 -4.4287429 -4.428782 -4.4288154 -4.4288073 -4.4287658 -4.4287381 -4.4287467 -4.428771 -4.4287848 -4.42877 -4.4287291 -4.4286895][-4.4287815 -4.4287596 -4.4287376 -4.4287496 -4.4287829 -4.428802 -4.4287758 -4.428719 -4.4286833 -4.4286928 -4.4287243 -4.4287462 -4.4287329 -4.4286914 -4.4286628][-4.4287672 -4.4287486 -4.4287391 -4.4287524 -4.42877 -4.4287605 -4.4286971 -4.4286189 -4.4285822 -4.4286089 -4.4286609 -4.4286985 -4.4286976 -4.4286709 -4.4286609][-4.4287572 -4.4287381 -4.4287367 -4.4287496 -4.4287429 -4.4286895 -4.4285645 -4.4284468 -4.428432 -4.4285135 -4.4286127 -4.428679 -4.4286971 -4.4286852 -4.4286809][-4.4287744 -4.4287534 -4.4287438 -4.428741 -4.4286985 -4.4285879 -4.4283919 -4.4282303 -4.4282713 -4.428442 -4.4285975 -4.428688 -4.4287176 -4.428709 -4.4287033][-4.4288063 -4.428781 -4.4287534 -4.4287205 -4.4286404 -4.4284887 -4.4282637 -4.4281034 -4.428216 -4.4284492 -4.428627 -4.4287148 -4.4287386 -4.4287219 -4.4287043][-4.4288321 -4.4288039 -4.4287572 -4.4286933 -4.4285932 -4.4284496 -4.4282804 -4.4282012 -4.4283319 -4.4285455 -4.4286957 -4.42876 -4.4287639 -4.4287271 -4.4286962][-4.42882 -4.4287882 -4.4287324 -4.4286585 -4.4285741 -4.4284921 -4.428422 -4.4284234 -4.42854 -4.4286933 -4.428793 -4.4288158 -4.4287848 -4.4287276 -4.4286866][-4.428761 -4.4287281 -4.4286842 -4.4286385 -4.428606 -4.4285879 -4.4285893 -4.4286346 -4.4287305 -4.4288249 -4.4288716 -4.4288583 -4.4288049 -4.42874 -4.4286942][-4.4286809 -4.42866 -4.4286504 -4.4286547 -4.428669 -4.4286871 -4.4287243 -4.428781 -4.4288497 -4.4288926 -4.4288974 -4.4288611 -4.4288044 -4.4287462 -4.4287052][-4.4286375 -4.4286485 -4.4286752 -4.428709 -4.4287376 -4.4287648 -4.4288049 -4.4288492 -4.4288821 -4.4288816 -4.4288554 -4.4288149 -4.4287777 -4.4287443 -4.4287167][-4.4286637 -4.4287047 -4.428751 -4.4287825 -4.4287996 -4.4288125 -4.428833 -4.4288526 -4.4288526 -4.4288182 -4.4287772 -4.4287543 -4.4287457 -4.42874 -4.4287305][-4.4287386 -4.4287868 -4.4288297 -4.4288397 -4.4288363 -4.4288259 -4.4288206 -4.4288216 -4.4288049 -4.4287543 -4.4287124 -4.4287047 -4.4287181 -4.4287333 -4.4287405]]...]
INFO - root - 2017-12-10 06:06:48.793172: step 12010, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:06m:16s remains)
INFO - root - 2017-12-10 06:06:51.478562: step 12020, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:59m:21s remains)
INFO - root - 2017-12-10 06:06:54.154665: step 12030, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 22h:28m:04s remains)
INFO - root - 2017-12-10 06:06:56.805635: step 12040, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:16m:54s remains)
INFO - root - 2017-12-10 06:06:59.485487: step 12050, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:13m:13s remains)
INFO - root - 2017-12-10 06:07:02.158197: step 12060, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:42m:22s remains)
INFO - root - 2017-12-10 06:07:04.876781: step 12070, loss = 2.28, batch loss = 2.23 (27.3 examples/sec; 0.294 sec/batch; 26h:07m:45s remains)
INFO - root - 2017-12-10 06:07:07.552819: step 12080, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 24h:40m:34s remains)
INFO - root - 2017-12-10 06:07:10.194600: step 12090, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:43m:02s remains)
INFO - root - 2017-12-10 06:07:12.832628: step 12100, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:33m:11s remains)
2017-12-10 06:07:13.131181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288387 -4.4288373 -4.4288187 -4.4287777 -4.4287429 -4.4287267 -4.4287562 -4.428822 -4.4288778 -4.42891 -4.4289284 -4.4289317 -4.4289274 -4.4289241 -4.4289227][-4.4288282 -4.4288321 -4.4288106 -4.4287629 -4.4287214 -4.4287066 -4.4287467 -4.4288239 -4.42888 -4.4289007 -4.4289107 -4.4289131 -4.4289136 -4.4289169 -4.4289184][-4.4288263 -4.428834 -4.4288149 -4.4287715 -4.4287248 -4.4287052 -4.4287515 -4.4288373 -4.4289002 -4.4289165 -4.4289165 -4.4289055 -4.4288974 -4.428896 -4.4288969][-4.42884 -4.428853 -4.4288449 -4.4288082 -4.4287515 -4.4287181 -4.4287519 -4.4288306 -4.4288993 -4.4289217 -4.42892 -4.4288988 -4.42888 -4.4288716 -4.4288745][-4.4288697 -4.4288816 -4.428885 -4.4288564 -4.4287829 -4.4287143 -4.4287057 -4.4287648 -4.4288435 -4.4288864 -4.4288945 -4.428874 -4.4288559 -4.4288507 -4.4288549][-4.4289103 -4.42892 -4.4289241 -4.4288917 -4.4287982 -4.4286766 -4.4286 -4.4286256 -4.4287286 -4.428813 -4.4288473 -4.4288425 -4.4288359 -4.4288392 -4.4288459][-4.4289436 -4.4289536 -4.4289608 -4.4289217 -4.4288015 -4.4286289 -4.4284697 -4.428443 -4.42857 -4.42871 -4.4287949 -4.4288225 -4.4288325 -4.42884 -4.4288468][-4.4289589 -4.4289675 -4.4289751 -4.4289484 -4.4288411 -4.4286556 -4.4284368 -4.4283271 -4.4284325 -4.4285965 -4.4287219 -4.4287944 -4.428834 -4.4288478 -4.4288521][-4.4289675 -4.4289761 -4.4289827 -4.4289746 -4.4289103 -4.4287696 -4.4285746 -4.4284205 -4.4284239 -4.4285216 -4.4286351 -4.4287329 -4.4288011 -4.4288335 -4.4288507][-4.4289613 -4.4289761 -4.428988 -4.4289913 -4.4289622 -4.4288793 -4.4287457 -4.4286122 -4.428556 -4.4285712 -4.4286218 -4.4286942 -4.428761 -4.4288063 -4.42884][-4.4289384 -4.4289594 -4.4289808 -4.4289889 -4.4289804 -4.4289346 -4.4288492 -4.4287577 -4.4287014 -4.4286871 -4.428699 -4.4287305 -4.4287634 -4.4287968 -4.4288287][-4.428906 -4.4289341 -4.4289584 -4.4289665 -4.4289627 -4.4289346 -4.4288721 -4.4288073 -4.4287686 -4.4287658 -4.4287834 -4.4288039 -4.4288135 -4.428822 -4.4288368][-4.4288926 -4.42892 -4.4289374 -4.428937 -4.428925 -4.4289 -4.4288454 -4.4287872 -4.4287572 -4.4287672 -4.4288044 -4.4288397 -4.4288526 -4.4288554 -4.42886][-4.4289002 -4.4289169 -4.4289236 -4.428916 -4.4288917 -4.4288611 -4.4288096 -4.4287491 -4.4287162 -4.4287324 -4.428781 -4.4288297 -4.428853 -4.4288669 -4.4288793][-4.4289169 -4.42892 -4.4289174 -4.4289026 -4.4288735 -4.4288383 -4.4287906 -4.4287343 -4.4287057 -4.4287238 -4.4287663 -4.4288111 -4.4288311 -4.4288459 -4.4288697]]...]
INFO - root - 2017-12-10 06:07:15.800741: step 12110, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:32m:47s remains)
INFO - root - 2017-12-10 06:07:18.443851: step 12120, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:22m:06s remains)
INFO - root - 2017-12-10 06:07:21.096867: step 12130, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:02m:18s remains)
INFO - root - 2017-12-10 06:07:23.801843: step 12140, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:56m:05s remains)
INFO - root - 2017-12-10 06:07:26.402532: step 12150, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:09m:55s remains)
INFO - root - 2017-12-10 06:07:29.068777: step 12160, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:48m:56s remains)
INFO - root - 2017-12-10 06:07:31.728821: step 12170, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:57m:39s remains)
INFO - root - 2017-12-10 06:07:34.350782: step 12180, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 23h:27m:05s remains)
INFO - root - 2017-12-10 06:07:36.992665: step 12190, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:17m:50s remains)
INFO - root - 2017-12-10 06:07:39.616738: step 12200, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:08m:25s remains)
2017-12-10 06:07:39.946217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287724 -4.4287581 -4.428719 -4.4286609 -4.4286137 -4.42867 -4.4287472 -4.4287839 -4.4288077 -4.4288116 -4.4288125 -4.42882 -4.4288597 -4.428895 -4.4289188][-4.4287653 -4.4287486 -4.4287057 -4.4286594 -4.4286189 -4.4286647 -4.4287329 -4.4287624 -4.4287796 -4.428771 -4.4287596 -4.4287643 -4.4288044 -4.4288435 -4.4288692][-4.4287639 -4.4287462 -4.4287095 -4.4286814 -4.4286537 -4.4286833 -4.4287319 -4.4287519 -4.4287615 -4.4287438 -4.428721 -4.4287219 -4.4287577 -4.4287953 -4.4288168][-4.4287596 -4.4287429 -4.4287128 -4.4286952 -4.4286857 -4.4287038 -4.4287319 -4.4287405 -4.4287481 -4.4287319 -4.4287081 -4.4287009 -4.4287248 -4.4287562 -4.4287748][-4.4287472 -4.4287314 -4.4287043 -4.4286938 -4.4287014 -4.4287114 -4.4287224 -4.4287248 -4.4287357 -4.428731 -4.4287138 -4.4287014 -4.4287143 -4.428741 -4.4287596][-4.428741 -4.4287319 -4.4287052 -4.4286923 -4.4287019 -4.4286995 -4.4286923 -4.4286952 -4.4287124 -4.4287271 -4.4287243 -4.4287128 -4.4287152 -4.4287438 -4.4287677][-4.4287491 -4.4287462 -4.4287162 -4.4286861 -4.42868 -4.428659 -4.4286313 -4.4286427 -4.4286714 -4.4287071 -4.4287305 -4.4287286 -4.428731 -4.4287615 -4.4287891][-4.4287591 -4.4287658 -4.4287381 -4.4286928 -4.4286675 -4.4286208 -4.4285669 -4.4285927 -4.4286346 -4.4286857 -4.4287353 -4.4287534 -4.4287658 -4.4287963 -4.4288235][-4.4287581 -4.4287815 -4.4287677 -4.428719 -4.4286852 -4.4286246 -4.4285579 -4.4285927 -4.4286451 -4.4287009 -4.4287605 -4.4287915 -4.4288087 -4.428834 -4.4288583][-4.4287448 -4.4287767 -4.4287829 -4.4287477 -4.4287219 -4.4286742 -4.4286184 -4.4286542 -4.4287009 -4.4287467 -4.4287972 -4.4288273 -4.4288383 -4.4288554 -4.4288788][-4.4287329 -4.428762 -4.428781 -4.4287663 -4.4287586 -4.4287353 -4.4287009 -4.42873 -4.428761 -4.428792 -4.4288263 -4.4288492 -4.4288568 -4.4288678 -4.4288874][-4.4287481 -4.4287715 -4.4287896 -4.4287882 -4.4287963 -4.42879 -4.4287696 -4.4287896 -4.4288073 -4.4288292 -4.428853 -4.4288726 -4.4288812 -4.42889 -4.4289007][-4.4287992 -4.4288177 -4.4288268 -4.4288297 -4.4288435 -4.4288464 -4.428833 -4.4288449 -4.428853 -4.4288664 -4.428884 -4.4289036 -4.428916 -4.4289227 -4.4289241][-4.4288607 -4.428875 -4.4288759 -4.4288778 -4.4288926 -4.4289021 -4.428894 -4.4289007 -4.4289017 -4.4289079 -4.4289203 -4.4289355 -4.4289465 -4.4289465 -4.4289384][-4.4288993 -4.4289131 -4.4289145 -4.428916 -4.4289265 -4.4289351 -4.42893 -4.4289327 -4.4289336 -4.4289389 -4.428947 -4.4289584 -4.428966 -4.4289608 -4.4289441]]...]
INFO - root - 2017-12-10 06:07:42.587724: step 12210, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:35m:56s remains)
INFO - root - 2017-12-10 06:07:45.250312: step 12220, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:30m:39s remains)
INFO - root - 2017-12-10 06:07:47.926758: step 12230, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:52m:18s remains)
INFO - root - 2017-12-10 06:07:50.607869: step 12240, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.282 sec/batch; 25h:03m:22s remains)
INFO - root - 2017-12-10 06:07:53.325541: step 12250, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:23m:03s remains)
INFO - root - 2017-12-10 06:07:56.003833: step 12260, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:56m:27s remains)
INFO - root - 2017-12-10 06:07:58.603503: step 12270, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:16m:20s remains)
INFO - root - 2017-12-10 06:08:01.245517: step 12280, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:37m:58s remains)
INFO - root - 2017-12-10 06:08:03.943202: step 12290, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:11m:12s remains)
INFO - root - 2017-12-10 06:08:06.568059: step 12300, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:18m:25s remains)
2017-12-10 06:08:06.889054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288759 -4.4288311 -4.4287539 -4.4286318 -4.4285321 -4.4285536 -4.4286594 -4.4287686 -4.4288268 -4.428833 -4.428803 -4.4287691 -4.4287543 -4.4287958 -4.4288592][-4.4288468 -4.4287906 -4.4287038 -4.42858 -4.4284835 -4.4285145 -4.4286308 -4.4287357 -4.428772 -4.4287663 -4.4287534 -4.4287448 -4.4287534 -4.4288039 -4.4288716][-4.4288254 -4.4287567 -4.4286747 -4.4285803 -4.4285107 -4.4285464 -4.4286308 -4.4286909 -4.4286919 -4.42867 -4.4286776 -4.4287043 -4.4287472 -4.4288116 -4.4288821][-4.4288158 -4.4287481 -4.4286828 -4.4286284 -4.4285879 -4.4286027 -4.4286165 -4.4286132 -4.4285812 -4.4285474 -4.4285769 -4.4286575 -4.4287477 -4.4288244 -4.4288955][-4.4288306 -4.4287748 -4.4287262 -4.4286942 -4.4286594 -4.4286327 -4.428565 -4.4284978 -4.428442 -4.4284158 -4.4284821 -4.4286218 -4.4287562 -4.4288468 -4.428916][-4.4288669 -4.428822 -4.4287815 -4.4287448 -4.4286823 -4.4285893 -4.4284329 -4.4283161 -4.4282932 -4.4283333 -4.4284544 -4.4286275 -4.42878 -4.428875 -4.428936][-4.4289012 -4.4288607 -4.4288154 -4.4287558 -4.4286618 -4.4285135 -4.4282928 -4.4281621 -4.4282155 -4.4283423 -4.4285021 -4.4286742 -4.4288144 -4.4288969 -4.4289403][-4.4289207 -4.4288831 -4.4288268 -4.4287524 -4.4286418 -4.4284692 -4.4282341 -4.428123 -4.4282465 -4.4284296 -4.4285965 -4.4287443 -4.4288573 -4.4289155 -4.4289412][-4.4289322 -4.4289002 -4.4288397 -4.4287624 -4.4286537 -4.4284925 -4.4282842 -4.4282184 -4.4283643 -4.4285488 -4.4286966 -4.4288111 -4.4288893 -4.4289217 -4.4289331][-4.4289422 -4.4289131 -4.4288516 -4.4287705 -4.4286704 -4.4285412 -4.4283934 -4.4283767 -4.4285107 -4.4286671 -4.4287887 -4.4288688 -4.4289093 -4.4289169 -4.4289217][-4.4289403 -4.4289117 -4.4288521 -4.4287724 -4.4286857 -4.4285836 -4.4284978 -4.4285159 -4.4286304 -4.4287596 -4.4288583 -4.428905 -4.4289155 -4.4289103 -4.4289136][-4.4289317 -4.428905 -4.4288554 -4.4287915 -4.428719 -4.4286513 -4.4286222 -4.4286551 -4.4287391 -4.428834 -4.4288993 -4.4289103 -4.4289036 -4.4288955 -4.4288979][-4.4289303 -4.4289021 -4.4288611 -4.4288163 -4.4287658 -4.4287333 -4.4287391 -4.4287753 -4.4288321 -4.4288878 -4.4289131 -4.4289002 -4.4288836 -4.4288754 -4.4288797][-4.4289403 -4.4289107 -4.4288726 -4.428843 -4.4288154 -4.4288025 -4.4288192 -4.4288568 -4.4288955 -4.4289165 -4.4289088 -4.4288769 -4.4288545 -4.4288516 -4.4288654][-4.4289522 -4.4289289 -4.4288979 -4.4288774 -4.4288611 -4.4288564 -4.4288726 -4.4289007 -4.4289179 -4.4289131 -4.4288883 -4.4288526 -4.428834 -4.42884 -4.4288669]]...]
INFO - root - 2017-12-10 06:08:09.585586: step 12310, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:27m:25s remains)
INFO - root - 2017-12-10 06:08:12.239752: step 12320, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:56m:37s remains)
INFO - root - 2017-12-10 06:08:14.883674: step 12330, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:47m:28s remains)
INFO - root - 2017-12-10 06:08:17.551489: step 12340, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:44m:28s remains)
INFO - root - 2017-12-10 06:08:20.237435: step 12350, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 23h:31m:11s remains)
INFO - root - 2017-12-10 06:08:22.919406: step 12360, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 23h:52m:50s remains)
INFO - root - 2017-12-10 06:08:25.586525: step 12370, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:37m:20s remains)
INFO - root - 2017-12-10 06:08:28.267668: step 12380, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:57m:15s remains)
INFO - root - 2017-12-10 06:08:30.930140: step 12390, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 24h:32m:17s remains)
INFO - root - 2017-12-10 06:08:33.595443: step 12400, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:11m:09s remains)
2017-12-10 06:08:33.902175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287171 -4.4287014 -4.4287167 -4.4287448 -4.4287739 -4.4288125 -4.4288497 -4.4288554 -4.4288278 -4.4288025 -4.4287949 -4.4287949 -4.4287972 -4.4287982 -4.4288168][-4.4286985 -4.4286771 -4.4286838 -4.4287009 -4.4287233 -4.4287639 -4.4288025 -4.428802 -4.4287629 -4.4287305 -4.4287229 -4.4287281 -4.4287353 -4.428741 -4.4287677][-4.4287925 -4.4287744 -4.4287643 -4.4287524 -4.4287472 -4.42877 -4.4287972 -4.4287934 -4.4287591 -4.4287362 -4.4287376 -4.4287515 -4.4287639 -4.4287724 -4.4287987][-4.4288878 -4.4288731 -4.4288516 -4.4288225 -4.4288015 -4.4288087 -4.4288187 -4.4288049 -4.4287753 -4.4287639 -4.4287753 -4.4288 -4.4288216 -4.4288373 -4.4288611][-4.4289 -4.4288783 -4.4288497 -4.4288292 -4.4288173 -4.4288139 -4.4287996 -4.428762 -4.4287233 -4.428721 -4.4287477 -4.42879 -4.4288259 -4.428853 -4.428875][-4.4288421 -4.4288034 -4.4287629 -4.4287548 -4.4287605 -4.4287515 -4.4287081 -4.4286284 -4.4285665 -4.4285688 -4.4286141 -4.428679 -4.4287357 -4.4287791 -4.4288011][-4.4287753 -4.4287171 -4.4286637 -4.4286604 -4.4286728 -4.4286537 -4.428575 -4.4284453 -4.4283524 -4.42836 -4.4284248 -4.4285073 -4.4285779 -4.4286327 -4.4286613][-4.4287639 -4.4287047 -4.4286523 -4.42865 -4.4286542 -4.4286184 -4.4285135 -4.4283519 -4.4282408 -4.4282541 -4.428329 -4.4284077 -4.4284606 -4.4284978 -4.4285183][-4.4287834 -4.4287457 -4.4287095 -4.4287052 -4.4286942 -4.428659 -4.4285769 -4.4284525 -4.4283657 -4.4283795 -4.4284377 -4.4284964 -4.428524 -4.4285264 -4.4285083][-4.4287753 -4.4287543 -4.4287281 -4.4287133 -4.4286919 -4.4286728 -4.4286437 -4.4285927 -4.428555 -4.4285731 -4.4286079 -4.4286475 -4.428669 -4.4286671 -4.4286366][-4.4287343 -4.4287186 -4.4286962 -4.4286752 -4.4286532 -4.4286504 -4.428658 -4.428658 -4.4286561 -4.4286838 -4.4287052 -4.4287295 -4.4287467 -4.4287515 -4.4287348][-4.4287105 -4.4286938 -4.4286757 -4.4286571 -4.4286389 -4.4286423 -4.4286542 -4.4286633 -4.42867 -4.4287 -4.42872 -4.4287357 -4.4287515 -4.428762 -4.4287615][-4.4287281 -4.4287238 -4.428721 -4.4287114 -4.4286933 -4.4286857 -4.4286742 -4.4286647 -4.42866 -4.4286742 -4.4286857 -4.4286957 -4.4287105 -4.4287267 -4.4287391][-4.4287438 -4.4287634 -4.4287839 -4.4287925 -4.4287791 -4.4287596 -4.4287238 -4.4286895 -4.4286661 -4.4286556 -4.4286485 -4.4286523 -4.4286709 -4.4286895 -4.4287066][-4.4287243 -4.4287672 -4.4288125 -4.4288378 -4.4288292 -4.4288073 -4.428771 -4.4287319 -4.4286952 -4.4286613 -4.4286308 -4.4286251 -4.428648 -4.4286714 -4.428688]]...]
INFO - root - 2017-12-10 06:08:36.558805: step 12410, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:56m:36s remains)
INFO - root - 2017-12-10 06:08:39.251625: step 12420, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:33m:41s remains)
INFO - root - 2017-12-10 06:08:41.887471: step 12430, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 23h:52m:36s remains)
INFO - root - 2017-12-10 06:08:44.524489: step 12440, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:13m:49s remains)
INFO - root - 2017-12-10 06:08:47.145413: step 12450, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 22h:49m:20s remains)
INFO - root - 2017-12-10 06:08:49.809845: step 12460, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:49m:07s remains)
INFO - root - 2017-12-10 06:08:52.435390: step 12470, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:59m:31s remains)
INFO - root - 2017-12-10 06:08:55.102537: step 12480, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:41m:40s remains)
INFO - root - 2017-12-10 06:08:57.754852: step 12490, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:06m:32s remains)
INFO - root - 2017-12-10 06:09:00.399738: step 12500, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.286 sec/batch; 25h:24m:54s remains)
2017-12-10 06:09:00.696619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286036 -4.4286351 -4.4286728 -4.4287162 -4.4287291 -4.4287162 -4.4286985 -4.4287148 -4.4287648 -4.428793 -4.4288111 -4.4288244 -4.4288282 -4.4287958 -4.4287381][-4.4285917 -4.4286146 -4.4286442 -4.4286857 -4.4287124 -4.4287167 -4.4287109 -4.4287291 -4.4287672 -4.4287796 -4.4287877 -4.4288034 -4.428813 -4.4287786 -4.4287066][-4.428616 -4.4286213 -4.4286413 -4.4286771 -4.4287119 -4.4287262 -4.4287229 -4.4287453 -4.4287796 -4.4287953 -4.4288039 -4.4288144 -4.4288192 -4.4287791 -4.4286904][-4.4286795 -4.4286623 -4.4286695 -4.4286966 -4.4287214 -4.4287257 -4.4287114 -4.4287295 -4.4287715 -4.4288058 -4.4288268 -4.4288378 -4.4288392 -4.4287906 -4.4286919][-4.4287505 -4.4287243 -4.4287233 -4.4287276 -4.4287162 -4.4286809 -4.4286275 -4.4286208 -4.4286852 -4.4287639 -4.4288182 -4.428843 -4.4288468 -4.4288054 -4.4287233][-4.4287834 -4.4287558 -4.4287443 -4.4287167 -4.4286566 -4.4285612 -4.4284377 -4.4283986 -4.4285178 -4.4286733 -4.4287758 -4.4288287 -4.4288435 -4.4288211 -4.4287663][-4.4287486 -4.4287133 -4.4286871 -4.4286423 -4.4285541 -4.4284043 -4.4281969 -4.4281259 -4.4283166 -4.4285555 -4.4287133 -4.4287996 -4.4288297 -4.4288211 -4.4287863][-4.4286995 -4.4286594 -4.4286289 -4.4285889 -4.4285035 -4.4283442 -4.4281116 -4.4280219 -4.4282336 -4.4284997 -4.4286785 -4.4287748 -4.4288063 -4.428802 -4.4287825][-4.4286909 -4.4286542 -4.4286346 -4.428617 -4.4285707 -4.4284663 -4.4282956 -4.4282146 -4.4283614 -4.4285693 -4.4287105 -4.4287772 -4.42879 -4.428781 -4.4287734][-4.4287276 -4.4287033 -4.4287062 -4.4287119 -4.4286981 -4.4286451 -4.4285445 -4.42849 -4.4285641 -4.4286885 -4.4287767 -4.4288096 -4.4287949 -4.4287677 -4.428762][-4.4287968 -4.4287806 -4.4287877 -4.4287996 -4.4287953 -4.4287624 -4.4286962 -4.428659 -4.4286947 -4.4287682 -4.4288235 -4.4288378 -4.4288068 -4.428762 -4.4287472][-4.4288487 -4.4288335 -4.4288344 -4.42884 -4.4288321 -4.4288006 -4.4287491 -4.4287252 -4.4287548 -4.4288116 -4.4288545 -4.4288611 -4.4288206 -4.4287667 -4.4287405][-4.4288788 -4.4288588 -4.4288483 -4.4288478 -4.4288392 -4.428812 -4.428772 -4.4287605 -4.4287939 -4.4288497 -4.4288878 -4.428885 -4.4288383 -4.4287877 -4.42876][-4.4288883 -4.4288645 -4.4288464 -4.4288368 -4.4288306 -4.4288116 -4.4287806 -4.4287758 -4.4288096 -4.4288673 -4.4289026 -4.4288964 -4.4288554 -4.4288116 -4.4287877][-4.4288831 -4.4288588 -4.428843 -4.4288354 -4.428834 -4.4288239 -4.4287996 -4.4287953 -4.4288249 -4.4288793 -4.4289122 -4.4289083 -4.4288759 -4.4288354 -4.428813]]...]
INFO - root - 2017-12-10 06:09:03.353984: step 12510, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:41m:45s remains)
INFO - root - 2017-12-10 06:09:06.044072: step 12520, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:46m:18s remains)
INFO - root - 2017-12-10 06:09:08.770132: step 12530, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:04m:24s remains)
INFO - root - 2017-12-10 06:09:11.385325: step 12540, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:31m:48s remains)
INFO - root - 2017-12-10 06:09:14.018565: step 12550, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:10m:20s remains)
INFO - root - 2017-12-10 06:09:16.682399: step 12560, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:06m:14s remains)
INFO - root - 2017-12-10 06:09:19.308045: step 12570, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 25h:19m:45s remains)
INFO - root - 2017-12-10 06:09:22.026749: step 12580, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 25h:19m:04s remains)
INFO - root - 2017-12-10 06:09:24.678500: step 12590, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:16m:11s remains)
INFO - root - 2017-12-10 06:09:27.349863: step 12600, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:17m:39s remains)
2017-12-10 06:09:27.648322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289346 -4.4289351 -4.4289474 -4.4289584 -4.4289656 -4.4289646 -4.4289527 -4.4289422 -4.4289436 -4.428946 -4.4289536 -4.4289589 -4.4289541 -4.4289317 -4.4289122][-4.4289122 -4.4289141 -4.4289336 -4.428946 -4.4289517 -4.4289489 -4.4289355 -4.4289222 -4.4289217 -4.4289293 -4.4289465 -4.4289613 -4.4289579 -4.4289336 -4.4289117][-4.428864 -4.4288721 -4.4288979 -4.4289136 -4.428925 -4.4289293 -4.4289231 -4.4289112 -4.42891 -4.4289222 -4.4289436 -4.4289618 -4.4289608 -4.4289408 -4.42892][-4.428781 -4.428802 -4.4288287 -4.4288421 -4.428853 -4.4288611 -4.4288626 -4.4288607 -4.4288726 -4.4289012 -4.4289327 -4.4289565 -4.4289622 -4.4289494 -4.4289308][-4.4286695 -4.428709 -4.4287395 -4.4287467 -4.4287415 -4.4287348 -4.4287338 -4.4287434 -4.4287772 -4.428834 -4.4288917 -4.4289341 -4.428956 -4.4289541 -4.4289393][-4.428587 -4.4286304 -4.4286585 -4.4286494 -4.4286118 -4.428576 -4.4285684 -4.4285874 -4.4286447 -4.4287314 -4.4288197 -4.4288859 -4.4289303 -4.4289451 -4.42894][-4.4285989 -4.42862 -4.428627 -4.4285946 -4.428524 -4.4284592 -4.428441 -4.4284592 -4.4285231 -4.4286265 -4.428731 -4.4288163 -4.4288826 -4.4289184 -4.4289284][-4.4286942 -4.4286866 -4.4286728 -4.4286327 -4.4285641 -4.4285011 -4.4284711 -4.4284592 -4.4284892 -4.428576 -4.4286752 -4.4287682 -4.4288492 -4.4288983 -4.4289188][-4.4288063 -4.4287815 -4.4287577 -4.4287248 -4.4286804 -4.4286423 -4.4286108 -4.4285727 -4.4285641 -4.4286189 -4.4286981 -4.4287863 -4.4288654 -4.4289126 -4.4289293][-4.4289 -4.4288735 -4.4288521 -4.4288306 -4.4288039 -4.4287829 -4.4287548 -4.428709 -4.4286842 -4.42872 -4.4287853 -4.4288583 -4.4289141 -4.4289441 -4.428946][-4.4289546 -4.4289379 -4.4289293 -4.4289236 -4.4289093 -4.4288945 -4.4288692 -4.42883 -4.4288054 -4.4288306 -4.4288812 -4.4289303 -4.4289565 -4.4289651 -4.4289532][-4.4289804 -4.4289737 -4.4289761 -4.4289804 -4.4289737 -4.4289646 -4.4289465 -4.4289188 -4.4288988 -4.4289136 -4.4289441 -4.4289703 -4.4289742 -4.428966 -4.4289451][-4.4289861 -4.4289956 -4.4290085 -4.4290185 -4.4290123 -4.4290061 -4.4289961 -4.4289784 -4.4289622 -4.4289656 -4.4289756 -4.42898 -4.4289684 -4.42895 -4.4289265][-4.4289851 -4.4290047 -4.4290228 -4.4290309 -4.4290242 -4.4290195 -4.4290166 -4.4290066 -4.4289932 -4.4289889 -4.4289823 -4.4289718 -4.4289513 -4.4289303 -4.4289088][-4.4289761 -4.4289966 -4.4290137 -4.429018 -4.4290128 -4.4290128 -4.4290142 -4.4290094 -4.4289994 -4.4289904 -4.4289756 -4.4289594 -4.4289389 -4.4289217 -4.4289026]]...]
INFO - root - 2017-12-10 06:09:30.254442: step 12610, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 22h:40m:41s remains)
INFO - root - 2017-12-10 06:09:32.952524: step 12620, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:00m:42s remains)
INFO - root - 2017-12-10 06:09:35.629702: step 12630, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:20m:29s remains)
INFO - root - 2017-12-10 06:09:38.339470: step 12640, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:27m:07s remains)
INFO - root - 2017-12-10 06:09:40.981928: step 12650, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:54m:41s remains)
INFO - root - 2017-12-10 06:09:43.661419: step 12660, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:40m:08s remains)
INFO - root - 2017-12-10 06:09:46.336132: step 12670, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:04m:51s remains)
INFO - root - 2017-12-10 06:09:49.004059: step 12680, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:56m:24s remains)
INFO - root - 2017-12-10 06:09:51.676010: step 12690, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:49m:34s remains)
INFO - root - 2017-12-10 06:09:54.333367: step 12700, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 23h:57m:36s remains)
2017-12-10 06:09:54.611040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289312 -4.428926 -4.428915 -4.4289069 -4.4289083 -4.428916 -4.4289241 -4.4289274 -4.4289279 -4.428925 -4.4289217 -4.4289169 -4.4289002 -4.4288712 -4.4288487][-4.4289742 -4.428966 -4.428956 -4.4289479 -4.4289474 -4.4289527 -4.428957 -4.4289575 -4.4289532 -4.4289479 -4.4289441 -4.4289417 -4.4289308 -4.4289069 -4.4288878][-4.428988 -4.4289756 -4.4289689 -4.4289641 -4.4289646 -4.42897 -4.4289732 -4.4289708 -4.4289641 -4.4289594 -4.4289565 -4.4289575 -4.4289536 -4.4289403 -4.4289312][-4.4289546 -4.4289312 -4.4289184 -4.4289093 -4.4289055 -4.4289112 -4.4289212 -4.428926 -4.428926 -4.4289284 -4.4289331 -4.4289403 -4.4289427 -4.4289412 -4.4289465][-4.4288626 -4.4288211 -4.4287882 -4.42876 -4.4287467 -4.4287567 -4.428781 -4.4288015 -4.4288144 -4.4288306 -4.42885 -4.4288688 -4.42888 -4.4288917 -4.428915][-4.4287305 -4.4286709 -4.4286065 -4.428544 -4.4285121 -4.4285274 -4.4285779 -4.4286246 -4.4286489 -4.4286714 -4.4287 -4.4287348 -4.4287663 -4.4287958 -4.4288392][-4.4286242 -4.4285622 -4.4284744 -4.42837 -4.4283047 -4.4283166 -4.4283881 -4.4284592 -4.4284906 -4.4285054 -4.4285288 -4.4285736 -4.428628 -4.4286823 -4.4287477][-4.428616 -4.4285784 -4.4285007 -4.428381 -4.4282923 -4.4282861 -4.4283509 -4.4284215 -4.4284482 -4.4284472 -4.4284496 -4.4284825 -4.4285378 -4.42861 -4.4286919][-4.4286804 -4.42868 -4.4286413 -4.4285469 -4.4284668 -4.428443 -4.4284782 -4.4285226 -4.428535 -4.4285221 -4.4285078 -4.4285197 -4.4285607 -4.4286332 -4.4287167][-4.4287562 -4.4287734 -4.4287682 -4.428709 -4.4286504 -4.4286246 -4.4286389 -4.4286628 -4.4286656 -4.4286475 -4.4286256 -4.4286208 -4.4286466 -4.42871 -4.4287844][-4.4288149 -4.4288416 -4.4288592 -4.428833 -4.4287977 -4.4287744 -4.4287729 -4.4287858 -4.4287868 -4.4287682 -4.4287419 -4.4287276 -4.4287415 -4.42879 -4.4288521][-4.4288764 -4.4288964 -4.4289203 -4.4289165 -4.4289036 -4.4288864 -4.428875 -4.4288759 -4.4288731 -4.4288573 -4.4288368 -4.4288225 -4.4288282 -4.4288635 -4.4289112][-4.4289236 -4.4289327 -4.4289503 -4.428957 -4.4289589 -4.4289508 -4.4289422 -4.4289403 -4.428936 -4.4289246 -4.4289122 -4.4289017 -4.4289012 -4.4289203 -4.4289494][-4.4289579 -4.4289584 -4.4289665 -4.4289737 -4.428978 -4.4289727 -4.428968 -4.4289675 -4.4289632 -4.4289541 -4.428946 -4.4289393 -4.4289351 -4.4289412 -4.4289522][-4.428956 -4.42896 -4.428967 -4.4289722 -4.4289742 -4.4289703 -4.4289694 -4.4289694 -4.4289632 -4.4289527 -4.4289451 -4.4289408 -4.4289379 -4.4289379 -4.4289374]]...]
INFO - root - 2017-12-10 06:09:57.260303: step 12710, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 23h:51m:51s remains)
INFO - root - 2017-12-10 06:09:59.915854: step 12720, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:11m:24s remains)
INFO - root - 2017-12-10 06:10:02.582185: step 12730, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:19m:28s remains)
INFO - root - 2017-12-10 06:10:05.265154: step 12740, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:01m:49s remains)
INFO - root - 2017-12-10 06:10:07.883466: step 12750, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:46m:03s remains)
INFO - root - 2017-12-10 06:10:10.494128: step 12760, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:53m:03s remains)
INFO - root - 2017-12-10 06:10:13.145396: step 12770, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:05m:44s remains)
INFO - root - 2017-12-10 06:10:15.795516: step 12780, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:43m:17s remains)
INFO - root - 2017-12-10 06:10:18.426552: step 12790, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:26m:33s remains)
INFO - root - 2017-12-10 06:10:21.074629: step 12800, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:56m:33s remains)
2017-12-10 06:10:21.358986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287825 -4.4287586 -4.4287438 -4.4287381 -4.4287629 -4.4288015 -4.4288368 -4.4288449 -4.4288278 -4.4287934 -4.4287682 -4.4287395 -4.4287119 -4.4286895 -4.4287028][-4.4287496 -4.4287291 -4.4287095 -4.4286952 -4.4287167 -4.4287548 -4.4287863 -4.4287958 -4.4287844 -4.4287596 -4.428751 -4.4287276 -4.4286914 -4.4286461 -4.4286513][-4.4287519 -4.4287333 -4.4287038 -4.4286795 -4.4286928 -4.4287238 -4.4287462 -4.4287548 -4.4287543 -4.4287505 -4.4287624 -4.4287515 -4.4287105 -4.428648 -4.4286447][-4.4287677 -4.4287486 -4.4287105 -4.4286757 -4.4286752 -4.4286919 -4.4286971 -4.4286947 -4.4287114 -4.4287362 -4.4287715 -4.4287748 -4.4287333 -4.4286575 -4.4286466][-4.4287796 -4.4287553 -4.428709 -4.4286633 -4.4286475 -4.4286423 -4.4286189 -4.4285994 -4.4286394 -4.4287066 -4.4287682 -4.4287887 -4.4287477 -4.4286604 -4.4286432][-4.4287915 -4.4287624 -4.4287114 -4.4286537 -4.4286132 -4.4285755 -4.4285049 -4.4284596 -4.4285321 -4.4286575 -4.4287567 -4.4288 -4.4287643 -4.4286709 -4.4286504][-4.4288054 -4.4287739 -4.428721 -4.4286518 -4.4285865 -4.4285078 -4.4283829 -4.4283009 -4.4284072 -4.4285965 -4.4287329 -4.4287968 -4.42877 -4.4286776 -4.4286604][-4.4288335 -4.4288039 -4.4287524 -4.4286814 -4.4286041 -4.4284973 -4.4283314 -4.4282169 -4.4283361 -4.4285626 -4.4287186 -4.4287958 -4.4287758 -4.4286928 -4.4286885][-4.4288683 -4.4288421 -4.428802 -4.4287424 -4.4286685 -4.4285631 -4.4284015 -4.4282913 -4.4283843 -4.4285822 -4.4287252 -4.4288015 -4.4287896 -4.4287229 -4.42873][-4.4288969 -4.4288716 -4.4288425 -4.428802 -4.4287438 -4.4286561 -4.4285254 -4.4284339 -4.4284878 -4.42863 -4.4287419 -4.4288058 -4.4288 -4.428751 -4.4287691][-4.4289012 -4.4288735 -4.4288483 -4.428823 -4.4287834 -4.4287176 -4.42862 -4.4285464 -4.4285688 -4.4286604 -4.4287457 -4.4287982 -4.4288006 -4.428772 -4.4287925][-4.4288816 -4.4288497 -4.428823 -4.4288054 -4.4287796 -4.4287305 -4.4286623 -4.4286089 -4.4286165 -4.4286747 -4.4287391 -4.4287834 -4.428793 -4.4287791 -4.4287972][-4.4288592 -4.4288263 -4.4288025 -4.4287915 -4.4287758 -4.4287405 -4.4286957 -4.4286623 -4.4286666 -4.4287014 -4.4287462 -4.42878 -4.42879 -4.428782 -4.4287939][-4.4288554 -4.4288244 -4.4288054 -4.4288 -4.4287925 -4.4287696 -4.4287415 -4.4287229 -4.4287243 -4.42874 -4.4287648 -4.4287877 -4.4287953 -4.4287896 -4.4287968][-4.428863 -4.4288344 -4.42882 -4.4288206 -4.4288192 -4.4288063 -4.4287915 -4.428782 -4.4287815 -4.4287848 -4.4287934 -4.4288058 -4.42881 -4.4288054 -4.4288096]]...]
INFO - root - 2017-12-10 06:10:24.027260: step 12810, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:50m:12s remains)
INFO - root - 2017-12-10 06:10:26.751338: step 12820, loss = 2.28, batch loss = 2.23 (25.8 examples/sec; 0.310 sec/batch; 27h:29m:35s remains)
INFO - root - 2017-12-10 06:10:29.392880: step 12830, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:11m:49s remains)
INFO - root - 2017-12-10 06:10:32.055257: step 12840, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:52m:55s remains)
INFO - root - 2017-12-10 06:10:34.689649: step 12850, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:45m:57s remains)
INFO - root - 2017-12-10 06:10:37.388291: step 12860, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:20m:37s remains)
INFO - root - 2017-12-10 06:10:40.090987: step 12870, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:25m:16s remains)
INFO - root - 2017-12-10 06:10:42.724629: step 12880, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:34m:56s remains)
INFO - root - 2017-12-10 06:10:45.359442: step 12890, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:17m:23s remains)
INFO - root - 2017-12-10 06:10:48.000959: step 12900, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:41m:00s remains)
2017-12-10 06:10:48.310529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428741 -4.4287472 -4.4287615 -4.4287863 -4.4288187 -4.4288421 -4.4288349 -4.4287953 -4.4287496 -4.4287257 -4.4287233 -4.4287438 -4.4287682 -4.4287953 -4.4288239][-4.428761 -4.4287405 -4.4287257 -4.42874 -4.4287724 -4.428792 -4.4287796 -4.4287319 -4.4286804 -4.4286671 -4.4286747 -4.4286985 -4.4287243 -4.4287438 -4.4287682][-4.4287462 -4.4287095 -4.4286828 -4.4286928 -4.4287224 -4.4287324 -4.4287138 -4.4286671 -4.4286251 -4.4286256 -4.4286447 -4.4286728 -4.4286962 -4.4287043 -4.42872][-4.4287205 -4.4286871 -4.4286695 -4.4286842 -4.4287047 -4.428688 -4.4286413 -4.4285889 -4.4285707 -4.4285941 -4.4286251 -4.4286585 -4.4286828 -4.4286814 -4.4286857][-4.4287229 -4.4287152 -4.4287148 -4.4287305 -4.4287286 -4.4286623 -4.4285612 -4.4284992 -4.4285088 -4.4285679 -4.4286227 -4.428668 -4.4286947 -4.4286885 -4.428679][-4.4287138 -4.4287295 -4.4287519 -4.4287724 -4.4287491 -4.4286394 -4.4284854 -4.4283967 -4.428431 -4.4285388 -4.4286356 -4.4286947 -4.4287148 -4.4287024 -4.428688][-4.4286985 -4.4287276 -4.4287653 -4.4287915 -4.4287529 -4.4286127 -4.4284034 -4.4282613 -4.4283247 -4.4284983 -4.4286366 -4.4287052 -4.4287286 -4.4287338 -4.4287271][-4.4286914 -4.4287243 -4.4287629 -4.4287834 -4.428731 -4.428566 -4.4282975 -4.4280968 -4.4282169 -4.428473 -4.4286551 -4.4287319 -4.4287715 -4.4287958 -4.4287896][-4.4286728 -4.4287076 -4.4287376 -4.4287467 -4.4286895 -4.4285226 -4.4282408 -4.42805 -4.4282308 -4.4285021 -4.4286833 -4.4287682 -4.4288206 -4.4288568 -4.4288468][-4.42865 -4.4286838 -4.4287114 -4.4287224 -4.4286819 -4.4285579 -4.4283552 -4.4282537 -4.4283962 -4.4285827 -4.428719 -4.4287972 -4.4288545 -4.4288893 -4.4288774][-4.4286494 -4.4286757 -4.4287043 -4.42872 -4.4286947 -4.4286165 -4.42851 -4.4284706 -4.4285507 -4.4286528 -4.4287443 -4.4288096 -4.4288578 -4.4288855 -4.4288688][-4.428679 -4.4286866 -4.4287071 -4.428719 -4.4286985 -4.4286532 -4.4286056 -4.4285984 -4.4286461 -4.4287038 -4.4287586 -4.4288011 -4.4288349 -4.4288559 -4.4288387][-4.4287038 -4.4287014 -4.4287143 -4.4287233 -4.4287047 -4.4286695 -4.4286427 -4.4286551 -4.42869 -4.4287238 -4.4287553 -4.4287786 -4.4287963 -4.4288068 -4.4287939][-4.4287224 -4.4287162 -4.4287176 -4.42872 -4.4286923 -4.4286437 -4.428617 -4.4286423 -4.4286766 -4.428699 -4.4287238 -4.42874 -4.42874 -4.4287438 -4.4287448][-4.4287376 -4.42873 -4.4287186 -4.428709 -4.4286757 -4.4286127 -4.4285831 -4.428618 -4.4286528 -4.4286733 -4.4287081 -4.4287262 -4.4287181 -4.4287133 -4.4287143]]...]
INFO - root - 2017-12-10 06:10:50.896093: step 12910, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 22h:40m:01s remains)
INFO - root - 2017-12-10 06:10:53.548887: step 12920, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:31m:47s remains)
INFO - root - 2017-12-10 06:10:56.179121: step 12930, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:38m:57s remains)
INFO - root - 2017-12-10 06:10:58.859978: step 12940, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 23h:56m:45s remains)
INFO - root - 2017-12-10 06:11:01.487835: step 12950, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:18m:17s remains)
INFO - root - 2017-12-10 06:11:04.182193: step 12960, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 25h:40m:34s remains)
INFO - root - 2017-12-10 06:11:06.892698: step 12970, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 25h:48m:15s remains)
INFO - root - 2017-12-10 06:11:09.537912: step 12980, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:29m:36s remains)
INFO - root - 2017-12-10 06:11:12.189980: step 12990, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:45m:18s remains)
INFO - root - 2017-12-10 06:11:14.855645: step 13000, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:35m:02s remains)
2017-12-10 06:11:15.143080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289384 -4.42894 -4.4289384 -4.4289336 -4.4289327 -4.4289351 -4.4289355 -4.4289317 -4.4289289 -4.4289289 -4.4289265 -4.4289227 -4.4289207 -4.4289222 -4.4289274][-4.4289403 -4.4289417 -4.4289403 -4.4289355 -4.4289346 -4.428936 -4.4289331 -4.4289255 -4.4289184 -4.428916 -4.4289131 -4.4289103 -4.4289107 -4.4289165 -4.4289269][-4.42894 -4.42894 -4.428937 -4.4289317 -4.4289279 -4.4289222 -4.42891 -4.4288907 -4.4288745 -4.4288692 -4.4288692 -4.4288731 -4.4288821 -4.4288974 -4.4289169][-4.4289427 -4.4289403 -4.4289327 -4.4289188 -4.4289017 -4.4288754 -4.4288397 -4.4288 -4.4287729 -4.4287691 -4.428782 -4.4288025 -4.4288282 -4.4288611 -4.428896][-4.4289384 -4.4289365 -4.4289227 -4.428894 -4.4288507 -4.4287882 -4.4287128 -4.4286413 -4.4286013 -4.4286065 -4.4286447 -4.4286976 -4.4287548 -4.4288149 -4.4288692][-4.4289093 -4.4289083 -4.4288912 -4.4288492 -4.4287777 -4.42867 -4.4285445 -4.4284372 -4.4283938 -4.4284225 -4.4285045 -4.4286075 -4.4287033 -4.4287896 -4.4288564][-4.4288435 -4.4288483 -4.4288368 -4.4287958 -4.4287152 -4.428586 -4.4284363 -4.428319 -4.42829 -4.4283485 -4.4284687 -4.4286103 -4.4287248 -4.4288125 -4.4288721][-4.4287553 -4.4287686 -4.4287796 -4.4287672 -4.4287157 -4.4286156 -4.4284964 -4.4284081 -4.4284005 -4.4284682 -4.4285817 -4.4287095 -4.4288058 -4.4288697 -4.4289036][-4.4286537 -4.42867 -4.4287033 -4.4287319 -4.4287333 -4.4286928 -4.4286313 -4.4285879 -4.428597 -4.4286594 -4.4287457 -4.4288359 -4.428896 -4.4289269 -4.4289336][-4.4285717 -4.4285874 -4.4286361 -4.4286952 -4.428741 -4.4287567 -4.4287515 -4.4287491 -4.4287705 -4.4288211 -4.4288807 -4.4289355 -4.4289646 -4.4289713 -4.4289613][-4.4285479 -4.4285593 -4.4286122 -4.4286828 -4.42875 -4.4288015 -4.4288368 -4.4288673 -4.4288974 -4.4289336 -4.4289675 -4.4289947 -4.4290023 -4.4289923 -4.4289737][-4.4285583 -4.428565 -4.4286127 -4.42868 -4.42875 -4.4288173 -4.4288754 -4.4289269 -4.428957 -4.4289718 -4.4289789 -4.4289851 -4.4289808 -4.4289665 -4.4289484][-4.428576 -4.4285755 -4.4286175 -4.4286819 -4.4287505 -4.42882 -4.4288788 -4.4289227 -4.4289327 -4.42892 -4.4289074 -4.4289055 -4.4289041 -4.428896 -4.4288855][-4.4285984 -4.4286003 -4.4286432 -4.4287047 -4.4287682 -4.4288321 -4.4288759 -4.4288917 -4.428864 -4.4288158 -4.4287848 -4.4287815 -4.4287882 -4.4287939 -4.4287944][-4.4286351 -4.4286466 -4.4286947 -4.4287519 -4.4288068 -4.4288588 -4.42888 -4.4288578 -4.4287848 -4.4286966 -4.4286442 -4.4286418 -4.4286628 -4.4286876 -4.4286985]]...]
INFO - root - 2017-12-10 06:11:17.783295: step 13010, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:20m:40s remains)
INFO - root - 2017-12-10 06:11:20.439881: step 13020, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:26m:10s remains)
INFO - root - 2017-12-10 06:11:23.064906: step 13030, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:47m:52s remains)
INFO - root - 2017-12-10 06:11:25.711710: step 13040, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:36m:37s remains)
INFO - root - 2017-12-10 06:11:28.410030: step 13050, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:12m:48s remains)
INFO - root - 2017-12-10 06:11:31.064139: step 13060, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:06m:29s remains)
INFO - root - 2017-12-10 06:11:33.748351: step 13070, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 23h:55m:43s remains)
INFO - root - 2017-12-10 06:11:36.409651: step 13080, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:38m:25s remains)
INFO - root - 2017-12-10 06:11:39.062391: step 13090, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:29m:12s remains)
INFO - root - 2017-12-10 06:11:41.707677: step 13100, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:04m:30s remains)
2017-12-10 06:11:42.008559: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289908 -4.4289875 -4.4289737 -4.4289641 -4.4289608 -4.428966 -4.4289784 -4.4289918 -4.4290023 -4.429008 -4.4290118 -4.4290142 -4.4290171 -4.4290204 -4.42902][-4.4289355 -4.4289274 -4.4289107 -4.4288979 -4.4288912 -4.428895 -4.428905 -4.4289212 -4.4289422 -4.4289613 -4.4289756 -4.4289832 -4.4289913 -4.4290004 -4.4290061][-4.4288826 -4.4288669 -4.4288421 -4.428823 -4.4288077 -4.428803 -4.4288039 -4.4288187 -4.4288492 -4.428884 -4.4289107 -4.4289274 -4.4289436 -4.4289589 -4.4289727][-4.4288459 -4.4288158 -4.4287825 -4.42876 -4.4287367 -4.4287181 -4.4287024 -4.428709 -4.4287467 -4.4287963 -4.4288325 -4.4288578 -4.4288831 -4.428905 -4.4289269][-4.4288306 -4.4287868 -4.4287462 -4.4287214 -4.4286895 -4.4286485 -4.4286141 -4.4286132 -4.4286561 -4.428719 -4.4287663 -4.4287982 -4.4288321 -4.428863 -4.4288917][-4.4288206 -4.4287691 -4.4287229 -4.4286919 -4.4286518 -4.4285936 -4.4285488 -4.4285483 -4.4286032 -4.4286795 -4.4287333 -4.428771 -4.4288139 -4.4288526 -4.4288797][-4.4288249 -4.4287691 -4.4287152 -4.4286723 -4.4286213 -4.4285512 -4.4285059 -4.4285078 -4.4285707 -4.4286547 -4.4287133 -4.4287591 -4.4288125 -4.4288573 -4.4288812][-4.4288526 -4.4288 -4.4287372 -4.4286747 -4.4286113 -4.4285388 -4.4285 -4.4285049 -4.4285674 -4.4286494 -4.4287105 -4.4287643 -4.4288235 -4.42887 -4.4288907][-4.4288888 -4.4288454 -4.4287791 -4.4287038 -4.4286323 -4.4285655 -4.4285374 -4.4285421 -4.4286 -4.4286728 -4.4287333 -4.4287877 -4.428843 -4.4288859 -4.4289041][-4.4289155 -4.428884 -4.4288311 -4.4287648 -4.4286976 -4.4286437 -4.4286256 -4.4286265 -4.4286652 -4.4287195 -4.428772 -4.4288182 -4.4288616 -4.4288969 -4.4289174][-4.4289274 -4.4289012 -4.4288678 -4.4288282 -4.4287834 -4.4287524 -4.4287486 -4.4287496 -4.4287724 -4.4288068 -4.4288435 -4.4288726 -4.428895 -4.4289188 -4.4289374][-4.4289312 -4.4289045 -4.4288816 -4.4288654 -4.42885 -4.428844 -4.4288554 -4.428863 -4.4288788 -4.4288936 -4.4289112 -4.4289236 -4.42893 -4.4289446 -4.4289594][-4.4289289 -4.4288983 -4.4288731 -4.4288626 -4.4288588 -4.428864 -4.4288826 -4.4289036 -4.428925 -4.4289336 -4.4289379 -4.4289379 -4.42894 -4.4289513 -4.428966][-4.4289174 -4.4288826 -4.4288521 -4.4288349 -4.4288239 -4.4288206 -4.4288316 -4.4288588 -4.428895 -4.428916 -4.4289212 -4.4289231 -4.4289265 -4.428937 -4.4289556][-4.428896 -4.4288559 -4.4288216 -4.4287939 -4.4287672 -4.4287415 -4.4287372 -4.4287672 -4.4288225 -4.428865 -4.4288812 -4.4288921 -4.4288983 -4.4289117 -4.428937]]...]
INFO - root - 2017-12-10 06:11:44.636495: step 13110, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:25m:42s remains)
INFO - root - 2017-12-10 06:11:47.311338: step 13120, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:57m:56s remains)
INFO - root - 2017-12-10 06:11:49.912225: step 13130, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 22h:46m:55s remains)
INFO - root - 2017-12-10 06:11:52.533006: step 13140, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 23h:32m:51s remains)
INFO - root - 2017-12-10 06:11:55.204238: step 13150, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:08m:39s remains)
INFO - root - 2017-12-10 06:11:57.811955: step 13160, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:21m:24s remains)
INFO - root - 2017-12-10 06:12:00.456191: step 13170, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:40m:53s remains)
INFO - root - 2017-12-10 06:12:03.158040: step 13180, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:05m:25s remains)
INFO - root - 2017-12-10 06:12:05.796888: step 13190, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:32m:03s remains)
INFO - root - 2017-12-10 06:12:08.478627: step 13200, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:06m:11s remains)
2017-12-10 06:12:08.810025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288559 -4.4288368 -4.4288206 -4.4288154 -4.4288216 -4.4288316 -4.4288311 -4.4288173 -4.4288116 -4.428812 -4.4288225 -4.4288368 -4.4288497 -4.4288592 -4.4288607][-4.4288182 -4.4287887 -4.4287643 -4.4287524 -4.4287548 -4.4287667 -4.4287672 -4.428751 -4.42875 -4.4287543 -4.4287658 -4.4287806 -4.4287944 -4.4288054 -4.4288054][-4.4287753 -4.4287305 -4.4286942 -4.42867 -4.4286594 -4.4286709 -4.4286857 -4.4286761 -4.4286814 -4.4287019 -4.4287252 -4.4287391 -4.4287391 -4.4287386 -4.428731][-4.4287438 -4.4286861 -4.4286342 -4.4285908 -4.4285564 -4.4285617 -4.4285979 -4.4286027 -4.4286156 -4.4286566 -4.4286933 -4.4287033 -4.4286761 -4.4286532 -4.4286461][-4.4287157 -4.4286485 -4.4285822 -4.428515 -4.4284487 -4.4284439 -4.4285026 -4.4285293 -4.428556 -4.4286094 -4.4286551 -4.4286537 -4.4286017 -4.4285722 -4.4285793][-4.4287 -4.4286246 -4.4285445 -4.4284639 -4.42837 -4.4283414 -4.4284129 -4.4284635 -4.4285007 -4.4285502 -4.4285913 -4.4285808 -4.4285169 -4.4284968 -4.428534][-4.4287019 -4.4286227 -4.4285431 -4.4284611 -4.4283605 -4.4283128 -4.4283791 -4.4284415 -4.4284735 -4.4284992 -4.4285293 -4.4285035 -4.4284296 -4.4284325 -4.4285049][-4.4287143 -4.4286571 -4.4285994 -4.42853 -4.4284515 -4.4284039 -4.4284515 -4.4285 -4.428504 -4.4284945 -4.4285 -4.428463 -4.4283991 -4.4284334 -4.4285288][-4.4287386 -4.4287148 -4.4286838 -4.4286413 -4.4285941 -4.4285569 -4.4285817 -4.4285989 -4.4285822 -4.4285526 -4.4285364 -4.428503 -4.4284654 -4.4285169 -4.4286089][-4.4287562 -4.4287486 -4.4287333 -4.4287109 -4.4286842 -4.4286518 -4.4286566 -4.4286542 -4.4286389 -4.428617 -4.4286046 -4.4285841 -4.4285579 -4.4286108 -4.4286861][-4.428772 -4.4287629 -4.4287472 -4.4287314 -4.4287167 -4.4286714 -4.428647 -4.4286437 -4.42865 -4.4286571 -4.4286604 -4.4286542 -4.4286289 -4.4286723 -4.4287229][-4.4287872 -4.428771 -4.4287543 -4.4287372 -4.4287229 -4.4286628 -4.4286294 -4.4286361 -4.4286623 -4.4286952 -4.4287138 -4.4287171 -4.428688 -4.4287076 -4.42872][-4.4288039 -4.4287839 -4.4287648 -4.4287481 -4.4287343 -4.4286776 -4.4286613 -4.428688 -4.4287181 -4.4287581 -4.4287829 -4.4287887 -4.4287539 -4.4287472 -4.4287267][-4.4288297 -4.4288168 -4.4288077 -4.4288015 -4.4287963 -4.4287605 -4.4287643 -4.4287977 -4.4288206 -4.4288487 -4.4288712 -4.4288764 -4.4288445 -4.4288244 -4.4287934][-4.4288607 -4.4288607 -4.428865 -4.4288692 -4.42887 -4.4288511 -4.4288597 -4.4288883 -4.4288988 -4.4289093 -4.4289274 -4.4289327 -4.4289145 -4.4288988 -4.4288764]]...]
INFO - root - 2017-12-10 06:12:11.478051: step 13210, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:18m:05s remains)
INFO - root - 2017-12-10 06:12:14.165585: step 13220, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:31m:13s remains)
INFO - root - 2017-12-10 06:12:16.770717: step 13230, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:14m:00s remains)
INFO - root - 2017-12-10 06:12:19.382648: step 13240, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:47m:25s remains)
INFO - root - 2017-12-10 06:12:21.992502: step 13250, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:31m:47s remains)
INFO - root - 2017-12-10 06:12:24.691679: step 13260, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 23h:32m:21s remains)
INFO - root - 2017-12-10 06:12:27.347567: step 13270, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:11m:19s remains)
INFO - root - 2017-12-10 06:12:30.028002: step 13280, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:57m:07s remains)
INFO - root - 2017-12-10 06:12:32.686915: step 13290, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:18m:56s remains)
INFO - root - 2017-12-10 06:12:35.316796: step 13300, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:06m:29s remains)
2017-12-10 06:12:35.607209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287486 -4.4287572 -4.4287682 -4.4287438 -4.4286919 -4.4286342 -4.4285617 -4.4284692 -4.4284048 -4.4283962 -4.4284396 -4.4285064 -4.4285455 -4.4285121 -4.4284296][-4.428803 -4.428803 -4.4287977 -4.4287639 -4.4287148 -4.4286671 -4.4286065 -4.4285312 -4.4284792 -4.4284773 -4.4285126 -4.42857 -4.4285955 -4.4285583 -4.42848][-4.4288177 -4.4288154 -4.4288049 -4.4287653 -4.428719 -4.42868 -4.42863 -4.4285731 -4.4285369 -4.4285426 -4.4285574 -4.4286 -4.428616 -4.4285841 -4.4285369][-4.4288096 -4.4288106 -4.428802 -4.4287558 -4.42871 -4.4286766 -4.4286318 -4.428586 -4.428556 -4.4285569 -4.4285564 -4.4285908 -4.4286065 -4.4285831 -4.4285593][-4.4287977 -4.4287949 -4.4287782 -4.428721 -4.4286671 -4.4286284 -4.4285793 -4.4285264 -4.4285007 -4.4284983 -4.4285007 -4.4285545 -4.4285922 -4.4285879 -4.4285803][-4.4287663 -4.4287534 -4.4287233 -4.4286466 -4.4285817 -4.42855 -4.4285121 -4.4284563 -4.4284306 -4.4284091 -4.4284139 -4.4284906 -4.4285593 -4.4285774 -4.4285812][-4.42871 -4.4286842 -4.4286442 -4.4285626 -4.428503 -4.428493 -4.4284835 -4.428441 -4.4284096 -4.4283614 -4.4283605 -4.4284391 -4.4285126 -4.4285421 -4.428565][-4.4286308 -4.4286013 -4.4285655 -4.4284978 -4.4284639 -4.4284806 -4.4284835 -4.428453 -4.4284229 -4.4283724 -4.4283695 -4.4284229 -4.4284749 -4.4284978 -4.4285312][-4.4285889 -4.4285707 -4.4285421 -4.4284863 -4.4284749 -4.428504 -4.4285073 -4.4284883 -4.428473 -4.4284453 -4.4284363 -4.4284492 -4.4284639 -4.428472 -4.4285045][-4.4286027 -4.4285903 -4.4285555 -4.4285035 -4.4285035 -4.4285398 -4.4285483 -4.4285536 -4.4285707 -4.4285703 -4.42855 -4.42852 -4.4284854 -4.4284716 -4.4285][-4.4286108 -4.4286 -4.4285622 -4.4285216 -4.4285307 -4.4285612 -4.4285617 -4.4285812 -4.4286237 -4.4286542 -4.428637 -4.4285836 -4.4285111 -4.4284749 -4.4284983][-4.4286175 -4.4285979 -4.4285588 -4.428525 -4.4285297 -4.4285502 -4.4285445 -4.42857 -4.428637 -4.42869 -4.42868 -4.4286208 -4.428544 -4.4285045 -4.4285169][-4.4286675 -4.4286437 -4.428606 -4.428565 -4.428545 -4.4285426 -4.4285254 -4.428555 -4.4286323 -4.4286914 -4.4286871 -4.4286327 -4.4285717 -4.4285436 -4.428544][-4.4287214 -4.4286928 -4.4286528 -4.4286 -4.4285593 -4.4285436 -4.4285254 -4.4285488 -4.4286227 -4.4286795 -4.4286747 -4.4286313 -4.4285884 -4.4285674 -4.4285626][-4.4287429 -4.4287081 -4.4286709 -4.4286237 -4.4285789 -4.4285574 -4.428546 -4.4285693 -4.4286294 -4.4286704 -4.4286633 -4.4286332 -4.4286046 -4.428586 -4.4285803]]...]
INFO - root - 2017-12-10 06:12:38.281148: step 13310, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.253 sec/batch; 22h:23m:18s remains)
INFO - root - 2017-12-10 06:12:40.926865: step 13320, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:12m:41s remains)
INFO - root - 2017-12-10 06:12:43.548904: step 13330, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:46m:39s remains)
INFO - root - 2017-12-10 06:12:46.249880: step 13340, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:16m:48s remains)
INFO - root - 2017-12-10 06:12:48.877221: step 13350, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:55m:36s remains)
INFO - root - 2017-12-10 06:12:51.547054: step 13360, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:05m:11s remains)
INFO - root - 2017-12-10 06:12:54.215245: step 13370, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:33m:27s remains)
INFO - root - 2017-12-10 06:12:56.895728: step 13380, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:02m:24s remains)
INFO - root - 2017-12-10 06:12:59.558122: step 13390, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:39m:14s remains)
INFO - root - 2017-12-10 06:13:02.191780: step 13400, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:57m:54s remains)
2017-12-10 06:13:02.515620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290223 -4.4289951 -4.4289479 -4.428874 -4.4287848 -4.42871 -4.4286728 -4.4286523 -4.4286346 -4.4286313 -4.4286695 -4.4287248 -4.4287968 -4.4288607 -4.4289112][-4.4290185 -4.4289846 -4.4289269 -4.4288421 -4.4287448 -4.4286714 -4.428638 -4.4286294 -4.4286304 -4.4286537 -4.4287167 -4.4287744 -4.4288363 -4.4288831 -4.4289227][-4.4290147 -4.428977 -4.4289131 -4.4288225 -4.4287262 -4.4286642 -4.4286375 -4.4286342 -4.4286451 -4.4286842 -4.4287567 -4.4288139 -4.4288735 -4.4289145 -4.4289451][-4.4290113 -4.4289703 -4.4289031 -4.4288039 -4.4287052 -4.4286509 -4.4286237 -4.4286146 -4.428628 -4.4286795 -4.4287562 -4.4288225 -4.42889 -4.428937 -4.428966][-4.4290104 -4.4289684 -4.4288979 -4.4287877 -4.4286747 -4.4286008 -4.4285464 -4.4285226 -4.4285541 -4.428638 -4.42873 -4.4288096 -4.4288869 -4.4289446 -4.4289789][-4.4290152 -4.4289732 -4.4288979 -4.4287696 -4.4286294 -4.4285069 -4.4283977 -4.4283581 -4.4284463 -4.4285955 -4.4287157 -4.4288135 -4.428895 -4.4289546 -4.4289889][-4.4290195 -4.4289713 -4.4288859 -4.4287391 -4.4285679 -4.4283886 -4.4282393 -4.4282184 -4.4283848 -4.4286013 -4.4287515 -4.4288554 -4.428926 -4.4289746 -4.4290028][-4.4290247 -4.4289737 -4.4288807 -4.4287271 -4.4285407 -4.428339 -4.4281931 -4.4282093 -4.4284191 -4.4286556 -4.4288092 -4.4289041 -4.4289579 -4.4289951 -4.4290161][-4.4290342 -4.42899 -4.428905 -4.4287634 -4.4285808 -4.4283881 -4.428266 -4.4283037 -4.4285092 -4.4287271 -4.4288659 -4.428946 -4.4289865 -4.4290109 -4.4290261][-4.4290376 -4.4290013 -4.4289351 -4.4288177 -4.4286561 -4.4284921 -4.4283986 -4.428442 -4.4286103 -4.4287906 -4.4289093 -4.428978 -4.429008 -4.4290223 -4.4290323][-4.4290385 -4.4290094 -4.4289594 -4.4288683 -4.4287372 -4.4286089 -4.4285417 -4.4285822 -4.4287052 -4.42884 -4.4289365 -4.4289937 -4.4290171 -4.4290242 -4.4290328][-4.429038 -4.4290152 -4.4289756 -4.428906 -4.428803 -4.4287081 -4.4286666 -4.4287052 -4.4287896 -4.4288836 -4.4289541 -4.4289947 -4.4290118 -4.4290185 -4.42903][-4.4290376 -4.429019 -4.4289894 -4.4289351 -4.4288559 -4.4287872 -4.4287648 -4.4287996 -4.4288588 -4.4289212 -4.4289646 -4.4289889 -4.4290047 -4.4290161 -4.4290304][-4.4290376 -4.4290214 -4.4290004 -4.42896 -4.4288993 -4.4288497 -4.4288373 -4.428864 -4.4289083 -4.42895 -4.4289761 -4.42899 -4.4290032 -4.42902 -4.4290357][-4.4290361 -4.4290237 -4.429008 -4.4289808 -4.4289384 -4.4289021 -4.4288926 -4.4289126 -4.4289441 -4.42897 -4.428987 -4.4289961 -4.429009 -4.4290247 -4.4290371]]...]
INFO - root - 2017-12-10 06:13:05.181693: step 13410, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:09m:07s remains)
INFO - root - 2017-12-10 06:13:07.840742: step 13420, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:57m:58s remains)
INFO - root - 2017-12-10 06:13:10.562326: step 13430, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:37m:13s remains)
INFO - root - 2017-12-10 06:13:13.173537: step 13440, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:51m:55s remains)
INFO - root - 2017-12-10 06:13:15.892375: step 13450, loss = 2.28, batch loss = 2.23 (28.2 examples/sec; 0.284 sec/batch; 25h:08m:03s remains)
INFO - root - 2017-12-10 06:13:18.520326: step 13460, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:47m:34s remains)
INFO - root - 2017-12-10 06:13:21.149122: step 13470, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:06m:57s remains)
INFO - root - 2017-12-10 06:13:23.816389: step 13480, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 23h:54m:02s remains)
INFO - root - 2017-12-10 06:13:26.461429: step 13490, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:35m:31s remains)
INFO - root - 2017-12-10 06:13:29.147197: step 13500, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:58m:09s remains)
2017-12-10 06:13:29.440478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289312 -4.4289412 -4.4289451 -4.4289446 -4.4289403 -4.4289365 -4.4289246 -4.4289031 -4.4288716 -4.428844 -4.4288268 -4.4288111 -4.4288025 -4.4288163 -4.4288354][-4.42895 -4.4289541 -4.4289508 -4.4289441 -4.4289384 -4.4289384 -4.4289293 -4.4289036 -4.4288673 -4.4288406 -4.4288273 -4.4288125 -4.4288073 -4.4288316 -4.4288549][-4.4289546 -4.4289517 -4.4289389 -4.4289227 -4.4289136 -4.428915 -4.4289069 -4.4288783 -4.428843 -4.4288297 -4.4288359 -4.4288349 -4.4288387 -4.4288645 -4.4288821][-4.4289517 -4.4289422 -4.4289188 -4.4288926 -4.4288726 -4.4288645 -4.4288497 -4.4288163 -4.4287882 -4.4287972 -4.4288325 -4.4288549 -4.4288712 -4.4288945 -4.4289036][-4.4289312 -4.4289126 -4.4288769 -4.4288344 -4.4287968 -4.4287715 -4.4287462 -4.4287105 -4.4286928 -4.4287243 -4.4287882 -4.4288383 -4.4288731 -4.4289036 -4.4289145][-4.428896 -4.4288673 -4.4288182 -4.428762 -4.4287062 -4.4286604 -4.4286146 -4.4285688 -4.4285645 -4.4286237 -4.4287181 -4.4287963 -4.4288497 -4.4288936 -4.4289136][-4.42888 -4.4288449 -4.4287887 -4.4287286 -4.4286647 -4.4285994 -4.4285226 -4.428453 -4.4284563 -4.4285431 -4.4286661 -4.4287682 -4.4288344 -4.4288845 -4.4289112][-4.4288859 -4.428853 -4.4288039 -4.42875 -4.4286847 -4.4286122 -4.4285226 -4.428441 -4.428441 -4.4285345 -4.4286661 -4.4287772 -4.4288445 -4.4288926 -4.4289169][-4.4288793 -4.4288483 -4.4288068 -4.428762 -4.4287052 -4.428647 -4.4285831 -4.4285278 -4.4285274 -4.4285989 -4.4287066 -4.4288034 -4.4288616 -4.4289036 -4.4289227][-4.4288759 -4.4288478 -4.4288092 -4.4287686 -4.4287248 -4.428689 -4.4286566 -4.4286327 -4.428638 -4.4286871 -4.4287605 -4.4288325 -4.4288764 -4.4289117 -4.4289284][-4.4288831 -4.4288616 -4.4288273 -4.4287891 -4.4287567 -4.4287376 -4.4287248 -4.428719 -4.4287305 -4.4287672 -4.4288125 -4.4288578 -4.4288845 -4.4289112 -4.4289255][-4.428895 -4.4288793 -4.428853 -4.4288211 -4.4287953 -4.4287834 -4.4287767 -4.4287715 -4.428782 -4.42881 -4.4288392 -4.4288654 -4.4288788 -4.4288964 -4.4289088][-4.4288845 -4.4288707 -4.4288507 -4.4288239 -4.4288015 -4.4287891 -4.4287815 -4.4287739 -4.4287825 -4.4288058 -4.4288244 -4.42884 -4.4288478 -4.4288607 -4.428875][-4.4288468 -4.4288316 -4.4288116 -4.42879 -4.428771 -4.4287596 -4.4287543 -4.428751 -4.428761 -4.42878 -4.4287925 -4.4288044 -4.428813 -4.4288273 -4.4288445][-4.4288111 -4.4287958 -4.4287758 -4.4287605 -4.4287481 -4.4287415 -4.4287415 -4.4287438 -4.4287539 -4.4287667 -4.4287744 -4.4287844 -4.4287944 -4.4288082 -4.4288268]]...]
INFO - root - 2017-12-10 06:13:32.081858: step 13510, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:38m:40s remains)
INFO - root - 2017-12-10 06:13:34.725796: step 13520, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:09m:34s remains)
INFO - root - 2017-12-10 06:13:37.408286: step 13530, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:29m:28s remains)
INFO - root - 2017-12-10 06:13:40.066807: step 13540, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:41m:06s remains)
INFO - root - 2017-12-10 06:13:42.716970: step 13550, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 22h:59m:26s remains)
INFO - root - 2017-12-10 06:13:45.348980: step 13560, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:36m:49s remains)
INFO - root - 2017-12-10 06:13:47.974310: step 13570, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 22h:43m:45s remains)
INFO - root - 2017-12-10 06:13:50.649372: step 13580, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:56m:06s remains)
INFO - root - 2017-12-10 06:13:53.355634: step 13590, loss = 2.28, batch loss = 2.23 (27.2 examples/sec; 0.294 sec/batch; 26h:04m:31s remains)
INFO - root - 2017-12-10 06:13:56.057890: step 13600, loss = 2.28, batch loss = 2.23 (27.3 examples/sec; 0.294 sec/batch; 26h:00m:16s remains)
2017-12-10 06:13:56.361528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286842 -4.4287162 -4.4287062 -4.4286466 -4.428596 -4.4285603 -4.4285297 -4.4285073 -4.4284554 -4.4283948 -4.428401 -4.4284458 -4.42848 -4.4285216 -4.4285545][-4.4286718 -4.4287014 -4.4286861 -4.4286175 -4.42855 -4.4285011 -4.428472 -4.4284577 -4.4284449 -4.4284277 -4.428441 -4.4284554 -4.4284372 -4.4284472 -4.4284849][-4.4286623 -4.4286876 -4.4286661 -4.4285917 -4.42851 -4.4284487 -4.4284182 -4.4284205 -4.4284482 -4.4284792 -4.428504 -4.4284935 -4.4284348 -4.4284244 -4.4284663][-4.4286618 -4.4286833 -4.4286518 -4.4285712 -4.4284797 -4.4284058 -4.4283719 -4.4283834 -4.4284472 -4.4285154 -4.4285555 -4.428535 -4.4284577 -4.4284263 -4.4284592][-4.4286666 -4.428688 -4.4286528 -4.4285665 -4.4284639 -4.4283705 -4.4283042 -4.4283071 -4.4284129 -4.4285288 -4.4285903 -4.4285717 -4.4284949 -4.4284368 -4.4284477][-4.4286709 -4.4286861 -4.4286547 -4.4285688 -4.4284477 -4.428319 -4.4281826 -4.4281678 -4.4283323 -4.4285069 -4.4286 -4.4286013 -4.4285445 -4.4284806 -4.42847][-4.4286761 -4.4286838 -4.4286509 -4.4285626 -4.4284296 -4.4282594 -4.4280438 -4.4280176 -4.4282484 -4.4284658 -4.4285874 -4.4286227 -4.4286041 -4.4285636 -4.4285412][-4.4286842 -4.428689 -4.4286571 -4.4285707 -4.4284348 -4.4282603 -4.4280419 -4.4280314 -4.4282494 -4.4284492 -4.428575 -4.4286332 -4.4286566 -4.4286442 -4.428616][-4.4286842 -4.428679 -4.4286509 -4.4285851 -4.4284787 -4.4283476 -4.4282074 -4.4282107 -4.4283385 -4.4284682 -4.4285746 -4.4286423 -4.4286923 -4.428709 -4.4286857][-4.4286861 -4.4286656 -4.4286375 -4.4285874 -4.4285197 -4.4284472 -4.42838 -4.4283814 -4.4284334 -4.4284973 -4.4285731 -4.4286342 -4.428699 -4.4287357 -4.4287186][-4.4286895 -4.4286623 -4.4286323 -4.4285865 -4.4285412 -4.4285107 -4.4284759 -4.4284668 -4.42848 -4.4285111 -4.4285669 -4.4286232 -4.4286828 -4.42872 -4.4287052][-4.4286819 -4.4286604 -4.4286242 -4.428575 -4.4285512 -4.4285502 -4.4285312 -4.4285083 -4.4285026 -4.4285274 -4.4285712 -4.4286194 -4.4286628 -4.4286847 -4.4286656][-4.42864 -4.428627 -4.4285903 -4.428546 -4.4285469 -4.4285812 -4.4285812 -4.4285536 -4.4285355 -4.428555 -4.4285827 -4.428616 -4.4286456 -4.4286556 -4.4286318][-4.4285603 -4.4285684 -4.4285488 -4.4285121 -4.4285283 -4.4285884 -4.4285984 -4.4285779 -4.4285622 -4.4285741 -4.4285932 -4.4286141 -4.428628 -4.4286323 -4.4286013][-4.4285283 -4.428556 -4.4285488 -4.4285073 -4.4285245 -4.42858 -4.4285803 -4.4285564 -4.4285588 -4.4285803 -4.4286041 -4.4286284 -4.4286427 -4.428648 -4.42861]]...]
INFO - root - 2017-12-10 06:13:58.965114: step 13610, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:31m:21s remains)
INFO - root - 2017-12-10 06:14:01.618117: step 13620, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 23h:30m:33s remains)
INFO - root - 2017-12-10 06:14:04.310744: step 13630, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:36m:01s remains)
INFO - root - 2017-12-10 06:14:06.948999: step 13640, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 25h:12m:43s remains)
INFO - root - 2017-12-10 06:14:09.616753: step 13650, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:45m:39s remains)
INFO - root - 2017-12-10 06:14:12.232949: step 13660, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:45m:10s remains)
INFO - root - 2017-12-10 06:14:14.832684: step 13670, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 23h:48m:35s remains)
INFO - root - 2017-12-10 06:14:17.495088: step 13680, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 22h:44m:27s remains)
INFO - root - 2017-12-10 06:14:20.199150: step 13690, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:32m:50s remains)
INFO - root - 2017-12-10 06:14:22.927350: step 13700, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 23h:59m:23s remains)
2017-12-10 06:14:23.204713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288836 -4.4288797 -4.4288754 -4.4288549 -4.4288354 -4.4288378 -4.4288535 -4.428875 -4.4288912 -4.42889 -4.4288645 -4.4288039 -4.4287186 -4.4286394 -4.4285641][-4.4288945 -4.4288816 -4.4288626 -4.4288278 -4.4287992 -4.4288025 -4.428823 -4.4288497 -4.4288669 -4.428863 -4.4288368 -4.4287753 -4.428688 -4.4286075 -4.4285297][-4.4288974 -4.4288654 -4.4288306 -4.4287858 -4.4287567 -4.4287653 -4.4287877 -4.4288173 -4.4288387 -4.4288383 -4.4288158 -4.4287639 -4.4286876 -4.4286146 -4.4285417][-4.4288855 -4.4288254 -4.4287691 -4.4287119 -4.4286857 -4.4287019 -4.4287357 -4.4287777 -4.4288087 -4.4288211 -4.4288125 -4.42877 -4.4287086 -4.4286513 -4.4285922][-4.42884 -4.4287539 -4.428679 -4.4286175 -4.4286022 -4.4286313 -4.4286828 -4.4287391 -4.4287772 -4.428803 -4.4288068 -4.4287734 -4.4287267 -4.4286861 -4.4286456][-4.4287939 -4.4286966 -4.42862 -4.4285717 -4.428575 -4.428616 -4.4286747 -4.4287276 -4.4287615 -4.42879 -4.4287982 -4.428771 -4.4287353 -4.4287143 -4.4286976][-4.42877 -4.42867 -4.4285979 -4.4285622 -4.4285803 -4.4286308 -4.4286809 -4.4287076 -4.4287248 -4.4287438 -4.4287496 -4.4287362 -4.4287252 -4.4287367 -4.4287491][-4.4287672 -4.4286695 -4.4286051 -4.4285841 -4.4286127 -4.428669 -4.4287057 -4.4286976 -4.4286828 -4.4286804 -4.4286857 -4.4286995 -4.4287229 -4.4287667 -4.42879][-4.4287663 -4.4286847 -4.4286389 -4.4286408 -4.4286766 -4.4287286 -4.4287562 -4.428719 -4.428669 -4.428648 -4.4286656 -4.428709 -4.4287615 -4.4288168 -4.4288306][-4.4287477 -4.4286942 -4.4286823 -4.4287114 -4.4287562 -4.4288049 -4.4288149 -4.4287515 -4.4286776 -4.4286556 -4.4286923 -4.4287596 -4.4288235 -4.4288683 -4.4288611][-4.4287128 -4.4286909 -4.4287171 -4.4287705 -4.4288292 -4.4288769 -4.4288731 -4.4287953 -4.4287109 -4.4286861 -4.4287329 -4.4288125 -4.4288745 -4.4288979 -4.4288678][-4.4286833 -4.428689 -4.4287415 -4.428813 -4.4288812 -4.4289284 -4.4289203 -4.428844 -4.4287615 -4.4287357 -4.4287782 -4.4288459 -4.4288898 -4.42889 -4.4288507][-4.4287014 -4.428731 -4.4287963 -4.4288731 -4.4289374 -4.4289742 -4.4289637 -4.4289 -4.4288273 -4.4288044 -4.4288344 -4.4288688 -4.4288783 -4.4288611 -4.4288244][-4.428761 -4.4288116 -4.428885 -4.4289527 -4.4289975 -4.4290128 -4.4289951 -4.4289451 -4.4288907 -4.4288754 -4.4288921 -4.4288878 -4.428863 -4.4288354 -4.4288087][-4.4288464 -4.4289079 -4.4289756 -4.4290257 -4.4290476 -4.4290423 -4.4290137 -4.4289727 -4.4289322 -4.4289203 -4.4289255 -4.4288921 -4.4288449 -4.4288139 -4.4287949]]...]
INFO - root - 2017-12-10 06:14:25.873051: step 13710, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:15m:38s remains)
INFO - root - 2017-12-10 06:14:28.572523: step 13720, loss = 2.28, batch loss = 2.23 (28.0 examples/sec; 0.285 sec/batch; 25h:15m:46s remains)
INFO - root - 2017-12-10 06:14:31.225946: step 13730, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:13m:38s remains)
INFO - root - 2017-12-10 06:14:33.977811: step 13740, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:56m:54s remains)
INFO - root - 2017-12-10 06:14:36.669217: step 13750, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:23m:35s remains)
INFO - root - 2017-12-10 06:14:39.365985: step 13760, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:42m:44s remains)
INFO - root - 2017-12-10 06:14:41.997741: step 13770, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:45m:34s remains)
INFO - root - 2017-12-10 06:14:44.620856: step 13780, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:24m:05s remains)
INFO - root - 2017-12-10 06:14:47.274830: step 13790, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:49m:16s remains)
INFO - root - 2017-12-10 06:14:49.943061: step 13800, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:30m:48s remains)
2017-12-10 06:14:50.228223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428803 -4.4287758 -4.4287715 -4.4287782 -4.4287753 -4.4287734 -4.4287667 -4.4287729 -4.4287987 -4.428843 -4.4288816 -4.4288969 -4.4288945 -4.4288883 -4.4288764][-4.4287744 -4.4287353 -4.4287248 -4.4287338 -4.4287395 -4.4287357 -4.4287114 -4.4286957 -4.4287205 -4.428782 -4.4288459 -4.4288745 -4.4288712 -4.4288659 -4.4288521][-4.4287591 -4.4287076 -4.4286952 -4.4287038 -4.4287157 -4.4287105 -4.4286647 -4.4286156 -4.42862 -4.4287 -4.4288044 -4.4288521 -4.4288526 -4.42885 -4.4288397][-4.4287448 -4.4286866 -4.4286771 -4.4286861 -4.4286947 -4.4286804 -4.4286079 -4.4285121 -4.4284778 -4.4285851 -4.4287415 -4.4288187 -4.4288235 -4.4288268 -4.4288239][-4.4287491 -4.4286909 -4.4286871 -4.4286885 -4.4286757 -4.4286413 -4.4285355 -4.4283791 -4.4282975 -4.4284468 -4.4286656 -4.42877 -4.428782 -4.4287872 -4.428791][-4.428772 -4.428721 -4.4287143 -4.4286971 -4.4286585 -4.4286041 -4.4284649 -4.4282351 -4.4281011 -4.4283047 -4.42859 -4.42873 -4.428751 -4.4287529 -4.428761][-4.4288177 -4.4287777 -4.4287534 -4.4287047 -4.4286432 -4.4285812 -4.4284315 -4.428153 -4.4279714 -4.4282045 -4.4285364 -4.4287086 -4.4287448 -4.4287381 -4.4287372][-4.4288535 -4.4288273 -4.4287853 -4.4287086 -4.428637 -4.4285917 -4.4284654 -4.4282017 -4.428031 -4.4282384 -4.428535 -4.4287014 -4.4287477 -4.4287281 -4.4287124][-4.4288607 -4.4288464 -4.428803 -4.428709 -4.4286375 -4.4286294 -4.4285545 -4.4283667 -4.4282608 -4.4284124 -4.4286094 -4.428719 -4.4287515 -4.4287262 -4.4287033][-4.4288597 -4.4288549 -4.4288168 -4.428719 -4.4286537 -4.4286752 -4.4286542 -4.428546 -4.4284954 -4.4286013 -4.4287076 -4.4287543 -4.4287562 -4.4287343 -4.4287252][-4.4288759 -4.4288807 -4.4288483 -4.4287696 -4.4287181 -4.4287505 -4.4287624 -4.4287162 -4.4286995 -4.4287648 -4.4288034 -4.4288025 -4.4287715 -4.4287515 -4.4287672][-4.4288979 -4.4289017 -4.4288716 -4.4288206 -4.4287925 -4.4288235 -4.4288416 -4.4288263 -4.4288197 -4.4288464 -4.4288516 -4.4288282 -4.4287777 -4.4287682 -4.428812][-4.4289088 -4.4289031 -4.4288678 -4.4288354 -4.4288321 -4.428863 -4.4288812 -4.4288735 -4.4288592 -4.4288578 -4.428843 -4.4288063 -4.4287553 -4.428771 -4.4288387][-4.4289112 -4.428895 -4.4288554 -4.4288278 -4.4288421 -4.4288778 -4.4289021 -4.4289041 -4.4288912 -4.4288769 -4.4288397 -4.4287906 -4.4287543 -4.428792 -4.42887][-4.428916 -4.4288926 -4.4288487 -4.4288249 -4.4288545 -4.4289041 -4.4289351 -4.4289465 -4.4289455 -4.4289317 -4.4288907 -4.4288449 -4.42882 -4.4288492 -4.4289036]]...]
INFO - root - 2017-12-10 06:14:52.861277: step 13810, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:35m:29s remains)
INFO - root - 2017-12-10 06:14:55.494256: step 13820, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:09m:22s remains)
INFO - root - 2017-12-10 06:14:58.126716: step 13830, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:33m:40s remains)
INFO - root - 2017-12-10 06:15:00.795782: step 13840, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:48m:26s remains)
INFO - root - 2017-12-10 06:15:03.468061: step 13850, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.284 sec/batch; 25h:10m:53s remains)
INFO - root - 2017-12-10 06:15:06.124173: step 13860, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:32m:52s remains)
INFO - root - 2017-12-10 06:15:08.795932: step 13870, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:48m:25s remains)
INFO - root - 2017-12-10 06:15:11.455622: step 13880, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:27m:02s remains)
INFO - root - 2017-12-10 06:15:14.136945: step 13890, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:42m:06s remains)
INFO - root - 2017-12-10 06:15:16.803629: step 13900, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 22h:36m:31s remains)
2017-12-10 06:15:17.098243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287777 -4.4288 -4.4288054 -4.4287934 -4.4287796 -4.4287481 -4.428721 -4.4287047 -4.4286823 -4.42863 -4.4286089 -4.4286475 -4.4286938 -4.4287281 -4.428761][-4.4288116 -4.4288273 -4.4288354 -4.4288349 -4.4288278 -4.4287996 -4.4287744 -4.4287529 -4.4287219 -4.4286733 -4.4286618 -4.4287038 -4.4287505 -4.4287934 -4.4288378][-4.4288025 -4.428822 -4.4288449 -4.4288607 -4.4288549 -4.4288235 -4.4287958 -4.4287658 -4.4287291 -4.428689 -4.4286928 -4.4287362 -4.4287772 -4.4288182 -4.4288592][-4.4287839 -4.4288044 -4.4288378 -4.4288616 -4.4288535 -4.4288125 -4.4287753 -4.4287424 -4.4287143 -4.4286914 -4.4287076 -4.4287515 -4.428782 -4.428802 -4.4288168][-4.4287925 -4.4288015 -4.4288292 -4.4288492 -4.4288297 -4.4287672 -4.4287124 -4.42868 -4.4286671 -4.428668 -4.4287028 -4.4287577 -4.4287877 -4.428782 -4.4287605][-4.4288125 -4.4288144 -4.4288368 -4.4288516 -4.4288163 -4.4287267 -4.4286456 -4.4286041 -4.4285951 -4.4286189 -4.4286737 -4.4287453 -4.4287825 -4.4287667 -4.4287271][-4.428822 -4.4288263 -4.4288492 -4.4288607 -4.4288177 -4.4287119 -4.4286075 -4.4285469 -4.4285316 -4.4285669 -4.4286361 -4.4287143 -4.4287534 -4.4287338 -4.4286957][-4.4288263 -4.4288316 -4.4288559 -4.4288611 -4.4288158 -4.4287057 -4.4285922 -4.4285331 -4.4285226 -4.42856 -4.4286222 -4.4286895 -4.4287167 -4.4286957 -4.4286728][-4.4288335 -4.4288425 -4.4288621 -4.4288592 -4.4288116 -4.4287024 -4.4285879 -4.42854 -4.4285398 -4.4285736 -4.4286313 -4.42869 -4.4287086 -4.428688 -4.4286733][-4.4288363 -4.4288473 -4.42886 -4.4288507 -4.4288034 -4.4287095 -4.4286122 -4.4285755 -4.4285808 -4.4286118 -4.4286618 -4.4287071 -4.4287229 -4.4287062 -4.4286904][-4.4288325 -4.4288392 -4.4288387 -4.4288192 -4.4287777 -4.4287143 -4.4286542 -4.428638 -4.4286518 -4.4286804 -4.4287162 -4.4287434 -4.4287543 -4.4287419 -4.4287233][-4.4288149 -4.4288082 -4.428793 -4.4287653 -4.4287338 -4.4287014 -4.4286723 -4.4286714 -4.4286962 -4.4287314 -4.4287629 -4.4287796 -4.4287782 -4.4287596 -4.4287343][-4.42879 -4.428772 -4.428751 -4.4287238 -4.4287019 -4.4286909 -4.4286809 -4.4286842 -4.4287124 -4.4287515 -4.4287796 -4.4287868 -4.4287724 -4.4287391 -4.42871][-4.4287677 -4.4287524 -4.4287353 -4.4287162 -4.4287062 -4.4287076 -4.4287076 -4.4287157 -4.4287453 -4.4287815 -4.4288073 -4.4288149 -4.4287887 -4.42874 -4.4287009][-4.428781 -4.4287705 -4.4287567 -4.4287429 -4.4287386 -4.4287481 -4.4287567 -4.42877 -4.4287972 -4.42883 -4.4288535 -4.4288554 -4.4288225 -4.4287682 -4.4287162]]...]
INFO - root - 2017-12-10 06:15:19.779831: step 13910, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:05m:14s remains)
INFO - root - 2017-12-10 06:15:22.433799: step 13920, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 22h:59m:08s remains)
INFO - root - 2017-12-10 06:15:25.071696: step 13930, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:03m:25s remains)
INFO - root - 2017-12-10 06:15:27.724414: step 13940, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:42m:17s remains)
INFO - root - 2017-12-10 06:15:30.368924: step 13950, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:13m:46s remains)
INFO - root - 2017-12-10 06:15:33.047018: step 13960, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:44m:53s remains)
INFO - root - 2017-12-10 06:15:35.728253: step 13970, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:09m:12s remains)
INFO - root - 2017-12-10 06:15:38.407027: step 13980, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:19m:42s remains)
INFO - root - 2017-12-10 06:15:41.054729: step 13990, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 25h:46m:05s remains)
INFO - root - 2017-12-10 06:15:43.701055: step 14000, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:34m:18s remains)
2017-12-10 06:15:43.994055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428834 -4.4288177 -4.4288149 -4.4288197 -4.4288273 -4.4288287 -4.4288392 -4.4288592 -4.4288716 -4.4288445 -4.4287882 -4.4287481 -4.4287477 -4.4287696 -4.4287753][-4.428834 -4.4288306 -4.4288287 -4.4288311 -4.4288344 -4.428833 -4.4288387 -4.4288559 -4.4288588 -4.4288087 -4.4287238 -4.428678 -4.4286995 -4.4287438 -4.4287629][-4.4288359 -4.42884 -4.4288359 -4.4288321 -4.4288306 -4.4288278 -4.4288249 -4.4288282 -4.4288082 -4.4287252 -4.4286156 -4.4285765 -4.428628 -4.4286995 -4.4287362][-4.4288311 -4.4288421 -4.428843 -4.42884 -4.4288344 -4.4288244 -4.4288044 -4.4287767 -4.4287219 -4.4286056 -4.4284935 -4.4284925 -4.4285865 -4.4286771 -4.42873][-4.4288225 -4.4288363 -4.4288411 -4.4288363 -4.428823 -4.4288006 -4.4287519 -4.4286909 -4.4286079 -4.4284949 -4.4284344 -4.4284983 -4.42861 -4.4286909 -4.4287419][-4.428823 -4.4288316 -4.4288244 -4.4288068 -4.4287791 -4.428731 -4.4286366 -4.4285231 -4.4284205 -4.4283676 -4.428411 -4.4285369 -4.4286485 -4.4287152 -4.4287524][-4.4288211 -4.428812 -4.4287806 -4.4287381 -4.4286895 -4.428597 -4.4284329 -4.428257 -4.428174 -4.4282308 -4.4283876 -4.4285493 -4.4286547 -4.4287057 -4.42873][-4.4288225 -4.4287953 -4.4287376 -4.4286728 -4.4286036 -4.428473 -4.4282579 -4.4280891 -4.4281006 -4.4282613 -4.428441 -4.4285784 -4.4286518 -4.42868 -4.428689][-4.4288473 -4.4288149 -4.4287562 -4.4286895 -4.4286175 -4.428483 -4.4282928 -4.4282107 -4.4282961 -4.4284458 -4.4285655 -4.4286451 -4.4286871 -4.4287038 -4.4287114][-4.4288769 -4.42885 -4.428802 -4.4287481 -4.4286866 -4.4285741 -4.4284558 -4.4284549 -4.4285445 -4.428637 -4.4286966 -4.4287348 -4.4287615 -4.428772 -4.4287777][-4.4289079 -4.42889 -4.428854 -4.4288106 -4.4287658 -4.4286914 -4.4286404 -4.4286733 -4.4287348 -4.4287791 -4.4288011 -4.4288149 -4.4288325 -4.4288387 -4.42884][-4.4289346 -4.4289265 -4.4289002 -4.4288616 -4.4288292 -4.4287934 -4.4287786 -4.4288106 -4.4288445 -4.4288626 -4.4288645 -4.4288664 -4.4288735 -4.4288731 -4.4288697][-4.4289436 -4.4289403 -4.4289227 -4.4288869 -4.4288592 -4.4288425 -4.4288383 -4.4288592 -4.428875 -4.4288821 -4.4288774 -4.428875 -4.4288731 -4.4288692 -4.4288654][-4.4289331 -4.428925 -4.4289131 -4.4288836 -4.4288573 -4.4288449 -4.4288435 -4.4288597 -4.4288721 -4.4288764 -4.42887 -4.428865 -4.4288583 -4.4288526 -4.4288497][-4.42892 -4.4289045 -4.4288926 -4.4288726 -4.428853 -4.428843 -4.4288483 -4.4288635 -4.4288731 -4.4288735 -4.4288678 -4.4288621 -4.4288573 -4.4288564 -4.4288535]]...]
INFO - root - 2017-12-10 06:15:46.654859: step 14010, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:00m:37s remains)
INFO - root - 2017-12-10 06:15:49.326581: step 14020, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:10m:35s remains)
INFO - root - 2017-12-10 06:15:52.021168: step 14030, loss = 2.28, batch loss = 2.23 (27.4 examples/sec; 0.292 sec/batch; 25h:48m:12s remains)
INFO - root - 2017-12-10 06:15:54.670908: step 14040, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 22h:56m:55s remains)
INFO - root - 2017-12-10 06:15:57.303642: step 14050, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 23h:38m:48s remains)
INFO - root - 2017-12-10 06:16:00.001061: step 14060, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:35m:20s remains)
INFO - root - 2017-12-10 06:16:02.639044: step 14070, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:55m:07s remains)
INFO - root - 2017-12-10 06:16:05.318794: step 14080, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:19m:48s remains)
INFO - root - 2017-12-10 06:16:07.945689: step 14090, loss = 2.28, batch loss = 2.23 (28.5 examples/sec; 0.280 sec/batch; 24h:47m:02s remains)
INFO - root - 2017-12-10 06:16:10.582298: step 14100, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:43m:23s remains)
2017-12-10 06:16:10.875816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288673 -4.428875 -4.4288836 -4.4288883 -4.4288926 -4.4288859 -4.4288669 -4.4288459 -4.4288177 -4.4287891 -4.428762 -4.428761 -4.4287887 -4.4288096 -4.4288282][-4.4288187 -4.4288106 -4.4288139 -4.4288182 -4.4288268 -4.4288216 -4.428802 -4.4287782 -4.4287443 -4.4287086 -4.4286776 -4.4286814 -4.4287162 -4.4287448 -4.428772][-4.4287672 -4.4287415 -4.4287348 -4.4287348 -4.4287443 -4.4287424 -4.42873 -4.4287119 -4.4286814 -4.4286528 -4.4286342 -4.4286456 -4.428678 -4.4287024 -4.4287343][-4.4286962 -4.4286604 -4.428647 -4.4286427 -4.4286475 -4.4286437 -4.4286356 -4.4286227 -4.4286027 -4.4285936 -4.4286003 -4.4286289 -4.4286652 -4.428688 -4.4287243][-4.4286437 -4.4285965 -4.4285731 -4.4285645 -4.4285688 -4.4285622 -4.4285488 -4.4285274 -4.4285169 -4.4285388 -4.4285774 -4.4286289 -4.4286785 -4.4287076 -4.4287453][-4.4286189 -4.4285555 -4.4285121 -4.4284878 -4.4284773 -4.428463 -4.4284363 -4.4283953 -4.4283862 -4.428442 -4.4285283 -4.4286084 -4.4286771 -4.4287224 -4.4287715][-4.4285951 -4.4285231 -4.4284625 -4.4284139 -4.42838 -4.4283543 -4.4283204 -4.4282589 -4.4282451 -4.428328 -4.4284558 -4.4285631 -4.4286513 -4.4287181 -4.4287844][-4.4285917 -4.4285398 -4.4284916 -4.4284372 -4.4283924 -4.4283671 -4.4283423 -4.4282866 -4.428268 -4.42835 -4.4284749 -4.4285765 -4.4286575 -4.428731 -4.4287996][-4.4286408 -4.4286246 -4.4286089 -4.4285669 -4.4285293 -4.4285092 -4.4284921 -4.4284515 -4.428432 -4.4284911 -4.4285793 -4.428647 -4.4287009 -4.4287596 -4.428813][-4.4287338 -4.428741 -4.4287467 -4.4287243 -4.4287 -4.4286852 -4.42867 -4.4286437 -4.4286237 -4.4286494 -4.4286957 -4.4287281 -4.4287586 -4.4287982 -4.4288335][-4.4288335 -4.4288492 -4.4288626 -4.4288545 -4.4288435 -4.428834 -4.4288225 -4.4288092 -4.4287915 -4.4287996 -4.4288211 -4.4288287 -4.4288397 -4.4288607 -4.4288754][-4.4289088 -4.4289207 -4.4289351 -4.4289289 -4.428926 -4.4289279 -4.428925 -4.4289165 -4.4289 -4.4289002 -4.4289117 -4.42891 -4.4289079 -4.428916 -4.4289174][-4.4289203 -4.4289227 -4.4289322 -4.4289293 -4.4289336 -4.4289432 -4.4289455 -4.4289351 -4.4289188 -4.4289212 -4.428937 -4.4289384 -4.4289355 -4.4289412 -4.4289432][-4.4288917 -4.4288921 -4.4288993 -4.4289002 -4.4289079 -4.4289188 -4.4289246 -4.4289169 -4.4289026 -4.428905 -4.4289236 -4.4289289 -4.4289293 -4.4289379 -4.4289474][-4.4288955 -4.4288974 -4.4289041 -4.4289074 -4.4289122 -4.4289203 -4.4289246 -4.42892 -4.4289107 -4.4289103 -4.4289207 -4.4289265 -4.42893 -4.4289384 -4.4289517]]...]
INFO - root - 2017-12-10 06:16:13.512164: step 14110, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:51m:55s remains)
INFO - root - 2017-12-10 06:16:16.130260: step 14120, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:28m:36s remains)
INFO - root - 2017-12-10 06:16:18.785260: step 14130, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:31m:44s remains)
INFO - root - 2017-12-10 06:16:21.456639: step 14140, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:36m:08s remains)
INFO - root - 2017-12-10 06:16:24.118618: step 14150, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:39m:03s remains)
INFO - root - 2017-12-10 06:16:26.767714: step 14160, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:21m:43s remains)
INFO - root - 2017-12-10 06:16:29.436764: step 14170, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:40m:04s remains)
INFO - root - 2017-12-10 06:16:32.089136: step 14180, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 23h:51m:03s remains)
INFO - root - 2017-12-10 06:16:34.774440: step 14190, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:26m:27s remains)
INFO - root - 2017-12-10 06:16:37.430964: step 14200, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:45m:57s remains)
2017-12-10 06:16:37.749446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289742 -4.4289703 -4.4289742 -4.4289718 -4.4289613 -4.4289436 -4.4289322 -4.4289312 -4.4289351 -4.4289446 -4.4289532 -4.4289551 -4.4289494 -4.4289503 -4.4289575][-4.4289584 -4.4289522 -4.4289541 -4.4289517 -4.4289384 -4.4289165 -4.4289036 -4.4288988 -4.4289021 -4.4289179 -4.4289322 -4.4289384 -4.4289322 -4.4289341 -4.4289451][-4.4289336 -4.4289231 -4.4289227 -4.42892 -4.4289069 -4.4288855 -4.4288731 -4.4288645 -4.428863 -4.4288769 -4.4288926 -4.4289031 -4.4288993 -4.4289017 -4.4289145][-4.4288807 -4.428863 -4.428864 -4.428865 -4.4288521 -4.4288306 -4.4288216 -4.4288106 -4.4288011 -4.428802 -4.4288144 -4.42883 -4.4288325 -4.4288349 -4.4288492][-4.4288216 -4.4287972 -4.4287963 -4.4287982 -4.428782 -4.4287519 -4.4287329 -4.428709 -4.428688 -4.4286828 -4.428699 -4.4287267 -4.4287386 -4.4287453 -4.4287596][-4.4287934 -4.4287686 -4.428762 -4.4287548 -4.4287252 -4.42867 -4.428618 -4.4285665 -4.4285178 -4.4285107 -4.42855 -4.4285994 -4.4286294 -4.428648 -4.4286637][-4.4287996 -4.4287744 -4.4287596 -4.4287443 -4.4287033 -4.4286213 -4.428524 -4.4284244 -4.4283371 -4.428318 -4.4283795 -4.42846 -4.4285207 -4.4285636 -4.4285846][-4.4288192 -4.4287949 -4.4287767 -4.428761 -4.4287224 -4.4286375 -4.4285154 -4.4283772 -4.4282584 -4.428225 -4.4282908 -4.4283886 -4.4284735 -4.428534 -4.4285579][-4.4288282 -4.4288092 -4.4287944 -4.4287829 -4.4287581 -4.4286981 -4.4285922 -4.4284635 -4.4283495 -4.4283166 -4.428369 -4.4284549 -4.4285359 -4.4285927 -4.4286132][-4.4288216 -4.428812 -4.4288082 -4.4288058 -4.4287972 -4.4287615 -4.4286904 -4.428606 -4.4285378 -4.4285159 -4.4285455 -4.4286003 -4.4286551 -4.4286876 -4.4286923][-4.4287992 -4.428791 -4.4287977 -4.4288044 -4.4288106 -4.4288015 -4.4287696 -4.4287324 -4.4287071 -4.4287 -4.4287105 -4.4287357 -4.4287586 -4.4287639 -4.42875][-4.4287882 -4.4287744 -4.428782 -4.4287953 -4.4288111 -4.428823 -4.4288182 -4.42881 -4.4288125 -4.4288187 -4.4288216 -4.4288292 -4.428833 -4.4288225 -4.42879][-4.4288011 -4.4287724 -4.4287705 -4.4287844 -4.4287992 -4.4288177 -4.42882 -4.4288216 -4.4288373 -4.4288535 -4.4288573 -4.42886 -4.4288583 -4.4288378 -4.428793][-4.428823 -4.4287786 -4.4287653 -4.4287724 -4.4287791 -4.4287891 -4.4287839 -4.42878 -4.4287944 -4.4288125 -4.4288206 -4.4288311 -4.428834 -4.4288139 -4.4287643][-4.4288387 -4.4287877 -4.4287696 -4.4287677 -4.4287615 -4.4287591 -4.4287453 -4.4287343 -4.4287357 -4.4287477 -4.4287567 -4.4287772 -4.428793 -4.4287796 -4.428731]]...]
INFO - root - 2017-12-10 06:16:40.435302: step 14210, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:49m:05s remains)
INFO - root - 2017-12-10 06:16:43.088889: step 14220, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:53m:10s remains)
INFO - root - 2017-12-10 06:16:45.748346: step 14230, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:36m:09s remains)
INFO - root - 2017-12-10 06:16:48.427458: step 14240, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:10m:35s remains)
INFO - root - 2017-12-10 06:16:51.065465: step 14250, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:19m:05s remains)
INFO - root - 2017-12-10 06:16:53.752497: step 14260, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 25h:31m:20s remains)
INFO - root - 2017-12-10 06:16:56.399652: step 14270, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:47m:06s remains)
INFO - root - 2017-12-10 06:16:59.021536: step 14280, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:14m:06s remains)
INFO - root - 2017-12-10 06:17:01.671094: step 14290, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:07m:15s remains)
INFO - root - 2017-12-10 06:17:04.305516: step 14300, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:27m:42s remains)
2017-12-10 06:17:04.620256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289174 -4.4288993 -4.4289064 -4.4289169 -4.4289093 -4.4289045 -4.4289036 -4.4289055 -4.4288912 -4.4288726 -4.4288526 -4.4288259 -4.428803 -4.4287958 -4.428803][-4.4289246 -4.4289083 -4.4289083 -4.4289055 -4.4288874 -4.4288793 -4.42887 -4.4288688 -4.4288578 -4.4288468 -4.4288359 -4.4288068 -4.4287753 -4.428761 -4.4287615][-4.428936 -4.4289217 -4.4289126 -4.4288931 -4.4288611 -4.4288487 -4.4288297 -4.4288144 -4.4287996 -4.428793 -4.4287949 -4.4287767 -4.4287548 -4.4287486 -4.42875][-4.4289436 -4.4289303 -4.4289131 -4.4288812 -4.4288359 -4.4288096 -4.428771 -4.4287333 -4.4287171 -4.4287248 -4.4287467 -4.4287491 -4.4287453 -4.4287581 -4.4287648][-4.42895 -4.4289355 -4.4289122 -4.4288716 -4.428813 -4.4287605 -4.4286795 -4.4286 -4.4285765 -4.4286151 -4.4286819 -4.4287238 -4.4287596 -4.4287949 -4.4288015][-4.428957 -4.4289446 -4.4289217 -4.4288754 -4.4288054 -4.4287176 -4.4285712 -4.4284248 -4.4283905 -4.4284825 -4.4286189 -4.4287124 -4.4287858 -4.4288349 -4.4288397][-4.428968 -4.4289613 -4.4289427 -4.4288964 -4.4288144 -4.4286876 -4.4284625 -4.4282136 -4.4281597 -4.4283385 -4.42856 -4.4287014 -4.4288054 -4.4288659 -4.4288707][-4.4289775 -4.4289742 -4.4289608 -4.4289203 -4.4288387 -4.4286995 -4.4284396 -4.4281244 -4.4280424 -4.4282761 -4.4285469 -4.4287095 -4.4288197 -4.4288816 -4.4288864][-4.428978 -4.4289727 -4.4289603 -4.4289289 -4.4288645 -4.4287558 -4.4285502 -4.4282961 -4.428215 -4.4283962 -4.4286232 -4.428761 -4.4288492 -4.428896 -4.428896][-4.428967 -4.4289551 -4.4289403 -4.428916 -4.4288731 -4.4288111 -4.4286833 -4.4285173 -4.4284525 -4.428556 -4.4287066 -4.4288077 -4.4288735 -4.42891 -4.4289088][-4.4289503 -4.4289284 -4.4289069 -4.428884 -4.4288554 -4.4288259 -4.4287462 -4.4286327 -4.4285803 -4.4286475 -4.42875 -4.4288244 -4.4288812 -4.4289193 -4.4289222][-4.4289379 -4.428905 -4.4288697 -4.4288425 -4.4288216 -4.4288082 -4.4287524 -4.4286704 -4.4286337 -4.42869 -4.428772 -4.4288335 -4.4288898 -4.4289284 -4.4289255][-4.4289346 -4.4289 -4.4288554 -4.4288206 -4.4288 -4.4287872 -4.4287333 -4.4286637 -4.4286437 -4.4286995 -4.4287729 -4.4288297 -4.4288883 -4.428925 -4.4289122][-4.4289441 -4.4289184 -4.4288774 -4.4288373 -4.4288073 -4.4287796 -4.4287171 -4.4286504 -4.4286423 -4.4287 -4.4287682 -4.4288211 -4.4288726 -4.4289007 -4.4288893][-4.4289575 -4.4289422 -4.4289093 -4.4288688 -4.4288273 -4.4287853 -4.4287162 -4.4286551 -4.4286561 -4.4287119 -4.4287739 -4.4288173 -4.4288535 -4.4288716 -4.4288688]]...]
INFO - root - 2017-12-10 06:17:07.239953: step 14310, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:19m:04s remains)
INFO - root - 2017-12-10 06:17:09.860480: step 14320, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:23m:51s remains)
INFO - root - 2017-12-10 06:17:12.539726: step 14330, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 23h:57m:07s remains)
INFO - root - 2017-12-10 06:17:15.231337: step 14340, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:39m:22s remains)
INFO - root - 2017-12-10 06:17:17.885781: step 14350, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:39m:55s remains)
INFO - root - 2017-12-10 06:17:20.560585: step 14360, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:29m:14s remains)
INFO - root - 2017-12-10 06:17:23.249114: step 14370, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:43m:00s remains)
INFO - root - 2017-12-10 06:17:25.845758: step 14380, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:25m:37s remains)
INFO - root - 2017-12-10 06:17:28.518002: step 14390, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:52m:28s remains)
INFO - root - 2017-12-10 06:17:31.231420: step 14400, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:16m:42s remains)
2017-12-10 06:17:31.566077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428854 -4.4288483 -4.4288306 -4.42881 -4.4287868 -4.4287848 -4.42881 -4.4288425 -4.4288597 -4.4288468 -4.4288068 -4.4287486 -4.4286623 -4.4285955 -4.4285889][-4.4288411 -4.4288282 -4.4287968 -4.4287677 -4.42874 -4.4287348 -4.4287624 -4.4288034 -4.4288325 -4.4288239 -4.4287796 -4.4287071 -4.4285965 -4.4285159 -4.428515][-4.4288507 -4.4288239 -4.4287796 -4.428741 -4.4287033 -4.4286904 -4.4287114 -4.4287639 -4.4288054 -4.4288063 -4.4287643 -4.4286909 -4.4285731 -4.4284906 -4.4284983][-4.4288597 -4.4288192 -4.4287629 -4.4287162 -4.4286757 -4.4286532 -4.4286675 -4.4287262 -4.4287825 -4.4287972 -4.428772 -4.4287105 -4.4286056 -4.4285283 -4.4285336][-4.42886 -4.4288082 -4.4287472 -4.4286981 -4.4286551 -4.4286265 -4.428638 -4.4286981 -4.4287581 -4.428782 -4.4287677 -4.4287271 -4.4286556 -4.428607 -4.4286127][-4.4288597 -4.4288182 -4.428762 -4.4287071 -4.4286485 -4.4285975 -4.4285955 -4.4286461 -4.4286938 -4.4287086 -4.4286976 -4.4286828 -4.4286666 -4.4286675 -4.4286919][-4.4288688 -4.428844 -4.428802 -4.4287419 -4.4286656 -4.4285822 -4.4285479 -4.4285693 -4.4285932 -4.4285793 -4.4285541 -4.4285607 -4.4286084 -4.4286671 -4.4287238][-4.4288874 -4.4288812 -4.4288483 -4.4287839 -4.4287004 -4.4285979 -4.428535 -4.4285254 -4.4285197 -4.4284654 -4.4284 -4.4284143 -4.4285154 -4.4286256 -4.4287128][-4.4289007 -4.4289055 -4.4288826 -4.4288268 -4.4287581 -4.4286661 -4.4285941 -4.4285579 -4.4285288 -4.4284596 -4.4283733 -4.4283776 -4.4284883 -4.4286084 -4.4287][-4.4288993 -4.4289136 -4.4289041 -4.428865 -4.42882 -4.4287553 -4.4286852 -4.4286318 -4.4285932 -4.4285469 -4.4284968 -4.4285078 -4.4285765 -4.428648 -4.4287076][-4.4288921 -4.4289055 -4.4288969 -4.4288731 -4.4288487 -4.428812 -4.4287572 -4.4286985 -4.42866 -4.4286432 -4.4286451 -4.4286714 -4.4287009 -4.428719 -4.4287415][-4.4288878 -4.4288826 -4.4288673 -4.4288554 -4.4288497 -4.4288378 -4.4287996 -4.4287429 -4.4287043 -4.4287038 -4.4287276 -4.4287615 -4.42878 -4.4287791 -4.4287858][-4.4288874 -4.4288616 -4.4288344 -4.4288349 -4.4288578 -4.4288707 -4.4288521 -4.4288082 -4.428771 -4.4287634 -4.4287806 -4.4288096 -4.42883 -4.4288306 -4.4288359][-4.428905 -4.4288726 -4.4288387 -4.4288406 -4.4288721 -4.4289 -4.4289002 -4.428874 -4.4288359 -4.4288177 -4.4288259 -4.42885 -4.4288716 -4.4288788 -4.4288855][-4.4289169 -4.428896 -4.4288697 -4.4288683 -4.4288936 -4.4289203 -4.4289293 -4.4289155 -4.4288855 -4.428865 -4.4288692 -4.4288869 -4.4289036 -4.4289112 -4.4289174]]...]
INFO - root - 2017-12-10 06:17:34.221500: step 14410, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:08m:20s remains)
INFO - root - 2017-12-10 06:17:36.943954: step 14420, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 25h:31m:16s remains)
INFO - root - 2017-12-10 06:17:39.594745: step 14430, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 22h:41m:09s remains)
INFO - root - 2017-12-10 06:17:42.251523: step 14440, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 22h:45m:27s remains)
INFO - root - 2017-12-10 06:17:44.917195: step 14450, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:34m:43s remains)
INFO - root - 2017-12-10 06:17:47.585165: step 14460, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 23h:57m:33s remains)
INFO - root - 2017-12-10 06:17:50.238615: step 14470, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:39m:16s remains)
INFO - root - 2017-12-10 06:17:52.874793: step 14480, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:41m:02s remains)
INFO - root - 2017-12-10 06:17:55.531780: step 14490, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.275 sec/batch; 24h:19m:56s remains)
INFO - root - 2017-12-10 06:17:58.209331: step 14500, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:35m:03s remains)
2017-12-10 06:17:58.492150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288158 -4.4288111 -4.4287705 -4.4287238 -4.4286709 -4.4286261 -4.4285989 -4.4286022 -4.42863 -4.4286752 -4.42872 -4.4287624 -4.42877 -4.4287224 -4.4286728][-4.4288573 -4.4288454 -4.4288049 -4.4287562 -4.4286985 -4.4286394 -4.4285884 -4.4285665 -4.428576 -4.4286194 -4.42867 -4.4287219 -4.4287391 -4.4286957 -4.4286432][-4.4288821 -4.4288697 -4.4288373 -4.4287963 -4.4287376 -4.4286556 -4.4285712 -4.4285173 -4.4285173 -4.428566 -4.428628 -4.4286952 -4.4287276 -4.4286957 -4.4286456][-4.4288831 -4.4288645 -4.4288416 -4.4288182 -4.4287648 -4.4286633 -4.4285474 -4.4284673 -4.4284663 -4.4285231 -4.428596 -4.428678 -4.4287219 -4.4287081 -4.4286704][-4.4288621 -4.4288416 -4.4288311 -4.42883 -4.4287839 -4.4286718 -4.4285307 -4.428432 -4.4284396 -4.4284978 -4.4285707 -4.4286571 -4.4287143 -4.4287205 -4.4287][-4.4288373 -4.428812 -4.4288073 -4.4288182 -4.4287796 -4.4286604 -4.4284945 -4.4283886 -4.4284239 -4.4284949 -4.428555 -4.4286337 -4.4286966 -4.4287205 -4.428719][-4.4288273 -4.4287972 -4.4287834 -4.4287887 -4.4287496 -4.4286218 -4.4284391 -4.4283395 -4.4284182 -4.4285154 -4.4285579 -4.4286065 -4.4286604 -4.4286919 -4.4286981][-4.4288273 -4.4288015 -4.4287786 -4.4287705 -4.4287248 -4.4285893 -4.4283872 -4.428288 -4.4284024 -4.4285278 -4.4285631 -4.4285769 -4.4286132 -4.4286413 -4.4286494][-4.428843 -4.4288306 -4.4288073 -4.4287872 -4.42873 -4.4285889 -4.4283719 -4.4282622 -4.4283829 -4.4285207 -4.4285569 -4.4285488 -4.428575 -4.4285941 -4.428607][-4.4288654 -4.428864 -4.4288473 -4.4288206 -4.428761 -4.42863 -4.4284258 -4.4283071 -4.4283938 -4.4285154 -4.4285479 -4.428534 -4.4285569 -4.4285769 -4.4285936][-4.4288807 -4.4288831 -4.4288745 -4.428854 -4.4288025 -4.4286866 -4.4285111 -4.42839 -4.42843 -4.4285178 -4.4285407 -4.4285307 -4.4285555 -4.4285789 -4.4286051][-4.4288912 -4.4288869 -4.4288759 -4.4288645 -4.4288282 -4.4287295 -4.4285817 -4.4284644 -4.4284706 -4.4285297 -4.428546 -4.428546 -4.4285684 -4.4285903 -4.4286256][-4.4289036 -4.4288898 -4.4288712 -4.4288607 -4.4288363 -4.4287572 -4.4286366 -4.4285326 -4.4285197 -4.4285479 -4.4285569 -4.428565 -4.4285889 -4.4286094 -4.428648][-4.4289203 -4.428894 -4.4288664 -4.4288487 -4.4288306 -4.4287786 -4.4286914 -4.4286065 -4.4285765 -4.4285765 -4.4285707 -4.4285779 -4.4285994 -4.4286189 -4.4286661][-4.4289393 -4.4289036 -4.4288673 -4.4288425 -4.4288254 -4.4288 -4.4287486 -4.4286876 -4.4286447 -4.4286189 -4.4285879 -4.4285736 -4.428586 -4.4286113 -4.428669]]...]
INFO - root - 2017-12-10 06:18:01.122598: step 14510, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 22h:32m:46s remains)
INFO - root - 2017-12-10 06:18:03.798494: step 14520, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:39m:05s remains)
INFO - root - 2017-12-10 06:18:06.428403: step 14530, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:36m:13s remains)
INFO - root - 2017-12-10 06:18:09.108856: step 14540, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 24h:39m:25s remains)
INFO - root - 2017-12-10 06:18:11.716086: step 14550, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:08m:45s remains)
INFO - root - 2017-12-10 06:18:14.380023: step 14560, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:46m:26s remains)
INFO - root - 2017-12-10 06:18:17.024843: step 14570, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:21m:43s remains)
INFO - root - 2017-12-10 06:18:19.701887: step 14580, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:28m:26s remains)
INFO - root - 2017-12-10 06:18:22.369197: step 14590, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:45m:33s remains)
INFO - root - 2017-12-10 06:18:24.993677: step 14600, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:34m:35s remains)
2017-12-10 06:18:25.323835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287853 -4.4288235 -4.4288363 -4.4288278 -4.428772 -4.4286709 -4.42858 -4.4285393 -4.4285641 -4.4286003 -4.4286532 -4.4286947 -4.4287014 -4.4287066 -4.4287324][-4.4288759 -4.4289083 -4.4289122 -4.4288945 -4.4288268 -4.4287238 -4.4286342 -4.4285889 -4.4285827 -4.4285779 -4.4285979 -4.4286213 -4.4286194 -4.4286408 -4.4286885][-4.4289336 -4.4289546 -4.4289527 -4.4289308 -4.4288568 -4.4287429 -4.4286404 -4.42859 -4.42859 -4.428587 -4.4285874 -4.4286013 -4.4285941 -4.4286213 -4.4286823][-4.4289455 -4.4289484 -4.4289389 -4.4289064 -4.4288187 -4.4286923 -4.42858 -4.4285355 -4.4285746 -4.4286208 -4.4286366 -4.4286532 -4.4286523 -4.428668 -4.4287109][-4.4289365 -4.4289265 -4.4289112 -4.4288597 -4.4287343 -4.4285789 -4.4284596 -4.42843 -4.4285245 -4.4286389 -4.4287028 -4.4287419 -4.4287596 -4.4287677 -4.42878][-4.4289317 -4.4289079 -4.4288788 -4.428793 -4.4286218 -4.4284244 -4.4282789 -4.4282527 -4.4283996 -4.4285913 -4.4287138 -4.4287834 -4.4288292 -4.4288487 -4.4288521][-4.4289422 -4.428906 -4.4288564 -4.428731 -4.4285131 -4.428267 -4.428082 -4.428031 -4.4281969 -4.4284558 -4.42864 -4.428751 -4.4288335 -4.4288807 -4.4288988][-4.4289341 -4.4288945 -4.4288397 -4.4287076 -4.4284892 -4.4282432 -4.4280658 -4.4280109 -4.4281454 -4.4283919 -4.4285831 -4.4287176 -4.4288287 -4.4289 -4.4289374][-4.4288735 -4.4288568 -4.4288344 -4.42875 -4.4285927 -4.4284029 -4.4282718 -4.428237 -4.4283261 -4.4284987 -4.4286404 -4.4287529 -4.4288545 -4.4289265 -4.428966][-4.4287667 -4.4288073 -4.4288449 -4.4288239 -4.4287419 -4.4286218 -4.4285278 -4.4284959 -4.4285493 -4.4286594 -4.4287524 -4.4288263 -4.4288979 -4.4289532 -4.4289827][-4.42866 -4.4287705 -4.4288731 -4.4288983 -4.4288726 -4.4288135 -4.4287539 -4.4287238 -4.4287477 -4.4288082 -4.4288616 -4.4289031 -4.4289422 -4.428977 -4.4289956][-4.4285994 -4.4287605 -4.4289026 -4.4289532 -4.4289608 -4.42894 -4.4289041 -4.4288783 -4.4288807 -4.4289031 -4.4289284 -4.4289479 -4.4289656 -4.4289846 -4.4289937][-4.4286218 -4.4287844 -4.4289212 -4.4289751 -4.428997 -4.4289966 -4.428978 -4.428956 -4.4289474 -4.4289522 -4.4289641 -4.4289718 -4.4289813 -4.4289942 -4.428998][-4.4287138 -4.4288406 -4.4289389 -4.4289751 -4.4289947 -4.4290009 -4.4289923 -4.4289784 -4.4289703 -4.4289708 -4.428977 -4.4289818 -4.4289904 -4.428997 -4.4289975][-4.4288158 -4.428894 -4.428947 -4.4289603 -4.428966 -4.4289689 -4.4289656 -4.42896 -4.4289565 -4.428957 -4.4289579 -4.4289608 -4.428966 -4.42897 -4.4289694]]...]
INFO - root - 2017-12-10 06:18:27.896149: step 14610, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:04m:43s remains)
INFO - root - 2017-12-10 06:18:30.580363: step 14620, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 22h:33m:26s remains)
INFO - root - 2017-12-10 06:18:33.174136: step 14630, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:25m:49s remains)
INFO - root - 2017-12-10 06:18:35.790648: step 14640, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:12m:01s remains)
INFO - root - 2017-12-10 06:18:38.411237: step 14650, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 23h:21m:00s remains)
INFO - root - 2017-12-10 06:18:41.046192: step 14660, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:18m:02s remains)
INFO - root - 2017-12-10 06:18:43.702219: step 14670, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 25h:40m:27s remains)
INFO - root - 2017-12-10 06:18:46.359426: step 14680, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:16m:52s remains)
INFO - root - 2017-12-10 06:18:48.982487: step 14690, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:40m:27s remains)
INFO - root - 2017-12-10 06:18:51.619628: step 14700, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:20m:16s remains)
2017-12-10 06:18:51.925493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290128 -4.4290037 -4.4289908 -4.4289784 -4.4289603 -4.4289441 -4.4289331 -4.42892 -4.42891 -4.428906 -4.4289165 -4.4289217 -4.4289327 -4.4289432 -4.428946][-4.42899 -4.4289942 -4.4289985 -4.4289975 -4.4289827 -4.428967 -4.428956 -4.4289455 -4.4289336 -4.4289255 -4.4289327 -4.4289393 -4.4289479 -4.4289541 -4.4289527][-4.42889 -4.4288969 -4.4289126 -4.428925 -4.4289227 -4.4289131 -4.4289026 -4.4288931 -4.4288816 -4.428874 -4.4288797 -4.4288836 -4.4288869 -4.4288888 -4.4288859][-4.4287276 -4.4287167 -4.4287305 -4.4287481 -4.4287586 -4.4287567 -4.4287486 -4.4287362 -4.4287171 -4.4287028 -4.4287009 -4.4286976 -4.4286976 -4.4286985 -4.4286981][-4.4285779 -4.4285307 -4.4285235 -4.4285374 -4.428556 -4.4285655 -4.4285645 -4.4285512 -4.4285245 -4.4284978 -4.4284787 -4.42846 -4.4284539 -4.4284506 -4.4284482][-4.4284768 -4.4283915 -4.4283528 -4.4283538 -4.4283733 -4.4283862 -4.4283891 -4.428381 -4.4283595 -4.4283314 -4.4283032 -4.4282708 -4.4282513 -4.4282379 -4.4282284][-4.42846 -4.4283509 -4.4282851 -4.428266 -4.4282737 -4.428278 -4.4282808 -4.4282846 -4.4282856 -4.4282794 -4.4282632 -4.4282308 -4.428206 -4.4281893 -4.4281764][-4.4285817 -4.4284821 -4.4284129 -4.4283838 -4.4283724 -4.4283562 -4.4283476 -4.4283628 -4.4283948 -4.4284244 -4.4284377 -4.428422 -4.4284053 -4.4283938 -4.4283853][-4.4287143 -4.4286366 -4.42858 -4.4285536 -4.4285336 -4.4285016 -4.4284806 -4.4284973 -4.4285455 -4.4286027 -4.4286451 -4.4286509 -4.4286461 -4.4286423 -4.4286342][-4.4287724 -4.4287109 -4.4286704 -4.428658 -4.4286461 -4.428618 -4.4285955 -4.4286056 -4.4286485 -4.4287081 -4.4287605 -4.4287777 -4.4287777 -4.4287734 -4.428761][-4.4287815 -4.4287262 -4.4286933 -4.4286938 -4.4287 -4.4286876 -4.4286718 -4.4286761 -4.4287028 -4.4287438 -4.4287844 -4.4287972 -4.4287949 -4.4287863 -4.4287686][-4.428782 -4.4287252 -4.428688 -4.4286923 -4.4287138 -4.4287186 -4.4287133 -4.4287095 -4.4287128 -4.4287319 -4.4287543 -4.4287624 -4.4287624 -4.4287548 -4.4287386][-4.4288025 -4.4287434 -4.4286971 -4.4286976 -4.4287252 -4.4287381 -4.4287367 -4.42872 -4.4287 -4.4286947 -4.4286976 -4.4287009 -4.4287071 -4.4287095 -4.4287033][-4.4288316 -4.428771 -4.4287219 -4.4287162 -4.4287381 -4.4287481 -4.4287424 -4.4287171 -4.4286847 -4.4286642 -4.4286523 -4.4286489 -4.4286613 -4.4286785 -4.4286847][-4.4288583 -4.4287958 -4.4287443 -4.4287333 -4.4287472 -4.4287539 -4.4287491 -4.4287252 -4.4286938 -4.4286695 -4.42865 -4.4286442 -4.4286609 -4.428689 -4.4287052]]...]
INFO - root - 2017-12-10 06:18:54.582451: step 14710, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:35m:51s remains)
INFO - root - 2017-12-10 06:18:57.231299: step 14720, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:30m:31s remains)
INFO - root - 2017-12-10 06:18:59.876366: step 14730, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 22h:56m:13s remains)
INFO - root - 2017-12-10 06:19:02.524028: step 14740, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:28m:07s remains)
INFO - root - 2017-12-10 06:19:05.254960: step 14750, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 23h:43m:24s remains)
INFO - root - 2017-12-10 06:19:07.919421: step 14760, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:23m:55s remains)
INFO - root - 2017-12-10 06:19:10.581457: step 14770, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:41m:48s remains)
INFO - root - 2017-12-10 06:19:13.208058: step 14780, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:40m:14s remains)
INFO - root - 2017-12-10 06:19:15.894103: step 14790, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:24m:13s remains)
INFO - root - 2017-12-10 06:19:18.528771: step 14800, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:34m:12s remains)
2017-12-10 06:19:18.832176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287667 -4.4288464 -4.4289 -4.4289293 -4.4289508 -4.4289646 -4.428966 -4.4289689 -4.4289761 -4.4289794 -4.428987 -4.4289994 -4.4290085 -4.4290118 -4.4290133][-4.4287615 -4.4288373 -4.4288869 -4.4289083 -4.4289289 -4.4289489 -4.4289579 -4.4289713 -4.4289923 -4.4290071 -4.429018 -4.4290271 -4.4290328 -4.4290328 -4.4290257][-4.4287925 -4.42885 -4.4288855 -4.4288936 -4.4289064 -4.4289284 -4.4289384 -4.4289536 -4.4289775 -4.4289947 -4.4290094 -4.4290228 -4.42903 -4.4290309 -4.4290223][-4.4288287 -4.4288588 -4.4288659 -4.4288545 -4.4288535 -4.4288688 -4.4288721 -4.4288878 -4.4289174 -4.4289417 -4.4289689 -4.4289937 -4.4290118 -4.4290171 -4.4290066][-4.4288239 -4.4288206 -4.4287992 -4.4287634 -4.42874 -4.4287429 -4.4287362 -4.4287553 -4.428803 -4.4288478 -4.4288979 -4.4289441 -4.4289784 -4.4289937 -4.428987][-4.4287872 -4.4287548 -4.4287057 -4.4286504 -4.4285955 -4.4285626 -4.4285316 -4.4285536 -4.4286342 -4.4287119 -4.4287934 -4.4288683 -4.4289255 -4.4289613 -4.4289708][-4.4287429 -4.4286866 -4.4286213 -4.4285564 -4.4284635 -4.4283633 -4.4282708 -4.42828 -4.4284167 -4.4285512 -4.4286709 -4.428772 -4.4288516 -4.4289088 -4.4289417][-4.4287319 -4.4286695 -4.4285984 -4.4285264 -4.4284115 -4.4282513 -4.428092 -4.4280791 -4.4282584 -4.428431 -4.4285684 -4.4286776 -4.4287686 -4.4288425 -4.4288969][-4.4287629 -4.4287219 -4.4286747 -4.42862 -4.428524 -4.4283738 -4.4282269 -4.4282 -4.4283276 -4.4284506 -4.428545 -4.4286304 -4.4287143 -4.4287934 -4.4288592][-4.4288177 -4.4288054 -4.4287839 -4.4287443 -4.428668 -4.4285536 -4.4284558 -4.42843 -4.4284811 -4.4285221 -4.4285445 -4.4285822 -4.4286551 -4.4287424 -4.4288168][-4.4289103 -4.42891 -4.4288855 -4.4288387 -4.4287672 -4.428679 -4.4286218 -4.4286075 -4.4286203 -4.4286165 -4.4285946 -4.4285879 -4.4286318 -4.4287038 -4.4287653][-4.4289865 -4.4289908 -4.4289594 -4.4289055 -4.4288397 -4.4287724 -4.4287415 -4.428741 -4.428751 -4.4287477 -4.4287205 -4.4286923 -4.4286966 -4.4287262 -4.4287553][-4.4290276 -4.4290347 -4.4290071 -4.4289575 -4.4289036 -4.4288545 -4.428833 -4.4288425 -4.4288626 -4.4288716 -4.4288583 -4.4288306 -4.4288154 -4.4288054 -4.4287977][-4.4290261 -4.429038 -4.4290214 -4.4289846 -4.4289412 -4.4289045 -4.4288907 -4.4289093 -4.4289408 -4.4289632 -4.4289675 -4.4289536 -4.428936 -4.428905 -4.4288735][-4.4290195 -4.4290352 -4.4290285 -4.429 -4.428956 -4.4289179 -4.4289021 -4.4289255 -4.428966 -4.4290018 -4.4290218 -4.4290242 -4.4290147 -4.4289842 -4.4289522]]...]
INFO - root - 2017-12-10 06:19:21.448637: step 14810, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:25m:54s remains)
INFO - root - 2017-12-10 06:19:24.145493: step 14820, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:51m:01s remains)
INFO - root - 2017-12-10 06:19:26.768926: step 14830, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:06m:02s remains)
INFO - root - 2017-12-10 06:19:29.394500: step 14840, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 22h:32m:18s remains)
INFO - root - 2017-12-10 06:19:32.028114: step 14850, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:13m:45s remains)
INFO - root - 2017-12-10 06:19:34.672783: step 14860, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:15m:55s remains)
INFO - root - 2017-12-10 06:19:37.312667: step 14870, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:23m:13s remains)
INFO - root - 2017-12-10 06:19:39.985093: step 14880, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 24h:29m:51s remains)
INFO - root - 2017-12-10 06:19:42.601639: step 14890, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 22h:32m:34s remains)
INFO - root - 2017-12-10 06:19:45.243164: step 14900, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 22h:56m:29s remains)
2017-12-10 06:19:45.532270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288626 -4.4288058 -4.4287457 -4.4287076 -4.4287109 -4.428741 -4.4287548 -4.4287176 -4.4286542 -4.4286342 -4.4286647 -4.4286828 -4.4286904 -4.4287291 -4.4287853][-4.4288182 -4.4287543 -4.4286976 -4.4286852 -4.4287362 -4.428792 -4.4288049 -4.4287548 -4.4286733 -4.4286366 -4.4286594 -4.4286809 -4.4286895 -4.4287219 -4.42877][-4.4287696 -4.4286828 -4.4286108 -4.4286261 -4.4287152 -4.4287868 -4.4288063 -4.4287724 -4.4287138 -4.42869 -4.4287014 -4.4287157 -4.4287271 -4.4287529 -4.4287772][-4.428709 -4.4286022 -4.4285259 -4.428555 -4.4286513 -4.4287233 -4.4287534 -4.4287529 -4.4287415 -4.4287477 -4.4287596 -4.4287643 -4.4287739 -4.4287915 -4.428791][-4.428669 -4.4285593 -4.428494 -4.4285059 -4.4285555 -4.4285827 -4.4286075 -4.4286561 -4.428721 -4.4287758 -4.4288006 -4.4288044 -4.4288087 -4.4288235 -4.4288125][-4.4286985 -4.428606 -4.4285393 -4.4284983 -4.4284582 -4.4283867 -4.42837 -4.4284749 -4.4286375 -4.4287634 -4.4288144 -4.4288268 -4.428833 -4.4288568 -4.428853][-4.4287648 -4.4286957 -4.4286222 -4.4285369 -4.4284143 -4.4282408 -4.4281559 -4.4282942 -4.428535 -4.4287171 -4.4287987 -4.4288292 -4.4288483 -4.4288874 -4.4289026][-4.4288449 -4.4288011 -4.42874 -4.428648 -4.428493 -4.4282718 -4.4281259 -4.4282269 -4.428472 -4.4286633 -4.428761 -4.4288092 -4.4288421 -4.4288926 -4.4289274][-4.428885 -4.4288597 -4.4288225 -4.4287567 -4.4286289 -4.4284334 -4.4282665 -4.4282804 -4.4284596 -4.4286284 -4.4287181 -4.4287734 -4.42882 -4.4288745 -4.4289179][-4.4288807 -4.4288626 -4.428844 -4.4288116 -4.4287348 -4.4285941 -4.428443 -4.4283934 -4.4284911 -4.4286213 -4.4286933 -4.4287415 -4.4287896 -4.4288383 -4.428885][-4.4288716 -4.4288526 -4.4288473 -4.4288354 -4.4287958 -4.4287076 -4.4285936 -4.42853 -4.4285655 -4.4286466 -4.4286976 -4.4287271 -4.4287562 -4.4287906 -4.4288263][-4.4288583 -4.428834 -4.4288325 -4.4288335 -4.4288187 -4.4287705 -4.4286947 -4.4286442 -4.4286513 -4.4286904 -4.4287252 -4.4287319 -4.4287376 -4.4287496 -4.4287615][-4.42883 -4.4287944 -4.4287763 -4.4287758 -4.4287739 -4.4287605 -4.4287281 -4.4287033 -4.428709 -4.4287314 -4.4287534 -4.4287448 -4.4287295 -4.42872 -4.4287138][-4.4288044 -4.428761 -4.4287162 -4.4286976 -4.4286947 -4.4287009 -4.4286985 -4.428699 -4.4287171 -4.4287448 -4.4287624 -4.428751 -4.4287238 -4.4286957 -4.4286814][-4.4287753 -4.4287348 -4.4286704 -4.4286294 -4.4286208 -4.4286375 -4.4286485 -4.4286609 -4.428688 -4.4287229 -4.4287453 -4.4287448 -4.4287252 -4.428688 -4.4286685]]...]
INFO - root - 2017-12-10 06:19:48.187264: step 14910, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 22h:54m:53s remains)
INFO - root - 2017-12-10 06:19:50.857069: step 14920, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:03m:21s remains)
INFO - root - 2017-12-10 06:19:53.490060: step 14930, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:25m:37s remains)
INFO - root - 2017-12-10 06:19:56.172581: step 14940, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:51m:09s remains)
INFO - root - 2017-12-10 06:19:58.827223: step 14950, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:43m:54s remains)
INFO - root - 2017-12-10 06:20:01.477179: step 14960, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:45m:45s remains)
INFO - root - 2017-12-10 06:20:04.123758: step 14970, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:33m:10s remains)
INFO - root - 2017-12-10 06:20:06.752878: step 14980, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:35m:45s remains)
INFO - root - 2017-12-10 06:20:09.354764: step 14990, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:04m:35s remains)
INFO - root - 2017-12-10 06:20:12.043431: step 15000, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:44m:54s remains)
2017-12-10 06:20:12.368548: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288535 -4.4288573 -4.4288564 -4.428853 -4.4288392 -4.4288239 -4.4288187 -4.4288206 -4.42883 -4.4288316 -4.4288144 -4.4287872 -4.4287882 -4.4288092 -4.4288373][-4.4288654 -4.428874 -4.4288793 -4.4288783 -4.4288659 -4.428854 -4.4288526 -4.428853 -4.4288521 -4.4288397 -4.4288044 -4.4287643 -4.42876 -4.42878 -4.4288092][-4.4288335 -4.4288473 -4.4288607 -4.4288673 -4.4288607 -4.4288564 -4.4288559 -4.4288573 -4.4288545 -4.4288406 -4.4288039 -4.428761 -4.4287481 -4.4287596 -4.4287853][-4.4287767 -4.428793 -4.4288106 -4.4288192 -4.4288116 -4.4288082 -4.4288063 -4.4288111 -4.4288177 -4.4288225 -4.4288087 -4.4287829 -4.4287696 -4.4287696 -4.4287896][-4.4287462 -4.4287605 -4.42878 -4.4287829 -4.4287591 -4.4287424 -4.4287305 -4.4287348 -4.4287591 -4.4287963 -4.4288168 -4.4288149 -4.4288096 -4.4288054 -4.4288187][-4.4287438 -4.4287519 -4.4287634 -4.428751 -4.4287009 -4.4286475 -4.4286056 -4.4286056 -4.4286585 -4.4287395 -4.4287968 -4.42881 -4.4288068 -4.4288025 -4.4288116][-4.4287438 -4.4287348 -4.4287243 -4.428689 -4.4286056 -4.4284921 -4.4283867 -4.4283719 -4.4284711 -4.4286141 -4.4287143 -4.4287457 -4.4287448 -4.4287438 -4.4287524][-4.4287276 -4.4287052 -4.4286771 -4.4286275 -4.4285278 -4.4283752 -4.4282203 -4.4281936 -4.4283319 -4.4285188 -4.4286427 -4.428679 -4.4286718 -4.428669 -4.4286861][-4.4287438 -4.4287214 -4.4286933 -4.4286542 -4.4285774 -4.4284525 -4.4283223 -4.428299 -4.4284053 -4.4285593 -4.4286637 -4.4286861 -4.4286618 -4.4286451 -4.4286652][-4.4287767 -4.4287596 -4.4287443 -4.428731 -4.4286947 -4.4286265 -4.4285488 -4.4285259 -4.4285812 -4.4286771 -4.4287491 -4.4287596 -4.4287271 -4.4287014 -4.4287138][-4.4288182 -4.4288049 -4.4288058 -4.4288173 -4.4288168 -4.4287925 -4.4287496 -4.4287257 -4.4287453 -4.4287944 -4.4288383 -4.4288445 -4.4288154 -4.4287906 -4.4287968][-4.4288297 -4.428822 -4.4288383 -4.4288692 -4.428894 -4.428896 -4.4288759 -4.4288559 -4.4288564 -4.428875 -4.4288979 -4.4289055 -4.4288888 -4.4288716 -4.428875][-4.428812 -4.4288111 -4.4288373 -4.4288788 -4.4289126 -4.4289255 -4.4289193 -4.4289055 -4.4289 -4.428906 -4.4289193 -4.4289322 -4.4289303 -4.4289241 -4.4289246][-4.4287643 -4.4287763 -4.4288135 -4.42886 -4.4288974 -4.4289145 -4.4289136 -4.4289045 -4.4289 -4.4289017 -4.4289136 -4.4289312 -4.4289403 -4.4289422 -4.4289417][-4.4287171 -4.4287424 -4.4287882 -4.4288368 -4.428875 -4.4288931 -4.428894 -4.4288878 -4.4288888 -4.4288945 -4.4289055 -4.4289222 -4.428937 -4.428946 -4.4289484]]...]
INFO - root - 2017-12-10 06:20:15.000404: step 15010, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:10m:15s remains)
INFO - root - 2017-12-10 06:20:17.645824: step 15020, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:47m:59s remains)
INFO - root - 2017-12-10 06:20:20.300170: step 15030, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:39m:58s remains)
INFO - root - 2017-12-10 06:20:22.950233: step 15040, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:14m:56s remains)
INFO - root - 2017-12-10 06:20:25.628034: step 15050, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:49m:57s remains)
INFO - root - 2017-12-10 06:20:28.210689: step 15060, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 22h:52m:50s remains)
INFO - root - 2017-12-10 06:20:30.885984: step 15070, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:07m:54s remains)
INFO - root - 2017-12-10 06:20:33.512088: step 15080, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:15m:20s remains)
INFO - root - 2017-12-10 06:20:36.143518: step 15090, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 23h:41m:22s remains)
INFO - root - 2017-12-10 06:20:38.825144: step 15100, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 22h:30m:05s remains)
2017-12-10 06:20:39.114820: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289737 -4.4289522 -4.4289269 -4.4288988 -4.4288669 -4.4288278 -4.4287877 -4.4287596 -4.4287658 -4.4288096 -4.4288449 -4.4288592 -4.4288678 -4.4288692 -4.4288535][-4.4289546 -4.4289312 -4.4289064 -4.4288831 -4.4288445 -4.4287906 -4.4287362 -4.4286957 -4.4286995 -4.428751 -4.4287968 -4.4288139 -4.4288173 -4.4288149 -4.4287977][-4.4289436 -4.4289193 -4.4288974 -4.4288831 -4.4288459 -4.4287686 -4.4286895 -4.42865 -4.4286661 -4.4287214 -4.4287748 -4.4287977 -4.4287987 -4.42879 -4.4287758][-4.4289212 -4.428896 -4.428875 -4.428864 -4.428834 -4.4287419 -4.4286261 -4.428575 -4.4286194 -4.4286976 -4.4287581 -4.4287858 -4.4287863 -4.4287767 -4.4287705][-4.4288936 -4.4288673 -4.4288459 -4.4288254 -4.428792 -4.4286923 -4.4285455 -4.4284611 -4.4285345 -4.4286513 -4.4287286 -4.4287624 -4.4287696 -4.4287648 -4.4287682][-4.4288688 -4.42884 -4.4288163 -4.428782 -4.4287338 -4.4286227 -4.4284611 -4.4283481 -4.42843 -4.4285855 -4.4286914 -4.4287372 -4.428751 -4.4287539 -4.4287658][-4.4288454 -4.4288144 -4.4287896 -4.4287405 -4.4286733 -4.4285531 -4.4283853 -4.4282522 -4.428328 -4.4285126 -4.4286456 -4.428709 -4.4287353 -4.4287438 -4.4287577][-4.428822 -4.4287925 -4.4287758 -4.42873 -4.4286556 -4.4285369 -4.4283772 -4.4282413 -4.4282923 -4.4284587 -4.4286003 -4.4286776 -4.4287233 -4.428741 -4.4287534][-4.428813 -4.428791 -4.4287863 -4.4287591 -4.4286985 -4.4286079 -4.4284921 -4.4283857 -4.4284186 -4.4285216 -4.4286256 -4.4286947 -4.4287395 -4.4287577 -4.4287658][-4.4288149 -4.4288015 -4.4288025 -4.4287825 -4.4287286 -4.4286623 -4.428597 -4.428544 -4.4285679 -4.428628 -4.4286995 -4.4287515 -4.4287829 -4.4287944 -4.4287958][-4.4288297 -4.4288306 -4.4288411 -4.4288268 -4.42878 -4.4287214 -4.4286766 -4.4286566 -4.4286795 -4.4287205 -4.4287777 -4.428823 -4.4288421 -4.4288435 -4.4288354][-4.4288473 -4.4288554 -4.4288697 -4.4288645 -4.428834 -4.428782 -4.4287438 -4.4287434 -4.4287763 -4.42881 -4.4288549 -4.4288917 -4.4289041 -4.4288945 -4.4288783][-4.4288573 -4.4288659 -4.4288797 -4.4288821 -4.4288707 -4.4288263 -4.428791 -4.4288 -4.4288454 -4.4288816 -4.4289155 -4.4289436 -4.42895 -4.4289351 -4.428915][-4.4288697 -4.4288754 -4.4288855 -4.4288893 -4.4288831 -4.4288511 -4.4288278 -4.4288435 -4.42889 -4.4289255 -4.42895 -4.4289637 -4.428967 -4.428957 -4.4289389][-4.4288855 -4.4288821 -4.4288831 -4.4288826 -4.42888 -4.4288669 -4.4288626 -4.4288812 -4.4289184 -4.428946 -4.4289584 -4.4289589 -4.4289579 -4.4289541 -4.4289441]]...]
INFO - root - 2017-12-10 06:20:41.761458: step 15110, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:15m:30s remains)
INFO - root - 2017-12-10 06:20:44.466009: step 15120, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:49m:18s remains)
INFO - root - 2017-12-10 06:20:47.109791: step 15130, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 22h:53m:32s remains)
INFO - root - 2017-12-10 06:20:49.756568: step 15140, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:31m:01s remains)
INFO - root - 2017-12-10 06:20:52.422069: step 15150, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:10m:36s remains)
INFO - root - 2017-12-10 06:20:55.091782: step 15160, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 22h:53m:43s remains)
INFO - root - 2017-12-10 06:20:57.772249: step 15170, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:30m:58s remains)
INFO - root - 2017-12-10 06:21:00.444393: step 15180, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 22h:42m:09s remains)
INFO - root - 2017-12-10 06:21:03.109791: step 15190, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:39m:08s remains)
INFO - root - 2017-12-10 06:21:05.736462: step 15200, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 22h:59m:59s remains)
2017-12-10 06:21:06.030345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288411 -4.4288368 -4.4288263 -4.4288044 -4.4287925 -4.4288034 -4.4288349 -4.4288683 -4.42888 -4.4288607 -4.42883 -4.4288206 -4.4288478 -4.4288931 -4.4289212][-4.4288592 -4.4288454 -4.4288206 -4.4287891 -4.4287748 -4.428791 -4.4288311 -4.4288697 -4.42888 -4.42886 -4.4288306 -4.4288216 -4.428844 -4.4288764 -4.4288898][-4.4288669 -4.428833 -4.4287934 -4.4287634 -4.4287643 -4.4287925 -4.4288311 -4.4288583 -4.4288621 -4.42885 -4.4288411 -4.4288459 -4.4288578 -4.428865 -4.4288545][-4.4288516 -4.4288077 -4.4287648 -4.4287467 -4.4287648 -4.4287987 -4.4288254 -4.4288325 -4.4288278 -4.4288311 -4.4288526 -4.4288735 -4.4288735 -4.4288568 -4.4288292][-4.4288282 -4.4287953 -4.42877 -4.4287667 -4.4287887 -4.4288111 -4.4288111 -4.4287958 -4.4287891 -4.4288116 -4.42886 -4.4288945 -4.4288874 -4.4288559 -4.4288216][-4.4288173 -4.4288054 -4.4288025 -4.4288068 -4.428812 -4.4287963 -4.4287543 -4.4287157 -4.4287176 -4.4287643 -4.4288321 -4.4288807 -4.428884 -4.4288588 -4.4288297][-4.4288149 -4.4288163 -4.4288287 -4.4288335 -4.428812 -4.4287443 -4.4286509 -4.4285913 -4.4286103 -4.428689 -4.4287791 -4.4288478 -4.4288745 -4.4288721 -4.4288535][-4.428793 -4.4287972 -4.4288168 -4.4288268 -4.4287949 -4.4286947 -4.4285669 -4.4284949 -4.4285374 -4.4286404 -4.4287472 -4.4288354 -4.4288869 -4.4289021 -4.4288874][-4.4287562 -4.4287534 -4.4287796 -4.4288073 -4.4287953 -4.4287124 -4.4286013 -4.4285464 -4.4285855 -4.4286671 -4.4287519 -4.4288321 -4.42889 -4.428915 -4.42891][-4.4287267 -4.42871 -4.4287429 -4.4287982 -4.428822 -4.4287834 -4.4287128 -4.4286718 -4.4286866 -4.4287281 -4.4287715 -4.428822 -4.4288688 -4.4288983 -4.428906][-4.428731 -4.4287071 -4.4287372 -4.4287996 -4.4288416 -4.4288316 -4.4287896 -4.4287534 -4.4287467 -4.4287624 -4.4287786 -4.4288058 -4.428844 -4.4288731 -4.4288878][-4.4287667 -4.4287362 -4.4287486 -4.4287968 -4.4288359 -4.4288325 -4.4288077 -4.4287815 -4.428771 -4.4287777 -4.4287863 -4.4288044 -4.428834 -4.4288545 -4.4288664][-4.4288173 -4.428782 -4.4287739 -4.4287996 -4.4288225 -4.4288163 -4.4287996 -4.4287858 -4.4287848 -4.4287949 -4.4288063 -4.4288211 -4.4288392 -4.4288445 -4.4288492][-4.4288616 -4.4288259 -4.4288058 -4.4288144 -4.4288244 -4.42881 -4.428792 -4.4287839 -4.4287944 -4.4288173 -4.4288349 -4.4288468 -4.4288516 -4.4288454 -4.4288445][-4.42888 -4.4288464 -4.4288244 -4.4288287 -4.4288363 -4.4288168 -4.4287963 -4.428792 -4.42881 -4.4288416 -4.428863 -4.4288712 -4.42887 -4.428864 -4.4288692]]...]
INFO - root - 2017-12-10 06:21:08.671842: step 15210, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:38m:07s remains)
INFO - root - 2017-12-10 06:21:11.360590: step 15220, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 23h:55m:51s remains)
INFO - root - 2017-12-10 06:21:14.011033: step 15230, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 23h:41m:51s remains)
INFO - root - 2017-12-10 06:21:16.649547: step 15240, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 23h:53m:19s remains)
INFO - root - 2017-12-10 06:21:19.317745: step 15250, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:24m:32s remains)
INFO - root - 2017-12-10 06:21:22.004177: step 15260, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 22h:57m:52s remains)
INFO - root - 2017-12-10 06:21:24.693662: step 15270, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.274 sec/batch; 24h:11m:13s remains)
INFO - root - 2017-12-10 06:21:27.375683: step 15280, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:04m:47s remains)
INFO - root - 2017-12-10 06:21:30.016193: step 15290, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 23h:18m:16s remains)
INFO - root - 2017-12-10 06:21:32.725047: step 15300, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 25h:37m:53s remains)
2017-12-10 06:21:33.037715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428792 -4.4287663 -4.428741 -4.4287486 -4.4287667 -4.428761 -4.4287429 -4.4287119 -4.4286833 -4.4286475 -4.4285913 -4.4285765 -4.4285936 -4.4285851 -4.4285259][-4.4287667 -4.4287539 -4.4287395 -4.4287519 -4.4287686 -4.4287539 -4.428731 -4.4286976 -4.4286733 -4.42865 -4.4286132 -4.4286175 -4.4286594 -4.428678 -4.4286551][-4.4287529 -4.4287453 -4.4287357 -4.4287496 -4.4287658 -4.4287453 -4.42872 -4.4286842 -4.4286594 -4.4286447 -4.4286218 -4.4286346 -4.4286928 -4.4287329 -4.42874][-4.4287205 -4.4287038 -4.4286904 -4.4287086 -4.4287343 -4.4287295 -4.4287086 -4.42867 -4.4286432 -4.4286294 -4.4286122 -4.4286284 -4.4286938 -4.4287519 -4.4287853][-4.4286766 -4.428647 -4.4286203 -4.4286361 -4.4286714 -4.4286766 -4.4286561 -4.4286227 -4.4286094 -4.4286089 -4.4286056 -4.4286294 -4.4286971 -4.42877 -4.4288216][-4.4286289 -4.4285979 -4.4285564 -4.4285579 -4.4285803 -4.4285693 -4.42853 -4.4284978 -4.4285092 -4.4285474 -4.4285831 -4.4286275 -4.4286971 -4.4287777 -4.4288411][-4.4285955 -4.4285626 -4.4285069 -4.4284911 -4.4284883 -4.4284415 -4.4283724 -4.4283381 -4.4283867 -4.4284844 -4.4285722 -4.4286304 -4.4287 -4.4287772 -4.4288406][-4.4285688 -4.4285274 -4.428462 -4.4284344 -4.4284244 -4.4283657 -4.4282889 -4.4282622 -4.4283323 -4.4284654 -4.4285822 -4.428648 -4.4287081 -4.428771 -4.4288192][-4.4285536 -4.428504 -4.4284353 -4.4284039 -4.4283986 -4.4283576 -4.4283028 -4.4282994 -4.4283805 -4.4285107 -4.4286222 -4.4286761 -4.4287157 -4.4287558 -4.42878][-4.4285936 -4.428544 -4.4284825 -4.4284511 -4.4284525 -4.4284315 -4.4284091 -4.4284291 -4.4284973 -4.4286 -4.4286804 -4.4287047 -4.4287238 -4.428741 -4.4287448][-4.42868 -4.4286423 -4.4286036 -4.4285789 -4.4285827 -4.4285736 -4.4285703 -4.428596 -4.4286466 -4.4287143 -4.4287496 -4.4287429 -4.4287477 -4.4287467 -4.4287362][-4.4287405 -4.4287224 -4.4287119 -4.428709 -4.4287205 -4.4287224 -4.4287262 -4.4287472 -4.4287839 -4.4288211 -4.4288263 -4.4288049 -4.4287992 -4.4287863 -4.4287677][-4.4287624 -4.4287529 -4.4287567 -4.4287715 -4.4287944 -4.4288068 -4.4288168 -4.4288378 -4.428865 -4.4288826 -4.4288754 -4.4288564 -4.4288497 -4.4288383 -4.428822][-4.4287672 -4.4287605 -4.4287729 -4.4287977 -4.4288297 -4.4288497 -4.428863 -4.4288836 -4.4289064 -4.4289165 -4.4289088 -4.4288964 -4.4288907 -4.4288855 -4.4288793][-4.4287724 -4.4287715 -4.4287887 -4.4288149 -4.4288487 -4.4288707 -4.4288859 -4.4289055 -4.4289246 -4.4289308 -4.4289231 -4.4289155 -4.4289126 -4.4289112 -4.4289122]]...]
INFO - root - 2017-12-10 06:21:35.698192: step 15310, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:35m:12s remains)
INFO - root - 2017-12-10 06:21:38.382049: step 15320, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 23h:45m:18s remains)
INFO - root - 2017-12-10 06:21:41.014742: step 15330, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:37m:14s remains)
INFO - root - 2017-12-10 06:21:43.695479: step 15340, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:05m:03s remains)
INFO - root - 2017-12-10 06:21:46.322664: step 15350, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 21h:59m:28s remains)
INFO - root - 2017-12-10 06:21:48.931915: step 15360, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 23h:35m:42s remains)
INFO - root - 2017-12-10 06:21:51.615556: step 15370, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 22h:36m:16s remains)
INFO - root - 2017-12-10 06:21:54.276528: step 15380, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 22h:54m:34s remains)
INFO - root - 2017-12-10 06:21:56.949490: step 15390, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:29m:08s remains)
INFO - root - 2017-12-10 06:21:59.645409: step 15400, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 23h:36m:58s remains)
2017-12-10 06:21:59.952119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290266 -4.4290257 -4.4290185 -4.42901 -4.4290018 -4.428988 -4.4289713 -4.4289637 -4.428967 -4.4289789 -4.4289808 -4.4289646 -4.4289446 -4.4288979 -4.4288268][-4.4290304 -4.4290252 -4.429009 -4.4289923 -4.4289742 -4.4289494 -4.4289241 -4.428916 -4.4289265 -4.4289436 -4.4289451 -4.428926 -4.4288988 -4.4288259 -4.42871][-4.4290295 -4.4290175 -4.4289923 -4.4289651 -4.4289355 -4.428896 -4.4288635 -4.4288592 -4.42888 -4.4289074 -4.4289112 -4.42889 -4.4288411 -4.4287276 -4.4285474][-4.4290195 -4.4290023 -4.428967 -4.4289312 -4.4288826 -4.4288158 -4.4287639 -4.4287658 -4.4288092 -4.4288578 -4.4288812 -4.4288559 -4.4287724 -4.4285975 -4.4283381][-4.4290118 -4.428988 -4.4289412 -4.4288859 -4.4288058 -4.4286957 -4.4286118 -4.4286246 -4.4287152 -4.428812 -4.4288611 -4.4288392 -4.4287338 -4.4285088 -4.42819][-4.4290042 -4.4289689 -4.4289055 -4.4288182 -4.4286942 -4.4285216 -4.4283824 -4.4284158 -4.4285851 -4.4287491 -4.4288306 -4.4288268 -4.4287391 -4.4285464 -4.428297][-4.4289918 -4.428947 -4.4288683 -4.4287467 -4.428566 -4.4283113 -4.4280972 -4.4281726 -4.4284334 -4.4286656 -4.4287834 -4.4288168 -4.4287825 -4.4286685 -4.4285307][-4.4289885 -4.4289322 -4.4288359 -4.4286938 -4.4284792 -4.4281836 -4.4279337 -4.4280429 -4.4283557 -4.42862 -4.4287605 -4.4288249 -4.4288278 -4.4287744 -4.4287081][-4.4289951 -4.4289351 -4.428833 -4.4287043 -4.4285297 -4.4283195 -4.4281673 -4.428246 -4.4284692 -4.4286723 -4.4287887 -4.4288521 -4.4288635 -4.4288349 -4.4288054][-4.4289966 -4.4289436 -4.4288592 -4.4287624 -4.4286537 -4.4285464 -4.4284816 -4.4285169 -4.42863 -4.4287453 -4.4288225 -4.4288788 -4.4288883 -4.4288487 -4.4288149][-4.4290066 -4.428968 -4.4289041 -4.4288316 -4.4287629 -4.4287133 -4.4286976 -4.4287181 -4.4287658 -4.4288154 -4.428854 -4.4288788 -4.4288526 -4.4287786 -4.42873][-4.4290042 -4.4289794 -4.4289365 -4.42888 -4.4288263 -4.4288049 -4.4288206 -4.428844 -4.428865 -4.4288778 -4.4288778 -4.42885 -4.42876 -4.4286289 -4.4285569][-4.4289846 -4.4289727 -4.4289479 -4.4288921 -4.4288273 -4.4288092 -4.428843 -4.4288816 -4.4289093 -4.4289179 -4.4288912 -4.428802 -4.4286242 -4.4284015 -4.4283013][-4.4289455 -4.4289312 -4.4289031 -4.4288244 -4.4287376 -4.4287195 -4.4287782 -4.4288507 -4.4289041 -4.4289246 -4.4288869 -4.4287472 -4.4284811 -4.4281697 -4.4280653][-4.4289041 -4.4288707 -4.4288116 -4.4286866 -4.4285512 -4.4285226 -4.4286275 -4.4287639 -4.4288616 -4.4288979 -4.4288654 -4.4287286 -4.4284816 -4.4282289 -4.4281731]]...]
INFO - root - 2017-12-10 06:22:02.657810: step 15410, loss = 2.28, batch loss = 2.23 (27.2 examples/sec; 0.294 sec/batch; 25h:52m:56s remains)
INFO - root - 2017-12-10 06:22:05.305111: step 15420, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 23h:28m:23s remains)
INFO - root - 2017-12-10 06:22:07.977812: step 15430, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 23h:42m:34s remains)
INFO - root - 2017-12-10 06:22:10.663879: step 15440, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:28m:04s remains)
INFO - root - 2017-12-10 06:22:13.299443: step 15450, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 22h:42m:01s remains)
INFO - root - 2017-12-10 06:22:15.915934: step 15460, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 22h:22m:40s remains)
INFO - root - 2017-12-10 06:22:18.583340: step 15470, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:00m:28s remains)
INFO - root - 2017-12-10 06:22:21.213055: step 15480, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 22h:39m:13s remains)
INFO - root - 2017-12-10 06:22:23.877650: step 15490, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:09m:45s remains)
INFO - root - 2017-12-10 06:22:26.526388: step 15500, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 22h:33m:25s remains)
2017-12-10 06:22:26.834467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287505 -4.4287372 -4.4287524 -4.4287815 -4.4287987 -4.4288111 -4.4288311 -4.4288435 -4.42884 -4.4288316 -4.4288168 -4.4288058 -4.4288092 -4.4288039 -4.4287858][-4.4287186 -4.4286933 -4.4287014 -4.4287291 -4.4287448 -4.4287553 -4.4287848 -4.4288116 -4.4288225 -4.4288173 -4.4287992 -4.4287882 -4.4287925 -4.428782 -4.4287672][-4.4286757 -4.4286432 -4.4286466 -4.4286656 -4.4286695 -4.4286747 -4.4287167 -4.4287643 -4.4287925 -4.4287887 -4.4287715 -4.4287691 -4.4287777 -4.4287629 -4.4287424][-4.4286695 -4.4286361 -4.4286304 -4.4286289 -4.4286137 -4.4286065 -4.428647 -4.4286962 -4.4287195 -4.4287043 -4.4286938 -4.4287066 -4.4287291 -4.4287181 -4.4286928][-4.4287066 -4.4286685 -4.4286423 -4.4286118 -4.4285812 -4.4285612 -4.4285789 -4.4286056 -4.4286041 -4.4285707 -4.4285712 -4.4286041 -4.4286437 -4.4286451 -4.4286218][-4.4287663 -4.4287119 -4.4286575 -4.4286017 -4.4285641 -4.4285388 -4.4285331 -4.4285359 -4.428514 -4.4284706 -4.4284797 -4.4285207 -4.4285712 -4.4285855 -4.4285703][-4.4288235 -4.4287558 -4.4286814 -4.4286132 -4.4285789 -4.4285603 -4.4285474 -4.4285321 -4.4284887 -4.42843 -4.4284353 -4.4284649 -4.4285035 -4.4285245 -4.42852][-4.42887 -4.4287972 -4.4287128 -4.4286485 -4.4286289 -4.428628 -4.4286213 -4.428597 -4.428546 -4.4284825 -4.4284787 -4.4284854 -4.4284892 -4.4284983 -4.4285049][-4.428894 -4.4288321 -4.4287581 -4.4287081 -4.4286938 -4.42869 -4.42868 -4.4286571 -4.4286213 -4.4285712 -4.42856 -4.4285464 -4.4285235 -4.4285235 -4.4285383][-4.4288979 -4.4288597 -4.4288058 -4.4287658 -4.42875 -4.4287372 -4.4287128 -4.4286838 -4.4286556 -4.4286141 -4.4285946 -4.4285703 -4.4285388 -4.4285359 -4.4285593][-4.4289021 -4.4288859 -4.4288535 -4.4288259 -4.42881 -4.428793 -4.4287615 -4.4287295 -4.4287014 -4.4286671 -4.428638 -4.4286041 -4.4285626 -4.42855 -4.4285746][-4.4289174 -4.4289155 -4.4288964 -4.4288754 -4.4288549 -4.42883 -4.4288011 -4.4287763 -4.4287548 -4.4287324 -4.4287019 -4.428668 -4.4286289 -4.4286132 -4.4286327][-4.4289427 -4.4289479 -4.4289341 -4.42891 -4.4288821 -4.4288545 -4.428834 -4.42882 -4.4288034 -4.4287853 -4.4287581 -4.4287329 -4.4287076 -4.4286942 -4.4287][-4.4289436 -4.4289522 -4.4289446 -4.4289246 -4.4289 -4.4288735 -4.4288568 -4.42885 -4.4288421 -4.4288325 -4.4288139 -4.4287968 -4.4287829 -4.4287672 -4.4287586][-4.4289284 -4.428936 -4.42893 -4.4289207 -4.4289103 -4.428895 -4.4288859 -4.4288836 -4.4288836 -4.4288821 -4.42887 -4.4288564 -4.4288483 -4.4288373 -4.4288268]]...]
INFO - root - 2017-12-10 06:22:29.493625: step 15510, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 22h:50m:47s remains)
INFO - root - 2017-12-10 06:22:32.165736: step 15520, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:06m:34s remains)
INFO - root - 2017-12-10 06:22:34.816100: step 15530, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 24h:31m:19s remains)
INFO - root - 2017-12-10 06:22:37.483854: step 15540, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:26m:12s remains)
INFO - root - 2017-12-10 06:22:40.184499: step 15550, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 23h:46m:33s remains)
INFO - root - 2017-12-10 06:22:42.863103: step 15560, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:06m:01s remains)
INFO - root - 2017-12-10 06:22:45.516350: step 15570, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:23m:42s remains)
INFO - root - 2017-12-10 06:22:48.188650: step 15580, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:27m:24s remains)
INFO - root - 2017-12-10 06:22:50.819670: step 15590, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 22h:46m:50s remains)
INFO - root - 2017-12-10 06:22:53.452977: step 15600, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:04m:28s remains)
2017-12-10 06:22:53.733052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289303 -4.4289155 -4.4288883 -4.4288464 -4.4288077 -4.4287829 -4.4287534 -4.4287114 -4.4286895 -4.4287171 -4.4287672 -4.42879 -4.428781 -4.4287562 -4.4287314][-4.4288926 -4.4288754 -4.4288559 -4.4288197 -4.4287786 -4.42875 -4.4287109 -4.4286623 -4.4286308 -4.4286432 -4.4286819 -4.4287019 -4.4287033 -4.4286933 -4.4286861][-4.4288564 -4.42884 -4.4288421 -4.4288306 -4.4288 -4.4287562 -4.4286981 -4.4286389 -4.428597 -4.4285946 -4.4286194 -4.4286432 -4.4286647 -4.4286771 -4.4286771][-4.4288039 -4.4288011 -4.4288259 -4.4288383 -4.4288125 -4.4287477 -4.4286709 -4.4286127 -4.4285755 -4.4285636 -4.4285703 -4.4285951 -4.4286337 -4.4286628 -4.4286623][-4.4287267 -4.4287462 -4.4287877 -4.4288039 -4.4287629 -4.4286742 -4.4285836 -4.4285393 -4.4285412 -4.4285569 -4.4285555 -4.4285674 -4.4286013 -4.4286318 -4.4286256][-4.4286184 -4.4286604 -4.4287109 -4.4287171 -4.4286542 -4.4285293 -4.4284158 -4.4283772 -4.428422 -4.428483 -4.4285045 -4.4285274 -4.4285603 -4.4285932 -4.4285874][-4.4285555 -4.4286094 -4.4286408 -4.4286113 -4.4285064 -4.4283261 -4.4281745 -4.4281182 -4.428194 -4.4283094 -4.4283767 -4.4284506 -4.42852 -4.4285793 -4.42859][-4.4286075 -4.428637 -4.4286222 -4.4285388 -4.4283876 -4.4281635 -4.4279804 -4.4279094 -4.4280109 -4.4281726 -4.4282942 -4.4284406 -4.4285703 -4.4286585 -4.4286909][-4.42871 -4.4287 -4.4286494 -4.4285316 -4.4283643 -4.4281497 -4.4279785 -4.4279208 -4.4280448 -4.4282355 -4.4283924 -4.4285684 -4.4287095 -4.4287953 -4.4288278][-4.4288182 -4.4287882 -4.4287252 -4.428596 -4.4284363 -4.4282579 -4.4281259 -4.4280949 -4.4282312 -4.4284244 -4.4285817 -4.4287376 -4.4288454 -4.4288912 -4.428894][-4.4289045 -4.4288821 -4.4288335 -4.4287224 -4.4285855 -4.4284496 -4.4283562 -4.4283404 -4.4284663 -4.4286337 -4.4287643 -4.4288731 -4.4289279 -4.4289217 -4.428885][-4.4289384 -4.4289327 -4.4289107 -4.4288378 -4.4287443 -4.42866 -4.4285936 -4.4285784 -4.4286709 -4.4287906 -4.4288721 -4.4289141 -4.4289112 -4.4288692 -4.4288092][-4.4289303 -4.4289327 -4.4289327 -4.4288993 -4.4288464 -4.4288025 -4.4287586 -4.4287367 -4.4287829 -4.4288416 -4.428864 -4.4288378 -4.4287829 -4.4287148 -4.4286461][-4.4289203 -4.4289207 -4.4289279 -4.4289174 -4.4288869 -4.4288607 -4.4288297 -4.4288034 -4.42881 -4.4288068 -4.4287667 -4.4286838 -4.4285851 -4.428493 -4.4284348][-4.4289384 -4.42893 -4.4289312 -4.4289279 -4.4289079 -4.4288869 -4.4288568 -4.4288235 -4.4288 -4.4287496 -4.4286594 -4.4285359 -4.4284086 -4.4283085 -4.428268]]...]
