INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "42"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
sdfah Tensor("siamese_fc/conv5/def/offset2/BiasAdd:0", shape=(8, 8, 8, 72), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/BiasAdd:0", shape=(8, 6, 6, 128), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
sdfah Tensor("siamese_fc_1/conv5/def/offset2/BiasAdd:0", shape=(8, 22, 22, 72), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/BiasAdd:0", shape=(8, 20, 20, 128), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-03 08:40:25.533910: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:40:25.533951: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:40:25.533958: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:40:25.533962: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:40:25.533967: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:40:26.089634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-03 08:40:26.089674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-03 08:40:26.089681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-03 08:40:26.089693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-03 08:40:28.800372: step 0, loss = 0.53, batch loss = 0.40 (4.0 examples/sec; 2.006 sec/batch; 185h:14m:03s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-03 08:40:30.992451: step 10, loss = 0.50, batch loss = 0.37 (53.7 examples/sec; 0.149 sec/batch; 13h:45m:33s remains)
INFO - root - 2017-12-03 08:40:32.507672: step 20, loss = 0.66, batch loss = 0.52 (53.8 examples/sec; 0.149 sec/batch; 13h:43m:56s remains)
INFO - root - 2017-12-03 08:40:34.035464: step 30, loss = 0.54, batch loss = 0.41 (52.9 examples/sec; 0.151 sec/batch; 13h:58m:16s remains)
INFO - root - 2017-12-03 08:40:35.590544: step 40, loss = 0.55, batch loss = 0.41 (52.5 examples/sec; 0.152 sec/batch; 14h:03m:48s remains)
INFO - root - 2017-12-03 08:40:37.124301: step 50, loss = 0.60, batch loss = 0.46 (53.8 examples/sec; 0.149 sec/batch; 13h:43m:33s remains)
INFO - root - 2017-12-03 08:40:38.641988: step 60, loss = 0.58, batch loss = 0.45 (53.4 examples/sec; 0.150 sec/batch; 13h:50m:41s remains)
INFO - root - 2017-12-03 08:40:40.194632: step 70, loss = 0.49, batch loss = 0.36 (52.2 examples/sec; 0.153 sec/batch; 14h:08m:49s remains)
INFO - root - 2017-12-03 08:40:41.750680: step 80, loss = 0.65, batch loss = 0.51 (51.7 examples/sec; 0.155 sec/batch; 14h:16m:46s remains)
INFO - root - 2017-12-03 08:40:43.290522: step 90, loss = 0.60, batch loss = 0.46 (51.6 examples/sec; 0.155 sec/batch; 14h:18m:29s remains)
INFO - root - 2017-12-03 08:40:44.810283: step 100, loss = 0.62, batch loss = 0.49 (52.2 examples/sec; 0.153 sec/batch; 14h:09m:34s remains)
INFO - root - 2017-12-03 08:40:46.440429: step 110, loss = 0.57, batch loss = 0.44 (50.9 examples/sec; 0.157 sec/batch; 14h:31m:21s remains)
INFO - root - 2017-12-03 08:40:47.960982: step 120, loss = 0.66, batch loss = 0.53 (52.5 examples/sec; 0.152 sec/batch; 14h:04m:36s remains)
INFO - root - 2017-12-03 08:40:49.495616: step 130, loss = 0.61, batch loss = 0.47 (53.1 examples/sec; 0.151 sec/batch; 13h:54m:16s remains)
INFO - root - 2017-12-03 08:40:51.062911: step 140, loss = 0.49, batch loss = 0.35 (51.8 examples/sec; 0.154 sec/batch; 14h:15m:34s remains)
INFO - root - 2017-12-03 08:40:52.587670: step 150, loss = 0.57, batch loss = 0.44 (51.5 examples/sec; 0.155 sec/batch; 14h:20m:10s remains)
INFO - root - 2017-12-03 08:40:54.122497: step 160, loss = 0.68, batch loss = 0.54 (52.3 examples/sec; 0.153 sec/batch; 14h:07m:16s remains)
INFO - root - 2017-12-03 08:40:55.677382: step 170, loss = 0.57, batch loss = 0.43 (52.4 examples/sec; 0.153 sec/batch; 14h:06m:08s remains)
INFO - root - 2017-12-03 08:40:57.240252: step 180, loss = 0.57, batch loss = 0.44 (50.5 examples/sec; 0.158 sec/batch; 14h:37m:46s remains)
INFO - root - 2017-12-03 08:40:58.803555: step 190, loss = 0.44, batch loss = 0.31 (52.1 examples/sec; 0.153 sec/batch; 14h:10m:04s remains)
INFO - root - 2017-12-03 08:41:00.366885: step 200, loss = 0.54, batch loss = 0.40 (52.8 examples/sec; 0.152 sec/batch; 13h:59m:35s remains)
INFO - root - 2017-12-03 08:41:02.003999: step 210, loss = 0.61, batch loss = 0.48 (50.6 examples/sec; 0.158 sec/batch; 14h:35m:31s remains)
INFO - root - 2017-12-03 08:41:03.559107: step 220, loss = 0.58, batch loss = 0.44 (53.2 examples/sec; 0.150 sec/batch; 13h:52m:59s remains)
INFO - root - 2017-12-03 08:41:05.110366: step 230, loss = 0.63, batch loss = 0.50 (51.7 examples/sec; 0.155 sec/batch; 14h:16m:26s remains)
INFO - root - 2017-12-03 08:41:06.664338: step 240, loss = 0.79, batch loss = 0.66 (53.2 examples/sec; 0.150 sec/batch; 13h:52m:28s remains)
INFO - root - 2017-12-03 08:41:08.267544: step 250, loss = 0.54, batch loss = 0.41 (50.5 examples/sec; 0.159 sec/batch; 14h:38m:00s remains)
INFO - root - 2017-12-03 08:41:09.807703: step 260, loss = 0.72, batch loss = 0.59 (51.7 examples/sec; 0.155 sec/batch; 14h:17m:03s remains)
INFO - root - 2017-12-03 08:41:11.351528: step 270, loss = 0.68, batch loss = 0.55 (47.7 examples/sec; 0.168 sec/batch; 15h:29m:21s remains)
INFO - root - 2017-12-03 08:41:12.911720: step 280, loss = 0.68, batch loss = 0.54 (51.2 examples/sec; 0.156 sec/batch; 14h:24m:41s remains)
INFO - root - 2017-12-03 08:41:14.453560: step 290, loss = 0.58, batch loss = 0.45 (51.9 examples/sec; 0.154 sec/batch; 14h:14m:11s remains)
INFO - root - 2017-12-03 08:41:16.017297: step 300, loss = 0.61, batch loss = 0.48 (50.7 examples/sec; 0.158 sec/batch; 14h:33m:18s remains)
INFO - root - 2017-12-03 08:41:17.643582: step 310, loss = 0.58, batch loss = 0.45 (50.7 examples/sec; 0.158 sec/batch; 14h:33m:46s remains)
INFO - root - 2017-12-03 08:41:19.204097: step 320, loss = 0.67, batch loss = 0.54 (51.6 examples/sec; 0.155 sec/batch; 14h:17m:54s remains)
INFO - root - 2017-12-03 08:41:20.768379: step 330, loss = 0.69, batch loss = 0.56 (52.4 examples/sec; 0.153 sec/batch; 14h:04m:51s remains)
INFO - root - 2017-12-03 08:41:22.320772: step 340, loss = 0.65, batch loss = 0.51 (52.3 examples/sec; 0.153 sec/batch; 14h:06m:22s remains)
INFO - root - 2017-12-03 08:41:23.865608: step 350, loss = 0.72, batch loss = 0.59 (52.0 examples/sec; 0.154 sec/batch; 14h:11m:02s remains)
INFO - root - 2017-12-03 08:41:25.466172: step 360, loss = 0.70, batch loss = 0.56 (50.9 examples/sec; 0.157 sec/batch; 14h:29m:32s remains)
INFO - root - 2017-12-03 08:41:27.018480: step 370, loss = 0.82, batch loss = 0.68 (51.4 examples/sec; 0.156 sec/batch; 14h:21m:18s remains)
INFO - root - 2017-12-03 08:41:28.593773: step 380, loss = 0.75, batch loss = 0.61 (51.4 examples/sec; 0.156 sec/batch; 14h:21m:50s remains)
INFO - root - 2017-12-03 08:41:30.170973: step 390, loss = 0.57, batch loss = 0.44 (50.2 examples/sec; 0.159 sec/batch; 14h:41m:36s remains)
INFO - root - 2017-12-03 08:41:31.726177: step 400, loss = 0.51, batch loss = 0.37 (51.8 examples/sec; 0.154 sec/batch; 14h:14m:45s remains)
INFO - root - 2017-12-03 08:41:33.351054: step 410, loss = 0.56, batch loss = 0.42 (53.3 examples/sec; 0.150 sec/batch; 13h:51m:18s remains)
INFO - root - 2017-12-03 08:41:34.929343: step 420, loss = 0.71, batch loss = 0.58 (51.0 examples/sec; 0.157 sec/batch; 14h:28m:15s remains)
INFO - root - 2017-12-03 08:41:36.506853: step 430, loss = 0.61, batch loss = 0.48 (50.1 examples/sec; 0.160 sec/batch; 14h:43m:20s remains)
INFO - root - 2017-12-03 08:41:38.052793: step 440, loss = 0.64, batch loss = 0.50 (52.7 examples/sec; 0.152 sec/batch; 13h:59m:47s remains)
INFO - root - 2017-12-03 08:41:39.625021: step 450, loss = 0.60, batch loss = 0.47 (53.3 examples/sec; 0.150 sec/batch; 13h:51m:04s remains)
INFO - root - 2017-12-03 08:41:41.192529: step 460, loss = 0.47, batch loss = 0.33 (50.7 examples/sec; 0.158 sec/batch; 14h:32m:28s remains)
INFO - root - 2017-12-03 08:41:42.751617: step 470, loss = 0.56, batch loss = 0.42 (50.7 examples/sec; 0.158 sec/batch; 14h:32m:50s remains)
INFO - root - 2017-12-03 08:41:44.319729: step 480, loss = 0.75, batch loss = 0.61 (50.9 examples/sec; 0.157 sec/batch; 14h:29m:56s remains)
INFO - root - 2017-12-03 08:41:45.886777: step 490, loss = 0.53, batch loss = 0.40 (49.9 examples/sec; 0.160 sec/batch; 14h:47m:43s remains)
INFO - root - 2017-12-03 08:41:47.450628: step 500, loss = 0.62, batch loss = 0.49 (53.3 examples/sec; 0.150 sec/batch; 13h:50m:35s remains)
INFO - root - 2017-12-03 08:41:49.103244: step 510, loss = 0.55, batch loss = 0.41 (50.3 examples/sec; 0.159 sec/batch; 14h:39m:52s remains)
INFO - root - 2017-12-03 08:41:50.664441: step 520, loss = 0.80, batch loss = 0.67 (51.9 examples/sec; 0.154 sec/batch; 14h:13m:26s remains)
INFO - root - 2017-12-03 08:41:52.242400: step 530, loss = 0.59, batch loss = 0.45 (51.6 examples/sec; 0.155 sec/batch; 14h:17m:53s remains)
INFO - root - 2017-12-03 08:41:53.803119: step 540, loss = 0.53, batch loss = 0.40 (51.8 examples/sec; 0.154 sec/batch; 14h:13m:58s remains)
INFO - root - 2017-12-03 08:41:55.367481: step 550, loss = 0.57, batch loss = 0.44 (49.3 examples/sec; 0.162 sec/batch; 14h:57m:19s remains)
INFO - root - 2017-12-03 08:41:56.923643: step 560, loss = 0.58, batch loss = 0.45 (53.0 examples/sec; 0.151 sec/batch; 13h:55m:15s remains)
INFO - root - 2017-12-03 08:41:58.476053: step 570, loss = 0.53, batch loss = 0.39 (52.4 examples/sec; 0.153 sec/batch; 14h:05m:13s remains)
INFO - root - 2017-12-03 08:42:00.068483: step 580, loss = 0.62, batch loss = 0.49 (50.8 examples/sec; 0.158 sec/batch; 14h:31m:38s remains)
INFO - root - 2017-12-03 08:42:01.632639: step 590, loss = 0.51, batch loss = 0.37 (50.9 examples/sec; 0.157 sec/batch; 14h:29m:12s remains)
INFO - root - 2017-12-03 08:42:03.206098: step 600, loss = 0.70, batch loss = 0.56 (48.9 examples/sec; 0.164 sec/batch; 15h:05m:47s remains)
INFO - root - 2017-12-03 08:42:04.858080: step 610, loss = 0.84, batch loss = 0.71 (51.0 examples/sec; 0.157 sec/batch; 14h:27m:47s remains)
INFO - root - 2017-12-03 08:42:06.421407: step 620, loss = 0.60, batch loss = 0.47 (50.6 examples/sec; 0.158 sec/batch; 14h:34m:46s remains)
INFO - root - 2017-12-03 08:42:07.974298: step 630, loss = 0.65, batch loss = 0.51 (51.4 examples/sec; 0.156 sec/batch; 14h:21m:16s remains)
INFO - root - 2017-12-03 08:42:09.557949: step 640, loss = 0.49, batch loss = 0.35 (51.6 examples/sec; 0.155 sec/batch; 14h:17m:35s remains)
INFO - root - 2017-12-03 08:42:11.130480: step 650, loss = 0.69, batch loss = 0.56 (52.2 examples/sec; 0.153 sec/batch; 14h:07m:02s remains)
INFO - root - 2017-12-03 08:42:12.690722: step 660, loss = 0.52, batch loss = 0.38 (50.6 examples/sec; 0.158 sec/batch; 14h:34m:45s remains)
INFO - root - 2017-12-03 08:42:14.317959: step 670, loss = 0.57, batch loss = 0.43 (51.2 examples/sec; 0.156 sec/batch; 14h:23m:57s remains)
INFO - root - 2017-12-03 08:42:15.886776: step 680, loss = 0.64, batch loss = 0.50 (51.6 examples/sec; 0.155 sec/batch; 14h:17m:40s remains)
INFO - root - 2017-12-03 08:42:17.474204: step 690, loss = 0.49, batch loss = 0.36 (50.7 examples/sec; 0.158 sec/batch; 14h:33m:26s remains)
INFO - root - 2017-12-03 08:42:19.026134: step 700, loss = 0.53, batch loss = 0.40 (51.9 examples/sec; 0.154 sec/batch; 14h:11m:38s remains)
INFO - root - 2017-12-03 08:42:20.698850: step 710, loss = 0.58, batch loss = 0.45 (50.4 examples/sec; 0.159 sec/batch; 14h:37m:42s remains)
INFO - root - 2017-12-03 08:42:22.265096: step 720, loss = 0.65, batch loss = 0.51 (48.6 examples/sec; 0.165 sec/batch; 15h:11m:02s remains)
INFO - root - 2017-12-03 08:42:23.861553: step 730, loss = 0.59, batch loss = 0.46 (51.4 examples/sec; 0.156 sec/batch; 14h:20m:39s remains)
INFO - root - 2017-12-03 08:42:25.449679: step 740, loss = 0.51, batch loss = 0.38 (51.4 examples/sec; 0.155 sec/batch; 14h:19m:46s remains)
INFO - root - 2017-12-03 08:42:27.011868: step 750, loss = 0.62, batch loss = 0.48 (49.8 examples/sec; 0.161 sec/batch; 14h:47m:42s remains)
INFO - root - 2017-12-03 08:42:28.601165: step 760, loss = 0.58, batch loss = 0.45 (51.4 examples/sec; 0.156 sec/batch; 14h:21m:03s remains)
INFO - root - 2017-12-03 08:42:30.169277: step 770, loss = 0.74, batch loss = 0.60 (51.6 examples/sec; 0.155 sec/batch; 14h:16m:37s remains)
INFO - root - 2017-12-03 08:42:31.749735: step 780, loss = 0.59, batch loss = 0.46 (49.6 examples/sec; 0.161 sec/batch; 14h:51m:09s remains)
INFO - root - 2017-12-03 08:42:33.318801: step 790, loss = 0.73, batch loss = 0.59 (50.2 examples/sec; 0.159 sec/batch; 14h:41m:25s remains)
INFO - root - 2017-12-03 08:42:34.884857: step 800, loss = 0.61, batch loss = 0.48 (51.7 examples/sec; 0.155 sec/batch; 14h:15m:32s remains)
INFO - root - 2017-12-03 08:42:36.566503: step 810, loss = 0.59, batch loss = 0.45 (45.0 examples/sec; 0.178 sec/batch; 16h:23m:25s remains)
INFO - root - 2017-12-03 08:42:38.183262: step 820, loss = 0.67, batch loss = 0.54 (48.9 examples/sec; 0.164 sec/batch; 15h:05m:06s remains)
INFO - root - 2017-12-03 08:42:39.788032: step 830, loss = 0.56, batch loss = 0.43 (50.4 examples/sec; 0.159 sec/batch; 14h:37m:16s remains)
INFO - root - 2017-12-03 08:42:41.385245: step 840, loss = 0.66, batch loss = 0.52 (48.8 examples/sec; 0.164 sec/batch; 15h:06m:03s remains)
INFO - root - 2017-12-03 08:42:42.956405: step 850, loss = 0.61, batch loss = 0.47 (51.7 examples/sec; 0.155 sec/batch; 14h:15m:39s remains)
INFO - root - 2017-12-03 08:42:44.534793: step 860, loss = 0.57, batch loss = 0.44 (49.6 examples/sec; 0.161 sec/batch; 14h:51m:25s remains)
INFO - root - 2017-12-03 08:42:46.110868: step 870, loss = 0.70, batch loss = 0.57 (51.6 examples/sec; 0.155 sec/batch; 14h:16m:56s remains)
INFO - root - 2017-12-03 08:42:47.704799: step 880, loss = 0.66, batch loss = 0.52 (52.0 examples/sec; 0.154 sec/batch; 14h:09m:50s remains)
INFO - root - 2017-12-03 08:42:49.266022: step 890, loss = 0.47, batch loss = 0.33 (50.3 examples/sec; 0.159 sec/batch; 14h:39m:06s remains)
INFO - root - 2017-12-03 08:42:50.838743: step 900, loss = 0.64, batch loss = 0.50 (52.9 examples/sec; 0.151 sec/batch; 13h:55m:03s remains)
INFO - root - 2017-12-03 08:42:52.491454: step 910, loss = 0.68, batch loss = 0.54 (49.7 examples/sec; 0.161 sec/batch; 14h:50m:15s remains)
INFO - root - 2017-12-03 08:42:54.075014: step 920, loss = 0.68, batch loss = 0.54 (47.5 examples/sec; 0.169 sec/batch; 15h:31m:36s remains)
INFO - root - 2017-12-03 08:42:55.701450: step 930, loss = 0.70, batch loss = 0.57 (50.4 examples/sec; 0.159 sec/batch; 14h:37m:51s remains)
INFO - root - 2017-12-03 08:42:57.263944: step 940, loss = 0.67, batch loss = 0.54 (50.3 examples/sec; 0.159 sec/batch; 14h:38m:59s remains)
INFO - root - 2017-12-03 08:42:58.820940: step 950, loss = 0.51, batch loss = 0.38 (51.8 examples/sec; 0.155 sec/batch; 14h:14m:10s remains)
INFO - root - 2017-12-03 08:43:00.418238: step 960, loss = 0.66, batch loss = 0.53 (50.9 examples/sec; 0.157 sec/batch; 14h:28m:30s remains)
INFO - root - 2017-12-03 08:43:02.016740: step 970, loss = 0.63, batch loss = 0.50 (43.1 examples/sec; 0.185 sec/batch; 17h:04m:31s remains)
INFO - root - 2017-12-03 08:43:03.605020: step 980, loss = 0.57, batch loss = 0.43 (52.9 examples/sec; 0.151 sec/batch; 13h:55m:39s remains)
INFO - root - 2017-12-03 08:43:05.241835: step 990, loss = 0.57, batch loss = 0.44 (48.4 examples/sec; 0.165 sec/batch; 15h:13m:27s remains)
INFO - root - 2017-12-03 08:43:06.818126: step 1000, loss = 0.55, batch loss = 0.42 (51.3 examples/sec; 0.156 sec/batch; 14h:21m:30s remains)
INFO - root - 2017-12-03 08:43:08.486631: step 1010, loss = 0.57, batch loss = 0.43 (49.4 examples/sec; 0.162 sec/batch; 14h:54m:34s remains)
INFO - root - 2017-12-03 08:43:10.082980: step 1020, loss = 0.58, batch loss = 0.44 (49.4 examples/sec; 0.162 sec/batch; 14h:54m:33s remains)
INFO - root - 2017-12-03 08:43:11.687586: step 1030, loss = 0.57, batch loss = 0.43 (50.4 examples/sec; 0.159 sec/batch; 14h:36m:23s remains)
INFO - root - 2017-12-03 08:43:13.243423: step 1040, loss = 0.80, batch loss = 0.67 (50.6 examples/sec; 0.158 sec/batch; 14h:32m:50s remains)
INFO - root - 2017-12-03 08:43:14.829506: step 1050, loss = 0.69, batch loss = 0.55 (50.0 examples/sec; 0.160 sec/batch; 14h:43m:57s remains)
INFO - root - 2017-12-03 08:43:16.430660: step 1060, loss = 0.67, batch loss = 0.54 (48.5 examples/sec; 0.165 sec/batch; 15h:11m:50s remains)
INFO - root - 2017-12-03 08:43:18.034837: step 1070, loss = 0.51, batch loss = 0.38 (48.4 examples/sec; 0.165 sec/batch; 15h:12m:15s remains)
INFO - root - 2017-12-03 08:43:19.633063: step 1080, loss = 0.62, batch loss = 0.48 (51.8 examples/sec; 0.155 sec/batch; 14h:13m:53s remains)
INFO - root - 2017-12-03 08:43:21.210159: step 1090, loss = 0.49, batch loss = 0.36 (50.5 examples/sec; 0.159 sec/batch; 14h:35m:33s remains)
INFO - root - 2017-12-03 08:43:22.783915: step 1100, loss = 0.76, batch loss = 0.62 (51.9 examples/sec; 0.154 sec/batch; 14h:10m:47s remains)
INFO - root - 2017-12-03 08:43:24.454542: step 1110, loss = 0.57, batch loss = 0.44 (47.1 examples/sec; 0.170 sec/batch; 15h:37m:14s remains)
INFO - root - 2017-12-03 08:43:26.026315: step 1120, loss = 0.54, batch loss = 0.40 (50.7 examples/sec; 0.158 sec/batch; 14h:31m:21s remains)
INFO - root - 2017-12-03 08:43:27.628093: step 1130, loss = 0.63, batch loss = 0.50 (51.0 examples/sec; 0.157 sec/batch; 14h:26m:24s remains)
INFO - root - 2017-12-03 08:43:29.190933: step 1140, loss = 0.70, batch loss = 0.56 (50.3 examples/sec; 0.159 sec/batch; 14h:37m:34s remains)
INFO - root - 2017-12-03 08:43:30.774048: step 1150, loss = 0.52, batch loss = 0.39 (49.2 examples/sec; 0.163 sec/batch; 14h:58m:43s remains)
INFO - root - 2017-12-03 08:43:32.365951: step 1160, loss = 0.67, batch loss = 0.54 (50.8 examples/sec; 0.158 sec/batch; 14h:30m:14s remains)
INFO - root - 2017-12-03 08:43:33.976320: step 1170, loss = 0.60, batch loss = 0.46 (48.3 examples/sec; 0.166 sec/batch; 15h:14m:50s remains)
INFO - root - 2017-12-03 08:43:35.563527: step 1180, loss = 0.81, batch loss = 0.68 (50.5 examples/sec; 0.158 sec/batch; 14h:34m:38s remains)
INFO - root - 2017-12-03 08:43:37.179557: step 1190, loss = 0.68, batch loss = 0.55 (51.7 examples/sec; 0.155 sec/batch; 14h:13m:50s remains)
INFO - root - 2017-12-03 08:43:38.745056: step 1200, loss = 0.72, batch loss = 0.59 (51.5 examples/sec; 0.155 sec/batch; 14h:17m:44s remains)
INFO - root - 2017-12-03 08:43:40.416986: step 1210, loss = 0.59, batch loss = 0.45 (49.8 examples/sec; 0.161 sec/batch; 14h:46m:44s remains)
INFO - root - 2017-12-03 08:43:41.999943: step 1220, loss = 0.54, batch loss = 0.41 (49.1 examples/sec; 0.163 sec/batch; 14h:59m:13s remains)
INFO - root - 2017-12-03 08:43:43.602522: step 1230, loss = 0.67, batch loss = 0.53 (50.8 examples/sec; 0.158 sec/batch; 14h:29m:51s remains)
INFO - root - 2017-12-03 08:43:45.197850: step 1240, loss = 0.65, batch loss = 0.52 (50.8 examples/sec; 0.157 sec/batch; 14h:29m:02s remains)
INFO - root - 2017-12-03 08:43:46.774400: step 1250, loss = 0.66, batch loss = 0.53 (51.9 examples/sec; 0.154 sec/batch; 14h:10m:38s remains)
INFO - root - 2017-12-03 08:43:48.364658: step 1260, loss = 0.62, batch loss = 0.48 (50.9 examples/sec; 0.157 sec/batch; 14h:26m:53s remains)
INFO - root - 2017-12-03 08:43:49.926521: step 1270, loss = 0.49, batch loss = 0.36 (51.0 examples/sec; 0.157 sec/batch; 14h:25m:52s remains)
INFO - root - 2017-12-03 08:43:51.524763: step 1280, loss = 0.66, batch loss = 0.52 (46.4 examples/sec; 0.172 sec/batch; 15h:51m:31s remains)
INFO - root - 2017-12-03 08:43:53.125017: step 1290, loss = 0.60, batch loss = 0.47 (50.9 examples/sec; 0.157 sec/batch; 14h:27m:14s remains)
INFO - root - 2017-12-03 08:43:54.702222: step 1300, loss = 0.74, batch loss = 0.61 (50.6 examples/sec; 0.158 sec/batch; 14h:32m:17s remains)
INFO - root - 2017-12-03 08:43:56.387016: step 1310, loss = 0.61, batch loss = 0.48 (50.3 examples/sec; 0.159 sec/batch; 14h:37m:51s remains)
INFO - root - 2017-12-03 08:43:57.958842: step 1320, loss = 0.66, batch loss = 0.53 (50.7 examples/sec; 0.158 sec/batch; 14h:30m:11s remains)
INFO - root - 2017-12-03 08:43:59.561148: step 1330, loss = 0.70, batch loss = 0.56 (50.5 examples/sec; 0.158 sec/batch; 14h:33m:47s remains)
INFO - root - 2017-12-03 08:44:01.153280: step 1340, loss = 0.54, batch loss = 0.40 (50.8 examples/sec; 0.157 sec/batch; 14h:28m:34s remains)
INFO - root - 2017-12-03 08:44:02.743682: step 1350, loss = 0.57, batch loss = 0.43 (50.9 examples/sec; 0.157 sec/batch; 14h:26m:47s remains)
INFO - root - 2017-12-03 08:44:04.325183: step 1360, loss = 0.65, batch loss = 0.52 (51.8 examples/sec; 0.155 sec/batch; 14h:12m:55s remains)
INFO - root - 2017-12-03 08:44:05.890793: step 1370, loss = 0.60, batch loss = 0.46 (50.9 examples/sec; 0.157 sec/batch; 14h:27m:07s remains)
INFO - root - 2017-12-03 08:44:07.483736: step 1380, loss = 0.68, batch loss = 0.55 (49.3 examples/sec; 0.162 sec/batch; 14h:56m:13s remains)
INFO - root - 2017-12-03 08:44:09.073632: step 1390, loss = 0.60, batch loss = 0.46 (50.3 examples/sec; 0.159 sec/batch; 14h:38m:27s remains)
INFO - root - 2017-12-03 08:44:10.691221: step 1400, loss = 0.64, batch loss = 0.51 (52.4 examples/sec; 0.153 sec/batch; 14h:02m:01s remains)
INFO - root - 2017-12-03 08:44:12.393930: step 1410, loss = 0.63, batch loss = 0.50 (49.2 examples/sec; 0.162 sec/batch; 14h:56m:28s remains)
INFO - root - 2017-12-03 08:44:13.989590: step 1420, loss = 0.62, batch loss = 0.49 (51.1 examples/sec; 0.157 sec/batch; 14h:24m:07s remains)
