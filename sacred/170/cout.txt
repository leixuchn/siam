INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "170"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-09 06:17:09.299207: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:17:09.299246: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:17:09.299252: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:17:09.299256: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:17:09.299260: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:17:12.982909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-09 06:17:12.982947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-09 06:17:12.982954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-09 06:17:12.982961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/Relu:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/Relu:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-09 06:17:28.801771: step 0, loss = 0.90, batch loss = 0.69 (0.8 examples/sec; 10.466 sec/batch; 966h:40m:52s remains)
2017-12-09 06:17:29.478471: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00012014503 0.00012900461 0.00013777231 0.00014284109 0.00014709089 0.00015331857 0.00015680316 0.000159223 0.0001591566 0.00015495626 0.00014908789 0.00014282274 0.00013500487 0.00012798874 0.00012321568][0.00012361782 0.00013574964 0.00014867824 0.00015642639 0.00016424652 0.00017363884 0.00018169812 0.00018379983 0.00018239512 0.00017810203 0.00016932077 0.00015745887 0.00014581918 0.0001355897 0.00012711227][0.00012647914 0.00013953773 0.00015444092 0.00016711213 0.00017809134 0.00019424658 0.00021275072 0.00021800312 0.00021571171 0.00021068321 0.00019644739 0.00017622217 0.00015790682 0.00014204171 0.0001303508][0.00013245523 0.00014639518 0.00016234978 0.00017685599 0.00019659825 0.00022794069 0.00025750627 0.00026558549 0.0002619166 0.00025139117 0.00022668831 0.00019892327 0.00017414044 0.00015142139 0.00013438211][0.00013782911 0.00015182115 0.00017062972 0.00019448665 0.00022546746 0.0002721278 0.00031072437 0.0003234405 0.00031313498 0.00029163639 0.00025695583 0.00021934626 0.00018750328 0.0001569419 0.00013743267][0.00014120746 0.00015609922 0.00017692264 0.00020885737 0.00025161245 0.0003125176 0.00036972825 0.00038498978 0.00035318491 0.00031665945 0.00026811418 0.00022300177 0.00018659477 0.00015778364 0.00013695715][0.00014168845 0.00015510665 0.00017997457 0.00021755464 0.00026942691 0.00034250109 0.00041911562 0.00040698258 0.00034245287 0.00029414095 0.00024705773 0.00020373966 0.000175222 0.00015340027 0.00013521321][0.00013969206 0.00015510287 0.00017918817 0.00021340627 0.00025696674 0.00031111386 0.00036059978 0.00033581385 0.00028366345 0.00024690051 0.00021355922 0.00018118547 0.00016182194 0.00014702241 0.00013195546][0.00013550668 0.00015142975 0.00017446166 0.00020119784 0.00022761889 0.00025616642 0.00026880958 0.00024663444 0.00022333454 0.00020430521 0.00018462623 0.00016523691 0.00015308116 0.00014110448 0.00013002395][0.00012989402 0.00014676862 0.00016566094 0.00018197109 0.00020240129 0.00021408813 0.00021091021 0.00019529165 0.00018079526 0.00016993674 0.00016089673 0.000149045 0.00014228409 0.00013588839 0.00012854433][0.00012436227 0.00013798929 0.00015049282 0.00016044576 0.00017384865 0.00017753222 0.00017441486 0.00016332637 0.00015605443 0.0001490868 0.00014758069 0.00013878841 0.00013415504 0.00013089368 0.000126402][0.00012336366 0.00013077322 0.00013817512 0.00014263226 0.00015268866 0.00015760721 0.00015689323 0.00015190788 0.00014528628 0.00013861299 0.00013975379 0.00013329781 0.00013071841 0.00012735772 0.00012484813][0.00012537751 0.00012826023 0.0001334151 0.00013988213 0.00014793203 0.00014822537 0.00014779312 0.00014667724 0.00013966308 0.00013454001 0.00013519595 0.00013072704 0.00012886821 0.00012521289 0.00012339442][0.00012918294 0.00013120948 0.00013510913 0.0001421034 0.00014568248 0.00014338274 0.0001445205 0.00014345498 0.00013902091 0.0001342857 0.00013442041 0.00012975791 0.00012798126 0.00012449994 0.000123309][0.00012876726 0.0001316569 0.00013428678 0.00013895151 0.00014012073 0.0001392534 0.00014075616 0.00014178226 0.00013921178 0.00013653014 0.00013776078 0.00013152954 0.00012939978 0.00012618763 0.00012515353]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip-zeroinit-relu-bias-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip-zeroinit-relu-bias-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 06:17:36.088609: step 10, loss = 0.90, batch loss = 0.69 (16.5 examples/sec; 0.484 sec/batch; 44h:44m:27s remains)
INFO - root - 2017-12-09 06:17:40.676108: step 20, loss = 0.90, batch loss = 0.69 (17.7 examples/sec; 0.453 sec/batch; 41h:47m:59s remains)
INFO - root - 2017-12-09 06:17:45.205046: step 30, loss = 0.90, batch loss = 0.69 (17.1 examples/sec; 0.469 sec/batch; 43h:18m:48s remains)
INFO - root - 2017-12-09 06:17:49.819103: step 40, loss = 0.90, batch loss = 0.69 (17.5 examples/sec; 0.456 sec/batch; 42h:09m:24s remains)
INFO - root - 2017-12-09 06:17:54.410481: step 50, loss = 0.90, batch loss = 0.69 (17.7 examples/sec; 0.451 sec/batch; 41h:37m:44s remains)
INFO - root - 2017-12-09 06:17:59.059913: step 60, loss = 0.90, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:59m:52s remains)
INFO - root - 2017-12-09 06:18:03.360579: step 70, loss = 0.90, batch loss = 0.69 (16.2 examples/sec; 0.495 sec/batch; 45h:44m:23s remains)
INFO - root - 2017-12-09 06:18:07.970401: step 80, loss = 0.90, batch loss = 0.69 (16.9 examples/sec; 0.472 sec/batch; 43h:36m:31s remains)
INFO - root - 2017-12-09 06:18:12.603917: step 90, loss = 0.90, batch loss = 0.69 (17.7 examples/sec; 0.452 sec/batch; 41h:45m:46s remains)
INFO - root - 2017-12-09 06:18:17.617021: step 100, loss = 0.90, batch loss = 0.69 (13.3 examples/sec; 0.602 sec/batch; 55h:33m:31s remains)
2017-12-09 06:18:19.164280: I tensorflow/core/kernels/logging_ops.cc:79] [[[8.1097278e-05 8.4442385e-05 8.8503344e-05 9.0762936e-05 9.8744895e-05 0.00011197535 0.00012540979 0.00013304934 0.00013614856 0.00013691113 0.00012838752 0.00011472201 0.00010457968 9.6681077e-05 9.1199159e-05][8.2947678e-05 8.8033827e-05 9.2296468e-05 9.4529147e-05 9.9871526e-05 0.00011126709 0.00012070178 0.00012646226 0.00012882466 0.000128051 0.00012069726 0.00010866974 0.00010062003 9.2687071e-05 8.7708184e-05][7.966502e-05 8.7047636e-05 9.4193929e-05 9.8101009e-05 0.0001046398 0.00011678285 0.00012287628 0.00012523984 0.00012409795 0.0001202027 0.00011296201 0.00010122076 9.379386e-05 8.6976026e-05 8.3102816e-05][7.7959783e-05 8.7417713e-05 9.8550925e-05 0.00010606804 0.00011301289 0.00012579944 0.00013178699 0.00012909507 0.00012380006 0.00011644444 0.00010987226 9.8160541e-05 8.9586531e-05 8.3044361e-05 7.8763951e-05][8.1432525e-05 9.2642971e-05 0.00010475779 0.00011653062 0.00012634063 0.00014063106 0.0001469611 0.00014110953 0.00013036872 0.00011946729 0.00011087155 9.8722143e-05 9.0359412e-05 8.2587234e-05 7.7360281e-05][8.2914506e-05 9.5808056e-05 0.00010891618 0.00012349499 0.00013577461 0.00014936313 0.00015519338 0.00014759481 0.00013278093 0.00011862024 0.00010761866 9.7667493e-05 8.9186455e-05 8.1655977e-05 7.7228673e-05][8.1142985e-05 9.4218471e-05 0.00010999746 0.00012688128 0.00014062712 0.00015082219 0.00015622117 0.00014925185 0.00013240355 0.00011807067 0.00010620296 9.5455463e-05 8.6157881e-05 7.93363e-05 7.5277036e-05][7.7629185e-05 8.9525529e-05 0.00010515196 0.00012198794 0.0001360515 0.00014442543 0.00015032571 0.00014363149 0.00012837228 0.00011483143 0.00010213137 9.0097732e-05 8.1039849e-05 7.3995405e-05 7.0400907e-05][7.4086107e-05 8.4846055e-05 9.7340206e-05 0.00011017272 0.00012267793 0.00012845969 0.00013238152 0.0001260573 0.00011522918 0.00010343886 9.131027e-05 8.1914062e-05 7.5277974e-05 7.0351787e-05 6.8075744e-05][6.7962705e-05 7.9091791e-05 9.0570189e-05 0.00010018582 0.00010897966 0.00011254942 0.00011326422 0.0001083033 9.94241e-05 9.0698129e-05 8.2381011e-05 7.6729171e-05 7.3657349e-05 7.0503767e-05 6.9600857e-05][6.5888868e-05 7.5131466e-05 8.2124388e-05 8.7121196e-05 9.1479851e-05 9.414371e-05 9.3579183e-05 9.125936e-05 8.5412976e-05 8.03009e-05 7.6380173e-05 7.3737632e-05 7.2519564e-05 7.1502822e-05 7.1341507e-05][6.4883774e-05 7.13658e-05 7.5146054e-05 7.8273304e-05 8.1091006e-05 8.158273e-05 8.2384504e-05 8.1733757e-05 7.8876394e-05 7.6361575e-05 7.3419993e-05 7.2265153e-05 7.1380739e-05 7.2040159e-05 7.1889583e-05][6.5127089e-05 6.91691e-05 7.149575e-05 7.2196766e-05 7.3428906e-05 7.3552117e-05 7.3742682e-05 7.3396077e-05 7.2593328e-05 7.2165123e-05 7.0774746e-05 7.0560971e-05 7.0734459e-05 7.2090377e-05 7.3492251e-05][6.2251347e-05 6.4800886e-05 6.6086177e-05 6.6018983e-05 6.6633511e-05 6.6835448e-05 6.6191169e-05 6.6075627e-05 6.6443332e-05 6.6547786e-05 6.6384753e-05 6.7663867e-05 6.9538968e-05 7.174036e-05 7.4770513e-05][6.167918e-05 6.4428415e-05 6.5265987e-05 6.496175e-05 6.4990927e-05 6.4866355e-05 6.44672e-05 6.4596075e-05 6.5060325e-05 6.49263e-05 6.5138694e-05 6.6168956e-05 6.8036781e-05 7.028658e-05 7.3816271e-05]]...]
INFO - root - 2017-12-09 06:18:25.121625: step 110, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.687 sec/batch; 63h:26m:34s remains)
INFO - root - 2017-12-09 06:18:31.900378: step 120, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.686 sec/batch; 63h:18m:13s remains)
INFO - root - 2017-12-09 06:18:38.612096: step 130, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:40m:41s remains)
INFO - root - 2017-12-09 06:18:45.470942: step 140, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:27m:43s remains)
INFO - root - 2017-12-09 06:18:52.284268: step 150, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.664 sec/batch; 61h:20m:03s remains)
INFO - root - 2017-12-09 06:18:58.983479: step 160, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:32m:00s remains)
INFO - root - 2017-12-09 06:19:05.564067: step 170, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.684 sec/batch; 63h:10m:19s remains)
INFO - root - 2017-12-09 06:19:12.419918: step 180, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.672 sec/batch; 62h:03m:29s remains)
INFO - root - 2017-12-09 06:19:19.216865: step 190, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.675 sec/batch; 62h:19m:29s remains)
INFO - root - 2017-12-09 06:19:26.072296: step 200, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 61h:27m:21s remains)
2017-12-09 06:19:26.804697: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.5663033e-05 3.6542908e-05 3.6287696e-05 3.5980131e-05 3.5777073e-05 3.5548772e-05 3.5405028e-05 3.519253e-05 3.5251593e-05 3.5519308e-05 3.5624224e-05 3.5720634e-05 3.597777e-05 3.6304831e-05 3.6545174e-05][3.5869816e-05 3.6736314e-05 3.6300225e-05 3.592292e-05 3.5557903e-05 3.5095578e-05 3.4759356e-05 3.4492041e-05 3.4802375e-05 3.5461962e-05 3.5732115e-05 3.589995e-05 3.60471e-05 3.6478006e-05 3.6718659e-05][3.6661197e-05 3.745623e-05 3.7091912e-05 3.6884459e-05 3.6588441e-05 3.6050045e-05 3.5696161e-05 3.6252026e-05 3.7654972e-05 3.8792765e-05 3.8787868e-05 3.8583497e-05 3.8317452e-05 3.8129645e-05 3.7590493e-05][3.6199657e-05 3.6690628e-05 3.6406618e-05 3.6908245e-05 3.7145837e-05 3.7695838e-05 3.8435806e-05 4.1033756e-05 4.4514265e-05 4.5668414e-05 4.5294437e-05 4.4600729e-05 4.3480424e-05 4.1764055e-05 3.9967868e-05][3.5052384e-05 3.5539018e-05 3.5447632e-05 3.5967103e-05 3.7188729e-05 3.9396586e-05 4.2845495e-05 4.781011e-05 5.204327e-05 5.3649528e-05 5.2849242e-05 5.1022733e-05 4.8320268e-05 4.5010827e-05 4.2182121e-05][3.4411303e-05 3.567e-05 3.5399826e-05 3.6774505e-05 3.9996547e-05 4.4909215e-05 5.0856925e-05 5.6671259e-05 6.0285976e-05 6.0752343e-05 5.9063834e-05 5.55814e-05 5.1279454e-05 4.6989571e-05 4.3344709e-05][3.4396413e-05 3.6542064e-05 3.7176575e-05 4.014057e-05 4.575411e-05 5.2943851e-05 6.1758044e-05 6.9287882e-05 7.0692782e-05 6.7932131e-05 6.3328742e-05 5.835859e-05 5.3335756e-05 4.8269347e-05 4.3963802e-05][3.5596484e-05 3.8252783e-05 4.0484851e-05 4.5541692e-05 5.3045314e-05 6.1100094e-05 7.1790273e-05 7.9701429e-05 7.7849669e-05 7.1722359e-05 6.5538887e-05 5.99643e-05 5.4907483e-05 4.9648475e-05 4.4979806e-05][3.8688133e-05 4.2250089e-05 4.6100391e-05 5.1219569e-05 5.7826303e-05 6.5243308e-05 7.3402545e-05 7.7032913e-05 7.4579715e-05 6.8037152e-05 6.240404e-05 5.8351081e-05 5.4547323e-05 4.9812032e-05 4.5061763e-05][4.3445958e-05 4.79559e-05 5.2141164e-05 5.5667555e-05 6.1162042e-05 6.5460954e-05 6.8393594e-05 6.8583126e-05 6.6012639e-05 6.1529609e-05 5.7644105e-05 5.5470879e-05 5.3648164e-05 4.9594721e-05 4.4910896e-05][4.64883e-05 5.1157618e-05 5.4507393e-05 5.7055095e-05 6.0869414e-05 6.2609448e-05 6.410788e-05 6.3369756e-05 6.0672075e-05 5.7510271e-05 5.517156e-05 5.39817e-05 5.2482454e-05 4.8969829e-05 4.4819571e-05][4.6289948e-05 5.066656e-05 5.287895e-05 5.4451419e-05 5.6373476e-05 5.743188e-05 5.8931659e-05 5.8983544e-05 5.6750931e-05 5.4650325e-05 5.3701271e-05 5.3295655e-05 5.1613231e-05 4.8251633e-05 4.4530821e-05][4.4483611e-05 4.7155489e-05 4.8400325e-05 4.9486047e-05 5.0987175e-05 5.2454103e-05 5.4111286e-05 5.4221659e-05 5.2377152e-05 5.0823121e-05 5.0738669e-05 5.1224557e-05 4.9446902e-05 4.6105444e-05 4.2796237e-05][4.1735333e-05 4.3905551e-05 4.4772354e-05 4.5584406e-05 4.6631481e-05 4.7682148e-05 4.8487491e-05 4.8504655e-05 4.7111716e-05 4.6215024e-05 4.6987487e-05 4.7317666e-05 4.5817385e-05 4.3069122e-05 4.0598323e-05][3.9164981e-05 4.1057388e-05 4.2122218e-05 4.2499665e-05 4.3112246e-05 4.3644643e-05 4.446214e-05 4.4126387e-05 4.3065822e-05 4.2654065e-05 4.3151467e-05 4.3278927e-05 4.194543e-05 4.0130948e-05 3.8705421e-05]]...]
INFO - root - 2017-12-09 06:19:33.362539: step 210, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.675 sec/batch; 62h:16m:19s remains)
INFO - root - 2017-12-09 06:19:40.041360: step 220, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:14m:04s remains)
INFO - root - 2017-12-09 06:19:46.770736: step 230, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:27m:32s remains)
INFO - root - 2017-12-09 06:19:53.725870: step 240, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.714 sec/batch; 65h:54m:39s remains)
INFO - root - 2017-12-09 06:20:00.519422: step 250, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.668 sec/batch; 61h:38m:19s remains)
INFO - root - 2017-12-09 06:20:07.285413: step 260, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.697 sec/batch; 64h:19m:55s remains)
INFO - root - 2017-12-09 06:20:13.980969: step 270, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.691 sec/batch; 63h:45m:17s remains)
INFO - root - 2017-12-09 06:20:20.818939: step 280, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.697 sec/batch; 64h:19m:19s remains)
INFO - root - 2017-12-09 06:20:27.607443: step 290, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.682 sec/batch; 62h:53m:32s remains)
INFO - root - 2017-12-09 06:20:34.371083: step 300, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:41m:05s remains)
2017-12-09 06:20:35.056713: I tensorflow/core/kernels/logging_ops.cc:79] [[[9.8316581e-05 0.0001009334 0.00010554823 0.00011784428 0.00015351961 0.00022016917 0.00030000834 0.00036384817 0.00039253128 0.00038155061 0.00033072088 0.000257956 0.00018732116 0.00013497211 0.00010770053][8.8805224e-05 9.086659e-05 0.00010298366 0.00015386553 0.00027883373 0.00049198186 0.00075739378 0.0010088069 0.0011804107 0.0012194064 0.0011044294 0.0008825392 0.00062413444 0.00039736953 0.00023754689][8.6699096e-05 9.2277012e-05 0.00012541226 0.00025227171 0.00053898728 0.0010094374 0.0016157512 0.0022413125 0.0027284641 0.0029347735 0.0027984162 0.0023699813 0.0017884681 0.0012056722 0.00072943466][8.7728469e-05 0.00010089877 0.00017773549 0.00042055879 0.0009306405 0.0017387753 0.00279134 0.0039300337 0.0048969388 0.0054311221 0.0053931605 0.0048252712 0.0039107515 0.0028670731 0.0019005849][8.4627769e-05 0.00011170939 0.00024879721 0.00063460553 0.0014011199 0.0025746822 0.0040975404 0.0057833767 0.0072942059 0.0082747 0.0084939208 0.007945329 0.0068083797 0.0053331819 0.003818091][9.4208983e-05 0.0001386433 0.00032624273 0.00083601219 0.0018221805 0.0033099798 0.0052387048 0.0074059791 0.0094373561 0.010914267 0.011528541 0.011204678 0.010055983 0.0082984539 0.006289342][9.4048526e-05 0.00014542369 0.00035440474 0.00091406633 0.0020086295 0.0036765756 0.00588111 0.0084216036 0.010889884 0.012845983 0.013944759 0.014041428 0.013117 0.011308894 0.0089723626][8.9570545e-05 0.00013315825 0.00032428626 0.00084974506 0.0019086763 0.0035802636 0.0058724359 0.0086273709 0.011426124 0.013798702 0.015390471 0.0160085 0.015538405 0.013991901 0.011619165][8.391335e-05 0.00011636045 0.00026611786 0.00070078281 0.001612629 0.003124269 0.00529304 0.008025011 0.010951993 0.013595881 0.015584467 0.016702596 0.016809331 0.015821505 0.013797286][8.2912949e-05 0.00010362437 0.00020505743 0.00052573375 0.0012354825 0.00247776 0.0043407478 0.0067993184 0.0095862169 0.012273178 0.014512843 0.016066983 0.016764976 0.016464876 0.015075902][0.00023943096 0.00023363117 0.00027444976 0.00044896227 0.00090080668 0.0017791885 0.0031943922 0.005177794 0.0075727897 0.01006749 0.01237157 0.014237024 0.01544397 0.015782664 0.015070879][0.00022600892 0.00021755937 0.00024375657 0.00036836389 0.00065877917 0.0012303456 0.002192548 0.0036064894 0.0054238406 0.0074715205 0.0095831221 0.011526531 0.013056902 0.013900808 0.013823244][0.00020074133 0.00019367436 0.00020644096 0.00026978509 0.00041814617 0.00073407177 0.0013042742 0.0022090145 0.0034466307 0.0049663843 0.0067144013 0.0085177822 0.010145104 0.011295734 0.011706387][0.00016639594 0.0001629515 0.00017148737 0.0001995217 0.00026412169 0.00041422516 0.00070671452 0.0012026895 0.0019288074 0.002906302 0.0041640522 0.0056370185 0.0071439166 0.0083988607 0.0091281394][0.00013534955 0.00013344172 0.00013869671 0.00014778218 0.00016970614 0.00023092917 0.00036468141 0.00060455542 0.00097249536 0.0015091518 0.0022892787 0.0033237478 0.0045139897 0.0056505408 0.006482156]]...]
INFO - root - 2017-12-09 06:20:41.666163: step 310, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:18m:40s remains)
INFO - root - 2017-12-09 06:20:48.338265: step 320, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:31m:23s remains)
INFO - root - 2017-12-09 06:20:55.017124: step 330, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 59h:15m:13s remains)
INFO - root - 2017-12-09 06:21:01.791549: step 340, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:37m:45s remains)
INFO - root - 2017-12-09 06:21:08.493276: step 350, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.688 sec/batch; 63h:28m:45s remains)
INFO - root - 2017-12-09 06:21:15.340374: step 360, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:37m:53s remains)
INFO - root - 2017-12-09 06:21:21.950125: step 370, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.641 sec/batch; 59h:08m:26s remains)
INFO - root - 2017-12-09 06:21:28.692755: step 380, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.695 sec/batch; 64h:08m:38s remains)
INFO - root - 2017-12-09 06:21:35.557277: step 390, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 61h:24m:56s remains)
INFO - root - 2017-12-09 06:21:42.402829: step 400, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 60h:59m:39s remains)
2017-12-09 06:21:43.040029: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00014617009 0.00018279708 0.00022371515 0.00027785296 0.0003442929 0.00042229207 0.00050354941 0.000578858 0.00064159225 0.0006872532 0.00072056986 0.00074869982 0.00077731156 0.00080725335 0.00083410408][7.0967428e-05 9.0354144e-05 0.00011427096 0.00014581709 0.00018555764 0.00023531669 0.00029209428 0.00035101431 0.00040665461 0.00045774909 0.00050604792 0.00055316993 0.00060151087 0.00065126125 0.00069763185][4.0239509e-05 4.7975642e-05 5.7788617e-05 7.2457151e-05 9.0969661e-05 0.00011483525 0.00014454234 0.00017876748 0.00021485443 0.00025422842 0.00029820076 0.00034786528 0.00040679117 0.00047333408 0.00054156565][2.6832422e-05 3.0593241e-05 3.5089812e-05 4.071321e-05 4.6920621e-05 5.5049903e-05 6.6140492e-05 8.036982e-05 9.721917e-05 0.00011949101 0.00014798471 0.00018852061 0.00024342546 0.00031652718 0.00040370572][1.9105963e-05 2.129253e-05 2.4183937e-05 2.6637637e-05 2.8562761e-05 3.0028874e-05 3.1786847e-05 3.6343481e-05 4.2636646e-05 5.2884112e-05 6.8873982e-05 9.5953139e-05 0.00013838848 0.00020587683 0.00029911881][1.859937e-05 2.0218635e-05 2.1240809e-05 2.1235734e-05 2.1428703e-05 2.1380773e-05 2.1757394e-05 2.2865119e-05 2.4516878e-05 2.8699804e-05 3.5946847e-05 5.14953e-05 8.0752543e-05 0.00013430422 0.00021985362][2.2121676e-05 2.4560206e-05 2.5276237e-05 2.3709319e-05 2.271729e-05 2.1726944e-05 2.188273e-05 2.2478627e-05 2.2566328e-05 2.3485311e-05 2.6355468e-05 3.4970886e-05 5.4194716e-05 9.2487615e-05 0.0001612193][2.3592449e-05 2.6818641e-05 2.6964044e-05 2.5114383e-05 2.3480592e-05 2.2434415e-05 2.2649354e-05 2.3221495e-05 2.3804801e-05 2.4099154e-05 2.4293651e-05 2.8394788e-05 3.9352744e-05 6.4425622e-05 0.00011484842][2.2705644e-05 2.7443908e-05 2.8388247e-05 2.7098562e-05 2.522194e-05 2.406535e-05 2.3655743e-05 2.2788667e-05 2.1966869e-05 2.1968441e-05 2.1742941e-05 2.4058932e-05 3.1096271e-05 4.7124187e-05 8.1267863e-05][2.0725289e-05 2.5988476e-05 2.8199429e-05 2.8674534e-05 2.7256268e-05 2.6817208e-05 2.714157e-05 2.726377e-05 2.6179427e-05 2.2384203e-05 2.1972672e-05 2.3285018e-05 2.8151007e-05 3.8440114e-05 6.0291502e-05][1.7173355e-05 2.1526292e-05 2.4226894e-05 2.5929974e-05 2.5789628e-05 2.5922091e-05 2.6129361e-05 2.6261878e-05 2.4767316e-05 2.1957509e-05 2.1447533e-05 2.1972875e-05 2.4962421e-05 3.0025731e-05 4.5037887e-05][1.6285721e-05 2.0031363e-05 2.2507786e-05 2.4373505e-05 2.465432e-05 2.5331079e-05 2.5548215e-05 2.5077523e-05 2.3545967e-05 2.1246826e-05 1.9781306e-05 1.937573e-05 2.0477415e-05 2.3238616e-05 3.2504686e-05][1.6408045e-05 1.9319923e-05 2.1782875e-05 2.4331352e-05 2.557872e-05 2.6222209e-05 2.5796689e-05 2.4822504e-05 2.3159639e-05 2.0932919e-05 1.8332856e-05 1.6421374e-05 1.6302474e-05 1.7634207e-05 2.2753055e-05][1.8290437e-05 2.1109765e-05 2.2692959e-05 2.4681802e-05 2.6576054e-05 2.7472277e-05 2.7286136e-05 2.6129586e-05 2.3879449e-05 2.1428474e-05 1.8239938e-05 1.5310641e-05 1.3827477e-05 1.4303871e-05 1.7199789e-05][1.9869389e-05 2.2405464e-05 2.3997996e-05 2.532345e-05 2.6729736e-05 2.7500369e-05 2.7868067e-05 2.7279697e-05 2.5518078e-05 2.2831286e-05 1.9348838e-05 1.5708716e-05 1.2958437e-05 1.2164375e-05 1.3913908e-05]]...]
INFO - root - 2017-12-09 06:21:49.522073: step 410, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:54m:47s remains)
INFO - root - 2017-12-09 06:21:56.174525: step 420, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.665 sec/batch; 61h:22m:42s remains)
INFO - root - 2017-12-09 06:22:02.915261: step 430, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:19m:58s remains)
INFO - root - 2017-12-09 06:22:09.637400: step 440, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.650 sec/batch; 59h:56m:42s remains)
INFO - root - 2017-12-09 06:22:16.442647: step 450, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.664 sec/batch; 61h:13m:06s remains)
INFO - root - 2017-12-09 06:22:23.236260: step 460, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:39m:53s remains)
INFO - root - 2017-12-09 06:22:29.830907: step 470, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 62h:06m:07s remains)
INFO - root - 2017-12-09 06:22:36.663625: step 480, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.684 sec/batch; 63h:05m:24s remains)
INFO - root - 2017-12-09 06:22:43.395061: step 490, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.682 sec/batch; 62h:51m:44s remains)
INFO - root - 2017-12-09 06:22:50.045660: step 500, loss = 0.90, batch loss = 0.69 (14.3 examples/sec; 0.558 sec/batch; 51h:29m:12s remains)
2017-12-09 06:22:50.797841: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0027353326 0.0035421138 0.0042176819 0.00470951 0.0049293907 0.0048148227 0.0043633953 0.0036867394 0.0029095223 0.0021388896 0.0014813327 0.0010269212 0.00083366112 0.00086243491 0.001012809][0.0038900597 0.0047451039 0.0053183208 0.0055766008 0.0054798238 0.0050463122 0.0043325415 0.0034973456 0.0026678967 0.0019182005 0.0012966465 0.00084347854 0.000604571 0.00054731313 0.00059954217][0.0047400156 0.0055640955 0.0059789005 0.0059750946 0.0055734995 0.0048675258 0.0039772326 0.0030819897 0.0022975686 0.0016578699 0.001153596 0.00077011652 0.00052657415 0.00040284265 0.00035806419][0.0052113659 0.0059808176 0.0062635052 0.006081549 0.0055081756 0.0046825595 0.0037648643 0.0029300181 0.00227711 0.0017908234 0.0014151306 0.001096084 0.00083213364 0.00061877083 0.00044933916][0.0054002455 0.0061279945 0.0063592708 0.0061515588 0.0056119519 0.0048835254 0.0041287891 0.0034811317 0.0030127633 0.0026717281 0.0023762309 0.0020513714 0.0016924897 0.0013220642 0.00096330506][0.0055274391 0.0062540374 0.0065366277 0.0064683296 0.0061510294 0.0057078046 0.0052705077 0.0048949695 0.0046146689 0.0043624472 0.0040591904 0.0036377951 0.003103249 0.0025056682 0.0018920921][0.0058642379 0.0066001727 0.0069731949 0.0071090763 0.0070766918 0.0069515971 0.006816511 0.0066641816 0.0064788368 0.0062081534 0.0058047767 0.005235875 0.0045214174 0.0037182046 0.0028864224][0.0062928163 0.0070360582 0.0074833212 0.0077749975 0.0079498142 0.008018231 0.0080253817 0.0079346532 0.0076977722 0.0072968472 0.0067342576 0.0060212361 0.0051861913 0.0042823595 0.0033688005][0.0064832093 0.0071934671 0.0076512313 0.0079844538 0.0082067633 0.0083048921 0.0082688751 0.0080711022 0.0076792194 0.0071133012 0.0064205844 0.0056352634 0.0047902651 0.0039247214 0.0030836854][0.0059964568 0.0065997965 0.0069731749 0.0072454126 0.0074110078 0.0074414634 0.0072922595 0.0069448436 0.0064027896 0.005730934 0.0050028842 0.0042692781 0.0035513553 0.0028643284 0.0022293425][0.0046888157 0.0051171547 0.0053392141 0.0054976842 0.0055856816 0.0055530537 0.0053433781 0.0049493564 0.0044001807 0.0037787014 0.0031597505 0.0025893897 0.0020805572 0.0016374561 0.0012595316][0.0029487594 0.0031815164 0.0032610744 0.0033121861 0.0033372191 0.003287043 0.0031117848 0.0028109851 0.0024163823 0.0019915251 0.0015881736 0.0012374102 0.00094711717 0.00071801181 0.00054133969][0.0014501911 0.0015555765 0.0015509769 0.0015368343 0.0015257343 0.0014895212 0.0013975495 0.0012498267 0.0010604806 0.00085749483 0.00066226308 0.00049062091 0.00034996489 0.00024043025 0.00016284063][0.00056137313 0.00062175439 0.00060551765 0.00058041228 0.00056697265 0.000555579 0.0005301974 0.00048449519 0.00041897071 0.00033928995 0.00025365519 0.00017176707 9.9799843e-05 4.153671e-05 7.0295937e-07][0.00014163075 0.00017892806 0.00017321353 0.00015742793 0.00015111479 0.00015232786 0.00015381297 0.00014733085 0.00013032746 0.00010160539 6.4707536e-05 2.5610061e-05 -1.3149242e-05 -4.5465407e-05 -6.83474e-05]]...]
INFO - root - 2017-12-09 06:22:57.593229: step 510, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.682 sec/batch; 62h:55m:09s remains)
INFO - root - 2017-12-09 06:23:04.370323: step 520, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:22m:53s remains)
INFO - root - 2017-12-09 06:23:11.076146: step 530, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.664 sec/batch; 61h:11m:04s remains)
INFO - root - 2017-12-09 06:23:17.853709: step 540, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.686 sec/batch; 63h:14m:14s remains)
INFO - root - 2017-12-09 06:23:24.703765: step 550, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.700 sec/batch; 64h:31m:52s remains)
INFO - root - 2017-12-09 06:23:31.419367: step 560, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:27m:19s remains)
INFO - root - 2017-12-09 06:23:38.008694: step 570, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:42m:25s remains)
INFO - root - 2017-12-09 06:23:44.746948: step 580, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.681 sec/batch; 62h:49m:23s remains)
INFO - root - 2017-12-09 06:23:51.393794: step 590, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:05m:49s remains)
INFO - root - 2017-12-09 06:23:57.978314: step 600, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:19m:48s remains)
2017-12-09 06:23:58.719883: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11275852 0.1219051 0.12956031 0.13656078 0.14273743 0.1478997 0.15169974 0.15378284 0.15378456 0.15174411 0.14788751 0.14297231 0.13781929 0.13291867 0.12876105][0.12329751 0.13566895 0.14648172 0.15647411 0.16544987 0.173083 0.17880331 0.182004 0.1822814 0.17978916 0.17443982 0.16720511 0.15923324 0.15129922 0.14436357][0.12978329 0.1454581 0.15964471 0.17304528 0.18516065 0.19544187 0.20318554 0.20758344 0.20832026 0.20545834 0.19886316 0.18945648 0.17856351 0.167328 0.15704733][0.13430148 0.15269156 0.16994511 0.18654126 0.20178233 0.21477152 0.224624 0.23029184 0.23148623 0.22848429 0.22110724 0.21008493 0.19677824 0.18275328 0.16941282][0.13799295 0.15864511 0.17829776 0.19731143 0.21477973 0.22990283 0.24113587 0.24757978 0.24913982 0.24624586 0.23865 0.22682373 0.21204104 0.1960354 0.18028826][0.14006028 0.16260704 0.18414156 0.20501648 0.22435179 0.240714 0.2525112 0.25894627 0.26035729 0.25744489 0.24989115 0.23792735 0.22261478 0.20558858 0.18832053][0.14083356 0.1645903 0.18746951 0.20979892 0.2303097 0.24741974 0.25929049 0.26538953 0.26648083 0.263525 0.2562342 0.24459586 0.22939597 0.21208353 0.19406046][0.13936353 0.16339834 0.18704337 0.21010664 0.23142917 0.24895535 0.26074708 0.26641345 0.26705065 0.26382658 0.25663573 0.24545027 0.23083702 0.21398431 0.19609393][0.13630149 0.15959719 0.18270105 0.20543174 0.22651792 0.24393263 0.25541887 0.2607874 0.26119867 0.25780967 0.25074863 0.24012561 0.22642088 0.21064706 0.19379072][0.1325312 0.15417989 0.17566016 0.19681366 0.21647282 0.23286173 0.24361065 0.24860846 0.24901639 0.24579181 0.23916912 0.22941826 0.21705857 0.20293668 0.18774557][0.12823941 0.14758891 0.166581 0.18495737 0.20222168 0.21660604 0.22623923 0.23090555 0.23152985 0.22883 0.22310111 0.21469578 0.20413169 0.19209425 0.17922764][0.12443794 0.140921 0.15663126 0.1714237 0.1854037 0.19724774 0.20533964 0.20952733 0.21042295 0.20851135 0.20405936 0.19739541 0.18899179 0.17943354 0.16923149][0.12095612 0.1343578 0.14664672 0.15760313 0.16796787 0.17701443 0.18348621 0.1872053 0.1884487 0.1874994 0.18450181 0.17975464 0.17368115 0.16671118 0.15919977][0.11780906 0.12849429 0.1376463 0.14534701 0.15239719 0.15876858 0.16358478 0.16676715 0.16821685 0.16811755 0.16649409 0.16352569 0.1595895 0.15498288 0.14994825][0.11577252 0.12454388 0.13136175 0.13658835 0.14091232 0.14491315 0.1482299 0.15067926 0.15206392 0.15247421 0.15186132 0.15034637 0.14815079 0.14547147 0.14246127]]...]
INFO - root - 2017-12-09 06:24:05.334606: step 610, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.635 sec/batch; 58h:31m:12s remains)
INFO - root - 2017-12-09 06:24:12.040273: step 620, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.653 sec/batch; 60h:12m:49s remains)
INFO - root - 2017-12-09 06:24:18.726307: step 630, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.665 sec/batch; 61h:20m:35s remains)
INFO - root - 2017-12-09 06:24:25.549740: step 640, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.684 sec/batch; 63h:01m:58s remains)
INFO - root - 2017-12-09 06:24:32.372202: step 650, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:47m:41s remains)
INFO - root - 2017-12-09 06:24:39.131106: step 660, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.675 sec/batch; 62h:11m:43s remains)
INFO - root - 2017-12-09 06:24:45.674681: step 670, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 62h:01m:38s remains)
INFO - root - 2017-12-09 06:24:52.371934: step 680, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:08m:34s remains)
INFO - root - 2017-12-09 06:24:59.147283: step 690, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.703 sec/batch; 64h:50m:04s remains)
INFO - root - 2017-12-09 06:25:05.874111: step 700, loss = 0.90, batch loss = 0.69 (11.0 examples/sec; 0.730 sec/batch; 67h:16m:22s remains)
2017-12-09 06:25:06.574499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00083193526 -0.00083230215 -0.00083291327 -0.00083343219 -0.00083370833 -0.00083371444 -0.00083345716 -0.0008330018 -0.00083242723 -0.00083191768 -0.00083154184 -0.00083130394 -0.00083116518 -0.00083110447 -0.00083107752][-0.00083190564 -0.00083240389 -0.00083322392 -0.00083395175 -0.00083436636 -0.00083438907 -0.00083403534 -0.00083339051 -0.00083259889 -0.00083185907 -0.00083131727 -0.00083100208 -0.00083082193 -0.00083073857 -0.00083069614][-0.00083217386 -0.00083289231 -0.00083396456 -0.00083493418 -0.00083548238 -0.00083547289 -0.000834962 -0.00083410507 -0.00083310314 -0.00083215529 -0.00083142595 -0.00083102606 -0.00083082332 -0.00083072117 -0.00083067676][-0.00083255296 -0.00083350384 -0.00083484146 -0.00083605875 -0.00083671941 -0.00083660934 -0.000835906 -0.00083485217 -0.00083362049 -0.00083249452 -0.00083159941 -0.00083113415 -0.00083090487 -0.00083077425 -0.00083070539][-0.00083299814 -0.00083417463 -0.00083575363 -0.00083717611 -0.00083793135 -0.00083767041 -0.0008367683 -0.00083551242 -0.00083409 -0.00083283166 -0.000831866 -0.00083139993 -0.00083111186 -0.00083092123 -0.00083079049][-0.000833394 -0.00083479658 -0.00083658122 -0.00083820469 -0.000839101 -0.00083875458 -0.000837732 -0.00083634938 -0.00083476806 -0.00083334959 -0.00083230226 -0.00083182339 -0.000831444 -0.00083114952 -0.0008309182][-0.00083355437 -0.00083507557 -0.00083694875 -0.00083869562 -0.00083968625 -0.00083938404 -0.00083836971 -0.00083703763 -0.00083545875 -0.000833989 -0.00083292747 -0.00083239627 -0.0008318742 -0.00083141244 -0.00083104073][-0.00083347346 -0.0008349336 -0.00083667639 -0.000838307 -0.0008392696 -0.00083904195 -0.00083815155 -0.000836986 -0.00083565922 -0.000834362 -0.00083339913 -0.00083282695 -0.00083219138 -0.00083158934 -0.00083111553][-0.0008331096 -0.00083438412 -0.0008358995 -0.00083728664 -0.00083812949 -0.00083801086 -0.00083729089 -0.00083632267 -0.00083528319 -0.00083426572 -0.00083349267 -0.0008329706 -0.000832321 -0.000831674 -0.00083115505][-0.00083257805 -0.00083359872 -0.00083480863 -0.0008358832 -0.00083655323 -0.0008365286 -0.00083601213 -0.00083527708 -0.00083454716 -0.00083388231 -0.00083330576 -0.00083286595 -0.00083225709 -0.000831645 -0.00083114445][-0.00083200913 -0.00083274755 -0.00083368027 -0.00083450717 -0.00083505508 -0.00083512906 -0.00083484343 -0.00083435292 -0.00083384471 -0.00083340035 -0.00083295681 -0.00083254831 -0.0008320245 -0.00083149108 -0.000831061][-0.00083147862 -0.00083190924 -0.00083257153 -0.00083316205 -0.00083359325 -0.00083376124 -0.00083366665 -0.00083341694 -0.00083309488 -0.000832786 -0.00083244708 -0.00083208276 -0.0008316561 -0.0008312359 -0.00083090656][-0.00083108613 -0.00083124684 -0.0008316423 -0.00083197985 -0.00083226303 -0.00083243335 -0.00083241262 -0.00083228049 -0.00083208637 -0.00083190214 -0.00083166163 -0.00083139737 -0.00083113939 -0.0008309033 -0.00083073252][-0.00083084282 -0.00083079445 -0.00083096989 -0.0008311164 -0.0008312462 -0.00083134 -0.00083130784 -0.0008312221 -0.00083111326 -0.00083101395 -0.00083090411 -0.00083078258 -0.00083070167 -0.00083062967 -0.00083059364][-0.00083072437 -0.00083055859 -0.00083062303 -0.00083067181 -0.0008307207 -0.00083074596 -0.00083070277 -0.00083062879 -0.00083053345 -0.00083046773 -0.00083042157 -0.00083039486 -0.00083040388 -0.00083043828 -0.00083048648]]...]
INFO - root - 2017-12-09 06:25:13.309009: step 710, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.672 sec/batch; 61h:55m:28s remains)
INFO - root - 2017-12-09 06:25:19.994880: step 720, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.683 sec/batch; 62h:57m:05s remains)
INFO - root - 2017-12-09 06:25:26.664657: step 730, loss = 0.91, batch loss = 0.70 (11.6 examples/sec; 0.687 sec/batch; 63h:20m:47s remains)
INFO - root - 2017-12-09 06:25:33.469594: step 740, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.675 sec/batch; 62h:12m:24s remains)
INFO - root - 2017-12-09 06:25:40.090958: step 750, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:36m:56s remains)
INFO - root - 2017-12-09 06:25:46.900536: step 760, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.684 sec/batch; 63h:01m:40s remains)
INFO - root - 2017-12-09 06:25:53.522087: step 770, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:34m:00s remains)
INFO - root - 2017-12-09 06:26:00.344909: step 780, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:19m:09s remains)
INFO - root - 2017-12-09 06:26:07.150558: step 790, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.713 sec/batch; 65h:40m:42s remains)
INFO - root - 2017-12-09 06:26:13.840753: step 800, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:38m:53s remains)
2017-12-09 06:26:14.577262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0010346426 -0.0010333755 -0.0010322316 -0.0010299 -0.0010257798 -0.0010207832 -0.0010158274 -0.0010116828 -0.0010090448 -0.0010087475 -0.001010273 -0.0010121282 -0.0010136444 -0.0010146174 -0.0010150108][-0.0010379021 -0.0010368648 -0.0010361074 -0.0010345536 -0.0010316549 -0.0010281713 -0.0010247563 -0.0010219513 -0.001020225 -0.0010203677 -0.001021562 -0.0010227805 -0.0010237176 -0.0010242254 -0.00102431][-0.0010416241 -0.001040878 -0.0010404502 -0.0010396269 -0.0010378874 -0.0010358887 -0.0010339536 -0.0010324363 -0.0010315628 -0.0010317453 -0.0010324265 -0.0010328429 -0.0010330383 -0.0010328298 -0.0010323807][-0.0010450457 -0.0010444194 -0.0010440501 -0.0010435727 -0.0010425771 -0.0010416061 -0.0010407335 -0.0010400269 -0.0010396233 -0.001039739 -0.0010400085 -0.0010398001 -0.0010394205 -0.0010388527 -0.0010381449][-0.0010480611 -0.0010472438 -0.0010465443 -0.0010458272 -0.0010449562 -0.0010442683 -0.0010436787 -0.001043239 -0.0010429429 -0.0010429203 -0.0010426824 -0.0010419681 -0.0010411367 -0.0010401104 -0.0010388388][-0.0010500277 -0.0010487669 -0.0010474554 -0.0010460989 -0.0010448048 -0.0010437112 -0.0010427479 -0.0010420716 -0.0010413871 -0.001040671 -0.0010393847 -0.0010376384 -0.0010359716 -0.0010341529 -0.0010321198][-0.0010500068 -0.0010480494 -0.0010458677 -0.0010434553 -0.0010411708 -0.0010391237 -0.001037379 -0.0010360356 -0.0010347349 -0.0010332277 -0.0010310044 -0.0010284012 -0.0010258836 -0.0010231377 -0.0010202435][-0.0010471065 -0.0010443036 -0.0010410857 -0.0010375563 -0.0010341996 -0.0010311401 -0.0010285006 -0.0010263675 -0.0010247083 -0.0010230162 -0.0010204811 -0.0010173639 -0.0010142083 -0.0010107482 -0.0010071273][-0.0010411426 -0.0010375459 -0.0010335345 -0.001029218 -0.0010251489 -0.0010214581 -0.0010183726 -0.0010160556 -0.0010144818 -0.0010129526 -0.0010103346 -0.0010070358 -0.0010035743 -0.00099964777 -0.00099549722][-0.00103234 -0.0010282858 -0.0010239901 -0.00101946 -0.0010152841 -0.0010116283 -0.0010087504 -0.0010066855 -0.0010053335 -0.0010039506 -0.0010014626 -0.00099835172 -0.00099485868 -0.000990723 -0.0009863103][-0.0010220448 -0.0010179336 -0.0010137911 -0.0010095959 -0.001005923 -0.001002849 -0.0010006186 -0.000999046 -0.00099800015 -0.00099685474 -0.00099481991 -0.00099215435 -0.000988874 -0.00098478911 -0.00098056474][-0.00101225 -0.0010083826 -0.0010046249 -0.0010009822 -0.00099798443 -0.00099562656 -0.00099412492 -0.00099323713 -0.00099280267 -0.00099223712 -0.00099086727 -0.000988786 -0.00098600017 -0.0009823885 -0.00097862817][-0.0010030448 -0.000999503 -0.00099639257 -0.000993606 -0.00099134247 -0.00098969927 -0.00098882115 -0.00098852208 -0.00098858308 -0.0009884882 -0.00098763441 -0.00098613254 -0.00098403846 -0.00098121667 -0.00097819045][-0.00099542015 -0.00099230884 -0.00098974886 -0.00098783686 -0.00098631461 -0.00098531484 -0.00098487223 -0.0009848288 -0.000984974 -0.0009849784 -0.00098446524 -0.00098347745 -0.00098205556 -0.00098005217 -0.00097781746][-0.00098972209 -0.00098698062 -0.0009850068 -0.00098362216 -0.0009825679 -0.00098192226 -0.000981646 -0.00098162808 -0.00098170619 -0.00098167267 -0.00098131341 -0.00098063378 -0.00097963435 -0.00097822014 -0.00097665016]]...]
INFO - root - 2017-12-09 06:26:21.243834: step 810, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:30m:59s remains)
INFO - root - 2017-12-09 06:26:27.915910: step 820, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:20m:17s remains)
INFO - root - 2017-12-09 06:26:34.669969: step 830, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.655 sec/batch; 60h:21m:04s remains)
INFO - root - 2017-12-09 06:26:41.416252: step 840, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.684 sec/batch; 63h:01m:46s remains)
INFO - root - 2017-12-09 06:26:48.180792: step 850, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.681 sec/batch; 62h:46m:27s remains)
INFO - root - 2017-12-09 06:26:54.891482: step 860, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:15m:32s remains)
INFO - root - 2017-12-09 06:27:01.452176: step 870, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.665 sec/batch; 61h:14m:31s remains)
INFO - root - 2017-12-09 06:27:08.344811: step 880, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.670 sec/batch; 61h:44m:29s remains)
INFO - root - 2017-12-09 06:27:15.177600: step 890, loss = 0.91, batch loss = 0.70 (11.5 examples/sec; 0.693 sec/batch; 63h:49m:48s remains)
INFO - root - 2017-12-09 06:27:21.886989: step 900, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.683 sec/batch; 62h:53m:40s remains)
2017-12-09 06:27:22.630044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012657949 -0.0012657525 -0.0012655458 -0.0012653478 -0.0012651809 -0.0012651205 -0.0012650333 -0.0012650173 -0.0012650457 -0.001265069 -0.0012651378 -0.0012649328 -0.0012644824 -0.0012618958 -0.0012522911][-0.0012655045 -0.0012657811 -0.0012659789 -0.001265683 -0.0012649153 -0.0012643188 -0.0012641053 -0.0012643551 -0.0012644182 -0.0012643721 -0.0012643473 -0.0012641528 -0.0012624899 -0.0012521124 -0.0012217651][-0.0012610777 -0.0012613063 -0.0012631573 -0.0012634466 -0.0012618338 -0.0012606111 -0.0012611111 -0.0012629896 -0.0012640234 -0.0012641588 -0.0012637775 -0.0012622402 -0.0012550348 -0.0012248643 -0.0011537427][-0.0012457127 -0.0012461926 -0.0012535894 -0.0012575934 -0.0012561739 -0.0012547079 -0.0012564583 -0.0012606907 -0.001263367 -0.0012639395 -0.0012621856 -0.0012551879 -0.0012325944 -0.0011656496 -0.0010371581][-0.0012100507 -0.0012127387 -0.0012331737 -0.0012472424 -0.0012486862 -0.0012477582 -0.0012511977 -0.0012577464 -0.0012622589 -0.0012633705 -0.0012575283 -0.0012366712 -0.0011842186 -0.0010665874 -0.0008755076][-0.0011547529 -0.0011644586 -0.0012042971 -0.00123433 -0.0012420129 -0.0012426927 -0.001247461 -0.0012551483 -0.0012609201 -0.0012622515 -0.0012480502 -0.001202875 -0.0011100884 -0.000944244 -0.00070418103][-0.0011129552 -0.0011303372 -0.0011829791 -0.0012261567 -0.0012397087 -0.0012418375 -0.0012468059 -0.0012536547 -0.0012596203 -0.001260415 -0.001235539 -0.0011613893 -0.0010275589 -0.00082569744 -0.00055944751][-0.0011068399 -0.0011321265 -0.0011830152 -0.0012267767 -0.0012420902 -0.0012451878 -0.0012493157 -0.001254166 -0.0012591713 -0.0012587431 -0.0012265379 -0.001134672 -0.00098028022 -0.00076914678 -0.00050734164][-0.0011417496 -0.0011612197 -0.0011977735 -0.0012326202 -0.0012466194 -0.0012506497 -0.0012538709 -0.0012570641 -0.0012601386 -0.0012578382 -0.0012273185 -0.0011388875 -0.00099272258 -0.00080267061 -0.00058046239][-0.0011793103 -0.0011940142 -0.0012178489 -0.0012392526 -0.0012499571 -0.0012548456 -0.0012581099 -0.0012602382 -0.0012611914 -0.0012580918 -0.0012359464 -0.0011673346 -0.0010536102 -0.00090595358 -0.00074602384][-0.0011952693 -0.0012056755 -0.0012246803 -0.0012384608 -0.0012473839 -0.0012534598 -0.0012580899 -0.0012606521 -0.0012614107 -0.0012591262 -0.0012459819 -0.0012025401 -0.0011312165 -0.0010318576 -0.00092950044][-0.0011784171 -0.0011853271 -0.0012070056 -0.0012213467 -0.0012325438 -0.0012416702 -0.0012495317 -0.0012549884 -0.0012578731 -0.0012582373 -0.001252766 -0.0012332487 -0.0012002254 -0.0011459967 -0.0010921039][-0.0011049506 -0.0011150944 -0.0011478465 -0.0011719383 -0.0011926382 -0.0012095196 -0.0012248135 -0.00123699 -0.0012455273 -0.0012509516 -0.0012520653 -0.0012481729 -0.0012400363 -0.0012222457 -0.0012025409][-0.00093662203 -0.0009669906 -0.0010261617 -0.0010727337 -0.0011126593 -0.0011437997 -0.0011718418 -0.0011946579 -0.0012127525 -0.0012268589 -0.0012359853 -0.0012424605 -0.001245581 -0.0012474528 -0.0012461909][-0.0006463998 -0.00070933113 -0.00080881332 -0.00089272531 -0.00096688129 -0.0010236058 -0.0010733939 -0.0011132475 -0.0011468051 -0.0011746375 -0.0011955268 -0.0012117595 -0.0012234841 -0.0012336888 -0.0012406087]]...]
INFO - root - 2017-12-09 06:27:29.360518: step 910, loss = 0.91, batch loss = 0.70 (12.7 examples/sec; 0.632 sec/batch; 58h:13m:21s remains)
INFO - root - 2017-12-09 06:27:36.046710: step 920, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 60h:55m:34s remains)
INFO - root - 2017-12-09 06:27:42.797780: step 930, loss = 0.91, batch loss = 0.70 (11.8 examples/sec; 0.676 sec/batch; 62h:15m:43s remains)
INFO - root - 2017-12-09 06:27:49.732743: step 940, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 61h:57m:17s remains)
INFO - root - 2017-12-09 06:27:56.427825: step 950, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:00m:58s remains)
INFO - root - 2017-12-09 06:28:03.173821: step 960, loss = 0.91, batch loss = 0.69 (11.6 examples/sec; 0.687 sec/batch; 63h:16m:41s remains)
INFO - root - 2017-12-09 06:28:09.671510: step 970, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.664 sec/batch; 61h:09m:55s remains)
INFO - root - 2017-12-09 06:28:16.464032: step 980, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.659 sec/batch; 60h:42m:56s remains)
INFO - root - 2017-12-09 06:28:23.185717: step 990, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:13m:00s remains)
INFO - root - 2017-12-09 06:28:29.848321: step 1000, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.668 sec/batch; 61h:31m:35s remains)
2017-12-09 06:28:30.518069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0020191611 -0.0020186747 -0.0020190512 -0.0020197546 -0.0020205772 -0.0020213965 -0.0020221707 -0.0020227234 -0.0020228687 -0.0020224634 -0.0020208554 -0.0020177683 -0.0020134188 -0.0020081555 -0.0020027878][-0.0020216084 -0.0020210049 -0.0020211365 -0.0020214072 -0.0020216224 -0.0020218005 -0.0020220233 -0.0020222277 -0.002022221 -0.002021736 -0.0020201702 -0.0020171485 -0.0020128211 -0.0020074344 -0.0020018525][-0.0020238408 -0.0020230385 -0.0020226878 -0.0020222878 -0.0020217479 -0.0020211285 -0.0020207241 -0.0020205572 -0.0020204773 -0.0020201898 -0.0020189439 -0.0020162256 -0.0020121722 -0.0020070111 -0.0020015824][-0.0020242624 -0.0020232336 -0.0020223225 -0.0020212191 -0.0020198529 -0.0020184864 -0.0020175797 -0.002017237 -0.0020173043 -0.0020173693 -0.0020166729 -0.0020145031 -0.0020108139 -0.0020060258 -0.0020008767][-0.0020231972 -0.0020221316 -0.0020209402 -0.0020193551 -0.0020173371 -0.0020155169 -0.0020143334 -0.0020139473 -0.0020141199 -0.0020145525 -0.0020144046 -0.0020127734 -0.0020094304 -0.0020049552 -0.0020000692][-0.0020210387 -0.0020198671 -0.0020184573 -0.0020165441 -0.0020142032 -0.0020121157 -0.0020107944 -0.0020104863 -0.0020108887 -0.0020116747 -0.0020119906 -0.0020108523 -0.0020078556 -0.002003578 -0.0019989971][-0.0020180126 -0.0020166321 -0.0020149613 -0.0020127632 -0.0020102544 -0.002008104 -0.002006792 -0.0020067364 -0.0020076795 -0.0020090104 -0.0020098218 -0.0020090761 -0.0020063445 -0.0020022353 -0.0019980045][-0.0020137781 -0.0020121871 -0.0020104165 -0.0020081883 -0.0020059543 -0.0020042353 -0.0020033778 -0.0020038744 -0.0020055678 -0.0020074619 -0.0020084966 -0.0020078069 -0.0020051391 -0.0020011847 -0.0019972487][-0.0020089939 -0.0020073266 -0.0020057447 -0.0020039536 -0.0020023666 -0.0020014716 -0.0020014329 -0.0020025973 -0.0020048232 -0.00200685 -0.0020077424 -0.002006917 -0.0020041934 -0.0020003328 -0.0019966085][-0.0020040725 -0.0020024811 -0.0020014166 -0.002000394 -0.0019997417 -0.001999825 -0.0020007023 -0.0020024355 -0.0020047524 -0.0020066248 -0.0020072125 -0.0020061014 -0.0020032767 -0.0019994669 -0.0019959344][-0.0019999561 -0.0019984143 -0.001997835 -0.0019975018 -0.0019976664 -0.0019985726 -0.0020001207 -0.0020021226 -0.0020042725 -0.0020057647 -0.0020059403 -0.0020045419 -0.0020016828 -0.0019981498 -0.001994998][-0.0019966718 -0.0019950881 -0.0019948047 -0.0019949395 -0.0019955949 -0.0019969547 -0.0019988024 -0.0020008041 -0.0020026467 -0.0020036639 -0.0020035408 -0.0020021356 -0.0019995549 -0.0019965067 -0.0019939109][-0.0019942804 -0.0019926685 -0.0019924962 -0.0019928478 -0.0019936727 -0.0019951002 -0.0019968918 -0.0019986625 -0.0020000925 -0.0020007433 -0.0020004623 -0.0019992159 -0.0019971153 -0.0019947002 -0.0019927511][-0.0019927847 -0.0019912014 -0.0019911407 -0.0019915544 -0.0019923225 -0.001993503 -0.0019949467 -0.0019963048 -0.0019973 -0.0019976723 -0.0019973598 -0.00199641 -0.001994858 -0.0019931055 -0.0019917639][-0.0019920142 -0.0019904764 -0.0019904189 -0.0019907723 -0.0019913479 -0.0019921802 -0.0019931935 -0.0019941027 -0.0019947004 -0.0019948587 -0.0019945954 -0.0019939563 -0.0019929374 -0.0019918145 -0.0019910082]]...]
INFO - root - 2017-12-09 06:28:37.123054: step 1010, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:15m:57s remains)
INFO - root - 2017-12-09 06:28:43.924921: step 1020, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.674 sec/batch; 62h:05m:05s remains)
INFO - root - 2017-12-09 06:28:50.660469: step 1030, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.681 sec/batch; 62h:44m:43s remains)
INFO - root - 2017-12-09 06:28:57.346020: step 1040, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.653 sec/batch; 60h:08m:35s remains)
INFO - root - 2017-12-09 06:29:04.139249: step 1050, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.688 sec/batch; 63h:21m:00s remains)
INFO - root - 2017-12-09 06:29:10.802690: step 1060, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.680 sec/batch; 62h:34m:48s remains)
INFO - root - 2017-12-09 06:29:17.448233: step 1070, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.682 sec/batch; 62h:47m:57s remains)
INFO - root - 2017-12-09 06:29:24.201222: step 1080, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 60h:53m:44s remains)
INFO - root - 2017-12-09 06:29:30.816692: step 1090, loss = 0.90, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:12m:32s remains)
INFO - root - 2017-12-09 06:29:37.572480: step 1100, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.684 sec/batch; 62h:55m:32s remains)
2017-12-09 06:29:38.310988: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.024880523 0.031442173 0.03783837 0.044654503 0.050738394 0.055747151 0.059461214 0.062188543 0.0645178 0.065881729 0.067001939 0.06812527 0.0693358 0.070741683 0.07152538][0.026053535 0.032884672 0.039701991 0.047386251 0.054549 0.060250394 0.064594649 0.067985006 0.071026735 0.072711714 0.074180983 0.075957 0.07807117 0.080672778 0.082700163][0.025624244 0.032367505 0.03919144 0.046792783 0.054150425 0.059943095 0.064618133 0.068289883 0.072022378 0.074487336 0.076706879 0.078925587 0.081537493 0.084391773 0.086663105][0.024433866 0.030617928 0.037006922 0.044353843 0.051569343 0.057069279 0.061817087 0.065691806 0.069876216 0.073176391 0.076371387 0.079810806 0.083678879 0.087669916 0.091119841][0.023171939 0.028735641 0.034483731 0.041085206 0.047568895 0.052604251 0.057348736 0.061122522 0.065212831 0.06926848 0.073305361 0.077756323 0.082923733 0.0883893 0.093567505][0.02242744 0.027323466 0.032058746 0.037744075 0.043280303 0.04762838 0.052056134 0.05570177 0.059875533 0.063615389 0.067468271 0.071779311 0.077025637 0.08359649 0.090130627][0.022856059 0.02730706 0.031288564 0.03599152 0.040339284 0.043736726 0.047413573 0.050459258 0.054117665 0.057171077 0.060658522 0.065034457 0.070459686 0.077316023 0.084547035][0.023315728 0.027623609 0.031239798 0.034919068 0.037753724 0.040375158 0.043366961 0.045559764 0.048477948 0.051218405 0.054761507 0.059405044 0.065112166 0.0721616 0.07964991][0.024010092 0.028098227 0.031209523 0.034309089 0.036222808 0.038183428 0.040514253 0.042329051 0.044900917 0.047067061 0.050366782 0.054982238 0.06068185 0.067734063 0.075340651][0.024801414 0.028769104 0.031612411 0.034153372 0.035363816 0.036833227 0.038719833 0.040397793 0.042735353 0.044772461 0.048069097 0.052236311 0.057391472 0.063866265 0.071056955][0.025179224 0.029319061 0.032204121 0.034445509 0.035350055 0.036505409 0.037957452 0.039504826 0.04144375 0.043255478 0.046235718 0.049852945 0.0543349 0.0598559 0.06614539][0.025218781 0.029500565 0.032721676 0.035141669 0.036161952 0.037219517 0.038527161 0.039747298 0.040997714 0.042483479 0.044869706 0.047978617 0.051813036 0.056417495 0.061796442][0.024531847 0.028881852 0.0321694 0.034822963 0.0360156 0.037128821 0.038498342 0.039438866 0.040314361 0.041459836 0.043284111 0.045779012 0.048885956 0.052809976 0.057372272][0.023698293 0.027936105 0.031296164 0.034486458 0.036076382 0.037205346 0.038507029 0.039402395 0.040010095 0.040659465 0.04172577 0.043595061 0.045975395 0.04909201 0.052820876][0.022067279 0.026096443 0.029540623 0.033011608 0.0350713 0.036418714 0.037791856 0.038737252 0.039303869 0.039922312 0.04074166 0.042145103 0.043959372 0.046457358 0.04953032]]...]
INFO - root - 2017-12-09 06:29:44.934276: step 1110, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.680 sec/batch; 62h:36m:10s remains)
INFO - root - 2017-12-09 06:29:51.557488: step 1120, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:18m:55s remains)
INFO - root - 2017-12-09 06:29:58.382391: step 1130, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:25m:54s remains)
INFO - root - 2017-12-09 06:30:05.041810: step 1140, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.697 sec/batch; 64h:07m:24s remains)
INFO - root - 2017-12-09 06:30:11.712419: step 1150, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.690 sec/batch; 63h:30m:10s remains)
INFO - root - 2017-12-09 06:30:18.446122: step 1160, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 60h:49m:41s remains)
INFO - root - 2017-12-09 06:30:25.102760: step 1170, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.716 sec/batch; 65h:54m:03s remains)
INFO - root - 2017-12-09 06:30:32.002395: step 1180, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.680 sec/batch; 62h:32m:19s remains)
INFO - root - 2017-12-09 06:30:38.562631: step 1190, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:31m:30s remains)
INFO - root - 2017-12-09 06:30:45.433754: step 1200, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.694 sec/batch; 63h:52m:56s remains)
2017-12-09 06:30:46.080387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018579485 -0.0019565367 -0.0021086072 -0.0022560437 -0.0023648534 -0.0024399329 -0.0024944497 -0.0025385055 -0.0025771337 -0.0026076625 -0.0026294529 -0.0026423677 -0.0026478011 -0.0026484015 -0.0026467617][-0.00092716678 -0.001080197 -0.0013604277 -0.0016511688 -0.0018649359 -0.0020101615 -0.002121133 -0.0022280321 -0.0023417177 -0.0024478864 -0.0025349411 -0.0025948195 -0.0026283441 -0.0026420588 -0.0026454788][0.00021567917 8.9507783e-05 -0.00024776929 -0.00063473149 -0.00092376966 -0.001117849 -0.0012760381 -0.0014652079 -0.0017124154 -0.0019842153 -0.0022372771 -0.0024315736 -0.0025541175 -0.0026145065 -0.0026374292][0.0010765335 0.0010918796 0.00087195914 0.00058226194 0.00038181129 0.00026103482 0.00014322321 -8.1113307e-05 -0.00047374656 -0.00099056051 -0.0015404702 -0.0020104363 -0.0023419505 -0.0025268565 -0.0026093505][0.0012784372 0.00140339 0.0013938076 0.0014053565 0.0015198085 0.0016692975 0.0017423471 0.0016184505 0.0011996073 0.00049749366 -0.00037713884 -0.0012162916 -0.0018841433 -0.0023074276 -0.0025249822][0.00088763121 0.0010266246 0.0011479901 0.0014379513 0.0019180374 0.0024387077 0.0028385231 0.002985619 0.0027468028 0.0020823348 0.0010568874 -7.3684845e-05 -0.0011048598 -0.0018556644 -0.0023099771][0.00016447762 0.00026922463 0.00043874583 0.00090342946 0.001660293 0.0024727015 0.0031186203 0.0034701573 0.0034314734 0.0029706969 0.0020691352 0.00092466618 -0.00026093144 -0.001244192 -0.0019411671][-0.00078721263 -0.00070724217 -0.0005120323 6.2782783e-05 0.0010053036 0.002029018 0.0028458862 0.003318558 0.0033950703 0.003047952 0.0022484306 0.0011852195 6.3914573e-05 -0.000901095 -0.0016455863][-0.001729606 -0.0016591619 -0.0014645015 -0.00089580973 6.2455423e-05 0.0011404431 0.0020308048 0.0025972296 0.0028100233 0.0026044762 0.0019096271 0.00087956735 -0.00021417579 -0.0010947057 -0.0017197396][-0.0023858687 -0.0023457 -0.002205573 -0.0017750218 -0.000991571 -4.4205459e-05 0.00078576268 0.0013731881 0.0016976185 0.0016629347 0.0011608356 0.00027746242 -0.00071648159 -0.0015194247 -0.0020329677][-0.0025558139 -0.0025699467 -0.0025319038 -0.0023293234 -0.0018715309 -0.001232581 -0.00061929016 -0.00013177795 0.00021032477 0.00031612511 5.3168973e-05 -0.00055678515 -0.0012961355 -0.0019113957 -0.0023035055][-0.0025591606 -0.0025794196 -0.0025811989 -0.0025351781 -0.0023594599 -0.0020528496 -0.0017204755 -0.0014239171 -0.0011806526 -0.0010542087 -0.0011412842 -0.0014522837 -0.0018588006 -0.0022086017 -0.0024416663][-0.0025754457 -0.0025800741 -0.0025771055 -0.00256797 -0.0025285927 -0.0024368905 -0.0023157133 -0.0021906181 -0.002067545 -0.0019800321 -0.0019847688 -0.0020997527 -0.002271988 -0.0024251165 -0.0025307541][-0.0026218996 -0.0026173971 -0.002607336 -0.0025887622 -0.0025746564 -0.0025581641 -0.0025371369 -0.0025069909 -0.0024662875 -0.0024234334 -0.0024060248 -0.0024330795 -0.0024964041 -0.0025586754 -0.0026028224][-0.0026421442 -0.0026416855 -0.0026417843 -0.0026331251 -0.0026213045 -0.0026141878 -0.0026153987 -0.0026103794 -0.0025987183 -0.0025805885 -0.00256413 -0.002563745 -0.0025833715 -0.002606943 -0.0026249252]]...]
INFO - root - 2017-12-09 06:30:52.739038: step 1210, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:34m:34s remains)
INFO - root - 2017-12-09 06:30:59.428170: step 1220, loss = 0.91, batch loss = 0.69 (11.6 examples/sec; 0.690 sec/batch; 63h:27m:49s remains)
INFO - root - 2017-12-09 06:31:06.114851: step 1230, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 61h:58m:16s remains)
INFO - root - 2017-12-09 06:31:12.855511: step 1240, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.689 sec/batch; 63h:26m:28s remains)
INFO - root - 2017-12-09 06:31:19.571923: step 1250, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:22m:22s remains)
INFO - root - 2017-12-09 06:31:26.299486: step 1260, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.671 sec/batch; 61h:46m:32s remains)
INFO - root - 2017-12-09 06:31:32.811207: step 1270, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.699 sec/batch; 64h:17m:58s remains)
INFO - root - 2017-12-09 06:31:39.584042: step 1280, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:23m:12s remains)
INFO - root - 2017-12-09 06:31:46.248482: step 1290, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.711 sec/batch; 65h:23m:12s remains)
INFO - root - 2017-12-09 06:31:53.073363: step 1300, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.662 sec/batch; 60h:54m:27s remains)
2017-12-09 06:31:53.758497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0028162552 -0.0028156182 -0.0028155243 -0.0028153546 -0.0028151416 -0.0028148848 -0.0028145239 -0.0028139472 -0.0028133534 -0.0028130133 -0.0028129523 -0.0028131776 -0.0028136109 -0.0028142759 -0.0028149926][-0.0028155767 -0.0028149108 -0.002814838 -0.0028146538 -0.0028144128 -0.002814135 -0.0028137583 -0.0028131597 -0.002812546 -0.0028122175 -0.0028121842 -0.0028124519 -0.0028129357 -0.0028136787 -0.002814471][-0.0028152163 -0.0028145849 -0.0028145409 -0.0028143518 -0.0028140938 -0.0028138147 -0.002813444 -0.0028128505 -0.0028122363 -0.0028119276 -0.0028119253 -0.0028122296 -0.0028127462 -0.0028135376 -0.0028143923][-0.0028148033 -0.0028142303 -0.0028142217 -0.0028140284 -0.0028137679 -0.0028135034 -0.0028131583 -0.0028125886 -0.0028119865 -0.002811695 -0.0028117138 -0.0028120466 -0.0028125911 -0.0028134221 -0.0028143278][-0.0028143493 -0.0028138377 -0.0028138778 -0.0028136855 -0.0028134349 -0.0028132084 -0.0028128992 -0.0028123632 -0.0028117816 -0.0028115038 -0.0028115413 -0.0028118924 -0.0028124589 -0.0028133271 -0.002814274][-0.0028138836 -0.002813421 -0.0028135139 -0.0028133213 -0.0028130885 -0.0028129176 -0.0028126631 -0.002812169 -0.002811617 -0.0028113606 -0.0028114174 -0.0028117872 -0.0028123681 -0.0028132633 -0.0028142459][-0.0028134685 -0.0028130135 -0.0028131481 -0.0028129623 -0.002812743 -0.0028126398 -0.002812448 -0.0028119939 -0.0028114764 -0.002811241 -0.0028113162 -0.002811695 -0.0028122922 -0.0028132102 -0.0028142245][-0.0028130929 -0.0028126466 -0.002812797 -0.0028126216 -0.0028124219 -0.002812386 -0.0028122461 -0.0028118223 -0.0028113376 -0.0028111218 -0.002811197 -0.0028115809 -0.0028121988 -0.0028131409 -0.0028141863][-0.0028128214 -0.0028123718 -0.0028125134 -0.00281236 -0.0028121837 -0.0028121755 -0.0028120656 -0.0028116482 -0.0028111786 -0.0028109772 -0.0028110489 -0.0028114417 -0.0028120917 -0.0028130712 -0.0028141455][-0.0028127506 -0.0028122398 -0.0028123481 -0.0028122046 -0.0028120261 -0.0028120198 -0.0028119166 -0.0028114985 -0.0028110275 -0.002810837 -0.002810911 -0.0028113052 -0.002811986 -0.0028129977 -0.0028140934][-0.0028128943 -0.0028123073 -0.0028123357 -0.0028121748 -0.0028119863 -0.0028119448 -0.0028118209 -0.0028113893 -0.0028109015 -0.0028107162 -0.0028107993 -0.0028112109 -0.0028119166 -0.0028129478 -0.0028140557][-0.002813122 -0.0028125271 -0.0028124726 -0.0028122931 -0.0028120966 -0.0028120121 -0.002811857 -0.0028114123 -0.0028109089 -0.0028107204 -0.0028108179 -0.0028112452 -0.002811952 -0.0028129769 -0.0028140631][-0.0028134724 -0.0028129118 -0.0028128235 -0.0028126466 -0.0028124689 -0.0028123532 -0.0028121579 -0.0028117276 -0.002811234 -0.002811031 -0.0028111127 -0.0028115131 -0.0028121741 -0.0028131229 -0.0028141327][-0.0028140245 -0.0028134861 -0.002813377 -0.0028132217 -0.0028130608 -0.0028129166 -0.0028126987 -0.0028123003 -0.0028118433 -0.0028116407 -0.0028116966 -0.0028120431 -0.0028126107 -0.0028134189 -0.0028142952][-0.0028145867 -0.0028140864 -0.0028139804 -0.0028138503 -0.002813698 -0.0028135537 -0.0028133518 -0.002813007 -0.0028126147 -0.0028124314 -0.0028124633 -0.0028127364 -0.0028131807 -0.0028138116 -0.0028145122]]...]
INFO - root - 2017-12-09 06:32:00.434809: step 1310, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 61h:14m:37s remains)
INFO - root - 2017-12-09 06:32:07.100198: step 1320, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.665 sec/batch; 61h:08m:39s remains)
INFO - root - 2017-12-09 06:32:13.911362: step 1330, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:23m:45s remains)
INFO - root - 2017-12-09 06:32:20.646229: step 1340, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.675 sec/batch; 62h:05m:39s remains)
INFO - root - 2017-12-09 06:32:27.369796: step 1350, loss = 0.91, batch loss = 0.70 (11.9 examples/sec; 0.674 sec/batch; 61h:58m:49s remains)
INFO - root - 2017-12-09 06:32:34.017267: step 1360, loss = 0.89, batch loss = 0.68 (12.1 examples/sec; 0.663 sec/batch; 60h:58m:51s remains)
INFO - root - 2017-12-09 06:32:40.624901: step 1370, loss = 0.96, batch loss = 0.75 (11.5 examples/sec; 0.695 sec/batch; 63h:52m:56s remains)
INFO - root - 2017-12-09 06:32:47.445626: step 1380, loss = 0.89, batch loss = 0.68 (12.1 examples/sec; 0.659 sec/batch; 60h:37m:38s remains)
INFO - root - 2017-12-09 06:32:54.043408: step 1390, loss = 0.94, batch loss = 0.72 (11.4 examples/sec; 0.703 sec/batch; 64h:42m:02s remains)
INFO - root - 2017-12-09 06:33:00.932607: step 1400, loss = 0.94, batch loss = 0.73 (11.9 examples/sec; 0.673 sec/batch; 61h:54m:38s remains)
2017-12-09 06:33:01.640005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034993577 -0.0034991419 -0.0034993123 -0.003499341 -0.00349933 -0.0034992136 -0.0034990574 -0.0034987642 -0.00349847 -0.0034983004 -0.0034981791 -0.0034980751 -0.0034980488 -0.0034981596 -0.0034982923][-0.0034989277 -0.0034988082 -0.0034990632 -0.003499256 -0.00349937 -0.0034993538 -0.0034991822 -0.0034988595 -0.0034985514 -0.0034982252 -0.0034978543 -0.0034975817 -0.0034974206 -0.0034973565 -0.0034973689][-0.0034997244 -0.003499751 -0.0035001161 -0.0035004318 -0.0035006555 -0.0035007056 -0.0035005417 -0.0035001636 -0.0034996863 -0.0034992918 -0.0034989549 -0.0034986043 -0.0034983743 -0.0034981482 -0.0034980294][-0.0035006595 -0.0035007403 -0.0035010674 -0.0035013102 -0.003501365 -0.0035011969 -0.0035009729 -0.003500686 -0.0035003813 -0.0035000795 -0.0034997799 -0.003499463 -0.0034991987 -0.0034989547 -0.003498774][-0.0035016991 -0.0035016465 -0.0035016262 -0.0035013261 -0.0035008721 -0.0035004597 -0.0035003463 -0.0035004562 -0.0035004357 -0.0035002446 -0.0035001226 -0.0035000122 -0.0034997307 -0.0034994392 -0.0034992073][-0.0035020655 -0.0035013254 -0.0035002744 -0.0034988113 -0.0034975104 -0.0034969072 -0.0034972825 -0.003498381 -0.0034997156 -0.0035005144 -0.0035008367 -0.0035008374 -0.0035006811 -0.0035004059 -0.0035000418][-0.003501876 -0.0034999566 -0.0034972143 -0.0034939188 -0.0034912464 -0.0034902724 -0.0034915074 -0.0034942233 -0.0034972278 -0.0034995386 -0.0035009021 -0.0035014006 -0.0035013675 -0.0035011272 -0.0035007771][-0.0035020334 -0.0034992155 -0.0034947337 -0.0034894403 -0.0034854994 -0.0034839513 -0.0034853446 -0.0034891628 -0.0034940329 -0.0034981398 -0.0035006371 -0.003501687 -0.0035018583 -0.0035016416 -0.0035013228][-0.0035026728 -0.0035000555 -0.0034952501 -0.0034893628 -0.0034845073 -0.0034820661 -0.0034826393 -0.0034861967 -0.0034916915 -0.003497052 -0.0035005722 -0.0035023324 -0.0035026178 -0.0035020832 -0.0035013291][-0.0035035356 -0.0035017203 -0.0034981593 -0.0034933938 -0.0034887996 -0.0034856983 -0.0034853928 -0.0034878384 -0.0034924331 -0.0034972869 -0.0035005768 -0.003501947 -0.0035015759 -0.0035002124 -0.0034986408][-0.0035040991 -0.0035032667 -0.0035015298 -0.0034987791 -0.003495622 -0.0034930413 -0.0034922636 -0.0034931961 -0.0034958012 -0.0034985752 -0.0034998725 -0.0034992355 -0.0034968194 -0.0034933614 -0.0034897872][-0.003504493 -0.0035040826 -0.0035035168 -0.0035024793 -0.0035010187 -0.0034994129 -0.0034986574 -0.0034986883 -0.0034993452 -0.0034993321 -0.0034972089 -0.0034927924 -0.0034865171 -0.0034794156 -0.0034730621][-0.0035042088 -0.0035038323 -0.0035036919 -0.0035035282 -0.0035032311 -0.0035028905 -0.003502765 -0.0035027305 -0.0035020439 -0.0034990632 -0.0034926622 -0.0034831488 -0.0034719119 -0.0034609656 -0.0034521678][-0.0035041857 -0.0035038369 -0.0035038984 -0.003504073 -0.0035044574 -0.0035049124 -0.0035054141 -0.003505497 -0.00350389 -0.0034987107 -0.0034886566 -0.0034746688 -0.0034595474 -0.0034456851 -0.0034351491][-0.0035042027 -0.0035038623 -0.0035038269 -0.0035039787 -0.0035043508 -0.003504887 -0.0035055571 -0.0035058961 -0.0035044535 -0.0034989067 -0.0034876124 -0.0034717554 -0.0034546524 -0.0034392397 -0.0034278119]]...]
INFO - root - 2017-12-09 06:33:08.315247: step 1410, loss = 0.92, batch loss = 0.71 (11.5 examples/sec; 0.694 sec/batch; 63h:49m:59s remains)
INFO - root - 2017-12-09 06:33:15.099482: step 1420, loss = 0.93, batch loss = 0.72 (12.1 examples/sec; 0.663 sec/batch; 60h:59m:32s remains)
INFO - root - 2017-12-09 06:33:21.966621: step 1430, loss = 0.93, batch loss = 0.72 (11.8 examples/sec; 0.675 sec/batch; 62h:05m:29s remains)
INFO - root - 2017-12-09 06:33:28.646216: step 1440, loss = 0.91, batch loss = 0.70 (12.3 examples/sec; 0.652 sec/batch; 59h:56m:51s remains)
INFO - root - 2017-12-09 06:33:35.345060: step 1450, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:39m:45s remains)
INFO - root - 2017-12-09 06:33:42.163166: step 1460, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:13m:00s remains)
INFO - root - 2017-12-09 06:33:48.729949: step 1470, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.671 sec/batch; 61h:42m:33s remains)
INFO - root - 2017-12-09 06:33:55.570157: step 1480, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.709 sec/batch; 65h:12m:40s remains)
INFO - root - 2017-12-09 06:34:02.234035: step 1490, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:11m:55s remains)
INFO - root - 2017-12-09 06:34:09.044132: step 1500, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.675 sec/batch; 62h:04m:58s remains)
2017-12-09 06:34:09.766121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0034384341 -0.0033878067 -0.0033365577 -0.0032793656 -0.0032321415 -0.0031987289 -0.0031823106 -0.0031781767 -0.0031777241 -0.0031791481 -0.0031793558 -0.0031799679 -0.0031654795 -0.0031326723 -0.0030968073][-0.0034270659 -0.003360993 -0.0032971394 -0.0032269657 -0.0031710749 -0.0031387247 -0.0031316332 -0.0031421157 -0.0031573474 -0.00317241 -0.0031828259 -0.0031889011 -0.0031735741 -0.0031318646 -0.0030854391][-0.0034128437 -0.0033334263 -0.0032620362 -0.0031881318 -0.0031329389 -0.0031101368 -0.0031193311 -0.0031483681 -0.0031807283 -0.0032125534 -0.0032355746 -0.0032435458 -0.0032212548 -0.003168494 -0.0031106684][-0.0033762171 -0.0032855896 -0.0032133574 -0.0031425091 -0.0030969032 -0.0030926529 -0.0031267833 -0.0031788666 -0.0032307694 -0.0032785772 -0.0033125335 -0.0033213333 -0.0032948691 -0.0032328302 -0.0031619295][-0.0033274204 -0.0032314614 -0.0031620208 -0.0031049377 -0.0030804204 -0.0031045727 -0.0031699124 -0.0032506676 -0.0033273716 -0.0033869059 -0.0034262119 -0.0034308122 -0.003397176 -0.0033305543 -0.003250197][-0.0032982789 -0.003205213 -0.0031486698 -0.0031104884 -0.0031093697 -0.0031598168 -0.0032513277 -0.0033555005 -0.0034466661 -0.0035123758 -0.0035535072 -0.0035579642 -0.0035259768 -0.0034646038 -0.0033857573][-0.0032974649 -0.0032203572 -0.0031825621 -0.0031692623 -0.0031944965 -0.003268963 -0.0033759593 -0.003487491 -0.0035791011 -0.0036392715 -0.0036735819 -0.0036765658 -0.0036511675 -0.0036007334 -0.0035333014][-0.003333735 -0.0032784883 -0.0032652388 -0.0032702081 -0.0033154269 -0.003402622 -0.0035136195 -0.0036214238 -0.0037063903 -0.0037576437 -0.0037856842 -0.0037891637 -0.0037707102 -0.003730146 -0.0036712412][-0.0033983667 -0.0033734851 -0.003383697 -0.0034082911 -0.0034626212 -0.0035472091 -0.0036484075 -0.00374364 -0.0038151436 -0.0038578892 -0.0038795627 -0.0038830156 -0.0038705633 -0.0038395089 -0.0037911413][-0.0034618638 -0.0034648995 -0.00349786 -0.0035363289 -0.0035933573 -0.0036669909 -0.0037483037 -0.0038233381 -0.0038772312 -0.0039083678 -0.0039246106 -0.00393187 -0.0039279391 -0.0039077154 -0.003867537][-0.0035087573 -0.00353082 -0.0035807567 -0.0036245026 -0.0036728184 -0.0037244391 -0.0037781936 -0.0038282329 -0.0038629328 -0.0038862117 -0.0039036607 -0.003917852 -0.0039230622 -0.0039103734 -0.0038794894][-0.0035211225 -0.0035485323 -0.0036014235 -0.0036422049 -0.0036766815 -0.0037059153 -0.0037354084 -0.0037644163 -0.0037850789 -0.0038027153 -0.0038208317 -0.003838547 -0.0038455729 -0.0038371554 -0.0038114251][-0.0034986206 -0.0035217039 -0.003571976 -0.0036032808 -0.0036231978 -0.0036340798 -0.0036454955 -0.0036557852 -0.0036678761 -0.0036843792 -0.0037032813 -0.0037194579 -0.0037233303 -0.0037141796 -0.003688301][-0.0034551006 -0.0034670022 -0.0035058 -0.0035290341 -0.0035374556 -0.0035384723 -0.0035385957 -0.0035411925 -0.0035465104 -0.0035625589 -0.0035821213 -0.0035935941 -0.0035912837 -0.0035782324 -0.0035511414][-0.0033897762 -0.003386992 -0.0034130015 -0.0034265616 -0.0034303581 -0.0034281812 -0.0034266342 -0.003426105 -0.0034318971 -0.0034465692 -0.0034654227 -0.0034753315 -0.0034709477 -0.003457281 -0.003428862]]...]
INFO - root - 2017-12-09 06:34:16.420366: step 1510, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:26m:18s remains)
INFO - root - 2017-12-09 06:34:23.099222: step 1520, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 59h:50m:47s remains)
INFO - root - 2017-12-09 06:34:29.713580: step 1530, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.664 sec/batch; 61h:01m:34s remains)
INFO - root - 2017-12-09 06:34:36.428479: step 1540, loss = 0.91, batch loss = 0.69 (11.7 examples/sec; 0.681 sec/batch; 62h:38m:14s remains)
INFO - root - 2017-12-09 06:34:43.132246: step 1550, loss = 0.91, batch loss = 0.69 (12.3 examples/sec; 0.653 sec/batch; 60h:00m:43s remains)
INFO - root - 2017-12-09 06:34:49.917543: step 1560, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:26m:57s remains)
INFO - root - 2017-12-09 06:34:56.421808: step 1570, loss = 0.91, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:41m:59s remains)
INFO - root - 2017-12-09 06:35:03.267028: step 1580, loss = 0.91, batch loss = 0.69 (11.6 examples/sec; 0.690 sec/batch; 63h:27m:33s remains)
INFO - root - 2017-12-09 06:35:09.945959: step 1590, loss = 0.91, batch loss = 0.69 (11.3 examples/sec; 0.710 sec/batch; 65h:15m:43s remains)
INFO - root - 2017-12-09 06:35:16.779939: step 1600, loss = 0.91, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:37m:42s remains)
2017-12-09 06:35:17.446145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0040584919 -0.0040583215 -0.0040584612 -0.0040590116 -0.0040596928 -0.0040604873 -0.0040612342 -0.0040618535 -0.0040620426 -0.0040616933 -0.00406105 -0.0040602502 -0.0040596928 -0.0040592891 -0.0040591327][-0.0040590358 -0.0040589925 -0.0040592188 -0.0040598405 -0.0040606256 -0.0040614726 -0.0040621879 -0.0040627262 -0.0040628123 -0.0040622922 -0.0040614847 -0.0040605445 -0.0040598749 -0.0040593925 -0.0040591462][-0.0040598447 -0.0040599154 -0.0040602414 -0.0040608882 -0.0040616775 -0.0040625236 -0.0040632184 -0.0040635881 -0.0040634917 -0.0040628361 -0.0040619313 -0.0040609292 -0.0040601739 -0.0040596477 -0.0040593394][-0.0040606251 -0.0040607844 -0.0040611117 -0.0040617259 -0.0040624188 -0.0040631518 -0.0040637385 -0.0040640133 -0.0040638139 -0.0040631145 -0.0040621948 -0.0040612062 -0.0040604556 -0.004059922 -0.0040595359][-0.0040611136 -0.0040612789 -0.0040615383 -0.0040620063 -0.0040625376 -0.0040630973 -0.0040635355 -0.0040637031 -0.0040634866 -0.004062844 -0.0040620626 -0.0040611946 -0.0040604449 -0.0040598419 -0.0040592323][-0.0040610386 -0.0040612239 -0.0040614079 -0.0040616672 -0.0040619727 -0.0040623466 -0.0040626726 -0.0040628025 -0.0040626368 -0.004062159 -0.0040615043 -0.0040606922 -0.0040599168 -0.0040591746 -0.0040586027][-0.0040597883 -0.0040598996 -0.0040599611 -0.004060009 -0.0040600356 -0.0040601441 -0.0040602735 -0.0040603331 -0.0040601431 -0.0040597483 -0.0040592728 -0.0040588123 -0.0040585566 -0.0040585664 -0.0040588882][-0.0040581105 -0.0040580253 -0.004057887 -0.0040577087 -0.0040574153 -0.0040571927 -0.0040570679 -0.0040571345 -0.0040572756 -0.0040576276 -0.0040582055 -0.0040588905 -0.0040595443 -0.0040601054 -0.0040606316][-0.0040578702 -0.004057751 -0.0040575913 -0.00405739 -0.0040572113 -0.00405719 -0.0040573147 -0.0040576649 -0.0040580281 -0.0040585985 -0.0040593767 -0.0040601371 -0.0040608551 -0.0040614218 -0.0040618489][-0.0040580896 -0.0040579275 -0.00405801 -0.0040580183 -0.00405802 -0.0040581468 -0.0040583392 -0.0040586721 -0.0040589776 -0.0040594474 -0.0040600258 -0.0040606312 -0.0040612365 -0.0040618274 -0.0040622465][-0.0040569114 -0.0040570116 -0.0040573706 -0.004057622 -0.0040578581 -0.0040581468 -0.0040584849 -0.004058932 -0.004059324 -0.0040597776 -0.0040603019 -0.0040607862 -0.00406131 -0.0040617818 -0.0040621734][-0.0040546223 -0.0040548849 -0.0040554367 -0.0040560169 -0.0040566181 -0.0040572593 -0.0040577664 -0.0040582838 -0.0040587732 -0.0040592914 -0.0040598479 -0.0040604407 -0.0040611154 -0.0040616877 -0.0040620468][-0.0040520132 -0.0040520588 -0.0040530059 -0.0040539037 -0.00405487 -0.00405586 -0.0040565613 -0.004057277 -0.0040579964 -0.0040587108 -0.0040594288 -0.0040602288 -0.0040610121 -0.004061569 -0.0040618554][-0.0040495982 -0.0040496043 -0.0040509636 -0.004052212 -0.0040533971 -0.004054728 -0.0040555829 -0.0040563988 -0.0040572658 -0.0040581692 -0.0040589836 -0.004059793 -0.0040605823 -0.0040611443 -0.0040614228][-0.0040482278 -0.0040482245 -0.0040497524 -0.0040512676 -0.0040526483 -0.0040541515 -0.00405515 -0.0040561236 -0.0040571094 -0.0040580244 -0.0040588607 -0.0040595345 -0.0040603285 -0.0040608305 -0.004060979]]...]
INFO - root - 2017-12-09 06:35:24.061575: step 1610, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:15m:45s remains)
INFO - root - 2017-12-09 06:35:30.797482: step 1620, loss = 0.91, batch loss = 0.69 (12.5 examples/sec; 0.643 sec/batch; 59h:03m:11s remains)
INFO - root - 2017-12-09 06:35:37.595334: step 1630, loss = 0.91, batch loss = 0.69 (11.1 examples/sec; 0.718 sec/batch; 65h:58m:30s remains)
INFO - root - 2017-12-09 06:35:44.319647: step 1640, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:05m:03s remains)
INFO - root - 2017-12-09 06:35:51.061678: step 1650, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:23m:37s remains)
INFO - root - 2017-12-09 06:35:57.839673: step 1660, loss = 0.91, batch loss = 0.69 (11.7 examples/sec; 0.682 sec/batch; 62h:37m:49s remains)
INFO - root - 2017-12-09 06:36:04.510125: step 1670, loss = 0.91, batch loss = 0.69 (11.7 examples/sec; 0.683 sec/batch; 62h:45m:14s remains)
INFO - root - 2017-12-09 06:36:11.288252: step 1680, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:05m:41s remains)
INFO - root - 2017-12-09 06:36:18.000008: step 1690, loss = 0.91, batch loss = 0.69 (11.6 examples/sec; 0.688 sec/batch; 63h:14m:20s remains)
INFO - root - 2017-12-09 06:36:24.840666: step 1700, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.680 sec/batch; 62h:27m:48s remains)
2017-12-09 06:36:25.524767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0039103231 -0.0039281766 -0.0039335391 -0.0039155418 -0.003885651 -0.0038457853 -0.0037920047 -0.0037220831 -0.0036392298 -0.0035538273 -0.0034746584 -0.003402485 -0.0033487149 -0.0033164462 -0.0033042806][-0.0038534787 -0.0038693764 -0.0038847749 -0.003889526 -0.0038901479 -0.0038811606 -0.0038583537 -0.0038102784 -0.0037421277 -0.0036653804 -0.0035803514 -0.0034963642 -0.0034246712 -0.0033759261 -0.0033505808][-0.0037780432 -0.0037902952 -0.0038180915 -0.0038469513 -0.0038727429 -0.0038920185 -0.0038994544 -0.0038789148 -0.0038269511 -0.0037540572 -0.003671681 -0.0035924034 -0.0035146712 -0.0034512081 -0.0034119086][-0.0037099316 -0.0037164169 -0.0037500439 -0.0037933937 -0.0038370248 -0.0038756807 -0.0039025571 -0.0039040961 -0.0038682702 -0.0038077333 -0.0037340338 -0.0036555906 -0.0035823614 -0.0035189725 -0.0034748102][-0.0036610751 -0.0036643487 -0.0036983346 -0.0037422439 -0.0037907755 -0.0038384534 -0.0038742691 -0.0038855181 -0.0038595423 -0.0038093817 -0.0037454041 -0.0036759274 -0.0036094207 -0.0035505153 -0.0035091627][-0.0036239363 -0.00362393 -0.0036519226 -0.0036896928 -0.0037332987 -0.0037765221 -0.0038093403 -0.003821495 -0.0038014709 -0.0037624799 -0.003710065 -0.0036473928 -0.0035909298 -0.0035429467 -0.0035089331][-0.0035936194 -0.0035862543 -0.0036062684 -0.0036355257 -0.0036659681 -0.0036942472 -0.0037155731 -0.0037233527 -0.003705509 -0.0036760306 -0.0036368475 -0.0035910427 -0.0035491935 -0.0035139991 -0.003486288][-0.0035604569 -0.0035458892 -0.0035605221 -0.0035808494 -0.00359807 -0.0036115688 -0.0036189854 -0.0036183279 -0.0036056035 -0.0035881533 -0.0035620572 -0.0035288464 -0.0035013082 -0.0034779718 -0.0034600259][-0.0035219674 -0.0035045971 -0.0035146535 -0.0035299743 -0.0035424011 -0.0035448684 -0.003542165 -0.003535327 -0.0035221507 -0.0035078083 -0.0034900024 -0.0034670781 -0.0034503415 -0.0034405747 -0.0034364024][-0.0034794854 -0.0034587518 -0.0034669752 -0.0034798873 -0.0034894682 -0.0034895516 -0.0034870021 -0.003478379 -0.003464557 -0.0034488989 -0.0034304687 -0.0034105845 -0.003404551 -0.0034035284 -0.0034075228][-0.0034375805 -0.0034133506 -0.0034217872 -0.0034324192 -0.0034411752 -0.0034386124 -0.0034379696 -0.0034301963 -0.003418739 -0.0034008184 -0.0033830111 -0.0033655495 -0.0033641024 -0.0033680317 -0.0033751698][-0.0034014105 -0.0033773049 -0.0033857981 -0.0033944719 -0.0034010538 -0.0033985232 -0.0033996007 -0.0033961562 -0.0033862139 -0.0033703905 -0.0033541042 -0.0033359313 -0.0033339488 -0.0033399169 -0.0033486388][-0.0033762688 -0.0033531666 -0.0033609546 -0.0033592361 -0.0033663511 -0.0033684168 -0.003367017 -0.0033643481 -0.003360095 -0.0033509226 -0.0033393127 -0.0033232849 -0.0033205203 -0.0033221995 -0.0033258596][-0.0033702569 -0.0033461368 -0.0033548593 -0.0033588144 -0.0033560162 -0.0033562349 -0.0033546968 -0.0033539291 -0.0033516444 -0.0033475675 -0.003342506 -0.0033291173 -0.0033222931 -0.0033191806 -0.0033155987][-0.0033748783 -0.0033521759 -0.0033582619 -0.0033609834 -0.003354839 -0.0033547317 -0.0033544134 -0.0033541773 -0.0033536146 -0.0033528772 -0.0033514891 -0.0033422282 -0.003334587 -0.0033277601 -0.003319052]]...]
INFO - root - 2017-12-09 06:36:32.218713: step 1710, loss = 0.91, batch loss = 0.69 (11.7 examples/sec; 0.686 sec/batch; 63h:03m:29s remains)
INFO - root - 2017-12-09 06:36:38.925624: step 1720, loss = 0.91, batch loss = 0.69 (11.9 examples/sec; 0.675 sec/batch; 61h:58m:56s remains)
INFO - root - 2017-12-09 06:36:45.826981: step 1730, loss = 0.91, batch loss = 0.69 (11.1 examples/sec; 0.722 sec/batch; 66h:18m:16s remains)
INFO - root - 2017-12-09 06:36:52.542781: step 1740, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:17m:28s remains)
INFO - root - 2017-12-09 06:36:59.313706: step 1750, loss = 0.91, batch loss = 0.69 (11.4 examples/sec; 0.701 sec/batch; 64h:25m:32s remains)
INFO - root - 2017-12-09 06:37:06.064368: step 1760, loss = 0.91, batch loss = 0.69 (12.0 examples/sec; 0.664 sec/batch; 61h:00m:20s remains)
INFO - root - 2017-12-09 06:37:12.599620: step 1770, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:05m:50s remains)
INFO - root - 2017-12-09 06:37:19.251173: step 1780, loss = 0.91, batch loss = 0.69 (12.1 examples/sec; 0.659 sec/batch; 60h:34m:45s remains)
INFO - root - 2017-12-09 06:37:25.844898: step 1790, loss = 0.91, batch loss = 0.69 (12.3 examples/sec; 0.653 sec/batch; 59h:58m:41s remains)
INFO - root - 2017-12-09 06:37:32.579402: step 1800, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:23m:07s remains)
2017-12-09 06:37:33.309441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018916428 -0.0017438103 -0.0016431853 -0.0016387403 -0.00165661 -0.0017158971 -0.0017665403 -0.0018285089 -0.00188511 -0.0019156672 -0.0019608762 -0.0019695004 -0.001987336 -0.0020267472 -0.0020756049][-0.0017475705 -0.001504665 -0.001334755 -0.0012936182 -0.0013423047 -0.0014446175 -0.0015745556 -0.0016966951 -0.0018164336 -0.0019052492 -0.0019731135 -0.0020146654 -0.0020689652 -0.0021234362 -0.002176573][-0.0017330875 -0.0013695757 -0.0010812243 -0.00092056394 -0.00089756586 -0.0010067965 -0.0012165024 -0.0014410601 -0.0016722213 -0.0018788534 -0.0020526161 -0.0021911697 -0.0023017609 -0.0024108468 -0.0025143663][-0.0012685542 -0.00078160525 -0.00037123216 -0.00010573491 -2.8523616e-05 -0.00011381088 -0.00035419082 -0.00066446909 -0.0010003238 -0.0013217281 -0.0016024413 -0.0018444113 -0.0020449278 -0.0022171019 -0.0023728367][-0.00055926805 2.9032119e-05 0.00055731973 0.00091266539 0.0010500136 0.0009926185 0.00076568313 0.00041216286 1.0319054e-05 -0.0003925995 -0.00076473108 -0.001101217 -0.0013872455 -0.0016321966 -0.0018479][-0.00010375888 0.00053334981 0.0011076932 0.0014982647 0.0016620937 0.0016161832 0.0013925666 0.0010530441 0.00065090368 0.0002282639 -0.00018183119 -0.00056129508 -0.00090024248 -0.001204445 -0.0014701311][-0.00013548392 0.00051383255 0.0010725353 0.0014637876 0.0016403464 0.0015864964 0.0013472605 0.0010085334 0.00062547671 0.00022206223 -0.00018087612 -0.00054988964 -0.00088286656 -0.0011854244 -0.0014453833][-0.00057980069 4.8358459e-05 0.00060524279 0.0010356912 0.001253916 0.0012188815 0.00096897408 0.00059430487 0.00016205944 -0.00029135263 -0.00072404672 -0.0010956954 -0.001395609 -0.0016350879 -0.001817635][-0.0012507334 -0.00064510247 -6.8695284e-05 0.00043532113 0.00074557168 0.00079805451 0.00060762046 0.00022637658 -0.00027339277 -0.00083078188 -0.0013546997 -0.0017746154 -0.0020731678 -0.0022601415 -0.0023557709][-0.0020628585 -0.0015586272 -0.0010413246 -0.00054893433 -0.00020412821 -6.6603534e-05 -0.00014754361 -0.00044636545 -0.00090570888 -0.0014527501 -0.0019856198 -0.0024190629 -0.0027129943 -0.0028648702 -0.0028961147][-0.0028616237 -0.0025277485 -0.0021637646 -0.0018064869 -0.0015309933 -0.0013763953 -0.0013802911 -0.00155519 -0.0018649008 -0.0022556661 -0.00264988 -0.0029884048 -0.0032253095 -0.0033429763 -0.0033574009][-0.00352398 -0.0033638729 -0.0031756093 -0.0029817724 -0.0028152068 -0.0027099918 -0.0026911502 -0.0027669515 -0.0029148473 -0.0031025833 -0.003292683 -0.0034616422 -0.0035805656 -0.0036377979 -0.0036430722][-0.0038299421 -0.0037783082 -0.00371039 -0.0036396671 -0.0035730721 -0.0035360656 -0.0035260483 -0.0035484948 -0.0035973962 -0.0036626107 -0.0037258887 -0.0037740581 -0.0038103494 -0.0038293418 -0.0038284436][-0.0039346619 -0.0039302139 -0.0039245849 -0.0039197323 -0.0039105085 -0.0039030402 -0.0038934862 -0.0038860168 -0.0038834226 -0.0038867474 -0.0038909395 -0.0038995384 -0.0039119115 -0.0039224993 -0.0039274874][-0.0039668623 -0.0039635394 -0.0039602527 -0.0039592874 -0.0039564669 -0.0039506662 -0.0039454335 -0.0039409935 -0.0039372169 -0.0039340751 -0.0039306795 -0.0039321887 -0.0039354749 -0.0039401078 -0.003944763]]...]
INFO - root - 2017-12-09 06:37:39.917132: step 1810, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:15m:30s remains)
INFO - root - 2017-12-09 06:37:46.650560: step 1820, loss = 0.91, batch loss = 0.69 (11.7 examples/sec; 0.686 sec/batch; 62h:59m:28s remains)
INFO - root - 2017-12-09 06:37:53.325012: step 1830, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:17m:11s remains)
INFO - root - 2017-12-09 06:38:00.173608: step 1840, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.690 sec/batch; 63h:21m:48s remains)
INFO - root - 2017-12-09 06:38:06.915832: step 1850, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 61h:12m:48s remains)
INFO - root - 2017-12-09 06:38:13.620285: step 1860, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.682 sec/batch; 62h:37m:28s remains)
INFO - root - 2017-12-09 06:38:20.306740: step 1870, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 61h:07m:33s remains)
INFO - root - 2017-12-09 06:38:27.058636: step 1880, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.670 sec/batch; 61h:34m:33s remains)
INFO - root - 2017-12-09 06:38:33.660091: step 1890, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:25m:37s remains)
INFO - root - 2017-12-09 06:38:40.531904: step 1900, loss = 0.91, batch loss = 0.69 (11.6 examples/sec; 0.690 sec/batch; 63h:21m:13s remains)
2017-12-09 06:38:41.190547: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.042144716 0.042122893 0.041623089 0.041068945 0.040577643 0.039658293 0.038430411 0.036359645 0.033426508 0.029917372 0.026571175 0.02441605 0.023610966 0.024655595 0.027067749][0.047843181 0.046906654 0.045353547 0.044048995 0.043069419 0.041922003 0.04034844 0.037610102 0.033661988 0.028969215 0.024362167 0.021143347 0.019787027 0.020871077 0.023898598][0.053712297 0.051923227 0.049345072 0.04728901 0.045858871 0.044623375 0.042908829 0.039743379 0.035036571 0.029310824 0.02348559 0.018973228 0.016521389 0.016909789 0.019870495][0.061257247 0.058935221 0.055353835 0.052662246 0.050991897 0.049948379 0.048304614 0.044938222 0.039647773 0.032922562 0.025786847 0.019847648 0.016017588 0.015249619 0.017483724][0.070178717 0.068089046 0.06410414 0.061164077 0.059553813 0.058833476 0.057354782 0.053943343 0.048282042 0.040863845 0.03261888 0.025378697 0.020197636 0.018128479 0.019214014][0.0789494 0.077883244 0.074408814 0.0718733 0.070664018 0.070314892 0.069013573 0.065758415 0.06005216 0.052361548 0.04360003 0.035612516 0.029433334 0.026107628 0.025780598][0.085756905 0.086170353 0.083841659 0.082292363 0.0819098 0.082076728 0.081078656 0.078135461 0.072739825 0.065370262 0.056798615 0.04877748 0.042164505 0.037801504 0.035970893][0.090012163 0.092114024 0.091225669 0.090902671 0.091472588 0.092255354 0.091857031 0.089506947 0.084878638 0.078369223 0.070667073 0.063340381 0.056913525 0.052016165 0.048886679][0.091978192 0.095313422 0.095689967 0.0964834 0.097901493 0.099328943 0.09973368 0.098441653 0.095089994 0.09000089 0.083778962 0.077659853 0.071893387 0.066944279 0.062877491][0.091784477 0.096007332 0.097581118 0.099244893 0.10129646 0.10317634 0.10425916 0.10396858 0.10199428 0.098639987 0.094269611 0.089727372 0.085020714 0.080298 0.075587012][0.088847563 0.093450412 0.095784217 0.098092981 0.10051218 0.10276747 0.10441991 0.1049484 0.1042252 0.10250404 0.10005651 0.097193353 0.093715675 0.089489833 0.084584266][0.084071547 0.0885893 0.091242872 0.093834616 0.096589684 0.099134892 0.10119126 0.10235224 0.10264239 0.1022955 0.10137828 0.099941224 0.097610004 0.09403009 0.089267842][0.078138664 0.082371935 0.08493381 0.087470539 0.090127267 0.092766628 0.095062993 0.096722126 0.097808734 0.0984219 0.098612815 0.098173968 0.096747831 0.093884192 0.089552671][0.071917556 0.075826541 0.078203313 0.080455437 0.082774527 0.085128866 0.08724343 0.089094669 0.090611488 0.091858573 0.092787161 0.093057595 0.092349552 0.090157256 0.086475581][0.065077856 0.06883955 0.071092933 0.073071614 0.075005308 0.076844536 0.07847403 0.080062047 0.081515811 0.082892589 0.084092245 0.084738269 0.084581517 0.083098009 0.080317162]]...]
INFO - root - 2017-12-09 06:38:47.940588: step 1910, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.681 sec/batch; 62h:31m:36s remains)
INFO - root - 2017-12-09 06:38:54.757959: step 1920, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.697 sec/batch; 64h:01m:30s remains)
INFO - root - 2017-12-09 06:39:01.506455: step 1930, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.680 sec/batch; 62h:24m:58s remains)
INFO - root - 2017-12-09 06:39:08.333622: step 1940, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.687 sec/batch; 63h:07m:32s remains)
INFO - root - 2017-12-09 06:39:15.121734: step 1950, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:19m:23s remains)
INFO - root - 2017-12-09 06:39:21.828475: step 1960, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.698 sec/batch; 64h:03m:36s remains)
INFO - root - 2017-12-09 06:39:28.271568: step 1970, loss = 0.91, batch loss = 0.69 (12.5 examples/sec; 0.638 sec/batch; 58h:36m:21s remains)
INFO - root - 2017-12-09 06:39:34.839197: step 1980, loss = 0.90, batch loss = 0.68 (14.0 examples/sec; 0.572 sec/batch; 52h:29m:51s remains)
INFO - root - 2017-12-09 06:39:41.373100: step 1990, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.675 sec/batch; 61h:57m:18s remains)
