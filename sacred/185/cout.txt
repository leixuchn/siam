INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "185"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.1-lastlr0.5-clip10000
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-10 04:51:28.965727: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:51:28.965768: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:51:28.965775: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:51:28.965779: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:51:28.965782: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:51:29.821305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 04:51:29.821344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 04:51:29.821351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 04:51:29.821360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 04:51:34.619253: step 0, loss = 2.28, batch loss = 2.23 (2.7 examples/sec; 2.984 sec/batch; 275h:34m:20s remains)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>]
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
2017-12-10 04:51:35.112416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290142 -4.4290047 -4.4289889 -4.4289665 -4.428936 -4.4288983 -4.42885 -4.4287968 -4.4287629 -4.42877 -4.4288111 -4.4288654 -4.4289203 -4.4289684 -4.4290047][-4.4290075 -4.4289927 -4.428966 -4.4289255 -4.4288726 -4.4288087 -4.4287333 -4.4286547 -4.4286079 -4.4286265 -4.4286942 -4.4287815 -4.428864 -4.4289312 -4.4289842][-4.428998 -4.4289842 -4.4289513 -4.4288921 -4.4288116 -4.4287138 -4.4285955 -4.4284697 -4.4283986 -4.428431 -4.4285383 -4.4286728 -4.4287939 -4.4288869 -4.4289594][-4.4289804 -4.4289756 -4.4289427 -4.428863 -4.4287496 -4.4286079 -4.4284334 -4.4282403 -4.4281416 -4.4282064 -4.4283705 -4.4285684 -4.4287343 -4.4288549 -4.4289412][-4.428957 -4.4289589 -4.42892 -4.4288149 -4.4286628 -4.4284682 -4.4282231 -4.4279485 -4.4278369 -4.4279704 -4.4282155 -4.42848 -4.42869 -4.4288363 -4.4289341][-4.4289265 -4.428926 -4.4288778 -4.4287562 -4.4285741 -4.4283309 -4.4280157 -4.4276752 -4.4276047 -4.4278464 -4.4281621 -4.4284563 -4.4286809 -4.4288397 -4.42894][-4.4288883 -4.4288745 -4.4288177 -4.4286933 -4.4285097 -4.4282579 -4.4279351 -4.4276214 -4.4276524 -4.4279542 -4.4282637 -4.4285226 -4.4287176 -4.4288611 -4.4289513][-4.4288406 -4.4288106 -4.4287524 -4.42865 -4.4285088 -4.4283156 -4.4280834 -4.4278913 -4.4279857 -4.4282355 -4.4284582 -4.4286394 -4.428781 -4.428894 -4.4289665][-4.4287958 -4.4287596 -4.4287119 -4.42864 -4.4285555 -4.4284439 -4.4283142 -4.4282217 -4.4283166 -4.4284797 -4.4286146 -4.4287286 -4.4288287 -4.4289193 -4.4289804][-4.4287686 -4.4287333 -4.4286933 -4.4286427 -4.4285989 -4.4285502 -4.4284906 -4.4284544 -4.4285264 -4.4286227 -4.4286995 -4.4287729 -4.4288497 -4.4289308 -4.4289865][-4.4287591 -4.4287271 -4.4286919 -4.4286542 -4.4286361 -4.428628 -4.4286137 -4.4286075 -4.4286518 -4.4287057 -4.4287486 -4.4288049 -4.428874 -4.4289489 -4.4289966][-4.4287705 -4.4287486 -4.4287229 -4.4287009 -4.4287043 -4.4287224 -4.42873 -4.4287276 -4.4287381 -4.4287548 -4.4287753 -4.4288244 -4.4288945 -4.428966 -4.4290051][-4.4288116 -4.4288039 -4.428792 -4.4287853 -4.4287982 -4.4288197 -4.4288192 -4.4287958 -4.4287691 -4.4287567 -4.4287677 -4.4288211 -4.4288988 -4.4289694 -4.4290047][-4.4288507 -4.428854 -4.4288549 -4.4288554 -4.4288626 -4.4288659 -4.4288411 -4.428792 -4.4287429 -4.4287195 -4.4287338 -4.4287987 -4.4288878 -4.4289589 -4.4289961][-4.4288559 -4.4288669 -4.4288774 -4.4288836 -4.4288816 -4.4288607 -4.4288096 -4.42874 -4.4286852 -4.4286666 -4.4286914 -4.4287724 -4.4288707 -4.4289446 -4.428987]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.1-lastlr0.5-clip10000/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.1-lastlr0.5-clip10000/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 04:51:38.249044: step 10, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:38m:45s remains)
INFO - root - 2017-12-10 04:51:40.884754: step 20, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:31m:43s remains)
INFO - root - 2017-12-10 04:51:43.466942: step 30, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:56m:15s remains)
INFO - root - 2017-12-10 04:51:46.129307: step 40, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:13m:31s remains)
INFO - root - 2017-12-10 04:51:48.716779: step 50, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:08m:31s remains)
INFO - root - 2017-12-10 04:51:51.323667: step 60, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:08m:43s remains)
INFO - root - 2017-12-10 04:51:53.944621: step 70, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 25h:05m:49s remains)
INFO - root - 2017-12-10 04:51:56.567432: step 80, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.282 sec/batch; 26h:00m:39s remains)
INFO - root - 2017-12-10 04:51:59.175069: step 90, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:57m:44s remains)
INFO - root - 2017-12-10 04:52:01.776614: step 100, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:53m:59s remains)
2017-12-10 04:52:02.146890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288268 -4.4288368 -4.42881 -4.4287796 -4.42874 -4.4287186 -4.42869 -4.428668 -4.4286928 -4.4287424 -4.42882 -4.42887 -4.4288931 -4.4288969 -4.428874][-4.4288387 -4.428844 -4.4288177 -4.4287744 -4.4287214 -4.4286938 -4.4286666 -4.4286432 -4.4286585 -4.42871 -4.4288025 -4.4288669 -4.4288874 -4.42889 -4.4288764][-4.428833 -4.4288268 -4.4288011 -4.4287539 -4.4287 -4.4286809 -4.4286666 -4.4286489 -4.4286523 -4.4286938 -4.4287858 -4.428854 -4.4288712 -4.4288754 -4.428875][-4.4287963 -4.4287696 -4.4287343 -4.4286804 -4.4286346 -4.4286442 -4.4286652 -4.4286718 -4.4286838 -4.42872 -4.4288015 -4.4288578 -4.4288678 -4.4288721 -4.4288783][-4.4287333 -4.4286804 -4.4286175 -4.4285455 -4.428514 -4.4285712 -4.4286494 -4.4287033 -4.4287395 -4.4287758 -4.4288378 -4.4288764 -4.4288807 -4.4288807 -4.4288855][-4.428668 -4.4285932 -4.4285007 -4.4284024 -4.4283729 -4.4284682 -4.4286094 -4.4287176 -4.4287834 -4.428823 -4.4288621 -4.4288759 -4.4288731 -4.4288735 -4.4288807][-4.4286494 -4.4285665 -4.4284596 -4.4283495 -4.4283171 -4.4284244 -4.4285994 -4.4287395 -4.4288192 -4.4288583 -4.4288731 -4.4288654 -4.4288521 -4.42885 -4.4288578][-4.4286809 -4.4286122 -4.4285164 -4.4284172 -4.4283895 -4.428483 -4.4286418 -4.4287744 -4.4288535 -4.4288907 -4.4288926 -4.4288726 -4.4288454 -4.4288321 -4.4288359][-4.4287491 -4.4287033 -4.4286389 -4.4285693 -4.4285507 -4.4286208 -4.4287353 -4.4288311 -4.4288883 -4.4289184 -4.4289203 -4.428895 -4.4288573 -4.4288287 -4.4288235][-4.4288354 -4.4288058 -4.428772 -4.42873 -4.4287186 -4.4287634 -4.428833 -4.4288893 -4.4289193 -4.4289403 -4.428946 -4.42892 -4.428875 -4.4288335 -4.4288187][-4.4289069 -4.4288836 -4.4288664 -4.4288425 -4.4288306 -4.42886 -4.4289055 -4.4289327 -4.4289422 -4.4289575 -4.4289637 -4.4289355 -4.4288821 -4.4288344 -4.4288116][-4.4289322 -4.4289122 -4.4289045 -4.4288917 -4.4288831 -4.4289026 -4.4289317 -4.4289436 -4.4289446 -4.4289541 -4.4289613 -4.4289351 -4.4288788 -4.4288254 -4.4287915][-4.4289289 -4.428916 -4.4289131 -4.4289064 -4.4289002 -4.4289103 -4.4289265 -4.4289327 -4.4289355 -4.428946 -4.4289474 -4.4289227 -4.4288678 -4.4288154 -4.4287739][-4.4289412 -4.4289384 -4.4289417 -4.4289427 -4.4289403 -4.4289417 -4.4289446 -4.4289451 -4.4289479 -4.4289551 -4.4289494 -4.4289207 -4.4288692 -4.4288225 -4.4287806][-4.4289703 -4.4289722 -4.428977 -4.4289823 -4.4289842 -4.4289851 -4.4289818 -4.4289765 -4.4289742 -4.4289742 -4.4289594 -4.4289289 -4.4288816 -4.4288383 -4.428793]]...]
INFO - root - 2017-12-10 04:52:04.760716: step 110, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:42m:58s remains)
INFO - root - 2017-12-10 04:52:07.396477: step 120, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:22m:53s remains)
INFO - root - 2017-12-10 04:52:10.001804: step 130, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:15m:03s remains)
INFO - root - 2017-12-10 04:52:12.628126: step 140, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 24h:09m:30s remains)
INFO - root - 2017-12-10 04:52:15.315036: step 150, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:20m:22s remains)
INFO - root - 2017-12-10 04:52:17.927189: step 160, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:07m:06s remains)
INFO - root - 2017-12-10 04:52:20.565040: step 170, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:52m:25s remains)
INFO - root - 2017-12-10 04:52:23.253738: step 180, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:54m:40s remains)
INFO - root - 2017-12-10 04:52:25.860729: step 190, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:50m:26s remains)
INFO - root - 2017-12-10 04:52:28.549744: step 200, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:55m:34s remains)
2017-12-10 04:52:28.963093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288988 -4.4288769 -4.4288588 -4.4288349 -4.4288197 -4.4288149 -4.4288177 -4.4288282 -4.4288344 -4.4288387 -4.4288273 -4.4288192 -4.4288225 -4.4288187 -4.428802][-4.4288721 -4.4288568 -4.4288473 -4.4288373 -4.4288316 -4.428844 -4.428864 -4.428875 -4.4288669 -4.4288473 -4.4288096 -4.4287777 -4.4287691 -4.4287658 -4.4287572][-4.4288311 -4.4288254 -4.4288278 -4.4288268 -4.4288244 -4.4288464 -4.428875 -4.4288812 -4.4288597 -4.4288311 -4.4287977 -4.4287691 -4.4287643 -4.4287686 -4.4287615][-4.428802 -4.4288011 -4.4288154 -4.4288168 -4.4288082 -4.4288263 -4.4288492 -4.4288397 -4.4288139 -4.4287944 -4.4287877 -4.4287872 -4.428802 -4.4288096 -4.4287982][-4.4287915 -4.428791 -4.4288063 -4.4288039 -4.4287839 -4.428771 -4.4287615 -4.4287295 -4.4287362 -4.428761 -4.4287877 -4.428822 -4.4288492 -4.4288588 -4.428844][-4.4288135 -4.4288011 -4.4287939 -4.4287763 -4.4287219 -4.4286423 -4.4285555 -4.4285135 -4.4286046 -4.4287229 -4.428802 -4.4288545 -4.4288836 -4.4289021 -4.4288874][-4.4288516 -4.4288363 -4.4288039 -4.4287553 -4.4286442 -4.4284587 -4.4282374 -4.4281626 -4.4283781 -4.4286242 -4.4287763 -4.4288535 -4.4288931 -4.4289255 -4.4289131][-4.4289 -4.4288859 -4.4288306 -4.428741 -4.4285789 -4.428319 -4.4280105 -4.4279094 -4.4282026 -4.4285274 -4.428731 -4.4288316 -4.4288883 -4.4289355 -4.4289145][-4.42893 -4.428916 -4.4288487 -4.4287515 -4.4286137 -4.4284153 -4.4282036 -4.4281316 -4.4283209 -4.4285669 -4.4287333 -4.4288211 -4.4288836 -4.428926 -4.4288878][-4.42893 -4.428915 -4.4288507 -4.4287724 -4.4286942 -4.4285989 -4.4285092 -4.4284716 -4.4285474 -4.4286809 -4.428782 -4.4288416 -4.4288917 -4.4289103 -4.42885][-4.4289145 -4.428894 -4.4288421 -4.4287882 -4.4287572 -4.4287381 -4.4287267 -4.4287219 -4.4287467 -4.4288039 -4.4288573 -4.4288898 -4.4289131 -4.4288964 -4.428812][-4.4289083 -4.4288797 -4.428843 -4.4288163 -4.4288111 -4.4288211 -4.4288464 -4.4288635 -4.4288754 -4.4288988 -4.4289193 -4.42893 -4.428925 -4.4288764 -4.4287744][-4.4289155 -4.4288821 -4.4288521 -4.4288392 -4.4288478 -4.4288692 -4.4289069 -4.4289346 -4.4289384 -4.4289412 -4.4289427 -4.4289393 -4.4289236 -4.4288692 -4.4287825][-4.4289341 -4.4289021 -4.4288721 -4.42886 -4.4288707 -4.4288917 -4.4289184 -4.4289384 -4.4289417 -4.4289417 -4.4289384 -4.4289317 -4.4289184 -4.4288731 -4.4288077][-4.4289484 -4.4289293 -4.4289021 -4.4288859 -4.4288878 -4.4288993 -4.4289117 -4.4289236 -4.4289293 -4.4289322 -4.4289312 -4.4289255 -4.4289169 -4.4288764 -4.428823]]...]
INFO - root - 2017-12-10 04:52:31.609896: step 210, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:37m:08s remains)
INFO - root - 2017-12-10 04:52:34.257933: step 220, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 24h:14m:11s remains)
INFO - root - 2017-12-10 04:52:36.871804: step 230, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:45m:17s remains)
INFO - root - 2017-12-10 04:52:39.550400: step 240, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:10m:53s remains)
INFO - root - 2017-12-10 04:52:42.224191: step 250, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:13m:34s remains)
INFO - root - 2017-12-10 04:52:44.830422: step 260, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:29m:33s remains)
INFO - root - 2017-12-10 04:52:47.493596: step 270, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:43m:04s remains)
INFO - root - 2017-12-10 04:52:50.154077: step 280, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:07m:32s remains)
INFO - root - 2017-12-10 04:52:52.780735: step 290, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:28m:41s remains)
INFO - root - 2017-12-10 04:52:55.458134: step 300, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 24h:03m:46s remains)
2017-12-10 04:52:55.845399: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286523 -4.4286394 -4.4286494 -4.4286838 -4.4287472 -4.4288106 -4.4288416 -4.42883 -4.4287934 -4.4287548 -4.4287291 -4.4287353 -4.4287438 -4.4287348 -4.4287133][-4.4287686 -4.4287477 -4.4287181 -4.4287009 -4.4287324 -4.4287939 -4.4288363 -4.4288435 -4.4288292 -4.428813 -4.4288044 -4.428812 -4.4288058 -4.4287724 -4.42873][-4.4288516 -4.4288244 -4.4287596 -4.4286971 -4.428688 -4.4287333 -4.4287891 -4.4288249 -4.4288383 -4.4288421 -4.4288425 -4.4288425 -4.4288192 -4.4287672 -4.4287138][-4.4288855 -4.4288497 -4.4287634 -4.4286675 -4.4286256 -4.4286513 -4.4287071 -4.4287596 -4.4287968 -4.4288182 -4.4288182 -4.428802 -4.4287634 -4.4287095 -4.4286623][-4.4288707 -4.4288287 -4.428741 -4.4286389 -4.4285827 -4.4285884 -4.4286313 -4.428688 -4.4287348 -4.4287629 -4.428762 -4.428731 -4.4286833 -4.4286284 -4.4285917][-4.4288492 -4.4288039 -4.4287257 -4.4286427 -4.4285979 -4.4285936 -4.4286203 -4.428669 -4.4287081 -4.4287233 -4.4287095 -4.4286642 -4.4286084 -4.4285517 -4.42852][-4.4288368 -4.4287906 -4.4287243 -4.4286623 -4.42863 -4.4286218 -4.4286356 -4.4286737 -4.4287009 -4.4286981 -4.4286666 -4.4286137 -4.4285541 -4.4284854 -4.4284544][-4.4288239 -4.428781 -4.4287291 -4.42868 -4.4286485 -4.4286327 -4.4286308 -4.4286594 -4.4286795 -4.428679 -4.4286509 -4.4285965 -4.4285274 -4.4284487 -4.4284191][-4.4288144 -4.4287853 -4.4287553 -4.4287262 -4.4287014 -4.4286828 -4.4286642 -4.4286752 -4.4286876 -4.4286942 -4.4286804 -4.42864 -4.42857 -4.4284964 -4.4284773][-4.4288034 -4.4287963 -4.4287863 -4.4287691 -4.4287558 -4.4287443 -4.4287214 -4.4287219 -4.4287271 -4.4287291 -4.4287233 -4.4287033 -4.4286509 -4.4285941 -4.4285851][-4.4287739 -4.4287915 -4.4287996 -4.4287972 -4.4287963 -4.428792 -4.4287672 -4.4287591 -4.4287558 -4.4287467 -4.428741 -4.4287453 -4.4287238 -4.4286933 -4.428688][-4.4287205 -4.428761 -4.42879 -4.4288092 -4.4288192 -4.4288187 -4.4287963 -4.4287844 -4.4287744 -4.4287491 -4.4287362 -4.4287553 -4.4287591 -4.42875 -4.4287524][-4.4286637 -4.42871 -4.4287467 -4.428781 -4.4288054 -4.4288144 -4.4288063 -4.4287996 -4.4287839 -4.4287462 -4.4287248 -4.4287481 -4.4287615 -4.4287591 -4.4287658][-4.4286609 -4.4286928 -4.4287219 -4.42876 -4.4287953 -4.4288177 -4.4288287 -4.4288268 -4.4287982 -4.4287496 -4.428719 -4.4287353 -4.4287524 -4.4287567 -4.4287686][-4.4287043 -4.4287186 -4.4287372 -4.4287658 -4.4288006 -4.4288325 -4.4288564 -4.4288611 -4.4288311 -4.42878 -4.4287415 -4.4287419 -4.4287539 -4.4287596 -4.428772]]...]
INFO - root - 2017-12-10 04:52:58.502447: step 310, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:21m:37s remains)
INFO - root - 2017-12-10 04:53:01.190304: step 320, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:52m:35s remains)
INFO - root - 2017-12-10 04:53:03.829111: step 330, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 25h:37m:20s remains)
INFO - root - 2017-12-10 04:53:06.534343: step 340, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 26h:15m:56s remains)
INFO - root - 2017-12-10 04:53:09.205925: step 350, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:11m:08s remains)
INFO - root - 2017-12-10 04:53:11.869531: step 360, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:39m:51s remains)
INFO - root - 2017-12-10 04:53:14.563195: step 370, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:52m:27s remains)
INFO - root - 2017-12-10 04:53:17.161188: step 380, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:48m:45s remains)
INFO - root - 2017-12-10 04:53:19.846571: step 390, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:32m:37s remains)
INFO - root - 2017-12-10 04:53:22.488451: step 400, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:34m:51s remains)
2017-12-10 04:53:22.848893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289045 -4.428894 -4.4288988 -4.4289083 -4.4289122 -4.4289088 -4.4289083 -4.4289141 -4.4289241 -4.4289355 -4.4289403 -4.42894 -4.4289393 -4.4289355 -4.4289241][-4.4288936 -4.428884 -4.4288921 -4.4289002 -4.4288969 -4.4288883 -4.4288869 -4.4288921 -4.4289036 -4.4289203 -4.4289289 -4.42893 -4.4289284 -4.4289231 -4.4289088][-4.4288735 -4.4288707 -4.4288826 -4.428885 -4.4288692 -4.4288483 -4.4288406 -4.4288435 -4.4288592 -4.4288812 -4.428894 -4.428895 -4.4288936 -4.42889 -4.4288769][-4.428854 -4.42886 -4.4288726 -4.428864 -4.4288321 -4.428793 -4.428771 -4.4287705 -4.4287939 -4.4288259 -4.4288464 -4.4288492 -4.4288497 -4.4288516 -4.4288454][-4.4288311 -4.4288411 -4.4288473 -4.4288287 -4.4287796 -4.4287143 -4.4286714 -4.4286752 -4.428721 -4.4287705 -4.4287996 -4.4288068 -4.428813 -4.4288249 -4.4288268][-4.4287944 -4.4287968 -4.4287906 -4.428762 -4.4286952 -4.4285936 -4.4285192 -4.4285378 -4.4286261 -4.4287057 -4.4287462 -4.4287562 -4.4287643 -4.4287834 -4.4287944][-4.4287395 -4.4287243 -4.4286995 -4.4286571 -4.428575 -4.4284296 -4.4283075 -4.4283442 -4.4284968 -4.4286246 -4.42869 -4.4287047 -4.4287157 -4.4287376 -4.4287615][-4.4286838 -4.42865 -4.4286132 -4.4285665 -4.42848 -4.4283037 -4.4281383 -4.4281969 -4.42841 -4.4285831 -4.4286652 -4.42868 -4.4286866 -4.4287033 -4.4287367][-4.4286456 -4.4286127 -4.4285874 -4.4285626 -4.4285035 -4.4283557 -4.4282165 -4.4282765 -4.4284711 -4.4286284 -4.4286895 -4.428688 -4.4286809 -4.428688 -4.4287181][-4.4286394 -4.4286213 -4.4286222 -4.4286289 -4.4286075 -4.428514 -4.4284229 -4.428463 -4.4285994 -4.4287019 -4.4287276 -4.4287109 -4.4286957 -4.4286947 -4.4287157][-4.4286585 -4.4286609 -4.4286847 -4.428709 -4.4287071 -4.4286528 -4.4285893 -4.4286094 -4.4286952 -4.4287539 -4.42876 -4.4287438 -4.4287295 -4.4287224 -4.4287305][-4.4286842 -4.4286966 -4.4287333 -4.4287643 -4.4287677 -4.4287357 -4.4286942 -4.4287033 -4.4287558 -4.4287896 -4.428792 -4.4287796 -4.4287691 -4.4287643 -4.4287672][-4.4287052 -4.4287181 -4.4287539 -4.4287825 -4.4287891 -4.4287715 -4.4287481 -4.4287562 -4.4287887 -4.4288106 -4.428812 -4.428802 -4.428792 -4.428792 -4.428793][-4.4287333 -4.428741 -4.428771 -4.4287953 -4.4288063 -4.4288034 -4.4287992 -4.4288087 -4.4288292 -4.4288435 -4.4288411 -4.4288268 -4.4288116 -4.4288063 -4.4288011][-4.4287839 -4.4287863 -4.4288087 -4.42883 -4.428843 -4.4288483 -4.4288578 -4.4288726 -4.4288874 -4.428896 -4.4288898 -4.4288716 -4.4288507 -4.4288378 -4.4288273]]...]
INFO - root - 2017-12-10 04:53:25.489401: step 410, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:39m:43s remains)
INFO - root - 2017-12-10 04:53:28.156744: step 420, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:52m:24s remains)
INFO - root - 2017-12-10 04:53:30.815717: step 430, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:53m:04s remains)
INFO - root - 2017-12-10 04:53:33.484969: step 440, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:35m:29s remains)
INFO - root - 2017-12-10 04:53:36.156993: step 450, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 24h:13m:12s remains)
INFO - root - 2017-12-10 04:53:38.825633: step 460, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:43m:55s remains)
INFO - root - 2017-12-10 04:53:41.485692: step 470, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:34m:07s remains)
INFO - root - 2017-12-10 04:53:44.187066: step 480, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 24h:07m:55s remains)
INFO - root - 2017-12-10 04:53:46.851908: step 490, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 24h:07m:31s remains)
INFO - root - 2017-12-10 04:53:49.493850: step 500, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:47m:09s remains)
2017-12-10 04:53:49.828482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288549 -4.428844 -4.4288378 -4.42883 -4.4288197 -4.4288163 -4.4288263 -4.4288359 -4.4288459 -4.4288516 -4.4288445 -4.4288135 -4.428771 -4.428731 -4.4286923][-4.428822 -4.4288106 -4.4288015 -4.4287872 -4.4287715 -4.42877 -4.42879 -4.4288158 -4.4288468 -4.4288616 -4.4288516 -4.4288034 -4.4287448 -4.4287028 -4.4286685][-4.4287925 -4.4287777 -4.4287648 -4.4287405 -4.4287205 -4.4287238 -4.4287596 -4.4288111 -4.42886 -4.4288721 -4.4288464 -4.4287834 -4.4287152 -4.4286742 -4.42864][-4.4287672 -4.4287443 -4.4287224 -4.4286842 -4.4286551 -4.4286661 -4.4287224 -4.4287968 -4.428853 -4.4288554 -4.428813 -4.4287362 -4.4286675 -4.4286351 -4.4286156][-4.4287372 -4.428709 -4.4286709 -4.4286122 -4.4285693 -4.4285803 -4.4286461 -4.4287357 -4.4287996 -4.4288034 -4.4287462 -4.4286613 -4.4286137 -4.4286056 -4.4286156][-4.4287038 -4.4286757 -4.4286222 -4.4285455 -4.4284911 -4.4284868 -4.4285359 -4.4286227 -4.4286952 -4.4287004 -4.4286375 -4.4285693 -4.4285607 -4.428587 -4.4286242][-4.4286847 -4.428658 -4.4285927 -4.4284997 -4.4284253 -4.4283819 -4.4283872 -4.4284487 -4.42853 -4.4285502 -4.4284954 -4.4284644 -4.4285088 -4.4285684 -4.4286208][-4.42866 -4.4286437 -4.4285851 -4.4284916 -4.4283915 -4.428287 -4.4282084 -4.4282155 -4.4283285 -4.4284019 -4.4283957 -4.4284148 -4.4284906 -4.4285579 -4.4286127][-4.428628 -4.4286132 -4.4285665 -4.4284906 -4.4283853 -4.4282484 -4.4281049 -4.4280629 -4.4281964 -4.428328 -4.4283862 -4.4284544 -4.4285359 -4.428586 -4.428628][-4.4286003 -4.4285693 -4.4285331 -4.4284911 -4.428421 -4.4283133 -4.4281912 -4.4281526 -4.4282603 -4.4283772 -4.4284554 -4.4285426 -4.4286151 -4.4286447 -4.4286575][-4.4285913 -4.4285607 -4.4285469 -4.4285436 -4.4285207 -4.4284568 -4.4283829 -4.4283705 -4.4284339 -4.4284945 -4.4285436 -4.428618 -4.4286785 -4.4286871 -4.4286723][-4.4286079 -4.4286017 -4.4286137 -4.4286308 -4.4286275 -4.4285927 -4.4285536 -4.4285488 -4.42857 -4.4285779 -4.4285846 -4.428628 -4.4286709 -4.4286742 -4.42866][-4.4286671 -4.4286771 -4.4286919 -4.4287076 -4.4287086 -4.4286885 -4.4286661 -4.4286537 -4.428648 -4.428627 -4.428607 -4.4286251 -4.4286575 -4.4286718 -4.428678][-4.4287305 -4.4287333 -4.4287415 -4.4287491 -4.4287481 -4.4287348 -4.428721 -4.4287057 -4.4286904 -4.4286695 -4.4286537 -4.4286637 -4.4286885 -4.4287105 -4.4287338][-4.4287562 -4.4287543 -4.428762 -4.4287639 -4.4287581 -4.4287457 -4.4287329 -4.4287238 -4.4287214 -4.42872 -4.4287219 -4.4287257 -4.4287357 -4.4287519 -4.4287696]]...]
INFO - root - 2017-12-10 04:53:52.504635: step 510, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:27m:03s remains)
INFO - root - 2017-12-10 04:53:55.168904: step 520, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:37m:35s remains)
INFO - root - 2017-12-10 04:53:57.834048: step 530, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 24h:12m:28s remains)
INFO - root - 2017-12-10 04:54:00.529842: step 540, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:30m:11s remains)
INFO - root - 2017-12-10 04:54:03.205240: step 550, loss = 2.28, batch loss = 2.23 (27.6 examples/sec; 0.290 sec/batch; 26h:46m:13s remains)
INFO - root - 2017-12-10 04:54:05.913522: step 560, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:33m:11s remains)
INFO - root - 2017-12-10 04:54:08.596108: step 570, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:41m:35s remains)
INFO - root - 2017-12-10 04:54:11.291168: step 580, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:48m:07s remains)
INFO - root - 2017-12-10 04:54:13.942766: step 590, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:27m:13s remains)
INFO - root - 2017-12-10 04:54:16.625997: step 600, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:45m:38s remains)
2017-12-10 04:54:16.962072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288449 -4.4288249 -4.4288173 -4.4288 -4.4287863 -4.4287782 -4.4287753 -4.4288154 -4.4288812 -4.428937 -4.4289556 -4.42896 -4.4289408 -4.4288955 -4.4288654][-4.4288144 -4.4287949 -4.4287953 -4.4287915 -4.4287806 -4.4287672 -4.4287558 -4.4287882 -4.4288511 -4.4289064 -4.4289203 -4.4289131 -4.4288945 -4.4288716 -4.4288669][-4.4287815 -4.4287558 -4.4287624 -4.4287777 -4.42878 -4.4287643 -4.4287443 -4.4287691 -4.4288192 -4.4288583 -4.428863 -4.4288535 -4.4288392 -4.4288263 -4.4288354][-4.428741 -4.4287167 -4.4287276 -4.42875 -4.4287572 -4.4287314 -4.4287019 -4.4287157 -4.4287534 -4.4287868 -4.4288011 -4.4288044 -4.4287949 -4.428781 -4.4287848][-4.4286971 -4.42869 -4.4287119 -4.4287286 -4.4287152 -4.428669 -4.428627 -4.4286304 -4.4286666 -4.4287148 -4.4287515 -4.4287696 -4.4287586 -4.42873 -4.4287162][-4.4286714 -4.4286838 -4.4287043 -4.4286957 -4.4286451 -4.4285665 -4.4285078 -4.4285078 -4.4285669 -4.4286475 -4.4287024 -4.42872 -4.4287033 -4.4286585 -4.4286189][-4.4286723 -4.4286857 -4.4286909 -4.4286489 -4.4285641 -4.4284534 -4.4283795 -4.4283824 -4.4284606 -4.4285574 -4.4286156 -4.4286313 -4.4286089 -4.4285479 -4.4284916][-4.4286771 -4.4286761 -4.4286695 -4.4286141 -4.4285135 -4.4283948 -4.4283204 -4.4283247 -4.428401 -4.4284792 -4.428515 -4.4285173 -4.4284892 -4.4284215 -4.4283767][-4.4287081 -4.4286938 -4.4286857 -4.4286413 -4.4285536 -4.4284511 -4.4283838 -4.4283772 -4.4284148 -4.4284472 -4.4284549 -4.4284439 -4.4284115 -4.4283528 -4.4283381][-4.42875 -4.4287415 -4.4287424 -4.4287224 -4.42867 -4.4286017 -4.4285526 -4.428535 -4.4285417 -4.4285378 -4.4285207 -4.4284949 -4.4284525 -4.4284005 -4.4284034][-4.428791 -4.428791 -4.4287987 -4.4288034 -4.4287868 -4.4287534 -4.4287286 -4.4287133 -4.4287028 -4.4286904 -4.42867 -4.4286385 -4.4285913 -4.4285436 -4.4285469][-4.4288173 -4.42882 -4.4288244 -4.4288359 -4.42884 -4.4288344 -4.4288378 -4.4288387 -4.4288306 -4.4288216 -4.4288082 -4.4287848 -4.4287481 -4.4287086 -4.4287057][-4.428833 -4.4288316 -4.42883 -4.428834 -4.428843 -4.4288568 -4.4288778 -4.4288883 -4.4288883 -4.428885 -4.4288778 -4.428865 -4.4288449 -4.4288168 -4.4288135][-4.4288397 -4.4288349 -4.4288278 -4.4288249 -4.4288363 -4.4288621 -4.4288888 -4.4289017 -4.4289021 -4.428896 -4.4288864 -4.4288769 -4.428863 -4.4288383 -4.4288359][-4.4288363 -4.4288378 -4.428834 -4.4288321 -4.4288478 -4.4288745 -4.4288964 -4.4289045 -4.4288983 -4.4288807 -4.4288597 -4.4288435 -4.42883 -4.4288087 -4.4288125]]...]
INFO - root - 2017-12-10 04:54:19.581623: step 610, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:58m:05s remains)
INFO - root - 2017-12-10 04:54:22.271385: step 620, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 25h:14m:45s remains)
INFO - root - 2017-12-10 04:54:24.974295: step 630, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:31m:43s remains)
INFO - root - 2017-12-10 04:54:27.601187: step 640, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:28m:57s remains)
INFO - root - 2017-12-10 04:54:30.210770: step 650, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:19m:15s remains)
INFO - root - 2017-12-10 04:54:32.907115: step 660, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:35m:23s remains)
INFO - root - 2017-12-10 04:54:35.535545: step 670, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:25m:36s remains)
INFO - root - 2017-12-10 04:54:38.179676: step 680, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:18m:25s remains)
INFO - root - 2017-12-10 04:54:40.823665: step 690, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 25h:22m:47s remains)
INFO - root - 2017-12-10 04:54:43.490077: step 700, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:34m:27s remains)
2017-12-10 04:54:43.937654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287429 -4.42873 -4.4287281 -4.4287305 -4.4287329 -4.428731 -4.4287262 -4.4287219 -4.4287238 -4.4287286 -4.4287319 -4.4287276 -4.4287157 -4.428699 -4.4286904][-4.42876 -4.428741 -4.4287338 -4.428731 -4.4287271 -4.4287181 -4.4287028 -4.428689 -4.4286852 -4.428688 -4.4286919 -4.4286919 -4.4286847 -4.4286709 -4.4286675][-4.4288058 -4.4287872 -4.428782 -4.428781 -4.4287777 -4.4287648 -4.4287438 -4.4287224 -4.4287109 -4.4287086 -4.4287086 -4.4287105 -4.42871 -4.4287028 -4.4287028][-4.4288278 -4.4288073 -4.428802 -4.4288073 -4.4288116 -4.4288039 -4.4287858 -4.4287658 -4.4287567 -4.4287548 -4.4287534 -4.4287543 -4.4287548 -4.4287477 -4.4287448][-4.4288096 -4.4287782 -4.4287663 -4.4287744 -4.4287887 -4.42879 -4.4287782 -4.4287677 -4.4287739 -4.4287853 -4.4287934 -4.4287987 -4.4287977 -4.4287844 -4.4287734][-4.428762 -4.4287171 -4.4286957 -4.4287047 -4.4287233 -4.4287243 -4.428709 -4.4287057 -4.4287314 -4.4287682 -4.4287996 -4.4288206 -4.4288259 -4.4288106 -4.428792][-4.4287243 -4.4286733 -4.428647 -4.4286523 -4.4286609 -4.4286451 -4.4286137 -4.4286051 -4.4286413 -4.4287014 -4.4287596 -4.4288073 -4.42883 -4.428823 -4.4288044][-4.4287205 -4.428679 -4.4286566 -4.4286556 -4.42865 -4.428617 -4.4285665 -4.4285445 -4.4285774 -4.4286442 -4.4287138 -4.4287758 -4.428812 -4.4288154 -4.4288039][-4.4287419 -4.4287181 -4.4287066 -4.4287071 -4.4287009 -4.4286709 -4.4286246 -4.4285984 -4.4286137 -4.4286594 -4.4287133 -4.4287653 -4.4287972 -4.428802 -4.4287949][-4.4287796 -4.4287667 -4.4287596 -4.4287586 -4.4287539 -4.4287353 -4.4287071 -4.4286942 -4.4287043 -4.4287319 -4.4287643 -4.4287934 -4.4288063 -4.4287996 -4.4287887][-4.4288173 -4.4288015 -4.4287887 -4.4287791 -4.4287696 -4.4287558 -4.4287429 -4.4287457 -4.4287615 -4.4287834 -4.4288054 -4.4288206 -4.4288173 -4.4287982 -4.4287806][-4.4288392 -4.4288163 -4.4287987 -4.4287863 -4.4287772 -4.4287672 -4.42876 -4.4287667 -4.4287796 -4.4287949 -4.4288111 -4.4288197 -4.42881 -4.4287872 -4.4287686][-4.4288363 -4.4288125 -4.4287972 -4.428793 -4.428792 -4.4287891 -4.4287839 -4.4287853 -4.4287896 -4.4287958 -4.4288054 -4.4288096 -4.4287968 -4.4287739 -4.4287596][-4.4288135 -4.4287925 -4.4287825 -4.4287872 -4.4287958 -4.4288011 -4.4288011 -4.4288006 -4.4287977 -4.4287949 -4.4287977 -4.4287968 -4.428782 -4.42876 -4.4287505][-4.4287891 -4.4287715 -4.4287639 -4.428772 -4.4287829 -4.4287906 -4.4287944 -4.4287958 -4.428792 -4.4287868 -4.4287853 -4.4287825 -4.4287682 -4.4287491 -4.428741]]...]
INFO - root - 2017-12-10 04:54:46.585338: step 710, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:09m:41s remains)
INFO - root - 2017-12-10 04:54:49.267300: step 720, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:50m:31s remains)
INFO - root - 2017-12-10 04:54:51.933224: step 730, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:30m:38s remains)
INFO - root - 2017-12-10 04:54:54.571918: step 740, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:29m:37s remains)
INFO - root - 2017-12-10 04:54:57.184748: step 750, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:24m:27s remains)
INFO - root - 2017-12-10 04:54:59.837538: step 760, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:27m:16s remains)
INFO - root - 2017-12-10 04:55:02.532244: step 770, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:27m:30s remains)
INFO - root - 2017-12-10 04:55:05.181218: step 780, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:07m:47s remains)
INFO - root - 2017-12-10 04:55:07.850343: step 790, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:24m:24s remains)
INFO - root - 2017-12-10 04:55:10.515615: step 800, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:37m:31s remains)
2017-12-10 04:55:10.897189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286852 -4.428731 -4.4287715 -4.4287748 -4.4287481 -4.4286933 -4.4286585 -4.4286804 -4.4287128 -4.4287343 -4.42874 -4.4287338 -4.4287314 -4.4287534 -4.4287877][-4.4287353 -4.4287782 -4.428812 -4.4288111 -4.4287853 -4.4287286 -4.4286852 -4.4286885 -4.4287086 -4.4287238 -4.4287186 -4.4287009 -4.4286904 -4.4287128 -4.4287553][-4.4287796 -4.4288125 -4.4288449 -4.428853 -4.4288411 -4.4287996 -4.4287515 -4.4287295 -4.4287329 -4.4287434 -4.4287224 -4.4286895 -4.4286714 -4.4286919 -4.4287362][-4.4288073 -4.4288297 -4.4288568 -4.4288731 -4.4288669 -4.4288378 -4.4287863 -4.4287477 -4.42874 -4.4287515 -4.4287271 -4.4286857 -4.4286671 -4.428689 -4.4287367][-4.4288197 -4.4288216 -4.4288397 -4.428853 -4.4288473 -4.4288278 -4.4287777 -4.4287186 -4.4287033 -4.4287181 -4.4287033 -4.4286671 -4.4286566 -4.4286776 -4.4287214][-4.4288034 -4.4287796 -4.4287806 -4.4287863 -4.4287777 -4.4287615 -4.4287 -4.4286051 -4.4285913 -4.4286251 -4.4286308 -4.428607 -4.4286103 -4.428637 -4.4286823][-4.4287443 -4.4286895 -4.4286671 -4.4286666 -4.42866 -4.4286413 -4.4285574 -4.4284158 -4.428411 -4.4284825 -4.42852 -4.4285283 -4.4285483 -4.4285936 -4.4286532][-4.4286804 -4.4286256 -4.428607 -4.428617 -4.4286265 -4.4286127 -4.4285231 -4.4283724 -4.42837 -4.42845 -4.428494 -4.4285216 -4.4285426 -4.428596 -4.4286613][-4.4286704 -4.4286466 -4.4286594 -4.4286876 -4.42871 -4.4287052 -4.428648 -4.4285564 -4.428546 -4.4285793 -4.4285951 -4.4286118 -4.4286118 -4.4286408 -4.42869][-4.4287152 -4.4287281 -4.4287677 -4.428803 -4.4288268 -4.4288268 -4.4287963 -4.42875 -4.4287295 -4.4287257 -4.428719 -4.4287162 -4.4286981 -4.4286962 -4.4287229][-4.428772 -4.4288034 -4.4288487 -4.4288836 -4.4289055 -4.428906 -4.4288816 -4.4288468 -4.4288254 -4.4288082 -4.4287896 -4.428771 -4.4287462 -4.4287395 -4.4287534][-4.4287868 -4.4288211 -4.4288573 -4.4288874 -4.4289117 -4.4289165 -4.4288983 -4.4288664 -4.4288483 -4.4288311 -4.4288073 -4.4287777 -4.4287581 -4.428772 -4.4287925][-4.4287653 -4.4287968 -4.428812 -4.4288235 -4.4288521 -4.4288716 -4.428864 -4.4288383 -4.4288282 -4.4288254 -4.42881 -4.42878 -4.4287705 -4.4287996 -4.4288282][-4.4287496 -4.428772 -4.4287605 -4.4287548 -4.4287872 -4.4288206 -4.428822 -4.4288092 -4.4288077 -4.4288154 -4.4288082 -4.4287887 -4.4287944 -4.4288225 -4.4288526][-4.4287786 -4.4287977 -4.4287758 -4.4287562 -4.428781 -4.4288111 -4.4288096 -4.4287987 -4.428802 -4.428813 -4.4288116 -4.4288034 -4.4288197 -4.4288425 -4.4288664]]...]
INFO - root - 2017-12-10 04:55:13.581816: step 810, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 24h:02m:36s remains)
INFO - root - 2017-12-10 04:55:16.247748: step 820, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:45m:21s remains)
INFO - root - 2017-12-10 04:55:18.870958: step 830, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.277 sec/batch; 25h:33m:03s remains)
INFO - root - 2017-12-10 04:55:21.546223: step 840, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 24h:12m:10s remains)
INFO - root - 2017-12-10 04:55:24.181814: step 850, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:44m:40s remains)
INFO - root - 2017-12-10 04:55:26.818187: step 860, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:43m:17s remains)
INFO - root - 2017-12-10 04:55:29.491928: step 870, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 24h:05m:55s remains)
INFO - root - 2017-12-10 04:55:32.161227: step 880, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:48m:34s remains)
INFO - root - 2017-12-10 04:55:34.851463: step 890, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:33m:37s remains)
INFO - root - 2017-12-10 04:55:37.484087: step 900, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:39m:04s remains)
2017-12-10 04:55:37.882994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285812 -4.4285469 -4.4285707 -4.428617 -4.4286704 -4.4287076 -4.4287324 -4.4287291 -4.4287152 -4.4286995 -4.428689 -4.4287004 -4.4287252 -4.4287591 -4.4287839][-4.4285622 -4.4285259 -4.4285417 -4.4285851 -4.42864 -4.4286785 -4.4287014 -4.4287028 -4.4286957 -4.4286828 -4.4286828 -4.4287004 -4.4287248 -4.4287505 -4.428771][-4.4285312 -4.428504 -4.4285192 -4.4285612 -4.4286051 -4.4286318 -4.4286447 -4.4286489 -4.4286494 -4.4286442 -4.4286551 -4.4286838 -4.4287181 -4.4287448 -4.4287586][-4.4285007 -4.4284916 -4.4285173 -4.4285564 -4.428587 -4.42859 -4.4285841 -4.4285932 -4.4286041 -4.4286041 -4.4286208 -4.4286675 -4.4287095 -4.4287238 -4.4287128][-4.4284773 -4.4284883 -4.4285264 -4.4285612 -4.428575 -4.4285531 -4.4285164 -4.4285183 -4.42854 -4.428556 -4.4285846 -4.4286332 -4.4286704 -4.4286804 -4.4286604][-4.42847 -4.4284887 -4.4285293 -4.4285541 -4.4285469 -4.4284949 -4.4284234 -4.42841 -4.4284496 -4.428493 -4.4285369 -4.4285784 -4.4286089 -4.4286251 -4.4286165][-4.4284759 -4.42848 -4.4285126 -4.4285283 -4.4285073 -4.4284282 -4.4283185 -4.4282627 -4.4283032 -4.4283834 -4.4284644 -4.4285207 -4.428556 -4.4285831 -4.4285879][-4.4284968 -4.4284849 -4.428504 -4.4285107 -4.4284806 -4.4283838 -4.4282441 -4.4281278 -4.4281497 -4.4282708 -4.4284124 -4.428515 -4.4285727 -4.4286051 -4.4286132][-4.428535 -4.4285097 -4.4285083 -4.4284987 -4.4284673 -4.4283881 -4.4282727 -4.4281478 -4.4281478 -4.4282727 -4.4284439 -4.42857 -4.4286294 -4.428658 -4.4286742][-4.4285946 -4.4285574 -4.4285274 -4.428494 -4.4284606 -4.4284148 -4.4283638 -4.4283004 -4.4283042 -4.4284 -4.4285412 -4.4286494 -4.4286976 -4.4287167 -4.4287291][-4.4286537 -4.4286075 -4.4285645 -4.4285164 -4.4284835 -4.4284611 -4.4284554 -4.42845 -4.4284749 -4.4285421 -4.4286337 -4.4287081 -4.4287376 -4.4287443 -4.4287491][-4.4286971 -4.4286537 -4.4286146 -4.4285741 -4.42855 -4.428535 -4.4285383 -4.4285536 -4.4285865 -4.428637 -4.4286885 -4.4287319 -4.4287467 -4.4287448 -4.4287448][-4.4287338 -4.4287014 -4.4286733 -4.4286432 -4.4286304 -4.42862 -4.4286194 -4.428627 -4.4286513 -4.4286909 -4.4287229 -4.4287462 -4.4287515 -4.4287477 -4.4287491][-4.4287534 -4.4287314 -4.4287148 -4.4286976 -4.4286904 -4.428689 -4.4286866 -4.4286866 -4.4286971 -4.4287233 -4.4287405 -4.4287524 -4.4287539 -4.4287505 -4.4287539][-4.4287677 -4.4287543 -4.4287467 -4.4287353 -4.42873 -4.4287324 -4.4287329 -4.428731 -4.4287338 -4.4287462 -4.4287534 -4.4287567 -4.4287562 -4.4287553 -4.4287577]]...]
INFO - root - 2017-12-10 04:55:40.494968: step 910, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:39m:24s remains)
INFO - root - 2017-12-10 04:55:43.145670: step 920, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 24h:21m:35s remains)
INFO - root - 2017-12-10 04:55:45.791883: step 930, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.275 sec/batch; 25h:22m:14s remains)
INFO - root - 2017-12-10 04:55:48.469670: step 940, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:48m:13s remains)
INFO - root - 2017-12-10 04:55:51.096957: step 950, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:50m:00s remains)
INFO - root - 2017-12-10 04:55:53.775567: step 960, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:23m:23s remains)
INFO - root - 2017-12-10 04:55:56.432707: step 970, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:10m:52s remains)
INFO - root - 2017-12-10 04:55:59.065397: step 980, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:03m:08s remains)
INFO - root - 2017-12-10 04:56:01.748032: step 990, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 26h:13m:50s remains)
INFO - root - 2017-12-10 04:56:04.426712: step 1000, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:50m:28s remains)
2017-12-10 04:56:04.792629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286609 -4.4286675 -4.4286723 -4.4286895 -4.4287195 -4.4287343 -4.4287133 -4.4286785 -4.4286752 -4.4286904 -4.4287038 -4.4287281 -4.4287734 -4.428822 -4.4288654][-4.4286337 -4.4286447 -4.4286528 -4.4286747 -4.4287071 -4.4287238 -4.4287028 -4.4286561 -4.4286447 -4.4286709 -4.4287014 -4.4287391 -4.42879 -4.42884 -4.4288807][-4.4286218 -4.4286375 -4.42866 -4.4286885 -4.4287167 -4.4287248 -4.4286981 -4.4286404 -4.4286146 -4.4286461 -4.4286919 -4.4287381 -4.4287882 -4.4288378 -4.42888][-4.4286175 -4.4286413 -4.4286785 -4.4287176 -4.4287415 -4.4287324 -4.4286923 -4.4286227 -4.4285879 -4.4286232 -4.4286747 -4.4287233 -4.428771 -4.4288211 -4.4288673][-4.4286466 -4.4286718 -4.4287071 -4.428741 -4.4287548 -4.4287267 -4.4286714 -4.428587 -4.4285426 -4.4285841 -4.4286437 -4.4286966 -4.4287481 -4.428803 -4.4288564][-4.4287224 -4.4287391 -4.4287572 -4.4287691 -4.428761 -4.4287128 -4.4286375 -4.4285312 -4.4284749 -4.4285307 -4.4286146 -4.4286804 -4.4287405 -4.4288006 -4.4288583][-4.4287772 -4.4287777 -4.4287763 -4.4287667 -4.4287424 -4.42868 -4.4285884 -4.4284592 -4.4283915 -4.42847 -4.4285889 -4.4286776 -4.4287486 -4.428812 -4.4288678][-4.4287786 -4.4287643 -4.428751 -4.4287333 -4.4287024 -4.428637 -4.4285374 -4.4283957 -4.4283237 -4.4284296 -4.4285769 -4.4286823 -4.4287615 -4.4288249 -4.4288769][-4.4287753 -4.4287539 -4.4287319 -4.4287109 -4.4286828 -4.4286308 -4.4285507 -4.4284306 -4.4283724 -4.428472 -4.428607 -4.4287004 -4.4287724 -4.4288316 -4.4288826][-4.42875 -4.4287262 -4.428699 -4.4286771 -4.4286609 -4.4286342 -4.4285908 -4.4285126 -4.4284673 -4.4285307 -4.4286251 -4.4286947 -4.4287558 -4.4288149 -4.4288712][-4.4287238 -4.4286966 -4.4286675 -4.4286475 -4.4286437 -4.4286394 -4.428628 -4.4285841 -4.428545 -4.4285746 -4.4286351 -4.4286852 -4.4287381 -4.4287992 -4.4288621][-4.4287043 -4.4286766 -4.4286489 -4.4286327 -4.4286385 -4.4286494 -4.428658 -4.4286346 -4.428597 -4.4286079 -4.4286518 -4.4286919 -4.4287424 -4.4288034 -4.4288664][-4.4286962 -4.4286718 -4.428648 -4.428638 -4.4286485 -4.4286652 -4.42868 -4.4286637 -4.428628 -4.4286337 -4.4286737 -4.4287128 -4.4287643 -4.4288249 -4.428884][-4.4287143 -4.4286971 -4.4286809 -4.4286766 -4.4286876 -4.4287 -4.4287081 -4.4286876 -4.4286551 -4.4286656 -4.4287124 -4.4287553 -4.4288058 -4.4288597 -4.4289093][-4.4287677 -4.4287577 -4.4287505 -4.4287505 -4.4287562 -4.4287591 -4.4287562 -4.4287333 -4.42871 -4.4287267 -4.4287763 -4.4288192 -4.42886 -4.4288993 -4.4289346]]...]
INFO - root - 2017-12-10 04:56:07.469830: step 1010, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:53m:23s remains)
INFO - root - 2017-12-10 04:56:10.084722: step 1020, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:58m:10s remains)
INFO - root - 2017-12-10 04:56:12.763196: step 1030, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:41m:06s remains)
INFO - root - 2017-12-10 04:56:15.421307: step 1040, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:43m:31s remains)
INFO - root - 2017-12-10 04:56:18.071359: step 1050, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:38m:58s remains)
INFO - root - 2017-12-10 04:56:20.702976: step 1060, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:27m:23s remains)
INFO - root - 2017-12-10 04:56:23.361465: step 1070, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:23m:19s remains)
INFO - root - 2017-12-10 04:56:26.014944: step 1080, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:54m:42s remains)
INFO - root - 2017-12-10 04:56:28.700134: step 1090, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:44m:57s remains)
INFO - root - 2017-12-10 04:56:31.335108: step 1100, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:26m:32s remains)
2017-12-10 04:56:31.693959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287171 -4.42864 -4.4286065 -4.4286184 -4.4286189 -4.428638 -4.4286876 -4.4287271 -4.4287596 -4.428792 -4.4288216 -4.4288578 -4.4288921 -4.428916 -4.4289246][-4.4287539 -4.4286861 -4.4286337 -4.4285946 -4.4285364 -4.4285278 -4.4285917 -4.4286637 -4.4287195 -4.4287705 -4.4288096 -4.4288473 -4.428884 -4.4289112 -4.428925][-4.4287634 -4.4286981 -4.4286284 -4.4285455 -4.4284449 -4.4284234 -4.4285073 -4.4286094 -4.4286952 -4.4287648 -4.4288096 -4.428844 -4.4288754 -4.4289002 -4.4289165][-4.4287658 -4.42871 -4.4286447 -4.4285536 -4.428443 -4.4284139 -4.4284887 -4.4285946 -4.4286895 -4.4287658 -4.4288116 -4.4288445 -4.4288712 -4.4288921 -4.4289107][-4.4287729 -4.4287314 -4.4286761 -4.4286 -4.4285045 -4.4284759 -4.428525 -4.4286 -4.4286771 -4.4287443 -4.4287925 -4.4288268 -4.42886 -4.4288855 -4.4289069][-4.428762 -4.4287281 -4.4286795 -4.4286156 -4.4285369 -4.4285021 -4.4285145 -4.4285564 -4.4286189 -4.4286871 -4.4287481 -4.4287982 -4.4288468 -4.428884 -4.4289064][-4.4287505 -4.428721 -4.4286733 -4.4286132 -4.4285398 -4.4284863 -4.4284544 -4.4284625 -4.4285221 -4.4286036 -4.4286876 -4.4287648 -4.4288325 -4.428884 -4.4289112][-4.4287748 -4.42875 -4.4287081 -4.4286532 -4.4285774 -4.4285088 -4.4284439 -4.4284239 -4.4284825 -4.4285731 -4.4286666 -4.4287567 -4.4288359 -4.4288931 -4.428925][-4.4288225 -4.4288044 -4.4287686 -4.4287138 -4.4286366 -4.428566 -4.4285 -4.4284792 -4.4285316 -4.4286218 -4.4287086 -4.4287934 -4.4288683 -4.4289203 -4.428947][-4.4288497 -4.42884 -4.4288144 -4.428762 -4.4286847 -4.4286213 -4.4285731 -4.4285579 -4.4285946 -4.4286695 -4.4287515 -4.4288297 -4.428896 -4.42894 -4.4289603][-4.4288359 -4.4288425 -4.4288359 -4.4287958 -4.4287305 -4.4286747 -4.4286375 -4.4286165 -4.4286237 -4.4286747 -4.4287543 -4.428833 -4.4288969 -4.4289384 -4.4289551][-4.4288235 -4.4288392 -4.4288483 -4.42883 -4.4287858 -4.4287405 -4.4287076 -4.4286766 -4.4286532 -4.4286766 -4.4287429 -4.4288158 -4.4288807 -4.4289246 -4.4289427][-4.4288487 -4.4288588 -4.4288678 -4.4288611 -4.4288378 -4.4288058 -4.4287806 -4.4287472 -4.4287033 -4.4287052 -4.4287505 -4.4288111 -4.42887 -4.4289131 -4.4289322][-4.4288774 -4.4288707 -4.4288664 -4.42886 -4.428854 -4.4288425 -4.4288306 -4.4288015 -4.4287505 -4.4287357 -4.4287653 -4.4288163 -4.4288659 -4.4289031 -4.4289222][-4.4288807 -4.4288635 -4.4288492 -4.4288373 -4.4288373 -4.4288421 -4.4288425 -4.4288192 -4.4287729 -4.4287534 -4.4287734 -4.4288182 -4.428863 -4.428895 -4.4289103]]...]
INFO - root - 2017-12-10 04:56:34.406093: step 1110, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:49m:00s remains)
INFO - root - 2017-12-10 04:56:37.049708: step 1120, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:54m:23s remains)
INFO - root - 2017-12-10 04:56:39.694661: step 1130, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 25h:00m:00s remains)
INFO - root - 2017-12-10 04:56:42.399989: step 1140, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:33m:22s remains)
INFO - root - 2017-12-10 04:56:45.030535: step 1150, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:48m:29s remains)
INFO - root - 2017-12-10 04:56:47.728438: step 1160, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:43m:55s remains)
INFO - root - 2017-12-10 04:56:50.406105: step 1170, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:28m:57s remains)
INFO - root - 2017-12-10 04:56:53.053376: step 1180, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.278 sec/batch; 25h:36m:37s remains)
INFO - root - 2017-12-10 04:56:55.696729: step 1190, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:28m:31s remains)
INFO - root - 2017-12-10 04:56:58.334772: step 1200, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:28m:41s remains)
2017-12-10 04:56:58.732715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289203 -4.4289207 -4.4289217 -4.4289231 -4.4289222 -4.4289203 -4.4289179 -4.4289169 -4.4289188 -4.4289241 -4.4289312 -4.4289403 -4.4289503 -4.4289637 -4.4289784][-4.4288969 -4.4288974 -4.4288993 -4.4289036 -4.4289041 -4.4288993 -4.428894 -4.4288921 -4.4288983 -4.4289103 -4.4289207 -4.42893 -4.4289379 -4.4289474 -4.4289603][-4.428874 -4.4288745 -4.4288769 -4.4288826 -4.4288845 -4.4288764 -4.428865 -4.4288611 -4.4288726 -4.428895 -4.4289112 -4.4289212 -4.4289279 -4.4289327 -4.4289389][-4.4288406 -4.4288406 -4.4288406 -4.4288445 -4.4288478 -4.4288378 -4.4288144 -4.4288015 -4.42882 -4.4288564 -4.4288855 -4.4289017 -4.4289107 -4.428915 -4.4289169][-4.4287977 -4.4287953 -4.4287891 -4.428792 -4.4287958 -4.4287791 -4.4287329 -4.4287 -4.4287295 -4.4287882 -4.428834 -4.4288616 -4.4288793 -4.4288878 -4.4288907][-4.4287343 -4.4287271 -4.4287152 -4.428719 -4.4287224 -4.4286923 -4.4286118 -4.4285455 -4.4285913 -4.4286871 -4.4287577 -4.4287977 -4.4288259 -4.4288468 -4.4288588][-4.428679 -4.42867 -4.4286613 -4.4286613 -4.4286537 -4.4286032 -4.4284787 -4.4283676 -4.4284282 -4.4285693 -4.4286723 -4.4287295 -4.4287724 -4.428812 -4.4288392][-4.4286795 -4.4286761 -4.4286718 -4.4286666 -4.4286413 -4.428575 -4.4284277 -4.4282918 -4.4283547 -4.4285192 -4.4286404 -4.428709 -4.4287615 -4.4288106 -4.4288449][-4.4287348 -4.4287376 -4.4287424 -4.4287395 -4.4287105 -4.4286509 -4.4285378 -4.4284358 -4.4284744 -4.4286027 -4.4287028 -4.4287629 -4.4288049 -4.4288411 -4.4288659][-4.4288096 -4.4288187 -4.4288311 -4.4288363 -4.4288154 -4.4287748 -4.428709 -4.4286475 -4.4286585 -4.4287291 -4.4287915 -4.4288321 -4.4288554 -4.4288716 -4.4288836][-4.4288678 -4.4288816 -4.4288993 -4.4289079 -4.4288921 -4.4288678 -4.4288363 -4.4287992 -4.4287877 -4.4288135 -4.4288473 -4.4288745 -4.4288855 -4.4288874 -4.4288907][-4.4289169 -4.4289308 -4.4289436 -4.4289503 -4.4289322 -4.4289103 -4.4288912 -4.4288607 -4.4288311 -4.4288316 -4.4288549 -4.4288812 -4.42889 -4.4288869 -4.4288893][-4.4289541 -4.4289613 -4.428966 -4.4289641 -4.42894 -4.4289079 -4.4288778 -4.4288363 -4.4287915 -4.4287872 -4.42882 -4.4288592 -4.4288764 -4.4288793 -4.4288883][-4.4289579 -4.4289613 -4.4289589 -4.4289465 -4.4289155 -4.4288745 -4.4288359 -4.4287829 -4.4287271 -4.4287262 -4.4287691 -4.4288216 -4.428853 -4.4288673 -4.428885][-4.4289408 -4.42894 -4.4289351 -4.4289184 -4.428885 -4.42884 -4.4287987 -4.4287438 -4.4286885 -4.4286914 -4.4287357 -4.428791 -4.4288306 -4.4288549 -4.4288807]]...]
INFO - root - 2017-12-10 04:57:01.403987: step 1210, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:58m:09s remains)
INFO - root - 2017-12-10 04:57:04.097827: step 1220, loss = 2.28, batch loss = 2.23 (28.2 examples/sec; 0.284 sec/batch; 26h:08m:06s remains)
INFO - root - 2017-12-10 04:57:06.749891: step 1230, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:30m:28s remains)
INFO - root - 2017-12-10 04:57:09.442801: step 1240, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.284 sec/batch; 26h:09m:16s remains)
INFO - root - 2017-12-10 04:57:12.155109: step 1250, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:49m:18s remains)
INFO - root - 2017-12-10 04:57:14.819032: step 1260, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:21m:28s remains)
INFO - root - 2017-12-10 04:57:17.488588: step 1270, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:29m:33s remains)
INFO - root - 2017-12-10 04:57:20.166109: step 1280, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:27m:18s remains)
INFO - root - 2017-12-10 04:57:22.806290: step 1290, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:26m:14s remains)
INFO - root - 2017-12-10 04:57:25.469351: step 1300, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:01m:35s remains)
2017-12-10 04:57:25.853835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287987 -4.4288116 -4.4288235 -4.4288254 -4.4288135 -4.428781 -4.428679 -4.4285502 -4.4285564 -4.4286346 -4.4287086 -4.4287868 -4.4288545 -4.42889 -4.4289045][-4.4287448 -4.4287562 -4.4287682 -4.4287744 -4.4287777 -4.4287553 -4.4286556 -4.4285264 -4.4285336 -4.42861 -4.4286823 -4.4287643 -4.4288397 -4.4288821 -4.4288979][-4.4286904 -4.4286981 -4.4287081 -4.4287233 -4.4287481 -4.4287381 -4.4286427 -4.4285173 -4.42853 -4.4286079 -4.4286766 -4.4287572 -4.428834 -4.4288783 -4.4288921][-4.4286427 -4.4286447 -4.4286509 -4.4286757 -4.4287224 -4.4287238 -4.4286318 -4.4285107 -4.4285359 -4.4286222 -4.4286914 -4.4287672 -4.4288397 -4.4288812 -4.4288898][-4.4286046 -4.4286008 -4.4286032 -4.4286332 -4.4286952 -4.4287014 -4.4286056 -4.428484 -4.4285254 -4.42863 -4.4287047 -4.4287806 -4.4288511 -4.4288888 -4.4288917][-4.4285865 -4.4285727 -4.4285641 -4.4285841 -4.4286456 -4.428648 -4.42854 -4.4284148 -4.4284816 -4.4286151 -4.4287043 -4.4287887 -4.428864 -4.4289007 -4.4288988][-4.4286132 -4.4285827 -4.4285526 -4.4285455 -4.4285874 -4.4285746 -4.4284453 -4.4283128 -4.4284096 -4.4285755 -4.4286847 -4.4287844 -4.4288712 -4.4289117 -4.4289074][-4.4286823 -4.4286432 -4.428597 -4.4285636 -4.4285784 -4.4285393 -4.4283857 -4.4282441 -4.428359 -4.4285417 -4.428659 -4.428772 -4.4288707 -4.4289165 -4.4289136][-4.4287629 -4.4287214 -4.428669 -4.4286275 -4.4286246 -4.42857 -4.4284143 -4.4282813 -4.4283905 -4.4285569 -4.4286594 -4.4287686 -4.4288683 -4.4289165 -4.4289155][-4.4288287 -4.4287858 -4.4287372 -4.4287024 -4.4286938 -4.4286356 -4.4284987 -4.4283881 -4.428483 -4.4286194 -4.4286995 -4.4287906 -4.4288769 -4.4289184 -4.4289165][-4.428865 -4.4288273 -4.4287887 -4.4287648 -4.4287539 -4.4287009 -4.4285893 -4.4285007 -4.4285822 -4.4286942 -4.42876 -4.428833 -4.4289002 -4.4289284 -4.4289203][-4.4288826 -4.4288526 -4.428823 -4.4288096 -4.4288006 -4.4287567 -4.4286666 -4.428597 -4.4286661 -4.4287615 -4.4288187 -4.4288778 -4.4289279 -4.4289432 -4.4289279][-4.42889 -4.4288669 -4.4288449 -4.4288378 -4.4288292 -4.4287939 -4.4287248 -4.4286752 -4.4287386 -4.4288206 -4.42887 -4.4289184 -4.4289522 -4.4289579 -4.4289355][-4.4288921 -4.4288769 -4.428863 -4.4288588 -4.4288478 -4.42882 -4.4287729 -4.4287424 -4.4288006 -4.4288721 -4.4289165 -4.4289551 -4.4289751 -4.4289722 -4.4289441][-4.4288945 -4.4288845 -4.4288788 -4.4288764 -4.4288673 -4.4288497 -4.4288206 -4.4288015 -4.4288497 -4.4289093 -4.4289484 -4.4289789 -4.4289908 -4.4289823 -4.4289513]]...]
INFO - root - 2017-12-10 04:57:28.502338: step 1310, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.253 sec/batch; 23h:14m:16s remains)
INFO - root - 2017-12-10 04:57:31.209277: step 1320, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:50m:41s remains)
INFO - root - 2017-12-10 04:57:33.847170: step 1330, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:19m:13s remains)
INFO - root - 2017-12-10 04:57:36.538246: step 1340, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 25h:03m:17s remains)
INFO - root - 2017-12-10 04:57:39.231677: step 1350, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:28m:54s remains)
INFO - root - 2017-12-10 04:57:41.902316: step 1360, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:34m:25s remains)
INFO - root - 2017-12-10 04:57:44.543564: step 1370, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:22m:43s remains)
INFO - root - 2017-12-10 04:57:47.170631: step 1380, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:40m:23s remains)
INFO - root - 2017-12-10 04:57:49.808876: step 1390, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:03m:57s remains)
INFO - root - 2017-12-10 04:57:52.498472: step 1400, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:50m:26s remains)
2017-12-10 04:57:52.853357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288926 -4.4288826 -4.4288797 -4.428875 -4.4288621 -4.42885 -4.42885 -4.4288616 -4.4288859 -4.4289088 -4.4289212 -4.4289331 -4.428947 -4.4289651 -4.4289842][-4.4288473 -4.428833 -4.4288292 -4.4288282 -4.4288177 -4.4288049 -4.4288049 -4.4288173 -4.4288445 -4.4288683 -4.428884 -4.4289069 -4.4289279 -4.4289513 -4.4289746][-4.4287987 -4.428771 -4.4287543 -4.4287567 -4.4287543 -4.4287467 -4.4287496 -4.428762 -4.428793 -4.42882 -4.4288392 -4.4288778 -4.4289117 -4.4289417 -4.4289684][-4.4287348 -4.4286914 -4.4286671 -4.4286737 -4.4286842 -4.4286852 -4.4286942 -4.4287 -4.4287357 -4.4287748 -4.4288015 -4.4288511 -4.4288955 -4.4289351 -4.428967][-4.4286747 -4.428616 -4.4285846 -4.4286046 -4.4286313 -4.4286366 -4.4286447 -4.42864 -4.4286828 -4.4287405 -4.4287744 -4.4288278 -4.4288812 -4.4289289 -4.4289665][-4.4286389 -4.4285712 -4.42854 -4.4285769 -4.4286237 -4.4286323 -4.4286222 -4.4285932 -4.4286404 -4.4287181 -4.4287577 -4.4288082 -4.4288669 -4.4289207 -4.4289641][-4.4286389 -4.4285736 -4.4285502 -4.4286013 -4.4286609 -4.4286633 -4.4286222 -4.4285636 -4.4286065 -4.4286923 -4.4287348 -4.4287891 -4.4288535 -4.4289126 -4.4289613][-4.4286642 -4.428607 -4.4285908 -4.4286423 -4.4286966 -4.4286852 -4.4286227 -4.4285479 -4.4285932 -4.4286776 -4.4287252 -4.4287839 -4.4288521 -4.4289126 -4.4289618][-4.4287224 -4.4286804 -4.4286695 -4.4287071 -4.4287395 -4.4287167 -4.4286494 -4.4285827 -4.4286389 -4.4287243 -4.4287682 -4.4288154 -4.4288731 -4.4289265 -4.4289689][-4.428793 -4.4287548 -4.4287395 -4.4287534 -4.428762 -4.4287343 -4.4286795 -4.4286408 -4.4287105 -4.4287939 -4.4288263 -4.4288549 -4.428895 -4.428937 -4.4289742][-4.4288335 -4.4287887 -4.4287596 -4.4287515 -4.4287424 -4.4287124 -4.4286709 -4.4286561 -4.4287348 -4.4288187 -4.4288507 -4.428874 -4.4289021 -4.42894 -4.4289756][-4.4288588 -4.4288082 -4.4287605 -4.4287248 -4.4287009 -4.4286652 -4.4286289 -4.4286313 -4.4287152 -4.428802 -4.4288397 -4.4288688 -4.4288974 -4.428936 -4.4289732][-4.4288821 -4.4288325 -4.4287763 -4.4287219 -4.4286814 -4.4286327 -4.4285812 -4.4285812 -4.42866 -4.4287519 -4.4287987 -4.4288383 -4.4288778 -4.428926 -4.428968][-4.4288993 -4.4288549 -4.4287977 -4.4287343 -4.428688 -4.4286327 -4.42856 -4.4285369 -4.4286022 -4.4287014 -4.4287577 -4.4288054 -4.4288592 -4.4289193 -4.4289646][-4.4289126 -4.4288807 -4.4288316 -4.4287729 -4.4287238 -4.4286633 -4.4285769 -4.428535 -4.4285932 -4.4286962 -4.4287577 -4.4288096 -4.4288654 -4.428925 -4.428968]]...]
INFO - root - 2017-12-10 04:57:55.490266: step 1410, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:27m:27s remains)
INFO - root - 2017-12-10 04:57:58.161971: step 1420, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:22m:57s remains)
INFO - root - 2017-12-10 04:58:00.857037: step 1430, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:33m:26s remains)
INFO - root - 2017-12-10 04:58:03.495530: step 1440, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:32m:55s remains)
INFO - root - 2017-12-10 04:58:06.169688: step 1450, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:05m:20s remains)
INFO - root - 2017-12-10 04:58:08.805262: step 1460, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:34m:07s remains)
INFO - root - 2017-12-10 04:58:11.531780: step 1470, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:37m:11s remains)
INFO - root - 2017-12-10 04:58:14.207685: step 1480, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:58m:06s remains)
INFO - root - 2017-12-10 04:58:16.885100: step 1490, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:58m:44s remains)
INFO - root - 2017-12-10 04:58:19.570520: step 1500, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:00m:07s remains)
2017-12-10 04:58:19.910319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288054 -4.42875 -4.4287119 -4.4286914 -4.4287071 -4.428741 -4.4287844 -4.4288335 -4.4288712 -4.4288654 -4.4288168 -4.428781 -4.4287453 -4.4287481 -4.428813][-4.4287944 -4.4287243 -4.4286661 -4.4286337 -4.4286585 -4.42871 -4.4287577 -4.428812 -4.4288611 -4.4288597 -4.4288082 -4.4287653 -4.42872 -4.4287176 -4.4287877][-4.4287744 -4.4286995 -4.4286184 -4.428566 -4.4285955 -4.4286704 -4.4287248 -4.4287863 -4.4288464 -4.428843 -4.4287939 -4.4287524 -4.4287071 -4.4287062 -4.4287791][-4.4287629 -4.4286809 -4.4285746 -4.4285021 -4.4285359 -4.4286323 -4.4286895 -4.4287534 -4.4288125 -4.4287982 -4.4287477 -4.4287119 -4.4286761 -4.4286838 -4.428762][-4.428741 -4.42865 -4.4285307 -4.4284434 -4.428472 -4.4285812 -4.4286437 -4.4287105 -4.4287672 -4.4287438 -4.42869 -4.4286618 -4.4286432 -4.4286642 -4.4287519][-4.428719 -4.4286203 -4.4284892 -4.4283781 -4.4283895 -4.4285121 -4.4285913 -4.4286675 -4.4287281 -4.4287014 -4.428647 -4.4286242 -4.4286256 -4.4286609 -4.4287534][-4.4287004 -4.4285889 -4.4284391 -4.4282928 -4.4282904 -4.4284334 -4.428545 -4.4286408 -4.4287148 -4.4287004 -4.4286418 -4.4286165 -4.4286265 -4.4286718 -4.4287667][-4.4286876 -4.4285789 -4.428432 -4.4282756 -4.4282732 -4.4284263 -4.4285545 -4.4286566 -4.4287343 -4.4287257 -4.4286618 -4.4286284 -4.4286361 -4.42868 -4.428771][-4.4287057 -4.428618 -4.4284949 -4.4283552 -4.42835 -4.4284854 -4.4285994 -4.4286761 -4.4287333 -4.42872 -4.4286571 -4.4286251 -4.4286351 -4.42868 -4.4287715][-4.4287486 -4.4286809 -4.4285889 -4.4284792 -4.4284697 -4.4285655 -4.4286456 -4.4286981 -4.4287357 -4.4287133 -4.4286652 -4.4286485 -4.4286633 -4.4287076 -4.4287992][-4.4287977 -4.428741 -4.4286757 -4.4286065 -4.428596 -4.4286523 -4.4287076 -4.4287496 -4.4287744 -4.4287505 -4.42872 -4.4287176 -4.4287372 -4.4287796 -4.4288573][-4.4288197 -4.4287748 -4.4287395 -4.4287124 -4.4287143 -4.4287453 -4.4287763 -4.4288077 -4.428823 -4.4288 -4.4287786 -4.4287791 -4.4288025 -4.428844 -4.4289055][-4.4288297 -4.4287949 -4.4287763 -4.4287715 -4.4287848 -4.4288068 -4.4288254 -4.4288478 -4.4288597 -4.4288421 -4.4288235 -4.4288278 -4.42885 -4.4288878 -4.4289355][-4.4288659 -4.4288397 -4.4288244 -4.4288225 -4.4288325 -4.4288468 -4.4288592 -4.4288778 -4.4288898 -4.42888 -4.4288697 -4.4288754 -4.4288921 -4.4289184 -4.4289551][-4.4289284 -4.4289117 -4.4288983 -4.4288931 -4.428896 -4.4289021 -4.4289088 -4.4289217 -4.4289331 -4.4289331 -4.4289293 -4.4289322 -4.4289389 -4.4289541 -4.4289794]]...]
INFO - root - 2017-12-10 04:58:22.539968: step 1510, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:31m:47s remains)
INFO - root - 2017-12-10 04:58:25.229118: step 1520, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:54m:05s remains)
INFO - root - 2017-12-10 04:58:27.924302: step 1530, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:07m:16s remains)
INFO - root - 2017-12-10 04:58:30.572431: step 1540, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:20m:34s remains)
INFO - root - 2017-12-10 04:58:33.204819: step 1550, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:23m:52s remains)
INFO - root - 2017-12-10 04:58:35.870324: step 1560, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:45m:18s remains)
INFO - root - 2017-12-10 04:58:38.560753: step 1570, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 25h:32m:38s remains)
INFO - root - 2017-12-10 04:58:41.232524: step 1580, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:57m:17s remains)
INFO - root - 2017-12-10 04:58:43.880326: step 1590, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:15m:44s remains)
INFO - root - 2017-12-10 04:58:46.572723: step 1600, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 25h:00m:32s remains)
2017-12-10 04:58:46.942311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42901 -4.42902 -4.4290247 -4.4290223 -4.4290109 -4.4289885 -4.428966 -4.4289608 -4.428967 -4.4289656 -4.4289627 -4.4289603 -4.4289556 -4.428947 -4.4289455][-4.4289889 -4.4290061 -4.4290209 -4.4290242 -4.4290137 -4.4289865 -4.4289546 -4.4289441 -4.4289541 -4.4289551 -4.4289546 -4.4289517 -4.4289403 -4.4289255 -4.4289179][-4.4289536 -4.4289732 -4.428997 -4.4290075 -4.4290023 -4.428978 -4.4289427 -4.4289312 -4.4289436 -4.4289451 -4.4289489 -4.4289479 -4.4289317 -4.4289107 -4.4288969][-4.4289203 -4.4289322 -4.4289541 -4.4289646 -4.4289613 -4.4289412 -4.4289117 -4.428905 -4.4289217 -4.4289289 -4.4289422 -4.42895 -4.4289365 -4.4289155 -4.4289][-4.4288988 -4.4288964 -4.4289031 -4.4288993 -4.42888 -4.4288507 -4.428823 -4.4288278 -4.4288607 -4.4288845 -4.4289145 -4.4289303 -4.4289207 -4.4289055 -4.4288926][-4.4288831 -4.4288621 -4.4288478 -4.428822 -4.428781 -4.42874 -4.4287047 -4.428719 -4.428781 -4.4288368 -4.4288869 -4.4289079 -4.4288988 -4.4288878 -4.4288793][-4.4288816 -4.428844 -4.4288068 -4.4287472 -4.428668 -4.4285903 -4.4285173 -4.4285216 -4.4286213 -4.4287353 -4.4288254 -4.4288678 -4.4288692 -4.4288664 -4.4288607][-4.4288993 -4.4288554 -4.4288034 -4.4287133 -4.4285917 -4.4284596 -4.4283147 -4.4282694 -4.4283948 -4.4285693 -4.4287109 -4.428792 -4.4288192 -4.428834 -4.4288416][-4.4289207 -4.4288921 -4.4288578 -4.4287853 -4.4286742 -4.4285316 -4.4283547 -4.42827 -4.4283671 -4.4285216 -4.4286571 -4.4287515 -4.4287896 -4.4288087 -4.4288249][-4.4289312 -4.428925 -4.4289217 -4.4288878 -4.4288135 -4.428699 -4.4285545 -4.4284806 -4.4285312 -4.4286141 -4.4287024 -4.4287729 -4.4287953 -4.428802 -4.4288144][-4.4289064 -4.4289083 -4.4289212 -4.4289093 -4.4288521 -4.428762 -4.4286585 -4.4286122 -4.4286427 -4.4286828 -4.4287329 -4.4287734 -4.42878 -4.4287815 -4.428793][-4.4288692 -4.4288645 -4.42888 -4.4288764 -4.4288311 -4.4287715 -4.4287148 -4.4286971 -4.4287243 -4.4287453 -4.4287729 -4.4287872 -4.4287853 -4.4287887 -4.4287992][-4.4288392 -4.4288268 -4.4288363 -4.4288363 -4.4288063 -4.4287767 -4.4287524 -4.4287553 -4.4287853 -4.4288034 -4.4288182 -4.4288149 -4.4288111 -4.4288182 -4.4288254][-4.428822 -4.4287996 -4.4287996 -4.4287944 -4.4287748 -4.4287663 -4.4287577 -4.42877 -4.4288058 -4.4288306 -4.4288435 -4.4288363 -4.4288306 -4.4288349 -4.4288373][-4.4288273 -4.4287963 -4.4287877 -4.4287782 -4.4287643 -4.4287629 -4.4287605 -4.4287763 -4.4288125 -4.4288368 -4.4288492 -4.4288483 -4.4288473 -4.4288507 -4.42885]]...]
INFO - root - 2017-12-10 04:58:49.598422: step 1610, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 24h:13m:28s remains)
INFO - root - 2017-12-10 04:58:52.225566: step 1620, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 23h:06m:40s remains)
INFO - root - 2017-12-10 04:58:54.907303: step 1630, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:52m:13s remains)
INFO - root - 2017-12-10 04:58:57.597008: step 1640, loss = 2.28, batch loss = 2.23 (28.5 examples/sec; 0.281 sec/batch; 25h:50m:16s remains)
INFO - root - 2017-12-10 04:59:00.295447: step 1650, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:38m:57s remains)
INFO - root - 2017-12-10 04:59:02.939955: step 1660, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:22m:03s remains)
INFO - root - 2017-12-10 04:59:05.591011: step 1670, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:07m:37s remains)
INFO - root - 2017-12-10 04:59:08.226450: step 1680, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:42m:21s remains)
INFO - root - 2017-12-10 04:59:10.861762: step 1690, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:50m:03s remains)
INFO - root - 2017-12-10 04:59:13.535676: step 1700, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:30m:44s remains)
2017-12-10 04:59:13.882667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286914 -4.4285879 -4.4285131 -4.4285522 -4.4286675 -4.4287815 -4.4288664 -4.428906 -4.4289093 -4.428894 -4.4288797 -4.4288845 -4.4288898 -4.4288669 -4.4288335][-4.4286308 -4.4284806 -4.4283824 -4.4284449 -4.4285984 -4.4287443 -4.428843 -4.4288883 -4.4288917 -4.4288764 -4.4288654 -4.4288673 -4.4288716 -4.4288449 -4.428802][-4.4285688 -4.4283834 -4.4282727 -4.4283557 -4.4285374 -4.42871 -4.4288206 -4.4288607 -4.4288549 -4.4288464 -4.42885 -4.4288549 -4.4288626 -4.4288349 -4.4287839][-4.4285722 -4.4283943 -4.4283047 -4.4283915 -4.4285612 -4.4287195 -4.4287939 -4.4287944 -4.4287663 -4.4287739 -4.4288063 -4.4288392 -4.4288611 -4.4288368 -4.4287815][-4.4286346 -4.4285154 -4.4284821 -4.4285622 -4.4286819 -4.42878 -4.4287791 -4.4287014 -4.4286361 -4.4286633 -4.4287305 -4.4288049 -4.4288497 -4.4288321 -4.4287767][-4.4287 -4.4286528 -4.4286728 -4.4287343 -4.4287858 -4.4287963 -4.4286971 -4.4285192 -4.4284196 -4.42849 -4.4286208 -4.4287443 -4.4288163 -4.4288039 -4.4287567][-4.4287553 -4.4287548 -4.428792 -4.4288173 -4.4288006 -4.4287176 -4.4285059 -4.4282184 -4.4281015 -4.428257 -4.4284768 -4.428658 -4.4287596 -4.4287639 -4.4287343][-4.4288125 -4.4288282 -4.4288487 -4.4288325 -4.4287682 -4.4286246 -4.4283376 -4.4279881 -4.4278841 -4.4281158 -4.4283938 -4.4286041 -4.428721 -4.4287381 -4.4287233][-4.428863 -4.4288783 -4.4288793 -4.4288363 -4.4287624 -4.4286213 -4.4283719 -4.428103 -4.4280553 -4.4282479 -4.4284678 -4.4286337 -4.4287262 -4.4287333 -4.4287171][-4.4288907 -4.4289017 -4.4288945 -4.4288478 -4.4287887 -4.4286861 -4.4285264 -4.4283843 -4.4283872 -4.4285035 -4.4286346 -4.428731 -4.4287663 -4.4287443 -4.4287233][-4.4288983 -4.4289103 -4.4289041 -4.4288673 -4.4288297 -4.4287691 -4.4286804 -4.4286189 -4.4286408 -4.4287086 -4.4287834 -4.4288278 -4.4288239 -4.4287825 -4.4287548][-4.4289074 -4.4289217 -4.4289179 -4.4288921 -4.4288769 -4.4288516 -4.4288082 -4.4287834 -4.4288025 -4.4288383 -4.4288774 -4.4288931 -4.4288764 -4.4288344 -4.4288058][-4.4289269 -4.4289341 -4.4289322 -4.42892 -4.4289203 -4.42892 -4.4289074 -4.4289002 -4.4289103 -4.4289217 -4.4289341 -4.4289317 -4.4289093 -4.4288726 -4.4288468][-4.42895 -4.4289446 -4.4289446 -4.428947 -4.4289594 -4.4289703 -4.42897 -4.4289656 -4.4289646 -4.4289618 -4.4289579 -4.4289441 -4.4289231 -4.4288945 -4.4288745][-4.4289641 -4.42895 -4.428947 -4.4289546 -4.4289713 -4.4289842 -4.4289856 -4.4289808 -4.4289713 -4.42896 -4.4289494 -4.4289346 -4.4289212 -4.428906 -4.4288964]]...]
INFO - root - 2017-12-10 04:59:16.583832: step 1710, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:43m:41s remains)
INFO - root - 2017-12-10 04:59:19.244549: step 1720, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:41m:41s remains)
INFO - root - 2017-12-10 04:59:21.889415: step 1730, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:50m:48s remains)
INFO - root - 2017-12-10 04:59:24.523667: step 1740, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:20m:08s remains)
INFO - root - 2017-12-10 04:59:27.180632: step 1750, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 24h:01m:34s remains)
INFO - root - 2017-12-10 04:59:29.871910: step 1760, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:05m:13s remains)
INFO - root - 2017-12-10 04:59:32.529109: step 1770, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:52m:00s remains)
INFO - root - 2017-12-10 04:59:35.141943: step 1780, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:05m:52s remains)
INFO - root - 2017-12-10 04:59:37.794301: step 1790, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:35m:37s remains)
INFO - root - 2017-12-10 04:59:40.479333: step 1800, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:44m:13s remains)
2017-12-10 04:59:40.846890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289303 -4.4288821 -4.4287806 -4.4286375 -4.4285393 -4.4284973 -4.4285345 -4.4286175 -4.4286833 -4.4286795 -4.4286323 -4.4286132 -4.4286528 -4.4287224 -4.4287834][-4.4289365 -4.42889 -4.4287915 -4.4286418 -4.4285283 -4.4284706 -4.4285054 -4.428596 -4.4286757 -4.4286752 -4.4286251 -4.4286079 -4.4286618 -4.4287419 -4.4288077][-4.4289355 -4.4288878 -4.4287906 -4.428638 -4.4285073 -4.428441 -4.4284792 -4.4285789 -4.4286695 -4.4286642 -4.4286094 -4.4286079 -4.4286771 -4.4287658 -4.4288282][-4.4289322 -4.4288836 -4.4287825 -4.4286222 -4.42848 -4.4284077 -4.4284468 -4.428556 -4.4286451 -4.4286289 -4.428587 -4.4286265 -4.4287152 -4.4287972 -4.4288425][-4.4289331 -4.428885 -4.428782 -4.4286141 -4.4284639 -4.4283819 -4.4284163 -4.4285269 -4.4286027 -4.4285722 -4.42857 -4.4286585 -4.428761 -4.4288311 -4.4288573][-4.428937 -4.4288907 -4.4287872 -4.428616 -4.4284563 -4.4283619 -4.4283886 -4.4284897 -4.4285355 -4.4284921 -4.4285512 -4.4286785 -4.4287848 -4.4288464 -4.4288692][-4.42894 -4.4288936 -4.4287953 -4.42863 -4.4284668 -4.4283633 -4.4283752 -4.4284549 -4.4284616 -4.4284282 -4.4285464 -4.4286985 -4.4288011 -4.4288554 -4.4288721][-4.4289408 -4.428896 -4.4288068 -4.428658 -4.4284983 -4.4283919 -4.4283934 -4.4284511 -4.4284463 -4.4284563 -4.4286008 -4.428751 -4.428843 -4.4288826 -4.4288812][-4.4289379 -4.4288936 -4.4288154 -4.4286866 -4.428544 -4.4284472 -4.4284444 -4.4284844 -4.4284873 -4.4285364 -4.42868 -4.4288163 -4.428895 -4.4289136 -4.428895][-4.4289336 -4.42889 -4.4288216 -4.4287133 -4.4285955 -4.4285107 -4.4284959 -4.428515 -4.42853 -4.42861 -4.4287534 -4.4288735 -4.4289422 -4.4289451 -4.428906][-4.4289293 -4.4288864 -4.4288287 -4.428741 -4.4286418 -4.4285564 -4.4285188 -4.4285226 -4.4285645 -4.4286633 -4.428803 -4.4289136 -4.4289742 -4.4289641 -4.4289112][-4.4289284 -4.4288869 -4.4288359 -4.4287581 -4.4286628 -4.428565 -4.4285083 -4.428525 -4.4286046 -4.4287205 -4.4288497 -4.4289455 -4.4289861 -4.4289584 -4.4288955][-4.4289331 -4.4288945 -4.4288435 -4.42876 -4.428648 -4.4285274 -4.4284744 -4.42853 -4.4286475 -4.4287772 -4.4288917 -4.428968 -4.4289818 -4.4289379 -4.4288716][-4.4289427 -4.4289069 -4.4288468 -4.4287391 -4.4285932 -4.4284549 -4.428431 -4.428535 -4.428688 -4.4288244 -4.4289246 -4.4289775 -4.4289675 -4.4289122 -4.4288464][-4.4289556 -4.42892 -4.4288468 -4.4287076 -4.428525 -4.4283853 -4.4284048 -4.4285583 -4.4287372 -4.4288673 -4.4289427 -4.4289713 -4.4289422 -4.42888 -4.4288216]]...]
INFO - root - 2017-12-10 04:59:43.509427: step 1810, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:47m:21s remains)
INFO - root - 2017-12-10 04:59:46.163926: step 1820, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:12m:00s remains)
INFO - root - 2017-12-10 04:59:48.851522: step 1830, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:38m:02s remains)
INFO - root - 2017-12-10 04:59:51.494561: step 1840, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:16m:55s remains)
INFO - root - 2017-12-10 04:59:54.161500: step 1850, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:38m:45s remains)
INFO - root - 2017-12-10 04:59:56.832573: step 1860, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.282 sec/batch; 25h:53m:34s remains)
INFO - root - 2017-12-10 04:59:59.507095: step 1870, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:56m:44s remains)
INFO - root - 2017-12-10 05:00:02.183347: step 1880, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:27m:15s remains)
INFO - root - 2017-12-10 05:00:04.861520: step 1890, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:56m:05s remains)
INFO - root - 2017-12-10 05:00:07.568136: step 1900, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 24h:00m:46s remains)
2017-12-10 05:00:07.914385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428772 -4.4287381 -4.4287314 -4.4287424 -4.4287643 -4.4287939 -4.4288235 -4.4288464 -4.4288607 -4.4288626 -4.4288564 -4.4288492 -4.4288425 -4.428833 -4.4288135][-4.4287834 -4.4287519 -4.4287443 -4.4287558 -4.4287791 -4.4288092 -4.4288392 -4.428863 -4.42888 -4.42888 -4.42887 -4.428865 -4.4288645 -4.4288616 -4.4288483][-4.4288249 -4.4288011 -4.428791 -4.4287963 -4.4288192 -4.4288521 -4.4288836 -4.4289079 -4.4289241 -4.4289265 -4.42892 -4.4289188 -4.4289227 -4.4289222 -4.4289141][-4.4288793 -4.428853 -4.42883 -4.4288216 -4.4288344 -4.4288673 -4.428896 -4.4289174 -4.4289336 -4.4289417 -4.4289422 -4.4289494 -4.4289532 -4.4289522 -4.4289465][-4.428905 -4.4288778 -4.4288492 -4.4288249 -4.42882 -4.4288311 -4.4288373 -4.4288435 -4.4288583 -4.4288759 -4.4288921 -4.4289093 -4.42892 -4.428925 -4.4289336][-4.4289069 -4.4288788 -4.428843 -4.4287944 -4.4287486 -4.4287095 -4.4286628 -4.4286284 -4.4286351 -4.4286656 -4.4287095 -4.4287534 -4.4287853 -4.4288096 -4.4288411][-4.42885 -4.4288125 -4.4287624 -4.4286838 -4.4285913 -4.4284978 -4.4283891 -4.4283013 -4.4282975 -4.4283581 -4.4284449 -4.4285283 -4.4285932 -4.4286494 -4.4287081][-4.428771 -4.4287281 -4.4286714 -4.4285789 -4.4284616 -4.4283423 -4.4282064 -4.4280977 -4.4281073 -4.4282112 -4.4283409 -4.4284587 -4.4285426 -4.4286027 -4.4286594][-4.4287605 -4.4287257 -4.4286852 -4.428616 -4.4285364 -4.4284663 -4.4283915 -4.4283304 -4.4283476 -4.4284315 -4.4285288 -4.4286141 -4.4286671 -4.4286947 -4.42872][-4.4287877 -4.4287629 -4.4287424 -4.4287105 -4.4286814 -4.4286675 -4.4286461 -4.4286184 -4.4286265 -4.4286752 -4.4287257 -4.4287658 -4.4287844 -4.4287848 -4.4287877][-4.4288216 -4.4288139 -4.4288096 -4.4288054 -4.4288125 -4.4288225 -4.4288139 -4.428793 -4.4287868 -4.4288073 -4.4288263 -4.42884 -4.4288445 -4.4288387 -4.4288292][-4.4288688 -4.4288678 -4.4288688 -4.4288745 -4.4288917 -4.4289041 -4.4288955 -4.4288769 -4.4288659 -4.428875 -4.4288797 -4.428875 -4.4288716 -4.4288664 -4.4288578][-4.428895 -4.4288931 -4.4288969 -4.4289045 -4.4289155 -4.4289184 -4.4289069 -4.4288921 -4.4288816 -4.4288845 -4.4288826 -4.42887 -4.428865 -4.4288635 -4.4288554][-4.4288783 -4.4288707 -4.4288707 -4.4288745 -4.4288745 -4.4288654 -4.4288588 -4.4288554 -4.4288516 -4.4288516 -4.4288507 -4.4288406 -4.4288416 -4.4288483 -4.4288406][-4.4288363 -4.428822 -4.4288125 -4.4288068 -4.4287968 -4.4287834 -4.4287872 -4.4288044 -4.4288149 -4.4288173 -4.42882 -4.4288206 -4.4288359 -4.4288535 -4.4288492]]...]
INFO - root - 2017-12-10 05:00:10.532552: step 1910, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:29m:49s remains)
INFO - root - 2017-12-10 05:00:13.237260: step 1920, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 25h:11m:40s remains)
INFO - root - 2017-12-10 05:00:15.892731: step 1930, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:57m:05s remains)
INFO - root - 2017-12-10 05:00:18.551548: step 1940, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:56m:18s remains)
INFO - root - 2017-12-10 05:00:21.200709: step 1950, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:45m:04s remains)
INFO - root - 2017-12-10 05:00:23.865345: step 1960, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 24h:06m:20s remains)
INFO - root - 2017-12-10 05:00:26.503567: step 1970, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:59m:29s remains)
INFO - root - 2017-12-10 05:00:29.166233: step 1980, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:48m:52s remains)
INFO - root - 2017-12-10 05:00:31.826127: step 1990, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:31m:38s remains)
INFO - root - 2017-12-10 05:00:34.515372: step 2000, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:14m:33s remains)
2017-12-10 05:00:34.846827: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290128 -4.4290032 -4.4289951 -4.42899 -4.428987 -4.4289894 -4.4289937 -4.4289932 -4.4289875 -4.4289832 -4.4289804 -4.4289851 -4.4289956 -4.4290066 -4.4290109][-4.4289985 -4.428977 -4.4289589 -4.4289441 -4.4289346 -4.4289365 -4.4289422 -4.42894 -4.4289312 -4.4289279 -4.4289231 -4.4289293 -4.428947 -4.4289737 -4.4289918][-4.4289727 -4.4289355 -4.4289055 -4.4288793 -4.4288611 -4.4288616 -4.4288669 -4.42886 -4.4288478 -4.4288359 -4.4288168 -4.4288144 -4.4288378 -4.4288888 -4.4289365][-4.4289336 -4.4288826 -4.428844 -4.4288077 -4.4287853 -4.4287868 -4.4287887 -4.4287705 -4.428741 -4.4287119 -4.4286718 -4.428659 -4.4286923 -4.4287715 -4.428854][-4.4288988 -4.4288383 -4.4287891 -4.4287429 -4.4287138 -4.4287114 -4.4287133 -4.4286809 -4.4286289 -4.4285731 -4.428514 -4.4285007 -4.4285541 -4.4286637 -4.4287739][-4.4288645 -4.4287963 -4.4287305 -4.4286675 -4.4286313 -4.428627 -4.428627 -4.4285812 -4.4285216 -4.4284649 -4.4284186 -4.4284258 -4.4285016 -4.4286246 -4.428741][-4.4288278 -4.4287477 -4.4286494 -4.4285493 -4.4285026 -4.428504 -4.4285088 -4.4284544 -4.4284191 -4.428411 -4.42841 -4.4284458 -4.4285274 -4.4286461 -4.4287548][-4.4287767 -4.4286723 -4.4285254 -4.4283757 -4.4283237 -4.4283533 -4.4283853 -4.4283414 -4.4283481 -4.4283991 -4.4284439 -4.4284987 -4.42858 -4.4286914 -4.4287906][-4.4287124 -4.4285822 -4.4283929 -4.42821 -4.4281807 -4.428266 -4.4283462 -4.4283385 -4.4283805 -4.4284577 -4.4285212 -4.4285793 -4.4286518 -4.4287448 -4.4288278][-4.4286923 -4.4285622 -4.4283938 -4.4282389 -4.4282408 -4.4283495 -4.4284496 -4.4284706 -4.4285235 -4.4285955 -4.4286509 -4.4286962 -4.42875 -4.4288149 -4.4288692][-4.4287586 -4.4286675 -4.4285712 -4.4284878 -4.4284921 -4.4285703 -4.4286509 -4.4286804 -4.4287205 -4.4287653 -4.4287977 -4.4288206 -4.42885 -4.428884 -4.4289122][-4.428864 -4.4288092 -4.4287672 -4.4287386 -4.4287415 -4.4287863 -4.4288416 -4.428863 -4.428884 -4.4288988 -4.4289088 -4.428916 -4.4289265 -4.4289393 -4.4289494][-4.4289551 -4.4289293 -4.4289136 -4.428905 -4.4289026 -4.4289207 -4.4289494 -4.4289622 -4.42897 -4.4289718 -4.4289718 -4.4289718 -4.4289727 -4.4289756 -4.4289761][-4.4290056 -4.429 -4.4289966 -4.4289908 -4.4289818 -4.4289818 -4.4289908 -4.4289956 -4.4289975 -4.4289966 -4.4289961 -4.4289956 -4.4289942 -4.4289927 -4.4289894][-4.4290071 -4.4290066 -4.4290066 -4.4290018 -4.4289961 -4.4289923 -4.4289932 -4.4289951 -4.428998 -4.429 -4.4290009 -4.4290018 -4.4290009 -4.4289985 -4.4289947]]...]
INFO - root - 2017-12-10 05:00:37.507361: step 2010, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:46m:42s remains)
INFO - root - 2017-12-10 05:00:40.149263: step 2020, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:00m:12s remains)
INFO - root - 2017-12-10 05:00:42.805233: step 2030, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:29m:10s remains)
INFO - root - 2017-12-10 05:00:45.493592: step 2040, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:33m:58s remains)
INFO - root - 2017-12-10 05:00:48.166854: step 2050, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:52m:20s remains)
INFO - root - 2017-12-10 05:00:50.791265: step 2060, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:04m:24s remains)
INFO - root - 2017-12-10 05:00:53.490840: step 2070, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.271 sec/batch; 24h:50m:09s remains)
INFO - root - 2017-12-10 05:00:56.104652: step 2080, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:54m:37s remains)
INFO - root - 2017-12-10 05:00:58.794226: step 2090, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 25h:29m:20s remains)
INFO - root - 2017-12-10 05:01:01.480863: step 2100, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 25h:09m:53s remains)
2017-12-10 05:01:01.839948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42871 -4.4287114 -4.4287338 -4.4287605 -4.4287596 -4.4287467 -4.4287539 -4.4287691 -4.4287868 -4.4287972 -4.4288249 -4.4288435 -4.428853 -4.4288621 -4.4288626][-4.428721 -4.4287353 -4.4287591 -4.4287815 -4.428782 -4.4287672 -4.4287748 -4.4288073 -4.4288435 -4.4288592 -4.4288692 -4.4288664 -4.4288592 -4.428865 -4.4288726][-4.4287448 -4.4287586 -4.4287834 -4.4287977 -4.428791 -4.428771 -4.4287744 -4.4288082 -4.4288511 -4.42887 -4.4288754 -4.4288697 -4.4288592 -4.4288568 -4.4288611][-4.4287615 -4.4287629 -4.4287829 -4.4287868 -4.4287653 -4.4287372 -4.4287391 -4.428771 -4.4288216 -4.4288511 -4.4288597 -4.428865 -4.428863 -4.4288535 -4.4288478][-4.428761 -4.428751 -4.4287605 -4.42875 -4.4287014 -4.4286494 -4.428638 -4.4286695 -4.428741 -4.4287977 -4.4288278 -4.42885 -4.428865 -4.4288583 -4.4288497][-4.4287615 -4.4287496 -4.4287486 -4.4287181 -4.4286375 -4.4285359 -4.4284811 -4.4285107 -4.4286137 -4.4287138 -4.4287858 -4.42884 -4.4288788 -4.428885 -4.4288812][-4.428793 -4.4287891 -4.428781 -4.428741 -4.4286389 -4.4284883 -4.428371 -4.4283819 -4.4285073 -4.4286418 -4.4287491 -4.4288359 -4.4288964 -4.4289131 -4.4289117][-4.4288321 -4.428834 -4.4288249 -4.4287891 -4.4286938 -4.4285359 -4.4283886 -4.42837 -4.4284844 -4.4286184 -4.4287286 -4.4288259 -4.4288964 -4.4289212 -4.4289193][-4.4288325 -4.4288354 -4.4288182 -4.428793 -4.4287243 -4.428586 -4.4284353 -4.4283934 -4.4284825 -4.4286103 -4.4287152 -4.4288096 -4.4288797 -4.4289055 -4.4288964][-4.4288168 -4.4288116 -4.4287815 -4.4287553 -4.428709 -4.4286003 -4.4284649 -4.4284153 -4.428483 -4.4286022 -4.4287057 -4.4287934 -4.428854 -4.4288797 -4.42887][-4.428802 -4.4287839 -4.4287472 -4.4287243 -4.428699 -4.4286222 -4.4285111 -4.4284663 -4.4285235 -4.4286232 -4.42871 -4.42878 -4.4288244 -4.428854 -4.4288526][-4.428803 -4.4287715 -4.4287314 -4.4287157 -4.4287143 -4.4286742 -4.4285932 -4.4285517 -4.4285955 -4.42868 -4.4287448 -4.4287834 -4.4288006 -4.4288249 -4.4288349][-4.4288244 -4.428792 -4.4287553 -4.4287419 -4.4287477 -4.4287314 -4.4286747 -4.4286389 -4.4286695 -4.4287372 -4.4287791 -4.4287872 -4.4287677 -4.4287639 -4.4287829][-4.4288464 -4.428823 -4.4287996 -4.4287877 -4.4287853 -4.4287744 -4.4287381 -4.4287143 -4.4287362 -4.4287882 -4.4288125 -4.4287944 -4.4287367 -4.428688 -4.4287014][-4.4288311 -4.4288192 -4.4288163 -4.4288182 -4.4288187 -4.4288158 -4.4287996 -4.4287891 -4.4288063 -4.4288397 -4.4288464 -4.4288111 -4.4287324 -4.4286489 -4.4286418]]...]
INFO - root - 2017-12-10 05:01:04.474713: step 2110, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:43m:58s remains)
INFO - root - 2017-12-10 05:01:07.135082: step 2120, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:55m:21s remains)
INFO - root - 2017-12-10 05:01:09.854998: step 2130, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:44m:46s remains)
INFO - root - 2017-12-10 05:01:12.494509: step 2140, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:25m:28s remains)
INFO - root - 2017-12-10 05:01:15.236984: step 2150, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:58m:32s remains)
INFO - root - 2017-12-10 05:01:17.898451: step 2160, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:19m:35s remains)
INFO - root - 2017-12-10 05:01:20.576009: step 2170, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 24h:10m:48s remains)
INFO - root - 2017-12-10 05:01:23.334710: step 2180, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:35m:27s remains)
INFO - root - 2017-12-10 05:01:25.976477: step 2190, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:03m:17s remains)
INFO - root - 2017-12-10 05:01:28.628101: step 2200, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 26h:39m:31s remains)
2017-12-10 05:01:28.958327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288969 -4.4289088 -4.4289222 -4.4288936 -4.4288197 -4.4287238 -4.428648 -4.428627 -4.4286761 -4.4287233 -4.4287653 -4.4288034 -4.4288311 -4.4288535 -4.4288497][-4.4288921 -4.4289064 -4.4289174 -4.4288807 -4.4287958 -4.4286952 -4.4286213 -4.4286165 -4.4286733 -4.4287333 -4.4287858 -4.4288297 -4.4288683 -4.4288917 -4.4288821][-4.4288955 -4.4289179 -4.4289246 -4.4288769 -4.4287796 -4.4286613 -4.4285784 -4.4285836 -4.4286547 -4.42874 -4.42881 -4.4288611 -4.4289107 -4.428936 -4.4289227][-4.4289 -4.4289269 -4.4289284 -4.4288683 -4.4287548 -4.4286113 -4.4285069 -4.4285159 -4.4286103 -4.4287305 -4.4288235 -4.4288878 -4.42894 -4.4289579 -4.4289351][-4.4289088 -4.428937 -4.428926 -4.4288511 -4.4287128 -4.4285431 -4.4284177 -4.4284348 -4.4285655 -4.4287233 -4.4288383 -4.428915 -4.4289613 -4.428966 -4.4289279][-4.4289131 -4.4289465 -4.4289274 -4.4288354 -4.4286671 -4.4284639 -4.4283094 -4.4283271 -4.4285007 -4.4287004 -4.428844 -4.4289355 -4.4289765 -4.4289689 -4.4289103][-4.4289179 -4.4289522 -4.4289312 -4.4288325 -4.428647 -4.4284163 -4.4282265 -4.4282265 -4.4284253 -4.4286566 -4.42883 -4.4289365 -4.4289746 -4.4289603 -4.4288874][-4.4289408 -4.428968 -4.4289432 -4.4288445 -4.4286642 -4.4284296 -4.4282203 -4.4281964 -4.4283915 -4.4286327 -4.428823 -4.4289365 -4.4289703 -4.4289556 -4.4288774][-4.4289775 -4.4289956 -4.4289694 -4.4288783 -4.4287176 -4.4285 -4.4282889 -4.42824 -4.4284043 -4.4286327 -4.42882 -4.428936 -4.4289665 -4.4289551 -4.4288859][-4.4290032 -4.429018 -4.4289994 -4.428926 -4.428792 -4.428607 -4.428412 -4.4283385 -4.4284558 -4.4286528 -4.4288211 -4.4289303 -4.4289594 -4.4289532 -4.4289][-4.42901 -4.4290233 -4.4290171 -4.4289632 -4.42886 -4.4287148 -4.4285536 -4.4284759 -4.428546 -4.4286976 -4.4288321 -4.4289222 -4.4289503 -4.4289532 -4.4289179][-4.4290075 -4.4290133 -4.4290128 -4.4289737 -4.4288945 -4.4287863 -4.4286685 -4.4286108 -4.4286532 -4.4287615 -4.4288597 -4.4289255 -4.4289517 -4.42896 -4.4289379][-4.4290042 -4.4289994 -4.4289951 -4.4289651 -4.4289007 -4.4288173 -4.4287376 -4.4287052 -4.42874 -4.4288192 -4.4288893 -4.4289355 -4.4289575 -4.4289694 -4.4289536][-4.4289951 -4.4289818 -4.4289703 -4.4289455 -4.428895 -4.4288344 -4.4287825 -4.4287658 -4.4287949 -4.428853 -4.4289026 -4.428936 -4.4289536 -4.428966 -4.4289565][-4.4289818 -4.4289622 -4.428947 -4.4289265 -4.4288898 -4.42885 -4.4288182 -4.4288073 -4.4288273 -4.4288678 -4.4289069 -4.4289331 -4.4289479 -4.4289589 -4.4289522]]...]
INFO - root - 2017-12-10 05:01:31.582505: step 2210, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:53m:25s remains)
INFO - root - 2017-12-10 05:01:34.213321: step 2220, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:23m:01s remains)
INFO - root - 2017-12-10 05:01:36.861345: step 2230, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:18m:59s remains)
INFO - root - 2017-12-10 05:01:39.515741: step 2240, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:36m:00s remains)
INFO - root - 2017-12-10 05:01:42.178453: step 2250, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:34m:34s remains)
INFO - root - 2017-12-10 05:01:44.829306: step 2260, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:35m:28s remains)
INFO - root - 2017-12-10 05:01:47.501861: step 2270, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:41m:19s remains)
INFO - root - 2017-12-10 05:01:50.155492: step 2280, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:15m:39s remains)
INFO - root - 2017-12-10 05:01:52.837688: step 2290, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:41m:33s remains)
INFO - root - 2017-12-10 05:01:55.544022: step 2300, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 24h:15m:28s remains)
2017-12-10 05:01:55.908276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288564 -4.4288378 -4.4288206 -4.4287863 -4.4287324 -4.4286923 -4.4286633 -4.4286427 -4.4286408 -4.4286308 -4.4285913 -4.4285269 -4.4285049 -4.42856 -4.4286523][-4.428843 -4.4288268 -4.4288077 -4.4287724 -4.4287195 -4.4286718 -4.428628 -4.4285908 -4.4285712 -4.4285445 -4.4285169 -4.4285026 -4.4285307 -4.4286065 -4.4286962][-4.4288349 -4.4288082 -4.428772 -4.42873 -4.4286809 -4.4286289 -4.4285746 -4.4285264 -4.4284854 -4.4284439 -4.4284563 -4.4285131 -4.4285893 -4.4286861 -4.4287734][-4.4288154 -4.4287696 -4.4287157 -4.4286709 -4.4286289 -4.4285827 -4.4285231 -4.4284697 -4.4284186 -4.42839 -4.4284487 -4.4285512 -4.4286489 -4.4287486 -4.4288273][-4.4287972 -4.4287424 -4.4286842 -4.4286361 -4.4285893 -4.4285369 -4.4284811 -4.42844 -4.4284139 -4.4284253 -4.4285088 -4.4286156 -4.4287076 -4.4287834 -4.42884][-4.4287715 -4.4287205 -4.4286685 -4.4286189 -4.4285607 -4.4285045 -4.4284286 -4.4283652 -4.4283843 -4.4284682 -4.4285793 -4.4286785 -4.42876 -4.4288211 -4.4288568][-4.4287443 -4.4287124 -4.4286785 -4.4286237 -4.4285378 -4.4284372 -4.4282937 -4.4281869 -4.4282842 -4.42847 -4.4286137 -4.4287219 -4.4288044 -4.4288535 -4.4288726][-4.4287114 -4.4286957 -4.4286733 -4.4286056 -4.4284821 -4.4283276 -4.42814 -4.4280467 -4.428226 -4.4284787 -4.4286447 -4.4287519 -4.4288254 -4.4288616 -4.4288735][-4.4286633 -4.4286404 -4.4286218 -4.4285631 -4.4284639 -4.4283504 -4.4282475 -4.4282365 -4.4283996 -4.4285936 -4.4287119 -4.4287829 -4.4288335 -4.4288564 -4.4288688][-4.4285669 -4.4285555 -4.4285831 -4.4285827 -4.4285488 -4.4285159 -4.4284983 -4.4285316 -4.42863 -4.4287276 -4.4287834 -4.4288163 -4.4288392 -4.4288511 -4.4288697][-4.4285197 -4.4285526 -4.428617 -4.4286504 -4.4286571 -4.428658 -4.4286637 -4.428699 -4.4287534 -4.4287963 -4.4288239 -4.4288435 -4.4288559 -4.4288654 -4.4288878][-4.4285588 -4.4286079 -4.4286771 -4.4287128 -4.4287248 -4.4287229 -4.4287214 -4.4287534 -4.4287891 -4.42881 -4.4288321 -4.4288578 -4.4288735 -4.4288869 -4.4289112][-4.4286385 -4.4286761 -4.4287205 -4.4287429 -4.4287505 -4.4287395 -4.4287333 -4.4287643 -4.4287963 -4.4288173 -4.4288383 -4.4288635 -4.4288855 -4.4289093 -4.4289346][-4.4287009 -4.4287119 -4.4287286 -4.4287329 -4.4287386 -4.4287405 -4.4287496 -4.4287796 -4.4288082 -4.4288259 -4.428843 -4.4288692 -4.4289 -4.4289289 -4.4289489][-4.4287076 -4.4287128 -4.4287152 -4.4287124 -4.42873 -4.4287605 -4.4287868 -4.4288077 -4.4288273 -4.4288435 -4.4288659 -4.4288931 -4.4289207 -4.4289432 -4.4289546]]...]
INFO - root - 2017-12-10 05:01:58.538884: step 2310, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:50m:00s remains)
INFO - root - 2017-12-10 05:02:01.180598: step 2320, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:31m:11s remains)
INFO - root - 2017-12-10 05:02:03.822897: step 2330, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:16m:37s remains)
INFO - root - 2017-12-10 05:02:06.466108: step 2340, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:31m:36s remains)
INFO - root - 2017-12-10 05:02:09.177965: step 2350, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 24h:15m:18s remains)
INFO - root - 2017-12-10 05:02:11.780556: step 2360, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:23m:59s remains)
INFO - root - 2017-12-10 05:02:14.468968: step 2370, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:53m:02s remains)
INFO - root - 2017-12-10 05:02:17.110850: step 2380, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:13m:41s remains)
INFO - root - 2017-12-10 05:02:19.816828: step 2390, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:31m:48s remains)
INFO - root - 2017-12-10 05:02:22.492546: step 2400, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:18m:23s remains)
2017-12-10 05:02:22.932147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286723 -4.4286857 -4.42872 -4.4287424 -4.4287572 -4.4287577 -4.4287605 -4.428781 -4.4287939 -4.4288096 -4.4288173 -4.4288135 -4.4288011 -4.428793 -4.4288073][-4.4286456 -4.4286594 -4.428699 -4.4287257 -4.4287548 -4.4287686 -4.4287858 -4.42881 -4.4288125 -4.4288168 -4.4288111 -4.4288068 -4.4288011 -4.4288011 -4.4288182][-4.4286938 -4.4286957 -4.4287248 -4.4287486 -4.4287834 -4.4288082 -4.4288363 -4.4288549 -4.4288435 -4.428834 -4.4288144 -4.4288082 -4.428802 -4.4287987 -4.4288073][-4.4287605 -4.4287562 -4.4287782 -4.4287939 -4.4288177 -4.4288354 -4.4288507 -4.4288492 -4.4288273 -4.4288163 -4.4288039 -4.4288039 -4.4287963 -4.4287839 -4.4287763][-4.4287977 -4.4288 -4.4288168 -4.42882 -4.4288249 -4.4288173 -4.4287987 -4.4287682 -4.4287453 -4.4287577 -4.4287791 -4.4288011 -4.4287992 -4.4287825 -4.4287543][-4.4287782 -4.4287896 -4.4288063 -4.4287977 -4.428772 -4.4287229 -4.4286575 -4.4286036 -4.4286036 -4.4286628 -4.4287381 -4.4287987 -4.4288187 -4.4288049 -4.4287605][-4.4287181 -4.4287314 -4.42875 -4.4287324 -4.4286714 -4.4285655 -4.4284487 -4.4283867 -4.4284344 -4.4285579 -4.4286952 -4.4287972 -4.4288445 -4.4288406 -4.4287925][-4.4286742 -4.4286847 -4.428699 -4.4286714 -4.4285841 -4.428441 -4.4283 -4.4282551 -4.4283543 -4.4285178 -4.42868 -4.428802 -4.4288654 -4.4288735 -4.4288325][-4.4287028 -4.4287095 -4.4287167 -4.42869 -4.4286094 -4.4284911 -4.4283934 -4.4283857 -4.4284768 -4.4286008 -4.42872 -4.4288158 -4.4288797 -4.4288988 -4.42887][-4.4287753 -4.4287715 -4.4287729 -4.4287567 -4.4287095 -4.4286485 -4.4286156 -4.4286308 -4.4286833 -4.4287362 -4.4287882 -4.4288425 -4.4288955 -4.4289188 -4.4289007][-4.4288392 -4.4288263 -4.4288197 -4.4288116 -4.4287963 -4.4287829 -4.4287934 -4.4288154 -4.4288297 -4.42883 -4.4288344 -4.4288583 -4.4288969 -4.4289165 -4.4289093][-4.4288664 -4.4288554 -4.4288487 -4.4288516 -4.4288559 -4.4288664 -4.4288917 -4.4289041 -4.4288888 -4.428863 -4.428844 -4.4288492 -4.4288688 -4.4288807 -4.4288826][-4.4288645 -4.428863 -4.4288645 -4.428874 -4.4288874 -4.4289031 -4.4289203 -4.4289136 -4.4288769 -4.4288325 -4.4287972 -4.4287829 -4.4287872 -4.4287944 -4.4288087][-4.4288321 -4.4288454 -4.4288611 -4.4288788 -4.4288912 -4.4289012 -4.4289045 -4.4288812 -4.4288311 -4.4287734 -4.428719 -4.4286838 -4.4286785 -4.4286923 -4.4287267][-4.4287667 -4.4287882 -4.4288197 -4.4288435 -4.428854 -4.4288597 -4.4288549 -4.4288239 -4.4287739 -4.4287243 -4.4286723 -4.4286327 -4.42863 -4.4286556 -4.4287114]]...]
INFO - root - 2017-12-10 05:02:25.582772: step 2410, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:32m:52s remains)
INFO - root - 2017-12-10 05:02:28.246840: step 2420, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:34m:38s remains)
INFO - root - 2017-12-10 05:02:30.895063: step 2430, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:00m:46s remains)
INFO - root - 2017-12-10 05:02:33.547018: step 2440, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:18m:44s remains)
INFO - root - 2017-12-10 05:02:36.223667: step 2450, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:49m:58s remains)
INFO - root - 2017-12-10 05:02:38.891862: step 2460, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:17m:11s remains)
INFO - root - 2017-12-10 05:02:41.550293: step 2470, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:16m:36s remains)
INFO - root - 2017-12-10 05:02:44.175014: step 2480, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:27m:00s remains)
INFO - root - 2017-12-10 05:02:46.813175: step 2490, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:09m:50s remains)
INFO - root - 2017-12-10 05:02:49.488386: step 2500, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.253 sec/batch; 23h:09m:13s remains)
2017-12-10 05:02:49.848276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289255 -4.4289141 -4.4289026 -4.4288893 -4.428885 -4.4288869 -4.4288898 -4.4288907 -4.4288926 -4.4288974 -4.4288979 -4.428896 -4.4288969 -4.428906 -4.4289174][-4.4289169 -4.4288988 -4.4288807 -4.4288635 -4.4288545 -4.4288521 -4.4288487 -4.4288487 -4.42886 -4.4288745 -4.4288783 -4.4288707 -4.4288635 -4.4288673 -4.4288788][-4.4289117 -4.4288764 -4.4288492 -4.4288263 -4.4288096 -4.4288058 -4.4287949 -4.428793 -4.4288173 -4.4288449 -4.4288545 -4.4288397 -4.4288225 -4.4288197 -4.4288273][-4.4288955 -4.42884 -4.4287934 -4.4287605 -4.4287415 -4.4287329 -4.4287086 -4.4287043 -4.4287443 -4.42879 -4.4288082 -4.4287977 -4.4287767 -4.4287643 -4.4287682][-4.4288659 -4.4288015 -4.4287357 -4.4286957 -4.4286742 -4.4286485 -4.4286056 -4.4285922 -4.428647 -4.4287186 -4.4287486 -4.4287462 -4.4287291 -4.4287162 -4.4287205][-4.4288263 -4.4287691 -4.4286981 -4.4286418 -4.4286065 -4.4285631 -4.4284968 -4.4284549 -4.4285121 -4.4286113 -4.4286647 -4.4286842 -4.4286823 -4.4286861 -4.4287004][-4.4287968 -4.4287496 -4.4286866 -4.428618 -4.4285469 -4.428473 -4.4283705 -4.4282737 -4.4283156 -4.4284587 -4.42856 -4.4286146 -4.4286423 -4.42867 -4.4286976][-4.4287786 -4.4287415 -4.4286966 -4.4286251 -4.4285235 -4.4284172 -4.4282756 -4.4281206 -4.4281178 -4.4282861 -4.4284344 -4.4285173 -4.4285684 -4.4286208 -4.428669][-4.428781 -4.4287405 -4.4287133 -4.4286504 -4.4285436 -4.428432 -4.4283042 -4.4281659 -4.4281507 -4.4282789 -4.4283948 -4.4284549 -4.4285035 -4.4285636 -4.4286304][-4.4287972 -4.4287558 -4.4287391 -4.428689 -4.4285922 -4.4284959 -4.4284053 -4.4283166 -4.4283009 -4.42836 -4.4284105 -4.4284377 -4.4284739 -4.4285197 -4.4285946][-4.4287968 -4.4287505 -4.4287276 -4.4286947 -4.4286265 -4.4285545 -4.4284964 -4.4284449 -4.4284267 -4.428422 -4.4284129 -4.4284239 -4.4284635 -4.4285069 -4.4285793][-4.4288096 -4.4287658 -4.4287362 -4.4287176 -4.428678 -4.4286313 -4.4285994 -4.4285784 -4.4285645 -4.4285355 -4.4284983 -4.4284878 -4.4285245 -4.4285655 -4.4286218][-4.4288611 -4.4288316 -4.4288054 -4.4287863 -4.4287572 -4.4287267 -4.4287138 -4.4287167 -4.4287124 -4.4286871 -4.4286513 -4.4286375 -4.4286675 -4.4287033 -4.4287362][-4.4289112 -4.4289017 -4.4288816 -4.4288635 -4.4288387 -4.4288177 -4.4288158 -4.4288311 -4.4288373 -4.4288287 -4.4288039 -4.4287953 -4.4288173 -4.4288387 -4.4288554][-4.4289308 -4.4289355 -4.4289289 -4.4289155 -4.4289 -4.4288921 -4.428894 -4.4289083 -4.4289188 -4.4289241 -4.4289107 -4.4289045 -4.4289165 -4.428926 -4.4289317]]...]
INFO - root - 2017-12-10 05:02:52.499395: step 2510, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:37m:25s remains)
INFO - root - 2017-12-10 05:02:55.137953: step 2520, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 24h:09m:13s remains)
INFO - root - 2017-12-10 05:02:57.815097: step 2530, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:34m:48s remains)
INFO - root - 2017-12-10 05:03:00.508114: step 2540, loss = 2.28, batch loss = 2.23 (28.1 examples/sec; 0.285 sec/batch; 26h:06m:15s remains)
INFO - root - 2017-12-10 05:03:03.179801: step 2550, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.282 sec/batch; 25h:51m:19s remains)
INFO - root - 2017-12-10 05:03:05.858335: step 2560, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:24m:56s remains)
INFO - root - 2017-12-10 05:03:08.525060: step 2570, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.264 sec/batch; 24h:09m:03s remains)
INFO - root - 2017-12-10 05:03:11.197274: step 2580, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 25h:03m:55s remains)
INFO - root - 2017-12-10 05:03:13.879965: step 2590, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:53m:22s remains)
INFO - root - 2017-12-10 05:03:16.562369: step 2600, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:28m:19s remains)
2017-12-10 05:03:16.952883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287777 -4.4287624 -4.42877 -4.4287777 -4.4287877 -4.4287972 -4.4287987 -4.4287782 -4.4287314 -4.4286823 -4.4286437 -4.4286137 -4.4286151 -4.4286385 -4.4286661][-4.4287314 -4.4287114 -4.4287286 -4.4287519 -4.4287724 -4.4287887 -4.4287882 -4.428761 -4.4287043 -4.4286437 -4.4285903 -4.4285531 -4.4285588 -4.42859 -4.4286304][-4.4286971 -4.4286861 -4.4287195 -4.4287596 -4.4287786 -4.4287829 -4.4287624 -4.4287229 -4.428668 -4.4286118 -4.4285617 -4.4285197 -4.4285131 -4.4285331 -4.4285727][-4.4286866 -4.4286857 -4.4287276 -4.4287715 -4.4287848 -4.4287715 -4.428731 -4.4286776 -4.4286337 -4.428596 -4.4285564 -4.4285111 -4.4284854 -4.4284892 -4.4285192][-4.4286962 -4.428699 -4.4287395 -4.4287786 -4.4287825 -4.4287553 -4.4286981 -4.428628 -4.4285851 -4.4285688 -4.4285545 -4.4285159 -4.42848 -4.4284716 -4.4284954][-4.4287143 -4.428719 -4.4287543 -4.428782 -4.428772 -4.4287305 -4.4286547 -4.4285645 -4.4285116 -4.4285145 -4.4285288 -4.4285121 -4.42849 -4.428493 -4.4285307][-4.4287152 -4.4287333 -4.4287643 -4.42878 -4.4287667 -4.4287214 -4.4286361 -4.4285321 -4.428473 -4.4284859 -4.4285245 -4.4285407 -4.4285469 -4.4285612 -4.4285994][-4.4287281 -4.428771 -4.4287992 -4.4287968 -4.4287753 -4.428731 -4.4286542 -4.4285631 -4.4285169 -4.42853 -4.4285727 -4.4286103 -4.4286304 -4.4286408 -4.42866][-4.4287491 -4.4288049 -4.4288263 -4.4288116 -4.4287829 -4.4287472 -4.4286919 -4.4286265 -4.4285984 -4.4286156 -4.4286528 -4.4286947 -4.4287138 -4.4287157 -4.4287214][-4.4287543 -4.4288049 -4.4288211 -4.4288087 -4.4287887 -4.4287643 -4.4287281 -4.4286847 -4.428678 -4.4287038 -4.4287329 -4.4287596 -4.4287653 -4.428762 -4.4287667][-4.4287591 -4.4287972 -4.42881 -4.4288125 -4.428813 -4.4287972 -4.4287677 -4.4287381 -4.4287486 -4.4287782 -4.4287944 -4.4287987 -4.4287877 -4.4287853 -4.4287958][-4.4287782 -4.4288106 -4.4288292 -4.4288507 -4.4288683 -4.4288592 -4.428834 -4.4288125 -4.4288244 -4.4288464 -4.4288492 -4.4288378 -4.4288149 -4.4288087 -4.4288163][-4.4288116 -4.428834 -4.4288549 -4.428885 -4.4289074 -4.4289064 -4.4288912 -4.42888 -4.4288898 -4.4289012 -4.4288964 -4.4288778 -4.428853 -4.4288473 -4.4288516][-4.4288645 -4.4288712 -4.428884 -4.4289064 -4.4289246 -4.4289284 -4.4289241 -4.4289217 -4.42893 -4.4289374 -4.4289346 -4.42892 -4.4289021 -4.4288988 -4.4289026][-4.4289212 -4.4289169 -4.4289231 -4.4289365 -4.4289494 -4.4289541 -4.4289546 -4.4289551 -4.4289613 -4.4289684 -4.428968 -4.42896 -4.42895 -4.428946 -4.4289479]]...]
INFO - root - 2017-12-10 05:03:19.650021: step 2610, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:10m:05s remains)
INFO - root - 2017-12-10 05:03:22.383529: step 2620, loss = 2.28, batch loss = 2.23 (27.4 examples/sec; 0.292 sec/batch; 26h:47m:32s remains)
INFO - root - 2017-12-10 05:03:25.067866: step 2630, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:43m:14s remains)
INFO - root - 2017-12-10 05:03:27.715034: step 2640, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:19m:54s remains)
INFO - root - 2017-12-10 05:03:30.419314: step 2650, loss = 2.28, batch loss = 2.23 (26.4 examples/sec; 0.303 sec/batch; 27h:46m:30s remains)
INFO - root - 2017-12-10 05:03:33.082003: step 2660, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:41m:50s remains)
INFO - root - 2017-12-10 05:03:35.739287: step 2670, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:49m:42s remains)
INFO - root - 2017-12-10 05:03:38.429235: step 2680, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:21m:10s remains)
INFO - root - 2017-12-10 05:03:41.087196: step 2690, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:36m:49s remains)
INFO - root - 2017-12-10 05:03:43.739066: step 2700, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:42m:53s remains)
2017-12-10 05:03:44.084093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428977 -4.4289656 -4.4289613 -4.4289637 -4.4289656 -4.42896 -4.4289455 -4.428926 -4.4289112 -4.4289069 -4.428915 -4.4289331 -4.428947 -4.4289484 -4.4289365][-4.4289823 -4.4289746 -4.4289722 -4.4289737 -4.4289703 -4.4289589 -4.4289355 -4.4289069 -4.4288864 -4.4288826 -4.4288931 -4.428915 -4.4289393 -4.4289513 -4.4289484][-4.4289885 -4.4289789 -4.4289722 -4.4289665 -4.4289508 -4.4289212 -4.4288783 -4.4288378 -4.4288154 -4.4288168 -4.4288335 -4.4288626 -4.4289012 -4.4289331 -4.4289451][-4.4289851 -4.4289732 -4.4289589 -4.4289374 -4.4289 -4.4288421 -4.4287667 -4.4287019 -4.4286695 -4.4286809 -4.4287143 -4.4287658 -4.4288292 -4.4288836 -4.4289126][-4.4289632 -4.4289455 -4.4289174 -4.4288764 -4.428813 -4.4287214 -4.4286013 -4.4284854 -4.4284263 -4.4284592 -4.4285383 -4.4286318 -4.4287305 -4.4288087 -4.428854][-4.428906 -4.42888 -4.4288416 -4.4287853 -4.4287004 -4.4285655 -4.4283819 -4.42819 -4.428093 -4.4281745 -4.4283342 -4.4284897 -4.4286232 -4.4287224 -4.4287834][-4.4288182 -4.4287872 -4.4287405 -4.4286733 -4.4285655 -4.4283915 -4.4281592 -4.427916 -4.4277973 -4.427949 -4.4281945 -4.4283924 -4.4285412 -4.428647 -4.4287162][-4.4287314 -4.4287019 -4.4286523 -4.4285784 -4.4284606 -4.4282851 -4.4280758 -4.427876 -4.4277887 -4.427938 -4.4281764 -4.4283624 -4.4284997 -4.4285946 -4.4286604][-4.4287033 -4.4286747 -4.4286304 -4.4285679 -4.428472 -4.4283452 -4.4282131 -4.4280987 -4.4280381 -4.4281178 -4.4282756 -4.4284134 -4.4285216 -4.4286 -4.4286566][-4.4287338 -4.4287076 -4.4286833 -4.4286523 -4.428597 -4.4285226 -4.428452 -4.4283957 -4.42835 -4.4283738 -4.4284606 -4.4285526 -4.4286332 -4.4286914 -4.4287319][-4.4287968 -4.4287748 -4.4287658 -4.4287629 -4.4287477 -4.4287143 -4.42868 -4.4286532 -4.4286251 -4.4286289 -4.4286757 -4.4287372 -4.4287958 -4.4288359 -4.4288554][-4.4288435 -4.4288206 -4.4288154 -4.4288259 -4.4288397 -4.428844 -4.4288421 -4.4288359 -4.4288273 -4.4288311 -4.4288588 -4.4288988 -4.4289384 -4.4289637 -4.428967][-4.428854 -4.4288244 -4.4288177 -4.4288354 -4.4288683 -4.4288974 -4.42892 -4.4289303 -4.4289327 -4.4289351 -4.4289474 -4.4289718 -4.4289985 -4.4290161 -4.4290161][-4.4288254 -4.4287829 -4.4287691 -4.4287868 -4.4288311 -4.4288812 -4.4289246 -4.4289494 -4.4289522 -4.4289474 -4.4289489 -4.4289622 -4.4289842 -4.4290047 -4.4290147][-4.4287953 -4.4287319 -4.4287004 -4.4287066 -4.4287524 -4.4288158 -4.4288726 -4.4289041 -4.428905 -4.4288888 -4.4288759 -4.4288793 -4.4289021 -4.4289379 -4.4289684]]...]
INFO - root - 2017-12-10 05:03:46.730660: step 2710, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:40m:21s remains)
INFO - root - 2017-12-10 05:03:49.369764: step 2720, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:12m:34s remains)
INFO - root - 2017-12-10 05:03:52.025556: step 2730, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:22m:18s remains)
INFO - root - 2017-12-10 05:03:54.670718: step 2740, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:01m:16s remains)
INFO - root - 2017-12-10 05:03:57.375067: step 2750, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:12m:22s remains)
INFO - root - 2017-12-10 05:04:00.042234: step 2760, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:52m:17s remains)
INFO - root - 2017-12-10 05:04:02.729265: step 2770, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:01m:39s remains)
INFO - root - 2017-12-10 05:04:05.373049: step 2780, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:11m:06s remains)
INFO - root - 2017-12-10 05:04:08.049256: step 2790, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:27m:35s remains)
INFO - root - 2017-12-10 05:04:10.696726: step 2800, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 24h:03m:12s remains)
2017-12-10 05:04:11.077328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428689 -4.4287224 -4.4287329 -4.4287257 -4.4287443 -4.428751 -4.4287038 -4.4286194 -4.4285712 -4.4286079 -4.4286942 -4.428771 -4.4288278 -4.428884 -4.4289351][-4.4286327 -4.428647 -4.4286551 -4.4286585 -4.4286852 -4.4287 -4.4286537 -4.42856 -4.4285035 -4.4285469 -4.4286528 -4.4287486 -4.4288173 -4.4288745 -4.4289289][-4.4285994 -4.42859 -4.4285789 -4.4285774 -4.4286041 -4.428627 -4.4285936 -4.4284968 -4.4284348 -4.4284844 -4.4286036 -4.4287105 -4.4287868 -4.42885 -4.4289103][-4.4286251 -4.4286 -4.4285674 -4.4285417 -4.428546 -4.4285531 -4.428525 -4.4284382 -4.42838 -4.4284315 -4.4285564 -4.4286714 -4.4287515 -4.428823 -4.4288931][-4.4286895 -4.4286652 -4.4286203 -4.42857 -4.4285331 -4.4285 -4.4284635 -4.4283919 -4.4283485 -4.4284024 -4.4285321 -4.4286542 -4.4287367 -4.4288096 -4.4288869][-4.4287386 -4.4287167 -4.428668 -4.4285932 -4.4285131 -4.4284353 -4.4283905 -4.4283309 -4.4282985 -4.4283605 -4.4285069 -4.428638 -4.4287267 -4.4288044 -4.4288836][-4.4287677 -4.4287457 -4.4286861 -4.4285855 -4.42846 -4.4283376 -4.4282861 -4.4282403 -4.4282231 -4.4283094 -4.4284825 -4.4286246 -4.4287171 -4.4287982 -4.4288764][-4.4287753 -4.42875 -4.4286823 -4.4285755 -4.4284186 -4.4282675 -4.4282165 -4.4281754 -4.428164 -4.428278 -4.4284682 -4.428616 -4.4287066 -4.4287887 -4.4288692][-4.4287858 -4.4287496 -4.4286776 -4.4285884 -4.4284577 -4.4283304 -4.428277 -4.4282174 -4.4281936 -4.4283013 -4.4284797 -4.4286156 -4.4287009 -4.4287829 -4.428865][-4.428761 -4.4287238 -4.4286618 -4.428607 -4.4285536 -4.4285016 -4.4284573 -4.4283767 -4.4283338 -4.4284039 -4.42853 -4.428627 -4.4286933 -4.4287758 -4.4288616][-4.4287181 -4.4286814 -4.4286385 -4.4286194 -4.428627 -4.4286304 -4.428596 -4.4285159 -4.4284687 -4.4285073 -4.4285803 -4.4286323 -4.4286804 -4.428761 -4.4288507][-4.4286985 -4.4286633 -4.4286375 -4.42863 -4.4286485 -4.42868 -4.4286618 -4.4285855 -4.4285412 -4.4285727 -4.4286251 -4.4286551 -4.4286861 -4.4287586 -4.4288473][-4.4286962 -4.4286728 -4.4286561 -4.4286613 -4.4286847 -4.4287162 -4.4286933 -4.4286184 -4.4285874 -4.4286222 -4.4286704 -4.4286914 -4.4287128 -4.4287753 -4.4288583][-4.4286923 -4.4286737 -4.4286556 -4.4286704 -4.4287057 -4.4287291 -4.4287 -4.4286337 -4.4286146 -4.4286509 -4.4286976 -4.4287248 -4.4287491 -4.4288054 -4.4288778][-4.4286714 -4.428647 -4.4286294 -4.4286437 -4.428688 -4.4287171 -4.4286966 -4.4286375 -4.4286213 -4.4286604 -4.428709 -4.4287453 -4.4287786 -4.42883 -4.4288969]]...]
INFO - root - 2017-12-10 05:04:13.760295: step 2810, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:28m:25s remains)
INFO - root - 2017-12-10 05:04:16.412399: step 2820, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:41m:24s remains)
INFO - root - 2017-12-10 05:04:19.055694: step 2830, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:41m:46s remains)
INFO - root - 2017-12-10 05:04:21.747849: step 2840, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:57m:56s remains)
INFO - root - 2017-12-10 05:04:24.455550: step 2850, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:36m:18s remains)
INFO - root - 2017-12-10 05:04:27.121396: step 2860, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:23m:56s remains)
INFO - root - 2017-12-10 05:04:29.781962: step 2870, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:38m:41s remains)
INFO - root - 2017-12-10 05:04:32.392638: step 2880, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:20m:20s remains)
INFO - root - 2017-12-10 05:04:35.067045: step 2890, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:21m:52s remains)
INFO - root - 2017-12-10 05:04:37.722369: step 2900, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 25h:16m:19s remains)
2017-12-10 05:04:38.075346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290247 -4.4290204 -4.4290175 -4.4290123 -4.4290032 -4.4289885 -4.4289656 -4.4289446 -4.4289422 -4.4289627 -4.4289918 -4.4290185 -4.4290357 -4.4290404 -4.4290361][-4.4290237 -4.4290195 -4.4290156 -4.429008 -4.4289975 -4.4289851 -4.4289708 -4.4289632 -4.428968 -4.4289846 -4.4290032 -4.429018 -4.429028 -4.429029 -4.4290237][-4.4290257 -4.429019 -4.429009 -4.4289923 -4.4289713 -4.4289527 -4.4289393 -4.4289365 -4.4289474 -4.42897 -4.4289942 -4.4290133 -4.4290257 -4.429029 -4.4290261][-4.4290247 -4.4290118 -4.4289913 -4.4289589 -4.4289174 -4.4288807 -4.4288559 -4.4288492 -4.4288673 -4.4289088 -4.4289546 -4.4289894 -4.4290128 -4.4290252 -4.4290295][-4.4290204 -4.4290004 -4.428966 -4.4289136 -4.4288406 -4.4287643 -4.4287 -4.4286675 -4.4286923 -4.428772 -4.4288645 -4.4289379 -4.4289875 -4.4290156 -4.4290304][-4.4290142 -4.4289894 -4.4289403 -4.4288583 -4.4287372 -4.4285917 -4.42845 -4.4283657 -4.4284024 -4.4285502 -4.4287219 -4.428863 -4.428956 -4.4290066 -4.42903][-4.4290109 -4.4289827 -4.4289188 -4.428803 -4.4286246 -4.4283943 -4.4281564 -4.4280076 -4.4280648 -4.4283009 -4.428565 -4.4287791 -4.4289212 -4.428997 -4.429028][-4.42901 -4.4289813 -4.4289112 -4.4287758 -4.4285603 -4.4282708 -4.4279652 -4.427772 -4.4278531 -4.4281511 -4.4284692 -4.4287219 -4.4288926 -4.4289842 -4.4290223][-4.4290061 -4.4289808 -4.4289227 -4.4288049 -4.4286122 -4.4283509 -4.4280877 -4.427937 -4.4280086 -4.42825 -4.4285164 -4.4287386 -4.4288969 -4.4289856 -4.4290228][-4.4289875 -4.4289656 -4.4289284 -4.4288554 -4.4287362 -4.4285722 -4.4284148 -4.4283342 -4.4283767 -4.428515 -4.428679 -4.42883 -4.4289441 -4.4290075 -4.4290304][-4.4289417 -4.4289188 -4.4289012 -4.4288745 -4.428834 -4.4287724 -4.4287143 -4.4286942 -4.428721 -4.4287858 -4.4288645 -4.4289446 -4.429008 -4.4290376 -4.4290404][-4.4288545 -4.4288168 -4.4288077 -4.4288239 -4.4288526 -4.4288721 -4.4288836 -4.4289026 -4.4289303 -4.4289632 -4.4289932 -4.4290223 -4.4290452 -4.4290504 -4.4290438][-4.4287062 -4.428618 -4.4285951 -4.4286551 -4.4287643 -4.4288654 -4.4289365 -4.4289851 -4.4290147 -4.4290357 -4.4290433 -4.4290476 -4.4290504 -4.4290471 -4.4290395][-4.428494 -4.428319 -4.4282641 -4.4283762 -4.4285812 -4.4287753 -4.4289155 -4.429 -4.4290371 -4.4290528 -4.4290495 -4.4290428 -4.42904 -4.4290361 -4.4290309][-4.4282756 -4.4280109 -4.4279175 -4.4280849 -4.4283814 -4.4286513 -4.4288464 -4.4289651 -4.429019 -4.4290414 -4.4290385 -4.4290309 -4.429028 -4.4290252 -4.4290214]]...]
INFO - root - 2017-12-10 05:04:40.742910: step 2910, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:58m:56s remains)
INFO - root - 2017-12-10 05:04:43.440772: step 2920, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 25h:08m:38s remains)
INFO - root - 2017-12-10 05:04:46.150778: step 2930, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:35m:16s remains)
INFO - root - 2017-12-10 05:04:48.790016: step 2940, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:27m:47s remains)
INFO - root - 2017-12-10 05:04:51.488236: step 2950, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:22m:25s remains)
INFO - root - 2017-12-10 05:04:54.145752: step 2960, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:29m:35s remains)
INFO - root - 2017-12-10 05:04:56.801022: step 2970, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:44m:00s remains)
INFO - root - 2017-12-10 05:04:59.480716: step 2980, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:51m:34s remains)
INFO - root - 2017-12-10 05:05:02.129518: step 2990, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:20m:03s remains)
INFO - root - 2017-12-10 05:05:04.802820: step 3000, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:42m:24s remains)
2017-12-10 05:05:05.173353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287043 -4.428751 -4.4288025 -4.4288282 -4.4287882 -4.4287162 -4.4286518 -4.4286313 -4.4286528 -4.4286871 -4.4287252 -4.4287767 -4.4287958 -4.4287891 -4.4288077][-4.4287014 -4.4287462 -4.4287996 -4.4288249 -4.4287796 -4.4287019 -4.4286261 -4.4285994 -4.4286213 -4.4286504 -4.4286814 -4.4287386 -4.4287605 -4.4287558 -4.4287724][-4.4286842 -4.4287229 -4.4287829 -4.4288206 -4.4287863 -4.4287138 -4.4286389 -4.4286113 -4.4286308 -4.4286509 -4.4286714 -4.4287195 -4.4287329 -4.4287305 -4.4287496][-4.4286466 -4.4286718 -4.4287362 -4.4287848 -4.4287677 -4.42872 -4.428678 -4.4286761 -4.4287057 -4.4287319 -4.4287543 -4.4287844 -4.4287772 -4.4287696 -4.4287848][-4.4286227 -4.4286218 -4.4286661 -4.4287047 -4.4286962 -4.428669 -4.4286561 -4.4286823 -4.4287367 -4.4287848 -4.4288287 -4.4288592 -4.4288535 -4.428853 -4.4288664][-4.4285994 -4.4285765 -4.4285946 -4.4286194 -4.4286032 -4.4285631 -4.4285388 -4.4285793 -4.4286613 -4.4287257 -4.428781 -4.4288173 -4.4288311 -4.4288526 -4.4288731][-4.4285512 -4.4284925 -4.428472 -4.4284835 -4.4284534 -4.42837 -4.4282994 -4.4283528 -4.4284773 -4.4285674 -4.4286361 -4.4286885 -4.4287362 -4.4287829 -4.4288111][-4.4285216 -4.42841 -4.4283328 -4.4283066 -4.4282413 -4.42809 -4.4279451 -4.4280305 -4.4282308 -4.4283676 -4.4284515 -4.4285173 -4.4285955 -4.4286671 -4.4286952][-4.4285846 -4.4284587 -4.4283514 -4.4282923 -4.428196 -4.4280119 -4.4278274 -4.4279227 -4.4281535 -4.4283075 -4.4283867 -4.4284339 -4.4284973 -4.4285603 -4.4285746][-4.4287152 -4.42862 -4.4285321 -4.4284697 -4.4283924 -4.4282689 -4.428153 -4.42821 -4.4283566 -4.4284711 -4.4285254 -4.4285264 -4.4285388 -4.4285669 -4.4285579][-4.4288368 -4.4287791 -4.4287219 -4.4286761 -4.4286284 -4.4285631 -4.4285097 -4.4285359 -4.428607 -4.4286628 -4.4286723 -4.4286337 -4.4286075 -4.4286027 -4.4285688][-4.4289 -4.4288692 -4.4288392 -4.4288144 -4.4287882 -4.4287529 -4.4287252 -4.4287291 -4.4287553 -4.4287567 -4.4287214 -4.4286542 -4.4286065 -4.4285679 -4.4285064][-4.428926 -4.4289079 -4.4288912 -4.4288793 -4.4288645 -4.4288406 -4.428813 -4.4287992 -4.4288011 -4.4287648 -4.428688 -4.4286 -4.4285321 -4.4284639 -4.4283824][-4.4289389 -4.4289246 -4.428915 -4.42891 -4.4289026 -4.4288883 -4.428864 -4.428843 -4.4288216 -4.428761 -4.4286623 -4.4285579 -4.4284697 -4.4283819 -4.4283013][-4.4289556 -4.4289417 -4.4289327 -4.4289274 -4.4289227 -4.4289117 -4.428894 -4.4288793 -4.428854 -4.428792 -4.4286971 -4.4285965 -4.4285059 -4.4284215 -4.4283476]]...]
INFO - root - 2017-12-10 05:05:07.839929: step 3010, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:53m:40s remains)
INFO - root - 2017-12-10 05:05:10.505123: step 3020, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:18m:17s remains)
INFO - root - 2017-12-10 05:05:13.204303: step 3030, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:40m:05s remains)
INFO - root - 2017-12-10 05:05:15.924106: step 3040, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:32m:22s remains)
INFO - root - 2017-12-10 05:05:18.568514: step 3050, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:37m:59s remains)
INFO - root - 2017-12-10 05:05:21.226579: step 3060, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.276 sec/batch; 25h:17m:27s remains)
INFO - root - 2017-12-10 05:05:23.963020: step 3070, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:47m:31s remains)
INFO - root - 2017-12-10 05:05:26.652476: step 3080, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:55m:48s remains)
INFO - root - 2017-12-10 05:05:29.346907: step 3090, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 25h:00m:00s remains)
INFO - root - 2017-12-10 05:05:31.996345: step 3100, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:00m:46s remains)
2017-12-10 05:05:32.340449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428813 -4.4288192 -4.428822 -4.4288263 -4.428834 -4.4288311 -4.4288239 -4.4288087 -4.4287887 -4.4287658 -4.4287539 -4.4287424 -4.4287114 -4.4286771 -4.4286585][-4.4288378 -4.4288549 -4.4288626 -4.4288626 -4.428854 -4.4288321 -4.4288049 -4.4287724 -4.4287467 -4.4287243 -4.4287186 -4.4287095 -4.4286833 -4.4286656 -4.4286633][-4.4288774 -4.4288955 -4.4289031 -4.4288974 -4.4288783 -4.4288487 -4.4288073 -4.4287548 -4.4287143 -4.4286847 -4.4286814 -4.4286776 -4.4286647 -4.4286652 -4.4286752][-4.4288969 -4.428915 -4.4289255 -4.4289212 -4.4288945 -4.4288583 -4.4288039 -4.4287319 -4.428679 -4.4286485 -4.4286456 -4.4286456 -4.4286437 -4.4286561 -4.4286771][-4.4289241 -4.4289393 -4.4289489 -4.4289374 -4.428896 -4.4288445 -4.4287734 -4.428689 -4.4286351 -4.4286175 -4.4286227 -4.4286242 -4.4286208 -4.4286318 -4.4286585][-4.42895 -4.4289579 -4.4289584 -4.4289341 -4.4288769 -4.4288039 -4.4287124 -4.4286189 -4.42858 -4.4285879 -4.4286108 -4.42862 -4.4286203 -4.4286313 -4.4286566][-4.4289551 -4.4289589 -4.4289474 -4.4289083 -4.4288359 -4.4287386 -4.4286151 -4.4285064 -4.428493 -4.4285326 -4.4285774 -4.4286027 -4.4286122 -4.4286251 -4.4286509][-4.4289575 -4.428956 -4.4289269 -4.4288688 -4.4287758 -4.4286513 -4.4284992 -4.42839 -4.4284167 -4.42849 -4.428555 -4.428596 -4.4286141 -4.4286265 -4.4286509][-4.4289603 -4.4289503 -4.4289026 -4.4288316 -4.428731 -4.4286079 -4.4284744 -4.4284077 -4.4284673 -4.4285474 -4.42861 -4.42865 -4.4286647 -4.4286723 -4.42869][-4.4289637 -4.42894 -4.4288793 -4.4288068 -4.4287138 -4.4286137 -4.4285288 -4.4285131 -4.4285741 -4.4286327 -4.4286733 -4.4286981 -4.4287043 -4.4287086 -4.4287229][-4.4289594 -4.4289231 -4.4288564 -4.4287868 -4.4287024 -4.4286227 -4.4285774 -4.4285874 -4.42864 -4.4286842 -4.4287128 -4.4287291 -4.4287305 -4.4287319 -4.4287419][-4.4289451 -4.4288993 -4.4288297 -4.4287624 -4.4286876 -4.428628 -4.4286089 -4.4286256 -4.4286695 -4.42871 -4.4287386 -4.4287529 -4.4287534 -4.4287524 -4.4287553][-4.4289293 -4.42888 -4.4288111 -4.4287457 -4.4286809 -4.4286432 -4.4286366 -4.4286504 -4.4286885 -4.42873 -4.428761 -4.4287767 -4.428782 -4.428781 -4.4287744][-4.4289184 -4.4288697 -4.4288077 -4.4287496 -4.428699 -4.428678 -4.4286747 -4.428688 -4.4287243 -4.4287653 -4.4287949 -4.4288125 -4.428823 -4.4288249 -4.4288106][-4.4289002 -4.4288487 -4.4287896 -4.4287367 -4.4287024 -4.4286952 -4.4286966 -4.4287157 -4.4287491 -4.4287834 -4.4288054 -4.42882 -4.4288354 -4.4288425 -4.4288273]]...]
INFO - root - 2017-12-10 05:05:34.974918: step 3110, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:41m:25s remains)
INFO - root - 2017-12-10 05:05:37.656279: step 3120, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:52m:11s remains)
INFO - root - 2017-12-10 05:05:40.291314: step 3130, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:40m:55s remains)
INFO - root - 2017-12-10 05:05:42.927416: step 3140, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:10m:00s remains)
INFO - root - 2017-12-10 05:05:45.590011: step 3150, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:42m:11s remains)
INFO - root - 2017-12-10 05:05:48.232421: step 3160, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:47m:42s remains)
INFO - root - 2017-12-10 05:05:50.913828: step 3170, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 25h:13m:18s remains)
INFO - root - 2017-12-10 05:05:53.537333: step 3180, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:31m:12s remains)
INFO - root - 2017-12-10 05:05:56.182175: step 3190, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:58m:49s remains)
INFO - root - 2017-12-10 05:05:58.808775: step 3200, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:26m:51s remains)
2017-12-10 05:05:59.158551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289875 -4.4289713 -4.4289646 -4.4289632 -4.4289703 -4.42898 -4.4289784 -4.4289737 -4.42897 -4.42897 -4.4289703 -4.4289722 -4.428977 -4.428987 -4.4289947][-4.428967 -4.4289389 -4.42892 -4.4289083 -4.4289136 -4.4289322 -4.428936 -4.428926 -4.4289236 -4.428926 -4.4289289 -4.4289351 -4.4289465 -4.4289656 -4.4289775][-4.428925 -4.42888 -4.4288478 -4.4288263 -4.4288244 -4.428844 -4.4288459 -4.4288335 -4.4288392 -4.4288487 -4.4288635 -4.4288807 -4.4289017 -4.4289327 -4.4289517][-4.4288788 -4.4288163 -4.4287696 -4.4287357 -4.4287248 -4.4287338 -4.4287281 -4.4287105 -4.4287214 -4.4287372 -4.4287653 -4.4287977 -4.4288354 -4.4288878 -4.4289222][-4.4288354 -4.42876 -4.4286947 -4.4286404 -4.4286089 -4.4286079 -4.4286008 -4.4285746 -4.4285831 -4.4285927 -4.428627 -4.4286876 -4.4287639 -4.4288492 -4.4289007][-4.428791 -4.4286976 -4.4286046 -4.4285212 -4.4284673 -4.4284563 -4.4284496 -4.4284129 -4.4284124 -4.4284019 -4.42843 -4.4285231 -4.4286547 -4.4287934 -4.4288745][-4.4287372 -4.42862 -4.4284987 -4.4283953 -4.4283247 -4.4283042 -4.4282894 -4.4282308 -4.4282 -4.4281545 -4.42817 -4.428299 -4.4284968 -4.4287028 -4.428823][-4.4286666 -4.4285336 -4.4284039 -4.4283056 -4.42824 -4.4282136 -4.4281778 -4.4280992 -4.4280457 -4.4279866 -4.4279966 -4.4281468 -4.4283881 -4.4286356 -4.4287777][-4.4286089 -4.4284854 -4.4283795 -4.4283066 -4.428256 -4.4282274 -4.4281783 -4.4281039 -4.4280624 -4.428031 -4.428061 -4.4282093 -4.4284306 -4.4286537 -4.428781][-4.428637 -4.4285398 -4.4284625 -4.4284124 -4.4283724 -4.4283485 -4.4283061 -4.4282575 -4.4282432 -4.4282408 -4.4282808 -4.4283981 -4.4285631 -4.4287319 -4.4288263][-4.4287162 -4.4286447 -4.4285922 -4.4285626 -4.4285364 -4.4285288 -4.4285131 -4.4284987 -4.4285011 -4.4285116 -4.428544 -4.4286218 -4.4287257 -4.4288373 -4.4288945][-4.4288092 -4.4287605 -4.4287257 -4.428709 -4.4286976 -4.4287071 -4.4287152 -4.428721 -4.4287319 -4.4287457 -4.42877 -4.4288158 -4.4288735 -4.4289379 -4.4289641][-4.4288793 -4.4288483 -4.4288263 -4.4288182 -4.4288192 -4.4288306 -4.4288406 -4.428853 -4.4288683 -4.42888 -4.4288964 -4.428926 -4.4289618 -4.428997 -4.4290071][-4.4289269 -4.4289083 -4.4288955 -4.4288898 -4.4288936 -4.4289007 -4.4289093 -4.4289222 -4.428936 -4.4289446 -4.4289546 -4.4289732 -4.4289951 -4.4290128 -4.4290185][-4.4289708 -4.4289594 -4.4289532 -4.4289517 -4.4289527 -4.4289546 -4.42896 -4.4289694 -4.4289789 -4.4289827 -4.4289885 -4.4290004 -4.4290123 -4.42902 -4.4290228]]...]
INFO - root - 2017-12-10 05:06:01.831123: step 3210, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.279 sec/batch; 25h:32m:53s remains)
INFO - root - 2017-12-10 05:06:04.480289: step 3220, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:02m:27s remains)
INFO - root - 2017-12-10 05:06:07.181461: step 3230, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:12m:26s remains)
INFO - root - 2017-12-10 05:06:09.817705: step 3240, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:59m:03s remains)
INFO - root - 2017-12-10 05:06:12.454285: step 3250, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:36m:34s remains)
INFO - root - 2017-12-10 05:06:15.291259: step 3260, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:19m:22s remains)
INFO - root - 2017-12-10 05:06:19.490506: step 3270, loss = 2.28, batch loss = 2.23 (24.4 examples/sec; 0.328 sec/batch; 30h:00m:06s remains)
INFO - root - 2017-12-10 05:06:23.386224: step 3280, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:20m:05s remains)
INFO - root - 2017-12-10 05:06:28.686721: step 3290, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.518 sec/batch; 47h:21m:45s remains)
INFO - root - 2017-12-10 05:06:34.012941: step 3300, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.530 sec/batch; 48h:25m:30s remains)
2017-12-10 05:06:34.626776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286332 -4.4286137 -4.4286571 -4.4287405 -4.4288154 -4.4288678 -4.4288983 -4.4289141 -4.4289265 -4.4289351 -4.4289355 -4.4289241 -4.428894 -4.4288425 -4.4287925][-4.4287286 -4.4287143 -4.4287333 -4.4287858 -4.4288392 -4.4288869 -4.4289193 -4.4289355 -4.4289432 -4.4289403 -4.4289265 -4.4288983 -4.4288435 -4.4287667 -4.4287066][-4.4288144 -4.4288144 -4.4288154 -4.4288344 -4.428863 -4.4289007 -4.4289331 -4.4289594 -4.4289684 -4.4289551 -4.4289203 -4.4288678 -4.42878 -4.4286695 -4.4285913][-4.4288659 -4.4288745 -4.4288588 -4.4288435 -4.428844 -4.4288669 -4.4288979 -4.4289312 -4.4289451 -4.4289312 -4.4288912 -4.4288225 -4.4287033 -4.4285569 -4.4284549][-4.4288831 -4.4289 -4.4288678 -4.4288206 -4.4287891 -4.4287887 -4.4288106 -4.4288421 -4.4288616 -4.4288626 -4.4288363 -4.4287729 -4.42864 -4.4284673 -4.4283385][-4.4288726 -4.4289007 -4.4288678 -4.4288063 -4.4287481 -4.4287162 -4.4287095 -4.4287233 -4.4287472 -4.4287758 -4.42878 -4.4287419 -4.4286294 -4.4284697 -4.4283438][-4.4288568 -4.4288807 -4.4288578 -4.4288 -4.4287343 -4.4286733 -4.4286246 -4.4286017 -4.428606 -4.4286513 -4.4286976 -4.4287133 -4.4286642 -4.4285703 -4.428493][-4.4288297 -4.4288282 -4.4287972 -4.4287539 -4.4287014 -4.4286332 -4.4285593 -4.4284959 -4.4284682 -4.4285169 -4.4286041 -4.4286766 -4.4286981 -4.4286804 -4.4286628][-4.4287953 -4.42875 -4.4286885 -4.4286427 -4.4286141 -4.4285631 -4.4284954 -4.4284248 -4.4283895 -4.428442 -4.4285593 -4.4286733 -4.4287424 -4.4287696 -4.4287844][-4.4287748 -4.428689 -4.4285965 -4.4285316 -4.4285064 -4.42847 -4.4284205 -4.4283733 -4.4283628 -4.4284353 -4.4285722 -4.4287109 -4.4288092 -4.4288592 -4.4288826][-4.4287605 -4.4286661 -4.4285712 -4.4284973 -4.4284568 -4.4284158 -4.4283686 -4.4283419 -4.4283633 -4.428463 -4.4286127 -4.4287553 -4.42886 -4.4289188 -4.4289432][-4.4287682 -4.4286804 -4.4285994 -4.4285336 -4.4284911 -4.428453 -4.4284067 -4.4283862 -4.4284196 -4.4285297 -4.4286747 -4.4288044 -4.4288964 -4.4289484 -4.428967][-4.4288025 -4.4287229 -4.4286594 -4.4286132 -4.4285836 -4.428556 -4.4285178 -4.4285011 -4.4285288 -4.4286251 -4.4287477 -4.4288516 -4.4289231 -4.4289622 -4.4289756][-4.4288521 -4.4287939 -4.4287529 -4.4287257 -4.4287047 -4.4286871 -4.4286633 -4.4286556 -4.4286747 -4.4287395 -4.428823 -4.4288931 -4.4289422 -4.4289713 -4.4289804][-4.4288855 -4.4288535 -4.428834 -4.4288244 -4.4288096 -4.4287963 -4.4287877 -4.428792 -4.4288049 -4.4288378 -4.4288821 -4.4289212 -4.4289517 -4.4289737 -4.4289818]]...]
INFO - root - 2017-12-10 05:06:39.925484: step 3310, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:09m:26s remains)
INFO - root - 2017-12-10 05:06:45.223625: step 3320, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 47h:47m:16s remains)
INFO - root - 2017-12-10 05:06:50.536649: step 3330, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:34m:20s remains)
INFO - root - 2017-12-10 05:06:55.889449: step 3340, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:16m:31s remains)
INFO - root - 2017-12-10 05:07:01.178931: step 3350, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 48h:45m:01s remains)
INFO - root - 2017-12-10 05:07:06.393215: step 3360, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.514 sec/batch; 47h:01m:20s remains)
INFO - root - 2017-12-10 05:07:11.718577: step 3370, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:12m:32s remains)
INFO - root - 2017-12-10 05:07:16.853121: step 3380, loss = 2.28, batch loss = 2.23 (13.8 examples/sec; 0.581 sec/batch; 53h:06m:33s remains)
INFO - root - 2017-12-10 05:07:22.053139: step 3390, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.517 sec/batch; 47h:13m:35s remains)
INFO - root - 2017-12-10 05:07:27.427238: step 3400, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.534 sec/batch; 48h:47m:39s remains)
2017-12-10 05:07:27.930542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286036 -4.4286308 -4.428678 -4.4287028 -4.4287 -4.4286852 -4.4286838 -4.4287186 -4.4287558 -4.4287839 -4.428803 -4.4288063 -4.4287949 -4.4287906 -4.428791][-4.4285274 -4.4285765 -4.4286437 -4.4286833 -4.4286962 -4.4286966 -4.4287047 -4.4287295 -4.4287515 -4.428772 -4.428781 -4.428781 -4.4287615 -4.4287386 -4.4287205][-4.4285536 -4.4286208 -4.4286947 -4.428741 -4.4287591 -4.4287663 -4.4287691 -4.4287744 -4.4287763 -4.4287815 -4.4287853 -4.4287825 -4.4287534 -4.4287057 -4.4286661][-4.4286366 -4.4287071 -4.4287739 -4.4288111 -4.4288259 -4.4288263 -4.4288211 -4.4288077 -4.4287906 -4.4287915 -4.4287968 -4.4287972 -4.4287634 -4.4287009 -4.4286461][-4.4287271 -4.428782 -4.4288254 -4.4288464 -4.4288445 -4.4288316 -4.4288116 -4.4287753 -4.4287496 -4.4287505 -4.4287639 -4.428781 -4.4287596 -4.4287038 -4.4286547][-4.4287686 -4.4287992 -4.4288197 -4.4288263 -4.4288092 -4.428772 -4.4287143 -4.4286451 -4.4286184 -4.4286447 -4.4286852 -4.4287305 -4.4287333 -4.4287024 -4.428678][-4.428741 -4.4287577 -4.4287648 -4.4287591 -4.4287252 -4.4286551 -4.428544 -4.4284348 -4.4284172 -4.4284935 -4.4285836 -4.4286656 -4.4287062 -4.4287066 -4.4287109][-4.4286804 -4.4286761 -4.4286675 -4.4286556 -4.428607 -4.428503 -4.428339 -4.4281883 -4.428206 -4.4283724 -4.4285264 -4.4286437 -4.4287195 -4.4287477 -4.4287686][-4.4286556 -4.4286323 -4.4286103 -4.428587 -4.4285269 -4.4284091 -4.4282417 -4.4281216 -4.4282026 -4.4284172 -4.4285879 -4.4287062 -4.4287815 -4.4288111 -4.4288249][-4.4286752 -4.4286485 -4.4286289 -4.4286041 -4.4285517 -4.428473 -4.4283876 -4.4283552 -4.4284425 -4.4286036 -4.4287143 -4.4287715 -4.428812 -4.4288259 -4.428833][-4.42871 -4.4286871 -4.428679 -4.4286656 -4.4286313 -4.4285946 -4.4285779 -4.4285903 -4.4286504 -4.4287395 -4.4287825 -4.4287829 -4.4287806 -4.4287691 -4.42877][-4.4287248 -4.4286957 -4.4286923 -4.428689 -4.4286718 -4.4286585 -4.4286714 -4.4286976 -4.4287353 -4.428781 -4.4287944 -4.4287658 -4.4287295 -4.4286919 -4.4286833][-4.428721 -4.4286795 -4.4286714 -4.4286709 -4.4286637 -4.4286561 -4.4286714 -4.4286976 -4.4287324 -4.428771 -4.4287848 -4.4287472 -4.4286885 -4.4286313 -4.4286156][-4.4287052 -4.4286561 -4.4286418 -4.4286413 -4.4286375 -4.4286289 -4.4286356 -4.4286666 -4.4287128 -4.428762 -4.4287868 -4.4287505 -4.4286771 -4.4286165 -4.4286017][-4.4287038 -4.4286656 -4.4286528 -4.4286461 -4.4286313 -4.4286113 -4.4286022 -4.4286318 -4.4286871 -4.4287467 -4.4287782 -4.4287419 -4.4286656 -4.4286065 -4.4285965]]...]
INFO - root - 2017-12-10 05:07:33.171499: step 3410, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 48h:42m:55s remains)
INFO - root - 2017-12-10 05:07:38.571407: step 3420, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:19m:40s remains)
INFO - root - 2017-12-10 05:07:43.962295: step 3430, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:13m:06s remains)
INFO - root - 2017-12-10 05:07:49.340620: step 3440, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.536 sec/batch; 48h:58m:54s remains)
INFO - root - 2017-12-10 05:07:54.605731: step 3450, loss = 2.28, batch loss = 2.23 (16.3 examples/sec; 0.490 sec/batch; 44h:48m:43s remains)
INFO - root - 2017-12-10 05:07:59.883139: step 3460, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.536 sec/batch; 49h:01m:44s remains)
INFO - root - 2017-12-10 05:08:05.089139: step 3470, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.508 sec/batch; 46h:23m:49s remains)
INFO - root - 2017-12-10 05:08:10.123035: step 3480, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 47h:43m:40s remains)
INFO - root - 2017-12-10 05:08:15.329363: step 3490, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.521 sec/batch; 47h:38m:43s remains)
INFO - root - 2017-12-10 05:08:20.566940: step 3500, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.528 sec/batch; 48h:17m:04s remains)
2017-12-10 05:08:21.142668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287171 -4.4286952 -4.4287138 -4.428741 -4.4287744 -4.4288 -4.4288 -4.4287887 -4.4287968 -4.4288063 -4.4288039 -4.4287963 -4.4287853 -4.4287753 -4.4287567][-4.4286928 -4.4286723 -4.4286938 -4.4287124 -4.428741 -4.4287663 -4.428771 -4.4287591 -4.4287658 -4.4287763 -4.4287825 -4.4287734 -4.42875 -4.4287267 -4.4286966][-4.4286604 -4.428647 -4.4286761 -4.4286895 -4.4287148 -4.4287395 -4.4287448 -4.4287286 -4.4287367 -4.4287529 -4.428772 -4.4287729 -4.4287462 -4.4287133 -4.4286737][-4.4286046 -4.428587 -4.4286203 -4.4286346 -4.4286628 -4.4286933 -4.4286895 -4.4286628 -4.4286733 -4.4286942 -4.4287229 -4.4287343 -4.4287114 -4.42867 -4.4286194][-4.4285851 -4.4285574 -4.4285932 -4.4286118 -4.4286375 -4.4286613 -4.4286437 -4.4286046 -4.4286127 -4.4286327 -4.4286609 -4.4286733 -4.4286556 -4.4286089 -4.428546][-4.4286489 -4.4286022 -4.42862 -4.4286342 -4.4286489 -4.4286547 -4.4286237 -4.4285769 -4.42858 -4.4286003 -4.4286275 -4.4286456 -4.4286366 -4.4285951 -4.4285316][-4.4287066 -4.4286447 -4.4286361 -4.4286337 -4.4286342 -4.4286246 -4.4285808 -4.4285331 -4.4285383 -4.4285607 -4.4285932 -4.4286265 -4.4286437 -4.4286227 -4.4285607][-4.4287224 -4.4286575 -4.4286327 -4.4286165 -4.4286022 -4.4285746 -4.4285169 -4.4284706 -4.4284782 -4.4285045 -4.4285345 -4.4285717 -4.4286089 -4.4286036 -4.4285469][-4.428731 -4.4286737 -4.4286466 -4.4286294 -4.4286184 -4.4285908 -4.4285398 -4.4285049 -4.4285107 -4.4285259 -4.4285321 -4.4285502 -4.428586 -4.4285812 -4.4285321][-4.42875 -4.4287043 -4.4286876 -4.4286876 -4.4286962 -4.428688 -4.4286528 -4.4286289 -4.4286284 -4.4286261 -4.4286141 -4.428618 -4.4286413 -4.428638 -4.4285989][-4.4287648 -4.4287267 -4.4287262 -4.4287467 -4.4287667 -4.4287705 -4.4287491 -4.4287376 -4.42874 -4.4287357 -4.4287243 -4.4287214 -4.4287314 -4.42873 -4.4287105][-4.4287658 -4.42873 -4.4287415 -4.428781 -4.4288149 -4.4288368 -4.4288416 -4.4288497 -4.4288597 -4.4288588 -4.42885 -4.4288425 -4.4288387 -4.4288292 -4.4288144][-4.4287696 -4.4287367 -4.4287572 -4.4288068 -4.4288526 -4.428885 -4.4289069 -4.4289255 -4.4289379 -4.4289432 -4.4289412 -4.4289331 -4.4289255 -4.4289103 -4.4288898][-4.4287815 -4.4287548 -4.4287767 -4.4288225 -4.4288669 -4.4289017 -4.4289265 -4.4289403 -4.4289446 -4.4289446 -4.4289441 -4.4289441 -4.4289441 -4.4289346 -4.4289184][-4.4287953 -4.4287696 -4.4287872 -4.4288311 -4.4288759 -4.4289088 -4.4289293 -4.428937 -4.4289355 -4.4289336 -4.4289365 -4.4289436 -4.4289527 -4.4289522 -4.4289455]]...]
INFO - root - 2017-12-10 05:08:26.378817: step 3510, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:03m:00s remains)
INFO - root - 2017-12-10 05:08:31.537322: step 3520, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.505 sec/batch; 46h:10m:31s remains)
INFO - root - 2017-12-10 05:08:36.825432: step 3530, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 47h:51m:28s remains)
INFO - root - 2017-12-10 05:08:42.114205: step 3540, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.545 sec/batch; 49h:46m:45s remains)
INFO - root - 2017-12-10 05:08:47.375927: step 3550, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:32m:55s remains)
INFO - root - 2017-12-10 05:08:52.623948: step 3560, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:14m:53s remains)
INFO - root - 2017-12-10 05:08:57.831905: step 3570, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:28m:45s remains)
INFO - root - 2017-12-10 05:09:02.879765: step 3580, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:03m:23s remains)
INFO - root - 2017-12-10 05:09:08.091391: step 3590, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.532 sec/batch; 48h:36m:06s remains)
INFO - root - 2017-12-10 05:09:13.350784: step 3600, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.518 sec/batch; 47h:18m:17s remains)
2017-12-10 05:09:13.880029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289274 -4.4289031 -4.4288497 -4.4287629 -4.4286861 -4.4286504 -4.4286513 -4.4286833 -4.4287305 -4.4287934 -4.4288478 -4.4288635 -4.428834 -4.4287848 -4.428751][-4.4289122 -4.4288907 -4.4288497 -4.4287682 -4.428668 -4.4286027 -4.4285975 -4.4286413 -4.4287019 -4.4287767 -4.4288392 -4.4288597 -4.4288268 -4.4287682 -4.4287276][-4.4289 -4.428896 -4.4288754 -4.4288077 -4.4287 -4.4286122 -4.4285936 -4.42864 -4.4287047 -4.4287791 -4.4288387 -4.4288645 -4.42884 -4.4287896 -4.4287457][-4.4288807 -4.428894 -4.4288926 -4.4288478 -4.4287519 -4.4286571 -4.4286261 -4.4286666 -4.4287319 -4.4288077 -4.42887 -4.4289017 -4.4288864 -4.428844 -4.4287958][-4.4288497 -4.4288774 -4.4288888 -4.4288616 -4.4287825 -4.4286909 -4.4286551 -4.42869 -4.4287529 -4.4288282 -4.428895 -4.4289322 -4.4289231 -4.4288859 -4.4288359][-4.4288054 -4.428844 -4.428863 -4.4288464 -4.4287777 -4.4286933 -4.4286618 -4.4287004 -4.4287615 -4.4288306 -4.4288969 -4.4289384 -4.4289351 -4.4289036 -4.4288564][-4.4287639 -4.4288 -4.4288216 -4.4288144 -4.4287577 -4.4286833 -4.428659 -4.4287057 -4.4287667 -4.4288239 -4.4288807 -4.4289241 -4.4289293 -4.428906 -4.4288621][-4.4287348 -4.4287553 -4.4287815 -4.4287882 -4.4287462 -4.4286847 -4.4286628 -4.4287071 -4.4287577 -4.4287992 -4.4288397 -4.4288812 -4.4289017 -4.428895 -4.4288635][-4.4287233 -4.4287133 -4.4287372 -4.4287591 -4.4287372 -4.4286923 -4.4286776 -4.4287128 -4.4287453 -4.4287686 -4.4287915 -4.4288225 -4.4288497 -4.4288583 -4.42885][-4.4287586 -4.4287238 -4.4287362 -4.4287658 -4.4287643 -4.4287381 -4.4287314 -4.4287567 -4.4287653 -4.4287696 -4.4287782 -4.4287972 -4.4288187 -4.4288368 -4.4288497][-4.4288025 -4.4287591 -4.4287548 -4.4287763 -4.4287829 -4.4287691 -4.4287639 -4.4287753 -4.4287715 -4.4287658 -4.42877 -4.4287829 -4.4287949 -4.4288158 -4.42884][-4.4288297 -4.4287882 -4.4287739 -4.4287844 -4.42879 -4.4287763 -4.428762 -4.4287596 -4.42875 -4.4287429 -4.4287539 -4.428772 -4.4287839 -4.4288034 -4.4288273][-4.4288387 -4.4288063 -4.4287925 -4.4287996 -4.4288049 -4.4287896 -4.4287677 -4.4287553 -4.4287415 -4.4287353 -4.4287519 -4.4287753 -4.4287896 -4.4288039 -4.428823][-4.4288344 -4.4288116 -4.4288044 -4.428813 -4.4288154 -4.4287977 -4.428772 -4.4287524 -4.4287338 -4.4287248 -4.4287434 -4.4287744 -4.4287949 -4.4288077 -4.428822][-4.4288559 -4.428844 -4.4288449 -4.4288564 -4.4288597 -4.4288449 -4.428823 -4.4288006 -4.4287806 -4.4287715 -4.4287872 -4.4288154 -4.4288363 -4.4288454 -4.4288554]]...]
INFO - root - 2017-12-10 05:09:19.129720: step 3610, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 47h:55m:00s remains)
INFO - root - 2017-12-10 05:09:24.431305: step 3620, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.517 sec/batch; 47h:13m:30s remains)
INFO - root - 2017-12-10 05:09:29.764501: step 3630, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.538 sec/batch; 49h:07m:15s remains)
INFO - root - 2017-12-10 05:09:35.018109: step 3640, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 46h:45m:53s remains)
INFO - root - 2017-12-10 05:09:40.274214: step 3650, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 47h:48m:22s remains)
INFO - root - 2017-12-10 05:09:45.506154: step 3660, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.517 sec/batch; 47h:15m:58s remains)
INFO - root - 2017-12-10 05:09:50.837714: step 3670, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 47h:56m:35s remains)
INFO - root - 2017-12-10 05:09:55.787117: step 3680, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 47h:35m:09s remains)
INFO - root - 2017-12-10 05:10:01.127461: step 3690, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:03m:27s remains)
INFO - root - 2017-12-10 05:10:06.380575: step 3700, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.532 sec/batch; 48h:36m:13s remains)
2017-12-10 05:10:06.984124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286528 -4.4286351 -4.4286432 -4.4286785 -4.4287205 -4.4287624 -4.428772 -4.4287534 -4.4287138 -4.428659 -4.4286504 -4.4286904 -4.42879 -4.4288912 -4.4289613][-4.4285903 -4.4285951 -4.4286256 -4.4286761 -4.4287262 -4.4287639 -4.4287543 -4.4287062 -4.4286475 -4.4285922 -4.4286 -4.428648 -4.4287486 -4.4288507 -4.4289331][-4.4285975 -4.4286227 -4.4286656 -4.4287157 -4.4287558 -4.4287758 -4.428741 -4.4286623 -4.4285927 -4.4285541 -4.4285889 -4.4286461 -4.4287386 -4.428834 -4.4289141][-4.4286113 -4.4286528 -4.4287066 -4.4287481 -4.4287624 -4.4287443 -4.4286695 -4.4285507 -4.4284797 -4.4284821 -4.4285679 -4.4286585 -4.4287558 -4.4288435 -4.4289112][-4.4286337 -4.4286714 -4.4287252 -4.4287539 -4.4287324 -4.4286656 -4.42854 -4.42838 -4.42832 -4.4283886 -4.4285412 -4.4286747 -4.4287896 -4.428875 -4.4289265][-4.4286828 -4.4286928 -4.4287224 -4.4287286 -4.4286728 -4.4285502 -4.4283557 -4.4281378 -4.4281068 -4.42827 -4.4284997 -4.4286809 -4.4288192 -4.4289064 -4.428946][-4.4287438 -4.4287372 -4.4287362 -4.4287105 -4.4286132 -4.4284277 -4.4281321 -4.4278245 -4.4278374 -4.4281297 -4.4284472 -4.4286728 -4.4288273 -4.4289188 -4.4289527][-4.4288177 -4.428813 -4.4287858 -4.4287233 -4.4285836 -4.4283514 -4.4279833 -4.4276147 -4.427681 -4.4280591 -4.4284196 -4.4286532 -4.4288034 -4.4288945 -4.4289365][-4.428864 -4.4288564 -4.4288116 -4.4287324 -4.4285908 -4.4283843 -4.4280763 -4.4278054 -4.427887 -4.4281945 -4.4284816 -4.4286613 -4.4287829 -4.4288635 -4.4289136][-4.4288769 -4.4288669 -4.4288197 -4.4287477 -4.4286332 -4.4284873 -4.4283009 -4.4281554 -4.4282312 -4.4284248 -4.4286027 -4.4287181 -4.4287987 -4.42886 -4.4289012][-4.4288864 -4.4288745 -4.4288244 -4.4287586 -4.4286737 -4.4285827 -4.4284883 -4.4284267 -4.4284973 -4.4286122 -4.4287052 -4.4287715 -4.4288225 -4.4288688 -4.4289026][-4.4288812 -4.428875 -4.4288306 -4.4287715 -4.4287124 -4.4286623 -4.4286242 -4.4286184 -4.428679 -4.4287405 -4.4287767 -4.4288049 -4.4288416 -4.4288783 -4.4289074][-4.4288597 -4.4288635 -4.4288297 -4.4287877 -4.4287581 -4.4287386 -4.4287324 -4.4287448 -4.4287844 -4.4288096 -4.4288187 -4.4288244 -4.4288545 -4.4288888 -4.428915][-4.4288359 -4.4288454 -4.428822 -4.4287934 -4.4287868 -4.4287887 -4.4287891 -4.4287939 -4.4288239 -4.4288454 -4.4288468 -4.4288454 -4.4288731 -4.4289041 -4.4289284][-4.4288268 -4.4288368 -4.4288254 -4.4288087 -4.4288073 -4.4288092 -4.4287977 -4.4287839 -4.4288011 -4.4288239 -4.4288321 -4.4288445 -4.4288807 -4.4289145 -4.4289389]]...]
INFO - root - 2017-12-10 05:10:12.282806: step 3710, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.542 sec/batch; 49h:30m:52s remains)
INFO - root - 2017-12-10 05:10:17.540460: step 3720, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.517 sec/batch; 47h:13m:12s remains)
INFO - root - 2017-12-10 05:10:22.807154: step 3730, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.539 sec/batch; 49h:13m:52s remains)
INFO - root - 2017-12-10 05:10:28.051748: step 3740, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 48h:41m:29s remains)
INFO - root - 2017-12-10 05:10:33.373213: step 3750, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:18m:20s remains)
INFO - root - 2017-12-10 05:10:38.673001: step 3760, loss = 2.28, batch loss = 2.23 (16.0 examples/sec; 0.499 sec/batch; 45h:31m:52s remains)
INFO - root - 2017-12-10 05:10:43.951368: step 3770, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.532 sec/batch; 48h:34m:35s remains)
INFO - root - 2017-12-10 05:10:48.954814: step 3780, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.534 sec/batch; 48h:46m:05s remains)
INFO - root - 2017-12-10 05:10:54.262013: step 3790, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.548 sec/batch; 50h:01m:26s remains)
INFO - root - 2017-12-10 05:10:59.455539: step 3800, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:04m:51s remains)
2017-12-10 05:10:59.958365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289665 -4.4289513 -4.42894 -4.4289293 -4.4289231 -4.4289217 -4.4289236 -4.4289274 -4.428926 -4.4289207 -4.4289184 -4.4289241 -4.4289274 -4.4289308 -4.4289317][-4.4289665 -4.4289522 -4.4289393 -4.4289269 -4.4289169 -4.4289074 -4.4289031 -4.4289041 -4.4289002 -4.4288917 -4.4288888 -4.4289002 -4.4289088 -4.4289174 -4.4289227][-4.4289565 -4.4289403 -4.4289222 -4.428905 -4.4288907 -4.4288797 -4.4288735 -4.4288697 -4.428863 -4.4288535 -4.4288521 -4.4288654 -4.42888 -4.4288964 -4.4289083][-4.428947 -4.4289222 -4.4288936 -4.4288626 -4.4288416 -4.4288363 -4.4288406 -4.4288397 -4.428834 -4.4288263 -4.428823 -4.4288344 -4.428853 -4.4288774 -4.4288921][-4.4289317 -4.4288917 -4.4288435 -4.4287872 -4.4287443 -4.4287362 -4.42875 -4.4287567 -4.4287634 -4.4287748 -4.4287848 -4.4288 -4.4288278 -4.4288568 -4.4288735][-4.4289289 -4.4288731 -4.4288011 -4.4287176 -4.42864 -4.4286041 -4.4285975 -4.4285913 -4.4286113 -4.428668 -4.4287171 -4.4287553 -4.4288015 -4.42884 -4.428864][-4.4289417 -4.42888 -4.4288 -4.4286942 -4.4285774 -4.4284897 -4.4284158 -4.4283381 -4.4283423 -4.4284644 -4.4285812 -4.4286604 -4.4287357 -4.4287963 -4.4288392][-4.4289465 -4.4288964 -4.4288211 -4.4287128 -4.428576 -4.4284425 -4.4282889 -4.4280849 -4.4280162 -4.4281955 -4.4283853 -4.4285264 -4.4286461 -4.4287338 -4.4287968][-4.4289484 -4.4289165 -4.4288588 -4.4287691 -4.4286537 -4.4285336 -4.4283776 -4.4281411 -4.4280076 -4.428133 -4.42831 -4.4284678 -4.4286079 -4.4287081 -4.4287782][-4.4289479 -4.428937 -4.4289002 -4.4288411 -4.4287639 -4.4286933 -4.4285936 -4.4284377 -4.4283342 -4.4283733 -4.4284658 -4.428575 -4.4286761 -4.4287477 -4.4287992][-4.4289436 -4.4289427 -4.4289241 -4.4288883 -4.4288459 -4.4288149 -4.4287624 -4.4286819 -4.4286294 -4.4286327 -4.4286723 -4.4287319 -4.4287863 -4.4288192 -4.4288445][-4.4289379 -4.428937 -4.4289303 -4.4289064 -4.4288888 -4.4288859 -4.4288659 -4.4288363 -4.4288197 -4.428813 -4.4288282 -4.4288564 -4.4288807 -4.4288917 -4.4288993][-4.4289312 -4.4289284 -4.42893 -4.428915 -4.42891 -4.42892 -4.4289217 -4.42892 -4.4289179 -4.42891 -4.4289169 -4.4289289 -4.4289379 -4.4289355 -4.4289289][-4.4289389 -4.4289331 -4.4289346 -4.4289265 -4.4289279 -4.4289408 -4.4289522 -4.4289603 -4.4289637 -4.4289594 -4.4289608 -4.4289646 -4.4289651 -4.4289551 -4.4289422][-4.4289675 -4.4289627 -4.4289622 -4.428957 -4.428957 -4.4289632 -4.4289708 -4.428977 -4.42898 -4.4289775 -4.4289761 -4.4289765 -4.4289727 -4.4289646 -4.4289556]]...]
INFO - root - 2017-12-10 05:11:05.182382: step 3810, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.532 sec/batch; 48h:32m:43s remains)
INFO - root - 2017-12-10 05:11:10.417614: step 3820, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 47h:40m:26s remains)
INFO - root - 2017-12-10 05:11:15.725876: step 3830, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:07m:26s remains)
INFO - root - 2017-12-10 05:11:20.975627: step 3840, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 46h:41m:55s remains)
INFO - root - 2017-12-10 05:11:26.300912: step 3850, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.536 sec/batch; 48h:53m:36s remains)
INFO - root - 2017-12-10 05:11:31.521152: step 3860, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 47h:46m:38s remains)
INFO - root - 2017-12-10 05:11:36.703810: step 3870, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.508 sec/batch; 46h:23m:24s remains)
INFO - root - 2017-12-10 05:11:41.654279: step 3880, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:24m:16s remains)
INFO - root - 2017-12-10 05:11:46.955349: step 3890, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.541 sec/batch; 49h:21m:09s remains)
INFO - root - 2017-12-10 05:11:52.285745: step 3900, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:01m:21s remains)
2017-12-10 05:11:52.901402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289155 -4.4288988 -4.42889 -4.4288917 -4.4289012 -4.428915 -4.4289207 -4.4289079 -4.4288754 -4.4288421 -4.4288321 -4.428865 -4.4289308 -4.4289937 -4.4290328][-4.428915 -4.4289026 -4.4289 -4.4289041 -4.4289145 -4.4289265 -4.4289312 -4.4289246 -4.428906 -4.4288888 -4.4288878 -4.4289169 -4.4289703 -4.4290195 -4.4290466][-4.4289212 -4.4289055 -4.4288993 -4.4288983 -4.4289007 -4.4289064 -4.4289136 -4.42892 -4.4289203 -4.4289193 -4.4289279 -4.4289536 -4.4289966 -4.4290361 -4.4290566][-4.4289246 -4.4289021 -4.4288864 -4.4288726 -4.4288607 -4.4288568 -4.428865 -4.4288831 -4.4288955 -4.4289041 -4.4289155 -4.4289384 -4.42898 -4.4290237 -4.4290514][-4.428925 -4.4288993 -4.4288726 -4.428834 -4.4287949 -4.42877 -4.4287715 -4.4287958 -4.4288144 -4.4288254 -4.4288344 -4.4288588 -4.4289122 -4.428977 -4.4290266][-4.4289193 -4.4288931 -4.4288497 -4.4287753 -4.4286971 -4.4286394 -4.4286246 -4.42865 -4.4286747 -4.4286885 -4.4287014 -4.4287438 -4.4288259 -4.4289217 -4.4290004][-4.4288988 -4.4288788 -4.4288187 -4.4287062 -4.4285803 -4.4284835 -4.4284496 -4.4284787 -4.4285254 -4.4285593 -4.428597 -4.42867 -4.4287815 -4.4289012 -4.4289951][-4.428853 -4.428844 -4.4287715 -4.42863 -4.4284663 -4.4283366 -4.4282842 -4.42833 -4.4284177 -4.4284983 -4.4285789 -4.4286804 -4.4288044 -4.4289217 -4.4290066][-4.4288034 -4.4288106 -4.42874 -4.4285936 -4.4284215 -4.4282818 -4.4282227 -4.4282861 -4.4284167 -4.4285445 -4.4286571 -4.4287653 -4.4288745 -4.4289651 -4.4290247][-4.428791 -4.4288168 -4.4287658 -4.4286432 -4.4285 -4.4283848 -4.4283371 -4.4284058 -4.4285455 -4.4286823 -4.4287906 -4.4288783 -4.4289546 -4.4290094 -4.4290419][-4.4288216 -4.4288597 -4.42884 -4.4287605 -4.4286637 -4.4285836 -4.4285493 -4.4286008 -4.4287124 -4.4288268 -4.4289122 -4.4289703 -4.4290109 -4.4290338 -4.4290466][-4.4288831 -4.4289227 -4.4289236 -4.4288845 -4.42883 -4.4287786 -4.4287515 -4.428781 -4.4288559 -4.4289341 -4.4289885 -4.4290171 -4.4290295 -4.4290342 -4.4290385][-4.4289546 -4.4289823 -4.4289875 -4.4289713 -4.428946 -4.4289188 -4.4289 -4.4289122 -4.4289536 -4.4289947 -4.4290195 -4.4290276 -4.4290247 -4.4290204 -4.4290223][-4.4290018 -4.4290147 -4.4290171 -4.4290104 -4.4289989 -4.4289842 -4.4289694 -4.4289708 -4.4289904 -4.4290075 -4.4290166 -4.4290175 -4.4290113 -4.4290056 -4.4290085][-4.4290085 -4.42901 -4.4290109 -4.4290075 -4.4290018 -4.4289904 -4.4289765 -4.4289742 -4.4289861 -4.4289975 -4.4290032 -4.4290028 -4.428998 -4.4289942 -4.429]]...]
INFO - root - 2017-12-10 05:11:58.213739: step 3910, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:04m:57s remains)
INFO - root - 2017-12-10 05:12:03.380144: step 3920, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 47h:57m:00s remains)
INFO - root - 2017-12-10 05:12:08.664875: step 3930, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:23m:40s remains)
INFO - root - 2017-12-10 05:12:14.029649: step 3940, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.538 sec/batch; 49h:05m:36s remains)
INFO - root - 2017-12-10 05:12:19.423187: step 3950, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 47h:59m:15s remains)
INFO - root - 2017-12-10 05:12:24.729183: step 3960, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 47h:57m:05s remains)
INFO - root - 2017-12-10 05:12:29.980600: step 3970, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 47h:42m:20s remains)
INFO - root - 2017-12-10 05:12:34.989013: step 3980, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.448 sec/batch; 40h:54m:49s remains)
INFO - root - 2017-12-10 05:12:40.227656: step 3990, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:24m:01s remains)
INFO - root - 2017-12-10 05:12:44.633522: step 4000, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:40m:03s remains)
2017-12-10 05:12:45.030067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289808 -4.4289689 -4.4289541 -4.4289384 -4.4289169 -4.4288845 -4.4288549 -4.4288511 -4.4288716 -4.4288936 -4.428906 -4.4289236 -4.4289484 -4.4289608 -4.4289575][-4.4289384 -4.4289308 -4.428916 -4.4288831 -4.428834 -4.4287724 -4.4287162 -4.4287028 -4.4287314 -4.4287705 -4.4287987 -4.4288263 -4.4288716 -4.42891 -4.4289184][-4.4288793 -4.4288821 -4.4288673 -4.4288116 -4.428719 -4.428618 -4.4285212 -4.4284959 -4.4285412 -4.4286113 -4.4286642 -4.4287009 -4.4287591 -4.4288273 -4.4288564][-4.4288063 -4.4288192 -4.4288116 -4.4287329 -4.4285941 -4.4284277 -4.4282646 -4.4282393 -4.4283452 -4.4284816 -4.428566 -4.4286079 -4.4286723 -4.4287567 -4.4288049][-4.4287076 -4.4287333 -4.4287405 -4.428647 -4.4284544 -4.4281921 -4.4279437 -4.427947 -4.4281721 -4.4284115 -4.4285398 -4.4285951 -4.4286542 -4.4287291 -4.4287772][-4.4285674 -4.4286056 -4.4286423 -4.4285564 -4.4283285 -4.4279561 -4.4275875 -4.4276595 -4.42805 -4.4283943 -4.4285722 -4.4286404 -4.4286819 -4.4287324 -4.4287663][-4.4284225 -4.4284692 -4.4285479 -4.4285069 -4.4282918 -4.4278808 -4.42743 -4.427536 -4.428031 -4.4284239 -4.4286346 -4.4287167 -4.4287395 -4.4287519 -4.4287605][-4.4283395 -4.4283662 -4.428472 -4.4284997 -4.4283657 -4.4280663 -4.4277263 -4.427773 -4.4281511 -4.4284725 -4.4286718 -4.4287581 -4.4287705 -4.4287577 -4.4287415][-4.42834 -4.4283333 -4.4284368 -4.4285173 -4.4284759 -4.4283371 -4.4281673 -4.4281726 -4.428359 -4.42855 -4.4286962 -4.4287572 -4.4287491 -4.42872 -4.4286833][-4.4283977 -4.4283729 -4.4284625 -4.4285545 -4.4285655 -4.428525 -4.4284687 -4.4284639 -4.4285111 -4.4285994 -4.4286976 -4.4287462 -4.4287248 -4.4286842 -4.4286308][-4.4284596 -4.4284339 -4.4285154 -4.4285941 -4.4286141 -4.4286075 -4.4285994 -4.4285827 -4.4285645 -4.4285932 -4.4286752 -4.4287372 -4.4287348 -4.4287004 -4.4286494][-4.4285116 -4.4284911 -4.4285645 -4.4286151 -4.4286151 -4.4286094 -4.4286146 -4.428606 -4.4285884 -4.4286036 -4.4286757 -4.4287472 -4.4287734 -4.4287677 -4.4287333][-4.4285989 -4.4285793 -4.4286284 -4.4286485 -4.4286261 -4.4286108 -4.428616 -4.4286242 -4.4286318 -4.4286556 -4.4287181 -4.4287915 -4.4288316 -4.42884 -4.4288111][-4.4287229 -4.4287086 -4.4287333 -4.4287276 -4.4286842 -4.4286542 -4.4286537 -4.4286742 -4.4286995 -4.4287314 -4.428782 -4.4288445 -4.4288864 -4.4288917 -4.4288497][-4.4288378 -4.4288268 -4.4288378 -4.4288154 -4.4287643 -4.4287271 -4.4287167 -4.4287305 -4.4287491 -4.4287782 -4.4288168 -4.428863 -4.4289026 -4.4289012 -4.4288588]]...]
