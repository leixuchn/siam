INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "133"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.001
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv2/split:0", shape=(8, 29, 29, 48), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose:0", shape=(8, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_1:0", shape=(8, 200, 29, 29), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose_2:0", shape=(8, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_3:0", shape=(8, 200, 29, 29), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 03:22:33.132773: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:22:33.132919: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:22:33.132925: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:22:33.132929: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:22:33.132933: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:22:34.088009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-07 03:22:34.088045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 03:22:34.088051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 03:22:34.088059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-07 03:22:39.181909: step 0, loss = 0.65, batch loss = 0.58 (2.2 examples/sec; 3.573 sec/batch; 329h:59m:25s remains)
2017-12-07 03:22:39.630675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0883508 -3.0517654 -3.0899725 -3.1949348 -3.294467 -3.3647408 -3.3868814 -3.39611 -3.3583803 -3.2446737 -3.1689873 -3.1071715 -3.0118771 -2.9438758 -2.8867049][-3.0968106 -3.0881639 -3.1978397 -3.3416774 -3.4225411 -3.4981902 -3.5436893 -3.5218394 -3.4427848 -3.2896886 -3.1734829 -3.0954604 -2.9935579 -2.9200273 -2.8696151][-3.134439 -3.1637549 -3.3481898 -3.477124 -3.48341 -3.527024 -3.5170724 -3.3746498 -3.2306373 -3.0608511 -2.955977 -2.9470222 -2.9076252 -2.8851557 -2.8897784][-3.1432142 -3.2805815 -3.5661604 -3.6332083 -3.4997451 -3.4333346 -3.2361276 -2.9295323 -2.8088036 -2.7400403 -2.7232046 -2.8130803 -2.8138936 -2.8302283 -2.9071205][-3.2224684 -3.5162377 -3.8869092 -3.852777 -3.5405295 -3.2581563 -2.7636137 -2.3615088 -2.4240572 -2.5654061 -2.6663423 -2.8245065 -2.8048069 -2.8114219 -2.9423695][-3.4601474 -3.8425996 -4.1606679 -3.9263725 -3.3585458 -2.7039781 -1.7937098 -1.3477762 -1.7069316 -2.144362 -2.4444494 -2.7262812 -2.7405729 -2.7720003 -2.9709125][-3.6448812 -3.9680867 -4.104485 -3.6255884 -2.7555504 -1.6285396 -0.30127668 0.084959507 -0.66856456 -1.4625535 -2.0391827 -2.4929242 -2.5938134 -2.6918404 -2.958395][-3.5907412 -3.7788172 -3.7555137 -3.1643376 -2.1737936 -0.84071279 0.54536343 0.64029455 -0.49863935 -1.5076632 -2.2062173 -2.6530535 -2.6910639 -2.7617474 -3.0038714][-3.4468415 -3.567924 -3.500119 -2.9720702 -2.0884986 -0.86554217 0.24259424 0.035933495 -1.1373725 -2.0681152 -2.7049952 -3.0121844 -2.9140515 -2.9453974 -3.1356511][-3.3726707 -3.5015945 -3.398123 -2.9469314 -2.2461886 -1.2899652 -0.58720469 -0.95110273 -1.8689282 -2.5373869 -3.0486469 -3.2253778 -3.0314505 -3.0686979 -3.2488964][-3.3996353 -3.4795008 -3.2987237 -2.9583187 -2.5251474 -1.9490569 -1.6827657 -2.0898111 -2.6467843 -2.9773269 -3.3105223 -3.3783851 -3.1209297 -3.1510329 -3.3096879][-3.4164693 -3.3643909 -3.13329 -2.9831061 -2.8507357 -2.6251931 -2.6540098 -2.9988658 -3.2303221 -3.2926941 -3.4772961 -3.481817 -3.2078896 -3.2157812 -3.3329067][-3.4575052 -3.3126721 -3.131567 -3.1738529 -3.2179816 -3.1573129 -3.2573035 -3.4792 -3.5072923 -3.44046 -3.55357 -3.5453739 -3.2844782 -3.2681789 -3.3323593][-3.6877255 -3.5473754 -3.4701807 -3.6142814 -3.6854546 -3.6405449 -3.6827552 -3.7658045 -3.6786575 -3.5654278 -3.6445446 -3.6195889 -3.354775 -3.2884445 -3.3010826][-4.0420518 -3.9206753 -3.8894732 -4.02604 -4.0702887 -4.015316 -3.9917049 -3.9701703 -3.8096728 -3.6806462 -3.7292402 -3.656323 -3.3662014 -3.2629209 -3.2581182]]...]
sdiufhasudf Tensor("siamese_fc_1/conv2/split:0", shape=(8, 57, 57, 48), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose:0", shape=(8, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_1:0", shape=(8, 200, 57, 57), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose_2:0", shape=(8, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_3:0", shape=(8, 200, 57, 57), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 03:22:43.840734: step 10, loss = 0.74, batch loss = 0.67 (22.2 examples/sec; 0.361 sec/batch; 33h:21m:19s remains)
INFO - root - 2017-12-07 03:22:47.616345: step 20, loss = 0.63, batch loss = 0.56 (21.0 examples/sec; 0.381 sec/batch; 35h:13m:30s remains)
INFO - root - 2017-12-07 03:22:51.379773: step 30, loss = 0.73, batch loss = 0.66 (21.4 examples/sec; 0.373 sec/batch; 34h:28m:41s remains)
INFO - root - 2017-12-07 03:22:55.115471: step 40, loss = 0.84, batch loss = 0.77 (21.5 examples/sec; 0.372 sec/batch; 34h:20m:12s remains)
INFO - root - 2017-12-07 03:22:58.902169: step 50, loss = 0.80, batch loss = 0.73 (21.0 examples/sec; 0.381 sec/batch; 35h:11m:16s remains)
INFO - root - 2017-12-07 03:23:02.691705: step 60, loss = 0.95, batch loss = 0.88 (20.9 examples/sec; 0.383 sec/batch; 35h:19m:33s remains)
INFO - root - 2017-12-07 03:23:06.444936: step 70, loss = 0.84, batch loss = 0.76 (21.4 examples/sec; 0.374 sec/batch; 34h:33m:36s remains)
INFO - root - 2017-12-07 03:23:10.254815: step 80, loss = 0.67, batch loss = 0.60 (21.4 examples/sec; 0.373 sec/batch; 34h:26m:51s remains)
INFO - root - 2017-12-07 03:23:14.072752: step 90, loss = 0.90, batch loss = 0.83 (21.4 examples/sec; 0.374 sec/batch; 34h:31m:24s remains)
INFO - root - 2017-12-07 03:23:17.892516: step 100, loss = 0.86, batch loss = 0.79 (20.8 examples/sec; 0.385 sec/batch; 35h:31m:00s remains)
2017-12-07 03:23:18.284961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8147542 -2.6061025 -2.5485687 -2.6309819 -2.6425071 -2.36644 -1.7526724 -1.2114537 -1.1256974 -1.5684812 -2.2924609 -2.9154587 -3.1804383 -3.241647 -3.3203344][-2.9349217 -2.794425 -2.7243443 -2.8126092 -2.8925023 -2.7097287 -2.1668367 -1.6068001 -1.3159573 -1.501231 -2.089694 -2.6996529 -3.0575933 -3.2629967 -3.4965982][-3.0336981 -2.8545651 -2.6590538 -2.7119346 -2.8978324 -2.9693158 -2.7268982 -2.3338237 -1.8991814 -1.7440379 -2.0321791 -2.4672585 -2.8286195 -3.1351919 -3.489676][-2.96104 -2.5204091 -1.9729075 -1.8291726 -2.0212386 -2.3931351 -2.6602476 -2.7027619 -2.3822579 -2.0230432 -2.01709 -2.2171123 -2.4828334 -2.7858996 -3.1605024][-2.6693015 -2.0816929 -1.3229535 -0.99141884 -1.0691922 -1.5696423 -2.2086995 -2.6381195 -2.5074937 -2.0726986 -1.8590822 -1.8557727 -1.9980559 -2.2596714 -2.6371641][-2.242234 -1.8635509 -1.2155714 -0.81836939 -0.73074794 -1.1639724 -1.896975 -2.4561346 -2.3817265 -1.841815 -1.4582202 -1.3307242 -1.4230843 -1.7266686 -2.1803169][-1.86374 -1.6880662 -1.2849255 -0.95187521 -0.836009 -1.2035637 -1.8878093 -2.4142663 -2.2795968 -1.6027932 -1.049845 -0.82043171 -0.87440133 -1.2412212 -1.8138397][-2.0297613 -1.9982979 -1.8274953 -1.6114733 -1.58903 -1.9425828 -2.5069871 -2.9426908 -2.743608 -1.9685593 -1.2539475 -0.85501218 -0.74743772 -1.0331514 -1.6445198][-2.7268381 -2.8424637 -2.859344 -2.7367747 -2.7532492 -2.9458997 -3.2389426 -3.5105882 -3.3293614 -2.6357064 -1.9150875 -1.3743758 -1.0542042 -1.1567292 -1.7157376][-3.2579417 -3.5216739 -3.6775286 -3.5890305 -3.5117769 -3.4238842 -3.4277577 -3.5958993 -3.5328364 -3.0093462 -2.3791978 -1.7659876 -1.3111482 -1.2887819 -1.7609215][-3.2614472 -3.5946498 -3.8142869 -3.7515612 -3.5913837 -3.3406944 -3.2205691 -3.3649943 -3.4024196 -3.04129 -2.53827 -1.9558899 -1.4681425 -1.41117 -1.7711353][-2.793406 -3.0966268 -3.3498063 -3.3889098 -3.2698903 -3.0297537 -2.895566 -2.9668226 -2.9836042 -2.7089539 -2.3477197 -1.9010148 -1.5311511 -1.567981 -1.8725901][-2.3765094 -2.5911913 -2.8508248 -2.9846163 -2.9458334 -2.8105032 -2.711513 -2.6912832 -2.6108165 -2.3377271 -2.0394721 -1.7103744 -1.5108027 -1.69455 -2.0363867][-2.3784306 -2.5102696 -2.7246184 -2.8867476 -2.9179261 -2.8844552 -2.8203814 -2.7540011 -2.6387293 -2.4076369 -2.1459403 -1.8735392 -1.7627687 -1.9992089 -2.3534212][-2.7459345 -2.7794447 -2.8711085 -2.9507477 -2.9582138 -2.9588346 -2.9309421 -2.8976068 -2.8483534 -2.7180355 -2.5350196 -2.3085032 -2.2595265 -2.4768062 -2.7599587]]...]
INFO - root - 2017-12-07 03:23:22.109802: step 110, loss = 1.12, batch loss = 1.05 (21.7 examples/sec; 0.369 sec/batch; 34h:05m:42s remains)
INFO - root - 2017-12-07 03:23:26.005586: step 120, loss = 0.80, batch loss = 0.73 (20.9 examples/sec; 0.384 sec/batch; 35h:25m:01s remains)
INFO - root - 2017-12-07 03:23:29.961781: step 130, loss = 0.98, batch loss = 0.90 (16.6 examples/sec; 0.481 sec/batch; 44h:26m:26s remains)
INFO - root - 2017-12-07 03:23:33.811951: step 140, loss = 0.79, batch loss = 0.71 (21.3 examples/sec; 0.376 sec/batch; 34h:42m:35s remains)
INFO - root - 2017-12-07 03:23:37.637727: step 150, loss = 1.02, batch loss = 0.95 (21.3 examples/sec; 0.376 sec/batch; 34h:40m:44s remains)
INFO - root - 2017-12-07 03:23:41.601292: step 160, loss = 0.81, batch loss = 0.74 (19.9 examples/sec; 0.403 sec/batch; 37h:12m:15s remains)
INFO - root - 2017-12-07 03:23:47.432583: step 170, loss = 0.79, batch loss = 0.72 (16.2 examples/sec; 0.493 sec/batch; 45h:32m:38s remains)
INFO - root - 2017-12-07 03:23:54.651838: step 180, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 73h:14m:49s remains)
INFO - root - 2017-12-07 03:24:02.317862: step 190, loss = 1.03, batch loss = 0.96 (10.7 examples/sec; 0.748 sec/batch; 69h:00m:07s remains)
INFO - root - 2017-12-07 03:24:09.958853: step 200, loss = 0.90, batch loss = 0.83 (10.1 examples/sec; 0.791 sec/batch; 73h:01m:28s remains)
2017-12-07 03:24:10.617925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6677251 -3.639169 -3.6307287 -3.6457036 -3.6776104 -3.7089863 -3.7142434 -3.6968949 -3.675832 -3.6708689 -3.6809235 -3.6887653 -3.6882291 -3.690057 -3.6919947][-3.4697833 -3.4346428 -3.4370341 -3.462656 -3.5045762 -3.5360975 -3.5124948 -3.45246 -3.403636 -3.3991857 -3.43677 -3.4812343 -3.5207546 -3.563983 -3.5999479][-3.2754905 -3.2503889 -3.2825556 -3.3257113 -3.3657475 -3.3952451 -3.3648679 -3.2848196 -3.2115538 -3.2030449 -3.2566919 -3.3242116 -3.3911772 -3.4591863 -3.5163102][-3.2516062 -3.2669227 -3.353446 -3.4304631 -3.4688549 -3.4901452 -3.4248114 -3.2676377 -3.10695 -3.0494056 -3.0926659 -3.1790371 -3.2765386 -3.3654284 -3.4460294][-3.3939464 -3.4415891 -3.5711813 -3.6999545 -3.7460074 -3.7100782 -3.5026886 -3.1406879 -2.826534 -2.7190604 -2.7803893 -2.9329815 -3.093291 -3.2203512 -3.3501384][-3.6102042 -3.6155276 -3.7228184 -3.8706884 -3.8675857 -3.6589975 -3.1863658 -2.5860815 -2.2277722 -2.2427635 -2.4711809 -2.7673714 -2.9947879 -3.1230693 -3.2694838][-3.7975874 -3.725718 -3.7577229 -3.8411334 -3.6936595 -3.2112556 -2.4573755 -1.7474377 -1.5563426 -1.8631124 -2.3037879 -2.6929359 -2.9204674 -3.0216103 -3.1770158][-3.7895489 -3.6373177 -3.545671 -3.4768631 -3.1544404 -2.4884903 -1.6662121 -1.0794213 -1.1790257 -1.7445102 -2.2653079 -2.5856137 -2.73005 -2.8223019 -3.0306664][-3.6301849 -3.4119418 -3.207844 -3.005554 -2.5975704 -1.9954557 -1.3883092 -1.0934074 -1.3997924 -2.0353854 -2.4606216 -2.5803597 -2.58911 -2.6888316 -2.960649][-3.5232158 -3.2936769 -3.0828061 -2.8618114 -2.5086765 -2.123682 -1.8316798 -1.8070495 -2.1734421 -2.6959548 -2.9059048 -2.7901297 -2.6584916 -2.7478402 -3.0438437][-3.5809488 -3.4159298 -3.3117142 -3.1699452 -2.9219604 -2.6962552 -2.6018667 -2.7449336 -3.0791693 -3.3707695 -3.3187275 -3.0310245 -2.8468633 -2.9349072 -3.2203648][-3.7117269 -3.665622 -3.7011213 -3.6635013 -3.5074029 -3.3507905 -3.336031 -3.5393176 -3.7859976 -3.8421068 -3.5899277 -3.2449384 -3.0981607 -3.2022629 -3.4376628][-3.795043 -3.8384595 -3.9437079 -3.9548805 -3.850239 -3.7190385 -3.7303522 -3.9119225 -4.0497804 -3.9630935 -3.6595407 -3.3758769 -3.3151689 -3.43667 -3.6165147][-3.7676911 -3.8267443 -3.9188414 -3.9305613 -3.8623104 -3.7784982 -3.8174047 -3.9446421 -3.9919546 -3.8639898 -3.6298347 -3.4717255 -3.4863706 -3.5994651 -3.7167044][-3.7115245 -3.750066 -3.7935021 -3.7881093 -3.7416048 -3.705987 -3.7534406 -3.832577 -3.8361912 -3.7379794 -3.6122222 -3.5668902 -3.6180332 -3.697798 -3.7569389]]...]
INFO - root - 2017-12-07 03:24:18.298745: step 210, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.760 sec/batch; 70h:10m:29s remains)
INFO - root - 2017-12-07 03:24:26.003410: step 220, loss = 0.88, batch loss = 0.81 (10.7 examples/sec; 0.748 sec/batch; 68h:59m:59s remains)
INFO - root - 2017-12-07 03:24:33.681936: step 230, loss = 0.66, batch loss = 0.59 (10.0 examples/sec; 0.801 sec/batch; 73h:54m:55s remains)
INFO - root - 2017-12-07 03:24:41.380394: step 240, loss = 0.57, batch loss = 0.49 (10.5 examples/sec; 0.761 sec/batch; 70h:14m:24s remains)
INFO - root - 2017-12-07 03:24:48.998526: step 250, loss = 0.98, batch loss = 0.91 (10.5 examples/sec; 0.759 sec/batch; 70h:01m:11s remains)
INFO - root - 2017-12-07 03:24:56.683195: step 260, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 70h:18m:15s remains)
INFO - root - 2017-12-07 03:25:04.340897: step 270, loss = 0.85, batch loss = 0.78 (10.1 examples/sec; 0.793 sec/batch; 73h:13m:13s remains)
INFO - root - 2017-12-07 03:25:11.775954: step 280, loss = 0.73, batch loss = 0.65 (10.7 examples/sec; 0.746 sec/batch; 68h:48m:49s remains)
INFO - root - 2017-12-07 03:25:19.504838: step 290, loss = 0.81, batch loss = 0.74 (10.0 examples/sec; 0.801 sec/batch; 73h:56m:22s remains)
INFO - root - 2017-12-07 03:25:27.188870: step 300, loss = 1.01, batch loss = 0.93 (10.5 examples/sec; 0.759 sec/batch; 70h:03m:37s remains)
2017-12-07 03:25:27.823049: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.54133177 0.59549856 0.19871902 -0.28625917 -0.8645227 -1.3320782 -1.3565521 -1.0582614 -0.66502285 -0.27390146 0.24935961 0.69781065 0.82344818 0.88017082 0.99999857][-0.21893787 -0.20825624 -0.40173006 -0.58731651 -0.953423 -1.2785926 -1.2255974 -0.91520047 -0.55368757 -0.26499748 0.096939087 0.40892696 0.49569035 0.58245564 0.72886276][-1.2773435 -1.2911956 -1.1909614 -0.95979238 -1.0017936 -1.1431072 -1.1247611 -0.99641585 -0.80139756 -0.69054961 -0.52012229 -0.33794117 -0.25340748 -0.086244106 0.15729761][-2.1494117 -2.2272317 -1.8683295 -1.2555165 -0.99785376 -1.0052536 -1.0732043 -1.1435306 -1.1194017 -1.1594017 -1.1251264 -1.0440226 -1.0104039 -0.84004712 -0.51471639][-2.8486376 -2.9605045 -2.4215488 -1.553427 -1.0764484 -0.9521966 -1.057668 -1.2637188 -1.339025 -1.4448833 -1.4764822 -1.4490662 -1.4596791 -1.3010545 -0.95623159][-3.3152909 -3.4710574 -2.9680405 -2.1698754 -1.6941292 -1.5028963 -1.5895178 -1.8049443 -1.8575923 -1.9097474 -1.9388962 -1.9312873 -1.9233372 -1.7293434 -1.3823638][-3.2842371 -3.4295659 -3.0977254 -2.6199644 -2.3826375 -2.2504508 -2.3192565 -2.5078545 -2.5069885 -2.4544244 -2.4240017 -2.3793941 -2.2641904 -1.9985414 -1.6956706][-3.03158 -3.0011749 -2.7664089 -2.5240877 -2.4461529 -2.3108845 -2.27367 -2.4030123 -2.4356537 -2.4256136 -2.4610615 -2.456598 -2.2481215 -1.8827035 -1.5793004][-2.8108234 -2.6756096 -2.5699325 -2.5250421 -2.5159488 -2.3010101 -2.0878384 -2.1154022 -2.2191894 -2.3050349 -2.395534 -2.3689311 -2.0572021 -1.5942092 -1.2661946][-2.9285111 -2.7570975 -2.7573662 -2.8409238 -2.8778372 -2.6763976 -2.3602951 -2.2153962 -2.2232518 -2.2213941 -2.1915052 -2.0283606 -1.6576719 -1.2439129 -1.0445852][-3.0788813 -3.0424285 -3.1783652 -3.3477621 -3.4061384 -3.2564597 -2.9235876 -2.6009364 -2.3849075 -2.1488166 -1.9488957 -1.7553513 -1.4929347 -1.23354 -1.1892605][-2.4731383 -2.7724404 -3.1842453 -3.5244436 -3.69768 -3.6916811 -3.436187 -3.0321627 -2.6125636 -2.1094055 -1.7170632 -1.5316925 -1.3966193 -1.2634962 -1.3307543][-1.2591884 -1.7744501 -2.343137 -2.825604 -3.1822472 -3.3768299 -3.2987039 -2.962842 -2.4740381 -1.8187346 -1.2841012 -1.076968 -0.96890879 -0.85183239 -0.92506695][-0.30023766 -0.80106807 -1.3372653 -1.8819456 -2.3600755 -2.6540012 -2.6727672 -2.4069808 -1.9018018 -1.2236052 -0.70018721 -0.56125212 -0.52890539 -0.4447298 -0.46065998][0.047924519 -0.25476408 -0.61041665 -1.086462 -1.5788045 -1.9241002 -2.0245125 -1.8423498 -1.3796656 -0.75673175 -0.25878382 -0.14062071 -0.22699785 -0.27978611 -0.32936478]]...]
INFO - root - 2017-12-07 03:25:35.522722: step 310, loss = 0.92, batch loss = 0.84 (10.6 examples/sec; 0.758 sec/batch; 69h:54m:35s remains)
INFO - root - 2017-12-07 03:25:43.136130: step 320, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.768 sec/batch; 70h:49m:21s remains)
INFO - root - 2017-12-07 03:25:50.732593: step 330, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.768 sec/batch; 70h:50m:55s remains)
INFO - root - 2017-12-07 03:25:58.412736: step 340, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.770 sec/batch; 71h:02m:23s remains)
INFO - root - 2017-12-07 03:26:06.062565: step 350, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.770 sec/batch; 71h:05m:14s remains)
INFO - root - 2017-12-07 03:26:13.807043: step 360, loss = 0.91, batch loss = 0.84 (10.7 examples/sec; 0.749 sec/batch; 69h:07m:40s remains)
INFO - root - 2017-12-07 03:26:21.392744: step 370, loss = 0.96, batch loss = 0.89 (10.5 examples/sec; 0.761 sec/batch; 70h:14m:24s remains)
INFO - root - 2017-12-07 03:26:28.825757: step 380, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.784 sec/batch; 72h:22m:13s remains)
INFO - root - 2017-12-07 03:26:36.407725: step 390, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.767 sec/batch; 70h:43m:53s remains)
INFO - root - 2017-12-07 03:26:44.055219: step 400, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.773 sec/batch; 71h:18m:44s remains)
2017-12-07 03:26:44.713139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5106425 -2.5951035 -2.7562914 -2.9268064 -2.9873991 -2.9307756 -2.86478 -2.8772407 -2.9872386 -3.2864132 -3.6949878 -3.9503391 -4.0492525 -3.986974 -3.7141681][-2.6324632 -2.74864 -2.9473779 -3.1691861 -3.2606015 -3.1800232 -3.0639927 -3.0517182 -3.1950045 -3.5209267 -3.9152381 -4.1386571 -4.2030354 -4.1244693 -3.8010912][-3.0025477 -3.1940708 -3.4397197 -3.6758661 -3.7499578 -3.6083055 -3.41728 -3.3598273 -3.5027478 -3.8104396 -4.1156311 -4.2240119 -4.1691561 -4.0139704 -3.6553583][-3.0457072 -3.3182697 -3.6307735 -3.9254808 -4.0692987 -3.9686027 -3.7789924 -3.6936166 -3.7930834 -4.0452375 -4.2799292 -4.3321996 -4.190763 -3.9257975 -3.4901257][-2.450866 -2.6744876 -2.9340444 -3.2130151 -3.451009 -3.5395195 -3.5546174 -3.6094921 -3.7357817 -3.9330988 -4.0933175 -4.1103773 -3.9519935 -3.6629705 -3.2292392][-1.868155 -1.941911 -1.9598198 -1.9883192 -2.0999582 -2.2394621 -2.4458745 -2.7511082 -3.0984302 -3.4117775 -3.577827 -3.5651565 -3.4207749 -3.199172 -2.880363][-1.9401975 -1.9434128 -1.8041158 -1.6245251 -1.5538933 -1.5856113 -1.7687562 -2.127996 -2.5887036 -2.9997282 -3.1813254 -3.1253889 -2.9700737 -2.8204279 -2.6348743][-2.4485638 -2.4717762 -2.3004117 -2.0549817 -1.9115663 -1.8444827 -1.8842423 -2.0891128 -2.4488888 -2.7887621 -2.9217682 -2.8378191 -2.7058291 -2.6506462 -2.5876722][-2.8795834 -3.0202866 -2.917202 -2.7015018 -2.5753379 -2.4885082 -2.4120889 -2.4071617 -2.5199516 -2.6281927 -2.6372368 -2.5309565 -2.4583423 -2.52446 -2.6012301][-2.6787772 -2.9489417 -3.0100393 -2.9463134 -2.9580307 -2.9797442 -2.9216328 -2.8319981 -2.7612689 -2.6602681 -2.5372815 -2.3882651 -2.3401608 -2.4760482 -2.6552405][-2.2982659 -2.5783505 -2.752037 -2.8377721 -2.9880838 -3.1407528 -3.2056437 -3.2210631 -3.1897349 -3.0789313 -2.9254313 -2.7407918 -2.6371915 -2.7031872 -2.845556][-2.5852935 -2.8329244 -3.0264759 -3.1597548 -3.3250031 -3.4980254 -3.6235385 -3.728775 -3.7738447 -3.7236831 -3.602082 -3.4045019 -3.2141957 -3.1366491 -3.1548085][-3.3616893 -3.5185418 -3.6212082 -3.6976969 -3.7982998 -3.8971052 -3.971658 -4.045352 -4.0807366 -4.0533462 -3.9777675 -3.8301086 -3.6340072 -3.4822886 -3.4151382][-3.9188008 -4.0009542 -4.0072975 -4.0107875 -4.0375686 -4.0593057 -4.0701189 -4.082551 -4.0727835 -4.0317259 -3.9792171 -3.8863492 -3.7428386 -3.600512 -3.5214138][-3.8163798 -3.8876503 -3.88207 -3.8661351 -3.862462 -3.8579559 -3.8616016 -3.8733726 -3.8719347 -3.8436825 -3.8059726 -3.7445028 -3.6563063 -3.5633781 -3.510082]]...]
INFO - root - 2017-12-07 03:26:52.272628: step 410, loss = 0.77, batch loss = 0.69 (10.7 examples/sec; 0.748 sec/batch; 68h:58m:07s remains)
INFO - root - 2017-12-07 03:26:59.939006: step 420, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.758 sec/batch; 69h:53m:33s remains)
INFO - root - 2017-12-07 03:27:07.528234: step 430, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.769 sec/batch; 70h:56m:14s remains)
INFO - root - 2017-12-07 03:27:15.236571: step 440, loss = 0.98, batch loss = 0.91 (10.5 examples/sec; 0.761 sec/batch; 70h:12m:31s remains)
INFO - root - 2017-12-07 03:27:22.874382: step 450, loss = 0.98, batch loss = 0.90 (10.5 examples/sec; 0.762 sec/batch; 70h:17m:48s remains)
INFO - root - 2017-12-07 03:27:30.526370: step 460, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.756 sec/batch; 69h:41m:11s remains)
INFO - root - 2017-12-07 03:27:38.201051: step 470, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.783 sec/batch; 72h:10m:52s remains)
INFO - root - 2017-12-07 03:27:45.709154: step 480, loss = 0.70, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 69h:41m:00s remains)
INFO - root - 2017-12-07 03:27:53.394390: step 490, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.779 sec/batch; 71h:51m:14s remains)
INFO - root - 2017-12-07 03:28:01.057152: step 500, loss = 0.88, batch loss = 0.81 (10.3 examples/sec; 0.773 sec/batch; 71h:19m:02s remains)
2017-12-07 03:28:01.690355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6903627 -1.6847034 -1.8792293 -2.3678391 -2.8708587 -3.1466699 -3.2532482 -3.0542085 -2.4080458 -1.780257 -1.5690958 -1.6802082 -1.8621249 -2.2130537 -2.6712084][-1.5744033 -1.6881976 -2.1222148 -2.6828845 -3.1950042 -3.6527677 -3.9985139 -4.0052924 -3.4790063 -2.7900534 -2.147274 -1.629796 -1.5359881 -2.0266795 -2.7008576][-1.6858816 -2.0234342 -2.5797358 -3.0111663 -3.38383 -3.8835459 -4.3606534 -4.5587234 -4.3093982 -3.7227263 -2.8383548 -1.8739617 -1.5449915 -2.08133 -2.8158402][-2.2165222 -2.6625614 -3.0567603 -3.18686 -3.2706058 -3.5049472 -3.7755327 -4.0116725 -4.1788173 -3.9350917 -3.1688707 -2.1905324 -1.8439567 -2.2738874 -2.7962685][-2.7261372 -3.1129458 -3.1803493 -3.0654237 -2.9485412 -2.7363274 -2.4809451 -2.5863113 -3.2037616 -3.4801614 -3.0460658 -2.3473966 -2.1247776 -2.4023049 -2.6010962][-2.9439092 -3.1172061 -2.8913567 -2.7149262 -2.5335705 -1.8296208 -0.87939787 -0.75828576 -1.8496122 -2.7667358 -2.75283 -2.3677332 -2.2815511 -2.4453969 -2.4300108][-3.0244222 -2.89083 -2.475821 -2.306309 -2.0712712 -0.99953818 0.526484 0.91614962 -0.53958631 -2.0396888 -2.4660368 -2.3539026 -2.352174 -2.4453897 -2.3953533][-3.173682 -2.6694984 -2.0812521 -1.8370025 -1.4927077 -0.35390091 1.2178702 1.5411048 -0.081748009 -1.7716956 -2.3779669 -2.3871446 -2.3625267 -2.5003691 -2.6003013][-3.319613 -2.6761894 -2.0634434 -1.7288778 -1.2931862 -0.32145405 0.83421183 0.8455658 -0.63199019 -2.119767 -2.5924926 -2.4529753 -2.2587676 -2.466439 -2.7778749][-3.3438668 -2.8735092 -2.4700511 -2.2037063 -1.8124607 -1.091543 -0.39311123 -0.61268163 -1.7456868 -2.6899445 -2.7926903 -2.33318 -1.9054401 -2.1746466 -2.6140437][-3.1813326 -2.9640131 -2.83847 -2.733427 -2.5242686 -2.1005285 -1.7507544 -2.042978 -2.8191562 -3.23894 -2.9715052 -2.1849554 -1.545923 -1.7910812 -2.20516][-3.0072279 -2.9788094 -3.0226419 -3.0422692 -3.0012143 -2.7781363 -2.5553491 -2.8092394 -3.3612635 -3.4770377 -3.0078998 -2.0936091 -1.4182355 -1.5798693 -1.8821452][-3.0067067 -3.061949 -3.0936589 -3.0894523 -3.0930176 -2.9694457 -2.7759027 -2.9304845 -3.3343468 -3.356472 -2.902267 -2.132858 -1.6283376 -1.7330861 -1.9064207][-3.1148577 -3.1703858 -3.098331 -2.9813862 -2.9310632 -2.8122497 -2.6157277 -2.6944242 -3.0397413 -3.1729116 -2.9395738 -2.4501967 -2.1045198 -2.086863 -2.1027877][-3.2127247 -3.2751074 -3.1406145 -2.9548337 -2.8479805 -2.7059624 -2.5259082 -2.5751081 -2.8933358 -3.1554151 -3.1458168 -2.8650076 -2.5739148 -2.4070354 -2.2834151]]...]
INFO - root - 2017-12-07 03:28:09.483994: step 510, loss = 0.82, batch loss = 0.75 (10.2 examples/sec; 0.781 sec/batch; 71h:59m:20s remains)
INFO - root - 2017-12-07 03:28:17.249323: step 520, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.751 sec/batch; 69h:15m:01s remains)
INFO - root - 2017-12-07 03:28:24.841264: step 530, loss = 0.61, batch loss = 0.54 (10.6 examples/sec; 0.752 sec/batch; 69h:21m:42s remains)
INFO - root - 2017-12-07 03:28:32.424826: step 540, loss = 0.71, batch loss = 0.64 (10.7 examples/sec; 0.746 sec/batch; 68h:45m:28s remains)
INFO - root - 2017-12-07 03:28:40.056726: step 550, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.769 sec/batch; 70h:54m:32s remains)
INFO - root - 2017-12-07 03:28:47.709810: step 560, loss = 0.65, batch loss = 0.58 (10.8 examples/sec; 0.738 sec/batch; 68h:04m:54s remains)
INFO - root - 2017-12-07 03:28:55.354506: step 570, loss = 0.86, batch loss = 0.79 (10.8 examples/sec; 0.743 sec/batch; 68h:30m:06s remains)
INFO - root - 2017-12-07 03:29:02.938213: step 580, loss = 0.82, batch loss = 0.74 (10.4 examples/sec; 0.773 sec/batch; 71h:13m:45s remains)
INFO - root - 2017-12-07 03:29:10.617628: step 590, loss = 1.01, batch loss = 0.94 (10.6 examples/sec; 0.755 sec/batch; 69h:39m:05s remains)
INFO - root - 2017-12-07 03:29:18.323860: step 600, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.786 sec/batch; 72h:30m:23s remains)
2017-12-07 03:29:18.961025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.38485 -3.3885264 -3.2758472 -3.2135944 -3.3079767 -3.4055402 -3.3617868 -3.1260266 -2.9588547 -2.9465382 -3.0402145 -3.168282 -3.1488802 -2.8404574 -2.3474288][-3.0112052 -3.075248 -2.9822259 -2.9429579 -3.0799794 -3.111856 -2.9114304 -2.5814552 -2.4232674 -2.4865394 -2.5698886 -2.6300101 -2.5601504 -2.3188906 -1.9775062][-2.92274 -2.9944079 -2.9353814 -2.9872098 -3.19139 -3.1525497 -2.7941198 -2.4202521 -2.3350275 -2.4763036 -2.5298538 -2.4680672 -2.2252162 -1.9074559 -1.5675168][-2.5688398 -2.7026525 -2.7565253 -2.9981837 -3.3403549 -3.2999663 -2.8703134 -2.5196986 -2.5542011 -2.7783914 -2.8438635 -2.6964417 -2.2998855 -1.8558736 -1.3973405][-2.0715575 -2.2725785 -2.4775414 -2.8735194 -3.2718902 -3.231529 -2.8310366 -2.593534 -2.7802496 -3.0734668 -3.1528277 -2.9755754 -2.4912233 -1.9492338 -1.3851278][-1.5649216 -1.7248752 -1.9691598 -2.4000838 -2.8065023 -2.8339624 -2.5828929 -2.4865661 -2.7108374 -2.9267063 -2.950232 -2.7709639 -2.3325231 -1.8392565 -1.2853093][-0.69764805 -0.7073307 -0.88392448 -1.3001657 -1.7883792 -2.0140076 -2.0102315 -2.0717902 -2.2621984 -2.3558376 -2.3368917 -2.1979289 -1.9174216 -1.6113901 -1.1601212][0.042342663 0.18814468 0.097719669 -0.30603838 -0.90385866 -1.3377707 -1.5321386 -1.6704266 -1.7821696 -1.8066833 -1.7874084 -1.6899245 -1.5652866 -1.4428642 -1.0956106][0.40917015 0.5834446 0.5571084 0.21568346 -0.37451982 -0.84639025 -1.1003211 -1.2596018 -1.3330443 -1.3515763 -1.3459396 -1.2695096 -1.2409954 -1.2564738 -1.0562036][0.17286968 0.31533051 0.35014057 0.11545849 -0.30187607 -0.62316728 -0.79021406 -0.90920162 -1.020859 -1.1235926 -1.1415427 -1.0447366 -1.009125 -1.0370891 -0.91027093][-0.33164263 -0.17759466 -0.091215611 -0.26926994 -0.57066178 -0.77529335 -0.84337521 -0.87219024 -0.97352123 -1.0729959 -1.0617409 -0.97338939 -0.92195559 -0.88969946 -0.7509954][-0.59723234 -0.41179466 -0.2780304 -0.42229366 -0.66425681 -0.86485314 -0.91436553 -0.9148438 -1.0112083 -1.0435278 -0.98142958 -0.91553307 -0.86006236 -0.75849295 -0.59621882][-0.75608635 -0.57787275 -0.42510128 -0.51467013 -0.66702628 -0.85770917 -0.90113544 -0.91406727 -1.007772 -0.970217 -0.88325214 -0.86332679 -0.82317829 -0.68173122 -0.53079677][-0.81871367 -0.64497423 -0.49838614 -0.54911566 -0.61028838 -0.767555 -0.80428934 -0.84743953 -0.93584466 -0.8550117 -0.79250312 -0.84139895 -0.83785152 -0.69599485 -0.55968475][-0.85946059 -0.73820472 -0.65283036 -0.68465614 -0.6533947 -0.74772096 -0.77333117 -0.82792544 -0.90551448 -0.8146286 -0.7684021 -0.83726811 -0.8596828 -0.74090886 -0.59884119]]...]
INFO - root - 2017-12-07 03:29:26.568400: step 610, loss = 0.87, batch loss = 0.80 (10.8 examples/sec; 0.742 sec/batch; 68h:25m:38s remains)
INFO - root - 2017-12-07 03:29:34.153620: step 620, loss = 0.61, batch loss = 0.54 (10.6 examples/sec; 0.752 sec/batch; 69h:19m:54s remains)
INFO - root - 2017-12-07 03:29:41.820272: step 630, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.784 sec/batch; 72h:18m:13s remains)
INFO - root - 2017-12-07 03:29:49.474487: step 640, loss = 0.78, batch loss = 0.70 (10.6 examples/sec; 0.754 sec/batch; 69h:31m:09s remains)
INFO - root - 2017-12-07 03:29:57.228084: step 650, loss = 0.79, batch loss = 0.71 (10.4 examples/sec; 0.771 sec/batch; 71h:03m:48s remains)
INFO - root - 2017-12-07 03:30:04.897511: step 660, loss = 0.80, batch loss = 0.72 (10.3 examples/sec; 0.775 sec/batch; 71h:28m:10s remains)
INFO - root - 2017-12-07 03:30:12.450741: step 670, loss = 0.96, batch loss = 0.88 (10.7 examples/sec; 0.746 sec/batch; 68h:43m:09s remains)
INFO - root - 2017-12-07 03:30:19.848597: step 680, loss = 1.08, batch loss = 1.01 (10.2 examples/sec; 0.788 sec/batch; 72h:36m:23s remains)
INFO - root - 2017-12-07 03:30:27.574858: step 690, loss = 0.86, batch loss = 0.78 (10.6 examples/sec; 0.752 sec/batch; 69h:20m:35s remains)
INFO - root - 2017-12-07 03:30:35.377546: step 700, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.785 sec/batch; 72h:20m:08s remains)
2017-12-07 03:30:35.966767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.93321 -2.9517632 -2.963387 -2.9734139 -2.981889 -2.9866772 -2.9895167 -2.9852161 -2.9629736 -2.9312134 -2.8882051 -2.8417869 -2.7999966 -2.761116 -2.7671165][-3.1050529 -3.1471114 -3.1732149 -3.1815331 -3.1767497 -3.1542442 -3.1192398 -3.086303 -3.0394647 -2.9950371 -2.9565926 -2.9108438 -2.8660147 -2.8088083 -2.7724564][-3.2060218 -3.297401 -3.3434772 -3.3492131 -3.3275185 -3.2682545 -3.1831789 -3.1171508 -3.0480256 -2.9876573 -2.9569352 -2.9221163 -2.8959975 -2.8545656 -2.80879][-3.004669 -3.1610241 -3.2500014 -3.2779627 -3.266778 -3.2016907 -3.1011517 -3.0274315 -2.9481535 -2.86965 -2.82576 -2.7664018 -2.7239938 -2.6906424 -2.6574225][-2.6543407 -2.8279204 -2.944293 -3.0059354 -3.0341921 -3.0109665 -2.9576554 -2.9304347 -2.8916841 -2.8358407 -2.7887664 -2.6838157 -2.5736485 -2.499521 -2.4465926][-2.3101676 -2.417063 -2.4999671 -2.5561156 -2.6119163 -2.6328883 -2.6438525 -2.6855972 -2.7270942 -2.7653713 -2.7851281 -2.6959448 -2.5543756 -2.4418602 -2.3453519][-2.0792711 -2.1100409 -2.1310141 -2.1490231 -2.1936374 -2.2131517 -2.230427 -2.279073 -2.3463662 -2.4638431 -2.5781302 -2.5796328 -2.5037532 -2.4366889 -2.3534133][-2.1582954 -2.1832674 -2.1719184 -2.1383433 -2.1211979 -2.0749893 -2.0295281 -2.0114965 -2.026834 -2.1489592 -2.305716 -2.3878949 -2.4029856 -2.413569 -2.388917][-2.2923703 -2.3780212 -2.4145265 -2.4050932 -2.389878 -2.3399205 -2.2786341 -2.2002301 -2.1193857 -2.1607382 -2.2571971 -2.3093348 -2.3258984 -2.3525879 -2.3665948][-2.4359405 -2.5485163 -2.6348162 -2.6855621 -2.726862 -2.7422948 -2.7391021 -2.6822193 -2.5765014 -2.5522926 -2.5509052 -2.5017972 -2.4383836 -2.4031465 -2.3944902][-2.6190913 -2.6745644 -2.7265413 -2.7655563 -2.7992082 -2.810216 -2.8194156 -2.8035445 -2.7496424 -2.7454047 -2.7267668 -2.6497021 -2.5596776 -2.4821038 -2.4479818][-2.6473384 -2.6391249 -2.6506844 -2.6792698 -2.7128563 -2.7177179 -2.71452 -2.7077665 -2.6776648 -2.682457 -2.6634245 -2.6061783 -2.5477967 -2.4839358 -2.463068][-2.5424016 -2.5104589 -2.5062041 -2.5265446 -2.5495038 -2.544836 -2.5335426 -2.5289736 -2.5098457 -2.5221691 -2.5193465 -2.5015061 -2.4815907 -2.440382 -2.4396052][-2.4507775 -2.4276121 -2.4296172 -2.4567432 -2.4711952 -2.4582438 -2.4315383 -2.403836 -2.37011 -2.3656585 -2.3675013 -2.3751643 -2.3888884 -2.3711014 -2.3849201][-2.5076103 -2.4990363 -2.5186663 -2.5670714 -2.5981414 -2.6081219 -2.6017253 -2.5826597 -2.5553393 -2.5386786 -2.5253034 -2.5196958 -2.5282671 -2.5091729 -2.5137267]]...]
INFO - root - 2017-12-07 03:30:43.603311: step 710, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.768 sec/batch; 70h:46m:54s remains)
INFO - root - 2017-12-07 03:30:51.392931: step 720, loss = 0.71, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 71h:32m:14s remains)
INFO - root - 2017-12-07 03:30:59.058419: step 730, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.755 sec/batch; 69h:32m:31s remains)
INFO - root - 2017-12-07 03:31:06.702259: step 740, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.747 sec/batch; 68h:50m:16s remains)
INFO - root - 2017-12-07 03:31:14.331463: step 750, loss = 0.66, batch loss = 0.59 (10.5 examples/sec; 0.758 sec/batch; 69h:53m:47s remains)
INFO - root - 2017-12-07 03:31:21.897992: step 760, loss = 1.03, batch loss = 0.96 (10.6 examples/sec; 0.755 sec/batch; 69h:36m:37s remains)
INFO - root - 2017-12-07 03:31:29.472695: step 770, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.753 sec/batch; 69h:25m:04s remains)
INFO - root - 2017-12-07 03:31:36.855900: step 780, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.746 sec/batch; 68h:43m:31s remains)
INFO - root - 2017-12-07 03:31:44.453843: step 790, loss = 0.92, batch loss = 0.85 (10.5 examples/sec; 0.765 sec/batch; 70h:29m:25s remains)
INFO - root - 2017-12-07 03:31:52.070649: step 800, loss = 1.02, batch loss = 0.95 (10.4 examples/sec; 0.771 sec/batch; 71h:02m:42s remains)
2017-12-07 03:31:52.625473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4105668 -3.3808289 -3.3502359 -3.4682236 -3.4801395 -3.0766258 -2.5934329 -2.4766903 -2.7303019 -2.9535077 -2.8853087 -2.792742 -2.6429203 -2.3276207 -2.2280648][-3.44696 -3.3931971 -3.3548818 -3.4528048 -3.401866 -2.9764814 -2.5563934 -2.4789042 -2.718333 -2.9037538 -2.7770259 -2.6133642 -2.4040406 -2.0346527 -1.9782054][-3.4443736 -3.3850577 -3.3524327 -3.4222298 -3.3123045 -2.876832 -2.4941921 -2.3641164 -2.4588592 -2.5252318 -2.3675666 -2.2547836 -2.1270785 -1.8040345 -1.822129][-3.409739 -3.3804932 -3.3722827 -3.3938313 -3.2226624 -2.8013048 -2.4358354 -2.1874566 -2.0670836 -2.0311086 -1.9100516 -1.9118407 -1.9165413 -1.6814396 -1.7605999][-3.3498588 -3.3107696 -3.2964683 -3.2663789 -3.0780017 -2.7388721 -2.4642873 -2.1816628 -1.9487121 -1.9206316 -1.8535745 -1.8869979 -1.932452 -1.7392497 -1.8733685][-3.2445581 -3.14291 -3.0464008 -2.9262943 -2.730587 -2.5240955 -2.4350882 -2.241574 -2.0486956 -2.1344612 -2.1135836 -2.0796797 -2.041048 -1.8090866 -1.9649725][-3.0790296 -2.9553051 -2.7789395 -2.55412 -2.2760491 -2.0374956 -1.95557 -1.7379336 -1.5845609 -1.8637424 -1.9985769 -2.0243542 -1.9737015 -1.6906903 -1.7932165][-3.0019414 -2.9230452 -2.7673011 -2.5424869 -2.1979198 -1.796222 -1.4886119 -1.0509446 -0.84208131 -1.2808001 -1.6101522 -1.8125856 -1.8556798 -1.5838971 -1.620723][-3.0515928 -3.0303802 -2.9777641 -2.8914976 -2.6293159 -2.2225833 -1.8313069 -1.2869427 -1.0378156 -1.4332008 -1.7352898 -1.9593318 -2.0263093 -1.7613621 -1.7224731][-3.1780152 -3.1614828 -3.1897612 -3.2685246 -3.1953335 -3.0041525 -2.7842827 -2.3199487 -2.0684783 -2.2750919 -2.3833027 -2.5249908 -2.5863686 -2.3568242 -2.26598][-3.3882787 -3.3386765 -3.3660493 -3.4812193 -3.4935892 -3.4813981 -3.4497979 -3.1648519 -3.0191658 -3.1295304 -3.1031728 -3.1636906 -3.1989889 -3.0117812 -2.9015002][-3.5443676 -3.4935935 -3.5182242 -3.5888238 -3.5333798 -3.4838638 -3.4592087 -3.3000803 -3.2890611 -3.4053402 -3.3532064 -3.3691604 -3.3881512 -3.2654274 -3.1686525][-3.4455924 -3.4222503 -3.4738586 -3.5226974 -3.4171848 -3.30063 -3.2175858 -3.1037555 -3.1383145 -3.225956 -3.1693902 -3.1552539 -3.1660275 -3.1023445 -3.0042617][-3.1029282 -3.1155477 -3.2029014 -3.262836 -3.1802042 -3.0805144 -2.9962108 -2.9162347 -2.9464893 -2.9811797 -2.9234307 -2.8942418 -2.8904152 -2.8503366 -2.7533689][-2.9449241 -2.9661074 -3.0516994 -3.1131716 -3.0855391 -3.0425963 -2.9927435 -2.9440804 -2.9547229 -2.9518917 -2.908576 -2.8886523 -2.878983 -2.8576734 -2.7859869]]...]
INFO - root - 2017-12-07 03:32:00.240876: step 810, loss = 0.86, batch loss = 0.79 (10.9 examples/sec; 0.735 sec/batch; 67h:45m:31s remains)
INFO - root - 2017-12-07 03:32:07.927255: step 820, loss = 0.60, batch loss = 0.53 (10.3 examples/sec; 0.777 sec/batch; 71h:35m:57s remains)
INFO - root - 2017-12-07 03:32:15.561592: step 830, loss = 0.71, batch loss = 0.64 (10.8 examples/sec; 0.737 sec/batch; 67h:56m:19s remains)
INFO - root - 2017-12-07 03:32:23.137507: step 840, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.758 sec/batch; 69h:52m:33s remains)
INFO - root - 2017-12-07 03:32:30.741045: step 850, loss = 0.67, batch loss = 0.60 (10.6 examples/sec; 0.756 sec/batch; 69h:38m:22s remains)
INFO - root - 2017-12-07 03:32:38.348898: step 860, loss = 0.80, batch loss = 0.72 (10.7 examples/sec; 0.748 sec/batch; 68h:57m:09s remains)
INFO - root - 2017-12-07 03:32:45.953769: step 870, loss = 0.75, batch loss = 0.67 (10.6 examples/sec; 0.757 sec/batch; 69h:42m:47s remains)
INFO - root - 2017-12-07 03:32:53.428405: step 880, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.786 sec/batch; 72h:23m:04s remains)
INFO - root - 2017-12-07 03:33:01.014599: step 890, loss = 0.85, batch loss = 0.78 (11.0 examples/sec; 0.724 sec/batch; 66h:43m:00s remains)
INFO - root - 2017-12-07 03:33:08.867779: step 900, loss = 0.64, batch loss = 0.57 (10.6 examples/sec; 0.757 sec/batch; 69h:44m:45s remains)
2017-12-07 03:33:09.437897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9019337 -2.5243902 -2.1561215 -1.8464663 -1.2644148 -0.64488769 -0.61418009 -1.1417041 -1.6220281 -1.6508665 -1.435077 -1.5166564 -1.6802809 -1.6319647 -1.6676471][-2.1542885 -2.0563569 -1.9815779 -1.7818661 -1.0103688 0.031555653 0.25862408 -0.54054856 -1.4432745 -1.6837211 -1.4836848 -1.5102954 -1.6469412 -1.5337186 -1.5192249][-1.0461624 -1.193944 -1.3989012 -1.4694443 -0.88579059 0.12118435 0.31800795 -0.64458561 -1.8212659 -2.1798391 -1.8858097 -1.7441335 -1.7879748 -1.5733552 -1.3685963][0.27721596 -0.16321135 -0.82406878 -1.410356 -1.258091 -0.45746017 -0.19158506 -0.97627115 -2.1019676 -2.5193591 -2.1641443 -1.8030794 -1.6780663 -1.3937831 -1.0602949][0.69547844 0.0079989433 -0.98041821 -1.8762672 -1.858479 -1.0037379 -0.41684818 -0.8246429 -1.8004084 -2.3802841 -2.1932678 -1.7876971 -1.5915871 -1.345695 -0.98436689][-0.12727165 -0.63244319 -1.4249475 -2.1899068 -2.0419495 -1.0317485 -0.11778259 -0.09318924 -0.89270186 -1.6388893 -1.7775524 -1.5262411 -1.2830019 -1.0557139 -0.67176127][-0.63547754 -0.81973886 -1.2315514 -1.6628785 -1.3388312 -0.288383 0.79199219 1.0823965 0.30741072 -0.69427037 -1.2578011 -1.2859137 -1.0255399 -0.76648784 -0.40636969][-0.85408616 -0.85195947 -0.87286305 -0.85056257 -0.29851627 0.69806337 1.7248588 1.9054475 0.91060066 -0.36203575 -1.1472487 -1.2272689 -0.77749133 -0.38814211 -0.01263237][-1.1875558 -1.1326237 -0.91353416 -0.54435039 0.17253685 1.0745144 1.8784709 1.7967381 0.64226246 -0.68510962 -1.413435 -1.2700753 -0.54239106 0.0392313 0.47947931][-1.4103279 -1.435317 -1.3111622 -0.97698951 -0.40441704 0.25093269 0.80047655 0.68983364 -0.21793509 -1.2772028 -1.8037732 -1.456296 -0.65874434 -0.076623917 0.31917381][-2.0016818 -2.1064897 -2.1086264 -1.7276962 -1.1189449 -0.52453136 -0.1231184 -0.1892314 -0.77565694 -1.5151794 -1.8446321 -1.4045436 -0.63166881 -0.14961386 0.10900116][-2.6390307 -2.8053679 -2.7979336 -2.2340863 -1.4158235 -0.75079608 -0.45465779 -0.47420526 -0.82180738 -1.3588748 -1.593652 -1.212091 -0.50500846 -0.069991589 0.12190819][-2.6733191 -3.017477 -3.134732 -2.6616554 -1.9182425 -1.3092639 -1.0159447 -0.86156154 -0.94653344 -1.2761431 -1.4523616 -1.2212601 -0.61268711 -0.10715628 0.24064016][-2.1795368 -2.624949 -2.99366 -3.0053039 -2.7859457 -2.5103488 -2.2577391 -1.8533933 -1.5586438 -1.5875871 -1.7090018 -1.6709025 -1.227659 -0.59443808 0.052874088][-1.7448199 -2.0127964 -2.3636639 -2.6660256 -2.9007807 -3.0991387 -3.243098 -3.1198368 -2.9007046 -2.8160138 -2.7916043 -2.7196527 -2.224514 -1.3352995 -0.35506773]]...]
INFO - root - 2017-12-07 03:33:17.115401: step 910, loss = 0.85, batch loss = 0.78 (10.1 examples/sec; 0.792 sec/batch; 72h:54m:43s remains)
INFO - root - 2017-12-07 03:33:24.734323: step 920, loss = 1.02, batch loss = 0.95 (10.3 examples/sec; 0.776 sec/batch; 71h:31m:05s remains)
INFO - root - 2017-12-07 03:33:32.413297: step 930, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.768 sec/batch; 70h:41m:29s remains)
INFO - root - 2017-12-07 03:33:40.073436: step 940, loss = 0.70, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 70h:52m:40s remains)
INFO - root - 2017-12-07 03:33:47.818402: step 950, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.791 sec/batch; 72h:48m:10s remains)
INFO - root - 2017-12-07 03:33:55.580903: step 960, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.766 sec/batch; 70h:35m:13s remains)
INFO - root - 2017-12-07 03:34:03.368326: step 970, loss = 0.84, batch loss = 0.77 (10.0 examples/sec; 0.803 sec/batch; 73h:59m:06s remains)
INFO - root - 2017-12-07 03:34:10.918651: step 980, loss = 0.70, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 71h:23m:55s remains)
INFO - root - 2017-12-07 03:34:18.629603: step 990, loss = 0.62, batch loss = 0.55 (10.6 examples/sec; 0.758 sec/batch; 69h:47m:31s remains)
INFO - root - 2017-12-07 03:34:26.318908: step 1000, loss = 0.67, batch loss = 0.60 (10.5 examples/sec; 0.760 sec/batch; 69h:57m:56s remains)
2017-12-07 03:34:26.924784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.23326302 -0.17385006 -0.25885677 -0.23517609 -0.1465435 -0.13413286 -0.19286585 -0.34593868 -0.49707484 -0.32464075 -0.19067955 -0.31041908 -0.40385056 -0.62472081 -0.9210968][-0.2054348 -0.13058329 -0.15794563 -0.10353374 -0.017857552 -0.050787449 -0.12423182 -0.27448654 -0.45414233 -0.28361845 -0.20377636 -0.42066526 -0.5295701 -0.72451258 -0.94840479][-0.24812174 -0.15986729 -0.1430254 -0.029777527 0.023724556 -0.13335085 -0.25472355 -0.41035557 -0.64180708 -0.51466036 -0.47793865 -0.7085011 -0.76050019 -0.869102 -1.0046198][-0.27222347 -0.18268776 -0.15826893 -0.016682625 -0.010045052 -0.29350615 -0.48775673 -0.68523359 -1.0099618 -0.99468088 -1.0123849 -1.196553 -1.1480036 -1.0942328 -1.0687582][-0.27106524 -0.18230295 -0.20066595 -0.069147587 -0.080680847 -0.39674997 -0.59678054 -0.79498458 -1.1092515 -1.1398482 -1.2149444 -1.4050603 -1.3441589 -1.1336608 -0.87057519][-0.13236046 -0.051107883 -0.098642826 0.051576138 0.049405098 -0.22710657 -0.36442709 -0.49920034 -0.72220325 -0.79972959 -1.0346882 -1.3463445 -1.4145932 -1.2401114 -0.9064188][-0.052082062 -0.058595181 -0.18731833 -0.092141151 -0.16295052 -0.35858917 -0.34146214 -0.30423784 -0.32609034 -0.41597652 -0.8273952 -1.2118473 -1.3518395 -1.2612166 -0.97546148][-0.034106255 -0.087443829 -0.30743217 -0.35321665 -0.59342742 -0.73984289 -0.59691143 -0.42693233 -0.23040152 -0.30602694 -0.84284806 -1.1772807 -1.32009 -1.2991717 -1.0501101][-0.11410189 -0.20267439 -0.41262627 -0.5306685 -0.87601566 -0.9490037 -0.74974585 -0.59966612 -0.33155012 -0.48629379 -1.0950871 -1.2788768 -1.343924 -1.3401792 -1.0721216][-0.45866442 -0.61444235 -0.73183227 -0.83378005 -1.1530037 -1.1206646 -0.9476459 -0.90838957 -0.69190311 -0.95316982 -1.5739422 -1.5846844 -1.5303872 -1.5011101 -1.2031538][-0.94732571 -1.0459604 -1.0160964 -1.095392 -1.3556368 -1.2916424 -1.2570388 -1.395638 -1.3292923 -1.6792066 -2.1837482 -2.0049398 -1.8283293 -1.7698734 -1.4991705][-1.3499587 -1.3330264 -1.2115471 -1.3192122 -1.5570815 -1.5339386 -1.6947644 -2.0121639 -2.1138313 -2.4785168 -2.7532845 -2.3723085 -2.0043254 -1.8346202 -1.6197717][-1.7591889 -1.6754332 -1.5151508 -1.5939212 -1.7393403 -1.7438843 -2.0503352 -2.4371707 -2.5979769 -2.9115005 -3.0608373 -2.6935372 -2.3004518 -2.0362966 -1.7632346][-2.0784233 -2.0130711 -1.9008455 -1.945009 -1.9408443 -1.8958235 -2.1843674 -2.4958258 -2.5843573 -2.8118858 -2.9520392 -2.7881052 -2.5344992 -2.2666228 -1.9518778][-2.1382113 -2.1334095 -2.1373889 -2.1797855 -2.0492151 -1.9537644 -2.1401336 -2.3072357 -2.3010051 -2.4873681 -2.6665449 -2.6514707 -2.4959478 -2.2397947 -1.942625]]...]
INFO - root - 2017-12-07 03:34:34.590514: step 1010, loss = 0.69, batch loss = 0.62 (10.1 examples/sec; 0.792 sec/batch; 72h:53m:10s remains)
INFO - root - 2017-12-07 03:34:42.139962: step 1020, loss = 0.62, batch loss = 0.55 (10.4 examples/sec; 0.768 sec/batch; 70h:40m:51s remains)
INFO - root - 2017-12-07 03:34:49.742130: step 1030, loss = 0.73, batch loss = 0.66 (10.8 examples/sec; 0.740 sec/batch; 68h:08m:24s remains)
INFO - root - 2017-12-07 03:34:57.437520: step 1040, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.779 sec/batch; 71h:44m:27s remains)
INFO - root - 2017-12-07 03:35:05.021577: step 1050, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.752 sec/batch; 69h:15m:11s remains)
INFO - root - 2017-12-07 03:35:12.733371: step 1060, loss = 0.75, batch loss = 0.68 (10.8 examples/sec; 0.740 sec/batch; 68h:08m:25s remains)
INFO - root - 2017-12-07 03:35:20.505514: step 1070, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.777 sec/batch; 71h:31m:27s remains)
INFO - root - 2017-12-07 03:35:27.915921: step 1080, loss = 0.71, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 72h:23m:27s remains)
INFO - root - 2017-12-07 03:35:35.599826: step 1090, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.775 sec/batch; 71h:19m:28s remains)
INFO - root - 2017-12-07 03:35:43.383264: step 1100, loss = 0.93, batch loss = 0.85 (10.4 examples/sec; 0.770 sec/batch; 70h:51m:17s remains)
2017-12-07 03:35:43.962638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.49201441 -0.5702126 -0.59864569 -0.58770084 -0.56561255 -0.53665352 -0.50933719 -0.46983314 -0.43870091 -0.43028879 -0.42772436 -0.42624569 -0.43225121 -0.42909718 -0.42074251][-0.25894165 -0.3891511 -0.44808173 -0.46634197 -0.47492647 -0.45506024 -0.42486525 -0.36528826 -0.33125877 -0.30762339 -0.2721405 -0.25815868 -0.24997282 -0.22071886 -0.1896677][-0.21903229 -0.37931728 -0.45295119 -0.4893868 -0.49935961 -0.45306325 -0.39941692 -0.32500124 -0.300241 -0.26974535 -0.22236061 -0.22532749 -0.22045612 -0.18088865 -0.14370155][-0.14094925 -0.30278349 -0.37861633 -0.44055104 -0.44785094 -0.38639641 -0.33341932 -0.2632823 -0.25079298 -0.19015694 -0.10536528 -0.1001873 -0.065643787 0.007566452 0.061733723][-0.025186539 -0.13424063 -0.16895151 -0.22965622 -0.23503876 -0.18564749 -0.14358807 -0.074130058 -0.061759472 0.03578043 0.16022444 0.18682337 0.25168848 0.34298229 0.38856125][0.099419117 0.058481216 0.096959591 0.085947037 0.085427761 0.12000847 0.18825626 0.29369593 0.31934595 0.44260168 0.61893368 0.67743158 0.73041821 0.76783991 0.72695208][0.17753315 0.22025967 0.36588669 0.45544147 0.46677828 0.48729372 0.60499239 0.73542738 0.71503735 0.81373692 1.037322 1.1261935 1.1400094 1.1001358 0.96255255][0.22313833 0.34543228 0.55691671 0.67886257 0.64798641 0.62519312 0.7761755 0.90876627 0.83215475 0.89798021 1.1381407 1.2178116 1.1550188 1.0393558 0.85883474][0.29405737 0.43263578 0.62092781 0.69327974 0.57625008 0.47095346 0.5618906 0.63322115 0.51153755 0.55594778 0.78927994 0.84878969 0.77459049 0.70053816 0.60236645][0.39061737 0.44162035 0.514637 0.51241779 0.3424654 0.18467665 0.19639349 0.25734663 0.20098877 0.25516605 0.44013071 0.47269297 0.43568468 0.43725061 0.4498229][0.40716505 0.340446 0.30245113 0.25619698 0.069277287 -0.13955784 -0.19989014 -0.10083723 -0.029362679 0.05443573 0.20327425 0.26037788 0.30125809 0.37908268 0.47849035][0.29813814 0.17043638 0.095284939 0.065775871 -0.086984634 -0.30648756 -0.39402819 -0.25969696 -0.10422611 -0.0011544228 0.13692856 0.24695349 0.37076855 0.50193548 0.62003851][0.2612977 0.11115646 0.013480663 -0.017416954 -0.14121056 -0.32529116 -0.3872776 -0.26893616 -0.12147951 -0.019779682 0.10305977 0.22731638 0.37899876 0.52669334 0.6228261][0.39919758 0.26444721 0.1522336 0.099882126 -0.0015449524 -0.11539936 -0.12733841 -0.034000397 0.093778133 0.1970768 0.29020596 0.36872387 0.48906946 0.60215425 0.63179493][0.63808584 0.56379175 0.48284864 0.44095516 0.38836098 0.35740042 0.38649607 0.45446539 0.54700756 0.62253189 0.66399431 0.68217278 0.74690914 0.79950428 0.77268362]]...]
INFO - root - 2017-12-07 03:35:51.645906: step 1110, loss = 0.92, batch loss = 0.85 (10.3 examples/sec; 0.779 sec/batch; 71h:44m:46s remains)
INFO - root - 2017-12-07 03:35:59.305378: step 1120, loss = 1.01, batch loss = 0.94 (10.4 examples/sec; 0.768 sec/batch; 70h:42m:12s remains)
INFO - root - 2017-12-07 03:36:06.962396: step 1130, loss = 0.74, batch loss = 0.66 (10.8 examples/sec; 0.742 sec/batch; 68h:18m:27s remains)
INFO - root - 2017-12-07 03:36:14.699551: step 1140, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 73h:08m:27s remains)
INFO - root - 2017-12-07 03:36:22.349138: step 1150, loss = 0.74, batch loss = 0.66 (10.5 examples/sec; 0.759 sec/batch; 69h:51m:37s remains)
INFO - root - 2017-12-07 03:36:30.141078: step 1160, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.759 sec/batch; 69h:52m:31s remains)
INFO - root - 2017-12-07 03:36:37.957213: step 1170, loss = 0.67, batch loss = 0.60 (10.2 examples/sec; 0.782 sec/batch; 72h:00m:44s remains)
INFO - root - 2017-12-07 03:36:45.439901: step 1180, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.772 sec/batch; 71h:03m:42s remains)
INFO - root - 2017-12-07 03:36:53.099333: step 1190, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.770 sec/batch; 70h:51m:49s remains)
INFO - root - 2017-12-07 03:37:00.673309: step 1200, loss = 0.76, batch loss = 0.68 (10.6 examples/sec; 0.755 sec/batch; 69h:31m:16s remains)
2017-12-07 03:37:01.272982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0772886 -4.77935 -4.5718408 -4.4288192 -4.1417966 -3.9479494 -4.0913887 -4.5303326 -4.9893012 -5.0583611 -4.8173671 -4.5782971 -4.2006392 -3.5862038 -3.0005121][-4.5788569 -4.4519572 -4.4125924 -4.3724909 -4.1448612 -3.9194286 -3.9928908 -4.4481449 -5.0432572 -5.2422905 -5.1561518 -4.9282308 -4.3711934 -3.5653987 -2.8351119][-3.8335989 -3.8828294 -3.8713312 -3.8657668 -3.7168803 -3.4526644 -3.455569 -3.9222035 -4.5988588 -4.885603 -4.9411073 -4.7391348 -4.1004586 -3.2879481 -2.6635776][-3.1856928 -3.4190211 -3.3633738 -3.2986398 -3.1793673 -2.8463264 -2.6888776 -3.1091728 -3.9360349 -4.5292511 -4.9061956 -4.8779216 -4.229135 -3.3364096 -2.6955354][-2.5132456 -3.022191 -3.0401902 -2.9155908 -2.6975448 -2.1159441 -1.5438664 -1.607923 -2.4914007 -3.5838037 -4.5001802 -4.9287357 -4.5987282 -3.7682962 -2.9804585][-1.8112428 -2.5633621 -2.7449863 -2.5951338 -2.3444941 -1.635453 -0.65489149 -0.15586424 -0.90285921 -2.3633029 -3.7090087 -4.6124349 -4.8265967 -4.3501134 -3.5121496][-1.5567863 -2.4224167 -2.7355797 -2.4651279 -2.0655959 -1.1835377 0.29935646 1.4933467 0.93652678 -0.87826276 -2.6755 -4.0423417 -4.7878876 -4.841897 -4.209506][-2.059093 -3.1328545 -3.6607988 -3.3469608 -2.7424164 -1.6048937 0.40354776 2.3884244 2.3307738 0.52980757 -1.5197532 -3.1950145 -4.2916946 -4.814384 -4.6124711][-2.4710696 -3.814692 -4.6566854 -4.5237494 -3.9755712 -2.9111118 -0.99876404 1.0505128 1.6004543 0.610868 -0.90546656 -2.322577 -3.3396285 -4.0513844 -4.2907524][-2.5447016 -3.9134574 -4.8814869 -5.0142717 -4.6457877 -3.8089919 -2.3924055 -0.862324 -0.090090752 -0.18601561 -0.77110147 -1.5828152 -2.2460604 -2.920064 -3.5077477][-2.5928488 -3.7149534 -4.5583696 -4.9610453 -4.7997789 -4.1429081 -3.2168398 -2.2868078 -1.5166006 -1.0546989 -0.93700385 -1.1964583 -1.4951711 -1.9498174 -2.571239][-2.4432797 -3.0592146 -3.6754572 -4.3164506 -4.4290524 -4.0512867 -3.6272893 -3.2507834 -2.5945063 -1.9246705 -1.5320349 -1.4875858 -1.553426 -1.7152131 -2.0098696][-2.4034371 -2.5713215 -3.0195789 -3.8233664 -4.1306005 -3.9869144 -3.9314864 -3.8598864 -3.3221409 -2.64924 -2.2059152 -2.0626626 -2.1817052 -2.4099188 -2.5730088][-3.216548 -3.2285879 -3.6239083 -4.4299059 -4.7548857 -4.637557 -4.5454626 -4.3651357 -3.8131359 -3.2031617 -2.796315 -2.666656 -2.9045525 -3.2962897 -3.6009574][-3.9544706 -3.8654835 -4.1259303 -4.7434812 -5.0038824 -4.8770647 -4.6686816 -4.3374448 -3.8729272 -3.4812684 -3.2616382 -3.2925477 -3.5890939 -3.896512 -4.157505]]...]
INFO - root - 2017-12-07 03:37:08.871817: step 1210, loss = 0.86, batch loss = 0.78 (10.3 examples/sec; 0.775 sec/batch; 71h:18m:43s remains)
INFO - root - 2017-12-07 03:37:16.456871: step 1220, loss = 0.73, batch loss = 0.66 (10.8 examples/sec; 0.740 sec/batch; 68h:05m:51s remains)
INFO - root - 2017-12-07 03:37:24.101297: step 1230, loss = 0.79, batch loss = 0.71 (10.3 examples/sec; 0.778 sec/batch; 71h:34m:34s remains)
INFO - root - 2017-12-07 03:37:31.831621: step 1240, loss = 0.91, batch loss = 0.84 (10.3 examples/sec; 0.774 sec/batch; 71h:13m:26s remains)
INFO - root - 2017-12-07 03:37:39.499859: step 1250, loss = 0.78, batch loss = 0.70 (10.3 examples/sec; 0.778 sec/batch; 71h:37m:25s remains)
INFO - root - 2017-12-07 03:37:47.116088: step 1260, loss = 0.56, batch loss = 0.49 (10.6 examples/sec; 0.756 sec/batch; 69h:32m:18s remains)
INFO - root - 2017-12-07 03:37:54.769795: step 1270, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.768 sec/batch; 70h:41m:41s remains)
INFO - root - 2017-12-07 03:38:02.192492: step 1280, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.765 sec/batch; 70h:23m:50s remains)
INFO - root - 2017-12-07 03:38:09.762144: step 1290, loss = 0.75, batch loss = 0.67 (10.6 examples/sec; 0.756 sec/batch; 69h:32m:17s remains)
INFO - root - 2017-12-07 03:38:17.329936: step 1300, loss = 0.91, batch loss = 0.84 (10.7 examples/sec; 0.748 sec/batch; 68h:48m:21s remains)
2017-12-07 03:38:17.951563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.65348 -2.7100003 -2.7690432 -2.7958612 -2.735671 -2.7813153 -2.7559228 -2.57369 -2.7067873 -2.8953938 -2.9022162 -2.8749568 -2.8580523 -2.8702316 -2.7859683][-2.5987272 -2.64214 -2.6775913 -2.705843 -2.6932631 -2.7947211 -2.8174295 -2.6496813 -2.8078108 -3.0331464 -3.032433 -2.9486547 -2.8711019 -2.854917 -2.762908][-2.5156193 -2.5459414 -2.5492382 -2.5720959 -2.6472578 -2.8282688 -2.869957 -2.6847658 -2.8591251 -3.1425874 -3.1595931 -3.0474539 -2.9304338 -2.8809791 -2.7676826][-2.4587886 -2.4681373 -2.412322 -2.39683 -2.5749817 -2.850657 -2.8939767 -2.672158 -2.8508167 -3.1824853 -3.2194638 -3.1086059 -2.9923997 -2.9289761 -2.79659][-2.4869144 -2.4636111 -2.333034 -2.2436721 -2.4929023 -2.849287 -2.8741808 -2.5968187 -2.7715049 -3.1463106 -3.2078171 -3.1229777 -3.0341883 -2.9761434 -2.8312922][-2.5496912 -2.4887176 -2.2973249 -2.1176469 -2.3792405 -2.7782552 -2.76865 -2.4263055 -2.600441 -3.0281281 -3.133826 -3.1114824 -3.0707321 -3.0216789 -2.8639033][-2.6622877 -2.5734997 -2.352051 -2.0795813 -2.2718382 -2.6455359 -2.5761223 -2.1675231 -2.3595202 -2.850348 -3.0091352 -3.0702679 -3.0946949 -3.0579419 -2.8908105][-2.8890629 -2.7937548 -2.5687613 -2.2221258 -2.317162 -2.6262288 -2.4643044 -1.9718871 -2.1622071 -2.6769876 -2.8543341 -2.9831395 -3.0725365 -3.0549297 -2.8897409][-3.1543877 -3.0884256 -2.8885107 -2.5017505 -2.5187936 -2.7777247 -2.5446553 -1.9783435 -2.1225109 -2.5975671 -2.7494831 -2.9046106 -3.0287421 -3.0189309 -2.8567042][-3.3579853 -3.3394217 -3.1894295 -2.8221116 -2.7975588 -3.0252774 -2.7690787 -2.1869137 -2.2893195 -2.6963806 -2.8025069 -2.9330268 -3.043376 -3.0128429 -2.8372285][-3.4437366 -3.4709117 -3.3795285 -3.0807664 -3.0439847 -3.2366629 -3.0008588 -2.470849 -2.5539238 -2.8950415 -2.9608593 -3.0422354 -3.1074896 -3.0442009 -2.8480449][-3.4245887 -3.4759851 -3.4312813 -3.2235329 -3.2010236 -3.3699021 -3.1819882 -2.744401 -2.8251929 -3.1127522 -3.1477654 -3.16637 -3.1650653 -3.0647326 -2.8627398][-3.3959608 -3.4452491 -3.4295232 -3.3071389 -3.315773 -3.4701009 -3.3380041 -3.0006728 -3.0773215 -3.3059926 -3.3124533 -3.2699034 -3.1951303 -3.0529423 -2.8598642][-3.3206701 -3.3701 -3.3820491 -3.329483 -3.3625107 -3.4992955 -3.4206116 -3.1864035 -3.2608581 -3.4280396 -3.407763 -3.3297789 -3.2090237 -3.0353215 -2.8567665][-3.1976657 -3.2393005 -3.2759678 -3.2733517 -3.3172169 -3.4349744 -3.4089766 -3.2770934 -3.3514218 -3.46733 -3.4288988 -3.3411846 -3.2083983 -3.0266781 -2.8678102]]...]
INFO - root - 2017-12-07 03:38:25.636946: step 1310, loss = 0.86, batch loss = 0.78 (10.6 examples/sec; 0.758 sec/batch; 69h:44m:19s remains)
INFO - root - 2017-12-07 03:38:33.289509: step 1320, loss = 0.68, batch loss = 0.61 (10.4 examples/sec; 0.771 sec/batch; 70h:54m:28s remains)
INFO - root - 2017-12-07 03:38:41.013825: step 1330, loss = 0.77, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 71h:47m:03s remains)
INFO - root - 2017-12-07 03:38:48.686831: step 1340, loss = 0.89, batch loss = 0.81 (10.2 examples/sec; 0.785 sec/batch; 72h:12m:40s remains)
INFO - root - 2017-12-07 03:38:56.329360: step 1350, loss = 0.65, batch loss = 0.58 (10.1 examples/sec; 0.791 sec/batch; 72h:48m:23s remains)
INFO - root - 2017-12-07 03:39:04.006212: step 1360, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.761 sec/batch; 70h:02m:33s remains)
INFO - root - 2017-12-07 03:39:11.692980: step 1370, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.784 sec/batch; 72h:09m:14s remains)
INFO - root - 2017-12-07 03:39:18.962065: step 1380, loss = 0.85, batch loss = 0.78 (10.9 examples/sec; 0.733 sec/batch; 67h:25m:21s remains)
INFO - root - 2017-12-07 03:39:26.517827: step 1390, loss = 0.83, batch loss = 0.76 (10.8 examples/sec; 0.741 sec/batch; 68h:10m:05s remains)
INFO - root - 2017-12-07 03:39:34.150149: step 1400, loss = 0.54, batch loss = 0.47 (10.0 examples/sec; 0.798 sec/batch; 73h:22m:43s remains)
2017-12-07 03:39:34.819573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.86436248 -0.10580301 0.6664052 0.0068964958 -1.6643746 -2.5651093 -1.7750463 -0.53173971 -0.37130547 -1.0964148 -1.348896 -0.65842724 0.50455952 0.78038359 -0.79635978][-0.96211076 -0.082580566 0.89452171 0.10960531 -1.7545042 -2.4144125 -1.2665765 -0.05701685 -0.42036343 -1.4974246 -1.7033002 -1.031234 -0.014540672 0.15760899 -1.167661][-1.2756858 -0.32785177 0.87496471 0.15156412 -1.6443701 -1.837924 -0.30524778 0.68366051 -0.30605268 -1.6385305 -1.8225982 -1.3918684 -0.66842055 -0.53634381 -1.4610193][-1.4303129 -0.54835868 0.77582312 0.19341135 -1.3742535 -0.94701505 0.9693594 1.5258212 -0.11117697 -1.5945613 -1.896076 -1.941781 -1.5003958 -1.2579353 -1.7274683][-1.4070067 -0.62626743 0.69500113 0.22753191 -1.1179547 -0.12237501 2.11665 2.2107038 0.13799286 -1.4115896 -1.9796748 -2.5549321 -2.2683027 -1.8360412 -1.8657939][-1.411283 -0.83456922 0.44052935 0.12612581 -0.87500453 0.55709028 2.8355947 2.5114355 0.33365774 -1.1625509 -2.0859423 -3.1189272 -2.9088016 -2.2995198 -1.9095592][-1.6044168 -1.1831064 0.12517643 0.066912174 -0.52804089 0.99070024 2.900682 2.38629 0.47969246 -0.88918042 -2.1822195 -3.5702877 -3.4562809 -2.7642932 -2.0639832][-1.8705702 -1.5075326 -0.13722563 0.078575134 -0.18451071 1.094882 2.5317955 2.1160588 0.69090223 -0.50389314 -2.0387132 -3.6908603 -3.7687893 -3.1306267 -2.2243826][-2.3277452 -2.0403337 -0.66751051 -0.14520168 -0.091274738 0.84977627 1.9014001 1.7508087 0.74858665 -0.32778788 -1.9788182 -3.7786825 -4.068193 -3.5170517 -2.4472551][-2.9433928 -2.6936512 -1.3842049 -0.61914182 -0.3430686 0.26190424 1.0483842 1.1927876 0.48965502 -0.49551821 -2.0664499 -3.805351 -4.2541618 -3.8145695 -2.6787281][-3.3531637 -3.202642 -2.111284 -1.2744305 -0.88540268 -0.45148778 0.255404 0.66583347 0.21458483 -0.67515063 -2.071238 -3.6525054 -4.2361622 -3.946856 -2.8814034][-3.5758479 -3.5470407 -2.7472017 -1.9947886 -1.6253138 -1.2559888 -0.56860805 -0.016838074 -0.22967768 -0.93053722 -2.0778732 -3.4764333 -4.1649733 -4.0119572 -3.104342][-3.5563743 -3.6464005 -3.1673298 -2.6107292 -2.3525994 -2.0295 -1.4124606 -0.87559533 -0.88487697 -1.3024211 -2.110178 -3.2668464 -3.981163 -3.9323018 -3.2568848][-3.5181694 -3.6695063 -3.4505818 -3.1107926 -2.9648476 -2.6784091 -2.1477783 -1.6968532 -1.583277 -1.7623367 -2.2553406 -3.1460426 -3.7927537 -3.797327 -3.344698][-3.4758253 -3.6026177 -3.5380352 -3.3638897 -3.3192863 -3.1309955 -2.7568674 -2.4238255 -2.2522728 -2.2782538 -2.5296998 -3.1423807 -3.6428294 -3.6739421 -3.4074845]]...]
INFO - root - 2017-12-07 03:39:42.374234: step 1410, loss = 0.86, batch loss = 0.79 (10.7 examples/sec; 0.748 sec/batch; 68h:49m:58s remains)
INFO - root - 2017-12-07 03:39:49.936981: step 1420, loss = 0.86, batch loss = 0.78 (10.6 examples/sec; 0.758 sec/batch; 69h:39m:57s remains)
INFO - root - 2017-12-07 03:39:57.736947: step 1430, loss = 0.88, batch loss = 0.81 (10.3 examples/sec; 0.777 sec/batch; 71h:25m:56s remains)
INFO - root - 2017-12-07 03:40:05.336969: step 1440, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.768 sec/batch; 70h:35m:09s remains)
INFO - root - 2017-12-07 03:40:12.985854: step 1450, loss = 0.83, batch loss = 0.75 (10.4 examples/sec; 0.768 sec/batch; 70h:37m:22s remains)
INFO - root - 2017-12-07 03:40:20.645159: step 1460, loss = 0.97, batch loss = 0.90 (10.9 examples/sec; 0.733 sec/batch; 67h:23m:10s remains)
INFO - root - 2017-12-07 03:40:28.270496: step 1470, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 71h:30m:09s remains)
INFO - root - 2017-12-07 03:40:35.729777: step 1480, loss = 0.95, batch loss = 0.88 (10.1 examples/sec; 0.794 sec/batch; 72h:59m:32s remains)
INFO - root - 2017-12-07 03:40:43.419315: step 1490, loss = 0.89, batch loss = 0.81 (10.3 examples/sec; 0.778 sec/batch; 71h:30m:50s remains)
INFO - root - 2017-12-07 03:40:51.023044: step 1500, loss = 0.92, batch loss = 0.85 (10.5 examples/sec; 0.759 sec/batch; 69h:47m:37s remains)
2017-12-07 03:40:51.664499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7640882 -2.4246736 -2.3991559 -2.7224612 -3.3395288 -3.9475615 -4.2184305 -4.1773968 -3.7199228 -3.069869 -2.5631466 -2.6557264 -3.5277042 -4.4460049 -4.36841][-2.3901978 -1.9036539 -1.9330375 -2.5331213 -3.5263932 -4.3505955 -4.5452056 -4.2189283 -3.4957538 -2.6732879 -2.1492341 -2.3857412 -3.5081773 -4.6731567 -4.6607947][-2.1481075 -1.5090816 -1.5576336 -2.3404522 -3.554461 -4.5141792 -4.6301236 -4.1026716 -3.2532649 -2.3624783 -1.8434434 -2.1918397 -3.4378166 -4.7337728 -4.8063607][-1.8894162 -1.2318757 -1.3166933 -2.1426065 -3.3544116 -4.3497276 -4.4536653 -3.8887303 -3.0677438 -2.2062376 -1.7183299 -2.1504788 -3.4351544 -4.7447109 -4.8427076][-1.704531 -1.2700872 -1.5184748 -2.3311884 -3.3912477 -4.26544 -4.3297458 -3.7704577 -3.06001 -2.3600602 -1.9847906 -2.47502 -3.7234678 -4.913321 -4.9143238][-1.6365116 -1.5350516 -1.9594424 -2.7073987 -3.5478735 -4.1757517 -4.1422358 -3.57729 -2.9564054 -2.4543552 -2.2345097 -2.7896512 -4.0171361 -5.0766983 -4.9610071][-1.901989 -2.0720468 -2.577785 -3.1977398 -3.831099 -4.2334991 -4.1530824 -3.6196916 -3.0498219 -2.6473997 -2.4977431 -3.0804634 -4.29271 -5.2359014 -5.0158038][-2.0043526 -2.3347759 -2.8657122 -3.3633084 -3.8387513 -4.117969 -4.0816684 -3.6610994 -3.1142051 -2.6549783 -2.4328527 -3.0362482 -4.3031435 -5.2162194 -4.963201][-2.0089378 -2.4066086 -2.8647528 -3.1545887 -3.4292109 -3.656131 -3.7513988 -3.5528018 -3.1013832 -2.5295897 -2.1258571 -2.6549234 -3.911103 -4.7655969 -4.5129008][-2.5883694 -3.0148244 -3.3516793 -3.396369 -3.4342895 -3.600698 -3.8473983 -3.9196167 -3.6623988 -3.1199336 -2.5903401 -2.9230847 -3.9523749 -4.5204763 -4.0837851][-3.2528548 -3.5610328 -3.7291796 -3.6316867 -3.5678034 -3.7407441 -4.115643 -4.3660817 -4.2296786 -3.7356918 -3.1417575 -3.2812 -4.0793552 -4.4078946 -3.8258724][-3.7058501 -3.8547564 -3.8897269 -3.7489002 -3.6740794 -3.8353009 -4.228703 -4.5626664 -4.5189686 -4.1038513 -3.4862523 -3.411335 -3.9622254 -4.1297894 -3.5320802][-3.4983919 -3.5825555 -3.6174166 -3.5440714 -3.5046539 -3.5989733 -3.8900421 -4.2346444 -4.3139381 -4.0652204 -3.5446846 -3.3557024 -3.6928272 -3.7962132 -3.3132424][-3.0424747 -3.074306 -3.1172938 -3.1100287 -3.1258833 -3.2116508 -3.4239049 -3.7490644 -3.9060452 -3.7621279 -3.3453012 -3.1310887 -3.3499825 -3.5056674 -3.2540436][-3.0118985 -3.0118217 -3.0123594 -2.9938824 -3.0105939 -3.0978217 -3.2698774 -3.5630064 -3.7405934 -3.6432302 -3.3139391 -3.1063437 -3.228138 -3.3842406 -3.2975035]]...]
INFO - root - 2017-12-07 03:40:59.286891: step 1510, loss = 0.88, batch loss = 0.81 (10.1 examples/sec; 0.790 sec/batch; 72h:38m:26s remains)
INFO - root - 2017-12-07 03:41:07.075001: step 1520, loss = 0.67, batch loss = 0.60 (10.8 examples/sec; 0.742 sec/batch; 68h:12m:14s remains)
INFO - root - 2017-12-07 03:41:14.732952: step 1530, loss = 0.81, batch loss = 0.74 (10.9 examples/sec; 0.735 sec/batch; 67h:32m:06s remains)
INFO - root - 2017-12-07 03:41:22.388311: step 1540, loss = 0.91, batch loss = 0.84 (10.8 examples/sec; 0.744 sec/batch; 68h:24m:50s remains)
INFO - root - 2017-12-07 03:41:30.027962: step 1550, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.750 sec/batch; 68h:56m:46s remains)
INFO - root - 2017-12-07 03:41:37.551826: step 1560, loss = 0.85, batch loss = 0.77 (10.3 examples/sec; 0.778 sec/batch; 71h:31m:15s remains)
INFO - root - 2017-12-07 03:41:45.230315: step 1570, loss = 0.73, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 72h:21m:43s remains)
INFO - root - 2017-12-07 03:41:52.705226: step 1580, loss = 0.70, batch loss = 0.62 (10.5 examples/sec; 0.760 sec/batch; 69h:53m:29s remains)
INFO - root - 2017-12-07 03:42:00.366347: step 1590, loss = 0.94, batch loss = 0.87 (10.9 examples/sec; 0.736 sec/batch; 67h:39m:03s remains)
INFO - root - 2017-12-07 03:42:07.914018: step 1600, loss = 0.83, batch loss = 0.76 (11.2 examples/sec; 0.717 sec/batch; 65h:55m:09s remains)
2017-12-07 03:42:08.570985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.960156 -4.0175676 -4.0294938 -4.030427 -4.0704169 -4.0969505 -4.0802059 -4.0555863 -4.033514 -3.9919777 -3.9274683 -3.8557074 -3.7865934 -3.7400837 -3.7130332][-4.233489 -4.3641009 -4.3683496 -4.3362951 -4.391221 -4.4737787 -4.5081177 -4.52182 -4.5227733 -4.4864197 -4.4216957 -4.3586245 -4.2816987 -4.1940165 -4.088984][-4.5152745 -4.6648073 -4.595325 -4.4780583 -4.4806066 -4.5346584 -4.5392914 -4.5356574 -4.5384679 -4.5283179 -4.5245085 -4.5398493 -4.5167956 -4.4528532 -4.3429475][-4.4244108 -4.5459871 -4.515439 -4.55393 -4.7567153 -4.9403443 -4.9727888 -4.9390664 -4.9039216 -4.9099293 -4.9802113 -5.0776162 -5.0567722 -4.9004221 -4.6501203][-3.7868514 -3.6603785 -3.5342438 -3.6686521 -4.0523691 -4.3823152 -4.5194316 -4.5787315 -4.6254563 -4.7622714 -4.9949822 -5.2220054 -5.239295 -5.0321918 -4.7162461][-3.0496249 -2.6697428 -2.3891315 -2.4601042 -2.8051438 -3.0950136 -3.2076135 -3.3018451 -3.4631436 -3.804935 -4.2622123 -4.6369662 -4.7202816 -4.5152607 -4.2517304][-2.6273751 -2.0744402 -1.6417112 -1.5689402 -1.7959962 -1.9808195 -1.9896703 -2.0519223 -2.3239756 -2.8745861 -3.5540047 -4.0952511 -4.2745633 -4.1026449 -3.8834963][-2.6542444 -1.9683704 -1.2761965 -0.82069397 -0.6420598 -0.45382047 -0.1794982 -0.17696905 -0.61847496 -1.4099362 -2.3145661 -3.0470769 -3.4197836 -3.4547591 -3.4397416][-3.0739665 -2.5743518 -1.9711292 -1.4250913 -1.0227323 -0.48207092 0.14569855 0.34179115 -0.064735413 -0.83994555 -1.735842 -2.4507079 -2.8400166 -2.9872777 -3.1281638][-3.5134864 -3.2779284 -2.9404125 -2.6092186 -2.3798783 -1.9568477 -1.4043474 -1.1628575 -1.3388751 -1.777452 -2.3534944 -2.8061154 -3.0598783 -3.2161498 -3.390022][-3.7638519 -3.7036543 -3.5276783 -3.3402705 -3.2329841 -2.9355807 -2.5279422 -2.3254449 -2.3213222 -2.4492664 -2.731307 -3.0190413 -3.2661402 -3.5288966 -3.7730861][-3.6743455 -3.6964495 -3.6136587 -3.5707433 -3.6157763 -3.4702449 -3.1981769 -3.0217533 -2.8980856 -2.7961993 -2.8857675 -3.1308761 -3.446337 -3.8113661 -4.0785551][-3.3243203 -3.2230749 -3.0721104 -3.0845828 -3.2546182 -3.2898314 -3.2190421 -3.1877441 -3.1274652 -2.9928782 -3.0299315 -3.2826309 -3.5891526 -3.9075313 -4.1120667][-3.0066066 -2.670846 -2.3812976 -2.3852274 -2.6175823 -2.7770197 -2.8384 -2.8871932 -2.8751016 -2.7729878 -2.8085995 -3.0135865 -3.2187166 -3.4258223 -3.61092][-3.0348611 -2.5803666 -2.2595365 -2.2864037 -2.5382385 -2.7716088 -2.9250283 -3.0322232 -3.08001 -3.0568051 -3.1139584 -3.217242 -3.2324595 -3.2248015 -3.2725129]]...]
INFO - root - 2017-12-07 03:42:16.250814: step 1610, loss = 0.93, batch loss = 0.86 (10.4 examples/sec; 0.768 sec/batch; 70h:36m:46s remains)
INFO - root - 2017-12-07 03:42:24.007174: step 1620, loss = 0.71, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 72h:09m:39s remains)
INFO - root - 2017-12-07 03:42:31.721186: step 1630, loss = 0.89, batch loss = 0.82 (9.9 examples/sec; 0.804 sec/batch; 73h:56m:08s remains)
INFO - root - 2017-12-07 03:42:39.434267: step 1640, loss = 0.72, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 71h:49m:43s remains)
INFO - root - 2017-12-07 03:42:47.103969: step 1650, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.753 sec/batch; 69h:13m:42s remains)
INFO - root - 2017-12-07 03:42:54.768381: step 1660, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.746 sec/batch; 68h:34m:11s remains)
INFO - root - 2017-12-07 03:43:02.475492: step 1670, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.747 sec/batch; 68h:39m:18s remains)
INFO - root - 2017-12-07 03:43:09.901562: step 1680, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.782 sec/batch; 71h:50m:46s remains)
INFO - root - 2017-12-07 03:43:17.547396: step 1690, loss = 0.79, batch loss = 0.71 (10.3 examples/sec; 0.779 sec/batch; 71h:35m:09s remains)
INFO - root - 2017-12-07 03:43:25.174215: step 1700, loss = 1.00, batch loss = 0.93 (10.6 examples/sec; 0.758 sec/batch; 69h:38m:57s remains)
2017-12-07 03:43:25.784523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0017838 -4.141736 -4.2116365 -4.2243733 -4.1141005 -3.9347472 -3.7667875 -3.4739673 -3.1206627 -3.0246451 -3.0930059 -3.1357045 -3.1461065 -2.9056635 -2.4692905][-4.2447014 -4.3727226 -4.5257812 -4.6718025 -4.6027293 -4.3476872 -4.0628076 -3.7087793 -3.4041936 -3.3320251 -3.4387808 -3.516113 -3.5220175 -3.3078496 -2.9800406][-4.2677522 -4.4725161 -4.7623296 -4.9924469 -4.8503213 -4.427155 -3.9345388 -3.5058708 -3.3020346 -3.3724248 -3.629796 -3.78307 -3.7608352 -3.5360258 -3.2658539][-4.3212428 -4.5971737 -4.9723949 -5.2068033 -4.9923711 -4.4320331 -3.7311018 -3.1572177 -2.9710212 -3.2763066 -3.7820106 -3.9647317 -3.84591 -3.5294635 -3.2105594][-4.1970506 -4.5815759 -5.1159596 -5.4244242 -5.1729689 -4.4974394 -3.507978 -2.5724316 -2.2206371 -2.80123 -3.6539721 -3.8827281 -3.6347976 -3.199543 -2.8365388][-3.6397288 -4.0437446 -4.6908207 -5.0628223 -4.7352581 -3.8551507 -2.52344 -1.1277087 -0.61482 -1.6165354 -2.9512982 -3.3593812 -3.0787027 -2.636622 -2.3886774][-3.3954012 -3.6110985 -4.1348042 -4.398088 -3.9089518 -2.8085861 -1.1278498 0.67982388 1.2703104 -0.20239973 -2.0170887 -2.711628 -2.6004407 -2.3369675 -2.3107028][-3.4969938 -3.4763796 -3.7690735 -3.8607612 -3.2850726 -2.1552703 -0.36776972 1.5929561 2.1376414 0.41542578 -1.5583529 -2.3957884 -2.4368997 -2.3643062 -2.5376964][-3.1541038 -2.9917126 -3.2215247 -3.3081532 -2.8589604 -2.0317585 -0.66635752 0.81252718 1.1469364 -0.22609377 -1.7129555 -2.2830334 -2.2337635 -2.2185109 -2.5014663][-2.6053381 -2.5081358 -2.8365374 -2.9813187 -2.6900964 -2.2400253 -1.5225093 -0.77778673 -0.66034389 -1.439569 -2.2004149 -2.3887424 -2.2178133 -2.1888285 -2.4461856][-2.7389343 -2.7558355 -3.0459926 -3.0447922 -2.7509081 -2.5417016 -2.3039517 -2.056623 -1.9892743 -2.185823 -2.3503006 -2.3581088 -2.3112762 -2.3798587 -2.56909][-3.3620486 -3.4045887 -3.4373446 -3.0860789 -2.685972 -2.6069231 -2.5908356 -2.4732718 -2.2739241 -2.0993612 -1.9990513 -2.06752 -2.279012 -2.4782715 -2.575017][-3.7983835 -3.8084226 -3.6023464 -2.9916577 -2.5312696 -2.5354161 -2.6126914 -2.5194221 -2.2894626 -2.072695 -1.9905057 -2.2164762 -2.645483 -2.9074004 -2.80689][-3.8900197 -3.8766348 -3.5881104 -2.956635 -2.5615206 -2.6641092 -2.8325672 -2.8164411 -2.67383 -2.565155 -2.5623455 -2.8548982 -3.3349171 -3.6001074 -3.3541248][-3.7225351 -3.721689 -3.5153043 -3.042388 -2.7626622 -2.9051356 -3.1050377 -3.1393747 -3.0550885 -2.9787502 -2.9486136 -3.1537962 -3.5453568 -3.8048687 -3.5952547]]...]
INFO - root - 2017-12-07 03:43:33.415670: step 1710, loss = 1.16, batch loss = 1.08 (10.6 examples/sec; 0.753 sec/batch; 69h:12m:45s remains)
INFO - root - 2017-12-07 03:43:41.181480: step 1720, loss = 0.68, batch loss = 0.61 (10.6 examples/sec; 0.755 sec/batch; 69h:21m:14s remains)
INFO - root - 2017-12-07 03:43:49.001012: step 1730, loss = 0.81, batch loss = 0.73 (10.4 examples/sec; 0.772 sec/batch; 70h:57m:51s remains)
INFO - root - 2017-12-07 03:43:56.650389: step 1740, loss = 0.95, batch loss = 0.88 (10.3 examples/sec; 0.776 sec/batch; 71h:16m:12s remains)
INFO - root - 2017-12-07 03:44:04.397412: step 1750, loss = 0.94, batch loss = 0.87 (10.0 examples/sec; 0.801 sec/batch; 73h:36m:20s remains)
INFO - root - 2017-12-07 03:44:12.058038: step 1760, loss = 0.97, batch loss = 0.89 (10.6 examples/sec; 0.756 sec/batch; 69h:25m:21s remains)
INFO - root - 2017-12-07 03:44:19.759028: step 1770, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.772 sec/batch; 70h:53m:24s remains)
INFO - root - 2017-12-07 03:44:27.227713: step 1780, loss = 0.93, batch loss = 0.86 (10.3 examples/sec; 0.778 sec/batch; 71h:27m:23s remains)
INFO - root - 2017-12-07 03:44:34.901547: step 1790, loss = 0.68, batch loss = 0.61 (10.0 examples/sec; 0.804 sec/batch; 73h:49m:06s remains)
INFO - root - 2017-12-07 03:44:42.697471: step 1800, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.758 sec/batch; 69h:35m:51s remains)
2017-12-07 03:44:43.284692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0729253 -2.8091857 -2.2413628 -1.4013867 -0.8991456 -0.88586164 -1.0487435 -0.87475276 -0.98928761 -1.4634273 -1.3803084 -1.0231481 -1.5366104 -2.2862771 -2.5863423][-3.0593472 -2.6529889 -1.9724348 -1.2634547 -0.89213538 -0.80378985 -0.81014514 -0.7973628 -1.235842 -1.6401238 -1.2086279 -0.87461686 -1.6175787 -2.4629755 -2.7636127][-2.9730773 -2.420723 -1.780261 -1.352361 -1.09256 -0.91241336 -0.831691 -1.0182288 -1.660903 -1.8395619 -1.0206707 -0.82726908 -1.8526678 -2.5851669 -2.6750932][-2.9387884 -2.3576231 -1.8714378 -1.6050768 -1.2470584 -0.90764713 -0.82110667 -1.246942 -2.0003011 -1.7961433 -0.6255331 -0.74899769 -2.0120683 -2.4732459 -2.3866878][-2.960202 -2.4918687 -2.2026956 -1.9389846 -1.3877068 -0.97406411 -0.95635939 -1.5683429 -2.2114983 -1.3780375 -0.00444746 -0.56907296 -1.8719001 -1.9502888 -1.8786867][-3.0203867 -2.7054796 -2.574399 -2.2326615 -1.5623059 -1.1428831 -1.1718616 -1.8041415 -2.0669162 -0.66090655 0.37743807 -0.736629 -1.7081697 -1.3512781 -1.4456487][-3.1194549 -2.891362 -2.7724438 -2.3001189 -1.6118557 -1.2016795 -1.1806455 -1.6312389 -1.3400402 0.24819994 0.46064234 -1.0032437 -1.4589996 -1.0880406 -1.6968889][-3.1321135 -2.8644447 -2.6463771 -2.0983479 -1.4689064 -1.0568287 -0.9971745 -1.2446568 -0.52462125 0.74700785 0.11995506 -1.1850641 -1.1312788 -1.1912947 -2.3378732][-3.0560994 -2.715992 -2.4043639 -1.8678682 -1.2650077 -0.79670048 -0.73431134 -0.79471445 0.040449619 0.56386089 -0.57259321 -1.4068909 -1.1503294 -1.7762814 -2.9650133][-3.0056934 -2.613894 -2.2253065 -1.7546761 -1.1317239 -0.61980152 -0.60356379 -0.51602268 0.16189766 -0.056754112 -1.2282658 -1.566942 -1.511579 -2.5515339 -3.3441417][-3.0082288 -2.5595007 -2.0853181 -1.6966116 -1.0827219 -0.57662535 -0.61204624 -0.45907092 -0.13882732 -0.78552604 -1.6805751 -1.8182077 -2.2490482 -3.4032953 -3.5979631][-2.9658117 -2.4515204 -1.9540989 -1.6871409 -1.0758154 -0.549947 -0.57080436 -0.43685126 -0.48579812 -1.2539318 -1.8377385 -2.084307 -2.9408422 -3.862772 -3.5195329][-2.878155 -2.3318248 -1.9172723 -1.7260852 -1.0068188 -0.40076637 -0.4606986 -0.42524385 -0.71697283 -1.4299173 -1.8643525 -2.3703623 -3.3824639 -3.9013994 -3.2375751][-2.7975168 -2.3094289 -2.0570626 -1.7677169 -0.7851367 -0.14985657 -0.40009737 -0.51837564 -0.89813161 -1.6264935 -2.1853013 -2.9676867 -3.989296 -4.2217927 -3.5340793][-2.7283196 -2.3747828 -2.2180822 -1.6471894 -0.40021944 0.10545254 -0.42659664 -0.723547 -1.1776633 -2.0194793 -2.7540705 -3.5929217 -4.4326315 -4.4500809 -3.8284712]]...]
INFO - root - 2017-12-07 03:44:51.142968: step 1810, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.761 sec/batch; 69h:51m:44s remains)
INFO - root - 2017-12-07 03:44:58.845674: step 1820, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 69h:42m:11s remains)
INFO - root - 2017-12-07 03:45:06.629665: step 1830, loss = 0.61, batch loss = 0.54 (10.2 examples/sec; 0.787 sec/batch; 72h:18m:49s remains)
INFO - root - 2017-12-07 03:45:14.288466: step 1840, loss = 0.76, batch loss = 0.68 (10.5 examples/sec; 0.765 sec/batch; 70h:17m:57s remains)
INFO - root - 2017-12-07 03:45:22.056483: step 1850, loss = 0.71, batch loss = 0.64 (10.7 examples/sec; 0.751 sec/batch; 68h:57m:47s remains)
INFO - root - 2017-12-07 03:45:29.723198: step 1860, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.755 sec/batch; 69h:21m:27s remains)
INFO - root - 2017-12-07 03:45:37.310399: step 1870, loss = 0.89, batch loss = 0.82 (11.0 examples/sec; 0.729 sec/batch; 66h:58m:26s remains)
INFO - root - 2017-12-07 03:45:44.873344: step 1880, loss = 0.80, batch loss = 0.73 (10.1 examples/sec; 0.794 sec/batch; 72h:52m:45s remains)
INFO - root - 2017-12-07 03:45:52.540032: step 1890, loss = 0.78, batch loss = 0.71 (10.8 examples/sec; 0.739 sec/batch; 67h:49m:33s remains)
INFO - root - 2017-12-07 03:46:00.269995: step 1900, loss = 0.74, batch loss = 0.66 (9.9 examples/sec; 0.808 sec/batch; 74h:14m:18s remains)
2017-12-07 03:46:00.874274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4915757 -1.3336217 -0.26637745 -0.1845808 -0.90130353 -1.4028938 -1.8497355 -2.3520582 -2.7818911 -2.9716086 -2.7249146 -2.2571867 -1.8475003 -1.6761446 -1.9999888][-2.0232396 -0.6843586 0.43250751 0.54348087 -0.13233471 -0.61866856 -1.1435175 -1.733175 -2.2182045 -2.4872296 -2.3533053 -1.9207029 -1.505933 -1.3622723 -1.7165062][-1.1840637 0.1524868 1.1174498 1.0938144 0.38150072 -0.097949505 -0.61300159 -1.1783595 -1.6406047 -1.9697049 -1.9719794 -1.6419206 -1.2874246 -1.1780174 -1.5004895][-0.509362 0.669364 1.3851161 1.198163 0.48852825 0.11061621 -0.19918489 -0.56766224 -0.96572924 -1.4091213 -1.5425682 -1.2925715 -1.0057802 -0.86643076 -1.0398819][-0.48249626 0.47735739 0.93332767 0.685236 0.07962656 -0.16270351 -0.20520353 -0.32223558 -0.715487 -1.3517699 -1.6637704 -1.4932384 -1.1738999 -0.83917093 -0.76332092][-0.90074515 -0.23776722 -0.01994276 -0.23973846 -0.66488171 -0.76424909 -0.58602262 -0.50061464 -0.85021591 -1.5866432 -2.0426195 -1.9507551 -1.551332 -0.99139142 -0.69117594][-1.3097136 -0.93041968 -0.80629325 -0.90835762 -1.1775875 -1.2583015 -1.0297117 -0.85537052 -1.0839427 -1.7144842 -2.2063155 -2.2291508 -1.8220656 -1.1785591 -0.80431914][-1.5376925 -1.2611618 -1.0906506 -1.0394492 -1.2240369 -1.3953729 -1.2828474 -1.1610065 -1.2866592 -1.6806564 -2.1159632 -2.3001614 -2.0520945 -1.548238 -1.2233953][-1.7507935 -1.4819291 -1.2440643 -1.043098 -1.0937085 -1.240726 -1.1931064 -1.1121004 -1.1346183 -1.3185246 -1.7365813 -2.1617551 -2.2225051 -1.9737811 -1.7345319][-1.8668799 -1.6267884 -1.3724403 -1.1396134 -1.1035414 -1.1745763 -1.1491261 -1.1136684 -1.075505 -1.1094611 -1.4486086 -1.9635048 -2.2385588 -2.1869631 -2.0446901][-1.9134102 -1.7425621 -1.5531445 -1.3927464 -1.3584058 -1.3690338 -1.3288624 -1.2822745 -1.2113352 -1.1596584 -1.3293841 -1.6734989 -1.8865743 -1.8487575 -1.7479091][-2.1662707 -2.06794 -1.9621031 -1.897362 -1.8950217 -1.8801515 -1.8281896 -1.7762015 -1.7273653 -1.6769555 -1.7276125 -1.8931 -1.98188 -1.9080946 -1.8068249][-2.63441 -2.5646887 -2.5067642 -2.5154114 -2.5587544 -2.5720983 -2.5547729 -2.53581 -2.5362098 -2.5406694 -2.5754077 -2.6726482 -2.7314262 -2.6699839 -2.5496435][-2.9709628 -2.9167221 -2.87676 -2.9039516 -2.9612687 -2.9950705 -3.0104556 -3.0100155 -3.0125098 -3.0128353 -3.0170574 -3.0568702 -3.0907345 -3.0540633 -2.955229][-3.1762507 -3.1363807 -3.1020994 -3.1124666 -3.1462088 -3.171047 -3.1897707 -3.1878605 -3.1802125 -3.1582255 -3.1174312 -3.09347 -3.0835547 -3.0443418 -2.9836578]]...]
INFO - root - 2017-12-07 03:46:08.515000: step 1910, loss = 0.66, batch loss = 0.59 (10.2 examples/sec; 0.785 sec/batch; 72h:05m:27s remains)
INFO - root - 2017-12-07 03:46:16.114584: step 1920, loss = 0.82, batch loss = 0.74 (10.2 examples/sec; 0.781 sec/batch; 71h:43m:55s remains)
INFO - root - 2017-12-07 03:46:23.780213: step 1930, loss = 1.02, batch loss = 0.95 (10.5 examples/sec; 0.765 sec/batch; 70h:17m:07s remains)
INFO - root - 2017-12-07 03:46:31.567876: step 1940, loss = 0.68, batch loss = 0.60 (10.1 examples/sec; 0.790 sec/batch; 72h:34m:12s remains)
INFO - root - 2017-12-07 03:46:39.323938: step 1950, loss = 0.97, batch loss = 0.90 (10.3 examples/sec; 0.778 sec/batch; 71h:28m:30s remains)
INFO - root - 2017-12-07 03:46:47.074223: step 1960, loss = 0.95, batch loss = 0.87 (10.3 examples/sec; 0.778 sec/batch; 71h:27m:28s remains)
INFO - root - 2017-12-07 03:46:54.929499: step 1970, loss = 0.94, batch loss = 0.86 (10.3 examples/sec; 0.778 sec/batch; 71h:27m:07s remains)
INFO - root - 2017-12-07 03:47:02.425860: step 1980, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.777 sec/batch; 71h:22m:18s remains)
INFO - root - 2017-12-07 03:47:10.133652: step 1990, loss = 0.91, batch loss = 0.84 (10.2 examples/sec; 0.786 sec/batch; 72h:09m:24s remains)
INFO - root - 2017-12-07 03:47:18.086012: step 2000, loss = 0.79, batch loss = 0.72 (9.9 examples/sec; 0.805 sec/batch; 73h:52m:46s remains)
2017-12-07 03:47:18.670305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3226893 -1.2222028 -1.2060745 -1.1564455 -1.0800207 -1.1179113 -1.1966202 -1.2480588 -1.2096078 -1.1615696 -1.187114 -1.2300777 -1.1810803 -1.0725441 -1.0353205][-1.4137089 -1.3266833 -1.2973392 -1.2602458 -1.218231 -1.2691183 -1.3678126 -1.4262574 -1.3786163 -1.2878382 -1.2095428 -1.1557689 -1.0225463 -0.88825059 -0.90761352][-1.6054659 -1.6089344 -1.6123631 -1.6266496 -1.6475956 -1.7059734 -1.8042777 -1.8477297 -1.7727866 -1.6193795 -1.4226677 -1.2786121 -1.0950475 -0.92405677 -0.91760087][-1.9078686 -2.0507748 -2.1399715 -2.2484725 -2.3657582 -2.4369731 -2.5114188 -2.4961722 -2.3524919 -2.1179235 -1.8092859 -1.5721149 -1.3570559 -1.1641452 -1.123513][-2.2223444 -2.4880698 -2.6123147 -2.7224691 -2.8144808 -2.7978885 -2.7860765 -2.7065532 -2.5662189 -2.3858049 -2.1079793 -1.8651574 -1.6911361 -1.5384626 -1.4948463][-2.250138 -2.5898604 -2.7089281 -2.775568 -2.7786489 -2.614078 -2.4573581 -2.3001869 -2.2517369 -2.3268592 -2.3115854 -2.2397194 -2.2116427 -2.153522 -2.1073372][-2.1974564 -2.5414929 -2.6178641 -2.6258516 -2.5173745 -2.1631813 -1.7851377 -1.4949846 -1.5487819 -1.9539945 -2.2940094 -2.5046935 -2.714545 -2.8251476 -2.8275757][-2.3590438 -2.5873747 -2.5641479 -2.5150414 -2.328722 -1.8368096 -1.2778411 -0.84619093 -0.92586064 -1.5099573 -2.0423377 -2.4369888 -2.8195772 -3.0707736 -3.1434581][-2.4072983 -2.4736812 -2.4019094 -2.4024103 -2.2866538 -1.8610623 -1.3502836 -0.9394114 -0.9846735 -1.4864163 -1.9473691 -2.3326139 -2.7252703 -2.9737506 -3.022814][-2.3905156 -2.420481 -2.4020505 -2.4592683 -2.4022923 -2.102432 -1.7484446 -1.4490974 -1.4613526 -1.777539 -2.0670335 -2.3542018 -2.6763966 -2.8526735 -2.8197627][-2.268636 -2.357398 -2.4358706 -2.530422 -2.5157285 -2.3404388 -2.1476774 -1.9527407 -1.9373903 -2.0979106 -2.2599475 -2.4552269 -2.6910324 -2.78155 -2.6602645][-1.9435673 -2.0509729 -2.19807 -2.3597574 -2.4651742 -2.4809556 -2.4660773 -2.3533216 -2.2794187 -2.2867796 -2.3254328 -2.4223728 -2.5821266 -2.6091206 -2.4286127][-1.7833951 -1.8511658 -1.9783063 -2.1404259 -2.3107677 -2.4635363 -2.5775313 -2.5333557 -2.4239297 -2.3358893 -2.2889955 -2.3181062 -2.4411535 -2.4631352 -2.2887173][-1.7376349 -1.7479448 -1.8141272 -1.938509 -2.1075907 -2.3284843 -2.5256424 -2.5492845 -2.4591746 -2.3476622 -2.260051 -2.2413528 -2.3112097 -2.3069403 -2.1538792][-1.607296 -1.5753758 -1.6097944 -1.717783 -1.8793852 -2.109637 -2.3179591 -2.3665824 -2.2958605 -2.1680713 -2.0327625 -1.9545155 -1.9389632 -1.8836908 -1.7374618]]...]
INFO - root - 2017-12-07 03:47:26.460659: step 2010, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.756 sec/batch; 69h:24m:18s remains)
INFO - root - 2017-12-07 03:47:34.379235: step 2020, loss = 0.83, batch loss = 0.75 (10.1 examples/sec; 0.790 sec/batch; 72h:30m:37s remains)
INFO - root - 2017-12-07 03:47:42.168094: step 2030, loss = 0.73, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 70h:46m:25s remains)
INFO - root - 2017-12-07 03:47:50.036669: step 2040, loss = 0.81, batch loss = 0.74 (10.0 examples/sec; 0.801 sec/batch; 73h:32m:38s remains)
INFO - root - 2017-12-07 03:47:57.700735: step 2050, loss = 1.03, batch loss = 0.96 (10.8 examples/sec; 0.738 sec/batch; 67h:45m:01s remains)
INFO - root - 2017-12-07 03:48:05.402762: step 2060, loss = 0.84, batch loss = 0.77 (10.3 examples/sec; 0.775 sec/batch; 71h:07m:05s remains)
INFO - root - 2017-12-07 03:48:13.014217: step 2070, loss = 0.77, batch loss = 0.70 (10.7 examples/sec; 0.749 sec/batch; 68h:47m:07s remains)
INFO - root - 2017-12-07 03:48:20.521518: step 2080, loss = 0.79, batch loss = 0.72 (9.9 examples/sec; 0.808 sec/batch; 74h:10m:00s remains)
INFO - root - 2017-12-07 03:48:28.283367: step 2090, loss = 0.65, batch loss = 0.58 (10.5 examples/sec; 0.764 sec/batch; 70h:06m:35s remains)
INFO - root - 2017-12-07 03:48:36.159748: step 2100, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.772 sec/batch; 70h:51m:50s remains)
2017-12-07 03:48:36.705276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3278012 -2.3086591 -2.2437654 -1.8457398 -1.3382494 -1.3034933 -1.5699801 -1.7554495 -1.8776472 -2.2051647 -2.60502 -2.7444787 -2.6724858 -2.532887 -2.3809509][-2.4464087 -2.4121814 -2.3030066 -1.7774515 -1.0026751 -0.81692839 -1.2049475 -1.526135 -1.6318817 -1.9615433 -2.5333433 -2.9330263 -3.0168207 -2.8783693 -2.6475005][-2.5436172 -2.4726555 -2.2801847 -1.6448874 -0.63832211 -0.19754076 -0.58751154 -1.0587888 -1.1915238 -1.489325 -2.1944642 -2.9386911 -3.2956376 -3.2301195 -2.9237878][-2.6418161 -2.5181956 -2.2180519 -1.5012162 -0.32299089 0.44970989 0.18051338 -0.47138143 -0.72296071 -0.93703175 -1.6461976 -2.6737223 -3.3696144 -3.4820697 -3.1761436][-2.7502747 -2.5786572 -2.1776381 -1.4199734 -0.16307926 0.94251347 0.89651346 0.040195942 -0.45249867 -0.54672527 -1.1086209 -2.2482586 -3.2016058 -3.5325036 -3.3341885][-2.8617325 -2.6390009 -2.168457 -1.4417491 -0.24143267 1.1057858 1.3433504 0.34452724 -0.44568825 -0.41858625 -0.72750425 -1.7931423 -2.8637338 -3.3768058 -3.3574486][-2.9964457 -2.7370749 -2.249819 -1.6086917 -0.56158376 0.88801336 1.5098677 0.57065296 -0.48079729 -0.49432921 -0.60059285 -1.4926169 -2.5609584 -3.186085 -3.3238554][-3.173625 -2.9306109 -2.4974661 -1.9722896 -1.0997281 0.32518482 1.3945117 0.81098032 -0.32860661 -0.56414628 -0.64160728 -1.3867121 -2.4341002 -3.126 -3.3428526][-3.3323476 -3.1518958 -2.8197136 -2.3873322 -1.6690876 -0.37965107 0.99117851 0.97708464 0.0016312599 -0.51008582 -0.71268487 -1.3921585 -2.433856 -3.1774445 -3.4168882][-3.3101709 -3.219229 -2.9985743 -2.6234832 -2.0132737 -0.93489385 0.48644829 0.99379539 0.33080006 -0.38480377 -0.746331 -1.3971419 -2.4196608 -3.1939821 -3.4329624][-3.1996102 -3.2215304 -3.1304431 -2.8355205 -2.3319247 -1.4973676 -0.22531176 0.58373308 0.28775072 -0.39922523 -0.806597 -1.3938835 -2.3711529 -3.1723332 -3.4392214][-3.2073774 -3.3408413 -3.4028165 -3.2372739 -2.8532495 -2.2385535 -1.1846664 -0.29475594 -0.23437405 -0.66742349 -1.0149796 -1.5419295 -2.4727683 -3.2862151 -3.5753775][-3.2086697 -3.4039195 -3.61521 -3.6072733 -3.3680935 -2.9098394 -2.0492752 -1.2014985 -0.87438488 -1.0019894 -1.2623525 -1.786907 -2.6851194 -3.4592748 -3.7306061][-2.9956994 -3.1711302 -3.4611075 -3.5732887 -3.4678617 -3.15816 -2.4982543 -1.7976458 -1.3579266 -1.2650256 -1.4297726 -1.9465926 -2.7519522 -3.407964 -3.6205952][-2.4512103 -2.5651052 -2.8415546 -3.0018051 -2.9762249 -2.8050151 -2.390518 -1.9235373 -1.5463195 -1.3685672 -1.4352853 -1.8557796 -2.4714861 -2.938694 -3.0675104]]...]
INFO - root - 2017-12-07 03:48:44.257013: step 2110, loss = 0.80, batch loss = 0.72 (10.4 examples/sec; 0.771 sec/batch; 70h:46m:21s remains)
INFO - root - 2017-12-07 03:48:51.935538: step 2120, loss = 0.84, batch loss = 0.77 (10.3 examples/sec; 0.774 sec/batch; 71h:02m:15s remains)
INFO - root - 2017-12-07 03:48:59.567145: step 2130, loss = 0.62, batch loss = 0.55 (10.7 examples/sec; 0.751 sec/batch; 68h:52m:56s remains)
INFO - root - 2017-12-07 03:49:07.207690: step 2140, loss = 0.82, batch loss = 0.74 (10.2 examples/sec; 0.783 sec/batch; 71h:51m:49s remains)
INFO - root - 2017-12-07 03:49:14.884392: step 2150, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 70h:55m:12s remains)
INFO - root - 2017-12-07 03:49:22.462726: step 2160, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.759 sec/batch; 69h:36m:38s remains)
INFO - root - 2017-12-07 03:49:30.171795: step 2170, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.760 sec/batch; 69h:43m:15s remains)
INFO - root - 2017-12-07 03:49:37.565329: step 2180, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.767 sec/batch; 70h:23m:54s remains)
INFO - root - 2017-12-07 03:49:45.127337: step 2190, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 69h:08m:31s remains)
INFO - root - 2017-12-07 03:49:52.864410: step 2200, loss = 0.65, batch loss = 0.58 (10.5 examples/sec; 0.765 sec/batch; 70h:09m:07s remains)
2017-12-07 03:49:53.456345: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.93813229 0.76674128 -0.088846207 -0.86073828 -0.99402928 -0.67017007 -0.352108 -0.225173 -0.3623538 -0.73715806 -1.0248406 -1.157264 -1.1686718 -1.1261334 -1.1192997][1.4348087 1.1647067 0.13744831 -0.72493911 -0.79528737 -0.34716654 0.062805653 0.23096991 -0.035848618 -0.61900306 -1.03234 -1.1766346 -1.1509993 -1.0803466 -1.0615234][1.6754179 1.3990951 0.326571 -0.5330112 -0.52496409 -0.0027599335 0.46075344 0.63464832 0.17709827 -0.6317091 -1.145503 -1.2775764 -1.1971405 -1.091126 -1.0363035][1.4508939 1.1405554 0.18328333 -0.54664063 -0.42897081 0.17622948 0.75180149 0.97970724 0.39118052 -0.54319549 -1.10934 -1.2519362 -1.1695802 -1.0795355 -1.0201268][1.336586 0.9438076 0.07593298 -0.56657481 -0.40747547 0.25636387 0.96185255 1.2612395 0.61038971 -0.3357358 -0.91104531 -1.0861163 -1.0385618 -0.984715 -0.95968366][1.1995788 0.76966572 0.014442444 -0.4616456 -0.21571445 0.47798014 1.2663155 1.6009202 0.88903904 -0.060491562 -0.65569592 -0.89345217 -0.89128995 -0.85680366 -0.86241484][0.87312269 0.38479424 -0.29759598 -0.54191947 -0.10037804 0.68705273 1.5664492 1.9697795 1.2549658 0.3059144 -0.32845831 -0.64914107 -0.73641515 -0.74700356 -0.7909832][0.74815226 0.15285969 -0.57698822 -0.67629886 -0.094785213 0.73342657 1.584156 1.9793534 1.3525362 0.47375441 -0.15882826 -0.53710866 -0.690732 -0.71888471 -0.77927017][0.72918653 0.075027466 -0.68544364 -0.72123265 -0.08295536 0.72419214 1.4268198 1.6651955 1.0927801 0.29441929 -0.34597158 -0.70376587 -0.78907394 -0.73664474 -0.76878738][0.86002636 0.15055037 -0.64957094 -0.70286679 -0.080983162 0.72365189 1.3260369 1.4263058 0.85854483 0.080327034 -0.5930748 -0.92250681 -0.90644932 -0.75084424 -0.74081945][1.2530537 0.50333786 -0.38604784 -0.57441735 -0.075953484 0.62273312 1.1096668 1.0996857 0.4982872 -0.2851572 -0.90147042 -1.1306801 -1.0135 -0.81204247 -0.76941514][1.484118 0.81868649 -0.062122345 -0.34006119 0.063613892 0.6558094 1.0249429 0.9036727 0.20666981 -0.61578631 -1.1418376 -1.2803142 -1.1340389 -0.95543075 -0.88374615][1.2511897 0.67333031 -0.11182356 -0.40057039 0.0038113594 0.59867573 1.0130363 0.94604063 0.22739887 -0.60285425 -1.0798635 -1.199569 -1.0960877 -0.97983217 -0.91949964][1.1187987 0.591352 -0.11200809 -0.43462229 -0.09189558 0.47851181 1.0020084 1.0818825 0.36720705 -0.48018742 -0.94859815 -1.0441539 -0.9287293 -0.81166387 -0.75838256][1.0283632 0.55749941 -0.043669224 -0.28031731 0.1112566 0.71443033 1.3616366 1.5366335 0.75697851 -0.19886827 -0.74613833 -0.85028768 -0.70595622 -0.54383135 -0.47740555]]...]
INFO - root - 2017-12-07 03:50:01.119230: step 2210, loss = 0.99, batch loss = 0.91 (9.9 examples/sec; 0.812 sec/batch; 74h:29m:55s remains)
INFO - root - 2017-12-07 03:50:08.798603: step 2220, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.750 sec/batch; 68h:48m:53s remains)
INFO - root - 2017-12-07 03:50:16.409068: step 2230, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.763 sec/batch; 69h:57m:53s remains)
INFO - root - 2017-12-07 03:50:24.091485: step 2240, loss = 0.78, batch loss = 0.70 (10.2 examples/sec; 0.784 sec/batch; 71h:56m:26s remains)
INFO - root - 2017-12-07 03:50:31.708325: step 2250, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.752 sec/batch; 69h:01m:21s remains)
INFO - root - 2017-12-07 03:50:39.418687: step 2260, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.776 sec/batch; 71h:09m:14s remains)
INFO - root - 2017-12-07 03:50:47.130492: step 2270, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.786 sec/batch; 72h:05m:58s remains)
INFO - root - 2017-12-07 03:50:54.556514: step 2280, loss = 0.91, batch loss = 0.83 (10.3 examples/sec; 0.780 sec/batch; 71h:31m:20s remains)
INFO - root - 2017-12-07 03:51:02.191403: step 2290, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 70h:41m:51s remains)
INFO - root - 2017-12-07 03:51:09.811370: step 2300, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.763 sec/batch; 69h:56m:52s remains)
2017-12-07 03:51:10.422687: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9255867 -1.8122683 -1.7351954 -1.6323659 -1.5775456 -1.7491271 -1.806618 -1.7977931 -1.8145554 -1.780406 -1.7771358 -1.5578966 -1.2754943 -1.3249123 -1.6416209][-1.5673344 -1.2328732 -1.028336 -0.92654395 -0.97360396 -1.1677468 -1.1708274 -1.0432553 -0.99475312 -0.97015309 -1.0686769 -1.0860686 -1.0876775 -1.3202908 -1.6042743][-1.0353415 -0.42891264 -0.090939045 -0.041367054 -0.21962309 -0.43193269 -0.38253164 -0.17786741 -0.17213583 -0.28287077 -0.6423254 -1.0761094 -1.4737759 -1.8599679 -2.0038836][-0.70404506 0.097201824 0.50194311 0.46546173 0.20739031 0.0251503 0.1743288 0.45597935 0.43416834 0.17152071 -0.42554283 -1.1868169 -1.8536143 -2.3067317 -2.3390577][-0.72151184 0.035640717 0.41542816 0.36812115 0.21871328 0.18103886 0.46347046 0.765255 0.60719824 0.12521553 -0.64721251 -1.5757687 -2.3130732 -2.6531391 -2.5867088][-1.0094767 -0.42727518 -0.097466946 -0.088770866 0.021568298 0.24854851 0.72120476 1.0516691 0.6273632 -0.18395138 -1.1233771 -2.0351732 -2.570926 -2.6458364 -2.5135326][-1.4022431 -1.0151942 -0.66820741 -0.493886 -0.067806721 0.51953745 1.3354812 1.8362594 1.1203475 -0.1403861 -1.3357592 -2.233058 -2.5602732 -2.4696989 -2.3607016][-1.5754735 -1.233047 -0.74095821 -0.39069414 0.20921946 1.0182996 2.06779 2.6100459 1.5723429 -0.11298943 -1.542367 -2.3653226 -2.5230031 -2.4069622 -2.3748651][-1.3898942 -0.99201107 -0.42154074 -0.0422225 0.53304958 1.2945561 2.1484542 2.4781795 1.289124 -0.46926832 -1.8241332 -2.3909006 -2.385911 -2.342803 -2.4313245][-0.85609651 -0.51758361 -0.10990381 0.085558414 0.43623877 0.87517309 1.2795701 1.2764297 0.2046361 -1.2115271 -2.1937189 -2.4438717 -2.3348789 -2.3674238 -2.5373454][-0.54436922 -0.35317278 -0.24971581 -0.29162121 -0.13371277 0.10352802 0.25654888 0.042307854 -0.85802221 -1.8225417 -2.3795736 -2.4718604 -2.3927863 -2.4917631 -2.6792698][-0.68599868 -0.69491124 -0.83668423 -0.94976044 -0.78669977 -0.54432774 -0.46054649 -0.78107834 -1.5470412 -2.1666772 -2.46912 -2.5448751 -2.5483537 -2.6551387 -2.7810137][-1.2641008 -1.3949654 -1.5080254 -1.3966613 -1.0542195 -0.78730011 -0.82578874 -1.284631 -1.9584994 -2.3949153 -2.5916026 -2.6832018 -2.7171879 -2.7789803 -2.8215096][-1.9399052 -1.9968593 -1.9201825 -1.5876076 -1.200047 -1.0641077 -1.2830877 -1.7827799 -2.3267107 -2.6459179 -2.776176 -2.8219767 -2.8228111 -2.8375049 -2.8233595][-1.973074 -2.0208972 -1.9253538 -1.6501741 -1.4617746 -1.5961483 -1.9689553 -2.4121695 -2.7901926 -2.9790926 -2.9994345 -2.9494052 -2.8968039 -2.863091 -2.8191121]]...]
INFO - root - 2017-12-07 03:51:18.163412: step 2310, loss = 0.90, batch loss = 0.82 (10.6 examples/sec; 0.752 sec/batch; 68h:56m:28s remains)
INFO - root - 2017-12-07 03:51:25.921191: step 2320, loss = 0.96, batch loss = 0.89 (10.3 examples/sec; 0.779 sec/batch; 71h:25m:32s remains)
INFO - root - 2017-12-07 03:51:33.675316: step 2330, loss = 1.15, batch loss = 1.08 (10.6 examples/sec; 0.758 sec/batch; 69h:32m:10s remains)
INFO - root - 2017-12-07 03:51:41.429176: step 2340, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.775 sec/batch; 71h:04m:10s remains)
INFO - root - 2017-12-07 03:51:49.024969: step 2350, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.761 sec/batch; 69h:46m:35s remains)
INFO - root - 2017-12-07 03:51:56.760532: step 2360, loss = 0.66, batch loss = 0.58 (10.5 examples/sec; 0.762 sec/batch; 69h:52m:25s remains)
INFO - root - 2017-12-07 03:52:04.447407: step 2370, loss = 0.74, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 70h:17m:27s remains)
INFO - root - 2017-12-07 03:52:12.035988: step 2380, loss = 0.69, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 71h:51m:36s remains)
INFO - root - 2017-12-07 03:52:19.779785: step 2390, loss = 0.86, batch loss = 0.78 (10.4 examples/sec; 0.770 sec/batch; 70h:34m:40s remains)
INFO - root - 2017-12-07 03:52:27.498124: step 2400, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 72h:55m:51s remains)
2017-12-07 03:52:28.082589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7407157 -2.5864997 -2.4929788 -2.3854649 -2.2130466 -2.1527708 -2.1717765 -2.2860804 -2.6285343 -2.82966 -2.750474 -2.7893794 -2.9400384 -2.9319673 -2.8803468][-2.5512495 -2.483881 -2.3674645 -2.2696774 -2.2485428 -2.3772538 -2.4886119 -2.5901186 -2.8065667 -2.7948494 -2.6374719 -2.7566175 -2.9374757 -2.8762145 -2.821512][-2.6936798 -2.7092438 -2.5393031 -2.4812593 -2.6091795 -2.892482 -3.0025568 -2.9589572 -2.9121728 -2.61674 -2.3783131 -2.6188293 -2.8752232 -2.8005836 -2.7612813][-3.0140553 -3.0527868 -2.8334928 -2.8048983 -2.9996195 -3.2892506 -3.2790909 -2.9927752 -2.6676531 -2.1666169 -1.8998382 -2.3059709 -2.7271149 -2.7169497 -2.6839294][-3.1592064 -3.108536 -2.843863 -2.8669977 -3.164768 -3.4259715 -3.2251034 -2.5998588 -1.9586399 -1.3228416 -1.1130178 -1.78405 -2.5045674 -2.6438575 -2.6344712][-3.1735702 -2.9849443 -2.6666291 -2.7399287 -3.1221509 -3.3418674 -2.8815689 -1.8503287 -0.93428349 -0.345407 -0.33025885 -1.336077 -2.4318435 -2.7551434 -2.7202096][-3.2857888 -3.0028806 -2.6098161 -2.6559477 -2.993619 -3.0698986 -2.35542 -1.0580812 -0.16089869 0.0649147 -0.26661634 -1.5216506 -2.7890661 -3.1397443 -2.944994][-3.4357016 -3.2184196 -2.850337 -2.9072475 -3.1679242 -3.049489 -2.1973679 -0.96803355 -0.4661305 -0.79759955 -1.3511763 -2.4437025 -3.4936891 -3.6293468 -3.2043853][-3.6742978 -3.6177304 -3.3341556 -3.418159 -3.6670961 -3.4597061 -2.6220202 -1.649888 -1.5563412 -2.1795912 -2.6216886 -3.2820084 -3.9497437 -3.8887994 -3.3791385][-4.0773854 -4.1068249 -3.8094196 -3.85877 -4.1324649 -3.8883667 -3.0787606 -2.3081746 -2.3545241 -2.9000258 -3.0638394 -3.3430538 -3.7945487 -3.7775593 -3.4022107][-4.3408484 -4.2806473 -3.868295 -3.8140283 -4.0713124 -3.8320923 -3.0783105 -2.4475393 -2.484118 -2.8741159 -2.8577781 -2.9788291 -3.435173 -3.5716667 -3.3539889][-4.1777887 -4.0079689 -3.602525 -3.5307899 -3.7602227 -3.4934249 -2.7899361 -2.2477829 -2.2603621 -2.5722518 -2.5607648 -2.737812 -3.2504549 -3.4562325 -3.3008099][-3.9837284 -3.745141 -3.3730085 -3.2686141 -3.3639116 -3.0217357 -2.3655663 -1.8627145 -1.8584483 -2.2068787 -2.3344975 -2.6610436 -3.2157414 -3.4154959 -3.2669382][-3.9135535 -3.6769097 -3.3307688 -3.1403203 -3.0738316 -2.738291 -2.1486175 -1.6704342 -1.6755369 -2.1019628 -2.3902147 -2.8143663 -3.3157854 -3.4212379 -3.2325416][-3.4687152 -3.3735394 -3.1677933 -2.991065 -2.8875575 -2.71075 -2.2558391 -1.8158832 -1.8692248 -2.3710854 -2.769608 -3.2294626 -3.6022465 -3.5349278 -3.2386703]]...]
INFO - root - 2017-12-07 03:52:35.700059: step 2410, loss = 0.90, batch loss = 0.82 (10.4 examples/sec; 0.767 sec/batch; 70h:18m:42s remains)
INFO - root - 2017-12-07 03:52:43.431323: step 2420, loss = 0.93, batch loss = 0.86 (10.8 examples/sec; 0.741 sec/batch; 67h:58m:47s remains)
INFO - root - 2017-12-07 03:52:51.044514: step 2430, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.772 sec/batch; 70h:46m:33s remains)
INFO - root - 2017-12-07 03:52:58.848908: step 2440, loss = 0.73, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 72h:40m:30s remains)
INFO - root - 2017-12-07 03:53:06.641302: step 2450, loss = 0.78, batch loss = 0.70 (10.5 examples/sec; 0.758 sec/batch; 69h:31m:59s remains)
INFO - root - 2017-12-07 03:53:14.383728: step 2460, loss = 0.61, batch loss = 0.54 (10.6 examples/sec; 0.751 sec/batch; 68h:52m:55s remains)
INFO - root - 2017-12-07 03:53:22.027051: step 2470, loss = 0.75, batch loss = 0.67 (10.2 examples/sec; 0.786 sec/batch; 72h:00m:54s remains)
INFO - root - 2017-12-07 03:53:29.388411: step 2480, loss = 0.75, batch loss = 0.67 (10.5 examples/sec; 0.764 sec/batch; 70h:02m:43s remains)
INFO - root - 2017-12-07 03:53:37.182601: step 2490, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.767 sec/batch; 70h:18m:09s remains)
INFO - root - 2017-12-07 03:53:44.810289: step 2500, loss = 0.73, batch loss = 0.65 (10.8 examples/sec; 0.739 sec/batch; 67h:43m:57s remains)
2017-12-07 03:53:45.399391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6610386 -1.7522926 -1.7337453 -1.6522503 -1.5452788 -1.4872556 -1.5072663 -1.5539362 -1.5653775 -1.5261333 -1.4590545 -1.3496025 -1.2322614 -1.1184781 -1.0218327][-1.6750259 -1.796818 -1.8097486 -1.7527208 -1.6525443 -1.6039753 -1.6542141 -1.7236147 -1.7235894 -1.6466999 -1.5591471 -1.4415619 -1.3243415 -1.2099383 -1.1048536][-1.6602869 -1.8326795 -1.9284456 -1.9431005 -1.8853407 -1.8342404 -1.854682 -1.8819456 -1.8095851 -1.6527343 -1.5371263 -1.4550877 -1.3840721 -1.2997923 -1.2047243][-1.7592421 -2.0134237 -2.2111194 -2.3003175 -2.2702703 -2.1650188 -2.0806582 -2.0079513 -1.8210218 -1.5463729 -1.4181871 -1.4200833 -1.4315822 -1.376672 -1.288609][-1.8777707 -2.2127552 -2.4258199 -2.4817457 -2.4223583 -2.231092 -2.0494587 -1.9279838 -1.6893256 -1.360568 -1.2937582 -1.4505148 -1.5653138 -1.5043015 -1.3823411][-1.9120574 -2.2590809 -2.3456442 -2.2487073 -2.1135471 -1.8273752 -1.5908554 -1.5067728 -1.3172536 -1.0422046 -1.1177785 -1.4724002 -1.7129116 -1.6448455 -1.4616842][-1.767489 -2.0034361 -1.8815711 -1.6117876 -1.4065783 -1.0776875 -0.87564778 -0.91853333 -0.84663987 -0.67576575 -0.86783242 -1.3527601 -1.6940207 -1.6405933 -1.412643][-1.4018941 -1.4886858 -1.2368391 -0.90009093 -0.71173263 -0.44918394 -0.35193491 -0.52777576 -0.55568409 -0.46429086 -0.64956856 -1.0971692 -1.4508247 -1.409766 -1.1826289][-0.91950107 -0.94097829 -0.72308087 -0.4823916 -0.41305828 -0.28898144 -0.25306368 -0.39845133 -0.40168476 -0.29798794 -0.37889433 -0.6829772 -0.98566413 -0.97289419 -0.83402371][-0.47951627 -0.51253581 -0.38760805 -0.28340816 -0.31229162 -0.25931168 -0.17390919 -0.18957138 -0.13316917 -0.019574165 -0.042561531 -0.28076124 -0.57383609 -0.62179494 -0.60064197][-0.17089367 -0.23146105 -0.18194675 -0.15796614 -0.19863605 -0.13718367 0.018354416 0.1155777 0.18050814 0.21976757 0.12478495 -0.17182732 -0.50606537 -0.5799098 -0.58304977][-0.011533737 -0.076525211 -0.10383701 -0.16611099 -0.24188375 -0.18401194 -0.0050640106 0.13517523 0.17620802 0.15568733 0.019296646 -0.29184103 -0.60975647 -0.65982437 -0.63154435][-0.0043139458 -0.035638332 -0.11768007 -0.27063322 -0.39809179 -0.38002825 -0.25281477 -0.13821459 -0.1081109 -0.11428642 -0.19327259 -0.41604757 -0.64148331 -0.63531876 -0.60274911][-0.050273895 -0.058088779 -0.15318298 -0.32976007 -0.45649862 -0.473171 -0.43502116 -0.38355732 -0.35882664 -0.32043791 -0.31560898 -0.43138695 -0.56259155 -0.54440928 -0.56466937][-0.21486998 -0.22895336 -0.31862497 -0.45658565 -0.50992846 -0.51353312 -0.53361154 -0.54011106 -0.53653765 -0.48839808 -0.44188356 -0.48226786 -0.55587316 -0.5640974 -0.6271708]]...]
INFO - root - 2017-12-07 03:53:53.001297: step 2510, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 69h:20m:45s remains)
INFO - root - 2017-12-07 03:54:00.603768: step 2520, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.774 sec/batch; 70h:54m:10s remains)
INFO - root - 2017-12-07 03:54:08.313666: step 2530, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.772 sec/batch; 70h:47m:48s remains)
INFO - root - 2017-12-07 03:54:15.907834: step 2540, loss = 0.97, batch loss = 0.90 (10.7 examples/sec; 0.745 sec/batch; 68h:16m:14s remains)
INFO - root - 2017-12-07 03:54:23.483991: step 2550, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.764 sec/batch; 69h:59m:02s remains)
INFO - root - 2017-12-07 03:54:31.096345: step 2560, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 70h:21m:54s remains)
INFO - root - 2017-12-07 03:54:38.745995: step 2570, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.747 sec/batch; 68h:25m:01s remains)
INFO - root - 2017-12-07 03:54:46.159879: step 2580, loss = 0.89, batch loss = 0.82 (11.1 examples/sec; 0.724 sec/batch; 66h:19m:42s remains)
INFO - root - 2017-12-07 03:54:53.765546: step 2590, loss = 0.88, batch loss = 0.81 (10.2 examples/sec; 0.784 sec/batch; 71h:52m:41s remains)
INFO - root - 2017-12-07 03:55:01.495035: step 2600, loss = 1.01, batch loss = 0.93 (10.1 examples/sec; 0.790 sec/batch; 72h:24m:46s remains)
2017-12-07 03:55:02.067112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8714895 -2.9187131 -2.9013212 -2.8789387 -2.8682144 -2.8405876 -2.8184667 -2.8069723 -2.8030267 -2.7952268 -2.7805805 -2.7672462 -2.7516513 -2.7643898 -2.7959304][-2.472327 -2.5093262 -2.5136275 -2.553246 -2.6187382 -2.6439497 -2.6555238 -2.663291 -2.6639643 -2.6527925 -2.6305449 -2.6067696 -2.560771 -2.5450382 -2.5528243][-2.212147 -2.2210977 -2.1890812 -2.1891346 -2.2268171 -2.2326541 -2.2263355 -2.2203431 -2.2105241 -2.2043211 -2.2015569 -2.2013125 -2.1657352 -2.1549439 -2.15994][-2.234741 -2.2178032 -2.1176448 -2.0321062 -1.9981642 -1.9541082 -1.9078114 -1.8655071 -1.8278298 -1.7967336 -1.7735021 -1.7554085 -1.7100778 -1.6960475 -1.7003675][-1.9187372 -1.8951716 -1.7892919 -1.7003913 -1.6639395 -1.6227982 -1.5905812 -1.5753517 -1.5740852 -1.5698783 -1.5579641 -1.5264881 -1.4713194 -1.4393086 -1.4157331][-1.5310876 -1.3819144 -1.2161844 -1.1352022 -1.1421824 -1.186033 -1.2634847 -1.378535 -1.530179 -1.7082109 -1.8762889 -1.9888844 -2.039845 -2.0396721 -1.9885015][-1.8058493 -1.5553181 -1.3217494 -1.2254777 -1.2369788 -1.279217 -1.3358226 -1.4295614 -1.5967321 -1.8471758 -2.1088936 -2.3340745 -2.5074959 -2.5915761 -2.5750489][-2.0456374 -1.8602698 -1.726454 -1.7304208 -1.8143234 -1.8618684 -1.854279 -1.8350546 -1.8555665 -1.9254534 -2.0012741 -2.0813866 -2.1739328 -2.2251644 -2.1855054][-2.2316654 -2.051105 -1.9265118 -1.945761 -2.0842133 -2.206784 -2.2579916 -2.2559919 -2.2125256 -2.1339936 -2.0153253 -1.8762498 -1.7631745 -1.6576765 -1.5219996][-2.8397195 -2.76053 -2.6602044 -2.624907 -2.6922963 -2.7882559 -2.8571043 -2.8778863 -2.8167863 -2.6810679 -2.4847493 -2.2342653 -2.0098691 -1.8397229 -1.7007425][-3.1388884 -3.2354486 -3.3052306 -3.3840454 -3.5112119 -3.6531136 -3.7827291 -3.8756614 -3.8987627 -3.8431768 -3.6989989 -3.4495149 -3.1683502 -2.9066715 -2.6729426][-2.7027426 -2.7880607 -2.8955967 -3.0281971 -3.2163396 -3.4202039 -3.6091156 -3.7828717 -3.9570107 -4.106626 -4.1703253 -4.116117 -3.9761462 -3.7610261 -3.4852977][-1.9983439 -1.8783705 -1.7872186 -1.7450335 -1.792021 -1.9006464 -2.0154397 -2.1485498 -2.3492219 -2.5843253 -2.781461 -2.9107065 -2.9905989 -2.97754 -2.8485751][-1.6088879 -1.4596827 -1.3038192 -1.1687663 -1.0863018 -1.049619 -1.0100465 -0.98282528 -1.0212295 -1.0852072 -1.1380653 -1.1884842 -1.2757041 -1.3426001 -1.3437548][-1.6885443 -1.6949229 -1.6663275 -1.6292603 -1.5872877 -1.5340645 -1.4270949 -1.2988713 -1.2050574 -1.1163335 -1.0173159 -0.94392872 -0.94559813 -0.98206282 -1.0078247]]...]
INFO - root - 2017-12-07 03:55:09.752480: step 2610, loss = 0.87, batch loss = 0.79 (10.5 examples/sec; 0.763 sec/batch; 69h:55m:47s remains)
INFO - root - 2017-12-07 03:55:17.436347: step 2620, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.771 sec/batch; 70h:38m:10s remains)
INFO - root - 2017-12-07 03:55:25.176318: step 2630, loss = 0.73, batch loss = 0.66 (10.2 examples/sec; 0.788 sec/batch; 72h:09m:38s remains)
INFO - root - 2017-12-07 03:55:32.822451: step 2640, loss = 0.83, batch loss = 0.76 (9.9 examples/sec; 0.806 sec/batch; 73h:50m:33s remains)
INFO - root - 2017-12-07 03:55:40.508978: step 2650, loss = 0.75, batch loss = 0.68 (10.7 examples/sec; 0.751 sec/batch; 68h:46m:30s remains)
INFO - root - 2017-12-07 03:55:48.261804: step 2660, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.768 sec/batch; 70h:22m:42s remains)
INFO - root - 2017-12-07 03:55:56.039571: step 2670, loss = 0.95, batch loss = 0.88 (10.7 examples/sec; 0.749 sec/batch; 68h:39m:42s remains)
INFO - root - 2017-12-07 03:56:03.552592: step 2680, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.768 sec/batch; 70h:19m:31s remains)
INFO - root - 2017-12-07 03:56:11.320612: step 2690, loss = 1.06, batch loss = 0.99 (10.3 examples/sec; 0.773 sec/batch; 70h:50m:09s remains)
INFO - root - 2017-12-07 03:56:19.013020: step 2700, loss = 0.87, batch loss = 0.79 (10.5 examples/sec; 0.763 sec/batch; 69h:52m:28s remains)
2017-12-07 03:56:19.577028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.43539 -2.3767116 -2.4245415 -2.4629073 -2.4756093 -2.4814305 -2.4295409 -2.3805559 -2.3620663 -2.3494658 -2.3670502 -2.3995209 -2.4116678 -2.4202893 -2.4183125][-2.1092241 -1.9869206 -2.0449333 -2.1303005 -2.1731043 -2.1907794 -2.1246936 -2.0427268 -1.9995031 -1.9447858 -1.9330409 -1.9463959 -1.9470489 -1.960233 -1.947396][-2.0731614 -1.9811356 -2.0684853 -2.1965485 -2.2878172 -2.339453 -2.2908206 -2.2093675 -2.1656477 -2.0724216 -2.012183 -1.9782288 -1.9396653 -1.9197791 -1.8521347][-2.3285284 -2.2866156 -2.3680804 -2.5101328 -2.6533489 -2.7293358 -2.6668491 -2.5617974 -2.5387387 -2.4711015 -2.4351361 -2.441246 -2.4814632 -2.531909 -2.4675484][-2.5037217 -2.4694469 -2.4791555 -2.5717797 -2.7229383 -2.8060155 -2.7449081 -2.6535082 -2.7130187 -2.7521217 -2.7591963 -2.7703903 -2.8494616 -2.9423728 -2.8868248][-1.9474542 -1.7884724 -1.6774781 -1.7243655 -1.8760045 -1.978246 -1.9789455 -1.9362981 -2.0902157 -2.2947404 -2.3976028 -2.4204459 -2.5066624 -2.595269 -2.53412][-0.8156383 -0.49152017 -0.19614649 -0.10970974 -0.14833927 -0.23994732 -0.35035372 -0.41104126 -0.6318109 -0.926152 -1.1163867 -1.2411203 -1.4492443 -1.6312504 -1.6956182][-0.14482546 0.17539501 0.53164625 0.76089287 0.94255495 1.0082245 0.89920044 0.74428797 0.42804289 0.070233345 -0.14636898 -0.29468441 -0.50238609 -0.65758491 -0.75732684][-0.505867 -0.28962326 -0.099213123 0.015033722 0.15390205 0.2283535 0.17796278 0.06803894 -0.24710989 -0.60727 -0.83970618 -1.0164289 -1.1833489 -1.2405119 -1.2253299][-1.6683455 -1.642489 -1.604358 -1.5677214 -1.5329337 -1.609241 -1.7080672 -1.7543299 -1.9436886 -2.1011994 -2.1380882 -2.1740773 -2.2268622 -2.2158675 -2.1538143][-2.3881783 -2.5349121 -2.6324964 -2.6228628 -2.6125674 -2.7523274 -2.8841021 -2.9731464 -3.1869872 -3.2403493 -3.0592198 -2.8031349 -2.5989289 -2.4322934 -2.2464964][-2.0780554 -2.2335279 -2.3919449 -2.4831071 -2.5973439 -2.8091838 -2.9336581 -3.0454078 -3.3077083 -3.3766818 -3.1572385 -2.7655048 -2.4405651 -2.207741 -1.9683616][-1.5266199 -1.6711955 -1.8404036 -1.9944386 -2.1913939 -2.4100347 -2.4429703 -2.4387279 -2.5590734 -2.5693386 -2.403306 -2.1042576 -1.9088855 -1.8445837 -1.8028295][-1.3927836 -1.5324008 -1.7102709 -1.8587849 -2.0219724 -2.1581964 -2.112324 -2.0620959 -2.1110437 -2.1279149 -2.107928 -2.0133049 -1.9546902 -1.9490652 -1.9851902][-1.6190922 -1.6727839 -1.7911766 -1.9280314 -2.1118503 -2.2461412 -2.2359452 -2.2288616 -2.2575462 -2.2746449 -2.3417444 -2.393574 -2.4157987 -2.3756931 -2.354881]]...]
INFO - root - 2017-12-07 03:56:27.204374: step 2710, loss = 1.07, batch loss = 0.99 (10.3 examples/sec; 0.775 sec/batch; 71h:00m:21s remains)
INFO - root - 2017-12-07 03:56:34.981846: step 2720, loss = 0.98, batch loss = 0.91 (10.1 examples/sec; 0.794 sec/batch; 72h:44m:35s remains)
INFO - root - 2017-12-07 03:56:42.723883: step 2730, loss = 0.63, batch loss = 0.55 (10.4 examples/sec; 0.768 sec/batch; 70h:19m:47s remains)
INFO - root - 2017-12-07 03:56:50.333210: step 2740, loss = 0.96, batch loss = 0.88 (10.8 examples/sec; 0.741 sec/batch; 67h:54m:21s remains)
INFO - root - 2017-12-07 03:56:58.020168: step 2750, loss = 0.71, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 72h:23m:09s remains)
INFO - root - 2017-12-07 03:57:05.818637: step 2760, loss = 0.85, batch loss = 0.78 (10.7 examples/sec; 0.747 sec/batch; 68h:26m:27s remains)
INFO - root - 2017-12-07 03:57:13.576411: step 2770, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.786 sec/batch; 71h:59m:05s remains)
INFO - root - 2017-12-07 03:57:20.877909: step 2780, loss = 0.68, batch loss = 0.60 (10.0 examples/sec; 0.799 sec/batch; 73h:10m:21s remains)
INFO - root - 2017-12-07 03:57:28.598818: step 2790, loss = 0.93, batch loss = 0.85 (10.1 examples/sec; 0.790 sec/batch; 72h:20m:11s remains)
INFO - root - 2017-12-07 03:57:36.312291: step 2800, loss = 0.65, batch loss = 0.57 (10.6 examples/sec; 0.758 sec/batch; 69h:23m:06s remains)
2017-12-07 03:57:36.934708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7006717 -3.2791281 -3.9133663 -4.3100905 -4.1946177 -4.1500559 -3.5608237 -2.5024424 -2.2898092 -2.8059282 -3.278935 -3.5282254 -3.4330127 -3.3301051 -3.5306339][-1.3563044 -1.864521 -2.7177415 -3.4089751 -3.5109305 -3.6420512 -3.067996 -1.7877951 -1.4305339 -1.9779689 -2.5778468 -2.893898 -2.8537326 -2.9355507 -3.346688][-1.2083375 -1.1800182 -1.6923254 -2.2431819 -2.466347 -2.8537745 -2.5623653 -1.3777618 -1.0107713 -1.5700881 -2.2098544 -2.5055225 -2.4789147 -2.6761718 -3.2016356][-2.2425225 -1.6334486 -1.3823469 -1.2715666 -1.2904377 -1.9371195 -2.3114457 -1.7597234 -1.7001836 -2.3249221 -2.8935206 -3.0122228 -2.8173008 -2.8551018 -3.23942][-3.6227512 -2.6653185 -1.628037 -0.68523 -0.27964354 -1.0909743 -2.1834695 -2.3944819 -2.6791375 -3.3614957 -3.8890314 -3.9319968 -3.608927 -3.4067678 -3.5125017][-4.1363072 -3.2703934 -1.7977002 -0.30509377 0.42336845 -0.3467536 -1.6913459 -2.2877913 -2.8620973 -3.8401473 -4.6160359 -4.8118439 -4.4418759 -3.9979942 -3.8297119][-3.7730579 -3.2109909 -1.7907693 -0.26728678 0.45646477 -0.090731621 -1.0823333 -1.5115762 -2.159544 -3.4851561 -4.65781 -5.1780481 -4.9204812 -4.3754826 -4.0280919][-3.2615166 -2.9270296 -1.6685607 -0.33314037 0.16576242 -0.29578495 -0.84212852 -0.88806272 -1.3105936 -2.5941367 -3.8354707 -4.6047115 -4.6481061 -4.2857718 -4.0005751][-2.9996698 -2.7447791 -1.7400472 -0.719697 -0.41609669 -0.66164589 -0.71053076 -0.4082036 -0.63841414 -1.7281353 -2.846581 -3.7526631 -4.109354 -4.0221043 -3.8939867][-2.9814076 -2.8637187 -2.2875409 -1.6871681 -1.4909809 -1.4703848 -1.1278379 -0.56320691 -0.58231807 -1.3715169 -2.2370808 -3.0732734 -3.5149198 -3.6328311 -3.7273257][-3.0493698 -3.1968081 -2.9287612 -2.5621276 -2.4428809 -2.4614272 -2.0923407 -1.3471553 -1.1536279 -1.7431564 -2.4376745 -3.0423193 -3.2966585 -3.4411104 -3.6625679][-2.9876018 -3.3737807 -3.2504063 -2.976346 -2.9480753 -3.2057247 -3.0115194 -2.1248333 -1.7512739 -2.3487635 -3.1181169 -3.6249342 -3.73362 -3.8048954 -3.9391575][-3.0539689 -3.5601926 -3.5644779 -3.4532089 -3.4812734 -3.8927336 -3.8139648 -2.8747134 -2.4237921 -3.0864081 -3.9277549 -4.3391857 -4.3466663 -4.2863331 -4.2295432][-3.5100517 -4.0189462 -4.0688839 -4.0209351 -3.9834881 -4.3704104 -4.3200946 -3.4358959 -2.9996996 -3.6509807 -4.5121694 -4.9207072 -4.9005408 -4.66549 -4.3892045][-4.1347966 -4.4630594 -4.4850988 -4.4980783 -4.5108004 -4.8896751 -4.7775683 -3.7546434 -3.0919104 -3.4560888 -4.3010678 -4.9682975 -5.1380796 -4.8454075 -4.4491286]]...]
INFO - root - 2017-12-07 03:57:44.577772: step 2810, loss = 0.92, batch loss = 0.84 (10.7 examples/sec; 0.746 sec/batch; 68h:21m:22s remains)
INFO - root - 2017-12-07 03:57:52.122961: step 2820, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.753 sec/batch; 68h:59m:17s remains)
INFO - root - 2017-12-07 03:57:59.729228: step 2830, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.763 sec/batch; 69h:50m:42s remains)
INFO - root - 2017-12-07 03:58:07.271183: step 2840, loss = 0.69, batch loss = 0.62 (10.8 examples/sec; 0.744 sec/batch; 68h:05m:21s remains)
INFO - root - 2017-12-07 03:58:14.941880: step 2850, loss = 0.90, batch loss = 0.83 (10.1 examples/sec; 0.794 sec/batch; 72h:42m:39s remains)
INFO - root - 2017-12-07 03:58:22.600294: step 2860, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.745 sec/batch; 68h:15m:30s remains)
INFO - root - 2017-12-07 03:58:30.303243: step 2870, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.754 sec/batch; 69h:04m:16s remains)
INFO - root - 2017-12-07 03:58:37.646040: step 2880, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.779 sec/batch; 71h:21m:06s remains)
INFO - root - 2017-12-07 03:58:45.285301: step 2890, loss = 0.64, batch loss = 0.56 (10.3 examples/sec; 0.773 sec/batch; 70h:46m:11s remains)
INFO - root - 2017-12-07 03:58:52.935755: step 2900, loss = 1.04, batch loss = 0.97 (10.8 examples/sec; 0.740 sec/batch; 67h:42m:56s remains)
2017-12-07 03:58:53.534353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1995716 -4.1173291 -3.9875383 -3.8893795 -3.7919328 -3.8658309 -3.9275744 -3.9985528 -4.0749135 -4.1038222 -4.0215383 -3.845551 -3.7741244 -3.7665033 -3.803411][-4.1583467 -4.1039472 -4.04134 -4.0042033 -3.9439192 -4.0091405 -4.0182734 -4.0722294 -4.1569223 -4.1275816 -3.9335847 -3.7229643 -3.7218876 -3.79186 -3.8812165][-4.1658907 -4.1532092 -4.1843495 -4.2285862 -4.2090349 -4.2531724 -4.2021837 -4.2098184 -4.2623568 -4.1597242 -3.8910618 -3.6839974 -3.7523236 -3.8890653 -4.0042825][-4.1363659 -4.1827807 -4.3145819 -4.46006 -4.5088792 -4.5447679 -4.4213781 -4.3413987 -4.3161807 -4.1374288 -3.849299 -3.6968327 -3.8432908 -4.0332222 -4.1399255][-4.178453 -4.2883763 -4.4909849 -4.6973176 -4.8006072 -4.8243394 -4.6439443 -4.4771476 -4.3295155 -4.03977 -3.7227011 -3.6156754 -3.81671 -4.0446248 -4.1421471][-4.4589348 -4.6237421 -4.834208 -5.0248308 -5.1508121 -5.1887197 -4.9942508 -4.7569146 -4.4726181 -4.06084 -3.6991243 -3.5857439 -3.7589784 -3.9750304 -4.0861926][-4.7861938 -4.9436922 -5.0856428 -5.1719861 -5.2350082 -5.2511277 -5.0563855 -4.7955303 -4.4854093 -4.0920076 -3.8118286 -3.7405386 -3.867516 -4.0322537 -4.1245103][-5.0936584 -5.2307439 -5.2947006 -5.2759972 -5.2465162 -5.2174892 -5.0367365 -4.7999845 -4.5595541 -4.2744374 -4.1122255 -4.0959091 -4.2016559 -4.3353338 -4.3950667][-5.2171254 -5.3456097 -5.3721638 -5.325213 -5.2713575 -5.2380867 -5.1019797 -4.9124713 -4.7477293 -4.5337706 -4.4192476 -4.4420929 -4.5834556 -4.7555027 -4.8295436][-5.1850352 -5.2452621 -5.2268505 -5.1698256 -5.0982471 -5.0591192 -4.9662828 -4.8367147 -4.7471704 -4.5742292 -4.4606905 -4.5194807 -4.7550035 -5.0203514 -5.14452][-5.2064848 -5.2167068 -5.163527 -5.0511584 -4.8729877 -4.7482843 -4.6130872 -4.48593 -4.4444551 -4.3198409 -4.2263479 -4.3239079 -4.6437263 -4.9833088 -5.175736][-5.2232132 -5.2067327 -5.1104307 -4.8814869 -4.5342331 -4.3252783 -4.1822367 -4.0994415 -4.1314311 -4.0715384 -4.0141535 -4.1044412 -4.4040647 -4.7187033 -4.9593215][-5.19884 -5.1856031 -5.047183 -4.722024 -4.2648215 -4.0593529 -4.011816 -4.0555811 -4.1636243 -4.0907969 -3.9664211 -3.9426594 -4.0816708 -4.2721081 -4.5312247][-5.15907 -5.1686749 -5.0316172 -4.706243 -4.2667079 -4.1286917 -4.1539483 -4.2381468 -4.3125138 -4.15781 -3.9414253 -3.7987523 -3.7896838 -3.8853538 -4.1636252][-5.1350508 -5.1921749 -5.0951405 -4.8193169 -4.456778 -4.3562593 -4.3501325 -4.3810315 -4.3986011 -4.2358365 -4.0119529 -3.8110285 -3.7023866 -3.7252088 -3.97615]]...]
INFO - root - 2017-12-07 03:59:01.162804: step 2910, loss = 0.93, batch loss = 0.85 (10.8 examples/sec; 0.738 sec/batch; 67h:36m:20s remains)
INFO - root - 2017-12-07 03:59:08.778510: step 2920, loss = 0.91, batch loss = 0.84 (10.2 examples/sec; 0.788 sec/batch; 72h:05m:50s remains)
INFO - root - 2017-12-07 03:59:16.411269: step 2930, loss = 1.00, batch loss = 0.92 (10.8 examples/sec; 0.741 sec/batch; 67h:49m:26s remains)
INFO - root - 2017-12-07 03:59:24.027897: step 2940, loss = 0.62, batch loss = 0.55 (10.4 examples/sec; 0.767 sec/batch; 70h:14m:39s remains)
INFO - root - 2017-12-07 03:59:31.638059: step 2950, loss = 1.02, batch loss = 0.94 (10.3 examples/sec; 0.775 sec/batch; 70h:54m:36s remains)
INFO - root - 2017-12-07 03:59:39.343418: step 2960, loss = 1.03, batch loss = 0.95 (10.6 examples/sec; 0.758 sec/batch; 69h:23m:36s remains)
INFO - root - 2017-12-07 03:59:46.905140: step 2970, loss = 0.94, batch loss = 0.86 (10.2 examples/sec; 0.786 sec/batch; 71h:58m:54s remains)
INFO - root - 2017-12-07 03:59:54.365793: step 2980, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.768 sec/batch; 70h:20m:34s remains)
INFO - root - 2017-12-07 04:00:01.934865: step 2990, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.754 sec/batch; 68h:59m:42s remains)
INFO - root - 2017-12-07 04:00:09.606442: step 3000, loss = 0.91, batch loss = 0.84 (10.7 examples/sec; 0.748 sec/batch; 68h:30m:27s remains)
2017-12-07 04:00:10.201112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0016723 -2.3217194 -2.8259242 -3.0145521 -2.8403425 -2.5309927 -2.3047297 -2.0586658 -1.893265 -1.9159198 -2.0455194 -2.1337898 -2.3839216 -2.5586257 -2.3007209][-2.0289919 -2.3409016 -2.80095 -2.9551368 -2.8151038 -2.5500045 -2.3134222 -2.033494 -1.8379078 -1.79568 -1.8422904 -1.8862088 -2.0967023 -2.2597861 -2.0122681][-1.7746305 -1.9174507 -2.2408135 -2.3919942 -2.4010844 -2.3555431 -2.2278643 -2.0112829 -1.888063 -1.8449371 -1.8309426 -1.8101535 -1.9302678 -2.0183291 -1.7248108][-1.4995966 -1.5363293 -1.7393112 -1.8883941 -2.0607197 -2.2303169 -2.1890719 -2.0434084 -2.0345683 -2.0467255 -2.0141842 -1.9500263 -1.974365 -1.9721413 -1.6767042][-1.2800889 -1.3406086 -1.4233074 -1.5376291 -1.8479822 -2.1854887 -2.1249657 -1.9444203 -1.9885926 -2.0803227 -2.0811017 -2.002527 -1.981986 -1.9591615 -1.7813256][-0.99012375 -0.95013905 -0.75879073 -0.80001473 -1.2758851 -1.7880237 -1.6876786 -1.4082239 -1.5438738 -1.8221648 -1.9584186 -1.9357796 -1.9204021 -1.936867 -1.9312439][-0.93270922 -0.56626582 -0.0012159348 0.076929569 -0.4749639 -0.99881077 -0.718601 -0.23953962 -0.48056173 -1.043658 -1.3978801 -1.4923475 -1.5182135 -1.6179056 -1.8247733][-1.4902811 -0.88931727 -0.13083458 0.12119818 -0.2439518 -0.551234 -0.090385914 0.54752493 0.31888819 -0.39138889 -0.86812854 -1.0668254 -1.0823724 -1.1381583 -1.4240804][-2.4728343 -1.9305398 -1.2609589 -0.94089794 -1.0612292 -1.2371135 -0.93559694 -0.50020671 -0.65276289 -1.1915393 -1.5478799 -1.6582701 -1.5132341 -1.3357797 -1.4466588][-3.3189676 -3.0435023 -2.6958866 -2.5009246 -2.5257559 -2.6676815 -2.5847006 -2.3434184 -2.421314 -2.6988068 -2.7907794 -2.6275604 -2.252059 -1.8789401 -1.7962337][-3.8832023 -3.8976841 -3.8881845 -3.9176846 -4.0119305 -4.1697345 -4.1622634 -3.9076 -3.8125319 -3.8110034 -3.5681739 -3.1007674 -2.6368008 -2.3149304 -2.2075253][-4.176661 -4.355165 -4.5243783 -4.7019229 -4.8726053 -5.0543437 -5.0598674 -4.7380624 -4.458755 -4.216989 -3.7022996 -3.0619683 -2.676538 -2.5329347 -2.4577234][-4.1161056 -4.3223491 -4.5268426 -4.7258983 -4.8812742 -5.0209975 -5.011971 -4.6823688 -4.3596377 -4.0937424 -3.5740838 -2.9394419 -2.5814767 -2.4684405 -2.3237078][-3.8403516 -3.9691443 -4.1009541 -4.22759 -4.319088 -4.4058175 -4.3986773 -4.1369262 -3.8914886 -3.7728081 -3.515718 -3.1523132 -2.9456496 -2.860899 -2.6433487][-3.6248341 -3.6749759 -3.7248833 -3.7676296 -3.7980883 -3.8564122 -3.8920591 -3.7799993 -3.6931772 -3.7584913 -3.8006387 -3.7796068 -3.8091955 -3.8204548 -3.6481812]]...]
INFO - root - 2017-12-07 04:00:17.893738: step 3010, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.755 sec/batch; 69h:03m:24s remains)
INFO - root - 2017-12-07 04:00:25.511374: step 3020, loss = 0.94, batch loss = 0.86 (10.4 examples/sec; 0.768 sec/batch; 70h:18m:23s remains)
INFO - root - 2017-12-07 04:00:33.075706: step 3030, loss = 0.75, batch loss = 0.68 (10.8 examples/sec; 0.744 sec/batch; 68h:04m:44s remains)
INFO - root - 2017-12-07 04:00:40.733520: step 3040, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.774 sec/batch; 70h:51m:04s remains)
INFO - root - 2017-12-07 04:00:48.366449: step 3050, loss = 0.67, batch loss = 0.59 (10.7 examples/sec; 0.745 sec/batch; 68h:09m:45s remains)
INFO - root - 2017-12-07 04:00:55.974227: step 3060, loss = 0.99, batch loss = 0.92 (10.3 examples/sec; 0.773 sec/batch; 70h:44m:52s remains)
INFO - root - 2017-12-07 04:01:03.635780: step 3070, loss = 0.72, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 71h:04m:39s remains)
INFO - root - 2017-12-07 04:01:10.961826: step 3080, loss = 0.84, batch loss = 0.76 (10.6 examples/sec; 0.752 sec/batch; 68h:51m:09s remains)
INFO - root - 2017-12-07 04:01:18.644544: step 3090, loss = 0.96, batch loss = 0.88 (10.2 examples/sec; 0.782 sec/batch; 71h:33m:03s remains)
INFO - root - 2017-12-07 04:01:26.418402: step 3100, loss = 0.87, batch loss = 0.79 (10.4 examples/sec; 0.766 sec/batch; 70h:03m:29s remains)
2017-12-07 04:01:27.061002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7373846 -2.669735 -2.6694951 -2.7355921 -2.8301792 -2.95511 -3.1133413 -3.2212689 -3.2894306 -3.262392 -3.1258502 -3.1502674 -3.3347239 -3.3962088 -3.4049742][-2.9314313 -2.8497329 -2.8225238 -2.8506875 -2.9169459 -3.0542879 -3.2388582 -3.366616 -3.4376113 -3.3418751 -3.048687 -2.9865689 -3.1572266 -3.1757517 -3.1038618][-2.8035083 -2.68958 -2.6600823 -2.697021 -2.7932911 -3.0010872 -3.2595873 -3.4764564 -3.6030178 -3.4736562 -3.0628819 -2.9076815 -2.9745631 -2.8392415 -2.6176827][-2.8785214 -2.7024708 -2.62757 -2.6438341 -2.7087746 -2.8587093 -3.0253882 -3.2014294 -3.3449862 -3.1970112 -2.7320457 -2.5262539 -2.5281656 -2.2853415 -1.94677][-2.90798 -2.6943378 -2.5548515 -2.4972043 -2.3864737 -2.2865365 -2.2157176 -2.2487531 -2.4085774 -2.3331444 -1.9058144 -1.6904874 -1.702265 -1.4703431 -1.0973942][-2.6291578 -2.3330321 -2.0686307 -1.9241698 -1.6492336 -1.3406851 -1.0716286 -0.95276976 -1.1933937 -1.3164792 -1.0121477 -0.78206396 -0.85652781 -0.75874138 -0.45933962][-2.1560533 -1.7047625 -1.2462783 -1.0227954 -0.63827777 -0.19772291 0.26888657 0.6212306 0.31032705 -0.11132431 -0.08641243 0.008307457 -0.26914167 -0.4778533 -0.47016478][-1.804404 -1.2991018 -0.71630692 -0.41953039 0.027017117 0.50447273 1.0456481 1.6109815 1.3508034 0.76955366 0.50467062 0.3544364 -0.11999512 -0.61605477 -0.93988013][-1.4094806 -1.0027759 -0.4498601 -0.10693979 0.3193388 0.66813707 0.98527622 1.4797568 1.4578571 1.0095544 0.58174419 0.23860168 -0.26358843 -0.84950995 -1.3573966][-1.1200552 -0.86440134 -0.43602872 -0.0513649 0.32938385 0.54414034 0.6012435 0.91853666 1.0723815 0.79829884 0.30987978 -0.12973452 -0.57542729 -1.1309125 -1.6924412][-1.1148705 -0.99008608 -0.73480105 -0.35881758 -0.063324451 0.060943127 -0.021428108 0.098330975 0.26452971 0.095240593 -0.30806637 -0.67162585 -1.0098805 -1.4907191 -2.017714][-1.364635 -1.3058114 -1.2019806 -0.92011404 -0.76215458 -0.74645305 -0.85333514 -0.84946465 -0.70272136 -0.78312135 -1.0254819 -1.2170446 -1.4184418 -1.7673597 -2.2274306][-1.7626061 -1.7106102 -1.6691403 -1.4798708 -1.4219642 -1.4740577 -1.5346603 -1.5455325 -1.4338574 -1.4633813 -1.5769346 -1.6185033 -1.6788538 -1.8693392 -2.298552][-2.0431218 -1.9918699 -1.9802771 -1.8885107 -1.9028697 -1.9844251 -1.9976678 -1.9983835 -1.9160752 -1.8865461 -1.8808534 -1.8137398 -1.7984304 -1.9197838 -2.388622][-2.1895504 -2.1523414 -2.1512887 -2.1364367 -2.1903656 -2.2732136 -2.2767088 -2.2696886 -2.2020209 -2.145591 -2.0916989 -2.0116141 -2.000514 -2.089025 -2.5735292]]...]
INFO - root - 2017-12-07 04:01:34.744646: step 3110, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 70h:14m:48s remains)
INFO - root - 2017-12-07 04:01:42.370424: step 3120, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.749 sec/batch; 68h:30m:55s remains)
INFO - root - 2017-12-07 04:01:50.130742: step 3130, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.752 sec/batch; 68h:46m:15s remains)
INFO - root - 2017-12-07 04:01:57.813901: step 3140, loss = 0.98, batch loss = 0.91 (10.6 examples/sec; 0.756 sec/batch; 69h:09m:55s remains)
INFO - root - 2017-12-07 04:02:05.439138: step 3150, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.765 sec/batch; 69h:57m:53s remains)
INFO - root - 2017-12-07 04:02:13.148704: step 3160, loss = 0.73, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 70h:12m:06s remains)
INFO - root - 2017-12-07 04:02:20.869120: step 3170, loss = 0.77, batch loss = 0.69 (10.3 examples/sec; 0.776 sec/batch; 70h:59m:37s remains)
INFO - root - 2017-12-07 04:02:28.193731: step 3180, loss = 0.86, batch loss = 0.78 (10.6 examples/sec; 0.758 sec/batch; 69h:21m:12s remains)
INFO - root - 2017-12-07 04:02:35.872874: step 3190, loss = 0.78, batch loss = 0.71 (10.1 examples/sec; 0.792 sec/batch; 72h:24m:33s remains)
INFO - root - 2017-12-07 04:02:43.507640: step 3200, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.767 sec/batch; 70h:09m:18s remains)
2017-12-07 04:02:44.107636: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1235337 -4.1519527 -4.200233 -4.2633686 -4.3248334 -4.3408637 -4.3444 -4.373394 -4.4062095 -4.4204445 -4.3799562 -4.2997189 -4.2106614 -4.091682 -3.9722581][-4.3398457 -4.4239016 -4.5102067 -4.6045437 -4.7067065 -4.7406845 -4.7570314 -4.8152604 -4.8640919 -4.8693628 -4.759553 -4.5981727 -4.4539156 -4.2733884 -4.0997891][-4.4206095 -4.5609741 -4.7075953 -4.8606367 -5.0150886 -5.0849848 -5.1405611 -5.2497535 -5.3110409 -5.2715421 -5.0287657 -4.7270017 -4.5037217 -4.2853851 -4.1152444][-4.2246366 -4.3535933 -4.5417361 -4.7435827 -4.9252324 -5.0099378 -5.0650954 -5.1716037 -5.1843057 -5.0461006 -4.657516 -4.2642641 -4.0683236 -3.9529085 -3.9300804][-3.7839952 -3.8408778 -4.01968 -4.2464967 -4.4193182 -4.4548788 -4.4381533 -4.4656672 -4.3494515 -4.0544944 -3.5546105 -3.2105327 -3.2166905 -3.3506474 -3.5778806][-3.2411366 -3.1614888 -3.2293587 -3.3893118 -3.4629657 -3.3418348 -3.1141706 -2.921195 -2.6126008 -2.2095692 -1.7639265 -1.6834304 -2.0927508 -2.6258512 -3.1694][-2.8454068 -2.5436382 -2.4307218 -2.4304686 -2.3047512 -2.0098035 -1.6400123 -1.305428 -0.8825345 -0.49094343 -0.20522022 -0.40652132 -1.1888742 -2.0913491 -2.8906429][-2.5312729 -2.0637591 -1.9397333 -1.8982224 -1.6354029 -1.2635086 -0.960757 -0.72439933 -0.39208364 -0.18523932 -0.15504026 -0.50844121 -1.2998071 -2.1798513 -2.9440589][-2.1558089 -1.6823869 -1.7587273 -1.8414302 -1.63936 -1.3592699 -1.2777457 -1.2806005 -1.1527619 -1.1589415 -1.3124201 -1.6001341 -2.0959868 -2.6636989 -3.1915102][-1.8607321 -1.4648006 -1.7459233 -1.9971023 -2.0346868 -2.0294309 -2.1811619 -2.3986616 -2.4309053 -2.5351939 -2.6777263 -2.7539339 -2.9094644 -3.1249104 -3.377737][-1.7918773 -1.4534216 -1.7685888 -2.0872982 -2.4036424 -2.6868188 -2.907124 -3.0901432 -3.1264181 -3.2428758 -3.3061881 -3.1939688 -3.1802888 -3.2837796 -3.4402366][-1.9921131 -1.6871142 -1.9615598 -2.2828715 -2.7163639 -3.0671868 -3.1492445 -3.1419287 -3.0952539 -3.1927 -3.2187018 -3.0494237 -3.0561666 -3.2593403 -3.4653182][-2.4062986 -2.1647494 -2.3906024 -2.6773911 -3.0642438 -3.3193526 -3.249939 -3.139205 -3.0905962 -3.19727 -3.2401712 -3.124099 -3.1919689 -3.4302452 -3.6013861][-3.2535119 -3.1128414 -3.2387695 -3.410579 -3.6428897 -3.7663286 -3.6564312 -3.5518284 -3.5529904 -3.6510189 -3.6805768 -3.5909202 -3.6209774 -3.7524204 -3.8032889][-4.0863695 -4.0203629 -4.0166178 -4.0486221 -4.1262951 -4.1643634 -4.0937595 -4.036099 -4.0692759 -4.1274133 -4.1181755 -4.0327039 -3.9959064 -3.9863396 -3.9261196]]...]
INFO - root - 2017-12-07 04:02:51.732236: step 3210, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.758 sec/batch; 69h:21m:56s remains)
INFO - root - 2017-12-07 04:02:59.351190: step 3220, loss = 0.92, batch loss = 0.84 (10.4 examples/sec; 0.772 sec/batch; 70h:39m:28s remains)
INFO - root - 2017-12-07 04:03:07.096738: step 3230, loss = 0.87, batch loss = 0.80 (10.2 examples/sec; 0.783 sec/batch; 71h:38m:07s remains)
INFO - root - 2017-12-07 04:03:14.766630: step 3240, loss = 0.87, batch loss = 0.80 (10.9 examples/sec; 0.732 sec/batch; 66h:55m:47s remains)
INFO - root - 2017-12-07 04:03:22.490882: step 3250, loss = 0.99, batch loss = 0.92 (10.1 examples/sec; 0.789 sec/batch; 72h:11m:18s remains)
INFO - root - 2017-12-07 04:03:30.251412: step 3260, loss = 0.72, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 70h:30m:50s remains)
INFO - root - 2017-12-07 04:03:37.895983: step 3270, loss = 0.97, batch loss = 0.90 (10.1 examples/sec; 0.793 sec/batch; 72h:31m:48s remains)
INFO - root - 2017-12-07 04:03:45.373304: step 3280, loss = 0.70, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 71h:45m:27s remains)
INFO - root - 2017-12-07 04:03:52.984283: step 3290, loss = 0.78, batch loss = 0.70 (10.1 examples/sec; 0.789 sec/batch; 72h:09m:53s remains)
INFO - root - 2017-12-07 04:04:00.668877: step 3300, loss = 0.58, batch loss = 0.51 (10.5 examples/sec; 0.760 sec/batch; 69h:31m:11s remains)
2017-12-07 04:04:01.326142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7927208 -2.791153 -2.8576319 -2.9806726 -3.0845492 -3.1700566 -3.2410598 -3.277086 -3.2917042 -3.3046024 -3.3122442 -3.3019888 -3.2789266 -3.2464528 -3.1462078][-2.659368 -2.6453333 -2.6780643 -2.7623649 -2.8290539 -2.8968704 -2.9821732 -3.0414238 -3.0656176 -3.0722275 -3.068954 -3.0628839 -3.0726261 -3.0821176 -3.0098829][-2.7567966 -2.7616267 -2.7553639 -2.7666254 -2.7542453 -2.7515712 -2.7866168 -2.8188562 -2.8161271 -2.8042781 -2.7998486 -2.8220124 -2.8870704 -2.95664 -2.9420989][-2.9006085 -2.9023447 -2.8545816 -2.7782831 -2.6703048 -2.5721917 -2.5200644 -2.4922185 -2.4497662 -2.4250298 -2.4322286 -2.5020781 -2.6501293 -2.8044837 -2.8564687][-2.9202545 -2.9126649 -2.8411655 -2.6924868 -2.494307 -2.3082423 -2.1695619 -2.0840685 -2.0348954 -2.0426083 -2.0891595 -2.1976643 -2.3997467 -2.6088171 -2.6858964][-3.0086961 -2.9936051 -2.8934093 -2.6734467 -2.3913136 -2.1316383 -1.9114664 -1.7653234 -1.7392077 -1.8218405 -1.9365988 -2.0925012 -2.3215933 -2.5425389 -2.6117854][-3.1072416 -3.0772069 -2.9576187 -2.7067518 -2.3962331 -2.1123381 -1.8475814 -1.6575241 -1.6627352 -1.8089073 -1.9777341 -2.1826639 -2.4389172 -2.6626916 -2.7270086][-3.0366235 -2.9949298 -2.8906918 -2.6725068 -2.4091508 -2.1760461 -1.9460847 -1.7826753 -1.8319466 -1.9997518 -2.159549 -2.3549411 -2.5928187 -2.779144 -2.8193078][-3.0978205 -3.0500722 -2.9605477 -2.7842572 -2.5786798 -2.4089606 -2.256577 -2.1729515 -2.2517269 -2.3892851 -2.483537 -2.6022048 -2.7581916 -2.8774967 -2.904834][-3.2667339 -3.2326634 -3.173439 -3.0619717 -2.9291024 -2.8125958 -2.7206812 -2.7009244 -2.7659006 -2.8261862 -2.8303843 -2.8579037 -2.9267526 -2.9982491 -3.0475783][-3.2339458 -3.2213604 -3.2057624 -3.1857176 -3.1531954 -3.1027145 -3.0690897 -3.1028709 -3.1431718 -3.1321731 -3.0707536 -3.0269208 -3.0219815 -3.0562809 -3.1087809][-3.1126285 -3.1158309 -3.1240561 -3.1467164 -3.1709166 -3.171762 -3.1841917 -3.2477899 -3.2614813 -3.2037456 -3.1108422 -3.0202093 -2.960156 -2.9676604 -3.0190392][-3.0228658 -3.0622311 -3.1001287 -3.1351969 -3.1710515 -3.1976533 -3.2314954 -3.2893674 -3.2809978 -3.213624 -3.1278172 -3.02353 -2.9368894 -2.9256048 -2.9685774][-2.8891444 -2.9668727 -3.0374889 -3.088419 -3.136796 -3.1785243 -3.210562 -3.2421744 -3.2256572 -3.1795464 -3.1187367 -3.0317879 -2.9475656 -2.9233532 -2.9413414][-2.7584724 -2.8347983 -2.9014041 -2.9521523 -3.0081639 -3.0582304 -3.0888276 -3.1045294 -3.0998363 -3.0933514 -3.0653319 -3.0036049 -2.9306495 -2.8984237 -2.8990679]]...]
INFO - root - 2017-12-07 04:04:09.042458: step 3310, loss = 0.69, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 71h:56m:18s remains)
INFO - root - 2017-12-07 04:04:16.710057: step 3320, loss = 1.03, batch loss = 0.96 (11.0 examples/sec; 0.727 sec/batch; 66h:28m:23s remains)
INFO - root - 2017-12-07 04:04:24.445196: step 3330, loss = 0.88, batch loss = 0.80 (10.0 examples/sec; 0.799 sec/batch; 73h:02m:56s remains)
INFO - root - 2017-12-07 04:04:32.062659: step 3340, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 70h:31m:55s remains)
INFO - root - 2017-12-07 04:04:39.709579: step 3350, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.772 sec/batch; 70h:36m:31s remains)
INFO - root - 2017-12-07 04:04:47.405597: step 3360, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 72h:29m:43s remains)
INFO - root - 2017-12-07 04:04:55.165075: step 3370, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 70h:38m:57s remains)
INFO - root - 2017-12-07 04:05:02.525666: step 3380, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.764 sec/batch; 69h:50m:29s remains)
INFO - root - 2017-12-07 04:05:10.119464: step 3390, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.769 sec/batch; 70h:16m:50s remains)
INFO - root - 2017-12-07 04:05:17.727921: step 3400, loss = 0.68, batch loss = 0.60 (10.3 examples/sec; 0.778 sec/batch; 71h:08m:54s remains)
2017-12-07 04:05:18.289605: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.315032 -1.8223126 -2.6937485 -3.3189287 -3.6318924 -3.5440316 -2.8959451 -2.4720843 -2.2250671 -1.7734201 -1.5850325 -1.4385741 -1.2672575 -1.4088092 -1.5364001][0.21620178 -0.50080156 -1.6663234 -2.5453589 -3.0573628 -3.1539111 -2.687851 -2.4459929 -2.305661 -1.9395947 -1.8841534 -1.7952454 -1.5888083 -1.6054282 -1.6151755][0.95990467 0.10932159 -1.2236199 -2.1984203 -2.7331214 -2.9035039 -2.6325679 -2.6143336 -2.71809 -2.6243961 -2.7753067 -2.7435684 -2.4285622 -2.2208066 -2.0524046][0.49945641 -0.36433172 -1.573164 -2.3513708 -2.6861153 -2.7873597 -2.6196957 -2.6739736 -2.8765488 -3.0106544 -3.3813639 -3.4950249 -3.1973662 -2.8611486 -2.5386968][-0.621444 -1.4323144 -2.3637857 -2.7662373 -2.79529 -2.7273688 -2.4880404 -2.3383057 -2.3775537 -2.6074383 -3.1723843 -3.5587769 -3.5312948 -3.310535 -2.9625382][-1.5965364 -2.2371638 -2.8057232 -2.8823836 -2.7343006 -2.5718689 -2.162905 -1.6415665 -1.3563929 -1.5741365 -2.2788832 -2.926333 -3.228282 -3.277951 -3.0552897][-2.0538356 -2.4237003 -2.6532466 -2.5577631 -2.4271238 -2.2264645 -1.5532606 -0.60888338 -0.018578053 -0.30847883 -1.2632887 -2.2257903 -2.8640356 -3.1727321 -3.0725627][-2.3245103 -2.543643 -2.6033704 -2.4552035 -2.3189466 -1.9277005 -0.8437736 0.53870487 1.3459988 0.83720827 -0.50248075 -1.8175106 -2.7530985 -3.303962 -3.3269773][-2.6495523 -2.8704343 -2.8849425 -2.7627015 -2.6202469 -2.0426226 -0.70393896 0.77420139 1.4733534 0.73928261 -0.69232678 -1.8859172 -2.6389065 -3.1051061 -3.1948171][-2.6858459 -2.9019489 -2.9835188 -3.0231504 -2.9782128 -2.4512918 -1.3616993 -0.35857582 -0.035509109 -0.69571161 -1.6865158 -2.276329 -2.4786069 -2.6022477 -2.6386182][-2.2524085 -2.2594342 -2.4094741 -2.7444005 -3.0199034 -2.7965102 -2.1365571 -1.5670741 -1.4471886 -1.918798 -2.476913 -2.6281743 -2.452651 -2.2868083 -2.187016][-1.9365175 -1.656903 -1.6908278 -2.1143773 -2.6140487 -2.7141747 -2.4593482 -2.1763866 -2.1359649 -2.5040534 -2.9424293 -2.9985983 -2.6994519 -2.3299689 -2.0316503][-2.076561 -1.6629689 -1.4716632 -1.7076788 -2.1565485 -2.3520565 -2.3009169 -2.2236323 -2.3489223 -2.8410006 -3.3489361 -3.3981652 -3.0462821 -2.5445538 -2.1070209][-2.4895215 -2.0999465 -1.7328234 -1.772064 -2.1259305 -2.3009584 -2.2826238 -2.2778363 -2.4871805 -3.0648031 -3.6251378 -3.6870384 -3.3510303 -2.8572776 -2.4097157][-2.8083739 -2.5652871 -2.2194583 -2.2123411 -2.5141299 -2.6206958 -2.5064611 -2.3728352 -2.4724171 -2.9665146 -3.5093226 -3.6256833 -3.3737311 -2.9968309 -2.6535864]]...]
INFO - root - 2017-12-07 04:05:25.930857: step 3410, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.770 sec/batch; 70h:21m:29s remains)
INFO - root - 2017-12-07 04:05:33.612207: step 3420, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 68h:52m:48s remains)
INFO - root - 2017-12-07 04:05:41.270934: step 3430, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.783 sec/batch; 71h:36m:38s remains)
INFO - root - 2017-12-07 04:05:48.965639: step 3440, loss = 0.75, batch loss = 0.68 (10.0 examples/sec; 0.799 sec/batch; 73h:03m:25s remains)
INFO - root - 2017-12-07 04:05:56.683017: step 3450, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.772 sec/batch; 70h:32m:53s remains)
INFO - root - 2017-12-07 04:06:04.281518: step 3460, loss = 1.07, batch loss = 1.00 (10.3 examples/sec; 0.776 sec/batch; 70h:54m:26s remains)
INFO - root - 2017-12-07 04:06:11.983894: step 3470, loss = 0.85, batch loss = 0.77 (10.3 examples/sec; 0.774 sec/batch; 70h:46m:20s remains)
INFO - root - 2017-12-07 04:06:19.610004: step 3480, loss = 1.06, batch loss = 0.99 (10.5 examples/sec; 0.763 sec/batch; 69h:42m:59s remains)
INFO - root - 2017-12-07 04:06:27.369158: step 3490, loss = 0.80, batch loss = 0.72 (10.5 examples/sec; 0.765 sec/batch; 69h:53m:06s remains)
INFO - root - 2017-12-07 04:06:35.094285: step 3500, loss = 0.71, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 70h:31m:35s remains)
2017-12-07 04:06:35.693345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4511967 -4.0764503 -3.961796 -3.6081715 -3.478008 -3.6590936 -3.9580543 -3.8378437 -3.3336296 -3.0871887 -3.2228253 -3.6146779 -3.8704784 -3.5816116 -2.8496485][-2.559906 -3.212296 -3.0379033 -2.5805345 -2.4513304 -2.7373919 -3.1190324 -3.0827155 -2.623673 -2.2672055 -2.2782423 -2.6272082 -2.8469834 -2.5090432 -1.7473683][-1.5698893 -2.1965027 -2.0045881 -1.4764686 -1.3683138 -1.6998968 -2.1210227 -2.2661662 -1.9736996 -1.5335267 -1.3672097 -1.5123489 -1.5658948 -1.2092786 -0.594054][-1.3569036 -1.9880133 -1.8507342 -1.3454435 -1.2887795 -1.632417 -2.0168054 -2.3156497 -2.2441282 -1.7911589 -1.4175887 -1.2611759 -1.0706959 -0.70165634 -0.25324965][-1.6038477 -2.2118223 -2.2099645 -1.8584781 -1.8843138 -2.1953669 -2.50885 -2.9541249 -3.1597295 -2.8038142 -2.2675834 -1.7722464 -1.2930336 -0.85052943 -0.53768229][-1.8910453 -2.4346867 -2.5323784 -2.3356206 -2.3820362 -2.5738473 -2.7814679 -3.2694559 -3.6751971 -3.490556 -2.9237583 -2.2483637 -1.5545545 -0.97187233 -0.64960814][-2.0581286 -2.5656328 -2.7675486 -2.6931248 -2.7240834 -2.7213287 -2.7035246 -3.0174358 -3.3942657 -3.29816 -2.8422775 -2.2555716 -1.6111624 -1.0479398 -0.72910976][-1.9798803 -2.431479 -2.6827972 -2.7732897 -2.8839705 -2.7835636 -2.5717707 -2.6228971 -2.7946916 -2.640075 -2.2292182 -1.7846546 -1.3131621 -0.9465909 -0.77894688][-1.4625156 -1.7766039 -1.9953568 -2.2449706 -2.5581937 -2.5874238 -2.3960583 -2.3802483 -2.4558949 -2.2459421 -1.8277621 -1.4568737 -1.103627 -0.901531 -0.87395859][-1.0232508 -1.2270937 -1.4201546 -1.7392576 -2.1935594 -2.4522781 -2.4782462 -2.5780845 -2.6656618 -2.4856277 -2.1178498 -1.7643602 -1.4774797 -1.4024701 -1.5023906][-1.1138911 -1.2551029 -1.4078739 -1.7203677 -2.2248869 -2.7008862 -2.9702525 -3.2126474 -3.3512416 -3.251821 -2.9495368 -2.5720248 -2.2794878 -2.2425256 -2.3905861][-1.540467 -1.6876044 -1.8282142 -2.0957785 -2.5728972 -3.097892 -3.4401648 -3.6602948 -3.7312288 -3.6564577 -3.4318855 -3.1238177 -2.9167652 -2.9214938 -3.0687113][-2.2763581 -2.4244068 -2.5328774 -2.731461 -3.0840602 -3.4641047 -3.7127247 -3.7893376 -3.6971388 -3.5736971 -3.3761175 -3.1464734 -3.0605979 -3.1917574 -3.4820342][-3.2119417 -3.3430161 -3.3982048 -3.4750614 -3.6423621 -3.8026896 -3.9053869 -3.8543763 -3.654695 -3.5198181 -3.3786669 -3.2605863 -3.2643771 -3.467165 -3.8774705][-3.44939 -3.5928268 -3.7133198 -3.7982857 -3.8850744 -3.9166017 -3.9044609 -3.7750568 -3.5466592 -3.4234412 -3.3648109 -3.3550463 -3.405457 -3.5971055 -3.995399]]...]
INFO - root - 2017-12-07 04:06:43.253910: step 3510, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.758 sec/batch; 69h:14m:45s remains)
INFO - root - 2017-12-07 04:06:50.829267: step 3520, loss = 0.77, batch loss = 0.69 (10.3 examples/sec; 0.779 sec/batch; 71h:09m:43s remains)
INFO - root - 2017-12-07 04:06:58.462561: step 3530, loss = 0.73, batch loss = 0.66 (10.8 examples/sec; 0.743 sec/batch; 67h:55m:56s remains)
INFO - root - 2017-12-07 04:07:06.108157: step 3540, loss = 1.00, batch loss = 0.93 (10.3 examples/sec; 0.775 sec/batch; 70h:47m:35s remains)
INFO - root - 2017-12-07 04:07:13.837716: step 3550, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.783 sec/batch; 71h:33m:46s remains)
INFO - root - 2017-12-07 04:07:21.503547: step 3560, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.764 sec/batch; 69h:51m:05s remains)
INFO - root - 2017-12-07 04:07:29.073941: step 3570, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.772 sec/batch; 70h:33m:36s remains)
INFO - root - 2017-12-07 04:07:36.367184: step 3580, loss = 0.84, batch loss = 0.76 (10.7 examples/sec; 0.749 sec/batch; 68h:24m:31s remains)
INFO - root - 2017-12-07 04:07:44.044582: step 3590, loss = 1.00, batch loss = 0.93 (10.0 examples/sec; 0.797 sec/batch; 72h:48m:06s remains)
INFO - root - 2017-12-07 04:07:51.663005: step 3600, loss = 0.83, batch loss = 0.75 (10.4 examples/sec; 0.769 sec/batch; 70h:14m:06s remains)
2017-12-07 04:07:52.273032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1762643 -3.0054908 -2.86237 -2.8765607 -3.1068664 -3.3022623 -3.1961863 -2.9712858 -2.6852336 -2.5455136 -2.7342839 -3.0020123 -3.0235183 -2.8375955 -2.6262531][-2.8288884 -2.7398672 -2.690943 -2.7891273 -3.002171 -3.1032457 -2.9132504 -2.6390343 -2.3863151 -2.3538172 -2.6617727 -3.0136995 -3.0569737 -2.8356087 -2.5801916][-2.8475795 -2.8666399 -2.929971 -3.0990438 -3.3171782 -3.3612442 -3.081655 -2.6932316 -2.4189594 -2.4902601 -2.9189744 -3.3054624 -3.3112218 -3.0122976 -2.6759467][-3.128242 -3.1396275 -3.1983576 -3.3319905 -3.5202839 -3.56525 -3.3035722 -2.8822613 -2.6323655 -2.8549886 -3.4113054 -3.7588196 -3.6463726 -3.2671328 -2.8873961][-3.2889009 -3.233593 -3.2144203 -3.2404695 -3.3312883 -3.3830721 -3.2558274 -2.9451084 -2.761167 -3.0883472 -3.7072067 -3.9924023 -3.8100986 -3.4568043 -3.1505861][-3.3354917 -3.1924992 -3.0296187 -2.8991783 -2.8696704 -2.9062984 -2.9128268 -2.7749262 -2.72292 -3.1260355 -3.755419 -4.0477815 -3.8989029 -3.6066051 -3.3637912][-3.2684455 -3.0759358 -2.8020859 -2.5585976 -2.4595435 -2.4638898 -2.489543 -2.4720957 -2.5909448 -3.0447569 -3.6096594 -3.9013534 -3.8437603 -3.6267948 -3.4023721][-3.1440558 -2.9614787 -2.6934493 -2.4588976 -2.3869936 -2.3945463 -2.3703635 -2.3873141 -2.6165564 -3.0076838 -3.3564973 -3.5549357 -3.5914426 -3.49486 -3.326355][-3.1172619 -2.9809639 -2.81672 -2.7068024 -2.738853 -2.7788148 -2.7300436 -2.7739458 -3.032865 -3.241632 -3.2801795 -3.3158879 -3.4133754 -3.4458504 -3.3706875][-3.1150894 -3.0237479 -2.9735289 -3.0510755 -3.2404981 -3.3467262 -3.3007612 -3.3538694 -3.5345068 -3.46793 -3.2200067 -3.1589463 -3.3071918 -3.4298296 -3.4325168][-3.0665007 -2.9879453 -2.9473052 -3.1314995 -3.4461927 -3.6074364 -3.5936532 -3.6463532 -3.6890554 -3.3675976 -2.9609685 -2.8971105 -3.0773852 -3.2410138 -3.3231421][-3.0504975 -2.9458938 -2.8033829 -2.9293363 -3.2229872 -3.3623471 -3.3849051 -3.4487553 -3.4172792 -3.0563922 -2.7146659 -2.670526 -2.7679322 -2.887423 -3.0406933][-3.0406687 -2.8867235 -2.6274958 -2.6041422 -2.7816467 -2.897862 -2.99545 -3.0965436 -3.0621109 -2.8473043 -2.7089248 -2.6828773 -2.6395841 -2.666594 -2.8215692][-3.076077 -2.9274268 -2.7058458 -2.6408534 -2.7349608 -2.8347526 -2.9944267 -3.1195893 -3.0962954 -3.0111294 -3.0055237 -2.9798191 -2.8682685 -2.8384769 -2.9341435][-3.292367 -3.2171044 -3.1566429 -3.1682177 -3.2409587 -3.3287919 -3.5073233 -3.630022 -3.6119585 -3.5664594 -3.5641952 -3.5234637 -3.4224968 -3.3845935 -3.4067535]]...]
INFO - root - 2017-12-07 04:07:59.905056: step 3610, loss = 0.97, batch loss = 0.90 (10.2 examples/sec; 0.788 sec/batch; 71h:58m:34s remains)
INFO - root - 2017-12-07 04:08:07.732974: step 3620, loss = 0.61, batch loss = 0.53 (9.9 examples/sec; 0.808 sec/batch; 73h:47m:11s remains)
INFO - root - 2017-12-07 04:08:15.521721: step 3630, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 70h:24m:01s remains)
INFO - root - 2017-12-07 04:08:23.180859: step 3640, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.767 sec/batch; 70h:05m:21s remains)
INFO - root - 2017-12-07 04:08:30.975546: step 3650, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.769 sec/batch; 70h:12m:27s remains)
INFO - root - 2017-12-07 04:08:38.533040: step 3660, loss = 0.91, batch loss = 0.83 (10.7 examples/sec; 0.747 sec/batch; 68h:11m:48s remains)
INFO - root - 2017-12-07 04:08:46.206365: step 3670, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.764 sec/batch; 69h:44m:24s remains)
INFO - root - 2017-12-07 04:08:53.779828: step 3680, loss = 0.78, batch loss = 0.71 (9.3 examples/sec; 0.856 sec/batch; 78h:10m:25s remains)
INFO - root - 2017-12-07 04:09:01.505317: step 3690, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.762 sec/batch; 69h:33m:34s remains)
INFO - root - 2017-12-07 04:09:09.162190: step 3700, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.751 sec/batch; 68h:37m:18s remains)
2017-12-07 04:09:09.741101: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3502398 -4.5318422 -4.5194793 -4.3060651 -4.0928507 -3.8908763 -3.6323032 -3.3969085 -3.1149423 -2.769336 -2.4065168 -2.1567292 -2.4023385 -2.810858 -2.889729][-3.6953108 -3.8204255 -3.8980532 -3.849973 -3.7878544 -3.7142091 -3.5282998 -3.2706141 -2.8887799 -2.3764708 -2.0128977 -1.9597602 -2.3317776 -2.6899853 -2.6552646][-3.0681973 -2.9952948 -2.9855225 -2.9776092 -3.0672905 -3.2136085 -3.2057109 -3.031738 -2.6981297 -2.26949 -2.1542027 -2.3599331 -2.6500869 -2.7073674 -2.43892][-2.8773036 -2.6498003 -2.5260668 -2.50882 -2.7158437 -3.0157661 -3.134532 -3.0593193 -2.8986459 -2.7314467 -2.8668892 -3.2169662 -3.2210319 -2.829525 -2.2742195][-3.2515702 -2.982904 -2.8030894 -2.7811282 -2.9718888 -3.1701174 -3.1832938 -3.0797186 -3.0672026 -3.0986028 -3.322391 -3.6413567 -3.3769035 -2.6978188 -2.0370381][-3.0712676 -2.7824855 -2.5902364 -2.5823727 -2.7098403 -2.7308631 -2.5218692 -2.3344965 -2.4036734 -2.5140128 -2.72953 -2.9948549 -2.7377784 -2.2538471 -1.9748621][-1.7985721 -1.4089544 -1.1675122 -1.1154795 -1.2326031 -1.2330987 -1.0116248 -0.92953539 -1.1523488 -1.357219 -1.5496819 -1.7735012 -1.6852517 -1.6447918 -2.0037484][-1.0758045 -0.59827638 -0.25421095 -0.093836784 -0.21418858 -0.31614494 -0.27656603 -0.40946245 -0.75316191 -0.95762706 -1.0161729 -1.0742922 -1.0213704 -1.2393584 -2.0068421][-2.249012 -1.9029274 -1.5881574 -1.4049008 -1.5073147 -1.6891017 -1.7998006 -1.9201782 -2.0888348 -2.0803373 -1.8876088 -1.6577938 -1.4263825 -1.5456612 -2.2355418][-3.7044044 -3.5260863 -3.3100672 -3.1597738 -3.2153044 -3.3569736 -3.4321184 -3.4636037 -3.4860568 -3.3750927 -3.1126313 -2.7721157 -2.4197483 -2.3485873 -2.7610025][-3.997155 -3.8899612 -3.7366946 -3.6190259 -3.6221771 -3.6842842 -3.7091684 -3.7185915 -3.736064 -3.6996717 -3.5722475 -3.3648086 -3.1201916 -3.0411773 -3.2798321][-3.623666 -3.5484691 -3.454071 -3.4020638 -3.4098921 -3.4306252 -3.4427385 -3.4434583 -3.4381373 -3.4103703 -3.3547962 -3.2780626 -3.190083 -3.2071257 -3.3884845][-3.548533 -3.5588474 -3.5625839 -3.5862207 -3.6198936 -3.6246753 -3.6123033 -3.5763311 -3.5207009 -3.4421835 -3.3609972 -3.3149629 -3.2881365 -3.3181481 -3.3996959][-3.718842 -3.7854877 -3.8314033 -3.8760507 -3.903676 -3.8683884 -3.8195722 -3.767328 -3.7208703 -3.6759393 -3.6520116 -3.661983 -3.6626089 -3.6644945 -3.6418304][-3.9552474 -3.9845581 -4.0025296 -4.0326304 -4.0602689 -4.0158672 -3.9508336 -3.8902614 -3.8538706 -3.8421741 -3.8689337 -3.922894 -3.9614086 -3.9837971 -3.9547822]]...]
INFO - root - 2017-12-07 04:09:17.402013: step 3710, loss = 0.72, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 70h:54m:01s remains)
INFO - root - 2017-12-07 04:09:25.209365: step 3720, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.756 sec/batch; 69h:00m:37s remains)
INFO - root - 2017-12-07 04:09:32.856059: step 3730, loss = 0.66, batch loss = 0.58 (10.6 examples/sec; 0.755 sec/batch; 68h:59m:34s remains)
INFO - root - 2017-12-07 04:09:40.625639: step 3740, loss = 0.83, batch loss = 0.75 (10.2 examples/sec; 0.785 sec/batch; 71h:42m:09s remains)
INFO - root - 2017-12-07 04:09:48.417672: step 3750, loss = 1.09, batch loss = 1.02 (10.0 examples/sec; 0.802 sec/batch; 73h:16m:09s remains)
INFO - root - 2017-12-07 04:09:55.991950: step 3760, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.745 sec/batch; 68h:02m:30s remains)
INFO - root - 2017-12-07 04:10:03.645054: step 3770, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.751 sec/batch; 68h:33m:31s remains)
INFO - root - 2017-12-07 04:10:11.181962: step 3780, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.769 sec/batch; 70h:15m:27s remains)
INFO - root - 2017-12-07 04:10:18.883341: step 3790, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.747 sec/batch; 68h:09m:42s remains)
INFO - root - 2017-12-07 04:10:26.778093: step 3800, loss = 0.85, batch loss = 0.78 (10.1 examples/sec; 0.794 sec/batch; 72h:30m:39s remains)
2017-12-07 04:10:27.363960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.234302 -3.0353923 -2.5855496 -2.2453814 -2.3766489 -2.6312056 -2.937223 -3.2057645 -3.0589442 -2.7461011 -2.6564991 -2.6655641 -2.3022683 -1.8388054 -2.1494648][-3.037755 -2.8342705 -2.3797331 -2.1398239 -2.376466 -2.6622643 -2.9210916 -3.1823182 -3.1280847 -2.841948 -2.6813364 -2.7566881 -2.7655363 -2.7209771 -3.10972][-2.9730592 -2.7989535 -2.3611097 -2.1695211 -2.3897953 -2.6626985 -2.8125918 -2.9042759 -2.8494132 -2.640223 -2.5316 -2.7164145 -3.043354 -3.3722112 -3.827332][-3.1974134 -3.0887337 -2.6646461 -2.483108 -2.6254735 -2.7528291 -2.580934 -2.2590292 -2.0443816 -1.9398918 -2.0563538 -2.5072057 -3.0978847 -3.6676512 -4.1192193][-3.5785856 -3.51451 -3.0963454 -2.8766022 -2.8698084 -2.8018131 -2.2058127 -1.2926598 -0.85191822 -0.9968679 -1.5415819 -2.3198352 -3.0798163 -3.6845474 -4.0358243][-3.9230194 -3.8326828 -3.4157441 -3.1169381 -2.9047976 -2.6254516 -1.5660172 -0.00935173 0.57465506 -0.075416088 -1.1877117 -2.2419388 -3.08239 -3.6233711 -3.8838339][-4.1481962 -3.9718444 -3.5317135 -3.1108766 -2.6757085 -2.1591902 -0.649976 1.426682 1.9027929 0.52935839 -1.1128342 -2.2697222 -3.0780416 -3.5333619 -3.7678897][-4.2813907 -4.03385 -3.5573583 -2.9967835 -2.3496547 -1.5576918 0.26875162 2.5059867 2.6160131 0.62126446 -1.2895615 -2.4113414 -3.1461492 -3.5390844 -3.7901137][-4.3393621 -4.0352416 -3.5097959 -2.8559375 -2.0987663 -1.1599774 0.60989571 2.4066062 2.0524812 -0.025365353 -1.733156 -2.6436396 -3.2769926 -3.6425567 -3.9305534][-4.3816156 -4.0113955 -3.4405973 -2.8025236 -2.1175835 -1.2583859 0.1106143 1.1955419 0.62245607 -1.045764 -2.2223997 -2.823019 -3.3569131 -3.7103319 -4.0082793][-4.29899 -3.875128 -3.3383813 -2.8622103 -2.3978791 -1.7376914 -0.79049087 -0.2352953 -0.7447083 -1.813489 -2.4778256 -2.8649304 -3.352674 -3.7065449 -3.973536][-4.1223664 -3.7009871 -3.3033228 -3.0549445 -2.7979634 -2.3371513 -1.7281117 -1.4616768 -1.7544129 -2.2890055 -2.6315522 -2.9230247 -3.3598318 -3.687423 -3.9070852][-3.9739008 -3.6176143 -3.3581181 -3.2505834 -3.07399 -2.7398896 -2.3731141 -2.2697518 -2.401248 -2.6340079 -2.8403549 -3.0864506 -3.4405422 -3.7134805 -3.8819613][-3.8097942 -3.5690975 -3.4267917 -3.3715878 -3.2049737 -2.974772 -2.7832975 -2.7533562 -2.7965951 -2.8999319 -3.04091 -3.2368293 -3.4930773 -3.696362 -3.8102975][-3.6279705 -3.505399 -3.4443767 -3.4046292 -3.2563152 -3.1196012 -3.0438972 -3.0371971 -3.0345688 -3.0831728 -3.1711955 -3.3066421 -3.472225 -3.5989668 -3.6559033]]...]
INFO - root - 2017-12-07 04:10:35.111346: step 3810, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.777 sec/batch; 70h:54m:38s remains)
INFO - root - 2017-12-07 04:10:42.931754: step 3820, loss = 0.89, batch loss = 0.81 (10.6 examples/sec; 0.758 sec/batch; 69h:12m:37s remains)
INFO - root - 2017-12-07 04:10:50.702083: step 3830, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.790 sec/batch; 72h:08m:18s remains)
INFO - root - 2017-12-07 04:10:58.407219: step 3840, loss = 0.89, batch loss = 0.82 (10.8 examples/sec; 0.742 sec/batch; 67h:43m:03s remains)
INFO - root - 2017-12-07 04:11:06.232169: step 3850, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.752 sec/batch; 68h:41m:27s remains)
INFO - root - 2017-12-07 04:11:13.806418: step 3860, loss = 0.82, batch loss = 0.75 (10.8 examples/sec; 0.738 sec/batch; 67h:21m:00s remains)
INFO - root - 2017-12-07 04:11:21.504515: step 3870, loss = 0.96, batch loss = 0.89 (10.4 examples/sec; 0.766 sec/batch; 69h:57m:39s remains)
INFO - root - 2017-12-07 04:11:28.991517: step 3880, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.780 sec/batch; 71h:12m:06s remains)
INFO - root - 2017-12-07 04:11:36.589908: step 3890, loss = 0.99, batch loss = 0.92 (10.7 examples/sec; 0.750 sec/batch; 68h:27m:10s remains)
INFO - root - 2017-12-07 04:11:44.300837: step 3900, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.774 sec/batch; 70h:38m:30s remains)
2017-12-07 04:11:44.878566: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8710938 -2.0255427 -2.0999949 -3.1628852 -3.8877738 -3.9096675 -3.6417861 -3.311883 -3.1273324 -3.0885944 -2.9835515 -2.8714356 -3.0675285 -3.2323582 -3.1520169][-2.7918024 -1.7259905 -1.4558454 -2.2079709 -2.9083631 -3.1197038 -2.9846997 -2.6535301 -2.5397172 -2.7377186 -2.9615369 -3.0765347 -3.2388425 -3.3192079 -3.2754118][-2.8568945 -1.6636989 -0.90818763 -1.1900516 -1.9716589 -2.6501789 -2.8483887 -2.6287169 -2.5751731 -2.872303 -3.2035625 -3.3001609 -3.2600336 -3.2683659 -3.3607364][-3.1079056 -1.9100177 -0.794662 -0.62410522 -1.4173625 -2.4395008 -2.8159351 -2.5454326 -2.4339323 -2.7504368 -3.1489906 -3.2486787 -3.166213 -3.2832723 -3.5292907][-3.0690007 -1.9857454 -0.76210761 -0.34391069 -1.1115007 -2.2557805 -2.6079254 -2.2159886 -2.0619044 -2.4084737 -2.8566394 -2.9933226 -2.9968939 -3.2489209 -3.5373926][-2.965364 -2.1861358 -1.1181693 -0.62989235 -1.2715778 -2.3401649 -2.6195667 -2.163404 -1.9131672 -2.1323538 -2.4173343 -2.4993343 -2.6046576 -3.0080309 -3.403471][-2.8510766 -2.3715374 -1.5524902 -1.1014054 -1.5662956 -2.4340789 -2.6634262 -2.2744296 -2.0264349 -2.1453078 -2.255197 -2.2621555 -2.3916843 -2.7882967 -3.1441612][-2.6944277 -2.3889647 -1.8296947 -1.5130315 -1.8914866 -2.5475264 -2.7155044 -2.4176168 -2.2238977 -2.3396003 -2.3881028 -2.2891526 -2.2986777 -2.5817466 -2.8617668][-2.5423477 -2.3385777 -2.0470881 -1.9526215 -2.3315325 -2.824121 -2.9239349 -2.6395268 -2.3715708 -2.3624344 -2.3279958 -2.135201 -2.0447683 -2.291286 -2.6582117][-2.3452229 -2.1345773 -2.0437336 -2.2161286 -2.6860228 -3.0928602 -3.0997052 -2.7730994 -2.4464719 -2.3909788 -2.3708661 -2.1401243 -1.9380436 -2.0969427 -2.5030596][-2.2485533 -1.969197 -1.9667165 -2.3252947 -2.9083173 -3.3046522 -3.2591357 -2.87353 -2.4834933 -2.3742983 -2.3605931 -2.1629808 -1.9129856 -1.9923453 -2.395951][-2.3428166 -1.999701 -1.981725 -2.3747985 -2.9996023 -3.4243207 -3.4097881 -3.0205991 -2.5450344 -2.2693276 -2.1168528 -1.8956459 -1.6788144 -1.7944148 -2.3135207][-2.5444303 -2.1559253 -2.0803812 -2.4332321 -3.0365162 -3.4626811 -3.4889622 -3.1722963 -2.7134519 -2.3282123 -2.0258181 -1.7310846 -1.5097957 -1.6084108 -2.135119][-2.7916846 -2.4140973 -2.279726 -2.54768 -3.0793724 -3.4756629 -3.514642 -3.2811012 -2.9150753 -2.5529172 -2.2689798 -2.0424688 -1.8871818 -1.9613729 -2.3304062][-3.1140957 -2.8142524 -2.641284 -2.7899494 -3.1893039 -3.5163996 -3.5637009 -3.4193363 -3.2013588 -2.9507942 -2.74579 -2.5845401 -2.4687619 -2.5178761 -2.7742672]]...]
INFO - root - 2017-12-07 04:11:52.474431: step 3910, loss = 0.99, batch loss = 0.91 (10.7 examples/sec; 0.749 sec/batch; 68h:20m:51s remains)
INFO - root - 2017-12-07 04:12:00.081599: step 3920, loss = 0.84, batch loss = 0.76 (10.6 examples/sec; 0.751 sec/batch; 68h:34m:05s remains)
INFO - root - 2017-12-07 04:12:07.702375: step 3930, loss = 0.85, batch loss = 0.77 (10.8 examples/sec; 0.742 sec/batch; 67h:45m:39s remains)
INFO - root - 2017-12-07 04:12:15.374599: step 3940, loss = 0.96, batch loss = 0.89 (10.7 examples/sec; 0.751 sec/batch; 68h:30m:38s remains)
INFO - root - 2017-12-07 04:12:23.018073: step 3950, loss = 1.03, batch loss = 0.96 (10.5 examples/sec; 0.761 sec/batch; 69h:28m:48s remains)
INFO - root - 2017-12-07 04:12:30.590933: step 3960, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 69h:54m:18s remains)
INFO - root - 2017-12-07 04:12:38.310254: step 3970, loss = 0.89, batch loss = 0.82 (10.0 examples/sec; 0.801 sec/batch; 73h:07m:02s remains)
INFO - root - 2017-12-07 04:12:45.783906: step 3980, loss = 0.97, batch loss = 0.89 (10.5 examples/sec; 0.765 sec/batch; 69h:45m:54s remains)
INFO - root - 2017-12-07 04:12:53.313035: step 3990, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.775 sec/batch; 70h:45m:00s remains)
INFO - root - 2017-12-07 04:13:00.993695: step 4000, loss = 1.03, batch loss = 0.96 (10.7 examples/sec; 0.750 sec/batch; 68h:25m:28s remains)
2017-12-07 04:13:01.661794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.479517 -4.3598332 -4.384377 -4.506537 -4.48164 -4.3383441 -4.3780837 -4.4709291 -4.5068893 -4.5453844 -4.5452509 -4.4078417 -4.1406269 -3.9324079 -3.8777199][-4.4201632 -4.2744613 -4.3440042 -4.4657855 -4.3416257 -4.0919967 -4.0400267 -3.9550102 -3.8070967 -3.8262117 -4.0058522 -4.04151 -3.8446004 -3.7034841 -3.6802583][-4.0291481 -3.8763208 -4.0035467 -4.0807786 -3.8041568 -3.3780308 -3.1270943 -2.7821259 -2.4444706 -2.4990468 -2.9311371 -3.2579889 -3.3064294 -3.3963156 -3.5447063][-3.4871728 -3.3443325 -3.4767044 -3.4064755 -2.9666512 -2.4497395 -2.0445471 -1.5347943 -1.1272802 -1.1922603 -1.6627395 -2.1224358 -2.3523362 -2.6966226 -3.0966148][-3.0256596 -2.871737 -2.93145 -2.6390767 -2.1078119 -1.6385758 -1.0857713 -0.48048759 -0.23155117 -0.39918518 -0.78081965 -1.2497165 -1.5588474 -2.0189116 -2.5500038][-2.5507979 -2.408066 -2.3755515 -1.9103665 -1.3668106 -0.91070151 -0.08011198 0.68107224 0.54063463 0.067072868 -0.2632513 -0.74515843 -1.1585586 -1.6255064 -2.1152458][-2.0948117 -1.9442501 -1.7959852 -1.1632688 -0.57600904 -0.054336071 1.107821 2.055263 1.3851972 0.49303913 0.0513916 -0.53708768 -1.0660636 -1.4584222 -1.8274486][-1.7851038 -1.4830132 -1.1370306 -0.4177289 0.11262512 0.603786 1.7913837 2.6865392 1.7759953 0.7523241 0.20595121 -0.56076217 -1.1257317 -1.2785995 -1.4172034][-1.6726751 -1.0703037 -0.56378078 0.047466755 0.34990597 0.63314915 1.4442716 1.8905311 1.1575761 0.50526142 0.082343578 -0.71700597 -1.1492219 -0.99508595 -0.9101615][-1.7190061 -0.86167669 -0.35616207 0.016217709 0.14348602 0.28574562 0.72435522 0.80757427 0.33430672 0.15793562 -0.10283518 -0.79725671 -1.0599759 -0.79457521 -0.67520714][-1.811445 -0.90886617 -0.54819751 -0.38344812 -0.24743032 -0.14877176 0.11402988 0.1124382 -0.073931694 -0.001595974 -0.22403526 -0.75756955 -0.90509915 -0.78932834 -0.83116388][-1.9627509 -1.1010504 -0.85413933 -0.75543046 -0.53457451 -0.50770545 -0.32385969 -0.19980478 -0.13243198 -0.020884037 -0.26807547 -0.66690922 -0.80863285 -0.9266355 -1.0774567][-2.2094724 -1.3960757 -1.1657341 -1.0398889 -0.79936004 -0.89813852 -0.79757571 -0.5246017 -0.36450291 -0.32832623 -0.53459764 -0.78702569 -0.98757529 -1.2498698 -1.3268621][-2.6024132 -1.8788152 -1.6230474 -1.3911266 -1.1222956 -1.3063588 -1.2488675 -0.95764852 -0.92066717 -1.058197 -1.2300675 -1.3538313 -1.5582786 -1.7925665 -1.7225711][-3.0935769 -2.4693692 -2.1915331 -1.8660917 -1.5138338 -1.6431556 -1.6251373 -1.4820304 -1.6352298 -1.8979304 -2.0098526 -2.0141082 -2.1556358 -2.3083873 -2.1954978]]...]
INFO - root - 2017-12-07 04:13:09.306074: step 4010, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.749 sec/batch; 68h:18m:00s remains)
INFO - root - 2017-12-07 04:13:16.925718: step 4020, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.778 sec/batch; 70h:57m:52s remains)
INFO - root - 2017-12-07 04:13:24.656768: step 4030, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.757 sec/batch; 69h:06m:07s remains)
INFO - root - 2017-12-07 04:13:32.329968: step 4040, loss = 0.67, batch loss = 0.59 (10.1 examples/sec; 0.789 sec/batch; 72h:00m:00s remains)
INFO - root - 2017-12-07 04:13:40.011543: step 4050, loss = 0.76, batch loss = 0.68 (10.1 examples/sec; 0.788 sec/batch; 71h:54m:43s remains)
INFO - root - 2017-12-07 04:13:47.649095: step 4060, loss = 0.84, batch loss = 0.76 (10.5 examples/sec; 0.765 sec/batch; 69h:46m:37s remains)
INFO - root - 2017-12-07 04:13:55.503194: step 4070, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.781 sec/batch; 71h:17m:02s remains)
INFO - root - 2017-12-07 04:14:02.962093: step 4080, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.766 sec/batch; 69h:55m:10s remains)
INFO - root - 2017-12-07 04:14:10.630205: step 4090, loss = 0.65, batch loss = 0.58 (10.2 examples/sec; 0.788 sec/batch; 71h:52m:07s remains)
INFO - root - 2017-12-07 04:14:18.295762: step 4100, loss = 1.00, batch loss = 0.93 (10.3 examples/sec; 0.774 sec/batch; 70h:36m:43s remains)
2017-12-07 04:14:18.876142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6528521 -3.6038067 -3.6183124 -3.6621256 -3.6987948 -3.7077823 -3.6663074 -3.6600657 -3.7110684 -3.7865264 -3.8318825 -3.7803254 -3.6515913 -3.5703671 -3.540216][-3.7648258 -3.7141128 -3.7473931 -3.8225656 -3.8849204 -3.9141793 -3.852797 -3.8163729 -3.8537333 -3.9287524 -3.9498045 -3.8561122 -3.7241619 -3.714005 -3.7795279][-3.5717375 -3.5001259 -3.5235269 -3.6154923 -3.6721637 -3.6833334 -3.5903723 -3.5311677 -3.563518 -3.6289263 -3.5993409 -3.4832728 -3.4111962 -3.5474684 -3.7554071][-3.1030962 -2.9681256 -2.9497964 -3.0392289 -3.0742674 -3.0565624 -2.9631891 -2.9386735 -3.0232353 -3.1208925 -3.0951753 -3.0364916 -3.0574577 -3.2757254 -3.4975767][-2.4579277 -2.2233336 -2.151567 -2.2411482 -2.2671621 -2.274574 -2.2265027 -2.2615635 -2.4296789 -2.5643663 -2.5816903 -2.6464748 -2.7761338 -3.0343084 -3.2065682][-1.8531625 -1.5450149 -1.4343288 -1.4722073 -1.4713931 -1.5107255 -1.4718077 -1.5501006 -1.8695791 -2.1327751 -2.2453079 -2.4044416 -2.5416651 -2.7273624 -2.7991393][-1.4755251 -1.1435328 -1.0127711 -0.97284055 -0.92901039 -0.90393376 -0.68740892 -0.70618963 -1.208988 -1.6959205 -1.9884999 -2.2797883 -2.4703054 -2.5955672 -2.5420527][-1.4801233 -1.1368053 -0.92288256 -0.72579861 -0.56151962 -0.31253338 0.21426153 0.26708269 -0.45719862 -1.1904943 -1.6649554 -2.0511327 -2.3063045 -2.4552631 -2.390393][-1.7731278 -1.4703367 -1.1616092 -0.79649544 -0.43665123 0.10444927 0.845994 0.8538661 -0.0862484 -1.0055785 -1.6134765 -2.0459111 -2.2925057 -2.4451241 -2.4387465][-2.2771974 -2.060142 -1.6948349 -1.221926 -0.74429655 -0.093281269 0.552073 0.44421577 -0.43017745 -1.2668209 -1.8435965 -2.2354212 -2.4242172 -2.576967 -2.7136333][-2.9642644 -2.8465791 -2.4366622 -1.8813734 -1.3995211 -0.85023594 -0.48282218 -0.70452762 -1.3294823 -1.838861 -2.1836112 -2.38087 -2.3587446 -2.4378233 -2.7749164][-3.4554143 -3.3951554 -2.9450159 -2.4035075 -2.0534124 -1.7226233 -1.6118982 -1.869343 -2.2183998 -2.4306092 -2.6030359 -2.6402159 -2.4136679 -2.4230895 -2.9080107][-3.5280128 -3.4099369 -2.9779365 -2.6238542 -2.5627446 -2.5182662 -2.6205239 -2.8888421 -3.0329118 -3.0447178 -3.036757 -2.8971596 -2.549685 -2.5971322 -3.1712584][-3.2218156 -3.0293376 -2.6936412 -2.6080747 -2.8113885 -2.9548955 -3.1918368 -3.5178676 -3.6461771 -3.5975199 -3.4296603 -3.1026134 -2.7132511 -2.7883079 -3.3482518][-2.6935811 -2.4337146 -2.2381794 -2.3769474 -2.70793 -2.8644848 -3.0770192 -3.3955989 -3.5480225 -3.5362458 -3.3648388 -3.0751445 -2.797184 -2.8857243 -3.3935018]]...]
INFO - root - 2017-12-07 04:14:26.563797: step 4110, loss = 0.85, batch loss = 0.77 (10.4 examples/sec; 0.767 sec/batch; 70h:00m:12s remains)
INFO - root - 2017-12-07 04:14:34.181306: step 4120, loss = 0.98, batch loss = 0.91 (10.1 examples/sec; 0.794 sec/batch; 72h:26m:48s remains)
INFO - root - 2017-12-07 04:14:42.003468: step 4130, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.772 sec/batch; 70h:25m:42s remains)
INFO - root - 2017-12-07 04:14:49.849307: step 4140, loss = 0.86, batch loss = 0.78 (9.9 examples/sec; 0.810 sec/batch; 73h:52m:50s remains)
INFO - root - 2017-12-07 04:14:57.686400: step 4150, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.773 sec/batch; 70h:28m:48s remains)
INFO - root - 2017-12-07 04:15:05.412295: step 4160, loss = 0.96, batch loss = 0.89 (10.6 examples/sec; 0.753 sec/batch; 68h:38m:49s remains)
INFO - root - 2017-12-07 04:15:13.028973: step 4170, loss = 1.27, batch loss = 1.20 (10.7 examples/sec; 0.749 sec/batch; 68h:16m:13s remains)
INFO - root - 2017-12-07 04:15:20.466666: step 4180, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.770 sec/batch; 70h:10m:57s remains)
INFO - root - 2017-12-07 04:15:28.260322: step 4190, loss = 1.10, batch loss = 1.03 (10.3 examples/sec; 0.780 sec/batch; 71h:09m:40s remains)
INFO - root - 2017-12-07 04:15:35.905029: step 4200, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.769 sec/batch; 70h:05m:30s remains)
2017-12-07 04:15:36.513590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5238981 -5.1727571 -4.9113 -3.4446969 -2.240931 -1.9991419 -1.4030085 -1.190989 -1.8831654 -2.1043425 -1.6082046 -1.1565974 -0.99195528 -0.79971504 -1.7558153][-4.2964053 -4.9705696 -4.5972924 -3.1562448 -2.2186913 -1.9045868 -1.1989672 -1.2557936 -2.0080247 -2.1119859 -1.6208293 -1.1097527 -0.69515538 -0.39138126 -1.631093][-3.9985695 -4.5244613 -4.0550971 -2.8682728 -2.2129319 -1.7624943 -1.0796866 -1.5095809 -2.3338511 -2.2427301 -1.593466 -1.0205472 -0.46841574 -0.17019415 -1.6358352][-3.6778183 -3.9900143 -3.4851365 -2.6043944 -2.068363 -1.4122088 -0.731009 -1.4488101 -2.3400486 -2.0933807 -1.2943439 -0.70431948 -0.14167738 0.010592461 -1.6364977][-3.522402 -3.6694977 -3.1633439 -2.4427276 -1.831552 -1.0059681 -0.27701139 -0.97585511 -1.7675414 -1.4525871 -0.78660059 -0.37582302 0.095513344 0.050694942 -1.6738682][-3.7401593 -3.7902353 -3.2878423 -2.5336812 -1.7514741 -0.87341094 -0.073723793 -0.49599862 -0.98797441 -0.68001628 -0.45138311 -0.41313028 -0.087330341 -0.25980139 -1.8527243][-4.1552677 -4.1141734 -3.5960119 -2.7671833 -1.861552 -1.0077853 -0.1833024 -0.36011457 -0.56845 -0.28151369 -0.47389627 -0.78730416 -0.60041666 -0.8197248 -2.1240025][-4.4492831 -4.3694711 -3.8301716 -2.94231 -1.9682121 -1.0586107 -0.2572403 -0.41861343 -0.537277 -0.30782366 -0.70351577 -1.1505954 -1.0978262 -1.4263163 -2.5158463][-4.5714955 -4.5425196 -4.0253472 -3.1244152 -2.0976512 -1.0350842 -0.19644928 -0.30851698 -0.32188416 -0.1762886 -0.7070899 -1.2599928 -1.409379 -1.9318128 -2.9097824][-4.6106987 -4.6289558 -4.1411996 -3.3151951 -2.324693 -1.1431842 -0.27597189 -0.19995785 -0.070624828 -0.10307264 -0.8158288 -1.4935737 -1.8666072 -2.5443614 -3.404448][-4.6894236 -4.7162476 -4.2742381 -3.6210828 -2.7155075 -1.4685228 -0.60674071 -0.42329979 -0.28318834 -0.53914142 -1.378401 -2.1095941 -2.5788741 -3.244545 -3.9073503][-4.8405409 -4.8830628 -4.55595 -4.1044922 -3.2964923 -2.0990775 -1.3630118 -1.2116418 -1.1440446 -1.4739358 -2.2035651 -2.8047 -3.240324 -3.8404577 -4.3661976][-4.9032164 -4.9544535 -4.7688885 -4.4824748 -3.7865713 -2.7820127 -2.2490025 -2.2003794 -2.160346 -2.3932054 -2.8669121 -3.2319155 -3.5553114 -4.0870743 -4.5654588][-4.6919875 -4.7318082 -4.6621537 -4.5027571 -3.9942346 -3.2657795 -2.892839 -2.8149688 -2.7201762 -2.8659277 -3.1908453 -3.4131176 -3.6317384 -4.0679135 -4.4622841][-4.1995025 -4.2364821 -4.2710609 -4.2430234 -3.9528627 -3.4690208 -3.1616 -3.0005171 -2.8764541 -3.0052865 -3.2839694 -3.4621775 -3.6142406 -3.9035401 -4.1587467]]...]
INFO - root - 2017-12-07 04:15:44.162249: step 4210, loss = 0.90, batch loss = 0.83 (10.5 examples/sec; 0.762 sec/batch; 69h:29m:17s remains)
INFO - root - 2017-12-07 04:15:51.786056: step 4220, loss = 1.02, batch loss = 0.95 (10.3 examples/sec; 0.777 sec/batch; 70h:51m:02s remains)
INFO - root - 2017-12-07 04:15:59.451236: step 4230, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.754 sec/batch; 68h:47m:50s remains)
INFO - root - 2017-12-07 04:16:07.062564: step 4240, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.774 sec/batch; 70h:33m:48s remains)
INFO - root - 2017-12-07 04:16:14.674582: step 4250, loss = 0.73, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 69h:20m:11s remains)
INFO - root - 2017-12-07 04:16:22.396439: step 4260, loss = 0.72, batch loss = 0.65 (10.7 examples/sec; 0.749 sec/batch; 68h:17m:50s remains)
INFO - root - 2017-12-07 04:16:29.992734: step 4270, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.775 sec/batch; 70h:37m:31s remains)
INFO - root - 2017-12-07 04:16:37.519322: step 4280, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 72h:56m:24s remains)
INFO - root - 2017-12-07 04:16:45.331522: step 4290, loss = 0.74, batch loss = 0.67 (10.0 examples/sec; 0.801 sec/batch; 73h:00m:55s remains)
INFO - root - 2017-12-07 04:16:53.174008: step 4300, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.789 sec/batch; 71h:53m:21s remains)
2017-12-07 04:16:53.851288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3843603 -3.1613903 -3.0656705 -3.0654931 -2.9162059 -2.4418981 -1.9052277 -1.8449206 -2.357348 -3.0137682 -3.2493274 -2.9050908 -2.7671337 -3.2389512 -3.6280565][-3.4134045 -3.2412276 -3.2148252 -3.2132082 -2.9418716 -2.3703511 -1.8649938 -2.0000193 -2.7405629 -3.354316 -3.3029051 -2.8187029 -2.7309012 -3.190659 -3.5791564][-3.3862834 -3.2278531 -3.1993475 -3.115376 -2.7711854 -2.2828948 -1.9279368 -2.197927 -2.9931402 -3.4565916 -3.102623 -2.5137396 -2.5443621 -3.0695267 -3.4883718][-3.3701453 -3.2272294 -3.146862 -2.9541435 -2.6417387 -2.4035971 -2.1880622 -2.3206482 -2.9535995 -3.2976894 -2.8249907 -2.2362068 -2.3836193 -2.9977274 -3.4187055][-3.3990746 -3.2685442 -3.111078 -2.8381763 -2.6187992 -2.5676503 -2.2388039 -1.909076 -2.2831516 -2.740196 -2.542666 -2.1378763 -2.3242884 -2.9354858 -3.2870817][-3.390765 -3.2376075 -3.0052009 -2.6841412 -2.4915576 -2.3779616 -1.666496 -0.7622869 -1.0720358 -2.0428872 -2.471312 -2.2965593 -2.2912288 -2.7297084 -2.99647][-3.3229861 -3.131052 -2.8400352 -2.4719548 -2.1789227 -1.8360217 -0.81098485 0.29642057 -0.32906771 -1.9520562 -2.8761325 -2.6455822 -2.2149675 -2.4165897 -2.6609211][-3.2567806 -3.0237808 -2.7178698 -2.3305824 -1.9387941 -1.4970174 -0.72962832 -0.15915442 -1.1453087 -2.881216 -3.7143586 -3.1287932 -2.2701144 -2.3175092 -2.648479][-3.2249205 -2.9720955 -2.7057619 -2.3407552 -1.9334564 -1.6800168 -1.5912673 -1.7070823 -2.59256 -3.7804272 -4.1573772 -3.345669 -2.4287291 -2.6083341 -3.1858864][-3.2568903 -3.0546494 -2.8988996 -2.6223378 -2.2669656 -2.2594562 -2.7108588 -3.2085459 -3.7104855 -4.1636047 -4.1060014 -3.2973337 -2.6106725 -2.9710908 -3.6725652][-3.3810821 -3.2692785 -3.2314434 -3.0301156 -2.6997781 -2.7911696 -3.4435856 -4.0630188 -4.2740297 -4.2774076 -4.074615 -3.501056 -3.107734 -3.3927982 -3.8265095][-3.5444627 -3.5562234 -3.6472979 -3.5289202 -3.211519 -3.1855538 -3.599647 -3.9978273 -3.9593248 -3.8102047 -3.717114 -3.5904238 -3.5971549 -3.7854133 -3.8084311][-3.6717219 -3.8026695 -4.0055718 -4.0043283 -3.7487788 -3.5566106 -3.5780408 -3.6537809 -3.472353 -3.270097 -3.2056341 -3.3535058 -3.6614273 -3.7614708 -3.4579527][-3.7118487 -3.8266199 -3.976469 -3.9473219 -3.6846836 -3.3790431 -3.14264 -3.0958798 -3.0345078 -2.9190502 -2.80695 -2.9569368 -3.3358219 -3.3740199 -2.9588015][-3.5963404 -3.5400577 -3.5005989 -3.3601959 -3.0890718 -2.7479649 -2.3702459 -2.3261762 -2.4604192 -2.4610043 -2.291471 -2.3303239 -2.6715984 -2.7145295 -2.418694]]...]
INFO - root - 2017-12-07 04:17:01.571011: step 4310, loss = 1.05, batch loss = 0.98 (10.5 examples/sec; 0.765 sec/batch; 69h:44m:20s remains)
INFO - root - 2017-12-07 04:17:09.165605: step 4320, loss = 0.75, batch loss = 0.67 (10.6 examples/sec; 0.754 sec/batch; 68h:42m:47s remains)
INFO - root - 2017-12-07 04:17:16.798171: step 4330, loss = 0.79, batch loss = 0.71 (10.1 examples/sec; 0.791 sec/batch; 72h:07m:27s remains)
INFO - root - 2017-12-07 04:17:24.573637: step 4340, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 70h:13m:50s remains)
INFO - root - 2017-12-07 04:17:32.237070: step 4350, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.771 sec/batch; 70h:15m:36s remains)
INFO - root - 2017-12-07 04:17:39.897173: step 4360, loss = 1.06, batch loss = 0.98 (10.2 examples/sec; 0.784 sec/batch; 71h:26m:58s remains)
INFO - root - 2017-12-07 04:17:47.575783: step 4370, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.753 sec/batch; 68h:40m:11s remains)
INFO - root - 2017-12-07 04:17:55.112493: step 4380, loss = 0.91, batch loss = 0.84 (10.2 examples/sec; 0.783 sec/batch; 71h:20m:32s remains)
INFO - root - 2017-12-07 04:18:02.724879: step 4390, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.756 sec/batch; 68h:51m:59s remains)
INFO - root - 2017-12-07 04:18:10.328331: step 4400, loss = 0.91, batch loss = 0.83 (10.5 examples/sec; 0.763 sec/batch; 69h:31m:04s remains)
2017-12-07 04:18:10.924825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.745008 -3.7704017 -3.6713958 -3.667454 -3.9338663 -4.1596427 -4.3568983 -4.4127674 -4.0535326 -3.8279316 -3.6622865 -3.7927792 -4.4921188 -4.7264328 -4.1288028][-3.47748 -3.6026797 -3.535367 -3.4412951 -3.532002 -3.6077957 -3.7179253 -3.8325849 -3.6013258 -3.4335299 -3.3379698 -3.4832237 -4.179956 -4.5312829 -4.076591][-3.2480733 -3.463511 -3.4790535 -3.3622446 -3.298254 -3.2121253 -3.1621256 -3.23695 -3.0995407 -3.0334649 -3.1112418 -3.3631172 -4.0513892 -4.4851451 -4.1370974][-3.2888796 -3.5108759 -3.6067295 -3.5538783 -3.4410989 -3.234545 -2.9996316 -2.9732869 -2.8916678 -2.9130516 -3.1320307 -3.467519 -4.0590754 -4.4871793 -4.1881361][-3.3121581 -3.4961791 -3.6382132 -3.6349602 -3.4887509 -3.2083082 -2.8473558 -2.7965109 -2.8628387 -3.0080676 -3.3377821 -3.7437348 -4.1953182 -4.52058 -4.2130685][-3.2558951 -3.3828373 -3.520767 -3.4932389 -3.2709882 -2.9179697 -2.4010329 -2.30745 -2.5683708 -2.8868279 -3.391588 -3.9834354 -4.3821063 -4.5683465 -4.2153721][-3.2384405 -3.292593 -3.4191413 -3.3412547 -3.057611 -2.6611476 -1.9649711 -1.7407188 -2.175133 -2.6815236 -3.3563147 -4.1731172 -4.6065025 -4.6709743 -4.271553][-3.2274437 -3.2370424 -3.4093726 -3.3166456 -2.9680786 -2.5334651 -1.7247355 -1.4046974 -2.0166717 -2.6843269 -3.4303067 -4.3792205 -4.8317332 -4.7692041 -4.3207741][-3.2593322 -3.259321 -3.4720521 -3.3916726 -2.9958029 -2.512063 -1.6572523 -1.2722874 -1.9971609 -2.7531302 -3.458385 -4.4275937 -4.8642931 -4.7003508 -4.2389374][-3.2331595 -3.2220068 -3.4238596 -3.3867979 -3.0384111 -2.5831077 -1.7921267 -1.4414637 -2.2217038 -2.9623079 -3.5381165 -4.4001813 -4.7574854 -4.5024915 -4.0363221][-3.2078083 -3.2451868 -3.3942614 -3.3470762 -3.0818679 -2.7198219 -2.0550804 -1.8086042 -2.5946424 -3.2300639 -3.6174774 -4.3066058 -4.5976224 -4.3282113 -3.9035194][-2.9922876 -3.1006532 -3.2097006 -3.1490738 -3.0239539 -2.7834806 -2.2419114 -2.1169996 -2.874397 -3.3845143 -3.6503096 -4.2606964 -4.5528555 -4.3258114 -3.9298415][-2.5273013 -2.6229639 -2.6876357 -2.6648731 -2.7114272 -2.6060395 -2.2066367 -2.2504408 -2.9971592 -3.3836203 -3.602845 -4.2353716 -4.6006646 -4.4617648 -4.0557165][-2.1238194 -2.0363252 -2.0359018 -2.072093 -2.2499304 -2.2519231 -1.947798 -2.0939076 -2.8316827 -3.1957805 -3.4587138 -4.1681876 -4.6454706 -4.6057611 -4.1528335][-2.0954733 -1.7947931 -1.794167 -1.916033 -2.1189287 -2.1262507 -1.7772932 -1.7731783 -2.3599873 -2.7753015 -3.2052035 -4.0304031 -4.6486974 -4.6980591 -4.1756043]]...]
INFO - root - 2017-12-07 04:18:18.607728: step 4410, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.752 sec/batch; 68h:31m:17s remains)
INFO - root - 2017-12-07 04:18:26.467678: step 4420, loss = 0.77, batch loss = 0.70 (10.1 examples/sec; 0.793 sec/batch; 72h:14m:56s remains)
INFO - root - 2017-12-07 04:18:34.168361: step 4430, loss = 0.71, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 71h:32m:57s remains)
INFO - root - 2017-12-07 04:18:41.935872: step 4440, loss = 0.77, batch loss = 0.69 (10.2 examples/sec; 0.783 sec/batch; 71h:20m:00s remains)
INFO - root - 2017-12-07 04:18:49.722038: step 4450, loss = 0.68, batch loss = 0.61 (10.7 examples/sec; 0.746 sec/batch; 67h:57m:38s remains)
INFO - root - 2017-12-07 04:18:57.354238: step 4460, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.772 sec/batch; 70h:18m:32s remains)
INFO - root - 2017-12-07 04:19:05.154764: step 4470, loss = 0.89, batch loss = 0.81 (9.8 examples/sec; 0.812 sec/batch; 74h:01m:53s remains)
INFO - root - 2017-12-07 04:19:12.668368: step 4480, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.787 sec/batch; 71h:44m:49s remains)
INFO - root - 2017-12-07 04:19:20.324926: step 4490, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.770 sec/batch; 70h:09m:52s remains)
INFO - root - 2017-12-07 04:19:27.926045: step 4500, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.754 sec/batch; 68h:40m:24s remains)
2017-12-07 04:19:28.486044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4864883 -2.8706875 -3.353538 -3.2168083 -2.7928152 -2.750689 -2.6949399 -2.4035408 -2.1837864 -2.1563437 -2.2933161 -2.231792 -2.1770883 -2.5600619 -3.0507326][-2.2696714 -2.8548496 -3.2691631 -2.9235728 -2.4036977 -2.5266509 -2.6370277 -2.3054612 -1.9737179 -1.7117949 -1.5463302 -1.2652972 -1.3174958 -2.0148768 -2.8096256][-1.9680545 -2.694684 -2.9969978 -2.5179508 -2.0150084 -2.2784538 -2.5304224 -2.3884432 -2.28775 -2.0636785 -1.6495044 -1.0060959 -0.95038152 -1.7674222 -2.7441347][-1.774415 -2.5126567 -2.6795223 -2.0645425 -1.5103185 -1.7150605 -1.9325206 -2.0803006 -2.4476047 -2.5818391 -2.225821 -1.3572707 -1.0478806 -1.7540476 -2.7765861][-1.8793311 -2.4713469 -2.5514855 -1.8749316 -1.1895645 -1.1141632 -1.0859821 -1.445241 -2.1481025 -2.5369461 -2.381047 -1.6356659 -1.2194364 -1.7540882 -2.723413][-2.3199902 -2.6544878 -2.7270255 -2.1417773 -1.4363768 -1.0489757 -0.63067818 -0.84089184 -1.4557462 -1.847903 -1.9350574 -1.4975355 -1.1534846 -1.6382754 -2.5827785][-2.7531323 -2.8457608 -3.058949 -2.7693796 -2.2407076 -1.746201 -1.1161437 -1.0419459 -1.2619195 -1.3893273 -1.499264 -1.2284279 -1.0029142 -1.5723972 -2.5341389][-2.8722527 -2.7328238 -3.0420649 -3.1156502 -2.9196968 -2.5512238 -2.0104594 -1.8391256 -1.7685053 -1.5940228 -1.4438279 -0.99408865 -0.76046109 -1.4587848 -2.5475926][-2.6066356 -2.1802707 -2.3102019 -2.5940328 -2.82522 -2.9087696 -2.7544115 -2.6171453 -2.4215167 -2.1382976 -1.75034 -1.032867 -0.67900634 -1.4053571 -2.6042933][-2.6447158 -1.9845643 -1.7632833 -1.9058409 -2.3223083 -2.7887535 -3.0026269 -2.935935 -2.7514539 -2.59304 -2.2678211 -1.5771194 -1.17855 -1.7309177 -2.8056016][-3.429667 -2.6650825 -2.2134869 -2.0917463 -2.3719182 -2.8380094 -3.0628986 -2.9094057 -2.6878455 -2.6293221 -2.4773331 -2.0225098 -1.7431903 -2.1616023 -3.0668936][-3.9944293 -3.264493 -2.7636013 -2.5208898 -2.6574097 -3.0400922 -3.1696632 -2.8527575 -2.4629297 -2.2907064 -2.2071953 -2.0387864 -2.0142438 -2.4265473 -3.2081943][-3.8128004 -3.2087593 -2.7755857 -2.5942874 -2.7147851 -3.061832 -3.1828732 -2.8842442 -2.445811 -2.1107481 -1.9548967 -1.9354455 -2.1176219 -2.5765283 -3.2729225][-3.4097984 -2.9788556 -2.7303302 -2.7390716 -2.9543962 -3.2977815 -3.4408615 -3.2605329 -2.9514315 -2.6558168 -2.4634705 -2.4204764 -2.5680609 -2.8975017 -3.388556][-3.5281911 -3.2785406 -3.1559703 -3.2434962 -3.4579639 -3.7170563 -3.8300133 -3.7525821 -3.5937645 -3.4166689 -3.2612042 -3.1811371 -3.2266245 -3.3560278 -3.5781453]]...]
INFO - root - 2017-12-07 04:19:36.274508: step 4510, loss = 0.64, batch loss = 0.56 (10.7 examples/sec; 0.751 sec/batch; 68h:25m:57s remains)
INFO - root - 2017-12-07 04:19:43.956692: step 4520, loss = 0.69, batch loss = 0.62 (10.1 examples/sec; 0.791 sec/batch; 72h:02m:44s remains)
INFO - root - 2017-12-07 04:19:51.668104: step 4530, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.785 sec/batch; 71h:32m:21s remains)
INFO - root - 2017-12-07 04:19:59.359324: step 4540, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.773 sec/batch; 70h:23m:42s remains)
INFO - root - 2017-12-07 04:20:07.031744: step 4550, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.755 sec/batch; 68h:46m:44s remains)
INFO - root - 2017-12-07 04:20:14.712636: step 4560, loss = 0.90, batch loss = 0.82 (10.3 examples/sec; 0.780 sec/batch; 71h:02m:50s remains)
INFO - root - 2017-12-07 04:20:22.398827: step 4570, loss = 0.64, batch loss = 0.57 (10.7 examples/sec; 0.747 sec/batch; 68h:01m:27s remains)
INFO - root - 2017-12-07 04:20:29.742060: step 4580, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.752 sec/batch; 68h:32m:27s remains)
INFO - root - 2017-12-07 04:20:37.392556: step 4590, loss = 0.70, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 69h:21m:51s remains)
INFO - root - 2017-12-07 04:20:45.083500: step 4600, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.770 sec/batch; 70h:08m:34s remains)
2017-12-07 04:20:45.655372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.354444 -1.4063833 -1.4415178 -1.4730384 -1.5109904 -1.5485296 -1.5698729 -1.5622518 -1.534698 -1.5155077 -1.5381002 -1.6137433 -1.6649804 -1.6838806 -1.7180302][-2.22066 -2.2985489 -2.3593738 -2.4129882 -2.4681654 -2.5187325 -2.55912 -2.5724764 -2.5788884 -2.6062756 -2.676533 -2.7474725 -2.7082078 -2.5818787 -2.4689074][-2.5620723 -2.694768 -2.8024864 -2.8611331 -2.8766603 -2.8701577 -2.8720217 -2.8759105 -2.8965554 -2.9370456 -2.9719939 -2.9319415 -2.7209749 -2.4377265 -2.2650285][-2.7893472 -3.0062413 -3.1744857 -3.2434473 -3.2088554 -3.1243377 -3.0660594 -3.0335088 -3.039638 -3.0401816 -2.9436789 -2.6896946 -2.2448361 -1.7811854 -1.5198617][-3.1245103 -3.3774414 -3.5618253 -3.6335506 -3.6036017 -3.5313594 -3.4897647 -3.4590147 -3.4508631 -3.422708 -3.2232306 -2.8047559 -2.1947148 -1.6108003 -1.2402611][-3.4254081 -3.4786756 -3.4691744 -3.425142 -3.3656573 -3.3377562 -3.3566952 -3.3751707 -3.3973598 -3.4177165 -3.2647014 -2.8439076 -2.2539227 -1.7073185 -1.3187096][-3.1978867 -3.0112448 -2.7673793 -2.5600557 -2.4327364 -2.4174044 -2.4852848 -2.60247 -2.7542045 -2.9492545 -3.0390544 -2.8323059 -2.4164457 -1.993448 -1.6349452][-2.1308515 -1.7653127 -1.3035054 -0.90848112 -0.70191 -0.63040638 -0.64809394 -0.80514312 -1.0359461 -1.3035545 -1.5299721 -1.5439041 -1.4440417 -1.3654056 -1.2951887][-1.0354431 -0.73644161 -0.31563187 0.039887905 0.21888828 0.34683132 0.486279 0.49416828 0.42835522 0.36914921 0.28180504 0.27770996 0.18760204 -0.078651905 -0.4108119][-1.0892878 -1.0958385 -1.0223498 -0.9822228 -0.99937224 -0.93661714 -0.72177434 -0.53093576 -0.38144732 -0.24873924 -0.20847654 -0.10548306 -0.082245827 -0.25320768 -0.507494][-1.6895564 -1.7933755 -1.9023039 -2.0512693 -2.2075558 -2.256696 -2.0836282 -1.8440228 -1.6188591 -1.5049222 -1.5304852 -1.4533567 -1.306407 -1.2200315 -1.1817861][-1.9513147 -1.9146268 -1.9416759 -2.035866 -2.1750598 -2.2865648 -2.2130692 -2.0741563 -1.9061399 -1.8651102 -1.9508669 -1.9290843 -1.7428381 -1.5362525 -1.3826332][-2.1526663 -2.064589 -2.0097461 -1.9909549 -2.0202272 -2.0744236 -2.0078685 -1.9682322 -1.9385288 -2.0096903 -2.1693397 -2.1855209 -1.9675646 -1.7001493 -1.5205669][-2.4777746 -2.4167137 -2.3467088 -2.2805231 -2.2422459 -2.2230022 -2.1298449 -2.1281517 -2.1731076 -2.2794778 -2.4479656 -2.4624181 -2.221885 -1.9509406 -1.7994933][-2.7559862 -2.729465 -2.6834011 -2.6514564 -2.6340027 -2.6358824 -2.5944133 -2.6344132 -2.6289749 -2.5610962 -2.5119896 -2.3780854 -2.0736854 -1.8139143 -1.7268064]]...]
INFO - root - 2017-12-07 04:20:53.388884: step 4610, loss = 0.69, batch loss = 0.62 (10.6 examples/sec; 0.756 sec/batch; 68h:49m:01s remains)
INFO - root - 2017-12-07 04:21:01.167873: step 4620, loss = 0.89, batch loss = 0.82 (10.3 examples/sec; 0.774 sec/batch; 70h:27m:01s remains)
INFO - root - 2017-12-07 04:21:08.827254: step 4630, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.773 sec/batch; 70h:23m:29s remains)
INFO - root - 2017-12-07 04:21:16.446684: step 4640, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.754 sec/batch; 68h:40m:17s remains)
INFO - root - 2017-12-07 04:21:24.060826: step 4650, loss = 0.65, batch loss = 0.57 (10.3 examples/sec; 0.776 sec/batch; 70h:41m:44s remains)
INFO - root - 2017-12-07 04:21:31.732939: step 4660, loss = 0.96, batch loss = 0.88 (10.5 examples/sec; 0.759 sec/batch; 69h:09m:36s remains)
INFO - root - 2017-12-07 04:21:39.332202: step 4670, loss = 0.59, batch loss = 0.52 (10.4 examples/sec; 0.769 sec/batch; 70h:01m:33s remains)
INFO - root - 2017-12-07 04:21:46.730684: step 4680, loss = 0.98, batch loss = 0.90 (10.6 examples/sec; 0.755 sec/batch; 68h:43m:12s remains)
INFO - root - 2017-12-07 04:21:54.421183: step 4690, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.768 sec/batch; 69h:56m:21s remains)
INFO - root - 2017-12-07 04:22:02.192940: step 4700, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.758 sec/batch; 69h:00m:46s remains)
2017-12-07 04:22:02.808301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5605645 -3.6656358 -3.740845 -3.7316132 -3.6762116 -3.6162944 -3.5268703 -3.4434366 -3.4201074 -3.4484572 -3.489383 -3.521147 -3.5415952 -3.5148938 -3.4481785][-3.594934 -3.6622782 -3.6981096 -3.6892924 -3.6725111 -3.6628575 -3.6181345 -3.5554636 -3.5151825 -3.4699306 -3.3944373 -3.3308151 -3.3036642 -3.2681413 -3.2312238][-3.480927 -3.5192764 -3.5617855 -3.6132047 -3.6797936 -3.7139394 -3.6529694 -3.522181 -3.3915715 -3.2339396 -3.0417795 -2.9243608 -2.9051747 -2.9040968 -2.9131031][-3.3252902 -3.3472004 -3.3999159 -3.4883108 -3.5977895 -3.6400948 -3.5775237 -3.4775379 -3.3824954 -3.2143803 -2.9737749 -2.8052049 -2.7375991 -2.6659944 -2.6020103][-3.3256712 -3.3561285 -3.432821 -3.5278687 -3.6195903 -3.5979922 -3.5016434 -3.4776552 -3.4792712 -3.3801589 -3.2017775 -3.0568023 -2.9338303 -2.6986113 -2.4338126][-3.3274372 -3.3854184 -3.4719815 -3.5137932 -3.4908061 -3.3025684 -3.0759306 -3.0771289 -3.1448002 -3.1433387 -3.130877 -3.1392155 -3.0916507 -2.8407993 -2.5090828][-2.9006119 -3.0028982 -3.0787582 -3.0315189 -2.8475571 -2.5077729 -2.224988 -2.3679314 -2.6355083 -2.835927 -3.0428324 -3.1631255 -3.1180325 -2.8578773 -2.5771329][-2.5128717 -2.6267493 -2.6475892 -2.5027125 -2.1759837 -1.7193894 -1.4183488 -1.7064788 -2.1914344 -2.6324291 -3.0514405 -3.2106667 -3.0388181 -2.6609681 -2.37374][-2.7207451 -2.7315776 -2.6397297 -2.4071994 -1.9924743 -1.4787858 -1.1799419 -1.4835212 -2.0439126 -2.6323237 -3.1802793 -3.3580513 -3.0943425 -2.6402647 -2.3427327][-3.2338145 -3.2026181 -3.0902257 -2.9002891 -2.5283587 -2.07177 -1.8551004 -2.1119587 -2.5919759 -3.1335816 -3.6053464 -3.6835563 -3.3411431 -2.8631992 -2.5798006][-3.5654197 -3.6747403 -3.6968381 -3.6711643 -3.4218798 -3.0631812 -2.8735526 -2.9292986 -3.1263988 -3.4231563 -3.6860948 -3.6584697 -3.3183539 -2.8898306 -2.6323547][-3.4287519 -3.6075711 -3.7091348 -3.7977376 -3.6811028 -3.4835455 -3.3920803 -3.3084521 -3.2068441 -3.1849399 -3.2117577 -3.1569712 -2.9633067 -2.7264285 -2.6193347][-2.9813011 -3.0622814 -3.14608 -3.2678246 -3.2724867 -3.2487793 -3.3099144 -3.2516561 -3.0430243 -2.8703942 -2.7558026 -2.6986036 -2.6237411 -2.5299075 -2.5736778][-2.6977541 -2.730968 -2.8543868 -3.0169916 -3.0792837 -3.05519 -3.0301051 -2.8909383 -2.6460423 -2.5278029 -2.5147419 -2.5702429 -2.5749002 -2.4827948 -2.5282524][-2.9622712 -2.9807658 -3.120852 -3.2677317 -3.3171687 -3.2136455 -3.0189424 -2.7536662 -2.4693184 -2.4298964 -2.5916259 -2.8276877 -2.9560957 -2.8586085 -2.8382688]]...]
INFO - root - 2017-12-07 04:22:10.473096: step 4710, loss = 0.72, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 71h:25m:23s remains)
INFO - root - 2017-12-07 04:22:18.126391: step 4720, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 71h:02m:32s remains)
INFO - root - 2017-12-07 04:22:25.884769: step 4730, loss = 0.90, batch loss = 0.82 (10.5 examples/sec; 0.762 sec/batch; 69h:24m:58s remains)
INFO - root - 2017-12-07 04:22:33.655731: step 4740, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.761 sec/batch; 69h:18m:10s remains)
INFO - root - 2017-12-07 04:22:41.335844: step 4750, loss = 0.62, batch loss = 0.55 (10.3 examples/sec; 0.779 sec/batch; 70h:57m:03s remains)
INFO - root - 2017-12-07 04:22:49.071090: step 4760, loss = 0.93, batch loss = 0.85 (10.3 examples/sec; 0.777 sec/batch; 70h:44m:35s remains)
INFO - root - 2017-12-07 04:22:56.886962: step 4770, loss = 0.96, batch loss = 0.89 (10.1 examples/sec; 0.794 sec/batch; 72h:16m:49s remains)
INFO - root - 2017-12-07 04:23:04.131679: step 4780, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.772 sec/batch; 70h:14m:13s remains)
INFO - root - 2017-12-07 04:23:11.729409: step 4790, loss = 0.68, batch loss = 0.60 (10.4 examples/sec; 0.773 sec/batch; 70h:20m:16s remains)
INFO - root - 2017-12-07 04:23:19.518883: step 4800, loss = 0.64, batch loss = 0.56 (10.5 examples/sec; 0.762 sec/batch; 69h:23m:26s remains)
2017-12-07 04:23:20.116489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.464596 -2.6671066 -2.8040204 -2.8504689 -2.7434945 -2.5717964 -2.3707929 -2.1860559 -2.253077 -2.4991241 -2.6541102 -2.5435166 -2.1921239 -1.7769513 -1.548779][-2.192481 -2.6454759 -2.9300647 -2.9925532 -2.819334 -2.5843072 -2.3196092 -2.024718 -2.0727923 -2.4409742 -2.7883124 -2.8421192 -2.5515511 -2.0755928 -1.7063599][-1.7983797 -2.501 -2.9476995 -3.0276785 -2.8083706 -2.4905772 -2.0717208 -1.6153975 -1.6268635 -2.1911664 -2.8559666 -3.1875706 -3.0766621 -2.6309295 -2.1394551][-1.5795538 -2.3351116 -2.7995033 -2.9194884 -2.7961395 -2.4824572 -1.890568 -1.1777453 -1.0659308 -1.8040791 -2.8110585 -3.4255071 -3.5420763 -3.208519 -2.6392412][-1.558989 -2.2187321 -2.5945253 -2.7655702 -2.800096 -2.5559916 -1.8047822 -0.7457056 -0.38530636 -1.2062526 -2.5074334 -3.4151707 -3.7980828 -3.6344142 -3.0138097][-1.6949298 -2.2359757 -2.4972017 -2.619307 -2.6399305 -2.3800516 -1.4775863 -0.064189911 0.53674889 -0.36962318 -1.9593947 -3.1904531 -3.850812 -3.8583832 -3.1949618][-2.0475328 -2.4710793 -2.5618887 -2.5038202 -2.4155774 -2.1666334 -1.228476 0.42321444 1.2412753 0.36779976 -1.3003349 -2.7323706 -3.6056061 -3.8025672 -3.2364311][-2.4500079 -2.7409711 -2.645416 -2.4298434 -2.3009751 -2.1294661 -1.2484407 0.45329618 1.3973322 0.72487688 -0.72694063 -2.1408148 -3.107707 -3.4704156 -3.1350498][-2.840138 -2.9621894 -2.7035987 -2.3885653 -2.2302012 -2.031594 -1.1948802 0.34536219 1.2168789 0.722847 -0.43377018 -1.7056937 -2.6304312 -3.0387878 -2.9079032][-3.1731811 -3.1719623 -2.8432758 -2.4907498 -2.2956629 -2.0414226 -1.3082204 -0.081916809 0.62529945 0.29439497 -0.57133555 -1.621948 -2.4045553 -2.7578516 -2.7803178][-3.2139764 -3.1586316 -2.9002366 -2.5907688 -2.3243315 -2.0286722 -1.5234697 -0.78215265 -0.38013077 -0.60701251 -1.1713619 -1.9094231 -2.4745691 -2.7268815 -2.8443582][-2.9617188 -2.8291678 -2.6918106 -2.4946797 -2.1763408 -1.8438802 -1.5876203 -1.3604112 -1.3395975 -1.5728152 -1.9213493 -2.3931296 -2.7729425 -2.9547539 -3.1336219][-2.5894518 -2.4133842 -2.4220598 -2.3999369 -2.1028879 -1.7587245 -1.6720231 -1.8022277 -2.0794325 -2.3515058 -2.5837219 -2.864819 -3.1219707 -3.3069744 -3.5780017][-2.2227831 -2.062531 -2.1849167 -2.3179934 -2.074378 -1.6794267 -1.5933387 -1.9026191 -2.3943174 -2.7697539 -2.9694524 -3.076282 -3.17272 -3.3323865 -3.7027016][-1.9093955 -1.6918402 -1.8286979 -2.0642445 -1.8918381 -1.4409382 -1.2167413 -1.5012035 -2.1003044 -2.6144583 -2.8337216 -2.8016481 -2.7411637 -2.8660803 -3.3271742]]...]
INFO - root - 2017-12-07 04:23:27.898386: step 4810, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 70h:55m:46s remains)
INFO - root - 2017-12-07 04:23:35.606155: step 4820, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.761 sec/batch; 69h:15m:35s remains)
INFO - root - 2017-12-07 04:23:43.343470: step 4830, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.787 sec/batch; 71h:35m:44s remains)
INFO - root - 2017-12-07 04:23:51.058150: step 4840, loss = 0.97, batch loss = 0.90 (10.6 examples/sec; 0.757 sec/batch; 68h:51m:33s remains)
INFO - root - 2017-12-07 04:23:58.727939: step 4850, loss = 0.86, batch loss = 0.78 (10.5 examples/sec; 0.761 sec/batch; 69h:13m:10s remains)
INFO - root - 2017-12-07 04:24:06.338079: step 4860, loss = 0.87, batch loss = 0.80 (10.7 examples/sec; 0.745 sec/batch; 67h:49m:15s remains)
INFO - root - 2017-12-07 04:24:14.052970: step 4870, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.748 sec/batch; 68h:02m:11s remains)
INFO - root - 2017-12-07 04:24:21.543230: step 4880, loss = 0.77, batch loss = 0.69 (10.9 examples/sec; 0.734 sec/batch; 66h:50m:04s remains)
INFO - root - 2017-12-07 04:24:29.300972: step 4890, loss = 0.84, batch loss = 0.76 (10.5 examples/sec; 0.762 sec/batch; 69h:19m:37s remains)
INFO - root - 2017-12-07 04:24:36.991875: step 4900, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 69h:31m:52s remains)
2017-12-07 04:24:37.595366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6505003 -3.8265371 -3.6794298 -2.9848971 -2.6724429 -2.3585498 -1.9328439 -2.7841315 -3.92301 -4.4908252 -4.889863 -4.7868552 -4.4637995 -4.3374958 -4.2139659][-3.6636903 -3.8184829 -3.6638079 -3.0938678 -2.9188936 -2.5926552 -2.0438337 -2.8171282 -3.9631708 -4.5223885 -4.8581457 -4.732388 -4.3951364 -4.2574449 -4.0736184][-3.754143 -3.9137959 -3.6915879 -3.1179678 -2.9635649 -2.6380835 -2.0779319 -2.8469121 -3.9265487 -4.405756 -4.6943855 -4.5649104 -4.1868358 -4.0040483 -3.757772][-3.8008528 -4.0111079 -3.6919172 -3.006834 -2.7564616 -2.4071643 -1.8752263 -2.6611645 -3.6328125 -3.9823062 -4.3350272 -4.3430405 -3.9979377 -3.8300421 -3.5835686][-3.7541656 -4.0457664 -3.6235433 -2.7758803 -2.3653471 -1.9048069 -1.3087389 -2.0513372 -2.9195156 -3.1225309 -3.6152384 -3.8462112 -3.5288315 -3.4080915 -3.2828836][-3.6696963 -4.0085411 -3.5023117 -2.5581756 -2.0296588 -1.4405444 -0.69778657 -1.3641541 -2.1927469 -2.3024652 -2.9574742 -3.3304009 -2.8937106 -2.8055282 -2.8632295][-3.5964239 -3.893847 -3.3568563 -2.4903629 -2.0016477 -1.3623164 -0.49762225 -1.0912254 -1.9601734 -2.1326 -2.9411173 -3.3405077 -2.7046695 -2.6563153 -2.8433297][-3.5706949 -3.6929662 -3.138176 -2.4732735 -2.1524436 -1.6346431 -0.87895393 -1.4768465 -2.4012339 -2.6620193 -3.4709895 -3.7668569 -2.9456818 -2.8575325 -3.0377035][-3.5794508 -3.4931107 -2.9387891 -2.4496365 -2.2437406 -1.904758 -1.4788156 -2.2030487 -3.1809316 -3.4974942 -4.1855073 -4.3298745 -3.4289923 -3.2656183 -3.4051559][-3.627919 -3.4221005 -2.9018946 -2.479908 -2.2084572 -1.9882455 -1.9336736 -2.8003993 -3.7521036 -4.0179548 -4.5076647 -4.5058718 -3.6415536 -3.4308579 -3.5901098][-3.7092187 -3.5008531 -3.0779347 -2.6979568 -2.3324144 -2.1776872 -2.3628144 -3.2524753 -4.0942507 -4.2661147 -4.5414758 -4.3997622 -3.6277571 -3.3921366 -3.4907808][-3.7146187 -3.5505264 -3.2671752 -2.9897902 -2.6750071 -2.5868087 -2.8323436 -3.60434 -4.27975 -4.3841987 -4.4906816 -4.2512479 -3.6262782 -3.4219527 -3.3991044][-3.5185318 -3.3536043 -3.1755989 -3.0457191 -2.912334 -2.9263773 -3.1484299 -3.7004049 -4.1524215 -4.2025394 -4.2157607 -3.9350116 -3.4682136 -3.3318655 -3.2461729][-3.1735563 -2.9641223 -2.8197377 -2.7893243 -2.8018315 -2.8896222 -3.0676649 -3.4003007 -3.6328826 -3.6466131 -3.6514554 -3.4321685 -3.1445661 -3.109416 -3.0625665][-2.6739738 -2.482717 -2.3467894 -2.3058586 -2.3101768 -2.3826153 -2.4977522 -2.6654329 -2.7388487 -2.7127452 -2.707839 -2.5768342 -2.4413638 -2.4817281 -2.5161924]]...]
INFO - root - 2017-12-07 04:24:45.189472: step 4910, loss = 1.03, batch loss = 0.96 (10.9 examples/sec; 0.737 sec/batch; 67h:05m:03s remains)
INFO - root - 2017-12-07 04:24:52.829964: step 4920, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 72h:11m:46s remains)
INFO - root - 2017-12-07 04:25:00.548928: step 4930, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.772 sec/batch; 70h:16m:43s remains)
INFO - root - 2017-12-07 04:25:08.322950: step 4940, loss = 0.79, batch loss = 0.71 (10.3 examples/sec; 0.780 sec/batch; 71h:00m:35s remains)
INFO - root - 2017-12-07 04:25:15.913284: step 4950, loss = 0.82, batch loss = 0.75 (10.9 examples/sec; 0.734 sec/batch; 66h:48m:52s remains)
INFO - root - 2017-12-07 04:25:23.607236: step 4960, loss = 1.06, batch loss = 0.99 (10.3 examples/sec; 0.777 sec/batch; 70h:43m:58s remains)
INFO - root - 2017-12-07 04:25:31.201464: step 4970, loss = 0.68, batch loss = 0.60 (10.7 examples/sec; 0.751 sec/batch; 68h:19m:56s remains)
INFO - root - 2017-12-07 04:25:38.808988: step 4980, loss = 0.90, batch loss = 0.83 (10.0 examples/sec; 0.798 sec/batch; 72h:33m:19s remains)
INFO - root - 2017-12-07 04:25:46.562249: step 4990, loss = 0.70, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 68h:49m:02s remains)
INFO - root - 2017-12-07 04:25:54.178128: step 5000, loss = 0.75, batch loss = 0.67 (10.8 examples/sec; 0.741 sec/batch; 67h:26m:30s remains)
2017-12-07 04:25:54.749885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.594383 -2.6247675 -2.6198812 -2.5045393 -2.384089 -2.4652519 -2.4942896 -2.4244671 -2.6721535 -3.0520606 -3.3688684 -3.3979831 -2.9577913 -2.6845882 -2.3568063][-2.6066675 -2.6710672 -2.6472197 -2.53658 -2.4123073 -2.4360137 -2.4699059 -2.3732984 -2.4802549 -2.7462549 -3.0009739 -3.0722837 -2.6653461 -2.3308523 -2.0009673][-2.4735689 -2.5297108 -2.5720053 -2.6346164 -2.6712234 -2.7447453 -2.7294726 -2.4806931 -2.4559202 -2.694783 -2.9222112 -3.0457172 -2.7474785 -2.4368765 -2.1734886][-2.4271371 -2.5326226 -2.6539512 -2.9065447 -3.1195278 -3.2107153 -3.0658073 -2.6117845 -2.5261195 -2.8142793 -3.0336084 -3.236738 -3.1179409 -2.9056952 -2.7387726][-2.6398077 -2.8044202 -2.9385366 -3.215837 -3.3891563 -3.3963847 -3.0880668 -2.5289526 -2.5707736 -3.0302663 -3.2600198 -3.4938726 -3.4694574 -3.2872131 -3.1673937][-3.0109737 -3.1508687 -3.1781092 -3.3529873 -3.3982902 -3.2524209 -2.7393556 -2.0464749 -2.2100432 -2.8772283 -3.2286367 -3.5697274 -3.6886563 -3.5692976 -3.5120232][-3.3909774 -3.3744168 -3.2568452 -3.303658 -3.1765876 -2.8156533 -2.0252666 -1.182107 -1.5198171 -2.4424541 -3.0197434 -3.5433102 -3.7975233 -3.8168135 -3.8514485][-3.372159 -3.128232 -2.9224491 -2.9481571 -2.7453678 -2.2199001 -1.1285491 -0.15462971 -0.83682442 -2.1431952 -2.9636984 -3.5567229 -3.8061874 -3.8515568 -3.8621171][-3.1913886 -2.7941318 -2.5639033 -2.6436539 -2.5528822 -2.0943735 -0.89358568 0.09476757 -0.80476832 -2.2297108 -3.0243464 -3.5028844 -3.6843247 -3.7367408 -3.6666858][-2.8622046 -2.4884758 -2.4147053 -2.6898208 -2.928627 -2.8707235 -1.9408169 -1.1237192 -1.9082797 -2.9395993 -3.30794 -3.4215264 -3.4616179 -3.5692482 -3.5013952][-2.5906436 -2.3487873 -2.49079 -2.9230061 -3.4529548 -3.7539973 -3.0871332 -2.3871188 -2.9316449 -3.5697408 -3.6047814 -3.4415159 -3.3566327 -3.5124528 -3.5004306][-2.7379317 -2.6712046 -2.902596 -3.2518573 -3.7615418 -4.1528196 -3.6149328 -2.9468296 -3.2390528 -3.6259305 -3.5686297 -3.3676736 -3.2542186 -3.4136183 -3.4269283][-3.030622 -3.1130662 -3.3180237 -3.4706812 -3.7856379 -4.0695257 -3.632813 -3.070189 -3.2073452 -3.4156594 -3.3336141 -3.15767 -3.0593662 -3.1755347 -3.2023444][-3.550154 -3.6252234 -3.653336 -3.531539 -3.5657268 -3.6677356 -3.3246903 -2.9482725 -3.0314183 -3.156585 -3.145412 -3.0538235 -2.9716682 -3.0126028 -3.0421951][-3.7239766 -3.7633183 -3.7150919 -3.5126224 -3.4140782 -3.4128337 -3.1662347 -2.9171886 -2.937531 -2.9862094 -3.0306 -2.9989028 -2.9301195 -2.9261303 -2.952158]]...]
INFO - root - 2017-12-07 04:26:02.400825: step 5010, loss = 0.84, batch loss = 0.77 (10.7 examples/sec; 0.750 sec/batch; 68h:15m:56s remains)
INFO - root - 2017-12-07 04:26:10.213182: step 5020, loss = 0.86, batch loss = 0.79 (10.0 examples/sec; 0.801 sec/batch; 72h:53m:39s remains)
INFO - root - 2017-12-07 04:26:17.950074: step 5030, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.758 sec/batch; 68h:55m:25s remains)
INFO - root - 2017-12-07 04:26:25.667761: step 5040, loss = 0.82, batch loss = 0.75 (10.1 examples/sec; 0.796 sec/batch; 72h:21m:56s remains)
INFO - root - 2017-12-07 04:26:33.339204: step 5050, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.756 sec/batch; 68h:46m:37s remains)
INFO - root - 2017-12-07 04:26:41.104893: step 5060, loss = 0.66, batch loss = 0.59 (10.0 examples/sec; 0.798 sec/batch; 72h:33m:44s remains)
INFO - root - 2017-12-07 04:26:48.759605: step 5070, loss = 1.02, batch loss = 0.95 (10.5 examples/sec; 0.760 sec/batch; 69h:07m:43s remains)
INFO - root - 2017-12-07 04:26:56.313436: step 5080, loss = 0.86, batch loss = 0.78 (10.4 examples/sec; 0.771 sec/batch; 70h:08m:01s remains)
INFO - root - 2017-12-07 04:27:03.932965: step 5090, loss = 0.97, batch loss = 0.90 (10.8 examples/sec; 0.742 sec/batch; 67h:30m:11s remains)
INFO - root - 2017-12-07 04:27:11.514834: step 5100, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.752 sec/batch; 68h:22m:10s remains)
2017-12-07 04:27:12.078317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2470376 -1.6400216 -1.4724712 -1.5800383 -1.8050234 -1.7824507 -1.6743312 -1.5191584 -1.4293053 -1.6597381 -1.437377 -1.086679 -1.1465931 -1.2505927 -1.4524143][-2.3244908 -1.6823769 -1.4275973 -1.3281538 -1.3746974 -1.3593688 -1.516928 -1.7254982 -1.8914351 -2.1788054 -1.7975056 -1.285985 -1.2778428 -1.3619153 -1.5593171][-1.8673863 -1.395896 -1.2580173 -1.0891969 -1.076756 -1.1885238 -1.7429168 -2.3326185 -2.6392705 -2.7953186 -2.1372538 -1.4563024 -1.4278615 -1.5104413 -1.6702158][-1.466815 -1.3436897 -1.4875 -1.4339337 -1.4921515 -1.661396 -2.3267474 -2.9792809 -3.1986659 -3.0799935 -2.1619134 -1.4111042 -1.4736857 -1.5836291 -1.7243185][-1.4706905 -1.7306833 -2.1483021 -2.3068669 -2.5297112 -2.5998409 -2.896044 -3.2363744 -3.2727704 -2.9734511 -2.0323796 -1.3368063 -1.5069749 -1.6244867 -1.7469723][-1.842011 -2.4010086 -2.9228644 -3.1501293 -3.2975965 -3.0101233 -2.6214683 -2.5385435 -2.5724187 -2.4503977 -1.8710747 -1.4154084 -1.5945795 -1.5881383 -1.6120276][-2.2136528 -2.9735997 -3.4077373 -3.3746727 -3.0726695 -2.2538173 -1.2087376 -0.77237248 -0.98841906 -1.3080359 -1.340276 -1.2802882 -1.466608 -1.3096666 -1.2099209][-2.6217995 -3.3244021 -3.5517504 -3.1553812 -2.2486804 -0.89227772 0.63230371 1.3337145 0.88469219 0.13984299 -0.38596535 -0.672842 -0.87219977 -0.66922379 -0.53600025][-2.8670228 -3.3372076 -3.4032707 -2.8375623 -1.496824 0.16117954 1.8083153 2.5725164 2.0081229 1.1658273 0.54248619 0.12696648 -0.09190464 0.02564764 0.10477638][-3.1048059 -3.3311758 -3.2858791 -2.6871853 -1.1249411 0.63496351 2.112761 2.7893481 2.2733364 1.6233625 1.1753521 0.72270727 0.37716961 0.27557802 0.2328186][-3.3763423 -3.437618 -3.3792446 -2.7923508 -1.1082773 0.65056419 1.8075027 2.3256345 1.9180923 1.5138063 1.2815113 0.85941029 0.4504199 0.20898342 0.12475443][-3.6076298 -3.5769949 -3.4898591 -2.7838016 -0.97655869 0.71631241 1.5678749 1.9030409 1.5880728 1.3254938 1.2121124 0.87100458 0.54511881 0.34412575 0.32603216][-3.5562315 -3.5125282 -3.2465086 -2.2028615 -0.27641869 1.1791611 1.6135421 1.6771212 1.400558 1.2224417 1.1607552 0.91415691 0.72938871 0.63983536 0.70490742][-3.2003791 -3.0627604 -2.4710567 -1.0697243 0.77762508 1.7054963 1.5738149 1.3519096 1.1741962 1.1013641 1.0705957 0.9108777 0.83040905 0.82562494 0.94643641][-2.82632 -2.450212 -1.4443274 0.22174168 1.7836189 2.0524688 1.3692465 0.9457593 0.96181107 1.050941 1.0428066 0.92687225 0.87649107 0.897449 1.0456572]]...]
INFO - root - 2017-12-07 04:27:19.760415: step 5110, loss = 0.73, batch loss = 0.65 (10.6 examples/sec; 0.754 sec/batch; 68h:32m:21s remains)
INFO - root - 2017-12-07 04:27:27.463090: step 5120, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.762 sec/batch; 69h:18m:36s remains)
INFO - root - 2017-12-07 04:27:35.053224: step 5130, loss = 0.78, batch loss = 0.70 (10.7 examples/sec; 0.745 sec/batch; 67h:44m:35s remains)
INFO - root - 2017-12-07 04:27:42.816576: step 5140, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 70h:48m:16s remains)
INFO - root - 2017-12-07 04:27:50.490060: step 5150, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.771 sec/batch; 70h:08m:19s remains)
INFO - root - 2017-12-07 04:27:58.143651: step 5160, loss = 1.00, batch loss = 0.93 (10.1 examples/sec; 0.792 sec/batch; 71h:59m:25s remains)
INFO - root - 2017-12-07 04:28:05.742443: step 5170, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.766 sec/batch; 69h:38m:24s remains)
INFO - root - 2017-12-07 04:28:13.198448: step 5180, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.773 sec/batch; 70h:16m:51s remains)
INFO - root - 2017-12-07 04:28:20.870609: step 5190, loss = 0.68, batch loss = 0.61 (10.4 examples/sec; 0.772 sec/batch; 70h:12m:03s remains)
INFO - root - 2017-12-07 04:28:28.495915: step 5200, loss = 0.80, batch loss = 0.72 (10.8 examples/sec; 0.741 sec/batch; 67h:22m:46s remains)
2017-12-07 04:28:29.104511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0784264 -2.0480525 -2.0186076 -2.011878 -2.0092769 -2.0134826 -2.0216384 -2.0119197 -1.9953644 -1.9778819 -1.9558034 -1.9292312 -1.8875623 -1.8486776 -1.8343201][-2.1998634 -2.1771977 -2.0851293 -2.0027015 -1.9598022 -1.9964635 -2.0915439 -2.1533539 -2.1562312 -2.1230686 -2.0564935 -1.9837017 -1.9114215 -1.8549285 -1.8369071][-2.0530641 -2.131412 -2.0200462 -1.8384676 -1.7066984 -1.7487528 -1.9677978 -2.1876564 -2.3052554 -2.3248904 -2.2301943 -2.0917387 -1.9660881 -1.87289 -1.836931][-1.5280192 -1.7669051 -1.6748698 -1.3942468 -1.1442497 -1.1465964 -1.4635999 -1.8731079 -2.2130144 -2.421345 -2.4193146 -2.2717817 -2.0898495 -1.9226429 -1.829097][-0.7664187 -1.1899729 -1.1034071 -0.67274022 -0.23967123 -0.15825319 -0.5406661 -1.1380329 -1.7219977 -2.1758416 -2.3845644 -2.3540304 -2.2074883 -1.9964206 -1.8279741][0.076909542 -0.56421566 -0.56842232 -0.0035667419 0.71382475 1.0230298 0.68470192 -0.067672253 -0.90690351 -1.6068008 -2.0632398 -2.2394707 -2.2242942 -2.0545704 -1.846123][0.6978569 -0.014225483 -0.19977856 0.26380444 1.1372776 1.8006983 1.768724 1.0925794 0.11934853 -0.81059885 -1.5406268 -1.9691088 -2.1148562 -2.0313382 -1.8395367][0.70476007 0.23309898 0.0049009323 0.23976326 0.95521688 1.6959629 1.9840121 1.7055106 0.92958784 -0.04728651 -0.97884321 -1.6569254 -1.9569294 -1.9491718 -1.7995903][0.16868734 0.067706108 0.021900654 0.14289284 0.5648756 1.0663104 1.3776016 1.4282041 1.0516276 0.26908541 -0.6708982 -1.4692864 -1.8722992 -1.9024372 -1.7782698][-0.52415538 -0.32689714 -0.153193 -0.01669693 0.20380545 0.43540812 0.61407423 0.75870609 0.62349606 0.068151951 -0.71251774 -1.4461308 -1.8803461 -1.9420559 -1.8291667][-1.0249898 -0.67653966 -0.36749268 -0.19184399 -0.049559593 0.040915489 0.11101103 0.22992134 0.16818333 -0.25911379 -0.87481046 -1.471982 -1.9216352 -2.0733202 -2.0012908][-1.182472 -0.86610913 -0.55576587 -0.35604143 -0.19424105 -0.071567535 0.051717758 0.23525572 0.25212669 -0.1176033 -0.68524194 -1.2641127 -1.7952754 -2.1085823 -2.1571615][-1.1138225 -0.97174931 -0.77457571 -0.59082294 -0.40348959 -0.17647076 0.15023279 0.61620569 0.89337111 0.64471292 0.038793564 -0.6839025 -1.3689475 -1.8660016 -2.0907497][-1.0366549 -1.109643 -1.0619123 -0.93291044 -0.7763958 -0.52759695 -0.078487396 0.69002247 1.4198122 1.5488153 1.052712 0.19500875 -0.70560265 -1.4136109 -1.8255911][-1.0654685 -1.2657473 -1.3143094 -1.2559111 -1.1684928 -0.98682 -0.57420635 0.26948452 1.2799335 1.8609757 1.7514324 1.0167727 0.022107124 -0.87020493 -1.4805202]]...]
INFO - root - 2017-12-07 04:28:36.638550: step 5210, loss = 0.70, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 69h:54m:09s remains)
INFO - root - 2017-12-07 04:28:44.321954: step 5220, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.757 sec/batch; 68h:48m:56s remains)
INFO - root - 2017-12-07 04:28:52.064502: step 5230, loss = 0.92, batch loss = 0.85 (10.2 examples/sec; 0.788 sec/batch; 71h:38m:21s remains)
INFO - root - 2017-12-07 04:28:59.622878: step 5240, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.764 sec/batch; 69h:29m:25s remains)
INFO - root - 2017-12-07 04:29:07.282103: step 5250, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.761 sec/batch; 69h:11m:08s remains)
INFO - root - 2017-12-07 04:29:15.133381: step 5260, loss = 0.94, batch loss = 0.87 (10.1 examples/sec; 0.794 sec/batch; 72h:10m:29s remains)
INFO - root - 2017-12-07 04:29:22.849344: step 5270, loss = 0.85, batch loss = 0.78 (10.8 examples/sec; 0.739 sec/batch; 67h:12m:45s remains)
INFO - root - 2017-12-07 04:29:30.207235: step 5280, loss = 0.93, batch loss = 0.86 (10.6 examples/sec; 0.752 sec/batch; 68h:21m:52s remains)
INFO - root - 2017-12-07 04:29:37.907836: step 5290, loss = 0.80, batch loss = 0.72 (10.8 examples/sec; 0.743 sec/batch; 67h:31m:41s remains)
INFO - root - 2017-12-07 04:29:45.678815: step 5300, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.777 sec/batch; 70h:34m:55s remains)
2017-12-07 04:29:46.276876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4706001 -2.1557434 -1.679121 -1.3855617 -1.4013534 -1.5956655 -1.7972155 -2.0049369 -2.2814991 -2.5163107 -2.6974769 -2.777173 -2.5010672 -1.9213204 -1.4189985][-2.5516136 -2.2401447 -1.7346392 -1.3535705 -1.2318797 -1.3445079 -1.5200446 -1.6806369 -1.805398 -1.7990086 -1.9055336 -2.116652 -1.9930432 -1.5412109 -1.1187732][-2.4900222 -2.2891905 -1.8205218 -1.382314 -1.1314836 -1.169245 -1.4156523 -1.6310132 -1.558141 -1.1240945 -0.95370436 -1.298557 -1.5631912 -1.4311967 -1.1515188][-2.3396146 -2.3772473 -2.0012479 -1.4797449 -0.98531461 -0.78768253 -1.109556 -1.574785 -1.5597908 -0.90994596 -0.38472366 -0.61635017 -1.138175 -1.3417635 -1.2995653][-1.9733987 -2.2637932 -2.1179659 -1.6458781 -0.909714 -0.24678707 -0.393672 -1.2251911 -1.6895208 -1.2817678 -0.52384877 -0.2829957 -0.54230285 -0.83102584 -1.0761712][-1.4403672 -1.8727663 -2.0614686 -1.8443155 -1.009392 0.1374526 0.44472837 -0.63293576 -1.7634768 -1.9325416 -1.227787 -0.52693486 -0.25042725 -0.29515934 -0.58729839][-0.8764739 -1.2577813 -1.8139861 -2.0760407 -1.4891086 -0.18523455 0.671288 -0.14873791 -1.5286407 -2.1705208 -1.7794237 -0.98267531 -0.40043116 -0.18108225 -0.28880024][-0.36729479 -0.52876544 -1.3166349 -2.1863656 -2.2389905 -1.2026129 0.027833939 0.031260967 -0.9151032 -1.6842775 -1.6928699 -1.1324615 -0.61756349 -0.35776091 -0.35210705][-0.079620838 -0.011599064 -0.81704187 -2.0466492 -2.7050815 -2.1871293 -0.94942784 -0.24934578 -0.50977921 -1.1373117 -1.439749 -1.1694591 -0.78420138 -0.54898977 -0.50210094][-0.27738619 -0.15204763 -0.78758407 -1.9328117 -2.8275216 -2.7941978 -1.9702375 -1.2238679 -1.0983927 -1.3974016 -1.6461413 -1.5192542 -1.221379 -0.9833324 -0.80395508][-0.92974043 -0.82269192 -1.1626296 -1.9075036 -2.7166412 -3.07854 -2.8575828 -2.4838686 -2.3229516 -2.2929354 -2.2491765 -2.0910389 -1.8695309 -1.6598971 -1.3578484][-1.6698096 -1.5421493 -1.6418667 -2.0289421 -2.6156483 -3.1567438 -3.3622828 -3.3455083 -3.2758741 -3.0997372 -2.8785286 -2.7484717 -2.686389 -2.5713329 -2.279067][-2.1285317 -1.959065 -1.9622719 -2.1828778 -2.5352926 -2.9706078 -3.2693248 -3.4134073 -3.4339719 -3.2981744 -3.1160553 -3.112648 -3.2430966 -3.305001 -3.1459789][-2.3027325 -2.1779585 -2.2086754 -2.3742948 -2.5462985 -2.7411113 -2.9300766 -3.1033263 -3.2476869 -3.2657089 -3.2182481 -3.3056166 -3.5116665 -3.6857979 -3.6822851][-2.4843569 -2.4286368 -2.4969263 -2.6215034 -2.6995237 -2.7353544 -2.8056326 -2.9426703 -3.1369817 -3.2634814 -3.3122358 -3.4036655 -3.5691149 -3.7384224 -3.8176427]]...]
INFO - root - 2017-12-07 04:29:53.935031: step 5310, loss = 1.08, batch loss = 1.01 (10.5 examples/sec; 0.758 sec/batch; 68h:56m:04s remains)
INFO - root - 2017-12-07 04:30:01.600371: step 5320, loss = 0.60, batch loss = 0.53 (10.7 examples/sec; 0.744 sec/batch; 67h:38m:34s remains)
INFO - root - 2017-12-07 04:30:09.217446: step 5330, loss = 0.90, batch loss = 0.83 (10.4 examples/sec; 0.769 sec/batch; 69h:51m:03s remains)
INFO - root - 2017-12-07 04:30:16.889389: step 5340, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.765 sec/batch; 69h:30m:57s remains)
INFO - root - 2017-12-07 04:30:24.637609: step 5350, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.755 sec/batch; 68h:35m:21s remains)
INFO - root - 2017-12-07 04:30:32.272017: step 5360, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.766 sec/batch; 69h:38m:25s remains)
INFO - root - 2017-12-07 04:30:39.967367: step 5370, loss = 0.62, batch loss = 0.55 (10.6 examples/sec; 0.755 sec/batch; 68h:34m:56s remains)
INFO - root - 2017-12-07 04:30:47.314171: step 5380, loss = 0.76, batch loss = 0.68 (10.8 examples/sec; 0.743 sec/batch; 67h:30m:17s remains)
INFO - root - 2017-12-07 04:30:54.960782: step 5390, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.767 sec/batch; 69h:41m:51s remains)
INFO - root - 2017-12-07 04:31:02.722789: step 5400, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.780 sec/batch; 70h:52m:57s remains)
2017-12-07 04:31:03.310972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0606647 -3.1567998 -3.2339282 -3.3090396 -3.4353302 -3.6200752 -3.7264638 -3.810523 -3.8778045 -3.831255 -3.7083032 -3.533875 -3.4134495 -3.3343406 -3.3692231][-3.071497 -3.162612 -3.2295794 -3.2660766 -3.3497787 -3.547147 -3.6399932 -3.8262496 -4.0755978 -4.1222272 -4.0198989 -3.7887456 -3.5614753 -3.4006963 -3.4439468][-3.0375667 -3.0103216 -2.9474154 -2.8399763 -2.860661 -3.1055183 -3.2529626 -3.6463156 -4.2283893 -4.4742684 -4.4415469 -4.1423731 -3.7509036 -3.4613864 -3.5022984][-2.8791976 -2.6855063 -2.4815278 -2.2121322 -2.2272484 -2.5631752 -2.7607479 -3.2317777 -3.9918432 -4.3486619 -4.4082661 -4.1613841 -3.7275376 -3.3637872 -3.3883576][-2.7557621 -2.5004282 -2.2973914 -1.9333768 -1.8739676 -2.1391766 -2.1139455 -2.32632 -3.0057664 -3.3810277 -3.5518878 -3.4837196 -3.1381354 -2.8342414 -2.9560738][-2.6142087 -2.2608914 -2.0350471 -1.5323429 -1.2268283 -1.2170742 -0.77476287 -0.69426012 -1.3847375 -1.9158399 -2.3096547 -2.5515618 -2.457813 -2.4093041 -2.7630866][-2.4177969 -1.9084821 -1.5677719 -0.89439654 -0.38278198 -0.19471455 0.49032402 0.8062911 0.17620182 -0.40274525 -1.0130384 -1.6748378 -1.9996088 -2.375159 -2.9894962][-2.2595036 -1.7029855 -1.3390493 -0.63820887 -0.10176563 0.062475681 0.71033621 1.1256981 0.68387318 0.25308561 -0.38022184 -1.3047156 -1.8861086 -2.5061152 -3.2485936][-2.1810377 -1.7007277 -1.4594944 -0.96517563 -0.63955784 -0.69536543 -0.31470156 0.026573658 -0.15895557 -0.27344465 -0.58415651 -1.3779318 -1.9462907 -2.6344607 -3.4448681][-2.4186649 -2.0558124 -1.9927609 -1.8024418 -1.732657 -1.9707541 -1.7791624 -1.5274107 -1.5619774 -1.3768229 -1.2342107 -1.6111436 -1.9602873 -2.6157856 -3.5428388][-2.7348905 -2.4925203 -2.5718267 -2.6421924 -2.8096442 -3.0948129 -2.9126816 -2.6654506 -2.6486497 -2.3495424 -1.9571517 -1.9803205 -2.0559113 -2.5633521 -3.5164423][-3.1560178 -3.0366268 -3.1610966 -3.3340721 -3.6234641 -3.9347134 -3.8392782 -3.6878464 -3.6786575 -3.404038 -2.9641457 -2.7809753 -2.5951405 -2.7951984 -3.53735][-3.5299697 -3.516397 -3.6283693 -3.8104827 -4.0795007 -4.3054881 -4.2729168 -4.2077417 -4.2390366 -4.0785842 -3.7309275 -3.4544644 -3.0903666 -2.9943514 -3.4918573][-3.6315813 -3.6078794 -3.7071233 -3.9254007 -4.1666565 -4.3148561 -4.3118653 -4.2777948 -4.329154 -4.26642 -3.9962926 -3.7049761 -3.3437819 -3.1433048 -3.4499407][-3.6649151 -3.5819077 -3.6291461 -3.80289 -3.954567 -4.0186138 -4.0206327 -4.0049634 -4.0746417 -4.0801287 -3.8910692 -3.6617215 -3.42171 -3.2604921 -3.4518211]]...]
INFO - root - 2017-12-07 04:31:11.003590: step 5410, loss = 0.72, batch loss = 0.64 (10.8 examples/sec; 0.741 sec/batch; 67h:16m:57s remains)
INFO - root - 2017-12-07 04:31:18.623111: step 5420, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 71h:48m:25s remains)
INFO - root - 2017-12-07 04:31:26.278367: step 5430, loss = 0.72, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 70h:40m:33s remains)
INFO - root - 2017-12-07 04:31:33.898234: step 5440, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.770 sec/batch; 69h:56m:50s remains)
INFO - root - 2017-12-07 04:31:41.528216: step 5450, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.760 sec/batch; 69h:00m:29s remains)
INFO - root - 2017-12-07 04:31:49.134083: step 5460, loss = 1.00, batch loss = 0.93 (10.6 examples/sec; 0.754 sec/batch; 68h:27m:14s remains)
INFO - root - 2017-12-07 04:31:56.764757: step 5470, loss = 0.82, batch loss = 0.75 (10.1 examples/sec; 0.792 sec/batch; 71h:57m:55s remains)
INFO - root - 2017-12-07 04:32:04.120900: step 5480, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.751 sec/batch; 68h:15m:36s remains)
INFO - root - 2017-12-07 04:32:11.808905: step 5490, loss = 0.69, batch loss = 0.61 (10.6 examples/sec; 0.753 sec/batch; 68h:24m:04s remains)
INFO - root - 2017-12-07 04:32:19.417035: step 5500, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.773 sec/batch; 70h:12m:28s remains)
2017-12-07 04:32:20.029450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0436134 -1.4091218 -1.7375672 -1.6656272 -1.2897747 -1.2895222 -1.3223042 -1.3179348 -1.6076438 -1.952451 -2.3758137 -2.7711034 -3.05622 -3.1058509 -3.0240381][-0.77854609 -1.1201146 -1.3206174 -1.0054162 -0.53523564 -0.61951685 -0.69287181 -0.74220753 -1.1831844 -1.6263616 -2.0325344 -2.4813161 -2.8970861 -3.0292773 -2.9776793][-0.78735971 -1.0108318 -1.0269201 -0.53498888 -0.077841759 -0.26382685 -0.33114004 -0.3864603 -0.912102 -1.3924499 -1.7091761 -2.1718557 -2.6680803 -2.825588 -2.77137][-0.68555713 -0.87672234 -0.76942348 -0.23554802 0.10979891 -0.19091415 -0.28452349 -0.36445856 -0.92855525 -1.3174744 -1.498848 -1.9171124 -2.4026687 -2.5096028 -2.3894122][0.009914875 -0.22569418 -0.12845325 0.31071854 0.5423584 0.21553612 0.10835457 -0.041407585 -0.6283884 -0.90318441 -0.95766187 -1.3208342 -1.7782419 -1.8740971 -1.7227292][0.99965382 0.8096323 0.81288815 1.0648136 1.2858772 1.1829553 1.2740002 1.0683665 0.384645 0.047020435 -0.0718689 -0.47662282 -0.93267179 -1.046942 -0.944721][1.4709249 1.4749308 1.4386387 1.5702758 1.8994217 2.328753 2.901494 2.6798019 1.7787833 1.1532812 0.78210258 0.28213787 -0.22159672 -0.44633532 -0.5541935][1.0389872 1.2287083 1.1506519 1.1901202 1.499176 2.206531 3.1291242 3.1176395 2.314373 1.5623565 1.0121765 0.48422861 -0.080075741 -0.46959972 -0.79064178][0.082015514 0.27140045 0.2090807 0.21538973 0.30315304 0.71027803 1.2935085 1.4036083 1.0890522 0.64409447 0.21331692 -0.16893339 -0.62031484 -1.0447805 -1.4819667][-0.84277105 -0.70776987 -0.670326 -0.65229845 -0.77057266 -0.73225808 -0.58900714 -0.55405045 -0.54995966 -0.63046813 -0.69976878 -0.72734237 -0.85707593 -1.1411932 -1.5862296][-1.2291272 -1.060149 -0.91899729 -0.93365932 -1.2035511 -1.3939402 -1.4677615 -1.4314909 -1.1873734 -0.95115995 -0.68689418 -0.45826674 -0.41128016 -0.68550277 -1.221386][-1.1113858 -0.76152754 -0.51805925 -0.56971931 -0.90825462 -1.1654217 -1.2649994 -1.177794 -0.85599041 -0.51546049 -0.095168114 0.19987392 0.16955853 -0.30969954 -1.0348673][-0.57353878 -0.21116924 -0.092233658 -0.3283124 -0.7795763 -1.0717831 -1.1038835 -0.93301511 -0.6478231 -0.39692259 -0.029752731 0.24796391 0.18387699 -0.36493111 -1.130899][-0.4655931 -0.26981258 -0.40077353 -0.8616488 -1.4357433 -1.7724667 -1.7778747 -1.5701082 -1.3317575 -1.1807957 -0.96239376 -0.79139113 -0.85237336 -1.2522843 -1.8013649][-1.4396038 -1.4237893 -1.657824 -2.0935988 -2.559468 -2.8371706 -2.8807082 -2.7776382 -2.663415 -2.571516 -2.3989046 -2.2544212 -2.2409956 -2.3645051 -2.5855188]]...]
INFO - root - 2017-12-07 04:32:27.681485: step 5510, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.773 sec/batch; 70h:13m:08s remains)
INFO - root - 2017-12-07 04:32:35.193703: step 5520, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.753 sec/batch; 68h:24m:23s remains)
INFO - root - 2017-12-07 04:32:42.877118: step 5530, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.752 sec/batch; 68h:19m:21s remains)
INFO - root - 2017-12-07 04:32:50.657865: step 5540, loss = 0.91, batch loss = 0.83 (10.8 examples/sec; 0.743 sec/batch; 67h:29m:55s remains)
INFO - root - 2017-12-07 04:32:58.343671: step 5550, loss = 0.87, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 68h:26m:15s remains)
INFO - root - 2017-12-07 04:33:05.988797: step 5560, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.766 sec/batch; 69h:32m:49s remains)
INFO - root - 2017-12-07 04:33:13.552649: step 5570, loss = 0.80, batch loss = 0.72 (10.8 examples/sec; 0.743 sec/batch; 67h:27m:47s remains)
INFO - root - 2017-12-07 04:33:20.910084: step 5580, loss = 0.86, batch loss = 0.79 (11.0 examples/sec; 0.730 sec/batch; 66h:17m:39s remains)
INFO - root - 2017-12-07 04:33:28.540212: step 5590, loss = 1.00, batch loss = 0.93 (10.8 examples/sec; 0.741 sec/batch; 67h:16m:11s remains)
INFO - root - 2017-12-07 04:33:36.129443: step 5600, loss = 0.78, batch loss = 0.70 (10.7 examples/sec; 0.751 sec/batch; 68h:09m:30s remains)
2017-12-07 04:33:36.702662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2604895 -3.2429006 -2.9658439 -2.6874266 -2.5011296 -2.4523327 -2.529825 -2.6732714 -2.9188197 -3.2869306 -3.7205751 -3.9621906 -3.9280038 -3.7394788 -3.4641438][-3.3198898 -3.3581109 -3.0693965 -2.6853709 -2.2991734 -1.9913018 -1.8079534 -1.8263049 -2.0732672 -2.5790997 -3.350523 -3.9450564 -4.1189766 -4.0002794 -3.643033][-3.2378335 -3.3776433 -3.1540093 -2.7175763 -2.1703479 -1.611902 -1.1529083 -1.0108361 -1.1474659 -1.643415 -2.7069468 -3.7215898 -4.1919703 -4.2147527 -3.8251891][-3.0136254 -3.2531576 -3.129297 -2.7134027 -2.123076 -1.4307585 -0.82122564 -0.55231214 -0.47315812 -0.80724168 -2.0293674 -3.3872905 -4.1522832 -4.3525038 -4.0031123][-2.7584109 -3.0685906 -3.0426459 -2.6792488 -2.0884309 -1.3237307 -0.68070006 -0.29125929 0.10803986 -0.0090641975 -1.3333397 -2.9464962 -3.9988437 -4.4161172 -4.17748][-2.467658 -2.7955985 -2.8283224 -2.511719 -1.9341526 -1.1201491 -0.4846487 0.058102608 0.840631 0.90933084 -0.48921251 -2.2847083 -3.6065607 -4.25745 -4.1733055][-2.213196 -2.4758964 -2.4562535 -2.1785023 -1.6537211 -0.85529113 -0.26946592 0.36002922 1.3661628 1.4437013 0.043873787 -1.7352092 -3.1532233 -3.9131131 -3.9536014][-2.1543059 -2.2903454 -2.1192682 -1.8337862 -1.3905854 -0.70960712 -0.31686354 0.20301437 1.1773133 1.251956 0.11513186 -1.3924565 -2.7402077 -3.5463114 -3.6980939][-2.4263518 -2.4512982 -2.0999463 -1.7279162 -1.3333073 -0.87393951 -0.80921555 -0.45693207 0.42726564 0.62920713 -0.052698612 -1.1631482 -2.383718 -3.233139 -3.487133][-2.8532379 -2.843848 -2.4068518 -1.9637709 -1.5872858 -1.3857536 -1.6269362 -1.361465 -0.48411202 -0.038346291 -0.2184701 -0.97157097 -2.0933175 -2.9884048 -3.3412137][-3.2106256 -3.2534738 -2.8738573 -2.4513927 -2.0828133 -2.0443597 -2.4760056 -2.2801857 -1.4640172 -0.8477819 -0.63722348 -1.0716758 -2.0303528 -2.9095168 -3.3339076][-3.4286981 -3.541116 -3.2762887 -2.9179037 -2.5739555 -2.5985227 -3.0721393 -2.9772463 -2.3797855 -1.8176525 -1.4185891 -1.5906892 -2.2918749 -3.0414453 -3.4682865][-3.5212727 -3.6935306 -3.5420957 -3.2661591 -3.0037589 -3.0846791 -3.5063219 -3.4795594 -3.0826674 -2.6416705 -2.2142649 -2.2417591 -2.7241704 -3.3055086 -3.6485124][-3.4967887 -3.7028928 -3.6679227 -3.5077 -3.3643048 -3.4791667 -3.7995248 -3.8110924 -3.5805073 -3.2660279 -2.8866689 -2.8202319 -3.095211 -3.4731851 -3.6909034][-3.3755484 -3.5614865 -3.5991945 -3.5341613 -3.4864779 -3.6052804 -3.8370085 -3.8911037 -3.7992017 -3.6119914 -3.320466 -3.1850982 -3.2625852 -3.43445 -3.5295796]]...]
INFO - root - 2017-12-07 04:33:44.366667: step 5610, loss = 0.92, batch loss = 0.84 (10.5 examples/sec; 0.762 sec/batch; 69h:10m:16s remains)
INFO - root - 2017-12-07 04:33:52.071784: step 5620, loss = 0.94, batch loss = 0.87 (10.8 examples/sec; 0.744 sec/batch; 67h:31m:43s remains)
INFO - root - 2017-12-07 04:33:59.674086: step 5630, loss = 1.03, batch loss = 0.95 (10.6 examples/sec; 0.754 sec/batch; 68h:28m:30s remains)
INFO - root - 2017-12-07 04:34:07.301448: step 5640, loss = 0.75, batch loss = 0.68 (10.7 examples/sec; 0.748 sec/batch; 67h:53m:17s remains)
INFO - root - 2017-12-07 04:34:14.934292: step 5650, loss = 0.79, batch loss = 0.71 (10.5 examples/sec; 0.766 sec/batch; 69h:30m:09s remains)
INFO - root - 2017-12-07 04:34:22.606916: step 5660, loss = 0.95, batch loss = 0.87 (10.2 examples/sec; 0.787 sec/batch; 71h:29m:43s remains)
INFO - root - 2017-12-07 04:34:30.260940: step 5670, loss = 0.99, batch loss = 0.92 (10.5 examples/sec; 0.760 sec/batch; 69h:01m:11s remains)
INFO - root - 2017-12-07 04:34:37.742016: step 5680, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.753 sec/batch; 68h:19m:00s remains)
INFO - root - 2017-12-07 04:34:45.381304: step 5690, loss = 0.59, batch loss = 0.51 (10.6 examples/sec; 0.758 sec/batch; 68h:46m:21s remains)
INFO - root - 2017-12-07 04:34:52.991701: step 5700, loss = 0.65, batch loss = 0.58 (10.2 examples/sec; 0.785 sec/batch; 71h:16m:07s remains)
2017-12-07 04:34:53.592687: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3689759 -3.3540969 -3.342272 -3.3072536 -3.2799337 -3.268568 -3.2546654 -3.2406158 -3.2531867 -3.2688775 -3.2637806 -3.2754114 -3.2908573 -3.299161 -3.3212442][-3.2819219 -3.222358 -3.1740365 -3.0956087 -3.0500188 -3.0344253 -3.0042048 -2.9615593 -2.9723949 -3.0221395 -3.054673 -3.1001542 -3.1293192 -3.1175292 -3.128149][-2.9279821 -2.8405132 -2.7550986 -2.6437571 -2.6149344 -2.6462979 -2.6437488 -2.6127813 -2.667974 -2.8034422 -2.8972857 -2.9922409 -3.0628166 -3.0589037 -3.0815568][-2.5687428 -2.4604735 -2.3422954 -2.2296014 -2.2739022 -2.4517491 -2.5902696 -2.6649785 -2.7996602 -2.9811263 -3.029803 -3.0375485 -3.0465817 -2.9938824 -2.9981527][-2.2530446 -2.1397483 -2.0292041 -1.9611804 -2.0906942 -2.4206688 -2.7179623 -2.9205005 -3.0963726 -3.2480369 -3.1920319 -3.0737958 -3.0374706 -2.9795151 -2.9905405][-2.2310402 -2.1195643 -2.0253389 -1.964201 -2.0932288 -2.4548423 -2.7820218 -2.9777675 -3.0608926 -3.102581 -2.9550376 -2.7701883 -2.8047395 -2.8632185 -2.9086058][-2.4992819 -2.4229686 -2.3509743 -2.2315147 -2.2258811 -2.4110432 -2.5628123 -2.5984659 -2.4984164 -2.4227931 -2.2512274 -2.0847478 -2.2869983 -2.5296621 -2.6045594][-2.6563704 -2.6484215 -2.6172261 -2.4210553 -2.2412481 -2.166806 -2.0409458 -1.8612244 -1.628284 -1.5400431 -1.4807336 -1.4627128 -1.8650994 -2.2621737 -2.303637][-2.6715655 -2.7324862 -2.7311902 -2.4691131 -2.1451116 -1.821979 -1.4692037 -1.186862 -0.99683022 -1.0576756 -1.2043338 -1.3882403 -1.8939271 -2.255187 -2.1155078][-2.6211848 -2.7237544 -2.7503333 -2.4809947 -2.0819943 -1.6055174 -1.1740012 -0.97681737 -0.97517347 -1.1938572 -1.446888 -1.6872766 -2.1040506 -2.3152306 -2.0351][-2.5063496 -2.6206787 -2.6838129 -2.4957104 -2.1453254 -1.6917992 -1.3769455 -1.3817551 -1.5297272 -1.7714367 -1.9788163 -2.1142254 -2.3029809 -2.3698378 -2.0626404][-2.4270909 -2.539156 -2.6492362 -2.5804436 -2.3591897 -2.061944 -1.9411242 -2.0624104 -2.1653514 -2.25917 -2.3294551 -2.3297467 -2.35993 -2.4023027 -2.1838961][-2.3832395 -2.5140991 -2.6757941 -2.6990855 -2.5786929 -2.4137321 -2.406301 -2.5121179 -2.4485712 -2.3316379 -2.2827034 -2.2157774 -2.2129495 -2.2956247 -2.217943][-2.2397952 -2.3770516 -2.5308211 -2.5484776 -2.4487371 -2.3487887 -2.3818326 -2.4759269 -2.3675649 -2.1919138 -2.1384299 -2.0772712 -2.0795653 -2.1630096 -2.1464455][-2.0909092 -2.2559187 -2.3816681 -2.3445063 -2.2320135 -2.1424985 -2.1682246 -2.2605736 -2.1790965 -2.0338757 -2.0077682 -1.9686913 -1.978168 -2.0280893 -1.9972847]]...]
INFO - root - 2017-12-07 04:35:01.291572: step 5710, loss = 0.71, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 70h:48m:50s remains)
INFO - root - 2017-12-07 04:35:08.937727: step 5720, loss = 0.70, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 72h:06m:15s remains)
INFO - root - 2017-12-07 04:35:16.654940: step 5730, loss = 0.76, batch loss = 0.68 (10.4 examples/sec; 0.766 sec/batch; 69h:29m:38s remains)
INFO - root - 2017-12-07 04:35:24.262909: step 5740, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.762 sec/batch; 69h:10m:34s remains)
INFO - root - 2017-12-07 04:35:31.916757: step 5750, loss = 0.97, batch loss = 0.90 (10.7 examples/sec; 0.746 sec/batch; 67h:41m:06s remains)
INFO - root - 2017-12-07 04:35:39.594964: step 5760, loss = 0.95, batch loss = 0.88 (10.7 examples/sec; 0.747 sec/batch; 67h:49m:42s remains)
INFO - root - 2017-12-07 04:35:47.177358: step 5770, loss = 0.89, batch loss = 0.81 (10.5 examples/sec; 0.760 sec/batch; 68h:59m:28s remains)
INFO - root - 2017-12-07 04:35:54.669161: step 5780, loss = 0.93, batch loss = 0.86 (10.5 examples/sec; 0.760 sec/batch; 68h:59m:48s remains)
INFO - root - 2017-12-07 04:36:02.277540: step 5790, loss = 1.01, batch loss = 0.94 (10.5 examples/sec; 0.759 sec/batch; 68h:50m:46s remains)
INFO - root - 2017-12-07 04:36:09.944941: step 5800, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.762 sec/batch; 69h:06m:58s remains)
2017-12-07 04:36:10.557889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0257158 -2.8656006 -2.7613018 -2.9455342 -3.0535135 -2.7448149 -2.2331502 -1.8094621 -1.5185359 -1.439657 -1.5944405 -1.9027922 -2.239697 -2.50606 -2.4882083][-3.3534358 -3.1399159 -2.9862256 -2.9452238 -2.7346656 -2.2209992 -1.7023811 -1.4825921 -1.4587567 -1.6186018 -1.8875721 -2.1359868 -2.3326035 -2.4834814 -2.3922522][-3.4686768 -3.15365 -2.9861031 -2.8548927 -2.5104489 -1.944886 -1.4868522 -1.4790876 -1.6751585 -1.8962669 -2.0425894 -2.0279999 -1.9276831 -1.8789041 -1.7446356][-3.2198443 -2.9030566 -2.8262093 -2.7418342 -2.4230871 -1.8642542 -1.4157062 -1.5360072 -1.8366389 -1.9858146 -1.9558158 -1.7237582 -1.3985667 -1.1650307 -0.95189023][-2.8321571 -2.5669248 -2.53617 -2.4287767 -2.0404735 -1.3592708 -0.79788208 -0.98407149 -1.39872 -1.5521848 -1.5027027 -1.2822204 -0.95464373 -0.65926766 -0.34289646][-2.4604366 -2.1773372 -2.0748293 -1.8521116 -1.2865648 -0.37178802 0.37418556 0.16819954 -0.39917946 -0.65428686 -0.7041657 -0.62342167 -0.46170521 -0.29711533 -0.00013780594][-2.1940048 -1.7615006 -1.5321646 -1.2221038 -0.56538725 0.44826174 1.2785025 1.0261626 0.31276512 -0.00492239 -0.070918083 -0.070006371 -0.086978912 -0.14835167 -0.0065422058][-1.9246264 -1.3525662 -1.0496821 -0.80644894 -0.36636686 0.34865522 0.91398525 0.60574722 -0.05800581 -0.2234726 -0.10229397 -0.028617382 -0.084207535 -0.24584436 -0.22358036][-1.6212125 -1.0465431 -0.74205661 -0.62199235 -0.50869012 -0.21033621 0.046102524 -0.32521105 -0.88219523 -0.79999352 -0.45141196 -0.33016109 -0.37529278 -0.50728917 -0.51306963][-1.4617865 -1.0224836 -0.77822828 -0.7727468 -0.91206956 -0.89074039 -0.82052016 -1.2080157 -1.6551383 -1.3930867 -0.93536997 -0.83594155 -0.87658858 -0.95278978 -0.94423509][-1.6793389 -1.4226472 -1.2635276 -1.3242135 -1.5291653 -1.4936898 -1.3555293 -1.5950012 -1.8880246 -1.5577796 -1.1784461 -1.2306972 -1.3282838 -1.36972 -1.3829534][-2.228761 -2.1132088 -2.0025291 -2.0544691 -2.1703005 -1.9611306 -1.6269245 -1.6469505 -1.7641277 -1.4440918 -1.2248752 -1.4621918 -1.640552 -1.6496356 -1.6769125][-2.6587749 -2.6464725 -2.556437 -2.586987 -2.6296329 -2.3052552 -1.85584 -1.7421656 -1.7666008 -1.507031 -1.4004099 -1.6960745 -1.8725019 -1.8429842 -1.887162][-2.8185728 -2.8406661 -2.7488985 -2.7595611 -2.7730172 -2.4311483 -1.9940391 -1.9010084 -1.9663978 -1.8326311 -1.8152599 -2.0968971 -2.2192669 -2.1407385 -2.1672561][-2.8037858 -2.7861862 -2.6926856 -2.727746 -2.7966609 -2.5471525 -2.224427 -2.2187345 -2.331598 -2.2604764 -2.263885 -2.4935091 -2.5535278 -2.4421754 -2.4412167]]...]
INFO - root - 2017-12-07 04:36:18.259135: step 5810, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 69h:54m:44s remains)
INFO - root - 2017-12-07 04:36:25.998947: step 5820, loss = 0.91, batch loss = 0.83 (10.2 examples/sec; 0.781 sec/batch; 70h:50m:05s remains)
INFO - root - 2017-12-07 04:36:33.677906: step 5830, loss = 0.68, batch loss = 0.61 (10.7 examples/sec; 0.747 sec/batch; 67h:49m:18s remains)
INFO - root - 2017-12-07 04:36:41.300938: step 5840, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.751 sec/batch; 68h:10m:54s remains)
INFO - root - 2017-12-07 04:36:48.964490: step 5850, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.767 sec/batch; 69h:35m:30s remains)
INFO - root - 2017-12-07 04:36:56.846108: step 5860, loss = 0.80, batch loss = 0.73 (9.8 examples/sec; 0.819 sec/batch; 74h:16m:11s remains)
INFO - root - 2017-12-07 04:37:04.595648: step 5870, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.777 sec/batch; 70h:30m:14s remains)
INFO - root - 2017-12-07 04:37:12.015639: step 5880, loss = 0.64, batch loss = 0.57 (10.5 examples/sec; 0.763 sec/batch; 69h:15m:59s remains)
INFO - root - 2017-12-07 04:37:19.613991: step 5890, loss = 0.66, batch loss = 0.59 (10.7 examples/sec; 0.748 sec/batch; 67h:52m:48s remains)
INFO - root - 2017-12-07 04:37:27.199385: step 5900, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.760 sec/batch; 68h:58m:26s remains)
2017-12-07 04:37:27.767552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0281944 -3.262527 -3.487803 -3.4105124 -3.2290468 -3.0310612 -2.7062161 -2.5822496 -2.7526498 -2.9544156 -3.0483959 -3.1298418 -3.2245922 -3.2636871 -3.2647786][-3.4801426 -3.7526283 -3.9706256 -3.889879 -3.700022 -3.4494033 -3.0685973 -2.9519053 -3.2117505 -3.5051918 -3.6335845 -3.7299376 -3.873451 -3.9776871 -4.0107331][-3.5627499 -3.8378856 -4.0371242 -3.9564939 -3.7337196 -3.3677649 -2.8974888 -2.7815948 -3.1212382 -3.5214453 -3.7432075 -3.9125941 -4.1122866 -4.2456737 -4.25693][-3.4389167 -3.6810393 -3.859308 -3.7767248 -3.4872398 -2.9659057 -2.4095128 -2.3384573 -2.7867546 -3.325356 -3.6873074 -3.9682021 -4.2436318 -4.4037204 -4.3753576][-3.0999527 -3.3109131 -3.5149012 -3.511795 -3.258266 -2.705296 -2.1752009 -2.1934741 -2.7164736 -3.3147101 -3.6873674 -3.957073 -4.2335191 -4.3776016 -4.3049965][-2.6976819 -2.8060782 -2.9954343 -3.0618293 -2.8622441 -2.3584661 -1.9433382 -2.0323339 -2.5006208 -2.9983177 -3.2705777 -3.4875546 -3.7672813 -3.9039214 -3.8081405][-2.3659809 -2.3652432 -2.5787272 -2.7572269 -2.6630425 -2.286927 -2.0859411 -2.3262196 -2.7807 -3.1761277 -3.3334084 -3.4328876 -3.5842576 -3.5679433 -3.3602028][-2.0308864 -2.0155232 -2.3430362 -2.6771574 -2.6683083 -2.3588474 -2.2525175 -2.5440412 -2.9683685 -3.3027978 -3.4219661 -3.4800129 -3.5268829 -3.3729949 -3.0908375][-1.7252278 -1.6700764 -2.0292673 -2.4142978 -2.439353 -2.2079418 -2.1872966 -2.5146346 -2.9192567 -3.2063181 -3.2893384 -3.3536696 -3.4087651 -3.2618637 -2.9764247][-1.7776086 -1.7112684 -2.0997946 -2.5474923 -2.6917706 -2.6499214 -2.7929759 -3.1731791 -3.5380433 -3.7065477 -3.6444783 -3.5966537 -3.587925 -3.4301257 -3.1424773][-2.1232741 -2.130661 -2.5464754 -3.0024967 -3.1737447 -3.1774158 -3.3265443 -3.6503768 -3.9422076 -4.0361156 -3.9261055 -3.8506415 -3.8268228 -3.6854904 -3.4360156][-2.2515483 -2.3048093 -2.6646957 -3.0298696 -3.1526775 -3.1378679 -3.2331436 -3.4699447 -3.705879 -3.7823615 -3.703274 -3.6656921 -3.662313 -3.5673137 -3.3911312][-2.2137444 -2.2647753 -2.5178416 -2.7611256 -2.8516092 -2.8572092 -2.9375591 -3.1185799 -3.3234737 -3.3894427 -3.32952 -3.3077044 -3.3056316 -3.2489836 -3.1197848][-2.2611313 -2.2985175 -2.4458597 -2.5771337 -2.6426885 -2.6776927 -2.752501 -2.8880715 -3.0578566 -3.1141832 -3.0629842 -3.046607 -3.0481811 -3.0238872 -2.9272652][-2.2998946 -2.3381505 -2.4301822 -2.505198 -2.5596261 -2.6191854 -2.68473 -2.7661777 -2.8602865 -2.8669596 -2.8025761 -2.7607229 -2.7438593 -2.7262211 -2.6714907]]...]
INFO - root - 2017-12-07 04:37:35.386493: step 5910, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 69h:45m:55s remains)
INFO - root - 2017-12-07 04:37:43.000812: step 5920, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.774 sec/batch; 70h:15m:26s remains)
INFO - root - 2017-12-07 04:37:50.698618: step 5930, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 70h:28m:53s remains)
INFO - root - 2017-12-07 04:37:58.583932: step 5940, loss = 0.76, batch loss = 0.69 (10.1 examples/sec; 0.796 sec/batch; 72h:11m:22s remains)
INFO - root - 2017-12-07 04:38:06.260673: step 5950, loss = 0.91, batch loss = 0.84 (10.3 examples/sec; 0.778 sec/batch; 70h:32m:05s remains)
INFO - root - 2017-12-07 04:38:14.065606: step 5960, loss = 0.62, batch loss = 0.55 (10.4 examples/sec; 0.772 sec/batch; 70h:03m:09s remains)
INFO - root - 2017-12-07 04:38:21.913650: step 5970, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.765 sec/batch; 69h:21m:52s remains)
INFO - root - 2017-12-07 04:38:29.326208: step 5980, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.780 sec/batch; 70h:45m:32s remains)
INFO - root - 2017-12-07 04:38:36.918681: step 5990, loss = 0.85, batch loss = 0.78 (10.9 examples/sec; 0.733 sec/batch; 66h:30m:06s remains)
INFO - root - 2017-12-07 04:38:44.522685: step 6000, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.775 sec/batch; 70h:17m:02s remains)
2017-12-07 04:38:45.105764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.78516436 -0.70759678 -0.6297574 -0.69890094 -0.94884086 -1.071034 -0.94116187 -0.75287318 -0.9228003 -1.0070939 -0.46453047 0.0344882 0.072938442 0.068873882 0.12774754][-0.98610854 -0.84095192 -0.63371897 -0.69562316 -1.0384724 -1.2128799 -1.0283482 -0.83341217 -1.1033742 -1.3259354 -0.97695637 -0.62578678 -0.53163886 -0.4220891 -0.42192888][-1.1785071 -0.91130877 -0.62449336 -0.77023911 -1.1381965 -1.2040701 -0.90916824 -0.76198053 -1.1592712 -1.5264027 -1.4573157 -1.3540418 -1.2400701 -0.97653675 -0.89705086][-0.93534684 -0.64057255 -0.36815643 -0.60620618 -0.92180634 -0.85331631 -0.48332357 -0.40036821 -0.83544016 -1.279593 -1.5401404 -1.7890627 -1.7899523 -1.4521563 -1.1973629][-0.57163739 -0.31465292 -0.06058979 -0.21786261 -0.32249451 -0.16348267 0.079480648 -0.0054893494 -0.4246583 -0.83879709 -1.3310633 -1.8889744 -2.0883288 -1.8300126 -1.5247104][-0.60150266 -0.46713352 -0.20537519 -0.0749464 0.22681618 0.45157385 0.4167304 0.14452934 -0.17332363 -0.41833878 -0.99098277 -1.7295983 -2.06862 -1.9299083 -1.6804826][-0.698931 -0.79682446 -0.64958954 -0.2872963 0.38444233 0.68447208 0.47889853 0.1863718 0.14320803 0.17143822 -0.37755966 -1.2571921 -1.821888 -1.9020908 -1.7727962][-0.52706647 -0.95413065 -1.0438323 -0.57656312 0.25925493 0.5309062 0.27639818 0.15414286 0.49137402 0.86886787 0.40348387 -0.59292245 -1.4813776 -1.9506481 -2.084374][-0.19844007 -0.98188138 -1.343241 -0.93138814 -0.18993568 -0.050348759 -0.2495389 -0.12752533 0.50406551 1.1147952 0.83782768 -0.042895317 -1.0673244 -1.8613498 -2.3122523][0.036552906 -1.0419362 -1.6431975 -1.376828 -0.83523154 -0.779716 -0.83439732 -0.53763914 0.15408564 0.83442879 0.85750866 0.28133821 -0.67710137 -1.6280584 -2.3375244][0.12438345 -1.0584161 -1.7320554 -1.5247757 -1.1493337 -1.1267917 -1.0574768 -0.68970084 -0.05980587 0.60378027 0.86879015 0.516654 -0.41950989 -1.4993455 -2.4043598][0.20147133 -0.88184476 -1.517458 -1.3422532 -1.0913737 -1.091984 -0.94974923 -0.57957435 -0.038238525 0.58057213 0.93414927 0.62226725 -0.33872557 -1.4610286 -2.4598751][0.18910217 -0.68638825 -1.2004988 -1.0099475 -0.80660176 -0.81192994 -0.69380736 -0.44827437 -0.097007751 0.38564539 0.6496706 0.24463272 -0.688401 -1.6586769 -2.5343375][-0.0035333633 -0.672138 -0.98579478 -0.64259624 -0.37449217 -0.36722565 -0.36701107 -0.36749363 -0.30637646 -0.042878151 0.063245773 -0.44663143 -1.2749636 -2.0050719 -2.6584382][-0.26803827 -0.91699767 -1.1210067 -0.61073542 -0.24005651 -0.18620634 -0.30768204 -0.52783275 -0.6720109 -0.56463504 -0.548512 -1.0685771 -1.7646267 -2.3161838 -2.8040023]]...]
INFO - root - 2017-12-07 04:38:52.763096: step 6010, loss = 0.92, batch loss = 0.85 (10.5 examples/sec; 0.763 sec/batch; 69h:13m:59s remains)
INFO - root - 2017-12-07 04:39:00.304842: step 6020, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.772 sec/batch; 70h:00m:49s remains)
INFO - root - 2017-12-07 04:39:08.094014: step 6030, loss = 0.66, batch loss = 0.59 (10.2 examples/sec; 0.782 sec/batch; 70h:55m:29s remains)
INFO - root - 2017-12-07 04:39:15.708408: step 6040, loss = 0.70, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 69h:36m:37s remains)
INFO - root - 2017-12-07 04:39:23.388215: step 6050, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.763 sec/batch; 69h:10m:33s remains)
INFO - root - 2017-12-07 04:39:31.035695: step 6060, loss = 0.98, batch loss = 0.90 (10.7 examples/sec; 0.750 sec/batch; 68h:02m:13s remains)
INFO - root - 2017-12-07 04:39:38.679607: step 6070, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.755 sec/batch; 68h:25m:44s remains)
INFO - root - 2017-12-07 04:39:46.259873: step 6080, loss = 0.63, batch loss = 0.56 (10.3 examples/sec; 0.779 sec/batch; 70h:37m:16s remains)
INFO - root - 2017-12-07 04:39:53.982382: step 6090, loss = 0.72, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 69h:16m:59s remains)
INFO - root - 2017-12-07 04:40:01.612795: step 6100, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.753 sec/batch; 68h:16m:28s remains)
2017-12-07 04:40:02.200603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5986812 -3.4164405 -3.1865113 -2.9398422 -2.6886597 -2.5712218 -2.583137 -2.5880344 -2.5812871 -2.6334743 -2.7087727 -2.7546167 -2.7163355 -2.7385523 -2.7955122][-3.4290671 -3.1451261 -2.8293698 -2.5364079 -2.2827864 -2.1907749 -2.1949949 -2.2012761 -2.2487669 -2.3403232 -2.4242454 -2.4506457 -2.38452 -2.40449 -2.4510007][-3.1823168 -2.8552766 -2.4800372 -2.1065333 -1.7768044 -1.6004663 -1.5324271 -1.5395193 -1.6807671 -1.8185031 -1.8927112 -1.9283557 -1.919513 -1.9894817 -2.0290172][-2.9918389 -2.6907759 -2.3022282 -1.8599694 -1.4762459 -1.2734447 -1.1777666 -1.1850765 -1.3715994 -1.5197508 -1.5668709 -1.6088724 -1.6473224 -1.7835555 -1.8927584][-2.8341939 -2.5646703 -2.197968 -1.7337496 -1.3655984 -1.2119083 -1.1554513 -1.2199235 -1.5135047 -1.7434516 -1.8127141 -1.8557272 -1.8322582 -1.8819077 -1.9194627][-2.7042537 -2.4206266 -2.0802019 -1.6794612 -1.457238 -1.4710646 -1.49649 -1.5903354 -1.8383782 -1.9271164 -1.8619425 -1.850198 -1.7686262 -1.7156618 -1.6207907][-2.6847672 -2.391119 -2.074018 -1.7109983 -1.5793512 -1.6933913 -1.7903407 -1.8913894 -2.0267034 -1.9440765 -1.7788239 -1.7506037 -1.6052318 -1.4218435 -1.1922982][-2.7262597 -2.5066566 -2.260911 -1.9151397 -1.7420838 -1.8416135 -1.9780126 -2.1021063 -2.1908054 -2.0790954 -1.9554408 -1.9732296 -1.8364353 -1.5975437 -1.2776754][-2.7745094 -2.682327 -2.5392065 -2.2068267 -1.9072185 -1.8405571 -1.8477595 -1.9054415 -1.9827693 -1.981833 -2.0473108 -2.2379451 -2.269237 -2.1675262 -1.9675474][-2.8739824 -2.9101386 -2.8553915 -2.5006731 -2.0351477 -1.7460511 -1.5994802 -1.6180017 -1.7796948 -1.9535859 -2.2304723 -2.5549741 -2.6708298 -2.6376848 -2.5293374][-3.0053682 -3.1291893 -3.1641607 -2.8605225 -2.4255435 -2.1543097 -2.0368905 -2.1289299 -2.3760047 -2.5804558 -2.8216 -3.085362 -3.2127035 -3.2644773 -3.2709618][-3.1846905 -3.3443687 -3.4021289 -3.1386154 -2.8089182 -2.6629939 -2.6475105 -2.7873693 -3.0200186 -3.1241264 -3.2301288 -3.4218574 -3.563293 -3.6503942 -3.7152824][-3.3841989 -3.5488448 -3.5979543 -3.3823524 -3.1520314 -3.1197793 -3.1759093 -3.2976136 -3.4463396 -3.4327109 -3.3894589 -3.434967 -3.4626255 -3.4696362 -3.5218093][-3.5928135 -3.7543728 -3.7942498 -3.6562576 -3.5491984 -3.6294134 -3.7449403 -3.8224833 -3.8560781 -3.7311904 -3.5816808 -3.5334144 -3.5145726 -3.5257995 -3.6029539][-3.8089037 -3.9490993 -3.9426477 -3.8097758 -3.7190692 -3.7854176 -3.8965645 -3.9477324 -3.9436593 -3.8097441 -3.6597152 -3.5769813 -3.5244207 -3.515367 -3.5541296]]...]
INFO - root - 2017-12-07 04:40:09.896824: step 6110, loss = 0.67, batch loss = 0.60 (10.5 examples/sec; 0.765 sec/batch; 69h:20m:15s remains)
INFO - root - 2017-12-07 04:40:17.473399: step 6120, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.751 sec/batch; 68h:02m:40s remains)
INFO - root - 2017-12-07 04:40:25.153044: step 6130, loss = 0.75, batch loss = 0.67 (10.3 examples/sec; 0.778 sec/batch; 70h:31m:02s remains)
INFO - root - 2017-12-07 04:40:32.782279: step 6140, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.758 sec/batch; 68h:42m:09s remains)
INFO - root - 2017-12-07 04:40:40.423477: step 6150, loss = 0.68, batch loss = 0.61 (10.2 examples/sec; 0.784 sec/batch; 71h:02m:03s remains)
INFO - root - 2017-12-07 04:40:47.981036: step 6160, loss = 0.79, batch loss = 0.72 (10.8 examples/sec; 0.741 sec/batch; 67h:09m:25s remains)
INFO - root - 2017-12-07 04:40:55.607253: step 6170, loss = 0.82, batch loss = 0.74 (10.6 examples/sec; 0.757 sec/batch; 68h:37m:38s remains)
INFO - root - 2017-12-07 04:41:03.189329: step 6180, loss = 0.95, batch loss = 0.88 (10.1 examples/sec; 0.791 sec/batch; 71h:41m:50s remains)
INFO - root - 2017-12-07 04:41:10.889166: step 6190, loss = 0.99, batch loss = 0.92 (10.7 examples/sec; 0.745 sec/batch; 67h:30m:18s remains)
INFO - root - 2017-12-07 04:41:18.576504: step 6200, loss = 0.66, batch loss = 0.59 (10.5 examples/sec; 0.764 sec/batch; 69h:14m:27s remains)
2017-12-07 04:41:19.144238: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.8573947 -0.49473596 -0.28317785 -0.45828104 -1.0463624 -1.7206111 -2.0533769 -1.8161852 -1.2097538 -0.63651252 -0.43360257 -0.54105425 -0.63082147 -0.58732271 -0.56908846][-0.41324949 -0.043751717 0.085042 -0.27462339 -0.95904374 -1.63918 -1.957139 -1.7205086 -1.1647985 -0.69175959 -0.5065949 -0.53849912 -0.58850718 -0.63232541 -0.80336356][-0.031416893 0.24732208 0.19726562 -0.31946516 -1.0255911 -1.6593745 -1.9697807 -1.7607546 -1.2001779 -0.67344356 -0.35580063 -0.34356117 -0.45136786 -0.691185 -1.0731144][0.019154072 0.12506151 -0.06387949 -0.57528567 -1.1862066 -1.6812558 -1.8801692 -1.6718845 -1.1559658 -0.58238649 -0.18227959 -0.22216606 -0.42505932 -0.81047058 -1.3109617][-0.11663532 -0.16388559 -0.38911438 -0.76950479 -1.2390289 -1.5763958 -1.5467644 -1.1709037 -0.67393827 -0.24227381 -0.030305862 -0.20896959 -0.45909381 -0.86879206 -1.3937533][-0.26727247 -0.37999964 -0.5529449 -0.81091094 -1.1695867 -1.3649559 -1.0550311 -0.36790228 0.23319769 0.42427778 0.27393341 -0.090381622 -0.39313316 -0.79845905 -1.2918382][-0.42778063 -0.54477406 -0.64097881 -0.81366968 -1.0751057 -1.1230726 -0.58815885 0.38227081 1.0697589 1.0096512 0.58872175 0.10885191 -0.28002071 -0.70427775 -1.1468635][-0.58142376 -0.72728252 -0.78192472 -0.84800768 -0.95724726 -0.82738972 -0.21394539 0.77966452 1.367312 1.1687737 0.70107031 0.22891188 -0.20074511 -0.58188367 -0.92034054][-0.61416483 -0.822057 -0.87060571 -0.84153628 -0.75684142 -0.43424988 0.1679163 0.9154706 1.1869678 0.91292667 0.48466492 0.092650414 -0.24331856 -0.4435792 -0.5953207][-0.50188446 -0.83568096 -0.91813922 -0.77124023 -0.44745374 0.02475071 0.5545125 1.0213451 1.032835 0.66602612 0.21847582 -0.12567186 -0.31032324 -0.3062501 -0.24578381][-0.34214973 -0.75717306 -0.82226539 -0.49445581 0.051706791 0.59077215 0.97454596 1.1791058 0.99454069 0.5102191 -0.016038895 -0.36211967 -0.4452126 -0.29870272 -0.072834969][-0.27828121 -0.590153 -0.49722338 0.038429737 0.68313074 1.0964632 1.2700424 1.2475524 0.87874889 0.2830348 -0.2546916 -0.58465004 -0.64177346 -0.4495647 -0.20620728][-0.33051586 -0.4508884 -0.16581249 0.51523924 1.1361051 1.2960782 1.1827989 0.96899223 0.5572691 0.024699211 -0.380548 -0.6457932 -0.72976422 -0.6014843 -0.49442315][-0.39964533 -0.3881259 -0.026868343 0.63854456 1.1324382 1.0404115 0.70445013 0.3523407 0.0069260597 -0.31045532 -0.45498133 -0.57327676 -0.66063333 -0.65719151 -0.73793721][-0.30095577 -0.25230598 0.045846462 0.5347209 0.78140593 0.47790146 0.065256119 -0.23398924 -0.40524006 -0.48807573 -0.42233944 -0.42021418 -0.4894011 -0.59582829 -0.76979995]]...]
INFO - root - 2017-12-07 04:41:26.963271: step 6210, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.755 sec/batch; 68h:26m:09s remains)
INFO - root - 2017-12-07 04:41:34.674941: step 6220, loss = 0.88, batch loss = 0.81 (10.1 examples/sec; 0.793 sec/batch; 71h:54m:38s remains)
INFO - root - 2017-12-07 04:41:42.495449: step 6230, loss = 0.85, batch loss = 0.77 (10.4 examples/sec; 0.766 sec/batch; 69h:23m:00s remains)
INFO - root - 2017-12-07 04:41:50.262013: step 6240, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.757 sec/batch; 68h:38m:58s remains)
INFO - root - 2017-12-07 04:41:57.965170: step 6250, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 70h:09m:14s remains)
INFO - root - 2017-12-07 04:42:05.596178: step 6260, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.780 sec/batch; 70h:41m:03s remains)
INFO - root - 2017-12-07 04:42:13.290684: step 6270, loss = 0.73, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 71h:03m:35s remains)
INFO - root - 2017-12-07 04:42:20.972320: step 6280, loss = 0.86, batch loss = 0.78 (10.2 examples/sec; 0.787 sec/batch; 71h:17m:10s remains)
INFO - root - 2017-12-07 04:42:28.820110: step 6290, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.771 sec/batch; 69h:54m:22s remains)
INFO - root - 2017-12-07 04:42:36.550368: step 6300, loss = 0.97, batch loss = 0.89 (10.2 examples/sec; 0.788 sec/batch; 71h:24m:27s remains)
2017-12-07 04:42:37.136316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3522348 -4.0651879 -4.4859228 -4.4374018 -4.2830868 -4.2270703 -4.133307 -3.9058719 -3.5753579 -3.3316929 -3.1428094 -3.1143274 -3.2700119 -3.3294954 -3.0747948][-3.444262 -4.002841 -4.352396 -4.3023224 -4.1218095 -4.1438618 -4.1567874 -4.017643 -3.8004353 -3.5938709 -3.4345584 -3.4140024 -3.5786257 -3.6128919 -3.2811313][-3.2633867 -3.7331069 -4.0649309 -4.1029406 -4.0457029 -4.2575941 -4.4296832 -4.3541126 -4.0925851 -3.7605097 -3.5353744 -3.515722 -3.6979342 -3.7043257 -3.3303246][-2.9392076 -3.3239565 -3.6012161 -3.7019181 -3.7901609 -4.1560659 -4.4834142 -4.4859691 -4.1869555 -3.8031356 -3.5822296 -3.5926983 -3.7135425 -3.6394005 -3.2971392][-2.7506461 -3.1442342 -3.3524404 -3.4139824 -3.4903014 -3.8002965 -4.0849075 -4.0210123 -3.5825541 -3.127779 -2.9634271 -3.0525031 -3.1482422 -3.0779037 -2.8995707][-2.2135155 -2.6684589 -2.9010677 -2.9780793 -2.9866049 -3.1424623 -3.3041115 -3.1370714 -2.4815581 -1.849339 -1.6518805 -1.7550669 -1.8827741 -1.9291134 -1.9967601][-1.0130403 -1.3382225 -1.5598755 -1.6664538 -1.6480711 -1.7107966 -1.8094471 -1.6587899 -1.0001216 -0.39325666 -0.31647873 -0.49720168 -0.71442842 -0.882138 -1.0742064][0.065642834 -0.05065012 -0.18591404 -0.21146679 -0.12578249 -0.14305401 -0.23107052 -0.17990971 0.32536411 0.73403311 0.61722946 0.28962231 -0.11707163 -0.42714977 -0.63653755][0.027343273 0.011467934 -0.093449116 -0.092880249 -0.035574913 -0.077921867 -0.19025612 -0.25061083 0.14011526 0.53606653 0.48604012 0.22431517 -0.23173571 -0.59184456 -0.69711494][-1.0402777 -1.026469 -1.1333213 -1.178045 -1.204675 -1.2711198 -1.3622892 -1.4799666 -1.1739519 -0.73975253 -0.61386919 -0.68466783 -1.0091243 -1.212682 -1.0835428][-2.2528815 -2.2864428 -2.4349537 -2.5356553 -2.6166022 -2.6617694 -2.67842 -2.7369015 -2.4254875 -1.9960616 -1.8326643 -1.8544719 -2.0271044 -2.0476973 -1.7471046][-2.9086537 -2.91641 -3.023479 -3.115375 -3.1933303 -3.2504578 -3.2729943 -3.3019912 -3.0375986 -2.7820077 -2.7758148 -2.8423986 -2.8746448 -2.7674816 -2.4491916][-3.2526636 -3.2116048 -3.2913008 -3.3945432 -3.4768848 -3.554152 -3.6101561 -3.649229 -3.4510915 -3.3302307 -3.4166851 -3.4354939 -3.3144464 -3.0940208 -2.7788267][-3.8263407 -3.80715 -3.9196544 -4.0910029 -4.2145991 -4.2787404 -4.28659 -4.2366967 -3.9743659 -3.7708404 -3.7172627 -3.5832429 -3.3515477 -3.062067 -2.791775][-4.4975958 -4.4771996 -4.5594873 -4.702044 -4.7948031 -4.8126016 -4.7647376 -4.6386 -4.3163953 -4.0021772 -3.7876189 -3.5668616 -3.3231444 -3.0398402 -2.859756]]...]
INFO - root - 2017-12-07 04:42:44.752520: step 6310, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.769 sec/batch; 69h:42m:19s remains)
INFO - root - 2017-12-07 04:42:52.537561: step 6320, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.787 sec/batch; 71h:18m:37s remains)
INFO - root - 2017-12-07 04:43:00.312945: step 6330, loss = 0.65, batch loss = 0.58 (10.6 examples/sec; 0.753 sec/batch; 68h:12m:44s remains)
INFO - root - 2017-12-07 04:43:08.213934: step 6340, loss = 1.00, batch loss = 0.93 (10.3 examples/sec; 0.775 sec/batch; 70h:10m:35s remains)
INFO - root - 2017-12-07 04:43:16.021782: step 6350, loss = 0.70, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 71h:03m:10s remains)
INFO - root - 2017-12-07 04:43:23.899800: step 6360, loss = 0.76, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 74h:55m:44s remains)
INFO - root - 2017-12-07 04:43:31.558756: step 6370, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.757 sec/batch; 68h:36m:07s remains)
INFO - root - 2017-12-07 04:43:38.945816: step 6380, loss = 0.66, batch loss = 0.58 (10.7 examples/sec; 0.746 sec/batch; 67h:33m:28s remains)
INFO - root - 2017-12-07 04:43:46.678625: step 6390, loss = 0.87, batch loss = 0.79 (10.3 examples/sec; 0.780 sec/batch; 70h:40m:27s remains)
INFO - root - 2017-12-07 04:43:54.368683: step 6400, loss = 0.76, batch loss = 0.68 (10.7 examples/sec; 0.745 sec/batch; 67h:31m:06s remains)
2017-12-07 04:43:54.983362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.336026 -2.4032667 -2.6661341 -2.8292925 -2.8484578 -2.7778029 -2.8976498 -3.0601361 -2.9306712 -2.67723 -2.35105 -2.2556379 -2.5177844 -2.3504238 -1.576628][-2.175596 -2.1516001 -2.3517108 -2.5169973 -2.5220304 -2.4891744 -2.7001121 -2.9728086 -3.0561967 -2.9422543 -2.5669165 -2.3900578 -2.5996242 -2.4449329 -1.7590072][-2.2929816 -2.1625798 -2.2491555 -2.3824255 -2.3819659 -2.3971548 -2.68104 -3.0965419 -3.4098554 -3.405232 -2.9914024 -2.7160215 -2.788465 -2.6383185 -2.0780752][-2.6960621 -2.3744531 -2.2957649 -2.3419831 -2.2970595 -2.2684958 -2.4863575 -2.9968605 -3.4868145 -3.5550642 -3.1486421 -2.8082848 -2.7785683 -2.6948671 -2.3507664][-2.9985943 -2.4424345 -2.144191 -2.0105369 -1.8592751 -1.7143781 -1.7886434 -2.3460546 -2.988575 -3.1159232 -2.7247169 -2.280268 -2.1426053 -2.1812084 -2.158211][-3.0429935 -2.3922782 -1.9304631 -1.6305914 -1.3725028 -1.1289387 -1.1032052 -1.7121863 -2.5010619 -2.720542 -2.32749 -1.6463377 -1.2338893 -1.2124786 -1.4221513][-2.950711 -2.3712234 -1.8598256 -1.4994612 -1.1931775 -0.86633706 -0.81062984 -1.417851 -2.3085063 -2.6676006 -2.3578765 -1.501843 -0.76661611 -0.56653214 -0.8850975][-2.8537853 -2.38103 -1.9005094 -1.587533 -1.2630265 -0.85839081 -0.77959347 -1.3069675 -2.2036576 -2.7528584 -2.6558366 -1.8572719 -0.97627616 -0.65324473 -1.01453][-2.538672 -2.1671019 -1.7891004 -1.5684226 -1.249404 -0.84189987 -0.78873992 -1.2411191 -2.0836356 -2.7930663 -2.9731979 -2.4796348 -1.7381895 -1.439507 -1.7293925][-2.3189631 -1.9967866 -1.7004919 -1.5558383 -1.2977145 -1.0061085 -1.0408096 -1.4345496 -2.1513221 -2.8787496 -3.2248256 -3.0523677 -2.5615478 -2.3447945 -2.4942517][-2.4832933 -2.2411003 -2.0211868 -1.9090331 -1.6981795 -1.5274613 -1.5941727 -1.8625505 -2.3790991 -2.9589944 -3.2655163 -3.2261436 -2.9277718 -2.8038714 -2.9030466][-3.0759811 -2.8874981 -2.648737 -2.4529986 -2.2189236 -2.1353636 -2.2342653 -2.3993297 -2.7033815 -2.9875751 -3.0236931 -2.8831866 -2.6918998 -2.7341795 -3.0045564][-3.5302281 -3.3663306 -3.1280296 -2.9180777 -2.7436795 -2.7832305 -2.9306984 -2.9906693 -3.0507283 -2.9704652 -2.6747026 -2.3705106 -2.2358983 -2.4390404 -2.91289][-3.5042858 -3.3988547 -3.276967 -3.2068315 -3.2025208 -3.3599021 -3.52248 -3.5045843 -3.4188492 -3.1394086 -2.6977649 -2.3502326 -2.2374704 -2.4435182 -2.9162526][-3.5888517 -3.4741368 -3.415122 -3.4398477 -3.5500979 -3.7453542 -3.873117 -3.8446608 -3.7579017 -3.4788623 -3.0909739 -2.8529928 -2.758589 -2.8404546 -3.129343]]...]
INFO - root - 2017-12-07 04:44:02.805818: step 6410, loss = 0.72, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 73h:26m:44s remains)
INFO - root - 2017-12-07 04:44:10.600070: step 6420, loss = 0.76, batch loss = 0.68 (10.4 examples/sec; 0.768 sec/batch; 69h:35m:02s remains)
INFO - root - 2017-12-07 04:44:18.307269: step 6430, loss = 0.93, batch loss = 0.86 (10.1 examples/sec; 0.795 sec/batch; 72h:02m:56s remains)
INFO - root - 2017-12-07 04:44:25.979068: step 6440, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.764 sec/batch; 69h:09m:41s remains)
INFO - root - 2017-12-07 04:44:33.806534: step 6450, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.766 sec/batch; 69h:23m:01s remains)
INFO - root - 2017-12-07 04:44:41.574503: step 6460, loss = 1.01, batch loss = 0.94 (10.0 examples/sec; 0.798 sec/batch; 72h:15m:50s remains)
INFO - root - 2017-12-07 04:44:49.207769: step 6470, loss = 0.88, batch loss = 0.80 (10.3 examples/sec; 0.780 sec/batch; 70h:39m:55s remains)
INFO - root - 2017-12-07 04:44:56.541158: step 6480, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.745 sec/batch; 67h:30m:16s remains)
INFO - root - 2017-12-07 04:45:04.204453: step 6490, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.752 sec/batch; 68h:06m:07s remains)
INFO - root - 2017-12-07 04:45:11.824615: step 6500, loss = 1.04, batch loss = 0.96 (10.4 examples/sec; 0.770 sec/batch; 69h:43m:34s remains)
2017-12-07 04:45:12.412989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3552279 -3.1892586 -2.9444013 -2.8878355 -2.5929372 -2.3066809 -2.2529054 -2.2881088 -2.4846661 -2.5446882 -2.3890395 -2.2962682 -2.3536942 -2.485456 -2.8245716][-3.4121218 -3.2372723 -2.927233 -2.7046666 -2.1825254 -1.848702 -1.9171722 -2.045831 -2.2875597 -2.3343439 -2.2552209 -2.3358068 -2.4854398 -2.6281734 -2.8687854][-3.4461136 -3.32085 -2.9795032 -2.5431597 -1.8156731 -1.4943931 -1.7884185 -2.0943835 -2.2977231 -2.2119973 -2.1661282 -2.4237483 -2.6916642 -2.8359284 -2.9054437][-3.3590193 -3.2645364 -2.8939977 -2.3070712 -1.5529644 -1.3963835 -1.9465485 -2.361423 -2.4135194 -2.1337352 -2.0644403 -2.4028823 -2.7104526 -2.804213 -2.6926332][-3.218914 -3.1102657 -2.7119 -2.0988216 -1.5252111 -1.6135423 -2.3050687 -2.608146 -2.3957732 -1.959866 -1.8464899 -2.1859415 -2.4915214 -2.5507693 -2.3523407][-3.1020427 -2.95571 -2.5375655 -1.9385896 -1.5308793 -1.7435129 -2.3964708 -2.4911265 -2.0514615 -1.5404799 -1.3975399 -1.718111 -2.0275457 -2.1024191 -1.9242258][-3.0847833 -2.8969941 -2.4271057 -1.7420623 -1.3044102 -1.4546833 -2.0022678 -1.9862204 -1.5213401 -1.1460924 -1.0715945 -1.3597057 -1.6724055 -1.7694714 -1.6356502][-3.1397381 -2.9244647 -2.3837495 -1.5204978 -0.91961694 -0.93295383 -1.422646 -1.5067279 -1.2224712 -1.058363 -1.0487316 -1.224194 -1.4498818 -1.5132637 -1.3921764][-3.1815186 -2.9710627 -2.4100969 -1.4442987 -0.70918107 -0.58114219 -1.0194829 -1.2870035 -1.2183313 -1.1858439 -1.1948211 -1.2297633 -1.319669 -1.3150344 -1.1723821][-3.2111404 -3.0339351 -2.5114737 -1.5221083 -0.7342 -0.51301479 -0.87493277 -1.2453456 -1.2366788 -1.1976013 -1.2210908 -1.2404656 -1.3036435 -1.3050199 -1.1554065][-3.2154691 -3.0555642 -2.5752907 -1.6518955 -0.9612534 -0.7325325 -0.97502851 -1.2938621 -1.2281647 -1.1799316 -1.3372948 -1.4847655 -1.5662653 -1.5314152 -1.3154085][-3.191056 -3.0152383 -2.5885599 -1.8451138 -1.4034526 -1.3011119 -1.4704196 -1.6910212 -1.5873733 -1.5555055 -1.8208103 -2.0544229 -2.1210299 -2.0211952 -1.7474649][-3.2468827 -3.0506463 -2.6624575 -2.1034715 -1.9198167 -2.0118968 -2.2094328 -2.3764758 -2.2320733 -2.1532197 -2.3811755 -2.5643702 -2.5656323 -2.4405406 -2.1991785][-3.3588033 -3.209599 -2.8871436 -2.4582889 -2.3901243 -2.5751452 -2.7902322 -2.9067674 -2.7092931 -2.5764062 -2.7271457 -2.8495603 -2.8225665 -2.7270226 -2.5683122][-3.4292448 -3.3576748 -3.1260633 -2.7740989 -2.6715789 -2.7977202 -2.9714236 -3.0578413 -2.8865619 -2.7610588 -2.8469739 -2.9219794 -2.9002385 -2.8713248 -2.8263907]]...]
INFO - root - 2017-12-07 04:45:20.053695: step 6510, loss = 0.60, batch loss = 0.53 (10.5 examples/sec; 0.761 sec/batch; 68h:55m:48s remains)
INFO - root - 2017-12-07 04:45:27.598653: step 6520, loss = 0.95, batch loss = 0.87 (10.5 examples/sec; 0.761 sec/batch; 68h:55m:38s remains)
INFO - root - 2017-12-07 04:45:35.196761: step 6530, loss = 0.72, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 68h:07m:44s remains)
INFO - root - 2017-12-07 04:45:42.769221: step 6540, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 68h:27m:58s remains)
INFO - root - 2017-12-07 04:45:50.412576: step 6550, loss = 0.95, batch loss = 0.88 (10.3 examples/sec; 0.778 sec/batch; 70h:27m:11s remains)
INFO - root - 2017-12-07 04:45:57.985958: step 6560, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.771 sec/batch; 69h:48m:21s remains)
INFO - root - 2017-12-07 04:46:05.495242: step 6570, loss = 0.75, batch loss = 0.68 (10.8 examples/sec; 0.737 sec/batch; 66h:45m:55s remains)
INFO - root - 2017-12-07 04:46:12.874159: step 6580, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 69h:25m:22s remains)
INFO - root - 2017-12-07 04:46:20.509409: step 6590, loss = 0.84, batch loss = 0.76 (10.4 examples/sec; 0.768 sec/batch; 69h:32m:14s remains)
INFO - root - 2017-12-07 04:46:28.166228: step 6600, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.765 sec/batch; 69h:14m:50s remains)
2017-12-07 04:46:28.828048: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1571779 -4.875988 -3.9519112 -2.7478995 -1.8768554 -1.3631134 -1.1612742 -0.734468 0.078987122 0.5114274 0.51635456 0.49680233 0.25776625 -0.58355284 -1.4469352][-4.92722 -4.6323032 -3.7499843 -2.6567554 -1.89395 -1.5831726 -1.6887023 -1.3985715 -0.49364185 0.15729618 0.31054974 0.22352839 -0.072244167 -0.729342 -1.3791013][-4.7371821 -4.4257059 -3.6327748 -2.674123 -2.0450203 -1.9690652 -2.3550053 -2.2690024 -1.5674069 -1.0421696 -0.9046638 -0.8742609 -0.7710855 -0.7521863 -0.8343358][-4.58064 -4.2100291 -3.4454546 -2.5433245 -1.9708631 -1.9516268 -2.2487907 -2.1964066 -1.8925428 -1.8679347 -2.0001986 -1.8595541 -1.326529 -0.71853638 -0.3123517][-4.3904734 -3.866627 -3.1041324 -2.3386755 -1.878082 -1.7263582 -1.6631653 -1.5430412 -1.7052271 -2.2641084 -2.7395191 -2.7218618 -2.2577729 -1.6785853 -1.1849446][-4.2291994 -3.5089803 -2.7661757 -2.2705872 -1.9804585 -1.646698 -1.1900804 -0.94887114 -1.4492059 -2.4833145 -3.2325063 -3.3800731 -3.155077 -2.8364685 -2.4379027][-4.0655694 -3.2033 -2.5404258 -2.3875816 -2.2805047 -1.7927272 -0.95040751 -0.34006262 -0.81448317 -2.0939684 -3.0798075 -3.4504013 -3.475544 -3.3647635 -3.1516995][-3.9592037 -3.0384259 -2.4451008 -2.5762906 -2.6885071 -2.2143669 -1.1029563 -0.0067167282 -0.18578434 -1.4507744 -2.5871894 -3.1307356 -3.2625132 -3.1816716 -3.159276][-3.986968 -2.9981189 -2.3378992 -2.5489876 -2.8833091 -2.6253672 -1.4964211 -0.15359974 -0.11259508 -1.2428889 -2.3480277 -2.8757796 -2.8797445 -2.5700243 -2.540297][-4.199317 -3.1854191 -2.381393 -2.4557748 -2.9163671 -2.961307 -2.077328 -0.87745571 -0.82066369 -1.8346026 -2.8668976 -3.2450285 -2.8774686 -2.0259304 -1.6216652][-4.5290351 -3.589407 -2.6491647 -2.4022036 -2.7374215 -2.968792 -2.4407892 -1.602977 -1.6103694 -2.4444914 -3.2498124 -3.3592072 -2.5394239 -1.1416433 -0.39195204][-4.7770457 -3.9797649 -2.9842629 -2.3601334 -2.3578703 -2.6397181 -2.5091352 -2.0703759 -2.1280007 -2.757369 -3.3244998 -3.2381337 -2.2073486 -0.65089035 0.13395548][-4.79141 -4.2445507 -3.4180694 -2.6294265 -2.3775887 -2.6868196 -2.9087765 -2.7902093 -2.8618002 -3.3263116 -3.7626994 -3.6761229 -2.787107 -1.4699421 -0.75033569][-4.6699476 -4.423789 -3.9252729 -3.284869 -3.0467384 -3.3883181 -3.7534947 -3.7529535 -3.7217717 -3.946157 -4.2371588 -4.2186408 -3.642374 -2.7122686 -2.0443377][-4.4546366 -4.36383 -4.0853209 -3.6557784 -3.5084705 -3.7952113 -4.120698 -4.1739554 -4.1297455 -4.2130318 -4.3832321 -4.4209137 -4.1218514 -3.5189404 -2.9368148]]...]
INFO - root - 2017-12-07 04:46:36.525936: step 6610, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 68h:58m:10s remains)
INFO - root - 2017-12-07 04:46:44.140203: step 6620, loss = 0.74, batch loss = 0.66 (10.7 examples/sec; 0.749 sec/batch; 67h:49m:36s remains)
INFO - root - 2017-12-07 04:46:51.712531: step 6630, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.759 sec/batch; 68h:43m:14s remains)
INFO - root - 2017-12-07 04:46:59.325608: step 6640, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.758 sec/batch; 68h:38m:28s remains)
INFO - root - 2017-12-07 04:47:06.952614: step 6650, loss = 1.02, batch loss = 0.95 (10.7 examples/sec; 0.745 sec/batch; 67h:25m:44s remains)
INFO - root - 2017-12-07 04:47:14.593827: step 6660, loss = 0.70, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 69h:34m:18s remains)
INFO - root - 2017-12-07 04:47:22.172030: step 6670, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.755 sec/batch; 68h:22m:15s remains)
INFO - root - 2017-12-07 04:47:29.554198: step 6680, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.753 sec/batch; 68h:11m:11s remains)
INFO - root - 2017-12-07 04:47:37.188179: step 6690, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.766 sec/batch; 69h:20m:41s remains)
INFO - root - 2017-12-07 04:47:44.833001: step 6700, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.758 sec/batch; 68h:34m:59s remains)
2017-12-07 04:47:45.458793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8730307 -3.3277674 -3.7648914 -4.0091372 -3.7039921 -3.0526078 -2.3121331 -2.1468451 -2.8625979 -3.6424565 -4.1906195 -4.6949759 -4.7310157 -3.9885569 -2.8061688][-2.7807555 -3.377306 -3.7878988 -3.9295554 -3.578083 -2.9616838 -2.4319739 -2.4649539 -3.1706276 -3.752408 -4.0044513 -4.326571 -4.3334446 -3.6549447 -2.59699][-2.6904445 -3.3965263 -3.812614 -3.958776 -3.6766808 -3.129456 -2.732389 -2.8494029 -3.447969 -3.811517 -3.8210371 -3.9592609 -3.8639469 -3.1438222 -2.2379053][-2.6147752 -3.3775342 -3.7860651 -4.0395079 -3.9212139 -3.4330735 -3.0094538 -2.95179 -3.3055015 -3.5253129 -3.4816968 -3.5798283 -3.4387832 -2.710696 -2.0112712][-2.449512 -3.2339616 -3.6498959 -4.0058184 -4.0285687 -3.5888767 -3.1174941 -2.8662481 -3.0727468 -3.2825 -3.2241695 -3.2411642 -3.0492103 -2.4718335 -2.0625408][-2.3103151 -3.0762329 -3.6167264 -4.0640397 -4.04733 -3.4668872 -2.8787322 -2.5212083 -2.8427637 -3.2690663 -3.2735035 -3.1400647 -2.8387413 -2.4454923 -2.2551157][-2.2778914 -3.028583 -3.7701538 -4.2813978 -4.1389551 -3.3350058 -2.5360377 -2.089241 -2.587954 -3.3048754 -3.4355888 -3.2129197 -2.8151531 -2.5247996 -2.404099][-2.156033 -2.8642974 -3.6883292 -4.1471744 -3.9772558 -3.2078922 -2.3852754 -1.9820721 -2.5531721 -3.4114962 -3.6208415 -3.28032 -2.75553 -2.4846931 -2.4218354][-1.8482718 -2.5884666 -3.4404964 -3.8092122 -3.6481965 -3.0447462 -2.3119047 -2.0534964 -2.5725298 -3.3329592 -3.5359192 -3.1137915 -2.5368946 -2.2939749 -2.3239517][-1.6625314 -2.471633 -3.3114309 -3.5862713 -3.3517656 -2.7658834 -2.0976019 -1.9792411 -2.4314299 -2.9626334 -3.0638261 -2.6464958 -2.1900189 -2.1157792 -2.3257203][-1.9975243 -2.7300634 -3.3763227 -3.4856803 -3.1918254 -2.648355 -2.1432314 -2.1469924 -2.51604 -2.8270607 -2.7716937 -2.3795884 -2.0988159 -2.2378957 -2.6363502][-2.5786457 -3.1147709 -3.5367293 -3.5340338 -3.2484274 -2.813118 -2.5121751 -2.6571808 -2.9669189 -3.1113782 -2.9364297 -2.5890796 -2.4558849 -2.6849134 -3.1104894][-2.8895783 -3.252933 -3.5775957 -3.6024747 -3.3936472 -3.0785534 -2.9347634 -3.1310763 -3.3303766 -3.3338592 -3.1276131 -2.8882766 -2.8629251 -3.0718918 -3.4067278][-2.9579344 -3.1950641 -3.4297841 -3.4733574 -3.3312619 -3.1336343 -3.1157041 -3.2946477 -3.3688054 -3.2551658 -3.0792174 -2.9761944 -3.0192916 -3.1501703 -3.3573885][-2.9472008 -3.0652227 -3.2093911 -3.2572794 -3.1688311 -3.0654967 -3.1057158 -3.24611 -3.2601323 -3.1399179 -3.0309439 -3.0030966 -3.0416415 -3.0773458 -3.1498513]]...]
INFO - root - 2017-12-07 04:47:53.249850: step 6710, loss = 0.70, batch loss = 0.62 (9.7 examples/sec; 0.824 sec/batch; 74h:35m:48s remains)
INFO - root - 2017-12-07 04:48:00.964820: step 6720, loss = 0.73, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 71h:31m:14s remains)
INFO - root - 2017-12-07 04:48:08.701317: step 6730, loss = 0.61, batch loss = 0.53 (10.1 examples/sec; 0.789 sec/batch; 71h:24m:31s remains)
INFO - root - 2017-12-07 04:48:16.369097: step 6740, loss = 0.68, batch loss = 0.61 (10.9 examples/sec; 0.732 sec/batch; 66h:14m:03s remains)
INFO - root - 2017-12-07 04:48:24.098042: step 6750, loss = 1.02, batch loss = 0.94 (10.5 examples/sec; 0.761 sec/batch; 68h:52m:32s remains)
INFO - root - 2017-12-07 04:48:31.823614: step 6760, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.771 sec/batch; 69h:47m:15s remains)
INFO - root - 2017-12-07 04:48:39.544291: step 6770, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.787 sec/batch; 71h:13m:35s remains)
INFO - root - 2017-12-07 04:48:46.980751: step 6780, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.757 sec/batch; 68h:27m:10s remains)
INFO - root - 2017-12-07 04:48:54.698363: step 6790, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.759 sec/batch; 68h:41m:00s remains)
INFO - root - 2017-12-07 04:49:02.297180: step 6800, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.776 sec/batch; 70h:11m:46s remains)
2017-12-07 04:49:02.875136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0143256 -3.167599 -2.9219103 -2.4159875 -2.4339936 -2.5349197 -2.8278153 -3.2462389 -3.5393925 -3.7461464 -3.3507335 -2.8940043 -2.5193341 -2.0504925 -1.8573387][-2.9390697 -3.0955691 -2.8774657 -2.3837318 -2.4874496 -2.7692194 -3.0764117 -3.2698116 -3.376622 -3.5665965 -3.2261357 -2.8990893 -2.5693297 -1.99405 -1.8177395][-2.7246079 -2.7867744 -2.5143118 -1.9789798 -2.0630369 -2.3688707 -2.525991 -2.5639982 -2.7302604 -3.0469894 -2.8584743 -2.6852155 -2.3295617 -1.6202233 -1.5288436][-2.6066391 -2.5427363 -2.2322292 -1.6870852 -1.6905046 -1.9111145 -1.8175256 -1.6676569 -1.862092 -2.2549973 -2.2892709 -2.2932649 -1.953752 -1.2980037 -1.4474082][-3.1193218 -2.9433861 -2.6221085 -2.0469627 -1.8685958 -1.8609338 -1.4092453 -0.9861083 -1.1555939 -1.726094 -2.2430182 -2.7018926 -2.5338118 -1.9945405 -2.2577596][-3.8614788 -3.7033067 -3.4671564 -2.8955231 -2.5033844 -2.1142571 -1.1092327 -0.278255 -0.39097261 -1.2955143 -2.4792676 -3.5047033 -3.5429769 -3.1046109 -3.3110402][-4.0754261 -4.0112143 -3.9606342 -3.5056536 -2.9712262 -2.1408441 -0.53904295 0.73356152 0.63150311 -0.70039797 -2.485239 -3.8230352 -3.8535271 -3.469943 -3.6658654][-3.894798 -3.9302568 -3.9790258 -3.5741034 -2.8880363 -1.7180495 0.19292307 1.6430264 1.4013395 -0.29743528 -2.408083 -3.7817931 -3.7243054 -3.360261 -3.5111938][-3.7664456 -3.9850903 -4.1022067 -3.6975579 -2.9269848 -1.6470923 0.26700068 1.6767015 1.3270621 -0.53550982 -2.5946126 -3.7585318 -3.6681495 -3.41753 -3.5726414][-3.8871605 -4.1902032 -4.2628617 -3.8868809 -3.2313528 -2.1301577 -0.42415762 0.80769634 0.40430641 -1.3796041 -3.11271 -3.9256082 -3.8123677 -3.7183244 -3.93328][-3.7935612 -4.1062865 -4.1399903 -3.8345437 -3.3381977 -2.4811244 -1.1044805 -0.19902277 -0.67896748 -2.2211728 -3.5095651 -3.9541464 -3.8271759 -3.8040125 -4.0582566][-3.7180324 -3.9920349 -4.0704012 -3.8272934 -3.3997936 -2.7199826 -1.7096126 -1.2305989 -1.7495811 -2.9533286 -3.7957594 -3.974999 -3.8558195 -3.8466358 -4.1337433][-3.7102246 -3.9282162 -4.0895319 -3.9107933 -3.5906744 -3.178297 -2.559715 -2.3701472 -2.6915252 -3.394851 -3.8736205 -3.9996119 -3.999651 -4.0453982 -4.3325548][-3.4919567 -3.749054 -4.0709829 -4.0088696 -3.816751 -3.6773093 -3.3584452 -3.2370744 -3.1618204 -3.2623355 -3.4886632 -3.7483811 -4.0141864 -4.2351623 -4.523334][-2.8142819 -3.1714339 -3.7788043 -3.9726415 -3.9506533 -3.965023 -3.6993666 -3.4101255 -2.8866506 -2.4656725 -2.6262279 -3.2083344 -3.8700557 -4.3224139 -4.512835]]...]
INFO - root - 2017-12-07 04:49:10.625060: step 6810, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.773 sec/batch; 69h:54m:10s remains)
INFO - root - 2017-12-07 04:49:18.356290: step 6820, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.769 sec/batch; 69h:36m:22s remains)
INFO - root - 2017-12-07 04:49:25.929755: step 6830, loss = 0.98, batch loss = 0.90 (10.9 examples/sec; 0.736 sec/batch; 66h:34m:53s remains)
INFO - root - 2017-12-07 04:49:33.561930: step 6840, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 69h:26m:24s remains)
INFO - root - 2017-12-07 04:49:41.206274: step 6850, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.773 sec/batch; 69h:53m:11s remains)
INFO - root - 2017-12-07 04:49:48.848430: step 6860, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.744 sec/batch; 67h:19m:24s remains)
INFO - root - 2017-12-07 04:49:56.680573: step 6870, loss = 0.78, batch loss = 0.71 (9.7 examples/sec; 0.829 sec/batch; 74h:57m:11s remains)
INFO - root - 2017-12-07 04:50:04.022735: step 6880, loss = 0.82, batch loss = 0.74 (10.6 examples/sec; 0.755 sec/batch; 68h:15m:38s remains)
INFO - root - 2017-12-07 04:50:11.689978: step 6890, loss = 0.83, batch loss = 0.76 (10.8 examples/sec; 0.744 sec/batch; 67h:16m:06s remains)
INFO - root - 2017-12-07 04:50:19.299918: step 6900, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.758 sec/batch; 68h:30m:54s remains)
2017-12-07 04:50:19.866433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3218672 -2.8500319 -2.7058558 -2.7831974 -2.7433028 -2.5320287 -2.3928902 -2.6591542 -3.3280444 -3.8521698 -3.9075284 -3.5646877 -3.3229969 -3.1859727 -2.8740611][-3.3861494 -2.9223611 -2.8314469 -3.0456498 -3.097436 -2.8775904 -2.7312791 -3.0169456 -3.6321819 -4.0716991 -4.1718054 -3.9400041 -3.7993743 -3.6118097 -3.1292686][-3.6264923 -3.2663279 -3.2594326 -3.5930386 -3.7571373 -3.5090203 -3.1856155 -3.1664987 -3.4262524 -3.6593211 -3.9000587 -4.0955749 -4.3066592 -4.2206526 -3.6713328][-3.8553667 -3.607919 -3.6075535 -3.9592702 -4.1593575 -3.7862146 -3.0127759 -2.41343 -2.3534055 -2.5980434 -3.1782002 -3.9802 -4.6696153 -4.8370748 -4.4005284][-4.1013818 -3.9385872 -3.9126523 -4.2006369 -4.30923 -3.6397164 -2.2000847 -0.91115403 -0.75958657 -1.381196 -2.4278061 -3.721297 -4.7859168 -5.2485757 -5.0983238][-4.2227979 -4.0339394 -3.9038336 -4.0963187 -4.0598388 -3.0414658 -0.98251796 0.79726362 0.7711935 -0.45197344 -1.9346437 -3.403616 -4.5207114 -5.1457171 -5.2864161][-4.1536822 -3.8543992 -3.5477228 -3.550019 -3.3490295 -2.1540353 0.078654289 1.8565655 1.4000025 -0.34655094 -2.0033448 -3.2006862 -4.0369053 -4.6963263 -4.9805679][-4.0596752 -3.6273875 -3.1290212 -2.9730706 -2.737031 -1.7461069 0.045025826 1.2881427 0.56237078 -1.1721766 -2.4952927 -3.131994 -3.556293 -4.1745057 -4.545979][-3.9538026 -3.4798083 -2.9172368 -2.6835408 -2.4684148 -1.9032912 -0.93095922 -0.4203229 -1.1762373 -2.4218202 -3.1032391 -3.1708298 -3.2648103 -3.7625637 -4.1059957][-3.9289265 -3.5697312 -3.0976558 -2.7822685 -2.479111 -2.1928494 -1.8776078 -1.9341631 -2.6317518 -3.3142996 -3.4377751 -3.2178869 -3.2306757 -3.5988681 -3.7765043][-3.965764 -3.8709037 -3.6104751 -3.2245045 -2.794364 -2.6227131 -2.6783342 -3.0215726 -3.5581322 -3.7462935 -3.5157151 -3.2917948 -3.4121571 -3.7039027 -3.7144365][-3.8942621 -4.0607848 -4.0652084 -3.7497149 -3.3146026 -3.2508497 -3.5004945 -3.8851151 -4.1330266 -3.9646719 -3.6678576 -3.5903296 -3.8120039 -4.0559077 -4.0212584][-3.7036445 -4.0263019 -4.2102227 -4.0267816 -3.7543986 -3.8135142 -4.0796714 -4.2442017 -4.1383762 -3.8000956 -3.624372 -3.7326403 -4.0070195 -4.2373843 -4.2800803][-3.5312338 -3.917695 -4.1915455 -4.1995859 -4.2018623 -4.3532677 -4.4743948 -4.3310571 -3.9451573 -3.5870855 -3.5902631 -3.8299623 -4.089942 -4.2800894 -4.3751955][-3.490932 -3.8392367 -4.0715351 -4.2116451 -4.4231477 -4.5786638 -4.5057135 -4.195015 -3.7539711 -3.4862008 -3.6158218 -3.9420896 -4.2006879 -4.3661823 -4.4730263]]...]
INFO - root - 2017-12-07 04:50:27.462450: step 6910, loss = 0.79, batch loss = 0.72 (10.8 examples/sec; 0.742 sec/batch; 67h:04m:13s remains)
INFO - root - 2017-12-07 04:50:35.131379: step 6920, loss = 0.76, batch loss = 0.69 (10.8 examples/sec; 0.742 sec/batch; 67h:04m:15s remains)
INFO - root - 2017-12-07 04:50:42.802870: step 6930, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.765 sec/batch; 69h:12m:31s remains)
INFO - root - 2017-12-07 04:50:50.523211: step 6940, loss = 0.78, batch loss = 0.70 (10.2 examples/sec; 0.787 sec/batch; 71h:08m:11s remains)
INFO - root - 2017-12-07 04:50:58.179995: step 6950, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.776 sec/batch; 70h:10m:41s remains)
INFO - root - 2017-12-07 04:51:05.814523: step 6960, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.746 sec/batch; 67h:29m:22s remains)
INFO - root - 2017-12-07 04:51:13.404297: step 6970, loss = 0.92, batch loss = 0.85 (10.6 examples/sec; 0.754 sec/batch; 68h:12m:09s remains)
INFO - root - 2017-12-07 04:51:20.754311: step 6980, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.751 sec/batch; 67h:57m:05s remains)
INFO - root - 2017-12-07 04:51:28.353989: step 6990, loss = 0.67, batch loss = 0.60 (10.6 examples/sec; 0.757 sec/batch; 68h:25m:49s remains)
INFO - root - 2017-12-07 04:51:35.899132: step 7000, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.759 sec/batch; 68h:38m:15s remains)
2017-12-07 04:51:36.466452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.5392673 -0.84685493 -1.1031027 -1.0765181 -0.99709487 -0.94127512 -1.2289338 -1.9782662 -2.3092289 -2.2262127 -2.1344349 -2.0545983 -2.0794196 -1.9453626 -1.8063042][-0.81683421 -1.0252435 -1.303973 -1.4017787 -1.3719051 -1.3136239 -1.5355883 -2.0021057 -1.9128401 -1.6122711 -1.6933312 -1.9293625 -2.1272516 -2.0821671 -2.0001709][-0.9510448 -1.1036599 -1.4583094 -1.6708043 -1.644012 -1.491075 -1.4523859 -1.4674048 -1.0679407 -0.82038164 -1.2185192 -1.6531849 -1.8070705 -1.6880322 -1.5940139][-0.94656968 -0.99540997 -1.4123011 -1.7682598 -1.8205233 -1.6367328 -1.2760627 -0.71070313 -0.058589935 -0.074080944 -0.83899951 -1.3608224 -1.2669265 -0.97464275 -0.87412047][-1.0040343 -0.94080043 -1.3663507 -1.8263001 -1.9932463 -1.8798275 -1.3863568 -0.36026239 0.53186274 0.24827909 -0.82007837 -1.3993123 -1.0961525 -0.65677118 -0.50270534][-1.1222417 -0.89149714 -1.3014326 -1.9054196 -2.2828546 -2.3308811 -1.7770863 -0.36093855 0.78807783 0.30460453 -1.0441263 -1.7440865 -1.4184954 -0.91552973 -0.62713265][-0.96615195 -0.55938196 -1.0024462 -1.7906244 -2.4150474 -2.6422884 -2.0714352 -0.39841747 0.90605164 0.22434187 -1.3344119 -2.1713533 -1.9703386 -1.4600239 -1.0357902][-0.46878672 0.050422192 -0.4958744 -1.4764435 -2.3098249 -2.6505175 -2.0646746 -0.3548131 0.84510851 -0.072977543 -1.6603742 -2.4521279 -2.3431051 -1.8979714 -1.4345725][-0.032009125 0.41121483 -0.27997208 -1.3679681 -2.2542624 -2.5519319 -1.9449768 -0.46817851 0.39044905 -0.53664327 -1.79936 -2.28937 -2.1841486 -1.9224021 -1.6210315][0.17082119 0.27527142 -0.55278397 -1.5609505 -2.285274 -2.4462419 -1.9051497 -0.79432344 -0.18273354 -0.84157562 -1.6349523 -1.8406155 -1.7395887 -1.6671171 -1.5556538][0.10360861 -0.18507051 -0.99618816 -1.7265894 -2.1957142 -2.2969618 -1.9552162 -1.2156413 -0.7349205 -1.0432127 -1.4936838 -1.6259077 -1.5487773 -1.49705 -1.3525515][-0.21883631 -0.73878193 -1.3478065 -1.7213314 -1.9503987 -2.0827243 -1.9895802 -1.5877039 -1.2466581 -1.3155246 -1.6187539 -1.8110139 -1.707711 -1.5276167 -1.2652092][-0.67500591 -1.3332155 -1.7281637 -1.7881203 -1.7919962 -1.9292996 -2.0671883 -1.9663796 -1.7143915 -1.5990548 -1.8008409 -2.0499828 -1.904278 -1.5447922 -1.1458492][-1.0970798 -1.835237 -2.1364965 -1.9795055 -1.728754 -1.7651052 -2.0361986 -2.08033 -1.7873397 -1.5678327 -1.7292962 -2.0205934 -1.8839817 -1.4708867 -0.98781037][-1.3656797 -2.0675046 -2.3008397 -2.048301 -1.6185832 -1.5259671 -1.8200803 -1.8674133 -1.4777112 -1.2385838 -1.456656 -1.7292883 -1.5800722 -1.1679869 -0.67047644]]...]
INFO - root - 2017-12-07 04:51:44.230282: step 7010, loss = 1.03, batch loss = 0.96 (10.4 examples/sec; 0.769 sec/batch; 69h:34m:16s remains)
INFO - root - 2017-12-07 04:51:51.845357: step 7020, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.780 sec/batch; 70h:29m:19s remains)
INFO - root - 2017-12-07 04:51:59.481941: step 7030, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 69h:24m:42s remains)
INFO - root - 2017-12-07 04:52:07.090807: step 7040, loss = 0.68, batch loss = 0.61 (10.7 examples/sec; 0.751 sec/batch; 67h:51m:52s remains)
INFO - root - 2017-12-07 04:52:14.742100: step 7050, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.747 sec/batch; 67h:32m:19s remains)
INFO - root - 2017-12-07 04:52:22.387242: step 7060, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.776 sec/batch; 70h:07m:58s remains)
INFO - root - 2017-12-07 04:52:30.072157: step 7070, loss = 1.00, batch loss = 0.92 (10.4 examples/sec; 0.766 sec/batch; 69h:13m:35s remains)
INFO - root - 2017-12-07 04:52:37.559067: step 7080, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.777 sec/batch; 70h:11m:50s remains)
INFO - root - 2017-12-07 04:52:45.273931: step 7090, loss = 1.03, batch loss = 0.96 (10.3 examples/sec; 0.776 sec/batch; 70h:08m:54s remains)
INFO - root - 2017-12-07 04:52:52.925209: step 7100, loss = 1.01, batch loss = 0.93 (10.3 examples/sec; 0.776 sec/batch; 70h:10m:18s remains)
2017-12-07 04:52:53.529436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7852197 -2.5995121 -2.0115111 -1.706785 -1.5656159 -1.482476 -2.0390921 -2.3985715 -2.2134218 -2.4705307 -2.9550767 -3.0622172 -2.903656 -2.9188972 -3.0835559][-2.5485725 -2.5152621 -2.020211 -1.6946681 -1.3984406 -1.204886 -1.7336619 -2.1213267 -1.99737 -2.2382739 -2.66221 -2.7023363 -2.4873955 -2.5192471 -2.7658901][-2.3892174 -2.4837306 -2.0994833 -1.7467687 -1.3330352 -1.1590011 -1.7297153 -2.1723971 -2.1735222 -2.3890359 -2.6825948 -2.6311142 -2.3369157 -2.3545072 -2.66719][-2.1577711 -2.2799387 -2.0532455 -1.771426 -1.4132576 -1.401844 -1.9944692 -2.425694 -2.5686874 -2.8593719 -3.0743742 -2.9100266 -2.5195107 -2.4536309 -2.6601992][-1.9158049 -1.9997592 -1.9803183 -1.8068635 -1.5191221 -1.5324671 -1.8613563 -1.9903364 -2.1635425 -2.6745572 -3.094727 -3.0233507 -2.6913881 -2.6173143 -2.6986156][-1.7900815 -1.8446145 -1.945673 -1.7814722 -1.4292877 -1.2478294 -1.0454428 -0.63173842 -0.72177792 -1.6111526 -2.5338316 -2.8177142 -2.7966204 -2.899534 -2.970295][-1.8880816 -1.8750515 -1.9167349 -1.7204225 -1.3567102 -1.0130587 -0.30659437 0.6649437 0.78335381 -0.39169168 -1.7684991 -2.4328067 -2.8060155 -3.19309 -3.4201918][-2.1149166 -2.0882671 -1.9782021 -1.7790713 -1.5718424 -1.3216803 -0.51483321 0.72278595 1.1839194 0.20248175 -1.1385472 -1.9474764 -2.603915 -3.2074285 -3.7018809][-2.1505942 -2.2217307 -2.0736825 -1.9686775 -2.0275187 -2.160162 -1.7237885 -0.58358622 0.17632914 -0.14858961 -0.79533172 -1.3033369 -1.9772553 -2.6582918 -3.4651055][-2.0754311 -2.1699688 -1.9906194 -1.9795179 -2.3016551 -2.9124851 -2.9819129 -2.0476356 -1.1084042 -0.75296164 -0.49831653 -0.44651794 -0.886734 -1.5418909 -2.5994406][-1.8388827 -1.8687129 -1.6826465 -1.6836507 -2.0788445 -2.9329927 -3.3873897 -2.8447595 -2.196779 -1.6207845 -0.81068492 -0.23917103 -0.17417908 -0.49957657 -1.4289238][-1.2939579 -1.1274011 -0.98218536 -1.0276399 -1.3655338 -2.1341405 -2.6879046 -2.6433635 -2.655395 -2.3732493 -1.579282 -0.82891154 -0.35246658 -0.24079275 -0.71756268][-0.75433779 -0.45240259 -0.42365456 -0.5502522 -0.80591226 -1.3284171 -1.736861 -2.0350013 -2.5906048 -2.6691906 -2.1290789 -1.4636354 -0.79900074 -0.46437955 -0.60780954][-0.89219332 -0.66634989 -0.73808503 -0.87136269 -1.0112088 -1.1640615 -1.1647048 -1.3913782 -2.0553954 -2.3582644 -2.1758466 -1.7694671 -1.1289387 -0.76712561 -0.798908][-1.9234183 -1.8163028 -1.831084 -1.7966335 -1.7446654 -1.5392449 -1.0976784 -0.99701452 -1.3618686 -1.6032569 -1.7043836 -1.7109678 -1.3980868 -1.2418997 -1.2969885]]...]
INFO - root - 2017-12-07 04:53:01.193156: step 7110, loss = 1.14, batch loss = 1.07 (10.7 examples/sec; 0.745 sec/batch; 67h:18m:46s remains)
INFO - root - 2017-12-07 04:53:08.902583: step 7120, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.777 sec/batch; 70h:12m:38s remains)
INFO - root - 2017-12-07 04:53:16.621646: step 7130, loss = 0.88, batch loss = 0.81 (10.2 examples/sec; 0.785 sec/batch; 70h:56m:41s remains)
INFO - root - 2017-12-07 04:53:24.219899: step 7140, loss = 0.73, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 69h:44m:05s remains)
INFO - root - 2017-12-07 04:53:31.869266: step 7150, loss = 0.71, batch loss = 0.64 (10.7 examples/sec; 0.746 sec/batch; 67h:25m:35s remains)
INFO - root - 2017-12-07 04:53:39.424976: step 7160, loss = 0.89, batch loss = 0.81 (10.6 examples/sec; 0.752 sec/batch; 67h:57m:36s remains)
INFO - root - 2017-12-07 04:53:47.067928: step 7170, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.745 sec/batch; 67h:19m:09s remains)
INFO - root - 2017-12-07 04:53:54.502542: step 7180, loss = 0.66, batch loss = 0.59 (10.5 examples/sec; 0.763 sec/batch; 68h:57m:47s remains)
INFO - root - 2017-12-07 04:54:02.091892: step 7190, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.751 sec/batch; 67h:53m:35s remains)
INFO - root - 2017-12-07 04:54:09.735071: step 7200, loss = 0.75, batch loss = 0.67 (10.5 examples/sec; 0.762 sec/batch; 68h:53m:36s remains)
2017-12-07 04:54:10.339406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5423748 -2.026545 -1.3960826 -0.9284246 -0.82828879 -1.0973034 -1.4580452 -1.602217 -1.7014031 -1.8013773 -1.7885733 -1.6766565 -1.7166228 -1.9289613 -1.7315536][-2.72797 -2.3607109 -1.8258009 -1.3104982 -1.1207206 -1.2305574 -1.4261863 -1.5276184 -1.6914289 -1.8562443 -1.7568953 -1.4630969 -1.3476636 -1.4947131 -1.3587763][-2.9276953 -2.7743883 -2.3255403 -1.7146182 -1.3717976 -1.3028142 -1.3967419 -1.511554 -1.7376156 -1.971698 -1.8658109 -1.5367293 -1.321064 -1.2941325 -1.1372406][-2.8776407 -2.99235 -2.704179 -2.0366914 -1.5249534 -1.2677717 -1.3336213 -1.5581081 -1.8849871 -2.1845207 -2.0898588 -1.8082807 -1.6007757 -1.404361 -1.1509202][-2.5213132 -2.8545585 -2.8227265 -2.2537277 -1.6302795 -1.0967166 -0.9672873 -1.2012019 -1.6809139 -2.2106404 -2.3267334 -2.2462738 -2.1314194 -1.8296764 -1.5108054][-2.2541404 -2.5857234 -2.7034955 -2.3111649 -1.7127082 -0.90062475 -0.27619743 -0.14637852 -0.62756634 -1.4459672 -1.9377174 -2.2017334 -2.2928014 -2.0623698 -1.8370965][-2.2174504 -2.4738629 -2.6104827 -2.378475 -1.949265 -1.0330493 0.12215328 0.83360338 0.60980034 -0.21558285 -0.93546653 -1.5648181 -1.9805939 -2.0142953 -2.023335][-2.3225257 -2.5461822 -2.6415443 -2.5102944 -2.2735021 -1.4924774 -0.21435738 0.76786423 0.93604565 0.60717058 0.087922573 -0.697983 -1.3825405 -1.7154255 -1.9513571][-2.3892353 -2.6046629 -2.670311 -2.6336007 -2.5521436 -2.0616252 -1.1748524 -0.43061781 0.066348076 0.45816898 0.45633888 -0.11141157 -0.72310472 -1.1351008 -1.491425][-2.3378086 -2.5949345 -2.7269979 -2.7911358 -2.764122 -2.5138452 -2.1673677 -1.8230152 -1.1480708 -0.3025732 0.025036812 -0.23855686 -0.54390097 -0.81799793 -1.1027207][-2.4158559 -2.6459584 -2.7823272 -2.8564539 -2.7933009 -2.7152274 -2.8296094 -2.7952948 -2.1109588 -1.2551672 -0.95335317 -1.0294461 -1.0272317 -1.0632918 -1.0627501][-2.5478272 -2.6643693 -2.7263331 -2.7135074 -2.5873411 -2.6776953 -3.1217608 -3.2434978 -2.689266 -2.1280496 -2.0102167 -1.9810271 -1.8170626 -1.7195444 -1.4466317][-2.4011695 -2.5063977 -2.5552766 -2.4697824 -2.3154109 -2.5407813 -3.1140184 -3.2751925 -2.9288282 -2.6818423 -2.6192169 -2.4903767 -2.3704133 -2.3149846 -1.9274795][-2.0727081 -2.1556818 -2.22316 -2.1380267 -2.0280793 -2.3283982 -2.8796272 -3.0636106 -2.9928331 -2.995172 -2.8688622 -2.643508 -2.6091366 -2.5910475 -2.1548648][-1.6697578 -1.7069783 -1.8311288 -1.8338208 -1.8208034 -2.1217682 -2.5194297 -2.657197 -2.7693641 -2.9433017 -2.8619068 -2.6876998 -2.6620317 -2.5504956 -2.1359081]]...]
INFO - root - 2017-12-07 04:54:17.962574: step 7210, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.768 sec/batch; 69h:21m:27s remains)
INFO - root - 2017-12-07 04:54:25.640048: step 7220, loss = 0.97, batch loss = 0.90 (10.4 examples/sec; 0.769 sec/batch; 69h:30m:39s remains)
INFO - root - 2017-12-07 04:54:33.238364: step 7230, loss = 0.92, batch loss = 0.84 (10.5 examples/sec; 0.759 sec/batch; 68h:32m:09s remains)
INFO - root - 2017-12-07 04:54:40.984970: step 7240, loss = 0.61, batch loss = 0.54 (10.6 examples/sec; 0.754 sec/batch; 68h:08m:32s remains)
INFO - root - 2017-12-07 04:54:48.638068: step 7250, loss = 0.70, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 70h:47m:51s remains)
INFO - root - 2017-12-07 04:54:56.337459: step 7260, loss = 0.78, batch loss = 0.70 (10.2 examples/sec; 0.781 sec/batch; 70h:33m:15s remains)
INFO - root - 2017-12-07 04:55:04.063049: step 7270, loss = 0.84, batch loss = 0.76 (10.4 examples/sec; 0.767 sec/batch; 69h:18m:58s remains)
INFO - root - 2017-12-07 04:55:11.588034: step 7280, loss = 0.83, batch loss = 0.75 (10.5 examples/sec; 0.764 sec/batch; 69h:00m:45s remains)
INFO - root - 2017-12-07 04:55:19.283907: step 7290, loss = 0.90, batch loss = 0.83 (10.7 examples/sec; 0.746 sec/batch; 67h:23m:05s remains)
INFO - root - 2017-12-07 04:55:26.911367: step 7300, loss = 0.86, batch loss = 0.78 (10.3 examples/sec; 0.777 sec/batch; 70h:11m:57s remains)
2017-12-07 04:55:27.515373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9034455 -3.9766612 -4.0216069 -4.0735388 -4.1505532 -4.1879826 -4.1961255 -4.2468681 -4.2495079 -4.1496911 -4.014627 -3.7608137 -3.5985096 -3.6366034 -3.8143444][-3.7365131 -3.818826 -3.7917769 -3.7390063 -3.738481 -3.7307589 -3.7790155 -3.9631715 -4.0770769 -4.0507469 -3.907918 -3.4953141 -3.1973467 -3.280684 -3.6020851][-3.3995705 -3.4411697 -3.3289742 -3.2054062 -3.143291 -3.0853086 -3.1470218 -3.3720584 -3.4946361 -3.563678 -3.504715 -3.0793023 -2.7950859 -3.0126193 -3.460906][-3.2242284 -3.0992439 -2.6976261 -2.4370036 -2.4386923 -2.5096893 -2.6292248 -2.7249148 -2.6892986 -2.8187675 -2.9572561 -2.7387676 -2.7095428 -3.1039691 -3.5076323][-3.2401533 -3.0345027 -2.307574 -1.8061011 -1.8343611 -1.9792547 -2.0107307 -1.8783243 -1.7777252 -2.0670877 -2.4565732 -2.5263374 -2.8334451 -3.3635321 -3.5413277][-3.0513587 -2.8888545 -2.0976446 -1.5173414 -1.4721341 -1.2963307 -0.83682442 -0.4577024 -0.59512067 -1.2764006 -1.9453366 -2.1924217 -2.6667714 -3.2297149 -3.2555225][-2.7491188 -2.6574926 -2.0120022 -1.4820647 -1.1589537 -0.27897263 0.90653276 1.3487978 0.55048227 -0.7701695 -1.7310114 -2.0269182 -2.3550763 -2.7159431 -2.6711791][-2.537324 -2.5218358 -2.0871737 -1.7079899 -1.1505108 0.29952812 1.9981337 2.4120479 1.1457944 -0.52123117 -1.6912944 -2.0853348 -2.2695298 -2.4418473 -2.3990552][-2.5326846 -2.5949574 -2.3005657 -2.125164 -1.7152689 -0.36059713 1.14146 1.4953475 0.6844883 -0.46961761 -1.4708948 -1.9876177 -2.3470376 -2.7097869 -2.8309202][-2.6050558 -2.6604104 -2.4206531 -2.3844061 -2.2604136 -1.4634221 -0.71106744 -0.58399105 -0.68285036 -0.87427568 -1.3129203 -1.6579092 -2.1429498 -2.8362455 -3.2581439][-2.5231514 -2.5474396 -2.3758578 -2.3890948 -2.429112 -2.1190188 -2.0682311 -2.2858119 -2.0770247 -1.7064402 -1.6758146 -1.5685513 -1.835336 -2.6345756 -3.2781312][-2.2806661 -2.3416712 -2.2729728 -2.2636282 -2.3042102 -2.221267 -2.4965694 -2.9165764 -2.7584472 -2.4070024 -2.314738 -1.9090197 -1.9147513 -2.6327639 -3.2884674][-2.1471286 -2.2986727 -2.2725129 -2.1459565 -2.1261921 -2.2337186 -2.6347647 -3.0344696 -2.9209027 -2.7089615 -2.7076635 -2.3648431 -2.383769 -3.0500991 -3.5841148][-2.4258418 -2.6402867 -2.5016108 -2.1425405 -2.0246696 -2.2668424 -2.7713175 -3.1380696 -3.0680523 -2.8965111 -2.8636513 -2.6972275 -2.94286 -3.622035 -3.9567523][-3.0255449 -3.1921563 -2.9227386 -2.4461334 -2.2551742 -2.4333038 -2.8339958 -3.1526322 -3.1971676 -3.1141369 -3.0658259 -3.035605 -3.3990707 -3.9582958 -4.0500617]]...]
INFO - root - 2017-12-07 04:55:35.263129: step 7310, loss = 0.92, batch loss = 0.84 (10.0 examples/sec; 0.803 sec/batch; 72h:31m:52s remains)
INFO - root - 2017-12-07 04:55:42.905906: step 7320, loss = 0.78, batch loss = 0.71 (10.1 examples/sec; 0.794 sec/batch; 71h:42m:14s remains)
INFO - root - 2017-12-07 04:55:50.540489: step 7330, loss = 0.75, batch loss = 0.68 (10.2 examples/sec; 0.784 sec/batch; 70h:49m:49s remains)
INFO - root - 2017-12-07 04:55:58.223741: step 7340, loss = 0.98, batch loss = 0.91 (10.5 examples/sec; 0.762 sec/batch; 68h:47m:50s remains)
INFO - root - 2017-12-07 04:56:05.863316: step 7350, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.785 sec/batch; 70h:51m:29s remains)
INFO - root - 2017-12-07 04:56:13.543961: step 7360, loss = 0.94, batch loss = 0.87 (10.3 examples/sec; 0.773 sec/batch; 69h:49m:06s remains)
INFO - root - 2017-12-07 04:56:21.160518: step 7370, loss = 0.62, batch loss = 0.55 (10.3 examples/sec; 0.774 sec/batch; 69h:53m:16s remains)
INFO - root - 2017-12-07 04:56:28.669221: step 7380, loss = 0.73, batch loss = 0.66 (9.9 examples/sec; 0.809 sec/batch; 73h:01m:27s remains)
INFO - root - 2017-12-07 04:56:36.376344: step 7390, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.762 sec/batch; 68h:48m:21s remains)
INFO - root - 2017-12-07 04:56:44.016968: step 7400, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.779 sec/batch; 70h:18m:46s remains)
2017-12-07 04:56:44.587294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6815414 -3.6939969 -3.7476592 -3.7451997 -3.6579466 -3.3933268 -3.1336215 -3.0477953 -2.9671266 -2.7873964 -2.7286582 -2.9222598 -3.1070175 -3.2388592 -3.3765037][-3.638567 -3.6029029 -3.5816183 -3.5198216 -3.4397728 -3.2566903 -3.1134915 -3.1372004 -3.1677146 -3.0948734 -3.0764761 -3.1223965 -3.0425875 -2.861918 -2.7497048][-3.5985126 -3.5208991 -3.4256263 -3.3359883 -3.3135905 -3.2832539 -3.288105 -3.4435084 -3.6285987 -3.7624984 -3.8389535 -3.759799 -3.4051473 -2.8804982 -2.5000505][-3.5864782 -3.5043206 -3.372057 -3.2651567 -3.2746005 -3.3363304 -3.4016633 -3.5938761 -3.858696 -4.1524706 -4.3156404 -4.205678 -3.8082154 -3.240953 -2.8341343][-3.6200163 -3.5516984 -3.4035714 -3.2605147 -3.2090738 -3.205637 -3.1826515 -3.29246 -3.5315099 -3.8841481 -4.0862255 -4.01276 -3.735878 -3.3528833 -3.1472297][-3.7034597 -3.6279044 -3.4590094 -3.2546983 -3.091325 -2.9160039 -2.6729884 -2.5857077 -2.6692483 -2.9298668 -3.1434565 -3.1645863 -3.0741136 -2.958128 -3.0188065][-3.7790458 -3.6656458 -3.4722331 -3.2227781 -2.9585114 -2.6143403 -2.1640146 -1.8906271 -1.8098567 -1.9220755 -2.1746447 -2.3887498 -2.5254722 -2.6512666 -2.8636613][-3.7925496 -3.6435604 -3.4382393 -3.1704381 -2.833777 -2.4076223 -1.8817821 -1.5525842 -1.3906004 -1.4017794 -1.7115514 -2.0834944 -2.3831184 -2.6311045 -2.8428233][-3.7658658 -3.6273668 -3.467654 -3.2434068 -2.8991694 -2.4944739 -2.0503197 -1.8099005 -1.6703246 -1.6398015 -1.9385872 -2.2785611 -2.5534286 -2.7885437 -2.9179306][-3.6982396 -3.5824244 -3.493063 -3.3749084 -3.1443181 -2.8813939 -2.6174479 -2.5350533 -2.4850802 -2.4497023 -2.6278775 -2.7914808 -2.9364738 -3.1422977 -3.2422509][-3.608705 -3.4904232 -3.4780881 -3.4749415 -3.441556 -3.4166572 -3.4103169 -3.4929013 -3.4979284 -3.3716645 -3.2887092 -3.1678827 -3.13794 -3.2850571 -3.350872][-3.5518064 -3.4158864 -3.4305048 -3.4508033 -3.4968598 -3.5691924 -3.691072 -3.8693371 -3.863503 -3.6195881 -3.3485122 -3.0829756 -2.9687738 -3.0588806 -3.0908136][-3.5610359 -3.4240475 -3.4184208 -3.3786242 -3.3721771 -3.3846722 -3.4803138 -3.6776147 -3.6695211 -3.3986416 -3.1321974 -2.9371824 -2.8319371 -2.8411021 -2.7946706][-3.6276886 -3.543292 -3.5249369 -3.4526711 -3.3824816 -3.288063 -3.2616062 -3.4025736 -3.4071918 -3.2082767 -3.0920706 -3.0799384 -3.0681925 -3.0215755 -2.8726139][-3.6933298 -3.721921 -3.7761819 -3.775317 -3.7339859 -3.608979 -3.4853234 -3.5160036 -3.5046141 -3.4009078 -3.4068947 -3.5372791 -3.619498 -3.5605335 -3.3375504]]...]
INFO - root - 2017-12-07 04:56:52.239209: step 7410, loss = 0.90, batch loss = 0.83 (10.4 examples/sec; 0.769 sec/batch; 69h:25m:30s remains)
INFO - root - 2017-12-07 04:56:59.941607: step 7420, loss = 0.63, batch loss = 0.56 (10.2 examples/sec; 0.781 sec/batch; 70h:30m:41s remains)
INFO - root - 2017-12-07 04:57:07.545811: step 7430, loss = 0.91, batch loss = 0.83 (10.3 examples/sec; 0.777 sec/batch; 70h:11m:32s remains)
INFO - root - 2017-12-07 04:57:15.142997: step 7440, loss = 0.87, batch loss = 0.80 (10.7 examples/sec; 0.749 sec/batch; 67h:35m:20s remains)
INFO - root - 2017-12-07 04:57:22.919928: step 7450, loss = 0.88, batch loss = 0.80 (10.2 examples/sec; 0.782 sec/batch; 70h:36m:59s remains)
INFO - root - 2017-12-07 04:57:30.637906: step 7460, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.771 sec/batch; 69h:36m:48s remains)
INFO - root - 2017-12-07 04:57:38.399618: step 7470, loss = 0.84, batch loss = 0.77 (9.9 examples/sec; 0.808 sec/batch; 72h:59m:43s remains)
INFO - root - 2017-12-07 04:57:45.818752: step 7480, loss = 0.77, batch loss = 0.69 (10.3 examples/sec; 0.774 sec/batch; 69h:53m:24s remains)
INFO - root - 2017-12-07 04:57:53.532346: step 7490, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 70h:24m:16s remains)
INFO - root - 2017-12-07 04:58:01.206996: step 7500, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 71h:51m:34s remains)
2017-12-07 04:58:01.864965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8111651 -3.1579142 -3.6534617 -3.5286231 -2.5923815 -1.49648 -0.70513153 -1.0424068 -2.4919407 -3.9849572 -4.3595018 -4.1048069 -3.9086533 -2.9402952 -1.7199404][-2.2485857 -2.7199123 -3.2880645 -2.9850464 -1.6553812 -0.1117425 0.9909687 0.54055119 -1.3296573 -3.4889309 -4.58127 -4.5931125 -4.24777 -3.1196117 -1.7291331][-1.3026295 -1.7748916 -2.4703574 -2.3343604 -1.1356001 0.42882395 1.6742573 1.4443536 -0.26036692 -2.6715565 -4.2970576 -4.5498724 -4.15047 -3.1392741 -1.7926502][-0.39262295 -0.9194026 -1.943886 -2.3913729 -1.8387103 -0.64450741 0.7379055 1.2158527 0.4023447 -1.6970248 -3.4804184 -3.7856665 -3.3994212 -2.7668264 -1.8449528][-0.22145271 -0.65558171 -1.8020415 -2.7383046 -2.8488817 -2.0671768 -0.6177361 0.59485626 0.72415113 -0.89845324 -2.5745087 -2.7391896 -2.3342636 -2.0381439 -1.7203612][-0.59842467 -0.63865566 -1.5386677 -2.5965433 -2.9900913 -2.4159689 -0.99363971 0.5949378 1.1483464 -0.37298584 -1.9628665 -1.9524431 -1.3918142 -1.0179353 -0.90017295][-0.66488576 -0.27166224 -0.94190788 -2.0844266 -2.4769073 -1.8707762 -0.42727804 1.32482 1.7995939 -0.0167737 -1.6777971 -1.6383216 -0.91351271 -0.17928457 0.19934654][-0.27546835 0.29656029 -0.39547348 -1.7814019 -2.1583037 -1.4057908 0.15516996 2.0354152 2.2093358 -0.094407558 -1.9872954 -2.0732064 -1.2829888 -0.19187403 0.62387371][0.072825909 0.61400986 -0.21726465 -1.8795991 -2.2547646 -1.3924587 0.23884964 2.1234727 2.0181761 -0.47319913 -2.4139321 -2.5885077 -1.8913426 -0.83182549 0.034756184][-0.10363293 0.56022882 -0.1557169 -1.8925908 -2.3259788 -1.5894387 -0.21595716 1.3630657 1.1246977 -0.92884779 -2.3909521 -2.3727055 -1.7442853 -1.0684803 -0.64887285][-0.44171023 0.48995781 0.14496946 -1.4067547 -1.9761708 -1.6454773 -0.82700229 0.25322723 0.0594306 -1.2198758 -1.9541931 -1.6477957 -1.1066227 -0.94856024 -1.1324046][-0.73431706 0.22031784 0.11846447 -1.1531234 -1.8387849 -1.8598514 -1.4477835 -0.72268915 -0.78891611 -1.4726017 -1.7888727 -1.3987234 -0.96311545 -1.1766486 -1.7644894][-1.3869548 -0.70341015 -0.7500999 -1.6400926 -2.2257938 -2.3183944 -2.0315702 -1.5539598 -1.6134484 -2.1046448 -2.3886366 -2.071166 -1.6377316 -1.8702683 -2.4800677][-2.5162454 -2.1198502 -2.1181717 -2.5735152 -2.8732467 -2.8417907 -2.5880005 -2.3110516 -2.3829041 -2.8104577 -3.2149093 -3.1159105 -2.7411838 -2.8353462 -3.202013][-3.384335 -3.1532257 -3.1175718 -3.2750416 -3.3618574 -3.2544787 -3.0471506 -2.8996518 -2.8913233 -3.1377523 -3.5277429 -3.6143801 -3.4022849 -3.4475217 -3.6419611]]...]
INFO - root - 2017-12-07 04:58:09.586388: step 7510, loss = 0.86, batch loss = 0.79 (10.1 examples/sec; 0.790 sec/batch; 71h:18m:08s remains)
INFO - root - 2017-12-07 04:58:17.296978: step 7520, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.758 sec/batch; 68h:26m:27s remains)
INFO - root - 2017-12-07 04:58:24.940662: step 7530, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.783 sec/batch; 70h:40m:48s remains)
INFO - root - 2017-12-07 04:58:32.609759: step 7540, loss = 1.00, batch loss = 0.93 (10.3 examples/sec; 0.775 sec/batch; 69h:57m:50s remains)
INFO - root - 2017-12-07 04:58:40.248584: step 7550, loss = 0.97, batch loss = 0.90 (10.5 examples/sec; 0.765 sec/batch; 69h:01m:37s remains)
INFO - root - 2017-12-07 04:58:47.943791: step 7560, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.773 sec/batch; 69h:43m:59s remains)
INFO - root - 2017-12-07 04:58:55.697120: step 7570, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.754 sec/batch; 68h:03m:44s remains)
INFO - root - 2017-12-07 04:59:03.084322: step 7580, loss = 0.78, batch loss = 0.71 (10.1 examples/sec; 0.792 sec/batch; 71h:30m:15s remains)
INFO - root - 2017-12-07 04:59:10.836476: step 7590, loss = 0.73, batch loss = 0.66 (9.6 examples/sec; 0.831 sec/batch; 75h:01m:49s remains)
INFO - root - 2017-12-07 04:59:18.586941: step 7600, loss = 0.61, batch loss = 0.54 (10.5 examples/sec; 0.763 sec/batch; 68h:50m:08s remains)
2017-12-07 04:59:19.224083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8180511 -2.3630121 -1.6645324 -1.4346418 -1.8624051 -2.6397161 -3.2375269 -3.4921627 -3.5990534 -3.3961439 -3.084558 -2.901417 -2.7637582 -2.7311997 -2.4874387][-3.1953356 -2.6539323 -1.8208287 -1.4942205 -1.8499055 -2.4916635 -2.8765359 -2.9583204 -3.1109042 -3.1612394 -3.10295 -3.0185456 -2.9000096 -2.857058 -2.5080395][-3.3801508 -2.7567601 -1.8935268 -1.4954441 -1.724818 -2.1453524 -2.3269813 -2.3627198 -2.6660564 -2.9945962 -3.1387498 -3.1266189 -3.0750027 -3.090775 -2.6563544][-3.3490241 -2.8068779 -2.0749321 -1.6173956 -1.5778759 -1.641571 -1.5641668 -1.5023987 -1.8266296 -2.3229058 -2.6844902 -2.8850737 -3.0522392 -3.1877036 -2.7396369][-3.2885675 -3.0507822 -2.6657057 -2.2895873 -2.082381 -1.8410962 -1.4752116 -1.1485639 -1.2029395 -1.5703523 -1.9761531 -2.2856867 -2.5723968 -2.748291 -2.3908315][-3.3084636 -3.250071 -3.1174045 -2.9758255 -2.9712477 -2.8500564 -2.4924393 -2.0058753 -1.6765969 -1.5888083 -1.6475403 -1.7842317 -2.0282798 -2.1617835 -1.9176679][-3.4627421 -3.2564876 -2.99062 -2.8736496 -3.0121527 -3.0737128 -2.896832 -2.5151365 -2.0565083 -1.6341736 -1.3799284 -1.4034905 -1.7014244 -1.9029627 -1.7945769][-3.5584254 -3.1598439 -2.7025795 -2.4303803 -2.4081612 -2.3928454 -2.3177106 -2.1811931 -1.9335222 -1.5730519 -1.311697 -1.3822169 -1.7745214 -2.0900021 -2.0816736][-3.5169988 -3.1852016 -2.8342881 -2.5506821 -2.3059964 -2.0170178 -1.8254426 -1.7341661 -1.6210792 -1.4100213 -1.2991376 -1.4800537 -1.8965614 -2.23052 -2.2588844][-3.4192402 -3.279664 -3.1906576 -3.0289412 -2.6884449 -2.2339354 -1.9494078 -1.773366 -1.5845833 -1.3923635 -1.3247414 -1.4792101 -1.8238771 -2.1405272 -2.2519028][-3.5131459 -3.4584935 -3.4374039 -3.2977223 -2.9283223 -2.4981391 -2.2724948 -2.0923569 -1.8629129 -1.6698532 -1.5417275 -1.5466483 -1.7451575 -1.9661934 -2.0660448][-3.5551329 -3.4605491 -3.3749461 -3.1936691 -2.7897 -2.3938022 -2.2573111 -2.2053721 -2.1205769 -2.020036 -1.8410141 -1.6845453 -1.7195461 -1.8077433 -1.8157337][-3.4571888 -3.3038468 -3.1756985 -3.0075722 -2.6669941 -2.3581731 -2.3122325 -2.4061036 -2.5007932 -2.5194926 -2.379581 -2.1891694 -2.1133239 -2.0560422 -1.9310911][-3.340682 -3.13414 -2.9575524 -2.8046932 -2.5733843 -2.3797758 -2.3911707 -2.5413332 -2.695066 -2.7456267 -2.6790481 -2.6000743 -2.5743575 -2.5194063 -2.3821454][-3.1511791 -2.9165552 -2.7013235 -2.5487144 -2.4052086 -2.3086977 -2.3328235 -2.4691415 -2.6215091 -2.6776814 -2.6689956 -2.6747751 -2.7106128 -2.7314539 -2.7032561]]...]
INFO - root - 2017-12-07 04:59:26.897898: step 7610, loss = 0.73, batch loss = 0.66 (10.8 examples/sec; 0.739 sec/batch; 66h:42m:49s remains)
INFO - root - 2017-12-07 04:59:34.461754: step 7620, loss = 0.80, batch loss = 0.72 (10.4 examples/sec; 0.772 sec/batch; 69h:38m:08s remains)
INFO - root - 2017-12-07 04:59:41.963737: step 7630, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.774 sec/batch; 69h:52m:05s remains)
INFO - root - 2017-12-07 04:59:49.617232: step 7640, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 69h:38m:52s remains)
INFO - root - 2017-12-07 04:59:57.297350: step 7650, loss = 0.78, batch loss = 0.71 (10.1 examples/sec; 0.790 sec/batch; 71h:14m:41s remains)
INFO - root - 2017-12-07 05:00:04.990842: step 7660, loss = 0.68, batch loss = 0.61 (9.8 examples/sec; 0.818 sec/batch; 73h:46m:15s remains)
INFO - root - 2017-12-07 05:00:12.750274: step 7670, loss = 0.80, batch loss = 0.72 (10.6 examples/sec; 0.755 sec/batch; 68h:07m:45s remains)
INFO - root - 2017-12-07 05:00:20.299710: step 7680, loss = 0.73, batch loss = 0.66 (9.8 examples/sec; 0.817 sec/batch; 73h:45m:04s remains)
INFO - root - 2017-12-07 05:00:27.885774: step 7690, loss = 0.74, batch loss = 0.67 (10.9 examples/sec; 0.734 sec/batch; 66h:12m:22s remains)
INFO - root - 2017-12-07 05:00:35.552701: step 7700, loss = 0.81, batch loss = 0.73 (10.3 examples/sec; 0.773 sec/batch; 69h:45m:36s remains)
2017-12-07 05:00:36.212094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0461936 -3.1489272 -2.8217678 -2.3064842 -1.6897702 -1.4003944 -1.7324431 -2.2016747 -2.8189943 -3.017312 -2.6058826 -2.3080242 -2.2154541 -2.0707595 -1.9992161][-2.8828604 -3.0817089 -2.6527319 -1.9201646 -1.0998909 -0.77258515 -1.2038825 -1.8469577 -2.6770267 -3.0684364 -2.8104196 -2.6281307 -2.569365 -2.4209459 -2.3978696][-2.5522218 -2.7203889 -2.2631433 -1.5554202 -0.87814331 -0.72776008 -1.2050245 -1.8574719 -2.6841297 -3.1568513 -3.0939937 -3.1196585 -3.1542449 -3.0174563 -3.0495076][-2.5325277 -2.5281463 -2.0791824 -1.5893826 -1.2669339 -1.2665858 -1.5380597 -1.879159 -2.3759532 -2.6984143 -2.7556255 -3.0503409 -3.2969916 -3.2284474 -3.2411394][-2.6538875 -2.4674339 -2.0499249 -1.8399444 -1.8344841 -1.8212571 -1.7226934 -1.6238008 -1.7364752 -1.7833829 -1.7735014 -2.2176027 -2.7030287 -2.8079574 -2.8592525][-2.5113106 -2.2328017 -1.8541954 -1.8263667 -1.8915985 -1.7324147 -1.3585389 -1.0807743 -1.109071 -0.9721086 -0.708843 -0.97448635 -1.4555836 -1.726804 -1.9599302][-2.5438733 -2.2541087 -1.8748775 -1.7486501 -1.5661683 -1.1536331 -0.6328094 -0.3870616 -0.48549461 -0.11388874 0.49913836 0.53079414 0.07827282 -0.53754067 -1.2271261][-3.0455217 -2.8730283 -2.4093583 -1.9237249 -1.3454223 -0.767956 -0.30586243 -0.23156738 -0.35161114 0.32085609 1.2892332 1.5791063 1.1129684 0.082201004 -1.1518884][-3.4446716 -3.4143815 -2.8110745 -1.997545 -1.2513359 -0.89969826 -0.85523653 -1.0711505 -1.1681759 -0.34749556 0.69223738 1.1097364 0.83256531 -0.16440058 -1.4527767][-3.6000445 -3.6051147 -2.8805747 -1.9457245 -1.3049269 -1.3911233 -1.8827581 -2.435276 -2.5989923 -2.0137634 -1.3451173 -1.0320277 -0.97346616 -1.3711433 -1.9808714][-3.4959478 -3.5299487 -2.9193568 -2.1761546 -1.7464485 -2.0230911 -2.6947711 -3.3064647 -3.4718208 -3.204 -2.987937 -2.8886163 -2.6668525 -2.5876131 -2.5186906][-3.2138083 -3.4407668 -3.2601433 -2.9833417 -2.7652025 -2.9315596 -3.3706238 -3.7230074 -3.7368708 -3.5649421 -3.4967463 -3.4093528 -3.1048563 -2.8578186 -2.5390627][-2.6974325 -3.219955 -3.5378041 -3.70818 -3.6189167 -3.55689 -3.6460011 -3.6567445 -3.4723654 -3.2679687 -3.1617484 -2.9920287 -2.6558771 -2.3821645 -2.0724256][-2.1057599 -2.8278418 -3.4002113 -3.7301095 -3.6355283 -3.4367785 -3.3714859 -3.2152891 -2.918725 -2.7062578 -2.5725114 -2.3785756 -2.0841355 -1.836973 -1.587441][-1.8258479 -2.6554575 -3.2924168 -3.5582857 -3.3518467 -3.0461881 -2.9036603 -2.6711709 -2.3196306 -2.0912433 -1.9396179 -1.7790852 -1.5799296 -1.408066 -1.2572827]]...]
INFO - root - 2017-12-07 05:00:43.855426: step 7710, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.762 sec/batch; 68h:43m:52s remains)
INFO - root - 2017-12-07 05:00:51.459415: step 7720, loss = 0.65, batch loss = 0.57 (10.8 examples/sec; 0.738 sec/batch; 66h:34m:39s remains)
INFO - root - 2017-12-07 05:00:59.264530: step 7730, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.773 sec/batch; 69h:42m:57s remains)
INFO - root - 2017-12-07 05:01:07.085955: step 7740, loss = 1.04, batch loss = 0.97 (10.2 examples/sec; 0.781 sec/batch; 70h:27m:06s remains)
INFO - root - 2017-12-07 05:01:14.773436: step 7750, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.777 sec/batch; 70h:04m:00s remains)
INFO - root - 2017-12-07 05:01:22.355115: step 7760, loss = 1.08, batch loss = 1.01 (10.3 examples/sec; 0.780 sec/batch; 70h:20m:41s remains)
INFO - root - 2017-12-07 05:01:30.029384: step 7770, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.772 sec/batch; 69h:36m:27s remains)
INFO - root - 2017-12-07 05:01:37.496905: step 7780, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.763 sec/batch; 68h:49m:12s remains)
INFO - root - 2017-12-07 05:01:45.179581: step 7790, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.761 sec/batch; 68h:35m:44s remains)
INFO - root - 2017-12-07 05:01:53.043750: step 7800, loss = 0.94, batch loss = 0.87 (9.8 examples/sec; 0.814 sec/batch; 73h:24m:17s remains)
2017-12-07 05:01:53.654493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6962426 -1.9443049 -1.8405476 -1.5175345 -1.6876962 -2.2987 -2.7791119 -2.9967906 -3.1237512 -3.3794384 -3.6356225 -3.7336063 -3.7683287 -3.9242473 -4.0370846][-1.3534021 -1.6463187 -1.5226262 -1.0276344 -1.007654 -1.4621007 -1.8664587 -2.0884764 -2.2372971 -2.5860367 -2.9172535 -3.053865 -3.1523211 -3.3667984 -3.5405655][-0.99375939 -1.1337204 -0.81870675 -0.20156574 -0.10030222 -0.48143077 -0.95222235 -1.3230124 -1.5680578 -1.9789762 -2.27701 -2.3638725 -2.4595027 -2.6629686 -2.8792417][-0.54989576 -0.42394781 0.12848043 0.77560329 0.83240509 0.3658309 -0.293005 -0.8202498 -1.1332164 -1.4748781 -1.6198401 -1.6394787 -1.7584457 -2.0221097 -2.3190007][-0.2060051 0.23264742 0.96684885 1.5383663 1.4489169 0.83921576 0.121665 -0.33665228 -0.51347494 -0.62762403 -0.58019304 -0.60647821 -0.90361905 -1.423032 -1.8963335][-0.081177235 0.5396924 1.3090496 1.7855124 1.6082177 0.94845963 0.36361313 0.16085052 0.21873236 0.35105562 0.53971004 0.49620056 0.048364162 -0.74972749 -1.3790436][-0.16354465 0.41782904 1.064786 1.4208393 1.2196941 0.62656164 0.21602249 0.1719141 0.33628368 0.59257793 0.81554079 0.86996508 0.53605366 -0.30712461 -0.99185967][-0.45148158 -0.015835762 0.42200041 0.62222338 0.41619825 -0.040574074 -0.30305958 -0.34692621 -0.21780539 0.038392544 0.2169342 0.37750769 0.26204157 -0.4703908 -1.1724126][-0.91899681 -0.59230161 -0.38337135 -0.377213 -0.55536747 -0.83301425 -0.99303722 -1.1158557 -1.0897908 -0.88631773 -0.72886896 -0.4905417 -0.436893 -0.99166679 -1.6656137][-1.6087313 -1.2676482 -1.2295103 -1.3621082 -1.4612863 -1.5747554 -1.661114 -1.8324373 -1.8984628 -1.7743855 -1.6264772 -1.3727944 -1.2076154 -1.5422249 -2.0957263][-2.2853818 -1.9347844 -2.0243316 -2.2205627 -2.2600892 -2.2937689 -2.3453097 -2.4715223 -2.5296078 -2.4725542 -2.3660364 -2.1476736 -1.9320695 -2.0629363 -2.4625111][-2.7923548 -2.616478 -2.8868256 -3.1481044 -3.168149 -3.149488 -3.1150491 -3.1046517 -3.0966277 -3.083159 -3.0455217 -2.9047642 -2.6958351 -2.6927834 -2.9425378][-3.2691169 -3.2403662 -3.5505781 -3.7854691 -3.7978165 -3.7634027 -3.6788917 -3.5855658 -3.5480671 -3.5903666 -3.6239212 -3.56774 -3.4165125 -3.3528147 -3.4467483][-3.6918404 -3.6158359 -3.7337406 -3.8192785 -3.8011665 -3.7705417 -3.705493 -3.626523 -3.6091838 -3.6825438 -3.7619863 -3.7653887 -3.6919456 -3.6329846 -3.65255][-3.7472782 -3.6281433 -3.6026328 -3.5834761 -3.5341623 -3.4976096 -3.4638343 -3.4322102 -3.4359279 -3.4965034 -3.5631394 -3.5897889 -3.5786095 -3.558629 -3.5660646]]...]
INFO - root - 2017-12-07 05:02:01.248316: step 7810, loss = 0.69, batch loss = 0.62 (10.7 examples/sec; 0.746 sec/batch; 67h:17m:06s remains)
INFO - root - 2017-12-07 05:02:08.810914: step 7820, loss = 0.76, batch loss = 0.69 (10.1 examples/sec; 0.791 sec/batch; 71h:19m:17s remains)
INFO - root - 2017-12-07 05:02:16.605655: step 7830, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.757 sec/batch; 68h:17m:46s remains)
INFO - root - 2017-12-07 05:02:24.319131: step 7840, loss = 0.61, batch loss = 0.54 (10.3 examples/sec; 0.777 sec/batch; 70h:05m:26s remains)
INFO - root - 2017-12-07 05:02:31.932116: step 7850, loss = 0.61, batch loss = 0.54 (10.4 examples/sec; 0.767 sec/batch; 69h:09m:27s remains)
INFO - root - 2017-12-07 05:02:40.659791: step 7860, loss = 0.71, batch loss = 0.64 (7.8 examples/sec; 1.025 sec/batch; 92h:23m:21s remains)
INFO - root - 2017-12-07 05:02:50.943558: step 7870, loss = 0.72, batch loss = 0.65 (7.9 examples/sec; 1.007 sec/batch; 90h:50m:42s remains)
INFO - root - 2017-12-07 05:03:01.038004: step 7880, loss = 0.64, batch loss = 0.57 (7.8 examples/sec; 1.032 sec/batch; 93h:02m:54s remains)
INFO - root - 2017-12-07 05:03:11.376704: step 7890, loss = 0.92, batch loss = 0.84 (7.7 examples/sec; 1.039 sec/batch; 93h:42m:50s remains)
INFO - root - 2017-12-07 05:03:21.688508: step 7900, loss = 0.78, batch loss = 0.71 (8.0 examples/sec; 0.999 sec/batch; 90h:07m:09s remains)
2017-12-07 05:03:22.423553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9707036 -4.0175767 -4.1563554 -4.3101578 -4.435358 -4.5113807 -4.5278435 -4.5004034 -4.4233837 -4.3363872 -4.3268032 -4.4450946 -4.5784435 -4.6010957 -4.5350895][-3.9245064 -4.0326018 -4.2572269 -4.4904375 -4.6381721 -4.6442962 -4.55679 -4.4428105 -4.3127561 -4.1892605 -4.1729918 -4.3412247 -4.5274224 -4.5654449 -4.5114937][-3.7617371 -3.8267922 -4.0276132 -4.2894578 -4.4827194 -4.5280972 -4.5072889 -4.4623361 -4.3689361 -4.2190089 -4.1395421 -4.2417436 -4.3483095 -4.334693 -4.3081088][-3.0698166 -3.0302138 -3.1929095 -3.5350065 -3.8474243 -4.0151715 -4.13993 -4.1857562 -4.0733113 -3.8146036 -3.6193461 -3.6314826 -3.6812429 -3.7109618 -3.85091][-2.2004421 -2.1567166 -2.3804333 -2.8431263 -3.2383952 -3.4225526 -3.4619946 -3.3373137 -3.0327873 -2.6201344 -2.3600516 -2.4033833 -2.5941133 -2.8594155 -3.2848017][-1.4626782 -1.4660974 -1.7506604 -2.2388942 -2.5879908 -2.6384473 -2.4285848 -2.0668085 -1.6518409 -1.3271735 -1.2138772 -1.3897517 -1.7969632 -2.3346682 -2.9930634][-0.99073625 -0.8562901 -0.93760133 -1.2065828 -1.3958664 -1.3177443 -1.007278 -0.60014629 -0.3149395 -0.36479712 -0.61940622 -1.0350978 -1.5920923 -2.2209051 -2.9338918][-1.3195159 -1.1108634 -0.96108556 -0.93245387 -0.88246727 -0.66637754 -0.33114767 0.0051288605 0.0896101 -0.26342869 -0.7770009 -1.3666472 -1.8980956 -2.3412151 -2.8490839][-2.1402864 -1.9732094 -1.7721112 -1.6650438 -1.5128427 -1.2110651 -0.91226673 -0.75941968 -0.84553719 -1.1652694 -1.5479829 -2.0072825 -2.3481057 -2.502707 -2.7214699][-2.8260822 -2.6299436 -2.4174123 -2.3372269 -2.2305572 -2.0323267 -1.8852751 -1.8940747 -2.0523255 -2.2310209 -2.3316951 -2.5040512 -2.6055183 -2.5599408 -2.6344635][-3.319391 -3.0946908 -2.888737 -2.8290584 -2.7877336 -2.77155 -2.8352871 -2.9636915 -3.1015668 -3.1282039 -3.0190153 -2.9847846 -2.9684191 -2.8591785 -2.9145319][-3.5915885 -3.3306661 -3.1151822 -3.0502424 -3.0663285 -3.1655416 -3.3265021 -3.5045743 -3.6377752 -3.6247942 -3.4590578 -3.3765883 -3.3913193 -3.3530626 -3.4404383][-3.8694835 -3.6535556 -3.4286904 -3.2915916 -3.2454181 -3.2900183 -3.3714561 -3.4784703 -3.6015506 -3.6540484 -3.6132858 -3.6223726 -3.6886983 -3.7165112 -3.8299415][-4.051177 -3.9981868 -3.897404 -3.8038468 -3.7569206 -3.7621415 -3.7807376 -3.8065457 -3.8644221 -3.9177773 -3.9626603 -4.0314403 -4.0628638 -4.0388885 -4.0731869][-4.0283017 -4.0865 -4.1172853 -4.1380658 -4.1732311 -4.23061 -4.2911949 -4.3464289 -4.4189196 -4.4818521 -4.5286441 -4.5698581 -4.54361 -4.4446115 -4.3542552]]...]
INFO - root - 2017-12-07 05:03:32.876822: step 7910, loss = 0.81, batch loss = 0.74 (7.7 examples/sec; 1.037 sec/batch; 93h:30m:33s remains)
INFO - root - 2017-12-07 05:03:43.140818: step 7920, loss = 0.95, batch loss = 0.88 (8.1 examples/sec; 0.992 sec/batch; 89h:28m:43s remains)
INFO - root - 2017-12-07 05:03:53.517871: step 7930, loss = 0.95, batch loss = 0.88 (7.6 examples/sec; 1.052 sec/batch; 94h:52m:09s remains)
INFO - root - 2017-12-07 05:04:03.882898: step 7940, loss = 0.87, batch loss = 0.80 (7.7 examples/sec; 1.044 sec/batch; 94h:05m:05s remains)
INFO - root - 2017-12-07 05:04:14.233578: step 7950, loss = 0.84, batch loss = 0.76 (7.7 examples/sec; 1.045 sec/batch; 94h:11m:25s remains)
INFO - root - 2017-12-07 05:04:24.655123: step 7960, loss = 0.77, batch loss = 0.69 (8.0 examples/sec; 0.996 sec/batch; 89h:47m:01s remains)
INFO - root - 2017-12-07 05:04:35.078446: step 7970, loss = 0.89, batch loss = 0.82 (7.7 examples/sec; 1.037 sec/batch; 93h:28m:09s remains)
INFO - root - 2017-12-07 05:04:45.229844: step 7980, loss = 0.73, batch loss = 0.65 (7.8 examples/sec; 1.019 sec/batch; 91h:52m:45s remains)
INFO - root - 2017-12-07 05:04:55.608396: step 7990, loss = 0.87, batch loss = 0.80 (7.7 examples/sec; 1.039 sec/batch; 93h:41m:45s remains)
INFO - root - 2017-12-07 05:05:06.021922: step 8000, loss = 0.68, batch loss = 0.60 (7.7 examples/sec; 1.035 sec/batch; 93h:17m:10s remains)
2017-12-07 05:05:06.761220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3953061 -1.487319 -1.5580502 -1.4262283 -1.2269108 -1.1111181 -1.1885664 -1.2294788 -1.3041422 -1.4123585 -1.6034079 -1.7136583 -1.6457224 -1.6595242 -1.7682097][-1.4060354 -1.4763398 -1.3912909 -1.1354113 -1.028235 -1.080436 -1.2339196 -1.2869885 -1.3680048 -1.5127101 -1.7370934 -1.862658 -1.8271492 -1.7486546 -1.656971][-1.4578271 -1.4701691 -1.236969 -0.87760329 -0.798646 -0.8737061 -0.97823858 -1.0462258 -1.1853426 -1.4113631 -1.6716676 -1.8057449 -1.8350685 -1.741389 -1.5423248][-1.457407 -1.4295588 -1.1527486 -0.79104495 -0.714669 -0.70093083 -0.64865708 -0.67502403 -0.8529675 -1.168839 -1.4729006 -1.591326 -1.6779563 -1.652329 -1.494575][-1.4307034 -1.3569162 -1.0644989 -0.752022 -0.66150212 -0.52634382 -0.32835579 -0.32495117 -0.54794383 -0.95636797 -1.3078046 -1.4216058 -1.549737 -1.6345851 -1.572088][-1.4179776 -1.2688379 -0.91157579 -0.61003494 -0.47047162 -0.20316076 0.095247746 0.036819935 -0.35617638 -0.89993525 -1.2456119 -1.307992 -1.4621446 -1.6962678 -1.7599974][-1.3698854 -1.1041589 -0.63198304 -0.24038982 0.056223392 0.50808 0.89521456 0.69205379 -0.00650692 -0.76642585 -1.1769862 -1.3043292 -1.5579252 -1.901583 -2.0339372][-1.2194052 -0.85219169 -0.30736256 0.17117834 0.60915709 1.1835175 1.5951705 1.2409277 0.30427313 -0.55813551 -1.0294938 -1.3331463 -1.7618508 -2.1956589 -2.36892][-1.0218327 -0.6294353 -0.12292242 0.31090021 0.68358374 1.1246924 1.3750019 0.97100067 0.088430405 -0.64774585 -1.0924191 -1.5419664 -2.0986681 -2.5825949 -2.8084207][-0.86897731 -0.51784396 -0.10821104 0.18566465 0.31845522 0.45859051 0.45266294 0.075615883 -0.53269458 -1.0049305 -1.3657627 -1.8647778 -2.4457102 -2.9084482 -3.1331687][-0.78814626 -0.46774483 -0.083219051 0.11312532 -0.00058507919 -0.16353369 -0.38950348 -0.71785808 -1.0464532 -1.2906096 -1.5482149 -1.9838974 -2.4987583 -2.9085441 -3.08109][-0.773227 -0.47190046 -0.0955925 0.051258564 -0.18476391 -0.4959166 -0.8234365 -1.0997515 -1.2604437 -1.4214191 -1.610961 -1.9210947 -2.3229921 -2.7071428 -2.86153][-0.74960613 -0.51088572 -0.21662807 -0.11748266 -0.35817385 -0.69402194 -0.98295546 -1.1469579 -1.2591033 -1.5045722 -1.7667685 -2.0129139 -2.3257322 -2.6564159 -2.7954407][-0.85718346 -0.66825652 -0.47233605 -0.40955639 -0.6100657 -0.91436696 -1.1122298 -1.1313879 -1.2320886 -1.549613 -1.8958347 -2.1568298 -2.483192 -2.8390307 -3.076458][-1.3317297 -1.1650229 -0.99504471 -0.89288688 -1.0094147 -1.2006867 -1.2471755 -1.1268597 -1.1861169 -1.4648356 -1.785125 -2.0741231 -2.4738078 -2.9435556 -3.3509583]]...]
INFO - root - 2017-12-07 05:05:17.241409: step 8010, loss = 0.91, batch loss = 0.84 (7.4 examples/sec; 1.075 sec/batch; 96h:51m:37s remains)
INFO - root - 2017-12-07 05:05:27.720756: step 8020, loss = 0.75, batch loss = 0.67 (7.7 examples/sec; 1.041 sec/batch; 93h:50m:45s remains)
INFO - root - 2017-12-07 05:05:38.052210: step 8030, loss = 0.85, batch loss = 0.78 (7.8 examples/sec; 1.019 sec/batch; 91h:51m:45s remains)
INFO - root - 2017-12-07 05:05:48.516503: step 8040, loss = 0.88, batch loss = 0.81 (7.9 examples/sec; 1.012 sec/batch; 91h:13m:51s remains)
INFO - root - 2017-12-07 05:05:58.962164: step 8050, loss = 0.73, batch loss = 0.65 (7.6 examples/sec; 1.057 sec/batch; 95h:15m:55s remains)
INFO - root - 2017-12-07 05:06:09.240648: step 8060, loss = 0.71, batch loss = 0.64 (8.1 examples/sec; 0.987 sec/batch; 88h:54m:28s remains)
INFO - root - 2017-12-07 05:06:19.572553: step 8070, loss = 0.83, batch loss = 0.75 (8.1 examples/sec; 0.993 sec/batch; 89h:28m:25s remains)
INFO - root - 2017-12-07 05:06:29.615433: step 8080, loss = 0.80, batch loss = 0.73 (7.7 examples/sec; 1.033 sec/batch; 93h:05m:35s remains)
INFO - root - 2017-12-07 05:06:39.771040: step 8090, loss = 0.85, batch loss = 0.78 (7.7 examples/sec; 1.036 sec/batch; 93h:22m:12s remains)
INFO - root - 2017-12-07 05:06:50.212724: step 8100, loss = 1.01, batch loss = 0.93 (7.7 examples/sec; 1.044 sec/batch; 94h:02m:51s remains)
2017-12-07 05:06:50.964285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1729515 -3.0673192 -3.1477685 -3.2388897 -3.1860967 -3.1596713 -3.105293 -2.9794004 -2.9809952 -3.1255231 -3.1911044 -3.190088 -3.1853194 -3.004899 -2.9410858][-3.1673942 -3.0278294 -3.0944092 -3.1402912 -3.0494797 -3.0105615 -2.9332576 -2.801003 -2.8445246 -3.0198827 -3.1174207 -3.1527643 -3.143075 -2.9264169 -2.8570008][-3.277945 -3.1322551 -3.1850245 -3.1525273 -3.0200415 -2.9593349 -2.8259072 -2.7014871 -2.8178535 -2.9991746 -3.0682735 -3.1116643 -3.0974219 -2.8405643 -2.7528706][-3.50457 -3.3759379 -3.4050756 -3.273005 -3.0862992 -2.9768879 -2.742779 -2.6233015 -2.8553696 -3.0510631 -3.0489459 -3.0837653 -3.0932338 -2.8221583 -2.6993761][-3.7118294 -3.5975955 -3.6052337 -3.4116721 -3.1962442 -3.036737 -2.6687217 -2.5312214 -2.9007244 -3.1616173 -3.1055496 -3.1232843 -3.179868 -2.9154229 -2.7518296][-3.8672602 -3.7172036 -3.6797464 -3.4752038 -3.2526937 -3.042716 -2.5520332 -2.3961198 -2.9073772 -3.2731297 -3.1992571 -3.1850078 -3.2638822 -2.9989676 -2.7907107][-3.8432872 -3.6478488 -3.5966895 -3.4380665 -3.2276449 -2.9664202 -2.3830905 -2.2049978 -2.8374062 -3.3424907 -3.3022845 -3.248687 -3.2992096 -3.0177903 -2.7711353][-3.6136169 -3.3961804 -3.384789 -3.3234391 -3.1850405 -2.9143596 -2.2846675 -2.0541017 -2.7255692 -3.3778529 -3.4260657 -3.3562322 -3.3414178 -3.0258021 -2.7555339][-3.3741302 -3.1307888 -3.1442146 -3.1537457 -3.1116657 -2.9288726 -2.3872073 -2.1480048 -2.7358418 -3.3937469 -3.46883 -3.3792644 -3.3139844 -3.0013394 -2.7848988][-3.1694443 -2.9626007 -3.0277958 -3.0740383 -3.075587 -3.0068212 -2.648911 -2.4745257 -2.9176769 -3.4448638 -3.4641328 -3.3174284 -3.1891313 -2.88519 -2.7771485][-3.1263895 -3.0097215 -3.1683307 -3.2496216 -3.249269 -3.2752059 -3.1015112 -2.9662004 -3.2147541 -3.5211177 -3.4579184 -3.2709587 -3.0904279 -2.8025584 -2.7880855][-3.2531672 -3.2305558 -3.4513936 -3.5316963 -3.4902925 -3.5902297 -3.58773 -3.476315 -3.5412583 -3.62567 -3.4702361 -3.2637181 -3.0796404 -2.8416262 -2.9068875][-3.5245781 -3.5306313 -3.7542939 -3.8278584 -3.7463975 -3.8501456 -3.9439368 -3.8539114 -3.8170412 -3.7860706 -3.5787449 -3.3511677 -3.1509705 -2.9362731 -3.02344][-3.8514719 -3.8134007 -3.9978561 -4.1077209 -4.0483003 -4.1224213 -4.2426853 -4.1554418 -4.0662603 -4.0035429 -3.808454 -3.5893512 -3.3820004 -3.1593311 -3.1717935][-4.028173 -3.9294553 -4.0621367 -4.1981235 -4.1587429 -4.224647 -4.4095521 -4.3790274 -4.2615628 -4.190546 -4.0411692 -3.8621151 -3.664 -3.4382792 -3.3741117]]...]
INFO - root - 2017-12-07 05:07:01.312762: step 8110, loss = 0.70, batch loss = 0.63 (7.7 examples/sec; 1.042 sec/batch; 93h:56m:14s remains)
INFO - root - 2017-12-07 05:07:11.675614: step 8120, loss = 0.91, batch loss = 0.84 (7.7 examples/sec; 1.044 sec/batch; 94h:02m:11s remains)
INFO - root - 2017-12-07 05:07:22.283920: step 8130, loss = 0.89, batch loss = 0.81 (7.2 examples/sec; 1.114 sec/batch; 100h:23m:11s remains)
INFO - root - 2017-12-07 05:07:32.735548: step 8140, loss = 0.54, batch loss = 0.47 (7.6 examples/sec; 1.053 sec/batch; 94h:51m:44s remains)
INFO - root - 2017-12-07 05:07:43.038631: step 8150, loss = 0.89, batch loss = 0.81 (7.6 examples/sec; 1.048 sec/batch; 94h:27m:25s remains)
INFO - root - 2017-12-07 05:07:53.430660: step 8160, loss = 0.84, batch loss = 0.77 (7.4 examples/sec; 1.078 sec/batch; 97h:05m:50s remains)
INFO - root - 2017-12-07 05:08:03.954114: step 8170, loss = 0.68, batch loss = 0.61 (7.4 examples/sec; 1.075 sec/batch; 96h:50m:33s remains)
INFO - root - 2017-12-07 05:08:14.283446: step 8180, loss = 1.01, batch loss = 0.94 (7.5 examples/sec; 1.063 sec/batch; 95h:47m:46s remains)
INFO - root - 2017-12-07 05:08:24.433447: step 8190, loss = 0.79, batch loss = 0.72 (7.6 examples/sec; 1.055 sec/batch; 95h:01m:56s remains)
INFO - root - 2017-12-07 05:08:34.822554: step 8200, loss = 0.89, batch loss = 0.81 (7.7 examples/sec; 1.038 sec/batch; 93h:31m:31s remains)
2017-12-07 05:08:35.570342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.879658 -1.2824893 -1.4818017 -1.436322 -1.1179547 -0.737129 -0.318748 -0.16118765 -0.2666254 -0.31462193 -0.24920034 -0.13945055 -0.14005995 -0.24254465 -0.35113573][-0.712116 -1.1101491 -1.3955076 -1.4742198 -1.2674921 -1.0033367 -0.65398431 -0.53402519 -0.68140841 -0.79378819 -0.82406044 -0.84245253 -0.92917037 -0.98336339 -0.93143034][-0.44230843 -0.8930068 -1.3348787 -1.6100564 -1.5938885 -1.5325296 -1.3345635 -1.2308903 -1.3173151 -1.375622 -1.3591831 -1.3500991 -1.3687115 -1.3234408 -1.1834793][-0.39011812 -0.82172513 -1.1811676 -1.3772836 -1.290782 -1.2212453 -1.0981619 -1.0649617 -1.2030821 -1.3151121 -1.3082533 -1.2598755 -1.1628666 -0.97957683 -0.72612929][-0.45854473 -0.80350971 -1.0587614 -1.1890135 -0.926471 -0.55453372 -0.099167347 0.2323184 0.32363844 0.31179857 0.27094793 0.1720047 0.090188026 0.1259408 0.29809141][-0.5750103 -0.858093 -1.1456838 -1.3682394 -1.0740848 -0.54558921 0.16845274 0.87000179 1.4405985 1.8099637 1.8737597 1.6719155 1.3692017 1.1822257 1.2110033][-0.76623416 -1.1602921 -1.6304286 -1.9777756 -1.6608829 -1.0003088 -0.077363014 0.8818202 1.7532678 2.29466 2.3219981 2.05129 1.6706085 1.4171481 1.4146404][-0.89982057 -1.4114966 -2.0045083 -2.3905938 -2.0556896 -1.3506515 -0.42226839 0.49669886 1.3040357 1.7220526 1.6506391 1.4328952 1.1571918 1.0449138 1.1689806][-1.0815167 -1.5964973 -2.1074619 -2.3740053 -2.0382044 -1.4100535 -0.69249654 -0.0847621 0.38635445 0.49081755 0.32887506 0.22693443 0.12776232 0.22744226 0.52642059][-1.1296964 -1.5910044 -2.0533996 -2.3864605 -2.3050184 -1.967257 -1.5201292 -1.0824959 -0.680212 -0.59314775 -0.58269787 -0.43972731 -0.33888054 -0.081362247 0.26200008][-0.88616419 -1.2697058 -1.7615368 -2.3257346 -2.6221862 -2.6956286 -2.5787294 -2.2896531 -1.9084988 -1.7330105 -1.4610806 -1.0004168 -0.61936593 -0.13647652 0.30567741][-0.66354847 -1.0010676 -1.4620345 -2.0822649 -2.4405913 -2.6156564 -2.5780797 -2.3631668 -2.15815 -2.1418042 -1.9523005 -1.5437298 -1.1299741 -0.54206848 0.0094332695][-0.67145777 -0.97697926 -1.3122368 -1.7825792 -1.8735576 -1.7972896 -1.5359616 -1.2192135 -1.1208944 -1.2008858 -1.1172748 -0.88907433 -0.62808704 -0.18579531 0.22140408][-0.70070195 -0.95614004 -1.2429342 -1.7042127 -1.6848016 -1.4604411 -0.99156308 -0.52144 -0.37863111 -0.33920908 -0.12800598 0.17226839 0.49011087 0.89448166 1.1658664][-0.5651474 -0.75178409 -1.0924242 -1.7012272 -1.8152888 -1.6925788 -1.2092767 -0.69679976 -0.49252748 -0.26063013 0.16261101 0.62869453 1.0726876 1.5185127 1.7711554]]...]
INFO - root - 2017-12-07 05:08:46.053835: step 8210, loss = 0.83, batch loss = 0.76 (7.4 examples/sec; 1.074 sec/batch; 96h:45m:29s remains)
INFO - root - 2017-12-07 05:08:56.358424: step 8220, loss = 0.83, batch loss = 0.76 (7.8 examples/sec; 1.025 sec/batch; 92h:19m:53s remains)
INFO - root - 2017-12-07 05:09:06.530146: step 8230, loss = 0.88, batch loss = 0.80 (7.8 examples/sec; 1.022 sec/batch; 92h:04m:57s remains)
INFO - root - 2017-12-07 05:09:16.865838: step 8240, loss = 0.94, batch loss = 0.86 (7.6 examples/sec; 1.057 sec/batch; 95h:11m:16s remains)
INFO - root - 2017-12-07 05:09:27.273898: step 8250, loss = 0.76, batch loss = 0.69 (7.6 examples/sec; 1.046 sec/batch; 94h:14m:12s remains)
INFO - root - 2017-12-07 05:09:37.885011: step 8260, loss = 0.74, batch loss = 0.67 (7.7 examples/sec; 1.037 sec/batch; 93h:26m:06s remains)
INFO - root - 2017-12-07 05:09:48.242394: step 8270, loss = 0.74, batch loss = 0.67 (7.7 examples/sec; 1.032 sec/batch; 92h:59m:00s remains)
INFO - root - 2017-12-07 05:09:58.393482: step 8280, loss = 0.93, batch loss = 0.86 (7.7 examples/sec; 1.033 sec/batch; 93h:02m:06s remains)
INFO - root - 2017-12-07 05:10:08.733483: step 8290, loss = 0.88, batch loss = 0.80 (7.8 examples/sec; 1.020 sec/batch; 91h:50m:38s remains)
INFO - root - 2017-12-07 05:10:19.163665: step 8300, loss = 0.77, batch loss = 0.70 (7.8 examples/sec; 1.031 sec/batch; 92h:48m:12s remains)
2017-12-07 05:10:19.925314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1993682 -2.2378318 -2.3197272 -2.4967399 -2.5336597 -2.51015 -2.4778416 -2.4240928 -2.3632493 -2.2628765 -2.2750115 -2.2699234 -2.1047044 -2.0576887 -2.1031444][-1.8207366 -1.8017435 -1.8670514 -2.0718446 -2.1331766 -2.1792142 -2.2494404 -2.2400739 -2.1738384 -2.0387492 -2.0446904 -2.0585089 -1.9278166 -1.8572199 -1.8433697][-1.465955 -1.3829026 -1.3953593 -1.5338693 -1.5089035 -1.5591559 -1.7513125 -1.880863 -1.9128451 -1.81305 -1.8439784 -1.901808 -1.7907226 -1.6004398 -1.4487674][-1.3320158 -1.2714596 -1.3227687 -1.4197154 -1.2705238 -1.2196755 -1.3630409 -1.5298259 -1.6250808 -1.5577455 -1.6136696 -1.7631257 -1.7374647 -1.4772396 -1.2122343][-1.6504683 -1.6498587 -1.7734489 -1.8080142 -1.5595794 -1.3995183 -1.4070525 -1.4854937 -1.531589 -1.4117494 -1.4850335 -1.7733347 -1.910696 -1.7045844 -1.3919647][-1.8811409 -1.9899578 -2.2293706 -2.2297976 -1.9397073 -1.708329 -1.5830085 -1.5432854 -1.4871125 -1.2446659 -1.362148 -1.8719959 -2.2128265 -2.1489141 -1.8640091][-2.183346 -2.3583598 -2.690619 -2.7075403 -2.4201522 -2.1406949 -1.8619654 -1.6655495 -1.4436491 -1.0576227 -1.2462938 -2.0341601 -2.5995302 -2.7471097 -2.5608013][-2.6347818 -2.80827 -3.1445577 -3.1926532 -2.9341829 -2.6195848 -2.1654694 -1.7800934 -1.3790312 -0.85097337 -1.0705402 -2.0943308 -2.8922129 -3.2807541 -3.2461331][-2.5831771 -2.8459153 -3.250839 -3.437268 -3.3471026 -3.1188238 -2.6338711 -2.1544666 -1.6534572 -1.0063753 -1.0714548 -2.0334871 -2.9190404 -3.5086825 -3.6701608][-2.4584553 -2.8329096 -3.3066769 -3.6217663 -3.721838 -3.6542828 -3.2894547 -2.8698211 -2.4112809 -1.7940516 -1.6601415 -2.2862415 -3.0190554 -3.6580153 -3.9563181][-2.9767919 -3.3201663 -3.6897 -3.9489737 -4.0688128 -4.0551438 -3.8330238 -3.5388834 -3.163522 -2.6564093 -2.4372602 -2.7356415 -3.2204137 -3.763185 -4.1017365][-3.525651 -3.8112526 -4.0612707 -4.2217779 -4.3038664 -4.3087583 -4.2185316 -4.0596037 -3.7968225 -3.4317241 -3.2161624 -3.2482481 -3.4141948 -3.72747 -4.0034442][-3.8513966 -4.1584377 -4.3754458 -4.4664788 -4.4899926 -4.4589915 -4.4105258 -4.3403034 -4.2097812 -4.0176163 -3.8632519 -3.7247114 -3.5786572 -3.5899322 -3.7094073][-3.9238663 -4.2732592 -4.496366 -4.5556049 -4.5308204 -4.4357467 -4.324996 -4.2352371 -4.1866803 -4.1176043 -4.0352988 -3.8408065 -3.5724318 -3.4339819 -3.4512649][-3.723891 -4.0224075 -4.2514429 -4.3831711 -4.4533253 -4.425714 -4.3019867 -4.1209731 -4.0270424 -3.9731879 -3.9309173 -3.7242718 -3.4572153 -3.3488145 -3.4154763]]...]
INFO - root - 2017-12-07 05:10:30.411912: step 8310, loss = 0.65, batch loss = 0.58 (7.7 examples/sec; 1.038 sec/batch; 93h:26m:10s remains)
INFO - root - 2017-12-07 05:10:40.730564: step 8320, loss = 0.72, batch loss = 0.65 (7.7 examples/sec; 1.036 sec/batch; 93h:17m:03s remains)
INFO - root - 2017-12-07 05:10:51.051843: step 8330, loss = 0.80, batch loss = 0.73 (7.8 examples/sec; 1.031 sec/batch; 92h:48m:51s remains)
INFO - root - 2017-12-07 05:11:01.443415: step 8340, loss = 0.94, batch loss = 0.87 (7.8 examples/sec; 1.022 sec/batch; 92h:01m:17s remains)
INFO - root - 2017-12-07 05:11:11.917188: step 8350, loss = 0.89, batch loss = 0.81 (7.6 examples/sec; 1.053 sec/batch; 94h:50m:01s remains)
INFO - root - 2017-12-07 05:11:22.389459: step 8360, loss = 0.84, batch loss = 0.77 (7.4 examples/sec; 1.074 sec/batch; 96h:42m:07s remains)
INFO - root - 2017-12-07 05:11:32.667973: step 8370, loss = 0.84, batch loss = 0.77 (7.4 examples/sec; 1.077 sec/batch; 96h:59m:05s remains)
INFO - root - 2017-12-07 05:11:42.950836: step 8380, loss = 0.74, batch loss = 0.67 (7.5 examples/sec; 1.074 sec/batch; 96h:40m:26s remains)
INFO - root - 2017-12-07 05:11:53.408531: step 8390, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.063 sec/batch; 95h:40m:56s remains)
INFO - root - 2017-12-07 05:12:03.996781: step 8400, loss = 0.89, batch loss = 0.82 (7.6 examples/sec; 1.053 sec/batch; 94h:49m:41s remains)
2017-12-07 05:12:04.766907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5210662 -1.5760541 -1.6382713 -1.6297488 -1.5893664 -1.5588775 -1.6228998 -1.8133349 -2.0917213 -2.0641179 -1.9540253 -2.0557277 -2.0271068 -1.787734 -1.5579615][-1.4647861 -1.6268375 -1.7253983 -1.7626297 -1.754951 -1.7047992 -1.6973166 -1.8302317 -2.0567455 -1.929652 -1.6687918 -1.6482525 -1.5521512 -1.2879739 -1.066509][-1.6515024 -1.8623528 -1.9202087 -1.8716168 -1.7486639 -1.5633767 -1.428668 -1.4929271 -1.7168446 -1.6366673 -1.3586628 -1.3187726 -1.2823691 -1.1238458 -0.99639726][-1.8022187 -2.0174158 -2.0544357 -1.9624817 -1.8040519 -1.5615704 -1.3520846 -1.3632913 -1.5908821 -1.6538107 -1.4908512 -1.5110104 -1.5571418 -1.4350967 -1.2842185][-1.6559184 -1.8253179 -1.8719187 -1.8290164 -1.8057854 -1.6900702 -1.5323765 -1.5067511 -1.6382833 -1.7182992 -1.5896866 -1.6318104 -1.77405 -1.7087908 -1.5398307][-1.4351072 -1.5840752 -1.6839378 -1.6875551 -1.7256069 -1.6549408 -1.5148361 -1.4910152 -1.5718739 -1.6906998 -1.6176527 -1.6293478 -1.7587171 -1.6861553 -1.4872644][-1.4129152 -1.6292396 -1.8148391 -1.8290672 -1.7855084 -1.5709603 -1.2481179 -1.0647073 -1.0289316 -1.1662581 -1.2115309 -1.274019 -1.419847 -1.3455076 -1.1289065][-1.3716228 -1.5473554 -1.6419435 -1.4942653 -1.2559426 -0.86922383 -0.40066862 -0.13138676 -0.067116261 -0.26289368 -0.45373249 -0.69148016 -1.0073426 -1.056865 -0.94834971][-1.3793614 -1.4060924 -1.3279843 -1.0108781 -0.61374807 -0.12577534 0.37662411 0.57755661 0.49585581 0.10439873 -0.31584263 -0.78355742 -1.260715 -1.3560402 -1.2415707][-1.7251525 -1.7070153 -1.6096795 -1.3309875 -0.9877677 -0.58052611 -0.16386843 -0.020705223 -0.14581537 -0.60855675 -1.1710498 -1.8080032 -2.3640366 -2.3925886 -2.1293631][-2.2014606 -2.2349968 -2.23215 -2.119782 -1.9547203 -1.7230165 -1.4690878 -1.3857043 -1.468472 -1.8093665 -2.2625647 -2.7926149 -3.1779227 -3.0327802 -2.6644669][-2.57156 -2.6649818 -2.7348154 -2.7247262 -2.6459229 -2.4980671 -2.359411 -2.331984 -2.4026041 -2.6205845 -2.9255786 -3.2722697 -3.4248421 -3.1409848 -2.7602623][-2.6254811 -2.73215 -2.8169706 -2.8287277 -2.7575302 -2.6279073 -2.5389795 -2.5365169 -2.5990295 -2.7412682 -2.9593925 -3.2139487 -3.3157482 -3.1229627 -2.8815575][-2.4763892 -2.5224783 -2.5669503 -2.55741 -2.4741225 -2.3530076 -2.2848294 -2.2867646 -2.3227959 -2.4055259 -2.5356419 -2.6708298 -2.712919 -2.6100149 -2.5021322][-2.3868365 -2.3752244 -2.3710244 -2.3449667 -2.2776768 -2.1949837 -2.1503913 -2.1460578 -2.1509485 -2.187155 -2.2493367 -2.2940109 -2.2900507 -2.2432778 -2.2132049]]...]
INFO - root - 2017-12-07 05:12:15.121007: step 8410, loss = 0.73, batch loss = 0.66 (7.6 examples/sec; 1.053 sec/batch; 94h:47m:17s remains)
INFO - root - 2017-12-07 05:12:25.430972: step 8420, loss = 0.71, batch loss = 0.64 (7.6 examples/sec; 1.050 sec/batch; 94h:28m:45s remains)
INFO - root - 2017-12-07 05:12:35.756266: step 8430, loss = 0.88, batch loss = 0.81 (8.0 examples/sec; 1.006 sec/batch; 90h:34m:57s remains)
INFO - root - 2017-12-07 05:12:46.024932: step 8440, loss = 0.87, batch loss = 0.80 (7.9 examples/sec; 1.008 sec/batch; 90h:43m:19s remains)
INFO - root - 2017-12-07 05:12:56.337271: step 8450, loss = 0.88, batch loss = 0.81 (7.9 examples/sec; 1.016 sec/batch; 91h:27m:48s remains)
INFO - root - 2017-12-07 05:13:06.796820: step 8460, loss = 1.02, batch loss = 0.95 (7.4 examples/sec; 1.088 sec/batch; 97h:55m:18s remains)
INFO - root - 2017-12-07 05:13:17.202558: step 8470, loss = 0.72, batch loss = 0.65 (7.6 examples/sec; 1.049 sec/batch; 94h:26m:32s remains)
INFO - root - 2017-12-07 05:13:27.411096: step 8480, loss = 0.85, batch loss = 0.78 (7.5 examples/sec; 1.067 sec/batch; 96h:00m:12s remains)
INFO - root - 2017-12-07 05:13:37.804892: step 8490, loss = 0.85, batch loss = 0.78 (7.8 examples/sec; 1.031 sec/batch; 92h:49m:17s remains)
INFO - root - 2017-12-07 05:13:48.114423: step 8500, loss = 0.76, batch loss = 0.69 (7.9 examples/sec; 1.013 sec/batch; 91h:11m:37s remains)
2017-12-07 05:13:48.881627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.53739119 -0.53320479 -0.52769804 -0.5549202 -0.60344362 -0.66299915 -0.7266705 -0.77208734 -0.7922101 -0.78283739 -0.77126789 -0.78761268 -0.81171846 -0.81984472 -0.79603863][-0.50807047 -0.50182009 -0.48851943 -0.49254131 -0.51003337 -0.5262444 -0.53859019 -0.53795576 -0.51436257 -0.4623704 -0.41767311 -0.41585207 -0.44882846 -0.47466803 -0.47162509][-1.0920305 -1.1332288 -1.095324 -1.0081902 -0.90558267 -0.7872498 -0.67916036 -0.59331036 -0.52347445 -0.4489665 -0.40734816 -0.43437552 -0.50563836 -0.55957007 -0.56839561][-1.6706624 -1.7965937 -1.8074698 -1.7334487 -1.6127186 -1.4415052 -1.2661126 -1.1128559 -0.97385764 -0.83819103 -0.77144217 -0.81413841 -0.91091537 -0.97907686 -0.96960878][-1.7062583 -1.8389122 -1.8905671 -1.8996804 -1.8965497 -1.853739 -1.8074145 -1.7449656 -1.6374478 -1.49318 -1.4304264 -1.5019839 -1.6190906 -1.6977959 -1.6719871][-1.8503966 -1.9437582 -1.9147573 -1.8076351 -1.6878214 -1.5670168 -1.5094573 -1.4640226 -1.3893404 -1.2905884 -1.2995591 -1.4567323 -1.6448636 -1.7979186 -1.8474133][-2.5192504 -2.6845412 -2.6152434 -2.3495677 -1.9876094 -1.5722311 -1.201112 -0.85943604 -0.57884765 -0.37742996 -0.354177 -0.53546667 -0.77356219 -0.99449778 -1.1171439][-2.7687111 -3.0199676 -2.9805117 -2.7389374 -2.4097769 -1.981384 -1.5174861 -1.0183871 -0.5818274 -0.24197292 -0.10292006 -0.20982361 -0.40658236 -0.61088347 -0.73133826][-2.3903403 -2.6757891 -2.6620564 -2.4922938 -2.290463 -2.0345898 -1.781141 -1.5055342 -1.2655399 -1.0368841 -0.92312765 -0.99858046 -1.1437876 -1.3098147 -1.4157805][-2.3206916 -2.594121 -2.5956576 -2.4552574 -2.2543161 -1.9955788 -1.7887044 -1.6256196 -1.5377612 -1.4574575 -1.4601927 -1.6018994 -1.7562742 -1.9220669 -2.0443325][-2.6464782 -2.8612676 -2.8673921 -2.7653108 -2.5624905 -2.2689686 -1.9864635 -1.7175591 -1.5183387 -1.3682513 -1.3421924 -1.459538 -1.5933664 -1.7519662 -1.8846176][-2.584271 -2.7412086 -2.6945224 -2.5483675 -2.3342116 -2.0968099 -1.8807912 -1.6485572 -1.4496586 -1.2845936 -1.2169006 -1.2445905 -1.3015563 -1.4041941 -1.5046997][-1.9844122 -2.1095169 -2.0482662 -1.8614478 -1.6160955 -1.4034948 -1.2520592 -1.122782 -1.0392635 -0.992435 -1.0043108 -1.0408025 -1.0789769 -1.1444461 -1.206306][-1.4288616 -1.5186603 -1.510242 -1.4027951 -1.2362506 -1.0918014 -0.980659 -0.89110613 -0.83973932 -0.83107209 -0.86930871 -0.90949011 -0.94003677 -0.97606993 -1.002737][-1.1509323 -1.1641583 -1.1543593 -1.1146438 -1.0649269 -1.0399842 -1.0256951 -1.004957 -0.97427249 -0.95001197 -0.94992042 -0.95141912 -0.94185686 -0.93597722 -0.93351865]]...]
INFO - root - 2017-12-07 05:13:59.294636: step 8510, loss = 1.00, batch loss = 0.93 (7.8 examples/sec; 1.025 sec/batch; 92h:12m:11s remains)
INFO - root - 2017-12-07 05:14:09.637458: step 8520, loss = 0.64, batch loss = 0.57 (7.6 examples/sec; 1.056 sec/batch; 95h:00m:34s remains)
INFO - root - 2017-12-07 05:14:20.030951: step 8530, loss = 0.95, batch loss = 0.88 (7.4 examples/sec; 1.079 sec/batch; 97h:07m:24s remains)
INFO - root - 2017-12-07 05:14:30.398623: step 8540, loss = 0.85, batch loss = 0.78 (7.7 examples/sec; 1.035 sec/batch; 93h:09m:40s remains)
INFO - root - 2017-12-07 05:14:40.683478: step 8550, loss = 0.77, batch loss = 0.70 (7.6 examples/sec; 1.049 sec/batch; 94h:24m:38s remains)
INFO - root - 2017-12-07 05:14:50.972408: step 8560, loss = 0.88, batch loss = 0.81 (7.7 examples/sec; 1.038 sec/batch; 93h:26m:40s remains)
INFO - root - 2017-12-07 05:15:01.355626: step 8570, loss = 0.79, batch loss = 0.72 (7.8 examples/sec; 1.029 sec/batch; 92h:33m:22s remains)
INFO - root - 2017-12-07 05:15:11.472684: step 8580, loss = 0.91, batch loss = 0.84 (7.9 examples/sec; 1.016 sec/batch; 91h:22m:48s remains)
INFO - root - 2017-12-07 05:15:21.816994: step 8590, loss = 0.76, batch loss = 0.69 (7.3 examples/sec; 1.090 sec/batch; 98h:02m:20s remains)
INFO - root - 2017-12-07 05:15:32.186747: step 8600, loss = 0.63, batch loss = 0.56 (7.8 examples/sec; 1.027 sec/batch; 92h:24m:48s remains)
2017-12-07 05:15:32.972534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2398024 -3.2282259 -3.2229309 -3.237617 -3.2606912 -3.25797 -3.2389688 -3.2342596 -3.2368319 -3.2294383 -3.212059 -3.1928263 -3.1755667 -3.1736772 -3.1648183][-4.2261696 -4.1761465 -4.1479988 -4.1636558 -4.2117519 -4.21169 -4.15997 -4.1582646 -4.2016029 -4.2229552 -4.2053695 -4.1644435 -4.1166921 -4.0976791 -4.0714188][-4.7366672 -4.6484847 -4.6110249 -4.6302285 -4.6918983 -4.6687355 -4.5523982 -4.5448918 -4.6485038 -4.7306118 -4.7408624 -4.6979318 -4.6260405 -4.590313 -4.5369468][-4.5594711 -4.5012608 -4.5400209 -4.6270404 -4.743185 -4.7270021 -4.5343709 -4.4869123 -4.6445613 -4.7852535 -4.8256764 -4.7968173 -4.7469797 -4.7639132 -4.7213912][-3.5712948 -3.4899247 -3.5407946 -3.6494632 -3.7772288 -3.7616856 -3.5348358 -3.5353513 -3.8794065 -4.1956549 -4.3012891 -4.3047533 -4.3499732 -4.5358753 -4.6101317][-2.1790955 -2.031158 -2.09147 -2.234268 -2.3071833 -2.1810668 -1.8569808 -1.9046831 -2.5153213 -3.124264 -3.3377359 -3.3637514 -3.5185432 -3.8861597 -4.1191978][-1.2916203 -1.1533408 -1.3317418 -1.5976775 -1.6205003 -1.2551985 -0.61888456 -0.45433569 -1.1231158 -1.9199026 -2.2956991 -2.4500711 -2.7711549 -3.3177524 -3.7103481][-1.2483022 -1.0127399 -1.0611877 -1.2444725 -1.221606 -0.792609 -0.086063862 0.21846056 -0.26556444 -0.9611876 -1.3600407 -1.6155987 -2.0420973 -2.6666458 -3.1955218][-1.803407 -1.4200933 -1.124902 -1.0449157 -1.0176733 -0.86296153 -0.562052 -0.47348022 -0.79602814 -1.2262468 -1.4862096 -1.6553922 -1.9376724 -2.3935401 -2.9187987][-2.9212403 -2.6527941 -2.265662 -2.0113471 -1.9066207 -1.8587282 -1.7900963 -1.7965236 -1.9178786 -2.0810208 -2.1690347 -2.1338265 -2.0615156 -2.091182 -2.3290718][-3.0731115 -2.9671149 -2.6810107 -2.474586 -2.4536216 -2.5078864 -2.5109916 -2.483223 -2.4423695 -2.4409823 -2.4265842 -2.2373879 -1.9138343 -1.6508167 -1.6329484][-3.0416069 -2.9406772 -2.7390873 -2.6110296 -2.62254 -2.6559992 -2.6056747 -2.5043921 -2.4238045 -2.4181309 -2.4405022 -2.318975 -2.0938182 -1.9519186 -2.0208113][-3.5282602 -3.4429975 -3.2984958 -3.2095637 -3.1594648 -3.011302 -2.718118 -2.4180839 -2.249872 -2.2545843 -2.3637304 -2.4109612 -2.405648 -2.51446 -2.7993345][-3.1905222 -3.2370968 -3.2654428 -3.3794937 -3.4974833 -3.4685454 -3.24076 -2.9857774 -2.8331277 -2.8042603 -2.8689089 -2.89705 -2.9228358 -3.0889947 -3.4084845][-2.8307991 -2.9279745 -3.0120788 -3.2003009 -3.4238994 -3.5450895 -3.527081 -3.4758449 -3.4738042 -3.4878058 -3.4964144 -3.4366288 -3.3957748 -3.5049918 -3.749578]]...]
INFO - root - 2017-12-07 05:15:43.397764: step 8610, loss = 0.79, batch loss = 0.71 (7.7 examples/sec; 1.038 sec/batch; 93h:23m:02s remains)
INFO - root - 2017-12-07 05:15:53.846154: step 8620, loss = 0.77, batch loss = 0.70 (7.6 examples/sec; 1.049 sec/batch; 94h:22m:23s remains)
INFO - root - 2017-12-07 05:16:04.259618: step 8630, loss = 0.79, batch loss = 0.72 (7.9 examples/sec; 1.013 sec/batch; 91h:08m:01s remains)
INFO - root - 2017-12-07 05:16:14.612340: step 8640, loss = 0.66, batch loss = 0.59 (7.6 examples/sec; 1.058 sec/batch; 95h:08m:05s remains)
INFO - root - 2017-12-07 05:16:25.063532: step 8650, loss = 0.56, batch loss = 0.49 (7.4 examples/sec; 1.080 sec/batch; 97h:10m:36s remains)
INFO - root - 2017-12-07 05:16:35.471976: step 8660, loss = 0.81, batch loss = 0.74 (7.8 examples/sec; 1.031 sec/batch; 92h:43m:49s remains)
INFO - root - 2017-12-07 05:16:45.763585: step 8670, loss = 0.86, batch loss = 0.79 (7.7 examples/sec; 1.041 sec/batch; 93h:36m:16s remains)
INFO - root - 2017-12-07 05:16:55.827178: step 8680, loss = 0.76, batch loss = 0.69 (7.9 examples/sec; 1.007 sec/batch; 90h:37m:07s remains)
INFO - root - 2017-12-07 05:17:06.203556: step 8690, loss = 0.98, batch loss = 0.91 (7.6 examples/sec; 1.048 sec/batch; 94h:15m:47s remains)
INFO - root - 2017-12-07 05:17:16.459442: step 8700, loss = 0.70, batch loss = 0.63 (7.8 examples/sec; 1.026 sec/batch; 92h:17m:49s remains)
2017-12-07 05:17:17.206772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7160749 -2.569603 -2.5543509 -2.8512754 -3.0166535 -3.0060563 -2.8624797 -2.7114968 -2.8738093 -2.8835742 -2.8052607 -2.8028278 -2.879626 -3.0175681 -3.0424852][-2.3398066 -2.1510668 -2.1664186 -2.5517256 -2.7018158 -2.5385702 -2.1489909 -1.8975227 -2.2608244 -2.4479394 -2.5191712 -2.5640507 -2.6367955 -2.7478452 -2.632834][-2.0395832 -1.9548771 -2.1894929 -2.6774974 -2.7773824 -2.3384442 -1.4958026 -1.0599265 -1.6204965 -2.0907161 -2.4524312 -2.58399 -2.65348 -2.7487249 -2.489687][-1.9444954 -2.1888766 -2.8205457 -3.3686314 -3.3001323 -2.3890057 -0.81737971 -0.20754099 -1.0835145 -2.0357361 -2.7094774 -2.7431436 -2.6461811 -2.6119211 -2.209147][-2.1239476 -2.6804643 -3.5541673 -3.9273312 -3.4210043 -1.7392766 0.69019985 1.2244802 -0.32043648 -2.02376 -3.079278 -2.996866 -2.6359677 -2.2437856 -1.5475471][-2.331732 -3.014118 -3.8166652 -3.7188745 -2.591651 -0.15723276 2.8110518 2.88831 0.62809849 -1.5445678 -2.7752504 -2.75726 -2.4389222 -1.8480916 -0.87417841][-2.4921124 -3.1434932 -3.6493945 -2.8322418 -0.94735765 2.0084658 4.8924122 4.1011658 1.2428346 -0.9489007 -2.0348451 -2.0985901 -2.0630827 -1.583822 -0.56927729][-2.5901041 -3.1361861 -3.3321109 -1.8786316 0.57929945 3.6430025 6.0599918 4.6257658 1.6391296 -0.32694626 -1.2694416 -1.4640636 -1.7593153 -1.5192678 -0.63138866][-2.9206729 -3.2633185 -3.151561 -1.4333439 0.89872122 3.3352976 5.1325426 3.8371258 1.546391 0.099485874 -0.75480652 -1.1083694 -1.6751356 -1.6380477 -0.93823075][-3.5468748 -3.7269201 -3.3200808 -1.684624 0.023367882 1.5028114 2.7691112 2.0831137 0.81482983 -0.010001659 -0.73257661 -1.2061672 -1.911902 -1.9630599 -1.3916876][-4.2286434 -4.4362893 -3.9140556 -2.6309953 -1.5557864 -0.732671 0.23784351 0.088635445 -0.4907198 -0.83729148 -1.3151834 -1.8373988 -2.5096643 -2.4921284 -1.9197469][-4.6670389 -4.8937187 -4.4134436 -3.6411216 -3.134459 -2.7238052 -1.9585402 -1.7541263 -1.8934622 -1.829855 -1.976702 -2.4751532 -3.0245643 -2.9337726 -2.3693187][-4.7441111 -4.8303423 -4.3582788 -3.9976516 -3.9150476 -3.7938595 -3.2971244 -2.9435182 -2.8071373 -2.5170536 -2.4917116 -2.9398794 -3.3197565 -3.1557827 -2.6470442][-4.4300179 -4.354773 -3.9096403 -3.8289533 -4.0140257 -4.061357 -3.7824337 -3.3820405 -3.0847702 -2.7877176 -2.7778809 -3.172123 -3.4095025 -3.2112203 -2.8028245][-4.0088177 -3.7993691 -3.4141827 -3.4839137 -3.8089979 -3.9552646 -3.8053606 -3.4442871 -3.1077509 -2.90759 -2.9821663 -3.3031886 -3.4481888 -3.2663507 -2.9686239]]...]
INFO - root - 2017-12-07 05:17:27.614439: step 8710, loss = 0.92, batch loss = 0.85 (7.5 examples/sec; 1.063 sec/batch; 95h:33m:56s remains)
INFO - root - 2017-12-07 05:17:37.956991: step 8720, loss = 0.87, batch loss = 0.80 (8.0 examples/sec; 1.003 sec/batch; 90h:11m:22s remains)
INFO - root - 2017-12-07 05:17:48.303141: step 8730, loss = 1.00, batch loss = 0.93 (8.1 examples/sec; 0.992 sec/batch; 89h:12m:19s remains)
INFO - root - 2017-12-07 05:17:58.454766: step 8740, loss = 0.87, batch loss = 0.80 (7.7 examples/sec; 1.040 sec/batch; 93h:31m:26s remains)
INFO - root - 2017-12-07 05:18:08.679135: step 8750, loss = 0.92, batch loss = 0.85 (7.5 examples/sec; 1.063 sec/batch; 95h:38m:12s remains)
INFO - root - 2017-12-07 05:18:18.878353: step 8760, loss = 0.84, batch loss = 0.77 (7.7 examples/sec; 1.043 sec/batch; 93h:47m:57s remains)
INFO - root - 2017-12-07 05:18:29.161238: step 8770, loss = 0.75, batch loss = 0.68 (7.7 examples/sec; 1.033 sec/batch; 92h:53m:31s remains)
INFO - root - 2017-12-07 05:18:39.255790: step 8780, loss = 0.85, batch loss = 0.78 (7.8 examples/sec; 1.027 sec/batch; 92h:20m:15s remains)
INFO - root - 2017-12-07 05:18:49.544478: step 8790, loss = 0.63, batch loss = 0.56 (7.7 examples/sec; 1.043 sec/batch; 93h:48m:06s remains)
INFO - root - 2017-12-07 05:18:59.906253: step 8800, loss = 0.79, batch loss = 0.71 (7.8 examples/sec; 1.021 sec/batch; 91h:48m:56s remains)
2017-12-07 05:19:00.703878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1366796 -2.9840238 -2.8464773 -2.6251049 -2.4386992 -2.4823904 -2.5807476 -2.6944456 -2.9141784 -3.0927019 -2.9888954 -2.7031507 -2.4972148 -2.4364352 -2.5986571][-3.4212382 -3.2802179 -3.1214795 -2.8472562 -2.6550193 -2.7521272 -2.8563066 -2.9461257 -3.1822476 -3.3459506 -3.1528015 -2.8977571 -2.9370046 -3.1827126 -3.5115633][-3.721602 -3.597167 -3.4531875 -3.1907363 -3.0497341 -3.2463613 -3.3756418 -3.4061906 -3.5373108 -3.5542734 -3.2300959 -3.0699253 -3.4350064 -4.0086637 -4.4562154][-3.9201136 -3.7230186 -3.4774117 -3.1541338 -3.053875 -3.3401377 -3.4728215 -3.3870564 -3.3586285 -3.2685199 -2.9489822 -2.9633832 -3.544611 -4.1750345 -4.5178523][-3.779635 -3.5041902 -3.1863537 -2.8379784 -2.7508793 -3.0059347 -2.9679065 -2.5839491 -2.3988 -2.4340634 -2.3926558 -2.71113 -3.3334379 -3.7235878 -3.7639606][-3.5510857 -3.4043503 -3.2348385 -2.9688854 -2.8234272 -2.8309774 -2.396991 -1.6824479 -1.5924647 -1.9997497 -2.3380349 -2.8945284 -3.3699734 -3.4290175 -3.2752886][-3.3490767 -3.3252192 -3.2613831 -2.9709537 -2.6443439 -2.2883763 -1.4905686 -0.67442393 -0.93674278 -1.790014 -2.357429 -2.8649564 -3.1108503 -3.0465512 -3.0355415][-3.1015429 -3.0938134 -3.013351 -2.63806 -2.1545067 -1.5955231 -0.75817132 -0.11418581 -0.63103938 -1.6467686 -2.2481728 -2.6203091 -2.6871328 -2.606842 -2.807189][-3.1480341 -3.213295 -3.1364956 -2.7508109 -2.2615774 -1.7591877 -1.2360749 -0.925472 -1.349597 -2.1163096 -2.6308668 -3.0053363 -3.1708627 -3.2269766 -3.4983172][-3.677022 -3.8744452 -3.7978852 -3.483619 -3.093771 -2.7342377 -2.4973009 -2.3823614 -2.6404672 -3.197201 -3.6954045 -4.1419139 -4.4360175 -4.6050539 -4.7665105][-4.2124767 -4.5669866 -4.6465278 -4.6004715 -4.424644 -4.2157974 -4.10869 -4.077086 -4.2575631 -4.7537832 -5.3010283 -5.7669468 -6.0221066 -6.073493 -5.88778][-4.4761333 -4.9077063 -5.1321983 -5.28434 -5.2495551 -5.1584282 -5.1202922 -5.1225839 -5.2243161 -5.5527496 -5.9887104 -6.3546391 -6.5030537 -6.448936 -6.0959172][-4.4755144 -4.846621 -5.0604706 -5.209393 -5.2039471 -5.1560984 -5.1088924 -5.089252 -5.1209512 -5.2656026 -5.5188422 -5.7376685 -5.8288283 -5.7965932 -5.5452414][-4.2698731 -4.5249834 -4.7059917 -4.8638506 -4.9255509 -4.9311566 -4.914947 -4.9029031 -4.9208875 -4.9520607 -5.0411768 -5.0926061 -5.0628119 -4.9811859 -4.8430772][-4.0264072 -4.1688323 -4.3165569 -4.4658413 -4.5507174 -4.5794086 -4.593761 -4.5929909 -4.583652 -4.534831 -4.5131831 -4.4894657 -4.4339371 -4.3785896 -4.3543615]]...]
INFO - root - 2017-12-07 05:19:10.949688: step 8810, loss = 0.78, batch loss = 0.71 (7.8 examples/sec; 1.029 sec/batch; 92h:32m:00s remains)
INFO - root - 2017-12-07 05:19:21.267117: step 8820, loss = 0.70, batch loss = 0.63 (8.2 examples/sec; 0.980 sec/batch; 88h:07m:46s remains)
INFO - root - 2017-12-07 05:19:31.593476: step 8830, loss = 0.76, batch loss = 0.69 (7.7 examples/sec; 1.033 sec/batch; 92h:53m:57s remains)
INFO - root - 2017-12-07 05:19:41.928789: step 8840, loss = 0.73, batch loss = 0.65 (7.8 examples/sec; 1.031 sec/batch; 92h:41m:35s remains)
INFO - root - 2017-12-07 05:19:52.115572: step 8850, loss = 0.92, batch loss = 0.84 (7.9 examples/sec; 1.014 sec/batch; 91h:08m:59s remains)
INFO - root - 2017-12-07 05:20:02.405961: step 8860, loss = 0.72, batch loss = 0.65 (7.8 examples/sec; 1.023 sec/batch; 91h:55m:53s remains)
INFO - root - 2017-12-07 05:20:12.794905: step 8870, loss = 0.88, batch loss = 0.81 (7.6 examples/sec; 1.050 sec/batch; 94h:25m:54s remains)
INFO - root - 2017-12-07 05:20:22.987253: step 8880, loss = 0.82, batch loss = 0.75 (7.9 examples/sec; 1.012 sec/batch; 91h:00m:08s remains)
INFO - root - 2017-12-07 05:20:33.121602: step 8890, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.047 sec/batch; 94h:05m:30s remains)
INFO - root - 2017-12-07 05:20:43.461116: step 8900, loss = 0.66, batch loss = 0.59 (8.1 examples/sec; 0.990 sec/batch; 89h:01m:19s remains)
2017-12-07 05:20:44.274675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5870676 -2.5148249 -2.2187598 -2.0708532 -2.3490734 -2.777101 -3.1869049 -3.4675415 -3.4648004 -3.4696493 -3.6837494 -3.9040194 -3.9309022 -3.821861 -3.5793676][-2.7251043 -2.689028 -2.475503 -2.4438305 -2.837867 -3.3450074 -3.7005873 -3.9070575 -3.9365776 -3.9177387 -3.9672728 -4.0063787 -3.9264512 -3.8408959 -3.7165675][-2.773093 -2.6116514 -2.3972266 -2.3468812 -2.6376507 -3.0577161 -3.3592262 -3.5292616 -3.6192763 -3.6414099 -3.7012167 -3.7727554 -3.7413833 -3.719336 -3.637197][-2.8728056 -2.7525361 -2.636374 -2.5376382 -2.5763693 -2.7099185 -2.811866 -2.8787706 -2.9807508 -3.0634835 -3.2500544 -3.477757 -3.5438747 -3.500953 -3.2963977][-2.8383732 -2.8679624 -2.9594493 -2.9277601 -2.8329725 -2.7243326 -2.5460172 -2.3569627 -2.2705905 -2.2367911 -2.3372459 -2.4990687 -2.6029541 -2.677711 -2.6026263][-2.7679043 -2.7688637 -2.8699355 -2.8684428 -2.7831519 -2.6480346 -2.3350582 -1.945997 -1.7231569 -1.6490955 -1.6555583 -1.672492 -1.7724597 -1.9311466 -1.8888936][-2.5230784 -2.3470452 -2.2602489 -2.1247449 -1.8830619 -1.5953381 -1.100759 -0.61820531 -0.59204745 -0.91719151 -1.2204704 -1.3745348 -1.6656876 -1.9736633 -1.8673172][-2.192034 -1.7921226 -1.5181458 -1.2352116 -0.775007 -0.24316835 0.47999811 1.0745096 0.85762405 0.11707497 -0.48648548 -0.81292248 -1.3356652 -1.8070447 -1.76474][-2.2563145 -1.8117995 -1.4207003 -1.0544116 -0.61618614 -0.10749912 0.62124538 1.1821671 0.77322626 -0.11937666 -0.629472 -0.78997874 -1.1895678 -1.5767868 -1.5968018][-2.2334661 -1.9601221 -1.643841 -1.4082475 -1.1678836 -0.79168868 -0.16144037 0.24397993 -0.24224949 -1.0429041 -1.2970569 -1.2630901 -1.4188814 -1.5594051 -1.5530879][-2.1580894 -2.2072973 -2.1156542 -2.0122631 -1.786082 -1.3245173 -0.64105296 -0.33115578 -0.80373454 -1.4232314 -1.4891033 -1.3952229 -1.4574444 -1.4652891 -1.4596043][-2.4378657 -2.6886909 -2.6378131 -2.4049361 -1.9775558 -1.402137 -0.79735327 -0.66234016 -1.121573 -1.5976536 -1.6607027 -1.6502707 -1.6592224 -1.514822 -1.4340076][-2.7097714 -2.8462749 -2.5617332 -2.0793633 -1.5547569 -1.1137228 -0.80990696 -0.89498234 -1.3192756 -1.6522083 -1.71434 -1.7419617 -1.663434 -1.3397999 -1.1720567][-2.4251108 -2.3686585 -1.8490512 -1.2820382 -0.95917344 -0.90790987 -0.9548161 -1.2238493 -1.5885196 -1.7183256 -1.640996 -1.619251 -1.4763808 -1.0326002 -0.827477][-1.8340321 -1.6566098 -1.2283132 -1.0366955 -1.1471341 -1.3752494 -1.4555089 -1.6485388 -1.8825495 -1.8116653 -1.5755706 -1.5181131 -1.4388216 -1.017112 -0.78819728]]...]
INFO - root - 2017-12-07 05:20:54.696351: step 8910, loss = 0.81, batch loss = 0.73 (8.1 examples/sec; 0.993 sec/batch; 89h:13m:34s remains)
INFO - root - 2017-12-07 05:21:05.036975: step 8920, loss = 0.77, batch loss = 0.70 (7.8 examples/sec; 1.029 sec/batch; 92h:27m:49s remains)
INFO - root - 2017-12-07 05:21:15.347339: step 8930, loss = 0.78, batch loss = 0.71 (7.6 examples/sec; 1.046 sec/batch; 94h:00m:54s remains)
INFO - root - 2017-12-07 05:21:25.562183: step 8940, loss = 0.74, batch loss = 0.67 (7.8 examples/sec; 1.023 sec/batch; 91h:54m:06s remains)
INFO - root - 2017-12-07 05:21:35.780920: step 8950, loss = 0.80, batch loss = 0.73 (8.3 examples/sec; 0.966 sec/batch; 86h:49m:28s remains)
INFO - root - 2017-12-07 05:21:46.091887: step 8960, loss = 0.83, batch loss = 0.76 (7.8 examples/sec; 1.021 sec/batch; 91h:43m:40s remains)
INFO - root - 2017-12-07 05:21:56.328819: step 8970, loss = 0.84, batch loss = 0.77 (7.8 examples/sec; 1.032 sec/batch; 92h:44m:53s remains)
INFO - root - 2017-12-07 05:22:06.504269: step 8980, loss = 0.78, batch loss = 0.70 (8.0 examples/sec; 1.001 sec/batch; 89h:59m:01s remains)
INFO - root - 2017-12-07 05:22:16.844165: step 8990, loss = 0.76, batch loss = 0.69 (7.9 examples/sec; 1.016 sec/batch; 91h:17m:06s remains)
INFO - root - 2017-12-07 05:22:27.308539: step 9000, loss = 0.82, batch loss = 0.75 (7.9 examples/sec; 1.017 sec/batch; 91h:23m:52s remains)
2017-12-07 05:22:28.080217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.46146464 -0.34887838 -0.69719028 -0.94810557 -1.1371193 -1.4539881 -1.6955602 -1.7572308 -1.9250832 -2.1026273 -2.109746 -2.2361405 -2.467145 -2.6175358 -2.6513805][-0.76099777 -0.7010076 -1.0616848 -1.2352777 -1.2255437 -1.3581588 -1.5341325 -1.6390314 -1.9119248 -2.1284676 -2.0648313 -2.078284 -2.2404761 -2.4354596 -2.5487967][-1.1243603 -1.0927262 -1.4563308 -1.6739604 -1.6077547 -1.5767729 -1.627284 -1.6325479 -1.789464 -1.8703444 -1.6256974 -1.4906087 -1.681052 -2.1278915 -2.4698133][-1.4014413 -1.297405 -1.5439496 -1.7993414 -1.8149314 -1.7839832 -1.855942 -1.9000356 -2.0783241 -2.1030908 -1.6469772 -1.1409025 -1.0849593 -1.6352122 -2.2314091][-1.4364963 -1.3484001 -1.4916761 -1.649652 -1.5674276 -1.3952677 -1.3863778 -1.4896858 -1.8797224 -2.1464126 -1.8187776 -1.2300837 -0.994678 -1.4582052 -2.1032679][-1.1781511 -1.1826859 -1.4492824 -1.8267663 -1.851784 -1.5343962 -1.1740086 -0.99131608 -1.3175564 -1.7089579 -1.589699 -1.147696 -1.0057638 -1.4607174 -2.1148405][-1.0880301 -1.1194983 -1.4412458 -2.0456386 -2.2718513 -1.9089794 -1.2109728 -0.71571159 -1.0845554 -1.8367722 -2.035758 -1.6320448 -1.3006678 -1.4575505 -1.9647629][-1.3499734 -1.4194679 -1.6722786 -2.2572439 -2.4972107 -1.9575088 -0.80373907 0.15840101 -0.26753426 -1.6665096 -2.541033 -2.3906019 -1.8412094 -1.618367 -1.8896127][-1.7941802 -1.740257 -1.7603509 -2.2385578 -2.6693935 -2.4365468 -1.3148825 0.041679382 0.15797091 -1.0826442 -2.1969185 -2.3443224 -1.9045315 -1.6751664 -1.9670994][-2.0749767 -1.950305 -1.6717215 -1.8655565 -2.3854992 -2.6036785 -2.0445075 -0.93078208 -0.42076206 -1.125982 -2.0704851 -2.2581072 -1.7857034 -1.436975 -1.6918013][-2.0635703 -2.1185155 -1.8062375 -1.7198372 -2.0089924 -2.2267609 -1.9509835 -1.1725421 -0.715935 -1.2550766 -2.1383519 -2.3626075 -1.8878477 -1.3963659 -1.4511166][-2.004045 -2.2027421 -2.0011625 -1.8354154 -1.9944823 -2.2273581 -2.1476035 -1.5800085 -1.1198936 -1.4186614 -2.038892 -2.1584334 -1.7380872 -1.3658314 -1.4278228][-1.9729621 -2.2253473 -2.1538978 -2.0393786 -2.1710806 -2.4234252 -2.5691442 -2.2926867 -1.9324701 -2.0383878 -2.3181393 -2.132931 -1.5813191 -1.2497039 -1.3431437][-1.9371552 -2.1866241 -2.2112179 -2.1567607 -2.205842 -2.3072109 -2.4480977 -2.3536317 -2.1775041 -2.3116922 -2.52311 -2.2921481 -1.7575932 -1.4491274 -1.4528642][-2.1383536 -2.3073192 -2.3365073 -2.3205817 -2.3409295 -2.3591273 -2.4236128 -2.3602617 -2.2231843 -2.291198 -2.4554296 -2.3221314 -1.9527745 -1.7017 -1.5958066]]...]
INFO - root - 2017-12-07 05:22:38.492811: step 9010, loss = 0.99, batch loss = 0.92 (7.9 examples/sec; 1.009 sec/batch; 90h:41m:53s remains)
INFO - root - 2017-12-07 05:22:48.961381: step 9020, loss = 0.83, batch loss = 0.75 (7.7 examples/sec; 1.040 sec/batch; 93h:24m:54s remains)
INFO - root - 2017-12-07 05:22:59.427532: step 9030, loss = 0.78, batch loss = 0.70 (7.7 examples/sec; 1.040 sec/batch; 93h:26m:21s remains)
INFO - root - 2017-12-07 05:23:09.628840: step 9040, loss = 0.71, batch loss = 0.63 (7.8 examples/sec; 1.023 sec/batch; 91h:54m:49s remains)
INFO - root - 2017-12-07 05:23:20.072410: step 9050, loss = 0.83, batch loss = 0.75 (7.6 examples/sec; 1.059 sec/batch; 95h:10m:00s remains)
INFO - root - 2017-12-07 05:23:30.532753: step 9060, loss = 0.95, batch loss = 0.88 (7.4 examples/sec; 1.083 sec/batch; 97h:17m:35s remains)
INFO - root - 2017-12-07 05:23:40.780290: step 9070, loss = 0.85, batch loss = 0.78 (8.1 examples/sec; 0.991 sec/batch; 89h:01m:18s remains)
INFO - root - 2017-12-07 05:23:50.828301: step 9080, loss = 0.85, batch loss = 0.78 (7.9 examples/sec; 1.015 sec/batch; 91h:12m:55s remains)
INFO - root - 2017-12-07 05:24:01.074694: step 9090, loss = 0.86, batch loss = 0.79 (7.7 examples/sec; 1.037 sec/batch; 93h:10m:24s remains)
INFO - root - 2017-12-07 05:24:11.301527: step 9100, loss = 0.79, batch loss = 0.72 (7.5 examples/sec; 1.061 sec/batch; 95h:18m:22s remains)
2017-12-07 05:24:12.050727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4821339 -2.6243553 -2.5737658 -2.1834123 -1.7057881 -1.601542 -2.0599806 -2.6167231 -2.8138852 -2.7582989 -2.6475015 -2.587115 -2.6477149 -2.8680358 -3.2454233][-2.5840931 -2.7229266 -2.6008992 -2.0208154 -1.340091 -1.0977554 -1.5395133 -2.2453191 -2.6560392 -2.8062258 -2.784256 -2.7171164 -2.723218 -2.8023887 -2.9751244][-2.6698413 -2.8141184 -2.6211371 -1.915467 -1.1446936 -0.81405878 -1.119415 -1.8268683 -2.3760874 -2.6635795 -2.7273908 -2.798811 -2.8995609 -2.7889729 -2.5718997][-2.8378968 -3.0029068 -2.7269659 -1.9271574 -1.1543815 -0.79096651 -0.88633323 -1.4777787 -2.1087615 -2.4647486 -2.5483215 -2.6699002 -2.7872043 -2.5090275 -2.0026767][-2.9982865 -3.161921 -2.8184273 -1.9609647 -1.1877522 -0.76177406 -0.61890793 -1.0343695 -1.7182174 -2.2174461 -2.4481828 -2.6332662 -2.6694856 -2.3032961 -1.7801545][-3.0750575 -3.2840126 -3.0186186 -2.2405231 -1.449173 -0.79016185 -0.2013607 -0.26677513 -0.97284341 -1.7920446 -2.4347296 -2.8526344 -2.9026828 -2.5372398 -2.0639815][-3.1042156 -3.448384 -3.361526 -2.7139606 -1.843128 -0.76103783 0.45334339 0.84272003 0.095615864 -1.1270456 -2.2308621 -2.907002 -3.0909638 -2.8590827 -2.4502139][-3.0325952 -3.514365 -3.6167789 -3.1431642 -2.2716532 -0.79359388 1.0372548 1.9111419 1.1729665 -0.33688831 -1.6825914 -2.4818549 -2.8297212 -2.8411288 -2.5426683][-2.8890471 -3.4490137 -3.7081137 -3.4667063 -2.7190461 -1.0547888 1.087728 2.2752452 1.615499 0.082810879 -1.2611105 -2.0394964 -2.4321966 -2.5786786 -2.3702219][-2.7827897 -3.3468175 -3.663945 -3.5814657 -2.9647 -1.3692703 0.59306431 1.6037745 1.03545 -0.25246191 -1.4422231 -2.1257741 -2.4453144 -2.5790088 -2.4270718][-2.7613964 -3.2966771 -3.5904171 -3.533602 -2.9745276 -1.5868673 -0.094434738 0.48152161 -0.037978649 -0.98301244 -1.9340119 -2.5109477 -2.742362 -2.833962 -2.7189221][-2.7390518 -3.2452202 -3.4856722 -3.3548698 -2.8199377 -1.6747909 -0.5942812 -0.32636642 -0.78591275 -1.5065353 -2.2616355 -2.6843319 -2.8050141 -2.8546419 -2.7393298][-2.6525607 -3.1212797 -3.2999725 -3.087944 -2.6108637 -1.7265344 -0.87687421 -0.65435171 -1.0442433 -1.7526267 -2.4895296 -2.7984657 -2.7707739 -2.7214088 -2.5465612][-2.5552011 -2.967804 -3.1171937 -2.9157305 -2.5869255 -1.9783757 -1.2610438 -0.9496944 -1.2154953 -1.8901517 -2.6389003 -2.9456897 -2.8738446 -2.7192535 -2.4460063][-2.4921951 -2.8725595 -3.092253 -3.0106969 -2.8441219 -2.4570532 -1.8490267 -1.5084269 -1.6418147 -2.1513646 -2.8015161 -3.1280303 -3.1184931 -2.9023118 -2.5506928]]...]
INFO - root - 2017-12-07 05:24:22.395165: step 9110, loss = 0.72, batch loss = 0.65 (7.9 examples/sec; 1.015 sec/batch; 91h:12m:23s remains)
INFO - root - 2017-12-07 05:24:32.787099: step 9120, loss = 0.66, batch loss = 0.58 (7.8 examples/sec; 1.028 sec/batch; 92h:18m:55s remains)
INFO - root - 2017-12-07 05:24:43.147380: step 9130, loss = 0.71, batch loss = 0.64 (7.8 examples/sec; 1.026 sec/batch; 92h:08m:04s remains)
INFO - root - 2017-12-07 05:24:53.581669: step 9140, loss = 0.86, batch loss = 0.78 (7.7 examples/sec; 1.032 sec/batch; 92h:44m:10s remains)
INFO - root - 2017-12-07 05:25:03.854241: step 9150, loss = 0.70, batch loss = 0.63 (7.8 examples/sec; 1.028 sec/batch; 92h:18m:52s remains)
INFO - root - 2017-12-07 05:25:14.192461: step 9160, loss = 0.63, batch loss = 0.56 (7.5 examples/sec; 1.064 sec/batch; 95h:31m:20s remains)
INFO - root - 2017-12-07 05:25:24.520028: step 9170, loss = 0.79, batch loss = 0.72 (7.8 examples/sec; 1.028 sec/batch; 92h:21m:39s remains)
INFO - root - 2017-12-07 05:25:34.436484: step 9180, loss = 0.85, batch loss = 0.77 (7.9 examples/sec; 1.009 sec/batch; 90h:36m:35s remains)
INFO - root - 2017-12-07 05:25:44.584062: step 9190, loss = 0.82, batch loss = 0.75 (8.0 examples/sec; 1.002 sec/batch; 89h:58m:23s remains)
INFO - root - 2017-12-07 05:25:55.044625: step 9200, loss = 0.75, batch loss = 0.68 (7.2 examples/sec; 1.104 sec/batch; 99h:08m:05s remains)
2017-12-07 05:25:55.827168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3456018 -3.3555987 -3.2239473 -3.2258754 -3.2957668 -3.1940892 -3.1944747 -3.3262026 -3.2647772 -3.21981 -3.1619167 -2.9800677 -2.7147684 -2.4527364 -2.5535967][-2.5056405 -2.348979 -2.0797806 -2.0951519 -2.3224337 -2.3835008 -2.5201817 -2.7137909 -2.6202841 -2.5001101 -2.3449998 -2.1405447 -1.9678533 -1.9910617 -2.456295][-1.6803896 -1.4339685 -1.03163 -1.0224133 -1.3911247 -1.599618 -1.8174546 -1.9940901 -1.8881369 -1.721411 -1.5423198 -1.435544 -1.4062014 -1.7421188 -2.5433047][-1.2290924 -0.96136689 -0.53104019 -0.54429317 -1.0146031 -1.3088598 -1.4362404 -1.3858635 -1.1761599 -1.0371478 -0.97860742 -1.0678928 -1.1391125 -1.6007268 -2.5068378][-1.1401553 -0.97270226 -0.633682 -0.67947292 -1.1457937 -1.4033961 -1.3584952 -1.0371749 -0.70577216 -0.62870479 -0.75447059 -1.0321999 -1.1248369 -1.4621005 -2.190021][-1.1332548 -1.0927989 -0.90643835 -0.93344164 -1.2098191 -1.2869232 -1.05424 -0.64906263 -0.3894968 -0.44921732 -0.74739337 -1.1600525 -1.2378168 -1.4582131 -2.0370724][-1.07879 -1.0640974 -0.92796707 -0.81505632 -0.74537969 -0.45146346 0.051336765 0.34567165 0.21662998 -0.1531086 -0.67400765 -1.1963115 -1.2958684 -1.5384009 -2.1661005][-0.77862287 -0.62834191 -0.42924213 -0.18291712 0.15116739 0.76311874 1.5147047 1.6094871 0.96552229 0.25109911 -0.50388217 -1.1180081 -1.2390471 -1.5015266 -2.170362][-0.65487313 -0.33825111 -0.019957066 0.31571102 0.59351635 1.0484824 1.6146092 1.4354835 0.52722788 -0.30442286 -0.98920918 -1.4637816 -1.5242977 -1.7081962 -2.2180808][-1.1433043 -0.81774783 -0.49012756 -0.16752481 -0.0939312 -0.05154562 0.08460474 -0.30433226 -1.1850419 -1.8958139 -2.2843142 -2.4020276 -2.3268058 -2.3820448 -2.5987198][-1.6404281 -1.4091148 -1.1282291 -0.82028294 -0.76739287 -0.88298035 -1.0124578 -1.4844334 -2.237551 -2.7808764 -2.9240742 -2.7762756 -2.6142721 -2.6296744 -2.7120769][-2.0703626 -1.8832638 -1.6190784 -1.3137872 -1.1661389 -1.2122483 -1.3934696 -1.8335853 -2.4198728 -2.7991126 -2.8484874 -2.6022639 -2.3521726 -2.3393483 -2.4935117][-2.62211 -2.4691265 -2.2408457 -2.009975 -1.8377106 -1.8240159 -1.979476 -2.3568411 -2.7926431 -3.010632 -2.9913363 -2.7325144 -2.4454384 -2.3550279 -2.5458956][-3.0290637 -2.943851 -2.8032832 -2.6882899 -2.59719 -2.6002805 -2.6934743 -2.9412124 -3.1878881 -3.2383771 -3.0929742 -2.8165565 -2.5924473 -2.5368586 -2.7475767][-3.065753 -3.0381703 -2.9829571 -2.9446301 -2.9256961 -2.9490461 -2.9668269 -3.0541613 -3.1554685 -3.1325922 -2.9382794 -2.7005529 -2.6163416 -2.7300158 -3.0525432]]...]
INFO - root - 2017-12-07 05:26:05.922985: step 9210, loss = 0.75, batch loss = 0.68 (7.7 examples/sec; 1.038 sec/batch; 93h:12m:38s remains)
INFO - root - 2017-12-07 05:26:16.187316: step 9220, loss = 0.64, batch loss = 0.57 (7.9 examples/sec; 1.018 sec/batch; 91h:22m:21s remains)
INFO - root - 2017-12-07 05:26:26.568907: step 9230, loss = 0.69, batch loss = 0.62 (7.5 examples/sec; 1.067 sec/batch; 95h:49m:12s remains)
INFO - root - 2017-12-07 05:26:36.865234: step 9240, loss = 0.90, batch loss = 0.83 (7.9 examples/sec; 1.013 sec/batch; 90h:56m:38s remains)
INFO - root - 2017-12-07 05:26:47.199090: step 9250, loss = 0.60, batch loss = 0.53 (7.6 examples/sec; 1.050 sec/batch; 94h:16m:01s remains)
INFO - root - 2017-12-07 05:26:57.545615: step 9260, loss = 0.79, batch loss = 0.72 (7.5 examples/sec; 1.072 sec/batch; 96h:13m:54s remains)
INFO - root - 2017-12-07 05:27:07.783888: step 9270, loss = 0.99, batch loss = 0.91 (7.7 examples/sec; 1.043 sec/batch; 93h:39m:50s remains)
INFO - root - 2017-12-07 05:27:17.914752: step 9280, loss = 0.91, batch loss = 0.84 (7.6 examples/sec; 1.048 sec/batch; 94h:04m:25s remains)
INFO - root - 2017-12-07 05:27:28.326852: step 9290, loss = 0.87, batch loss = 0.80 (7.6 examples/sec; 1.057 sec/batch; 94h:52m:33s remains)
INFO - root - 2017-12-07 05:27:38.741297: step 9300, loss = 0.80, batch loss = 0.72 (7.8 examples/sec; 1.027 sec/batch; 92h:14m:35s remains)
2017-12-07 05:27:39.522354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3256741 -3.2394829 -3.0443091 -2.7666154 -2.4984703 -2.3596985 -2.4276683 -2.6730042 -3.0828791 -3.455961 -3.493253 -3.1579089 -2.5787454 -2.0835142 -1.916054][-3.0350666 -2.8313589 -2.5424852 -2.2444627 -2.0292878 -2.0034807 -2.2053368 -2.5254493 -2.9527225 -3.2888615 -3.2448914 -2.8249407 -2.2254174 -1.8195324 -1.7944183][-2.7129576 -2.4160986 -2.0828855 -1.8232253 -1.7457771 -1.8790846 -2.2021399 -2.555089 -2.9489629 -3.2132225 -3.0812621 -2.6050363 -2.0151765 -1.6896813 -1.7529461][-2.6037359 -2.2965496 -1.9658754 -1.7427781 -1.74302 -1.9075906 -2.1962917 -2.4816337 -2.7809129 -2.9616923 -2.7857838 -2.3299778 -1.8194726 -1.5805588 -1.7021892][-2.6056128 -2.3324127 -2.0269661 -1.8453004 -1.8667791 -1.9756081 -2.1324012 -2.2890797 -2.4815116 -2.6093709 -2.4433744 -2.042557 -1.6343188 -1.4719729 -1.6281755][-2.6475816 -2.4153709 -2.1072969 -1.9125881 -1.8973804 -1.9236648 -1.9545486 -2.0041702 -2.1205177 -2.2390985 -2.1297331 -1.8155851 -1.5243921 -1.4368296 -1.6153255][-2.6726952 -2.5048048 -2.1577568 -1.8482497 -1.693841 -1.6099451 -1.5656598 -1.5816512 -1.6826499 -1.8248608 -1.7993982 -1.6091764 -1.4554975 -1.4551153 -1.6620204][-2.6284018 -2.5170836 -2.1532032 -1.7585251 -1.4866233 -1.3303082 -1.2940638 -1.3782604 -1.5149345 -1.6499598 -1.6578815 -1.5456886 -1.484009 -1.536114 -1.7510071][-2.4519272 -2.3542895 -2.0452394 -1.6856222 -1.3942981 -1.2594864 -1.3141513 -1.5083683 -1.6979907 -1.8162303 -1.8303695 -1.7407327 -1.6917326 -1.7286804 -1.903249][-2.1849136 -2.0858381 -1.7990775 -1.4591689 -1.1939671 -1.1634231 -1.3420799 -1.6081462 -1.8232956 -1.9560266 -2.0115242 -1.9617496 -1.9042494 -1.9028733 -2.033258][-2.1318052 -2.0416803 -1.7263708 -1.3607869 -1.1305466 -1.2144947 -1.4875305 -1.7603264 -1.9581115 -2.1056824 -2.1974788 -2.1795321 -2.0929248 -2.036454 -2.1209912][-2.1546397 -2.108124 -1.7985222 -1.4485466 -1.2828636 -1.4414089 -1.7408049 -1.9914854 -2.1976161 -2.3959491 -2.5214059 -2.4988995 -2.3551209 -2.22352 -2.2388225][-2.0532796 -2.0280342 -1.7737792 -1.5056987 -1.4129007 -1.5762029 -1.8453631 -2.0839436 -2.3356059 -2.5980167 -2.7594459 -2.7343054 -2.5442886 -2.3497996 -2.3002577][-2.0672991 -2.0163689 -1.7989304 -1.6061511 -1.56264 -1.6993141 -1.9082682 -2.1099157 -2.3494701 -2.6020415 -2.7524905 -2.7243259 -2.5223031 -2.3051066 -2.2271256][-2.2732182 -2.1914637 -1.9902132 -1.828841 -1.8006082 -1.9209154 -2.0921669 -2.2465935 -2.413269 -2.5884645 -2.675226 -2.6153116 -2.405051 -2.1837571 -2.1001351]]...]
INFO - root - 2017-12-07 05:27:50.024245: step 9310, loss = 0.72, batch loss = 0.65 (7.8 examples/sec; 1.032 sec/batch; 92h:36m:34s remains)
INFO - root - 2017-12-07 05:28:00.307848: step 9320, loss = 0.81, batch loss = 0.73 (7.7 examples/sec; 1.044 sec/batch; 93h:42m:34s remains)
INFO - root - 2017-12-07 05:28:10.697442: step 9330, loss = 0.80, batch loss = 0.73 (7.8 examples/sec; 1.030 sec/batch; 92h:30m:00s remains)
INFO - root - 2017-12-07 05:28:21.048274: step 9340, loss = 0.93, batch loss = 0.86 (7.5 examples/sec; 1.071 sec/batch; 96h:06m:33s remains)
INFO - root - 2017-12-07 05:28:31.387919: step 9350, loss = 0.74, batch loss = 0.67 (7.6 examples/sec; 1.056 sec/batch; 94h:45m:33s remains)
INFO - root - 2017-12-07 05:28:41.765644: step 9360, loss = 0.93, batch loss = 0.86 (7.7 examples/sec; 1.038 sec/batch; 93h:10m:50s remains)
INFO - root - 2017-12-07 05:28:52.050889: step 9370, loss = 1.19, batch loss = 1.11 (8.0 examples/sec; 1.003 sec/batch; 90h:00m:27s remains)
INFO - root - 2017-12-07 05:29:02.234073: step 9380, loss = 0.71, batch loss = 0.64 (7.3 examples/sec; 1.096 sec/batch; 98h:22m:41s remains)
INFO - root - 2017-12-07 05:29:12.621429: step 9390, loss = 0.72, batch loss = 0.64 (7.7 examples/sec; 1.038 sec/batch; 93h:11m:36s remains)
INFO - root - 2017-12-07 05:29:23.036910: step 9400, loss = 0.97, batch loss = 0.90 (7.7 examples/sec; 1.037 sec/batch; 93h:05m:01s remains)
2017-12-07 05:29:23.848748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1007717 -1.0711155 -0.99267936 -0.95124078 -0.99951982 -1.0592451 -1.0692091 -1.0633397 -1.0470278 -1.0289767 -1.0486317 -1.0846035 -1.1041467 -1.1016629 -1.0777056][-1.1381829 -1.1400197 -1.1104782 -1.0860026 -1.1331105 -1.2022688 -1.2204738 -1.2193236 -1.201545 -1.1855173 -1.2015467 -1.2125502 -1.1941268 -1.1581376 -1.1095624][-1.3140011 -1.3784845 -1.4613786 -1.5059373 -1.5513215 -1.5658152 -1.4953527 -1.3801262 -1.2777843 -1.2354636 -1.2774322 -1.3310895 -1.3083279 -1.2419853 -1.1579781][-1.4844353 -1.6472578 -1.8875158 -2.0329769 -2.0523214 -1.9259033 -1.6539271 -1.3368998 -1.1468253 -1.1547775 -1.297399 -1.4757102 -1.4982584 -1.3793123 -1.2128706][-1.5864451 -1.8066809 -2.1828051 -2.4276743 -2.4036796 -2.0914631 -1.5388179 -0.93463159 -0.67704654 -0.85324121 -1.2128084 -1.5886149 -1.7139168 -1.5549803 -1.2726107][-1.9959533 -2.2873967 -2.7304683 -2.9744918 -2.7898097 -2.1688511 -1.2085726 -0.15830183 0.20519161 -0.22789717 -0.9110949 -1.5171077 -1.7811511 -1.6242509 -1.2521145][-1.7639201 -2.1521418 -2.636467 -2.8672714 -2.5617471 -1.742588 -0.48847556 0.93477058 1.3095722 0.48693132 -0.60713267 -1.4532747 -1.842634 -1.7359035 -1.3380013][-1.2047341 -1.5764837 -1.9900331 -2.1411848 -1.7697439 -0.9308846 0.36832714 1.9510288 2.2095814 0.96879816 -0.48217463 -1.4793861 -1.9120359 -1.8591199 -1.4824967][-1.3422463 -1.6242757 -1.8461514 -1.8413777 -1.4146347 -0.60499668 0.62044334 2.184587 2.3563504 0.94473219 -0.62217259 -1.5996408 -1.9405413 -1.8820691 -1.5219495][-1.7210035 -1.9138932 -1.9665871 -1.8662741 -1.4730949 -0.75397277 0.27859354 1.5994411 1.6665163 0.29585218 -1.1503718 -1.9487867 -2.1038771 -1.9793622 -1.6214964][-2.4667845 -2.5135677 -2.3663123 -2.1275675 -1.7225213 -1.0478623 -0.18455076 0.80867195 0.7446413 -0.45934772 -1.638762 -2.180145 -2.1730433 -1.9996934 -1.6619487][-2.7763848 -2.7259028 -2.4599082 -2.1439724 -1.7197235 -1.0609632 -0.30448914 0.37684965 0.14014435 -0.90435672 -1.8186285 -2.1557903 -2.0880048 -1.9399958 -1.6621487][-2.3812821 -2.3522508 -2.153877 -1.9243832 -1.6035225 -1.0593493 -0.44025898 -0.027007103 -0.36929798 -1.2343171 -1.8922994 -2.0703495 -1.9849522 -1.8618157 -1.6332185][-2.096072 -2.1508336 -2.1038046 -2.0171428 -1.8126309 -1.3859227 -0.86933827 -0.594363 -0.88918233 -1.495002 -1.8831208 -1.9303689 -1.8508341 -1.7465794 -1.5620251][-1.8155422 -1.9423864 -2.0232971 -2.063633 -1.981704 -1.70398 -1.3198431 -1.1298268 -1.3115966 -1.6451759 -1.7856326 -1.7402184 -1.6868742 -1.6150315 -1.4837155]]...]
INFO - root - 2017-12-07 05:29:34.394951: step 9410, loss = 0.82, batch loss = 0.75 (7.7 examples/sec; 1.044 sec/batch; 93h:41m:32s remains)
INFO - root - 2017-12-07 05:29:44.747701: step 9420, loss = 0.72, batch loss = 0.65 (7.7 examples/sec; 1.034 sec/batch; 92h:49m:40s remains)
INFO - root - 2017-12-07 05:29:55.111303: step 9430, loss = 0.78, batch loss = 0.71 (7.9 examples/sec; 1.006 sec/batch; 90h:18m:44s remains)
INFO - root - 2017-12-07 05:30:05.412248: step 9440, loss = 0.84, batch loss = 0.77 (7.9 examples/sec; 1.012 sec/batch; 90h:46m:28s remains)
INFO - root - 2017-12-07 05:30:15.743447: step 9450, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.055 sec/batch; 94h:42m:12s remains)
INFO - root - 2017-12-07 05:30:26.126654: step 9460, loss = 0.94, batch loss = 0.87 (7.6 examples/sec; 1.048 sec/batch; 94h:00m:38s remains)
INFO - root - 2017-12-07 05:30:36.588145: step 9470, loss = 0.70, batch loss = 0.63 (7.5 examples/sec; 1.066 sec/batch; 95h:41m:12s remains)
INFO - root - 2017-12-07 05:30:46.675945: step 9480, loss = 0.78, batch loss = 0.71 (8.0 examples/sec; 0.994 sec/batch; 89h:13m:04s remains)
INFO - root - 2017-12-07 05:30:57.055339: step 9490, loss = 0.64, batch loss = 0.57 (7.8 examples/sec; 1.020 sec/batch; 91h:31m:54s remains)
INFO - root - 2017-12-07 05:31:07.490667: step 9500, loss = 0.83, batch loss = 0.76 (7.7 examples/sec; 1.046 sec/batch; 93h:49m:26s remains)
2017-12-07 05:31:08.248281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8116703 -4.2271266 -3.8014228 -3.2407403 -2.1711135 -0.73635077 0.045629025 -0.051938534 -0.22528982 -0.58568168 -1.4208577 -2.4077468 -3.6364653 -4.8821797 -5.460433][-3.8510115 -4.2596393 -3.8086374 -3.2103424 -2.1472747 -0.67408586 0.13793945 0.15923834 0.11650562 -0.2069788 -0.95923519 -1.8492119 -3.0568504 -4.3362727 -5.0274005][-3.7946272 -4.0607681 -3.5912814 -3.002651 -2.0516844 -0.65291476 0.17686319 0.32657623 0.3135438 -0.051311493 -0.85158181 -1.7964094 -3.0553718 -4.39387 -5.2006822][-3.5356123 -3.5249238 -3.0587234 -2.5491776 -1.7740858 -0.5820291 0.16253805 0.45816946 0.47136688 0.097782612 -0.67666078 -1.6464074 -2.9487185 -4.3597112 -5.3057241][-3.00879 -2.7650943 -2.2912726 -1.8182249 -1.2321055 -0.43276119 0.10261726 0.53739882 0.70570707 0.33295202 -0.39973497 -1.3541234 -2.6235385 -4.0099669 -5.0144439][-2.5996606 -2.2396145 -1.6864295 -1.090574 -0.57531571 -0.14924145 0.13542318 0.62612104 1.0617876 0.83412361 0.19069529 -0.74914289 -2.0893533 -3.5739768 -4.7498937][-2.5774369 -2.1303835 -1.3477736 -0.4016881 0.32807779 0.59660149 0.60718155 0.91952658 1.4136629 1.3555183 0.92159605 0.081942081 -1.2994747 -2.9432735 -4.4269514][-2.8821726 -2.3763132 -1.4833732 -0.27704 0.77990007 1.167376 1.0176783 1.0513983 1.3247275 1.3275971 1.0171037 0.25788403 -0.98720407 -2.5619626 -4.1850214][-2.917011 -2.4437349 -1.6475124 -0.46874619 0.62716961 1.0263739 0.68324566 0.41147995 0.36286259 0.42263031 0.34598637 -0.20467615 -1.2302527 -2.6090231 -4.1459417][-2.5217123 -2.3139007 -1.823988 -0.9129734 -0.075985432 0.1476512 -0.237566 -0.68281293 -0.93522978 -0.81777334 -0.67603135 -0.97632 -1.7123053 -2.7637691 -4.0049648][-2.0173273 -2.134047 -1.9337389 -1.2911177 -0.73625112 -0.69348955 -1.0691061 -1.5025852 -1.7341001 -1.6072443 -1.5086572 -1.7228985 -2.1102557 -2.649477 -3.424819][-1.7551918 -1.9789774 -1.8522062 -1.3198967 -0.94617009 -1.0365129 -1.4550574 -1.818007 -1.93824 -1.8135662 -1.8770428 -2.0969503 -2.1689632 -2.2622144 -2.6480422][-1.9120793 -2.1574497 -2.0844791 -1.6672935 -1.3642628 -1.4034297 -1.7598128 -2.0400646 -2.034595 -1.8665633 -1.9610338 -2.1148329 -1.9917173 -1.8652303 -2.0192559][-2.0606058 -2.2749813 -2.2817452 -2.0543027 -1.8318505 -1.793951 -2.0230575 -2.2391121 -2.2364318 -2.1171813 -2.1497884 -2.1746302 -1.9811573 -1.7826157 -1.7738082][-2.1118982 -2.2509487 -2.3115466 -2.2744417 -2.1967483 -2.148083 -2.2138131 -2.3134768 -2.3370056 -2.3139343 -2.3313231 -2.3052692 -2.1564462 -2.01227 -1.9672139]]...]
INFO - root - 2017-12-07 05:31:18.635715: step 9510, loss = 0.66, batch loss = 0.59 (7.8 examples/sec; 1.028 sec/batch; 92h:14m:11s remains)
INFO - root - 2017-12-07 05:31:28.982348: step 9520, loss = 0.86, batch loss = 0.79 (7.7 examples/sec; 1.042 sec/batch; 93h:30m:33s remains)
INFO - root - 2017-12-07 05:31:39.323832: step 9530, loss = 0.84, batch loss = 0.77 (7.8 examples/sec; 1.028 sec/batch; 92h:13m:17s remains)
INFO - root - 2017-12-07 05:31:49.665987: step 9540, loss = 0.89, batch loss = 0.82 (7.9 examples/sec; 1.015 sec/batch; 91h:03m:19s remains)
INFO - root - 2017-12-07 05:31:59.994113: step 9550, loss = 0.75, batch loss = 0.68 (7.7 examples/sec; 1.039 sec/batch; 93h:12m:10s remains)
INFO - root - 2017-12-07 05:32:10.376184: step 9560, loss = 0.93, batch loss = 0.85 (8.1 examples/sec; 0.985 sec/batch; 88h:21m:11s remains)
INFO - root - 2017-12-07 05:32:20.851284: step 9570, loss = 0.80, batch loss = 0.73 (7.6 examples/sec; 1.048 sec/batch; 94h:02m:10s remains)
INFO - root - 2017-12-07 05:32:31.091266: step 9580, loss = 0.62, batch loss = 0.55 (7.5 examples/sec; 1.065 sec/batch; 95h:31m:27s remains)
INFO - root - 2017-12-07 05:32:41.551901: step 9590, loss = 0.84, batch loss = 0.76 (7.7 examples/sec; 1.036 sec/batch; 92h:55m:42s remains)
INFO - root - 2017-12-07 05:32:51.837110: step 9600, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.048 sec/batch; 93h:57m:46s remains)
2017-12-07 05:32:52.555800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5572495 -4.569509 -4.5225525 -4.2335525 -3.6807625 -3.0561867 -2.582983 -2.3447585 -2.4179051 -2.506289 -2.387151 -1.912255 -1.203656 -0.5599699 -0.19092131][-4.249455 -4.3831325 -4.5089097 -4.2284222 -3.4800913 -2.637481 -2.1070843 -1.9729075 -2.3047056 -2.6908903 -2.7317967 -2.2825959 -1.4859436 -0.6737206 -0.18184662][-3.744405 -4.0508881 -4.370698 -4.1734772 -3.3997402 -2.532706 -2.0185199 -1.9129426 -2.2873578 -2.7594008 -2.8995357 -2.6109557 -1.9830012 -1.2559607 -0.90238976][-2.8875942 -3.3661945 -3.9265203 -4.0000639 -3.4917314 -2.8371387 -2.4391727 -2.3788669 -2.69283 -3.0986202 -3.2897363 -3.1920562 -2.7306571 -2.1418154 -2.0534191][-1.5196097 -2.1185026 -2.9626451 -3.5186875 -3.4586911 -3.0290587 -2.7259104 -2.7495866 -3.0482063 -3.457922 -3.8544426 -3.9826474 -3.5873613 -3.077352 -3.1933863][-0.25701475 -0.83519268 -1.864382 -2.8505492 -3.1247334 -2.7036824 -2.3603275 -2.4451785 -2.7747936 -3.3163228 -4.119215 -4.5432186 -4.2365389 -3.8607225 -4.0958343][0.064027786 -0.34725571 -1.2868395 -2.3711565 -2.6757131 -2.1377821 -1.7300744 -1.8131623 -2.1098886 -2.7428222 -3.8545597 -4.42525 -4.2222257 -4.1029873 -4.420794][-0.42181587 -0.67171979 -1.3073957 -2.0938601 -2.1533875 -1.5402083 -1.1395216 -1.1717684 -1.3596423 -2.0061033 -3.216053 -3.7364969 -3.6454487 -3.8239369 -4.20206][-0.93810034 -1.0684943 -1.3483856 -1.6724436 -1.4749315 -0.97446561 -0.69430876 -0.657789 -0.767869 -1.3874063 -2.4918761 -2.8402412 -2.8711338 -3.3092375 -3.7571216][-1.1816759 -1.1762493 -1.153214 -1.0873473 -0.79886317 -0.55949259 -0.46168256 -0.40301609 -0.50581217 -1.0754914 -2.0080905 -2.2324784 -2.3716996 -2.937871 -3.4028149][-1.6616442 -1.5244651 -1.3133249 -0.96288848 -0.73528886 -0.83418584 -0.9637723 -0.92091084 -0.9579134 -1.372617 -2.0301871 -2.1368299 -2.3016496 -2.8736959 -3.3014395][-2.5697265 -2.391098 -2.0746043 -1.5370734 -1.4085729 -1.8167686 -2.1312664 -2.1415021 -2.1098855 -2.3610203 -2.6862645 -2.5700223 -2.6333547 -3.0720134 -3.3985338][-3.4347467 -3.3418019 -2.992352 -2.367435 -2.2892952 -2.796314 -3.1216409 -3.1635501 -3.168021 -3.3647823 -3.4670672 -3.1534593 -3.0824022 -3.2881513 -3.4594028][-3.6228244 -3.6178231 -3.3237174 -2.8216732 -2.8257408 -3.2150352 -3.3794119 -3.4179955 -3.5340972 -3.7730587 -3.7831554 -3.4493709 -3.3370717 -3.343133 -3.342216][-3.2828302 -3.3499537 -3.2023554 -2.9657235 -3.0667033 -3.2385712 -3.1690927 -3.1725049 -3.407722 -3.7221887 -3.769479 -3.5912132 -3.5396554 -3.4271235 -3.3049693]]...]
INFO - root - 2017-12-07 05:33:02.911928: step 9610, loss = 0.78, batch loss = 0.71 (7.8 examples/sec; 1.030 sec/batch; 92h:24m:03s remains)
INFO - root - 2017-12-07 05:33:13.311239: step 9620, loss = 0.83, batch loss = 0.76 (7.4 examples/sec; 1.079 sec/batch; 96h:46m:07s remains)
INFO - root - 2017-12-07 05:33:23.644937: step 9630, loss = 0.76, batch loss = 0.69 (7.9 examples/sec; 1.008 sec/batch; 90h:21m:53s remains)
INFO - root - 2017-12-07 05:33:33.902265: step 9640, loss = 0.84, batch loss = 0.76 (7.6 examples/sec; 1.047 sec/batch; 93h:51m:21s remains)
INFO - root - 2017-12-07 05:33:44.071348: step 9650, loss = 0.73, batch loss = 0.66 (7.6 examples/sec; 1.048 sec/batch; 93h:56m:44s remains)
INFO - root - 2017-12-07 05:33:54.344272: step 9660, loss = 0.81, batch loss = 0.73 (7.7 examples/sec; 1.037 sec/batch; 92h:59m:45s remains)
INFO - root - 2017-12-07 05:34:04.703642: step 9670, loss = 0.78, batch loss = 0.70 (7.7 examples/sec; 1.034 sec/batch; 92h:41m:46s remains)
INFO - root - 2017-12-07 05:34:14.742933: step 9680, loss = 0.84, batch loss = 0.77 (7.9 examples/sec; 1.015 sec/batch; 91h:00m:21s remains)
INFO - root - 2017-12-07 05:34:24.862920: step 9690, loss = 1.04, batch loss = 0.97 (7.8 examples/sec; 1.024 sec/batch; 91h:50m:07s remains)
INFO - root - 2017-12-07 05:34:35.123382: step 9700, loss = 0.82, batch loss = 0.75 (7.7 examples/sec; 1.036 sec/batch; 92h:53m:10s remains)
2017-12-07 05:34:35.869402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9812171 -3.9028621 -3.8633454 -4.0011239 -4.2386718 -4.4203358 -4.3475828 -4.1444221 -4.0767379 -4.1223044 -4.2695708 -4.3404379 -4.1857076 -3.9819109 -3.7942855][-3.3449025 -3.2856984 -3.2342303 -3.4822042 -3.8678226 -4.1620159 -4.0791688 -3.8506484 -3.8175762 -3.9899795 -4.3826985 -4.6359982 -4.5077567 -4.2260151 -3.9512386][-2.6465836 -2.5173254 -2.4461746 -2.7444811 -3.1179242 -3.376709 -3.1876402 -2.8770947 -2.9042931 -3.3146381 -4.09263 -4.640903 -4.6364493 -4.348536 -4.0673194][-2.1052949 -1.726676 -1.6023273 -1.9336975 -2.2232976 -2.3357894 -1.928755 -1.4117444 -1.5179234 -2.2988205 -3.5817106 -4.4849811 -4.6414328 -4.3901515 -4.1401467][-1.8572488 -1.1676447 -0.91304064 -1.1867185 -1.3531423 -1.3251388 -0.74593043 -0.10088778 -0.32808638 -1.4233835 -3.1148524 -4.331984 -4.6693873 -4.4636564 -4.1926932][-2.363348 -1.5155687 -0.98795104 -0.8817966 -0.63380241 -0.27767277 0.49836779 1.0484004 0.51497078 -0.84136391 -2.7417471 -4.0929017 -4.5695386 -4.4382949 -4.1685047][-3.3395252 -2.4331279 -1.678113 -1.1008472 -0.27561474 0.68724012 1.8748741 2.25204 1.2174911 -0.49552608 -2.4828496 -3.7826943 -4.2749596 -4.2551603 -4.0946813][-3.7046247 -2.9106946 -2.2195554 -1.5596495 -0.59810328 0.68956518 2.2832599 2.7375321 1.4553466 -0.45042706 -2.4650097 -3.6686254 -4.0692163 -4.084518 -4.0407033][-3.3467956 -2.7609062 -2.3075085 -1.9318762 -1.3520818 -0.3995657 0.997447 1.7204032 0.945065 -0.63005924 -2.4653025 -3.6005602 -3.938169 -3.966687 -4.0060477][-2.8922899 -2.5061836 -2.2684247 -2.1608987 -1.9610579 -1.4917381 -0.49434519 0.33856916 0.05597353 -1.0330706 -2.4821467 -3.4813774 -3.8345194 -3.9259973 -3.9890764][-2.3654847 -2.2353559 -2.2018633 -2.2212605 -2.161736 -1.9439352 -1.2835262 -0.61520028 -0.85001063 -1.6925774 -2.73563 -3.4777777 -3.795387 -3.9056897 -3.938715][-2.1928155 -2.2506561 -2.4000492 -2.5135965 -2.5098374 -2.3515894 -1.829879 -1.3816311 -1.7579451 -2.5284612 -3.2533255 -3.6923232 -3.8312397 -3.8447111 -3.83219][-2.7003193 -2.8143785 -3.026581 -3.1856172 -3.2016895 -3.0347905 -2.5250401 -2.2130387 -2.6822793 -3.4275942 -3.9884477 -4.1589203 -3.9955707 -3.7923412 -3.7275825][-3.2850986 -3.3425708 -3.5297685 -3.7000971 -3.759727 -3.6602082 -3.2414238 -3.0321167 -3.5182619 -4.2043004 -4.6491585 -4.6121693 -4.1838431 -3.7962027 -3.6745007][-3.8417578 -3.8186018 -3.8932149 -3.9852543 -4.0392051 -3.9887714 -3.6680245 -3.5678623 -4.0823393 -4.7150736 -5.0283551 -4.8498149 -4.3129196 -3.8597703 -3.6864004]]...]
INFO - root - 2017-12-07 05:34:46.213375: step 9710, loss = 0.65, batch loss = 0.58 (7.9 examples/sec; 1.015 sec/batch; 91h:00m:12s remains)
INFO - root - 2017-12-07 05:34:56.404001: step 9720, loss = 0.78, batch loss = 0.71 (7.6 examples/sec; 1.053 sec/batch; 94h:26m:43s remains)
INFO - root - 2017-12-07 05:35:06.553315: step 9730, loss = 0.95, batch loss = 0.88 (8.2 examples/sec; 0.981 sec/batch; 87h:57m:57s remains)
INFO - root - 2017-12-07 05:35:16.800511: step 9740, loss = 0.75, batch loss = 0.68 (7.8 examples/sec; 1.020 sec/batch; 91h:27m:42s remains)
INFO - root - 2017-12-07 05:35:27.134467: step 9750, loss = 0.85, batch loss = 0.78 (7.7 examples/sec; 1.037 sec/batch; 92h:57m:25s remains)
INFO - root - 2017-12-07 05:35:37.464995: step 9760, loss = 0.68, batch loss = 0.61 (7.7 examples/sec; 1.038 sec/batch; 93h:02m:46s remains)
INFO - root - 2017-12-07 05:35:47.779324: step 9770, loss = 0.76, batch loss = 0.69 (7.7 examples/sec; 1.037 sec/batch; 92h:59m:05s remains)
INFO - root - 2017-12-07 05:35:57.915559: step 9780, loss = 0.83, batch loss = 0.76 (7.7 examples/sec; 1.037 sec/batch; 92h:59m:04s remains)
INFO - root - 2017-12-07 05:36:08.316735: step 9790, loss = 0.82, batch loss = 0.75 (7.4 examples/sec; 1.082 sec/batch; 97h:00m:08s remains)
INFO - root - 2017-12-07 05:36:18.655246: step 9800, loss = 0.98, batch loss = 0.91 (7.9 examples/sec; 1.014 sec/batch; 90h:52m:21s remains)
2017-12-07 05:36:19.442170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1907151 -2.1997263 -2.2093434 -2.2193937 -2.2216463 -2.1958704 -2.1520569 -2.0917416 -2.0195739 -1.9326639 -1.8423831 -1.7667942 -1.6542501 -1.4987197 -1.3888795][-3.3285332 -3.3203762 -3.3437943 -3.3848293 -3.4282484 -3.4374082 -3.4200931 -3.366518 -3.3028827 -3.2312236 -3.1489353 -3.0466588 -2.8054428 -2.3988979 -2.0097258][-3.921669 -3.8723893 -3.8705721 -3.9070096 -3.985328 -4.0545263 -4.1053619 -4.0933919 -4.0785966 -4.0814972 -4.0547051 -3.9317484 -3.5287237 -2.802768 -2.0662878][-2.8453357 -2.7554407 -2.678529 -2.6348186 -2.70636 -2.8285377 -2.9571869 -3.0071979 -3.0925126 -3.2914112 -3.4656374 -3.4417877 -2.9867249 -2.0571175 -1.1005261][-1.1568825 -1.0546567 -0.88425779 -0.70549726 -0.71587324 -0.85057044 -1.0270262 -1.0984666 -1.264401 -1.7180843 -2.2122428 -2.4218676 -2.1060166 -1.2172546 -0.25919628][-0.25626135 -0.16930389 0.13838625 0.52098465 0.65256548 0.56597233 0.380538 0.31243706 0.0561471 -0.71912956 -1.6228743 -2.1610641 -2.1041055 -1.3795006 -0.50919509][0.17989159 0.20051622 0.63437033 1.2588067 1.6598454 1.8185859 1.8465419 1.9631152 1.7193007 0.628633 -0.70872307 -1.6532428 -1.9695041 -1.5273156 -0.83076048][0.316195 0.22295904 0.63908958 1.3330936 1.9151444 2.3633523 2.7678084 3.2865148 3.3399391 2.2636466 0.75882387 -0.47953773 -1.1972537 -1.125114 -0.70782256][-0.20018768 -0.39740658 -0.17722797 0.25977087 0.650754 1.0592532 1.5872459 2.3836017 2.8737531 2.2495995 1.0915098 0.0081915855 -0.76002407 -0.8126843 -0.58803868][-0.18182707 -0.44970536 -0.47413635 -0.41066456 -0.39504814 -0.29908323 -0.015564442 0.66394806 1.3200641 1.1098299 0.43171883 -0.20473671 -0.63930869 -0.47741675 -0.27378702][0.47736406 0.19403362 0.017513275 -0.19038057 -0.47593522 -0.70136046 -0.78872085 -0.48587751 -0.0062828064 -0.12577009 -0.51245165 -0.722106 -0.7042551 -0.17076111 0.1560154][1.0393877 0.86781836 0.72656918 0.46097565 0.076525688 -0.32044744 -0.68511057 -0.730325 -0.502362 -0.67710423 -0.95667624 -0.92140579 -0.59591007 0.19373035 0.6029][1.135396 1.1586165 1.2372017 1.1502891 0.9258213 0.59825373 0.15849543 -0.09877634 -0.11898041 -0.43247128 -0.76604557 -0.69395375 -0.286129 0.56046009 0.9642477][0.75178766 0.93203211 1.2648931 1.490664 1.6119695 1.5730577 1.2844939 0.98926353 0.7852602 0.26543427 -0.24059582 -0.25521946 0.09820509 0.87201309 1.1971364][0.15917349 0.47712278 1.0011964 1.4859681 1.9233494 2.2073116 2.2030535 2.0721645 1.8534198 1.1809263 0.47547817 0.31043673 0.51991081 1.1433291 1.343935]]...]
INFO - root - 2017-12-07 05:36:29.913960: step 9810, loss = 0.91, batch loss = 0.84 (7.8 examples/sec; 1.032 sec/batch; 92h:30m:25s remains)
INFO - root - 2017-12-07 05:36:40.282010: step 9820, loss = 0.60, batch loss = 0.53 (7.9 examples/sec; 1.014 sec/batch; 90h:54m:55s remains)
INFO - root - 2017-12-07 05:36:50.695709: step 9830, loss = 0.64, batch loss = 0.57 (7.6 examples/sec; 1.049 sec/batch; 94h:03m:01s remains)
INFO - root - 2017-12-07 05:37:00.878654: step 9840, loss = 0.96, batch loss = 0.89 (7.6 examples/sec; 1.055 sec/batch; 94h:33m:19s remains)
INFO - root - 2017-12-07 05:37:11.284781: step 9850, loss = 0.55, batch loss = 0.48 (7.9 examples/sec; 1.010 sec/batch; 90h:31m:24s remains)
INFO - root - 2017-12-07 05:37:21.618297: step 9860, loss = 0.68, batch loss = 0.61 (7.5 examples/sec; 1.069 sec/batch; 95h:46m:46s remains)
INFO - root - 2017-12-07 05:37:31.964856: step 9870, loss = 0.99, batch loss = 0.92 (8.0 examples/sec; 0.996 sec/batch; 89h:14m:35s remains)
INFO - root - 2017-12-07 05:37:42.163721: step 9880, loss = 0.86, batch loss = 0.79 (7.6 examples/sec; 1.057 sec/batch; 94h:44m:38s remains)
INFO - root - 2017-12-07 05:37:52.425745: step 9890, loss = 0.73, batch loss = 0.66 (7.6 examples/sec; 1.052 sec/batch; 94h:17m:51s remains)
INFO - root - 2017-12-07 05:38:02.686425: step 9900, loss = 0.83, batch loss = 0.75 (7.8 examples/sec; 1.024 sec/batch; 91h:43m:28s remains)
2017-12-07 05:38:03.524678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0383592 -4.1227255 -4.1453714 -4.1558838 -4.2020445 -4.2811685 -4.2470059 -4.1804624 -4.1268177 -4.0652966 -4.0610847 -4.1163425 -4.1944447 -4.1797543 -3.982971][-3.9287648 -4.0369606 -4.1063762 -4.1926742 -4.2731209 -4.2639966 -4.0497379 -3.8542724 -3.807878 -3.8318393 -3.8957314 -3.9554029 -4.0367074 -3.9861529 -3.6014948][-3.4955826 -3.542866 -3.6352382 -3.810066 -3.9351902 -3.9299066 -3.7226133 -3.5771246 -3.6187303 -3.7025557 -3.7635117 -3.7976451 -3.8393159 -3.702898 -3.1185551][-3.3302641 -3.2985318 -3.3361297 -3.4141502 -3.3668032 -3.3249059 -3.2694602 -3.3012152 -3.4090285 -3.4646449 -3.5416293 -3.6527188 -3.6485486 -3.3451405 -2.6577988][-3.4678326 -3.4584246 -3.4589953 -3.3892856 -3.1487856 -3.0744822 -3.10684 -3.1151888 -3.1150391 -3.0796566 -3.2709339 -3.55966 -3.4769561 -2.9921169 -2.3086886][-3.6574087 -3.6116214 -3.4823089 -3.2448769 -2.9670081 -3.0022192 -3.0844626 -3.0022643 -2.8799038 -2.8250079 -3.1037006 -3.4417565 -3.2308297 -2.6330733 -2.0407329][-3.7479358 -3.7161155 -3.4942088 -3.1379898 -2.8377271 -2.9221296 -3.0051515 -2.8880448 -2.7669177 -2.7530975 -2.954298 -3.1124372 -2.7643454 -2.1811624 -1.7436492][-3.3490632 -3.3762932 -3.2381544 -2.9919496 -2.7127557 -2.7617984 -2.8098373 -2.721535 -2.6603608 -2.6677654 -2.7056594 -2.7069366 -2.4075024 -1.9538856 -1.5881779][-2.9392207 -2.8898938 -2.8127527 -2.7234247 -2.4573727 -2.4277437 -2.5240874 -2.5568624 -2.555022 -2.5139427 -2.4211476 -2.4314518 -2.3628948 -2.0688944 -1.6749969][-3.1092987 -2.959373 -2.7793746 -2.665298 -2.3297327 -2.1704512 -2.2502506 -2.3459187 -2.369663 -2.2464006 -2.1103783 -2.2444527 -2.4108987 -2.2478712 -1.8383076][-3.3647032 -3.1186051 -2.756789 -2.5249205 -2.1878247 -2.0067737 -2.0659797 -2.1754613 -2.2288322 -2.0684752 -1.9557922 -2.1635828 -2.3965759 -2.311341 -1.9600496][-3.1712945 -2.8707988 -2.4036014 -2.1447802 -1.9755177 -1.9673314 -2.0721853 -2.1397753 -2.2014325 -2.1169209 -2.0789461 -2.2474456 -2.3473685 -2.2307003 -1.9490819][-2.5736973 -2.2958941 -1.8783786 -1.7190707 -1.7761116 -1.9960186 -2.1820698 -2.216742 -2.2995179 -2.3350389 -2.3274207 -2.3431692 -2.2601411 -2.1048052 -1.9165387][-1.9884951 -1.8365064 -1.6197104 -1.575783 -1.7424304 -2.0867913 -2.3323119 -2.3622143 -2.4746246 -2.5635881 -2.5085735 -2.3719339 -2.1912806 -2.028167 -1.9137814][-1.8504188 -1.8249907 -1.8220754 -1.8955135 -2.0923719 -2.421308 -2.6488016 -2.7140403 -2.871542 -2.9553781 -2.8292472 -2.6075053 -2.4029651 -2.2538385 -2.1820664]]...]
INFO - root - 2017-12-07 05:38:13.711433: step 9910, loss = 0.89, batch loss = 0.82 (8.1 examples/sec; 0.992 sec/batch; 88h:51m:13s remains)
INFO - root - 2017-12-07 05:38:23.913405: step 9920, loss = 0.78, batch loss = 0.71 (7.8 examples/sec; 1.021 sec/batch; 91h:31m:16s remains)
INFO - root - 2017-12-07 05:38:34.075510: step 9930, loss = 0.76, batch loss = 0.69 (7.8 examples/sec; 1.021 sec/batch; 91h:29m:20s remains)
INFO - root - 2017-12-07 05:38:44.389944: step 9940, loss = 0.74, batch loss = 0.67 (8.0 examples/sec; 1.005 sec/batch; 90h:04m:04s remains)
INFO - root - 2017-12-07 05:38:54.629739: step 9950, loss = 1.04, batch loss = 0.97 (7.6 examples/sec; 1.047 sec/batch; 93h:50m:11s remains)
INFO - root - 2017-12-07 05:39:04.963904: step 9960, loss = 0.76, batch loss = 0.69 (7.9 examples/sec; 1.018 sec/batch; 91h:13m:59s remains)
INFO - root - 2017-12-07 05:39:15.294245: step 9970, loss = 0.90, batch loss = 0.83 (7.6 examples/sec; 1.052 sec/batch; 94h:13m:20s remains)
INFO - root - 2017-12-07 05:39:25.380962: step 9980, loss = 0.67, batch loss = 0.59 (7.6 examples/sec; 1.050 sec/batch; 94h:01m:43s remains)
INFO - root - 2017-12-07 05:39:35.555241: step 9990, loss = 0.72, batch loss = 0.65 (7.7 examples/sec; 1.042 sec/batch; 93h:23m:23s remains)
INFO - root - 2017-12-07 05:39:45.769973: step 10000, loss = 0.73, batch loss = 0.65 (8.0 examples/sec; 1.004 sec/batch; 89h:56m:23s remains)
2017-12-07 05:39:46.575967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1158373 -1.7849865 -1.5261288 -1.4943473 -1.7003977 -2.0505581 -2.439589 -2.7442312 -2.895129 -2.7328935 -2.2436404 -1.7113667 -1.3452966 -1.1382322 -1.0701268][-2.0146399 -1.7648151 -1.5794163 -1.5879502 -1.7994478 -2.1142631 -2.4422398 -2.6843076 -2.8040352 -2.607085 -2.0281749 -1.4378421 -1.0358825 -0.76560926 -0.59887767][-1.5946736 -1.39866 -1.3083503 -1.4400988 -1.8043156 -2.2187748 -2.514359 -2.6297784 -2.6532354 -2.4581704 -1.9591825 -1.4736321 -1.0569534 -0.64942288 -0.36457968][-1.1434171 -0.95634556 -0.93218923 -1.1345901 -1.6270843 -2.1751549 -2.5070763 -2.5744581 -2.5363045 -2.3029311 -1.9019601 -1.5780344 -1.2292469 -0.81134248 -0.55485225][-0.95270514 -0.75425267 -0.71789122 -0.89207172 -1.4044402 -1.9700677 -2.326741 -2.4595256 -2.4260621 -2.1349857 -1.7647932 -1.5817938 -1.414433 -1.1810391 -1.1120393][-1.0852072 -0.89157748 -0.78929758 -0.89391613 -1.3290975 -1.7930961 -2.1300156 -2.3342144 -2.296788 -1.9076807 -1.5053837 -1.4039538 -1.4757199 -1.5535481 -1.7494202][-1.3990028 -1.2556934 -1.1245413 -1.1838639 -1.5067458 -1.814719 -2.0732396 -2.2557673 -2.1440687 -1.6744344 -1.2582974 -1.2036405 -1.4496281 -1.7771611 -2.1670022][-1.7041235 -1.5742824 -1.4450443 -1.4934268 -1.6864781 -1.8377562 -1.9993258 -2.0821106 -1.9064236 -1.49347 -1.171762 -1.1639664 -1.4703693 -1.8711257 -2.2813976][-1.9541647 -1.7399964 -1.5622966 -1.569387 -1.6399453 -1.6938908 -1.7722807 -1.7434485 -1.5794792 -1.3334484 -1.1465623 -1.1749358 -1.48995 -1.8781757 -2.2169359][-2.0603995 -1.7459817 -1.4831419 -1.4033847 -1.3800461 -1.3988361 -1.434263 -1.3452713 -1.2682302 -1.1843371 -1.074719 -1.0935986 -1.4059212 -1.7641413 -2.0253367][-2.1504338 -1.7531159 -1.3916156 -1.1746042 -1.0296955 -0.9941926 -1.0093546 -0.95915008 -1.0106335 -1.0713253 -1.006942 -0.99381924 -1.2507508 -1.5219934 -1.7080321][-2.2692354 -1.8192558 -1.3785365 -1.0111699 -0.70786405 -0.55745816 -0.56742144 -0.63290238 -0.85140252 -1.0341833 -1.0050099 -0.97734928 -1.1354263 -1.2774992 -1.3725138][-2.2763243 -1.8572853 -1.4119093 -0.97836566 -0.61612558 -0.42371702 -0.40923071 -0.48763108 -0.75113773 -1.0004251 -1.0529256 -1.0671556 -1.1132016 -1.0948982 -1.1141267][-2.2252412 -1.9147317 -1.5458679 -1.1423113 -0.84366989 -0.70164609 -0.63241625 -0.61855578 -0.83547354 -1.085381 -1.1877301 -1.2241905 -1.1827238 -1.0645502 -1.0787077][-2.2071047 -2.024811 -1.7694507 -1.4466779 -1.239634 -1.1477883 -1.0435612 -0.99666333 -1.1606829 -1.2904551 -1.2886481 -1.3112679 -1.27529 -1.2011936 -1.3056715]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 05:39:57.539116: step 10010, loss = 0.77, batch loss = 0.69 (7.4 examples/sec; 1.079 sec/batch; 96h:37m:58s remains)
INFO - root - 2017-12-07 05:40:07.774313: step 10020, loss = 1.13, batch loss = 1.06 (7.5 examples/sec; 1.060 sec/batch; 94h:58m:19s remains)
INFO - root - 2017-12-07 05:40:17.946190: step 10030, loss = 0.99, batch loss = 0.92 (8.1 examples/sec; 0.989 sec/batch; 88h:35m:40s remains)
INFO - root - 2017-12-07 05:40:28.210745: step 10040, loss = 1.00, batch loss = 0.92 (8.0 examples/sec; 0.995 sec/batch; 89h:05m:10s remains)
INFO - root - 2017-12-07 05:40:38.394447: step 10050, loss = 0.85, batch loss = 0.78 (8.0 examples/sec; 0.995 sec/batch; 89h:08m:38s remains)
INFO - root - 2017-12-07 05:40:48.641849: step 10060, loss = 0.80, batch loss = 0.72 (7.7 examples/sec; 1.036 sec/batch; 92h:48m:04s remains)
INFO - root - 2017-12-07 05:40:58.829064: step 10070, loss = 0.93, batch loss = 0.86 (8.0 examples/sec; 1.004 sec/batch; 89h:55m:16s remains)
INFO - root - 2017-12-07 05:41:08.806119: step 10080, loss = 0.88, batch loss = 0.81 (7.5 examples/sec; 1.066 sec/batch; 95h:28m:22s remains)
INFO - root - 2017-12-07 05:41:19.069901: step 10090, loss = 0.67, batch loss = 0.60 (7.7 examples/sec; 1.035 sec/batch; 92h:44m:12s remains)
INFO - root - 2017-12-07 05:41:29.179244: step 10100, loss = 0.70, batch loss = 0.63 (7.7 examples/sec; 1.045 sec/batch; 93h:35m:57s remains)
2017-12-07 05:41:29.963496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0228038 -3.3392148 -3.7060461 -3.7270117 -3.2540169 -2.8253193 -2.5373955 -2.2094154 -1.9776027 -1.9125504 -2.1156592 -2.4380634 -2.6376262 -2.7343109 -2.7579508][-2.838906 -3.2088509 -3.6256437 -3.6014757 -3.0329289 -2.5297685 -2.2409041 -2.0214417 -1.9354007 -1.9678402 -2.2523377 -2.5627141 -2.6738079 -2.73905 -2.8062027][-2.7917752 -3.1358149 -3.507612 -3.424983 -2.8401089 -2.3303919 -2.030925 -1.9174809 -1.9887674 -2.1483846 -2.5103762 -2.7816834 -2.7874002 -2.8065381 -2.9142261][-2.9025865 -3.2263718 -3.4954572 -3.330199 -2.7435076 -2.1905842 -1.8191812 -1.7657683 -1.9560447 -2.2317886 -2.6404366 -2.8639293 -2.8059006 -2.8232746 -2.9694958][-2.8886201 -3.2141914 -3.4368412 -3.2790852 -2.751255 -2.1599193 -1.6755803 -1.5604899 -1.7616367 -2.0946853 -2.5255835 -2.7438378 -2.7220142 -2.7922993 -2.9707742][-2.5210254 -2.7757328 -2.9516997 -2.8947558 -2.5646462 -2.0722406 -1.6045783 -1.4377942 -1.5924819 -1.9135246 -2.3331306 -2.5709748 -2.6268826 -2.7327714 -2.8940597][-2.1628027 -2.2871218 -2.3664997 -2.3464692 -2.1484628 -1.7550123 -1.3462057 -1.1846116 -1.3406281 -1.6681778 -2.0633612 -2.2899382 -2.4050267 -2.536221 -2.6982055][-2.0830438 -2.1341636 -2.1500063 -2.1179175 -1.9205585 -1.51652 -1.0957122 -0.90737748 -1.1047082 -1.5256107 -1.986028 -2.2652431 -2.4170432 -2.5166283 -2.6493871][-2.1387284 -2.1239135 -2.0842545 -2.0315115 -1.8273723 -1.4524915 -1.0654528 -0.85778975 -1.0457487 -1.4894249 -1.9844422 -2.3115168 -2.4581223 -2.4832494 -2.547873][-2.2283049 -2.2284989 -2.2305486 -2.2428758 -2.1408846 -1.935993 -1.6984534 -1.5207932 -1.6324375 -1.9610868 -2.3873384 -2.7332728 -2.8679523 -2.8304172 -2.8110998][-2.2869511 -2.3193686 -2.3928018 -2.4683232 -2.4326148 -2.361697 -2.2877464 -2.2021749 -2.316041 -2.5572338 -2.9044938 -3.2906294 -3.4911265 -3.4598026 -3.3395042][-2.3371055 -2.3355346 -2.3916478 -2.4434795 -2.4146988 -2.420234 -2.4385133 -2.4394875 -2.599937 -2.7939825 -3.0683868 -3.4636891 -3.7518466 -3.8183434 -3.7174363][-2.8734674 -2.8517461 -2.8717635 -2.8396485 -2.7500281 -2.7597909 -2.7906113 -2.8141613 -2.9646094 -3.0952945 -3.2146878 -3.4168122 -3.5630722 -3.6145146 -3.6189377][-3.7730594 -3.7995753 -3.8311694 -3.7270637 -3.5645022 -3.5150483 -3.5042739 -3.5202396 -3.6892509 -3.8418469 -3.8989847 -3.9107382 -3.8166716 -3.695363 -3.6495733][-4.2626228 -4.3741884 -4.4911861 -4.4567442 -4.336184 -4.2633014 -4.2042131 -4.1969628 -4.3630395 -4.5624995 -4.6988373 -4.7444582 -4.63409 -4.4581585 -4.2910414]]...]
INFO - root - 2017-12-07 05:41:40.377275: step 10110, loss = 0.69, batch loss = 0.62 (7.5 examples/sec; 1.061 sec/batch; 95h:03m:23s remains)
INFO - root - 2017-12-07 05:41:50.697516: step 10120, loss = 0.97, batch loss = 0.90 (7.7 examples/sec; 1.034 sec/batch; 92h:37m:10s remains)
INFO - root - 2017-12-07 05:42:00.854914: step 10130, loss = 0.77, batch loss = 0.70 (8.0 examples/sec; 0.998 sec/batch; 89h:23m:37s remains)
INFO - root - 2017-12-07 05:42:10.932565: step 10140, loss = 0.76, batch loss = 0.69 (8.2 examples/sec; 0.981 sec/batch; 87h:51m:42s remains)
INFO - root - 2017-12-07 05:42:21.280232: step 10150, loss = 0.86, batch loss = 0.79 (7.9 examples/sec; 1.010 sec/batch; 90h:28m:06s remains)
INFO - root - 2017-12-07 05:42:31.638416: step 10160, loss = 0.63, batch loss = 0.56 (7.8 examples/sec; 1.020 sec/batch; 91h:17m:43s remains)
INFO - root - 2017-12-07 05:42:41.707861: step 10170, loss = 0.84, batch loss = 0.77 (8.0 examples/sec; 0.996 sec/batch; 89h:09m:51s remains)
INFO - root - 2017-12-07 05:42:51.325617: step 10180, loss = 0.83, batch loss = 0.76 (8.0 examples/sec; 0.994 sec/batch; 89h:01m:16s remains)
INFO - root - 2017-12-07 05:43:01.671874: step 10190, loss = 0.67, batch loss = 0.59 (7.7 examples/sec; 1.045 sec/batch; 93h:35m:21s remains)
INFO - root - 2017-12-07 05:43:11.851857: step 10200, loss = 0.89, batch loss = 0.82 (7.8 examples/sec; 1.026 sec/batch; 91h:53m:11s remains)
2017-12-07 05:43:12.688319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7261987 -2.7954898 -2.8923438 -2.9628615 -2.9149337 -2.8130407 -2.7225125 -2.740196 -2.79174 -2.7853479 -2.7892079 -2.8570528 -2.9255309 -3.0238891 -2.9443045][-2.7185473 -2.7846518 -2.8536522 -2.948128 -3.0123258 -2.9973688 -2.9189095 -2.9169102 -2.9647312 -2.9942455 -3.021013 -3.0511599 -3.0431213 -3.0909288 -2.9829371][-2.7726464 -2.8475866 -2.9118328 -3.0244863 -3.1637256 -3.1849384 -3.0624259 -3.0226116 -3.0812883 -3.154716 -3.1947472 -3.1603956 -3.067162 -3.0762093 -2.9578152][-2.8335648 -2.9275506 -3.0291328 -3.16319 -3.3392949 -3.3269987 -3.0940168 -2.9743562 -3.0252554 -3.1267865 -3.188024 -3.1290767 -3.0187891 -3.0465677 -2.9601731][-2.85351 -2.9820647 -3.1394825 -3.2911923 -3.470844 -3.3756952 -2.9982858 -2.7739592 -2.8107114 -2.9680457 -3.1062417 -3.0893941 -3.0104022 -3.078146 -3.040174][-2.928462 -3.0971913 -3.2696047 -3.3731787 -3.4638047 -3.1966267 -2.6270006 -2.3071785 -2.3934326 -2.6859713 -2.9741588 -3.0657873 -3.0461359 -3.1351509 -3.118058][-3.0862961 -3.2789583 -3.3902028 -3.3548546 -3.2525592 -2.7339792 -1.9929638 -1.7019095 -1.9710221 -2.4773254 -2.9271712 -3.1421852 -3.173327 -3.2235377 -3.1624594][-3.2003751 -3.3599277 -3.3468509 -3.1233125 -2.7829258 -2.0317748 -1.2516868 -1.1655648 -1.7248659 -2.4628949 -3.0306766 -3.3377061 -3.387495 -3.3371272 -3.1750407][-3.2031317 -3.2400804 -3.0806332 -2.7025332 -2.2296495 -1.4691718 -0.89007545 -1.1087701 -1.8905897 -2.7088003 -3.231108 -3.4979429 -3.5053017 -3.3488843 -3.1254592][-3.0714135 -2.9347143 -2.6734304 -2.276907 -1.9182954 -1.4709506 -1.2821491 -1.70468 -2.4567568 -3.1218615 -3.4381633 -3.5287514 -3.4221053 -3.1861453 -2.996382][-3.0330837 -2.7813592 -2.4990277 -2.2147925 -2.1296959 -2.0707445 -2.1419168 -2.4956648 -2.9619555 -3.3182564 -3.4208956 -3.3624372 -3.1703544 -2.9354148 -2.8820624][-3.0786133 -2.880477 -2.6870513 -2.5908079 -2.7616549 -2.9312623 -3.0064807 -3.1035991 -3.2148442 -3.2831671 -3.2626898 -3.1352611 -2.9192104 -2.7815771 -2.917892][-3.1241884 -3.0511339 -2.9654598 -3.0250368 -3.3166509 -3.5307813 -3.5195577 -3.4188 -3.3197076 -3.2209969 -3.1567161 -3.021663 -2.8343675 -2.8221679 -3.0455809][-3.0947614 -3.1068358 -3.100668 -3.2437949 -3.5510767 -3.752023 -3.7191491 -3.5807965 -3.4325516 -3.2781367 -3.1987848 -3.1077042 -3.0025282 -3.0651424 -3.2130337][-2.9836707 -3.0080638 -3.0333366 -3.1974387 -3.4784627 -3.6826761 -3.6992741 -3.5941107 -3.4571249 -3.3126545 -3.2507091 -3.2550662 -3.2405632 -3.271898 -3.2343519]]...]
INFO - root - 2017-12-07 05:43:23.023358: step 10210, loss = 0.70, batch loss = 0.62 (7.7 examples/sec; 1.036 sec/batch; 92h:46m:27s remains)
INFO - root - 2017-12-07 05:43:33.291184: step 10220, loss = 0.82, batch loss = 0.75 (8.1 examples/sec; 0.982 sec/batch; 87h:54m:35s remains)
INFO - root - 2017-12-07 05:43:43.669589: step 10230, loss = 1.01, batch loss = 0.94 (7.9 examples/sec; 1.019 sec/batch; 91h:11m:32s remains)
INFO - root - 2017-12-07 05:43:54.098682: step 10240, loss = 0.75, batch loss = 0.68 (7.6 examples/sec; 1.052 sec/batch; 94h:12m:46s remains)
INFO - root - 2017-12-07 05:44:04.406094: step 10250, loss = 0.75, batch loss = 0.67 (7.6 examples/sec; 1.047 sec/batch; 93h:41m:57s remains)
INFO - root - 2017-12-07 05:44:14.789438: step 10260, loss = 0.89, batch loss = 0.82 (7.9 examples/sec; 1.015 sec/batch; 90h:53m:26s remains)
INFO - root - 2017-12-07 05:44:24.690461: step 10270, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.766 sec/batch; 68h:34m:11s remains)
INFO - root - 2017-12-07 05:44:32.280371: step 10280, loss = 0.75, batch loss = 0.67 (10.3 examples/sec; 0.779 sec/batch; 69h:42m:03s remains)
INFO - root - 2017-12-07 05:44:39.983972: step 10290, loss = 0.73, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 68h:38m:30s remains)
INFO - root - 2017-12-07 05:44:47.524869: step 10300, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.757 sec/batch; 67h:47m:46s remains)
2017-12-07 05:44:48.161354: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.0609546 1.8952141 1.905395 1.9864206 2.0611653 2.1517444 2.1927996 2.1462803 2.0275207 1.8097644 1.4239511 0.71380377 -0.1483078 -0.69474387 -0.75356865][0.84643555 0.60735512 0.57594967 0.616385 0.6626482 0.76077318 0.842947 0.87380171 0.91069221 0.91756344 0.65399027 0.025200367 -0.69095826 -1.078198 -1.0467894][-0.60174251 -0.942724 -1.1495273 -1.3129611 -1.4089875 -1.4132233 -1.3962564 -1.3678887 -1.2073681 -0.96974134 -1.0440383 -1.4547102 -1.8901374 -2.0531628 -1.9000666][-1.4042473 -1.7067339 -1.9817445 -2.251575 -2.4057875 -2.4282229 -2.3746326 -2.2883191 -2.0935349 -1.9052808 -2.0670438 -2.5093327 -2.8685865 -2.9330025 -2.6719222][-1.4428623 -1.6580415 -1.8390727 -2.0656254 -2.22308 -2.2231374 -2.0808091 -1.8541937 -1.5818717 -1.460602 -1.7176113 -2.3243461 -2.8519731 -3.0702176 -2.8443136][-0.73671341 -1.0208025 -1.2134485 -1.4198284 -1.5419908 -1.4973986 -1.3243918 -1.094806 -0.9243567 -0.95570612 -1.2788038 -1.9328501 -2.5585732 -2.9027042 -2.7072787][0.17886019 -0.060492992 -0.32722235 -0.57240582 -0.59771037 -0.342659 -0.010424137 0.18976259 0.12475348 -0.23437452 -0.89182997 -1.7115862 -2.4720984 -2.9025626 -2.6945605][0.12854385 -0.1309967 -0.59274578 -1.0156374 -1.1205065 -0.820575 -0.36615705 -0.032005787 0.059982777 -0.25644398 -1.0312066 -1.8832138 -2.6953137 -3.191257 -3.0454483][-0.7197597 -1.1091425 -1.6748359 -2.1658804 -2.4385438 -2.4048693 -2.221591 -1.9982905 -1.7068622 -1.708781 -2.1865008 -2.7523148 -3.3762932 -3.778527 -3.6032503][-2.0690024 -2.368305 -2.655683 -2.8618913 -3.0373726 -3.0649929 -3.0060763 -2.9090562 -2.6927676 -2.6528573 -2.8676372 -3.177994 -3.6338692 -3.9594166 -3.7930696][-3.2565324 -3.5565257 -3.6159716 -3.645978 -3.7831869 -3.7752566 -3.5526392 -3.3127785 -3.0544128 -2.8457446 -2.6878803 -2.762464 -3.1771324 -3.5794444 -3.5761123][-3.4066682 -3.8300235 -3.9404106 -4.0493417 -4.3143654 -4.3558526 -3.9928427 -3.6272621 -3.3636367 -2.9730878 -2.4886796 -2.3993509 -2.770946 -3.1611366 -3.2368441][-3.1527338 -3.6709909 -4.0497379 -4.3559365 -4.6568 -4.6392131 -4.1123343 -3.6520445 -3.4494121 -3.034555 -2.5475612 -2.4985557 -2.7518165 -2.9705024 -3.01482][-2.9676418 -3.4964128 -4.0874295 -4.5142446 -4.7799072 -4.7336535 -4.1760945 -3.6287892 -3.3977118 -2.9501672 -2.5851734 -2.6865106 -2.8449998 -2.9402423 -2.97176][-2.8548632 -3.341126 -3.9316273 -4.3163 -4.54681 -4.5311465 -4.0978537 -3.584156 -3.3544354 -2.9280953 -2.6452579 -2.7676635 -2.8057775 -2.8627496 -2.9859889]]...]
INFO - root - 2017-12-07 05:44:55.801873: step 10310, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.762 sec/batch; 68h:11m:23s remains)
INFO - root - 2017-12-07 05:45:03.676098: step 10320, loss = 0.87, batch loss = 0.80 (9.9 examples/sec; 0.812 sec/batch; 72h:37m:31s remains)
INFO - root - 2017-12-07 05:45:11.440242: step 10330, loss = 0.92, batch loss = 0.84 (10.0 examples/sec; 0.798 sec/batch; 71h:22m:38s remains)
INFO - root - 2017-12-07 05:45:19.147323: step 10340, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 67h:26m:46s remains)
INFO - root - 2017-12-07 05:45:26.793799: step 10350, loss = 1.11, batch loss = 1.03 (10.4 examples/sec; 0.766 sec/batch; 68h:33m:03s remains)
INFO - root - 2017-12-07 05:45:34.391794: step 10360, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.749 sec/batch; 67h:00m:46s remains)
INFO - root - 2017-12-07 05:45:42.007539: step 10370, loss = 0.64, batch loss = 0.56 (10.6 examples/sec; 0.758 sec/batch; 67h:47m:13s remains)
INFO - root - 2017-12-07 05:45:49.437327: step 10380, loss = 0.77, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 70h:07m:43s remains)
INFO - root - 2017-12-07 05:45:57.118974: step 10390, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.784 sec/batch; 70h:09m:22s remains)
INFO - root - 2017-12-07 05:46:04.732686: step 10400, loss = 0.95, batch loss = 0.88 (10.7 examples/sec; 0.750 sec/batch; 67h:06m:59s remains)
2017-12-07 05:46:05.324809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8031602 -2.7833517 -2.7540886 -2.7493882 -2.8119655 -2.9265079 -3.0753469 -3.2184196 -3.2806578 -3.2547936 -3.15715 -3.0335407 -2.9328485 -2.8790693 -2.8706698][-2.7963679 -2.7791095 -2.7531595 -2.768507 -2.8618488 -3.0069163 -3.1963778 -3.3511329 -3.390518 -3.333015 -3.203301 -3.0573454 -2.9476695 -2.885623 -2.8711951][-2.7978539 -2.7811904 -2.7552209 -2.7823536 -2.8938518 -3.0681322 -3.302918 -3.4790769 -3.5159645 -3.4475863 -3.2948365 -3.1266026 -3.0024409 -2.919488 -2.8812416][-2.8081069 -2.7987623 -2.7692976 -2.7807136 -2.8746767 -3.05778 -3.3239264 -3.5291905 -3.5931022 -3.5506418 -3.4122009 -3.2399826 -3.1012526 -2.9923961 -2.9212348][-2.8090901 -2.8291545 -2.8235254 -2.8293834 -2.9138641 -3.1100388 -3.3691626 -3.5628979 -3.6374438 -3.6258097 -3.5365543 -3.3877826 -3.2318125 -3.0863717 -2.9760675][-2.8070891 -2.8447504 -2.8629751 -2.8806956 -2.9843764 -3.196348 -3.4290435 -3.5847116 -3.6685207 -3.7124915 -3.7092361 -3.5978351 -3.4172678 -3.2113237 -3.0339489][-2.8792496 -2.9343991 -2.9799261 -3.0049455 -3.0874615 -3.234611 -3.3899295 -3.4873691 -3.577678 -3.7101288 -3.8359053 -3.8043392 -3.6344938 -3.3779743 -3.1193993][-3.050822 -3.1724594 -3.2888598 -3.3425512 -3.3580534 -3.3537252 -3.3724523 -3.3777664 -3.4553356 -3.6833174 -3.9484751 -4.0193167 -3.8877316 -3.603461 -3.2609777][-3.2157264 -3.4116855 -3.6046898 -3.7057104 -3.6767354 -3.5437775 -3.4504211 -3.3836169 -3.4445453 -3.7327309 -4.0808382 -4.2201934 -4.1155586 -3.8221588 -3.4243929][-3.3281426 -3.5574772 -3.7807043 -3.9035196 -3.8489106 -3.6498382 -3.496264 -3.3901486 -3.43398 -3.7450869 -4.1195154 -4.28767 -4.2036819 -3.9203949 -3.512732][-3.3797574 -3.5872526 -3.7818832 -3.89187 -3.8340266 -3.6348886 -3.4788837 -3.3689222 -3.3872216 -3.6715715 -4.0220213 -4.181181 -4.1130753 -3.8567519 -3.4855728][-3.3573642 -3.510922 -3.652745 -3.7366574 -3.6966171 -3.5469811 -3.4362855 -3.3695817 -3.376039 -3.5997834 -3.8842301 -4.0026374 -3.9358323 -3.7154574 -3.4049671][-3.3002429 -3.3978646 -3.4902151 -3.5540442 -3.5378516 -3.4463229 -3.3816767 -3.3542619 -3.3636785 -3.522085 -3.7284119 -3.8016028 -3.7348466 -3.5534222 -3.3086443][-3.2406511 -3.3086772 -3.3728113 -3.4251261 -3.4271188 -3.3860998 -3.3571887 -3.3501892 -3.3544908 -3.4435649 -3.5659077 -3.5969934 -3.5299051 -3.3873715 -3.2114286][-3.1667497 -3.2182751 -3.2630584 -3.3029408 -3.3118181 -3.2984729 -3.2904265 -3.2903883 -3.2902093 -3.3275352 -3.3838081 -3.3891983 -3.3347347 -3.233747 -3.1200774]]...]
INFO - root - 2017-12-07 05:46:12.954525: step 10410, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.783 sec/batch; 70h:01m:33s remains)
INFO - root - 2017-12-07 05:46:20.642082: step 10420, loss = 0.98, batch loss = 0.90 (10.2 examples/sec; 0.782 sec/batch; 69h:56m:33s remains)
INFO - root - 2017-12-07 05:46:28.289539: step 10430, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.753 sec/batch; 67h:22m:03s remains)
INFO - root - 2017-12-07 05:46:35.935745: step 10440, loss = 0.87, batch loss = 0.79 (10.6 examples/sec; 0.756 sec/batch; 67h:39m:28s remains)
INFO - root - 2017-12-07 05:46:43.665836: step 10450, loss = 0.92, batch loss = 0.84 (10.3 examples/sec; 0.774 sec/batch; 69h:15m:55s remains)
INFO - root - 2017-12-07 05:46:51.372416: step 10460, loss = 0.69, batch loss = 0.62 (10.7 examples/sec; 0.747 sec/batch; 66h:48m:55s remains)
INFO - root - 2017-12-07 05:46:59.183553: step 10470, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.757 sec/batch; 67h:44m:05s remains)
INFO - root - 2017-12-07 05:47:06.594079: step 10480, loss = 0.67, batch loss = 0.60 (10.5 examples/sec; 0.764 sec/batch; 68h:19m:29s remains)
INFO - root - 2017-12-07 05:47:14.442441: step 10490, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.811 sec/batch; 72h:31m:18s remains)
INFO - root - 2017-12-07 05:47:22.236160: step 10500, loss = 0.69, batch loss = 0.61 (9.9 examples/sec; 0.811 sec/batch; 72h:34m:16s remains)
2017-12-07 05:47:22.883277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5136621 -3.5146134 -3.3851719 -3.146189 -2.929234 -2.9009757 -2.9308286 -2.9572263 -3.2798162 -3.8100054 -4.1675754 -4.0133843 -3.1178336 -1.7478995 -0.5509212][-3.7856538 -3.8501954 -3.6726394 -3.3378515 -3.1158662 -3.2083774 -3.3892245 -3.4895582 -3.7917104 -4.175632 -4.2648048 -3.8503335 -2.8088403 -1.4692271 -0.49828315][-4.0609117 -4.22862 -4.0163317 -3.5934203 -3.3590405 -3.4855905 -3.6557703 -3.7007232 -3.9059069 -4.1165123 -3.9880958 -3.4483008 -2.4624214 -1.3658991 -0.75407577][-4.2885189 -4.5483546 -4.3330097 -3.8649681 -3.6252458 -3.6738477 -3.6372924 -3.4875183 -3.5684345 -3.6715055 -3.4871612 -2.9949775 -2.2134266 -1.4683912 -1.2428102][-4.3795505 -4.668519 -4.3895907 -3.828306 -3.5341125 -3.4401312 -3.1648588 -2.8804951 -2.9306147 -3.0303538 -2.9328787 -2.6222672 -2.1411643 -1.7773299 -1.882678][-4.4098048 -4.72803 -4.4136257 -3.725369 -3.2409353 -2.8392897 -2.2754614 -1.9575276 -2.1247962 -2.3623128 -2.4605694 -2.4247243 -2.3012204 -2.2763877 -2.5586963][-4.2239742 -4.5264821 -4.2819633 -3.5615423 -2.8398576 -2.02862 -1.179987 -0.95565462 -1.4213176 -1.9377148 -2.3207421 -2.561841 -2.7045445 -2.8442774 -3.1417811][-3.848573 -4.0049543 -3.8186882 -3.1496587 -2.2489097 -1.1168008 -0.13709354 -0.22227287 -1.1654348 -2.014842 -2.5818863 -2.9102693 -3.1195188 -3.2996342 -3.5678184][-3.6770253 -3.5908871 -3.359271 -2.7304444 -1.7518568 -0.51388693 0.3594861 -0.16924715 -1.5241227 -2.4900589 -2.9565063 -3.1691437 -3.3593752 -3.567071 -3.81112][-3.6165881 -3.3683395 -3.1063032 -2.5456605 -1.6506405 -0.56839919 -0.0039777756 -0.83567023 -2.2406673 -2.983875 -3.1409216 -3.1612828 -3.34984 -3.6670868 -3.9847038][-3.325695 -3.0435538 -2.8365693 -2.4148219 -1.7149277 -0.95205283 -0.73421025 -1.6702166 -2.9344373 -3.3571851 -3.1472735 -2.9228673 -3.0632124 -3.5119781 -3.9709287][-2.9423375 -2.6824188 -2.5952988 -2.3527019 -1.8865089 -1.4290595 -1.4473515 -2.3041096 -3.3266706 -3.4885802 -3.0626991 -2.7330031 -2.8695607 -3.4170027 -3.937145][-2.7740233 -2.5268078 -2.5332189 -2.4432821 -2.2110474 -2.0279305 -2.1691966 -2.7877362 -3.4309301 -3.3363943 -2.8624086 -2.6727238 -2.9928312 -3.6416192 -4.1212587][-2.6992033 -2.4090924 -2.427047 -2.4699147 -2.5341239 -2.6528232 -2.8421841 -3.1381755 -3.3696048 -3.0809345 -2.6580224 -2.6450324 -3.0963166 -3.7369895 -4.1436925][-2.6752753 -2.3560712 -2.3412163 -2.4721622 -2.7735968 -3.1212037 -3.3023627 -3.3294954 -3.2934327 -2.9744766 -2.6513531 -2.704504 -3.0850894 -3.5207512 -3.7771301]]...]
INFO - root - 2017-12-07 05:47:30.502791: step 10510, loss = 0.86, batch loss = 0.79 (10.8 examples/sec; 0.738 sec/batch; 66h:01m:18s remains)
INFO - root - 2017-12-07 05:47:38.213805: step 10520, loss = 0.78, batch loss = 0.71 (9.7 examples/sec; 0.824 sec/batch; 73h:40m:08s remains)
INFO - root - 2017-12-07 05:47:45.881434: step 10530, loss = 0.72, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 67h:47m:59s remains)
INFO - root - 2017-12-07 05:47:53.594011: step 10540, loss = 0.79, batch loss = 0.72 (10.0 examples/sec; 0.797 sec/batch; 71h:17m:47s remains)
INFO - root - 2017-12-07 05:48:01.430919: step 10550, loss = 0.70, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 70h:17m:54s remains)
INFO - root - 2017-12-07 05:48:09.123347: step 10560, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.752 sec/batch; 67h:15m:24s remains)
INFO - root - 2017-12-07 05:48:16.720856: step 10570, loss = 0.83, batch loss = 0.76 (10.7 examples/sec; 0.748 sec/batch; 66h:53m:51s remains)
INFO - root - 2017-12-07 05:48:24.228029: step 10580, loss = 0.84, batch loss = 0.76 (10.6 examples/sec; 0.752 sec/batch; 67h:15m:45s remains)
INFO - root - 2017-12-07 05:48:31.886707: step 10590, loss = 0.71, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 69h:40m:13s remains)
INFO - root - 2017-12-07 05:48:39.495381: step 10600, loss = 0.96, batch loss = 0.88 (10.7 examples/sec; 0.751 sec/batch; 67h:06m:59s remains)
2017-12-07 05:48:40.175762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8004303 -1.9155228 -0.81805682 -0.34028053 -0.94791484 -1.6056678 -1.658715 -1.6624315 -1.7821331 -1.6846414 -1.2284491 -0.672287 -0.46097136 -0.86553431 -1.4737873][-2.7385666 -1.8741362 -0.93726087 -0.677505 -1.2786989 -1.7765012 -1.7471948 -1.6868553 -1.6664488 -1.4734159 -1.0698483 -0.67491436 -0.69268966 -1.2710383 -1.9022043][-2.9047971 -2.1780012 -1.5153852 -1.4393079 -1.8531451 -2.0388036 -1.8384078 -1.6748977 -1.5829611 -1.4225218 -1.2011888 -1.0812557 -1.3338234 -1.9260349 -2.4291949][-3.1735396 -2.5957413 -2.2167251 -2.2601957 -2.4507256 -2.3862622 -2.1022022 -1.8912191 -1.8280909 -1.8034618 -1.7420156 -1.7485116 -1.9959512 -2.3505096 -2.6145134][-3.3805919 -2.9083564 -2.767179 -2.9090524 -2.9390631 -2.6958888 -2.3460033 -2.1531243 -2.2444096 -2.4796722 -2.5537093 -2.5011749 -2.5207238 -2.5171204 -2.5170412][-3.4870131 -3.0578246 -3.0589464 -3.2626312 -3.197643 -2.8394585 -2.4367442 -2.2775335 -2.5372787 -2.9908018 -3.0986886 -2.8871064 -2.6314182 -2.38006 -2.3143008][-3.5181575 -3.043447 -3.0439239 -3.1829047 -3.0367222 -2.6579614 -2.2996202 -2.2535837 -2.6775591 -3.1806238 -3.0969398 -2.6386728 -2.217658 -1.9589717 -2.1098149][-3.4069316 -2.7989275 -2.6836348 -2.6975741 -2.5091505 -2.1787946 -1.9174156 -2.030036 -2.6100893 -2.9944005 -2.5842333 -1.9489524 -1.5828264 -1.5713108 -2.0887067][-3.0901971 -2.2934239 -2.0174696 -1.9205236 -1.7436974 -1.4528122 -1.2417336 -1.5126164 -2.2097116 -2.3928239 -1.7490408 -1.2072065 -1.1735749 -1.567765 -2.3799553][-2.7218709 -1.7586625 -1.3351057 -1.1699591 -1.0430896 -0.770216 -0.60713911 -1.0192654 -1.7292235 -1.7235227 -1.0526018 -0.78499627 -1.122426 -1.8083162 -2.7030823][-2.5026712 -1.4950533 -1.0574398 -0.94753528 -0.94712138 -0.81621313 -0.78668857 -1.2495439 -1.823513 -1.6488905 -1.0871053 -1.0893953 -1.6038342 -2.2908583 -3.0055702][-2.4996316 -1.5566423 -1.2118752 -1.2513947 -1.4172814 -1.5261652 -1.7032244 -2.14151 -2.4744482 -2.1672702 -1.7578332 -1.9364433 -2.4277084 -2.8981717 -3.2709279][-2.5723574 -1.7146609 -1.4805286 -1.6830363 -1.9907303 -2.264761 -2.5342884 -2.8303084 -2.8977084 -2.5659814 -2.3838103 -2.6785126 -3.0891237 -3.3514616 -3.4372833][-2.6085827 -1.8078294 -1.65011 -1.9840553 -2.3913224 -2.7252312 -2.9675596 -3.0754056 -2.9399791 -2.6648073 -2.691627 -3.0257325 -3.3257337 -3.4508691 -3.3910954][-2.6215031 -1.8799584 -1.772692 -2.1961842 -2.6549315 -2.9691386 -3.1602833 -3.1833274 -2.9677768 -2.7472339 -2.8754854 -3.1761942 -3.3529191 -3.3768721 -3.2457886]]...]
INFO - root - 2017-12-07 05:48:47.896577: step 10610, loss = 0.82, batch loss = 0.75 (10.2 examples/sec; 0.784 sec/batch; 70h:04m:40s remains)
INFO - root - 2017-12-07 05:48:55.557746: step 10620, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.746 sec/batch; 66h:40m:13s remains)
INFO - root - 2017-12-07 05:49:03.320470: step 10630, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.766 sec/batch; 68h:31m:39s remains)
INFO - root - 2017-12-07 05:49:10.982225: step 10640, loss = 0.84, batch loss = 0.76 (10.6 examples/sec; 0.752 sec/batch; 67h:14m:27s remains)
INFO - root - 2017-12-07 05:49:18.620808: step 10650, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.755 sec/batch; 67h:28m:56s remains)
INFO - root - 2017-12-07 05:49:26.368807: step 10660, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.753 sec/batch; 67h:20m:30s remains)
INFO - root - 2017-12-07 05:49:34.072032: step 10670, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 70h:51m:35s remains)
INFO - root - 2017-12-07 05:49:41.454510: step 10680, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.756 sec/batch; 67h:32m:29s remains)
INFO - root - 2017-12-07 05:49:49.121101: step 10690, loss = 0.68, batch loss = 0.61 (10.6 examples/sec; 0.753 sec/batch; 67h:16m:03s remains)
INFO - root - 2017-12-07 05:49:56.813466: step 10700, loss = 0.72, batch loss = 0.64 (10.8 examples/sec; 0.739 sec/batch; 66h:03m:48s remains)
2017-12-07 05:49:57.486191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1032498 -2.165184 -2.4473033 -3.1033981 -3.557267 -3.4645197 -3.0799584 -3.0010459 -3.3441634 -3.5759261 -3.8632479 -4.0028648 -3.700304 -3.2592051 -2.9912868][-2.1670625 -2.2775877 -2.62365 -3.2760139 -3.5407872 -3.2209561 -2.6707835 -2.5255249 -2.9792674 -3.3423924 -3.7540698 -4.054543 -3.8431523 -3.3152111 -2.9141557][-2.3082802 -2.4844389 -2.8653274 -3.4394727 -3.5482137 -3.1709719 -2.6382308 -2.5522575 -3.1200876 -3.5080266 -3.9300075 -4.322113 -4.1860557 -3.5322635 -2.9670246][-2.4149902 -2.546792 -2.7958269 -3.0632188 -2.8949564 -2.4912033 -2.0363677 -2.0615859 -2.792609 -3.3083673 -3.8104851 -4.3034668 -4.221303 -3.4743323 -2.8295822][-2.4934111 -2.5503626 -2.6010036 -2.4734573 -1.9218099 -1.341922 -0.81000519 -0.80674672 -1.6527054 -2.4010482 -3.1050365 -3.6893578 -3.6063426 -2.7823739 -2.1169434][-2.5701473 -2.6307673 -2.5907376 -2.1811645 -1.3034465 -0.4760704 0.25006962 0.36917496 -0.61715627 -1.746495 -2.689945 -3.1780868 -2.8416309 -1.8316243 -1.0840602][-2.6259556 -2.7484517 -2.7333617 -2.2643733 -1.3045316 -0.31190443 0.71679068 1.1610518 0.23392916 -1.1607382 -2.3022232 -2.7139363 -2.2428198 -1.1713352 -0.37318611][-2.6454675 -2.7818046 -2.7266188 -2.2348719 -1.3746161 -0.41410685 0.80438852 1.6000485 0.93147087 -0.473881 -1.7523127 -2.2542784 -1.9104917 -1.0710154 -0.45292258][-2.584538 -2.6722445 -2.5206718 -2.031287 -1.3492718 -0.51341724 0.72670412 1.6021824 1.1604877 -0.078074455 -1.375937 -1.9800758 -1.8292651 -1.2831013 -1.012526][-2.4888656 -2.5325284 -2.3691165 -2.052331 -1.7143381 -1.1613021 -0.11470509 0.66344595 0.60379362 -0.079991341 -1.0539596 -1.576468 -1.5513418 -1.2691224 -1.3482151][-2.313695 -2.2636132 -2.0761251 -1.9048071 -1.8912029 -1.659013 -0.99946547 -0.5321331 -0.33233261 -0.38553381 -0.81574821 -1.0298092 -0.97107005 -0.8706882 -1.209362][-2.1173995 -1.9446483 -1.6753359 -1.5173492 -1.5958514 -1.5153885 -1.1708713 -1.0395913 -0.84252048 -0.61154866 -0.74986553 -0.75542164 -0.61825705 -0.5391221 -0.92869759][-2.0284383 -1.7904077 -1.44221 -1.2246113 -1.2590373 -1.2481194 -1.1480398 -1.3003361 -1.2774298 -1.1091564 -1.2614231 -1.2739532 -1.1006238 -0.90674949 -1.0742178][-2.062722 -1.8539605 -1.5162256 -1.2970145 -1.3389065 -1.4576213 -1.6033664 -1.9383788 -2.0306945 -1.9151189 -2.0250919 -2.0197508 -1.8380082 -1.6267796 -1.6570275][-2.1677113 -2.0797532 -1.8554919 -1.716346 -1.7977414 -1.9969792 -2.225745 -2.5207338 -2.566236 -2.4134426 -2.4067144 -2.3718841 -2.2353666 -2.1036751 -2.1371131]]...]
INFO - root - 2017-12-07 05:50:05.096561: step 10710, loss = 0.81, batch loss = 0.74 (10.8 examples/sec; 0.743 sec/batch; 66h:26m:26s remains)
INFO - root - 2017-12-07 05:50:12.738776: step 10720, loss = 0.68, batch loss = 0.61 (10.5 examples/sec; 0.765 sec/batch; 68h:21m:11s remains)
INFO - root - 2017-12-07 05:50:20.357053: step 10730, loss = 0.90, batch loss = 0.82 (10.6 examples/sec; 0.756 sec/batch; 67h:31m:42s remains)
INFO - root - 2017-12-07 05:50:27.902921: step 10740, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.751 sec/batch; 67h:06m:20s remains)
INFO - root - 2017-12-07 05:50:35.495816: step 10750, loss = 0.66, batch loss = 0.59 (10.4 examples/sec; 0.766 sec/batch; 68h:30m:17s remains)
INFO - root - 2017-12-07 05:50:43.091934: step 10760, loss = 0.80, batch loss = 0.72 (10.5 examples/sec; 0.762 sec/batch; 68h:07m:11s remains)
INFO - root - 2017-12-07 05:50:50.857427: step 10770, loss = 0.65, batch loss = 0.58 (10.2 examples/sec; 0.781 sec/batch; 69h:48m:58s remains)
INFO - root - 2017-12-07 05:50:58.284994: step 10780, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 68h:23m:02s remains)
INFO - root - 2017-12-07 05:51:05.935269: step 10790, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.764 sec/batch; 68h:17m:39s remains)
INFO - root - 2017-12-07 05:51:13.568204: step 10800, loss = 0.72, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 67h:09m:42s remains)
2017-12-07 05:51:14.173388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1507249 -2.0101306 -1.8690035 -1.6238306 -1.2980354 -1.0765109 -0.86642313 -0.66165853 -0.59179115 -0.44456506 -0.16315937 0.010493755 -0.044957638 -0.16705465 -0.23167896][-1.4480436 -1.2997375 -1.1799192 -1.0357914 -0.90767407 -0.94105053 -0.94336653 -0.86200452 -0.9571507 -0.91882133 -0.71277189 -0.60324669 -0.66395712 -0.67604208 -0.5467875][-0.93827033 -0.78927326 -0.74439788 -0.84649181 -1.018415 -1.3220329 -1.5163851 -1.5547893 -1.8533747 -2.0151534 -1.9535711 -1.9148571 -1.8998506 -1.7073338 -1.3782728][-0.86555123 -0.64737415 -0.57423449 -0.78790283 -1.1001749 -1.5231731 -1.8312473 -1.9798489 -2.467386 -2.8549304 -2.9388862 -3.0294337 -3.04769 -2.7867887 -2.3754187][-1.0734363 -0.78892946 -0.62793088 -0.78752565 -1.0211902 -1.3449554 -1.56843 -1.639755 -2.0762877 -2.5077944 -2.6520844 -2.9332423 -3.1515865 -3.0240815 -2.6901698][-1.447252 -1.1748595 -0.91680717 -0.85416627 -0.74558806 -0.67119741 -0.47602654 -0.14165545 -0.27673674 -0.67916584 -0.9152 -1.4469192 -1.978874 -2.1627028 -2.1647563][-1.7462492 -1.4970815 -1.1465013 -0.76265836 -0.20809174 0.31541967 0.96078682 1.7549882 1.8681855 1.3493438 0.86807823 0.097644329 -0.68576717 -1.2336314 -1.7061057][-1.6353428 -1.4158952 -1.0641675 -0.51323318 0.269413 0.95916653 1.6606488 2.419292 2.4483476 1.8351855 1.3427749 0.65214682 -0.16697693 -0.95184016 -1.7754967][-1.3088679 -1.1200652 -0.89807773 -0.42831469 0.2362771 0.74471569 1.0626502 1.3664265 1.27391 0.90212631 0.71908522 0.35139132 -0.28258038 -0.98959827 -1.7931101][-1.0348177 -0.89639163 -0.84070539 -0.63182449 -0.32503271 -0.22699833 -0.35107327 -0.42235947 -0.49500704 -0.53594923 -0.3686204 -0.38484669 -0.74812222 -1.2083642 -1.7139938][-0.71672487 -0.72171211 -0.88358855 -0.99633574 -1.0682569 -1.3431222 -1.7308903 -1.9435256 -1.9385383 -1.794256 -1.4328694 -1.1923842 -1.2543023 -1.4169471 -1.5539939][-0.23132658 -0.49806905 -0.92695665 -1.2898827 -1.5841761 -1.9572499 -2.3184314 -2.4705982 -2.4796321 -2.4662318 -2.25603 -1.9834509 -1.8355808 -1.7620273 -1.6762028][0.064042091 -0.48224568 -1.0519905 -1.4487598 -1.7041364 -1.958925 -2.2295146 -2.3701115 -2.4937253 -2.6701608 -2.6909618 -2.5882487 -2.4990878 -2.4441602 -2.3940506][-0.22530508 -0.84852552 -1.2965035 -1.5147102 -1.5979249 -1.7497342 -2.02764 -2.2929149 -2.536664 -2.7624812 -2.8784664 -2.9274664 -3.0061994 -3.1284065 -3.2329178][-0.82567811 -1.2787449 -1.4885392 -1.5412009 -1.5945852 -1.8015168 -2.1374607 -2.4866376 -2.792995 -2.9965792 -3.1108122 -3.1898963 -3.2767448 -3.4209104 -3.5727057]]...]
INFO - root - 2017-12-07 05:51:21.965044: step 10810, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.787 sec/batch; 70h:19m:31s remains)
INFO - root - 2017-12-07 05:51:29.585306: step 10820, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.753 sec/batch; 67h:15m:14s remains)
INFO - root - 2017-12-07 05:51:37.386898: step 10830, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.765 sec/batch; 68h:21m:10s remains)
INFO - root - 2017-12-07 05:51:45.020750: step 10840, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.759 sec/batch; 67h:50m:54s remains)
INFO - root - 2017-12-07 05:51:52.683982: step 10850, loss = 0.89, batch loss = 0.82 (10.8 examples/sec; 0.744 sec/batch; 66h:29m:01s remains)
INFO - root - 2017-12-07 05:52:00.369383: step 10860, loss = 0.69, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 70h:04m:22s remains)
INFO - root - 2017-12-07 05:52:08.167540: step 10870, loss = 0.77, batch loss = 0.70 (10.7 examples/sec; 0.746 sec/batch; 66h:40m:05s remains)
INFO - root - 2017-12-07 05:52:15.691700: step 10880, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.751 sec/batch; 67h:06m:26s remains)
INFO - root - 2017-12-07 05:52:23.402215: step 10890, loss = 0.97, batch loss = 0.90 (10.5 examples/sec; 0.765 sec/batch; 68h:19m:34s remains)
INFO - root - 2017-12-07 05:52:31.055006: step 10900, loss = 0.94, batch loss = 0.86 (10.6 examples/sec; 0.755 sec/batch; 67h:25m:26s remains)
2017-12-07 05:52:31.673444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.771409 -2.1281772 -2.0435612 -2.3808446 -2.5870974 -2.7689512 -3.1832283 -3.4335248 -3.4847295 -3.2964644 -2.772032 -2.3060896 -1.8785739 -1.449445 -1.6488173][-2.3690059 -1.7079237 -1.6904054 -1.958456 -1.9564481 -2.1281662 -2.6975412 -2.9464707 -2.9267383 -2.7958899 -2.341435 -1.9353657 -1.5759904 -1.2040629 -1.4954324][-1.9486654 -1.4706032 -1.6254766 -1.8692904 -1.736306 -1.9255099 -2.4293985 -2.4185038 -2.2133305 -2.1174552 -1.8478413 -1.6115704 -1.3619909 -1.1081233 -1.521533][-1.7038512 -1.4597638 -1.7505913 -2.0085287 -1.9034183 -2.1858444 -2.6134851 -2.4036946 -2.0214436 -1.8500838 -1.6228127 -1.4310441 -1.2494783 -1.186512 -1.7674472][-1.5686862 -1.502624 -1.7983148 -2.0179632 -1.9999158 -2.3743689 -2.7945633 -2.5479381 -2.0770984 -1.8322639 -1.5951512 -1.3848259 -1.2361691 -1.3710144 -2.073895][-1.5274906 -1.584548 -1.8287594 -1.9710047 -1.9953871 -2.3790064 -2.7454143 -2.4289675 -1.8081915 -1.4113376 -1.2555542 -1.2104309 -1.2188039 -1.5426188 -2.3004489][-1.59056 -1.7229025 -1.9126132 -1.9977307 -2.0165682 -2.2540429 -2.38639 -1.8969553 -1.0472932 -0.45103049 -0.42552996 -0.65361929 -0.8817687 -1.4405956 -2.3134344][-1.6988144 -1.8333268 -1.944438 -2.0259738 -2.0634868 -2.1141088 -1.9525876 -1.3129437 -0.31966019 0.37700367 0.27025843 -0.14495707 -0.49923825 -1.2239749 -2.1648581][-1.8643587 -1.9918773 -1.987042 -2.0715151 -2.1136065 -2.0116675 -1.7089427 -1.0527639 -0.09156847 0.5111599 0.20566082 -0.32615328 -0.69286561 -1.4060502 -2.2143295][-2.182452 -2.2851298 -2.2182178 -2.3390374 -2.3563941 -2.1956503 -1.9008884 -1.2960789 -0.48944402 -0.1569767 -0.6478796 -1.1929414 -1.4624519 -1.987977 -2.5384593][-2.4956207 -2.5030708 -2.3960991 -2.5471978 -2.5003223 -2.3212311 -2.148375 -1.7267849 -1.2239549 -1.1871536 -1.7326028 -2.1858885 -2.3077312 -2.5469291 -2.819957][-2.761529 -2.7147412 -2.5446506 -2.6304593 -2.4819417 -2.3174055 -2.2948055 -2.0325577 -1.7367344 -1.8973205 -2.4549572 -2.8530459 -2.835618 -2.7744679 -2.8269436][-2.8179114 -2.9272358 -2.785846 -2.8137827 -2.6378756 -2.5613966 -2.6069665 -2.2621369 -1.924032 -2.1228988 -2.6150641 -2.9619288 -2.8085346 -2.50871 -2.50391][-2.6462457 -2.8721263 -2.7571549 -2.7760286 -2.669323 -2.7351213 -2.8901236 -2.4698863 -2.0329804 -2.163662 -2.5066237 -2.7055798 -2.326638 -1.8271687 -1.898746][-2.7804847 -3.0313039 -2.907773 -2.887 -2.7988424 -2.9182932 -3.109911 -2.646235 -2.1940615 -2.3014948 -2.5149741 -2.5587797 -1.9738417 -1.3255188 -1.4513562]]...]
INFO - root - 2017-12-07 05:52:39.343102: step 10910, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.774 sec/batch; 69h:08m:12s remains)
INFO - root - 2017-12-07 05:52:46.968796: step 10920, loss = 0.74, batch loss = 0.66 (10.4 examples/sec; 0.771 sec/batch; 68h:52m:17s remains)
INFO - root - 2017-12-07 05:52:54.642643: step 10930, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.755 sec/batch; 67h:27m:01s remains)
INFO - root - 2017-12-07 05:53:02.315345: step 10940, loss = 0.95, batch loss = 0.88 (10.3 examples/sec; 0.776 sec/batch; 69h:18m:50s remains)
INFO - root - 2017-12-07 05:53:10.050261: step 10950, loss = 0.81, batch loss = 0.74 (9.9 examples/sec; 0.808 sec/batch; 72h:09m:35s remains)
INFO - root - 2017-12-07 05:53:17.730622: step 10960, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.771 sec/batch; 68h:51m:57s remains)
INFO - root - 2017-12-07 05:53:25.484944: step 10970, loss = 0.66, batch loss = 0.59 (10.2 examples/sec; 0.782 sec/batch; 69h:48m:24s remains)
INFO - root - 2017-12-07 05:53:32.853460: step 10980, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.773 sec/batch; 69h:04m:20s remains)
INFO - root - 2017-12-07 05:53:40.607056: step 10990, loss = 0.80, batch loss = 0.73 (10.1 examples/sec; 0.791 sec/batch; 70h:39m:46s remains)
INFO - root - 2017-12-07 05:53:48.269655: step 11000, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.750 sec/batch; 67h:01m:02s remains)
2017-12-07 05:53:48.888463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0408261 -2.8997712 -2.8815727 -2.9680047 -3.1819205 -3.3875086 -3.4335804 -3.354075 -3.2836509 -3.1838174 -3.1359923 -3.1461878 -3.1025805 -3.0116391 -2.9656327][-2.62624 -2.2295349 -2.1400042 -2.3629322 -2.8711843 -3.4028072 -3.6603436 -3.6386149 -3.5231328 -3.3260002 -3.1879916 -3.1925938 -3.180768 -3.1710851 -3.1677136][-1.81213 -1.1899836 -0.99952364 -1.3302121 -2.0939748 -2.9500511 -3.5058911 -3.6962872 -3.6336949 -3.3484683 -3.09521 -3.015388 -3.0048282 -3.1080928 -3.1492777][-1.4418845 -0.65889168 -0.24608755 -0.4106226 -1.0459552 -1.8742602 -2.5534592 -2.9754949 -3.0495224 -2.8067055 -2.5828581 -2.490798 -2.5465856 -2.7645135 -2.7699003][-1.9327965 -1.3182964 -0.71403384 -0.43384385 -0.43483639 -0.66738939 -1.0858226 -1.5696509 -1.8173535 -1.8029211 -1.790036 -1.821012 -2.0625432 -2.4538946 -2.495141][-2.6202917 -2.2292573 -1.5483081 -0.92907667 -0.35449982 0.044913769 -0.0014472008 -0.38353682 -0.75672889 -0.99393177 -1.1542487 -1.23547 -1.4948444 -1.9194946 -2.1345389][-2.664814 -2.3945611 -1.7832983 -1.1022232 -0.39232588 0.15457487 0.26215029 -0.012086868 -0.47761989 -0.8849957 -1.1047859 -1.1303577 -1.2025442 -1.4180617 -1.7223952][-2.0947995 -2.0799353 -1.7823608 -1.2290974 -0.55048418 0.013515949 0.31532192 0.23692083 -0.21959352 -0.78706789 -1.193512 -1.3475389 -1.3131554 -1.2126074 -1.2984457][-2.100013 -2.3962703 -2.4320343 -2.0017681 -1.3156075 -0.62708426 -0.087309837 -0.0023612976 -0.38349104 -0.96280813 -1.4647064 -1.7944725 -1.8566308 -1.6514318 -1.4758732][-2.5930409 -2.9900641 -3.2048106 -2.9888954 -2.4546046 -1.7677126 -1.2428906 -1.2471008 -1.5723543 -1.98738 -2.3178182 -2.5690966 -2.6996279 -2.618722 -2.3990767][-2.6818223 -2.9004116 -3.0618041 -3.0543118 -2.870724 -2.521426 -2.2998383 -2.4886293 -2.7404108 -2.9044247 -2.956562 -3.0002131 -3.1646843 -3.2550232 -3.1381464][-2.5253386 -2.5389862 -2.5713406 -2.6342757 -2.652195 -2.5748801 -2.5843513 -2.7576842 -2.757427 -2.6242394 -2.5548494 -2.6742797 -3.0590417 -3.4114177 -3.510077][-2.6620231 -2.6572635 -2.6643147 -2.6613784 -2.6141 -2.5249686 -2.4173846 -2.2295403 -1.8009529 -1.4156339 -1.4524512 -1.9125364 -2.6314411 -3.2192776 -3.4961016][-2.8785753 -2.8892565 -2.8850594 -2.7939644 -2.6185036 -2.4188995 -2.0798345 -1.5237019 -0.82458258 -0.4324224 -0.68107247 -1.4132524 -2.2852921 -2.9248028 -3.2716885][-2.8395772 -2.8323328 -2.7934141 -2.6670189 -2.4528131 -2.2100635 -1.7594984 -1.0813315 -0.39369106 -0.1716938 -0.57101774 -1.2925375 -2.001951 -2.4971943 -2.8730481]]...]
INFO - root - 2017-12-07 05:53:56.604261: step 11010, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.788 sec/batch; 70h:20m:04s remains)
INFO - root - 2017-12-07 05:54:04.389116: step 11020, loss = 0.75, batch loss = 0.68 (11.0 examples/sec; 0.730 sec/batch; 65h:13m:19s remains)
INFO - root - 2017-12-07 05:54:12.220353: step 11030, loss = 0.73, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 72h:17m:08s remains)
INFO - root - 2017-12-07 05:54:19.928461: step 11040, loss = 0.73, batch loss = 0.66 (10.7 examples/sec; 0.751 sec/batch; 67h:04m:24s remains)
INFO - root - 2017-12-07 05:54:27.660344: step 11050, loss = 0.71, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 71h:01m:17s remains)
INFO - root - 2017-12-07 05:54:35.304800: step 11060, loss = 0.71, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 72h:13m:47s remains)
INFO - root - 2017-12-07 05:54:42.972688: step 11070, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.775 sec/batch; 69h:12m:45s remains)
INFO - root - 2017-12-07 05:54:50.392929: step 11080, loss = 1.13, batch loss = 1.06 (10.2 examples/sec; 0.781 sec/batch; 69h:43m:13s remains)
INFO - root - 2017-12-07 05:54:57.987459: step 11090, loss = 1.01, batch loss = 0.94 (10.6 examples/sec; 0.755 sec/batch; 67h:22m:04s remains)
INFO - root - 2017-12-07 05:55:05.749460: step 11100, loss = 0.77, batch loss = 0.70 (10.8 examples/sec; 0.742 sec/batch; 66h:13m:22s remains)
2017-12-07 05:55:06.395747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0396295 -3.2064795 -3.3044221 -3.2200551 -3.1385698 -3.2409105 -3.469728 -3.6251264 -3.6202874 -3.4679127 -3.3069968 -3.3252208 -3.3702643 -3.2073593 -3.1065006][-2.4097455 -2.6784294 -2.7723989 -2.7034178 -2.6457574 -2.7281742 -2.8775487 -2.8692987 -2.7366176 -2.5982413 -2.5060878 -2.5974894 -2.6639059 -2.4852653 -2.4631009][-2.0804796 -2.2829144 -2.3043277 -2.2769022 -2.2903402 -2.4639556 -2.6374407 -2.6051874 -2.445889 -2.238878 -2.0016212 -1.9906702 -2.0766611 -2.0379725 -2.2100594][-1.9419899 -1.9424729 -1.8727229 -1.8656731 -1.9274025 -2.1762807 -2.4867597 -2.628835 -2.6197829 -2.3718758 -1.8592346 -1.560117 -1.5770404 -1.7051694 -2.0165994][-1.6996 -1.5251064 -1.4092557 -1.3902502 -1.4489763 -1.714529 -2.0513635 -2.3432837 -2.6197386 -2.5712447 -2.0238645 -1.5368822 -1.4121749 -1.4914587 -1.7992175][-1.4920123 -1.2688568 -1.0972428 -1.0014799 -0.96098709 -1.0115345 -1.0322647 -1.1602705 -1.6158798 -1.9531901 -1.591471 -1.0944369 -0.97633362 -1.1249101 -1.5452161][-1.4154856 -1.2693126 -1.1027825 -0.95550513 -0.72740078 -0.36032057 0.10683298 0.23954773 -0.32561874 -0.977885 -0.79585624 -0.38576078 -0.45265079 -0.80462122 -1.3859775][-1.4481964 -1.4387083 -1.3674793 -1.2752864 -0.89354253 -0.21754074 0.46331024 0.6943078 0.1509428 -0.50331759 -0.37129021 -0.11999702 -0.44550014 -0.91777062 -1.4754755][-1.5844538 -1.6644683 -1.6726608 -1.6107235 -1.1818621 -0.5488987 -0.052327156 0.11667633 -0.22639084 -0.62627697 -0.43465948 -0.26426172 -0.62609839 -0.94568706 -1.3401067][-1.7536628 -1.8156314 -1.8603916 -1.8525476 -1.5581954 -1.1346614 -0.77707338 -0.57185388 -0.73917294 -1.017535 -0.84240294 -0.65976286 -0.79738927 -0.83492661 -1.0611846][-1.9921505 -1.9849517 -2.0460041 -2.1668093 -2.1286449 -1.9110539 -1.5638361 -1.2480431 -1.3475125 -1.5612249 -1.4453905 -1.3530602 -1.436861 -1.3772309 -1.4317327][-2.4272835 -2.3576512 -2.4064019 -2.6251478 -2.7599185 -2.6670065 -2.3610175 -2.0914001 -2.1937528 -2.2546694 -2.0707366 -2.1485596 -2.4444358 -2.4843454 -2.4228764][-3.1695352 -3.0972278 -3.0918143 -3.2425375 -3.3781919 -3.3422265 -3.1410584 -2.9946747 -3.045188 -2.918416 -2.688704 -2.9141967 -3.397315 -3.509901 -3.4091368][-3.8125274 -3.7726383 -3.7054176 -3.7415071 -3.7984533 -3.7656317 -3.655437 -3.5821896 -3.5650022 -3.3549495 -3.1436384 -3.362762 -3.7894924 -3.8820753 -3.8242011][-4.0243907 -4.0209045 -3.9530587 -3.9304259 -3.9319496 -3.8985496 -3.8444121 -3.8094487 -3.7669275 -3.5746422 -3.373333 -3.436425 -3.633707 -3.683831 -3.7150745]]...]
INFO - root - 2017-12-07 05:55:14.058064: step 11110, loss = 1.07, batch loss = 1.00 (10.1 examples/sec; 0.795 sec/batch; 70h:57m:10s remains)
INFO - root - 2017-12-07 05:55:21.801248: step 11120, loss = 0.60, batch loss = 0.52 (9.8 examples/sec; 0.818 sec/batch; 73h:01m:12s remains)
INFO - root - 2017-12-07 05:55:29.449386: step 11130, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.763 sec/batch; 68h:07m:43s remains)
INFO - root - 2017-12-07 05:55:37.153422: step 11140, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.751 sec/batch; 67h:02m:47s remains)
INFO - root - 2017-12-07 05:55:44.802998: step 11150, loss = 0.95, batch loss = 0.88 (10.4 examples/sec; 0.767 sec/batch; 68h:27m:36s remains)
INFO - root - 2017-12-07 05:55:52.605780: step 11160, loss = 0.96, batch loss = 0.89 (10.4 examples/sec; 0.767 sec/batch; 68h:26m:26s remains)
INFO - root - 2017-12-07 05:56:00.237710: step 11170, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.763 sec/batch; 68h:07m:07s remains)
INFO - root - 2017-12-07 05:56:07.744146: step 11180, loss = 0.79, batch loss = 0.71 (10.5 examples/sec; 0.763 sec/batch; 68h:06m:12s remains)
INFO - root - 2017-12-07 05:56:15.429022: step 11190, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.779 sec/batch; 69h:32m:07s remains)
INFO - root - 2017-12-07 05:56:23.155738: step 11200, loss = 0.80, batch loss = 0.72 (10.1 examples/sec; 0.789 sec/batch; 70h:25m:59s remains)
2017-12-07 05:56:23.884861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7851615 -2.7175603 -2.5367165 -2.1800163 -1.8882737 -1.8377171 -1.8213406 -1.760597 -1.7030423 -1.6243503 -1.5379953 -1.5044119 -1.5017705 -1.4520781 -1.3422618][-2.8535171 -2.730742 -2.5173306 -2.1529353 -1.8949471 -1.9716778 -2.0971186 -2.1088371 -2.0462668 -1.9334679 -1.7752635 -1.6665843 -1.6418693 -1.610533 -1.5069368][-2.6473398 -2.4594307 -2.2119513 -1.8520119 -1.6275706 -1.8117688 -2.0976233 -2.2202845 -2.2025671 -2.1281893 -2.0052693 -1.8910756 -1.8409338 -1.7427654 -1.5567911][-2.3847718 -2.1004822 -1.801513 -1.4245713 -1.1912761 -1.445859 -1.8917713 -2.1327097 -2.1455407 -2.1089153 -2.0452635 -1.9447334 -1.8633723 -1.7075956 -1.4811008][-2.359628 -1.9764621 -1.5781634 -1.0943782 -0.71789718 -0.91934705 -1.4710407 -1.8336453 -1.8969114 -1.9061186 -1.9075301 -1.8371725 -1.7646255 -1.6514597 -1.4818237][-2.5717211 -2.1608367 -1.6521451 -0.99398232 -0.32778454 -0.27623749 -0.81024671 -1.3090107 -1.4827566 -1.5728648 -1.6748242 -1.733341 -1.8034585 -1.8474047 -1.7899506][-2.8087342 -2.4470043 -1.8641427 -1.0383351 -0.007361412 0.50855064 0.11323738 -0.62410522 -1.0810597 -1.3357546 -1.5802269 -1.8188214 -2.0730877 -2.2228768 -2.2011011][-2.9335651 -2.647635 -2.0587566 -1.1424763 0.16088867 1.152544 1.0084724 0.070484161 -0.70731711 -1.1699073 -1.536056 -1.9057388 -2.3098552 -2.5115566 -2.4713941][-2.9486194 -2.7470603 -2.2577331 -1.4366784 -0.16582918 0.946784 1.0590997 0.28385639 -0.51062274 -1.0057104 -1.3032806 -1.6002522 -2.0365574 -2.303098 -2.3286288][-2.988306 -2.86038 -2.5336027 -1.9538171 -0.97378135 -0.062316418 0.19743443 -0.1957984 -0.68838143 -0.95697331 -0.96189833 -1.014009 -1.4070904 -1.8103595 -1.971755][-3.1236529 -3.0517402 -2.8631976 -2.5154805 -1.8424549 -1.1909797 -0.89050031 -0.92412424 -1.0034964 -0.898926 -0.47686052 -0.13138008 -0.45248938 -1.1463048 -1.6007507][-3.2568512 -3.2260349 -3.1048927 -2.8642533 -2.3449955 -1.8649678 -1.6007183 -1.4584997 -1.2940331 -0.94819546 -0.2131424 0.5636797 0.465158 -0.4459548 -1.2072618][-3.2863405 -3.2807069 -3.1793561 -2.9494438 -2.4795785 -2.1088088 -1.9270678 -1.7715461 -1.5964437 -1.2938995 -0.57845831 0.33007145 0.51370907 -0.20450401 -0.95691466][-3.2410417 -3.2615561 -3.1908903 -2.9725707 -2.5505435 -2.2745006 -2.1487215 -2.0042539 -1.8818629 -1.7418318 -1.2651174 -0.52254558 -0.18655777 -0.5091846 -0.9586668][-3.2051628 -3.2519183 -3.2124176 -3.0230167 -2.6758697 -2.5136061 -2.4445577 -2.2833557 -2.1560419 -2.1016743 -1.831073 -1.3074899 -0.95531964 -0.97313094 -1.0762401]]...]
INFO - root - 2017-12-07 05:56:31.535680: step 11210, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.779 sec/batch; 69h:29m:20s remains)
INFO - root - 2017-12-07 05:56:39.268444: step 11220, loss = 0.97, batch loss = 0.89 (10.4 examples/sec; 0.766 sec/batch; 68h:20m:03s remains)
INFO - root - 2017-12-07 05:56:46.941021: step 11230, loss = 0.83, batch loss = 0.76 (9.9 examples/sec; 0.809 sec/batch; 72h:12m:34s remains)
INFO - root - 2017-12-07 05:56:54.668624: step 11240, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.772 sec/batch; 68h:51m:28s remains)
INFO - root - 2017-12-07 05:57:02.272544: step 11250, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 68h:34m:57s remains)
INFO - root - 2017-12-07 05:57:09.884077: step 11260, loss = 0.93, batch loss = 0.86 (10.4 examples/sec; 0.768 sec/batch; 68h:31m:52s remains)
INFO - root - 2017-12-07 05:57:17.694403: step 11270, loss = 0.78, batch loss = 0.71 (10.0 examples/sec; 0.798 sec/batch; 71h:13m:46s remains)
INFO - root - 2017-12-07 05:57:25.181875: step 11280, loss = 0.81, batch loss = 0.73 (10.6 examples/sec; 0.752 sec/batch; 67h:08m:23s remains)
INFO - root - 2017-12-07 05:57:32.875395: step 11290, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.782 sec/batch; 69h:45m:25s remains)
INFO - root - 2017-12-07 05:57:40.588702: step 11300, loss = 0.65, batch loss = 0.58 (10.3 examples/sec; 0.779 sec/batch; 69h:29m:00s remains)
2017-12-07 05:57:41.183562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2372377 -3.3420296 -3.5471861 -3.4368219 -3.4443774 -3.6657944 -3.5888638 -3.24295 -3.1555727 -3.2707663 -3.284 -3.3548167 -3.5878029 -3.7010112 -3.5913644][-3.0898156 -3.2031066 -3.3849869 -3.3559203 -3.5145481 -3.7568939 -3.65705 -3.405798 -3.369956 -3.4182167 -3.362293 -3.4760411 -3.723906 -3.7480626 -3.5235765][-3.0520463 -3.1956935 -3.2972167 -3.2619939 -3.3969393 -3.5685194 -3.4731154 -3.3327892 -3.3456721 -3.3268268 -3.2266486 -3.3089142 -3.4289107 -3.3391793 -3.1258039][-3.0431495 -3.0926473 -2.9559126 -2.7436132 -2.6986995 -2.7728863 -2.8212607 -2.9222865 -3.1144493 -3.2026305 -3.2473826 -3.3569236 -3.3474085 -3.2117915 -3.1161792][-2.6031489 -2.4931879 -2.2489328 -2.0584989 -2.0585878 -2.1980724 -2.4373472 -2.6783736 -2.8999133 -3.041733 -3.2277675 -3.4307547 -3.423291 -3.4072194 -3.51574][-1.8535106 -1.7601764 -1.7379673 -1.8364527 -2.0602541 -2.3125789 -2.5880656 -2.7282615 -2.7317879 -2.6727829 -2.6568975 -2.6529264 -2.5991716 -2.827899 -3.2394319][-1.6373897 -1.6917598 -1.8644397 -2.0641174 -2.2861459 -2.4406948 -2.523561 -2.443202 -2.2370806 -1.98769 -1.6686058 -1.3040838 -1.1251521 -1.5372972 -2.1837859][-1.9674959 -2.2288136 -2.4429452 -2.4491889 -2.3601191 -2.206265 -2.0018368 -1.7556741 -1.5275533 -1.2846837 -0.891134 -0.40540743 -0.20473766 -0.68031526 -1.3452256][-2.4857707 -2.8271759 -3.0438375 -2.9342856 -2.6304197 -2.2184482 -1.8234105 -1.5829098 -1.5228913 -1.4415693 -1.2259538 -1.0116985 -0.99825907 -1.3492379 -1.6834106][-3.0402212 -3.199019 -3.3316574 -3.241652 -2.9385505 -2.50078 -2.1303916 -2.1055198 -2.303571 -2.3864014 -2.33551 -2.3921618 -2.527956 -2.7070565 -2.7359781][-3.2485616 -3.2556725 -3.3411875 -3.3226335 -3.1134942 -2.8049803 -2.6396008 -2.8711317 -3.1917663 -3.2327957 -3.1469998 -3.2574046 -3.3783734 -3.3830838 -3.2583385][-3.4747875 -3.3782487 -3.390265 -3.37637 -3.2386079 -3.0982461 -3.1826911 -3.552717 -3.8037405 -3.6450176 -3.3892274 -3.3953581 -3.3954122 -3.2417121 -3.0628893][-3.2182879 -3.0728426 -3.0573444 -3.0616183 -3.0195594 -3.1234431 -3.4487433 -3.8314776 -3.9618242 -3.7376277 -3.4773164 -3.4017067 -3.2726583 -3.0442109 -2.9033432][-2.4583788 -2.4054189 -2.4203987 -2.4314051 -2.4999771 -2.8522897 -3.2850385 -3.5226054 -3.5362465 -3.4502919 -3.4163334 -3.3861668 -3.2439826 -3.063138 -3.0061417][-2.4785814 -2.5655847 -2.5505857 -2.4137013 -2.4785492 -2.8998 -3.2348876 -3.2305431 -3.1216605 -3.1731412 -3.3851266 -3.4477344 -3.3422611 -3.2220709 -3.1653366]]...]
INFO - root - 2017-12-07 05:57:48.937303: step 11310, loss = 0.62, batch loss = 0.55 (10.1 examples/sec; 0.792 sec/batch; 70h:38m:35s remains)
INFO - root - 2017-12-07 05:57:56.773405: step 11320, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.784 sec/batch; 69h:58m:59s remains)
INFO - root - 2017-12-07 05:58:04.511392: step 11330, loss = 0.60, batch loss = 0.52 (10.6 examples/sec; 0.757 sec/batch; 67h:31m:50s remains)
INFO - root - 2017-12-07 05:58:12.175958: step 11340, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.779 sec/batch; 69h:27m:42s remains)
INFO - root - 2017-12-07 05:58:19.867453: step 11350, loss = 1.00, batch loss = 0.93 (10.5 examples/sec; 0.765 sec/batch; 68h:12m:43s remains)
INFO - root - 2017-12-07 05:58:27.562148: step 11360, loss = 0.78, batch loss = 0.70 (10.5 examples/sec; 0.762 sec/batch; 67h:58m:26s remains)
INFO - root - 2017-12-07 05:58:35.289623: step 11370, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.768 sec/batch; 68h:28m:08s remains)
INFO - root - 2017-12-07 05:58:42.739165: step 11380, loss = 0.93, batch loss = 0.85 (10.6 examples/sec; 0.752 sec/batch; 67h:07m:11s remains)
INFO - root - 2017-12-07 05:58:50.437265: step 11390, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.760 sec/batch; 67h:47m:10s remains)
INFO - root - 2017-12-07 05:58:58.178285: step 11400, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.768 sec/batch; 68h:30m:05s remains)
2017-12-07 05:58:58.790984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2077298 -3.1309061 -3.2543597 -3.3732691 -3.3764288 -3.3747714 -3.3126221 -3.2501822 -3.2437649 -3.2436512 -3.2275376 -3.1967497 -3.1853437 -3.1570969 -3.1247506][-3.1473262 -2.9956384 -3.1509109 -3.3645496 -3.4381158 -3.4944746 -3.3987122 -3.2573812 -3.2347016 -3.2429013 -3.2030067 -3.1358411 -3.1149583 -3.0809193 -3.0464351][-3.1881208 -2.9046874 -3.015974 -3.3203568 -3.5146365 -3.6314611 -3.4597969 -3.1814651 -3.0997524 -3.0999725 -3.0319412 -2.9473844 -2.9652319 -2.9624324 -2.9343436][-3.541198 -3.0985155 -3.0225358 -3.3016372 -3.565444 -3.684273 -3.418479 -3.0333881 -2.9221973 -2.9299715 -2.8345528 -2.7339401 -2.8189435 -2.8731558 -2.8414214][-3.9397159 -3.3186111 -2.9985325 -3.1632304 -3.3909595 -3.4351478 -3.0813322 -2.6541429 -2.5927653 -2.6784747 -2.6346807 -2.5803404 -2.7419295 -2.8583219 -2.801085][-3.8290017 -3.0870457 -2.6180861 -2.7039347 -2.8384619 -2.7484217 -2.3189905 -1.9052191 -1.9214904 -2.130342 -2.2118669 -2.2925253 -2.5827065 -2.8127561 -2.7669508][-3.5331981 -2.8609948 -2.3964086 -2.4371071 -2.4926975 -2.2796438 -1.8215706 -1.4444871 -1.4625118 -1.6730218 -1.8105192 -1.99366 -2.3910234 -2.7307801 -2.7359791][-3.6544836 -3.2111092 -2.8419094 -2.817667 -2.8069315 -2.5232086 -2.1127527 -1.7909849 -1.7247586 -1.7572038 -1.7932196 -1.9503365 -2.3716335 -2.7736773 -2.7989383][-3.9958997 -3.8019609 -3.5576222 -3.4884892 -3.3973298 -3.0627456 -2.7512012 -2.5591216 -2.4378939 -2.2915413 -2.1834002 -2.2598376 -2.6546783 -3.0540969 -3.0179663][-4.3016925 -4.2764783 -4.1481853 -4.0645514 -3.8816648 -3.5306683 -3.3480575 -3.2935381 -3.1396303 -2.8585393 -2.6198611 -2.6213975 -3.0081873 -3.4261665 -3.3458209][-4.1306696 -4.1548419 -4.1419835 -4.1143351 -3.9028742 -3.5441885 -3.4072123 -3.3766737 -3.1777697 -2.8279037 -2.5338144 -2.5169446 -2.9702482 -3.5046215 -3.5067952][-3.6803195 -3.692575 -3.6842945 -3.683217 -3.5085585 -3.19168 -3.0490327 -2.956213 -2.6676335 -2.2731493 -2.013654 -2.0858347 -2.6491766 -3.3155169 -3.4665132][-3.4329422 -3.4595795 -3.3905592 -3.3716726 -3.2801194 -3.0798345 -2.9914312 -2.8729868 -2.5528536 -2.1893609 -2.0049305 -2.1319168 -2.6806741 -3.3039608 -3.4725289][-3.128782 -3.175549 -3.0717287 -3.0543077 -3.0621271 -3.0086122 -3.0360315 -2.9960809 -2.7752414 -2.5491076 -2.4762487 -2.6256747 -3.0382295 -3.4553003 -3.525651][-2.9764376 -3.0120313 -2.9069822 -2.892827 -2.9630036 -2.9955404 -3.077425 -3.0890574 -2.9848027 -2.8922517 -2.8964787 -3.0211191 -3.2725563 -3.4748983 -3.4627964]]...]
INFO - root - 2017-12-07 05:59:06.573846: step 11410, loss = 1.00, batch loss = 0.92 (10.1 examples/sec; 0.790 sec/batch; 70h:28m:20s remains)
INFO - root - 2017-12-07 05:59:14.429013: step 11420, loss = 0.81, batch loss = 0.74 (10.1 examples/sec; 0.793 sec/batch; 70h:41m:47s remains)
INFO - root - 2017-12-07 05:59:22.246906: step 11430, loss = 0.85, batch loss = 0.77 (10.5 examples/sec; 0.760 sec/batch; 67h:48m:27s remains)
INFO - root - 2017-12-07 05:59:29.965474: step 11440, loss = 0.75, batch loss = 0.67 (10.4 examples/sec; 0.771 sec/batch; 68h:44m:26s remains)
INFO - root - 2017-12-07 05:59:37.777383: step 11450, loss = 0.53, batch loss = 0.46 (10.7 examples/sec; 0.745 sec/batch; 66h:26m:56s remains)
INFO - root - 2017-12-07 05:59:45.453038: step 11460, loss = 0.72, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 68h:30m:57s remains)
INFO - root - 2017-12-07 05:59:53.039254: step 11470, loss = 0.82, batch loss = 0.75 (10.8 examples/sec; 0.743 sec/batch; 66h:15m:11s remains)
INFO - root - 2017-12-07 06:00:00.426805: step 11480, loss = 0.82, batch loss = 0.74 (10.3 examples/sec; 0.774 sec/batch; 68h:59m:27s remains)
INFO - root - 2017-12-07 06:00:07.996357: step 11490, loss = 0.97, batch loss = 0.89 (10.6 examples/sec; 0.757 sec/batch; 67h:31m:44s remains)
INFO - root - 2017-12-07 06:00:15.532730: step 11500, loss = 1.18, batch loss = 1.10 (11.1 examples/sec; 0.724 sec/batch; 64h:33m:06s remains)
2017-12-07 06:00:16.210978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6321015 -2.5153241 -2.2190652 -1.8205295 -1.4415858 -1.2291105 -1.3186257 -1.5049846 -1.3260071 -1.167798 -1.4796658 -2.0368292 -2.4695716 -2.5011125 -2.2830904][-2.420346 -2.4448357 -2.2172439 -1.7777743 -1.2670109 -0.91438746 -0.9232161 -0.99551177 -0.74581075 -0.58822441 -1.0456574 -1.9059231 -2.5671756 -2.5892849 -2.2538474][-2.1894329 -2.4337721 -2.3120112 -1.8119786 -1.1352918 -0.61680079 -0.5016911 -0.40407753 -0.1115694 -0.085790634 -0.82005143 -2.078371 -2.9765887 -2.8911152 -2.3267324][-2.0221498 -2.4707375 -2.4369826 -1.8743939 -1.0918519 -0.4976294 -0.30670166 -0.070793152 0.25101948 0.093246937 -0.91093564 -2.4773045 -3.5149803 -3.2746568 -2.4616704][-1.6982303 -2.3143229 -2.4392056 -1.9338474 -1.1507051 -0.52391005 -0.2698245 0.022310257 0.31429863 -0.044357777 -1.2478533 -2.929009 -3.9488595 -3.5644794 -2.5724602][-1.1073003 -1.9314005 -2.3229911 -1.9363987 -1.1343789 -0.31940937 0.17529106 0.52856636 0.63705063 -0.076298237 -1.5923846 -3.3800573 -4.313396 -3.7720714 -2.6238742][-0.11041784 -1.1937287 -1.9790478 -1.7730033 -0.87968087 0.29673338 1.1929746 1.613287 1.3502021 0.12687778 -1.8011928 -3.7203531 -4.5781994 -3.9248338 -2.6357558][1.1244669 -0.11483335 -1.3584862 -1.5491927 -0.65008473 0.9500823 2.382946 2.918921 2.2467971 0.52826214 -1.7554293 -3.780761 -4.6228232 -3.9614487 -2.6161113][1.9941087 0.88401556 -0.62797093 -1.3127325 -0.65770531 1.1382918 3.0095096 3.7648554 2.9151411 0.93170118 -1.5891502 -3.6780481 -4.5289359 -3.9390278 -2.6058223][1.958612 1.147491 -0.29482937 -1.2684259 -0.99353576 0.658134 2.5829754 3.5309854 2.8378086 0.90390205 -1.6655433 -3.6910436 -4.4618154 -3.9070644 -2.5955877][1.3957319 0.8691287 -0.32776737 -1.4226005 -1.5200701 -0.21348858 1.4993067 2.5017376 2.177628 0.56086493 -1.8672132 -3.7734563 -4.451623 -3.9019089 -2.6120062][0.82029104 0.44925117 -0.49809241 -1.4900477 -1.793088 -0.87386537 0.46684647 1.2943807 1.2106299 0.0022916794 -2.0581 -3.7578125 -4.3578415 -3.8333662 -2.6449003][0.10633326 -0.25356913 -0.94440293 -1.6288733 -1.8985379 -1.3691866 -0.49703979 0.094984531 0.094355106 -0.71884775 -2.2541931 -3.6163495 -4.0949316 -3.6340637 -2.6547337][-0.86514306 -1.1485217 -1.5850601 -1.9860492 -2.2171102 -2.0185926 -1.5296447 -1.1409984 -1.0999274 -1.5591614 -2.5444951 -3.47905 -3.7728868 -3.3637142 -2.6381421][-1.8671014 -1.9966757 -2.2254496 -2.4271975 -2.5875714 -2.5840139 -2.3816593 -2.1876082 -2.1820331 -2.42011 -2.9455919 -3.4446287 -3.5125036 -3.1324482 -2.6234388]]...]
INFO - root - 2017-12-07 06:00:23.749445: step 11510, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.766 sec/batch; 68h:17m:15s remains)
INFO - root - 2017-12-07 06:00:31.312892: step 11520, loss = 0.67, batch loss = 0.60 (10.9 examples/sec; 0.731 sec/batch; 65h:11m:17s remains)
INFO - root - 2017-12-07 06:00:38.971668: step 11530, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.776 sec/batch; 69h:10m:37s remains)
INFO - root - 2017-12-07 06:00:46.623969: step 11540, loss = 0.87, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 67h:15m:22s remains)
INFO - root - 2017-12-07 06:00:54.240599: step 11550, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.769 sec/batch; 68h:35m:53s remains)
INFO - root - 2017-12-07 06:01:01.799271: step 11560, loss = 0.75, batch loss = 0.67 (10.4 examples/sec; 0.769 sec/batch; 68h:34m:51s remains)
INFO - root - 2017-12-07 06:01:09.419621: step 11570, loss = 0.64, batch loss = 0.57 (10.4 examples/sec; 0.768 sec/batch; 68h:27m:39s remains)
INFO - root - 2017-12-07 06:01:16.828453: step 11580, loss = 1.08, batch loss = 1.01 (10.5 examples/sec; 0.758 sec/batch; 67h:36m:31s remains)
INFO - root - 2017-12-07 06:01:24.541359: step 11590, loss = 0.87, batch loss = 0.80 (10.9 examples/sec; 0.734 sec/batch; 65h:24m:42s remains)
INFO - root - 2017-12-07 06:01:32.165966: step 11600, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.774 sec/batch; 68h:58m:17s remains)
2017-12-07 06:01:32.770936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6486695 -3.0383563 -3.5710223 -4.0100503 -4.2084951 -4.1167746 -3.9011478 -3.7151709 -3.6031988 -3.6000104 -3.6444464 -3.6947908 -3.7577686 -3.8240018 -3.8944201][-2.6250339 -3.1959629 -3.9927337 -4.6689439 -4.9996614 -4.9120064 -4.6198225 -4.3795576 -4.2520571 -4.2515373 -4.3352337 -4.4347639 -4.5406175 -4.6488366 -4.744596][-2.4370685 -3.0903349 -4.0497336 -4.8910728 -5.3327346 -5.2923765 -4.9480777 -4.6838837 -4.61744 -4.685812 -4.8530765 -5.0309782 -5.1839576 -5.3423138 -5.4585342][-2.1547706 -2.7124188 -3.61517 -4.4305358 -4.8836303 -4.8639812 -4.448472 -4.1495023 -4.189795 -4.4180613 -4.772748 -5.1030474 -5.3005905 -5.4565907 -5.5529037][-1.9575524 -2.3620636 -3.0569491 -3.6361921 -3.8772316 -3.7345135 -3.2323818 -2.920481 -3.0674005 -3.4850025 -4.0999336 -4.6486588 -4.9119506 -5.022213 -5.0531988][-1.9663568 -2.3123515 -2.793293 -3.0691113 -2.9763017 -2.6132932 -2.0261757 -1.6656981 -1.844065 -2.3891883 -3.235805 -4.0140114 -4.3756933 -4.4888577 -4.487864][-2.1548963 -2.510139 -2.8209279 -2.9045372 -2.6082916 -2.1136272 -1.4447331 -0.95144367 -1.0647573 -1.6682606 -2.6485918 -3.5676868 -3.9811127 -4.0833516 -4.09283][-2.4206741 -2.7291188 -2.8629522 -2.8554516 -2.5353351 -2.0293787 -1.2976892 -0.65851068 -0.68912959 -1.3512392 -2.3981824 -3.3425252 -3.7107749 -3.7288046 -3.7407157][-2.6141863 -2.8565049 -2.8455982 -2.7769227 -2.4840412 -1.9907308 -1.2640646 -0.61379862 -0.64346862 -1.3250487 -2.3509939 -3.2025108 -3.4649534 -3.37453 -3.3502784][-2.7029042 -2.9595461 -2.8974349 -2.7912304 -2.4896865 -1.9811285 -1.3324628 -0.79066515 -0.85117888 -1.4980574 -2.4222102 -3.0987625 -3.2397916 -3.0950139 -3.0560596][-2.7637265 -3.076489 -3.0202222 -2.9091163 -2.5974109 -2.1119349 -1.5834656 -1.1400998 -1.1856639 -1.7831032 -2.6063964 -3.1278563 -3.1791453 -3.0083628 -2.9434505][-2.7280977 -3.1038432 -3.1203828 -3.0614648 -2.803607 -2.3850617 -1.9523301 -1.5322995 -1.5173752 -2.0414081 -2.7735698 -3.1861641 -3.2024698 -3.0329695 -2.9305856][-2.5180006 -2.8778296 -2.9618025 -2.968574 -2.8120892 -2.5106692 -2.1561511 -1.7448063 -1.6872241 -2.1239333 -2.7416155 -3.0763409 -3.1143355 -2.9906511 -2.8810787][-2.3216174 -2.5621536 -2.6293566 -2.6388652 -2.5656948 -2.3928671 -2.118741 -1.7554226 -1.6823859 -1.9944701 -2.4434779 -2.6898096 -2.7702618 -2.7351098 -2.6757956][-2.4207408 -2.5349956 -2.5289936 -2.47456 -2.4221647 -2.3358028 -2.1425192 -1.8683581 -1.8047912 -1.995311 -2.2678502 -2.4259727 -2.5228715 -2.5429893 -2.5174544]]...]
INFO - root - 2017-12-07 06:01:40.425391: step 11610, loss = 0.78, batch loss = 0.71 (10.7 examples/sec; 0.750 sec/batch; 66h:51m:52s remains)
INFO - root - 2017-12-07 06:01:48.057709: step 11620, loss = 0.59, batch loss = 0.51 (10.5 examples/sec; 0.762 sec/batch; 67h:55m:21s remains)
INFO - root - 2017-12-07 06:01:55.648079: step 11630, loss = 0.73, batch loss = 0.66 (10.9 examples/sec; 0.735 sec/batch; 65h:31m:05s remains)
INFO - root - 2017-12-07 06:02:03.250182: step 11640, loss = 0.76, batch loss = 0.68 (10.5 examples/sec; 0.759 sec/batch; 67h:37m:45s remains)
INFO - root - 2017-12-07 06:02:10.912998: step 11650, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.763 sec/batch; 67h:58m:53s remains)
INFO - root - 2017-12-07 06:02:18.592045: step 11660, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.777 sec/batch; 69h:14m:38s remains)
INFO - root - 2017-12-07 06:02:26.255963: step 11670, loss = 1.06, batch loss = 0.99 (10.5 examples/sec; 0.761 sec/batch; 67h:50m:33s remains)
INFO - root - 2017-12-07 06:02:33.701348: step 11680, loss = 1.06, batch loss = 0.98 (9.9 examples/sec; 0.810 sec/batch; 72h:11m:39s remains)
INFO - root - 2017-12-07 06:02:41.287944: step 11690, loss = 0.89, batch loss = 0.82 (10.8 examples/sec; 0.741 sec/batch; 66h:03m:54s remains)
INFO - root - 2017-12-07 06:02:48.837323: step 11700, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.773 sec/batch; 68h:53m:38s remains)
2017-12-07 06:02:49.453283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5766745 -2.4483321 -2.7120848 -2.9557824 -3.2096381 -3.4896555 -3.3594112 -2.7736585 -2.4889259 -2.6680746 -2.4716473 -1.8942776 -1.6869643 -2.1391549 -2.3130214][-2.6079097 -2.4715176 -2.6141782 -2.6960282 -2.9238889 -3.3311558 -3.3727951 -2.9524856 -2.7618604 -2.8549023 -2.4636223 -1.8116136 -1.6964765 -2.2305744 -2.4238453][-2.409265 -2.1198771 -2.2016027 -2.41333 -2.8125303 -3.3226933 -3.4945419 -3.3314722 -3.3018491 -3.2737775 -2.6297438 -1.8285677 -1.7433298 -2.3827231 -2.6731181][-2.1274607 -1.7377882 -1.9127321 -2.4298515 -3.0294957 -3.5147877 -3.6420617 -3.5232332 -3.4791541 -3.2926805 -2.5627732 -1.8143113 -1.8361409 -2.609488 -3.0198][-1.8797121 -1.5432467 -1.9473846 -2.7434969 -3.3654475 -3.5821629 -3.395551 -3.0952377 -2.971606 -2.7845087 -2.2192829 -1.6983981 -1.9001338 -2.82417 -3.3718405][-1.8205597 -1.6034195 -2.1083074 -2.9564977 -3.3759265 -3.1613193 -2.5616798 -2.163898 -2.2447665 -2.3490679 -2.0488024 -1.6852746 -1.977777 -2.951695 -3.5692177][-1.7115409 -1.5737815 -2.0209734 -2.6758318 -2.751967 -2.1104128 -1.2297056 -0.94045019 -1.4912987 -1.9686799 -1.8358636 -1.5152049 -1.847867 -2.7731094 -3.3537016][-1.7709916 -1.6806223 -2.006218 -2.4031873 -2.1851709 -1.3081985 -0.36616325 -0.28145504 -1.1864457 -1.8065658 -1.6941395 -1.3983333 -1.7133338 -2.4318681 -2.8198152][-2.3395283 -2.3700423 -2.5842915 -2.7633173 -2.4329257 -1.6427927 -0.93596935 -1.0223033 -1.8661165 -2.2910111 -2.1002264 -1.8711886 -2.0806992 -2.436265 -2.5317745][-2.80057 -2.9394832 -3.0694594 -3.1110878 -2.792872 -2.1955695 -1.7530668 -1.8798699 -2.4714398 -2.6600342 -2.5460706 -2.5346651 -2.6557646 -2.6720984 -2.5652721][-3.0441594 -3.163193 -3.2182488 -3.2555881 -3.0677013 -2.60706 -2.2621679 -2.2918167 -2.5968089 -2.6278296 -2.6766567 -2.9342909 -2.9977005 -2.7701037 -2.5511165][-3.3837166 -3.4363315 -3.4795642 -3.6276402 -3.5947421 -3.147604 -2.7578788 -2.6463957 -2.6749482 -2.5442824 -2.6694217 -3.135463 -3.1984293 -2.8065479 -2.5027208][-3.6178422 -3.5522676 -3.5547452 -3.7943749 -3.8662047 -3.3890228 -2.9439566 -2.7199373 -2.5408113 -2.2979429 -2.5038619 -3.18499 -3.3687816 -2.9280756 -2.6072721][-3.4424331 -3.3311043 -3.331398 -3.6092105 -3.6762347 -3.2026672 -2.7596314 -2.4435122 -2.0959387 -1.806829 -2.1636064 -3.0842681 -3.4667594 -3.1228733 -2.8833752][-3.1993058 -3.1869152 -3.2567804 -3.4978085 -3.4628923 -3.0604119 -2.7410192 -2.4031651 -1.8852961 -1.5138378 -1.9948957 -3.0770478 -3.5864213 -3.3119717 -3.0541596]]...]
INFO - root - 2017-12-07 06:02:57.070991: step 11710, loss = 0.64, batch loss = 0.57 (10.8 examples/sec; 0.743 sec/batch; 66h:10m:39s remains)
INFO - root - 2017-12-07 06:03:04.774816: step 11720, loss = 0.92, batch loss = 0.85 (10.3 examples/sec; 0.777 sec/batch; 69h:12m:52s remains)
INFO - root - 2017-12-07 06:03:12.524180: step 11730, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.776 sec/batch; 69h:10m:53s remains)
INFO - root - 2017-12-07 06:03:20.342094: step 11740, loss = 0.65, batch loss = 0.58 (10.1 examples/sec; 0.793 sec/batch; 70h:40m:20s remains)
INFO - root - 2017-12-07 06:03:27.903573: step 11750, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.747 sec/batch; 66h:31m:56s remains)
INFO - root - 2017-12-07 06:03:35.579296: step 11760, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.748 sec/batch; 66h:38m:30s remains)
INFO - root - 2017-12-07 06:03:43.373510: step 11770, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.781 sec/batch; 69h:35m:48s remains)
INFO - root - 2017-12-07 06:03:50.848704: step 11780, loss = 0.90, batch loss = 0.82 (10.3 examples/sec; 0.775 sec/batch; 69h:02m:33s remains)
INFO - root - 2017-12-07 06:03:58.544012: step 11790, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 66h:25m:21s remains)
INFO - root - 2017-12-07 06:04:06.189039: step 11800, loss = 1.01, batch loss = 0.94 (10.6 examples/sec; 0.755 sec/batch; 67h:17m:50s remains)
2017-12-07 06:04:06.796636: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6206703 -3.6922302 -3.6972129 -3.6759522 -3.6730561 -3.6210623 -3.5268242 -3.4251461 -3.4147091 -3.4235559 -3.3435163 -3.3013022 -3.2621479 -3.1234095 -2.9900696][-3.081286 -3.1853056 -3.1799963 -3.092406 -3.0196371 -2.9199908 -2.7988319 -2.6937265 -2.6741736 -2.7035892 -2.6879044 -2.7402403 -2.8082743 -2.7258587 -2.6502986][-2.9621634 -3.1127603 -3.132606 -2.9913387 -2.7992806 -2.5780058 -2.395782 -2.3044555 -2.3217485 -2.4392865 -2.5268242 -2.620193 -2.6494293 -2.4937105 -2.3932049][-3.1887434 -3.236969 -3.2168663 -3.0893474 -2.8897915 -2.656667 -2.4994497 -2.4597168 -2.4746966 -2.588608 -2.6836791 -2.7210374 -2.6300573 -2.3806932 -2.2573831][-3.1674938 -3.0741057 -2.9836905 -2.9060411 -2.7846682 -2.6350923 -2.5633264 -2.595757 -2.6445942 -2.794003 -2.9265487 -2.9272327 -2.7584507 -2.4549174 -2.297302][-2.9082091 -2.8627868 -2.8131673 -2.7735133 -2.67797 -2.5452237 -2.502439 -2.55734 -2.666255 -2.9322193 -3.1755219 -3.2264428 -3.0643425 -2.7449756 -2.5198441][-2.561825 -2.596597 -2.5999126 -2.5524883 -2.3981986 -2.204005 -2.10134 -2.0625439 -2.1141551 -2.3801968 -2.6548147 -2.8244576 -2.8630595 -2.7443516 -2.6352506][-1.8199124 -1.8268402 -1.8217511 -1.7943025 -1.6941988 -1.6026814 -1.5815637 -1.5029244 -1.45437 -1.5813303 -1.6866546 -1.8384857 -2.0516872 -2.2082734 -2.3733027][-1.269573 -1.2234159 -1.1889484 -1.2050519 -1.2685139 -1.4466209 -1.6636434 -1.6632917 -1.5951564 -1.6040316 -1.5037589 -1.4675999 -1.5692317 -1.7140603 -1.9828572][-1.6623337 -1.6372976 -1.5502062 -1.4666188 -1.5204227 -1.814158 -2.1901526 -2.2976916 -2.2802904 -2.2715645 -2.1233821 -2.0197313 -1.9588723 -1.8756912 -1.978797][-2.2260683 -2.3216898 -2.2570844 -2.0628064 -1.9565706 -2.1211207 -2.412415 -2.5260825 -2.5731225 -2.6017509 -2.4936333 -2.4508996 -2.4005821 -2.2404404 -2.2321913][-2.1844473 -2.4072411 -2.4761467 -2.3549733 -2.1980741 -2.1748011 -2.2311938 -2.2421844 -2.3152742 -2.3914032 -2.3432946 -2.3948903 -2.4591236 -2.381762 -2.418546][-2.1324258 -2.3200953 -2.4446011 -2.4534438 -2.3894062 -2.3271954 -2.2563329 -2.1808064 -2.2253616 -2.2935407 -2.2776632 -2.3710322 -2.5044742 -2.502682 -2.5823474][-2.568532 -2.6646061 -2.7571712 -2.8176861 -2.8422995 -2.8451061 -2.8158445 -2.7771177 -2.8092046 -2.8383548 -2.7961235 -2.8249803 -2.8810601 -2.8220108 -2.8369939][-3.1058807 -3.15158 -3.1831803 -3.2115963 -3.243578 -3.2782171 -3.300359 -3.3145828 -3.358726 -3.3907351 -3.355895 -3.3230159 -3.2693295 -3.1452682 -3.091042]]...]
INFO - root - 2017-12-07 06:04:14.458473: step 11810, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.757 sec/batch; 67h:23m:59s remains)
INFO - root - 2017-12-07 06:04:22.155309: step 11820, loss = 0.75, batch loss = 0.68 (9.9 examples/sec; 0.809 sec/batch; 72h:03m:22s remains)
INFO - root - 2017-12-07 06:04:29.773543: step 11830, loss = 0.66, batch loss = 0.59 (10.6 examples/sec; 0.757 sec/batch; 67h:24m:49s remains)
INFO - root - 2017-12-07 06:04:37.413172: step 11840, loss = 1.10, batch loss = 1.02 (10.9 examples/sec; 0.733 sec/batch; 65h:16m:27s remains)
INFO - root - 2017-12-07 06:04:44.965209: step 11850, loss = 0.83, batch loss = 0.76 (10.7 examples/sec; 0.747 sec/batch; 66h:29m:53s remains)
INFO - root - 2017-12-07 06:04:52.556309: step 11860, loss = 0.64, batch loss = 0.56 (11.1 examples/sec; 0.724 sec/batch; 64h:27m:18s remains)
INFO - root - 2017-12-07 06:05:00.187331: step 11870, loss = 0.65, batch loss = 0.58 (10.3 examples/sec; 0.779 sec/batch; 69h:25m:29s remains)
INFO - root - 2017-12-07 06:05:07.682227: step 11880, loss = 0.76, batch loss = 0.69 (10.2 examples/sec; 0.785 sec/batch; 69h:53m:29s remains)
INFO - root - 2017-12-07 06:05:15.377345: step 11890, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.782 sec/batch; 69h:38m:57s remains)
INFO - root - 2017-12-07 06:05:23.118663: step 11900, loss = 0.88, batch loss = 0.81 (10.7 examples/sec; 0.747 sec/batch; 66h:28m:54s remains)
2017-12-07 06:05:23.789363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6047654 -1.8773901 -2.107204 -3.0794466 -3.3946404 -2.9907961 -2.2549412 -1.4914367 -0.76125264 -0.17660761 -0.44203973 -1.1960135 -1.2766175 -0.935992 -0.91077614][-2.5193405 -2.0254719 -2.3503504 -3.0294652 -3.080193 -2.6890979 -2.0384459 -1.4522953 -0.79772305 -0.15642166 -0.45087624 -1.1572454 -1.1388776 -0.87258363 -0.9934783][-2.6748137 -2.4297209 -2.6755314 -2.8947077 -2.7278357 -2.4398599 -1.8622696 -1.3227348 -0.7242403 -0.21998072 -0.65978622 -1.3531442 -1.252656 -0.95621634 -1.055526][-2.7006226 -2.6043887 -2.6368389 -2.3800931 -2.1205082 -2.0637932 -1.4581878 -0.73930573 -0.26813745 -0.21476078 -0.87329483 -1.4980206 -1.34726 -0.94255447 -0.88168097][-2.6399109 -2.6607251 -2.5197563 -2.0137908 -1.8179579 -1.9593701 -1.152117 -0.0753994 0.15964937 -0.38166142 -1.2010245 -1.701138 -1.5075212 -0.87677646 -0.51450372][-2.6418033 -2.7224247 -2.4510961 -1.8787448 -1.8273196 -2.0362189 -0.80148125 0.76456594 0.70785761 -0.48439527 -1.4940093 -1.9892552 -1.8027332 -0.81500411 -0.1165328][-2.5898812 -2.712008 -2.2986293 -1.7676516 -1.9165936 -2.0708044 -0.20741415 1.9929676 1.6210008 -0.3048439 -1.6402073 -2.3278494 -2.1678002 -0.73384929 0.23213053][-2.4555993 -2.5744202 -2.0014377 -1.5128598 -1.7827151 -1.7903152 0.70600462 3.5031114 2.8650732 0.29008389 -1.4960968 -2.5718987 -2.465436 -0.64646769 0.4130497][-2.3814056 -2.4813838 -1.7993748 -1.3306663 -1.6327755 -1.5640812 1.2108765 4.3377352 3.6034641 0.77099514 -1.3484695 -2.7962904 -2.6736693 -0.59847522 0.37138748][-2.6201904 -2.8672361 -2.2161806 -1.7966366 -2.1313303 -2.0831861 0.672873 3.89884 3.3583407 0.77291441 -1.4386296 -3.0080144 -2.7137134 -0.5397861 0.16636515][-2.8606896 -3.2996221 -2.7560594 -2.3876464 -2.7494807 -2.7879581 -0.24588299 2.8271856 2.6090202 0.52927303 -1.5523489 -2.9987316 -2.4769955 -0.39937544 -0.062383175][-2.7071428 -3.1799095 -2.691365 -2.3778827 -2.7937899 -2.9833703 -1.0009005 1.4333682 1.4308863 -0.06330204 -1.8868308 -3.094501 -2.3459222 -0.41460371 -0.33043909][-2.3625052 -2.7867193 -2.3662698 -2.1019945 -2.4996388 -2.7862718 -1.5159969 0.044624805 0.17206526 -0.73649669 -2.266552 -3.2316968 -2.3288472 -0.57097387 -0.59628677][-2.0352578 -2.3500397 -2.0304666 -1.9087899 -2.3720992 -2.758739 -2.1352444 -1.3211248 -1.1027424 -1.5030556 -2.6607533 -3.3571849 -2.3630168 -0.78871989 -0.80333614][-1.9494727 -2.1579847 -1.9478364 -1.9687004 -2.4663734 -2.898747 -2.6922603 -2.3398767 -2.102572 -2.1967669 -3.018482 -3.4984505 -2.5209498 -1.1060803 -0.9950738]]...]
INFO - root - 2017-12-07 06:05:31.396843: step 11910, loss = 0.86, batch loss = 0.78 (10.4 examples/sec; 0.771 sec/batch; 68h:39m:23s remains)
INFO - root - 2017-12-07 06:05:39.181586: step 11920, loss = 1.00, batch loss = 0.93 (10.6 examples/sec; 0.754 sec/batch; 67h:07m:04s remains)
INFO - root - 2017-12-07 06:05:46.919467: step 11930, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.778 sec/batch; 69h:18m:50s remains)
INFO - root - 2017-12-07 06:05:54.578531: step 11940, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.774 sec/batch; 68h:54m:08s remains)
INFO - root - 2017-12-07 06:06:02.281115: step 11950, loss = 0.68, batch loss = 0.60 (10.5 examples/sec; 0.759 sec/batch; 67h:35m:16s remains)
INFO - root - 2017-12-07 06:06:10.014808: step 11960, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.758 sec/batch; 67h:28m:08s remains)
INFO - root - 2017-12-07 06:06:17.731124: step 11970, loss = 0.75, batch loss = 0.68 (9.9 examples/sec; 0.808 sec/batch; 71h:54m:28s remains)
INFO - root - 2017-12-07 06:06:25.073459: step 11980, loss = 0.95, batch loss = 0.88 (10.3 examples/sec; 0.779 sec/batch; 69h:21m:52s remains)
INFO - root - 2017-12-07 06:06:32.709025: step 11990, loss = 0.75, batch loss = 0.68 (10.7 examples/sec; 0.750 sec/batch; 66h:44m:47s remains)
INFO - root - 2017-12-07 06:06:40.238114: step 12000, loss = 0.83, batch loss = 0.75 (10.8 examples/sec; 0.739 sec/batch; 65h:45m:23s remains)
2017-12-07 06:06:40.848187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8275084 -3.8990846 -3.9115567 -3.9574821 -4.0366297 -4.055831 -4.0745335 -4.1002455 -4.0987988 -4.0687685 -4.0348921 -4.0402541 -4.0228195 -3.9568858 -3.7709398][-4.0167246 -3.9990056 -3.9783466 -4.0286217 -4.1175294 -4.1363015 -4.1402154 -4.1655221 -4.181602 -4.178133 -4.1604009 -4.1598873 -4.0850973 -3.9607956 -3.7031581][-3.9990039 -3.9749761 -3.9438932 -4.0058031 -4.1150522 -4.1436591 -4.1303439 -4.1153326 -4.1183128 -4.1449652 -4.1663084 -4.1877079 -4.0621929 -3.8550115 -3.5218031][-3.8959045 -3.9216213 -3.9400854 -4.0334392 -4.1285872 -4.1021452 -4.0047035 -3.8816071 -3.7988458 -3.8129444 -3.8589761 -3.9409974 -3.8350878 -3.5688229 -3.1340246][-3.9146705 -3.9307895 -3.9831262 -4.1110115 -4.1982365 -4.1275144 -3.9648216 -3.7251382 -3.4846749 -3.4149828 -3.4751968 -3.6861949 -3.7598174 -3.5650105 -3.072154][-3.8711255 -3.8452144 -3.8648348 -3.9722075 -4.0303349 -3.9393845 -3.7477007 -3.3594372 -2.8676457 -2.6638522 -2.787446 -3.2316411 -3.6793439 -3.7912135 -3.4610908][-3.0528688 -3.0425339 -3.0663126 -3.1734457 -3.2230906 -3.1278961 -2.9240718 -2.4028914 -1.6970212 -1.4437087 -1.7545457 -2.5268075 -3.3993294 -3.8555486 -3.7792518][-2.2582421 -2.1487947 -2.1241195 -2.2000217 -2.2091317 -2.0670555 -1.8128517 -1.2349601 -0.48855782 -0.33074236 -0.90048647 -1.9902225 -3.2070348 -3.8911467 -3.9212582][-2.6969361 -2.3795581 -2.2014332 -2.1284914 -1.9791384 -1.7087886 -1.391362 -0.9137888 -0.39129639 -0.39206553 -1.0122197 -2.0996106 -3.3682005 -4.077991 -4.0675516][-3.3181963 -3.0564404 -2.9290268 -2.8516717 -2.6569185 -2.3816907 -2.1088948 -1.8253312 -1.566864 -1.5450275 -1.8676302 -2.5885124 -3.5833471 -4.14566 -4.0848918][-3.0041854 -2.9714534 -3.0619431 -3.1664243 -3.137496 -3.0869114 -3.0306134 -2.9627767 -2.7905607 -2.5161486 -2.3931317 -2.6750741 -3.3410406 -3.7628028 -3.7662578][-2.4738035 -2.5356998 -2.6745691 -2.8315034 -2.9082725 -3.0337543 -3.186666 -3.3200827 -3.2031982 -2.6918826 -2.1958148 -2.1201169 -2.4952831 -2.8545189 -3.0647459][-2.5432558 -2.5384216 -2.5159664 -2.5065506 -2.4575071 -2.5129185 -2.6924343 -2.9294162 -2.936094 -2.5016003 -2.0240037 -1.8788972 -2.0760624 -2.3037417 -2.5375817][-3.1037321 -3.0274715 -2.8582523 -2.6952286 -2.4774265 -2.3188474 -2.2935879 -2.3796568 -2.4156554 -2.2133191 -2.035464 -2.1051204 -2.3591955 -2.563889 -2.6957872][-3.8297715 -3.7297616 -3.4936838 -3.2535002 -2.9196515 -2.56301 -2.3123899 -2.2042925 -2.1866126 -2.0889933 -2.0632234 -2.2621596 -2.631403 -2.9853806 -3.1665511]]...]
INFO - root - 2017-12-07 06:06:48.442488: step 12010, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.772 sec/batch; 68h:43m:43s remains)
INFO - root - 2017-12-07 06:06:56.146782: step 12020, loss = 0.77, batch loss = 0.70 (10.7 examples/sec; 0.749 sec/batch; 66h:41m:40s remains)
INFO - root - 2017-12-07 06:07:03.705600: step 12030, loss = 0.80, batch loss = 0.72 (10.6 examples/sec; 0.756 sec/batch; 67h:20m:23s remains)
INFO - root - 2017-12-07 06:07:11.383800: step 12040, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.747 sec/batch; 66h:30m:46s remains)
INFO - root - 2017-12-07 06:07:19.126155: step 12050, loss = 0.96, batch loss = 0.89 (10.6 examples/sec; 0.758 sec/batch; 67h:29m:20s remains)
INFO - root - 2017-12-07 06:07:26.844090: step 12060, loss = 0.99, batch loss = 0.91 (10.4 examples/sec; 0.773 sec/batch; 68h:47m:21s remains)
INFO - root - 2017-12-07 06:07:34.542411: step 12070, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.776 sec/batch; 69h:02m:14s remains)
INFO - root - 2017-12-07 06:07:41.915953: step 12080, loss = 0.92, batch loss = 0.85 (10.6 examples/sec; 0.753 sec/batch; 67h:01m:41s remains)
INFO - root - 2017-12-07 06:07:49.506726: step 12090, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.763 sec/batch; 67h:54m:35s remains)
INFO - root - 2017-12-07 06:07:57.117457: step 12100, loss = 0.80, batch loss = 0.72 (10.4 examples/sec; 0.767 sec/batch; 68h:14m:34s remains)
2017-12-07 06:07:57.725075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1032774 -2.0411234 -1.7743614 -1.5879686 -1.8573122 -2.3007655 -2.5824876 -2.5971885 -2.2497709 -1.7309036 -1.4619641 -1.7479627 -2.4801044 -3.0932937 -3.1959686][-1.3602059 -1.3701696 -1.2285538 -1.2013531 -1.5038874 -1.7787445 -1.8520823 -1.7420764 -1.3620667 -0.93594217 -0.75443649 -1.0588233 -1.8279765 -2.54803 -2.7427926][-0.94652653 -0.92562175 -0.85034776 -0.94051576 -1.2390022 -1.3237212 -1.0921636 -0.79113054 -0.54441309 -0.43400216 -0.43537521 -0.74440551 -1.5117865 -2.2601395 -2.4822311][-0.8446579 -0.68017936 -0.62272739 -0.85215211 -1.2617848 -1.2663953 -0.71743727 -0.1440177 -0.031702518 -0.3034935 -0.50533962 -0.78189039 -1.4840086 -2.195066 -2.4204423][-0.79287457 -0.50967312 -0.4522922 -0.81367111 -1.3441947 -1.263942 -0.3826952 0.48388577 0.52117062 -0.072934628 -0.46939516 -0.72131515 -1.327143 -2.0051694 -2.2922709][-0.75920963 -0.51790738 -0.47809529 -0.85896707 -1.3654976 -1.1500192 -0.0072655678 1.061749 1.0104785 0.18481588 -0.35043097 -0.53632569 -0.97018719 -1.5582564 -1.9084859][-0.70584154 -0.61626959 -0.66337419 -1.0645492 -1.5298259 -1.2907183 -0.13985443 0.91409063 0.78570795 -0.085022449 -0.58566833 -0.6076 -0.76136327 -1.1404171 -1.48088][-0.77376604 -0.84808946 -1.0341573 -1.5246058 -1.9456882 -1.6878343 -0.64305878 0.27439451 0.10704279 -0.66606116 -1.0505383 -0.92635441 -0.85694289 -1.0692999 -1.3804486][-1.0582917 -1.2333012 -1.53668 -2.0916572 -2.4575343 -2.1835446 -1.262584 -0.4435358 -0.56947851 -1.1933625 -1.4512837 -1.2493303 -1.0796337 -1.2588727 -1.5944743][-1.4491761 -1.6381483 -1.9231336 -2.4052286 -2.70196 -2.4965818 -1.7891927 -1.1476746 -1.2341254 -1.6896684 -1.8190749 -1.555594 -1.3690696 -1.5543563 -1.8885453][-1.9195037 -2.0601697 -2.2626328 -2.6142638 -2.8608572 -2.7332482 -2.2558045 -1.8601823 -1.9557486 -2.2530782 -2.2587297 -1.9476826 -1.7405334 -1.8689935 -2.1200109][-2.3665586 -2.4155886 -2.5225444 -2.7431202 -2.9141049 -2.8391733 -2.5981603 -2.5037088 -2.6994178 -2.9074891 -2.794426 -2.4321682 -2.1808109 -2.20878 -2.3296032][-2.906564 -2.8894444 -2.9063663 -3.0051703 -3.1124711 -3.1089907 -3.0695624 -3.1790202 -3.3945599 -3.5098438 -3.3557723 -3.0225849 -2.7732697 -2.697329 -2.665817][-3.5017114 -3.4974668 -3.4936972 -3.5215008 -3.5779512 -3.5945439 -3.6086724 -3.7103884 -3.8273182 -3.845629 -3.718092 -3.4781113 -3.2723947 -3.1310205 -3.0080433][-3.7130275 -3.7458487 -3.7337189 -3.7099228 -3.7002575 -3.6832538 -3.6885238 -3.7463925 -3.791538 -3.7682343 -3.6679575 -3.5056505 -3.3548768 -3.2289433 -3.1253972]]...]
INFO - root - 2017-12-07 06:08:05.362390: step 12110, loss = 0.90, batch loss = 0.83 (10.4 examples/sec; 0.770 sec/batch; 68h:31m:10s remains)
INFO - root - 2017-12-07 06:08:13.043228: step 12120, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.761 sec/batch; 67h:43m:07s remains)
INFO - root - 2017-12-07 06:08:20.775583: step 12130, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.785 sec/batch; 69h:50m:06s remains)
INFO - root - 2017-12-07 06:08:28.322928: step 12140, loss = 0.79, batch loss = 0.71 (10.5 examples/sec; 0.760 sec/batch; 67h:38m:27s remains)
INFO - root - 2017-12-07 06:08:35.858574: step 12150, loss = 0.82, batch loss = 0.74 (10.7 examples/sec; 0.748 sec/batch; 66h:34m:42s remains)
INFO - root - 2017-12-07 06:08:43.586049: step 12160, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.773 sec/batch; 68h:46m:48s remains)
INFO - root - 2017-12-07 06:08:51.305979: step 12170, loss = 0.71, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 69h:01m:02s remains)
INFO - root - 2017-12-07 06:08:58.735178: step 12180, loss = 0.71, batch loss = 0.64 (10.8 examples/sec; 0.739 sec/batch; 65h:44m:18s remains)
INFO - root - 2017-12-07 06:09:06.375201: step 12190, loss = 0.65, batch loss = 0.58 (10.6 examples/sec; 0.758 sec/batch; 67h:25m:45s remains)
INFO - root - 2017-12-07 06:09:14.171391: step 12200, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 70h:35m:55s remains)
2017-12-07 06:09:14.770993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2244744 -3.3347983 -3.4219775 -3.5044136 -3.5594778 -3.5760813 -3.5736573 -3.5578549 -3.5111361 -3.4455838 -3.4217334 -3.4072495 -3.3739204 -3.3103528 -3.2073534][-3.5372155 -3.6601191 -3.7261229 -3.799062 -3.842392 -3.8228874 -3.7809238 -3.7441578 -3.6745718 -3.5831192 -3.5670295 -3.5723076 -3.5508573 -3.4915869 -3.3644824][-3.7281454 -3.8094118 -3.7729478 -3.7762067 -3.7984869 -3.7396464 -3.6375532 -3.5969741 -3.5577869 -3.4892235 -3.5172482 -3.5750978 -3.5821762 -3.5231984 -3.3601682][-3.6828446 -3.7094443 -3.5257077 -3.4031577 -3.3881912 -3.2907519 -3.1453114 -3.171536 -3.2479305 -3.2543619 -3.3597794 -3.5276921 -3.6242337 -3.5889323 -3.3819513][-3.4548957 -3.4604702 -3.1525946 -2.8756049 -2.77986 -2.599438 -2.3664603 -2.4568574 -2.6869221 -2.784524 -2.9523072 -3.2269263 -3.4185257 -3.4012167 -3.1423111][-3.1520591 -3.1499155 -2.7642186 -2.3516638 -2.1424637 -1.7996018 -1.3581262 -1.4134357 -1.8142693 -2.0879595 -2.3669095 -2.7531605 -3.0304868 -3.0216103 -2.7006874][-2.837832 -2.7662523 -2.2516718 -1.6685412 -1.3272238 -0.8124783 -0.12184191 -0.062193394 -0.62041306 -1.1435735 -1.5685115 -2.0482268 -2.4582767 -2.5888157 -2.334486][-2.5963113 -2.4031868 -1.7336721 -1.0028286 -0.6109457 -0.091758251 0.69364357 0.86596489 0.26094103 -0.40841341 -0.86251378 -1.2946887 -1.759613 -2.061682 -1.9991837][-2.406975 -2.0999081 -1.3483582 -0.58706093 -0.32648563 -0.076174736 0.48454094 0.70098639 0.28437996 -0.29690504 -0.64641809 -0.9274857 -1.3030956 -1.6363227 -1.7345996][-2.3494306 -2.0485454 -1.3707516 -0.68821454 -0.58088732 -0.64515972 -0.40922737 -0.17853546 -0.27701378 -0.56497 -0.70362329 -0.79520774 -1.0143766 -1.2771809 -1.4749823][-2.3861475 -2.1967425 -1.7155256 -1.1978488 -1.1760678 -1.4126408 -1.4332414 -1.2657018 -1.1831632 -1.2323105 -1.186466 -1.1056669 -1.1037529 -1.1902056 -1.3689528][-2.3834291 -2.293514 -2.0269859 -1.7036912 -1.7090726 -1.9500246 -2.081511 -2.0298455 -1.9740705 -1.9824338 -1.9016595 -1.7599769 -1.6074827 -1.5133703 -1.5712636][-2.4096873 -2.3839638 -2.2900078 -2.1351316 -2.1430461 -2.294565 -2.4068592 -2.424078 -2.4322445 -2.4686964 -2.4453578 -2.3639023 -2.2176094 -2.059665 -2.0059569][-2.5414181 -2.5465212 -2.5358195 -2.4706469 -2.467114 -2.5344453 -2.6023073 -2.6397493 -2.680655 -2.7455571 -2.785346 -2.78033 -2.7102504 -2.5854166 -2.4895036][-2.6907184 -2.703351 -2.7128594 -2.6866655 -2.6765976 -2.6994705 -2.7375898 -2.7792187 -2.8309579 -2.8939223 -2.9410009 -2.9562554 -2.9279556 -2.8541279 -2.7715802]]...]
INFO - root - 2017-12-07 06:09:22.405658: step 12210, loss = 0.57, batch loss = 0.50 (10.6 examples/sec; 0.754 sec/batch; 67h:04m:05s remains)
INFO - root - 2017-12-07 06:09:30.117181: step 12220, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.778 sec/batch; 69h:11m:58s remains)
INFO - root - 2017-12-07 06:09:37.858898: step 12230, loss = 0.88, batch loss = 0.81 (10.2 examples/sec; 0.782 sec/batch; 69h:32m:35s remains)
INFO - root - 2017-12-07 06:09:45.496437: step 12240, loss = 0.58, batch loss = 0.51 (10.6 examples/sec; 0.758 sec/batch; 67h:26m:37s remains)
INFO - root - 2017-12-07 06:09:53.120160: step 12250, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.775 sec/batch; 68h:58m:54s remains)
INFO - root - 2017-12-07 06:10:00.777052: step 12260, loss = 0.81, batch loss = 0.74 (10.1 examples/sec; 0.794 sec/batch; 70h:36m:32s remains)
INFO - root - 2017-12-07 06:10:08.418066: step 12270, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.763 sec/batch; 67h:50m:30s remains)
INFO - root - 2017-12-07 06:10:15.921911: step 12280, loss = 0.97, batch loss = 0.89 (10.2 examples/sec; 0.783 sec/batch; 69h:38m:44s remains)
INFO - root - 2017-12-07 06:10:23.528765: step 12290, loss = 0.62, batch loss = 0.55 (10.7 examples/sec; 0.751 sec/batch; 66h:45m:50s remains)
INFO - root - 2017-12-07 06:10:31.241805: step 12300, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.766 sec/batch; 68h:07m:31s remains)
2017-12-07 06:10:31.863737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0849352 -3.3594737 -3.4415333 -3.3948798 -3.2718031 -3.2812428 -3.3850777 -3.7789721 -4.136364 -4.2097869 -4.0993862 -3.8443232 -3.6162632 -3.4526484 -3.3883281][-2.5306306 -2.7950873 -2.8602562 -2.766964 -2.5778408 -2.5385423 -2.74507 -3.3971581 -4.0508924 -4.3526855 -4.2946038 -3.9774666 -3.6791887 -3.52864 -3.5172007][-2.1307662 -2.4045966 -2.5495305 -2.5590572 -2.3784766 -2.2080739 -2.3201492 -2.9716802 -3.7325006 -4.2402329 -4.344768 -4.1224809 -3.8529692 -3.7394457 -3.7354717][-1.9971964 -2.2028353 -2.3705568 -2.5282786 -2.4627843 -2.2563884 -2.2471189 -2.6701403 -3.252435 -3.7382839 -3.8980505 -3.7631276 -3.5832031 -3.6062307 -3.738447][-1.9606104 -2.1939142 -2.3991981 -2.6933575 -2.7198105 -2.4480133 -2.2241981 -2.2846351 -2.5921566 -2.9481516 -3.0580392 -2.9356065 -2.816227 -2.9255166 -3.1166332][-2.0378473 -2.4226685 -2.7436404 -3.1660187 -3.2540042 -2.9020965 -2.4396706 -2.1130872 -2.1392715 -2.3612554 -2.3978939 -2.2580509 -2.1613512 -2.2803841 -2.4026132][-2.4204185 -2.8710585 -3.1484709 -3.4831707 -3.4409328 -2.9183989 -2.2210045 -1.6264918 -1.5095065 -1.7211883 -1.7653093 -1.6402125 -1.5639472 -1.690968 -1.7743542][-2.5961971 -3.1252832 -3.3803196 -3.57199 -3.3250666 -2.5768979 -1.6599627 -0.95384 -0.91851687 -1.303364 -1.5124817 -1.4847195 -1.4433177 -1.5161498 -1.5039215][-2.3571665 -2.8741853 -3.1473961 -3.3001709 -3.0727823 -2.4127607 -1.6084709 -1.0246272 -1.0785918 -1.5345976 -1.8757546 -1.9892023 -2.0305743 -2.1140258 -2.0970867][-2.2703636 -2.635778 -2.7864273 -2.8331418 -2.6919041 -2.2929947 -1.7942545 -1.4454112 -1.5337882 -1.902698 -2.2498703 -2.4353616 -2.5225823 -2.6318269 -2.6589954][-2.4244039 -2.715265 -2.8041947 -2.7772393 -2.7088039 -2.5366201 -2.3312855 -2.1986067 -2.2950323 -2.5355568 -2.8005834 -2.9660525 -3.0595665 -3.1659265 -3.1733239][-2.9982386 -3.1984143 -3.2133713 -3.1301377 -3.1068332 -3.0848532 -3.0361233 -2.9968569 -3.0401995 -3.1235995 -3.2651782 -3.3868818 -3.4954238 -3.6244984 -3.6439865][-3.4971402 -3.5935695 -3.5380919 -3.4467263 -3.464941 -3.5085378 -3.5126536 -3.5034535 -3.5215404 -3.5274637 -3.5747902 -3.63456 -3.6998327 -3.783926 -3.7956367][-3.9458516 -4.0285029 -3.9915845 -3.9397018 -3.9653058 -4.0142627 -4.0472274 -4.0775805 -4.1160083 -4.1315269 -4.1592565 -4.1965485 -4.2193289 -4.2401109 -4.2167406][-4.3495255 -4.4308982 -4.4300385 -4.4216666 -4.450016 -4.4799442 -4.4980016 -4.5242643 -4.5575352 -4.5735703 -4.57926 -4.5953531 -4.591032 -4.5761333 -4.5472817]]...]
INFO - root - 2017-12-07 06:10:39.572995: step 12310, loss = 1.11, batch loss = 1.03 (10.5 examples/sec; 0.761 sec/batch; 67h:42m:23s remains)
INFO - root - 2017-12-07 06:10:47.171947: step 12320, loss = 0.89, batch loss = 0.82 (10.9 examples/sec; 0.733 sec/batch; 65h:09m:35s remains)
INFO - root - 2017-12-07 06:10:54.892403: step 12330, loss = 0.79, batch loss = 0.71 (10.2 examples/sec; 0.785 sec/batch; 69h:46m:19s remains)
INFO - root - 2017-12-07 06:11:02.717787: step 12340, loss = 1.02, batch loss = 0.95 (10.8 examples/sec; 0.742 sec/batch; 65h:59m:37s remains)
INFO - root - 2017-12-07 06:11:10.365913: step 12350, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.756 sec/batch; 67h:12m:27s remains)
INFO - root - 2017-12-07 06:11:18.219456: step 12360, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.769 sec/batch; 68h:23m:50s remains)
INFO - root - 2017-12-07 06:11:25.946985: step 12370, loss = 0.84, batch loss = 0.76 (10.5 examples/sec; 0.763 sec/batch; 67h:49m:10s remains)
INFO - root - 2017-12-07 06:11:33.428441: step 12380, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.809 sec/batch; 71h:57m:04s remains)
INFO - root - 2017-12-07 06:11:41.105174: step 12390, loss = 0.96, batch loss = 0.88 (10.6 examples/sec; 0.756 sec/batch; 67h:13m:37s remains)
INFO - root - 2017-12-07 06:11:48.723217: step 12400, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.755 sec/batch; 67h:05m:18s remains)
2017-12-07 06:11:49.332317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6421742 -3.7768946 -3.8968534 -4.0290642 -4.1658483 -4.2331514 -4.184269 -4.0757518 -4.0193639 -4.0451245 -4.0582438 -4.0349298 -4.0501566 -4.1231494 -4.1264963][-3.5756021 -3.6590593 -3.714473 -3.8186302 -3.9697375 -4.0430789 -3.9890428 -3.8934793 -3.8754816 -3.9056511 -3.8214355 -3.6473529 -3.6105015 -3.7793887 -3.9286435][-3.4092278 -3.4215918 -3.3780856 -3.4235437 -3.5684004 -3.6417818 -3.6061223 -3.5757308 -3.6894755 -3.7829361 -3.5600157 -3.1261005 -2.9618168 -3.2196126 -3.576139][-3.2414742 -3.2400866 -3.1066213 -3.0838485 -3.2081668 -3.2339821 -3.1312127 -3.1398172 -3.4741092 -3.7579451 -3.4964225 -2.8486648 -2.5606573 -2.8379936 -3.3171933][-3.1016092 -3.1125312 -2.8676548 -2.7245 -2.70896 -2.4820743 -2.0949748 -2.1002002 -2.8059337 -3.5201521 -3.4795625 -2.8464153 -2.5656528 -2.8626947 -3.3448493][-3.015676 -3.0073137 -2.6343713 -2.3148785 -2.0438547 -1.3439021 -0.49009681 -0.51174426 -1.6856623 -2.9779077 -3.4237719 -3.1102595 -2.9999318 -3.31866 -3.6888669][-2.989779 -2.9524937 -2.4915581 -2.0171881 -1.5330117 -0.45175791 0.7816453 0.70791197 -0.83322 -2.6119788 -3.5950356 -3.6875105 -3.720922 -3.967988 -4.1351929][-3.0071509 -2.9465902 -2.4736342 -1.9621594 -1.4999182 -0.43745232 0.8017869 0.69401646 -0.85255766 -2.7248039 -3.9689775 -4.3076224 -4.3673506 -4.4850626 -4.4514003][-3.0410042 -2.9669151 -2.5605254 -2.1320229 -1.8722954 -1.154578 -0.18017769 -0.25670528 -1.496979 -3.0441642 -4.1622763 -4.5009737 -4.4976082 -4.4949484 -4.3812838][-3.0078514 -2.9080615 -2.6105151 -2.3348799 -2.29955 -1.9621217 -1.288331 -1.2961686 -2.1589658 -3.2200713 -3.9295385 -4.0391011 -3.9245157 -3.8723412 -3.8255548][-2.8941784 -2.7338853 -2.5361991 -2.4291365 -2.5976939 -2.6137788 -2.2770286 -2.2822623 -2.8189631 -3.3986511 -3.6325781 -3.4954071 -3.3104906 -3.221782 -3.2284951][-2.8316853 -2.6462486 -2.5373154 -2.5494792 -2.8440156 -3.1135218 -3.0950966 -3.1674209 -3.4717741 -3.7158997 -3.6763425 -3.4530721 -3.2557969 -3.094131 -3.0612822][-2.9702425 -2.8592806 -2.86022 -2.9529953 -3.2522774 -3.5771508 -3.6966348 -3.798053 -3.9530797 -4.0447307 -3.939604 -3.7364445 -3.5393333 -3.3177323 -3.2212236][-3.2843893 -3.3154218 -3.4558237 -3.6404285 -3.8937683 -4.1092014 -4.1816497 -4.2129149 -4.2275767 -4.2303753 -4.1265855 -3.9636078 -3.7931085 -3.57502 -3.4564941][-3.6084776 -3.6931882 -3.8587632 -4.0441742 -4.2208786 -4.3159609 -4.3189678 -4.2885036 -4.2426748 -4.2207651 -4.1503935 -4.0294719 -3.8829663 -3.6996305 -3.5818317]]...]
INFO - root - 2017-12-07 06:11:56.986625: step 12410, loss = 0.82, batch loss = 0.75 (10.2 examples/sec; 0.787 sec/batch; 69h:56m:46s remains)
INFO - root - 2017-12-07 06:12:04.622006: step 12420, loss = 0.72, batch loss = 0.65 (10.8 examples/sec; 0.742 sec/batch; 65h:57m:48s remains)
INFO - root - 2017-12-07 06:12:12.215064: step 12430, loss = 0.64, batch loss = 0.57 (10.6 examples/sec; 0.752 sec/batch; 66h:49m:16s remains)
INFO - root - 2017-12-07 06:12:19.944391: step 12440, loss = 0.61, batch loss = 0.54 (10.2 examples/sec; 0.786 sec/batch; 69h:54m:53s remains)
INFO - root - 2017-12-07 06:12:27.629398: step 12450, loss = 1.00, batch loss = 0.93 (10.0 examples/sec; 0.797 sec/batch; 70h:50m:02s remains)
INFO - root - 2017-12-07 06:12:35.288043: step 12460, loss = 0.77, batch loss = 0.69 (10.7 examples/sec; 0.745 sec/batch; 66h:15m:34s remains)
INFO - root - 2017-12-07 06:12:43.005559: step 12470, loss = 0.70, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 71h:17m:39s remains)
INFO - root - 2017-12-07 06:12:50.339922: step 12480, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.748 sec/batch; 66h:27m:24s remains)
INFO - root - 2017-12-07 06:12:58.017984: step 12490, loss = 0.82, batch loss = 0.74 (10.2 examples/sec; 0.784 sec/batch; 69h:39m:05s remains)
INFO - root - 2017-12-07 06:13:05.765756: step 12500, loss = 0.66, batch loss = 0.59 (10.7 examples/sec; 0.749 sec/batch; 66h:35m:59s remains)
2017-12-07 06:13:06.370143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1458206 -2.46561 -2.7274451 -2.7111549 -2.4716191 -2.060802 -1.6578367 -1.4339547 -1.5702386 -1.8637028 -1.5637567 -0.85959196 -0.57597637 -0.9988029 -1.5563142][-2.1665959 -2.377322 -2.4766765 -2.3504632 -2.1643448 -1.8731217 -1.4794197 -1.2594557 -1.5934992 -2.1359594 -1.948606 -1.1416404 -0.57266808 -0.71311593 -1.1188259][-2.5183921 -2.5697825 -2.3707023 -2.0707946 -1.9805858 -1.9079075 -1.6202176 -1.4605651 -1.963021 -2.6974492 -2.6536026 -1.8060987 -0.99296308 -0.8637588 -1.0440118][-3.26229 -3.3004527 -2.8236461 -2.333096 -2.198451 -2.1154411 -1.7520177 -1.549778 -2.0918243 -2.8643568 -2.888473 -2.0752063 -1.2226975 -1.0187483 -1.109657][-3.842113 -3.8295758 -3.1805089 -2.6346526 -2.4254642 -2.1269295 -1.4253602 -1.0042088 -1.5780389 -2.5046415 -2.718394 -2.1096597 -1.4465914 -1.3459408 -1.4610953][-3.7411151 -3.4140897 -2.5748625 -2.0026224 -1.756325 -1.2493846 -0.20601654 0.43641472 -0.30411339 -1.6134822 -2.1879988 -1.9650633 -1.7061744 -1.8491838 -1.9894044][-3.1976752 -2.5844846 -1.6654212 -1.1196973 -0.90283704 -0.25229883 1.0807242 1.9752517 1.0953965 -0.54584026 -1.4151683 -1.5314231 -1.6560135 -2.0357363 -2.1846793][-2.7190409 -2.0996633 -1.3103883 -0.90081859 -0.8396306 -0.29751253 1.0318503 2.0575562 1.2976179 -0.27073526 -1.1328914 -1.3691134 -1.672015 -2.1463375 -2.24381][-2.4128804 -1.9702959 -1.4435623 -1.2138233 -1.3605793 -1.1148891 -0.12456083 0.71447468 0.15668583 -1.0672717 -1.7191229 -1.9265125 -2.2489393 -2.64218 -2.5798688][-2.4398406 -2.0444705 -1.6411228 -1.478519 -1.7076411 -1.6975715 -1.1548576 -0.646719 -1.0246105 -1.8549085 -2.2607443 -2.3861113 -2.6315196 -2.8257143 -2.5936823][-2.9101162 -2.3973427 -1.842994 -1.5016673 -1.6296878 -1.7308171 -1.5561278 -1.3672392 -1.623173 -2.1375551 -2.343116 -2.3839371 -2.5308151 -2.5039263 -2.1587405][-3.5549846 -2.9562011 -2.2142498 -1.6420906 -1.5505145 -1.6060216 -1.6216569 -1.6611321 -1.8919992 -2.2457674 -2.3929226 -2.4548163 -2.5875149 -2.4056752 -1.9430542][-3.8771114 -3.3243442 -2.6271083 -2.0764148 -1.887356 -1.8661218 -1.8891995 -1.9939497 -2.2439392 -2.5116529 -2.627146 -2.7025385 -2.819119 -2.5418983 -2.0113752][-3.78718 -3.3812394 -2.9024777 -2.5895534 -2.4913912 -2.4650621 -2.4239333 -2.4892879 -2.7096405 -2.8655858 -2.9000823 -2.9365544 -2.9916878 -2.63493 -2.1018128][-3.4755945 -3.2854483 -3.05718 -2.986393 -3.0086546 -2.9773159 -2.8837709 -2.9088054 -3.0765252 -3.1067753 -3.036571 -3.0316348 -3.0366929 -2.6702678 -2.2265294]]...]
INFO - root - 2017-12-07 06:13:14.143123: step 12510, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.771 sec/batch; 68h:33m:32s remains)
INFO - root - 2017-12-07 06:13:21.937014: step 12520, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.768 sec/batch; 68h:16m:48s remains)
INFO - root - 2017-12-07 06:13:29.608981: step 12530, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 68h:59m:23s remains)
INFO - root - 2017-12-07 06:13:37.263906: step 12540, loss = 0.92, batch loss = 0.84 (10.1 examples/sec; 0.788 sec/batch; 70h:04m:10s remains)
INFO - root - 2017-12-07 06:13:44.977897: step 12550, loss = 0.94, batch loss = 0.87 (10.6 examples/sec; 0.755 sec/batch; 67h:06m:31s remains)
INFO - root - 2017-12-07 06:13:52.703388: step 12560, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 68h:36m:17s remains)
INFO - root - 2017-12-07 06:14:00.442807: step 12570, loss = 0.71, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 70h:50m:26s remains)
INFO - root - 2017-12-07 06:14:07.871748: step 12580, loss = 0.75, batch loss = 0.67 (10.3 examples/sec; 0.780 sec/batch; 69h:17m:53s remains)
INFO - root - 2017-12-07 06:14:15.516311: step 12590, loss = 0.69, batch loss = 0.61 (10.6 examples/sec; 0.755 sec/batch; 67h:05m:16s remains)
INFO - root - 2017-12-07 06:14:23.129651: step 12600, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.758 sec/batch; 67h:23m:29s remains)
2017-12-07 06:14:23.759481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1545482 -3.9526019 -3.7835073 -3.6261163 -3.4973269 -3.3733878 -3.2776482 -3.3419127 -3.4182987 -3.5276971 -3.7498679 -3.8306704 -3.6098356 -3.3953977 -3.5749073][-4.2539096 -4.0775013 -3.9595385 -3.7829335 -3.5287848 -3.2743325 -3.1094966 -3.1473484 -3.2275229 -3.3467855 -3.6136861 -3.7714705 -3.6122351 -3.4318798 -3.6105826][-4.1675096 -4.0425792 -3.9378171 -3.7423527 -3.4461994 -3.1725287 -3.0277636 -3.0936127 -3.2190595 -3.378484 -3.6625836 -3.8335865 -3.6810751 -3.5072699 -3.66784][-3.9382861 -3.9027565 -3.8085814 -3.6076365 -3.3201427 -3.1060843 -3.0419981 -3.1588054 -3.3084826 -3.4645708 -3.7174134 -3.859364 -3.7112131 -3.5500803 -3.6991789][-3.5624287 -3.5812602 -3.5274215 -3.3362231 -3.0943918 -3.0092125 -3.1090882 -3.3437114 -3.5320935 -3.6585531 -3.8221169 -3.8597057 -3.6671324 -3.516613 -3.6793642][-3.1208797 -3.117455 -3.0634584 -2.8313766 -2.5792055 -2.5597725 -2.7922673 -3.1355467 -3.3648796 -3.4814177 -3.5897367 -3.565908 -3.3669729 -3.279397 -3.5273323][-2.5125153 -2.5214553 -2.5268035 -2.3117857 -2.0178974 -1.9505651 -2.1781721 -2.5167723 -2.7165689 -2.8142581 -2.9545827 -3.0111275 -2.9282622 -2.9750631 -3.3438163][-1.9603832 -1.9426038 -1.949544 -1.7219017 -1.3691792 -1.2515042 -1.4890547 -1.8200579 -1.97001 -2.0427332 -2.2572141 -2.463871 -2.54994 -2.7342219 -3.1996474][-2.1262603 -2.0696745 -1.9724922 -1.6270192 -1.1866183 -1.0376949 -1.2525573 -1.5071073 -1.5669763 -1.6123023 -1.8862987 -2.1915905 -2.3571672 -2.5847588 -3.0839798][-2.5306904 -2.4941311 -2.375138 -2.0034885 -1.5509348 -1.4125798 -1.5625329 -1.6877241 -1.6422141 -1.6635242 -1.9660099 -2.3090792 -2.5044138 -2.7316165 -3.1776342][-2.980505 -2.9288063 -2.7928066 -2.4377544 -2.0261731 -1.8989685 -2.000457 -2.0644023 -2.0021858 -2.0316994 -2.3281207 -2.6413126 -2.8173411 -3.0109868 -3.36558][-3.2330155 -3.1770663 -3.0271888 -2.716836 -2.3659756 -2.2453737 -2.3008385 -2.3172102 -2.2288342 -2.254159 -2.5394964 -2.827781 -2.9944141 -3.1576557 -3.4442229][-3.1517782 -3.1311316 -3.0139112 -2.8045042 -2.5699732 -2.4900589 -2.520427 -2.4860187 -2.3670378 -2.4020236 -2.6854129 -2.9582441 -3.1341705 -3.2777443 -3.51004][-2.9746909 -2.9724696 -2.9262679 -2.8416023 -2.7307892 -2.7191451 -2.7653883 -2.6924872 -2.540926 -2.5877304 -2.8379722 -3.048955 -3.2135973 -3.3541169 -3.5539508][-2.8802962 -2.8433118 -2.8627634 -2.8744192 -2.822926 -2.8201408 -2.8656135 -2.7820153 -2.6451654 -2.7245636 -2.9607382 -3.164762 -3.3541651 -3.4926515 -3.6427321]]...]
INFO - root - 2017-12-07 06:14:31.501811: step 12610, loss = 0.72, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 70h:52m:05s remains)
INFO - root - 2017-12-07 06:14:39.245040: step 12620, loss = 0.73, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 68h:15m:21s remains)
INFO - root - 2017-12-07 06:14:46.973356: step 12630, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.765 sec/batch; 67h:58m:48s remains)
INFO - root - 2017-12-07 06:14:54.563280: step 12640, loss = 0.59, batch loss = 0.51 (10.7 examples/sec; 0.751 sec/batch; 66h:41m:57s remains)
INFO - root - 2017-12-07 06:15:02.271774: step 12650, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.784 sec/batch; 69h:39m:39s remains)
INFO - root - 2017-12-07 06:15:10.048170: step 12660, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.765 sec/batch; 67h:57m:05s remains)
INFO - root - 2017-12-07 06:15:17.646275: step 12670, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.774 sec/batch; 68h:45m:17s remains)
INFO - root - 2017-12-07 06:15:25.039293: step 12680, loss = 0.99, batch loss = 0.92 (10.3 examples/sec; 0.774 sec/batch; 68h:47m:08s remains)
INFO - root - 2017-12-07 06:15:32.632128: step 12690, loss = 0.95, batch loss = 0.88 (10.9 examples/sec; 0.731 sec/batch; 64h:57m:19s remains)
INFO - root - 2017-12-07 06:15:40.366481: step 12700, loss = 0.96, batch loss = 0.89 (10.7 examples/sec; 0.750 sec/batch; 66h:36m:30s remains)
2017-12-07 06:15:40.967309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9720178 -4.1071715 -4.1580286 -4.2767892 -4.4160485 -4.4889688 -4.5259271 -4.5159674 -4.4824653 -4.3708825 -4.0204616 -3.2904243 -2.4597073 -2.1422439 -2.3977695][-4.215661 -4.3357487 -4.3587427 -4.5067992 -4.7338314 -4.8989363 -4.988934 -4.9860759 -4.8718176 -4.5958633 -4.0247259 -3.1520445 -2.3277061 -2.1108987 -2.4865842][-4.330049 -4.3630819 -4.3223805 -4.4765139 -4.7526755 -4.9412961 -4.9994774 -4.9372525 -4.7444377 -4.3897634 -3.7787609 -2.99025 -2.3696747 -2.2947028 -2.6862767][-4.5102024 -4.5122671 -4.44302 -4.5585747 -4.7618136 -4.8248072 -4.7612267 -4.6251554 -4.4002872 -4.0466347 -3.4971125 -2.8425274 -2.4096906 -2.4068522 -2.7443581][-4.4727349 -4.453506 -4.3280277 -4.3604155 -4.4615149 -4.415998 -4.2876048 -4.1153064 -3.8936987 -3.5238948 -2.968673 -2.3481493 -2.0095608 -2.0422781 -2.3392684][-4.1588893 -4.0524058 -3.8041868 -3.7112937 -3.7100847 -3.5882573 -3.3652122 -3.1303458 -2.9139247 -2.5520425 -2.0297937 -1.5133386 -1.3232718 -1.4258406 -1.7021503][-3.5961113 -3.3272619 -2.9672976 -2.7858205 -2.6949615 -2.4034419 -1.8705194 -1.400286 -1.2203443 -1.0810935 -0.843642 -0.63481712 -0.67220044 -0.8360858 -1.0173674][-2.7380595 -2.2829485 -1.8462982 -1.5849226 -1.3505392 -0.86525369 -0.094758987 0.41889906 0.19082975 -0.22621393 -0.46349335 -0.57395244 -0.72265458 -0.80002785 -0.77834439][-1.8088825 -1.3199353 -0.89886737 -0.56401539 -0.22681379 0.22426891 0.77138662 0.86942673 0.10797024 -0.76085615 -1.2578235 -1.4398017 -1.5494545 -1.5108986 -1.3205764][-1.332701 -1.0053194 -0.790076 -0.61431503 -0.4527688 -0.34506989 -0.2445879 -0.43181968 -1.2437208 -2.1329384 -2.6007586 -2.705163 -2.7134485 -2.5789878 -2.3401074][-1.7145679 -1.7101722 -1.7678752 -1.7897196 -1.7893476 -1.8768117 -1.9649069 -2.1546822 -2.7543528 -3.4412436 -3.7616553 -3.7567797 -3.6518114 -3.472199 -3.3146696][-2.5905104 -2.8177764 -2.9583292 -2.9509468 -2.8600368 -2.8555865 -2.9062264 -3.0106215 -3.3921561 -3.8690283 -4.0767694 -4.032793 -3.8824618 -3.7125185 -3.6551836][-3.0851471 -3.3283486 -3.3968208 -3.2847638 -3.086031 -2.9799342 -2.9882431 -3.0405467 -3.2612267 -3.5865688 -3.7327538 -3.6737974 -3.529062 -3.3983648 -3.4041204][-3.2206998 -3.3597879 -3.3135233 -3.1384344 -2.9295669 -2.8063011 -2.7754855 -2.7289724 -2.7659883 -2.9233987 -3.0152464 -2.9933109 -2.9449375 -2.9347908 -3.004858][-3.2635393 -3.3848023 -3.3535228 -3.2329016 -3.0991621 -3.015636 -2.9687607 -2.8639374 -2.7839241 -2.8084631 -2.8420303 -2.8504443 -2.8857832 -2.9636464 -3.0672216]]...]
INFO - root - 2017-12-07 06:15:48.695086: step 12710, loss = 0.64, batch loss = 0.57 (11.0 examples/sec; 0.726 sec/batch; 64h:30m:59s remains)
INFO - root - 2017-12-07 06:15:56.446818: step 12720, loss = 1.00, batch loss = 0.92 (10.8 examples/sec; 0.744 sec/batch; 66h:05m:06s remains)
INFO - root - 2017-12-07 06:16:04.100383: step 12730, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.749 sec/batch; 66h:30m:32s remains)
INFO - root - 2017-12-07 06:16:11.790342: step 12740, loss = 0.71, batch loss = 0.63 (10.5 examples/sec; 0.759 sec/batch; 67h:25m:54s remains)
INFO - root - 2017-12-07 06:16:19.371494: step 12750, loss = 0.80, batch loss = 0.72 (10.1 examples/sec; 0.788 sec/batch; 70h:01m:12s remains)
INFO - root - 2017-12-07 06:16:27.116242: step 12760, loss = 1.16, batch loss = 1.09 (10.6 examples/sec; 0.756 sec/batch; 67h:09m:58s remains)
INFO - root - 2017-12-07 06:16:34.729742: step 12770, loss = 0.77, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 67h:38m:54s remains)
INFO - root - 2017-12-07 06:16:42.118746: step 12780, loss = 0.66, batch loss = 0.59 (10.6 examples/sec; 0.753 sec/batch; 66h:50m:10s remains)
INFO - root - 2017-12-07 06:16:49.707684: step 12790, loss = 0.97, batch loss = 0.90 (10.3 examples/sec; 0.774 sec/batch; 68h:46m:21s remains)
INFO - root - 2017-12-07 06:16:57.312131: step 12800, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.753 sec/batch; 66h:53m:51s remains)
2017-12-07 06:16:57.950143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4260621 -3.5124042 -3.4537451 -3.2106824 -2.9496086 -2.8936014 -2.8912148 -2.9958725 -2.9711571 -2.9162676 -3.1130586 -3.2777791 -2.8262811 -2.5537286 -2.8811755][-3.2658892 -3.344872 -3.3005316 -2.9344971 -2.6399508 -2.6842637 -2.7461953 -2.8389513 -2.8029728 -2.7732925 -2.9966314 -3.057591 -2.2338645 -1.787473 -2.2871623][-2.9879704 -3.0853457 -3.1104145 -2.6757677 -2.3977032 -2.5248494 -2.4958644 -2.4248006 -2.3931878 -2.479073 -2.8203795 -2.7950053 -1.632319 -1.0774801 -1.8092382][-2.6011481 -2.7782249 -2.9708266 -2.610671 -2.4391079 -2.6229372 -2.3333054 -1.9765255 -1.9764445 -2.1823301 -2.603014 -2.5140352 -1.1368923 -0.60028744 -1.6547058][-2.2810323 -2.5448451 -2.935452 -2.7131462 -2.6482048 -2.7824931 -2.1611409 -1.5788331 -1.7624836 -2.0876594 -2.4955795 -2.3567383 -0.85405922 -0.37955475 -1.7671287][-2.0864329 -2.3674884 -2.8775549 -2.7592511 -2.7015929 -2.6677175 -1.7515388 -1.1431091 -1.658031 -2.1141317 -2.4513729 -2.2801228 -0.69679689 -0.25497007 -1.8672092][-1.9229016 -2.1530566 -2.6760321 -2.5992117 -2.4901166 -2.2553654 -1.1510243 -0.7174952 -1.6220872 -2.1757741 -2.4404321 -2.2814474 -0.67297029 -0.25190735 -2.0061378][-1.7886336 -1.954489 -2.4404466 -2.4216821 -2.2621 -1.8527792 -0.72860813 -0.55949354 -1.6941409 -2.1917083 -2.3745868 -2.2515969 -0.69708753 -0.37805033 -2.2558527][-1.9407108 -2.0354178 -2.4475636 -2.4871674 -2.3129065 -1.8207338 -0.84213686 -0.92067766 -2.0351074 -2.3193367 -2.373807 -2.2499681 -0.82135081 -0.6549511 -2.5925782][-2.3117492 -2.3317826 -2.6471596 -2.7281551 -2.57266 -2.0842869 -1.3167629 -1.5313094 -2.4406815 -2.4937725 -2.4340224 -2.2817082 -1.0595446 -1.0676897 -2.9319551][-2.5655837 -2.5351441 -2.7492383 -2.8407192 -2.7180552 -2.2832234 -1.7175474 -1.9420624 -2.5511918 -2.4426987 -2.3476927 -2.1986623 -1.2949781 -1.4980562 -3.1724777][-2.6593947 -2.6176562 -2.7481227 -2.8284276 -2.7365656 -2.3697157 -1.9526114 -2.0847683 -2.3728709 -2.1978714 -2.1563871 -2.064538 -1.5094416 -1.8507307 -3.2289557][-2.7601047 -2.7464402 -2.8293715 -2.9042401 -2.8429027 -2.5587277 -2.2749112 -2.3163679 -2.3740709 -2.2207029 -2.252135 -2.1943417 -1.8764284 -2.2146504 -3.2110524][-2.8946958 -2.9162602 -2.9758854 -3.0392642 -3.0063744 -2.8157153 -2.6629069 -2.6675091 -2.6154079 -2.5153131 -2.575995 -2.5248661 -2.3466177 -2.5969498 -3.2379346][-2.9674902 -3.0100403 -3.053149 -3.0938678 -3.0794797 -2.966356 -2.9010327 -2.890584 -2.8096178 -2.7593708 -2.818429 -2.7788782 -2.6945539 -2.8670478 -3.2419205]]...]
INFO - root - 2017-12-07 06:17:05.498644: step 12810, loss = 0.77, batch loss = 0.70 (10.7 examples/sec; 0.751 sec/batch; 66h:41m:31s remains)
INFO - root - 2017-12-07 06:17:13.140448: step 12820, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.767 sec/batch; 68h:04m:03s remains)
INFO - root - 2017-12-07 06:17:20.795856: step 12830, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.770 sec/batch; 68h:22m:32s remains)
INFO - root - 2017-12-07 06:17:28.379992: step 12840, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.781 sec/batch; 69h:18m:47s remains)
INFO - root - 2017-12-07 06:17:36.040873: step 12850, loss = 0.96, batch loss = 0.89 (10.7 examples/sec; 0.746 sec/batch; 66h:14m:39s remains)
INFO - root - 2017-12-07 06:17:43.637082: step 12860, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 71h:11m:46s remains)
INFO - root - 2017-12-07 06:17:51.273017: step 12870, loss = 0.78, batch loss = 0.70 (10.4 examples/sec; 0.771 sec/batch; 68h:28m:30s remains)
INFO - root - 2017-12-07 06:17:58.593353: step 12880, loss = 0.82, batch loss = 0.75 (10.9 examples/sec; 0.736 sec/batch; 65h:18m:48s remains)
INFO - root - 2017-12-07 06:18:06.279151: step 12890, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.752 sec/batch; 66h:44m:52s remains)
INFO - root - 2017-12-07 06:18:13.949862: step 12900, loss = 0.66, batch loss = 0.59 (10.4 examples/sec; 0.772 sec/batch; 68h:29m:47s remains)
2017-12-07 06:18:14.597299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9610071 -3.3417661 -3.6878798 -3.8744917 -3.930475 -3.9862776 -4.0564103 -4.0540032 -3.8941855 -3.6805251 -3.541882 -3.4956279 -3.696811 -3.9624417 -4.0695419][-2.7919769 -3.4278841 -3.979506 -4.2146592 -4.2083859 -4.1605396 -4.1576843 -4.1827183 -4.1463513 -4.0876775 -4.0437188 -4.074625 -4.3650427 -4.6756587 -4.7391768][-2.3477576 -3.2396679 -4.0836797 -4.4026189 -4.3246584 -4.1580253 -4.0809369 -4.2004952 -4.3736062 -4.4530573 -4.4206152 -4.4193015 -4.706521 -4.9897995 -4.9484873][-1.7368956 -2.8096485 -3.9756315 -4.4003596 -4.1922741 -3.8211524 -3.6162977 -3.8852317 -4.32082 -4.5069537 -4.4395161 -4.3791919 -4.6142612 -4.8157096 -4.6399751][-1.1112995 -2.2717376 -3.6620626 -4.1036649 -3.5915732 -2.7919555 -2.3328893 -2.8049655 -3.5920091 -3.9692483 -4.01095 -4.0554595 -4.2936773 -4.3682418 -4.0375171][-0.68677759 -1.8848133 -3.368006 -3.7313602 -2.8035746 -1.4201927 -0.66017532 -1.4388921 -2.7041454 -3.3810663 -3.6288414 -3.8749831 -4.1530676 -4.079988 -3.5791798][-0.81455922 -2.026701 -3.5295079 -3.7835722 -2.4692726 -0.52255774 0.49557495 -0.62329912 -2.3891141 -3.4024277 -3.8018188 -4.1336122 -4.3663487 -4.1330652 -3.4790771][-1.2069948 -2.4027421 -3.8756096 -4.0738907 -2.5733447 -0.2800436 0.93706846 -0.29759216 -2.3167765 -3.5533988 -4.0067887 -4.3361621 -4.56853 -4.3634539 -3.7602873][-1.7545023 -2.8768759 -4.2227635 -4.3942237 -2.9784579 -0.72171855 0.47898149 -0.57337427 -2.4272671 -3.571105 -3.8909044 -4.1165452 -4.3595762 -4.3466034 -4.0532432][-2.6396112 -3.5679078 -4.5827932 -4.6499372 -3.4409404 -1.5411465 -0.55765057 -1.3342936 -2.7593861 -3.5937624 -3.7218874 -3.8053868 -3.9818978 -4.0760345 -4.0762968][-3.3669202 -4.0495257 -4.6261969 -4.4741354 -3.4449162 -2.0433407 -1.407588 -1.9807684 -2.9409289 -3.4741006 -3.5498061 -3.6081338 -3.6965289 -3.7617097 -3.8995044][-3.3679454 -3.850914 -4.1445274 -3.8844655 -3.0836558 -2.231802 -1.9880359 -2.4131196 -2.914083 -3.1012573 -3.0865712 -3.1000695 -3.0912304 -3.0797324 -3.23165][-3.1220536 -3.4850304 -3.6897089 -3.4491229 -2.9082365 -2.5027854 -2.5295653 -2.7949162 -2.9079816 -2.8201556 -2.7819474 -2.8622255 -2.9434838 -3.0051522 -3.156424][-3.1288328 -3.488565 -3.7187326 -3.5294657 -3.1717825 -2.9836559 -3.0780258 -3.1533647 -3.0142555 -2.7924695 -2.7728679 -2.9527574 -3.1667721 -3.3351526 -3.4788721][-3.1508455 -3.5829644 -3.9534612 -3.9011531 -3.6912873 -3.5915878 -3.6297598 -3.5493269 -3.31286 -3.0965364 -3.1024256 -3.2597742 -3.4109294 -3.4998462 -3.5374339]]...]
INFO - root - 2017-12-07 06:18:22.311667: step 12910, loss = 0.99, batch loss = 0.92 (10.1 examples/sec; 0.789 sec/batch; 70h:01m:22s remains)
INFO - root - 2017-12-07 06:18:29.899108: step 12920, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.777 sec/batch; 68h:58m:22s remains)
INFO - root - 2017-12-07 06:18:37.513585: step 12930, loss = 1.02, batch loss = 0.95 (10.6 examples/sec; 0.756 sec/batch; 67h:05m:17s remains)
INFO - root - 2017-12-07 06:18:45.107078: step 12940, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.777 sec/batch; 68h:55m:44s remains)
INFO - root - 2017-12-07 06:18:52.726081: step 12950, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.779 sec/batch; 69h:06m:19s remains)
INFO - root - 2017-12-07 06:19:00.477680: step 12960, loss = 1.03, batch loss = 0.95 (10.0 examples/sec; 0.800 sec/batch; 71h:00m:23s remains)
INFO - root - 2017-12-07 06:19:08.219676: step 12970, loss = 0.73, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 69h:37m:22s remains)
INFO - root - 2017-12-07 06:19:15.748759: step 12980, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.785 sec/batch; 69h:41m:05s remains)
INFO - root - 2017-12-07 06:19:23.429151: step 12990, loss = 0.95, batch loss = 0.88 (10.4 examples/sec; 0.767 sec/batch; 68h:05m:01s remains)
INFO - root - 2017-12-07 06:19:30.999930: step 13000, loss = 1.01, batch loss = 0.94 (10.4 examples/sec; 0.769 sec/batch; 68h:15m:47s remains)
2017-12-07 06:19:31.647460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5481639 -1.5147972 -1.270725 -1.1490219 -1.130456 -1.0354819 -0.94125247 -0.7730248 -0.73080373 -0.93026233 -1.1452694 -1.3022323 -1.5565212 -1.6826243 -1.5087752][-1.9046409 -1.9136159 -1.6729383 -1.5977952 -1.6313446 -1.5531681 -1.4199314 -1.2125671 -1.1142263 -1.2743669 -1.482759 -1.693763 -2.006634 -2.0810935 -1.8370147][-2.3071043 -2.5475128 -2.5477309 -2.6374688 -2.734036 -2.6354589 -2.4378347 -2.2178566 -2.0559475 -2.0245643 -1.9995251 -2.0612087 -2.2688425 -2.2283695 -1.9559734][-2.225163 -2.6231589 -2.8443916 -3.0879078 -3.3286333 -3.3497539 -3.172652 -2.9313741 -2.7088847 -2.4662378 -2.2252433 -2.1943002 -2.3745317 -2.3199811 -2.0781956][-1.8506482 -2.2812853 -2.5943477 -2.9277949 -3.3129492 -3.5297866 -3.4359238 -3.1361582 -2.8455939 -2.4652176 -2.1015925 -2.0665412 -2.2995203 -2.3427525 -2.250309][-1.753428 -2.1334691 -2.42739 -2.7936163 -3.286375 -3.7218845 -3.8203137 -3.545043 -3.2035766 -2.6966145 -2.2022586 -2.0848045 -2.2608957 -2.3148713 -2.3189888][-1.9714358 -2.1332412 -2.2072861 -2.4386826 -2.8422308 -3.3124666 -3.6018567 -3.4425392 -3.1094007 -2.6418335 -2.1863801 -2.0243659 -2.0903294 -2.1696835 -2.3087933][-2.5488229 -2.56518 -2.4410877 -2.4449859 -2.5648837 -2.812959 -3.0897083 -3.0005193 -2.7115047 -2.4205296 -2.1854372 -2.0442395 -2.0235243 -2.2023132 -2.5190067][-2.8270037 -2.8354177 -2.702466 -2.65553 -2.65489 -2.7411494 -2.9437761 -2.8875527 -2.6766334 -2.6092324 -2.6127205 -2.4700851 -2.3568721 -2.5668397 -2.8625281][-2.4502082 -2.4933932 -2.5101643 -2.6583986 -2.769949 -2.7920685 -2.8806581 -2.7924078 -2.61742 -2.6252565 -2.6660194 -2.4625235 -2.294126 -2.5151572 -2.7660787][-1.7854192 -1.8095198 -1.9373863 -2.2868471 -2.5952873 -2.6162145 -2.6300592 -2.5186725 -2.314718 -2.2577317 -2.1765831 -1.9150784 -1.722847 -1.9430366 -2.2113888][-1.408365 -1.3717625 -1.48082 -1.8465548 -2.218456 -2.2874508 -2.3886685 -2.4163368 -2.2526364 -2.1292121 -1.9671876 -1.6658225 -1.4220963 -1.5876553 -1.8627262][-1.815115 -1.6989079 -1.6548724 -1.8507018 -2.0895016 -2.1309676 -2.3209231 -2.4642787 -2.3397262 -2.2307072 -2.1216013 -1.8673942 -1.6379724 -1.7199819 -1.9444499][-2.7943726 -2.6185894 -2.4699295 -2.5071809 -2.5615528 -2.5473027 -2.79017 -3.0024059 -2.9241631 -2.8915815 -2.8969061 -2.759228 -2.65516 -2.7264395 -2.8669674][-3.7576962 -3.6021523 -3.419044 -3.318759 -3.1766782 -3.0725203 -3.3267713 -3.644017 -3.7070932 -3.7957561 -3.8750334 -3.8090911 -3.8002529 -3.8501198 -3.8874369]]...]
INFO - root - 2017-12-07 06:19:39.319116: step 13010, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.766 sec/batch; 68h:01m:26s remains)
INFO - root - 2017-12-07 06:19:46.972021: step 13020, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.747 sec/batch; 66h:17m:00s remains)
INFO - root - 2017-12-07 06:19:54.749536: step 13030, loss = 0.81, batch loss = 0.74 (10.1 examples/sec; 0.796 sec/batch; 70h:37m:04s remains)
INFO - root - 2017-12-07 06:20:02.315990: step 13040, loss = 1.04, batch loss = 0.96 (10.5 examples/sec; 0.762 sec/batch; 67h:39m:03s remains)
INFO - root - 2017-12-07 06:20:10.027459: step 13050, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.789 sec/batch; 70h:03m:11s remains)
INFO - root - 2017-12-07 06:20:17.655380: step 13060, loss = 0.90, batch loss = 0.82 (10.5 examples/sec; 0.764 sec/batch; 67h:45m:14s remains)
INFO - root - 2017-12-07 06:20:25.363303: step 13070, loss = 0.70, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 70h:32m:29s remains)
INFO - root - 2017-12-07 06:20:32.838379: step 13080, loss = 0.72, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 70h:57m:37s remains)
INFO - root - 2017-12-07 06:20:40.618133: step 13090, loss = 0.74, batch loss = 0.66 (10.5 examples/sec; 0.762 sec/batch; 67h:38m:31s remains)
INFO - root - 2017-12-07 06:20:48.360755: step 13100, loss = 0.94, batch loss = 0.87 (10.5 examples/sec; 0.763 sec/batch; 67h:41m:00s remains)
2017-12-07 06:20:49.013709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6411977 -3.6480815 -3.6506271 -3.6542323 -3.7667091 -3.8935673 -3.9048028 -3.9519579 -3.9871016 -3.9227452 -3.9535613 -4.0791092 -4.0988474 -4.0305796 -3.84715][-3.6800349 -3.721226 -3.7330382 -3.670296 -3.75366 -3.9061863 -3.9694452 -4.0597968 -4.0441947 -3.8744888 -3.8788977 -4.0193515 -4.0261035 -3.9015145 -3.5293639][-3.6770868 -3.7617881 -3.7607782 -3.575911 -3.5318966 -3.6070883 -3.6828363 -3.8603394 -3.8642766 -3.59613 -3.4749644 -3.4814451 -3.4369349 -3.3517809 -3.0115128][-3.5999479 -3.6627584 -3.5445976 -3.16195 -2.947356 -2.9004679 -2.9877193 -3.3068271 -3.4659126 -3.2149479 -2.9520507 -2.7096221 -2.6055379 -2.7095017 -2.6753955][-3.4374671 -3.4088917 -3.1231537 -2.5627904 -2.2064602 -2.0806682 -2.195719 -2.6529183 -2.9979048 -2.9249873 -2.7053406 -2.3730471 -2.272886 -2.5278883 -2.7461245][-3.2103248 -3.1219616 -2.7529998 -2.0896869 -1.543756 -1.2423742 -1.2841094 -1.8002925 -2.3370161 -2.6273651 -2.7274089 -2.5682385 -2.5514946 -2.7567086 -2.9188194][-3.059679 -2.976733 -2.5520968 -1.7293148 -0.90609121 -0.35347319 -0.26610088 -0.8748157 -1.7191193 -2.5011935 -2.979672 -2.9984555 -2.9808035 -2.9574766 -2.8813672][-3.009711 -2.9271057 -2.4197187 -1.5051999 -0.54815745 0.12496471 0.22964811 -0.45025706 -1.4579079 -2.467448 -3.1407425 -3.31423 -3.3481891 -3.1758041 -2.8974183][-2.8484364 -2.7112765 -2.2164664 -1.4836287 -0.71659923 -0.14350843 -0.031877041 -0.55443239 -1.361295 -2.2488918 -2.9271269 -3.3045774 -3.526166 -3.4347517 -3.1627822][-2.5957165 -2.4951825 -2.1703646 -1.7487288 -1.2464671 -0.76098061 -0.56347036 -0.76449108 -1.215694 -1.8741453 -2.5568724 -3.1492658 -3.5538259 -3.5936942 -3.444469][-2.5604477 -2.5681376 -2.4506631 -2.2677355 -1.9593482 -1.5766668 -1.365463 -1.3952703 -1.653656 -2.1445432 -2.762691 -3.364737 -3.7596827 -3.8167627 -3.7281754][-2.689517 -2.6803012 -2.6393785 -2.631079 -2.5715461 -2.45532 -2.4131143 -2.4559398 -2.6116896 -2.8946128 -3.2286339 -3.5236726 -3.7066188 -3.7445681 -3.7628675][-2.7511687 -2.6284466 -2.5950966 -2.6923296 -2.831583 -2.9622266 -3.0515819 -3.0942469 -3.116544 -3.1486473 -3.1921096 -3.2298017 -3.2779787 -3.3434992 -3.480155][-2.9067838 -2.751178 -2.7361777 -2.8657546 -3.0310445 -3.1952579 -3.2772856 -3.2787638 -3.2511544 -3.212261 -3.2205026 -3.243228 -3.2749338 -3.3320346 -3.4176898][-3.3236322 -3.230875 -3.2040083 -3.251996 -3.3090012 -3.3905158 -3.4245672 -3.4145925 -3.4197285 -3.4321129 -3.4863911 -3.5276761 -3.5215468 -3.5034237 -3.4951162]]...]
INFO - root - 2017-12-07 06:20:56.619434: step 13110, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.760 sec/batch; 67h:23m:40s remains)
INFO - root - 2017-12-07 06:21:04.411410: step 13120, loss = 0.72, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 71h:30m:10s remains)
INFO - root - 2017-12-07 06:21:12.092553: step 13130, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.771 sec/batch; 68h:22m:04s remains)
INFO - root - 2017-12-07 06:21:19.839191: step 13140, loss = 1.00, batch loss = 0.93 (10.4 examples/sec; 0.766 sec/batch; 67h:56m:35s remains)
INFO - root - 2017-12-07 06:21:27.527278: step 13150, loss = 1.01, batch loss = 0.94 (10.4 examples/sec; 0.767 sec/batch; 68h:03m:11s remains)
INFO - root - 2017-12-07 06:21:35.142484: step 13160, loss = 0.78, batch loss = 0.71 (10.9 examples/sec; 0.731 sec/batch; 64h:48m:55s remains)
INFO - root - 2017-12-07 06:21:42.718017: step 13170, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.770 sec/batch; 68h:19m:42s remains)
INFO - root - 2017-12-07 06:21:50.207565: step 13180, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.783 sec/batch; 69h:25m:43s remains)
INFO - root - 2017-12-07 06:21:57.910951: step 13190, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.758 sec/batch; 67h:15m:45s remains)
INFO - root - 2017-12-07 06:22:05.591895: step 13200, loss = 0.90, batch loss = 0.83 (10.9 examples/sec; 0.732 sec/batch; 64h:54m:32s remains)
2017-12-07 06:22:06.200268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9578681 -3.9266996 -3.9171154 -3.9530249 -4.0934587 -4.2939944 -4.40737 -4.2654872 -3.9764442 -3.7113402 -3.5699451 -3.6297643 -3.8239284 -3.9754641 -4.0307093][-3.840982 -3.8015943 -3.7418609 -3.7034881 -3.8788955 -4.2161369 -4.3730378 -4.0258064 -3.4474301 -3.0228064 -2.9349194 -3.1634626 -3.5023136 -3.7041564 -3.7522306][-3.5626647 -3.4843554 -3.3304791 -3.2262053 -3.4847484 -3.9449804 -4.0652266 -3.4251897 -2.5141046 -1.9752908 -2.0574617 -2.5572405 -2.998724 -3.1108093 -3.04922][-3.2320552 -3.0171659 -2.6448987 -2.4099572 -2.7427378 -3.3130989 -3.4176388 -2.5749497 -1.4772556 -1.0097136 -1.373795 -2.119103 -2.4668453 -2.2150936 -1.8586931][-3.0054274 -2.6116426 -1.9684703 -1.5285163 -1.8332238 -2.4749835 -2.7635312 -2.0716774 -1.0620024 -0.69280195 -1.129319 -1.8035016 -1.8117404 -1.1232853 -0.48802304][-2.9444723 -2.3985069 -1.5359423 -0.88157868 -1.0336804 -1.6468556 -2.2401052 -2.0555441 -1.370575 -1.0145304 -1.2254424 -1.54601 -1.1547222 -0.18323183 0.52455568][-3.0041053 -2.3375914 -1.2854373 -0.34612465 -0.13806486 -0.55111074 -1.4246936 -1.8163674 -1.4787648 -1.081069 -0.95457125 -0.86206913 -0.16782618 0.8446064 1.3755088][-3.0137653 -2.3103664 -1.1894042 -0.071667671 0.45547056 0.24623537 -0.74909663 -1.4086165 -1.15972 -0.6320653 -0.032022476 0.60774374 1.5514441 2.2972651 2.2950439][-2.7882128 -2.1881335 -1.2683668 -0.35041523 0.11228371 -0.055365562 -0.87814856 -1.3039207 -0.79141951 -0.026411533 0.93000269 1.9049053 2.8705363 3.179862 2.5331984][-2.5294607 -2.0223074 -1.4317777 -0.95640826 -0.78807569 -0.94081569 -1.3901975 -1.3565428 -0.57428837 0.26636839 1.1035128 1.8988152 2.53904 2.4766231 1.5974727][-2.4923232 -2.0052438 -1.6464314 -1.4834845 -1.4725406 -1.4920275 -1.4695647 -1.0248654 -0.2034173 0.396348 0.76890087 1.1294193 1.3885622 1.2009759 0.51405764][-2.681046 -2.2471685 -2.037308 -1.9951098 -1.9176667 -1.7168996 -1.3531227 -0.73211813 -0.13831186 0.070009232 -0.025526047 -0.058757782 -0.03013134 -0.14402628 -0.41755009][-2.9329019 -2.5966196 -2.4406333 -2.3825488 -2.2261686 -1.9323082 -1.5083485 -0.94514465 -0.59328341 -0.60802174 -0.85422587 -1.0425453 -1.0525696 -1.0513582 -1.0443764][-3.1321926 -2.8929105 -2.734262 -2.5844159 -2.3458352 -2.0807879 -1.7764115 -1.4237692 -1.291981 -1.410594 -1.6470628 -1.8077517 -1.7601664 -1.6321206 -1.5123675][-3.3770771 -3.2553382 -3.1348214 -2.9657285 -2.7190681 -2.5118055 -2.3056469 -2.1136467 -2.1122332 -2.2596178 -2.4656587 -2.6065755 -2.5721953 -2.4358845 -2.3620405]]...]
INFO - root - 2017-12-07 06:22:13.949140: step 13210, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.779 sec/batch; 69h:06m:53s remains)
INFO - root - 2017-12-07 06:22:21.615172: step 13220, loss = 0.78, batch loss = 0.70 (10.2 examples/sec; 0.783 sec/batch; 69h:24m:10s remains)
INFO - root - 2017-12-07 06:22:29.213763: step 13230, loss = 0.69, batch loss = 0.62 (10.8 examples/sec; 0.740 sec/batch; 65h:37m:18s remains)
INFO - root - 2017-12-07 06:22:36.960707: step 13240, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.769 sec/batch; 68h:13m:16s remains)
INFO - root - 2017-12-07 06:22:44.555884: step 13250, loss = 0.96, batch loss = 0.89 (10.3 examples/sec; 0.775 sec/batch; 68h:41m:37s remains)
INFO - root - 2017-12-07 06:22:52.254695: step 13260, loss = 0.83, batch loss = 0.75 (10.5 examples/sec; 0.764 sec/batch; 67h:47m:21s remains)
INFO - root - 2017-12-07 06:22:59.920501: step 13270, loss = 1.02, batch loss = 0.95 (10.3 examples/sec; 0.776 sec/batch; 68h:50m:18s remains)
INFO - root - 2017-12-07 06:23:07.322943: step 13280, loss = 0.85, batch loss = 0.78 (10.7 examples/sec; 0.745 sec/batch; 66h:05m:42s remains)
INFO - root - 2017-12-07 06:23:14.852501: step 13290, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.768 sec/batch; 68h:03m:58s remains)
INFO - root - 2017-12-07 06:23:22.603336: step 13300, loss = 0.93, batch loss = 0.86 (10.2 examples/sec; 0.781 sec/batch; 69h:13m:20s remains)
2017-12-07 06:23:23.248002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5286055 -2.1491606 -2.0019383 -2.1563303 -2.3690135 -2.7709527 -3.1448269 -2.9761591 -2.60668 -2.4880006 -1.9605317 -1.228816 -0.96626759 -1.326046 -2.0998449][-2.4784603 -2.167958 -2.02147 -2.0440836 -2.1982327 -2.5923581 -2.7905669 -2.357511 -1.9185517 -1.7881048 -1.1361983 -0.31116867 -0.048989296 -0.44460368 -1.1458075][-2.5592053 -2.3309104 -2.0933945 -1.8768692 -1.8563454 -2.1057928 -2.0617776 -1.5231223 -1.3014491 -1.3440771 -0.8307128 -0.2159791 -0.023821354 -0.16764545 -0.52622485][-2.7042389 -2.4982915 -2.0676854 -1.6273186 -1.439651 -1.528528 -1.2453818 -0.67184019 -0.68945861 -0.86807323 -0.579303 -0.31509113 -0.13149357 0.040523052 -0.069934845][-2.7821972 -2.6076758 -2.0705028 -1.5923085 -1.3997943 -1.3123198 -0.68203044 0.033048153 -0.079744339 -0.39195871 -0.35653067 -0.3442626 -0.14644194 0.15973186 0.010803699][-2.5789764 -2.4144013 -1.8462648 -1.4219973 -1.2858984 -0.9257462 0.260571 1.3228111 1.0166707 0.3411231 0.10337019 -0.07290411 0.06872654 0.21710491 -0.22047853][-2.2416081 -1.9965882 -1.3642964 -0.96258116 -0.73517823 0.01513958 1.6417031 2.8014159 2.0184441 0.90824795 0.48868561 0.28873348 0.36006403 0.20938826 -0.63045144][-2.1907451 -1.8614836 -1.2862689 -0.94198465 -0.50232935 0.53507805 2.0781732 2.703794 1.5879121 0.50688744 0.25253582 0.25560045 0.27861214 -0.13613415 -1.2454133][-2.3625691 -2.0087719 -1.5979817 -1.3092132 -0.68023562 0.38473129 1.4100771 1.4090948 0.28310347 -0.42854309 -0.381186 -0.28526497 -0.39749956 -1.0271196 -2.2334504][-2.7318974 -2.4229522 -2.1688886 -1.9026384 -1.1968579 -0.35847473 0.11377144 -0.26850796 -1.1893048 -1.5208521 -1.3525803 -1.360075 -1.6029456 -2.2426598 -3.2209511][-3.1889734 -2.9642634 -2.8074582 -2.5647273 -1.9719214 -1.4868591 -1.4298496 -1.8843849 -2.4679856 -2.5410075 -2.448525 -2.6090999 -2.83037 -3.2207694 -3.770483][-3.4826572 -3.3503315 -3.2496986 -3.053031 -2.6945927 -2.5408802 -2.6971097 -3.0465293 -3.3224268 -3.2883058 -3.318018 -3.5144038 -3.6192262 -3.7484207 -3.9250457][-3.6229548 -3.5864506 -3.5460634 -3.4155874 -3.259644 -3.2701373 -3.4269636 -3.5925045 -3.6584234 -3.6143677 -3.6937237 -3.8453763 -3.8551927 -3.8583663 -3.8628407][-3.615684 -3.6334562 -3.628767 -3.5570912 -3.53076 -3.57698 -3.6354661 -3.6444468 -3.6238074 -3.6065061 -3.6754808 -3.7524495 -3.7199695 -3.7044761 -3.6857154][-3.5420761 -3.5783856 -3.5899332 -3.5639374 -3.5796838 -3.612102 -3.6054463 -3.5612433 -3.5381753 -3.5423865 -3.5739851 -3.591203 -3.5607119 -3.5648353 -3.5693231]]...]
INFO - root - 2017-12-07 06:23:31.067312: step 13310, loss = 0.82, batch loss = 0.75 (10.0 examples/sec; 0.797 sec/batch; 70h:41m:54s remains)
INFO - root - 2017-12-07 06:23:38.764310: step 13320, loss = 0.72, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 70h:10m:14s remains)
INFO - root - 2017-12-07 06:23:46.507754: step 13330, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.781 sec/batch; 69h:12m:27s remains)
INFO - root - 2017-12-07 06:23:54.254975: step 13340, loss = 0.68, batch loss = 0.61 (9.8 examples/sec; 0.813 sec/batch; 72h:06m:15s remains)
INFO - root - 2017-12-07 06:24:01.915138: step 13350, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.762 sec/batch; 67h:33m:18s remains)
INFO - root - 2017-12-07 06:24:09.540591: step 13360, loss = 1.02, batch loss = 0.94 (10.1 examples/sec; 0.796 sec/batch; 70h:33m:49s remains)
INFO - root - 2017-12-07 06:24:17.377297: step 13370, loss = 0.84, batch loss = 0.76 (10.5 examples/sec; 0.763 sec/batch; 67h:38m:10s remains)
INFO - root - 2017-12-07 06:24:24.780124: step 13380, loss = 1.08, batch loss = 1.01 (10.8 examples/sec; 0.744 sec/batch; 65h:55m:46s remains)
INFO - root - 2017-12-07 06:24:32.396745: step 13390, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 67h:43m:06s remains)
INFO - root - 2017-12-07 06:24:40.114685: step 13400, loss = 0.72, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 69h:34m:20s remains)
2017-12-07 06:24:40.705308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5137513 -1.2991478 -0.9173 -0.41943026 -0.62307692 -1.3119683 -1.6976805 -1.739784 -1.8014293 -1.8392489 -1.8753753 -1.7551062 -1.6002395 -1.6629553 -1.5821123][-2.0576842 -1.8472941 -1.4219999 -0.77560139 -0.77897382 -1.2125213 -1.401263 -1.3814135 -1.3900619 -1.4020987 -1.3980138 -1.2819293 -1.3369591 -1.6635153 -1.7059515][-2.4456198 -2.2733741 -1.8669763 -1.112371 -0.85084057 -0.98266506 -1.0391958 -1.0509555 -1.0339525 -0.98084879 -0.9403975 -0.92370081 -1.2306218 -1.7545056 -1.894875][-2.7196174 -2.5902069 -2.15261 -1.2844696 -0.76937485 -0.70070791 -0.77434921 -0.84814525 -0.7628839 -0.64842749 -0.6093781 -0.6679306 -1.0460391 -1.5789518 -1.8118565][-2.8646507 -2.7962048 -2.3038967 -1.406296 -0.80578518 -0.75764942 -0.97297335 -1.0756326 -0.86022925 -0.66010284 -0.56997085 -0.561095 -0.81857252 -1.2660911 -1.5879593][-2.7717848 -2.7150397 -2.1839566 -1.3834004 -0.78952885 -0.77617693 -1.0282421 -1.020678 -0.64229918 -0.38917589 -0.32061434 -0.32531691 -0.55006027 -0.94765306 -1.3436275][-2.5042632 -2.4218152 -1.9196615 -1.3245053 -0.80475116 -0.75698304 -0.82447577 -0.53188491 0.029612064 0.29499674 0.31511545 0.24031973 -0.063561916 -0.49368644 -0.90580153][-2.4156787 -2.3407013 -1.917448 -1.4115846 -0.84757042 -0.63834929 -0.47451258 -0.021413803 0.51949549 0.69410467 0.70822334 0.66927481 0.41227913 0.070457935 -0.23596525][-2.5698416 -2.6323276 -2.360429 -1.9091418 -1.2784956 -0.89715791 -0.58137226 -0.13757515 0.29417658 0.46903849 0.6147933 0.77625179 0.74259186 0.60576963 0.45719337][-2.8173323 -3.0854526 -3.0620909 -2.7995985 -2.3034778 -1.8173461 -1.3598783 -0.92200327 -0.5748322 -0.37082767 -0.043208122 0.38819695 0.639133 0.73805046 0.79133272][-3.0152411 -3.4822292 -3.6776838 -3.600625 -3.2671466 -2.7462296 -2.2281151 -1.8278739 -1.6270769 -1.5400441 -1.2027278 -0.6868856 -0.33186054 -0.11182976 0.10925961][-3.195457 -3.7795737 -4.049624 -4.0316339 -3.7913625 -3.2781367 -2.7443624 -2.4038234 -2.3707049 -2.48463 -2.3225241 -1.9779856 -1.7427962 -1.5479913 -1.2990084][-3.3216629 -3.7855844 -3.9962022 -3.9880939 -3.8561153 -3.4572544 -2.9846334 -2.6833463 -2.6800561 -2.8285203 -2.760637 -2.6183345 -2.5714271 -2.5056605 -2.3985763][-3.2250838 -3.4545915 -3.5508013 -3.5818129 -3.599262 -3.4241087 -3.1741238 -2.9990418 -2.9745359 -3.0059683 -2.8803804 -2.7628148 -2.7103286 -2.6382918 -2.6144836][-2.2518251 -2.3117194 -2.2935009 -2.332489 -2.5026722 -2.6339331 -2.7152195 -2.7866731 -2.8461347 -2.8429556 -2.7324708 -2.6657043 -2.6358657 -2.604131 -2.6358061]]...]
INFO - root - 2017-12-07 06:24:48.322869: step 13410, loss = 0.95, batch loss = 0.87 (10.3 examples/sec; 0.775 sec/batch; 68h:42m:50s remains)
INFO - root - 2017-12-07 06:24:55.882430: step 13420, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.745 sec/batch; 66h:00m:17s remains)
INFO - root - 2017-12-07 06:25:03.484435: step 13430, loss = 0.97, batch loss = 0.89 (10.8 examples/sec; 0.743 sec/batch; 65h:53m:36s remains)
INFO - root - 2017-12-07 06:25:11.160354: step 13440, loss = 0.61, batch loss = 0.54 (10.7 examples/sec; 0.744 sec/batch; 65h:58m:16s remains)
INFO - root - 2017-12-07 06:25:18.827987: step 13450, loss = 0.73, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 67h:25m:39s remains)
INFO - root - 2017-12-07 06:25:26.500402: step 13460, loss = 0.67, batch loss = 0.60 (10.2 examples/sec; 0.788 sec/batch; 69h:48m:36s remains)
INFO - root - 2017-12-07 06:25:34.187520: step 13470, loss = 1.00, batch loss = 0.93 (10.4 examples/sec; 0.772 sec/batch; 68h:27m:22s remains)
INFO - root - 2017-12-07 06:25:41.542194: step 13480, loss = 0.68, batch loss = 0.60 (10.2 examples/sec; 0.784 sec/batch; 69h:28m:07s remains)
INFO - root - 2017-12-07 06:25:49.254913: step 13490, loss = 0.82, batch loss = 0.74 (10.4 examples/sec; 0.768 sec/batch; 68h:05m:03s remains)
INFO - root - 2017-12-07 06:25:56.962201: step 13500, loss = 0.96, batch loss = 0.89 (10.1 examples/sec; 0.790 sec/batch; 69h:57m:34s remains)
2017-12-07 06:25:57.647229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0924923 -2.9107952 -2.583087 -2.3194447 -2.2738314 -2.3249826 -2.2528884 -2.1431043 -2.2225902 -2.4286773 -2.5478883 -2.6720135 -2.875813 -3.0417185 -3.0902162][-3.8492157 -3.5772407 -3.1318464 -2.7409365 -2.6232281 -2.6302533 -2.5099306 -2.3498321 -2.4152558 -2.6721389 -2.8531127 -3.0343676 -3.3119736 -3.5396812 -3.5856335][-4.507247 -4.1664214 -3.6316116 -3.1309776 -2.9278846 -2.8865838 -2.7139282 -2.5171137 -2.5772667 -2.8910789 -3.1595275 -3.40469 -3.7329962 -3.9881582 -4.0114388][-4.8503757 -4.4547143 -3.8334064 -3.229815 -2.92446 -2.7990739 -2.5171776 -2.2599542 -2.318011 -2.6868243 -3.0794139 -3.4306326 -3.833868 -4.1201148 -4.1442256][-4.8733692 -4.3873172 -3.6650441 -2.9781787 -2.5550656 -2.2781804 -1.8190646 -1.4588463 -1.5537727 -2.0059443 -2.5334523 -3.0349126 -3.5630918 -3.9297214 -4.0132766][-4.6255965 -4.0659561 -3.3516669 -2.7043452 -2.1984978 -1.7277658 -1.0824513 -0.6003387 -0.73871708 -1.290077 -1.9422617 -2.6098685 -3.262464 -3.6967769 -3.8453631][-4.4125853 -3.899956 -3.3461714 -2.8612685 -2.3463216 -1.701371 -0.94019628 -0.3899107 -0.55857992 -1.1887794 -1.859776 -2.5572824 -3.2281079 -3.6436887 -3.8013842][-4.4199562 -4.0546317 -3.6696489 -3.3494008 -2.9241815 -2.263762 -1.5748131 -1.109349 -1.2465422 -1.7966902 -2.3115492 -2.8664234 -3.4424317 -3.7669058 -3.8728538][-4.425703 -4.2364144 -3.990766 -3.8029408 -3.5389686 -3.0182731 -2.5100622 -2.2036028 -2.2880883 -2.6954041 -2.9975555 -3.3311968 -3.7561488 -3.9777942 -4.011971][-4.3660989 -4.2887225 -4.1444073 -4.0503449 -3.9303215 -3.5770433 -3.2506995 -3.1050258 -3.179811 -3.4509964 -3.5789573 -3.7247813 -4.0203886 -4.1718206 -4.1443186][-4.4955869 -4.4576845 -4.3423624 -4.2540903 -4.1650558 -3.8912232 -3.6426477 -3.577625 -3.6501021 -3.8194098 -3.8633423 -3.9312143 -4.1671252 -4.2835178 -4.2158618][-4.8665137 -4.8270698 -4.6790071 -4.5136986 -4.3746529 -4.129158 -3.8956225 -3.8242397 -3.8551977 -3.9283943 -3.9574485 -4.04382 -4.2758389 -4.3736849 -4.2761188][-5.1077914 -5.0177178 -4.8226328 -4.6165624 -4.4950886 -4.3451829 -4.1651611 -4.0699639 -4.035605 -4.012763 -4.0331545 -4.1519437 -4.3774223 -4.4521456 -4.3445063][-4.757525 -4.6512427 -4.5056868 -4.378387 -4.373652 -4.3887353 -4.3210325 -4.2525468 -4.18311 -4.0929565 -4.0861926 -4.1937742 -4.379076 -4.4234858 -4.3209405][-4.1720924 -4.0836844 -4.0184469 -3.9949448 -4.0800672 -4.204772 -4.2392321 -4.2443867 -4.2052813 -4.1153874 -4.0940552 -4.1631136 -4.2820892 -4.2876182 -4.186604]]...]
INFO - root - 2017-12-07 06:26:05.286713: step 13510, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 67h:20m:06s remains)
INFO - root - 2017-12-07 06:26:12.854757: step 13520, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 68h:28m:26s remains)
INFO - root - 2017-12-07 06:26:20.532021: step 13530, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.762 sec/batch; 67h:31m:42s remains)
INFO - root - 2017-12-07 06:26:28.233515: step 13540, loss = 0.86, batch loss = 0.79 (10.1 examples/sec; 0.789 sec/batch; 69h:54m:59s remains)
INFO - root - 2017-12-07 06:26:35.946581: step 13550, loss = 0.86, batch loss = 0.78 (10.1 examples/sec; 0.795 sec/batch; 70h:25m:14s remains)
INFO - root - 2017-12-07 06:26:43.608048: step 13560, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.763 sec/batch; 67h:36m:50s remains)
INFO - root - 2017-12-07 06:26:51.247742: step 13570, loss = 0.57, batch loss = 0.50 (10.4 examples/sec; 0.771 sec/batch; 68h:19m:07s remains)
INFO - root - 2017-12-07 06:26:58.773218: step 13580, loss = 0.69, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 69h:52m:51s remains)
INFO - root - 2017-12-07 06:27:06.518613: step 13590, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.772 sec/batch; 68h:23m:08s remains)
INFO - root - 2017-12-07 06:27:14.453153: step 13600, loss = 0.77, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 70h:19m:29s remains)
2017-12-07 06:27:15.085149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5313549 -1.3687575 -1.476413 -1.6563511 -1.6940877 -1.6367698 -1.4414954 -1.2761998 -1.3147976 -1.3142858 -1.2941892 -1.2847238 -1.2724581 -1.3241749 -1.3709323][-1.3373599 -1.2966213 -1.5276821 -1.7431369 -1.7330544 -1.6367264 -1.4760725 -1.3438289 -1.3723958 -1.4135489 -1.4595804 -1.4667611 -1.5015979 -1.5798242 -1.5594542][-1.3006268 -1.3697083 -1.6417625 -1.8304534 -1.8229241 -1.774055 -1.7012405 -1.6200693 -1.6290426 -1.7074769 -1.7991476 -1.8115051 -1.8804278 -1.9112136 -1.7147038][-1.4019589 -1.5385604 -1.7356677 -1.8278704 -1.8314095 -1.8445613 -1.8303275 -1.7646542 -1.738102 -1.8437541 -1.9631219 -1.9734855 -2.044569 -1.9937668 -1.6373832][-1.3991413 -1.4926677 -1.5627873 -1.5947106 -1.6280651 -1.6322834 -1.5664632 -1.4543011 -1.4014323 -1.5217333 -1.6404798 -1.6230612 -1.6593688 -1.5770957 -1.2381399][-1.202734 -1.1514814 -1.1051321 -1.1426883 -1.2168715 -1.1618872 -1.0042696 -0.87241578 -0.8453064 -1.0056007 -1.1169236 -1.0548201 -1.0413382 -0.99249077 -0.83959579][-0.87401843 -0.73368835 -0.6678257 -0.71930575 -0.754344 -0.60956955 -0.47902179 -0.50096059 -0.58519554 -0.75211048 -0.84273553 -0.77527571 -0.75541091 -0.75143075 -0.70931911][-0.5927639 -0.43396187 -0.40410233 -0.47243333 -0.4672029 -0.32857561 -0.38354397 -0.64013743 -0.78304029 -0.87106133 -0.9465909 -0.95615959 -0.9956224 -1.0358737 -0.97194219][-0.57065296 -0.42155361 -0.39606333 -0.45260525 -0.44177127 -0.39847231 -0.67832422 -1.08443 -1.2123523 -1.2288015 -1.3281057 -1.4008875 -1.4764245 -1.5454953 -1.463362][-0.82961059 -0.72432971 -0.67303085 -0.64267492 -0.59420419 -0.61148095 -0.97831416 -1.3702009 -1.4541261 -1.5135658 -1.6708958 -1.750001 -1.7940745 -1.8630252 -1.8302772][-1.0569503 -1.0158575 -0.9845345 -0.93204236 -0.887357 -0.89961004 -1.1694021 -1.4190779 -1.4874058 -1.6236253 -1.808342 -1.8670354 -1.8905792 -1.9940169 -2.05938][-1.1514776 -1.1549447 -1.1825671 -1.1967611 -1.2225618 -1.2352157 -1.3667686 -1.5102034 -1.5964172 -1.735486 -1.847111 -1.8626046 -1.8997335 -2.0569315 -2.1978171][-1.2913947 -1.3154695 -1.3856049 -1.439239 -1.4809592 -1.4800081 -1.5358808 -1.639611 -1.7135687 -1.7981644 -1.8520818 -1.8787255 -1.9550881 -2.1131165 -2.2539744][-1.4421594 -1.4740729 -1.5740569 -1.6441994 -1.6658573 -1.6434119 -1.6746352 -1.7555707 -1.7905736 -1.8268051 -1.8778756 -1.9625127 -2.0688195 -2.1765199 -2.2504964][-1.5130668 -1.5532331 -1.6595829 -1.7376144 -1.7608631 -1.7511039 -1.7634494 -1.7946241 -1.7952507 -1.8064129 -1.8557343 -1.9422498 -2.009656 -2.0258064 -2.0175502]]...]
INFO - root - 2017-12-07 06:27:22.708140: step 13610, loss = 0.75, batch loss = 0.68 (10.8 examples/sec; 0.742 sec/batch; 65h:45m:31s remains)
INFO - root - 2017-12-07 06:27:30.391666: step 13620, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.772 sec/batch; 68h:20m:46s remains)
INFO - root - 2017-12-07 06:27:38.048561: step 13630, loss = 0.90, batch loss = 0.82 (10.3 examples/sec; 0.777 sec/batch; 68h:51m:47s remains)
INFO - root - 2017-12-07 06:27:45.636968: step 13640, loss = 0.83, batch loss = 0.76 (10.7 examples/sec; 0.748 sec/batch; 66h:17m:36s remains)
INFO - root - 2017-12-07 06:27:53.158694: step 13650, loss = 0.71, batch loss = 0.64 (10.7 examples/sec; 0.750 sec/batch; 66h:25m:40s remains)
INFO - root - 2017-12-07 06:28:00.745502: step 13660, loss = 0.94, batch loss = 0.87 (10.1 examples/sec; 0.791 sec/batch; 70h:02m:39s remains)
INFO - root - 2017-12-07 06:28:08.391875: step 13670, loss = 0.70, batch loss = 0.62 (10.6 examples/sec; 0.758 sec/batch; 67h:08m:48s remains)
INFO - root - 2017-12-07 06:28:15.892950: step 13680, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.769 sec/batch; 68h:06m:16s remains)
INFO - root - 2017-12-07 06:28:23.545781: step 13690, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.772 sec/batch; 68h:19m:39s remains)
INFO - root - 2017-12-07 06:28:31.242235: step 13700, loss = 1.09, batch loss = 1.01 (10.1 examples/sec; 0.790 sec/batch; 69h:56m:44s remains)
2017-12-07 06:28:31.899759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0724087 -3.5259595 -3.6624365 -3.3388362 -3.0251174 -2.4014435 -2.2381814 -2.6604776 -2.7167544 -3.0120602 -3.5202365 -3.6929176 -3.7992506 -3.7869077 -3.7973254][-3.0231137 -3.6033812 -3.8072278 -3.4979703 -3.0389318 -2.3114271 -1.9816971 -2.1888988 -2.2640378 -2.6125896 -3.1062884 -3.3168917 -3.5363612 -3.6328549 -3.5997152][-3.0316947 -3.698879 -4.0201254 -3.829241 -3.3020723 -2.456784 -1.8530533 -1.758507 -1.9388132 -2.493001 -3.02357 -3.1748784 -3.2748842 -3.3033042 -3.2379096][-3.0821533 -3.8126309 -4.2422714 -4.182044 -3.6346364 -2.7281241 -1.9065385 -1.530956 -1.7864742 -2.483285 -3.0361514 -3.1898842 -3.2014966 -3.2328 -3.2313766][-2.9637508 -3.659163 -4.1418934 -4.1914086 -3.724726 -2.9388413 -2.1034296 -1.5883152 -1.8442814 -2.4787555 -2.9510498 -3.1736078 -3.2637603 -3.4210906 -3.518676][-2.6955907 -3.3212433 -3.7969666 -3.903512 -3.4935317 -2.7612853 -1.9120216 -1.3864279 -1.6693738 -2.2032444 -2.6036127 -2.9158967 -3.121294 -3.3950725 -3.542587][-2.4697945 -3.0097928 -3.4103632 -3.4960091 -3.056397 -2.1597321 -1.0346799 -0.42534637 -0.8566246 -1.5283103 -2.083271 -2.5169883 -2.7391696 -3.0022912 -3.1156974][-2.3823993 -2.7314496 -2.9747188 -2.9810719 -2.4426517 -1.2700133 0.20850658 0.92707682 0.23933887 -0.77571416 -1.6027191 -2.1106546 -2.2584348 -2.4589524 -2.5206313][-2.4475465 -2.556891 -2.5661159 -2.4500146 -1.8540475 -0.68743634 0.65067196 1.1615262 0.31705236 -0.745275 -1.4876549 -1.8368776 -1.8337495 -1.9387238 -1.9882388][-2.4892137 -2.4674659 -2.3096457 -2.1312382 -1.66049 -0.870806 -0.11021805 0.053449154 -0.57039523 -1.2187006 -1.566355 -1.6839569 -1.6552846 -1.7893395 -1.9261601][-2.4223089 -2.3115206 -2.1210358 -2.0182216 -1.7757394 -1.3608296 -1.0179353 -0.98233104 -1.2902019 -1.5534604 -1.6407278 -1.6523864 -1.7135158 -1.9589312 -2.1972334][-2.2400572 -1.9769256 -1.7654066 -1.7487721 -1.6578748 -1.4549122 -1.3583455 -1.4427872 -1.6710081 -1.8913636 -1.9882729 -2.0067513 -2.0899353 -2.2991123 -2.4524279][-1.9553654 -1.549715 -1.3261013 -1.3798199 -1.4012351 -1.3749831 -1.5153801 -1.7713263 -2.0652955 -2.3917608 -2.6185107 -2.7236855 -2.8466067 -2.9797597 -2.9399562][-1.730943 -1.220706 -0.9996419 -1.1606886 -1.3896151 -1.6045101 -1.996073 -2.3803108 -2.6263661 -2.9143498 -3.1923671 -3.3678484 -3.5429814 -3.6760857 -3.6086488][-1.6148822 -1.1063373 -0.99443865 -1.3227127 -1.7333684 -2.1098957 -2.6247911 -3.028646 -3.1520963 -3.3665156 -3.6713545 -3.8742681 -4.0628986 -4.2235522 -4.228147]]...]
INFO - root - 2017-12-07 06:28:39.482813: step 13710, loss = 0.70, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 67h:58m:01s remains)
INFO - root - 2017-12-07 06:28:47.050027: step 13720, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.756 sec/batch; 66h:54m:11s remains)
INFO - root - 2017-12-07 06:28:54.709567: step 13730, loss = 0.98, batch loss = 0.90 (10.1 examples/sec; 0.792 sec/batch; 70h:05m:08s remains)
INFO - root - 2017-12-07 06:29:02.290375: step 13740, loss = 0.69, batch loss = 0.61 (10.6 examples/sec; 0.752 sec/batch; 66h:35m:56s remains)
INFO - root - 2017-12-07 06:29:09.964648: step 13750, loss = 0.69, batch loss = 0.61 (10.2 examples/sec; 0.785 sec/batch; 69h:31m:19s remains)
INFO - root - 2017-12-07 06:29:17.560405: step 13760, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.755 sec/batch; 66h:51m:14s remains)
INFO - root - 2017-12-07 06:29:25.154284: step 13770, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.764 sec/batch; 67h:37m:04s remains)
INFO - root - 2017-12-07 06:29:32.509236: step 13780, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.761 sec/batch; 67h:22m:09s remains)
INFO - root - 2017-12-07 06:29:40.109326: step 13790, loss = 0.68, batch loss = 0.61 (10.9 examples/sec; 0.734 sec/batch; 65h:01m:25s remains)
INFO - root - 2017-12-07 06:29:47.664920: step 13800, loss = 0.77, batch loss = 0.70 (10.9 examples/sec; 0.737 sec/batch; 65h:15m:36s remains)
2017-12-07 06:29:48.317698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6925287 -2.1152771 -2.422071 -2.9881473 -3.0597339 -2.8407059 -2.6122127 -2.3688464 -1.913377 -1.5812263 -1.4654758 -1.1753607 -0.65571737 -1.1057615 -2.3564751][-2.1410391 -1.5032427 -1.8280778 -2.5104761 -2.6408882 -2.3699164 -2.1477995 -1.9751656 -1.4343669 -0.794935 -0.52373743 -0.33483219 -0.21078348 -1.0588782 -2.4757323][-1.736203 -1.2267573 -1.5717335 -2.2197542 -2.1638987 -1.7734268 -1.6764417 -1.62344 -0.96918988 -0.10229111 0.22174788 0.19932699 -0.18696404 -1.4495354 -2.9631524][-1.7388804 -1.4817793 -1.8424437 -2.323765 -1.8976109 -1.333653 -1.4208224 -1.4740314 -0.689692 0.30192661 0.56400442 0.28138733 -0.60397887 -2.204566 -3.6795421][-1.8039 -1.8147247 -2.1812375 -2.4507074 -1.6682415 -0.93408942 -1.1971736 -1.3300807 -0.45752597 0.52406931 0.66601229 0.17856169 -1.0006638 -2.739459 -4.1014633][-1.7217727 -1.9038587 -2.2107408 -2.2362595 -1.1783307 -0.37653017 -0.8187108 -1.0789645 -0.23108816 0.577858 0.56904411 -0.073832035 -1.3348684 -2.9242437 -4.0747914][-1.5468524 -1.821305 -2.0326872 -1.876307 -0.74265552 0.0050439835 -0.56841469 -0.85336304 0.00038719177 0.63927126 0.52736616 -0.26082134 -1.5500865 -2.9328218 -3.8788157][-1.3704309 -1.6936886 -1.7953689 -1.5702939 -0.55535364 0.063917637 -0.61213732 -0.90356946 -0.0774703 0.4624753 0.36050653 -0.42995286 -1.6007619 -2.7529144 -3.5755839][-1.202507 -1.4772503 -1.3981819 -1.2032037 -0.41843367 0.010239124 -0.81749606 -1.2083631 -0.47699332 0.056190968 0.07868576 -0.63452744 -1.6356804 -2.5411763 -3.2738681][-1.1794269 -1.2578113 -0.91670775 -0.775759 -0.23888731 0.0068192482 -1.0069833 -1.5346832 -0.91347432 -0.34647036 -0.20203352 -0.87763357 -1.7994196 -2.5521805 -3.1903577][-1.3752556 -1.2194495 -0.64296985 -0.53443432 -0.17226315 -0.044146061 -1.1687639 -1.7320158 -1.1803341 -0.68079448 -0.54684734 -1.2034671 -2.0559623 -2.6968932 -3.2506604][-1.4875708 -1.1367843 -0.42124748 -0.35835457 -0.16914606 -0.1818099 -1.3464775 -1.839777 -1.3124387 -0.97590995 -1.0175598 -1.6960728 -2.4227805 -2.9043207 -3.3345366][-1.4450319 -0.94160843 -0.16894341 -0.1448431 -0.17172289 -0.45186162 -1.6642611 -2.0678778 -1.5238543 -1.3306417 -1.6052995 -2.3296709 -2.8921952 -3.1525047 -3.3964338][-1.3046799 -0.73031282 -0.025844097 -0.032893181 -0.21952534 -0.7446897 -1.9452302 -2.3229992 -1.8960006 -1.8618395 -2.3066628 -2.9942598 -3.3743863 -3.4068499 -3.4310026][-1.1123686 -0.5153532 0.023023129 -0.034240723 -0.3297 -1.015502 -2.1039491 -2.4824002 -2.2876883 -2.4393649 -2.9592669 -3.525383 -3.7499411 -3.6333127 -3.4837723]]...]
INFO - root - 2017-12-07 06:29:56.023571: step 13810, loss = 0.73, batch loss = 0.66 (11.0 examples/sec; 0.725 sec/batch; 64h:08m:49s remains)
INFO - root - 2017-12-07 06:30:03.698415: step 13820, loss = 0.73, batch loss = 0.65 (10.6 examples/sec; 0.753 sec/batch; 66h:38m:04s remains)
INFO - root - 2017-12-07 06:30:11.298084: step 13830, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.759 sec/batch; 67h:09m:43s remains)
INFO - root - 2017-12-07 06:30:18.999488: step 13840, loss = 0.79, batch loss = 0.72 (10.0 examples/sec; 0.800 sec/batch; 70h:51m:05s remains)
INFO - root - 2017-12-07 06:30:26.748312: step 13850, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.786 sec/batch; 69h:35m:31s remains)
INFO - root - 2017-12-07 06:30:34.369358: step 13860, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.748 sec/batch; 66h:13m:35s remains)
INFO - root - 2017-12-07 06:30:42.035549: step 13870, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.763 sec/batch; 67h:32m:07s remains)
INFO - root - 2017-12-07 06:30:49.424519: step 13880, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 71h:00m:04s remains)
INFO - root - 2017-12-07 06:30:57.137298: step 13890, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 67h:12m:47s remains)
INFO - root - 2017-12-07 06:31:04.767814: step 13900, loss = 0.67, batch loss = 0.60 (10.5 examples/sec; 0.759 sec/batch; 67h:11m:35s remains)
2017-12-07 06:31:05.348692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6564014 -1.8782117 -1.8746583 -1.7699788 -1.6689246 -1.6046183 -1.3377035 -0.86886644 -0.47300768 0.0062980652 0.30174112 -0.19998169 -1.12467 -1.6809313 -1.7949696][-1.9525478 -2.0013769 -1.9436748 -1.8696072 -1.8246074 -1.8012931 -1.5312386 -1.0287902 -0.53519464 0.15379 0.60925579 0.099543095 -0.91648364 -1.5782959 -1.7695358][-2.6755018 -2.5456302 -2.4136751 -2.36414 -2.3466239 -2.2953522 -2.011806 -1.5337806 -1.0203507 -0.25262928 0.25005674 -0.23661852 -1.2221301 -1.9311657 -2.2068992][-3.3259063 -3.0227988 -2.8473234 -2.9047771 -2.9657149 -2.924757 -2.6673498 -2.2562952 -1.7754908 -1.068244 -0.64280486 -1.0563877 -1.8383625 -2.4626288 -2.7523763][-3.7802467 -3.3990653 -3.2442918 -3.3949523 -3.4913933 -3.4321287 -3.1831369 -2.8111262 -2.3982737 -1.8685746 -1.6134865 -1.9324079 -2.4743567 -2.9453518 -3.1592271][-3.9494753 -3.5991058 -3.5144119 -3.7112112 -3.7352414 -3.5569041 -3.2297165 -2.8700757 -2.5728629 -2.3314826 -2.3218074 -2.6202688 -3.0061021 -3.326051 -3.4332561][-3.7105064 -3.4353967 -3.4180102 -3.6157236 -3.5135598 -3.1983714 -2.8408284 -2.5608134 -2.4217832 -2.4832239 -2.7116847 -2.993468 -3.2380495 -3.4143431 -3.5153179][-3.2183201 -3.0051095 -3.0227239 -3.1616611 -2.9225235 -2.527667 -2.2216146 -2.0821025 -2.1314573 -2.42337 -2.7079775 -2.8310361 -2.8840008 -2.9821229 -3.2386923][-2.7410541 -2.5811863 -2.5617607 -2.5831041 -2.2640169 -1.8587563 -1.5963702 -1.5380328 -1.7030706 -2.0597289 -2.2474647 -2.1708338 -2.0949392 -2.1371639 -2.458271][-2.2213528 -2.0835066 -2.0046091 -1.9511852 -1.6676652 -1.3374581 -1.0835519 -1.0378416 -1.2130916 -1.5280683 -1.6182339 -1.4802802 -1.4152946 -1.3780823 -1.5827396][-1.8199878 -1.6432326 -1.4813347 -1.3683755 -1.1523964 -0.93479323 -0.71660423 -0.66278934 -0.79882646 -1.0516198 -1.1200731 -1.0178714 -0.995425 -0.89287233 -0.940778][-1.5888834 -1.3781347 -1.1317384 -0.94773173 -0.80610085 -0.74509 -0.60498142 -0.51079822 -0.58842659 -0.82650256 -0.949167 -0.89647675 -0.8298645 -0.63763571 -0.53027821][-1.4255586 -1.237236 -1.0082777 -0.84832692 -0.82872295 -0.91170979 -0.80418324 -0.57741332 -0.52206111 -0.72600126 -0.91722918 -0.93087482 -0.81074309 -0.54473543 -0.3226366][-1.3281832 -1.2429194 -1.1511388 -1.1113508 -1.2104495 -1.3292181 -1.1735516 -0.83256316 -0.70558977 -0.906425 -1.1115136 -1.1172876 -0.92244411 -0.63671803 -0.41056252][-1.441747 -1.5252085 -1.6128905 -1.6868773 -1.7972479 -1.8143613 -1.5481341 -1.1874771 -1.1114731 -1.3390086 -1.5111177 -1.4457681 -1.1515739 -0.87493277 -0.74600077]]...]
INFO - root - 2017-12-07 06:31:12.960408: step 13910, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.759 sec/batch; 67h:10m:06s remains)
INFO - root - 2017-12-07 06:31:20.535030: step 13920, loss = 0.63, batch loss = 0.56 (10.5 examples/sec; 0.762 sec/batch; 67h:23m:56s remains)
INFO - root - 2017-12-07 06:31:28.194827: step 13930, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.757 sec/batch; 67h:01m:39s remains)
INFO - root - 2017-12-07 06:31:35.846655: step 13940, loss = 0.68, batch loss = 0.61 (10.4 examples/sec; 0.770 sec/batch; 68h:10m:33s remains)
INFO - root - 2017-12-07 06:31:43.468529: step 13950, loss = 0.63, batch loss = 0.55 (10.7 examples/sec; 0.749 sec/batch; 66h:18m:17s remains)
INFO - root - 2017-12-07 06:31:51.178493: step 13960, loss = 0.94, batch loss = 0.87 (10.0 examples/sec; 0.798 sec/batch; 70h:34m:32s remains)
INFO - root - 2017-12-07 06:31:58.912454: step 13970, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.765 sec/batch; 67h:40m:38s remains)
INFO - root - 2017-12-07 06:32:06.382964: step 13980, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.762 sec/batch; 67h:26m:37s remains)
INFO - root - 2017-12-07 06:32:14.094928: step 13990, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.749 sec/batch; 66h:16m:14s remains)
INFO - root - 2017-12-07 06:32:21.689185: step 14000, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.756 sec/batch; 66h:54m:22s remains)
2017-12-07 06:32:22.291817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1928358 -1.9295866 -2.0500453 -2.4071763 -2.8148878 -3.0040259 -3.0002866 -2.9815207 -2.9896333 -3.001193 -3.0023589 -2.9996378 -2.9890769 -2.9865298 -2.9995711][-2.584141 -2.5424349 -2.904274 -3.3968382 -3.8108664 -3.8989573 -3.7828569 -3.7234962 -3.7364538 -3.7483778 -3.7232118 -3.6851978 -3.6426542 -3.6320293 -3.6758423][-2.8189082 -2.9349208 -3.4848714 -4.1071215 -4.5793505 -4.6764216 -4.5490685 -4.4929729 -4.5001431 -4.4536195 -4.3361268 -4.2168226 -4.1383276 -4.1434536 -4.2363782][-2.8159409 -3.0076284 -3.6443858 -4.3118191 -4.7749114 -4.8477087 -4.6871238 -4.6080942 -4.5968413 -4.487112 -4.3000822 -4.1733537 -4.1670117 -4.2576284 -4.3506956][-2.7568808 -2.9993076 -3.6330876 -4.2135568 -4.5059791 -4.4162321 -4.1166496 -3.9188347 -3.8150334 -3.6050997 -3.3526363 -3.3123074 -3.5441415 -3.8681438 -3.9889529][-2.6752009 -2.8488493 -3.2606244 -3.5075426 -3.4083314 -3.0172005 -2.5334527 -2.2603517 -2.1775062 -2.0344968 -1.8916585 -2.0712063 -2.6103969 -3.183238 -3.3829045][-2.6645281 -2.7258933 -2.8463311 -2.6576867 -2.0478532 -1.2147062 -0.4893918 -0.22638988 -0.42190695 -0.69814968 -1.0005159 -1.6050901 -2.4205959 -3.0586858 -3.1913915][-2.7977295 -2.8178139 -2.7978997 -2.3602293 -1.4786124 -0.42017651 0.41997337 0.65230751 0.26299095 -0.31132317 -0.96812177 -1.939486 -2.9916797 -3.6627 -3.765908][-2.9291863 -2.984535 -3.0589228 -2.7806535 -2.1158314 -1.3132687 -0.71419477 -0.62030339 -0.92724085 -1.3622575 -1.9157455 -2.855211 -3.8676462 -4.4444962 -4.5300851][-3.1067848 -3.303225 -3.6617622 -3.7907062 -3.5699885 -3.1256566 -2.7362919 -2.6788254 -2.825449 -3.0114641 -3.2937317 -3.9078965 -4.5516095 -4.8107438 -4.7932596][-3.2519615 -3.4925303 -3.9382119 -4.22588 -4.2247019 -3.9861634 -3.7312536 -3.7467115 -3.8655767 -3.9212713 -3.9729276 -4.2650318 -4.5794945 -4.6413884 -4.6448326][-3.327395 -3.5888743 -4.0411525 -4.3639054 -4.4406848 -4.3006644 -4.0923 -4.1099639 -4.1558132 -4.0615115 -3.9083302 -3.9601996 -4.1068788 -4.1744051 -4.3294377][-3.252707 -3.5394039 -4.0288162 -4.4035459 -4.5557804 -4.4566092 -4.2137227 -4.1056089 -3.9977841 -3.7918224 -3.5936563 -3.5854046 -3.6974978 -3.8343177 -4.1365404][-2.9922161 -3.1718335 -3.5573969 -3.9009326 -4.1130791 -4.1183143 -3.9639978 -3.8688264 -3.7362187 -3.5069389 -3.3422036 -3.3338523 -3.4486594 -3.6185365 -3.9370711][-2.7782323 -2.8133368 -3.0080647 -3.2036848 -3.3400173 -3.3328261 -3.22931 -3.2169542 -3.1977935 -3.0678072 -2.9701986 -2.9633682 -3.0570378 -3.2155445 -3.4855468]]...]
INFO - root - 2017-12-07 06:32:29.893635: step 14010, loss = 0.83, batch loss = 0.75 (10.1 examples/sec; 0.793 sec/batch; 70h:11m:14s remains)
INFO - root - 2017-12-07 06:32:37.474947: step 14020, loss = 0.78, batch loss = 0.71 (10.8 examples/sec; 0.738 sec/batch; 65h:19m:16s remains)
INFO - root - 2017-12-07 06:32:45.151059: step 14030, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.759 sec/batch; 67h:06m:34s remains)
INFO - root - 2017-12-07 06:32:52.769437: step 14040, loss = 0.90, batch loss = 0.83 (10.5 examples/sec; 0.759 sec/batch; 67h:06m:10s remains)
INFO - root - 2017-12-07 06:33:00.363116: step 14050, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.759 sec/batch; 67h:06m:16s remains)
INFO - root - 2017-12-07 06:33:08.008822: step 14060, loss = 0.64, batch loss = 0.56 (10.5 examples/sec; 0.759 sec/batch; 67h:09m:07s remains)
INFO - root - 2017-12-07 06:33:15.654513: step 14070, loss = 0.71, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 70h:54m:49s remains)
INFO - root - 2017-12-07 06:33:22.960822: step 14080, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.761 sec/batch; 67h:16m:35s remains)
INFO - root - 2017-12-07 06:33:30.600876: step 14090, loss = 0.65, batch loss = 0.58 (10.3 examples/sec; 0.776 sec/batch; 68h:37m:08s remains)
INFO - root - 2017-12-07 06:33:38.293030: step 14100, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 69h:00m:04s remains)
2017-12-07 06:33:39.008913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8368337 -1.7431264 -1.6648896 -1.6107256 -1.614135 -1.6741109 -1.79057 -1.9043067 -1.933157 -1.8743424 -1.7748823 -1.6519105 -1.4786305 -1.3214078 -1.2827132][-1.7474694 -1.6826758 -1.5965989 -1.4919105 -1.4360726 -1.4700236 -1.6218817 -1.8155344 -1.9117582 -1.8774331 -1.7735574 -1.6258297 -1.3964577 -1.1690471 -1.0728254][-1.5734632 -1.5526843 -1.4780452 -1.3426821 -1.2281122 -1.2631397 -1.4591634 -1.6711116 -1.7250006 -1.6060884 -1.4843497 -1.3944137 -1.2809293 -1.1589117 -1.0971463][-1.0292556 -0.99673057 -0.92032313 -0.78974319 -0.67797589 -0.75918555 -1.0218627 -1.2504792 -1.2766564 -1.1217508 -1.0585554 -1.0873194 -1.0843122 -1.0206792 -0.97981572][-0.38269806 -0.332623 -0.25868082 -0.15669489 -0.0586915 -0.1284337 -0.36145353 -0.53444457 -0.56884742 -0.48418903 -0.57023 -0.72927284 -0.74167442 -0.64067507 -0.61362362][0.02435112 0.077055454 0.11008978 0.087431431 0.10975885 0.1759119 0.26727486 0.41866589 0.43919325 0.32543421 0.0091104507 -0.29424477 -0.30651903 -0.18949175 -0.2967186][-0.11434174 -0.027897835 0.0263443 -0.055567741 0.011661053 0.43758249 1.1303425 1.8345037 1.8939257 1.3132882 0.51047087 -0.033638954 -0.055315495 0.089412212 -0.14924192][-0.561609 -0.50260997 -0.37875414 -0.35831785 -0.11109591 0.67535543 1.8480048 2.9382811 2.9512353 1.9506755 0.82884359 0.2129693 0.25088596 0.42338991 0.093658924][-0.7680378 -0.73000455 -0.61022639 -0.60324049 -0.38846159 0.3161149 1.3787508 2.31466 2.2822118 1.4222636 0.58958006 0.2518692 0.41432714 0.56170654 0.18918896][-0.82832742 -0.77686357 -0.71797752 -0.87580633 -0.93844652 -0.59407091 0.12026453 0.76448679 0.76287508 0.3684907 0.12786961 0.19053411 0.43461609 0.47669315 0.0815258][-1.2490232 -1.2763965 -1.3036735 -1.5064018 -1.7103701 -1.626395 -1.1821544 -0.73761821 -0.6616447 -0.647383 -0.4307363 -0.057120323 0.29778004 0.30264187 -0.095436573][-1.819639 -1.8950455 -1.889395 -1.9608088 -2.12343 -2.1915863 -2.0074592 -1.8039539 -1.7600636 -1.6095433 -1.2666292 -0.78367877 -0.3474288 -0.29266071 -0.618675][-2.2009892 -2.2266192 -2.1682444 -2.148221 -2.271539 -2.4342079 -2.43706 -2.4063151 -2.4199018 -2.3043206 -2.0709743 -1.7044082 -1.3267 -1.2298255 -1.4016111][-2.4085944 -2.4193466 -2.3753126 -2.353621 -2.4444833 -2.6096916 -2.6947622 -2.7369068 -2.7735605 -2.7050233 -2.6105161 -2.4353576 -2.213263 -2.1185269 -2.1209042][-2.5955248 -2.6470726 -2.68411 -2.7180009 -2.7758594 -2.867969 -2.932548 -2.9804187 -2.9910533 -2.9163415 -2.8564491 -2.7786655 -2.674834 -2.5841131 -2.4858041]]...]
INFO - root - 2017-12-07 06:33:46.611349: step 14110, loss = 0.94, batch loss = 0.86 (10.3 examples/sec; 0.775 sec/batch; 68h:33m:34s remains)
INFO - root - 2017-12-07 06:33:54.186311: step 14120, loss = 1.01, batch loss = 0.93 (10.6 examples/sec; 0.754 sec/batch; 66h:38m:42s remains)
INFO - root - 2017-12-07 06:34:01.795849: step 14130, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.769 sec/batch; 68h:00m:20s remains)
INFO - root - 2017-12-07 06:34:09.492130: step 14140, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.746 sec/batch; 65h:55m:50s remains)
INFO - root - 2017-12-07 06:34:17.085345: step 14150, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.767 sec/batch; 67h:49m:52s remains)
INFO - root - 2017-12-07 06:34:24.748231: step 14160, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.784 sec/batch; 69h:21m:33s remains)
INFO - root - 2017-12-07 06:34:32.329630: step 14170, loss = 0.95, batch loss = 0.88 (10.5 examples/sec; 0.765 sec/batch; 67h:39m:31s remains)
INFO - root - 2017-12-07 06:34:39.788375: step 14180, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.766 sec/batch; 67h:44m:12s remains)
INFO - root - 2017-12-07 06:34:47.412465: step 14190, loss = 0.92, batch loss = 0.85 (10.7 examples/sec; 0.745 sec/batch; 65h:51m:42s remains)
INFO - root - 2017-12-07 06:34:55.045105: step 14200, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 65h:55m:29s remains)
2017-12-07 06:34:55.687442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1206479 -2.1229653 -2.017863 -1.9652491 -2.1873543 -2.4221771 -2.3573089 -2.2249959 -2.1924207 -2.05638 -1.8332083 -1.7643166 -1.8035514 -1.9228938 -2.1594026][-2.2593107 -2.2900441 -2.117738 -1.9864545 -2.2192385 -2.4605646 -2.3540249 -2.2621028 -2.35147 -2.2840369 -2.0489457 -1.9621704 -1.9949884 -2.0966268 -2.3069668][-2.2314603 -2.2970538 -2.1322863 -2.0283425 -2.3544395 -2.6503525 -2.5027614 -2.3495872 -2.3748229 -2.2108436 -1.8737922 -1.7448201 -1.8042209 -1.9345291 -2.1511033][-2.0095661 -2.1192708 -2.0415235 -2.0489149 -2.4693112 -2.8105826 -2.6370411 -2.4078426 -2.3466659 -2.0535681 -1.5424206 -1.2824173 -1.3275928 -1.4674246 -1.7174361][-1.800998 -1.9679031 -2.0252998 -2.1517107 -2.57624 -2.8329473 -2.5502865 -2.2330725 -2.2160213 -1.9813068 -1.4063716 -1.065588 -1.0709403 -1.147536 -1.3942297][-1.731739 -1.9168937 -2.016607 -2.1415136 -2.4454932 -2.5159721 -2.0840461 -1.7416565 -1.9528809 -2.0273619 -1.6194098 -1.3302042 -1.2911546 -1.2690771 -1.4507296][-1.7467926 -1.917814 -1.9606049 -1.9646604 -2.0351634 -1.8294065 -1.1767309 -0.83192992 -1.382549 -1.9069834 -1.8275127 -1.6860797 -1.6219685 -1.5241096 -1.6459923][-1.8535013 -2.003057 -1.9873161 -1.942554 -1.9172287 -1.5261633 -0.6434269 -0.22027302 -0.9384315 -1.7305412 -1.9128416 -1.9091761 -1.8440926 -1.695174 -1.7771282][-2.0541973 -2.2147093 -2.2129371 -2.2863326 -2.4275546 -2.1387448 -1.2798264 -0.83441234 -1.3760452 -2.0226831 -2.1997151 -2.1970408 -2.0732203 -1.8711555 -1.98242][-2.1714768 -2.3151889 -2.3373582 -2.5743906 -2.9536562 -2.900795 -2.3000677 -1.9627883 -2.2744291 -2.6250863 -2.6701317 -2.6047778 -2.3842063 -2.1109352 -2.2565095][-2.2334287 -2.337657 -2.3685424 -2.6601009 -3.107106 -3.2241294 -2.936202 -2.7528994 -2.9152756 -3.0766046 -3.0367432 -2.9243817 -2.6435561 -2.349962 -2.521431][-2.5552845 -2.7066073 -2.8087556 -3.1049786 -3.486202 -3.6234722 -3.4907269 -3.3585873 -3.4015198 -3.5034833 -3.5151858 -3.4879389 -3.3106544 -3.1026649 -3.2532339][-3.0614295 -3.3509402 -3.566422 -3.8344076 -4.0952115 -4.20328 -4.1369095 -4.0061569 -3.9855237 -4.088048 -4.2109971 -4.3551378 -4.3661904 -4.275764 -4.3452735][-3.1403396 -3.4662511 -3.7252717 -3.9391975 -4.1076269 -4.2082453 -4.2188163 -4.1643491 -4.1735129 -4.2888641 -4.4563951 -4.6801181 -4.8088307 -4.7993903 -4.7997346][-2.6791162 -2.8434834 -3.0020924 -3.1212511 -3.2159421 -3.2999637 -3.3518367 -3.3647327 -3.4110889 -3.5232773 -3.6728344 -3.8649461 -3.9890974 -4.0019469 -4.0037627]]...]
INFO - root - 2017-12-07 06:35:03.371310: step 14210, loss = 1.01, batch loss = 0.93 (10.5 examples/sec; 0.764 sec/batch; 67h:32m:56s remains)
INFO - root - 2017-12-07 06:35:10.977125: step 14220, loss = 0.78, batch loss = 0.71 (10.7 examples/sec; 0.746 sec/batch; 65h:57m:23s remains)
INFO - root - 2017-12-07 06:35:18.708125: step 14230, loss = 1.10, batch loss = 1.02 (10.2 examples/sec; 0.785 sec/batch; 69h:22m:50s remains)
INFO - root - 2017-12-07 06:35:26.371388: step 14240, loss = 0.77, batch loss = 0.70 (10.8 examples/sec; 0.739 sec/batch; 65h:21m:08s remains)
INFO - root - 2017-12-07 06:35:33.955134: step 14250, loss = 0.77, batch loss = 0.69 (10.6 examples/sec; 0.756 sec/batch; 66h:51m:27s remains)
INFO - root - 2017-12-07 06:35:41.720464: step 14260, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.775 sec/batch; 68h:31m:22s remains)
INFO - root - 2017-12-07 06:35:49.375868: step 14270, loss = 0.88, batch loss = 0.81 (10.8 examples/sec; 0.744 sec/batch; 65h:46m:02s remains)
INFO - root - 2017-12-07 06:35:56.797001: step 14280, loss = 0.70, batch loss = 0.62 (10.9 examples/sec; 0.731 sec/batch; 64h:35m:38s remains)
INFO - root - 2017-12-07 06:36:04.456255: step 14290, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.776 sec/batch; 68h:36m:56s remains)
INFO - root - 2017-12-07 06:36:12.144195: step 14300, loss = 0.84, batch loss = 0.76 (10.3 examples/sec; 0.776 sec/batch; 68h:36m:27s remains)
2017-12-07 06:36:12.734943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.75127721 -0.53421545 -0.37109232 -0.29369783 -0.45827794 -0.93549013 -1.3754041 -1.5175886 -1.4279568 -1.1749799 -0.883029 -0.72304034 -0.754859 -0.92732882 -1.0792856][-0.58727026 -0.45024538 -0.37652206 -0.39737082 -0.64660811 -1.2143724 -1.7468479 -1.9582307 -1.8826063 -1.570014 -1.1744404 -0.92614412 -0.89608955 -1.0097609 -1.1103146][-0.50438309 -0.56819272 -0.66926885 -0.84136891 -1.1764514 -1.723762 -2.2210991 -2.4374051 -2.38405 -2.07073 -1.6393719 -1.3172655 -1.1826608 -1.1597157 -1.1300557][-0.43612909 -0.70006371 -0.98443913 -1.3270783 -1.7363181 -2.1863534 -2.5478382 -2.7163539 -2.6731524 -2.3904941 -1.9645612 -1.598428 -1.3878436 -1.264107 -1.1589248][-0.29129839 -0.7156477 -1.1326001 -1.5734324 -1.9795687 -2.2857804 -2.4889452 -2.6399112 -2.7046151 -2.5614302 -2.238024 -1.9220192 -1.7140377 -1.5843036 -1.513973][-0.14541912 -0.62804842 -1.0696988 -1.4630587 -1.7234793 -1.7942786 -1.8180578 -2.0083094 -2.3031776 -2.4249532 -2.3061423 -2.1349468 -2.0072482 -1.9519007 -2.022362][-0.087099075 -0.49339962 -0.82771945 -1.0444977 -1.038765 -0.78295279 -0.56930566 -0.77954078 -1.3264802 -1.740653 -1.8582053 -1.87204 -1.8925691 -1.996089 -2.2726507][-0.015665531 -0.29359007 -0.49530053 -0.53831816 -0.2909317 0.23124647 0.63323545 0.42504978 -0.30125809 -0.90079474 -1.1471007 -1.271136 -1.4650798 -1.7844827 -2.2479148][0.043681622 -0.15409136 -0.31186676 -0.31316805 -0.0078372955 0.54484844 0.95009661 0.75608587 0.056176186 -0.47473502 -0.61154127 -0.69698811 -1.042412 -1.573072 -2.14902][-0.030862331 -0.21760321 -0.3814311 -0.39941835 -0.1524806 0.27565432 0.58865023 0.446527 -0.067629814 -0.36927032 -0.27025986 -0.23737288 -0.66663432 -1.3466752 -1.9804349][-0.15033102 -0.33592749 -0.46742797 -0.45973468 -0.2653513 0.026340485 0.24600267 0.20245457 -0.07504034 -0.17680597 0.051541805 0.1340847 -0.33184481 -1.078187 -1.7178655][-0.18207884 -0.3499279 -0.44183373 -0.42107058 -0.28661203 -0.11191225 0.026461601 0.075487614 0.011175156 0.023918629 0.2191968 0.24053478 -0.23993635 -0.98772 -1.5726016][-0.13290548 -0.3247838 -0.45333028 -0.49919295 -0.4648006 -0.39754629 -0.33037329 -0.23350763 -0.146976 -0.064038277 0.052404404 -0.0089249611 -0.49523759 -1.2482553 -1.7909899][-0.04777813 -0.28669739 -0.49797869 -0.65433812 -0.75490975 -0.81942749 -0.85116577 -0.77776289 -0.61764383 -0.48427749 -0.41624784 -0.523607 -0.98543024 -1.7140374 -2.2174435][0.037064075 -0.2185812 -0.48942447 -0.74946237 -0.99726486 -1.2291472 -1.4079592 -1.4444189 -1.3276947 -1.2211094 -1.2154078 -1.3523831 -1.7364924 -2.3196361 -2.658062]]...]
INFO - root - 2017-12-07 06:36:20.380294: step 14310, loss = 0.81, batch loss = 0.74 (10.8 examples/sec; 0.741 sec/batch; 65h:27m:38s remains)
INFO - root - 2017-12-07 06:36:28.060176: step 14320, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.764 sec/batch; 67h:29m:47s remains)
INFO - root - 2017-12-07 06:36:35.760583: step 14330, loss = 0.82, batch loss = 0.75 (10.1 examples/sec; 0.788 sec/batch; 69h:41m:02s remains)
INFO - root - 2017-12-07 06:36:43.401405: step 14340, loss = 0.96, batch loss = 0.89 (10.3 examples/sec; 0.774 sec/batch; 68h:25m:37s remains)
INFO - root - 2017-12-07 06:36:51.004855: step 14350, loss = 0.82, batch loss = 0.75 (10.9 examples/sec; 0.732 sec/batch; 64h:43m:17s remains)
INFO - root - 2017-12-07 06:36:58.604478: step 14360, loss = 0.85, batch loss = 0.78 (10.7 examples/sec; 0.744 sec/batch; 65h:47m:20s remains)
INFO - root - 2017-12-07 06:37:06.218796: step 14370, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.778 sec/batch; 68h:43m:10s remains)
INFO - root - 2017-12-07 06:37:13.582589: step 14380, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.776 sec/batch; 68h:36m:09s remains)
INFO - root - 2017-12-07 06:37:21.242606: step 14390, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.756 sec/batch; 66h:45m:58s remains)
INFO - root - 2017-12-07 06:37:28.944591: step 14400, loss = 0.88, batch loss = 0.80 (10.7 examples/sec; 0.748 sec/batch; 66h:04m:34s remains)
2017-12-07 06:37:29.555369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3873582 -4.6596131 -4.5450745 -4.3005424 -4.0259895 -3.8676805 -3.9495218 -4.1475878 -4.2896862 -4.3153119 -4.2915759 -4.3340597 -4.4388866 -4.5580573 -4.6363049][-4.3980961 -4.627254 -4.5174718 -4.3279114 -4.1263843 -4.0193386 -4.1108747 -4.2541504 -4.30775 -4.2668452 -4.2207832 -4.2683992 -4.3397412 -4.3667645 -4.3532748][-4.2458205 -4.4060526 -4.2864523 -4.1472464 -4.0416241 -4.0819206 -4.3083348 -4.4963264 -4.5366726 -4.4249854 -4.2552991 -4.1651845 -4.0984564 -3.9649706 -3.8196929][-3.9967461 -4.1196437 -4.0010643 -3.8055294 -3.5808282 -3.5813172 -3.815259 -4.0620894 -4.2860055 -4.3679581 -4.2693005 -4.1556377 -4.0393958 -3.8668838 -3.6756315][-3.8218679 -4.0175538 -4.0478806 -3.899967 -3.5380006 -3.3347764 -3.3103924 -3.4073093 -3.7729981 -4.1524644 -4.314558 -4.3368287 -4.258028 -4.104681 -3.9152265][-3.8089492 -3.9918768 -4.1275916 -4.082067 -3.7701604 -3.5125928 -3.2846918 -3.1999671 -3.5674837 -4.0799146 -4.4194088 -4.5296712 -4.4189854 -4.1625123 -3.8494782][-3.8507185 -3.8776572 -3.869482 -3.6605892 -3.223846 -2.7678514 -2.2749727 -2.045012 -2.4511194 -3.1080616 -3.6681802 -3.9583664 -3.9178257 -3.6151705 -3.1686432][-3.9043069 -3.8757417 -3.7172816 -3.262217 -2.5829136 -1.7592037 -0.86864972 -0.43227649 -0.83607578 -1.6363592 -2.4251418 -2.9511614 -3.1244726 -3.0139074 -2.7363436][-4.0410767 -4.12072 -4.0087852 -3.5631919 -2.9238415 -2.0724564 -1.1326361 -0.67945027 -0.98035812 -1.645118 -2.3155537 -2.7589707 -2.92208 -2.9428654 -2.9685562][-4.2808685 -4.494112 -4.4636889 -4.1234159 -3.6445358 -2.9896851 -2.3294351 -2.0801325 -2.3234646 -2.7472262 -3.0919585 -3.2163463 -3.1539462 -3.1011527 -3.1993744][-4.378397 -4.7219086 -4.8243637 -4.6291633 -4.2588334 -3.7219181 -3.2640202 -3.1499715 -3.352354 -3.6096826 -3.7256587 -3.6391897 -3.4405632 -3.3105917 -3.3267579][-4.1334095 -4.4949589 -4.7264943 -4.7306571 -4.5319257 -4.1871638 -3.9487348 -3.9327955 -4.0827279 -4.1789389 -4.077425 -3.795609 -3.5009522 -3.35749 -3.3679285][-3.777915 -3.9560132 -4.1277966 -4.1991153 -4.1244683 -3.9668384 -3.9193423 -4.0013118 -4.152658 -4.1940861 -4.0304508 -3.7185364 -3.4337223 -3.3169475 -3.3734188][-3.7313085 -3.7201016 -3.7296166 -3.6977336 -3.5961976 -3.5036571 -3.5342045 -3.6533313 -3.8008668 -3.8368874 -3.7104819 -3.4906583 -3.3138938 -3.2640123 -3.3592544][-4.0264945 -3.9840302 -3.9311025 -3.8279858 -3.6964645 -3.6221213 -3.6602881 -3.760118 -3.8569734 -3.8697798 -3.7777436 -3.63515 -3.5445144 -3.5511823 -3.6577988]]...]
INFO - root - 2017-12-07 06:37:37.222753: step 14410, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.752 sec/batch; 66h:25m:25s remains)
INFO - root - 2017-12-07 06:37:44.898944: step 14420, loss = 0.91, batch loss = 0.84 (10.1 examples/sec; 0.789 sec/batch; 69h:42m:53s remains)
INFO - root - 2017-12-07 06:37:52.588042: step 14430, loss = 0.96, batch loss = 0.88 (10.7 examples/sec; 0.751 sec/batch; 66h:20m:42s remains)
INFO - root - 2017-12-07 06:38:00.287903: step 14440, loss = 0.72, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 67h:37m:08s remains)
INFO - root - 2017-12-07 06:38:07.876052: step 14450, loss = 0.75, batch loss = 0.68 (10.6 examples/sec; 0.755 sec/batch; 66h:44m:10s remains)
INFO - root - 2017-12-07 06:38:15.514597: step 14460, loss = 0.63, batch loss = 0.56 (10.5 examples/sec; 0.759 sec/batch; 67h:05m:33s remains)
INFO - root - 2017-12-07 06:38:23.113925: step 14470, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.776 sec/batch; 68h:34m:25s remains)
INFO - root - 2017-12-07 06:38:30.528112: step 14480, loss = 1.03, batch loss = 0.96 (10.6 examples/sec; 0.753 sec/batch; 66h:30m:02s remains)
INFO - root - 2017-12-07 06:38:38.207918: step 14490, loss = 0.93, batch loss = 0.86 (10.5 examples/sec; 0.765 sec/batch; 67h:34m:16s remains)
INFO - root - 2017-12-07 06:38:45.817539: step 14500, loss = 0.87, batch loss = 0.80 (10.8 examples/sec; 0.743 sec/batch; 65h:37m:15s remains)
2017-12-07 06:38:46.436189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5843754 -4.0739403 -3.7464638 -3.5182226 -2.7908323 -1.9586189 -1.8651044 -2.1178651 -2.2606802 -2.0717087 -1.8598452 -1.8550487 -1.965452 -2.3672245 -2.7710948][-5.0085273 -4.5338192 -4.3563862 -4.1891484 -3.1410851 -1.89727 -1.6852937 -2.1353035 -2.4221742 -2.1092212 -1.7789104 -1.7233548 -1.7223094 -2.0818303 -2.5788012][-4.9664493 -4.6308842 -4.7560382 -4.8386669 -3.7446144 -2.2965355 -1.9209406 -2.4362729 -2.8196049 -2.2965329 -1.6868241 -1.469224 -1.3910215 -1.7804875 -2.3733509][-4.4143782 -4.1323981 -4.3577185 -4.5221024 -3.425806 -1.7971585 -1.0546737 -1.5157743 -2.1670549 -1.7686937 -1.1979473 -1.0998504 -1.1687515 -1.6436467 -2.231828][-4.0437074 -3.9117541 -4.1453047 -4.1519051 -2.8045778 -0.73834872 0.54828787 0.056364059 -1.1519797 -1.1874566 -0.87432504 -1.0383718 -1.4338932 -2.0313983 -2.5032668][-3.9555326 -3.9250402 -4.1548133 -4.0534863 -2.5744002 -0.11118984 1.7527595 1.252512 -0.49728966 -0.99412465 -0.93263149 -1.3461607 -2.0878284 -2.8133478 -3.1626506][-3.8525314 -3.6598513 -3.7005961 -3.5079045 -2.1127172 0.43520498 2.5233793 1.8803687 -0.32551861 -1.2180016 -1.3026054 -1.7559752 -2.5689266 -3.2747138 -3.5619531][-3.691083 -3.3257661 -3.1671567 -2.9527657 -1.7867568 0.55343533 2.4543881 1.6232338 -0.67273211 -1.6895354 -1.7621887 -2.0138714 -2.5272093 -3.0212712 -3.3494902][-3.6390369 -3.240273 -3.1040072 -3.1615899 -2.4090409 -0.46116948 1.0546608 0.35017443 -1.383266 -2.0886028 -2.0392258 -2.0177305 -2.111423 -2.3609173 -2.8050556][-3.66749 -3.1661687 -3.0651309 -3.4254768 -3.0494239 -1.5282869 -0.49684453 -1.0492024 -2.1198356 -2.4123054 -2.2056549 -1.8877909 -1.6234851 -1.7898946 -2.3944361][-3.7643671 -3.0702055 -2.865973 -3.2850318 -3.0266411 -1.8140197 -1.2048991 -1.7092502 -2.3877807 -2.5567122 -2.4312782 -2.1003737 -1.7685804 -1.9602795 -2.5625563][-4.1192536 -3.489219 -3.3532684 -3.7011268 -3.3077195 -2.1203928 -1.5436189 -1.8050535 -2.1916034 -2.3604805 -2.4212751 -2.2999187 -2.1080391 -2.3237166 -2.7691255][-4.4292531 -4.0532556 -4.17139 -4.5452957 -4.1457043 -3.0160871 -2.2484848 -2.1133704 -2.1940012 -2.2410774 -2.3469429 -2.4268417 -2.4196258 -2.5875297 -2.8547544][-4.5469904 -4.4309373 -4.6980104 -5.0628171 -4.7969537 -3.8772302 -3.0236742 -2.7242651 -2.7749367 -2.8281128 -3.0150533 -3.2971377 -3.4004245 -3.3711767 -3.3637714][-4.6926322 -4.8877234 -5.177146 -5.3649974 -5.0325031 -4.1567082 -3.3092265 -3.1596746 -3.5006506 -3.7920845 -4.110405 -4.4286194 -4.4233069 -4.1037059 -3.7809806]]...]
INFO - root - 2017-12-07 06:38:54.010182: step 14510, loss = 0.77, batch loss = 0.70 (10.8 examples/sec; 0.737 sec/batch; 65h:08m:23s remains)
INFO - root - 2017-12-07 06:39:01.616506: step 14520, loss = 0.69, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 69h:03m:02s remains)
INFO - root - 2017-12-07 06:39:09.208105: step 14530, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.762 sec/batch; 67h:20m:02s remains)
INFO - root - 2017-12-07 06:39:16.807268: step 14540, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.774 sec/batch; 68h:22m:41s remains)
INFO - root - 2017-12-07 06:39:24.484444: step 14550, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.764 sec/batch; 67h:29m:06s remains)
INFO - root - 2017-12-07 06:39:32.205485: step 14560, loss = 0.82, batch loss = 0.74 (10.3 examples/sec; 0.776 sec/batch; 68h:33m:51s remains)
INFO - root - 2017-12-07 06:39:39.865751: step 14570, loss = 0.77, batch loss = 0.69 (10.2 examples/sec; 0.783 sec/batch; 69h:09m:37s remains)
INFO - root - 2017-12-07 06:39:47.354044: step 14580, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.771 sec/batch; 68h:05m:28s remains)
INFO - root - 2017-12-07 06:39:55.015767: step 14590, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.759 sec/batch; 67h:00m:57s remains)
INFO - root - 2017-12-07 06:40:02.719833: step 14600, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.778 sec/batch; 68h:43m:50s remains)
2017-12-07 06:40:03.362265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4185066 -3.2769959 -2.6746051 -1.4981174 -0.63111496 -0.27296448 -0.31731415 -0.7346251 -1.4013555 -2.2419167 -2.8660958 -3.058629 -2.9260077 -2.4429491 -2.0307078][-3.3731303 -3.2775974 -2.7439048 -1.755641 -0.93785954 -0.54182363 -0.46416044 -0.51065779 -0.87807107 -1.4468544 -1.9095004 -2.284229 -2.3841937 -2.0695531 -1.8510406][-3.2050571 -3.0531039 -2.4620619 -1.5170746 -0.72775316 -0.43457055 -0.42255116 -0.40895557 -0.77926064 -1.1182346 -1.2568398 -1.6046698 -1.8328166 -1.6584699 -1.5792439][-3.2268829 -2.9242682 -2.1490343 -1.0764139 -0.23575735 -0.032031536 -0.12969112 -0.25590515 -0.84578729 -1.1164026 -1.0384786 -1.4019184 -1.7397482 -1.643909 -1.5546653][-3.1160576 -2.730963 -1.8661129 -0.72196436 0.10015488 0.18819237 0.01142931 -0.29050636 -1.1151409 -1.3612709 -1.1964989 -1.6311443 -2.001487 -1.9056065 -1.736793][-2.6947441 -2.2817986 -1.4725268 -0.3715415 0.38182068 0.44146967 0.33257818 -0.072370052 -1.1228926 -1.4087019 -1.2798252 -1.8218992 -2.1896129 -2.11289 -1.9814439][-2.3756211 -1.9032531 -1.1889298 -0.18875933 0.54862118 0.82428074 1.0686765 0.76438618 -0.45371389 -0.86018968 -0.93395567 -1.7155578 -2.2097197 -2.2693915 -2.3197434][-2.0263188 -1.5291858 -0.95098495 -0.15886116 0.46202087 0.87532949 1.4485459 1.3348355 0.076049805 -0.35891342 -0.57605934 -1.5595171 -2.2197273 -2.4378481 -2.6650615][-1.613344 -1.2204721 -0.8082931 -0.29383802 0.075331688 0.38242531 0.98457289 0.921957 -0.20661688 -0.53513145 -0.73104692 -1.7274084 -2.3776231 -2.5640497 -2.7722061][-1.1720717 -1.0465696 -0.84085393 -0.5558753 -0.41420317 -0.2998457 0.16323805 0.048101425 -0.91030478 -1.0941858 -1.2110951 -2.0594449 -2.5558569 -2.5791788 -2.6360676][-1.0480289 -1.2649572 -1.2632883 -1.1469929 -1.110276 -1.0667667 -0.70556784 -0.84485674 -1.622683 -1.7233491 -1.8010163 -2.4200194 -2.7246807 -2.6319022 -2.5861859][-1.4349926 -1.8763225 -1.9902427 -1.9713962 -1.9681568 -1.917932 -1.5889626 -1.678638 -2.2407782 -2.2961888 -2.3641174 -2.775929 -2.9445724 -2.8136115 -2.7140243][-2.0688136 -2.5395851 -2.6840563 -2.7226286 -2.73023 -2.6608858 -2.3563294 -2.3634818 -2.6804037 -2.6877544 -2.7599516 -3.0216379 -3.1261971 -3.023335 -2.9261637][-2.6592083 -2.9714248 -3.0716219 -3.1296597 -3.1336865 -3.0379462 -2.761626 -2.6776066 -2.7697077 -2.7428689 -2.8374207 -3.0336924 -3.1509848 -3.1268771 -3.0705681][-3.0343175 -3.1391027 -3.1738505 -3.2205219 -3.2301774 -3.1742072 -3.0063214 -2.91765 -2.9002976 -2.8756638 -2.9843621 -3.1413298 -3.2675366 -3.2879744 -3.234848]]...]
INFO - root - 2017-12-07 06:40:11.090036: step 14610, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 68h:26m:22s remains)
INFO - root - 2017-12-07 06:40:18.767137: step 14620, loss = 0.87, batch loss = 0.79 (10.1 examples/sec; 0.790 sec/batch; 69h:43m:13s remains)
INFO - root - 2017-12-07 06:40:26.383704: step 14630, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.781 sec/batch; 68h:58m:12s remains)
INFO - root - 2017-12-07 06:40:34.075091: step 14640, loss = 0.84, batch loss = 0.77 (10.3 examples/sec; 0.773 sec/batch; 68h:15m:44s remains)
INFO - root - 2017-12-07 06:40:41.901460: step 14650, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.745 sec/batch; 65h:47m:49s remains)
INFO - root - 2017-12-07 06:40:49.728061: step 14660, loss = 0.93, batch loss = 0.86 (10.2 examples/sec; 0.786 sec/batch; 69h:24m:07s remains)
INFO - root - 2017-12-07 06:40:57.506395: step 14670, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.784 sec/batch; 69h:12m:13s remains)
INFO - root - 2017-12-07 06:41:04.920637: step 14680, loss = 0.94, batch loss = 0.86 (10.5 examples/sec; 0.762 sec/batch; 67h:17m:52s remains)
INFO - root - 2017-12-07 06:41:12.636385: step 14690, loss = 0.85, batch loss = 0.77 (10.3 examples/sec; 0.778 sec/batch; 68h:38m:31s remains)
INFO - root - 2017-12-07 06:41:20.162228: step 14700, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.751 sec/batch; 66h:15m:41s remains)
2017-12-07 06:41:20.748685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9168727 -3.9845171 -4.0358782 -3.8820572 -3.6978936 -3.5783095 -3.5146222 -3.412024 -3.3590107 -3.4267893 -3.4509425 -3.4795151 -3.4656653 -3.465024 -3.4597826][-3.8293836 -3.8981335 -3.9868855 -3.8504326 -3.673625 -3.56517 -3.4774621 -3.3805175 -3.364449 -3.45845 -3.4699068 -3.4662337 -3.4689169 -3.4615195 -3.4543242][-3.0906465 -3.1391096 -3.2917948 -3.2664313 -3.1949883 -3.1780286 -3.1131768 -3.0489898 -3.0315926 -3.0630968 -3.0180264 -2.9437091 -2.8880734 -2.8009281 -2.7396948][-2.091888 -2.1145537 -2.3399553 -2.4477413 -2.4652817 -2.5175965 -2.4883404 -2.4422271 -2.3381402 -2.2021041 -2.0402861 -1.8605218 -1.7443128 -1.6339476 -1.579633][-1.1800575 -1.1812255 -1.4635348 -1.6684344 -1.7325058 -1.7972419 -1.8023553 -1.7852767 -1.6290791 -1.3435378 -1.0427678 -0.74610925 -0.58298421 -0.50633359 -0.48767853][-0.99882579 -1.0003762 -1.2222371 -1.3318667 -1.3112288 -1.3199885 -1.3389723 -1.3842733 -1.3044515 -1.0778592 -0.81511474 -0.52476788 -0.35747051 -0.28364849 -0.28677273][-1.4911568 -1.4903541 -1.6127365 -1.5461981 -1.3661482 -1.2497203 -1.2373493 -1.3688006 -1.4777303 -1.4958606 -1.4541628 -1.3061254 -1.1713293 -1.075552 -1.06422][-1.7336693 -1.7062614 -1.8182511 -1.7396481 -1.5490456 -1.4117587 -1.4191678 -1.6210451 -1.8541038 -2.0581355 -2.2158 -2.2249827 -2.1264188 -2.0104866 -1.9556935][-1.6800537 -1.6233244 -1.7845085 -1.8360488 -1.7875376 -1.7539089 -1.8058665 -1.9759798 -2.1447337 -2.3098783 -2.4755125 -2.5329871 -2.4610014 -2.3745582 -2.313987][-1.5735741 -1.4819007 -1.6447802 -1.8092515 -1.9273047 -2.0021572 -2.0506282 -2.1149378 -2.1344635 -2.1642716 -2.243197 -2.2985442 -2.2631843 -2.2465072 -2.2457578][-1.780468 -1.6706789 -1.7753804 -1.9514234 -2.1245823 -2.2256532 -2.2291851 -2.2164168 -2.1221545 -2.0267079 -2.0206652 -2.0426619 -2.0346365 -2.0713656 -2.1226413][-2.1697118 -2.0413938 -2.0636759 -2.1891127 -2.3209705 -2.3944695 -2.37179 -2.327204 -2.1577528 -1.9723611 -1.9086478 -1.8686657 -1.8582764 -1.9449883 -2.0410888][-2.4547708 -2.3194571 -2.30364 -2.3807995 -2.4455228 -2.4949756 -2.46387 -2.3906252 -2.1833992 -1.9835925 -1.9324427 -1.8591621 -1.8293693 -1.9548459 -2.0861788][-2.5062578 -2.3636482 -2.3630593 -2.418999 -2.4156291 -2.4349828 -2.4018435 -2.3515487 -2.2013674 -2.079674 -2.0992498 -2.0462282 -1.9936113 -2.0960243 -2.1933749][-2.3855464 -2.2392478 -2.2888606 -2.3597054 -2.317404 -2.3301663 -2.3181238 -2.3072569 -2.2271945 -2.1393981 -2.1801682 -2.1686811 -2.1434138 -2.2407141 -2.3281732]]...]
INFO - root - 2017-12-07 06:41:28.405782: step 14710, loss = 0.81, batch loss = 0.73 (10.4 examples/sec; 0.766 sec/batch; 67h:35m:28s remains)
INFO - root - 2017-12-07 06:41:36.008129: step 14720, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.761 sec/batch; 67h:11m:51s remains)
INFO - root - 2017-12-07 06:41:43.645313: step 14730, loss = 0.98, batch loss = 0.90 (10.5 examples/sec; 0.764 sec/batch; 67h:28m:41s remains)
INFO - root - 2017-12-07 06:41:51.234414: step 14740, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.761 sec/batch; 67h:10m:26s remains)
INFO - root - 2017-12-07 06:41:58.893222: step 14750, loss = 0.70, batch loss = 0.63 (10.9 examples/sec; 0.734 sec/batch; 64h:48m:28s remains)
INFO - root - 2017-12-07 06:42:06.582371: step 14760, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.788 sec/batch; 69h:33m:20s remains)
INFO - root - 2017-12-07 06:42:14.133137: step 14770, loss = 0.70, batch loss = 0.63 (11.2 examples/sec; 0.713 sec/batch; 62h:56m:04s remains)
INFO - root - 2017-12-07 06:42:21.542409: step 14780, loss = 0.83, batch loss = 0.76 (10.1 examples/sec; 0.790 sec/batch; 69h:43m:56s remains)
INFO - root - 2017-12-07 06:42:29.229917: step 14790, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.769 sec/batch; 67h:51m:57s remains)
INFO - root - 2017-12-07 06:42:36.823655: step 14800, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.748 sec/batch; 66h:00m:05s remains)
2017-12-07 06:42:37.502102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4069245 -1.4093823 -1.4791439 -1.5922508 -1.7151725 -1.8204563 -1.8940887 -1.9449494 -1.9181139 -1.8554218 -1.66994 -1.4215896 -1.2589324 -1.0962284 -0.97411323][-1.2336502 -1.2478738 -1.3580019 -1.5255151 -1.7065196 -1.8598983 -1.9678674 -2.0364847 -2.0106697 -1.9469225 -1.7690861 -1.5133622 -1.3223486 -1.1164885 -0.9511342][-0.97460032 -1.0327547 -1.1818769 -1.3719318 -1.5617101 -1.7267892 -1.8602507 -1.9288459 -1.8681927 -1.7795362 -1.6154945 -1.3737319 -1.1984549 -1.029686 -0.90507436][-0.39833784 -0.58590865 -0.80928683 -1.0140862 -1.1765912 -1.3229997 -1.4814255 -1.5693936 -1.5085695 -1.4295869 -1.3161056 -1.134779 -1.0150895 -0.92477083 -0.87858486][0.18276405 -0.16990995 -0.4512918 -0.60821414 -0.62401867 -0.60536933 -0.65374684 -0.69089174 -0.62649965 -0.590405 -0.58482933 -0.55138493 -0.58359146 -0.64249015 -0.73572874][0.629292 0.22432852 -0.037969112 -0.14185333 -0.040108681 0.11876154 0.19667864 0.22815323 0.26526451 0.23311901 0.14738846 0.050680161 -0.1562624 -0.40655136 -0.66282821][1.1486602 0.8458643 0.65440178 0.55660963 0.63280725 0.75178909 0.81975317 0.84589243 0.80498838 0.69686794 0.57824707 0.4578495 0.16970015 -0.22289085 -0.60437155][1.398448 1.2943907 1.2076268 1.1317496 1.1528592 1.1638093 1.1406856 1.1017938 0.98496914 0.86008692 0.78532743 0.72653818 0.440475 -0.016602039 -0.44794369][1.1677499 1.1459398 1.081254 1.0093446 0.95390224 0.8606329 0.76496553 0.66476011 0.51121378 0.43323135 0.475111 0.54262877 0.34520912 -0.068754196 -0.42960453][0.32050562 0.32335997 0.27932692 0.22132587 0.10084772 -0.042100906 -0.11487293 -0.18512201 -0.28436518 -0.25074053 -0.06561327 0.12312508 0.032978535 -0.27048779 -0.48805737][-0.61131716 -0.60154963 -0.60387135 -0.6312964 -0.77485871 -0.93645096 -1.010499 -1.075722 -1.1452429 -1.0644565 -0.82302713 -0.58913803 -0.58128166 -0.72448373 -0.75244451][-1.1154368 -1.1304095 -1.1448882 -1.1902437 -1.330075 -1.4660547 -1.5402722 -1.6103168 -1.6719968 -1.6120021 -1.4152477 -1.2169755 -1.1582408 -1.1783838 -1.0882294][-1.2144682 -1.2277904 -1.2429953 -1.2831697 -1.3702469 -1.45278 -1.5056169 -1.5574007 -1.6017234 -1.5771031 -1.4733317 -1.3672996 -1.3292003 -1.3157501 -1.2135785][-1.0134523 -1.045752 -1.0724993 -1.1013079 -1.1418405 -1.1904542 -1.2320812 -1.2745183 -1.3273263 -1.3489242 -1.3187995 -1.2752466 -1.2604058 -1.2513928 -1.1768658][-0.66920567 -0.69428968 -0.71098232 -0.72566819 -0.75527167 -0.80752516 -0.84905648 -0.882838 -0.94071221 -0.97967505 -0.96743274 -0.93764949 -0.94015837 -0.96512461 -0.964653]]...]
INFO - root - 2017-12-07 06:42:45.055461: step 14810, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.762 sec/batch; 67h:13m:45s remains)
INFO - root - 2017-12-07 06:42:52.703071: step 14820, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.783 sec/batch; 69h:05m:51s remains)
INFO - root - 2017-12-07 06:43:00.403980: step 14830, loss = 1.03, batch loss = 0.95 (10.3 examples/sec; 0.777 sec/batch; 68h:35m:40s remains)
INFO - root - 2017-12-07 06:43:07.954679: step 14840, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.767 sec/batch; 67h:40m:43s remains)
INFO - root - 2017-12-07 06:43:15.699490: step 14850, loss = 0.84, batch loss = 0.77 (10.0 examples/sec; 0.803 sec/batch; 70h:50m:24s remains)
INFO - root - 2017-12-07 06:43:23.336491: step 14860, loss = 0.92, batch loss = 0.85 (10.8 examples/sec; 0.744 sec/batch; 65h:36m:42s remains)
INFO - root - 2017-12-07 06:43:30.999857: step 14870, loss = 0.67, batch loss = 0.60 (10.5 examples/sec; 0.760 sec/batch; 67h:01m:40s remains)
INFO - root - 2017-12-07 06:43:38.539618: step 14880, loss = 0.80, batch loss = 0.73 (9.8 examples/sec; 0.816 sec/batch; 71h:59m:44s remains)
INFO - root - 2017-12-07 06:43:46.304527: step 14890, loss = 0.90, batch loss = 0.83 (10.5 examples/sec; 0.758 sec/batch; 66h:54m:44s remains)
INFO - root - 2017-12-07 06:43:53.949892: step 14900, loss = 0.84, batch loss = 0.76 (10.7 examples/sec; 0.748 sec/batch; 65h:58m:27s remains)
2017-12-07 06:43:54.621842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5340037 -3.8800473 -3.7944293 -3.4585447 -2.6078153 -1.2098362 0.23363543 1.3038855 1.8184657 1.527904 0.36292505 -1.1581657 -2.4393854 -3.171937 -3.3727746][-3.2946789 -3.6230121 -3.6171763 -3.6020374 -3.4301336 -2.8061821 -1.7857933 -0.44235945 1.1371336 2.106195 1.5782337 -0.074640274 -1.7599993 -2.7345676 -3.0453596][-2.3128533 -2.581898 -2.6438022 -2.8913031 -3.350008 -3.6401434 -3.532146 -2.678648 -0.7536056 1.2800245 1.8525367 0.68333292 -1.0700412 -2.2396083 -2.6173348][-1.093092 -1.253114 -1.2852669 -1.5455086 -2.3104827 -3.3229694 -4.1809869 -4.2106228 -2.7337866 -0.3290844 1.304862 1.073617 -0.34968328 -1.6723292 -2.213613][-0.42358971 -0.43471384 -0.30722189 -0.24368095 -0.75542545 -1.897321 -3.3391507 -4.3791742 -4.0738559 -2.1967845 0.039070129 0.93105364 0.17707396 -1.2174327 -1.9411123][-0.443681 -0.30529118 0.067192554 0.68322945 0.9698205 0.46437263 -0.96976829 -2.9217107 -4.2533622 -3.764564 -1.7796278 -0.13320351 -0.063643932 -1.2372792 -2.0283551][-1.1076286 -0.9693644 -0.438097 0.68017483 1.9890614 2.7643924 2.052763 -0.31787348 -3.1311715 -4.3671522 -3.3650079 -1.6728256 -1.0554292 -1.7968602 -2.5173776][-2.4701235 -2.5922923 -2.2531149 -1.1361246 0.74382257 2.8537598 3.6639633 2.067091 -1.1861672 -3.7118912 -3.9833102 -2.9249754 -2.296762 -2.7476916 -3.3319683][-3.879626 -4.3545556 -4.5083814 -3.9150016 -2.1684475 0.69323206 3.0420609 3.1160626 0.69565535 -2.2460554 -3.5865755 -3.4950652 -3.2485521 -3.5556436 -3.9434123][-4.16811 -4.8293796 -5.4626861 -5.5812798 -4.4838047 -1.7566187 1.2886176 2.8787766 1.8896122 -0.68805432 -2.6566415 -3.4026413 -3.5628264 -3.8019683 -4.0046816][-3.2308965 -3.7806258 -4.62444 -5.3772249 -5.30142 -3.5180535 -0.71220851 1.7625365 2.209167 0.51084757 -1.5468342 -2.8306947 -3.3952575 -3.7541122 -3.9014034][-2.3022606 -2.6350822 -3.3980284 -4.4236336 -5.1612105 -4.4712439 -2.3244104 0.35085058 1.6683583 0.9322238 -0.755569 -2.1614411 -2.9727056 -3.5131917 -3.7123523][-2.2473171 -2.4330609 -2.816164 -3.4747825 -4.2520604 -4.192214 -2.6901474 -0.38833523 0.97349787 0.70828819 -0.54505229 -1.6899481 -2.3460965 -2.8380146 -3.1310992][-2.2102783 -2.3328836 -2.3701243 -2.5747519 -3.0451069 -3.1682351 -2.0930588 -0.29734278 0.7252841 0.49922228 -0.46793175 -1.2376559 -1.4555535 -1.5936882 -1.9270947][-2.3872936 -2.4395838 -2.2394271 -2.182498 -2.429966 -2.6015477 -1.7354527 -0.20804405 0.67088747 0.48749781 -0.33698368 -0.97503948 -0.88998675 -0.47799587 -0.44415689]]...]
INFO - root - 2017-12-07 06:44:02.147089: step 14910, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.766 sec/batch; 67h:33m:19s remains)
INFO - root - 2017-12-07 06:44:09.761142: step 14920, loss = 0.74, batch loss = 0.66 (10.0 examples/sec; 0.801 sec/batch; 70h:42m:07s remains)
INFO - root - 2017-12-07 06:44:17.350166: step 14930, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.769 sec/batch; 67h:52m:45s remains)
INFO - root - 2017-12-07 06:44:24.915226: step 14940, loss = 0.60, batch loss = 0.53 (10.8 examples/sec; 0.739 sec/batch; 65h:11m:35s remains)
INFO - root - 2017-12-07 06:44:32.544489: step 14950, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.763 sec/batch; 67h:19m:37s remains)
INFO - root - 2017-12-07 06:44:40.142051: step 14960, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.753 sec/batch; 66h:27m:00s remains)
INFO - root - 2017-12-07 06:44:47.681239: step 14970, loss = 0.87, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 66h:29m:49s remains)
INFO - root - 2017-12-07 06:44:55.269906: step 14980, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.784 sec/batch; 69h:09m:40s remains)
INFO - root - 2017-12-07 06:45:02.965785: step 14990, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.746 sec/batch; 65h:45m:45s remains)
INFO - root - 2017-12-07 06:45:10.579274: step 15000, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.781 sec/batch; 68h:53m:26s remains)
2017-12-07 06:45:11.157303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7024581 -3.7672443 -3.7990406 -3.701036 -3.4716144 -3.2273986 -3.126826 -3.332108 -3.9540298 -4.6529446 -4.9675331 -4.7207875 -4.1810803 -3.6744313 -3.4306295][-3.716289 -3.8002663 -3.8173013 -3.6978543 -3.4169841 -3.0152287 -2.6512988 -2.672332 -3.4346156 -4.5491071 -5.2632775 -5.162581 -4.484694 -3.7644424 -3.3668413][-3.7167296 -3.7808385 -3.7663674 -3.69057 -3.4348724 -2.8398008 -2.0756731 -1.7240748 -2.5052819 -4.062542 -5.3003163 -5.4958329 -4.8101797 -3.946039 -3.4049802][-3.6923118 -3.7147007 -3.6620603 -3.6398759 -3.4107347 -2.6379519 -1.5219917 -0.83708262 -1.6609221 -3.6768124 -5.3992529 -5.8609552 -5.2006297 -4.2180519 -3.5180938][-3.576287 -3.4995489 -3.352232 -3.2956197 -3.0407662 -2.1805091 -0.89454508 -0.091107368 -1.0231352 -3.3948126 -5.4781547 -6.1929708 -5.6896667 -4.6682453 -3.7735252][-3.2648935 -3.053812 -2.7537553 -2.4961851 -2.0954146 -1.2739706 -0.12436724 0.544014 -0.565526 -3.1018555 -5.3381786 -6.3104067 -6.1026754 -5.163321 -4.1002784][-2.7016222 -2.312278 -1.8256876 -1.3418949 -0.80864859 -0.076356411 0.90231466 1.4792047 0.37022018 -2.0329857 -4.268023 -5.5858459 -5.8749342 -5.2556953 -4.2236357][-2.1464179 -1.5083485 -0.80203247 -0.17217302 0.3472805 0.866622 1.6292558 2.1942697 1.3567381 -0.57311988 -2.6388259 -4.2951837 -5.1775851 -5.0419359 -4.2300439][-2.0605996 -1.2438085 -0.41193151 0.18591213 0.51243734 0.73687553 1.2206211 1.6854873 1.2116852 -0.030750275 -1.6371565 -3.3097305 -4.5171494 -4.7659917 -4.2289152][-2.4173095 -1.5753868 -0.81544328 -0.49842691 -0.56895089 -0.68803 -0.38235426 0.089581966 0.0093865395 -0.59962082 -1.7182937 -3.1866364 -4.3811212 -4.7215929 -4.3287096][-2.9931922 -2.3342702 -1.7802005 -1.7382181 -2.0976317 -2.4375052 -2.304507 -1.9401965 -1.8118677 -1.9800224 -2.65676 -3.7453909 -4.6546 -4.8737411 -4.5064006][-3.5732739 -3.1648669 -2.8153 -2.8796329 -3.2644324 -3.6225274 -3.6218395 -3.4133859 -3.2665977 -3.2555642 -3.6110044 -4.2694135 -4.8174047 -4.8780007 -4.5314288][-3.8859906 -3.6848288 -3.4879582 -3.5490479 -3.8427725 -4.14006 -4.2262521 -4.1369767 -4.0148072 -3.9563169 -4.1362824 -4.4891882 -4.754674 -4.70042 -4.3934164][-3.9095397 -3.8446424 -3.7706177 -3.8198466 -4.004807 -4.2230196 -4.34473 -4.3200831 -4.2114539 -4.1379681 -4.2087097 -4.372025 -4.4722419 -4.3855996 -4.1523542][-3.7933285 -3.7925441 -3.8035145 -3.8667407 -3.9758496 -4.0999165 -4.1766806 -4.1479564 -4.0686545 -4.0313859 -4.0703373 -4.1316266 -4.1500258 -4.0782728 -3.9370139]]...]
INFO - root - 2017-12-07 06:45:18.750308: step 15010, loss = 0.80, batch loss = 0.73 (10.6 examples/sec; 0.757 sec/batch; 66h:46m:03s remains)
INFO - root - 2017-12-07 06:45:26.504223: step 15020, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.774 sec/batch; 68h:16m:08s remains)
INFO - root - 2017-12-07 06:45:34.026362: step 15030, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.748 sec/batch; 65h:59m:12s remains)
INFO - root - 2017-12-07 06:45:41.658357: step 15040, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.762 sec/batch; 67h:13m:49s remains)
INFO - root - 2017-12-07 06:45:49.389493: step 15050, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.748 sec/batch; 65h:57m:51s remains)
INFO - root - 2017-12-07 06:45:57.057895: step 15060, loss = 0.68, batch loss = 0.61 (10.6 examples/sec; 0.756 sec/batch; 66h:40m:18s remains)
INFO - root - 2017-12-07 06:46:04.667347: step 15070, loss = 0.90, batch loss = 0.83 (10.0 examples/sec; 0.802 sec/batch; 70h:41m:14s remains)
INFO - root - 2017-12-07 06:46:12.077417: step 15080, loss = 0.61, batch loss = 0.54 (10.4 examples/sec; 0.773 sec/batch; 68h:07m:18s remains)
INFO - root - 2017-12-07 06:46:19.795055: step 15090, loss = 0.92, batch loss = 0.85 (10.3 examples/sec; 0.780 sec/batch; 68h:47m:37s remains)
INFO - root - 2017-12-07 06:46:27.463211: step 15100, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.760 sec/batch; 67h:01m:06s remains)
2017-12-07 06:46:28.137635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8428164 -4.0327682 -3.984309 -3.6289921 -3.2412379 -2.5848403 -1.9464195 -1.9765301 -2.4195898 -3.1061125 -3.9562216 -4.4114218 -4.4578347 -4.2946672 -4.0852909][-4.1043625 -4.4681721 -4.4513617 -4.0585532 -3.5378149 -2.4948323 -1.4068737 -1.3051567 -1.9588542 -2.976073 -4.0943475 -4.6260319 -4.7124138 -4.5790973 -4.3066459][-4.3305378 -4.8071823 -4.824317 -4.4806719 -3.8813615 -2.4023509 -0.72716212 -0.26841736 -0.99824905 -2.4074969 -3.8408833 -4.4806571 -4.66957 -4.6850905 -4.477808][-4.48223 -5.0043588 -5.0576086 -4.7960882 -4.1080375 -2.1802444 0.094806671 1.1226649 0.47278595 -1.4340887 -3.37332 -4.2741594 -4.6735358 -4.848784 -4.6973605][-4.5870547 -5.0916524 -5.1528349 -4.9634466 -4.1847138 -1.9928977 0.61952353 2.2393913 1.8970337 -0.36960316 -2.8183508 -4.1651921 -4.9083271 -5.1727796 -4.930892][-4.6599836 -5.0572939 -5.0482821 -4.86352 -4.0765581 -1.9053986 0.74270391 2.8417702 3.0373263 0.89824581 -1.7392197 -3.6131272 -4.9671016 -5.415843 -5.0240388][-4.7026339 -4.9965968 -4.8186936 -4.4526868 -3.6824725 -1.8097131 0.62106848 2.8765554 3.5457587 1.9103155 -0.56091857 -2.887506 -4.8526607 -5.4954834 -4.9885669][-4.8244815 -5.1679015 -4.8142447 -4.098474 -3.221365 -1.6596529 0.38161898 2.3961535 3.3205109 2.3400607 0.22813177 -2.3874514 -4.7301869 -5.4663081 -4.8756361][-4.9925661 -5.511446 -5.0718703 -4.0232735 -2.9771957 -1.6137147 0.0190835 1.4852548 2.3092985 1.8638558 0.16707754 -2.4819942 -4.8579917 -5.5284324 -4.8570271][-5.0772624 -5.763731 -5.365026 -4.1309261 -2.896204 -1.5743906 -0.25987434 0.541389 0.93662882 0.64487076 -0.75575376 -3.189497 -5.2434759 -5.6820726 -4.9402914][-4.9682484 -5.7021294 -5.4944649 -4.2882543 -2.8591223 -1.4444563 -0.3937521 -0.2420516 -0.33827353 -0.72253847 -1.9202995 -3.8736298 -5.3410215 -5.5448837 -4.8365741][-4.7214036 -5.3767834 -5.421442 -4.4374075 -2.9318025 -1.4456568 -0.61288261 -0.87052608 -1.3392575 -1.9546905 -3.0163202 -4.2845826 -5.0550818 -5.1170969 -4.6064653][-4.4786773 -4.9998145 -5.250237 -4.5954185 -3.2244062 -1.840199 -1.0966597 -1.3443737 -1.9876955 -2.9138446 -3.9188547 -4.5327134 -4.701632 -4.7147641 -4.485302][-4.2331843 -4.6114955 -4.9251642 -4.6108937 -3.6068568 -2.4890184 -1.7037227 -1.6527884 -2.3190978 -3.4849143 -4.4295764 -4.5997577 -4.4198046 -4.4323845 -4.4293165][-3.9977627 -4.2536764 -4.5216966 -4.5109839 -3.996774 -3.2440848 -2.4906311 -2.2410643 -2.7941244 -3.8160698 -4.4985552 -4.4471273 -4.1938529 -4.2269492 -4.3086238]]...]
INFO - root - 2017-12-07 06:46:35.778096: step 15110, loss = 0.60, batch loss = 0.53 (10.5 examples/sec; 0.760 sec/batch; 66h:59m:24s remains)
INFO - root - 2017-12-07 06:46:43.426358: step 15120, loss = 0.83, batch loss = 0.76 (10.0 examples/sec; 0.799 sec/batch; 70h:25m:57s remains)
INFO - root - 2017-12-07 06:46:51.049630: step 15130, loss = 0.94, batch loss = 0.87 (10.6 examples/sec; 0.751 sec/batch; 66h:14m:37s remains)
INFO - root - 2017-12-07 06:46:58.830631: step 15140, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.761 sec/batch; 67h:04m:41s remains)
INFO - root - 2017-12-07 06:47:06.515142: step 15150, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.756 sec/batch; 66h:38m:06s remains)
INFO - root - 2017-12-07 06:47:14.195311: step 15160, loss = 0.86, batch loss = 0.78 (10.7 examples/sec; 0.747 sec/batch; 65h:52m:54s remains)
INFO - root - 2017-12-07 06:47:21.839923: step 15170, loss = 0.99, batch loss = 0.92 (10.7 examples/sec; 0.747 sec/batch; 65h:51m:00s remains)
INFO - root - 2017-12-07 06:47:29.371866: step 15180, loss = 0.67, batch loss = 0.59 (10.7 examples/sec; 0.748 sec/batch; 65h:58m:12s remains)
INFO - root - 2017-12-07 06:47:37.056592: step 15190, loss = 0.68, batch loss = 0.60 (10.4 examples/sec; 0.773 sec/batch; 68h:05m:32s remains)
INFO - root - 2017-12-07 06:47:44.922633: step 15200, loss = 0.65, batch loss = 0.57 (10.2 examples/sec; 0.784 sec/batch; 69h:08m:38s remains)
2017-12-07 06:47:45.496759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.156415 -3.0733719 -2.8776042 -2.753896 -2.804975 -3.1292403 -3.6192434 -3.9264185 -4.0248327 -3.9203346 -3.7688107 -3.7312233 -3.689739 -3.5257568 -3.358943][-3.2529535 -3.0986171 -2.9336462 -2.8277516 -2.8386493 -3.1078825 -3.5531988 -3.794682 -3.7300875 -3.4508495 -3.2579246 -3.3942156 -3.5608304 -3.5345953 -3.426055][-3.2561221 -3.0914216 -2.9923244 -2.951067 -2.9591846 -3.192718 -3.5528398 -3.6637194 -3.431525 -3.0301428 -2.8295031 -3.0978868 -3.4194756 -3.5151842 -3.4487247][-3.1493075 -2.9404461 -2.855545 -2.8368783 -2.8528509 -3.0939009 -3.3871379 -3.3854866 -3.1148896 -2.7387357 -2.5749106 -2.9070616 -3.3218336 -3.4844332 -3.4009142][-3.0250778 -2.7023747 -2.5385952 -2.4486532 -2.43636 -2.6907468 -2.9426384 -2.8972363 -2.7100236 -2.5192344 -2.5145741 -2.9286828 -3.3949561 -3.5288985 -3.3467531][-2.8818433 -2.3983703 -2.1319628 -2.0272989 -2.0422447 -2.2853451 -2.4701982 -2.3902864 -2.3285587 -2.4211955 -2.6771231 -3.1885176 -3.621917 -3.6256814 -3.3087859][-2.7568049 -2.1986723 -1.8748024 -1.8192234 -1.8831551 -2.0715737 -2.1495645 -2.0302784 -2.0762331 -2.4467111 -2.980372 -3.5489285 -3.8494325 -3.6973212 -3.2903202][-2.7316732 -2.2728665 -1.999794 -1.9701548 -1.9792225 -2.0067468 -1.9485412 -1.8177478 -1.9695783 -2.5292444 -3.2328329 -3.7689071 -3.9154494 -3.673619 -3.2753806][-3.0589709 -2.8212228 -2.6625733 -2.5659745 -2.3887687 -2.1847005 -1.9859612 -1.9185045 -2.2440073 -2.9326687 -3.6088 -3.9824934 -3.9689486 -3.6679678 -3.310569][-3.6881633 -3.7697172 -3.7623529 -3.5407269 -3.1257129 -2.7481813 -2.5189481 -2.5960298 -3.0869429 -3.7493157 -4.2071404 -4.3320575 -4.1376677 -3.7559807 -3.4000511][-4.0570369 -4.4449162 -4.59348 -4.3148942 -3.8193982 -3.4466851 -3.3094068 -3.4784627 -3.9660535 -4.4415832 -4.6478381 -4.5775471 -4.2585878 -3.8205664 -3.4515457][-3.8073497 -4.2284942 -4.4091983 -4.1840792 -3.8565257 -3.7277844 -3.7764888 -3.9805915 -4.3247004 -4.5868154 -4.628037 -4.4934344 -4.171083 -3.7717772 -3.4396882][-3.2132387 -3.4028275 -3.4333167 -3.2654996 -3.2056704 -3.3743596 -3.6224 -3.8599877 -4.0897536 -4.2114978 -4.2011991 -4.1297174 -3.9253063 -3.6425982 -3.3896365][-2.8708944 -2.7621417 -2.5827823 -2.4517589 -2.6017578 -2.9646883 -3.3118806 -3.5550175 -3.7214539 -3.762053 -3.7405014 -3.7503796 -3.6613955 -3.4883759 -3.3124318][-3.1558976 -2.8989067 -2.6081021 -2.4843664 -2.6673992 -3.0124388 -3.323174 -3.5315607 -3.6348467 -3.6096342 -3.5427694 -3.5321813 -3.4710352 -3.3461204 -3.2187161]]...]
INFO - root - 2017-12-07 06:47:53.165560: step 15210, loss = 0.77, batch loss = 0.70 (10.7 examples/sec; 0.746 sec/batch; 65h:44m:20s remains)
INFO - root - 2017-12-07 06:48:00.750320: step 15220, loss = 0.66, batch loss = 0.59 (10.4 examples/sec; 0.769 sec/batch; 67h:45m:48s remains)
INFO - root - 2017-12-07 06:48:08.382205: step 15230, loss = 0.68, batch loss = 0.61 (10.7 examples/sec; 0.748 sec/batch; 65h:55m:18s remains)
INFO - root - 2017-12-07 06:48:16.038626: step 15240, loss = 0.76, batch loss = 0.68 (10.1 examples/sec; 0.791 sec/batch; 69h:44m:30s remains)
INFO - root - 2017-12-07 06:48:23.695734: step 15250, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 67h:42m:35s remains)
INFO - root - 2017-12-07 06:48:31.408978: step 15260, loss = 0.71, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 67h:32m:34s remains)
INFO - root - 2017-12-07 06:48:39.022683: step 15270, loss = 0.78, batch loss = 0.71 (10.2 examples/sec; 0.785 sec/batch; 69h:08m:00s remains)
INFO - root - 2017-12-07 06:48:46.499127: step 15280, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.777 sec/batch; 68h:26m:44s remains)
INFO - root - 2017-12-07 06:48:54.235675: step 15290, loss = 0.99, batch loss = 0.92 (10.5 examples/sec; 0.765 sec/batch; 67h:23m:01s remains)
INFO - root - 2017-12-07 06:49:01.878467: step 15300, loss = 0.75, batch loss = 0.67 (10.5 examples/sec; 0.758 sec/batch; 66h:49m:31s remains)
2017-12-07 06:49:02.453334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9098654 -2.3912427 -2.8345575 -2.987042 -2.7285209 -2.106425 -1.6688278 -1.6248097 -1.7292111 -1.7057817 -1.5295749 -1.1979489 -0.93527079 -1.4385369 -2.2346218][-2.0406432 -2.734658 -3.0455182 -3.0609651 -2.7819686 -2.2014856 -1.8341706 -1.8593078 -1.8713641 -1.6214802 -1.2402279 -0.85683513 -0.73382926 -1.504915 -2.522234][-1.9471283 -2.6281676 -2.8356309 -2.818285 -2.5921404 -2.118329 -1.8485463 -1.9268978 -1.882678 -1.5220058 -1.0544348 -0.6315589 -0.57878208 -1.5198596 -2.6805968][-1.6247835 -2.1865456 -2.3491311 -2.3626268 -2.1991971 -1.7931662 -1.5676427 -1.6669073 -1.6091278 -1.3057871 -0.95944309 -0.56997943 -0.52798772 -1.5286629 -2.770581][-1.1929443 -1.6608703 -1.8341935 -1.8542614 -1.6847508 -1.2514446 -0.97854471 -1.0103433 -0.97119045 -0.95550752 -0.9719317 -0.71534848 -0.64787579 -1.6138422 -2.8896489][-0.79554844 -1.2296069 -1.4340646 -1.4405513 -1.249625 -0.81997895 -0.481565 -0.33508444 -0.224648 -0.49650216 -0.90686107 -0.80960464 -0.71289992 -1.6473927 -2.9805655][-0.61339378 -1.0314674 -1.2531042 -1.2488873 -1.0893707 -0.76567793 -0.43233967 -0.0736289 0.21934795 -0.14197874 -0.7231946 -0.68389082 -0.57342649 -1.4968333 -2.9327869][-0.57378149 -0.94235587 -1.1281359 -1.1306756 -1.058151 -0.92758083 -0.705914 -0.2351613 0.18555641 -0.086167812 -0.57331038 -0.48852992 -0.3442421 -1.2109916 -2.6943283][-0.63410258 -0.901701 -0.97248936 -0.94854069 -0.96443939 -1.0483713 -0.9702456 -0.45822525 0.0057749748 -0.1688838 -0.48981571 -0.3433485 -0.15839863 -0.88041973 -2.252073][-1.0451329 -1.1548171 -1.0605056 -0.94170427 -0.97078109 -1.1854923 -1.1911979 -0.67645621 -0.24214029 -0.36227798 -0.56960583 -0.38997507 -0.16676712 -0.71823049 -1.8865066][-1.8233092 -1.7488708 -1.5060315 -1.2947094 -1.270925 -1.4732337 -1.4894111 -1.0285742 -0.68961549 -0.81511879 -0.96113944 -0.7897346 -0.57245469 -0.99323988 -1.9184372][-2.7002039 -2.4759498 -2.1436274 -1.9077022 -1.852447 -1.9848194 -1.9856858 -1.6460931 -1.4414222 -1.5954733 -1.7007568 -1.5501282 -1.3734939 -1.6944499 -2.3567612][-3.3782408 -3.1031399 -2.778064 -2.568671 -2.51994 -2.6153467 -2.6645498 -2.4878626 -2.3841481 -2.4877 -2.5092752 -2.3753235 -2.2609262 -2.4767404 -2.8821139][-3.5594888 -3.3278425 -3.0896945 -2.95328 -2.9356513 -3.0080433 -3.1050777 -3.0656042 -3.0128837 -3.0441749 -3.0040793 -2.9029274 -2.8578756 -2.9955335 -3.2178378][-3.1874447 -3.0194397 -2.8722057 -2.8106906 -2.8031554 -2.84054 -2.9252009 -2.9300423 -2.882905 -2.8692656 -2.8394456 -2.8038788 -2.8146431 -2.9186945 -3.060056]]...]
INFO - root - 2017-12-07 06:49:10.048358: step 15310, loss = 0.84, batch loss = 0.76 (10.6 examples/sec; 0.756 sec/batch; 66h:37m:56s remains)
INFO - root - 2017-12-07 06:49:17.662950: step 15320, loss = 1.13, batch loss = 1.05 (10.7 examples/sec; 0.747 sec/batch; 65h:51m:01s remains)
INFO - root - 2017-12-07 06:49:25.291240: step 15330, loss = 0.59, batch loss = 0.52 (10.5 examples/sec; 0.760 sec/batch; 66h:59m:33s remains)
INFO - root - 2017-12-07 06:49:32.875474: step 15340, loss = 0.70, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 68h:12m:21s remains)
INFO - root - 2017-12-07 06:49:40.493352: step 15350, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.766 sec/batch; 67h:29m:10s remains)
INFO - root - 2017-12-07 06:49:48.116212: step 15360, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.776 sec/batch; 68h:20m:34s remains)
INFO - root - 2017-12-07 06:49:55.743923: step 15370, loss = 0.89, batch loss = 0.81 (10.4 examples/sec; 0.766 sec/batch; 67h:30m:05s remains)
INFO - root - 2017-12-07 06:50:03.200975: step 15380, loss = 0.74, batch loss = 0.67 (10.0 examples/sec; 0.802 sec/batch; 70h:38m:04s remains)
INFO - root - 2017-12-07 06:50:10.920100: step 15390, loss = 0.87, batch loss = 0.79 (10.0 examples/sec; 0.803 sec/batch; 70h:43m:49s remains)
INFO - root - 2017-12-07 06:50:18.567504: step 15400, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.756 sec/batch; 66h:34m:17s remains)
2017-12-07 06:50:19.196461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4173298 -3.4538574 -3.4692078 -3.4426684 -3.3814478 -3.2892337 -3.1733673 -3.1009769 -3.1412821 -3.27108 -3.3990655 -3.4494205 -3.4031816 -3.2480755 -2.9857802][-3.3859148 -3.4245861 -3.4667983 -3.47409 -3.4184361 -3.2723069 -3.0242319 -2.7909966 -2.7641835 -2.9672227 -3.1922812 -3.2663562 -3.1515307 -2.9170537 -2.5902812][-3.3350339 -3.3457832 -3.3759298 -3.3822327 -3.3012004 -3.0878453 -2.6973062 -2.3070745 -2.2746074 -2.6236973 -2.9813557 -3.0768151 -2.85081 -2.464607 -2.0261774][-3.2911816 -3.2296023 -3.1674643 -3.0840468 -2.9198627 -2.6095185 -2.067637 -1.564245 -1.6220486 -2.175616 -2.6995745 -2.8621175 -2.5714688 -2.0328145 -1.4442751][-3.3558888 -3.2318606 -3.0537977 -2.8556256 -2.5996547 -2.231698 -1.604727 -1.0651841 -1.2876012 -2.0955572 -2.825141 -3.1145184 -2.8445446 -2.2343714 -1.5920351][-3.4282551 -3.1959672 -2.8240747 -2.4104631 -1.9835844 -1.5365567 -0.84337306 -0.31157827 -0.74453449 -1.8389466 -2.8222198 -3.3012586 -3.1750531 -2.667037 -2.0925596][-3.5132403 -3.1816268 -2.6011977 -1.9412527 -1.2823088 -0.67389917 0.09887886 0.60919094 0.044130325 -1.1447361 -2.1770589 -2.7593837 -2.8530517 -2.6414647 -2.3078732][-3.6663318 -3.4163523 -2.8310084 -2.1074338 -1.3706052 -0.71893716 -0.013337135 0.40997839 -0.061439037 -1.0482965 -1.9152763 -2.4190614 -2.534338 -2.4721465 -2.3795187][-3.6943455 -3.5545049 -3.0624681 -2.4592037 -1.8737302 -1.3037312 -0.64375973 -0.25557232 -0.58526039 -1.3906922 -2.2338498 -2.84946 -3.0173547 -2.8926644 -2.6278212][-3.6069412 -3.4720058 -3.0125313 -2.5282984 -2.1459963 -1.6946833 -1.1026087 -0.75213552 -0.991451 -1.5513246 -2.3033412 -3.1187892 -3.5294929 -3.439101 -2.9319329][-3.6291695 -3.4962208 -3.0534637 -2.6104784 -2.327246 -2.0372052 -1.7013304 -1.6408136 -2.0097234 -2.3423491 -2.6908078 -3.2514062 -3.591697 -3.4461038 -2.8207235][-3.6586533 -3.5502214 -3.1771142 -2.7610946 -2.412936 -2.0864029 -1.8651073 -2.1065583 -2.8165379 -3.2805262 -3.4030061 -3.5587068 -3.5268619 -3.1133451 -2.3567731][-3.6245294 -3.5036469 -3.1599894 -2.7260675 -2.2676113 -1.8518577 -1.6268508 -1.9769363 -2.8755674 -3.5586784 -3.6949482 -3.5650744 -3.137404 -2.4132607 -1.4975193][-3.4315374 -3.2520852 -2.8755844 -2.378742 -1.8226128 -1.3781857 -1.2858903 -1.8028646 -2.7666774 -3.538449 -3.6716647 -3.3235979 -2.5855799 -1.6614523 -0.63781023][-3.1410356 -2.890413 -2.4590731 -1.9183581 -1.3106816 -0.86840391 -0.98506045 -1.7530644 -2.8085411 -3.6418178 -3.7684336 -3.2899568 -2.4543085 -1.5890472 -0.75501251]]...]
INFO - root - 2017-12-07 06:50:26.859370: step 15410, loss = 0.70, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 67h:20m:08s remains)
INFO - root - 2017-12-07 06:50:34.559386: step 15420, loss = 0.82, batch loss = 0.74 (10.3 examples/sec; 0.777 sec/batch; 68h:28m:44s remains)
INFO - root - 2017-12-07 06:50:42.264419: step 15430, loss = 1.00, batch loss = 0.93 (10.7 examples/sec; 0.748 sec/batch; 65h:53m:15s remains)
INFO - root - 2017-12-07 06:50:49.902598: step 15440, loss = 0.72, batch loss = 0.65 (10.7 examples/sec; 0.745 sec/batch; 65h:37m:25s remains)
INFO - root - 2017-12-07 06:50:57.557566: step 15450, loss = 0.73, batch loss = 0.66 (10.8 examples/sec; 0.741 sec/batch; 65h:17m:20s remains)
INFO - root - 2017-12-07 06:51:05.187084: step 15460, loss = 0.98, batch loss = 0.91 (10.3 examples/sec; 0.775 sec/batch; 68h:17m:13s remains)
INFO - root - 2017-12-07 06:51:13.092690: step 15470, loss = 0.81, batch loss = 0.74 (9.8 examples/sec; 0.815 sec/batch; 71h:43m:52s remains)
INFO - root - 2017-12-07 06:51:20.683487: step 15480, loss = 0.96, batch loss = 0.89 (10.3 examples/sec; 0.777 sec/batch; 68h:26m:41s remains)
INFO - root - 2017-12-07 06:51:28.374002: step 15490, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.760 sec/batch; 66h:56m:07s remains)
INFO - root - 2017-12-07 06:51:35.996802: step 15500, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.770 sec/batch; 67h:49m:30s remains)
2017-12-07 06:51:36.567877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.326699 -1.856452 -1.3225782 -0.79030252 -0.50047684 -0.72737074 -1.088155 -1.5309076 -1.9525571 -2.0745957 -2.0682955 -1.9612172 -1.8787549 -1.9553747 -2.1336377][-1.3872356 -0.94708514 -0.53228283 -0.20816517 -0.040463448 -0.36229229 -0.80824018 -1.3268347 -1.790658 -1.8733308 -1.8105569 -1.6795759 -1.5921674 -1.6995726 -1.9448147][-0.64130497 -0.3997097 -0.33193874 -0.40888691 -0.49264288 -0.89050031 -1.3564372 -1.8662336 -2.2314632 -2.1423631 -1.9215193 -1.7094357 -1.58759 -1.6667407 -1.8651004][-0.42130947 -0.46180367 -0.77581859 -1.2332003 -1.5154769 -1.824178 -2.1183109 -2.4576249 -2.622797 -2.3580842 -1.9803152 -1.6710231 -1.5232029 -1.5671523 -1.7005913][-1.0166891 -1.1631157 -1.5187209 -1.9673986 -2.1510863 -2.1470716 -2.1090226 -2.2065227 -2.2243931 -1.9780951 -1.6669123 -1.3975806 -1.2413063 -1.2629666 -1.4214075][-1.7572074 -1.7961974 -1.8796914 -2.0136378 -1.9276774 -1.5916619 -1.2283087 -1.0588648 -0.94030881 -0.79761004 -0.78330207 -0.84613609 -0.89594269 -1.034024 -1.2949822][-2.0332756 -1.8587093 -1.5762951 -1.3358395 -1.0364604 -0.53699565 -0.020037651 0.22595024 0.27774572 0.21877909 -0.079269886 -0.49472189 -0.79608727 -1.0813127 -1.4176028][-1.9702003 -1.6169949 -1.0926309 -0.64632225 -0.33277607 0.054409027 0.51556158 0.64689207 0.4681406 0.19006062 -0.25040531 -0.7681551 -1.2037163 -1.5796688 -1.8961942][-2.1035776 -1.6361079 -1.0491016 -0.62267494 -0.50360274 -0.41728544 -0.18357229 -0.10025597 -0.25403261 -0.58480287 -1.0253186 -1.5080633 -1.9783385 -2.361594 -2.602931][-2.4214957 -1.9062369 -1.3478689 -0.997406 -0.99777794 -1.0517516 -0.95487905 -0.8775456 -0.96050906 -1.2913921 -1.7277756 -2.1801665 -2.6075053 -2.9090247 -3.0436893][-2.4755101 -1.9341855 -1.4373693 -1.1808467 -1.2194796 -1.3032207 -1.3197579 -1.3178711 -1.4066982 -1.6942937 -2.0576029 -2.4549832 -2.8285489 -3.0488377 -3.1377411][-2.0810289 -1.4938321 -1.007406 -0.807044 -0.92156172 -1.1116471 -1.3086009 -1.4776208 -1.6580343 -1.8831489 -2.0807381 -2.3306155 -2.6277525 -2.8542638 -3.0197308][-1.4722321 -0.86689472 -0.3820467 -0.23120451 -0.454571 -0.79347086 -1.1587191 -1.4831758 -1.7415593 -1.9255812 -1.9503801 -1.9872489 -2.1548729 -2.39695 -2.6665092][-1.1199162 -0.61678195 -0.26410055 -0.22063684 -0.51027632 -0.88641882 -1.2046378 -1.4293649 -1.5509531 -1.5856633 -1.4431558 -1.3082647 -1.3610947 -1.5978997 -1.9241564][-1.369067 -1.0349753 -0.82996511 -0.84757757 -1.0907121 -1.3614531 -1.5188015 -1.5153282 -1.3724756 -1.1786139 -0.88263941 -0.6541996 -0.68254876 -0.94253874 -1.3090339]]...]
INFO - root - 2017-12-07 06:51:44.281251: step 15510, loss = 0.66, batch loss = 0.58 (10.2 examples/sec; 0.785 sec/batch; 69h:05m:13s remains)
INFO - root - 2017-12-07 06:51:51.979632: step 15520, loss = 0.74, batch loss = 0.67 (10.7 examples/sec; 0.746 sec/batch; 65h:42m:21s remains)
INFO - root - 2017-12-07 06:51:59.653636: step 15530, loss = 0.97, batch loss = 0.89 (10.7 examples/sec; 0.745 sec/batch; 65h:34m:33s remains)
INFO - root - 2017-12-07 06:52:07.325965: step 15540, loss = 0.76, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 71h:04m:38s remains)
INFO - root - 2017-12-07 06:52:15.091140: step 15550, loss = 0.76, batch loss = 0.69 (10.2 examples/sec; 0.788 sec/batch; 69h:21m:01s remains)
INFO - root - 2017-12-07 06:52:22.751287: step 15560, loss = 0.70, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 68h:26m:52s remains)
INFO - root - 2017-12-07 06:52:30.499450: step 15570, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.769 sec/batch; 67h:44m:26s remains)
INFO - root - 2017-12-07 06:52:37.993330: step 15580, loss = 0.61, batch loss = 0.54 (10.4 examples/sec; 0.766 sec/batch; 67h:24m:39s remains)
INFO - root - 2017-12-07 06:52:45.512033: step 15590, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.760 sec/batch; 66h:55m:52s remains)
INFO - root - 2017-12-07 06:52:53.203075: step 15600, loss = 0.72, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 69h:17m:56s remains)
2017-12-07 06:52:53.786045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.2449255 -0.45945621 -0.73421025 -1.1122952 -1.433244 -1.8320041 -2.2459943 -2.3280218 -2.1424978 -1.8069849 -1.2853537 -0.87614894 -0.823436 -0.86929893 -1.0766618][-0.019897461 -0.16130352 -0.39235067 -0.83203316 -1.3107326 -1.9050124 -2.3951921 -2.3571732 -1.9103112 -1.4100177 -0.94031906 -0.68635082 -0.7796433 -0.96346807 -1.2464907][-0.12060738 -0.23864937 -0.46516347 -0.90625763 -1.4583576 -2.0841248 -2.4442682 -2.1501148 -1.4097297 -0.81041288 -0.52672791 -0.52117872 -0.79511762 -1.1281481 -1.4876916][-0.65871453 -0.75815535 -0.90504026 -1.1908803 -1.6173441 -2.0073149 -2.0493324 -1.5301039 -0.73083019 -0.33639812 -0.3980217 -0.59714365 -0.89259052 -1.1753235 -1.5089142][-1.287353 -1.4283597 -1.403482 -1.4000545 -1.5142312 -1.4826288 -1.153372 -0.517745 0.088788509 0.035520554 -0.45835471 -0.80496073 -1.0051141 -1.0648072 -1.2268395][-1.533962 -1.78638 -1.6484535 -1.378305 -1.1242776 -0.67435908 -0.11927509 0.4246788 0.70002937 0.25296164 -0.46683192 -0.90188789 -1.0661666 -0.92403769 -0.91881037][-1.4630246 -1.6322391 -1.3544383 -0.93905306 -0.44204903 0.22803879 0.79045391 1.0087438 0.82909012 0.0730257 -0.75762415 -1.2111115 -1.221734 -0.79056382 -0.63322973][-1.5060754 -1.2953377 -0.76061463 -0.27319288 0.20385933 0.74266577 1.1180134 1.1196475 0.74882126 -0.034728527 -0.86597133 -1.3107178 -1.1640282 -0.51667047 -0.29104328][-1.4347634 -0.72687483 0.050572872 0.45565271 0.60773993 0.72106838 0.8894 1.0205255 0.91058159 0.41604376 -0.3380537 -0.87316871 -0.79143238 -0.23010111 -0.099892616][-1.2175281 -0.28251219 0.51571941 0.743793 0.52715826 0.17411661 0.075639725 0.31484556 0.54861593 0.49964142 0.038132191 -0.43085694 -0.38431358 0.041604996 0.098364353][-1.1359704 -0.36557961 0.261734 0.34625292 -0.042987347 -0.64542937 -1.0429881 -0.90839338 -0.52341366 -0.16119623 -0.12673426 -0.27802563 -0.071429729 0.39109755 0.52416086][-1.1849618 -0.80672383 -0.4074235 -0.38167477 -0.71523762 -1.2371819 -1.6971931 -1.6623096 -1.2848518 -0.771461 -0.45674062 -0.38647366 -0.12601089 0.33873367 0.5642705][-1.2481468 -1.2495146 -1.0857723 -1.1069536 -1.3112116 -1.5948513 -1.9228537 -1.910146 -1.5958445 -1.1715369 -0.83429074 -0.69785666 -0.48985624 -0.13586092 0.08742857][-1.3371773 -1.5234981 -1.4596858 -1.4564445 -1.5614305 -1.6776447 -1.9318907 -2.0074 -1.7959259 -1.4433594 -1.0776057 -0.91213655 -0.83970094 -0.70963049 -0.63409305][-1.6563201 -1.8274007 -1.7771716 -1.7003226 -1.6808302 -1.6214969 -1.7752874 -1.9156384 -1.8057022 -1.5135329 -1.215925 -1.1309466 -1.2154348 -1.3030496 -1.402494]]...]
INFO - root - 2017-12-07 06:53:01.555585: step 15610, loss = 1.04, batch loss = 0.97 (10.4 examples/sec; 0.766 sec/batch; 67h:26m:43s remains)
INFO - root - 2017-12-07 06:53:09.158555: step 15620, loss = 0.96, batch loss = 0.89 (10.7 examples/sec; 0.747 sec/batch; 65h:45m:59s remains)
INFO - root - 2017-12-07 06:53:16.734334: step 15630, loss = 1.04, batch loss = 0.96 (10.8 examples/sec; 0.742 sec/batch; 65h:17m:53s remains)
INFO - root - 2017-12-07 06:53:24.358532: step 15640, loss = 0.93, batch loss = 0.85 (10.6 examples/sec; 0.755 sec/batch; 66h:24m:47s remains)
INFO - root - 2017-12-07 06:53:31.950576: step 15650, loss = 0.92, batch loss = 0.85 (10.5 examples/sec; 0.761 sec/batch; 66h:58m:18s remains)
INFO - root - 2017-12-07 06:53:39.636573: step 15660, loss = 0.90, batch loss = 0.83 (10.8 examples/sec; 0.742 sec/batch; 65h:15m:49s remains)
INFO - root - 2017-12-07 06:53:47.347872: step 15670, loss = 0.70, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 69h:58m:54s remains)
INFO - root - 2017-12-07 06:53:54.772460: step 15680, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.759 sec/batch; 66h:49m:50s remains)
INFO - root - 2017-12-07 06:54:02.418413: step 15690, loss = 0.87, batch loss = 0.80 (10.8 examples/sec; 0.743 sec/batch; 65h:21m:39s remains)
INFO - root - 2017-12-07 06:54:10.227442: step 15700, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.765 sec/batch; 67h:21m:46s remains)
2017-12-07 06:54:10.906392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7721295 -3.8417392 -3.9139442 -3.9470429 -3.987927 -4.0565534 -4.1218657 -4.1298752 -4.1088414 -4.0729189 -4.0644484 -4.0537992 -4.0068951 -3.9376867 -3.8617332][-3.3935516 -3.3951442 -3.469023 -3.5496795 -3.6781249 -3.8657293 -4.0241146 -4.061728 -4.0390153 -3.9683015 -3.9519472 -3.9670262 -3.9543631 -3.9227631 -3.8788023][-2.9028978 -2.7549598 -2.765584 -2.8329601 -2.9945078 -3.2743099 -3.4930971 -3.5824566 -3.5983033 -3.4934297 -3.43174 -3.4569769 -3.47673 -3.5038185 -3.5626438][-2.64858 -2.397002 -2.3551743 -2.3800852 -2.5316544 -2.8113647 -2.9700232 -3.0395157 -3.0856678 -2.9164319 -2.7712257 -2.7854934 -2.8159251 -2.8877227 -3.0454626][-2.6462827 -2.3599267 -2.2649531 -2.2106273 -2.2668943 -2.4255619 -2.4017599 -2.4477236 -2.64952 -2.5922217 -2.4902043 -2.5405316 -2.549335 -2.5570328 -2.6587687][-2.8311577 -2.5895014 -2.4905949 -2.3396659 -2.1955783 -2.0253046 -1.6451533 -1.6280053 -2.1297479 -2.4295661 -2.5728302 -2.782516 -2.8172903 -2.69624 -2.5656707][-3.1249876 -2.9113421 -2.8084536 -2.5678525 -2.189944 -1.5846446 -0.74517679 -0.630466 -1.4224157 -2.0964086 -2.5730619 -3.0414357 -3.178781 -2.965929 -2.5925565][-3.3275476 -3.061111 -2.9233377 -2.66339 -2.1956551 -1.3262043 -0.19526148 -0.0018968582 -0.8307786 -1.6084726 -2.2728162 -2.9853697 -3.2742243 -3.0899978 -2.6177921][-3.3717341 -3.0637019 -2.9427385 -2.7730885 -2.449194 -1.7525115 -0.847687 -0.67242146 -1.1916058 -1.6490638 -2.2025893 -2.9294925 -3.2569957 -3.1072078 -2.638176][-3.328536 -3.0706837 -3.0245557 -3.013144 -2.9553607 -2.6645341 -2.2219515 -2.1884966 -2.4111943 -2.4885969 -2.7829046 -3.2964654 -3.4769049 -3.2740788 -2.8177552][-3.5667229 -3.355768 -3.295815 -3.3829114 -3.5672336 -3.6212482 -3.5538082 -3.6260698 -3.6888266 -3.5895886 -3.7349193 -3.9970775 -3.919764 -3.5782912 -3.1256788][-4.1232405 -3.921737 -3.7781706 -3.8795943 -4.1664867 -4.3559251 -4.4324961 -4.5514903 -4.59904 -4.5226631 -4.6080918 -4.6329923 -4.3242984 -3.8538857 -3.4117451][-4.524797 -4.3496857 -4.1636829 -4.2515469 -4.5003972 -4.6562195 -4.72008 -4.8248615 -4.9270644 -4.9358644 -4.9815884 -4.8382964 -4.4320483 -3.9548383 -3.5813189][-4.6100192 -4.4894667 -4.3303351 -4.3706789 -4.5150366 -4.6003156 -4.6228523 -4.69143 -4.79659 -4.8219481 -4.7663984 -4.5109248 -4.1276069 -3.7820587 -3.5899878][-4.378911 -4.3387923 -4.2637239 -4.2712064 -4.3254681 -4.3721504 -4.4071875 -4.4761415 -4.5557251 -4.5534849 -4.4389796 -4.1936579 -3.9198952 -3.7314901 -3.665463]]...]
INFO - root - 2017-12-07 06:54:18.571682: step 15710, loss = 0.77, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 67h:06m:26s remains)
INFO - root - 2017-12-07 06:54:26.265748: step 15720, loss = 1.04, batch loss = 0.96 (10.4 examples/sec; 0.766 sec/batch; 67h:24m:14s remains)
INFO - root - 2017-12-07 06:54:33.983048: step 15730, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.761 sec/batch; 66h:59m:44s remains)
INFO - root - 2017-12-07 06:54:41.720701: step 15740, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.783 sec/batch; 68h:52m:58s remains)
INFO - root - 2017-12-07 06:54:49.551969: step 15750, loss = 0.78, batch loss = 0.71 (10.0 examples/sec; 0.801 sec/batch; 70h:26m:28s remains)
INFO - root - 2017-12-07 06:54:57.265415: step 15760, loss = 0.81, batch loss = 0.74 (9.9 examples/sec; 0.811 sec/batch; 71h:18m:40s remains)
INFO - root - 2017-12-07 06:55:04.907767: step 15770, loss = 0.71, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 68h:48m:16s remains)
INFO - root - 2017-12-07 06:55:12.374097: step 15780, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.770 sec/batch; 67h:43m:12s remains)
INFO - root - 2017-12-07 06:55:20.043946: step 15790, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.751 sec/batch; 66h:05m:31s remains)
INFO - root - 2017-12-07 06:55:27.660117: step 15800, loss = 1.12, batch loss = 1.05 (10.5 examples/sec; 0.761 sec/batch; 66h:57m:42s remains)
2017-12-07 06:55:28.282526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0983582 -1.8991342 -1.9000819 -1.9770043 -2.00254 -2.0866096 -2.3762274 -2.7813406 -3.0772684 -2.9674721 -2.6982334 -2.7132666 -2.7303348 -2.6429682 -2.6956997][-2.024111 -1.7818 -1.8023188 -1.9055192 -1.9389207 -1.9717457 -2.260215 -2.7629452 -3.1045814 -3.0100083 -2.7684131 -2.7900648 -2.8246427 -2.8060293 -2.8384933][-1.7278347 -1.5118346 -1.5861232 -1.703382 -1.8035197 -1.9197586 -2.2266114 -2.6885409 -2.98238 -2.9210529 -2.7995358 -2.8266425 -2.8529217 -2.9148579 -2.9847026][-1.6828516 -1.5008368 -1.5812511 -1.7090421 -1.9143236 -2.1326077 -2.3482974 -2.5603075 -2.7016759 -2.7689438 -2.8645163 -2.9195728 -2.920258 -3.0317106 -3.1552386][-2.0014429 -1.9170623 -2.0136306 -2.1464698 -2.3806987 -2.5380559 -2.5066171 -2.397054 -2.4433157 -2.7754145 -3.1403432 -3.2542021 -3.2007577 -3.2239311 -3.2907486][-2.2116292 -2.220803 -2.3916152 -2.5683866 -2.777374 -2.7966871 -2.5410886 -2.2790096 -2.4128094 -3.014111 -3.5343704 -3.6673179 -3.5482481 -3.4188244 -3.357662][-1.985672 -1.9240742 -2.1761446 -2.42499 -2.6074317 -2.5688095 -2.2898455 -2.1434875 -2.5068593 -3.2218983 -3.7222738 -3.8572404 -3.7446043 -3.5422506 -3.3739371][-1.4880984 -1.2246375 -1.4170916 -1.6526113 -1.8519258 -1.9615924 -1.9512913 -2.1200395 -2.6373649 -3.2374678 -3.629427 -3.7959831 -3.7281618 -3.5113544 -3.3142712][-1.089678 -0.64747286 -0.73901963 -0.92967248 -1.1549988 -1.4325228 -1.6891961 -2.0476527 -2.4734483 -2.8408647 -3.1851015 -3.4732289 -3.5198288 -3.3887079 -3.2557194][-1.0042984 -0.53838444 -0.59596395 -0.75694013 -0.92843747 -1.1512167 -1.3771675 -1.6457093 -1.8860793 -2.0968313 -2.4634347 -2.8697712 -3.0179899 -2.9690225 -2.9153726][-1.1897609 -0.92301917 -1.0912507 -1.3360589 -1.4793458 -1.529202 -1.5128665 -1.5104346 -1.5394325 -1.6374192 -1.9176326 -2.2331307 -2.3365052 -2.2977884 -2.2775576][-1.6351013 -1.7254653 -2.0728047 -2.4176497 -2.5479875 -2.4965312 -2.370472 -2.2583978 -2.1650951 -2.0958622 -2.1136413 -2.1671119 -2.0992131 -1.9574125 -1.8473191][-2.3284924 -2.6489158 -3.0134392 -3.3081439 -3.3868692 -3.31036 -3.225939 -3.2002954 -3.1528125 -3.0389843 -2.9012475 -2.7854013 -2.6011109 -2.3386629 -2.0800245][-2.8661966 -3.1416531 -3.3818204 -3.58704 -3.6569939 -3.6242256 -3.633203 -3.7241907 -3.7651923 -3.7035289 -3.5449131 -3.3590446 -3.1257377 -2.8307583 -2.5578651][-3.1290777 -3.2307434 -3.3051808 -3.4146581 -3.4936826 -3.5156002 -3.5855179 -3.713707 -3.7695754 -3.7159185 -3.5616028 -3.3821969 -3.2005439 -2.9736238 -2.7974157]]...]
INFO - root - 2017-12-07 06:55:36.058067: step 15810, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.768 sec/batch; 67h:31m:36s remains)
INFO - root - 2017-12-07 06:55:43.763053: step 15820, loss = 0.85, batch loss = 0.77 (10.9 examples/sec; 0.735 sec/batch; 64h:41m:07s remains)
INFO - root - 2017-12-07 06:55:51.382483: step 15830, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.774 sec/batch; 68h:06m:06s remains)
INFO - root - 2017-12-07 06:55:59.047039: step 15840, loss = 0.81, batch loss = 0.73 (10.2 examples/sec; 0.782 sec/batch; 68h:48m:32s remains)
INFO - root - 2017-12-07 06:56:06.584291: step 15850, loss = 0.64, batch loss = 0.57 (10.3 examples/sec; 0.776 sec/batch; 68h:13m:09s remains)
INFO - root - 2017-12-07 06:56:14.167192: step 15860, loss = 0.67, batch loss = 0.60 (10.7 examples/sec; 0.750 sec/batch; 66h:00m:20s remains)
INFO - root - 2017-12-07 06:56:21.728394: step 15870, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.759 sec/batch; 66h:44m:51s remains)
INFO - root - 2017-12-07 06:56:29.152810: step 15880, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 66h:17m:06s remains)
INFO - root - 2017-12-07 06:56:36.849962: step 15890, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.766 sec/batch; 67h:20m:39s remains)
INFO - root - 2017-12-07 06:56:44.490594: step 15900, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.767 sec/batch; 67h:26m:41s remains)
2017-12-07 06:56:45.073885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.21784 -2.079592 -1.9572608 -1.8942864 -1.9048622 -1.9345298 -1.974755 -2.0090199 -1.981472 -1.9299426 -1.9130013 -1.8895164 -1.8532376 -1.8427281 -1.9271536][-2.2252526 -2.091069 -1.9510136 -1.883337 -1.9005368 -1.9352179 -2.0145607 -2.120369 -2.1161797 -2.0572162 -2.0208044 -2.0084071 -1.9806144 -1.9570329 -2.0142148][-2.0728157 -1.9238839 -1.7947099 -1.7569852 -1.7825098 -1.8153503 -1.9426425 -2.1250522 -2.166642 -2.1326208 -2.1170471 -2.1817391 -2.2316797 -2.2342389 -2.2019901][-1.6509111 -1.4818335 -1.4188874 -1.4516428 -1.5051844 -1.5588031 -1.7361648 -1.9503236 -2.0383747 -2.0467093 -2.0646966 -2.2120345 -2.3770881 -2.4523354 -2.3329909][-1.2060413 -1.098417 -1.1574731 -1.2508662 -1.2733281 -1.2881844 -1.4329073 -1.6004546 -1.7145083 -1.7703836 -1.8225646 -2.0301523 -2.3175123 -2.4806645 -2.3349724][-0.90879035 -0.96037459 -1.1569133 -1.2734871 -1.1724422 -1.0127316 -0.95911 -0.96583033 -1.1108589 -1.2805417 -1.432627 -1.7267809 -2.1404386 -2.4047036 -2.3217044][-0.75330377 -1.0143201 -1.3824382 -1.5312042 -1.2956278 -0.86699867 -0.48867154 -0.264812 -0.4750607 -0.87242651 -1.1942327 -1.5405133 -1.9573395 -2.2495229 -2.2704217][-0.60788846 -1.0404482 -1.5602272 -1.81324 -1.5578282 -0.96308827 -0.31539059 0.16973734 -0.050272942 -0.65049767 -1.0830593 -1.3527641 -1.5934997 -1.8320861 -1.9919016][-0.4194932 -0.94063735 -1.5057995 -1.801055 -1.605618 -1.1143372 -0.56017852 -0.084863663 -0.23988199 -0.79002738 -1.1080678 -1.1852059 -1.2165556 -1.3687835 -1.6312971][-0.45168996 -0.95010185 -1.420218 -1.6759398 -1.5427599 -1.2884915 -1.064934 -0.81855917 -0.93204808 -1.235316 -1.2618752 -1.1131067 -1.0101912 -1.1137278 -1.3744144][-0.819185 -1.2031379 -1.513613 -1.7047954 -1.6186652 -1.5759969 -1.6223662 -1.5334053 -1.5664065 -1.6542609 -1.5475779 -1.3759818 -1.2741916 -1.3178856 -1.4181983][-1.2866991 -1.448565 -1.6145213 -1.8059437 -1.8027716 -1.8714902 -2.0319057 -1.9918861 -1.9169354 -1.8814273 -1.8525565 -1.8903425 -1.8910425 -1.845597 -1.6969743][-1.6009874 -1.5111616 -1.566498 -1.7828484 -1.8760741 -1.99193 -2.1428914 -2.0967751 -1.9969745 -1.9750829 -2.0998747 -2.3342395 -2.4122152 -2.2788582 -1.9477236][-1.8335755 -1.5935943 -1.5890579 -1.7891471 -1.9216306 -2.0407836 -2.1368845 -2.0991189 -2.0548952 -2.0946667 -2.289345 -2.5482693 -2.5872362 -2.3895679 -2.0248208][-2.1169505 -1.898212 -1.8973932 -2.04735 -2.1561189 -2.2397664 -2.2607932 -2.2079382 -2.2032576 -2.2948122 -2.4795613 -2.6498041 -2.617012 -2.3907707 -2.0695941]]...]
INFO - root - 2017-12-07 06:56:52.664688: step 15910, loss = 0.94, batch loss = 0.87 (10.1 examples/sec; 0.792 sec/batch; 69h:37m:48s remains)
INFO - root - 2017-12-07 06:57:00.294767: step 15920, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.752 sec/batch; 66h:05m:55s remains)
INFO - root - 2017-12-07 06:57:07.948785: step 15930, loss = 0.74, batch loss = 0.67 (10.8 examples/sec; 0.741 sec/batch; 65h:08m:03s remains)
INFO - root - 2017-12-07 06:57:15.623992: step 15940, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.778 sec/batch; 68h:27m:09s remains)
INFO - root - 2017-12-07 06:57:23.272231: step 15950, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.752 sec/batch; 66h:07m:17s remains)
INFO - root - 2017-12-07 06:57:30.902374: step 15960, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.759 sec/batch; 66h:46m:18s remains)
INFO - root - 2017-12-07 06:57:38.602721: step 15970, loss = 0.89, batch loss = 0.81 (10.7 examples/sec; 0.747 sec/batch; 65h:42m:02s remains)
INFO - root - 2017-12-07 06:57:45.978827: step 15980, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.754 sec/batch; 66h:19m:42s remains)
INFO - root - 2017-12-07 06:57:53.603881: step 15990, loss = 0.75, batch loss = 0.68 (10.8 examples/sec; 0.742 sec/batch; 65h:15m:46s remains)
INFO - root - 2017-12-07 06:58:01.121240: step 16000, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.756 sec/batch; 66h:30m:16s remains)
2017-12-07 06:58:01.744245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3061171 -3.6314769 -3.1469793 -3.2434797 -3.6102121 -3.6363444 -3.2212253 -2.789222 -2.6590784 -2.8297081 -3.1180048 -3.4599974 -3.4013557 -2.81987 -2.1715267][-4.1372261 -3.5406048 -3.0813441 -3.2858253 -3.802213 -3.8925455 -3.5193944 -3.0451155 -2.6842165 -2.7616906 -3.161509 -3.4446998 -3.1871808 -2.6128016 -2.1646678][-3.63643 -3.0832248 -2.7153788 -3.0715959 -3.6578519 -3.719156 -3.3172598 -2.850841 -2.5919266 -2.865725 -3.308785 -3.283155 -2.674674 -2.0556321 -1.8007438][-3.2858186 -2.5288215 -2.1121695 -2.5849781 -3.2033186 -3.2413716 -2.8164027 -2.3547986 -2.2166834 -2.575222 -2.9495063 -2.6892214 -1.8729784 -1.1666586 -0.94298172][-3.1212895 -2.0299037 -1.5006084 -2.0708492 -2.7339265 -2.8079133 -2.4262853 -1.9470267 -1.7591941 -2.0645578 -2.43404 -2.2280622 -1.5178154 -0.8361702 -0.56791282][-2.9556236 -1.6918252 -1.0810499 -1.6028402 -2.1394079 -2.193476 -1.843735 -1.2996547 -1.0460677 -1.4823458 -2.0944471 -2.142673 -1.6292248 -0.9530251 -0.59791493][-2.7004223 -1.5648758 -1.0685682 -1.4814744 -1.7282321 -1.6092863 -1.1445961 -0.32294846 0.10746527 -0.55906844 -1.5269446 -1.8409326 -1.4893768 -0.84245634 -0.46387959][-2.3493392 -1.3948491 -1.1732569 -1.5929775 -1.598825 -1.3330016 -0.727438 0.43495512 1.0627189 0.29023695 -0.80348206 -1.2414246 -0.98991585 -0.42016554 -0.15579557][-1.9541898 -0.88982248 -0.69041228 -1.062053 -1.0412858 -0.93110514 -0.56173944 0.48947382 1.0068722 0.32997942 -0.49314547 -0.80215263 -0.52303624 -0.055844784 0.040510654][-1.8005536 -0.49821043 -0.020267487 -0.18885946 -0.26092434 -0.49228978 -0.53641272 0.12486887 0.42823124 0.020685196 -0.443506 -0.66720486 -0.42548656 -0.044557571 -0.035914421][-2.04924 -0.8228879 -0.16105175 -0.10665655 -0.19148159 -0.54414415 -0.73790145 -0.38249063 -0.21965265 -0.35754919 -0.60676384 -0.89647436 -0.77810812 -0.4183116 -0.37308121][-2.1417975 -1.2753952 -0.670491 -0.45701408 -0.41946745 -0.6801827 -0.897959 -0.795383 -0.771729 -0.80795145 -1.0248704 -1.3890486 -1.36041 -1.0420725 -1.0039821][-1.94561 -1.3919971 -0.90880775 -0.60234571 -0.42077589 -0.62577152 -0.95665 -1.1409805 -1.2835951 -1.3341668 -1.5535328 -1.8787038 -1.8814638 -1.7040706 -1.8144808][-1.6760716 -1.28457 -0.94810152 -0.68679404 -0.53743243 -0.83051777 -1.3142443 -1.6893632 -1.895694 -1.9443483 -2.1200471 -2.3412378 -2.3743725 -2.3820565 -2.6285315][-1.6751745 -1.4513478 -1.3308806 -1.2364824 -1.2153955 -1.5263641 -1.972847 -2.3151569 -2.4594765 -2.4773679 -2.5899155 -2.7080674 -2.7764592 -2.9271483 -3.2250149]]...]
INFO - root - 2017-12-07 06:58:09.348727: step 16010, loss = 0.90, batch loss = 0.82 (10.6 examples/sec; 0.758 sec/batch; 66h:39m:00s remains)
INFO - root - 2017-12-07 06:58:16.879046: step 16020, loss = 0.80, batch loss = 0.72 (10.4 examples/sec; 0.771 sec/batch; 67h:45m:16s remains)
INFO - root - 2017-12-07 06:58:24.580770: step 16030, loss = 0.80, batch loss = 0.73 (10.9 examples/sec; 0.734 sec/batch; 64h:30m:51s remains)
INFO - root - 2017-12-07 06:58:32.280392: step 16040, loss = 0.80, batch loss = 0.73 (10.0 examples/sec; 0.797 sec/batch; 70h:02m:58s remains)
INFO - root - 2017-12-07 06:58:39.981529: step 16050, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.773 sec/batch; 67h:58m:33s remains)
INFO - root - 2017-12-07 06:58:47.717247: step 16060, loss = 0.82, batch loss = 0.74 (10.2 examples/sec; 0.784 sec/batch; 68h:54m:21s remains)
INFO - root - 2017-12-07 06:58:55.363761: step 16070, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 67h:27m:07s remains)
INFO - root - 2017-12-07 06:59:02.805425: step 16080, loss = 0.80, batch loss = 0.72 (10.3 examples/sec; 0.780 sec/batch; 68h:32m:04s remains)
INFO - root - 2017-12-07 06:59:10.642619: step 16090, loss = 1.03, batch loss = 0.95 (10.2 examples/sec; 0.782 sec/batch; 68h:41m:51s remains)
INFO - root - 2017-12-07 06:59:18.351443: step 16100, loss = 0.71, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 71h:55m:33s remains)
2017-12-07 06:59:19.007309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.106256 -2.1760011 -2.1669893 -2.1173649 -2.1084774 -2.14118 -2.1801026 -2.1863728 -2.1798844 -2.1590798 -2.1131423 -2.0483663 -2.0235448 -2.1341584 -2.343013][-1.7984061 -1.9003053 -1.9179313 -1.8976443 -1.944345 -2.0359094 -2.1139979 -2.1269538 -2.114934 -2.070133 -1.9626443 -1.80251 -1.7014556 -1.8192933 -2.1119049][-1.6974802 -1.782701 -1.7725589 -1.7462595 -1.820385 -1.947968 -2.0584691 -2.0964148 -2.1192653 -2.0948842 -1.9445722 -1.684052 -1.4902637 -1.581425 -1.9047227][-1.7462094 -1.7744679 -1.6782928 -1.6059051 -1.6639748 -1.8029797 -1.933238 -2.0245843 -2.1555293 -2.2417126 -2.10646 -1.7535017 -1.4548502 -1.4896574 -1.816045][-1.5878046 -1.4659736 -1.212168 -1.0675423 -1.1134939 -1.2650988 -1.4109154 -1.5692718 -1.882714 -2.1963367 -2.1754973 -1.7936101 -1.3907905 -1.3260026 -1.6257532][-1.0460558 -0.699198 -0.2868619 -0.088394642 -0.12877083 -0.28227949 -0.41181755 -0.59499335 -1.100522 -1.7389328 -2.0102246 -1.7805667 -1.3662813 -1.186233 -1.3599281][-0.70681477 -0.21525192 0.260633 0.47551537 0.47774172 0.41987896 0.49110556 0.53161907 0.028236866 -0.87078929 -1.5237794 -1.6167772 -1.3710876 -1.2032382 -1.2402766][-1.0354521 -0.56102896 -0.15917015 0.042908192 0.1734395 0.35034275 0.76209164 1.2301674 1.0192037 0.19474602 -0.60331655 -0.96272278 -1.0024204 -1.0657523 -1.1638672][-1.8557477 -1.6066926 -1.4496937 -1.3782756 -1.2330651 -0.94301033 -0.3574748 0.32083559 0.47534657 0.065889835 -0.45658994 -0.73122644 -0.8301034 -1.027442 -1.2317038][-2.4070249 -2.4728904 -2.6663485 -2.8824716 -2.9581091 -2.8405411 -2.3946433 -1.7901227 -1.458329 -1.4479725 -1.5111616 -1.4261224 -1.2615442 -1.2980311 -1.4382544][-1.9435201 -2.2321062 -2.7662497 -3.3422976 -3.7869256 -4.0346341 -3.9858556 -3.7355878 -3.4937937 -3.2887397 -3.0037978 -2.551389 -2.0641067 -1.8308635 -1.7938344][-0.907583 -1.2566671 -1.9774392 -2.7909136 -3.5129862 -4.074748 -4.414525 -4.5460472 -4.4947371 -4.249589 -3.7992277 -3.126302 -2.461134 -2.0795505 -1.9350424][-0.26428223 -0.43524909 -1.0166759 -1.7418859 -2.4715846 -3.1353011 -3.6691904 -4.0227108 -4.1199813 -3.9354846 -3.5263553 -2.8840165 -2.2534468 -1.8859529 -1.7490246][-0.70644116 -0.58746076 -0.812011 -1.2096066 -1.7054746 -2.2378118 -2.7436209 -3.1576703 -3.369276 -3.3518362 -3.1675878 -2.7601805 -2.2952907 -1.9632332 -1.7926233][-1.9645975 -1.7220805 -1.670682 -1.7479818 -1.9377275 -2.1981273 -2.4877524 -2.7643607 -2.9445622 -3.0202012 -3.0229592 -2.8585908 -2.5910783 -2.331948 -2.1434321]]...]
INFO - root - 2017-12-07 06:59:26.581809: step 16110, loss = 0.82, batch loss = 0.74 (10.4 examples/sec; 0.772 sec/batch; 67h:49m:07s remains)
INFO - root - 2017-12-07 06:59:34.206720: step 16120, loss = 0.70, batch loss = 0.62 (10.8 examples/sec; 0.743 sec/batch; 65h:20m:06s remains)
INFO - root - 2017-12-07 06:59:41.744129: step 16130, loss = 0.95, batch loss = 0.88 (10.8 examples/sec; 0.743 sec/batch; 65h:16m:24s remains)
INFO - root - 2017-12-07 06:59:49.262067: step 16140, loss = 0.85, batch loss = 0.77 (10.7 examples/sec; 0.750 sec/batch; 65h:56m:17s remains)
INFO - root - 2017-12-07 06:59:56.994174: step 16150, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.754 sec/batch; 66h:15m:57s remains)
INFO - root - 2017-12-07 07:00:04.671537: step 16160, loss = 0.65, batch loss = 0.58 (10.5 examples/sec; 0.764 sec/batch; 67h:07m:07s remains)
INFO - root - 2017-12-07 07:00:12.556150: step 16170, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.784 sec/batch; 68h:54m:48s remains)
INFO - root - 2017-12-07 07:00:19.904835: step 16180, loss = 0.83, batch loss = 0.76 (10.7 examples/sec; 0.746 sec/batch; 65h:33m:35s remains)
INFO - root - 2017-12-07 07:00:27.622627: step 16190, loss = 1.07, batch loss = 1.00 (10.7 examples/sec; 0.746 sec/batch; 65h:30m:14s remains)
INFO - root - 2017-12-07 07:00:35.246910: step 16200, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 67h:07m:06s remains)
2017-12-07 07:00:35.860532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8019383 -2.8996186 -2.959868 -2.9162335 -2.7132831 -2.4184871 -2.1137564 -1.8675826 -1.7258587 -1.6948378 -1.7491338 -1.9850452 -2.2304676 -2.4097922 -2.5459719][-2.8962574 -2.96634 -3.0174427 -2.9522753 -2.6837587 -2.2477498 -1.7383552 -1.3679826 -1.274446 -1.3577607 -1.5619071 -1.9583032 -2.3720846 -2.712534 -2.9301448][-3.243958 -3.2475932 -3.2160258 -3.0222831 -2.5892835 -1.9228947 -1.1259573 -0.57538795 -0.55062175 -0.85165048 -1.356009 -2.02453 -2.6830518 -3.206882 -3.527863][-3.8554394 -3.7750285 -3.6064775 -3.2035351 -2.6095591 -1.8139622 -0.8330822 -0.077991486 0.022652149 -0.35526037 -1.0724828 -1.9912517 -2.8870053 -3.6153109 -4.0761566][-4.2227292 -4.0664225 -3.7459955 -3.1331944 -2.4539967 -1.7144256 -0.79267406 -0.026145458 0.11850977 -0.28530502 -1.0327914 -1.9110501 -2.8114986 -3.6552746 -4.2765589][-4.2321687 -4.0040107 -3.544364 -2.76505 -2.0836761 -1.5223567 -0.805253 -0.1770916 -0.11987448 -0.5852313 -1.2336571 -1.80654 -2.4255939 -3.2204142 -3.9506335][-4.1808863 -3.912864 -3.3767514 -2.5457044 -1.956147 -1.587615 -1.0425656 -0.569407 -0.65204167 -1.1117363 -1.5402286 -1.6956403 -1.9137249 -2.5075445 -3.2313061][-4.0990114 -3.7743387 -3.2216377 -2.4319901 -1.9519477 -1.7210374 -1.364213 -1.1518366 -1.4173689 -1.7709391 -1.8996379 -1.6806815 -1.5707834 -1.9335053 -2.5656796][-3.8711734 -3.4855359 -2.9866054 -2.3043973 -1.9002678 -1.7089448 -1.4726207 -1.4781268 -1.86607 -2.1176465 -2.0827551 -1.7223461 -1.4553416 -1.6722829 -2.1948407][-3.5536928 -3.1305211 -2.7057829 -2.1509836 -1.7954471 -1.5971859 -1.3982415 -1.4924111 -1.8303039 -2.0047092 -1.9996023 -1.7509866 -1.5051877 -1.6665184 -2.0606415][-3.4053161 -2.9825015 -2.6282215 -2.2124243 -1.9302781 -1.7696009 -1.6273134 -1.6781423 -1.8376546 -1.8707581 -1.8946111 -1.816674 -1.6715355 -1.7559423 -1.9637041][-3.5372081 -3.1875978 -2.9265528 -2.6353755 -2.390115 -2.2528133 -2.1279912 -2.0815537 -2.0936682 -2.0607376 -2.113451 -2.1534939 -2.0746126 -2.0485013 -2.0622361][-3.8410022 -3.6236324 -3.4854369 -3.3164673 -3.1196558 -2.9827447 -2.8188126 -2.6863217 -2.6640625 -2.695816 -2.8069644 -2.8958182 -2.8114438 -2.6529775 -2.4979224][-4.0517859 -3.9774868 -3.9656267 -3.9479465 -3.8854244 -3.8325396 -3.6860669 -3.5277598 -3.5239332 -3.6446321 -3.804029 -3.8751557 -3.7201195 -3.4318 -3.1331873][-4.0432076 -4.079648 -4.1651435 -4.254468 -4.3296032 -4.3963008 -4.3243546 -4.200912 -4.2183032 -4.3665357 -4.51502 -4.5464015 -4.3309875 -3.9767604 -3.5988021]]...]
INFO - root - 2017-12-07 07:00:43.711831: step 16210, loss = 0.75, batch loss = 0.68 (10.0 examples/sec; 0.803 sec/batch; 70h:35m:09s remains)
INFO - root - 2017-12-07 07:00:51.384569: step 16220, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.770 sec/batch; 67h:39m:41s remains)
INFO - root - 2017-12-07 07:00:59.194578: step 16230, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.765 sec/batch; 67h:10m:54s remains)
INFO - root - 2017-12-07 07:01:06.877654: step 16240, loss = 0.76, batch loss = 0.68 (10.1 examples/sec; 0.788 sec/batch; 69h:15m:30s remains)
INFO - root - 2017-12-07 07:01:14.568683: step 16250, loss = 0.75, batch loss = 0.67 (10.0 examples/sec; 0.797 sec/batch; 69h:58m:33s remains)
INFO - root - 2017-12-07 07:01:22.193424: step 16260, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.806 sec/batch; 70h:49m:54s remains)
INFO - root - 2017-12-07 07:01:29.774708: step 16270, loss = 0.85, batch loss = 0.78 (10.9 examples/sec; 0.734 sec/batch; 64h:29m:17s remains)
INFO - root - 2017-12-07 07:01:37.264927: step 16280, loss = 0.71, batch loss = 0.63 (10.7 examples/sec; 0.748 sec/batch; 65h:42m:07s remains)
INFO - root - 2017-12-07 07:01:45.023897: step 16290, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 70h:17m:25s remains)
INFO - root - 2017-12-07 07:01:52.672617: step 16300, loss = 0.62, batch loss = 0.55 (10.4 examples/sec; 0.766 sec/batch; 67h:14m:37s remains)
2017-12-07 07:01:53.275868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7981136 -1.9707487 -2.2098846 -2.2815444 -2.3324418 -2.1756306 -1.8691189 -1.7424519 -1.6959186 -1.7319059 -1.7775726 -1.8106053 -1.9298415 -2.0626736 -2.2164621][-1.8513856 -2.1764941 -2.6290143 -2.8713117 -2.9587789 -2.614177 -2.0155358 -1.741221 -1.7513759 -1.9206781 -2.068675 -2.1469111 -2.2851658 -2.480247 -2.7361121][-2.0599399 -2.5841255 -3.2030396 -3.5587485 -3.6160073 -3.013483 -2.0760961 -1.6166253 -1.7719476 -2.2300889 -2.5977807 -2.7330546 -2.7608919 -2.8669806 -3.1221278][-2.3870962 -3.1318007 -3.83031 -4.1601486 -4.0320249 -3.0955396 -1.8296432 -1.2208533 -1.5963249 -2.4489448 -3.122648 -3.3159204 -3.17799 -3.0948968 -3.2770567][-2.83114 -3.7567534 -4.4400024 -4.6292896 -4.2292442 -2.978045 -1.4672883 -0.76743865 -1.3595786 -2.6062403 -3.5907309 -3.8514419 -3.5787065 -3.3136911 -3.426897][-3.1314912 -4.1453137 -4.7725339 -4.8049269 -4.1483445 -2.6727285 -1.0147493 -0.25370741 -0.97012997 -2.5183835 -3.7791767 -4.1753373 -3.8560352 -3.4295409 -3.4628129][-3.3787174 -4.3213506 -4.8377767 -4.7337937 -3.8943653 -2.2986128 -0.52487373 0.33474398 -0.36334753 -2.0845315 -3.6278937 -4.2743359 -4.06047 -3.5330551 -3.4540181][-3.6258707 -4.3782573 -4.7335563 -4.54578 -3.6890492 -2.1704063 -0.43373227 0.45660067 -0.10004807 -1.7920225 -3.5087669 -4.42649 -4.4429159 -3.9068837 -3.7026961][-3.8749156 -4.452189 -4.6707144 -4.4723158 -3.745172 -2.497555 -1.0302534 -0.28041124 -0.65581608 -2.0814867 -3.7132521 -4.7117867 -4.8498993 -4.3396339 -4.0328298][-4.1610765 -4.6240582 -4.7334762 -4.5164342 -3.9676802 -3.1141577 -2.1043754 -1.5878005 -1.7655733 -2.7597373 -4.0755553 -4.9307513 -5.0471687 -4.5702519 -4.2090716][-4.321753 -4.688724 -4.7244415 -4.5174475 -4.1629791 -3.6877556 -3.129735 -2.8271637 -2.85625 -3.4438214 -4.3793359 -4.9643612 -4.949821 -4.5349946 -4.1870809][-4.1590238 -4.4270062 -4.4198675 -4.2701182 -4.1248779 -3.9631252 -3.7224693 -3.5494215 -3.4915638 -3.7686307 -4.3102326 -4.5759354 -4.4348121 -4.1473932 -3.9064157][-3.6647296 -3.8271072 -3.7774105 -3.6747646 -3.6652763 -3.6950731 -3.6502306 -3.5772176 -3.5061474 -3.5936265 -3.8153303 -3.8316426 -3.6411183 -3.48873 -3.3683882][-3.069129 -3.1583829 -3.0965612 -3.029294 -3.061183 -3.1500721 -3.1775424 -3.1667252 -3.1373582 -3.1606429 -3.2030454 -3.1035719 -2.9305944 -2.8561339 -2.8082922][-2.5364943 -2.5928779 -2.5459342 -2.5025687 -2.5230494 -2.6029181 -2.6698995 -2.7085891 -2.7035174 -2.6742821 -2.6074133 -2.4893515 -2.3982306 -2.4006937 -2.4075861]]...]
INFO - root - 2017-12-07 07:02:01.029565: step 16310, loss = 0.94, batch loss = 0.87 (10.3 examples/sec; 0.774 sec/batch; 68h:00m:14s remains)
INFO - root - 2017-12-07 07:02:08.698650: step 16320, loss = 0.68, batch loss = 0.61 (10.2 examples/sec; 0.786 sec/batch; 69h:00m:07s remains)
INFO - root - 2017-12-07 07:02:16.339350: step 16330, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.784 sec/batch; 68h:50m:02s remains)
INFO - root - 2017-12-07 07:02:24.038764: step 16340, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.754 sec/batch; 66h:12m:35s remains)
INFO - root - 2017-12-07 07:02:31.766837: step 16350, loss = 0.76, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 68h:41m:01s remains)
INFO - root - 2017-12-07 07:02:39.439944: step 16360, loss = 1.00, batch loss = 0.93 (10.5 examples/sec; 0.759 sec/batch; 66h:38m:49s remains)
INFO - root - 2017-12-07 07:02:47.049160: step 16370, loss = 0.80, batch loss = 0.73 (10.8 examples/sec; 0.742 sec/batch; 65h:09m:37s remains)
INFO - root - 2017-12-07 07:02:54.432229: step 16380, loss = 1.07, batch loss = 0.99 (10.3 examples/sec; 0.776 sec/batch; 68h:07m:42s remains)
INFO - root - 2017-12-07 07:03:02.128586: step 16390, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.756 sec/batch; 66h:24m:32s remains)
INFO - root - 2017-12-07 07:03:09.805569: step 16400, loss = 0.81, batch loss = 0.73 (10.5 examples/sec; 0.763 sec/batch; 66h:57m:29s remains)
2017-12-07 07:03:10.541988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2333283 -2.2890298 -2.2155058 -2.1303508 -1.9554551 -1.8368092 -1.6780703 -1.1341281 -0.73718643 -0.71211123 -0.812047 -1.208735 -1.6504171 -2.0095682 -2.4747846][-1.7125635 -1.6546092 -1.5399377 -1.4060314 -1.1815677 -1.0361254 -0.772763 -0.12563705 0.15877295 -0.0080647469 -0.26283026 -0.78111982 -1.2602684 -1.6974261 -2.334482][-1.3891685 -1.281348 -1.2031698 -1.1122136 -0.92981148 -0.7961247 -0.43181062 0.27537203 0.37903118 -0.0027832985 -0.37225628 -0.8489666 -1.0785804 -1.3900559 -2.1331046][-1.6168311 -1.5468047 -1.5127814 -1.4901369 -1.4592998 -1.4867823 -1.116467 -0.35906839 -0.30389023 -0.63593554 -0.90381813 -1.2390144 -1.1444342 -1.2301378 -1.9707291][-1.8340778 -1.720629 -1.6136997 -1.6534579 -1.8481047 -2.0543551 -1.6195772 -0.75729704 -0.66605687 -0.87083173 -1.0609603 -1.4609923 -1.3080671 -1.2967243 -1.9871051][-1.7326713 -1.538274 -1.3227615 -1.3836019 -1.6019917 -1.6231952 -0.85832453 0.14200783 0.092528343 -0.21462059 -0.55272079 -1.2367444 -1.2832043 -1.3808084 -2.0997059][-1.5193982 -1.2636585 -0.99453235 -0.97427678 -0.88332987 -0.34568357 0.86664152 1.8190103 1.2686968 0.65975142 0.097104549 -0.805773 -1.007679 -1.2643933 -2.1296165][-1.1106346 -0.8509903 -0.57951093 -0.4443121 -0.06120348 0.82195234 2.1961727 2.8534722 1.8047523 1.0422869 0.39706707 -0.54168129 -0.735538 -1.102699 -2.1109869][-0.56262732 -0.43001103 -0.24476385 -0.11166811 0.18005371 0.82130957 1.8478789 2.1281452 1.1694946 0.74844646 0.28432751 -0.53911424 -0.6493125 -1.0543118 -2.1381028][-0.25185776 -0.27840567 -0.18753242 -0.15543127 -0.17111397 -0.033174038 0.5363493 0.6506424 0.18592882 0.32024479 0.074428082 -0.6413002 -0.70281196 -1.1100433 -2.1856983][-0.23290205 -0.29813671 -0.19382715 -0.13871193 -0.26008224 -0.38989735 -0.077912331 -0.053781509 -0.19196796 0.26496744 0.0728178 -0.68138242 -0.80902576 -1.2349997 -2.2564933][-0.36869526 -0.3239212 -0.10451794 0.090209961 0.089147091 0.016590595 0.29386139 0.22067785 0.124084 0.54286766 0.21416473 -0.63252139 -0.90967917 -1.4016891 -2.338722][-0.4784503 -0.25495768 0.069680691 0.29581833 0.36170197 0.39719629 0.70223045 0.62127447 0.52389956 0.78993607 0.28425217 -0.58183527 -1.0067165 -1.5718172 -2.3942084][-0.42862892 -0.13756609 0.14410639 0.25004721 0.23551655 0.26327705 0.53448439 0.500062 0.47279739 0.62641096 0.034088612 -0.73400784 -1.1912467 -1.7372179 -2.4157975][-0.27322006 0.0070924759 0.16694689 0.11402607 -0.025322914 -0.10379791 0.032327175 0.0051679611 0.070365906 0.16642857 -0.43939567 -1.1131172 -1.5197654 -1.9433799 -2.4396076]]...]
INFO - root - 2017-12-07 07:03:18.154036: step 16410, loss = 0.68, batch loss = 0.60 (10.8 examples/sec; 0.740 sec/batch; 65h:00m:59s remains)
INFO - root - 2017-12-07 07:03:25.722890: step 16420, loss = 0.94, batch loss = 0.87 (10.7 examples/sec; 0.747 sec/batch; 65h:35m:45s remains)
INFO - root - 2017-12-07 07:03:33.383546: step 16430, loss = 0.71, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 66h:35m:56s remains)
INFO - root - 2017-12-07 07:03:40.937194: step 16440, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.763 sec/batch; 66h:57m:45s remains)
INFO - root - 2017-12-07 07:03:48.610825: step 16450, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.792 sec/batch; 69h:33m:48s remains)
INFO - root - 2017-12-07 07:03:56.245382: step 16460, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.782 sec/batch; 68h:37m:23s remains)
INFO - root - 2017-12-07 07:04:03.930207: step 16470, loss = 0.70, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 66h:26m:34s remains)
INFO - root - 2017-12-07 07:04:11.320375: step 16480, loss = 1.06, batch loss = 0.99 (10.5 examples/sec; 0.764 sec/batch; 67h:02m:50s remains)
INFO - root - 2017-12-07 07:04:18.959437: step 16490, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.769 sec/batch; 67h:29m:41s remains)
INFO - root - 2017-12-07 07:04:26.661091: step 16500, loss = 1.04, batch loss = 0.96 (10.1 examples/sec; 0.791 sec/batch; 69h:25m:37s remains)
2017-12-07 07:04:27.258519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1524498 -3.0996528 -3.0596607 -3.0384781 -3.0367079 -3.0495796 -3.0713456 -3.1053834 -3.1407018 -3.1571093 -3.1631529 -3.1884546 -3.1706166 -3.1444101 -3.1283579][-3.3169584 -3.247509 -3.1778803 -3.1235359 -3.0856342 -3.0267727 -2.9991064 -3.0725081 -3.1684813 -3.2253361 -3.2570386 -3.2904572 -3.2793486 -3.2612619 -3.2554483][-3.2619205 -3.2010746 -3.1018758 -3.0026531 -2.9084356 -2.776371 -2.7213614 -2.8428075 -2.9982052 -3.1033421 -3.1894016 -3.2482147 -3.2496886 -3.2307711 -3.1884692][-2.9818234 -2.9393973 -2.8213968 -2.6834884 -2.5089288 -2.2557573 -2.1272917 -2.2239423 -2.3565784 -2.4580989 -2.5772123 -2.685709 -2.7631955 -2.8152189 -2.7838545][-2.4876387 -2.4457214 -2.3084779 -2.1696496 -2.0137537 -1.7871847 -1.6628523 -1.7093666 -1.762758 -1.8008695 -1.8872356 -1.9699652 -2.0659449 -2.1607356 -2.1555843][-1.8630617 -1.8407106 -1.7146297 -1.6171541 -1.5618098 -1.4661527 -1.4082606 -1.4324932 -1.4482167 -1.4395561 -1.4737103 -1.4906716 -1.5334108 -1.6275024 -1.6649082][-1.3906574 -1.4065979 -1.3543246 -1.339426 -1.3529124 -1.2987127 -1.207895 -1.140631 -1.0833781 -1.0402911 -0.99182582 -0.90371108 -0.90127587 -1.0400209 -1.1923106][-1.3150744 -1.3703821 -1.3832936 -1.4273465 -1.4917402 -1.4801033 -1.3468847 -1.1493866 -0.99853039 -0.96433544 -0.854651 -0.5998528 -0.49143434 -0.67011833 -0.92925715][-1.5679779 -1.619077 -1.598114 -1.5525231 -1.5766888 -1.6319356 -1.5437949 -1.284236 -1.0905485 -1.1587243 -1.1424894 -0.86435795 -0.7280426 -0.91700053 -1.208389][-1.8404689 -1.8852267 -1.8699541 -1.7350936 -1.6379235 -1.7001629 -1.7168405 -1.5150964 -1.3782647 -1.5617988 -1.6579537 -1.4798789 -1.4535749 -1.62959 -1.7668295][-2.1469743 -2.1964169 -2.2349195 -2.0645134 -1.8189118 -1.8223805 -1.9211233 -1.8004706 -1.7354383 -1.9649489 -2.0657623 -1.8962324 -1.8900423 -1.9839456 -1.9518862][-2.4914749 -2.6204777 -2.7842853 -2.6486809 -2.3233366 -2.2471552 -2.3147748 -2.1720567 -2.0735061 -2.2157788 -2.2087076 -1.9580514 -1.8413224 -1.7811081 -1.6331091][-2.7970035 -3.078238 -3.3478851 -3.2086053 -2.81894 -2.6836843 -2.6944423 -2.5096774 -2.3317413 -2.3060112 -2.1411297 -1.8295002 -1.6547773 -1.5381312 -1.3663239][-2.826467 -3.0800028 -3.2449899 -3.0062804 -2.5925527 -2.4780207 -2.5237608 -2.4100969 -2.2769783 -2.1673932 -1.9236858 -1.6122994 -1.484081 -1.4975457 -1.4945486][-2.9557433 -2.9897792 -2.8918266 -2.5199373 -2.1146748 -1.9941514 -2.0436854 -2.0550227 -2.0404801 -1.9816988 -1.8200202 -1.621635 -1.5459328 -1.6507094 -1.7851381]]...]
INFO - root - 2017-12-07 07:04:34.945095: step 16510, loss = 0.78, batch loss = 0.71 (10.1 examples/sec; 0.789 sec/batch; 69h:14m:45s remains)
INFO - root - 2017-12-07 07:04:42.606631: step 16520, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.764 sec/batch; 67h:03m:44s remains)
INFO - root - 2017-12-07 07:04:50.315768: step 16530, loss = 0.84, batch loss = 0.76 (10.5 examples/sec; 0.763 sec/batch; 66h:59m:29s remains)
INFO - root - 2017-12-07 07:04:58.063143: step 16540, loss = 0.74, batch loss = 0.67 (9.8 examples/sec; 0.820 sec/batch; 71h:59m:50s remains)
INFO - root - 2017-12-07 07:05:05.811631: step 16550, loss = 0.72, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 67h:54m:47s remains)
INFO - root - 2017-12-07 07:05:13.497069: step 16560, loss = 0.87, batch loss = 0.80 (10.2 examples/sec; 0.784 sec/batch; 68h:50m:12s remains)
INFO - root - 2017-12-07 07:05:21.143989: step 16570, loss = 0.85, batch loss = 0.78 (10.2 examples/sec; 0.788 sec/batch; 69h:07m:35s remains)
INFO - root - 2017-12-07 07:05:28.754546: step 16580, loss = 0.98, batch loss = 0.90 (10.0 examples/sec; 0.803 sec/batch; 70h:29m:52s remains)
INFO - root - 2017-12-07 07:05:36.402970: step 16590, loss = 0.97, batch loss = 0.90 (10.6 examples/sec; 0.758 sec/batch; 66h:32m:31s remains)
INFO - root - 2017-12-07 07:05:44.112628: step 16600, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.762 sec/batch; 66h:50m:53s remains)
2017-12-07 07:05:44.667006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5472329 -3.6754589 -3.7346694 -3.675061 -3.5708282 -3.4338536 -3.3310618 -3.3048706 -3.2537985 -3.2163279 -3.1896553 -3.1696117 -3.2113209 -3.3120258 -3.4020376][-3.6754823 -3.7888298 -3.8192317 -3.7624221 -3.678627 -3.5542479 -3.4534774 -3.4482923 -3.4203756 -3.3678708 -3.2973382 -3.2133789 -3.1737852 -3.19282 -3.2386603][-3.5827875 -3.632268 -3.5905371 -3.4851995 -3.3917964 -3.3137074 -3.2802458 -3.3407779 -3.3950748 -3.3958118 -3.3516352 -3.2636833 -3.1832347 -3.14423 -3.1389885][-3.5010407 -3.472445 -3.3882561 -3.2808785 -3.1978164 -3.1636329 -3.1818039 -3.2833545 -3.394285 -3.4339259 -3.4202709 -3.3635397 -3.2939904 -3.2388594 -3.1995397][-3.4887738 -3.4332666 -3.369925 -3.3325052 -3.3049698 -3.2947054 -3.3213155 -3.4137816 -3.5229042 -3.5565388 -3.5292413 -3.4710023 -3.3925934 -3.3217742 -3.2523789][-3.5978751 -3.6190805 -3.6552718 -3.7088692 -3.7081385 -3.6613493 -3.6216211 -3.6399751 -3.70114 -3.724689 -3.7004991 -3.6346192 -3.523191 -3.4098687 -3.3030784][-3.7065558 -3.7676587 -3.8426104 -3.9253688 -3.9172041 -3.8339515 -3.7518673 -3.7331238 -3.7898617 -3.8588576 -3.8889554 -3.8354275 -3.6910934 -3.5257552 -3.4060471][-3.870178 -3.9099345 -3.9541988 -4.0127521 -3.9941607 -3.8952425 -3.7952631 -3.7852755 -3.8837688 -4.0153852 -4.0917912 -4.0415721 -3.8716657 -3.6674786 -3.5453343][-4.11355 -4.156383 -4.186564 -4.2522745 -4.2693496 -4.2021427 -4.1275558 -4.1408348 -4.2509422 -4.3585672 -4.3955188 -4.3051386 -4.1037064 -3.8854568 -3.7699549][-4.276485 -4.325263 -4.3416362 -4.4037671 -4.4533849 -4.4550166 -4.4496346 -4.4796586 -4.5474081 -4.5677972 -4.5313005 -4.4089956 -4.200109 -4.0076976 -3.907783][-4.3742709 -4.4254713 -4.4456391 -4.4851933 -4.5060868 -4.5446663 -4.5765271 -4.6050596 -4.6223545 -4.5601859 -4.4724393 -4.3407426 -4.1537743 -4.0077505 -3.9263649][-4.465651 -4.446671 -4.4227891 -4.3994684 -4.3595819 -4.3989367 -4.4335804 -4.444912 -4.4527078 -4.3929877 -4.3262053 -4.2267404 -4.0958581 -4.0254846 -3.9937725][-4.6128368 -4.5017118 -4.3876019 -4.2676449 -4.184268 -4.2303128 -4.2677369 -4.2581749 -4.2707624 -4.2460089 -4.2138538 -4.1454186 -4.056612 -4.0325608 -4.03073][-4.6942067 -4.5625668 -4.4011464 -4.23355 -4.1673532 -4.2541757 -4.35069 -4.3544831 -4.3448911 -4.2972136 -4.237545 -4.1673665 -4.0822415 -4.0406 -4.0205374][-4.6490827 -4.5403767 -4.3874631 -4.2285328 -4.184257 -4.2733054 -4.4086246 -4.4523087 -4.4551616 -4.4019446 -4.3212633 -4.2658515 -4.197094 -4.1268067 -4.0690928]]...]
INFO - root - 2017-12-07 07:05:52.289356: step 16610, loss = 0.76, batch loss = 0.68 (10.5 examples/sec; 0.764 sec/batch; 67h:02m:16s remains)
INFO - root - 2017-12-07 07:05:59.878228: step 16620, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.758 sec/batch; 66h:28m:06s remains)
INFO - root - 2017-12-07 07:06:07.589947: step 16630, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.755 sec/batch; 66h:13m:05s remains)
INFO - root - 2017-12-07 07:06:15.313637: step 16640, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.745 sec/batch; 65h:22m:34s remains)
INFO - root - 2017-12-07 07:06:23.115655: step 16650, loss = 0.93, batch loss = 0.86 (10.7 examples/sec; 0.746 sec/batch; 65h:25m:19s remains)
INFO - root - 2017-12-07 07:06:30.821819: step 16660, loss = 0.85, batch loss = 0.78 (9.9 examples/sec; 0.812 sec/batch; 71h:13m:33s remains)
INFO - root - 2017-12-07 07:06:38.525547: step 16670, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.754 sec/batch; 66h:10m:44s remains)
INFO - root - 2017-12-07 07:06:45.908352: step 16680, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.759 sec/batch; 66h:36m:43s remains)
INFO - root - 2017-12-07 07:06:53.720186: step 16690, loss = 0.64, batch loss = 0.56 (10.6 examples/sec; 0.752 sec/batch; 66h:00m:13s remains)
INFO - root - 2017-12-07 07:07:01.435796: step 16700, loss = 0.88, batch loss = 0.81 (10.3 examples/sec; 0.775 sec/batch; 67h:57m:34s remains)
2017-12-07 07:07:02.045040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.565378 -1.8649917 -2.2719951 -2.2212288 -1.8406494 -0.98977757 -0.33405066 -1.2984109 -2.5203614 -2.3881538 -1.764194 -0.4790535 -0.031956673 -0.6183641 -0.058458328][-1.0551608 -1.5026789 -2.0795717 -1.9799709 -1.4164104 -0.13050985 0.97692013 -0.33965302 -2.1347656 -2.2018075 -1.532305 0.038937569 0.63107681 -0.12296486 0.44770575][-0.81031084 -1.3519828 -2.0554764 -1.8835208 -1.0473557 0.871706 2.6555791 0.94797134 -1.6406124 -2.1235678 -1.4949718 0.41971731 1.3657832 0.55296516 1.0499496][-0.93735194 -1.4671247 -2.253056 -2.0053952 -0.87074423 1.7503533 4.2871914 2.2214627 -1.2100239 -2.2271421 -1.7478724 0.38903809 1.7495399 0.97377443 1.3292937][-1.3530476 -1.8192332 -2.6174717 -2.2973928 -0.86470389 2.3733292 5.6266346 3.341877 -0.78746271 -2.2974064 -2.0248644 0.24853945 2.0006084 1.3067408 1.421277][-1.72476 -2.078002 -2.7896671 -2.4370492 -0.85979104 2.744575 6.5767231 4.3235884 -0.11090136 -1.9442809 -1.8901927 0.4190402 2.4194393 1.7950892 1.645134][-1.9516268 -2.1770451 -2.7393489 -2.4171553 -0.88471794 2.7816739 6.8945312 4.8654337 0.50907087 -1.4846861 -1.6463239 0.65348196 2.7677398 2.2325506 1.8777089][-2.3228719 -2.451925 -2.8749948 -2.6630502 -1.3478825 2.1228228 6.1476326 4.4595051 0.50147867 -1.5020051 -1.7594495 0.63086843 2.8044062 2.3668408 1.775187][-2.7785902 -2.8851457 -3.2463629 -3.176374 -2.1318147 0.98838949 4.6943064 3.3855824 0.010984421 -1.8568001 -2.067075 0.46411037 2.5539966 2.1250067 1.2697263][-3.1601043 -3.2858434 -3.6131797 -3.6287808 -2.8028748 -0.098749638 3.2141147 2.2547011 -0.5579977 -2.2972517 -2.4856002 0.099426746 2.0160809 1.6675386 0.69651604][-3.3603084 -3.4445353 -3.6852255 -3.71135 -3.0933514 -0.90948296 1.9101624 1.2304268 -1.0598631 -2.6557264 -2.8781338 -0.39153004 1.3237762 1.1291299 0.14295244][-3.2275479 -3.2770469 -3.4888165 -3.5439243 -3.0942535 -1.4124079 0.892014 0.37547255 -1.5150013 -2.902153 -3.1015739 -0.83507562 0.72042942 0.704164 -0.27431393][-3.1795387 -3.2546883 -3.5066848 -3.5876181 -3.1897507 -1.8820179 -0.10298061 -0.572011 -2.1842878 -3.3241878 -3.450572 -1.5028014 -0.032152653 0.18828869 -0.69429064][-3.2436013 -3.3622961 -3.6378131 -3.6980391 -3.3379359 -2.3735938 -1.1110921 -1.5147982 -2.8261046 -3.6997252 -3.7678714 -2.230037 -0.87734437 -0.45053148 -1.1382456][-3.2010112 -3.3351011 -3.61255 -3.6616237 -3.3792806 -2.7227337 -1.8616943 -2.1119063 -3.0742702 -3.7431331 -3.8603382 -2.8124201 -1.6699433 -1.1155159 -1.5390697]]...]
INFO - root - 2017-12-07 07:07:09.759778: step 16710, loss = 0.84, batch loss = 0.76 (10.3 examples/sec; 0.780 sec/batch; 68h:25m:01s remains)
INFO - root - 2017-12-07 07:07:17.528265: step 16720, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.774 sec/batch; 67h:55m:56s remains)
INFO - root - 2017-12-07 07:07:25.180564: step 16730, loss = 0.73, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 69h:53m:14s remains)
INFO - root - 2017-12-07 07:07:32.879055: step 16740, loss = 0.85, batch loss = 0.78 (10.6 examples/sec; 0.755 sec/batch; 66h:10m:41s remains)
INFO - root - 2017-12-07 07:07:40.605978: step 16750, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.762 sec/batch; 66h:50m:55s remains)
INFO - root - 2017-12-07 07:07:48.311471: step 16760, loss = 0.90, batch loss = 0.83 (10.4 examples/sec; 0.766 sec/batch; 67h:12m:45s remains)
INFO - root - 2017-12-07 07:07:58.101769: step 16770, loss = 0.82, batch loss = 0.74 (7.6 examples/sec; 1.053 sec/batch; 92h:21m:19s remains)
INFO - root - 2017-12-07 07:08:08.310586: step 16780, loss = 0.78, batch loss = 0.71 (7.8 examples/sec; 1.028 sec/batch; 90h:09m:15s remains)
INFO - root - 2017-12-07 07:08:18.786095: step 16790, loss = 1.02, batch loss = 0.95 (7.7 examples/sec; 1.039 sec/batch; 91h:05m:28s remains)
INFO - root - 2017-12-07 07:08:29.201730: step 16800, loss = 0.59, batch loss = 0.51 (7.6 examples/sec; 1.058 sec/batch; 92h:48m:39s remains)
2017-12-07 07:08:30.000918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9955277 -4.0406427 -4.0996351 -4.0962524 -4.01478 -3.9551609 -3.9609892 -4.0045171 -3.9865739 -3.9501605 -4.0125351 -4.0860748 -4.1309581 -4.1943994 -4.1321616][-3.9730268 -3.9833453 -4.025898 -4.0451584 -3.9613743 -3.8923841 -3.9117513 -3.9674561 -3.9382067 -3.8807945 -3.94234 -4.0196195 -4.105978 -4.2383146 -4.1873679][-3.7896771 -3.733681 -3.7444782 -3.7883406 -3.7135468 -3.6592953 -3.7205951 -3.8224185 -3.7984824 -3.7092948 -3.7329824 -3.7912958 -3.9340611 -4.1726937 -4.1831865][-3.4827633 -3.3786459 -3.3981202 -3.4798372 -3.4426849 -3.4412761 -3.558794 -3.6863804 -3.6475925 -3.5172787 -3.4918427 -3.5281439 -3.7080555 -4.0295224 -4.1084595][-3.1531725 -3.059236 -3.1265428 -3.2250729 -3.1900463 -3.1802382 -3.2454028 -3.3162389 -3.2465553 -3.132127 -3.1474152 -3.2488275 -3.5036471 -3.8922083 -4.0160623][-3.00355 -2.8883996 -2.9005957 -2.9135079 -2.8193264 -2.749526 -2.6787302 -2.6192064 -2.529449 -2.4882917 -2.6098223 -2.8517115 -3.2432652 -3.7294235 -3.9019942][-2.9278274 -2.8049843 -2.7424071 -2.6313965 -2.4402633 -2.2868798 -2.0835054 -1.8962424 -1.8000464 -1.8211286 -2.0062644 -2.3496509 -2.8843672 -3.4844179 -3.720084][-2.9405699 -2.8425217 -2.7443442 -2.5699835 -2.3191082 -2.1167073 -1.8785427 -1.6974118 -1.6848686 -1.7455578 -1.8842716 -2.1752198 -2.7314477 -3.3653343 -3.6259272][-3.0007474 -2.9717884 -2.9162359 -2.8299565 -2.675849 -2.5402105 -2.3543675 -2.2337439 -2.2650721 -2.2831092 -2.2671704 -2.3742883 -2.816488 -3.3871188 -3.6411088][-3.111954 -3.1884389 -3.1815014 -3.1757469 -3.09557 -2.9720352 -2.7997942 -2.6898992 -2.6641865 -2.56722 -2.3720431 -2.297931 -2.6301062 -3.1639161 -3.4677749][-2.9676452 -3.083431 -3.0658829 -3.0599127 -2.9870663 -2.8376303 -2.6633995 -2.5489204 -2.4494629 -2.256274 -1.9598343 -1.8121662 -2.1355841 -2.6874022 -3.0695052][-2.4147944 -2.5427675 -2.560533 -2.584506 -2.5754149 -2.4692042 -2.3241522 -2.2192912 -2.0992568 -1.9194305 -1.6694236 -1.5773084 -1.928195 -2.4331141 -2.7920454][-2.1822569 -2.3889127 -2.5031023 -2.5494375 -2.5624657 -2.5001349 -2.399828 -2.334492 -2.2620769 -2.1587615 -2.0088003 -1.981426 -2.2577443 -2.5719604 -2.7712598][-2.5959096 -2.8245258 -2.9651256 -2.9953847 -2.9952893 -2.9670262 -2.9192159 -2.8887696 -2.8383656 -2.7702672 -2.6765463 -2.6470094 -2.7756348 -2.8768644 -2.9104965][-3.1631541 -3.279707 -3.3605556 -3.386261 -3.4095087 -3.4343097 -3.4405127 -3.4331889 -3.3849158 -3.3246632 -3.2429028 -3.1771359 -3.182071 -3.1539674 -3.0967278]]...]
INFO - root - 2017-12-07 07:08:40.428913: step 16810, loss = 0.83, batch loss = 0.75 (7.6 examples/sec; 1.050 sec/batch; 92h:03m:34s remains)
INFO - root - 2017-12-07 07:08:50.794771: step 16820, loss = 0.79, batch loss = 0.71 (7.5 examples/sec; 1.060 sec/batch; 92h:57m:10s remains)
INFO - root - 2017-12-07 07:09:01.183001: step 16830, loss = 0.78, batch loss = 0.71 (7.8 examples/sec; 1.025 sec/batch; 89h:53m:09s remains)
INFO - root - 2017-12-07 07:09:11.559638: step 16840, loss = 0.79, batch loss = 0.72 (7.7 examples/sec; 1.037 sec/batch; 90h:53m:46s remains)
INFO - root - 2017-12-07 07:09:21.923467: step 16850, loss = 0.83, batch loss = 0.76 (7.9 examples/sec; 1.017 sec/batch; 89h:08m:14s remains)
INFO - root - 2017-12-07 07:09:32.249738: step 16860, loss = 0.83, batch loss = 0.76 (7.9 examples/sec; 1.008 sec/batch; 88h:24m:16s remains)
INFO - root - 2017-12-07 07:09:42.592749: step 16870, loss = 0.71, batch loss = 0.64 (7.7 examples/sec; 1.035 sec/batch; 90h:44m:36s remains)
INFO - root - 2017-12-07 07:09:52.662004: step 16880, loss = 0.96, batch loss = 0.89 (7.8 examples/sec; 1.029 sec/batch; 90h:12m:30s remains)
INFO - root - 2017-12-07 07:10:02.967403: step 16890, loss = 0.87, batch loss = 0.80 (7.7 examples/sec; 1.033 sec/batch; 90h:33m:25s remains)
INFO - root - 2017-12-07 07:10:13.247710: step 16900, loss = 0.91, batch loss = 0.84 (7.9 examples/sec; 1.015 sec/batch; 88h:58m:51s remains)
2017-12-07 07:10:14.015933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9991698 -4.0548835 -4.062963 -4.0894566 -4.192914 -4.2929134 -4.2929506 -4.26903 -4.2490211 -4.2426496 -4.2187 -4.1654463 -4.134366 -4.163527 -4.2076912][-4.1417131 -4.2007771 -4.1799054 -4.1727443 -4.2923088 -4.4409051 -4.4809012 -4.4572077 -4.4031496 -4.3658137 -4.3164353 -4.234467 -4.2120228 -4.2891817 -4.368207][-4.1580954 -4.1830225 -4.1206317 -4.0967293 -4.2485623 -4.4560142 -4.572063 -4.5891914 -4.4834518 -4.3435125 -4.1969266 -4.0364327 -4.0026245 -4.1143131 -4.2371492][-3.7310343 -3.5817351 -3.387351 -3.3554406 -3.6168084 -3.9561882 -4.2218337 -4.402463 -4.347991 -4.16612 -3.9417672 -3.677844 -3.5564103 -3.6166282 -3.7273314][-3.5726788 -3.2224426 -2.78707 -2.65177 -2.9121203 -3.2258134 -3.4876487 -3.7567687 -3.7863243 -3.6655097 -3.5228148 -3.2766566 -3.0575991 -2.96115 -2.93067][-3.8792734 -3.5776458 -3.0497293 -2.7604508 -2.7849753 -2.7976394 -2.8207154 -3.0097251 -3.0281045 -2.9839611 -2.9855418 -2.8197589 -2.5533943 -2.3178794 -2.1007133][-3.8706408 -3.73984 -3.2984438 -2.9564977 -2.7847123 -2.5560992 -2.4495497 -2.5450468 -2.5126247 -2.5499468 -2.6788027 -2.5464175 -2.2621529 -1.9320729 -1.5542209][-3.3572402 -3.2679405 -2.9034691 -2.5884838 -2.4000702 -2.2072453 -2.2352912 -2.3914695 -2.4244382 -2.6032424 -2.8159909 -2.7015572 -2.4505444 -2.1489663 -1.756922][-3.1204591 -2.904577 -2.4383123 -2.0958309 -1.9562926 -1.9225731 -2.1143544 -2.3548162 -2.5103929 -2.840369 -3.0790133 -2.9683411 -2.7982416 -2.6181421 -2.3422425][-3.5446761 -3.2012525 -2.5121999 -2.01661 -1.8673205 -1.9379416 -2.1749582 -2.3981607 -2.569288 -2.8923981 -3.0674124 -2.9714015 -2.8966255 -2.8127232 -2.6532903][-3.8780708 -3.5173054 -2.8017366 -2.2698476 -2.0783989 -2.1501753 -2.3467205 -2.4747405 -2.5495749 -2.6935673 -2.6412547 -2.44755 -2.4454646 -2.4959176 -2.4984765][-3.7583158 -3.3858829 -2.7792978 -2.3397653 -2.1967459 -2.2802908 -2.4261622 -2.4450412 -2.4049342 -2.3685327 -2.1279111 -1.8443093 -1.8718455 -2.0028851 -2.1391797][-3.5871835 -3.1971519 -2.7279072 -2.4010129 -2.3114133 -2.397594 -2.4581306 -2.3702676 -2.2796369 -2.1961946 -1.9050217 -1.6433165 -1.7109711 -1.8617043 -2.0336902][-3.6892438 -3.3459823 -2.976655 -2.7109056 -2.6262243 -2.7094154 -2.6954374 -2.4989991 -2.3608186 -2.2898245 -2.0756447 -1.9079065 -1.9548233 -2.0130706 -2.0810397][-3.8498993 -3.6197486 -3.3598764 -3.1426873 -3.02983 -3.0914106 -3.0529132 -2.8290477 -2.6792779 -2.6359787 -2.5314302 -2.4393964 -2.4202507 -2.3267632 -2.2373796]]...]
INFO - root - 2017-12-07 07:10:24.335535: step 16910, loss = 0.72, batch loss = 0.65 (7.8 examples/sec; 1.031 sec/batch; 90h:23m:48s remains)
INFO - root - 2017-12-07 07:10:34.675388: step 16920, loss = 0.90, batch loss = 0.83 (7.7 examples/sec; 1.035 sec/batch; 90h:45m:55s remains)
INFO - root - 2017-12-07 07:10:44.978192: step 16930, loss = 0.78, batch loss = 0.71 (7.7 examples/sec; 1.042 sec/batch; 91h:22m:27s remains)
INFO - root - 2017-12-07 07:10:55.374240: step 16940, loss = 0.76, batch loss = 0.69 (7.8 examples/sec; 1.026 sec/batch; 89h:58m:18s remains)
INFO - root - 2017-12-07 07:11:05.731439: step 16950, loss = 0.92, batch loss = 0.85 (7.8 examples/sec; 1.021 sec/batch; 89h:30m:06s remains)
INFO - root - 2017-12-07 07:11:16.007386: step 16960, loss = 0.67, batch loss = 0.60 (7.7 examples/sec; 1.034 sec/batch; 90h:36m:40s remains)
INFO - root - 2017-12-07 07:11:26.421383: step 16970, loss = 0.66, batch loss = 0.59 (7.6 examples/sec; 1.046 sec/batch; 91h:41m:31s remains)
INFO - root - 2017-12-07 07:11:36.568508: step 16980, loss = 0.81, batch loss = 0.74 (7.7 examples/sec; 1.045 sec/batch; 91h:37m:31s remains)
INFO - root - 2017-12-07 07:11:47.009997: step 16990, loss = 0.71, batch loss = 0.64 (7.5 examples/sec; 1.062 sec/batch; 93h:03m:07s remains)
INFO - root - 2017-12-07 07:11:57.419261: step 17000, loss = 0.66, batch loss = 0.59 (7.5 examples/sec; 1.068 sec/batch; 93h:36m:03s remains)
2017-12-07 07:11:58.217848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9856091 -1.9445634 -1.9217637 -1.970612 -2.0199971 -2.0548205 -2.0774455 -2.0595436 -2.2232273 -2.4510608 -2.4244089 -2.2637422 -2.0796449 -1.8897786 -1.7627971][-2.0054574 -2.0408587 -2.07789 -2.0851345 -2.0336874 -1.9547069 -1.8390999 -1.7423272 -1.9205797 -2.0946052 -2.0352674 -1.9992659 -2.0043879 -1.919692 -1.8058822][-2.2251959 -2.3373828 -2.3950014 -2.3515909 -2.2380478 -2.1074696 -1.9469402 -1.8373983 -1.9810994 -2.0345626 -1.8860359 -1.8649704 -1.9120452 -1.8518825 -1.7860751][-2.3119798 -2.4961338 -2.5539641 -2.5042398 -2.404891 -2.3020115 -2.1983929 -2.2016282 -2.4140279 -2.4197514 -2.1992722 -2.123055 -2.088284 -1.969882 -1.9002018][-2.1179812 -2.3506882 -2.41788 -2.4467454 -2.4293211 -2.3562427 -2.2770882 -2.3810203 -2.676451 -2.6624165 -2.4134443 -2.329181 -2.2660003 -2.1410491 -2.062443][-1.7258465 -1.9153759 -1.9716489 -2.0991795 -2.1549866 -2.0507538 -1.9630744 -2.1807957 -2.5740323 -2.5792227 -2.3721638 -2.3422019 -2.3093348 -2.212888 -2.1162283][-1.5256779 -1.546006 -1.5238206 -1.6744499 -1.6969817 -1.4833663 -1.3573332 -1.6431491 -2.0674207 -2.0822196 -1.9714274 -2.0551791 -2.1156054 -2.0809381 -1.9810243][-1.7461591 -1.5281672 -1.3829088 -1.462667 -1.3885198 -1.0899601 -0.92044878 -1.1436276 -1.4311945 -1.4129076 -1.4113963 -1.6133165 -1.7733777 -1.8433528 -1.8250902][-2.1254838 -1.7684343 -1.5613711 -1.5380476 -1.3927848 -1.1345875 -1.0058322 -1.1585922 -1.2957375 -1.2311184 -1.2747781 -1.490845 -1.6509554 -1.7742131 -1.8545632][-2.5219965 -2.1648457 -1.9625764 -1.8380435 -1.651695 -1.4815838 -1.4335175 -1.5769858 -1.6593239 -1.5890815 -1.6453817 -1.8144431 -1.9164901 -2.0345705 -2.1529269][-2.7955751 -2.4934309 -2.3185141 -2.1644323 -2.02119 -1.9430809 -1.9549084 -2.13341 -2.2303138 -2.1976883 -2.2735329 -2.3858547 -2.4137993 -2.511713 -2.6389451][-3.1078176 -2.8382711 -2.6175208 -2.4584641 -2.3970203 -2.39036 -2.4373217 -2.6351452 -2.74589 -2.757617 -2.8518038 -2.8793468 -2.8163521 -2.8633161 -2.9534936][-3.4088326 -3.1438584 -2.8549683 -2.7053254 -2.7169778 -2.7585709 -2.8283195 -2.9940457 -3.0680375 -3.103765 -3.2024035 -3.1588202 -3.0347433 -3.0220392 -3.0539019][-3.4100981 -3.1911941 -2.9272575 -2.8220329 -2.8624058 -2.9313974 -3.0177617 -3.125864 -3.1511288 -3.1786242 -3.2439709 -3.1497555 -2.9958911 -2.9318905 -2.9110126][-3.2353559 -3.0515592 -2.8368082 -2.7440629 -2.7498116 -2.8192439 -2.9224577 -3.0096164 -3.0253558 -3.0446227 -3.0665603 -2.9547148 -2.7908623 -2.6790304 -2.6087465]]...]
INFO - root - 2017-12-07 07:12:08.600856: step 17010, loss = 0.81, batch loss = 0.74 (7.8 examples/sec; 1.030 sec/batch; 90h:16m:48s remains)
INFO - root - 2017-12-07 07:12:19.020289: step 17020, loss = 1.12, batch loss = 1.04 (7.7 examples/sec; 1.037 sec/batch; 90h:54m:10s remains)
INFO - root - 2017-12-07 07:12:29.394421: step 17030, loss = 0.92, batch loss = 0.85 (7.6 examples/sec; 1.057 sec/batch; 92h:39m:53s remains)
INFO - root - 2017-12-07 07:12:39.707280: step 17040, loss = 0.81, batch loss = 0.73 (7.8 examples/sec; 1.022 sec/batch; 89h:35m:44s remains)
INFO - root - 2017-12-07 07:12:50.013908: step 17050, loss = 0.91, batch loss = 0.84 (7.9 examples/sec; 1.009 sec/batch; 88h:26m:53s remains)
INFO - root - 2017-12-07 07:13:00.381520: step 17060, loss = 0.71, batch loss = 0.64 (7.6 examples/sec; 1.049 sec/batch; 91h:56m:30s remains)
INFO - root - 2017-12-07 07:13:10.803713: step 17070, loss = 0.80, batch loss = 0.73 (7.6 examples/sec; 1.058 sec/batch; 92h:43m:24s remains)
INFO - root - 2017-12-07 07:13:21.050797: step 17080, loss = 0.83, batch loss = 0.76 (7.5 examples/sec; 1.060 sec/batch; 92h:54m:20s remains)
INFO - root - 2017-12-07 07:13:31.652656: step 17090, loss = 1.01, batch loss = 0.94 (7.5 examples/sec; 1.065 sec/batch; 93h:20m:11s remains)
INFO - root - 2017-12-07 07:13:42.140015: step 17100, loss = 0.72, batch loss = 0.65 (7.8 examples/sec; 1.021 sec/batch; 89h:25m:12s remains)
2017-12-07 07:13:43.044965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5712035 -1.5801582 -1.6105547 -1.6473017 -1.670573 -1.690367 -1.7175312 -1.7491136 -1.763068 -1.7655094 -1.7686467 -1.7591643 -1.733988 -1.7147272 -1.7082291][-1.9673152 -2.0067539 -2.0778394 -2.1487844 -2.1988528 -2.2304306 -2.2323456 -2.2140532 -2.1809387 -2.1592665 -2.1531627 -2.1631439 -2.1875594 -2.2291622 -2.2713239][-2.2338607 -2.2886386 -2.3534496 -2.3961649 -2.4220791 -2.4486802 -2.4509692 -2.4141178 -2.3737357 -2.3526416 -2.3383117 -2.3276052 -2.3233423 -2.3444278 -2.3745143][-2.1982706 -2.2663977 -2.29701 -2.2631631 -2.2146211 -2.1873152 -2.174886 -2.1686885 -2.212425 -2.2782562 -2.3152249 -2.3049448 -2.2449265 -2.1767616 -2.1317744][-2.2814374 -2.2888463 -2.2497315 -2.1669176 -2.0899117 -2.0191793 -1.9631481 -1.962534 -2.0834608 -2.215481 -2.287514 -2.3023539 -2.279422 -2.2411325 -2.2170362][-2.3342445 -2.2848849 -2.1968272 -2.1213672 -2.0814435 -2.0127566 -1.9231296 -1.8996677 -2.00364 -2.0920384 -2.10063 -2.0683379 -2.0380871 -2.0151434 -2.0217862][-1.9027586 -1.9204054 -1.9025123 -1.9121647 -1.927495 -1.8242311 -1.6329055 -1.4851556 -1.4813178 -1.5338449 -1.5623288 -1.556638 -1.5156569 -1.467731 -1.4541409][-1.6410995 -1.7486854 -1.8044674 -1.8829002 -1.9111147 -1.760886 -1.4838669 -1.2246954 -1.122803 -1.1879113 -1.3195879 -1.3949859 -1.3629127 -1.3077798 -1.2857687][-1.7691681 -1.9432728 -2.0503592 -2.1720977 -2.204073 -2.0422328 -1.741931 -1.416554 -1.2499611 -1.3080695 -1.4644358 -1.5334072 -1.474443 -1.4091933 -1.380722][-1.881201 -1.9904153 -2.0387716 -2.1157854 -2.1415069 -2.0467584 -1.8592074 -1.6144691 -1.4924057 -1.5628958 -1.6851966 -1.6982918 -1.5818326 -1.4629762 -1.380096][-1.8259604 -1.8083236 -1.7608871 -1.7746649 -1.8213348 -1.8402722 -1.8163099 -1.7321012 -1.7254808 -1.8346722 -1.9084206 -1.8515215 -1.6585355 -1.4443655 -1.2763281][-1.8039217 -1.7101135 -1.6069996 -1.5878875 -1.6589484 -1.7463405 -1.8061628 -1.811532 -1.856638 -1.9492366 -1.9636824 -1.8522205 -1.6268404 -1.3938997 -1.2360437][-2.0788786 -1.996047 -1.8875477 -1.8462093 -1.8951418 -1.9656949 -2.0130107 -2.0226912 -2.0557439 -2.0925937 -2.0581286 -1.9397795 -1.7551 -1.5889421 -1.5030949][-2.1602285 -2.1286428 -2.0570657 -2.0187528 -2.0376921 -2.0629241 -2.0780919 -2.09061 -2.1336193 -2.1747074 -2.1597395 -2.0905457 -1.9688201 -1.8482194 -1.7777338][-1.686156 -1.7246404 -1.7406411 -1.7470994 -1.7623317 -1.7514236 -1.7374027 -1.7535565 -1.8170056 -1.9010096 -1.9444211 -1.9584692 -1.9211223 -1.8498971 -1.7961946]]...]
INFO - root - 2017-12-07 07:13:53.549486: step 17110, loss = 0.71, batch loss = 0.64 (7.6 examples/sec; 1.050 sec/batch; 91h:59m:18s remains)
INFO - root - 2017-12-07 07:14:03.951697: step 17120, loss = 0.87, batch loss = 0.80 (7.5 examples/sec; 1.069 sec/batch; 93h:39m:09s remains)
INFO - root - 2017-12-07 07:14:14.375659: step 17130, loss = 0.77, batch loss = 0.70 (7.9 examples/sec; 1.016 sec/batch; 89h:02m:21s remains)
INFO - root - 2017-12-07 07:14:24.757535: step 17140, loss = 0.82, batch loss = 0.74 (7.6 examples/sec; 1.057 sec/batch; 92h:36m:54s remains)
INFO - root - 2017-12-07 07:14:35.151184: step 17150, loss = 0.83, batch loss = 0.75 (7.6 examples/sec; 1.058 sec/batch; 92h:38m:56s remains)
INFO - root - 2017-12-07 07:14:45.491770: step 17160, loss = 0.78, batch loss = 0.71 (7.9 examples/sec; 1.007 sec/batch; 88h:14m:10s remains)
INFO - root - 2017-12-07 07:14:55.893304: step 17170, loss = 1.00, batch loss = 0.93 (7.7 examples/sec; 1.040 sec/batch; 91h:03m:29s remains)
INFO - root - 2017-12-07 07:15:06.084915: step 17180, loss = 0.76, batch loss = 0.69 (7.9 examples/sec; 1.019 sec/batch; 89h:12m:50s remains)
INFO - root - 2017-12-07 07:15:16.464730: step 17190, loss = 0.78, batch loss = 0.71 (7.8 examples/sec; 1.019 sec/batch; 89h:16m:24s remains)
INFO - root - 2017-12-07 07:15:26.728271: step 17200, loss = 0.82, batch loss = 0.75 (7.8 examples/sec; 1.023 sec/batch; 89h:37m:31s remains)
2017-12-07 07:15:27.480110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.413579 -1.4365139 -1.445775 -1.4380071 -1.4159935 -1.3990319 -1.4316006 -1.4819996 -1.4980326 -1.493886 -1.5019963 -1.5504446 -1.6067986 -1.6454592 -1.6576824][-1.3960719 -1.4222054 -1.4374132 -1.4180446 -1.3648226 -1.3242302 -1.3566668 -1.4116526 -1.422529 -1.4187546 -1.4369435 -1.4984274 -1.5731614 -1.6318481 -1.656671][-1.3990817 -1.4291203 -1.4458709 -1.4072917 -1.3050256 -1.2159777 -1.223963 -1.2667344 -1.2703326 -1.2747624 -1.3183303 -1.4101434 -1.5135717 -1.600729 -1.6443951][-1.4180939 -1.4585965 -1.4882226 -1.4463973 -1.3111389 -1.176945 -1.1591721 -1.1828346 -1.1747806 -1.1887004 -1.2522995 -1.3631842 -1.475477 -1.5733182 -1.6286061][-1.4542096 -1.5090046 -1.5516191 -1.516983 -1.3629553 -1.1933596 -1.1394553 -1.1243916 -1.0970714 -1.1350017 -1.2406611 -1.3801854 -1.5030253 -1.5974636 -1.6481843][-1.4469578 -1.5145333 -1.5660105 -1.5427463 -1.3951042 -1.2176917 -1.139802 -1.0767047 -1.01284 -1.0656173 -1.210921 -1.3869977 -1.524838 -1.6204958 -1.6705573][-1.3874745 -1.4620774 -1.5170856 -1.5009587 -1.373193 -1.215945 -1.1428821 -1.0407374 -0.9269712 -0.97108412 -1.1421995 -1.3521762 -1.5040774 -1.6023018 -1.6601863][-1.4375701 -1.4787865 -1.5056682 -1.4838054 -1.3794558 -1.2595408 -1.2043283 -1.0768454 -0.91246676 -0.92844653 -1.1129248 -1.3412154 -1.4945989 -1.5803099 -1.6390135][-1.5247149 -1.5371521 -1.5352192 -1.5006325 -1.427635 -1.3713143 -1.3713017 -1.268548 -1.096024 -1.1001205 -1.2875428 -1.4983399 -1.6053102 -1.6323359 -1.6508505][-1.5100296 -1.5239499 -1.5227129 -1.4931343 -1.4542434 -1.4653757 -1.5316296 -1.4716334 -1.3048382 -1.3013041 -1.4913015 -1.6881642 -1.7591803 -1.7398462 -1.7104766][-1.4390266 -1.4757216 -1.5064013 -1.5108924 -1.5099409 -1.569381 -1.6726182 -1.6303191 -1.4429255 -1.3984907 -1.5635872 -1.7504826 -1.8276279 -1.8132963 -1.7689757][-1.411447 -1.4732738 -1.5423126 -1.595181 -1.6448977 -1.7570319 -1.8943877 -1.8562553 -1.6264164 -1.4935288 -1.5663707 -1.6990573 -1.7773285 -1.7900367 -1.761652][-1.4489944 -1.5047498 -1.5798602 -1.6585457 -1.7566333 -1.9289162 -2.1194568 -2.1190281 -1.8933809 -1.6943412 -1.6525812 -1.6918869 -1.7314041 -1.7465513 -1.728318][-1.4862137 -1.5225086 -1.5707102 -1.6416276 -1.7517962 -1.9499555 -2.1684303 -2.2157161 -2.054569 -1.8646979 -1.7686052 -1.737505 -1.7314649 -1.7278342 -1.7067094][-1.5035682 -1.5145862 -1.5299094 -1.5732 -1.6565628 -1.8197384 -2.0112867 -2.0817878 -2.0004528 -1.8831713 -1.8072491 -1.7699056 -1.7502196 -1.7334509 -1.707629]]...]
INFO - root - 2017-12-07 07:15:38.085835: step 17210, loss = 0.56, batch loss = 0.49 (7.9 examples/sec; 1.017 sec/batch; 89h:05m:38s remains)
INFO - root - 2017-12-07 07:15:48.565669: step 17220, loss = 0.86, batch loss = 0.78 (7.9 examples/sec; 1.015 sec/batch; 88h:55m:53s remains)
INFO - root - 2017-12-07 07:15:58.867091: step 17230, loss = 0.87, batch loss = 0.80 (7.8 examples/sec; 1.025 sec/batch; 89h:46m:59s remains)
INFO - root - 2017-12-07 07:16:09.227245: step 17240, loss = 0.77, batch loss = 0.70 (7.7 examples/sec; 1.034 sec/batch; 90h:33m:34s remains)
INFO - root - 2017-12-07 07:16:19.593986: step 17250, loss = 0.89, batch loss = 0.81 (7.5 examples/sec; 1.061 sec/batch; 92h:53m:06s remains)
INFO - root - 2017-12-07 07:16:29.982693: step 17260, loss = 0.80, batch loss = 0.73 (7.7 examples/sec; 1.039 sec/batch; 90h:57m:35s remains)
INFO - root - 2017-12-07 07:16:40.630532: step 17270, loss = 0.87, batch loss = 0.80 (7.6 examples/sec; 1.054 sec/batch; 92h:18m:51s remains)
INFO - root - 2017-12-07 07:16:50.847937: step 17280, loss = 0.67, batch loss = 0.60 (7.7 examples/sec; 1.032 sec/batch; 90h:23m:48s remains)
INFO - root - 2017-12-07 07:17:01.402826: step 17290, loss = 1.01, batch loss = 0.93 (7.4 examples/sec; 1.076 sec/batch; 94h:15m:10s remains)
INFO - root - 2017-12-07 07:17:11.854224: step 17300, loss = 0.82, batch loss = 0.75 (7.8 examples/sec; 1.032 sec/batch; 90h:22m:43s remains)
2017-12-07 07:17:12.705397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.196682 -2.8592925 -2.55442 -2.2755926 -2.2571752 -2.6136718 -3.0240593 -3.1764927 -3.2561355 -3.4837351 -3.6435292 -3.6946898 -3.7418511 -3.5866199 -3.3462744][-3.4335179 -3.0217705 -2.6417818 -2.2722902 -2.1482952 -2.4515667 -2.8429604 -2.9709525 -3.031919 -3.2497225 -3.3793111 -3.3818712 -3.3675711 -3.1005197 -2.76761][-3.4787967 -3.0000238 -2.5795968 -2.2131732 -2.0530546 -2.2716637 -2.5995076 -2.6889396 -2.7273619 -2.8927233 -2.9524064 -2.9038358 -2.86876 -2.5975232 -2.295994][-3.5063686 -3.0086441 -2.5547464 -2.2200692 -2.0434694 -2.1476223 -2.3917665 -2.4707785 -2.4930396 -2.5674784 -2.5148473 -2.4244604 -2.438843 -2.2883718 -2.1045032][-3.7221634 -3.2969723 -2.8124604 -2.442965 -2.1579313 -2.0368483 -2.1099162 -2.16494 -2.1827281 -2.1838241 -2.0562196 -1.9580929 -2.0683358 -2.0999937 -2.0648694][-3.9275522 -3.60307 -3.1273916 -2.7696996 -2.4353166 -2.1355169 -1.9959877 -1.9288752 -1.8829319 -1.8085823 -1.6799886 -1.657341 -1.8914216 -2.0867844 -2.1503284][-3.9304342 -3.611433 -3.1490326 -2.908011 -2.6859093 -2.3913214 -2.1189883 -1.8882208 -1.7053518 -1.5495691 -1.479722 -1.5788491 -1.9148011 -2.23795 -2.4088109][-3.8263087 -3.4777045 -3.041357 -2.9305391 -2.849297 -2.6423812 -2.3588312 -2.0079119 -1.6948185 -1.5040936 -1.5603209 -1.7990198 -2.1880202 -2.5810919 -2.8647456][-3.768712 -3.3819294 -2.9482415 -2.905839 -2.8612361 -2.6562033 -2.3508737 -1.9358151 -1.5789886 -1.4723318 -1.7125912 -2.0965886 -2.5158889 -2.9200358 -3.2656136][-3.9104953 -3.4292169 -2.89338 -2.7945926 -2.7174847 -2.4837065 -2.1816428 -1.8187795 -1.5605173 -1.6135745 -1.9684191 -2.3823891 -2.7411652 -3.0899863 -3.4243488][-4.1403513 -3.6174695 -2.9642143 -2.7509346 -2.628809 -2.3893373 -2.1496193 -1.9476039 -1.896929 -2.1001079 -2.4250369 -2.7390091 -2.981678 -3.2502646 -3.5354183][-4.199079 -3.7766974 -3.1467278 -2.867322 -2.7141042 -2.5031991 -2.336781 -2.2754056 -2.3383729 -2.5002203 -2.6490059 -2.7859259 -2.8607383 -3.0002818 -3.2036386][-4.142415 -3.8734038 -3.3034372 -2.9669516 -2.8415895 -2.7757897 -2.7802298 -2.8255513 -2.8468413 -2.8028636 -2.7047973 -2.6348655 -2.5325532 -2.5440464 -2.6662598][-3.9194126 -3.7737241 -3.249867 -2.845964 -2.7645566 -2.884326 -3.0954754 -3.2749016 -3.2823129 -3.0858378 -2.8483438 -2.6882062 -2.5121036 -2.4553576 -2.5111094][-3.4535522 -3.3576334 -2.8883739 -2.5234368 -2.5712709 -2.8794374 -3.2434969 -3.5059929 -3.5354643 -3.3250589 -3.1092708 -2.9590642 -2.7814732 -2.696198 -2.7196689]]...]
INFO - root - 2017-12-07 07:17:23.111550: step 17310, loss = 0.95, batch loss = 0.88 (8.0 examples/sec; 1.002 sec/batch; 87h:43m:10s remains)
INFO - root - 2017-12-07 07:17:33.524197: step 17320, loss = 0.74, batch loss = 0.67 (7.7 examples/sec; 1.039 sec/batch; 90h:58m:54s remains)
INFO - root - 2017-12-07 07:17:43.830827: step 17330, loss = 0.72, batch loss = 0.65 (7.7 examples/sec; 1.033 sec/batch; 90h:27m:17s remains)
INFO - root - 2017-12-07 07:17:54.201608: step 17340, loss = 0.71, batch loss = 0.64 (7.8 examples/sec; 1.030 sec/batch; 90h:08m:15s remains)
INFO - root - 2017-12-07 07:18:04.597762: step 17350, loss = 0.86, batch loss = 0.79 (7.8 examples/sec; 1.022 sec/batch; 89h:27m:01s remains)
INFO - root - 2017-12-07 07:18:15.069812: step 17360, loss = 0.86, batch loss = 0.79 (7.3 examples/sec; 1.091 sec/batch; 95h:31m:12s remains)
INFO - root - 2017-12-07 07:18:25.488795: step 17370, loss = 0.61, batch loss = 0.54 (7.7 examples/sec; 1.038 sec/batch; 90h:52m:51s remains)
INFO - root - 2017-12-07 07:18:35.714712: step 17380, loss = 0.91, batch loss = 0.83 (7.7 examples/sec; 1.040 sec/batch; 90h:59m:58s remains)
INFO - root - 2017-12-07 07:18:46.320832: step 17390, loss = 0.80, batch loss = 0.73 (7.9 examples/sec; 1.010 sec/batch; 88h:23m:53s remains)
INFO - root - 2017-12-07 07:18:56.844618: step 17400, loss = 0.97, batch loss = 0.90 (7.4 examples/sec; 1.086 sec/batch; 95h:03m:39s remains)
2017-12-07 07:18:57.598595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.003602 -1.3019035 -1.8261776 -2.26332 -2.5312471 -2.7208195 -2.5070448 -2.1635573 -2.2466815 -2.6948395 -3.2693706 -3.5423405 -3.4454105 -3.445673 -3.6970835][-0.98134923 -1.0284588 -1.2477839 -1.467983 -1.5729554 -1.7300155 -1.9216568 -2.1157756 -2.5132051 -3.0655184 -3.4972498 -3.5039966 -3.2488866 -3.2751393 -3.6453893][-2.097029 -1.9463353 -2.0041814 -2.1110711 -2.0888152 -2.2460253 -2.8074889 -3.378767 -3.8411257 -4.312717 -4.4831705 -4.1781626 -3.8293786 -3.9028261 -4.2364411][-3.0927658 -2.9070737 -2.9596181 -3.0815697 -2.9540071 -2.9456234 -3.4437253 -3.966028 -4.3265052 -4.7963486 -4.9759655 -4.583663 -4.2327929 -4.3593316 -4.6492438][-3.0791259 -2.843821 -2.9732242 -3.2070942 -2.9972007 -2.730473 -2.7925553 -2.8901911 -3.0989783 -3.8351438 -4.4632835 -4.279294 -3.8743896 -3.8613544 -4.15116][-2.4534538 -2.1628416 -2.4731443 -2.9496164 -2.8552413 -2.3780529 -1.6682758 -1.0286696 -1.2341189 -2.6056402 -3.9050672 -3.9541032 -3.402782 -3.1175022 -3.3608081][-1.8831565 -1.6082938 -2.0855324 -2.6972127 -2.6196682 -1.7794251 -0.062460423 1.4367709 1.0924544 -1.1183295 -3.1079106 -3.357512 -2.6543834 -2.1390305 -2.396409][-1.7065563 -1.3716574 -1.8420298 -2.3899608 -2.2282825 -1.019537 1.3903584 3.4094324 2.9090219 0.19430065 -2.1379826 -2.493994 -1.7584376 -1.1779482 -1.4521906][-2.1340909 -1.7693279 -2.1540656 -2.6593566 -2.5937252 -1.4013906 0.95815277 2.7635946 2.2507792 -0.13406563 -2.1471336 -2.3174686 -1.5739279 -0.96725655 -1.1057806][-2.4410491 -2.1709597 -2.4643769 -2.9978826 -3.1793728 -2.33635 -0.49205589 0.77985096 0.29476213 -1.4474676 -2.8486428 -2.7351072 -1.9216688 -1.2188292 -1.0827994][-2.3490105 -2.1668241 -2.3309183 -2.7149532 -2.9386764 -2.4639487 -1.2192907 -0.4177289 -0.79145718 -1.9617419 -2.9482849 -2.815402 -2.0770247 -1.2970922 -0.85786939][-2.0840125 -1.9141221 -1.9097669 -1.9578817 -2.0156946 -1.8655293 -1.2447386 -0.81791258 -1.0485706 -1.7537167 -2.4841239 -2.5085597 -2.0391793 -1.4429193 -0.89099741][-1.6794534 -1.6220446 -1.6207378 -1.5084925 -1.4216614 -1.5345657 -1.4766276 -1.3894837 -1.5117493 -1.9153571 -2.5571294 -2.7692895 -2.5773237 -2.2902567 -1.8555925][-1.6444051 -1.7550292 -1.8629637 -1.7875903 -1.6286857 -1.8212259 -2.1432784 -2.3656101 -2.4512854 -2.620425 -3.1485085 -3.4224892 -3.3801541 -3.3096094 -2.9726334][-2.4400628 -2.5133882 -2.5648079 -2.520766 -2.275449 -2.3790929 -2.8112617 -3.12948 -3.1498692 -3.139905 -3.4878986 -3.6595898 -3.6755886 -3.7268317 -3.3735678]]...]
INFO - root - 2017-12-07 07:19:08.101145: step 17410, loss = 0.73, batch loss = 0.66 (7.8 examples/sec; 1.019 sec/batch; 89h:12m:07s remains)
INFO - root - 2017-12-07 07:19:18.616836: step 17420, loss = 0.76, batch loss = 0.69 (7.6 examples/sec; 1.057 sec/batch; 92h:32m:20s remains)
INFO - root - 2017-12-07 07:19:29.020876: step 17430, loss = 0.84, batch loss = 0.77 (7.5 examples/sec; 1.063 sec/batch; 93h:01m:37s remains)
INFO - root - 2017-12-07 07:19:39.462361: step 17440, loss = 0.86, batch loss = 0.79 (7.6 examples/sec; 1.046 sec/batch; 91h:34m:17s remains)
INFO - root - 2017-12-07 07:19:49.999914: step 17450, loss = 0.80, batch loss = 0.73 (7.7 examples/sec; 1.037 sec/batch; 90h:42m:48s remains)
INFO - root - 2017-12-07 07:20:00.470383: step 17460, loss = 0.77, batch loss = 0.70 (7.7 examples/sec; 1.034 sec/batch; 90h:30m:06s remains)
INFO - root - 2017-12-07 07:20:10.965334: step 17470, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.061 sec/batch; 92h:48m:53s remains)
INFO - root - 2017-12-07 07:20:22.581040: step 17480, loss = 0.75, batch loss = 0.67 (6.1 examples/sec; 1.319 sec/batch; 115h:27m:29s remains)
INFO - root - 2017-12-07 07:20:35.865620: step 17490, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.330 sec/batch; 116h:22m:20s remains)
INFO - root - 2017-12-07 07:20:49.251619: step 17500, loss = 0.82, batch loss = 0.75 (6.1 examples/sec; 1.320 sec/batch; 115h:30m:56s remains)
2017-12-07 07:20:50.204138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4370048 -3.4318864 -3.4090362 -3.3805304 -3.3881683 -3.4405143 -3.50192 -3.5430009 -3.5750997 -3.6035161 -3.6203392 -3.6293554 -3.639173 -3.659328 -3.6895638][-3.0949407 -2.973485 -2.836518 -2.7197149 -2.7179275 -2.8374696 -3.0023298 -3.1528511 -3.3038192 -3.4468317 -3.5735316 -3.650871 -3.6754322 -3.6935916 -3.731545][-2.3967774 -2.1790936 -1.9561081 -1.7796605 -1.7978756 -1.9992568 -2.2262969 -2.4108436 -2.6118262 -2.8415182 -3.1161432 -3.3243532 -3.4157662 -3.482317 -3.5646405][-1.8910754 -1.631609 -1.3728864 -1.1697214 -1.2297418 -1.5132778 -1.7445989 -1.8547471 -1.9511046 -2.1093132 -2.3875082 -2.6128254 -2.7209125 -2.850955 -3.01243][-1.636389 -1.3179519 -1.0454788 -0.86228418 -0.99311662 -1.3694963 -1.5920837 -1.609792 -1.5623727 -1.6164873 -1.8529351 -2.0322473 -2.101474 -2.2614157 -2.5054283][-1.2362034 -0.94778252 -0.791528 -0.72774482 -0.94043326 -1.334599 -1.4726985 -1.3480105 -1.1744504 -1.217324 -1.4714134 -1.6337223 -1.6948197 -1.8697135 -2.1540878][-0.56656551 -0.47341776 -0.59671545 -0.72670126 -0.98299265 -1.2666376 -1.1681149 -0.77589726 -0.45731831 -0.61024117 -0.99611163 -1.1930099 -1.2587018 -1.3960624 -1.6326814][0.043852329 -0.16088438 -0.5650425 -0.79543376 -0.98027158 -1.0433352 -0.62433958 0.10404491 0.583168 0.26536036 -0.32415628 -0.61991811 -0.72152519 -0.82443285 -0.98777127][-0.021514416 -0.43781471 -0.90985441 -1.0128269 -1.0126257 -0.87296176 -0.28791714 0.57697725 1.1112323 0.64685297 -0.019363403 -0.28575039 -0.32824945 -0.40344334 -0.552845][-0.75324726 -1.1882033 -1.4989798 -1.3742635 -1.2166634 -1.0461071 -0.62786984 -0.01028204 0.33900166 -0.11431313 -0.63358474 -0.70173645 -0.58491874 -0.62225294 -0.8189044][-1.6543894 -1.9502075 -1.9947157 -1.7154024 -1.5416794 -1.4947703 -1.372319 -1.105202 -0.98263168 -1.3634226 -1.6764982 -1.5683024 -1.3667746 -1.4206827 -1.6706831][-2.0521882 -2.1722224 -2.044306 -1.7801716 -1.739274 -1.8774672 -1.993576 -1.9689245 -1.9883535 -2.2793787 -2.4286261 -2.2398982 -2.0560617 -2.1526291 -2.4141247][-1.7002161 -1.6739068 -1.5122905 -1.3609381 -1.4685552 -1.7534041 -2.0046184 -2.0973225 -2.1817489 -2.3908777 -2.4413314 -2.2448013 -2.0965757 -2.1972749 -2.4186578][-1.5091009 -1.3874142 -1.237494 -1.173816 -1.3128302 -1.5766921 -1.7951126 -1.892849 -1.9763849 -2.0991874 -2.105674 -1.9569364 -1.8553503 -1.9231417 -2.0604525][-2.2013805 -2.0958414 -2.010957 -2.0138302 -2.1181893 -2.264385 -2.3622541 -2.4003863 -2.4477463 -2.4988172 -2.4953041 -2.4295354 -2.3858981 -2.4100909 -2.4439104]]...]
INFO - root - 2017-12-07 07:21:03.402056: step 17510, loss = 0.89, batch loss = 0.82 (5.9 examples/sec; 1.364 sec/batch; 119h:22m:18s remains)
INFO - root - 2017-12-07 07:21:16.489852: step 17520, loss = 0.73, batch loss = 0.66 (6.2 examples/sec; 1.296 sec/batch; 113h:21m:45s remains)
INFO - root - 2017-12-07 07:21:29.784686: step 17530, loss = 0.75, batch loss = 0.68 (6.0 examples/sec; 1.328 sec/batch; 116h:12m:18s remains)
INFO - root - 2017-12-07 07:21:42.999253: step 17540, loss = 0.73, batch loss = 0.66 (6.1 examples/sec; 1.308 sec/batch; 114h:27m:16s remains)
INFO - root - 2017-12-07 07:21:56.205494: step 17550, loss = 0.67, batch loss = 0.60 (6.2 examples/sec; 1.300 sec/batch; 113h:44m:04s remains)
INFO - root - 2017-12-07 07:22:09.393663: step 17560, loss = 0.86, batch loss = 0.78 (6.2 examples/sec; 1.293 sec/batch; 113h:07m:49s remains)
INFO - root - 2017-12-07 07:22:22.629474: step 17570, loss = 1.02, batch loss = 0.95 (5.8 examples/sec; 1.371 sec/batch; 119h:58m:10s remains)
INFO - root - 2017-12-07 07:22:36.024739: step 17580, loss = 0.80, batch loss = 0.73 (5.9 examples/sec; 1.345 sec/batch; 117h:38m:42s remains)
INFO - root - 2017-12-07 07:22:49.358214: step 17590, loss = 0.79, batch loss = 0.72 (6.0 examples/sec; 1.333 sec/batch; 116h:37m:06s remains)
INFO - root - 2017-12-07 07:23:02.682668: step 17600, loss = 1.08, batch loss = 1.01 (6.1 examples/sec; 1.314 sec/batch; 114h:56m:22s remains)
2017-12-07 07:23:03.680096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8984947 -2.022733 -2.1785505 -2.2984157 -2.3962531 -2.4668355 -2.4992843 -2.5633898 -2.6238165 -2.657336 -2.7698107 -2.8785725 -2.9130023 -2.9090061 -2.8576269][-1.6308482 -1.74455 -1.8870828 -2.0124323 -2.1115072 -2.1915991 -2.2807162 -2.4234903 -2.5720696 -2.6477158 -2.689374 -2.6644371 -2.5649347 -2.429256 -2.2584014][-2.3938322 -2.4118586 -2.4514704 -2.5008564 -2.5436091 -2.6047802 -2.6926618 -2.8292356 -2.9737921 -3.0475578 -3.0843983 -3.0475459 -2.9127536 -2.7062001 -2.4401231][-3.332963 -3.3865092 -3.4385369 -3.4859867 -3.487396 -3.4596169 -3.4003787 -3.4028165 -3.4792001 -3.5455561 -3.6506283 -3.7072673 -3.6258705 -3.4405682 -3.2485611][-3.9632192 -4.0829177 -4.1794415 -4.2134104 -4.1485863 -3.9748282 -3.7255361 -3.6129925 -3.7176065 -3.8908885 -4.1272755 -4.2689919 -4.2038488 -4.0331359 -3.9753337][-3.7633941 -3.801414 -3.8053939 -3.6912162 -3.4028337 -2.9172626 -2.3845367 -2.1821997 -2.4285471 -2.8232794 -3.2440157 -3.4905863 -3.4686732 -3.3249586 -3.4040666][-3.0695658 -3.0061474 -2.9105008 -2.6171861 -2.02562 -1.1836219 -0.40071249 -0.17507029 -0.63061881 -1.28951 -1.8581588 -2.1243064 -2.0759811 -1.9322317 -2.0873115][-2.8312597 -2.8039327 -2.7429152 -2.4366164 -1.7777781 -0.931087 -0.27438641 -0.1982131 -0.68912315 -1.2999718 -1.7270315 -1.7976501 -1.5847669 -1.3449891 -1.4267371][-2.6277242 -2.7788467 -2.8826656 -2.7332706 -2.267195 -1.7152166 -1.3859777 -1.4471114 -1.7742357 -2.1137974 -2.2580876 -2.0837045 -1.7101789 -1.3783257 -1.3385749][-2.560113 -2.8481784 -3.042016 -2.9921143 -2.7296596 -2.4889014 -2.4628243 -2.6503873 -2.8672278 -3.0094881 -2.961513 -2.6578655 -2.2685287 -1.9822774 -1.9312851][-2.986619 -3.2330482 -3.3652334 -3.3277674 -3.2175746 -3.2010813 -3.35037 -3.5808632 -3.7194023 -3.7359846 -3.5888066 -3.3113039 -3.0844636 -2.95547 -2.9462214][-3.9256005 -3.9727986 -3.9204922 -3.7957144 -3.7017853 -3.7299821 -3.877986 -4.0609312 -4.1904316 -4.2492118 -4.2045841 -4.1157045 -4.1065412 -4.123251 -4.1605606][-4.65624 -4.5827742 -4.4465022 -4.3007994 -4.2177496 -4.237524 -4.3328176 -4.4529738 -4.5856409 -4.7064414 -4.7707791 -4.8091378 -4.8914671 -4.9679832 -5.0339069][-4.4901724 -4.4393997 -4.3721604 -4.3295741 -4.33213 -4.3864722 -4.4554663 -4.5234122 -4.6112027 -4.7002664 -4.7545719 -4.78479 -4.8143859 -4.8213768 -4.818264][-3.4614339 -3.4483864 -3.4404519 -3.4578874 -3.4978116 -3.5587835 -3.616214 -3.665092 -3.7300119 -3.7941756 -3.830976 -3.8374395 -3.811517 -3.7639613 -3.720823]]...]
INFO - root - 2017-12-07 07:23:17.022254: step 17610, loss = 0.93, batch loss = 0.86 (5.9 examples/sec; 1.361 sec/batch; 119h:04m:46s remains)
INFO - root - 2017-12-07 07:23:30.458526: step 17620, loss = 0.92, batch loss = 0.84 (5.8 examples/sec; 1.380 sec/batch; 120h:42m:10s remains)
INFO - root - 2017-12-07 07:23:43.895613: step 17630, loss = 0.81, batch loss = 0.74 (6.1 examples/sec; 1.321 sec/batch; 115h:30m:49s remains)
INFO - root - 2017-12-07 07:23:57.180237: step 17640, loss = 0.73, batch loss = 0.66 (6.0 examples/sec; 1.329 sec/batch; 116h:14m:52s remains)
INFO - root - 2017-12-07 07:24:10.510239: step 17650, loss = 0.75, batch loss = 0.67 (6.0 examples/sec; 1.328 sec/batch; 116h:07m:42s remains)
INFO - root - 2017-12-07 07:24:23.779016: step 17660, loss = 0.90, batch loss = 0.83 (6.0 examples/sec; 1.333 sec/batch; 116h:35m:06s remains)
INFO - root - 2017-12-07 07:24:37.253013: step 17670, loss = 0.76, batch loss = 0.69 (5.9 examples/sec; 1.356 sec/batch; 118h:33m:21s remains)
INFO - root - 2017-12-07 07:24:50.442720: step 17680, loss = 0.76, batch loss = 0.68 (5.8 examples/sec; 1.368 sec/batch; 119h:35m:33s remains)
INFO - root - 2017-12-07 07:25:03.829319: step 17690, loss = 0.87, batch loss = 0.79 (5.9 examples/sec; 1.349 sec/batch; 117h:56m:00s remains)
INFO - root - 2017-12-07 07:25:17.274298: step 17700, loss = 1.04, batch loss = 0.96 (5.9 examples/sec; 1.355 sec/batch; 118h:31m:02s remains)
2017-12-07 07:25:18.278651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1358919 -2.217176 -2.3038938 -2.6492538 -3.0003607 -2.7058659 -2.4836605 -2.7097497 -2.7367992 -2.6368356 -2.5953827 -2.2048194 -1.7853847 -1.7860763 -1.9443533][-2.6609657 -2.7523181 -2.9106328 -3.2447374 -3.5703955 -3.2700703 -2.950345 -2.9972496 -2.9509034 -2.8596749 -2.7999349 -2.2744176 -1.6238782 -1.5924637 -1.8074496][-3.0899096 -3.2006731 -3.3478279 -3.6017637 -3.8750455 -3.6256797 -3.2317362 -3.0888464 -2.9961035 -2.9092841 -2.6910539 -1.9963489 -1.2891004 -1.4135916 -1.849849][-2.9712787 -3.0175571 -3.0348983 -3.2247219 -3.5408573 -3.4995997 -3.2338698 -3.064924 -3.0363979 -3.0001581 -2.6644487 -1.9698699 -1.3760452 -1.5304735 -1.9533143][-2.9303463 -3.0537453 -3.0681825 -3.2507048 -3.6029994 -3.6785824 -3.5198162 -3.2792568 -3.1314089 -2.9060302 -2.3211663 -1.6485724 -1.1466081 -1.1530252 -1.3932867][-3.4818344 -3.7618511 -3.8246217 -3.8946486 -4.0949802 -4.0283175 -3.8347609 -3.4923384 -3.1141777 -2.6073115 -1.7893238 -1.2041056 -0.8834939 -0.7934103 -0.96125984][-3.9336762 -4.2593169 -4.2833486 -4.088954 -3.9411008 -3.5238662 -3.2029638 -2.8904972 -2.428422 -1.8120315 -1.044456 -0.78432393 -0.90080237 -0.89912462 -0.93567538][-3.7013297 -3.9902577 -3.912694 -3.451169 -2.8847914 -2.0432875 -1.5934286 -1.4499235 -1.085624 -0.46341777 0.16722679 0.091333866 -0.45892048 -0.62638354 -0.55331659][-3.1140943 -3.2657275 -3.0710816 -2.4964767 -1.7341752 -0.72502112 -0.33548498 -0.502264 -0.38670015 0.13419724 0.67934704 0.5766139 -0.0064473152 -0.19805622 -0.12724733][-3.1033816 -3.0108173 -2.7259238 -2.2272186 -1.504966 -0.51833224 -0.11881256 -0.3540926 -0.36862421 0.0052948 0.41064119 0.3740468 -0.036920071 -0.17285585 -0.15720034][-3.372046 -3.0788054 -2.7411489 -2.3058596 -1.5892677 -0.5630002 0.034337521 0.017786026 0.019494057 0.22097826 0.40131998 0.32804012 0.0091319084 -0.15613461 -0.24847269][-3.1060548 -2.6676126 -2.2116868 -1.6973863 -1.0021927 -0.13431835 0.43212986 0.4903841 0.42061138 0.44508553 0.50114155 0.43108368 0.1992197 0.052438259 -0.077353477][-1.4978669 -1.066865 -0.63296866 -0.13807058 0.33920765 0.70697832 0.89195681 0.78879786 0.59942484 0.55264235 0.59305191 0.56369734 0.466125 0.45696783 0.41750908][-0.087023258 0.16506624 0.45248938 0.86785936 1.1772814 1.2288465 1.2028708 1.0605865 0.87222338 0.8404727 0.89581156 0.88214445 0.88146973 0.98962879 1.0180531][-0.25151682 -0.11998224 0.1099658 0.54912519 0.94442797 1.0924845 1.1960421 1.2034402 1.0897141 1.0643973 1.1178689 1.1245456 1.1977787 1.359436 1.4017262]]...]
INFO - root - 2017-12-07 07:25:31.539901: step 17710, loss = 0.67, batch loss = 0.60 (5.8 examples/sec; 1.370 sec/batch; 119h:48m:53s remains)
INFO - root - 2017-12-07 07:25:44.851057: step 17720, loss = 0.84, batch loss = 0.76 (6.0 examples/sec; 1.344 sec/batch; 117h:30m:21s remains)
INFO - root - 2017-12-07 07:25:58.172774: step 17730, loss = 0.73, batch loss = 0.65 (5.9 examples/sec; 1.356 sec/batch; 118h:34m:15s remains)
INFO - root - 2017-12-07 07:26:11.712729: step 17740, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.324 sec/batch; 115h:46m:23s remains)
INFO - root - 2017-12-07 07:26:25.307611: step 17750, loss = 0.86, batch loss = 0.79 (5.9 examples/sec; 1.366 sec/batch; 119h:26m:14s remains)
INFO - root - 2017-12-07 07:26:38.743231: step 17760, loss = 1.02, batch loss = 0.95 (6.0 examples/sec; 1.325 sec/batch; 115h:52m:49s remains)
INFO - root - 2017-12-07 07:26:52.157143: step 17770, loss = 0.87, batch loss = 0.80 (6.0 examples/sec; 1.338 sec/batch; 116h:56m:42s remains)
INFO - root - 2017-12-07 07:27:05.275034: step 17780, loss = 0.73, batch loss = 0.66 (6.0 examples/sec; 1.344 sec/batch; 117h:28m:25s remains)
INFO - root - 2017-12-07 07:27:18.550405: step 17790, loss = 0.93, batch loss = 0.86 (6.0 examples/sec; 1.323 sec/batch; 115h:36m:52s remains)
INFO - root - 2017-12-07 07:27:32.045483: step 17800, loss = 0.68, batch loss = 0.61 (6.0 examples/sec; 1.333 sec/batch; 116h:31m:01s remains)
2017-12-07 07:27:33.054866: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.064362049 0.016390324 -0.059799671 -0.12296295 -0.18977833 -0.23513508 -0.28397465 -0.34793091 -0.37677908 -0.40348816 -0.44314265 -0.50933695 -0.58195043 -0.61844778 -0.64468408][0.21316576 0.1586442 0.075694084 -0.0061535835 -0.0961113 -0.14746428 -0.1679163 -0.19929028 -0.20340586 -0.20814705 -0.21055603 -0.22974348 -0.27505207 -0.31972408 -0.37315989][0.37488461 0.31708717 0.23505163 0.14640045 0.049804211 -0.0068588257 -0.032272816 -0.07201767 -0.08625412 -0.085261822 -0.058231831 -0.037038326 -0.041077614 -0.067460537 -0.12196684][0.44008303 0.38925171 0.3043828 0.20154333 0.094594 0.023571491 -0.014938354 -0.0591774 -0.0862484 -0.093582153 -0.056078434 -0.015022755 0.025504589 0.0390625 0.0029644966][0.2823348 0.24100065 0.15198326 0.026385784 -0.10457754 -0.21824837 -0.26935387 -0.27604103 -0.28578329 -0.28798437 -0.23760176 -0.16484022 -0.051207542 0.024794579 0.019769669][0.082603931 0.016146183 -0.095252514 -0.25092888 -0.42109919 -0.60817313 -0.69701815 -0.66764283 -0.6747849 -0.68871689 -0.62504053 -0.48894262 -0.24657631 -0.062028408 -0.0095720291][0.035476208 -0.051362514 -0.18373966 -0.36731386 -0.548146 -0.73864269 -0.78284454 -0.69250131 -0.74385476 -0.85499072 -0.86308694 -0.70766449 -0.36723614 -0.0907774 0.011023045][0.25827122 0.19561481 0.076329231 -0.11293077 -0.27770996 -0.39307404 -0.28564787 -0.091049194 -0.2146945 -0.51905322 -0.6987257 -0.60205722 -0.25864077 0.039820671 0.15132332][0.61425591 0.61063719 0.51766396 0.31477833 0.13624907 0.078091145 0.33136034 0.64542484 0.50989056 0.056573391 -0.26444435 -0.24211216 0.025686741 0.27305508 0.35654736][0.68227434 0.71932983 0.65558958 0.45113611 0.2627964 0.22881222 0.52018738 0.88169146 0.80672264 0.33501244 -0.01999712 -0.029139996 0.15930271 0.34161043 0.40542269][0.56248713 0.61002874 0.56971073 0.37716675 0.18768167 0.14485359 0.35893965 0.63841295 0.61938524 0.25110769 -0.035802841 -0.031700134 0.11571503 0.26434565 0.3435998][0.47255754 0.51496506 0.49352121 0.32614231 0.15773058 0.12978697 0.29250574 0.49233913 0.50645018 0.26504564 0.052426815 0.037667751 0.12178755 0.22627926 0.31656218][0.48502016 0.52087688 0.51552725 0.38638115 0.23739433 0.20919704 0.32802439 0.45210934 0.46427059 0.32378292 0.18789387 0.15462732 0.16873789 0.21821356 0.30044603][0.51655006 0.54820395 0.54906082 0.45146275 0.31890202 0.27586508 0.34133148 0.39705515 0.38452053 0.31089354 0.24539423 0.21461725 0.18195677 0.17782021 0.23539066][0.45300865 0.47658014 0.48482561 0.42918062 0.34583092 0.32611036 0.37285233 0.39810753 0.36923885 0.31209087 0.25435352 0.20304632 0.14099026 0.1084981 0.15091085]]...]
INFO - root - 2017-12-07 07:27:46.381609: step 17810, loss = 0.79, batch loss = 0.72 (5.9 examples/sec; 1.359 sec/batch; 118h:47m:03s remains)
INFO - root - 2017-12-07 07:27:59.780613: step 17820, loss = 0.72, batch loss = 0.65 (5.9 examples/sec; 1.346 sec/batch; 117h:40m:24s remains)
INFO - root - 2017-12-07 07:28:13.204047: step 17830, loss = 0.77, batch loss = 0.70 (6.1 examples/sec; 1.322 sec/batch; 115h:31m:20s remains)
INFO - root - 2017-12-07 07:28:26.426610: step 17840, loss = 0.89, batch loss = 0.82 (6.3 examples/sec; 1.270 sec/batch; 111h:02m:42s remains)
INFO - root - 2017-12-07 07:28:39.748460: step 17850, loss = 0.69, batch loss = 0.61 (5.9 examples/sec; 1.354 sec/batch; 118h:21m:43s remains)
INFO - root - 2017-12-07 07:28:53.152257: step 17860, loss = 0.71, batch loss = 0.63 (6.0 examples/sec; 1.341 sec/batch; 117h:10m:15s remains)
INFO - root - 2017-12-07 07:29:06.382349: step 17870, loss = 0.83, batch loss = 0.76 (6.2 examples/sec; 1.297 sec/batch; 113h:21m:38s remains)
INFO - root - 2017-12-07 07:29:19.534028: step 17880, loss = 0.69, batch loss = 0.62 (5.9 examples/sec; 1.361 sec/batch; 118h:57m:19s remains)
INFO - root - 2017-12-07 07:29:32.744774: step 17890, loss = 0.81, batch loss = 0.73 (5.9 examples/sec; 1.354 sec/batch; 118h:18m:15s remains)
INFO - root - 2017-12-07 07:29:46.113533: step 17900, loss = 0.91, batch loss = 0.84 (5.8 examples/sec; 1.388 sec/batch; 121h:20m:18s remains)
2017-12-07 07:29:47.003787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1339626 -3.1092873 -2.8689349 -2.5134377 -2.3049338 -2.2617073 -2.1916521 -2.074903 -2.0853853 -2.1068952 -2.188916 -2.302428 -2.4281523 -2.4337661 -2.3105927][-3.0050776 -2.8521907 -2.4104385 -1.8593559 -1.5598404 -1.5722833 -1.6504636 -1.6248205 -1.6240647 -1.651222 -1.7751839 -1.9883924 -2.1015174 -2.0038586 -1.7829983][-2.433358 -2.2555356 -1.8787529 -1.4748249 -1.3188443 -1.4432731 -1.585108 -1.4799025 -1.2794132 -1.1654174 -1.2412202 -1.4644265 -1.5733366 -1.4796457 -1.3271825][-2.0752287 -2.0026855 -1.8089676 -1.5866761 -1.576849 -1.7511687 -1.842598 -1.5875559 -1.205214 -1.0069938 -1.0725436 -1.2953978 -1.3712168 -1.2238517 -1.0837791][-1.3756115 -1.4917841 -1.532012 -1.4808283 -1.5769162 -1.7800794 -1.860882 -1.6085768 -1.2286794 -1.0226419 -1.03351 -1.1769869 -1.210243 -1.04603 -0.9246614][-0.48430347 -0.77025008 -0.9924705 -1.0701971 -1.2558639 -1.4935856 -1.6143143 -1.5328996 -1.349628 -1.1934814 -1.1259637 -1.1810944 -1.1977596 -1.0539269 -0.91485548][-0.27873755 -0.57696104 -0.75618029 -0.77091956 -0.94356012 -1.2026131 -1.3861609 -1.4916129 -1.5080099 -1.3357244 -1.0576019 -0.95218039 -0.9783814 -0.94570541 -0.84489083][-0.46425509 -0.81883621 -0.97225904 -0.93161011 -1.0624924 -1.2482576 -1.3647468 -1.506366 -1.6544509 -1.4440808 -0.95255685 -0.73328543 -0.81580877 -0.88730097 -0.85189795][-0.77131677 -1.0989504 -1.2235129 -1.1979053 -1.3669949 -1.554384 -1.5590589 -1.5728581 -1.6889572 -1.4899807 -0.9746604 -0.77208328 -0.82390523 -0.83140588 -0.83841348][-1.1798451 -1.4701624 -1.6148152 -1.6630418 -1.8446507 -2.0080714 -1.9304922 -1.8114674 -1.7909517 -1.539187 -1.0838561 -0.88375759 -0.76888752 -0.61404443 -0.65812373][-1.8234558 -2.0032761 -2.1605134 -2.2909265 -2.439827 -2.5355649 -2.4345665 -2.2694857 -2.1658878 -1.8770261 -1.4951203 -1.28972 -1.0357342 -0.80776429 -0.927022][-1.7263875 -1.8251455 -1.9603593 -2.1359479 -2.3059621 -2.4019527 -2.3668423 -2.2329051 -2.1129105 -1.9115076 -1.6971433 -1.4599435 -1.0468621 -0.75602889 -0.92526865][-0.93589091 -1.1531384 -1.3933942 -1.6309485 -1.8432896 -1.9825869 -1.9956658 -1.7994397 -1.580359 -1.5038221 -1.5205424 -1.3098598 -0.82093811 -0.5152092 -0.674072][-1.6959164 -1.922725 -2.1645129 -2.3661933 -2.5068846 -2.5778115 -2.5364242 -2.249886 -1.9518132 -1.9690311 -2.131382 -1.9737763 -1.5473762 -1.3403294 -1.4379399][-2.593632 -2.7046061 -2.849894 -2.9514413 -3.0059443 -3.0357115 -3.0336838 -2.8515959 -2.6872137 -2.8154981 -3.0030112 -2.8490233 -2.4753036 -2.3152115 -2.340158]]...]
INFO - root - 2017-12-07 07:30:00.450751: step 17910, loss = 0.95, batch loss = 0.88 (5.9 examples/sec; 1.365 sec/batch; 119h:18m:32s remains)
INFO - root - 2017-12-07 07:30:13.880582: step 17920, loss = 0.95, batch loss = 0.88 (6.0 examples/sec; 1.334 sec/batch; 116h:36m:19s remains)
INFO - root - 2017-12-07 07:30:27.314152: step 17930, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.337 sec/batch; 116h:49m:25s remains)
INFO - root - 2017-12-07 07:30:40.718316: step 17940, loss = 0.81, batch loss = 0.74 (6.0 examples/sec; 1.330 sec/batch; 116h:13m:06s remains)
INFO - root - 2017-12-07 07:30:54.187541: step 17950, loss = 0.74, batch loss = 0.67 (6.0 examples/sec; 1.340 sec/batch; 117h:06m:00s remains)
INFO - root - 2017-12-07 07:31:07.422272: step 17960, loss = 0.69, batch loss = 0.62 (6.1 examples/sec; 1.307 sec/batch; 114h:13m:58s remains)
INFO - root - 2017-12-07 07:31:20.633864: step 17970, loss = 0.81, batch loss = 0.73 (6.2 examples/sec; 1.295 sec/batch; 113h:07m:55s remains)
INFO - root - 2017-12-07 07:31:33.573614: step 17980, loss = 0.77, batch loss = 0.70 (6.2 examples/sec; 1.294 sec/batch; 113h:04m:22s remains)
INFO - root - 2017-12-07 07:31:46.683926: step 17990, loss = 0.86, batch loss = 0.79 (6.2 examples/sec; 1.295 sec/batch; 113h:10m:20s remains)
INFO - root - 2017-12-07 07:31:59.972431: step 18000, loss = 0.66, batch loss = 0.59 (6.0 examples/sec; 1.344 sec/batch; 117h:27m:06s remains)
2017-12-07 07:32:00.916560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1256106 -2.0280929 -1.8580265 -1.8187561 -1.9890795 -2.1345706 -2.2789338 -2.4887667 -2.7188525 -2.7770638 -2.6062808 -2.1954083 -1.8117101 -2.0711637 -2.9148049][-1.9867606 -1.7379589 -1.4556553 -1.4107943 -1.630079 -1.7934804 -1.9660568 -2.1427996 -2.3466408 -2.4569454 -2.3934424 -1.8843718 -1.2006037 -1.2900813 -2.2801836][-1.9742088 -1.6736975 -1.3954647 -1.3704047 -1.4640579 -1.3937671 -1.3389635 -1.3184562 -1.4810214 -1.7494276 -1.9143679 -1.4318914 -0.54792738 -0.44179654 -1.4280891][-2.2276373 -1.9600046 -1.7415564 -1.701993 -1.5981677 -1.1876855 -0.75301528 -0.35575867 -0.38978386 -0.84064746 -1.2770076 -0.968951 -0.11168194 0.066738605 -0.84649229][-2.7914379 -2.5481095 -2.3186214 -2.147305 -1.7459266 -0.96575952 -0.14270449 0.71548223 0.8704071 0.16639423 -0.60293245 -0.57500219 0.10819101 0.2660284 -0.5956955][-3.1457512 -2.7749338 -2.4153571 -2.1053319 -1.4800687 -0.37309504 0.88966179 2.1994395 2.4012341 1.3424616 0.25154877 -0.032834053 0.37975883 0.47452879 -0.36791611][-3.0578408 -2.4832997 -2.0098372 -1.7184939 -1.0048175 0.35032225 2.0529656 3.6748285 3.6941004 2.2452006 0.967947 0.50512886 0.72621155 0.82350683 0.034906864][-2.7049825 -2.1519997 -1.8355701 -1.7468026 -1.0713782 0.36503983 2.2682686 3.882576 3.7228498 2.1739764 0.98287153 0.52536011 0.68968439 0.87690496 0.23162603][-2.2902722 -2.0368109 -2.1383533 -2.3640041 -1.8472762 -0.51360774 1.3147559 2.7906895 2.7535653 1.472487 0.49468517 0.10073996 0.25464153 0.55837107 0.093613625][-1.5946448 -1.729845 -2.3757918 -2.9394059 -2.5949378 -1.3669553 0.32490396 1.6420431 1.7328839 0.71244812 -0.1736908 -0.59782529 -0.48148847 -0.11887407 -0.42173195][-1.0969667 -1.4487205 -2.3767626 -3.0945518 -2.8525119 -1.7428563 -0.25271797 0.84492254 0.90424585 0.022791862 -0.85374141 -1.2987909 -1.25635 -0.96911645 -1.1516569][-0.951679 -1.2755632 -2.0279081 -2.5228124 -2.2447698 -1.3793068 -0.30657816 0.33122158 0.20429611 -0.55367804 -1.3729923 -1.7984939 -1.8638763 -1.7776344 -1.9033217][-1.1847804 -1.434406 -1.8034663 -1.80356 -1.3513951 -0.77198744 -0.22917128 -0.062777996 -0.31006956 -0.89288163 -1.6198444 -2.0046377 -2.2100935 -2.3476384 -2.4423766][-1.7856865 -1.9018309 -1.9551129 -1.6686194 -1.2057104 -0.81776929 -0.5422287 -0.56866622 -0.73711133 -1.1654172 -1.8124197 -2.121243 -2.379329 -2.6015911 -2.6316814][-2.3282702 -2.1990366 -2.0864012 -1.8953421 -1.6983299 -1.463484 -1.2138841 -1.1542273 -1.1501496 -1.4932795 -2.0750601 -2.2918282 -2.5745301 -2.842483 -2.818012]]...]
INFO - root - 2017-12-07 07:32:14.172139: step 18010, loss = 0.90, batch loss = 0.83 (6.3 examples/sec; 1.262 sec/batch; 110h:17m:00s remains)
INFO - root - 2017-12-07 07:32:27.355159: step 18020, loss = 0.93, batch loss = 0.86 (6.3 examples/sec; 1.280 sec/batch; 111h:47m:10s remains)
INFO - root - 2017-12-07 07:32:40.671020: step 18030, loss = 0.77, batch loss = 0.70 (6.3 examples/sec; 1.273 sec/batch; 111h:14m:36s remains)
INFO - root - 2017-12-07 07:32:54.087649: step 18040, loss = 1.16, batch loss = 1.08 (6.0 examples/sec; 1.329 sec/batch; 116h:03m:15s remains)
INFO - root - 2017-12-07 07:33:07.356244: step 18050, loss = 0.74, batch loss = 0.67 (5.9 examples/sec; 1.352 sec/batch; 118h:03m:41s remains)
INFO - root - 2017-12-07 07:33:20.491792: step 18060, loss = 0.89, batch loss = 0.82 (6.0 examples/sec; 1.344 sec/batch; 117h:24m:33s remains)
INFO - root - 2017-12-07 07:33:33.856392: step 18070, loss = 0.95, batch loss = 0.88 (5.8 examples/sec; 1.373 sec/batch; 119h:57m:16s remains)
INFO - root - 2017-12-07 07:33:46.895519: step 18080, loss = 0.78, batch loss = 0.71 (6.2 examples/sec; 1.290 sec/batch; 112h:41m:57s remains)
INFO - root - 2017-12-07 07:34:00.103192: step 18090, loss = 0.78, batch loss = 0.71 (6.1 examples/sec; 1.310 sec/batch; 114h:23m:44s remains)
INFO - root - 2017-12-07 07:34:13.356011: step 18100, loss = 0.87, batch loss = 0.80 (6.0 examples/sec; 1.337 sec/batch; 116h:43m:49s remains)
2017-12-07 07:34:14.353307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9238067 -2.9262643 -2.7503412 -2.5880365 -2.6748328 -2.8860345 -3.0478189 -3.0740156 -3.0587409 -3.1071784 -3.1298923 -3.0589852 -2.8793483 -2.7504191 -2.736527][-2.5547051 -2.5898871 -2.3965364 -2.1589749 -2.1483319 -2.2593014 -2.3925266 -2.4020078 -2.3137679 -2.2880969 -2.315799 -2.3676429 -2.341624 -2.2392037 -2.1752996][-2.414305 -2.5857518 -2.5636692 -2.3845668 -2.3213542 -2.3997457 -2.5496082 -2.5225723 -2.2793479 -2.1057174 -2.1571956 -2.3913529 -2.5207114 -2.3308353 -2.0112488][-1.9652848 -2.1990149 -2.2653821 -2.1057966 -2.0122292 -2.1891472 -2.4633808 -2.4572968 -2.0936184 -1.8152087 -1.9048412 -2.3222854 -2.6898165 -2.6575189 -2.3730478][-2.127845 -2.2718503 -2.2747748 -2.0358381 -1.8927073 -2.2057519 -2.6379247 -2.6796312 -2.2342322 -1.8015049 -1.7483702 -2.0669172 -2.4533887 -2.5987542 -2.6031327][-2.6354675 -2.6871164 -2.6354628 -2.3150029 -2.082967 -2.4417522 -3.0067422 -3.1969547 -2.8647418 -2.3844528 -2.1142855 -2.0440066 -2.0642996 -2.0484807 -2.1305861][-2.22465 -2.2098475 -2.0893724 -1.6254759 -1.1820197 -1.4215615 -2.0789769 -2.5403061 -2.5332613 -2.1365931 -1.6548867 -1.2203186 -0.91642642 -0.7161572 -0.75679946][-1.7363153 -1.8452561 -1.8373406 -1.339148 -0.64142728 -0.62115979 -1.2565093 -1.9393117 -2.2595775 -2.0222139 -1.44291 -0.85221076 -0.38299942 -0.037466049 0.13195229][-1.1781697 -1.4105315 -1.6148851 -1.2457218 -0.47181749 -0.24259567 -0.725549 -1.5012703 -2.1077011 -2.1254702 -1.6819315 -1.2450125 -0.842154 -0.43698978 -0.071035385][-1.5438783 -1.7443745 -2.0889647 -1.9269714 -1.3127351 -0.99676085 -1.1807423 -1.7304168 -2.2771506 -2.3409293 -2.0197122 -1.8063345 -1.6238272 -1.358393 -1.0122619][-2.3629398 -2.4129057 -2.7897763 -2.8412595 -2.523644 -2.2634907 -2.1783845 -2.3928406 -2.648479 -2.5580826 -2.2408352 -2.1687193 -2.2633643 -2.2996602 -2.200515][-2.1520185 -2.0144107 -2.2625744 -2.3755238 -2.2907639 -2.1696453 -2.0181537 -2.112345 -2.2355709 -2.0940936 -1.8212204 -1.7471678 -1.8583581 -1.9578416 -2.0018427][-1.8506231 -1.6351204 -1.7341738 -1.851347 -1.9673121 -2.0418298 -1.9388301 -1.9493639 -1.9490945 -1.7820315 -1.5817735 -1.4537156 -1.3669212 -1.2249234 -1.132118][-1.6416557 -1.3592718 -1.2631881 -1.301615 -1.5227568 -1.7593036 -1.7338703 -1.6356876 -1.4491136 -1.1902151 -1.0729258 -1.0502439 -0.97909808 -0.73989987 -0.49732494][-1.5223234 -1.2537951 -1.0432458 -0.99444604 -1.2123239 -1.5598304 -1.6903098 -1.6862805 -1.5480101 -1.3258557 -1.2827072 -1.3548434 -1.3624492 -1.1717486 -0.9278791]]...]
INFO - root - 2017-12-07 07:34:27.811728: step 18110, loss = 0.69, batch loss = 0.62 (5.9 examples/sec; 1.359 sec/batch; 118h:42m:08s remains)
INFO - root - 2017-12-07 07:34:41.192222: step 18120, loss = 0.98, batch loss = 0.91 (5.8 examples/sec; 1.368 sec/batch; 119h:29m:04s remains)
INFO - root - 2017-12-07 07:34:54.375941: step 18130, loss = 0.67, batch loss = 0.59 (6.2 examples/sec; 1.285 sec/batch; 112h:10m:34s remains)
INFO - root - 2017-12-07 07:35:07.764343: step 18140, loss = 0.88, batch loss = 0.81 (5.8 examples/sec; 1.372 sec/batch; 119h:48m:10s remains)
INFO - root - 2017-12-07 07:35:21.149342: step 18150, loss = 0.62, batch loss = 0.55 (5.9 examples/sec; 1.363 sec/batch; 118h:59m:16s remains)
INFO - root - 2017-12-07 07:35:34.521104: step 18160, loss = 0.86, batch loss = 0.79 (6.0 examples/sec; 1.330 sec/batch; 116h:07m:58s remains)
INFO - root - 2017-12-07 07:35:47.973684: step 18170, loss = 0.84, batch loss = 0.77 (6.1 examples/sec; 1.319 sec/batch; 115h:07m:49s remains)
INFO - root - 2017-12-07 07:36:01.110741: step 18180, loss = 0.73, batch loss = 0.66 (6.1 examples/sec; 1.319 sec/batch; 115h:08m:33s remains)
INFO - root - 2017-12-07 07:36:14.413712: step 18190, loss = 0.75, batch loss = 0.68 (6.0 examples/sec; 1.332 sec/batch; 116h:15m:09s remains)
INFO - root - 2017-12-07 07:36:27.775428: step 18200, loss = 0.90, batch loss = 0.83 (6.1 examples/sec; 1.312 sec/batch; 114h:33m:43s remains)
2017-12-07 07:36:28.711283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9635534 -3.6966841 -3.5760486 -3.7202382 -3.9787912 -4.2070827 -4.3657794 -4.3760667 -4.3025622 -4.2009521 -4.0557442 -3.9333198 -3.7845986 -3.6064703 -3.536104][-3.9507868 -3.4574671 -3.1392093 -3.1989164 -3.4466805 -3.8040814 -4.1985722 -4.4016285 -4.3779216 -4.186904 -3.9255929 -3.8456252 -3.8521876 -3.7445641 -3.6723294][-3.6980126 -2.9055107 -2.4404786 -2.5419464 -2.81747 -3.2705517 -3.8420181 -4.1688242 -4.12617 -3.8082352 -3.4610493 -3.5359433 -3.8633053 -3.9814854 -3.9464817][-3.2340002 -1.9529445 -1.2968659 -1.5771382 -2.0203426 -2.5703201 -3.2209432 -3.5755467 -3.4692235 -3.0689635 -2.7053671 -2.916127 -3.5212519 -3.8657165 -3.8986475][-2.6159744 -0.83872032 0.010993958 -0.57429457 -1.3697643 -2.0036697 -2.5007796 -2.5409238 -2.1145291 -1.6606467 -1.5169904 -2.1077898 -3.1028342 -3.7235179 -3.8741262][-2.1644781 -0.10744667 0.89587593 0.039847374 -1.163203 -1.8824737 -2.058594 -1.5075028 -0.54322839 0.0092029572 -0.14707041 -1.242924 -2.729578 -3.6836638 -4.006906][-2.1347933 -0.016845226 1.0677109 0.097349644 -1.3466296 -2.0481074 -1.8348069 -0.67023754 0.76703787 1.3141117 0.87623215 -0.56659865 -2.3403513 -3.4564393 -3.8535328][-2.4136968 -0.45491791 0.60443544 -0.28164244 -1.7324178 -2.312232 -1.7268469 -0.027694702 1.738224 2.1721258 1.4585567 -0.10583448 -1.9690163 -3.1689806 -3.6027572][-2.5283575 -0.8297472 0.13430166 -0.56218815 -1.8655486 -2.3172903 -1.5419943 0.40849113 2.3032951 2.529932 1.5830545 0.099378586 -1.6144788 -2.785804 -3.2327719][-2.7376184 -1.3172607 -0.42226267 -0.80916715 -1.7824183 -2.1326492 -1.4365315 0.40418196 2.236896 2.2664905 1.1170049 -0.18736839 -1.4907668 -2.3686059 -2.6602187][-3.236043 -2.0915644 -1.2737508 -1.371136 -1.9805374 -2.2136452 -1.6578841 -0.10957336 1.528533 1.4781327 0.28320551 -0.73759174 -1.4743943 -1.9108825 -1.9666522][-3.5474281 -2.5920982 -1.8565683 -1.7944603 -2.1734469 -2.3636625 -2.000896 -0.79222965 0.605628 0.64917946 -0.39048004 -1.1289113 -1.3809311 -1.4244778 -1.269191][-3.624428 -2.9172888 -2.3791316 -2.2787971 -2.5273335 -2.70863 -2.5091925 -1.6658485 -0.65807533 -0.60933781 -1.4787045 -2.0167928 -1.9197814 -1.6551027 -1.3083255][-3.4567013 -3.0444198 -2.7862077 -2.7743177 -2.9794655 -3.0997076 -2.9085131 -2.3594582 -1.838311 -2.0012288 -2.76201 -3.0844071 -2.6971052 -2.1381466 -1.6162424][-3.2979398 -3.0160003 -2.890676 -2.930999 -3.1117527 -3.2171428 -3.0376711 -2.6986723 -2.5554829 -2.8906791 -3.5296304 -3.6487703 -3.0456095 -2.2896154 -1.6842675]]...]
INFO - root - 2017-12-07 07:36:42.117632: step 18210, loss = 0.87, batch loss = 0.80 (6.0 examples/sec; 1.339 sec/batch; 116h:55m:56s remains)
INFO - root - 2017-12-07 07:36:55.358667: step 18220, loss = 0.88, batch loss = 0.81 (6.1 examples/sec; 1.301 sec/batch; 113h:34m:43s remains)
INFO - root - 2017-12-07 07:37:08.707390: step 18230, loss = 0.73, batch loss = 0.66 (6.2 examples/sec; 1.300 sec/batch; 113h:30m:15s remains)
INFO - root - 2017-12-07 07:37:22.082689: step 18240, loss = 0.86, batch loss = 0.78 (5.9 examples/sec; 1.345 sec/batch; 117h:23m:01s remains)
INFO - root - 2017-12-07 07:37:35.354828: step 18250, loss = 0.89, batch loss = 0.82 (6.0 examples/sec; 1.327 sec/batch; 115h:52m:29s remains)
INFO - root - 2017-12-07 07:37:48.740476: step 18260, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.340 sec/batch; 116h:59m:30s remains)
INFO - root - 2017-12-07 07:38:02.138720: step 18270, loss = 0.79, batch loss = 0.72 (6.0 examples/sec; 1.330 sec/batch; 116h:07m:41s remains)
INFO - root - 2017-12-07 07:38:15.214837: step 18280, loss = 0.65, batch loss = 0.58 (6.2 examples/sec; 1.288 sec/batch; 112h:26m:05s remains)
INFO - root - 2017-12-07 07:38:28.428285: step 18290, loss = 0.84, batch loss = 0.77 (6.0 examples/sec; 1.327 sec/batch; 115h:50m:58s remains)
INFO - root - 2017-12-07 07:38:41.869490: step 18300, loss = 1.06, batch loss = 0.99 (5.8 examples/sec; 1.387 sec/batch; 121h:03m:07s remains)
2017-12-07 07:38:42.844359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0545206 -3.8259425 -3.531812 -3.3113959 -3.1464839 -3.1831489 -3.4631131 -3.7838688 -4.11236 -4.3872185 -4.4981127 -4.465064 -4.3744149 -4.2848234 -4.1687613][-3.9923148 -3.5637007 -3.0805101 -2.6645291 -2.2599728 -2.1440277 -2.4103332 -2.8650017 -3.4377789 -3.9618325 -4.2707667 -4.3795018 -4.3622961 -4.2842402 -4.1695156][-3.8559418 -3.3563461 -2.872858 -2.4247012 -1.9065676 -1.6248562 -1.7055364 -2.063385 -2.6650288 -3.2685752 -3.6783664 -3.9649413 -4.1357107 -4.170898 -4.1185751][-3.5878031 -3.1034141 -2.7750378 -2.4917541 -2.1037579 -1.8321462 -1.7211852 -1.7964451 -2.1384258 -2.5327969 -2.8243032 -3.1860261 -3.5601132 -3.7825494 -3.8829923][-3.3268113 -2.834471 -2.6247988 -2.4430246 -2.1270349 -1.8315332 -1.5916188 -1.4819012 -1.620158 -1.8315814 -2.0021443 -2.3365324 -2.8161521 -3.2032962 -3.4930701][-3.2487292 -2.760643 -2.5095313 -2.2446156 -1.8142362 -1.3075607 -0.87290263 -0.65479803 -0.80165339 -1.092128 -1.32483 -1.6701565 -2.2023113 -2.7104182 -3.1467435][-3.2519107 -2.7609777 -2.3176901 -1.910409 -1.364367 -0.59582162 0.18211317 0.65479755 0.47805071 -0.027380466 -0.48002648 -0.94682 -1.5556066 -2.1822627 -2.7465987][-3.41069 -2.9585583 -2.3356779 -1.8188889 -1.2356751 -0.32514381 0.68160009 1.3390579 1.1989422 0.67017937 0.15062809 -0.38570833 -1.0483699 -1.7721052 -2.4405289][-3.9130743 -3.6154726 -3.0535116 -2.6185508 -2.1580529 -1.3904123 -0.59180593 -0.11051464 -0.13220119 -0.38037205 -0.69810152 -1.1162078 -1.6224747 -2.1472201 -2.6128321][-4.4275393 -4.3478069 -4.029192 -3.7984176 -3.5262842 -3.041249 -2.6049824 -2.3901038 -2.3502553 -2.3284569 -2.3909523 -2.5858912 -2.822103 -3.0015442 -3.1137323][-4.557579 -4.6297 -4.5585055 -4.5053177 -4.3391185 -4.0455995 -3.8756604 -3.8685479 -3.8305943 -3.6996617 -3.6579142 -3.6976895 -3.7510848 -3.7197368 -3.6067762][-4.30869 -4.3718462 -4.4173021 -4.473978 -4.3724885 -4.1671042 -4.0710907 -4.1050749 -4.082283 -4.0005069 -3.9809718 -3.9666452 -3.9603491 -3.9112363 -3.7974138][-4.1023655 -4.118825 -4.1574273 -4.2219377 -4.1665969 -4.0563283 -4.0052567 -4.0166407 -3.9933646 -3.9370918 -3.9285269 -3.8855965 -3.8430364 -3.8082952 -3.7439561][-3.9680102 -4.0304651 -4.0994158 -4.1786304 -4.1823559 -4.1540356 -4.1299181 -4.1173015 -4.0875177 -4.03664 -3.9904318 -3.8925586 -3.7879078 -3.7169981 -3.6451559][-3.6402419 -3.6835575 -3.7494168 -3.8223777 -3.8627281 -3.8784025 -3.8649461 -3.8408577 -3.8174026 -3.7990494 -3.7758248 -3.7098963 -3.6299152 -3.5741673 -3.5257037]]...]
INFO - root - 2017-12-07 07:38:56.184639: step 18310, loss = 0.62, batch loss = 0.55 (6.1 examples/sec; 1.309 sec/batch; 114h:14m:16s remains)
INFO - root - 2017-12-07 07:39:09.530512: step 18320, loss = 0.95, batch loss = 0.88 (6.0 examples/sec; 1.333 sec/batch; 116h:18m:19s remains)
INFO - root - 2017-12-07 07:39:22.998158: step 18330, loss = 0.73, batch loss = 0.65 (5.9 examples/sec; 1.357 sec/batch; 118h:24m:34s remains)
INFO - root - 2017-12-07 07:39:36.377006: step 18340, loss = 0.89, batch loss = 0.82 (5.9 examples/sec; 1.349 sec/batch; 117h:42m:23s remains)
INFO - root - 2017-12-07 07:39:49.726646: step 18350, loss = 0.83, batch loss = 0.75 (5.9 examples/sec; 1.353 sec/batch; 118h:05m:30s remains)
INFO - root - 2017-12-07 07:40:02.997971: step 18360, loss = 0.71, batch loss = 0.64 (6.0 examples/sec; 1.335 sec/batch; 116h:27m:34s remains)
INFO - root - 2017-12-07 07:40:16.328161: step 18370, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.331 sec/batch; 116h:08m:30s remains)
INFO - root - 2017-12-07 07:40:29.416678: step 18380, loss = 0.85, batch loss = 0.78 (6.1 examples/sec; 1.315 sec/batch; 114h:42m:26s remains)
INFO - root - 2017-12-07 07:40:42.937959: step 18390, loss = 0.94, batch loss = 0.87 (5.9 examples/sec; 1.354 sec/batch; 118h:09m:12s remains)
INFO - root - 2017-12-07 07:40:56.390914: step 18400, loss = 0.97, batch loss = 0.90 (5.9 examples/sec; 1.349 sec/batch; 117h:40m:05s remains)
2017-12-07 07:40:57.330305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9428217 -1.9660554 -2.2794316 -2.5449691 -2.5933046 -2.4068902 -2.063904 -1.70732 -1.495013 -1.6100791 -1.8390212 -2.1299114 -2.5179062 -2.7838988 -2.7820246][-2.0279663 -1.9967337 -2.1570208 -2.2724123 -2.2925687 -2.2196686 -1.9936302 -1.8052809 -1.7586844 -1.9001987 -2.1094031 -2.39178 -2.7304587 -2.8723552 -2.8061862][-2.1422544 -2.1034651 -2.1082771 -2.0205877 -1.972055 -1.9787402 -1.8694441 -1.8423278 -1.9561141 -2.1096663 -2.2265682 -2.3920643 -2.6471491 -2.7768307 -2.7493935][-2.0839152 -2.0742955 -2.0689683 -2.0189834 -2.0563486 -2.1073353 -1.9835203 -1.9304729 -2.0297143 -2.1290359 -2.1845193 -2.268662 -2.4529543 -2.5947104 -2.6080718][-1.9801834 -1.9630404 -2.0004172 -2.1369805 -2.3808289 -2.5479605 -2.4746909 -2.4166946 -2.3495705 -2.171659 -2.0012472 -1.9262755 -2.0385377 -2.2442074 -2.3611584][-1.8317959 -1.8144972 -1.9390392 -2.262356 -2.6588542 -2.9497769 -3.0041037 -3.050436 -2.8839631 -2.4513149 -1.9732625 -1.6522815 -1.6195285 -1.8732097 -2.1069682][-1.719203 -1.7315438 -1.9558454 -2.3422198 -2.7408247 -3.0788679 -3.2506289 -3.3824508 -3.1931446 -2.6936479 -2.162446 -1.7667909 -1.6306527 -1.8438289 -2.0835769][-1.7723963 -1.8179123 -2.0599673 -2.3645039 -2.6410718 -2.9544392 -3.2071438 -3.3180928 -3.0299602 -2.5219669 -2.1273053 -1.8774784 -1.8356824 -2.0596387 -2.2512915][-1.6844201 -1.6663966 -1.8006644 -1.9717336 -2.1282458 -2.4150677 -2.787992 -2.9283285 -2.5838261 -2.0926437 -1.7779772 -1.6272929 -1.6851518 -1.9828343 -2.2051845][-1.3345962 -1.2720513 -1.3317964 -1.4596336 -1.5667028 -1.7698059 -2.1080656 -2.241266 -2.0159905 -1.7306595 -1.5133626 -1.3810291 -1.4401479 -1.7533381 -2.0288224][-1.1702678 -1.1627638 -1.2320974 -1.3590117 -1.4203799 -1.5138547 -1.7483981 -1.8552477 -1.8208895 -1.7813015 -1.6534071 -1.5564973 -1.6534455 -1.9536591 -2.2158773][-1.5642502 -1.5345328 -1.5303283 -1.5554178 -1.5140727 -1.5405676 -1.7514477 -1.9616156 -2.1286058 -2.2327094 -2.118674 -2.0041602 -2.1180623 -2.4211533 -2.6445189][-2.0481257 -1.9961383 -1.9770689 -1.9688032 -1.9077299 -1.9242117 -2.103723 -2.3520551 -2.5759349 -2.657414 -2.4771616 -2.2856629 -2.3754377 -2.6967025 -2.88904][-2.3765142 -2.3223848 -2.3348517 -2.3849883 -2.4118581 -2.4699216 -2.6032612 -2.803514 -2.9753797 -2.9906402 -2.7766094 -2.535008 -2.6106186 -2.936986 -3.0511923][-2.6936064 -2.6578197 -2.6755919 -2.733568 -2.8105969 -2.903564 -3.0276241 -3.207813 -3.355396 -3.3503218 -3.1325631 -2.8801053 -2.945282 -3.234561 -3.2565958]]...]
INFO - root - 2017-12-07 07:41:10.607386: step 18410, loss = 0.68, batch loss = 0.60 (6.0 examples/sec; 1.341 sec/batch; 117h:01m:25s remains)
INFO - root - 2017-12-07 07:41:23.968311: step 18420, loss = 0.76, batch loss = 0.69 (6.0 examples/sec; 1.332 sec/batch; 116h:12m:50s remains)
INFO - root - 2017-12-07 07:41:37.307809: step 18430, loss = 0.85, batch loss = 0.78 (5.9 examples/sec; 1.358 sec/batch; 118h:30m:50s remains)
INFO - root - 2017-12-07 07:41:50.669920: step 18440, loss = 0.88, batch loss = 0.81 (5.9 examples/sec; 1.358 sec/batch; 118h:26m:13s remains)
INFO - root - 2017-12-07 07:42:04.154114: step 18450, loss = 0.80, batch loss = 0.73 (5.9 examples/sec; 1.363 sec/batch; 118h:54m:45s remains)
INFO - root - 2017-12-07 07:42:17.330308: step 18460, loss = 0.75, batch loss = 0.67 (5.9 examples/sec; 1.359 sec/batch; 118h:30m:48s remains)
INFO - root - 2017-12-07 07:42:30.752452: step 18470, loss = 0.84, batch loss = 0.77 (6.0 examples/sec; 1.324 sec/batch; 115h:29m:04s remains)
INFO - root - 2017-12-07 07:42:44.044717: step 18480, loss = 0.97, batch loss = 0.90 (6.0 examples/sec; 1.339 sec/batch; 116h:50m:18s remains)
INFO - root - 2017-12-07 07:42:57.563586: step 18490, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.333 sec/batch; 116h:18m:44s remains)
INFO - root - 2017-12-07 07:43:11.098023: step 18500, loss = 0.63, batch loss = 0.56 (6.0 examples/sec; 1.323 sec/batch; 115h:23m:27s remains)
2017-12-07 07:43:12.084286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5358696 -2.5594501 -2.5826659 -2.6032553 -2.615839 -2.6177516 -2.6106977 -2.5996389 -2.5881107 -2.5787733 -2.5721519 -2.5578384 -2.557909 -2.6269617 -2.8080339][-2.5637431 -2.6085486 -2.6496964 -2.6823294 -2.6993041 -2.6957762 -2.6761367 -2.6496897 -2.6223209 -2.5985956 -2.5781863 -2.5541525 -2.5483646 -2.6156487 -2.7977805][-2.6198378 -2.6883647 -2.7415748 -2.7772918 -2.7917755 -2.7797623 -2.7488208 -2.7113209 -2.6738627 -2.6366224 -2.5971122 -2.5552948 -2.53576 -2.5948925 -2.7746348][-2.6811204 -2.7739592 -2.8363905 -2.868751 -2.8747072 -2.8555984 -2.8200874 -2.77815 -2.7352762 -2.6855533 -2.6224751 -2.5571928 -2.518728 -2.5653615 -2.7393994][-2.7256255 -2.8452926 -2.9238787 -2.9539478 -2.9455066 -2.9161391 -2.875283 -2.8289788 -2.777945 -2.7146957 -2.6295829 -2.5439873 -2.4920118 -2.5305345 -2.7022858][-2.7460752 -2.8954177 -2.9973915 -3.0269141 -2.9961815 -2.9450779 -2.8934102 -2.8379667 -2.7762189 -2.6999257 -2.6008089 -2.5049577 -2.4489679 -2.4894783 -2.6677165][-2.7416196 -2.9117088 -3.0339608 -3.0651202 -3.0144696 -2.934258 -2.8643661 -2.7998784 -2.7312934 -2.6470327 -2.5431337 -2.4482086 -2.3981445 -2.4503226 -2.6414504][-2.7183933 -2.8858025 -3.0123806 -3.0476303 -2.9892743 -2.8868608 -2.7955556 -2.7270098 -2.6625967 -2.5811291 -2.4837151 -2.3987141 -2.3621087 -2.4309661 -2.6362042][-2.6834543 -2.8282433 -2.9448619 -2.9861546 -2.9389191 -2.8373847 -2.733953 -2.6600146 -2.5981488 -2.5242491 -2.4392185 -2.3680842 -2.3491404 -2.4366355 -2.6565938][-2.6213076 -2.7363167 -2.8355193 -2.8830433 -2.8632536 -2.7911863 -2.6997437 -2.6183083 -2.5499909 -2.4796174 -2.4059567 -2.3467703 -2.3450897 -2.4539595 -2.6920469][-2.5287833 -2.615139 -2.6953683 -2.7470932 -2.7573543 -2.7262745 -2.6640978 -2.5837767 -2.5063031 -2.435637 -2.370055 -2.3189454 -2.3311203 -2.4647231 -2.7280231][-2.4454904 -2.5025594 -2.5618858 -2.6105814 -2.6410456 -2.6441889 -2.6103024 -2.5393834 -2.4565554 -2.3840351 -2.3247313 -2.284873 -2.3161869 -2.4811969 -2.7801452][-2.4166441 -2.4429004 -2.4743977 -2.5063028 -2.5340705 -2.5485749 -2.5302205 -2.4712553 -2.3921716 -2.3215044 -2.2712541 -2.2513583 -2.3094633 -2.5100553 -2.8453417][-2.4271355 -2.42862 -2.4316339 -2.4371116 -2.4434605 -2.4469881 -2.4289365 -2.378969 -2.3113885 -2.2504303 -2.2114623 -2.2157757 -2.304812 -2.5358696 -2.8962808][-2.4590626 -2.4392433 -2.4156425 -2.3941915 -2.3719852 -2.3526483 -2.3246315 -2.28074 -2.2286975 -2.1810205 -2.1540172 -2.1827884 -2.3045857 -2.5594783 -2.9293892]]...]
INFO - root - 2017-12-07 07:43:25.517760: step 18510, loss = 0.73, batch loss = 0.66 (6.0 examples/sec; 1.329 sec/batch; 115h:56m:27s remains)
INFO - root - 2017-12-07 07:43:38.943206: step 18520, loss = 0.70, batch loss = 0.63 (5.9 examples/sec; 1.360 sec/batch; 118h:35m:54s remains)
INFO - root - 2017-12-07 07:43:52.224943: step 18530, loss = 0.79, batch loss = 0.72 (6.0 examples/sec; 1.330 sec/batch; 115h:58m:44s remains)
INFO - root - 2017-12-07 07:44:05.533848: step 18540, loss = 1.01, batch loss = 0.94 (6.1 examples/sec; 1.313 sec/batch; 114h:28m:27s remains)
INFO - root - 2017-12-07 07:44:18.963393: step 18550, loss = 0.70, batch loss = 0.63 (5.9 examples/sec; 1.351 sec/batch; 117h:47m:44s remains)
INFO - root - 2017-12-07 07:44:32.379894: step 18560, loss = 0.73, batch loss = 0.66 (5.7 examples/sec; 1.396 sec/batch; 121h:45m:58s remains)
INFO - root - 2017-12-07 07:44:45.668088: step 18570, loss = 0.70, batch loss = 0.63 (5.9 examples/sec; 1.353 sec/batch; 118h:01m:19s remains)
INFO - root - 2017-12-07 07:44:58.839993: step 18580, loss = 0.70, batch loss = 0.63 (6.2 examples/sec; 1.286 sec/batch; 112h:07m:22s remains)
INFO - root - 2017-12-07 07:45:12.257463: step 18590, loss = 0.79, batch loss = 0.72 (6.0 examples/sec; 1.340 sec/batch; 116h:51m:03s remains)
INFO - root - 2017-12-07 07:45:25.537855: step 18600, loss = 0.75, batch loss = 0.68 (6.2 examples/sec; 1.293 sec/batch; 112h:43m:21s remains)
2017-12-07 07:45:26.515489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.70096 -3.6936731 -3.5278785 -3.2192512 -2.9438941 -2.8803658 -3.020927 -3.1286573 -3.1837919 -3.3390536 -3.5466368 -3.72412 -3.7954566 -3.8501606 -3.9345536][-3.4561181 -3.4533882 -3.2031944 -2.720933 -2.2839694 -2.1615651 -2.3852346 -2.609417 -2.7384009 -2.9626408 -3.2132788 -3.391638 -3.4472356 -3.493808 -3.6141446][-3.0690145 -3.0690022 -2.7257261 -2.0732243 -1.5111194 -1.3632405 -1.6941316 -2.0506077 -2.2125802 -2.4049778 -2.5928369 -2.7381377 -2.8305531 -2.9536707 -3.1754055][-3.0055346 -2.9941585 -2.5856471 -1.8140397 -1.1816792 -1.0061336 -1.3794677 -1.7752411 -1.8411486 -1.8937163 -1.9567723 -2.0543435 -2.212831 -2.4831848 -2.8350172][-3.3063765 -3.2240725 -2.7517867 -1.9128718 -1.2941561 -1.1416368 -1.4999793 -1.8541813 -1.7381208 -1.6287811 -1.6445062 -1.7926571 -2.0957546 -2.559463 -3.0177691][-3.5409243 -3.3804245 -2.8896325 -2.0412436 -1.4668322 -1.3493435 -1.6501856 -1.8903968 -1.5806639 -1.3483806 -1.404067 -1.6713097 -2.1811662 -2.8796525 -3.4383163][-3.4012957 -3.1845772 -2.7243652 -1.9235115 -1.4019454 -1.3091512 -1.4967196 -1.5879183 -1.1550493 -0.90907955 -1.1350968 -1.6252015 -2.3992269 -3.2828007 -3.838733][-2.8111765 -2.5813341 -2.2213788 -1.5623484 -1.1214778 -1.0447683 -1.0665984 -0.95760131 -0.5227468 -0.4569602 -0.99178052 -1.7583735 -2.7314949 -3.6475019 -4.07366][-2.1383193 -1.9382536 -1.7093518 -1.2274797 -0.87507105 -0.78825951 -0.64555883 -0.34502172 -0.0030355453 -0.16749191 -0.93055534 -1.8032207 -2.7260079 -3.4546487 -3.7102246][-1.7483547 -1.6158991 -1.4891167 -1.1155515 -0.81516051 -0.73905587 -0.51841521 -0.14592028 0.033804417 -0.32204819 -1.1365631 -1.9360726 -2.6661124 -3.1505511 -3.2782381][-1.5824945 -1.6119065 -1.6516759 -1.378278 -1.1067965 -1.0088048 -0.77441335 -0.47801328 -0.48284578 -0.96321726 -1.644809 -2.2168849 -2.6724877 -2.9280329 -2.9873319][-1.744272 -1.9543376 -2.143981 -1.9599588 -1.7126789 -1.5597806 -1.3728478 -1.2664812 -1.4908407 -2.0185175 -2.4575038 -2.7658567 -2.9631248 -2.9723136 -2.9349978][-2.1239054 -2.4558542 -2.7160969 -2.5682325 -2.3735731 -2.2512448 -2.1985257 -2.3005228 -2.7081153 -3.2172141 -3.4576545 -3.6168246 -3.6688972 -3.5257304 -3.4237452][-2.4590931 -2.85528 -3.1172729 -3.0215836 -2.9804344 -3.0290091 -3.1557493 -3.4131188 -3.9260831 -4.397522 -4.52432 -4.5948009 -4.5594325 -4.3386111 -4.1855717][-2.9220724 -3.3547125 -3.5989473 -3.5520384 -3.631474 -3.8268394 -4.0730572 -4.386817 -4.9233103 -5.3586841 -5.4486156 -5.4514647 -5.3361635 -5.1036515 -4.9582453]]...]
INFO - root - 2017-12-07 07:45:39.777476: step 18610, loss = 0.94, batch loss = 0.87 (6.0 examples/sec; 1.338 sec/batch; 116h:37m:31s remains)
INFO - root - 2017-12-07 07:45:53.244027: step 18620, loss = 0.87, batch loss = 0.79 (5.9 examples/sec; 1.347 sec/batch; 117h:26m:52s remains)
INFO - root - 2017-12-07 07:46:06.659929: step 18630, loss = 0.81, batch loss = 0.74 (6.0 examples/sec; 1.337 sec/batch; 116h:32m:28s remains)
INFO - root - 2017-12-07 07:46:19.888954: step 18640, loss = 0.88, batch loss = 0.80 (5.9 examples/sec; 1.361 sec/batch; 118h:40m:15s remains)
INFO - root - 2017-12-07 07:46:33.097907: step 18650, loss = 0.90, batch loss = 0.83 (6.1 examples/sec; 1.310 sec/batch; 114h:14m:30s remains)
INFO - root - 2017-12-07 07:46:46.510507: step 18660, loss = 0.80, batch loss = 0.73 (5.9 examples/sec; 1.362 sec/batch; 118h:42m:23s remains)
INFO - root - 2017-12-07 07:46:59.924589: step 18670, loss = 0.86, batch loss = 0.79 (6.6 examples/sec; 1.214 sec/batch; 105h:47m:13s remains)
INFO - root - 2017-12-07 07:47:13.240625: step 18680, loss = 0.57, batch loss = 0.50 (6.0 examples/sec; 1.331 sec/batch; 116h:03m:32s remains)
INFO - root - 2017-12-07 07:47:26.724282: step 18690, loss = 0.67, batch loss = 0.60 (6.1 examples/sec; 1.317 sec/batch; 114h:48m:53s remains)
INFO - root - 2017-12-07 07:47:40.155710: step 18700, loss = 0.73, batch loss = 0.65 (6.1 examples/sec; 1.314 sec/batch; 114h:34m:32s remains)
2017-12-07 07:47:41.128303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8108144 -3.7149162 -3.2331731 -2.4196985 -1.6893449 -1.6990035 -2.3785596 -2.9846053 -3.1866021 -3.1531715 -3.0513434 -3.0090642 -2.8812871 -2.5567663 -2.356977][-3.7164679 -3.7257118 -3.1867425 -2.1230416 -1.0965946 -0.91565442 -1.4938293 -2.1350386 -2.559407 -2.886353 -3.066093 -3.1326618 -2.9344161 -2.4799981 -2.2120974][-3.5523677 -3.5785675 -3.0161843 -1.8872247 -0.80312753 -0.47212768 -0.68652368 -0.97838783 -1.3898306 -2.1391246 -2.8220387 -3.1416793 -2.9765258 -2.543323 -2.3143482][-3.8349242 -3.8802769 -3.2483284 -2.0338979 -0.87401319 -0.34570885 -0.13906574 -0.042882919 -0.32014608 -1.2017472 -2.1830306 -2.7460835 -2.7345924 -2.4486077 -2.3383076][-4.016932 -4.1421566 -3.6329648 -2.6080742 -1.4678464 -0.56537771 0.11119556 0.33070421 -0.086718082 -1.0417728 -1.991612 -2.5376191 -2.5910611 -2.4377761 -2.3934896][-3.686559 -3.7440131 -3.3614445 -2.7378907 -1.9747727 -1.0107989 0.0090942383 0.44092846 0.043523788 -0.81843519 -1.6837709 -2.2982175 -2.54067 -2.5481505 -2.4783664][-3.2451043 -3.2132781 -2.8595614 -2.38626 -1.8463762 -1.0796807 -0.16415119 0.24492645 -0.028996468 -0.5706346 -1.1348898 -1.6473987 -2.0902474 -2.3680367 -2.378006][-3.159833 -3.0754614 -2.7227218 -2.2198043 -1.5931158 -0.783968 0.0048394203 0.19363976 -0.18724918 -0.58229518 -0.79056191 -0.98021746 -1.4297929 -1.95626 -2.1807442][-3.1124039 -2.9814129 -2.6701064 -2.2825558 -1.7789743 -0.99326897 -0.20847178 -0.087496281 -0.47594285 -0.67034435 -0.51360559 -0.40041208 -0.80921793 -1.4860797 -1.9063189][-3.0650017 -2.9949279 -2.7535405 -2.5615311 -2.2957244 -1.6850729 -1.0947118 -1.1421816 -1.4773788 -1.356061 -0.79334879 -0.41534185 -0.73689628 -1.3714929 -1.8097041][-3.029634 -3.0444171 -2.9124572 -2.9079747 -2.8118234 -2.2755759 -1.7886648 -1.9144094 -2.2480979 -2.1228738 -1.6167576 -1.2704079 -1.4836216 -1.8753612 -2.1227257][-3.0507884 -3.0862014 -2.9860859 -3.0201702 -2.9814596 -2.5631456 -2.2223723 -2.3497629 -2.6063671 -2.5539446 -2.2793348 -2.1290643 -2.2927089 -2.4754357 -2.5127983][-3.0369987 -3.0930576 -2.9702053 -2.9032125 -2.7863202 -2.4594758 -2.2954869 -2.5209413 -2.7988257 -2.8312118 -2.7231853 -2.6893435 -2.8334897 -2.9354711 -2.9111862][-2.9733491 -3.0742927 -2.9655676 -2.8581872 -2.6918344 -2.4399166 -2.3720458 -2.6207922 -2.9198921 -3.0287685 -3.0249653 -3.0295224 -3.1141748 -3.1792073 -3.18483][-3.3593724 -3.4480286 -3.3750753 -3.2942164 -3.2009764 -3.0950117 -3.1115026 -3.2842846 -3.4592283 -3.5186398 -3.502934 -3.4687626 -3.4588234 -3.4597278 -3.4550152]]...]
INFO - root - 2017-12-07 07:47:54.578778: step 18710, loss = 1.02, batch loss = 0.95 (5.9 examples/sec; 1.349 sec/batch; 117h:36m:24s remains)
INFO - root - 2017-12-07 07:48:07.916802: step 18720, loss = 0.61, batch loss = 0.54 (6.0 examples/sec; 1.330 sec/batch; 115h:54m:37s remains)
INFO - root - 2017-12-07 07:48:21.227862: step 18730, loss = 0.60, batch loss = 0.53 (5.9 examples/sec; 1.346 sec/batch; 117h:16m:57s remains)
INFO - root - 2017-12-07 07:48:34.636218: step 18740, loss = 0.77, batch loss = 0.70 (5.9 examples/sec; 1.358 sec/batch; 118h:19m:11s remains)
INFO - root - 2017-12-07 07:48:47.776116: step 18750, loss = 0.91, batch loss = 0.84 (6.4 examples/sec; 1.253 sec/batch; 109h:13m:07s remains)
INFO - root - 2017-12-07 07:49:01.119446: step 18760, loss = 0.81, batch loss = 0.74 (5.9 examples/sec; 1.347 sec/batch; 117h:24m:03s remains)
INFO - root - 2017-12-07 07:49:14.545267: step 18770, loss = 0.85, batch loss = 0.78 (6.5 examples/sec; 1.225 sec/batch; 106h:47m:16s remains)
INFO - root - 2017-12-07 07:49:27.981835: step 18780, loss = 0.98, batch loss = 0.91 (5.8 examples/sec; 1.371 sec/batch; 119h:29m:36s remains)
INFO - root - 2017-12-07 07:49:41.476093: step 18790, loss = 0.77, batch loss = 0.70 (6.0 examples/sec; 1.344 sec/batch; 117h:06m:34s remains)
INFO - root - 2017-12-07 07:49:54.854652: step 18800, loss = 0.78, batch loss = 0.71 (5.9 examples/sec; 1.353 sec/batch; 117h:53m:36s remains)
2017-12-07 07:49:55.827312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8192792 -3.073164 -3.1096075 -3.0889077 -3.0755558 -3.0657043 -3.0530934 -2.8954983 -2.5877295 -2.1197987 -1.8018796 -1.7292507 -1.6489463 -1.7000721 -1.9739394][-2.5198164 -2.8131337 -2.8274417 -2.7876484 -2.7730598 -2.7872767 -2.8256001 -2.6653185 -2.3646443 -1.8967371 -1.5405481 -1.472214 -1.3443904 -1.3670464 -1.6833098][-2.0231137 -2.3212464 -2.3518932 -2.3384278 -2.3375616 -2.3786545 -2.4641595 -2.2820909 -2.0192497 -1.7058225 -1.4897175 -1.5168738 -1.3601007 -1.257406 -1.5191338][-1.5088637 -1.6842253 -1.7321229 -1.8000286 -1.861644 -1.9556973 -2.0678868 -1.8070052 -1.5115843 -1.3413274 -1.2945626 -1.4893718 -1.4145422 -1.2761502 -1.4777][-1.3354385 -1.2915106 -1.2695515 -1.3402662 -1.4062335 -1.4800589 -1.5491517 -1.2104118 -0.93112636 -0.96389937 -1.1001334 -1.437427 -1.4692018 -1.3570423 -1.5200911][-1.4630697 -1.26122 -1.1566916 -1.1543472 -1.1277428 -0.9948163 -0.82411551 -0.33497429 -0.16252661 -0.59693718 -1.0519509 -1.5318444 -1.6858201 -1.6341884 -1.7128222][-1.5292773 -1.3170807 -1.2146573 -1.1782124 -1.0588806 -0.63076043 -0.11914968 0.57675314 0.66052532 -0.15959263 -0.89689779 -1.4730949 -1.7788246 -1.8703194 -1.9217024][-1.4758279 -1.3181686 -1.2184889 -1.1195142 -0.9200089 -0.36165524 0.24669838 0.93454313 0.96678686 0.040978432 -0.7002399 -1.2294202 -1.5989192 -1.7725291 -1.7949829][-1.4876535 -1.4292972 -1.2760198 -1.0219109 -0.76947427 -0.3186512 0.01636219 0.37406206 0.41280317 -0.21318245 -0.71236491 -1.0812092 -1.3813028 -1.5200877 -1.466938][-1.711982 -1.7692897 -1.6101761 -1.2975383 -1.0634043 -0.76315284 -0.68489218 -0.55805731 -0.39043379 -0.60252619 -0.81984544 -0.99355006 -1.1565044 -1.2262263 -1.1179674][-1.951045 -2.097157 -2.0121436 -1.8157513 -1.7100441 -1.5188 -1.4543626 -1.2820842 -0.89383507 -0.76002908 -0.78010654 -0.77861428 -0.7716043 -0.8330667 -0.77743912][-2.108638 -2.2680233 -2.2189088 -2.1486261 -2.1650658 -2.0460463 -1.921773 -1.6421804 -1.1011331 -0.78640652 -0.71583867 -0.56233859 -0.38559008 -0.47918224 -0.56010675][-2.1201541 -2.3040614 -2.2959759 -2.3071337 -2.365093 -2.2241745 -2.0170257 -1.7093577 -1.2015462 -0.9042902 -0.87536025 -0.67865896 -0.42043686 -0.568275 -0.77825856][-1.9635701 -2.1796033 -2.2504482 -2.3546557 -2.4292212 -2.2459831 -1.9979939 -1.7569225 -1.4038625 -1.2421331 -1.3277259 -1.2183478 -1.0214672 -1.225596 -1.505837][-1.9182835 -2.075408 -2.1474836 -2.2820573 -2.3746963 -2.2315805 -2.0443354 -1.9383893 -1.7630575 -1.7346005 -1.911025 -1.8958089 -1.8134243 -2.0453596 -2.27805]]...]
INFO - root - 2017-12-07 07:50:09.169730: step 18810, loss = 0.91, batch loss = 0.84 (5.9 examples/sec; 1.367 sec/batch; 119h:08m:52s remains)
INFO - root - 2017-12-07 07:50:22.445740: step 18820, loss = 0.98, batch loss = 0.91 (5.9 examples/sec; 1.348 sec/batch; 117h:25m:34s remains)
INFO - root - 2017-12-07 07:50:35.767741: step 18830, loss = 0.74, batch loss = 0.67 (6.2 examples/sec; 1.299 sec/batch; 113h:09m:42s remains)
INFO - root - 2017-12-07 07:50:49.201624: step 18840, loss = 0.97, batch loss = 0.89 (6.0 examples/sec; 1.337 sec/batch; 116h:27m:07s remains)
INFO - root - 2017-12-07 07:51:02.468442: step 18850, loss = 0.85, batch loss = 0.78 (5.9 examples/sec; 1.353 sec/batch; 117h:51m:39s remains)
INFO - root - 2017-12-07 07:51:15.944145: step 18860, loss = 0.73, batch loss = 0.66 (6.0 examples/sec; 1.332 sec/batch; 116h:01m:32s remains)
INFO - root - 2017-12-07 07:51:29.019536: step 18870, loss = 0.84, batch loss = 0.76 (6.2 examples/sec; 1.297 sec/batch; 113h:00m:04s remains)
INFO - root - 2017-12-07 07:51:42.270729: step 18880, loss = 0.83, batch loss = 0.75 (6.1 examples/sec; 1.319 sec/batch; 114h:54m:20s remains)
INFO - root - 2017-12-07 07:51:55.620020: step 18890, loss = 0.68, batch loss = 0.61 (6.0 examples/sec; 1.332 sec/batch; 116h:02m:00s remains)
INFO - root - 2017-12-07 07:52:08.832705: step 18900, loss = 0.85, batch loss = 0.78 (6.1 examples/sec; 1.310 sec/batch; 114h:07m:49s remains)
2017-12-07 07:52:09.811268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3638968 -3.4546862 -3.5513058 -3.5952303 -3.6044686 -3.6422942 -3.7233438 -3.7364976 -3.6664751 -3.6159523 -3.5786963 -3.510323 -3.4509583 -3.4724531 -3.5635149][-3.4315042 -3.5933437 -3.7555671 -3.8109336 -3.8151364 -3.8442116 -3.8993697 -3.871618 -3.7831528 -3.7733872 -3.7798846 -3.6943722 -3.569144 -3.5075376 -3.5454493][-3.3432262 -3.544939 -3.759356 -3.8557758 -3.9199967 -3.996712 -4.0512271 -4.0042405 -3.9523573 -4.0133567 -4.0514522 -3.9252918 -3.7219625 -3.6172385 -3.6782105][-2.86371 -3.0993552 -3.3720288 -3.5078971 -3.6125665 -3.6903093 -3.6703651 -3.5644133 -3.5637 -3.6978977 -3.7301528 -3.5527461 -3.3426633 -3.3006287 -3.4182243][-2.5217476 -2.7676902 -3.0104978 -3.0344195 -3.0076561 -2.9165292 -2.6505384 -2.3593085 -2.3985941 -2.6421809 -2.7201846 -2.6310809 -2.5860252 -2.6851242 -2.7224398][-2.0680976 -2.1254318 -2.135381 -1.8974764 -1.6106684 -1.2304063 -0.68138814 -0.34226847 -0.68289089 -1.2035587 -1.3443179 -1.3533854 -1.5018787 -1.7374525 -1.6483724][-1.4268489 -1.2850358 -1.1655359 -0.85124707 -0.42820644 0.18173695 0.96564484 1.2566972 0.47123098 -0.3794775 -0.5535202 -0.51395369 -0.6416564 -0.79835224 -0.55150056][-1.5079124 -1.5338082 -1.6762831 -1.6259725 -1.2880633 -0.649297 0.14394426 0.3717494 -0.47333574 -1.2094026 -1.1399996 -0.83504653 -0.74774909 -0.76198554 -0.47649336][-2.0624368 -2.3476233 -2.7862463 -2.9657941 -2.709146 -2.1597004 -1.5759501 -1.4744053 -2.0967731 -2.431843 -2.0494797 -1.5210989 -1.2748551 -1.2494538 -1.0887933][-2.37144 -2.6918535 -3.1730881 -3.391537 -3.1734867 -2.74249 -2.3836093 -2.3576312 -2.7424307 -2.857187 -2.4916282 -2.0749788 -1.8635395 -1.8471444 -1.7523749][-2.613698 -2.8998373 -3.3308754 -3.5406046 -3.3404126 -2.9213822 -2.5719194 -2.5094044 -2.7497511 -2.8586245 -2.6876974 -2.5407908 -2.5142365 -2.5347204 -2.3865669][-2.8623109 -3.1050448 -3.4495068 -3.6269748 -3.4488089 -3.050807 -2.7393668 -2.6774619 -2.8224187 -2.9027252 -2.8035538 -2.7421608 -2.7561028 -2.719985 -2.5139923][-3.006999 -3.1928935 -3.4557657 -3.6381307 -3.5588663 -3.2998838 -3.1196892 -3.081933 -3.1171684 -3.0996509 -2.9988081 -2.9310749 -2.9132583 -2.815629 -2.6233578][-3.2155414 -3.366508 -3.5689921 -3.7475631 -3.7467952 -3.604454 -3.5328102 -3.5367584 -3.5541112 -3.5498581 -3.5168819 -3.5046806 -3.5319073 -3.5142281 -3.4376073][-3.405149 -3.5232663 -3.6661241 -3.7975879 -3.823252 -3.766608 -3.7814934 -3.8482025 -3.9040625 -3.9431326 -3.9614384 -3.9837852 -4.0237679 -4.0509253 -4.0382233]]...]
INFO - root - 2017-12-07 07:52:23.216573: step 18910, loss = 0.77, batch loss = 0.69 (5.9 examples/sec; 1.350 sec/batch; 117h:35m:01s remains)
INFO - root - 2017-12-07 07:52:36.599047: step 18920, loss = 0.91, batch loss = 0.84 (6.0 examples/sec; 1.336 sec/batch; 116h:24m:51s remains)
INFO - root - 2017-12-07 07:52:49.867234: step 18930, loss = 0.80, batch loss = 0.73 (6.0 examples/sec; 1.325 sec/batch; 115h:24m:28s remains)
INFO - root - 2017-12-07 07:53:03.217920: step 18940, loss = 1.03, batch loss = 0.95 (6.1 examples/sec; 1.318 sec/batch; 114h:46m:02s remains)
INFO - root - 2017-12-07 07:53:16.463184: step 18950, loss = 0.52, batch loss = 0.44 (6.3 examples/sec; 1.274 sec/batch; 110h:56m:51s remains)
INFO - root - 2017-12-07 07:53:29.782887: step 18960, loss = 0.76, batch loss = 0.69 (6.1 examples/sec; 1.305 sec/batch; 113h:41m:36s remains)
INFO - root - 2017-12-07 07:53:42.915384: step 18970, loss = 0.83, batch loss = 0.76 (6.2 examples/sec; 1.281 sec/batch; 111h:32m:49s remains)
INFO - root - 2017-12-07 07:53:56.088100: step 18980, loss = 0.75, batch loss = 0.68 (6.0 examples/sec; 1.332 sec/batch; 116h:00m:21s remains)
INFO - root - 2017-12-07 07:54:09.305620: step 18990, loss = 0.77, batch loss = 0.70 (6.1 examples/sec; 1.315 sec/batch; 114h:32m:42s remains)
INFO - root - 2017-12-07 07:54:22.764870: step 19000, loss = 0.81, batch loss = 0.74 (6.0 examples/sec; 1.329 sec/batch; 115h:45m:19s remains)
2017-12-07 07:54:23.767718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4977851 -2.4599507 -2.5028503 -2.5503602 -2.4762893 -2.4070508 -2.2339568 -2.1781955 -2.5520287 -2.7839355 -2.8175442 -2.8084564 -2.5058887 -2.0769358 -1.9086964][-2.332952 -2.2221282 -2.2040622 -2.2554209 -2.2624803 -2.29296 -2.1842513 -2.1405118 -2.5065384 -2.6991172 -2.661001 -2.6007414 -2.2948911 -1.9139192 -1.7969148][-2.1960173 -2.0394208 -1.9771609 -2.0358334 -2.1353931 -2.25615 -2.2200763 -2.1765571 -2.483772 -2.6053252 -2.4824963 -2.3784106 -2.1394012 -1.8573375 -1.7798545][-2.3021185 -2.0891578 -1.9723666 -1.988255 -2.1292264 -2.2872508 -2.310909 -2.2908304 -2.507381 -2.530045 -2.3202479 -2.1767771 -2.0689692 -1.9236789 -1.8691907][-2.5361927 -2.2663875 -2.114403 -2.1201408 -2.29165 -2.3940036 -2.362489 -2.3292432 -2.46946 -2.4132254 -2.133852 -1.9742894 -2.0448318 -2.0727677 -2.0263577][-2.7259815 -2.453166 -2.3121684 -2.3341296 -2.5013425 -2.4445772 -2.2222073 -2.1345124 -2.2712708 -2.2233338 -1.947216 -1.8190856 -2.060514 -2.2407112 -2.1838899][-2.9699512 -2.6485758 -2.4537623 -2.4662473 -2.6032858 -2.3725815 -1.9088845 -1.7501233 -1.9549689 -1.9993594 -1.7835071 -1.7470281 -2.1417561 -2.4081161 -2.3138611][-3.1711555 -2.7861838 -2.5299973 -2.5193472 -2.6055584 -2.2457771 -1.6607263 -1.5247362 -1.856514 -2.0257127 -1.8748977 -1.8834555 -2.3006828 -2.544446 -2.3965406][-3.1798167 -2.7367129 -2.4841344 -2.5402632 -2.6620617 -2.337219 -1.825314 -1.7969265 -2.1892822 -2.3754032 -2.2117338 -2.1623909 -2.4909658 -2.642581 -2.4486768][-2.9931808 -2.5672238 -2.4147007 -2.5957708 -2.7971728 -2.6043818 -2.2684329 -2.3374202 -2.6854415 -2.7647 -2.5292308 -2.3756604 -2.54741 -2.5944438 -2.3958704][-2.7686286 -2.4829276 -2.4664485 -2.7240267 -2.9775033 -2.9413393 -2.7863145 -2.9090009 -3.1421604 -3.0435469 -2.7327094 -2.487843 -2.5054874 -2.448138 -2.2522688][-2.626431 -2.5533278 -2.6427808 -2.8722649 -3.0947556 -3.1806159 -3.1695361 -3.2896147 -3.4176917 -3.1841283 -2.8649325 -2.5962615 -2.4768646 -2.32347 -2.1176178][-2.6292009 -2.7168608 -2.8361039 -2.9815116 -3.1336665 -3.2794604 -3.3299415 -3.4243817 -3.5144331 -3.2517157 -2.9574175 -2.6792417 -2.4629824 -2.247618 -2.023597][-2.7064092 -2.9186077 -3.0346062 -3.1076024 -3.1884184 -3.3083985 -3.3423641 -3.4129767 -3.5402331 -3.3651557 -3.1334896 -2.8295975 -2.5458522 -2.2771316 -2.0010476][-2.7277627 -2.9998803 -3.0941949 -3.109364 -3.1070638 -3.1428771 -3.130662 -3.2007701 -3.3862462 -3.3334715 -3.1602149 -2.8537939 -2.5876198 -2.3342021 -2.0375135]]...]
INFO - root - 2017-12-07 07:54:37.120941: step 19010, loss = 0.78, batch loss = 0.70 (6.1 examples/sec; 1.311 sec/batch; 114h:07m:09s remains)
INFO - root - 2017-12-07 07:54:50.510005: step 19020, loss = 0.76, batch loss = 0.69 (6.2 examples/sec; 1.298 sec/batch; 113h:01m:49s remains)
INFO - root - 2017-12-07 07:55:03.746056: step 19030, loss = 0.86, batch loss = 0.79 (6.1 examples/sec; 1.316 sec/batch; 114h:37m:56s remains)
INFO - root - 2017-12-07 07:55:17.028970: step 19040, loss = 0.96, batch loss = 0.88 (6.1 examples/sec; 1.305 sec/batch; 113h:36m:47s remains)
INFO - root - 2017-12-07 07:55:30.484511: step 19050, loss = 0.89, batch loss = 0.82 (6.0 examples/sec; 1.331 sec/batch; 115h:52m:09s remains)
INFO - root - 2017-12-07 07:55:43.851701: step 19060, loss = 0.90, batch loss = 0.83 (6.1 examples/sec; 1.312 sec/batch; 114h:13m:48s remains)
INFO - root - 2017-12-07 07:55:56.857522: step 19070, loss = 0.69, batch loss = 0.62 (6.4 examples/sec; 1.242 sec/batch; 108h:06m:08s remains)
INFO - root - 2017-12-07 07:56:10.325635: step 19080, loss = 0.72, batch loss = 0.65 (5.9 examples/sec; 1.352 sec/batch; 117h:44m:02s remains)
INFO - root - 2017-12-07 07:56:23.481169: step 19090, loss = 0.85, batch loss = 0.78 (6.1 examples/sec; 1.315 sec/batch; 114h:28m:48s remains)
INFO - root - 2017-12-07 07:56:36.793895: step 19100, loss = 0.82, batch loss = 0.75 (5.7 examples/sec; 1.400 sec/batch; 121h:52m:12s remains)
2017-12-07 07:56:37.728246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5057714 -2.0717666 -1.2958395 -0.917269 -1.40524 -2.1433456 -2.7780545 -2.9466939 -2.6434636 -2.3545148 -2.6724546 -3.4106376 -3.6282015 -3.117435 -2.6409402][-2.7900257 -2.3024459 -1.2991722 -0.61995173 -0.95541954 -1.6912639 -2.3457918 -2.4481082 -2.0771618 -1.8814998 -2.4876966 -3.5086761 -3.7597523 -3.0672965 -2.4389791][-2.6922927 -2.4896035 -1.4484792 -0.44483709 -0.4031744 -0.89589429 -1.4930134 -1.5877604 -1.2243893 -1.0977671 -1.8737316 -3.0868552 -3.4577446 -2.7922218 -2.1855392][-1.8576052 -2.2337494 -1.5607684 -0.603678 -0.25394821 -0.36431789 -0.79151511 -0.98093724 -0.84577012 -0.85827065 -1.5218084 -2.4340811 -2.6005025 -2.0103626 -1.7097683][-0.93942976 -1.8418834 -1.6898017 -1.080255 -0.62753415 -0.295444 -0.31327486 -0.46005011 -0.66646934 -1.0748165 -1.7588761 -2.2314432 -1.8375924 -1.1019528 -1.0945349][-0.42193985 -1.6591759 -1.8726094 -1.5044682 -0.91524339 -0.07422924 0.47429657 0.52658796 -0.022343159 -0.99473262 -1.9903808 -2.3634422 -1.541574 -0.53961587 -0.61998296][-0.1810236 -1.5566478 -1.9546254 -1.6454339 -0.79562521 0.57140541 1.7582426 2.0950499 1.1552515 -0.45559955 -1.8710356 -2.3669088 -1.4139833 -0.27573776 -0.37669945][-0.20372009 -1.5086656 -1.9273939 -1.6524372 -0.72698927 0.84366226 2.5829949 3.4656358 2.3042259 0.0934 -1.7521002 -2.4189532 -1.4616759 -0.28972006 -0.33129454][-0.4475255 -1.6177101 -2.0324774 -1.8287551 -1.099457 0.18197823 1.989121 3.4082255 2.5983996 0.33927345 -1.6814609 -2.5301044 -1.7645202 -0.75029039 -0.74096894][-0.77939963 -1.8324933 -2.3359697 -2.2849989 -1.7711692 -0.77216077 0.76356792 2.2505841 1.9680762 0.21949482 -1.5092413 -2.3538055 -1.9301426 -1.3347971 -1.4476881][-1.1328244 -2.0154359 -2.6311908 -2.7936115 -2.4916081 -1.6152124 -0.29550505 0.97445774 0.96132517 -0.21713781 -1.4576979 -2.1040654 -1.9323826 -1.81689 -2.1719751][-1.442601 -2.0336771 -2.6223574 -2.9151883 -2.8484452 -2.1877918 -1.1607409 -0.24554205 -0.19451284 -0.91743183 -1.728025 -2.1756184 -2.1783981 -2.4380803 -3.0309713][-2.2185342 -2.4742675 -2.8744707 -3.1205006 -3.176223 -2.7745104 -2.1431839 -1.6686389 -1.6836183 -2.0839319 -2.5075648 -2.7522786 -2.8129132 -3.2004023 -3.8580821][-3.2744298 -3.3702407 -3.5944438 -3.6792316 -3.6559026 -3.3898404 -3.0525589 -2.9045391 -2.9812596 -3.1552825 -3.2748952 -3.3353024 -3.3497953 -3.6580663 -4.1905584][-3.7524662 -3.8243661 -3.9892879 -4.0186291 -3.9415503 -3.7283 -3.5385907 -3.5195258 -3.598804 -3.6616006 -3.6382124 -3.6068263 -3.579391 -3.759542 -4.11876]]...]
INFO - root - 2017-12-07 07:56:51.250819: step 19110, loss = 0.72, batch loss = 0.64 (5.8 examples/sec; 1.371 sec/batch; 119h:18m:55s remains)
INFO - root - 2017-12-07 07:57:04.560093: step 19120, loss = 0.84, batch loss = 0.77 (6.0 examples/sec; 1.339 sec/batch; 116h:31m:30s remains)
INFO - root - 2017-12-07 07:57:17.912372: step 19130, loss = 0.73, batch loss = 0.66 (6.0 examples/sec; 1.328 sec/batch; 115h:33m:38s remains)
INFO - root - 2017-12-07 07:57:31.106658: step 19140, loss = 0.93, batch loss = 0.86 (6.1 examples/sec; 1.306 sec/batch; 113h:40m:49s remains)
INFO - root - 2017-12-07 07:57:44.495535: step 19150, loss = 0.79, batch loss = 0.72 (6.0 examples/sec; 1.328 sec/batch; 115h:35m:20s remains)
INFO - root - 2017-12-07 07:57:57.775370: step 19160, loss = 0.67, batch loss = 0.59 (6.1 examples/sec; 1.318 sec/batch; 114h:41m:39s remains)
INFO - root - 2017-12-07 07:58:10.909855: step 19170, loss = 0.61, batch loss = 0.54 (6.1 examples/sec; 1.313 sec/batch; 114h:18m:24s remains)
INFO - root - 2017-12-07 07:58:24.159273: step 19180, loss = 0.87, batch loss = 0.80 (6.2 examples/sec; 1.284 sec/batch; 111h:42m:59s remains)
INFO - root - 2017-12-07 07:58:37.557944: step 19190, loss = 0.70, batch loss = 0.63 (5.7 examples/sec; 1.397 sec/batch; 121h:33m:01s remains)
INFO - root - 2017-12-07 07:58:50.938326: step 19200, loss = 0.68, batch loss = 0.61 (6.3 examples/sec; 1.280 sec/batch; 111h:21m:17s remains)
2017-12-07 07:58:51.980194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.81041479 -0.94237065 -1.1994786 -1.5060227 -1.5055985 -1.2084451 -0.95864582 -0.68416739 -0.31436014 -0.073267937 -0.18265772 -0.53526759 -0.90382457 -1.2277572 -1.3958671][-1.1285625 -1.3239496 -1.6513996 -1.9153454 -1.7192972 -1.3304658 -1.1967306 -1.0598996 -0.66693711 -0.20448351 -0.036259174 -0.23079872 -0.58883333 -1.0911062 -1.4201097][-1.4591556 -1.4982607 -1.6954987 -1.8430526 -1.5280647 -1.305613 -1.5457664 -1.7182326 -1.3868985 -0.720134 -0.14961338 0.0089216232 -0.20754671 -0.87776136 -1.4107537][-1.5720398 -1.2919531 -1.2249811 -1.2411604 -0.93661809 -0.99142313 -1.5308743 -1.8586876 -1.6411276 -1.0957611 -0.42579603 0.020043373 0.033904552 -0.69690013 -1.3852429][-1.2182662 -0.84974313 -0.87254214 -1.0508611 -0.91054273 -1.0251889 -1.3755002 -1.3682005 -1.1512327 -1.0170984 -0.69492722 -0.23499155 0.069361687 -0.4692421 -1.0838938][-0.98185039 -0.64245105 -0.87911057 -1.2959719 -1.3241196 -1.2854097 -1.1704972 -0.71436882 -0.57201767 -0.9861958 -1.1801989 -0.87763095 -0.3592658 -0.58447242 -0.96379662][-1.4712288 -1.2910392 -1.6814651 -2.0617073 -1.863467 -1.3117332 -0.6062386 0.17318583 0.13301039 -0.76592112 -1.4575593 -1.4130437 -0.96371531 -1.1517529 -1.5496221][-1.8488376 -1.8977225 -2.443095 -2.8509762 -2.6052961 -1.7949729 -0.75880933 0.24350023 0.3696022 -0.40881252 -1.1279595 -1.2015274 -0.9795711 -1.3103538 -1.8579273][-1.7578909 -1.884517 -2.4017153 -2.923768 -3.1022711 -2.717994 -1.9481637 -1.0097189 -0.53747034 -0.71550584 -0.9786129 -0.79259682 -0.58941078 -0.99814773 -1.6274953][-1.188494 -1.3876936 -1.7740519 -2.3482614 -2.9356751 -3.1016779 -2.7691178 -2.1118498 -1.6064055 -1.4847043 -1.4057069 -0.83991909 -0.32323837 -0.59287596 -1.2051063][-0.55420423 -0.73872423 -1.0211964 -1.667115 -2.519213 -2.9597566 -2.8172731 -2.4200845 -2.1500235 -2.0738668 -1.9147413 -1.1994915 -0.4374733 -0.44566321 -0.814626][-0.50651288 -0.66337943 -0.85172868 -1.3772101 -2.1341517 -2.4650726 -2.3032568 -2.0592184 -2.0312536 -2.1144383 -2.0881526 -1.572727 -1.0180755 -1.025789 -1.146208][-0.37829304 -0.59382677 -0.84778023 -1.2911389 -1.8139496 -1.9502254 -1.7767434 -1.6507084 -1.7522235 -1.9457512 -2.0602872 -1.8797126 -1.669944 -1.7011514 -1.606446][-0.712872 -0.76302648 -0.9234972 -1.265137 -1.6453838 -1.7672205 -1.7345719 -1.7293725 -1.8633654 -2.055027 -2.1649623 -2.1196873 -2.0582192 -1.9801605 -1.6387098][-1.4013567 -1.3023751 -1.3315163 -1.6240733 -1.9461648 -2.0822227 -2.1405623 -2.1772234 -2.327286 -2.5023518 -2.5126762 -2.3817153 -2.2566488 -2.0551877 -1.6147559]]...]
INFO - root - 2017-12-07 07:59:05.431687: step 19210, loss = 0.75, batch loss = 0.68 (5.9 examples/sec; 1.347 sec/batch; 117h:12m:44s remains)
INFO - root - 2017-12-07 07:59:18.749181: step 19220, loss = 0.76, batch loss = 0.69 (6.0 examples/sec; 1.330 sec/batch; 115h:42m:21s remains)
INFO - root - 2017-12-07 07:59:32.119237: step 19230, loss = 0.66, batch loss = 0.59 (6.0 examples/sec; 1.338 sec/batch; 116h:26m:26s remains)
INFO - root - 2017-12-07 07:59:45.438237: step 19240, loss = 0.95, batch loss = 0.88 (6.0 examples/sec; 1.333 sec/batch; 115h:59m:52s remains)
INFO - root - 2017-12-07 07:59:58.815641: step 19250, loss = 1.07, batch loss = 1.00 (6.1 examples/sec; 1.316 sec/batch; 114h:29m:59s remains)
INFO - root - 2017-12-07 08:00:12.084032: step 19260, loss = 0.66, batch loss = 0.59 (6.0 examples/sec; 1.333 sec/batch; 115h:59m:40s remains)
INFO - root - 2017-12-07 08:00:25.201008: step 19270, loss = 0.73, batch loss = 0.66 (6.0 examples/sec; 1.324 sec/batch; 115h:09m:49s remains)
INFO - root - 2017-12-07 08:00:38.544996: step 19280, loss = 0.83, batch loss = 0.76 (6.0 examples/sec; 1.339 sec/batch; 116h:30m:05s remains)
INFO - root - 2017-12-07 08:00:51.776029: step 19290, loss = 0.69, batch loss = 0.62 (6.0 examples/sec; 1.329 sec/batch; 115h:40m:07s remains)
INFO - root - 2017-12-07 08:01:05.134816: step 19300, loss = 0.77, batch loss = 0.70 (6.0 examples/sec; 1.327 sec/batch; 115h:26m:41s remains)
2017-12-07 08:01:06.008084: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2127216 -3.818995 -4.3258605 -4.5540948 -4.5764546 -4.2598705 -3.8446178 -3.934294 -4.4500766 -4.9571953 -5.5301847 -6.0273561 -5.9828548 -5.217051 -3.846509][-2.9301815 -3.9472406 -4.6546226 -4.8284516 -4.7211528 -4.1564178 -3.4843693 -3.541213 -4.1756387 -4.7878833 -5.5738568 -6.3442945 -6.4089479 -5.4579225 -3.7111654][-2.6499827 -3.8764205 -4.5377669 -4.5170197 -4.336338 -3.6704814 -2.8308539 -2.8918028 -3.6231952 -4.2442441 -5.1984825 -6.2399149 -6.4330006 -5.3357081 -3.3737533][-2.7072432 -3.995863 -4.4185061 -4.1202765 -3.8224361 -2.9797144 -1.8554044 -1.8908334 -2.8168695 -3.6092134 -4.8460131 -6.1506271 -6.4403753 -5.1719422 -3.0024428][-2.8678341 -4.2002621 -4.4660025 -4.0827045 -3.8177128 -2.8339281 -1.4128766 -1.4640346 -2.6936569 -3.7412062 -5.0830164 -6.2828712 -6.3646374 -4.7780108 -2.4098296][-2.8698871 -4.0685816 -4.0534282 -3.4828985 -3.0230191 -1.6233399 0.17997742 -0.097537994 -1.9275801 -3.4605699 -4.9893503 -6.1549578 -6.1398344 -4.3485241 -1.9547217][-2.8007488 -3.9501483 -3.8850343 -3.3034897 -2.622241 -0.62983131 1.6767764 1.277041 -0.90762448 -2.6734583 -4.3446074 -5.7280464 -5.8169303 -3.9769619 -1.6577985][-2.6722717 -3.8596871 -4.1128316 -4.0602422 -3.7623973 -1.8493013 0.39470005 0.062851429 -1.6842918 -2.9404244 -4.3107386 -5.6651249 -5.6818919 -3.7425957 -1.4891973][-2.4392438 -3.5019805 -3.8631032 -4.1209683 -4.2214432 -2.8133798 -1.1678789 -1.5795529 -2.8024025 -3.5212898 -4.6019111 -5.7609282 -5.5491796 -3.5321088 -1.4214983][-2.1704161 -3.0510137 -3.3816772 -3.6734278 -3.924567 -2.9372902 -1.7835546 -2.1902103 -3.0438485 -3.4990468 -4.3497572 -5.1968727 -4.7503181 -2.8846078 -1.1925056][-2.3935182 -3.0967095 -3.2919881 -3.43929 -3.6387377 -2.9006331 -2.0618751 -2.435009 -3.1166587 -3.4687376 -4.037756 -4.4842534 -3.8968863 -2.4005566 -1.3018792][-2.8723984 -3.4623 -3.54485 -3.5951827 -3.7320111 -3.1227236 -2.4033875 -2.6430931 -3.20674 -3.5435767 -3.9234924 -4.141314 -3.5998373 -2.4941905 -1.8251317][-3.2628241 -3.7676356 -3.8194909 -3.9072797 -4.1488638 -3.7899768 -3.2259731 -3.2905362 -3.5649424 -3.659544 -3.7567654 -3.7934372 -3.3977447 -2.6887698 -2.3564][-3.4723656 -3.7949376 -3.7038898 -3.6774921 -3.8837142 -3.70987 -3.3870039 -3.4749575 -3.6363761 -3.5906444 -3.5195291 -3.4763527 -3.2270317 -2.8292379 -2.7564521][-3.5216613 -3.6922407 -3.5511253 -3.475193 -3.6135869 -3.5391104 -3.3859754 -3.4945192 -3.589879 -3.492599 -3.3686585 -3.2907209 -3.1143413 -2.9227118 -3.0393863]]...]
INFO - root - 2017-12-07 08:01:19.315019: step 19310, loss = 0.69, batch loss = 0.62 (6.1 examples/sec; 1.319 sec/batch; 114h:44m:17s remains)
INFO - root - 2017-12-07 08:01:32.640013: step 19320, loss = 0.77, batch loss = 0.70 (6.0 examples/sec; 1.328 sec/batch; 115h:33m:39s remains)
INFO - root - 2017-12-07 08:01:45.787802: step 19330, loss = 0.95, batch loss = 0.88 (6.1 examples/sec; 1.312 sec/batch; 114h:10m:13s remains)
INFO - root - 2017-12-07 08:01:59.114090: step 19340, loss = 0.80, batch loss = 0.73 (5.9 examples/sec; 1.358 sec/batch; 118h:07m:12s remains)
INFO - root - 2017-12-07 08:02:12.405886: step 19350, loss = 0.82, batch loss = 0.75 (6.1 examples/sec; 1.306 sec/batch; 113h:38m:20s remains)
INFO - root - 2017-12-07 08:02:25.722969: step 19360, loss = 0.69, batch loss = 0.62 (6.1 examples/sec; 1.317 sec/batch; 114h:34m:45s remains)
INFO - root - 2017-12-07 08:02:39.027184: step 19370, loss = 0.93, batch loss = 0.86 (5.8 examples/sec; 1.375 sec/batch; 119h:38m:24s remains)
INFO - root - 2017-12-07 08:02:52.302233: step 19380, loss = 0.92, batch loss = 0.85 (6.2 examples/sec; 1.285 sec/batch; 111h:48m:21s remains)
INFO - root - 2017-12-07 08:03:05.597374: step 19390, loss = 0.81, batch loss = 0.74 (6.0 examples/sec; 1.322 sec/batch; 115h:00m:40s remains)
INFO - root - 2017-12-07 08:03:18.854702: step 19400, loss = 0.71, batch loss = 0.64 (6.1 examples/sec; 1.305 sec/batch; 113h:32m:03s remains)
2017-12-07 08:03:19.755110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.81945 -2.8635623 -3.0132627 -3.158926 -3.3068562 -3.3894727 -3.2418706 -2.4455853 -1.0872433 -0.59714961 -1.3746243 -2.1823313 -2.4448092 -2.4525256 -2.3636203][-2.6162176 -2.7937016 -3.0541322 -3.2411771 -3.4336553 -3.5415566 -3.3081059 -2.2092755 -0.49955845 -0.08241415 -1.2062557 -2.2536736 -2.6131086 -2.5544629 -2.3368394][-2.5125828 -2.8797393 -3.3324986 -3.6162944 -3.7860544 -3.73501 -3.2086291 -1.7174397 0.28741264 0.50736523 -1.0093942 -2.2743244 -2.7585883 -2.6934247 -2.3887186][-2.6631255 -3.1120312 -3.6884692 -4.1164575 -4.3723006 -4.1994371 -3.3181169 -1.3702567 0.93990993 0.94668007 -0.93977666 -2.2897317 -2.7897062 -2.7589574 -2.466104][-2.1265333 -2.4698124 -3.0066907 -3.6636009 -4.29749 -4.3094516 -3.3130517 -1.0214689 1.5147223 1.3382392 -0.84653878 -2.1543946 -2.5776734 -2.6724956 -2.5352][-1.3433833 -1.4574683 -1.7633626 -2.5989633 -3.6541457 -3.9293869 -2.9848683 -0.57719851 2.000381 1.6848702 -0.55925989 -1.6447067 -1.9699051 -2.3370345 -2.5055161][-1.1723988 -0.96615839 -0.84778786 -1.6645637 -2.9585364 -3.3497615 -2.4669619 -0.18195772 2.1607289 1.7220645 -0.28612185 -0.96189094 -1.1885445 -1.8920012 -2.3987906][-1.3752763 -0.77040243 -0.11608267 -0.79884171 -2.2472079 -2.7526822 -1.9718566 -0.027434826 1.7703009 1.1879535 -0.35596085 -0.4321425 -0.47458911 -1.4777923 -2.2868271][-1.7444389 -0.91723084 0.12665129 -0.35588455 -1.8465672 -2.4596758 -1.7951379 -0.29921436 0.85319519 0.19453478 -0.73640037 -0.069508076 0.18616867 -1.0561163 -2.1577663][-2.1844971 -1.4270031 -0.24520493 -0.48492074 -1.8060458 -2.3417864 -1.7491164 -0.70525742 -0.16591311 -0.84512281 -1.1741061 0.21244431 0.81866121 -0.5869801 -1.9792376][-2.5097933 -2.0032814 -0.877903 -0.91782188 -1.9060371 -2.1712477 -1.5781491 -0.927423 -0.87256 -1.5951316 -1.5808389 0.23014355 1.1663995 -0.20705271 -1.7675731][-2.8659706 -2.6767473 -1.771383 -1.6933873 -2.3251781 -2.3092108 -1.6828108 -1.2266307 -1.3805485 -2.1293075 -2.106452 -0.30004978 0.82493544 -0.30547428 -1.8013635][-3.3949466 -3.4993811 -2.8417764 -2.6350617 -2.9135664 -2.7362223 -2.182138 -1.8348293 -2.0348914 -2.7473822 -2.8568933 -1.3593571 -0.21172571 -0.97773027 -2.1812418][-3.8008027 -4.0919294 -3.6431246 -3.294802 -3.2638917 -3.041749 -2.6577449 -2.4813676 -2.7540367 -3.3718 -3.540592 -2.4093478 -1.3723578 -1.780066 -2.6093693][-4.0929384 -4.3782763 -4.007463 -3.5394404 -3.3090463 -3.0987151 -2.8812423 -2.892127 -3.2592311 -3.7869534 -3.9474125 -3.1609025 -2.3215964 -2.4333138 -2.9099503]]...]
INFO - root - 2017-12-07 08:03:33.314413: step 19410, loss = 0.80, batch loss = 0.73 (5.9 examples/sec; 1.358 sec/batch; 118h:04m:40s remains)
INFO - root - 2017-12-07 08:03:46.775325: step 19420, loss = 0.78, batch loss = 0.70 (5.8 examples/sec; 1.383 sec/batch; 120h:15m:19s remains)
INFO - root - 2017-12-07 08:04:00.083413: step 19430, loss = 0.88, batch loss = 0.81 (6.3 examples/sec; 1.273 sec/batch; 110h:42m:06s remains)
INFO - root - 2017-12-07 08:04:13.306765: step 19440, loss = 0.78, batch loss = 0.71 (6.0 examples/sec; 1.333 sec/batch; 115h:53m:11s remains)
INFO - root - 2017-12-07 08:04:26.652434: step 19450, loss = 0.89, batch loss = 0.81 (5.9 examples/sec; 1.367 sec/batch; 118h:51m:29s remains)
INFO - root - 2017-12-07 08:04:39.979560: step 19460, loss = 0.60, batch loss = 0.53 (6.1 examples/sec; 1.314 sec/batch; 114h:13m:28s remains)
INFO - root - 2017-12-07 08:04:53.239765: step 19470, loss = 0.80, batch loss = 0.72 (5.9 examples/sec; 1.352 sec/batch; 117h:32m:30s remains)
INFO - root - 2017-12-07 08:05:06.579297: step 19480, loss = 0.79, batch loss = 0.72 (6.1 examples/sec; 1.310 sec/batch; 113h:56m:49s remains)
INFO - root - 2017-12-07 08:05:20.132271: step 19490, loss = 0.69, batch loss = 0.61 (6.0 examples/sec; 1.330 sec/batch; 115h:37m:22s remains)
INFO - root - 2017-12-07 08:05:33.588413: step 19500, loss = 0.92, batch loss = 0.85 (5.9 examples/sec; 1.347 sec/batch; 117h:08m:08s remains)
2017-12-07 08:05:34.506356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8448193 -1.8768058 -1.9182208 -1.9503543 -1.9651561 -1.9626086 -1.9473443 -1.9125662 -1.8644571 -1.8332412 -1.8101954 -1.7984657 -1.7995467 -1.7976987 -1.7932959][-2.0552678 -2.1302962 -2.1779609 -2.1968343 -2.1810431 -2.1477814 -2.1049764 -2.0473034 -1.9870765 -1.9411001 -1.898087 -1.8582551 -1.829953 -1.8044479 -1.7898939][-2.3398752 -2.4493093 -2.4915497 -2.4792638 -2.3977661 -2.2853954 -2.1731062 -2.0760491 -2.0050974 -1.9660335 -1.9443827 -1.9254315 -1.9051454 -1.8727195 -1.8443816][-2.6481071 -2.7690649 -2.8194003 -2.805614 -2.7006705 -2.5431643 -2.3677261 -2.1897285 -2.026444 -1.9070935 -1.8364172 -1.8140182 -1.8178329 -1.826052 -1.8371508][-2.6731052 -2.7650621 -2.8008122 -2.7884941 -2.7504153 -2.7388072 -2.7111833 -2.5962307 -2.4198143 -2.240057 -2.06798 -1.9249487 -1.8191783 -1.7658763 -1.7606394][-2.5202074 -2.5009398 -2.3979158 -2.2442191 -2.203094 -2.3747945 -2.61052 -2.72421 -2.7534575 -2.7540231 -2.680048 -2.5310907 -2.3271997 -2.1366014 -1.9902194][-2.54568 -2.4406838 -2.1391487 -1.7072268 -1.4296536 -1.5026438 -1.7264996 -1.8977923 -2.0834882 -2.3096974 -2.4818573 -2.5456762 -2.4905906 -2.3590622 -2.1838164][-2.2562678 -2.2125337 -1.9096971 -1.3779929 -0.92661333 -0.82062483 -0.870945 -0.89877939 -0.99385428 -1.2059946 -1.4673991 -1.6966448 -1.8495066 -1.9232829 -1.9106567][-1.6608236 -1.791434 -1.7088592 -1.3957825 -1.0848315 -0.9712522 -0.91116905 -0.77072382 -0.66076279 -0.67620564 -0.81193662 -1.004391 -1.2083051 -1.4032614 -1.5412412][-1.5798943 -1.7932882 -1.8766477 -1.7946248 -1.7436936 -1.8104837 -1.8510051 -1.7069271 -1.4745305 -1.3003943 -1.2356477 -1.2564323 -1.3218071 -1.4263377 -1.526082][-2.1399994 -2.2788372 -2.3565593 -2.3622043 -2.4681344 -2.6842782 -2.8694587 -2.8575768 -2.7073874 -2.5353661 -2.3721344 -2.231194 -2.0938971 -1.9838293 -1.8968575][-2.7304564 -2.781569 -2.7803433 -2.7420852 -2.8092871 -2.9974027 -3.1704082 -3.1865573 -3.0847673 -2.9433537 -2.7792296 -2.6135902 -2.4411392 -2.2713706 -2.1088839][-2.7622755 -2.81756 -2.7973208 -2.7333312 -2.7346597 -2.8349488 -2.9298058 -2.9188676 -2.829087 -2.7140121 -2.5848312 -2.4533894 -2.3335478 -2.2230308 -2.1147807][-2.5196462 -2.6005282 -2.5887423 -2.5236897 -2.4743989 -2.4780083 -2.4790394 -2.43558 -2.3716135 -2.314682 -2.2624812 -2.207078 -2.1618598 -2.118926 -2.0710571][-2.3284185 -2.4035935 -2.4067037 -2.3604872 -2.3008847 -2.2466922 -2.1863782 -2.1183238 -2.0645823 -2.0395784 -2.0299623 -2.0201116 -2.0085244 -1.9903159 -1.9682825]]...]
INFO - root - 2017-12-07 08:05:47.916698: step 19510, loss = 0.87, batch loss = 0.80 (5.9 examples/sec; 1.347 sec/batch; 117h:05m:44s remains)
INFO - root - 2017-12-07 08:06:01.286189: step 19520, loss = 0.84, batch loss = 0.77 (5.9 examples/sec; 1.350 sec/batch; 117h:23m:46s remains)
INFO - root - 2017-12-07 08:06:14.585620: step 19530, loss = 0.81, batch loss = 0.74 (6.0 examples/sec; 1.335 sec/batch; 116h:05m:56s remains)
INFO - root - 2017-12-07 08:06:27.863675: step 19540, loss = 0.66, batch loss = 0.59 (6.1 examples/sec; 1.322 sec/batch; 114h:54m:37s remains)
INFO - root - 2017-12-07 08:06:41.219224: step 19550, loss = 0.77, batch loss = 0.70 (6.0 examples/sec; 1.337 sec/batch; 116h:15m:35s remains)
INFO - root - 2017-12-07 08:06:54.551748: step 19560, loss = 0.68, batch loss = 0.61 (6.0 examples/sec; 1.344 sec/batch; 116h:50m:48s remains)
INFO - root - 2017-12-07 08:07:07.623229: step 19570, loss = 0.87, batch loss = 0.80 (6.0 examples/sec; 1.327 sec/batch; 115h:20m:32s remains)
INFO - root - 2017-12-07 08:07:20.883388: step 19580, loss = 1.00, batch loss = 0.92 (6.3 examples/sec; 1.275 sec/batch; 110h:47m:22s remains)
INFO - root - 2017-12-07 08:07:34.160909: step 19590, loss = 0.73, batch loss = 0.66 (5.9 examples/sec; 1.356 sec/batch; 117h:51m:49s remains)
INFO - root - 2017-12-07 08:07:47.356189: step 19600, loss = 0.98, batch loss = 0.91 (6.1 examples/sec; 1.322 sec/batch; 114h:53m:37s remains)
2017-12-07 08:07:48.284799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0201025 -3.8874211 -3.635479 -3.2305579 -2.9844027 -3.0187693 -3.0895312 -3.1581678 -3.323168 -3.4128382 -3.396162 -3.3874416 -3.494173 -3.5910056 -3.5551124][-3.7930052 -3.5702932 -3.169405 -2.6182199 -2.3693612 -2.4376574 -2.4656634 -2.5535531 -2.8399673 -3.0071886 -2.9731 -2.9279985 -3.058512 -3.1748528 -3.0916595][-3.6120474 -3.3362498 -2.8129063 -2.1852162 -1.9755633 -2.0400782 -1.9524491 -2.044131 -2.4918046 -2.7356851 -2.6459477 -2.5413809 -2.7015295 -2.8839791 -2.7884822][-3.6569796 -3.41535 -2.8895879 -2.2742183 -2.0556586 -1.9402306 -1.5820594 -1.6682093 -2.3273969 -2.6352234 -2.4795136 -2.3363152 -2.5542362 -2.8425307 -2.7788587][-3.6661148 -3.4298153 -2.9268422 -2.3644218 -2.1358509 -1.764781 -1.0338089 -1.0922506 -1.8966434 -2.1922443 -2.0359359 -1.9945838 -2.3357646 -2.7640109 -2.762408][-3.5719471 -3.2674806 -2.7706542 -2.2216206 -1.88868 -1.1985991 -0.068766594 -0.12684059 -1.0244257 -1.2357454 -1.1551795 -1.3548727 -1.87855 -2.4581966 -2.538656][-3.6426942 -3.3101151 -2.8221629 -2.1939554 -1.6731691 -0.65866041 0.80438757 0.65501165 -0.35158253 -0.47799993 -0.47692156 -0.85880923 -1.4293966 -1.9837317 -2.0434206][-3.9077873 -3.5601847 -3.057054 -2.3300154 -1.7298603 -0.69374347 0.67481327 0.33431959 -0.65675426 -0.6296525 -0.63091516 -1.0417228 -1.4711862 -1.7770414 -1.6317213][-4.1855407 -3.850419 -3.2872114 -2.4576643 -1.8797731 -1.0809448 -0.17074823 -0.73159122 -1.555644 -1.2637467 -1.1163282 -1.4524651 -1.7730603 -1.9275327 -1.6582439][-4.3163524 -4.0671496 -3.4653795 -2.5549216 -1.9669352 -1.4281237 -0.95431709 -1.5768597 -2.1307185 -1.6325686 -1.4121313 -1.666569 -1.9126174 -2.0244665 -1.7809389][-4.315402 -4.1193113 -3.4765058 -2.5739288 -2.0326 -1.7932141 -1.6400168 -2.0476651 -2.220911 -1.6471953 -1.5199442 -1.7721076 -2.0138748 -2.1485963 -1.9621813][-4.3626275 -4.2214069 -3.608489 -2.7681193 -2.2754004 -2.2751706 -2.2932212 -2.3737366 -2.2041621 -1.7040532 -1.7519441 -2.038023 -2.3663061 -2.5988517 -2.4473128][-4.4408679 -4.458756 -4.0332 -3.3610649 -2.923099 -2.9762883 -2.9579692 -2.7553144 -2.42157 -2.0932469 -2.3100705 -2.6132154 -2.965641 -3.1972413 -2.9469018][-4.542932 -4.6545529 -4.33303 -3.7888885 -3.4795399 -3.5969315 -3.5542877 -3.2454488 -2.8609042 -2.658617 -2.922194 -3.1540365 -3.431108 -3.514348 -3.1412272][-4.4127536 -4.5172062 -4.2220864 -3.7571547 -3.5887723 -3.7783248 -3.7731349 -3.4957397 -3.1596768 -3.0318153 -3.2852628 -3.4376526 -3.5838306 -3.5045593 -3.0991569]]...]
INFO - root - 2017-12-07 08:08:01.632545: step 19610, loss = 0.85, batch loss = 0.78 (6.0 examples/sec; 1.340 sec/batch; 116h:29m:19s remains)
INFO - root - 2017-12-07 08:08:15.043252: step 19620, loss = 0.83, batch loss = 0.76 (5.8 examples/sec; 1.372 sec/batch; 119h:13m:23s remains)
INFO - root - 2017-12-07 08:08:28.377162: step 19630, loss = 0.81, batch loss = 0.74 (6.1 examples/sec; 1.309 sec/batch; 113h:46m:59s remains)
INFO - root - 2017-12-07 08:08:41.772925: step 19640, loss = 0.78, batch loss = 0.71 (6.0 examples/sec; 1.327 sec/batch; 115h:18m:12s remains)
INFO - root - 2017-12-07 08:08:55.167836: step 19650, loss = 0.64, batch loss = 0.57 (5.9 examples/sec; 1.348 sec/batch; 117h:07m:49s remains)
INFO - root - 2017-12-07 08:09:08.463948: step 19660, loss = 0.71, batch loss = 0.64 (5.8 examples/sec; 1.368 sec/batch; 118h:55m:00s remains)
INFO - root - 2017-12-07 08:09:19.128686: step 19670, loss = 0.81, batch loss = 0.74 (7.4 examples/sec; 1.083 sec/batch; 94h:06m:09s remains)
INFO - root - 2017-12-07 08:09:29.626347: step 19680, loss = 0.70, batch loss = 0.62 (7.7 examples/sec; 1.033 sec/batch; 89h:44m:46s remains)
INFO - root - 2017-12-07 08:09:40.000133: step 19690, loss = 0.66, batch loss = 0.59 (8.0 examples/sec; 1.000 sec/batch; 86h:55m:51s remains)
INFO - root - 2017-12-07 08:09:50.301694: step 19700, loss = 0.82, batch loss = 0.75 (7.6 examples/sec; 1.047 sec/batch; 90h:56m:06s remains)
2017-12-07 08:09:51.177374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.425118 -3.2466469 -3.1590319 -3.0745821 -2.9920142 -3.2683387 -3.6509352 -3.6082909 -3.2350149 -2.9141488 -2.7364821 -2.5845776 -2.6439 -3.0507605 -3.4063559][-3.5571446 -3.5188293 -3.4916952 -3.4426255 -3.3104467 -3.3113322 -3.3781128 -3.1423943 -2.7388904 -2.4887223 -2.3319504 -2.2874441 -2.5315127 -3.0120168 -3.3754401][-3.5123582 -3.6168759 -3.6883585 -3.6819916 -3.4830387 -3.1732306 -2.9338818 -2.6297038 -2.3422868 -2.2318895 -2.1126373 -2.1905625 -2.5578666 -2.9701681 -3.256197][-3.3092895 -3.4739382 -3.6343887 -3.6802921 -3.4477031 -2.9621606 -2.5679364 -2.3526182 -2.2556305 -2.3112662 -2.3071835 -2.5221436 -2.9021919 -3.1498947 -3.3328941][-3.0760226 -3.1312335 -3.2728481 -3.3465419 -3.1873169 -2.7909355 -2.5041134 -2.4291506 -2.3945408 -2.4963543 -2.56531 -2.8080554 -3.1532376 -3.3322182 -3.5493813][-2.8054147 -2.6537304 -2.70822 -2.7770472 -2.752511 -2.58197 -2.4460535 -2.4205995 -2.3131635 -2.3445489 -2.4124861 -2.6262436 -3.0277414 -3.3287435 -3.6999273][-2.5861053 -2.2587042 -2.1738412 -2.1322291 -2.1896312 -2.264991 -2.2980881 -2.2624626 -2.0116036 -1.8905027 -1.9440954 -2.2258499 -2.8180821 -3.3126216 -3.7871461][-2.4789977 -2.0124409 -1.7176785 -1.447094 -1.5032728 -1.823581 -2.0491135 -2.0322235 -1.6853032 -1.4791479 -1.6320217 -2.0831747 -2.8118362 -3.3692436 -3.8119204][-2.4524035 -1.7985291 -1.2099633 -0.69187164 -0.66024017 -1.0678103 -1.3284249 -1.3265128 -1.0930524 -1.1318998 -1.6372185 -2.3070133 -3.029778 -3.5256958 -3.8871431][-2.5074117 -1.7017634 -0.87445855 -0.21811342 -0.037013531 -0.26812553 -0.35543013 -0.41812658 -0.58251786 -1.1087928 -1.9663069 -2.7135787 -3.3358204 -3.7295909 -4.0114589][-2.6126204 -1.8495574 -1.0678661 -0.50668979 -0.20967531 -0.080612659 0.13688564 0.031731129 -0.51757979 -1.3853197 -2.3146155 -2.9651151 -3.4700232 -3.7944777 -4.0065274][-2.7127669 -2.1973832 -1.6875937 -1.3320444 -0.96693826 -0.57565236 -0.22552347 -0.36924028 -1.0506275 -1.908674 -2.6719522 -3.1667175 -3.5455103 -3.7864494 -3.913331][-2.8900859 -2.6407151 -2.3814781 -2.1433816 -1.7772951 -1.4068177 -1.1941564 -1.3730783 -1.8737693 -2.4165764 -2.8943393 -3.2286143 -3.4943 -3.6842175 -3.7860835][-3.1394024 -3.0231085 -2.8559585 -2.6603203 -2.4286578 -2.2727098 -2.2548604 -2.3854194 -2.5891082 -2.7584105 -2.9083321 -3.0217533 -3.1436768 -3.3300128 -3.5383291][-3.3082786 -3.2114673 -3.0906892 -2.9714105 -2.9145341 -2.9181502 -2.9448822 -2.9684148 -2.9872346 -2.9737589 -2.897711 -2.7728741 -2.7057848 -2.8464804 -3.1484313]]...]
INFO - root - 2017-12-07 08:10:01.579702: step 19710, loss = 0.72, batch loss = 0.64 (7.5 examples/sec; 1.062 sec/batch; 92h:18m:56s remains)
INFO - root - 2017-12-07 08:10:12.053360: step 19720, loss = 1.06, batch loss = 0.99 (7.8 examples/sec; 1.029 sec/batch; 89h:21m:53s remains)
INFO - root - 2017-12-07 08:10:22.508399: step 19730, loss = 0.73, batch loss = 0.66 (7.7 examples/sec; 1.036 sec/batch; 89h:59m:39s remains)
INFO - root - 2017-12-07 08:10:32.948963: step 19740, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.046 sec/batch; 90h:52m:50s remains)
INFO - root - 2017-12-07 08:10:43.409278: step 19750, loss = 0.83, batch loss = 0.75 (7.5 examples/sec; 1.063 sec/batch; 92h:19m:05s remains)
INFO - root - 2017-12-07 08:10:53.733220: step 19760, loss = 0.75, batch loss = 0.67 (7.7 examples/sec; 1.045 sec/batch; 90h:45m:18s remains)
INFO - root - 2017-12-07 08:11:03.952684: step 19770, loss = 0.77, batch loss = 0.69 (7.8 examples/sec; 1.027 sec/batch; 89h:13m:53s remains)
INFO - root - 2017-12-07 08:11:14.394250: step 19780, loss = 1.03, batch loss = 0.96 (7.6 examples/sec; 1.051 sec/batch; 91h:15m:41s remains)
INFO - root - 2017-12-07 08:11:24.871936: step 19790, loss = 0.66, batch loss = 0.59 (7.5 examples/sec; 1.072 sec/batch; 93h:07m:58s remains)
INFO - root - 2017-12-07 08:11:35.266879: step 19800, loss = 1.00, batch loss = 0.93 (7.7 examples/sec; 1.034 sec/batch; 89h:50m:09s remains)
2017-12-07 08:11:36.062503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9853222 -2.2489104 -2.419404 -2.5522466 -2.6443348 -2.8689635 -3.4255776 -3.9522607 -4.1423984 -3.9702518 -3.3059714 -2.3857412 -1.6091762 -1.4424131 -2.1590657][-1.873656 -2.13674 -2.3567419 -2.5789084 -2.7009544 -2.7987885 -3.142159 -3.6770053 -4.03419 -3.9116962 -3.1832104 -2.0877597 -1.0938139 -0.77892685 -1.4603534][-2.2918894 -2.4274476 -2.55483 -2.6659448 -2.6351271 -2.5503561 -2.6789398 -3.1386042 -3.6147592 -3.6216443 -2.9727626 -1.8607991 -0.80382919 -0.46884584 -1.0680354][-3.0598974 -3.0522294 -3.0116355 -2.9219866 -2.6834044 -2.4436 -2.3415256 -2.5776463 -3.0369866 -3.2946405 -2.9800358 -2.031621 -0.99165058 -0.62175703 -0.99897885][-3.6195011 -3.4102154 -3.1688116 -2.9564357 -2.664474 -2.4146485 -2.1573582 -2.1252553 -2.495785 -3.0385208 -3.2495828 -2.6178281 -1.5304456 -0.94837117 -1.0076501][-3.7796404 -3.3345852 -2.9191437 -2.6413016 -2.3405373 -2.0834312 -1.6991472 -1.4525623 -1.716563 -2.4258049 -3.1677675 -3.023572 -1.983973 -1.1516564 -0.94942856][-3.7813556 -3.2345924 -2.7633021 -2.4220083 -2.028151 -1.616761 -1.0084391 -0.56076956 -0.74747705 -1.4686782 -2.569833 -2.9932747 -2.2621686 -1.4205592 -1.0620847][-3.7473207 -3.2706947 -2.8688555 -2.4755023 -1.9407253 -1.3091097 -0.54911351 -0.037952423 -0.14463997 -0.70235348 -1.8423691 -2.6775517 -2.4242496 -1.8326399 -1.3960989][-3.7231834 -3.4069176 -3.1066761 -2.6943908 -2.0012059 -1.1037121 -0.22854567 0.24399471 0.11577845 -0.36355305 -1.386071 -2.3591926 -2.4650536 -2.1799297 -1.7950871][-3.5257983 -3.3334835 -3.1297107 -2.7638664 -1.958616 -0.8247335 0.11463356 0.50825357 0.25791073 -0.32763195 -1.2503543 -2.1493149 -2.4184825 -2.3033473 -2.0485857][-3.0238013 -2.9623556 -2.8840163 -2.6348393 -1.8397405 -0.69845748 0.10638046 0.34867811 -0.058172226 -0.7885139 -1.518965 -2.1586316 -2.3727572 -2.221607 -2.0762265][-2.5148649 -2.5807877 -2.6351683 -2.5580997 -1.9197512 -0.99416375 -0.44496298 -0.36958504 -0.86642504 -1.6408558 -2.1292536 -2.4268913 -2.4184971 -2.0819545 -1.9730799][-2.1037629 -2.3331892 -2.499485 -2.5709057 -2.1844881 -1.6150706 -1.3702171 -1.3916693 -1.7721522 -2.3443935 -2.5633974 -2.6137958 -2.4521031 -1.9725666 -1.8316433][-1.7694221 -2.1370151 -2.3630755 -2.5225472 -2.401973 -2.1629286 -2.1103857 -2.1388581 -2.3286533 -2.6261287 -2.6336324 -2.5627675 -2.3594162 -1.8478997 -1.6642065][-1.6785743 -2.0476427 -2.2420847 -2.3992908 -2.4604039 -2.4077537 -2.3610885 -2.3424475 -2.4180138 -2.5133634 -2.4131567 -2.3250504 -2.2031097 -1.8029442 -1.6012681]]...]
INFO - root - 2017-12-07 08:11:46.642762: step 19810, loss = 0.84, batch loss = 0.76 (7.7 examples/sec; 1.036 sec/batch; 89h:56m:34s remains)
INFO - root - 2017-12-07 08:11:57.270283: step 19820, loss = 0.70, batch loss = 0.63 (7.3 examples/sec; 1.093 sec/batch; 94h:57m:58s remains)
INFO - root - 2017-12-07 08:12:07.743429: step 19830, loss = 0.73, batch loss = 0.66 (7.7 examples/sec; 1.037 sec/batch; 90h:03m:35s remains)
INFO - root - 2017-12-07 08:12:18.159554: step 19840, loss = 0.87, batch loss = 0.79 (7.5 examples/sec; 1.060 sec/batch; 92h:01m:56s remains)
INFO - root - 2017-12-07 08:12:28.511218: step 19850, loss = 0.87, batch loss = 0.80 (7.5 examples/sec; 1.063 sec/batch; 92h:19m:11s remains)
INFO - root - 2017-12-07 08:12:38.860001: step 19860, loss = 0.80, batch loss = 0.73 (7.7 examples/sec; 1.040 sec/batch; 90h:16m:29s remains)
INFO - root - 2017-12-07 08:12:49.071475: step 19870, loss = 0.89, batch loss = 0.82 (7.6 examples/sec; 1.050 sec/batch; 91h:10m:46s remains)
INFO - root - 2017-12-07 08:12:59.514707: step 19880, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.061 sec/batch; 92h:08m:31s remains)
INFO - root - 2017-12-07 08:13:09.905918: step 19890, loss = 0.94, batch loss = 0.87 (8.1 examples/sec; 0.993 sec/batch; 86h:15m:13s remains)
INFO - root - 2017-12-07 08:13:20.430014: step 19900, loss = 0.92, batch loss = 0.84 (7.6 examples/sec; 1.055 sec/batch; 91h:35m:41s remains)
2017-12-07 08:13:21.192450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9848452 -1.6578708 -1.5985715 -1.676116 -1.7615023 -1.8602507 -2.0685902 -2.3391535 -2.4432788 -2.2679183 -2.0173991 -1.9017835 -1.7359459 -1.4990835 -1.2077582][-1.8762894 -1.6334951 -1.719197 -1.8355463 -1.864259 -1.9746385 -2.2488487 -2.5152059 -2.5434718 -2.3720467 -2.1933448 -2.042124 -1.773973 -1.5247011 -1.3243804][-2.0391343 -1.9664569 -2.0898824 -2.0806541 -1.9708812 -2.0157292 -2.2813592 -2.531605 -2.5416193 -2.4554343 -2.4398346 -2.316915 -2.001657 -1.7780638 -1.6916361][-2.3651679 -2.3440356 -2.4021456 -2.3094029 -2.1629643 -2.1098571 -2.2128129 -2.364696 -2.416364 -2.443202 -2.5248637 -2.3676918 -1.9917891 -1.7712111 -1.7351239][-2.4209611 -2.3527308 -2.3942702 -2.3915358 -2.3607595 -2.2573197 -2.1606803 -2.1989694 -2.3131425 -2.3846555 -2.3892803 -2.1019139 -1.6514094 -1.4156682 -1.3758712][-2.2436459 -2.1802464 -2.329967 -2.4747684 -2.5581274 -2.3968904 -2.1034985 -1.9876766 -2.0765946 -2.1235232 -2.0194972 -1.6834083 -1.295191 -1.1295843 -1.0805347][-2.1847813 -2.2200403 -2.499249 -2.6876853 -2.714169 -2.4300389 -1.9279783 -1.5930035 -1.5579352 -1.5679319 -1.4685223 -1.2673025 -1.1390216 -1.1629171 -1.1597576][-2.3115549 -2.408252 -2.6879036 -2.8166668 -2.7800126 -2.458703 -1.8856964 -1.4156454 -1.2884669 -1.277446 -1.2376492 -1.1895413 -1.2492385 -1.4114761 -1.4763706][-2.359638 -2.4041655 -2.5482683 -2.6115744 -2.5884976 -2.3371487 -1.8328474 -1.4466922 -1.3587089 -1.3034749 -1.2502291 -1.2109098 -1.2559333 -1.3627326 -1.4103625][-2.2155061 -2.116137 -2.1036949 -2.1806033 -2.259619 -2.1151648 -1.7467873 -1.5429795 -1.54005 -1.4179943 -1.2231281 -1.0682039 -1.0192609 -0.99638247 -0.96556425][-2.1475213 -1.9243698 -1.7989385 -1.9071517 -2.0650668 -1.9904757 -1.7314773 -1.6658537 -1.7125843 -1.5428526 -1.2183416 -0.98146272 -0.88303089 -0.7732501 -0.70483112][-2.3884091 -2.1759949 -2.0162048 -2.1336844 -2.2848318 -2.1923206 -2.0000534 -1.9769704 -2.0257771 -1.8509202 -1.5115192 -1.2687898 -1.1417358 -1.0161331 -0.96351719][-2.729203 -2.6165783 -2.4869285 -2.5585904 -2.6346316 -2.5218215 -2.4088347 -2.4206152 -2.4580302 -2.3085334 -2.0201008 -1.809701 -1.6850388 -1.5849884 -1.5335648][-2.9547224 -2.8841591 -2.7609181 -2.7422323 -2.7271404 -2.6200523 -2.5695782 -2.6124539 -2.6673689 -2.5791593 -2.3729031 -2.2316351 -2.1739523 -2.1282761 -2.0861993][-3.050808 -2.95779 -2.8161039 -2.7281637 -2.6519923 -2.5566196 -2.5258064 -2.5637443 -2.614285 -2.5810633 -2.4730887 -2.4196398 -2.4462566 -2.4660671 -2.4605787]]...]
INFO - root - 2017-12-07 08:13:31.632356: step 19910, loss = 0.81, batch loss = 0.74 (7.8 examples/sec; 1.020 sec/batch; 88h:33m:18s remains)
INFO - root - 2017-12-07 08:13:42.017782: step 19920, loss = 0.72, batch loss = 0.65 (7.4 examples/sec; 1.074 sec/batch; 93h:15m:04s remains)
INFO - root - 2017-12-07 08:13:52.446068: step 19930, loss = 0.92, batch loss = 0.85 (7.6 examples/sec; 1.048 sec/batch; 91h:00m:53s remains)
INFO - root - 2017-12-07 08:14:02.891331: step 19940, loss = 0.91, batch loss = 0.83 (7.6 examples/sec; 1.049 sec/batch; 91h:02m:00s remains)
INFO - root - 2017-12-07 08:14:13.372319: step 19950, loss = 0.66, batch loss = 0.59 (7.8 examples/sec; 1.029 sec/batch; 89h:19m:12s remains)
INFO - root - 2017-12-07 08:14:23.712361: step 19960, loss = 0.75, batch loss = 0.68 (7.7 examples/sec; 1.042 sec/batch; 90h:25m:50s remains)
INFO - root - 2017-12-07 08:14:34.018720: step 19970, loss = 0.86, batch loss = 0.79 (7.6 examples/sec; 1.057 sec/batch; 91h:45m:45s remains)
INFO - root - 2017-12-07 08:14:44.346445: step 19980, loss = 0.67, batch loss = 0.60 (7.8 examples/sec; 1.026 sec/batch; 89h:06m:11s remains)
INFO - root - 2017-12-07 08:14:54.711120: step 19990, loss = 0.75, batch loss = 0.68 (7.9 examples/sec; 1.012 sec/batch; 87h:53m:01s remains)
INFO - root - 2017-12-07 08:15:05.075923: step 20000, loss = 0.56, batch loss = 0.49 (7.7 examples/sec; 1.039 sec/batch; 90h:09m:34s remains)
2017-12-07 08:15:05.849845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0592356 -1.1220641 -1.1237381 -1.2889948 -1.5191276 -1.6288776 -1.7836437 -1.9093254 -1.9082701 -1.932246 -1.878777 -1.7253504 -1.5687296 -1.3117537 -1.1008258][-1.0850091 -1.0794494 -0.96554756 -1.0272317 -1.1416004 -1.2278121 -1.4268961 -1.5964422 -1.6921351 -1.8545105 -1.9209588 -1.8716393 -1.7484372 -1.4298275 -1.1549807][-1.1349144 -1.1035335 -0.94241548 -0.95021939 -0.97500777 -1.0714476 -1.3150301 -1.4956374 -1.6323874 -1.883157 -2.0446639 -2.1077063 -2.0675638 -1.7491322 -1.4552569][-1.0653055 -1.0389948 -0.90719318 -0.93264818 -0.90829062 -1.0086849 -1.2089434 -1.266474 -1.3271527 -1.6316228 -1.9152117 -2.1368675 -2.26288 -2.0178869 -1.7236111][-0.90177274 -0.82626295 -0.71155739 -0.74763823 -0.6829474 -0.771368 -0.8386507 -0.65438747 -0.57533264 -0.95802522 -1.3899553 -1.7888896 -2.114342 -1.9834924 -1.7046139][-0.71785522 -0.55124378 -0.41699076 -0.41758823 -0.29948473 -0.32684946 -0.18132067 0.30986595 0.49945307 0.0010957718 -0.561378 -1.0862038 -1.6036699 -1.6423893 -1.4577434][-0.56260419 -0.31478834 -0.17182302 -0.13550806 0.0031542778 -0.0065422058 0.30352736 1.0041566 1.2140751 0.70445013 0.23836327 -0.2171979 -0.81844664 -1.0361466 -1.0312228][-0.48810124 -0.23691511 -0.17268848 -0.17713118 -0.11000538 -0.18254089 0.17304564 0.83897734 0.959291 0.64884663 0.53616953 0.34999371 -0.10805702 -0.34518909 -0.42519665][-0.54073048 -0.41744661 -0.55571532 -0.67848492 -0.71186948 -0.79998374 -0.41813993 0.11563158 0.14683104 0.11306953 0.38551855 0.49515676 0.26836348 0.13167858 0.14069319][-0.70502281 -0.74445248 -1.1083694 -1.3347194 -1.389432 -1.4483163 -1.0642371 -0.65472865 -0.67431545 -0.52153635 -0.014151573 0.33138657 0.32032919 0.32730818 0.48851967][-0.936656 -1.083339 -1.559468 -1.7904921 -1.8085332 -1.8492551 -1.5274858 -1.2249694 -1.2406945 -1.0005231 -0.38492775 0.10011673 0.22379351 0.32559443 0.54583025][-1.0802698 -1.2568104 -1.7034621 -1.8505962 -1.8204465 -1.8554971 -1.6375766 -1.4346528 -1.434572 -1.1655977 -0.56067085 -0.0935998 0.03913784 0.15653658 0.34143972][-1.0194824 -1.1565456 -1.505075 -1.5703437 -1.5098572 -1.5535307 -1.4474778 -1.3689687 -1.4119039 -1.1780097 -0.66840935 -0.31693411 -0.23770809 -0.10639143 0.038589][-0.81730008 -0.89945722 -1.1536334 -1.1744137 -1.0800562 -1.1297958 -1.144201 -1.1838768 -1.2796817 -1.0900471 -0.68427658 -0.45659637 -0.42670584 -0.28229809 -0.15616608][-0.6285646 -0.68865561 -0.90040016 -0.91088986 -0.78657222 -0.81672287 -0.8872757 -0.93007326 -0.9768126 -0.79381037 -0.49720478 -0.388299 -0.39341831 -0.25140476 -0.1277833]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 08:15:16.756416: step 20010, loss = 0.72, batch loss = 0.64 (7.7 examples/sec; 1.039 sec/batch; 90h:12m:32s remains)
INFO - root - 2017-12-07 08:15:27.205132: step 20020, loss = 0.51, batch loss = 0.44 (7.4 examples/sec; 1.076 sec/batch; 93h:21m:59s remains)
INFO - root - 2017-12-07 08:15:37.558314: step 20030, loss = 0.77, batch loss = 0.70 (7.9 examples/sec; 1.017 sec/batch; 88h:17m:31s remains)
INFO - root - 2017-12-07 08:15:47.952229: step 20040, loss = 0.84, batch loss = 0.77 (7.6 examples/sec; 1.054 sec/batch; 91h:31m:15s remains)
INFO - root - 2017-12-07 08:15:58.341234: step 20050, loss = 0.77, batch loss = 0.69 (7.6 examples/sec; 1.055 sec/batch; 91h:32m:37s remains)
INFO - root - 2017-12-07 08:16:08.801966: step 20060, loss = 0.86, batch loss = 0.79 (7.4 examples/sec; 1.081 sec/batch; 93h:49m:23s remains)
INFO - root - 2017-12-07 08:16:18.953565: step 20070, loss = 0.90, batch loss = 0.83 (7.8 examples/sec; 1.031 sec/batch; 89h:27m:10s remains)
INFO - root - 2017-12-07 08:16:29.454153: step 20080, loss = 0.89, batch loss = 0.82 (7.8 examples/sec; 1.028 sec/batch; 89h:14m:17s remains)
INFO - root - 2017-12-07 08:16:39.924478: step 20090, loss = 0.65, batch loss = 0.58 (7.8 examples/sec; 1.021 sec/batch; 88h:37m:33s remains)
INFO - root - 2017-12-07 08:16:50.506506: step 20100, loss = 0.91, batch loss = 0.84 (7.6 examples/sec; 1.050 sec/batch; 91h:05m:56s remains)
2017-12-07 08:16:51.277493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.767673 -2.9102967 -3.0006981 -3.0594287 -3.0232663 -2.8929987 -2.8193674 -2.8026667 -2.7142081 -2.6717494 -2.8698468 -3.1524866 -3.3304515 -3.466702 -3.5747271][-1.882195 -2.1069837 -2.2730663 -2.3976829 -2.3953531 -2.2541146 -2.0997739 -1.967339 -1.7010779 -1.533174 -1.7534761 -2.0903146 -2.3761113 -2.7051973 -3.0581083][-1.1596985 -1.3155119 -1.5413888 -1.7703333 -1.8637352 -1.8558435 -1.8330395 -1.8329582 -1.6487606 -1.4649999 -1.5950594 -1.721874 -1.7812037 -2.0069377 -2.4346085][-1.1342976 -1.1034524 -1.3193297 -1.6437438 -1.8184216 -1.8948436 -1.9110417 -2.0349481 -2.1022224 -2.10545 -2.2421877 -2.1454487 -1.7752595 -1.5690851 -1.8339677][-1.937607 -1.7624598 -1.7954264 -1.9546027 -2.0279255 -2.1309032 -2.2120693 -2.4647694 -2.7892728 -3.0216839 -3.3065543 -3.1714473 -2.4359088 -1.6779716 -1.5785248][-2.6211805 -2.4932265 -2.4769382 -2.4612598 -2.3851902 -2.3813291 -2.4360533 -2.7657838 -3.2279844 -3.54766 -3.88574 -3.7735741 -2.9163532 -1.9287219 -1.6390135][-3.0372031 -2.9587154 -2.9203961 -2.7569523 -2.5341461 -2.2344148 -1.9471927 -2.0775373 -2.4624281 -2.7625585 -3.1751778 -3.2096443 -2.5926571 -1.8836906 -1.7981856][-2.9844732 -2.9755526 -2.8628886 -2.5076673 -2.0611203 -1.3924091 -0.78366518 -0.83634162 -1.2932894 -1.6991394 -2.2728999 -2.506979 -2.1212077 -1.7129807 -1.8898447][-2.1570959 -2.3579121 -2.4549429 -2.3295238 -1.9655137 -1.1709969 -0.41137838 -0.4347558 -0.87850118 -1.340827 -2.032258 -2.3761055 -2.0337434 -1.659435 -1.8808331][-1.5707507 -1.8455062 -2.0553212 -2.1616006 -2.0674973 -1.5418794 -1.0574822 -1.1640007 -1.432853 -1.7499466 -2.3366573 -2.6271682 -2.2481811 -1.7792497 -1.833045][-1.3650017 -1.5415647 -1.6423802 -1.775341 -1.8385506 -1.6313548 -1.5398633 -1.8456898 -2.095494 -2.3662419 -2.775352 -2.8986998 -2.5185513 -1.9909093 -1.8497524][-1.5096936 -1.4606326 -1.3438537 -1.4097419 -1.5057473 -1.4852931 -1.641396 -1.9897292 -2.2150657 -2.5146258 -2.8471518 -2.900362 -2.6460986 -2.1898327 -2.0040514][-2.1950362 -2.1278982 -1.9052205 -1.7825408 -1.5790231 -1.3634684 -1.4811399 -1.7635171 -2.0278184 -2.4827175 -2.9289434 -3.0666537 -2.9435482 -2.5766273 -2.4112456][-2.7403998 -2.876756 -2.816802 -2.5883203 -2.1080174 -1.6422327 -1.5076458 -1.5713131 -1.8032062 -2.3980951 -3.0287464 -3.3130088 -3.2664766 -2.9548149 -2.8272829][-3.1839952 -3.5565195 -3.7105539 -3.5441391 -3.0773406 -2.585052 -2.3094478 -2.1919744 -2.2934213 -2.8120241 -3.426116 -3.720443 -3.6248388 -3.2856603 -3.1392081]]...]
INFO - root - 2017-12-07 08:17:01.711560: step 20110, loss = 0.86, batch loss = 0.79 (7.6 examples/sec; 1.047 sec/batch; 90h:48m:38s remains)
INFO - root - 2017-12-07 08:17:12.131492: step 20120, loss = 0.91, batch loss = 0.84 (7.5 examples/sec; 1.062 sec/batch; 92h:11m:03s remains)
INFO - root - 2017-12-07 08:17:22.520313: step 20130, loss = 0.86, batch loss = 0.78 (7.8 examples/sec; 1.025 sec/batch; 88h:55m:42s remains)
INFO - root - 2017-12-07 08:17:32.922041: step 20140, loss = 0.97, batch loss = 0.90 (7.8 examples/sec; 1.031 sec/batch; 89h:26m:55s remains)
INFO - root - 2017-12-07 08:17:43.285001: step 20150, loss = 0.97, batch loss = 0.90 (7.8 examples/sec; 1.025 sec/batch; 88h:54m:10s remains)
INFO - root - 2017-12-07 08:17:53.731893: step 20160, loss = 0.84, batch loss = 0.77 (7.5 examples/sec; 1.067 sec/batch; 92h:34m:33s remains)
INFO - root - 2017-12-07 08:18:03.785472: step 20170, loss = 0.82, batch loss = 0.74 (7.9 examples/sec; 1.010 sec/batch; 87h:40m:09s remains)
INFO - root - 2017-12-07 08:18:14.196920: step 20180, loss = 0.86, batch loss = 0.78 (7.6 examples/sec; 1.057 sec/batch; 91h:39m:53s remains)
INFO - root - 2017-12-07 08:18:24.591912: step 20190, loss = 0.81, batch loss = 0.74 (7.7 examples/sec; 1.044 sec/batch; 90h:34m:47s remains)
INFO - root - 2017-12-07 08:18:35.049440: step 20200, loss = 0.76, batch loss = 0.68 (7.8 examples/sec; 1.032 sec/batch; 89h:32m:08s remains)
2017-12-07 08:18:35.811684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9338424 -2.6415062 -2.6066942 -2.6971767 -2.5080795 -2.3171179 -1.6898928 -0.49382949 0.42289877 0.6803174 0.20759964 -0.740088 -1.3293216 -1.4824257 -1.6404822][-3.6611788 -3.4696574 -3.5406868 -3.5685036 -3.2496438 -2.998158 -2.4335313 -1.4409657 -0.60985875 -0.12701368 -0.013355255 -0.33607006 -0.74477363 -1.1291544 -1.6483552][-3.7361121 -3.4650996 -3.3750234 -3.0776284 -2.4327893 -2.0632195 -1.7219176 -1.3752313 -1.2510002 -1.1058972 -0.79579425 -0.515218 -0.39076948 -0.51078343 -1.0140026][-3.7405303 -3.4071341 -3.1457386 -2.5410337 -1.5852745 -0.85841227 -0.29465628 -0.14549541 -0.65774369 -1.2697775 -1.4768775 -1.1875455 -0.6613133 -0.24165821 -0.31265926][-3.5947289 -3.2816687 -2.9345732 -2.2505369 -1.234823 -0.23776579 0.73133183 1.1508245 0.46893311 -0.74146104 -1.7330692 -2.0096619 -1.5361433 -0.81333447 -0.49294758][-3.3637009 -3.0291293 -2.5661588 -1.8338978 -0.86592674 0.28166676 1.5956831 2.4544845 1.9783683 0.50053644 -1.139925 -2.1834056 -2.168839 -1.5789835 -1.2837424][-2.9595482 -2.7156916 -2.3349743 -1.73001 -0.94404411 0.23143148 1.8193164 3.2321887 3.2945371 1.9897714 0.1407342 -1.359926 -1.6911128 -1.3376744 -1.2558074][-2.0103834 -1.8381107 -1.716722 -1.4374876 -1.0509465 -0.16453457 1.3397102 2.9433279 3.568953 2.8345327 1.2226071 -0.35548592 -0.861923 -0.74928641 -0.83920717][-1.0960329 -1.005971 -1.1903958 -1.1993237 -1.0946012 -0.52001762 0.66747427 2.0277138 2.964982 2.8462696 1.6928692 0.39748144 -0.084059238 -0.15843821 -0.36417341][-0.19751358 -0.19037628 -0.68413973 -1.0010085 -1.1255288 -0.81583953 -0.031592369 0.85422564 1.701385 1.8858886 1.2307334 0.62395859 0.58668661 0.63648605 0.51935482][0.658092 0.77659845 0.1575017 -0.41741276 -0.84609342 -0.92052555 -0.56547618 -0.11575317 0.44287157 0.59968758 0.20928192 0.11250639 0.42765284 0.68467569 0.76653767][0.725965 1.086915 0.61385632 0.023003101 -0.52567506 -0.85388088 -0.82738566 -0.71291828 -0.47892714 -0.4268527 -0.62889266 -0.47325325 -0.014648438 0.2672739 0.3043232][0.31391382 0.9106493 0.77495384 0.4076376 0.037183285 -0.34340382 -0.55665874 -0.78424883 -0.92366791 -0.98099637 -1.0214949 -0.73567939 -0.2367506 -0.003390789 -0.060078144][-0.30415726 0.36974049 0.57950544 0.60071039 0.68017387 0.53918266 0.28297424 -0.13584995 -0.59567404 -0.83140254 -0.96011114 -0.84079313 -0.46680856 -0.28106833 -0.34380293][-1.3138878 -0.8202951 -0.50796223 -0.17667437 0.35369396 0.61577129 0.5784502 0.22320223 -0.3415103 -0.64123464 -0.79630876 -0.82788634 -0.6717124 -0.6287868 -0.78779411]]...]
INFO - root - 2017-12-07 08:18:46.455783: step 20210, loss = 1.00, batch loss = 0.92 (7.5 examples/sec; 1.064 sec/batch; 92h:15m:54s remains)
INFO - root - 2017-12-07 08:18:56.820599: step 20220, loss = 0.84, batch loss = 0.76 (7.6 examples/sec; 1.046 sec/batch; 90h:42m:48s remains)
INFO - root - 2017-12-07 08:19:07.247639: step 20230, loss = 0.64, batch loss = 0.56 (7.7 examples/sec; 1.035 sec/batch; 89h:47m:43s remains)
INFO - root - 2017-12-07 08:19:17.616811: step 20240, loss = 0.90, batch loss = 0.83 (7.8 examples/sec; 1.021 sec/batch; 88h:34m:57s remains)
INFO - root - 2017-12-07 08:19:28.053397: step 20250, loss = 0.79, batch loss = 0.72 (7.7 examples/sec; 1.037 sec/batch; 89h:58m:13s remains)
INFO - root - 2017-12-07 08:19:38.454653: step 20260, loss = 0.78, batch loss = 0.71 (7.8 examples/sec; 1.032 sec/batch; 89h:29m:16s remains)
INFO - root - 2017-12-07 08:19:48.772932: step 20270, loss = 0.91, batch loss = 0.84 (7.6 examples/sec; 1.054 sec/batch; 91h:23m:11s remains)
INFO - root - 2017-12-07 08:19:59.163234: step 20280, loss = 0.89, batch loss = 0.82 (7.7 examples/sec; 1.038 sec/batch; 90h:00m:48s remains)
INFO - root - 2017-12-07 08:20:09.289052: step 20290, loss = 0.74, batch loss = 0.67 (10.5 examples/sec; 0.761 sec/batch; 66h:01m:14s remains)
INFO - root - 2017-12-07 08:20:17.027482: step 20300, loss = 0.95, batch loss = 0.88 (9.9 examples/sec; 0.806 sec/batch; 69h:55m:38s remains)
2017-12-07 08:20:17.613123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7982595 -3.7895381 -3.7710114 -3.7183702 -3.8078277 -3.9439802 -3.8830528 -3.7893596 -3.6655865 -3.5583618 -3.6685297 -3.8518305 -3.8539195 -3.7357283 -3.6418951][-4.0302839 -3.9933219 -3.8525331 -3.6533098 -3.6785181 -3.8377972 -3.9073126 -3.9362905 -3.7525158 -3.447907 -3.4154935 -3.5800631 -3.651907 -3.6279271 -3.6082144][-4.0764823 -4.032434 -3.7444737 -3.3405406 -3.1746964 -3.2548151 -3.4590962 -3.6633644 -3.5554419 -3.1846042 -3.0117264 -3.0719609 -3.1546204 -3.22654 -3.2940731][-3.9268613 -3.857939 -3.4414883 -2.8613615 -2.4884195 -2.4842741 -2.7884803 -3.1457975 -3.2295656 -3.0069776 -2.8154945 -2.7012503 -2.5821419 -2.5124426 -2.4837966][-3.6873088 -3.6217213 -3.2093749 -2.5953138 -2.062959 -1.8532801 -2.0276132 -2.4234381 -2.8288302 -3.0498304 -3.0785732 -2.9132528 -2.6074615 -2.3295565 -2.084826][-3.6697316 -3.5894721 -3.2094574 -2.6828842 -2.0879705 -1.6071174 -1.467319 -1.6960843 -2.3245721 -3.0015435 -3.3421454 -3.3166 -3.0957286 -2.8531637 -2.534811][-3.4991302 -3.3362393 -2.9470239 -2.4724464 -1.7880335 -1.0652897 -0.52364182 -0.42860174 -1.1561272 -2.1959486 -2.9097347 -3.1948175 -3.2968588 -3.322865 -3.1505218][-3.338865 -3.2349119 -2.9556973 -2.5188537 -1.7440386 -0.85145283 0.039813519 0.591156 0.14618826 -0.83847284 -1.730993 -2.3942759 -2.9488449 -3.3411465 -3.4043663][-3.282393 -3.4596639 -3.4941719 -3.26857 -2.6716738 -1.8798864 -0.91447568 0.015495777 0.24631453 -0.080709934 -0.71828437 -1.5631566 -2.4712925 -3.1925468 -3.4884844][-3.077179 -3.4188862 -3.6773329 -3.739495 -3.5191016 -3.0957432 -2.3592639 -1.3563473 -0.58543348 -0.14580393 -0.28846359 -1.0059285 -2.0135067 -2.9926558 -3.6340694][-2.8227978 -3.0765619 -3.3624907 -3.6420329 -3.7917967 -3.759217 -3.3794 -2.6332016 -1.764689 -0.9343884 -0.51661134 -0.67259073 -1.3171992 -2.3079743 -3.3058043][-2.616709 -2.5889678 -2.69514 -3.0446491 -3.4299221 -3.6753833 -3.6684642 -3.3619504 -2.8295052 -2.1599789 -1.5574725 -1.172478 -1.202383 -1.7999249 -2.7681823][-2.7084837 -2.4019215 -2.1978953 -2.3645282 -2.7031038 -2.9719303 -3.1940563 -3.2988439 -3.2643943 -3.0740917 -2.7263303 -2.2904115 -1.9928355 -2.0769012 -2.5615442][-2.7830055 -2.4663043 -2.0957489 -1.9974625 -2.0920408 -2.1865892 -2.3667517 -2.6091728 -2.8636665 -3.0651097 -3.0697961 -2.9286323 -2.7249942 -2.552968 -2.5235159][-2.7459922 -2.6155124 -2.2467875 -2.0425286 -1.9875784 -1.9336596 -1.9746649 -2.0919466 -2.2611594 -2.4757814 -2.6307731 -2.7882829 -2.9104252 -2.9054539 -2.7901239]]...]
INFO - root - 2017-12-07 08:20:25.452609: step 20310, loss = 0.91, batch loss = 0.83 (10.4 examples/sec; 0.768 sec/batch; 66h:36m:17s remains)
INFO - root - 2017-12-07 08:20:33.250678: step 20320, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.755 sec/batch; 65h:30m:12s remains)
INFO - root - 2017-12-07 08:20:41.076544: step 20330, loss = 0.71, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 66h:09m:32s remains)
INFO - root - 2017-12-07 08:20:48.779189: step 20340, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.780 sec/batch; 67h:37m:05s remains)
INFO - root - 2017-12-07 08:20:56.427476: step 20350, loss = 0.69, batch loss = 0.62 (10.6 examples/sec; 0.757 sec/batch; 65h:37m:44s remains)
INFO - root - 2017-12-07 08:21:03.948579: step 20360, loss = 0.78, batch loss = 0.71 (10.6 examples/sec; 0.758 sec/batch; 65h:41m:43s remains)
INFO - root - 2017-12-07 08:21:11.400302: step 20370, loss = 0.84, batch loss = 0.76 (10.3 examples/sec; 0.776 sec/batch; 67h:18m:36s remains)
INFO - root - 2017-12-07 08:21:18.928880: step 20380, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.782 sec/batch; 67h:48m:31s remains)
INFO - root - 2017-12-07 08:21:26.591840: step 20390, loss = 0.69, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 66h:33m:16s remains)
INFO - root - 2017-12-07 08:21:34.288042: step 20400, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.766 sec/batch; 66h:22m:24s remains)
2017-12-07 08:21:34.891008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7688587 -2.0325148 -2.4075613 -2.9125791 -3.5396802 -4.0645609 -4.3213806 -4.4035153 -4.411572 -4.3045506 -4.1308479 -3.9507966 -3.7750559 -3.6696908 -3.6869278][-1.845108 -2.0679197 -2.3804133 -2.8402622 -3.4224682 -3.9036729 -4.1666713 -4.321506 -4.4220881 -4.378006 -4.2142677 -3.9814384 -3.6858883 -3.4455683 -3.3981578][-1.92384 -2.1154304 -2.3773801 -2.7725024 -3.2824926 -3.712137 -3.9600477 -4.1252694 -4.2166967 -4.1642127 -4.0107169 -3.7913589 -3.4883468 -3.2223604 -3.1478453][-2.2215686 -2.3819911 -2.5677514 -2.8372126 -3.1913805 -3.508157 -3.707917 -3.8536887 -3.8861299 -3.7855058 -3.6340184 -3.4580319 -3.2270098 -3.0264516 -2.9548664][-2.5831 -2.7093558 -2.8321636 -2.9907265 -3.1751537 -3.3322978 -3.4256465 -3.5113215 -3.4867857 -3.3744025 -3.2748158 -3.1676743 -3.0049162 -2.847636 -2.7358947][-2.627501 -2.7541995 -2.8881609 -3.0213737 -3.1132863 -3.1310072 -3.0992942 -3.1036773 -3.0632038 -3.0216737 -3.0463786 -3.0485129 -2.9475539 -2.7909055 -2.6137953][-2.3188994 -2.4632907 -2.6465926 -2.8264143 -2.9280784 -2.9380398 -2.9027638 -2.8824942 -2.8193426 -2.7949114 -2.8654287 -2.8988731 -2.7995443 -2.61444 -2.39973][-1.8260229 -1.9736316 -2.1826172 -2.4014547 -2.5668273 -2.6832919 -2.7726316 -2.8295479 -2.7819018 -2.7204733 -2.7024233 -2.6337156 -2.4574223 -2.2305198 -2.0170465][-1.3904362 -1.5165133 -1.7109644 -1.9086897 -2.0766478 -2.2399175 -2.4122369 -2.5427389 -2.556246 -2.5246358 -2.473484 -2.3326364 -2.1017299 -1.8522062 -1.6494131][-1.1262319 -1.2078528 -1.3525147 -1.486073 -1.6053743 -1.7472837 -1.9124529 -2.0464065 -2.0978482 -2.1211679 -2.1027596 -1.9781499 -1.7686169 -1.5398445 -1.357919][-1.0049634 -1.0370305 -1.1122873 -1.1698139 -1.2293787 -1.3198195 -1.4235594 -1.5080044 -1.5606627 -1.615469 -1.6320598 -1.546747 -1.3909581 -1.2151017 -1.0661385][-0.88768339 -0.86960173 -0.87253046 -0.86622238 -0.87908411 -0.93298006 -0.9874115 -1.0271652 -1.0689654 -1.1287174 -1.1596086 -1.099925 -0.98870945 -0.868942 -0.764004][-0.83526134 -0.77699208 -0.7260232 -0.68086052 -0.66693616 -0.689692 -0.7017374 -0.70400071 -0.72669649 -0.77109289 -0.78907728 -0.73571444 -0.65434432 -0.58630919 -0.52921796][-0.842679 -0.76240015 -0.68098259 -0.61533475 -0.57790732 -0.5586369 -0.5232718 -0.48663211 -0.47783589 -0.49667048 -0.49967051 -0.44971609 -0.38595152 -0.35552502 -0.34033871][-0.80050755 -0.70878148 -0.61080265 -0.53918552 -0.49103093 -0.43776298 -0.36270571 -0.29848194 -0.27457333 -0.28553152 -0.29761934 -0.27488518 -0.23912287 -0.23952579 -0.2544837]]...]
INFO - root - 2017-12-07 08:21:42.512588: step 20410, loss = 0.81, batch loss = 0.74 (10.3 examples/sec; 0.774 sec/batch; 67h:06m:36s remains)
INFO - root - 2017-12-07 08:21:50.193030: step 20420, loss = 0.96, batch loss = 0.89 (10.6 examples/sec; 0.753 sec/batch; 65h:18m:46s remains)
INFO - root - 2017-12-07 08:21:57.932155: step 20430, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.754 sec/batch; 65h:20m:18s remains)
INFO - root - 2017-12-07 08:22:05.570255: step 20440, loss = 0.66, batch loss = 0.59 (10.8 examples/sec; 0.741 sec/batch; 64h:12m:27s remains)
INFO - root - 2017-12-07 08:22:13.403098: step 20450, loss = 0.65, batch loss = 0.57 (10.1 examples/sec; 0.792 sec/batch; 68h:38m:56s remains)
INFO - root - 2017-12-07 08:22:21.208744: step 20460, loss = 0.65, batch loss = 0.58 (10.1 examples/sec; 0.794 sec/batch; 68h:48m:30s remains)
INFO - root - 2017-12-07 08:22:28.755419: step 20470, loss = 0.73, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 69h:46m:54s remains)
INFO - root - 2017-12-07 08:22:36.445335: step 20480, loss = 0.98, batch loss = 0.90 (10.5 examples/sec; 0.761 sec/batch; 65h:55m:27s remains)
INFO - root - 2017-12-07 08:22:44.146187: step 20490, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.776 sec/batch; 67h:13m:10s remains)
INFO - root - 2017-12-07 08:22:51.800473: step 20500, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.761 sec/batch; 65h:58m:46s remains)
2017-12-07 08:22:52.382636: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0823972 -0.97809196 -1.2666183 -1.4784572 -1.6976981 -1.6830606 -1.2000399 -0.66214609 -0.37417459 -0.42994118 -0.92989922 -1.4828837 -1.6382143 -1.5668526 -1.4128308][-1.0007267 -0.97088146 -1.1123018 -1.2942336 -1.5808752 -1.6103098 -1.2176688 -0.80046105 -0.48274255 -0.41453886 -0.79771209 -1.2564099 -1.4292762 -1.5001853 -1.6271818][-0.890131 -1.0534012 -1.1370316 -1.2271032 -1.3597064 -1.248378 -0.908828 -0.715286 -0.53956962 -0.44304276 -0.65914273 -0.85590792 -0.85792136 -0.99568272 -1.3755054][-1.009156 -1.2842689 -1.3256106 -1.2878594 -1.1449528 -0.778111 -0.38384533 -0.335567 -0.37551069 -0.39632082 -0.51540732 -0.44323659 -0.21145678 -0.39131451 -0.96169591][-1.2943151 -1.4812996 -1.4910491 -1.3814993 -1.0196643 -0.41588497 0.084597588 0.10865879 -0.17925406 -0.46896005 -0.61527538 -0.338068 0.072914124 -0.13110733 -0.77238727][-1.5001538 -1.5910337 -1.6772268 -1.6385427 -1.1590178 -0.3742547 0.34187174 0.56770658 0.11184072 -0.5756371 -0.98275852 -0.74887037 -0.30494833 -0.48055387 -1.0344853][-1.6348317 -1.6134739 -1.776896 -1.8200853 -1.265389 -0.36652184 0.58544159 1.2645092 0.7576952 -0.42294788 -1.27248 -1.2870011 -0.89240861 -0.8934164 -1.1825387][-1.8288922 -1.7128806 -1.8120325 -1.7799153 -1.1986756 -0.38687038 0.64989138 1.78721 1.4340153 0.017483234 -1.1355002 -1.3661566 -1.0765092 -0.91004491 -1.0215909][-1.724848 -1.6685653 -1.7533464 -1.6660516 -1.2375438 -0.799479 -0.078965187 1.069694 1.1593766 0.088719368 -0.92426085 -1.1921542 -1.0132327 -0.80099607 -0.92095542][-1.1558287 -1.30953 -1.5106483 -1.5041721 -1.4170499 -1.3905814 -1.0251555 -0.089793682 0.32946253 -0.27925634 -1.0067523 -1.303091 -1.2837243 -1.0232227 -1.0862186][-0.72569895 -0.93851733 -1.1755061 -1.2650504 -1.5134997 -1.7832534 -1.6272106 -0.97456336 -0.49129963 -0.77255011 -1.2662165 -1.5877206 -1.7099526 -1.4172249 -1.3076735][-0.58539915 -0.590153 -0.70642138 -0.88276649 -1.3505538 -1.7389073 -1.6753056 -1.2893908 -0.94233537 -1.1113355 -1.4608479 -1.7666209 -1.9220514 -1.5861092 -1.3179669][-0.80051184 -0.39614487 -0.26418447 -0.49131608 -1.096024 -1.4998438 -1.4382169 -1.1901541 -0.9849968 -1.1564958 -1.4766858 -1.7951336 -2.0172722 -1.8009491 -1.5059268][-1.5222635 -0.7266593 -0.28060484 -0.41545582 -0.98950076 -1.3373778 -1.2787957 -1.1382272 -1.0339816 -1.1963682 -1.4854987 -1.8153002 -2.0760779 -1.9917378 -1.7263734][-2.6946731 -1.7728684 -1.0220268 -0.79293966 -0.99520326 -1.1228929 -1.0456865 -1.0426223 -1.1220279 -1.349988 -1.6087108 -1.8725481 -2.1128621 -2.1272755 -1.9524324]]...]
INFO - root - 2017-12-07 08:23:00.041096: step 20510, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.751 sec/batch; 65h:03m:17s remains)
INFO - root - 2017-12-07 08:23:07.803362: step 20520, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.759 sec/batch; 65h:45m:41s remains)
INFO - root - 2017-12-07 08:23:15.485270: step 20530, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.766 sec/batch; 66h:24m:36s remains)
INFO - root - 2017-12-07 08:23:23.104286: step 20540, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.784 sec/batch; 67h:58m:21s remains)
INFO - root - 2017-12-07 08:23:30.854018: step 20550, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.778 sec/batch; 67h:26m:11s remains)
INFO - root - 2017-12-07 08:23:38.389429: step 20560, loss = 0.96, batch loss = 0.89 (10.5 examples/sec; 0.760 sec/batch; 65h:53m:34s remains)
INFO - root - 2017-12-07 08:23:45.769534: step 20570, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.768 sec/batch; 66h:33m:53s remains)
INFO - root - 2017-12-07 08:23:53.521108: step 20580, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.761 sec/batch; 65h:57m:33s remains)
INFO - root - 2017-12-07 08:24:01.235413: step 20590, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.779 sec/batch; 67h:29m:23s remains)
INFO - root - 2017-12-07 08:24:09.131534: step 20600, loss = 0.95, batch loss = 0.88 (10.2 examples/sec; 0.783 sec/batch; 67h:49m:18s remains)
2017-12-07 08:24:09.763083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2948346 -4.4125104 -4.3699188 -4.2548933 -4.0576248 -3.8313336 -3.7466149 -3.8469236 -3.8760567 -3.8483713 -3.9166389 -3.9265 -3.84511 -3.7842355 -3.8244269][-4.7593 -4.8395495 -4.7476416 -4.6401672 -4.5385337 -4.3958721 -4.2905164 -4.2853789 -4.1837263 -4.0233622 -4.0535846 -4.0938892 -4.08252 -4.0200233 -3.9578395][-4.744513 -4.7767596 -4.6968536 -4.7150412 -4.8623552 -4.9593272 -4.9257612 -4.8511 -4.6857224 -4.4575062 -4.447495 -4.4744062 -4.4200683 -4.23063 -3.9726942][-3.9492481 -3.9041932 -3.8859954 -4.1156321 -4.5755978 -4.95985 -5.033371 -4.91952 -4.764904 -4.5756893 -4.5798287 -4.6002369 -4.47916 -4.16459 -3.7772331][-2.6052475 -2.5422673 -2.6708555 -3.1031601 -3.7079566 -4.1529145 -4.2088122 -4.0437946 -3.9482555 -3.9329453 -4.0886364 -4.2052984 -4.1039314 -3.8020704 -3.4644089][-1.5222981 -1.5704253 -1.9145207 -2.452549 -2.9080286 -3.0924692 -2.9281092 -2.6454778 -2.59474 -2.8103175 -3.2205224 -3.5171852 -3.529408 -3.363153 -3.2329202][-1.4046898 -1.7185652 -2.2351594 -2.6552286 -2.7345536 -2.5034626 -2.0793691 -1.6887655 -1.6637721 -2.0472839 -2.63757 -3.0654531 -3.1698987 -3.1086049 -3.1521206][-2.1674666 -2.5559 -2.9876246 -3.1399982 -2.9157612 -2.5253749 -2.1372559 -1.8653953 -1.8468485 -2.1367886 -2.5846214 -2.9203978 -3.0444808 -3.0307589 -3.1064439][-2.8540044 -3.1558549 -3.4184365 -3.3739021 -3.11827 -2.9150219 -2.82057 -2.7248521 -2.6071515 -2.6244063 -2.7387118 -2.8430426 -2.9452138 -2.9833422 -3.0150986][-2.8181198 -3.0234094 -3.1698661 -3.0764523 -2.9941931 -3.1059284 -3.3097608 -3.3436017 -3.1494284 -2.9915757 -2.8786964 -2.7747474 -2.7901044 -2.8056958 -2.7530351][-2.3188505 -2.4457071 -2.4955668 -2.4045947 -2.4942091 -2.8237915 -3.2171905 -3.3429313 -3.1537189 -2.9622152 -2.7869096 -2.5632339 -2.4579377 -2.3854551 -2.2388403][-1.9061301 -2.0146594 -2.0077565 -1.934427 -2.0956063 -2.4315763 -2.8013065 -2.8726239 -2.6768236 -2.5278959 -2.4020317 -2.1888578 -2.0315213 -1.9148426 -1.7460735][-1.766037 -1.8969648 -1.8904293 -1.8496702 -2.0207644 -2.2706165 -2.4921784 -2.4216416 -2.1596243 -2.0273497 -1.9538176 -1.8384476 -1.7349067 -1.6684735 -1.5729377][-1.7391217 -1.8607585 -1.8673205 -1.8643415 -2.0331166 -2.2225902 -2.3323181 -2.187988 -1.9093041 -1.7863393 -1.7304118 -1.699867 -1.6788957 -1.7199042 -1.7601929][-1.7216063 -1.7778106 -1.7692635 -1.7699842 -1.8931262 -2.0146213 -2.0662546 -1.9557054 -1.7636948 -1.7052767 -1.6820886 -1.6957912 -1.7255859 -1.8602321 -2.0129]]...]
INFO - root - 2017-12-07 08:24:17.344706: step 20610, loss = 0.93, batch loss = 0.86 (10.6 examples/sec; 0.754 sec/batch; 65h:18m:40s remains)
INFO - root - 2017-12-07 08:24:24.959950: step 20620, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.765 sec/batch; 66h:18m:37s remains)
INFO - root - 2017-12-07 08:24:32.508040: step 20630, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.761 sec/batch; 65h:53m:18s remains)
INFO - root - 2017-12-07 08:24:40.216803: step 20640, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.761 sec/batch; 65h:56m:05s remains)
INFO - root - 2017-12-07 08:24:47.853270: step 20650, loss = 0.94, batch loss = 0.87 (10.8 examples/sec; 0.742 sec/batch; 64h:16m:18s remains)
INFO - root - 2017-12-07 08:24:55.637545: step 20660, loss = 0.92, batch loss = 0.85 (10.2 examples/sec; 0.788 sec/batch; 68h:14m:14s remains)
INFO - root - 2017-12-07 08:25:03.190270: step 20670, loss = 0.87, batch loss = 0.80 (10.2 examples/sec; 0.786 sec/batch; 68h:02m:35s remains)
INFO - root - 2017-12-07 08:25:10.782663: step 20680, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.755 sec/batch; 65h:22m:35s remains)
INFO - root - 2017-12-07 08:25:18.578709: step 20690, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 68h:50m:17s remains)
INFO - root - 2017-12-07 08:25:26.386283: step 20700, loss = 0.75, batch loss = 0.68 (10.6 examples/sec; 0.757 sec/batch; 65h:35m:59s remains)
2017-12-07 08:25:26.966056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0623813 -4.0131626 -3.9782376 -3.9287143 -3.8434534 -3.7116177 -3.5224848 -3.2631564 -3.0760281 -2.9890304 -2.8673995 -2.718668 -2.5556974 -2.4100604 -2.3228068][-3.6298978 -3.677527 -3.8078933 -3.9634914 -4.0694532 -4.0825891 -3.9809484 -3.7675905 -3.619766 -3.590852 -3.5471425 -3.4740789 -3.3809423 -3.2694707 -3.1548052][-2.8152843 -2.8612885 -3.0014548 -3.1736412 -3.3041849 -3.3582623 -3.2929676 -3.1268535 -3.0729938 -3.2230029 -3.4360375 -3.6452661 -3.795764 -3.8547816 -3.8270116][-2.0758231 -2.0547988 -2.1286879 -2.2103918 -2.2359958 -2.1968884 -2.0493906 -1.8319035 -1.798171 -2.0468552 -2.4204123 -2.8613224 -3.2708149 -3.5824447 -3.7693539][-1.7123153 -1.6527078 -1.6939723 -1.6999183 -1.6242795 -1.4988987 -1.3143294 -1.1113625 -1.1070418 -1.3175671 -1.5827396 -1.9382443 -2.3261089 -2.7043605 -3.0085709][-1.9399829 -1.9441402 -2.0162044 -1.9735348 -1.8245292 -1.640897 -1.4620235 -1.3588026 -1.3999944 -1.4500628 -1.4203064 -1.4926901 -1.6678607 -1.9455874 -2.2318602][-2.3168006 -2.4509392 -2.671401 -2.7611451 -2.6879096 -2.4966714 -2.2534564 -2.1357913 -2.1538274 -2.0786915 -1.8614025 -1.7080896 -1.6343591 -1.6637859 -1.739723][-2.3436971 -2.4990537 -2.7897873 -3.054944 -3.1586545 -3.0291939 -2.7664828 -2.6624985 -2.7256837 -2.6522841 -2.4279156 -2.2208505 -1.994482 -1.8078623 -1.6477511][-2.52526 -2.5521054 -2.6575065 -2.8419228 -2.9642444 -2.8944046 -2.6800241 -2.6289444 -2.72615 -2.6397924 -2.4496825 -2.3462722 -2.2195935 -2.1335065 -2.0216508][-3.0668674 -3.054553 -2.9555104 -2.9259357 -2.9265575 -2.8400328 -2.7096753 -2.7340047 -2.8399096 -2.6697588 -2.3796346 -2.2023842 -2.0328324 -1.9923935 -1.9961851][-3.4618449 -3.5539775 -3.4649305 -3.3496795 -3.2423916 -3.0842168 -2.9724154 -3.0538955 -3.1935034 -3.0210662 -2.6834455 -2.4178834 -2.1196814 -1.9434597 -1.8578112][-3.1964576 -3.4735756 -3.5926657 -3.5955431 -3.5473814 -3.4180155 -3.3312533 -3.4440069 -3.6326706 -3.5569553 -3.3219004 -3.1149504 -2.8171487 -2.5609403 -2.3523402][-2.648314 -2.9169927 -3.0971937 -3.1924021 -3.2928252 -3.3451848 -3.4573026 -3.7346344 -4.0431485 -4.1063809 -4.0125337 -3.9008994 -3.6811986 -3.4696126 -3.280251][-2.6194675 -2.6378751 -2.5727119 -2.4798579 -2.4703188 -2.5223663 -2.7508373 -3.1610069 -3.5924211 -3.8376527 -3.9703054 -4.0652747 -4.0323725 -3.9714925 -3.9152682][-2.8138294 -2.6727922 -2.4538953 -2.2440925 -2.0677295 -1.9632912 -2.0687802 -2.3013523 -2.5681705 -2.75355 -2.9364676 -3.1578143 -3.3085985 -3.4650686 -3.650192]]...]
INFO - root - 2017-12-07 08:25:34.616750: step 20710, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.766 sec/batch; 66h:21m:06s remains)
INFO - root - 2017-12-07 08:25:42.240355: step 20720, loss = 0.62, batch loss = 0.55 (10.0 examples/sec; 0.799 sec/batch; 69h:11m:11s remains)
INFO - root - 2017-12-07 08:25:49.846137: step 20730, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.779 sec/batch; 67h:29m:27s remains)
INFO - root - 2017-12-07 08:25:57.581854: step 20740, loss = 0.95, batch loss = 0.87 (10.5 examples/sec; 0.762 sec/batch; 65h:59m:04s remains)
INFO - root - 2017-12-07 08:26:05.269287: step 20750, loss = 0.69, batch loss = 0.61 (10.4 examples/sec; 0.769 sec/batch; 66h:33m:29s remains)
INFO - root - 2017-12-07 08:26:12.889399: step 20760, loss = 0.75, batch loss = 0.67 (10.4 examples/sec; 0.769 sec/batch; 66h:33m:23s remains)
INFO - root - 2017-12-07 08:26:20.219793: step 20770, loss = 0.82, batch loss = 0.74 (10.8 examples/sec; 0.741 sec/batch; 64h:11m:27s remains)
INFO - root - 2017-12-07 08:26:27.820311: step 20780, loss = 0.88, batch loss = 0.80 (10.6 examples/sec; 0.757 sec/batch; 65h:32m:11s remains)
INFO - root - 2017-12-07 08:26:35.536139: step 20790, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.784 sec/batch; 67h:51m:38s remains)
INFO - root - 2017-12-07 08:26:43.111327: step 20800, loss = 0.80, batch loss = 0.72 (10.8 examples/sec; 0.740 sec/batch; 64h:02m:51s remains)
2017-12-07 08:26:43.743424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2582421 -4.2072687 -3.9727876 -3.7960415 -3.7138686 -3.8566582 -4.219892 -4.4336295 -4.3706746 -4.3031206 -4.3436365 -4.4191985 -4.4357343 -4.3473325 -4.2388358][-3.722476 -3.715436 -3.5342364 -3.3982115 -3.2574754 -3.3475106 -3.7723691 -4.1787291 -4.3090258 -4.3602872 -4.4042678 -4.4745803 -4.4854264 -4.4105382 -4.3350458][-3.2239804 -3.2653012 -3.1966732 -3.1083667 -2.8234639 -2.6654983 -2.9938097 -3.5163536 -3.8603809 -4.14778 -4.3381047 -4.4268041 -4.413373 -4.3623552 -4.3696909][-3.2619438 -3.3009117 -3.2867637 -3.2044191 -2.7504587 -2.305202 -2.4164782 -2.8462238 -3.2569299 -3.8110464 -4.2517695 -4.3871865 -4.3245287 -4.21346 -4.1783719][-3.3248119 -3.3392458 -3.3020985 -3.0822613 -2.3724468 -1.633522 -1.3801463 -1.4353547 -1.6805351 -2.5289495 -3.4155493 -3.8136883 -3.9528759 -3.9573591 -3.9478314][-3.0932307 -3.1425836 -3.125262 -2.8133073 -1.9458909 -1.0326903 -0.43202496 -0.013788223 0.0099687576 -0.973403 -2.1299512 -2.6996846 -3.07983 -3.2494278 -3.2538724][-2.8422666 -2.936635 -3.0447781 -2.7916265 -2.0665963 -1.2919366 -0.62212777 0.042551994 0.29428196 -0.375844 -1.1982975 -1.5957677 -2.0632577 -2.362026 -2.3717682][-2.6955237 -2.7238033 -2.909987 -2.8183675 -2.4315028 -1.9979837 -1.5023215 -0.87809443 -0.45580649 -0.55301213 -0.78700519 -0.94260216 -1.5128703 -1.9779305 -2.0754418][-2.82975 -2.7022254 -2.7810087 -2.7171421 -2.5773363 -2.4961791 -2.3114436 -1.9442687 -1.5165472 -1.2044134 -0.91880727 -0.7483933 -1.0787404 -1.3056257 -1.2202229][-3.1919916 -2.8016496 -2.567246 -2.3197448 -2.2298124 -2.3678589 -2.5185897 -2.5194802 -2.2506769 -1.8376522 -1.3647833 -1.0683396 -1.1777029 -1.1668403 -0.93722773][-3.494312 -2.9584208 -2.5039692 -2.1342621 -2.0667858 -2.2918389 -2.6441908 -2.9184074 -2.8628533 -2.5679245 -2.24879 -2.1326389 -2.2897165 -2.2539802 -2.0266855][-3.6345472 -3.1541448 -2.6569784 -2.2755008 -2.2363913 -2.4733334 -2.8654993 -3.2732224 -3.3945081 -3.2538748 -3.0653868 -2.9739304 -2.9806728 -2.7575893 -2.4701946][-3.6160333 -3.27402 -2.8112595 -2.4111969 -2.2595408 -2.3132107 -2.4845943 -2.7669897 -2.9083915 -2.8760791 -2.8379064 -2.8466129 -2.8085258 -2.5259156 -2.2406626][-3.386857 -3.1836078 -2.7372184 -2.2684495 -1.94982 -1.7965412 -1.7902064 -1.991986 -2.1796575 -2.185905 -2.1737292 -2.204953 -2.0857072 -1.7346437 -1.4272072][-2.8294158 -2.7830775 -2.3805032 -1.9155896 -1.5817351 -1.4340358 -1.4283171 -1.6490884 -1.9273722 -1.9823995 -1.9779413 -1.9708226 -1.8011258 -1.4622493 -1.1684027]]...]
INFO - root - 2017-12-07 08:26:51.464654: step 20810, loss = 0.70, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 67h:18m:48s remains)
INFO - root - 2017-12-07 08:26:59.136770: step 20820, loss = 0.66, batch loss = 0.59 (10.5 examples/sec; 0.759 sec/batch; 65h:42m:03s remains)
INFO - root - 2017-12-07 08:27:06.811258: step 20830, loss = 0.50, batch loss = 0.43 (10.6 examples/sec; 0.758 sec/batch; 65h:36m:31s remains)
INFO - root - 2017-12-07 08:27:14.404205: step 20840, loss = 0.69, batch loss = 0.62 (10.6 examples/sec; 0.757 sec/batch; 65h:34m:24s remains)
INFO - root - 2017-12-07 08:27:22.072539: step 20850, loss = 0.67, batch loss = 0.60 (10.7 examples/sec; 0.751 sec/batch; 65h:00m:32s remains)
INFO - root - 2017-12-07 08:27:29.714594: step 20860, loss = 0.77, batch loss = 0.69 (10.6 examples/sec; 0.754 sec/batch; 65h:17m:45s remains)
INFO - root - 2017-12-07 08:27:37.185005: step 20870, loss = 0.61, batch loss = 0.54 (10.4 examples/sec; 0.766 sec/batch; 66h:18m:32s remains)
INFO - root - 2017-12-07 08:27:44.861335: step 20880, loss = 0.64, batch loss = 0.57 (10.2 examples/sec; 0.785 sec/batch; 67h:56m:15s remains)
INFO - root - 2017-12-07 08:27:52.517009: step 20890, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.773 sec/batch; 66h:56m:11s remains)
INFO - root - 2017-12-07 08:28:00.116775: step 20900, loss = 0.81, batch loss = 0.74 (10.3 examples/sec; 0.777 sec/batch; 67h:16m:07s remains)
2017-12-07 08:28:00.694406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.61392689 -1.1192276 -1.6811833 -2.0805428 -2.274991 -2.2939987 -2.2007947 -1.9983108 -1.7650583 -1.5767927 -1.38854 -1.210304 -1.0742042 -0.98448253 -0.88203192][0.48891592 -0.13226366 -0.88282871 -1.4927506 -1.8608162 -2.0277152 -2.0689516 -1.9577107 -1.7989643 -1.6755834 -1.5343552 -1.3865643 -1.2698503 -1.199976 -1.1363654][0.53385639 0.1268611 -0.4720962 -1.0136008 -1.3267353 -1.5041554 -1.5855515 -1.5261524 -1.4160421 -1.2951052 -1.1041634 -0.87589192 -0.68725109 -0.58476782 -0.56290054][-0.33392859 -0.44915271 -0.70428395 -0.9502697 -1.0195634 -1.0864778 -1.1378107 -1.1138585 -1.06513 -0.95270872 -0.74113345 -0.47366142 -0.23318148 -0.064442635 0.0087189674][-1.1189828 -1.0307603 -1.0115941 -1.0192416 -0.91540551 -0.9369874 -0.9968009 -1.0049984 -1.0017385 -0.88546753 -0.68548155 -0.44415903 -0.19105244 0.022122383 0.16777515][-1.5544949 -1.329787 -1.0763967 -0.841207 -0.55328536 -0.51420569 -0.62644744 -0.74809885 -0.897387 -0.90208936 -0.85741091 -0.80018425 -0.64549661 -0.42410851 -0.17935371][-1.6899021 -1.3671765 -0.93275547 -0.45442748 0.080992222 0.31163645 0.28623247 0.15804386 -0.092144012 -0.24296427 -0.42404079 -0.697546 -0.83954382 -0.80867887 -0.663074][-1.6976099 -1.3568285 -0.85192895 -0.208457 0.58642435 1.1110091 1.3307977 1.3331332 1.0417061 0.76054811 0.41465616 -0.11965132 -0.5210743 -0.65907621 -0.63119745][-1.831491 -1.6361706 -1.2633328 -0.67107272 0.19390869 0.9057169 1.3662505 1.5858932 1.4043307 1.1618724 0.82980013 0.25778961 -0.23088217 -0.40916634 -0.38422203][-2.0618236 -2.047389 -1.8927619 -1.5214489 -0.86676741 -0.2641449 0.20279026 0.53397942 0.54343271 0.50272131 0.37032127 0.0048279762 -0.35809183 -0.45484734 -0.36039495][-2.2423763 -2.3431587 -2.3559785 -2.1968656 -1.8094165 -1.4160609 -1.0647156 -0.75450993 -0.63637662 -0.56483865 -0.5564642 -0.72324944 -0.91492033 -0.92735052 -0.79093742][-2.2688634 -2.3973424 -2.4866867 -2.46488 -2.3003991 -2.1057599 -1.9110138 -1.7114782 -1.5978694 -1.5169809 -1.4726403 -1.5303099 -1.6247108 -1.6311917 -1.5540812][-2.2172506 -2.3034716 -2.3738434 -2.3930485 -2.3498044 -2.2796032 -2.1959314 -2.1013615 -2.0518143 -2.0187716 -2.00495 -2.0330987 -2.0747459 -2.0889218 -2.0887635][-2.17544 -2.2112722 -2.2467849 -2.2740648 -2.2783685 -2.2573617 -2.2200749 -2.1765344 -2.1564531 -2.1513422 -2.1597736 -2.1774256 -2.1831741 -2.19041 -2.221446][-2.1515832 -2.1467528 -2.1523712 -2.1788855 -2.2101586 -2.2202322 -2.214829 -2.197011 -2.187598 -2.1879802 -2.1904929 -2.1922908 -2.1793933 -2.1721406 -2.1960962]]...]
INFO - root - 2017-12-07 08:28:08.513725: step 20910, loss = 0.73, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 66h:06m:23s remains)
INFO - root - 2017-12-07 08:28:16.222806: step 20920, loss = 1.10, batch loss = 1.02 (10.8 examples/sec; 0.743 sec/batch; 64h:16m:22s remains)
INFO - root - 2017-12-07 08:28:23.945671: step 20930, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.764 sec/batch; 66h:06m:51s remains)
INFO - root - 2017-12-07 08:28:31.589756: step 20940, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.752 sec/batch; 65h:02m:50s remains)
INFO - root - 2017-12-07 08:28:39.189407: step 20950, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.762 sec/batch; 65h:54m:57s remains)
INFO - root - 2017-12-07 08:28:46.855035: step 20960, loss = 1.00, batch loss = 0.93 (10.4 examples/sec; 0.769 sec/batch; 66h:33m:47s remains)
INFO - root - 2017-12-07 08:28:54.272962: step 20970, loss = 0.70, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 68h:38m:00s remains)
INFO - root - 2017-12-07 08:29:02.113031: step 20980, loss = 0.89, batch loss = 0.82 (10.0 examples/sec; 0.803 sec/batch; 69h:27m:28s remains)
INFO - root - 2017-12-07 08:29:09.864776: step 20990, loss = 0.95, batch loss = 0.88 (10.0 examples/sec; 0.796 sec/batch; 68h:54m:35s remains)
INFO - root - 2017-12-07 08:29:17.753715: step 21000, loss = 0.88, batch loss = 0.81 (10.4 examples/sec; 0.772 sec/batch; 66h:46m:09s remains)
2017-12-07 08:29:18.403457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8168621 -2.7894096 -2.7913837 -2.792032 -2.7892704 -2.8070476 -2.8269236 -2.851109 -2.9022737 -2.9134619 -2.8698177 -2.8300352 -2.7926805 -2.7499697 -2.7475069][-2.7431793 -2.7206521 -2.7312276 -2.74659 -2.7416077 -2.7334614 -2.7072983 -2.689466 -2.700809 -2.7185826 -2.7420337 -2.789619 -2.8235998 -2.8439245 -2.8986781][-2.7060626 -2.71165 -2.7424636 -2.7782865 -2.7744799 -2.738553 -2.6572394 -2.5429974 -2.4580526 -2.4548838 -2.5603623 -2.7158957 -2.842006 -2.9412806 -3.0226769][-2.6706295 -2.6973042 -2.744874 -2.7992597 -2.7976732 -2.750144 -2.6053379 -2.3610759 -2.1338575 -2.0859807 -2.2720559 -2.5491073 -2.8082783 -2.9957476 -3.0702395][-2.6272001 -2.6190479 -2.6477654 -2.6836171 -2.6563206 -2.5732808 -2.3565018 -1.9874587 -1.6253371 -1.5570555 -1.8504674 -2.2911515 -2.7164845 -3.0061808 -3.1010003][-2.5543251 -2.4347763 -2.3708854 -2.3152428 -2.1949813 -2.0456839 -1.7837198 -1.3417654 -0.885921 -0.79148316 -1.2013571 -1.82922 -2.4228659 -2.8430209 -3.0535479][-2.3624551 -2.0615306 -1.8233221 -1.588572 -1.3485081 -1.1919572 -1.0001717 -0.61624336 -0.16692781 -0.10920048 -0.63903594 -1.4313884 -2.1309917 -2.6447206 -2.9585974][-2.0764692 -1.5942357 -1.1558738 -0.73593235 -0.42156315 -0.31885862 -0.27445555 -0.07539463 0.1571002 0.017611504 -0.63847089 -1.5166066 -2.2212675 -2.7002478 -2.9895182][-1.9553983 -1.401942 -0.85250807 -0.37466526 -0.13662624 -0.19926786 -0.36600637 -0.41736937 -0.43568039 -0.69544411 -1.2963068 -2.0623229 -2.6569924 -3.0136707 -3.1943769][-2.1756628 -1.7340879 -1.2522604 -0.8732748 -0.80918312 -1.0261531 -1.3114839 -1.4985988 -1.6454964 -1.9103363 -2.3256042 -2.8226838 -3.2019646 -3.374954 -3.4031472][-2.5500164 -2.3263669 -2.0426812 -1.8431752 -1.9074523 -2.1387391 -2.3719778 -2.5136063 -2.6260262 -2.8196454 -3.0551124 -3.3069241 -3.4813514 -3.4994209 -3.423491][-2.817976 -2.7592092 -2.6486149 -2.5970659 -2.7082472 -2.8981266 -3.0582082 -3.1267998 -3.1532784 -3.225421 -3.29334 -3.357434 -3.393002 -3.3335521 -3.220273][-2.9201944 -2.947742 -2.9430995 -2.9739742 -3.07694 -3.195303 -3.2849259 -3.2965159 -3.2632704 -3.2522011 -3.221508 -3.1883855 -3.1510603 -3.0588706 -2.9296017][-2.9486558 -2.9874687 -3.0106897 -3.0552218 -3.1234484 -3.1846747 -3.2168813 -3.1879835 -3.1231596 -3.0809977 -3.0365257 -2.9855552 -2.9262388 -2.8411522 -2.7344837][-2.9735851 -2.9951305 -3.0100455 -3.0326653 -3.0616548 -3.0799289 -3.074198 -3.0269544 -2.9593267 -2.91681 -2.8954828 -2.8744316 -2.8379927 -2.7915735 -2.7367539]]...]
INFO - root - 2017-12-07 08:29:26.051800: step 21010, loss = 0.76, batch loss = 0.68 (10.7 examples/sec; 0.749 sec/batch; 64h:46m:18s remains)
INFO - root - 2017-12-07 08:29:33.740572: step 21020, loss = 0.85, batch loss = 0.78 (10.1 examples/sec; 0.794 sec/batch; 68h:40m:51s remains)
INFO - root - 2017-12-07 08:29:41.307305: step 21030, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.766 sec/batch; 66h:13m:58s remains)
INFO - root - 2017-12-07 08:29:48.987708: step 21040, loss = 0.92, batch loss = 0.85 (10.8 examples/sec; 0.739 sec/batch; 63h:54m:37s remains)
INFO - root - 2017-12-07 08:29:56.819614: step 21050, loss = 0.79, batch loss = 0.71 (10.0 examples/sec; 0.798 sec/batch; 69h:03m:38s remains)
INFO - root - 2017-12-07 08:30:04.506227: step 21060, loss = 0.93, batch loss = 0.86 (10.5 examples/sec; 0.762 sec/batch; 65h:54m:10s remains)
INFO - root - 2017-12-07 08:30:12.006210: step 21070, loss = 0.81, batch loss = 0.74 (10.6 examples/sec; 0.754 sec/batch; 65h:12m:04s remains)
INFO - root - 2017-12-07 08:30:19.740538: step 21080, loss = 0.82, batch loss = 0.75 (10.1 examples/sec; 0.794 sec/batch; 68h:38m:39s remains)
INFO - root - 2017-12-07 08:30:27.465648: step 21090, loss = 0.84, batch loss = 0.76 (10.4 examples/sec; 0.767 sec/batch; 66h:19m:42s remains)
INFO - root - 2017-12-07 08:30:35.155823: step 21100, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.765 sec/batch; 66h:10m:33s remains)
2017-12-07 08:30:35.765299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8858752 -3.9158611 -3.9220109 -3.9110925 -3.8749435 -3.8154917 -3.7427692 -3.6849761 -3.6858056 -3.75554 -3.8349 -3.8855948 -3.8920724 -3.8563423 -3.8107204][-3.8640342 -3.8760157 -3.7897882 -3.6402259 -3.4969153 -3.428129 -3.4288721 -3.4559295 -3.5079577 -3.5862193 -3.6608715 -3.7143214 -3.7449698 -3.7275734 -3.6776741][-3.911094 -3.8264217 -3.5822022 -3.2747746 -3.0880947 -3.1301122 -3.348079 -3.5583963 -3.63032 -3.5931778 -3.5447302 -3.5336015 -3.5914814 -3.6444826 -3.6449146][-3.9162753 -3.7248111 -3.3981428 -3.0770483 -2.9428225 -3.0805249 -3.4371104 -3.7705011 -3.838238 -3.7022898 -3.526674 -3.3992534 -3.425149 -3.5091681 -3.549957][-3.8852963 -3.6814711 -3.4079022 -3.1529241 -3.0216355 -3.0394077 -3.242511 -3.5033238 -3.5694594 -3.4880874 -3.3234923 -3.1784387 -3.2063358 -3.268667 -3.3225324][-3.8255963 -3.646466 -3.4174557 -3.1490383 -2.9056835 -2.681951 -2.5999885 -2.6731932 -2.758286 -2.83534 -2.8433676 -2.8815179 -3.0312724 -3.0870867 -3.1192193][-3.5808814 -3.3360085 -3.0508797 -2.6884961 -2.3210926 -1.918154 -1.5914946 -1.484175 -1.5858853 -1.8368325 -2.0829725 -2.3612072 -2.6469007 -2.7259645 -2.8012486][-3.2507925 -2.8452678 -2.3886375 -1.8467157 -1.3367796 -0.80338955 -0.27665854 -0.017261982 -0.080008507 -0.43514824 -0.91078782 -1.4698341 -1.9063122 -1.981559 -2.0958188][-3.1749215 -2.5434704 -1.8126543 -1.0065277 -0.35897112 0.17939138 0.73318863 0.98117685 0.91835642 0.55225182 -0.018584251 -0.75836825 -1.3410408 -1.4042244 -1.4695783][-3.2365789 -2.50172 -1.6520171 -0.78651381 -0.21335506 0.024819851 0.16965914 0.13153982 0.0956583 0.017253399 -0.17928028 -0.67864561 -1.2063475 -1.303442 -1.3937366][-3.2970896 -2.6623406 -2.00276 -1.376946 -0.96729612 -0.83933377 -0.87916136 -1.1217058 -1.2049513 -1.074228 -0.86435461 -0.998193 -1.3859642 -1.579324 -1.7964246][-3.4518828 -2.9187841 -2.4093289 -1.9376276 -1.5800571 -1.3875105 -1.3651857 -1.582304 -1.6796715 -1.5489411 -1.2755399 -1.2931149 -1.6037521 -1.8455646 -2.1212][-3.5591831 -3.1367922 -2.7033639 -2.2786348 -1.8820906 -1.571908 -1.4029567 -1.4623611 -1.5142365 -1.3985462 -1.1928356 -1.1920969 -1.4362781 -1.7082751 -2.0336671][-3.6266408 -3.3426852 -3.0288496 -2.6748929 -2.2237117 -1.7232869 -1.2769442 -1.089654 -1.071821 -1.0583444 -0.98833323 -0.97848511 -1.1194501 -1.4239831 -1.880204][-3.6492815 -3.4486542 -3.1892729 -2.9009697 -2.5338519 -2.0660589 -1.5472424 -1.1848996 -1.0536377 -1.0800238 -1.1011808 -1.1204739 -1.1997521 -1.5001237 -2.0125237]]...]
INFO - root - 2017-12-07 08:30:43.417435: step 21110, loss = 0.71, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 65h:53m:30s remains)
INFO - root - 2017-12-07 08:30:51.028457: step 21120, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.764 sec/batch; 66h:06m:34s remains)
INFO - root - 2017-12-07 08:30:58.707405: step 21130, loss = 0.98, batch loss = 0.90 (10.5 examples/sec; 0.764 sec/batch; 66h:03m:39s remains)
INFO - root - 2017-12-07 08:31:06.371053: step 21140, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.784 sec/batch; 67h:45m:54s remains)
INFO - root - 2017-12-07 08:31:14.058738: step 21150, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.762 sec/batch; 65h:53m:11s remains)
INFO - root - 2017-12-07 08:31:21.756553: step 21160, loss = 0.73, batch loss = 0.66 (10.8 examples/sec; 0.741 sec/batch; 64h:02m:49s remains)
INFO - root - 2017-12-07 08:31:29.220473: step 21170, loss = 0.75, batch loss = 0.67 (10.5 examples/sec; 0.762 sec/batch; 65h:54m:27s remains)
INFO - root - 2017-12-07 08:31:36.871769: step 21180, loss = 0.91, batch loss = 0.83 (10.5 examples/sec; 0.761 sec/batch; 65h:48m:10s remains)
INFO - root - 2017-12-07 08:31:44.420927: step 21190, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.761 sec/batch; 65h:48m:28s remains)
INFO - root - 2017-12-07 08:31:52.085895: step 21200, loss = 0.77, batch loss = 0.70 (10.7 examples/sec; 0.748 sec/batch; 64h:43m:04s remains)
2017-12-07 08:31:52.744932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.962064 -1.789223 -1.6783953 -1.7237945 -1.9371605 -2.1180189 -2.1625004 -2.0943904 -2.037349 -2.0507693 -2.2120152 -2.5508847 -2.8342819 -3.068068 -3.2651544][-1.30616 -1.1775808 -1.2205374 -1.3191195 -1.5361111 -1.795737 -1.9563997 -1.9135623 -1.8187604 -1.8320417 -2.0351777 -2.3460848 -2.5484142 -2.6981366 -2.8412313][-0.73293996 -0.77682829 -1.0302076 -1.1946323 -1.4451458 -1.8318889 -2.1902523 -2.2355912 -2.0749481 -2.0314536 -2.2016993 -2.4150891 -2.5290735 -2.558918 -2.5933251][-0.50848937 -0.84548068 -1.258724 -1.3808136 -1.554976 -1.9914544 -2.5085363 -2.6414325 -2.4541001 -2.4016879 -2.5373609 -2.6315107 -2.6598108 -2.652422 -2.7088742][-0.75511932 -1.3455882 -1.8026571 -1.7200787 -1.5954602 -1.837723 -2.3064685 -2.4887838 -2.4160159 -2.5082498 -2.7356181 -2.7478914 -2.6378279 -2.6197264 -2.8094263][-1.3875709 -2.0297532 -2.3030276 -1.8626213 -1.3177524 -1.2144396 -1.4153838 -1.5531273 -1.6510525 -1.9246264 -2.2809119 -2.2515168 -2.0588744 -2.0938129 -2.4569745][-2.1647179 -2.6959462 -2.6086609 -1.7684944 -0.86821604 -0.42406464 -0.29092026 -0.38722277 -0.661216 -1.0365436 -1.3697069 -1.2537687 -1.041434 -1.1754217 -1.6846106][-2.9931536 -3.3433471 -2.9452219 -1.8739572 -0.85251832 -0.2797513 -0.054419041 -0.31358624 -0.740407 -1.0493002 -1.1294303 -0.83544326 -0.5960393 -0.78052521 -1.2840691][-3.6833386 -3.8740354 -3.3221464 -2.2238882 -1.2506745 -0.68666816 -0.6117897 -1.2548094 -1.882432 -2.0683582 -1.7921503 -1.2803473 -0.96968031 -1.086838 -1.4575741][-3.870353 -3.9684713 -3.4304771 -2.4296138 -1.5091097 -0.90883112 -0.9816246 -1.9708827 -2.789474 -2.8874788 -2.3667195 -1.7237594 -1.3878198 -1.4048586 -1.6272116][-3.617734 -3.6788249 -3.2315557 -2.3568251 -1.4839993 -0.84734988 -0.97892809 -2.1101623 -2.9855361 -2.9643993 -2.273886 -1.5694873 -1.2686884 -1.2332556 -1.4234674][-3.339422 -3.4079418 -3.1017585 -2.4188259 -1.6963325 -1.1102936 -1.2108424 -2.20789 -2.9398086 -2.7868304 -2.0427544 -1.3019948 -1.0013099 -0.9611032 -1.2181878][-3.2904415 -3.4045973 -3.262846 -2.8449011 -2.365015 -1.8851297 -1.9055698 -2.657032 -3.2041526 -2.9740989 -2.2234542 -1.4159586 -0.99624729 -0.95241523 -1.3220778][-3.3266501 -3.482605 -3.4519026 -3.2474842 -3.003866 -2.644258 -2.5926433 -3.0983875 -3.509403 -3.2984989 -2.5782743 -1.6524529 -0.98548746 -0.83297729 -1.2446902][-3.2885113 -3.4338615 -3.4530296 -3.3961587 -3.3391056 -3.1119564 -3.0216069 -3.3034594 -3.6367204 -3.5223675 -2.886369 -1.8888793 -0.92353678 -0.520715 -0.83212852]]...]
INFO - root - 2017-12-07 08:32:00.545879: step 21210, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.771 sec/batch; 66h:38m:48s remains)
INFO - root - 2017-12-07 08:32:08.372704: step 21220, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.775 sec/batch; 67h:01m:43s remains)
INFO - root - 2017-12-07 08:32:16.119399: step 21230, loss = 0.70, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 67h:32m:51s remains)
INFO - root - 2017-12-07 08:32:23.841340: step 21240, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.771 sec/batch; 66h:40m:17s remains)
INFO - root - 2017-12-07 08:32:31.572069: step 21250, loss = 0.90, batch loss = 0.82 (10.1 examples/sec; 0.789 sec/batch; 68h:12m:13s remains)
INFO - root - 2017-12-07 08:32:39.257459: step 21260, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.769 sec/batch; 66h:30m:35s remains)
INFO - root - 2017-12-07 08:32:46.774025: step 21270, loss = 0.90, batch loss = 0.83 (10.1 examples/sec; 0.796 sec/batch; 68h:47m:14s remains)
INFO - root - 2017-12-07 08:32:54.499168: step 21280, loss = 0.90, batch loss = 0.83 (10.8 examples/sec; 0.742 sec/batch; 64h:08m:56s remains)
INFO - root - 2017-12-07 08:33:02.198839: step 21290, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.762 sec/batch; 65h:49m:58s remains)
INFO - root - 2017-12-07 08:33:10.003447: step 21300, loss = 0.63, batch loss = 0.55 (10.4 examples/sec; 0.771 sec/batch; 66h:40m:32s remains)
2017-12-07 08:33:10.611344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0858893 -1.9381623 -1.9473314 -2.3959992 -2.8476691 -2.6144183 -1.9994957 -1.8962791 -2.3366747 -2.3386774 -1.8105032 -1.3848083 -1.5870509 -2.1363566 -2.4692512][-1.5672033 -1.4325163 -1.5798976 -2.1462021 -2.6159282 -2.2633569 -1.5047135 -1.4044769 -2.010658 -2.2175546 -1.9092023 -1.6340613 -1.7936101 -2.2464352 -2.5077338][-1.2832034 -1.2078497 -1.4358301 -1.9833384 -2.4069397 -2.0487995 -1.2485847 -1.0868058 -1.6720238 -2.0200603 -1.9425349 -1.8303032 -1.9820881 -2.3497674 -2.5421407][-1.4668257 -1.4559832 -1.7618852 -2.275162 -2.6661758 -2.3056729 -1.4170618 -1.0198786 -1.4224854 -1.8338418 -1.9325387 -1.9609857 -2.1296148 -2.4347067 -2.5736876][-1.7413573 -1.7115993 -1.945106 -2.3942955 -2.8172793 -2.5396256 -1.630533 -1.0063734 -1.2201579 -1.6901801 -1.9131186 -2.0300655 -2.210223 -2.481461 -2.5995598][-1.643002 -1.5711002 -1.6655736 -2.0676215 -2.6072264 -2.5208588 -1.7556314 -1.0544207 -1.0909388 -1.5121801 -1.7411454 -1.9166269 -2.1633112 -2.4634166 -2.606451][-1.259408 -1.250459 -1.2901866 -1.6854157 -2.3675754 -2.5809021 -2.1050906 -1.4834156 -1.3434324 -1.5220425 -1.5736339 -1.7369032 -2.0557239 -2.406908 -2.5963869][-1.0810494 -1.2685177 -1.3549495 -1.6623547 -2.2825081 -2.7479947 -2.6280756 -2.1856778 -1.9267647 -1.764909 -1.5199869 -1.5840788 -1.9339044 -2.3332834 -2.5752933][-0.91750932 -1.4123933 -1.6792619 -1.8385224 -2.2399578 -2.8231311 -2.9851208 -2.7442024 -2.4377792 -1.9724422 -1.4415305 -1.4227507 -1.809449 -2.2690716 -2.559005][-0.68378687 -1.3939791 -1.8185513 -1.8205886 -1.9748592 -2.582119 -2.9412656 -2.9374585 -2.69488 -2.064 -1.3683567 -1.2933712 -1.7163103 -2.2328103 -2.5507421][-0.72546935 -1.3568466 -1.7472501 -1.6022294 -1.649807 -2.3030934 -2.84762 -3.0764794 -2.9314957 -2.2612681 -1.4946191 -1.3305035 -1.7216995 -2.2334085 -2.5438766][-1.0081973 -1.3983569 -1.5964992 -1.3390455 -1.4194419 -2.1381705 -2.8552217 -3.2421138 -3.1369834 -2.4855993 -1.7098494 -1.4606078 -1.7815325 -2.2619841 -2.5409493][-1.3463843 -1.4540174 -1.4299734 -1.1197467 -1.2396653 -1.911325 -2.6872811 -3.1990676 -3.1721158 -2.6011529 -1.8651586 -1.5394888 -1.8029437 -2.26889 -2.5332041][-1.786911 -1.7152956 -1.5144596 -1.1769826 -1.2337253 -1.7055461 -2.4322939 -3.0455329 -3.155194 -2.7308269 -2.0267236 -1.5871964 -1.7834744 -2.2534797 -2.5213232][-2.4733257 -2.39082 -2.119489 -1.7684903 -1.6773148 -1.9250884 -2.6086655 -3.28898 -3.4581556 -3.0963085 -2.3538249 -1.7600174 -1.8486683 -2.2776453 -2.5252438]]...]
INFO - root - 2017-12-07 08:33:18.245204: step 21310, loss = 0.75, batch loss = 0.68 (10.4 examples/sec; 0.771 sec/batch; 66h:37m:25s remains)
INFO - root - 2017-12-07 08:33:25.901718: step 21320, loss = 0.84, batch loss = 0.77 (10.3 examples/sec; 0.774 sec/batch; 66h:55m:28s remains)
INFO - root - 2017-12-07 08:33:33.482933: step 21330, loss = 0.56, batch loss = 0.49 (10.6 examples/sec; 0.756 sec/batch; 65h:22m:25s remains)
INFO - root - 2017-12-07 08:33:41.063227: step 21340, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.781 sec/batch; 67h:31m:16s remains)
INFO - root - 2017-12-07 08:33:48.723062: step 21350, loss = 0.84, batch loss = 0.77 (10.7 examples/sec; 0.750 sec/batch; 64h:49m:24s remains)
INFO - root - 2017-12-07 08:33:56.449342: step 21360, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.784 sec/batch; 67h:47m:05s remains)
INFO - root - 2017-12-07 08:34:03.825886: step 21370, loss = 0.66, batch loss = 0.59 (10.1 examples/sec; 0.791 sec/batch; 68h:23m:47s remains)
INFO - root - 2017-12-07 08:34:11.566134: step 21380, loss = 0.96, batch loss = 0.88 (10.4 examples/sec; 0.772 sec/batch; 66h:44m:12s remains)
INFO - root - 2017-12-07 08:34:19.276747: step 21390, loss = 1.06, batch loss = 0.99 (9.9 examples/sec; 0.807 sec/batch; 69h:42m:40s remains)
INFO - root - 2017-12-07 08:34:26.984959: step 21400, loss = 1.15, batch loss = 1.07 (9.6 examples/sec; 0.833 sec/batch; 71h:59m:48s remains)
2017-12-07 08:34:27.592792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5151615 -4.4794278 -4.248877 -4.0237536 -3.913106 -4.0431657 -4.2333775 -4.2855239 -4.224575 -4.0609984 -3.90558 -3.9615481 -4.1601067 -4.4033351 -4.5834923][-5.0311813 -4.8838172 -4.5788136 -4.3994012 -4.36507 -4.5363927 -4.6670232 -4.62704 -4.529789 -4.3606348 -4.1803584 -4.2625556 -4.4925351 -4.7487383 -4.9195852][-5.2404065 -4.9304581 -4.547339 -4.4163303 -4.4172988 -4.5458088 -4.509738 -4.3633132 -4.3059411 -4.2707329 -4.1749964 -4.32447 -4.53152 -4.6894126 -4.7796974][-4.9718943 -4.4696546 -4.056283 -4.0159845 -4.0716438 -4.1917753 -4.0770082 -3.9232337 -3.9568682 -4.0743575 -4.0497642 -4.2259712 -4.3259492 -4.24647 -4.1458344][-4.3082323 -3.6970525 -3.3831844 -3.5008874 -3.5965986 -3.6268635 -3.390754 -3.2055349 -3.2834949 -3.4965599 -3.5443838 -3.7874808 -3.8605261 -3.5956454 -3.3315654][-3.6314893 -3.0583315 -2.8976026 -3.1345284 -3.1927135 -2.9982605 -2.4907606 -2.1413767 -2.1874545 -2.5006986 -2.7245989 -3.1727586 -3.4279537 -3.2140088 -2.9615488][-3.4860759 -3.0065756 -2.8247144 -2.9010761 -2.7788801 -2.2920823 -1.3925741 -0.67712903 -0.57496715 -1.0763924 -1.6901293 -2.5531936 -3.1620908 -3.1982036 -3.1177933][-3.7569947 -3.312 -2.9013491 -2.5984244 -2.2713184 -1.63555 -0.52557969 0.45470572 0.67717123 -0.047332287 -1.1028156 -2.3688769 -3.2273469 -3.3997192 -3.4256182][-3.950036 -3.4863615 -2.836535 -2.238698 -1.9379478 -1.5814412 -0.78201461 0.011350155 0.19711208 -0.54340863 -1.6478963 -2.8964624 -3.6647189 -3.7123551 -3.6677997][-3.9958706 -3.542737 -2.8018472 -2.0991032 -1.9510384 -2.075237 -1.8797987 -1.5247035 -1.393682 -1.8396559 -2.5383885 -3.4034657 -3.9146347 -3.8245533 -3.748096][-3.9035268 -3.510479 -2.8079691 -2.1269379 -2.0410104 -2.4369473 -2.6941638 -2.695914 -2.5818315 -2.6624327 -2.8875055 -3.3392906 -3.6364279 -3.5158818 -3.4967036][-3.4204845 -3.0927396 -2.5264187 -1.9789383 -1.9227073 -2.3871853 -2.8741269 -3.0834324 -2.9772735 -2.8153458 -2.7407107 -2.9211864 -3.0215073 -2.8385823 -2.8412788][-2.6340928 -2.3543797 -1.9799776 -1.714752 -1.8238668 -2.3093297 -2.8185759 -3.0643177 -2.9642839 -2.7416434 -2.59227 -2.658391 -2.5908604 -2.2628076 -2.1558][-2.0985637 -1.9095197 -1.7597406 -1.7984447 -2.0833561 -2.516398 -2.8785915 -3.039444 -2.9780929 -2.8361754 -2.7551246 -2.7789421 -2.6028576 -2.1783884 -1.942575][-2.3506815 -2.29147 -2.291517 -2.4347043 -2.6873105 -2.9341259 -3.087965 -3.1374893 -3.1029105 -3.0491776 -3.0560279 -3.0882058 -2.9149268 -2.5679407 -2.3634858]]...]
INFO - root - 2017-12-07 08:34:35.198332: step 21410, loss = 0.82, batch loss = 0.74 (10.2 examples/sec; 0.782 sec/batch; 67h:35m:14s remains)
INFO - root - 2017-12-07 08:34:43.094762: step 21420, loss = 0.78, batch loss = 0.71 (10.0 examples/sec; 0.801 sec/batch; 69h:13m:50s remains)
INFO - root - 2017-12-07 08:34:50.848306: step 21430, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.766 sec/batch; 66h:13m:06s remains)
INFO - root - 2017-12-07 08:34:58.501292: step 21440, loss = 0.96, batch loss = 0.89 (10.6 examples/sec; 0.754 sec/batch; 65h:10m:25s remains)
INFO - root - 2017-12-07 08:35:06.132577: step 21450, loss = 0.84, batch loss = 0.77 (10.5 examples/sec; 0.761 sec/batch; 65h:43m:13s remains)
INFO - root - 2017-12-07 08:35:13.904523: step 21460, loss = 0.64, batch loss = 0.56 (10.4 examples/sec; 0.766 sec/batch; 66h:12m:11s remains)
INFO - root - 2017-12-07 08:35:21.322022: step 21470, loss = 0.95, batch loss = 0.88 (10.4 examples/sec; 0.766 sec/batch; 66h:12m:24s remains)
INFO - root - 2017-12-07 08:35:29.129462: step 21480, loss = 0.95, batch loss = 0.88 (10.1 examples/sec; 0.793 sec/batch; 68h:32m:56s remains)
INFO - root - 2017-12-07 08:35:36.775876: step 21490, loss = 0.68, batch loss = 0.61 (10.4 examples/sec; 0.771 sec/batch; 66h:38m:40s remains)
INFO - root - 2017-12-07 08:35:44.398703: step 21500, loss = 0.96, batch loss = 0.88 (10.7 examples/sec; 0.746 sec/batch; 64h:28m:07s remains)
2017-12-07 08:35:45.018320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7315955 -2.9079671 -3.1540964 -3.4655793 -3.7002642 -3.8434238 -3.9145617 -3.8740294 -3.7045481 -3.4990368 -3.2838569 -3.1938086 -3.2994237 -3.4553571 -3.616457][-2.6964781 -2.75101 -3.0232382 -3.3689151 -3.6382387 -3.7431803 -3.6963525 -3.5427451 -3.3445528 -3.2020717 -3.0460572 -2.9069681 -2.9173694 -3.0027547 -3.1965961][-2.9149728 -2.8076482 -3.0342255 -3.2805114 -3.3881226 -3.3238738 -3.1858284 -3.0934172 -3.0536833 -3.0367322 -2.8518271 -2.4957924 -2.2794847 -2.2312236 -2.5856214][-3.0138583 -2.8425207 -3.1071653 -3.4151425 -3.507107 -3.4059794 -3.2607427 -3.2376547 -3.2652025 -3.1785436 -2.8085957 -2.2243536 -1.8897562 -1.8760045 -2.4348924][-3.24162 -2.9540534 -3.1113596 -3.3586521 -3.356998 -3.1552892 -2.9329505 -2.9127297 -2.9795516 -2.9035296 -2.5335293 -1.9819214 -1.7146926 -1.8392 -2.4599836][-3.3466339 -2.9362736 -2.944787 -3.0527391 -2.8923998 -2.5184774 -2.1996844 -2.2371225 -2.4939976 -2.6566534 -2.4788036 -1.9863317 -1.6522601 -1.7209344 -2.1757927][-2.6924238 -2.1931806 -2.10649 -2.1207075 -1.9091516 -1.5626528 -1.333385 -1.5427165 -2.0316081 -2.3885863 -2.3409104 -1.9257321 -1.6143672 -1.6328025 -1.8995106][-1.6715822 -1.2484641 -1.1880159 -1.056658 -0.658149 -0.21142769 0.032315254 -0.3401432 -1.158298 -1.8337047 -2.0553966 -1.8863146 -1.7049236 -1.6229062 -1.6376572][-1.2895508 -1.0608652 -0.9893465 -0.53564596 0.17352915 0.71841574 0.95346403 0.46355438 -0.63458753 -1.6023972 -2.1099324 -2.2347994 -2.144872 -1.8479753 -1.5457742][-1.5957203 -1.4833615 -1.2753198 -0.54897928 0.28224707 0.66865873 0.73346853 0.21294212 -0.90592766 -1.9078069 -2.4455018 -2.6841922 -2.6450543 -2.2121205 -1.7250731][-1.9055388 -1.891104 -1.6295676 -0.9051857 -0.27480221 -0.17774391 -0.25443363 -0.67741966 -1.6172414 -2.3899169 -2.7259169 -2.9343071 -2.9394679 -2.4771829 -1.8629973][-2.1324685 -2.3694572 -2.2813933 -1.8023684 -1.4633987 -1.5401838 -1.6515763 -1.8748064 -2.4779286 -2.9109406 -3.0299697 -3.1717558 -3.2149773 -2.8010368 -2.1342795][-2.4835958 -3.0204415 -3.207716 -3.04365 -2.9353151 -3.0524244 -3.0817132 -3.0666387 -3.2522616 -3.3284304 -3.2634068 -3.3115182 -3.3595762 -3.0783038 -2.4965677][-2.6489985 -3.301156 -3.6636162 -3.7255192 -3.7333946 -3.7830784 -3.7231894 -3.5847127 -3.5273082 -3.4630089 -3.3625617 -3.3534408 -3.4296575 -3.3016536 -2.804306][-2.7514338 -3.3596733 -3.7540128 -3.8795922 -3.8485012 -3.7846472 -3.7155926 -3.5815856 -3.4557955 -3.3826151 -3.2980905 -3.3081079 -3.4639266 -3.4533544 -2.9764903]]...]
INFO - root - 2017-12-07 08:35:52.565758: step 21510, loss = 0.80, batch loss = 0.72 (10.8 examples/sec; 0.739 sec/batch; 63h:50m:49s remains)
INFO - root - 2017-12-07 08:36:00.243527: step 21520, loss = 0.75, batch loss = 0.68 (10.2 examples/sec; 0.781 sec/batch; 67h:29m:29s remains)
INFO - root - 2017-12-07 08:36:07.989826: step 21530, loss = 0.72, batch loss = 0.65 (10.6 examples/sec; 0.752 sec/batch; 64h:57m:22s remains)
INFO - root - 2017-12-07 08:36:15.490012: step 21540, loss = 0.58, batch loss = 0.51 (10.4 examples/sec; 0.770 sec/batch; 66h:29m:53s remains)
INFO - root - 2017-12-07 08:36:23.118580: step 21550, loss = 0.97, batch loss = 0.90 (10.3 examples/sec; 0.779 sec/batch; 67h:16m:23s remains)
INFO - root - 2017-12-07 08:36:30.853972: step 21560, loss = 1.03, batch loss = 0.96 (10.2 examples/sec; 0.784 sec/batch; 67h:43m:45s remains)
INFO - root - 2017-12-07 08:36:38.163170: step 21570, loss = 1.12, batch loss = 1.05 (10.3 examples/sec; 0.779 sec/batch; 67h:19m:00s remains)
INFO - root - 2017-12-07 08:36:45.730112: step 21580, loss = 0.73, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 65h:22m:27s remains)
INFO - root - 2017-12-07 08:36:53.426323: step 21590, loss = 0.91, batch loss = 0.84 (10.0 examples/sec; 0.799 sec/batch; 68h:59m:16s remains)
INFO - root - 2017-12-07 08:37:00.969260: step 21600, loss = 0.93, batch loss = 0.86 (10.5 examples/sec; 0.764 sec/batch; 65h:56m:47s remains)
2017-12-07 08:37:01.583171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3867202 -3.36696 -3.3754082 -3.3917513 -3.405571 -3.4326525 -3.4191141 -3.3679914 -3.1877794 -3.0041556 -2.9746251 -3.0064616 -2.9687324 -2.911931 -2.970161][-3.2518458 -3.2422209 -3.23425 -3.2288284 -3.2430944 -3.2746723 -3.25877 -3.1913338 -3.0160539 -2.8762202 -2.8899384 -2.9588411 -2.9346714 -2.8672144 -2.9158974][-2.9599895 -2.9376864 -2.9044657 -2.8828409 -2.8959875 -2.911859 -2.8885894 -2.8162651 -2.655808 -2.5590785 -2.6278346 -2.7776074 -2.8308535 -2.8040576 -2.8607168][-2.6391487 -2.5954878 -2.5441477 -2.5067039 -2.51509 -2.5184603 -2.5096641 -2.4583116 -2.3131669 -2.2323713 -2.3173845 -2.5307584 -2.6737938 -2.7140639 -2.7999735][-2.3050985 -2.2816675 -2.2473912 -2.204411 -2.2145281 -2.2232635 -2.2551558 -2.2535286 -2.1477168 -2.0732522 -2.1382127 -2.3498745 -2.5274832 -2.6180363 -2.7379494][-2.0363262 -2.0459089 -2.0446734 -2.0164163 -2.0357454 -2.0405238 -2.0816512 -2.1117516 -2.0547209 -2.0082967 -2.0697606 -2.2532372 -2.4236073 -2.5432467 -2.6978447][-1.8621829 -1.878263 -1.9146292 -1.9242461 -1.9511237 -1.9266884 -1.9451616 -1.9810133 -1.955158 -1.9430265 -2.0301566 -2.2164831 -2.38385 -2.5135446 -2.6870544][-1.8044806 -1.7902489 -1.8174455 -1.8379261 -1.8547392 -1.7937777 -1.7838049 -1.8144352 -1.8010447 -1.8131425 -1.9436102 -2.1787629 -2.3690794 -2.5048013 -2.6904159][-1.7017357 -1.6341071 -1.5923607 -1.5809398 -1.5959508 -1.5470548 -1.5452595 -1.5972931 -1.6203332 -1.673358 -1.8557303 -2.1463878 -2.3561039 -2.4903388 -2.6837401][-1.6924856 -1.6229689 -1.5304601 -1.4712925 -1.4692354 -1.449677 -1.4725921 -1.557328 -1.6196687 -1.6961515 -1.8899658 -2.1760466 -2.3685336 -2.4898863 -2.6824884][-2.0232754 -2.0121424 -1.9306931 -1.8414702 -1.7969158 -1.7696395 -1.7745662 -1.8544421 -1.9277961 -1.9761693 -2.11644 -2.3357918 -2.4747305 -2.5611224 -2.7221985][-2.4396436 -2.4997513 -2.4848523 -2.4118001 -2.3387685 -2.2903497 -2.2540979 -2.2942009 -2.3421085 -2.3426414 -2.4273477 -2.5801282 -2.6647148 -2.7075443 -2.8119516][-2.7325518 -2.8136358 -2.8434894 -2.8037374 -2.7353768 -2.6818845 -2.6190717 -2.6242704 -2.6352298 -2.6015964 -2.6657617 -2.7847152 -2.8425646 -2.8601322 -2.9140055][-2.8801622 -2.920342 -2.9464054 -2.9311047 -2.8860488 -2.8514376 -2.8081627 -2.8179128 -2.8157518 -2.7727251 -2.829011 -2.9240627 -2.9623237 -2.9636059 -2.9899487][-2.9613211 -2.9710436 -2.9753485 -2.95398 -2.903964 -2.8715935 -2.8661325 -2.9108815 -2.9131508 -2.8686836 -2.917326 -2.9962049 -3.0180483 -3.0079551 -3.0254085]]...]
INFO - root - 2017-12-07 08:37:09.229631: step 21610, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 66h:26m:57s remains)
INFO - root - 2017-12-07 08:37:16.774946: step 21620, loss = 1.04, batch loss = 0.97 (10.5 examples/sec; 0.763 sec/batch; 65h:55m:26s remains)
INFO - root - 2017-12-07 08:37:24.460417: step 21630, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.770 sec/batch; 66h:27m:39s remains)
INFO - root - 2017-12-07 08:37:32.217670: step 21640, loss = 1.09, batch loss = 1.02 (10.1 examples/sec; 0.794 sec/batch; 68h:33m:34s remains)
INFO - root - 2017-12-07 08:37:39.869137: step 21650, loss = 0.68, batch loss = 0.61 (10.3 examples/sec; 0.774 sec/batch; 66h:47m:48s remains)
INFO - root - 2017-12-07 08:37:47.662242: step 21660, loss = 0.73, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 66h:03m:34s remains)
INFO - root - 2017-12-07 08:37:55.124415: step 21670, loss = 0.77, batch loss = 0.70 (10.1 examples/sec; 0.795 sec/batch; 68h:39m:53s remains)
INFO - root - 2017-12-07 08:38:02.905507: step 21680, loss = 0.61, batch loss = 0.54 (10.7 examples/sec; 0.750 sec/batch; 64h:46m:56s remains)
INFO - root - 2017-12-07 08:38:10.746164: step 21690, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.782 sec/batch; 67h:28m:42s remains)
INFO - root - 2017-12-07 08:38:18.437515: step 21700, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.762 sec/batch; 65h:45m:46s remains)
2017-12-07 08:38:19.067371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5962014 -2.8591065 -3.0118966 -3.3689561 -3.4958022 -3.2622249 -3.0560982 -2.9772279 -2.9634223 -3.0438032 -3.1310492 -3.1200838 -3.1625671 -3.3344281 -3.4994032][-2.7560892 -3.118885 -3.4228592 -3.8131766 -3.9171333 -3.5818262 -3.2054615 -2.9932094 -2.9117508 -2.9264181 -2.9885628 -2.9738464 -3.0348306 -3.2129278 -3.3271129][-2.709403 -3.2017076 -3.620806 -3.9799888 -3.9968433 -3.6242146 -3.229229 -2.9941244 -2.8055413 -2.6148386 -2.5290637 -2.4596386 -2.61313 -2.9414968 -3.1790719][-3.0758047 -3.5224149 -3.8358419 -3.9826231 -3.7796385 -3.3337965 -3.0296383 -2.8858302 -2.6169333 -2.2294295 -1.9137428 -1.7219872 -1.9939549 -2.5587111 -2.9973307][-3.5004315 -3.8191693 -3.8543134 -3.6497185 -3.1247578 -2.5706482 -2.3831449 -2.4083929 -2.2447343 -1.9090197 -1.5696511 -1.4020994 -1.8090913 -2.5169997 -3.0236766][-3.3549361 -3.5137506 -3.2911954 -2.8209209 -2.125021 -1.6543391 -1.69138 -1.8718636 -1.7898288 -1.5000091 -1.1692207 -1.1314123 -1.6871109 -2.4443884 -2.9141045][-2.7589364 -2.7233386 -2.3218675 -1.7186508 -1.0631902 -0.851928 -1.1732268 -1.4277835 -1.3180125 -0.94895148 -0.53370953 -0.55087972 -1.1448507 -1.8880193 -2.3573596][-2.6326387 -2.4100406 -1.9405882 -1.3442078 -0.86793232 -0.95525527 -1.4742079 -1.7049634 -1.4675536 -0.96194649 -0.54437232 -0.6821444 -1.2238669 -1.7606583 -2.0237322][-2.8699617 -2.6330295 -2.2990348 -1.8690736 -1.6533535 -1.94097 -2.4757974 -2.618969 -2.2604094 -1.6555347 -1.2968469 -1.5932643 -2.1394126 -2.4672236 -2.4262011][-2.8849921 -2.6995938 -2.5770912 -2.3390551 -2.316679 -2.6512885 -3.0990105 -3.2404099 -2.9461024 -2.412962 -2.1357162 -2.432749 -2.9462023 -3.2186468 -3.091795][-3.0662725 -2.8945937 -2.87011 -2.6980791 -2.6691473 -2.8566303 -3.0710428 -3.1645741 -3.0008016 -2.7140667 -2.6120226 -2.8937745 -3.3107593 -3.527102 -3.4064879][-3.3023181 -3.2639985 -3.3288732 -3.1732666 -3.055831 -3.048069 -3.0486903 -3.0820115 -3.0327978 -2.9546933 -3.0079725 -3.295938 -3.6372602 -3.809607 -3.7288342][-3.4354866 -3.503567 -3.5999072 -3.4654026 -3.3401098 -3.2708449 -3.2130098 -3.2682164 -3.3458655 -3.3820975 -3.4769015 -3.6995933 -3.9424822 -4.08392 -4.0823188][-3.4469943 -3.5478921 -3.6574497 -3.6479421 -3.6742022 -3.6878121 -3.6660242 -3.7209041 -3.8322232 -3.9004316 -3.9773684 -4.1136475 -4.2099533 -4.2540674 -4.2554226][-3.4096594 -3.5520539 -3.7077961 -3.8470311 -4.0293427 -4.1342435 -4.1752992 -4.2161078 -4.2713146 -4.2962618 -4.3389187 -4.3991308 -4.4031773 -4.378962 -4.3468661]]...]
INFO - root - 2017-12-07 08:38:26.681068: step 21710, loss = 0.66, batch loss = 0.59 (10.7 examples/sec; 0.751 sec/batch; 64h:50m:26s remains)
INFO - root - 2017-12-07 08:38:34.409059: step 21720, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.766 sec/batch; 66h:08m:06s remains)
INFO - root - 2017-12-07 08:38:41.996811: step 21730, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.788 sec/batch; 68h:00m:06s remains)
INFO - root - 2017-12-07 08:38:49.568776: step 21740, loss = 1.02, batch loss = 0.95 (10.7 examples/sec; 0.746 sec/batch; 64h:23m:15s remains)
INFO - root - 2017-12-07 08:38:57.275920: step 21750, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.773 sec/batch; 66h:45m:35s remains)
INFO - root - 2017-12-07 08:39:05.048496: step 21760, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.766 sec/batch; 66h:08m:27s remains)
INFO - root - 2017-12-07 08:39:12.502638: step 21770, loss = 0.95, batch loss = 0.88 (10.2 examples/sec; 0.784 sec/batch; 67h:38m:35s remains)
INFO - root - 2017-12-07 08:39:20.149454: step 21780, loss = 0.74, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 66h:15m:31s remains)
INFO - root - 2017-12-07 08:39:27.851967: step 21790, loss = 1.08, batch loss = 1.01 (10.7 examples/sec; 0.750 sec/batch; 64h:45m:51s remains)
INFO - root - 2017-12-07 08:39:35.564463: step 21800, loss = 0.74, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 67h:13m:42s remains)
2017-12-07 08:39:36.202147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.767679 -1.1731994 -1.6702693 -1.97417 -1.6074109 -1.024595 -0.88656855 -1.0669303 -1.3301058 -1.6595094 -2.1033449 -2.3302224 -2.3795881 -2.4956591 -2.6075206][-0.75112224 -1.0604284 -1.4162116 -1.6095378 -1.2306917 -0.77170849 -0.78091574 -1.0856454 -1.4521365 -1.8109889 -2.2334032 -2.4767861 -2.5700607 -2.6834636 -2.7604413][-0.94792914 -1.1394436 -1.312017 -1.3648694 -1.0615759 -0.7421217 -0.77587175 -1.1400239 -1.6807649 -2.1455476 -2.5307567 -2.7520814 -2.8682904 -2.9540155 -2.9760962][-1.0919569 -1.1500301 -1.197026 -1.1920562 -1.055994 -0.88311458 -0.88203216 -1.2500358 -1.9166141 -2.5058198 -2.8911221 -3.0972571 -3.2086406 -3.204567 -3.1254807][-1.1877162 -1.2081306 -1.2218649 -1.174932 -1.0832458 -0.96633911 -0.990222 -1.4046333 -2.1540227 -2.8339362 -3.2360392 -3.4173641 -3.476434 -3.3392758 -3.1577358][-1.2597146 -1.3412473 -1.4206178 -1.3590477 -1.1817806 -0.99204755 -1.0297487 -1.4976575 -2.2937679 -3.0290933 -3.47292 -3.6601467 -3.6518531 -3.4064007 -3.1365228][-1.4078906 -1.5074875 -1.6489499 -1.5850461 -1.3048191 -1.0304053 -1.0621159 -1.532764 -2.297116 -2.9946175 -3.4543121 -3.6524053 -3.5914645 -3.3093245 -3.0347762][-1.643301 -1.6349037 -1.7267115 -1.6216938 -1.2855692 -1.0199656 -1.058285 -1.4807093 -2.1335406 -2.717031 -3.1554735 -3.3808703 -3.300971 -3.0559244 -2.8883319][-1.8240983 -1.5780129 -1.4833162 -1.3207364 -1.0377657 -0.9349165 -1.09974 -1.501982 -2.0419769 -2.5165339 -2.9038868 -3.1222262 -3.0296791 -2.8510661 -2.8002419][-1.8947244 -1.4604175 -1.1541238 -0.899735 -0.71813869 -0.840317 -1.1743834 -1.5823343 -2.0723426 -2.5094748 -2.8277993 -2.9918172 -2.8925364 -2.7638416 -2.7651236][-1.8615277 -1.4511545 -1.0845256 -0.76738834 -0.62539268 -0.86224794 -1.2852571 -1.6632352 -2.1230328 -2.5770359 -2.8590384 -2.9803209 -2.8884764 -2.7890067 -2.7978263][-1.6920242 -1.4979134 -1.2795911 -1.0244186 -0.90607214 -1.1404216 -1.5524578 -1.8570971 -2.2607617 -2.716315 -3.007072 -3.1280451 -3.0502777 -2.9553981 -2.9273989][-1.5396569 -1.5212483 -1.4963322 -1.3766088 -1.31741 -1.5490263 -1.9374962 -2.1980224 -2.5179377 -2.9016588 -3.1440845 -3.2366133 -3.1735663 -3.1000705 -3.0515437][-1.5167487 -1.5466487 -1.638633 -1.6545334 -1.7059038 -1.9776547 -2.355809 -2.5984435 -2.8258519 -3.0693436 -3.2061577 -3.2328954 -3.160995 -3.0980139 -3.0510468][-1.6493759 -1.6494997 -1.7839952 -1.9303684 -2.1082504 -2.3864634 -2.6828871 -2.8565552 -2.9607995 -3.0512357 -3.1068549 -3.0992055 -3.0371289 -2.9839721 -2.9441071]]...]
INFO - root - 2017-12-07 08:39:43.797427: step 21810, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.770 sec/batch; 66h:24m:37s remains)
INFO - root - 2017-12-07 08:39:51.505612: step 21820, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.760 sec/batch; 65h:37m:05s remains)
INFO - root - 2017-12-07 08:39:59.251266: step 21830, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.763 sec/batch; 65h:49m:41s remains)
INFO - root - 2017-12-07 08:40:06.969603: step 21840, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.764 sec/batch; 65h:58m:13s remains)
INFO - root - 2017-12-07 08:40:14.737572: step 21850, loss = 0.69, batch loss = 0.62 (9.9 examples/sec; 0.811 sec/batch; 69h:58m:51s remains)
INFO - root - 2017-12-07 08:40:22.278937: step 21860, loss = 0.97, batch loss = 0.89 (10.5 examples/sec; 0.764 sec/batch; 65h:56m:08s remains)
INFO - root - 2017-12-07 08:40:29.819733: step 21870, loss = 0.75, batch loss = 0.68 (9.6 examples/sec; 0.832 sec/batch; 71h:48m:11s remains)
INFO - root - 2017-12-07 08:40:37.587241: step 21880, loss = 1.10, batch loss = 1.03 (10.2 examples/sec; 0.786 sec/batch; 67h:48m:41s remains)
INFO - root - 2017-12-07 08:40:45.331740: step 21890, loss = 0.72, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 67h:56m:27s remains)
INFO - root - 2017-12-07 08:40:52.967227: step 21900, loss = 0.87, batch loss = 0.80 (10.1 examples/sec; 0.789 sec/batch; 68h:02m:17s remains)
2017-12-07 08:40:53.573968: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4982045 -3.6051085 -3.728457 -3.6223717 -3.4452758 -3.4181249 -3.4244218 -3.3440733 -3.1947811 -2.954232 -2.7504611 -2.8667982 -3.2545259 -3.6327772 -3.7651825][-2.8677924 -2.9279633 -3.0911927 -3.1521876 -3.2400513 -3.4847245 -3.656343 -3.5554371 -3.2216563 -2.7737174 -2.4734678 -2.6793079 -3.270153 -3.8103466 -3.9901934][-2.4420476 -2.4585166 -2.7031326 -3.0215507 -3.3560495 -3.7185454 -3.86671 -3.640615 -3.1510816 -2.6085198 -2.2941062 -2.5594893 -3.2035065 -3.7254369 -3.8703527][-2.5076437 -2.3825431 -2.6038437 -3.0172625 -3.3746624 -3.6581161 -3.7334976 -3.5054314 -3.0927372 -2.67649 -2.4577689 -2.7226725 -3.236156 -3.5060897 -3.4542592][-2.9118352 -2.5679784 -2.6319451 -2.9713373 -3.2206969 -3.3443298 -3.3079925 -3.0952835 -2.8240621 -2.5924506 -2.5222425 -2.8419676 -3.2788007 -3.3615904 -3.1380091][-3.330714 -2.921721 -2.8233767 -2.9340324 -2.9164526 -2.696739 -2.331033 -1.9129772 -1.7244909 -1.7396517 -1.9448867 -2.4875627 -3.0514836 -3.1867094 -2.9759023][-3.0245671 -2.8203502 -2.67383 -2.5175371 -2.1495409 -1.4906354 -0.69068789 -0.063243389 -0.11879826 -0.66146111 -1.3749728 -2.2722292 -3.0395508 -3.2982845 -3.1420794][-2.7290895 -2.8096 -2.6623454 -2.2716112 -1.626013 -0.64724779 0.45207882 1.1468573 0.78443146 -0.20401239 -1.2620122 -2.3705666 -3.2771859 -3.5845656 -3.3863666][-3.3051882 -3.5439343 -3.4196973 -3.0413022 -2.4972806 -1.7207234 -0.8674593 -0.35320759 -0.59147286 -1.2270894 -1.9506721 -2.8362663 -3.6510367 -3.8937664 -3.5933893][-3.7851267 -4.0284791 -3.9547009 -3.7265761 -3.405962 -3.0017838 -2.5859134 -2.3362663 -2.412811 -2.6119318 -2.923008 -3.5040894 -4.0970192 -4.1768408 -3.7606869][-3.3343444 -3.4609795 -3.4862676 -3.5166228 -3.4702139 -3.3822291 -3.2841048 -3.1968818 -3.1499162 -3.0888896 -3.19281 -3.6102266 -4.04601 -4.0511971 -3.6686974][-2.4163182 -2.3050554 -2.2808468 -2.4315839 -2.6039658 -2.7845855 -2.972368 -3.0500665 -2.9828405 -2.7944965 -2.7887478 -3.12409 -3.5101581 -3.5787585 -3.3572531][-2.1666288 -1.9056234 -1.7234085 -1.7399435 -1.8440788 -2.0521042 -2.3684578 -2.6143382 -2.6666822 -2.547152 -2.541362 -2.7976522 -3.1065493 -3.2149825 -3.1512489][-2.8055923 -2.6123645 -2.4253614 -2.3592336 -2.3692884 -2.46526 -2.655211 -2.8230948 -2.8824234 -2.8426313 -2.85814 -3.0122776 -3.1847391 -3.2507815 -3.250201][-3.5155241 -3.4502051 -3.3564658 -3.2965813 -3.2748468 -3.28029 -3.3204021 -3.3484781 -3.3531342 -3.3266947 -3.3244207 -3.3893132 -3.452996 -3.4619737 -3.4555678]]...]
INFO - root - 2017-12-07 08:41:01.227109: step 21910, loss = 0.98, batch loss = 0.91 (10.6 examples/sec; 0.758 sec/batch; 65h:22m:16s remains)
INFO - root - 2017-12-07 08:41:08.972616: step 21920, loss = 0.67, batch loss = 0.60 (10.4 examples/sec; 0.766 sec/batch; 66h:05m:41s remains)
INFO - root - 2017-12-07 08:41:16.661454: step 21930, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.773 sec/batch; 66h:43m:43s remains)
INFO - root - 2017-12-07 08:41:24.320610: step 21940, loss = 0.68, batch loss = 0.60 (10.6 examples/sec; 0.756 sec/batch; 65h:12m:38s remains)
INFO - root - 2017-12-07 08:41:31.967862: step 21950, loss = 0.83, batch loss = 0.75 (10.4 examples/sec; 0.772 sec/batch; 66h:37m:44s remains)
INFO - root - 2017-12-07 08:41:39.636376: step 21960, loss = 0.85, batch loss = 0.78 (10.2 examples/sec; 0.782 sec/batch; 67h:28m:17s remains)
INFO - root - 2017-12-07 08:41:47.032242: step 21970, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.794 sec/batch; 68h:27m:58s remains)
INFO - root - 2017-12-07 08:41:54.630230: step 21980, loss = 1.10, batch loss = 1.02 (10.9 examples/sec; 0.733 sec/batch; 63h:12m:03s remains)
INFO - root - 2017-12-07 08:42:02.307763: step 21990, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.778 sec/batch; 67h:07m:58s remains)
INFO - root - 2017-12-07 08:42:09.947365: step 22000, loss = 0.67, batch loss = 0.60 (10.6 examples/sec; 0.755 sec/batch; 65h:08m:32s remains)
2017-12-07 08:42:10.523357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6499922 -2.405443 -2.2829678 -2.3092468 -2.2480071 -2.215728 -2.4090314 -2.5158551 -2.5158174 -2.5613656 -2.7901802 -3.0240755 -2.8532796 -2.3322222 -1.7863753][-2.663393 -2.2867639 -2.1593266 -2.2739918 -2.3061612 -2.2861683 -2.4725418 -2.5118217 -2.4635534 -2.5835075 -2.9834979 -3.405884 -3.2833517 -2.6363358 -1.9451985][-2.5593162 -2.0201941 -1.916863 -2.1571383 -2.2940094 -2.2444854 -2.2673211 -2.0666707 -1.7912488 -1.8410404 -2.3775268 -3.0943542 -3.2353785 -2.6660323 -1.9148939][-2.312495 -1.5954988 -1.5529821 -2.006278 -2.316843 -2.2666941 -2.090661 -1.6262252 -1.1208098 -1.0559196 -1.6964371 -2.6377316 -3.0258279 -2.6134853 -1.8824201][-2.0273416 -1.2239993 -1.339818 -2.0470448 -2.4836893 -2.3107531 -1.8385136 -1.1849103 -0.68915939 -0.72928429 -1.5803459 -2.723947 -3.2657685 -2.849102 -1.9896429][-1.8053 -0.98847914 -1.2507052 -2.0970798 -2.467802 -1.9130921 -0.86756349 0.054866791 0.34961414 -0.14126396 -1.3632669 -2.7158775 -3.3293562 -2.816916 -1.8295724][-1.6457527 -0.83776546 -1.1596501 -1.986799 -2.1626594 -1.1368539 0.50480413 1.6460094 1.6258435 0.60242605 -0.89859557 -2.2066975 -2.7628412 -2.2681849 -1.3883157][-1.5177453 -0.73769045 -1.1089516 -1.8877468 -1.8827136 -0.53286076 1.3911819 2.5040565 2.1281343 0.66121387 -0.85888839 -1.8268259 -2.1006296 -1.6940222 -1.1324747][-1.5076613 -0.80597377 -1.2159593 -1.9576097 -1.8678141 -0.49363589 1.2586713 2.0772128 1.4308901 -0.19663382 -1.5464513 -2.1016579 -2.0173652 -1.6235023 -1.3503394][-1.7166173 -1.1510272 -1.6196401 -2.3374894 -2.2048495 -0.89667583 0.53906107 0.93978977 0.11712313 -1.3517528 -2.3532767 -2.5931039 -2.3663347 -2.010891 -1.9090438][-2.0519516 -1.5940442 -2.0290232 -2.6690261 -2.54103 -1.4073422 -0.29328918 -0.16390991 -0.97711062 -2.088618 -2.6404614 -2.6011012 -2.3748007 -2.1839881 -2.2331829][-2.3858061 -1.9660773 -2.2645888 -2.7965937 -2.6981027 -1.8281956 -1.0035632 -0.94258785 -1.5468168 -2.2656829 -2.4824708 -2.3166041 -2.2079642 -2.2629702 -2.4271274][-2.6864288 -2.3344181 -2.5365293 -2.9977672 -2.9983673 -2.4673805 -1.8971879 -1.7722449 -2.0616105 -2.404449 -2.4535306 -2.3768208 -2.4906158 -2.7593458 -2.9859896][-2.8101673 -2.5795808 -2.7552691 -3.178014 -3.2670527 -2.9901438 -2.6392045 -2.5252166 -2.6478782 -2.7945504 -2.825305 -2.8937714 -3.1665125 -3.5169442 -3.7655027][-2.7137229 -2.5919368 -2.73951 -3.0668306 -3.1746795 -3.070601 -2.903801 -2.8713892 -2.9807868 -3.0773926 -3.0983949 -3.1381297 -3.3157997 -3.4930813 -3.5654917]]...]
INFO - root - 2017-12-07 08:42:18.135922: step 22010, loss = 0.70, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 66h:43m:17s remains)
INFO - root - 2017-12-07 08:42:25.810781: step 22020, loss = 0.82, batch loss = 0.74 (10.4 examples/sec; 0.772 sec/batch; 66h:35m:20s remains)
INFO - root - 2017-12-07 08:42:33.402276: step 22030, loss = 0.63, batch loss = 0.56 (10.3 examples/sec; 0.773 sec/batch; 66h:40m:07s remains)
INFO - root - 2017-12-07 08:42:41.081991: step 22040, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.778 sec/batch; 67h:07m:17s remains)
INFO - root - 2017-12-07 08:42:48.895031: step 22050, loss = 0.67, batch loss = 0.60 (10.1 examples/sec; 0.791 sec/batch; 68h:11m:19s remains)
INFO - root - 2017-12-07 08:42:56.601637: step 22060, loss = 0.78, batch loss = 0.71 (10.7 examples/sec; 0.748 sec/batch; 64h:31m:49s remains)
INFO - root - 2017-12-07 08:43:04.276537: step 22070, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.757 sec/batch; 65h:16m:37s remains)
INFO - root - 2017-12-07 08:43:11.990545: step 22080, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.762 sec/batch; 65h:42m:11s remains)
INFO - root - 2017-12-07 08:43:19.625775: step 22090, loss = 0.67, batch loss = 0.59 (10.4 examples/sec; 0.769 sec/batch; 66h:16m:42s remains)
INFO - root - 2017-12-07 08:43:27.361175: step 22100, loss = 1.06, batch loss = 0.99 (10.2 examples/sec; 0.788 sec/batch; 67h:55m:43s remains)
2017-12-07 08:43:27.958223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6647172 -3.1455183 -2.3109996 -1.7122936 -1.7782238 -2.342608 -2.8142133 -2.9149439 -2.6078901 -1.8870447 -1.1956897 -0.89236689 -1.0154698 -1.5485969 -2.4243882][-3.7403169 -3.1590157 -2.3456161 -1.8629022 -2.0881217 -2.686924 -3.070595 -3.0494418 -2.6724491 -1.8711517 -0.98111653 -0.44529295 -0.42656183 -1.0619864 -2.2791507][-3.2970672 -2.8454108 -2.237056 -1.9249663 -2.2061751 -2.717236 -3.0192947 -3.0081964 -2.7224073 -2.0227342 -1.1234856 -0.43942571 -0.23675346 -0.849478 -2.2700982][-2.3764131 -2.214592 -1.8813179 -1.6770799 -1.9072869 -2.3136463 -2.6016574 -2.6444261 -2.4460654 -1.8627093 -1.0799866 -0.42003393 -0.15546608 -0.755368 -2.2647433][-1.7243869 -1.7801175 -1.5530248 -1.268697 -1.3661869 -1.7013254 -1.9893093 -2.0201418 -1.7968347 -1.2408519 -0.60141373 -0.13174486 -0.044068336 -0.77509356 -2.3273249][-1.5197937 -1.5002975 -1.1414421 -0.70646405 -0.67832041 -0.91704273 -1.0873556 -1.0385871 -0.78536987 -0.28156996 0.18260336 0.35991144 0.11914349 -0.82830858 -2.3714116][-1.5523744 -1.3098743 -0.91376638 -0.5079298 -0.33936787 -0.31356859 -0.23133612 -0.16348076 -0.031489372 0.29522657 0.55577707 0.51662779 0.026144505 -1.013932 -2.4394383][-1.5412591 -1.1274157 -0.90332508 -0.75560951 -0.53167295 -0.23784399 0.065921783 0.17022705 0.21693468 0.39294672 0.48024416 0.28897524 -0.34537315 -1.363261 -2.5675893][-1.1607854 -0.79562664 -0.86041 -1.0488629 -0.94759607 -0.6362462 -0.38140678 -0.33662796 -0.34719706 -0.21669245 -0.071993351 -0.20015144 -0.78772807 -1.6885386 -2.6739807][-0.64647245 -0.37719107 -0.626678 -1.0340261 -1.1644454 -1.0952971 -1.1167035 -1.2501917 -1.3445296 -1.2589927 -1.0494657 -1.0625288 -1.493104 -2.1791654 -2.9052184][-0.7522614 -0.51718378 -0.65743685 -0.99275827 -1.2492781 -1.4414289 -1.6891189 -1.9152048 -2.0080578 -1.9496257 -1.7922068 -1.8045647 -2.110285 -2.6002972 -3.10806][-1.1607213 -1.1049364 -1.1909046 -1.387008 -1.6315963 -1.8807099 -2.1087816 -2.2327471 -2.2604713 -2.2761233 -2.296134 -2.3949656 -2.5931215 -2.8858705 -3.1802773][-1.5732429 -1.6685369 -1.8075736 -2.0139139 -2.2893329 -2.5184369 -2.6302562 -2.6099436 -2.4976056 -2.4431324 -2.4872978 -2.5806017 -2.706192 -2.8920689 -3.08453][-2.3247626 -2.4163108 -2.5049963 -2.6498365 -2.8864694 -3.0721915 -3.1702085 -3.1445138 -2.9779353 -2.8085237 -2.7296615 -2.7212396 -2.7608485 -2.8550925 -2.9700589][-2.9461102 -3.035192 -3.1090364 -3.1901193 -3.3244119 -3.4124832 -3.4693944 -3.442703 -3.2831721 -3.128516 -3.0387523 -2.9942334 -2.9626675 -2.9438512 -2.9503539]]...]
INFO - root - 2017-12-07 08:43:35.597155: step 22110, loss = 0.80, batch loss = 0.72 (10.3 examples/sec; 0.776 sec/batch; 66h:53m:43s remains)
INFO - root - 2017-12-07 08:43:43.252921: step 22120, loss = 0.72, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 67h:35m:53s remains)
INFO - root - 2017-12-07 08:43:51.075406: step 22130, loss = 0.80, batch loss = 0.72 (10.2 examples/sec; 0.787 sec/batch; 67h:53m:35s remains)
INFO - root - 2017-12-07 08:43:58.819462: step 22140, loss = 0.81, batch loss = 0.73 (10.3 examples/sec; 0.779 sec/batch; 67h:11m:15s remains)
INFO - root - 2017-12-07 08:44:06.376742: step 22150, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.753 sec/batch; 64h:53m:36s remains)
INFO - root - 2017-12-07 08:44:13.992067: step 22160, loss = 0.73, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 65h:50m:31s remains)
INFO - root - 2017-12-07 08:44:21.400068: step 22170, loss = 0.69, batch loss = 0.62 (10.6 examples/sec; 0.754 sec/batch; 65h:02m:07s remains)
INFO - root - 2017-12-07 08:44:29.054062: step 22180, loss = 0.92, batch loss = 0.85 (10.7 examples/sec; 0.751 sec/batch; 64h:41m:42s remains)
INFO - root - 2017-12-07 08:44:36.667839: step 22190, loss = 0.89, batch loss = 0.82 (10.4 examples/sec; 0.767 sec/batch; 66h:08m:26s remains)
INFO - root - 2017-12-07 08:44:44.404021: step 22200, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.775 sec/batch; 66h:48m:03s remains)
2017-12-07 08:44:45.020718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3747349 -3.2324448 -3.1337755 -3.0471587 -3.0117288 -3.0226593 -2.9047852 -2.7641926 -2.783628 -2.8837614 -3.0209336 -3.2499948 -3.4538641 -3.4806907 -3.4423511][-3.579051 -3.3958764 -3.2684712 -3.179214 -3.1154647 -3.0919888 -2.9180822 -2.7273564 -2.7738812 -2.8929358 -3.0112238 -3.2049494 -3.3468723 -3.3347301 -3.2961206][-3.7740378 -3.6111157 -3.4661882 -3.3646257 -3.2682161 -3.183476 -2.9131682 -2.6356621 -2.7235556 -2.9233346 -3.0811148 -3.2567992 -3.3261604 -3.2490706 -3.1861973][-3.8422465 -3.7363794 -3.5966239 -3.5003216 -3.3861504 -3.2526522 -2.8885231 -2.5147533 -2.6374893 -2.9446392 -3.1811428 -3.3749673 -3.4087586 -3.2964411 -3.2290373][-3.6748023 -3.6104064 -3.4895151 -3.42485 -3.3214664 -3.190429 -2.7925873 -2.3612804 -2.4985144 -2.8817832 -3.1634092 -3.3583128 -3.3792057 -3.2543249 -3.1820445][-3.3726726 -3.3391457 -3.2621315 -3.2275324 -3.1132684 -3.0016663 -2.6259789 -2.2051837 -2.3625479 -2.7737081 -3.04247 -3.189343 -3.1731226 -3.0265369 -2.9240334][-3.0835626 -3.0750475 -3.0488997 -3.0023236 -2.832763 -2.7043686 -2.3681428 -2.0336411 -2.2648013 -2.7071071 -2.9570556 -3.0326378 -2.9476724 -2.7562537 -2.6167176][-2.8157368 -2.8196378 -2.8322313 -2.7680357 -2.5533957 -2.3836617 -2.0559225 -1.8052177 -2.1154263 -2.5787578 -2.8116932 -2.8349652 -2.6932476 -2.4737725 -2.3235185][-2.5204 -2.5160768 -2.5726571 -2.5243616 -2.3130941 -2.1208246 -1.7724562 -1.5485315 -1.8795607 -2.3322656 -2.5644341 -2.5841537 -2.4306116 -2.2253089 -2.0899444][-2.2357392 -2.2247393 -2.3059285 -2.29338 -2.1547651 -2.0126088 -1.6926749 -1.4900882 -1.8081174 -2.2371333 -2.4822907 -2.5064075 -2.3347547 -2.1247423 -1.9689593][-1.9681675 -1.974149 -2.0545936 -2.0721605 -2.0653362 -2.0508325 -1.8507733 -1.7147913 -1.997611 -2.3553002 -2.5719185 -2.5836511 -2.3985753 -2.1880186 -2.0090032][-1.8070767 -1.820924 -1.8721945 -1.9107003 -2.0416415 -2.161201 -2.0966077 -2.0422895 -2.2492859 -2.4807448 -2.637028 -2.6532862 -2.5074716 -2.3444443 -2.1818342][-1.8491018 -1.8391588 -1.840677 -1.900435 -2.1380091 -2.3480809 -2.3864048 -2.4000151 -2.5031977 -2.57705 -2.6505609 -2.667284 -2.5631838 -2.4457266 -2.3031027][-2.0800905 -2.0216901 -1.9578128 -2.0074422 -2.2814178 -2.5221481 -2.6286736 -2.6978555 -2.69776 -2.6159382 -2.6049795 -2.5945187 -2.5110154 -2.4275742 -2.2958429][-2.3932965 -2.2855461 -2.1454213 -2.1274908 -2.3566723 -2.5807853 -2.7233386 -2.8305314 -2.7635307 -2.5828176 -2.5213633 -2.4784131 -2.4175806 -2.3796029 -2.2770109]]...]
INFO - root - 2017-12-07 08:44:52.627045: step 22210, loss = 0.88, batch loss = 0.80 (10.5 examples/sec; 0.760 sec/batch; 65h:29m:58s remains)
INFO - root - 2017-12-07 08:45:00.230255: step 22220, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.766 sec/batch; 66h:01m:54s remains)
INFO - root - 2017-12-07 08:45:07.841895: step 22230, loss = 0.96, batch loss = 0.89 (10.5 examples/sec; 0.762 sec/batch; 65h:41m:17s remains)
INFO - root - 2017-12-07 08:45:15.389068: step 22240, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.771 sec/batch; 66h:26m:12s remains)
INFO - root - 2017-12-07 08:45:23.181440: step 22250, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.765 sec/batch; 65h:55m:04s remains)
INFO - root - 2017-12-07 08:45:30.857612: step 22260, loss = 0.66, batch loss = 0.59 (10.8 examples/sec; 0.743 sec/batch; 64h:01m:39s remains)
INFO - root - 2017-12-07 08:45:40.964722: step 22270, loss = 0.64, batch loss = 0.56 (7.5 examples/sec; 1.064 sec/batch; 91h:40m:13s remains)
INFO - root - 2017-12-07 08:45:51.640879: step 22280, loss = 0.74, batch loss = 0.66 (7.8 examples/sec; 1.032 sec/batch; 88h:56m:48s remains)
INFO - root - 2017-12-07 08:46:02.316793: step 22290, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.065 sec/batch; 91h:45m:00s remains)
INFO - root - 2017-12-07 08:46:12.995277: step 22300, loss = 0.83, batch loss = 0.76 (7.1 examples/sec; 1.128 sec/batch; 97h:12m:19s remains)
2017-12-07 08:46:13.777169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5001278 -4.5454922 -4.585731 -4.5346537 -4.2836895 -3.7304316 -3.1137357 -2.6387746 -2.4686134 -2.8311665 -3.5705793 -4.11051 -4.2216353 -4.0441389 -3.9458573][-4.9832058 -5.1068063 -5.1806989 -5.0899239 -4.7101769 -3.9171944 -2.9633327 -2.1995127 -1.898212 -2.4150434 -3.3713369 -4.0259957 -4.1126819 -3.8279309 -3.708566][-5.1244855 -5.2400064 -5.30837 -5.1906543 -4.756659 -3.8110325 -2.5034032 -1.4070489 -1.040118 -1.7851684 -2.9474897 -3.7208498 -3.8373702 -3.5427353 -3.4709783][-4.825727 -4.8525114 -4.8877978 -4.7314181 -4.2233605 -3.1187122 -1.5106976 -0.24438953 -0.10027361 -1.2627058 -2.6376839 -3.478828 -3.5353987 -3.1986728 -3.1970043][-4.3210506 -4.2389073 -4.2369308 -3.9844685 -3.2324142 -1.7698808 0.16733503 1.3546257 0.85803795 -0.89284945 -2.4621119 -3.2877951 -3.2017634 -2.77492 -2.8351235][-3.9879651 -3.8943319 -3.9793184 -3.6863322 -2.5679226 -0.54511166 1.7966537 2.8122354 1.5013857 -0.75931668 -2.4120057 -3.192821 -3.0648417 -2.6114361 -2.6911278][-3.9278028 -3.9034581 -4.1205382 -3.8265274 -2.3970165 0.032566071 2.6074023 3.3716679 1.4773765 -0.95428872 -2.5160789 -3.1900969 -3.0238142 -2.5647335 -2.6114116][-3.9742117 -4.0331197 -4.3361716 -4.0830417 -2.5654626 -0.10471106 2.2932072 2.739572 0.94153643 -1.2261593 -2.5956063 -3.1773906 -2.9806252 -2.5303512 -2.5507631][-3.9467192 -4.0825324 -4.3985739 -4.201725 -2.7647295 -0.57745576 1.3022571 1.5100608 0.17671776 -1.4935422 -2.6236067 -3.1470127 -2.9659204 -2.542944 -2.5385795][-3.6855226 -3.8099542 -4.0526147 -3.9312286 -2.7349086 -0.9515965 0.3987422 0.42664957 -0.54186773 -1.7897713 -2.6717608 -3.0631621 -2.8568773 -2.5002508 -2.5265613][-3.2564316 -3.3003855 -3.4519145 -3.4113228 -2.5363197 -1.1397164 -0.1422081 -0.20812416 -1.0857339 -2.1759446 -2.8543024 -3.0117583 -2.7392888 -2.4932859 -2.6493716][-2.9633994 -2.8895125 -2.9332192 -2.901444 -2.3098834 -1.2379315 -0.44069767 -0.511641 -1.3908429 -2.5165019 -3.1718369 -3.2340045 -2.9248967 -2.757108 -2.9625509][-2.6728148 -2.4865105 -2.4398222 -2.3995879 -2.0650249 -1.3481274 -0.75225258 -0.85550666 -1.6692576 -2.7647321 -3.4347856 -3.5171502 -3.2527728 -3.1018844 -3.2314205][-2.4099891 -2.1905506 -2.1605432 -2.1282871 -1.9161043 -1.4556615 -1.0899642 -1.2580817 -1.9318039 -2.8217607 -3.4400053 -3.5740595 -3.4194412 -3.2946248 -3.3342068][-2.3630159 -2.1520431 -2.168447 -2.1535583 -1.9824777 -1.6904321 -1.5043612 -1.6978297 -2.1831329 -2.7842076 -3.2767816 -3.4683521 -3.4564753 -3.4114571 -3.4221563]]...]
INFO - root - 2017-12-07 08:46:24.483472: step 22310, loss = 0.86, batch loss = 0.79 (7.4 examples/sec; 1.075 sec/batch; 92h:35m:32s remains)
INFO - root - 2017-12-07 08:46:35.231931: step 22320, loss = 0.71, batch loss = 0.64 (7.4 examples/sec; 1.078 sec/batch; 92h:52m:48s remains)
INFO - root - 2017-12-07 08:46:45.815960: step 22330, loss = 1.07, batch loss = 0.99 (7.5 examples/sec; 1.068 sec/batch; 91h:58m:29s remains)
INFO - root - 2017-12-07 08:46:56.636729: step 22340, loss = 0.86, batch loss = 0.79 (7.2 examples/sec; 1.116 sec/batch; 96h:08m:43s remains)
INFO - root - 2017-12-07 08:47:07.150926: step 22350, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.048 sec/batch; 90h:15m:30s remains)
INFO - root - 2017-12-07 08:47:17.664030: step 22360, loss = 0.80, batch loss = 0.73 (7.6 examples/sec; 1.059 sec/batch; 91h:16m:14s remains)
INFO - root - 2017-12-07 08:47:28.080634: step 22370, loss = 0.86, batch loss = 0.79 (7.4 examples/sec; 1.085 sec/batch; 93h:27m:16s remains)
INFO - root - 2017-12-07 08:47:38.620273: step 22380, loss = 0.76, batch loss = 0.68 (8.1 examples/sec; 0.992 sec/batch; 85h:29m:30s remains)
INFO - root - 2017-12-07 08:47:49.277269: step 22390, loss = 0.72, batch loss = 0.65 (7.3 examples/sec; 1.092 sec/batch; 94h:01m:43s remains)
INFO - root - 2017-12-07 08:47:59.921968: step 22400, loss = 0.71, batch loss = 0.64 (7.4 examples/sec; 1.087 sec/batch; 93h:39m:13s remains)
2017-12-07 08:48:00.729161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1692181 -3.395669 -3.6675296 -3.8602624 -3.9129329 -3.8585956 -3.7726061 -3.7168045 -3.6804869 -3.651283 -3.6125529 -3.5555234 -3.5176005 -3.4961076 -3.4849529][-3.1816084 -3.4329972 -3.7370865 -3.9363389 -3.9730191 -3.8883095 -3.7777567 -3.7311049 -3.7262836 -3.7092826 -3.6587808 -3.5769229 -3.5111856 -3.457619 -3.4306893][-3.1608033 -3.4058969 -3.6896586 -3.8448904 -3.8311884 -3.7036552 -3.5623674 -3.5289979 -3.5759931 -3.5771849 -3.5078149 -3.3990674 -3.3088179 -3.2423444 -3.235157][-3.0658786 -3.2660241 -3.4825544 -3.5618966 -3.5001597 -3.3664556 -3.2286365 -3.2311497 -3.3322911 -3.3581023 -3.2723985 -3.1409786 -3.0266221 -2.9491892 -2.9741211][-2.9567964 -3.119128 -3.2851367 -3.3367734 -3.2956748 -3.2337253 -3.1629443 -3.2181048 -3.3412333 -3.3637033 -3.2447929 -3.062284 -2.8757625 -2.7204356 -2.7090149][-2.8556447 -3.0001695 -3.14485 -3.1862967 -3.1712341 -3.16743 -3.1535378 -3.2524548 -3.3923783 -3.4011755 -3.2342672 -2.9802279 -2.7177906 -2.4952211 -2.4468484][-2.7708316 -2.933913 -3.1036191 -3.1566186 -3.1461678 -3.1340117 -3.0952783 -3.1849813 -3.3380752 -3.3612289 -3.1854773 -2.9099674 -2.6297367 -2.3809576 -2.3084278][-2.7432323 -2.9315844 -3.113184 -3.1644969 -3.1133847 -3.0200715 -2.9095578 -2.9848032 -3.203927 -3.3689058 -3.3378785 -3.1505508 -2.897094 -2.5999744 -2.453558][-2.7065578 -2.8695817 -3.0246272 -3.0762763 -3.0013566 -2.8338227 -2.6473079 -2.6800508 -2.9435081 -3.263449 -3.4315395 -3.3939388 -3.1923265 -2.8604293 -2.6531615][-2.6673861 -2.7818217 -2.9182208 -3.0218909 -2.9929752 -2.8309228 -2.6336484 -2.6071844 -2.8081899 -3.1288877 -3.3576274 -3.4050407 -3.2523775 -2.9390268 -2.7515106][-2.673749 -2.758235 -2.8873296 -3.0409906 -3.0729814 -2.9355233 -2.7231269 -2.5951037 -2.6467075 -2.8276348 -3.0023804 -3.1037717 -3.0434527 -2.8427866 -2.7539196][-2.677937 -2.7451379 -2.8729367 -3.0459704 -3.1110921 -3.0114398 -2.8284366 -2.655323 -2.5847054 -2.6015344 -2.6671207 -2.7743404 -2.8107772 -2.7732542 -2.8086607][-2.6862292 -2.7125669 -2.8125098 -2.9582982 -3.0197639 -2.9526067 -2.8438654 -2.7366037 -2.6962366 -2.7054787 -2.7439873 -2.8183331 -2.8656614 -2.8871689 -2.9454389][-2.7104204 -2.6879916 -2.7258265 -2.8123538 -2.8488185 -2.8083544 -2.7641361 -2.7370138 -2.7738514 -2.8503375 -2.9295988 -2.9767921 -2.9627452 -2.9103763 -2.8594189][-2.7592258 -2.7206335 -2.7163358 -2.7495499 -2.7679181 -2.7520728 -2.7588558 -2.7808406 -2.8405304 -2.9126749 -2.9581273 -2.9234171 -2.8027613 -2.6569848 -2.5266938]]...]
INFO - root - 2017-12-07 08:48:11.508095: step 22410, loss = 0.78, batch loss = 0.70 (7.5 examples/sec; 1.074 sec/batch; 92h:29m:01s remains)
INFO - root - 2017-12-07 08:48:22.291531: step 22420, loss = 0.96, batch loss = 0.89 (7.6 examples/sec; 1.057 sec/batch; 91h:01m:04s remains)
INFO - root - 2017-12-07 08:48:33.057314: step 22430, loss = 0.92, batch loss = 0.84 (7.4 examples/sec; 1.078 sec/batch; 92h:53m:24s remains)
INFO - root - 2017-12-07 08:48:43.714977: step 22440, loss = 0.86, batch loss = 0.78 (7.5 examples/sec; 1.070 sec/batch; 92h:08m:18s remains)
INFO - root - 2017-12-07 08:48:54.397445: step 22450, loss = 0.68, batch loss = 0.61 (7.4 examples/sec; 1.087 sec/batch; 93h:36m:29s remains)
INFO - root - 2017-12-07 08:49:04.930159: step 22460, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.061 sec/batch; 91h:21m:47s remains)
INFO - root - 2017-12-07 08:49:15.468902: step 22470, loss = 0.80, batch loss = 0.73 (7.5 examples/sec; 1.063 sec/batch; 91h:33m:08s remains)
INFO - root - 2017-12-07 08:49:26.148905: step 22480, loss = 1.32, batch loss = 1.25 (7.6 examples/sec; 1.051 sec/batch; 90h:28m:46s remains)
INFO - root - 2017-12-07 08:49:36.808115: step 22490, loss = 0.79, batch loss = 0.72 (7.5 examples/sec; 1.070 sec/batch; 92h:09m:56s remains)
INFO - root - 2017-12-07 08:49:47.637874: step 22500, loss = 0.67, batch loss = 0.59 (7.3 examples/sec; 1.099 sec/batch; 94h:40m:10s remains)
2017-12-07 08:49:48.458216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.95347214 -0.91091323 -0.87059093 -0.82012773 -0.76925159 -0.78737235 -0.80318046 -0.807405 -0.87004805 -1.1134858 -1.5384715 -1.8273346 -1.9476283 -2.0369232 -2.04989][-0.62885737 -0.62044239 -0.61365294 -0.57479429 -0.53585505 -0.56281877 -0.59221268 -0.652622 -0.74572086 -0.91646743 -1.2019649 -1.3755612 -1.5068409 -1.75981 -2.0270996][-0.51675224 -0.5084765 -0.49986386 -0.43870831 -0.37536764 -0.37348318 -0.3954134 -0.51721168 -0.68488431 -0.87302208 -1.0900059 -1.1473937 -1.1868014 -1.4105222 -1.7592247][-0.12917614 -0.12388992 -0.097567558 0.0073795319 0.10457039 0.12270403 0.072066784 -0.15612984 -0.47447014 -0.81726575 -1.1093142 -1.1403739 -1.0940642 -1.1859059 -1.4474862][0.23942184 0.22920752 0.27108049 0.42379951 0.54880667 0.56650019 0.46744442 0.14442253 -0.30707359 -0.81216192 -1.204159 -1.2476585 -1.139318 -1.1002114 -1.2504933][0.3353653 0.30116558 0.35415506 0.53316116 0.64560461 0.63226509 0.50072956 0.17414236 -0.31079435 -0.910486 -1.3513064 -1.4030006 -1.2787356 -1.1772141 -1.2658694][0.30808496 0.25476694 0.2965107 0.41553879 0.40330553 0.28979874 0.13716984 -0.077770233 -0.46051931 -1.0314288 -1.4264393 -1.48755 -1.4234235 -1.3502162 -1.4276755][0.27423239 0.19281721 0.1990757 0.19494581 -0.00081777573 -0.23133993 -0.38181639 -0.43260169 -0.65078759 -1.0910819 -1.3525355 -1.402343 -1.4414532 -1.4305859 -1.4915361][0.23132706 0.11828852 0.0832057 -0.035150051 -0.35529423 -0.63072062 -0.74056363 -0.65221071 -0.73124766 -1.0597339 -1.2559216 -1.3906803 -1.5965958 -1.6492722 -1.6189687][0.23788357 0.095680714 0.022136688 -0.16018677 -0.55554342 -0.90118861 -1.1050069 -1.037303 -1.0561965 -1.3010237 -1.4532542 -1.6406507 -1.9032204 -1.9421029 -1.7801065][0.26807261 0.11339712 0.038610458 -0.13925838 -0.58326578 -1.0755963 -1.5494742 -1.7326512 -1.8585045 -2.1015198 -2.1953146 -2.2996657 -2.4275656 -2.3268809 -1.9927001][0.22668934 0.079323292 0.027364254 -0.11704779 -0.55244637 -1.1168103 -1.7820573 -2.2044156 -2.4677536 -2.7348332 -2.8133831 -2.873445 -2.8914049 -2.7136974 -2.3047709][-0.036131382 -0.16101122 -0.17946482 -0.28918552 -0.68904161 -1.2750549 -2.062273 -2.6529236 -2.9638577 -3.1461754 -3.1512113 -3.111834 -2.9637671 -2.7291908 -2.3791547][-0.53109384 -0.66300941 -0.64201736 -0.65604997 -0.90952611 -1.4275548 -2.257946 -2.9650021 -3.3188343 -3.4708529 -3.4390814 -3.2071471 -2.7825336 -2.4485941 -2.2348366][-0.72422934 -0.89526939 -0.87691665 -0.81127906 -0.91143346 -1.2882395 -2.0610464 -2.8446493 -3.3324177 -3.5825508 -3.5290647 -3.0703223 -2.4238584 -2.0735345 -2.0471079]]...]
INFO - root - 2017-12-07 08:49:59.271673: step 22510, loss = 0.86, batch loss = 0.79 (7.5 examples/sec; 1.066 sec/batch; 91h:49m:21s remains)
INFO - root - 2017-12-07 08:50:09.950322: step 22520, loss = 0.90, batch loss = 0.83 (7.6 examples/sec; 1.051 sec/batch; 90h:32m:17s remains)
INFO - root - 2017-12-07 08:50:20.612918: step 22530, loss = 0.79, batch loss = 0.72 (7.5 examples/sec; 1.064 sec/batch; 91h:35m:49s remains)
INFO - root - 2017-12-07 08:50:31.323427: step 22540, loss = 0.90, batch loss = 0.82 (7.6 examples/sec; 1.051 sec/batch; 90h:30m:41s remains)
INFO - root - 2017-12-07 08:50:42.029204: step 22550, loss = 0.74, batch loss = 0.67 (7.4 examples/sec; 1.081 sec/batch; 93h:05m:30s remains)
INFO - root - 2017-12-07 08:50:52.879138: step 22560, loss = 1.05, batch loss = 0.97 (7.1 examples/sec; 1.131 sec/batch; 97h:21m:15s remains)
INFO - root - 2017-12-07 08:51:03.398546: step 22570, loss = 0.65, batch loss = 0.57 (7.6 examples/sec; 1.056 sec/batch; 90h:56m:39s remains)
INFO - root - 2017-12-07 08:51:14.008391: step 22580, loss = 0.94, batch loss = 0.87 (7.5 examples/sec; 1.070 sec/batch; 92h:05m:14s remains)
INFO - root - 2017-12-07 08:51:24.743921: step 22590, loss = 1.03, batch loss = 0.96 (7.4 examples/sec; 1.075 sec/batch; 92h:33m:19s remains)
INFO - root - 2017-12-07 08:51:35.514079: step 22600, loss = 1.08, batch loss = 1.01 (7.6 examples/sec; 1.059 sec/batch; 91h:08m:28s remains)
2017-12-07 08:51:36.364054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1865177 -4.2032943 -4.1591811 -4.1337371 -4.237711 -4.3874121 -4.4885058 -4.5356059 -4.5326214 -4.484015 -4.4369459 -4.4080381 -4.3530688 -4.2373624 -4.11793][-4.3671823 -4.3654141 -4.2428975 -4.2271748 -4.4702582 -4.7435288 -4.8738379 -4.9184761 -4.9312978 -4.9010887 -4.8558712 -4.8152018 -4.7183356 -4.5316734 -4.3369379][-4.7098832 -4.5262923 -4.1782951 -4.0871739 -4.415432 -4.7254086 -4.8710556 -4.9665561 -5.0445 -5.0424042 -5.0386834 -5.0748539 -5.0168309 -4.7890091 -4.5170808][-4.9264879 -4.3676143 -3.6358624 -3.3409073 -3.6085434 -3.8627939 -4.0391159 -4.229228 -4.3963361 -4.3898611 -4.4379692 -4.6362028 -4.6994109 -4.4822931 -4.157074][-4.813477 -3.8719 -2.8417687 -2.3835073 -2.5812073 -2.7439075 -2.8541698 -2.955442 -3.0793016 -3.0513296 -3.0546153 -3.3150849 -3.4007401 -3.1711631 -2.947094][-4.3343043 -3.1952429 -2.0900345 -1.5987222 -1.8028424 -1.9632835 -1.9075053 -1.6428812 -1.6061087 -1.6184313 -1.5460384 -1.6807585 -1.6266935 -1.410367 -1.487968][-3.894917 -2.6964087 -1.4758778 -0.80022144 -0.89873171 -1.0442858 -0.68948817 0.040669918 0.051388741 -0.21065903 -0.025217533 0.22413254 0.49811411 0.4927249 -0.11971521][-3.6350124 -2.3829536 -0.92910218 0.091570377 0.33555222 0.61729956 1.6777511 2.9129772 2.4802904 1.4909258 1.4133058 1.823071 2.1144338 1.8782625 0.939878][-3.4348421 -2.2160146 -0.77469015 0.19541788 0.5600028 1.2916331 3.023447 4.577877 3.5874004 2.0192862 1.5897226 1.8242579 1.9226046 1.7618208 1.0862103][-3.3282328 -2.2997015 -1.2216985 -0.68763351 -0.5145371 0.11303902 1.5959353 2.6373067 1.7220216 0.54839849 0.28055191 0.32266665 0.29998446 0.36251736 0.13947725][-3.3662505 -2.5968456 -1.9264615 -1.6130531 -1.428638 -0.95075893 0.00069332123 0.524312 -0.15439892 -0.79470134 -0.86606956 -0.904197 -0.96025443 -0.83602071 -0.93039322][-3.5064936 -3.03092 -2.7012002 -2.418057 -2.0569875 -1.6281457 -1.0681629 -0.84644938 -1.3140714 -1.664592 -1.6697242 -1.6310546 -1.6173093 -1.5403869 -1.6955945][-3.7146935 -3.4855497 -3.3942256 -3.1674485 -2.7862287 -2.5091934 -2.2131813 -2.1033034 -2.3339393 -2.5075574 -2.4570699 -2.3084104 -2.2315295 -2.2316022 -2.4209476][-3.8653877 -3.790859 -3.8584161 -3.7676425 -3.4972305 -3.3352127 -3.1638322 -3.0990841 -3.2041216 -3.3309922 -3.3176653 -3.1883631 -3.1350453 -3.1866627 -3.3068776][-3.9284592 -3.9330428 -4.0796585 -4.1210022 -4.0120697 -3.9582636 -3.8895648 -3.8775928 -3.9509158 -4.0439072 -4.0450888 -3.9732063 -3.9677756 -4.0291409 -4.0620217]]...]
INFO - root - 2017-12-07 08:51:47.107765: step 22610, loss = 0.85, batch loss = 0.78 (7.4 examples/sec; 1.077 sec/batch; 92h:40m:15s remains)
INFO - root - 2017-12-07 08:51:57.805323: step 22620, loss = 0.93, batch loss = 0.85 (7.4 examples/sec; 1.081 sec/batch; 93h:04m:44s remains)
INFO - root - 2017-12-07 08:52:08.499344: step 22630, loss = 0.78, batch loss = 0.70 (7.6 examples/sec; 1.052 sec/batch; 90h:32m:21s remains)
INFO - root - 2017-12-07 08:52:19.191460: step 22640, loss = 0.98, batch loss = 0.90 (7.5 examples/sec; 1.072 sec/batch; 92h:17m:34s remains)
INFO - root - 2017-12-07 08:52:30.002369: step 22650, loss = 0.66, batch loss = 0.59 (7.4 examples/sec; 1.084 sec/batch; 93h:19m:59s remains)
INFO - root - 2017-12-07 08:52:40.668206: step 22660, loss = 0.87, batch loss = 0.79 (7.7 examples/sec; 1.041 sec/batch; 89h:33m:48s remains)
INFO - root - 2017-12-07 08:52:51.218268: step 22670, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.065 sec/batch; 91h:39m:53s remains)
INFO - root - 2017-12-07 08:53:01.924212: step 22680, loss = 1.01, batch loss = 0.93 (7.5 examples/sec; 1.074 sec/batch; 92h:24m:32s remains)
INFO - root - 2017-12-07 08:53:12.546264: step 22690, loss = 0.83, batch loss = 0.76 (7.8 examples/sec; 1.020 sec/batch; 87h:47m:59s remains)
INFO - root - 2017-12-07 08:53:23.316511: step 22700, loss = 0.82, batch loss = 0.75 (7.5 examples/sec; 1.061 sec/batch; 91h:17m:58s remains)
2017-12-07 08:53:24.119776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0202026 -3.9547727 -3.8578968 -3.6720223 -3.4439545 -3.2543669 -3.1114721 -3.0310817 -3.0471883 -2.9516087 -2.552783 -2.1866295 -2.1179621 -2.404031 -2.806097][-3.9906578 -3.819128 -3.6443584 -3.3433509 -3.0112014 -2.7876983 -2.5894165 -2.3755467 -2.2037868 -1.9873714 -1.6797693 -1.4409471 -1.496731 -1.9100542 -2.2492759][-3.9152777 -3.66922 -3.457515 -3.0273011 -2.515729 -2.2151573 -1.9719088 -1.6107817 -1.2737331 -1.1208858 -1.1331012 -1.1489661 -1.3762689 -1.8148835 -1.9068518][-3.8017986 -3.4873378 -3.2369137 -2.6459951 -1.8819592 -1.4326987 -1.1851828 -0.8415668 -0.59634447 -0.786144 -1.1317854 -1.3219061 -1.7172244 -2.0796971 -1.7983758][-3.7314427 -3.3497372 -3.0226674 -2.2511559 -1.2533481 -0.65376163 -0.37277555 -0.14865351 -0.23376989 -0.88336277 -1.3300862 -1.4273753 -1.8886862 -2.1367292 -1.5289373][-3.7874875 -3.3564451 -2.8969131 -1.9323189 -0.76211929 -0.023833752 0.47302341 0.75813484 0.29101849 -0.79972339 -1.2235518 -1.101506 -1.4509788 -1.5035384 -0.73506045][-3.9416287 -3.4911103 -2.8660297 -1.7369726 -0.5305686 0.25578785 0.98856878 1.4882421 0.76414251 -0.5859592 -0.97677326 -0.73697734 -0.87569737 -0.65834665 0.073541641][-4.0895567 -3.6679616 -2.9056177 -1.7038987 -0.6441834 0.0029792786 0.70494986 1.2422795 0.59381056 -0.55922055 -0.82545471 -0.61286807 -0.57671905 -0.14071321 0.40338278][-4.2203007 -3.8380313 -2.9936571 -1.7984672 -0.927969 -0.44821882 0.0016961098 0.34586096 0.0097446442 -0.64325547 -0.78801823 -0.72574377 -0.61122012 -0.042244434 0.34409046][-4.3290992 -4.0135064 -3.2007055 -2.1314113 -1.3602948 -0.88038254 -0.6192553 -0.47960925 -0.5523715 -0.79302359 -0.90777707 -0.94940639 -0.77791119 -0.18322134 0.1188035][-4.2981019 -4.058197 -3.428508 -2.65383 -2.0041857 -1.5066206 -1.3732884 -1.3631017 -1.3273177 -1.3855338 -1.4654391 -1.4593616 -1.2053099 -0.71914864 -0.55780339][-4.0685849 -3.857717 -3.4490187 -3.019722 -2.591563 -2.2744753 -2.3200104 -2.4086652 -2.3921268 -2.4437253 -2.4955907 -2.394598 -2.1111419 -1.8185055 -1.8158748][-3.7898595 -3.5887346 -3.3518336 -3.1695557 -2.9478183 -2.8501451 -3.0157444 -3.139823 -3.1745529 -3.2830174 -3.326813 -3.2206182 -3.01136 -2.8870649 -2.9685018][-3.6657362 -3.5273833 -3.4281473 -3.3936908 -3.3187242 -3.3324089 -3.451267 -3.4807069 -3.4691191 -3.5391223 -3.5591466 -3.5303481 -3.4618444 -3.4339426 -3.4820697][-3.7152998 -3.6708567 -3.6673324 -3.7006896 -3.7067466 -3.7509036 -3.7921236 -3.7398202 -3.6614819 -3.6241636 -3.581094 -3.5846262 -3.60184 -3.6019428 -3.5867431]]...]
INFO - root - 2017-12-07 08:53:35.024153: step 22710, loss = 0.88, batch loss = 0.81 (7.6 examples/sec; 1.059 sec/batch; 91h:06m:33s remains)
INFO - root - 2017-12-07 08:53:45.769986: step 22720, loss = 0.75, batch loss = 0.68 (7.3 examples/sec; 1.089 sec/batch; 93h:43m:23s remains)
INFO - root - 2017-12-07 08:53:56.514087: step 22730, loss = 0.82, batch loss = 0.74 (7.5 examples/sec; 1.070 sec/batch; 92h:02m:42s remains)
INFO - root - 2017-12-07 08:54:07.196695: step 22740, loss = 0.99, batch loss = 0.92 (7.4 examples/sec; 1.079 sec/batch; 92h:51m:10s remains)
INFO - root - 2017-12-07 08:54:17.862452: step 22750, loss = 0.88, batch loss = 0.81 (7.7 examples/sec; 1.045 sec/batch; 89h:54m:06s remains)
INFO - root - 2017-12-07 08:54:28.616450: step 22760, loss = 0.70, batch loss = 0.63 (7.7 examples/sec; 1.042 sec/batch; 89h:38m:57s remains)
INFO - root - 2017-12-07 08:54:39.134745: step 22770, loss = 0.99, batch loss = 0.91 (7.3 examples/sec; 1.089 sec/batch; 93h:41m:14s remains)
INFO - root - 2017-12-07 08:54:49.800838: step 22780, loss = 0.86, batch loss = 0.79 (7.5 examples/sec; 1.066 sec/batch; 91h:40m:07s remains)
INFO - root - 2017-12-07 08:55:00.553536: step 22790, loss = 0.76, batch loss = 0.69 (7.4 examples/sec; 1.078 sec/batch; 92h:46m:22s remains)
INFO - root - 2017-12-07 08:55:11.244913: step 22800, loss = 1.06, batch loss = 0.98 (7.6 examples/sec; 1.049 sec/batch; 90h:13m:12s remains)
2017-12-07 08:55:12.019857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4294636 -1.3745968 -1.3462062 -1.5722375 -1.4797034 -1.0526965 -0.9251523 -1.0571032 -1.2185125 -0.99326634 -0.87961006 -1.0915241 -1.2322559 -1.287148 -1.3939996][-1.4641113 -1.4852462 -1.4990604 -1.6803586 -1.5508318 -1.1777956 -1.1061172 -1.2519548 -1.3294015 -1.0622523 -0.88452125 -1.0212116 -1.1262951 -1.2193804 -1.3866115][-1.6309228 -1.681318 -1.661236 -1.777225 -1.6862464 -1.4330134 -1.3833628 -1.4580588 -1.536284 -1.3368263 -1.0568817 -1.0245769 -1.0282192 -1.1249435 -1.3243911][-1.763746 -1.7994339 -1.6859994 -1.7335284 -1.7518234 -1.569906 -1.4402971 -1.4590287 -1.6911581 -1.6473455 -1.240571 -1.0689485 -1.0531354 -1.129756 -1.3148656][-1.7634578 -1.7753191 -1.5604682 -1.5690105 -1.6833367 -1.4722362 -1.2209928 -1.2952027 -1.7739449 -1.8924115 -1.3646781 -1.1423266 -1.1478977 -1.1097617 -1.2219539][-1.7797287 -1.7411199 -1.4273894 -1.4273033 -1.563221 -1.1849513 -0.80564427 -1.0246758 -1.6801844 -1.8767142 -1.2940385 -1.1209564 -1.1482933 -0.92966127 -0.97140551][-1.7898176 -1.6654897 -1.299731 -1.3272445 -1.4074314 -0.81378007 -0.37291527 -0.76028895 -1.4543786 -1.6340725 -1.1366599 -1.0881886 -1.0820212 -0.68588161 -0.7090168][-1.6626291 -1.5048025 -1.1772602 -1.2581301 -1.2492225 -0.54880738 -0.20748043 -0.75261784 -1.3278737 -1.4384503 -1.1531394 -1.1956599 -1.051717 -0.50123262 -0.52858233][-1.4392335 -1.3850486 -1.1767018 -1.3002181 -1.226773 -0.57103896 -0.39588594 -0.91924095 -1.2177658 -1.3035114 -1.3147492 -1.4048793 -1.081758 -0.43484473 -0.41153002][-1.2164946 -1.3587077 -1.300972 -1.4762483 -1.4614563 -0.98077583 -0.89851046 -1.1541896 -1.1085985 -1.1934483 -1.4374943 -1.5080318 -1.0760784 -0.47173309 -0.39856958][-1.1245823 -1.4010994 -1.3893106 -1.5630653 -1.6765945 -1.4156549 -1.3650501 -1.343787 -1.0706193 -1.1796873 -1.4993975 -1.4626286 -1.0141165 -0.5605247 -0.50326967][-1.0470088 -1.3470309 -1.3288543 -1.4994035 -1.7162845 -1.6211448 -1.6040833 -1.5065591 -1.1948118 -1.293052 -1.517688 -1.3603945 -0.99536824 -0.71027112 -0.67005396][-0.95701814 -1.2125556 -1.20738 -1.3599875 -1.5881495 -1.5526876 -1.6041136 -1.6015031 -1.3620667 -1.3995235 -1.457073 -1.2653937 -1.041693 -0.82994413 -0.7405355][-0.92017007 -1.0719254 -1.0723133 -1.1261325 -1.268991 -1.2643807 -1.4106247 -1.5522439 -1.394897 -1.3754659 -1.3090627 -1.1508451 -1.0550106 -0.85071063 -0.77826953][-0.8452208 -0.8832922 -0.87722921 -0.82140851 -0.88933587 -0.95972848 -1.2294989 -1.4401281 -1.2852693 -1.2102957 -1.0938356 -1.0241938 -1.0268841 -0.85438848 -0.88717461]]...]
INFO - root - 2017-12-07 08:55:22.669358: step 22810, loss = 0.79, batch loss = 0.72 (7.3 examples/sec; 1.097 sec/batch; 94h:21m:34s remains)
INFO - root - 2017-12-07 08:55:33.347211: step 22820, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.076 sec/batch; 92h:33m:18s remains)
INFO - root - 2017-12-07 08:55:44.032628: step 22830, loss = 0.91, batch loss = 0.84 (7.5 examples/sec; 1.067 sec/batch; 91h:46m:53s remains)
INFO - root - 2017-12-07 08:55:54.686124: step 22840, loss = 0.63, batch loss = 0.56 (7.7 examples/sec; 1.042 sec/batch; 89h:36m:46s remains)
INFO - root - 2017-12-07 08:56:05.545820: step 22850, loss = 0.66, batch loss = 0.59 (7.4 examples/sec; 1.084 sec/batch; 93h:13m:08s remains)
INFO - root - 2017-12-07 08:56:16.264081: step 22860, loss = 1.00, batch loss = 0.93 (7.5 examples/sec; 1.063 sec/batch; 91h:27m:25s remains)
INFO - root - 2017-12-07 08:56:26.763511: step 22870, loss = 0.56, batch loss = 0.49 (7.6 examples/sec; 1.059 sec/batch; 91h:05m:15s remains)
INFO - root - 2017-12-07 08:56:37.571900: step 22880, loss = 1.04, batch loss = 0.97 (7.5 examples/sec; 1.073 sec/batch; 92h:15m:33s remains)
INFO - root - 2017-12-07 08:56:48.303977: step 22890, loss = 1.13, batch loss = 1.05 (7.4 examples/sec; 1.084 sec/batch; 93h:13m:09s remains)
INFO - root - 2017-12-07 08:56:58.924974: step 22900, loss = 0.55, batch loss = 0.48 (7.5 examples/sec; 1.066 sec/batch; 91h:41m:36s remains)
2017-12-07 08:56:59.734392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3871014 -1.4537678 -1.5026381 -1.5294468 -1.5054543 -1.4573598 -1.3958654 -1.3499851 -1.3551409 -1.3805311 -1.4098556 -1.4346335 -1.4348495 -1.4097302 -1.384923][-1.5537953 -1.6141186 -1.658561 -1.6841497 -1.643327 -1.5871611 -1.5161223 -1.4541409 -1.4541163 -1.4714231 -1.4815416 -1.4930584 -1.4682875 -1.3991427 -1.344274][-1.7529478 -1.840589 -1.9179578 -1.9726784 -1.9348388 -1.869962 -1.8044257 -1.7527888 -1.763494 -1.7536035 -1.7167828 -1.6999173 -1.6307569 -1.4968688 -1.4059105][-1.7074232 -1.8141243 -1.954613 -2.0809264 -2.0676684 -1.9471295 -1.7864549 -1.6813347 -1.7131593 -1.7478192 -1.7620168 -1.817697 -1.8000636 -1.6822569 -1.6052163][-1.3865104 -1.4978704 -1.7234325 -1.9845028 -2.0781443 -1.9346199 -1.6257994 -1.4141667 -1.471386 -1.5947378 -1.6838548 -1.7947214 -1.8281486 -1.7609117 -1.7362926][-1.1808836 -1.2559099 -1.5073738 -1.8306458 -1.9328358 -1.6257799 -1.0381486 -0.65668035 -0.78956127 -1.1144319 -1.3489835 -1.4884043 -1.5301034 -1.5090842 -1.6102617][-0.9377315 -0.890105 -1.0225875 -1.2917297 -1.4237771 -1.113512 -0.43512416 0.017912388 -0.23130512 -0.77549863 -1.1518576 -1.3170192 -1.3187137 -1.2686 -1.4410055][-1.081388 -0.89205527 -0.87205887 -1.1002452 -1.3912628 -1.3858416 -1.047694 -0.85022283 -1.1406462 -1.5108976 -1.6294703 -1.5845823 -1.41891 -1.259383 -1.4192851][-1.3125467 -1.1801372 -1.2305691 -1.5550749 -2.0358834 -2.3154647 -2.2690434 -2.2543006 -2.4975588 -2.6401715 -2.474216 -2.2124872 -1.8962455 -1.6912189 -1.8414469][-1.973135 -1.9318914 -2.0569017 -2.421659 -2.9282892 -3.2811615 -3.304292 -3.28439 -3.401403 -3.3868153 -3.12437 -2.8269711 -2.4400249 -2.1953025 -2.2612424][-3.1137676 -3.0632825 -3.1280608 -3.3086052 -3.5161376 -3.6280892 -3.5428085 -3.4846334 -3.5351815 -3.4403381 -3.2054191 -3.0012321 -2.7318475 -2.6414204 -2.8071909][-3.7932794 -3.7251353 -3.7468503 -3.7820673 -3.7499352 -3.6276398 -3.4022489 -3.3059039 -3.3792002 -3.3917198 -3.3524981 -3.3080454 -3.1579976 -3.1390891 -3.3271196][-3.3449392 -3.3768306 -3.4907787 -3.5579009 -3.57185 -3.5263364 -3.3935592 -3.3543391 -3.4094667 -3.437151 -3.447968 -3.4099653 -3.2342806 -3.1293173 -3.1843405][-2.3012383 -2.4211226 -2.6158261 -2.7366776 -2.8126283 -2.8697321 -2.8971617 -2.9950333 -3.0926261 -3.09633 -3.040719 -2.8919928 -2.6407099 -2.4646463 -2.4311113][-2.3138878 -2.4232528 -2.5849242 -2.6983023 -2.7869115 -2.8732262 -2.9528863 -3.0882223 -3.1989317 -3.2035432 -3.1196983 -2.9350305 -2.7050357 -2.5578415 -2.516788]]...]
INFO - root - 2017-12-07 08:57:10.762353: step 22910, loss = 0.92, batch loss = 0.85 (7.2 examples/sec; 1.106 sec/batch; 95h:05m:25s remains)
INFO - root - 2017-12-07 08:57:21.459479: step 22920, loss = 0.70, batch loss = 0.63 (7.5 examples/sec; 1.063 sec/batch; 91h:22m:43s remains)
INFO - root - 2017-12-07 08:57:32.106219: step 22930, loss = 0.73, batch loss = 0.66 (7.8 examples/sec; 1.028 sec/batch; 88h:26m:04s remains)
INFO - root - 2017-12-07 08:57:42.802428: step 22940, loss = 0.88, batch loss = 0.81 (7.7 examples/sec; 1.034 sec/batch; 88h:54m:22s remains)
INFO - root - 2017-12-07 08:57:53.580976: step 22950, loss = 0.71, batch loss = 0.64 (7.5 examples/sec; 1.067 sec/batch; 91h:42m:31s remains)
INFO - root - 2017-12-07 08:58:04.036678: step 22960, loss = 0.73, batch loss = 0.66 (7.5 examples/sec; 1.064 sec/batch; 91h:30m:51s remains)
INFO - root - 2017-12-07 08:58:14.691260: step 22970, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 1.122 sec/batch; 96h:29m:09s remains)
INFO - root - 2017-12-07 08:58:25.340388: step 22980, loss = 0.79, batch loss = 0.72 (7.6 examples/sec; 1.047 sec/batch; 90h:03m:28s remains)
INFO - root - 2017-12-07 08:58:36.025239: step 22990, loss = 1.14, batch loss = 1.07 (7.5 examples/sec; 1.072 sec/batch; 92h:08m:08s remains)
INFO - root - 2017-12-07 08:58:46.608662: step 23000, loss = 0.57, batch loss = 0.50 (7.7 examples/sec; 1.044 sec/batch; 89h:43m:35s remains)
2017-12-07 08:58:47.423919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1633708 -2.2964749 -1.9079792 -1.3032024 -1.0167027 -0.96949005 -0.9807477 -1.1628206 -1.3019593 -1.4388518 -1.5769489 -1.3790164 -1.060673 -0.94402552 -0.73193717][-2.1690588 -2.0891774 -1.5561175 -1.0434821 -0.82791185 -0.84064579 -0.96864533 -1.3690257 -1.8156154 -2.0801623 -2.1577113 -1.9773979 -1.7150822 -1.6718762 -1.5276875][-2.6069231 -2.5039229 -2.0395973 -1.6805613 -1.5029142 -1.5347397 -1.6965523 -2.084327 -2.5273788 -2.7189755 -2.6916592 -2.5334525 -2.2835996 -2.2640557 -2.3150909][-1.7980902 -1.8675432 -1.7496102 -1.6784394 -1.5791383 -1.5611374 -1.6030579 -1.8739877 -2.4050441 -2.8821197 -3.0837464 -3.0042586 -2.5289145 -2.2199085 -2.2390335][-0.59501624 -0.90538955 -1.1741815 -1.4348829 -1.518966 -1.4773917 -1.2944138 -1.2125823 -1.66832 -2.4977245 -3.2192664 -3.4250007 -2.8748031 -2.3291438 -2.1002152][-0.51923752 -0.99350238 -1.4502816 -1.8331418 -1.9912372 -1.7950697 -1.2622161 -0.69881868 -0.76233983 -1.4692974 -2.340394 -2.7375607 -2.4947937 -2.2418702 -2.0983317][-0.71066 -1.2560644 -1.7681365 -2.2108781 -2.4437819 -2.1790066 -1.4350955 -0.59694839 -0.32989216 -0.66061306 -1.2408762 -1.6303699 -1.817513 -2.1162829 -2.2955246][-1.1239157 -1.5639288 -1.950654 -2.323894 -2.6301246 -2.564774 -2.0663295 -1.38924 -1.0474389 -0.97994494 -1.0373735 -1.1535583 -1.5277009 -2.2068746 -2.7297][-1.6689363 -1.9369943 -2.0608053 -2.2026753 -2.41115 -2.4875212 -2.4003949 -2.1828325 -2.0995319 -2.0008631 -1.8063431 -1.7059875 -2.0295017 -2.6933722 -3.2821593][-1.7323294 -1.9149613 -1.8409557 -1.7979553 -1.8625627 -2.0199409 -2.2259071 -2.2965655 -2.4607735 -2.5957069 -2.6130912 -2.6938252 -2.98452 -3.373064 -3.6677437][-1.8056574 -1.9895587 -1.7941442 -1.6004689 -1.4703972 -1.4790885 -1.625093 -1.6812704 -1.9215555 -2.3025346 -2.6461205 -3.1027987 -3.5388441 -3.7919753 -3.8445435][-1.969939 -2.2127481 -2.0304344 -1.8654301 -1.643966 -1.427407 -1.2543898 -1.0583522 -1.1791048 -1.602587 -2.1207175 -2.7977037 -3.3628578 -3.6179438 -3.6454473][-1.6112077 -1.8993816 -1.8041997 -1.7835495 -1.6787045 -1.4434426 -1.1852477 -0.907882 -0.96816039 -1.3366094 -1.8569527 -2.4811273 -2.9059086 -3.0622382 -3.1278462][-1.3264956 -1.444927 -1.3415852 -1.4495769 -1.4876101 -1.2946868 -1.0971951 -0.98600125 -1.2582304 -1.8006799 -2.3728127 -2.8319564 -2.9774129 -2.9259195 -2.9278944][-1.2341096 -0.87775612 -0.68310905 -0.93912554 -1.1461432 -1.0251744 -0.89015007 -0.93765688 -1.4445612 -2.1417067 -2.6889191 -2.9171681 -2.79329 -2.5744939 -2.5894485]]...]
INFO - root - 2017-12-07 08:58:58.208833: step 23010, loss = 0.77, batch loss = 0.70 (7.7 examples/sec; 1.045 sec/batch; 89h:49m:08s remains)
INFO - root - 2017-12-07 08:59:08.966352: step 23020, loss = 0.74, batch loss = 0.66 (7.7 examples/sec; 1.044 sec/batch; 89h:42m:47s remains)
INFO - root - 2017-12-07 08:59:19.621186: step 23030, loss = 1.01, batch loss = 0.94 (7.3 examples/sec; 1.089 sec/batch; 93h:34m:22s remains)
INFO - root - 2017-12-07 08:59:30.304004: step 23040, loss = 0.64, batch loss = 0.57 (7.2 examples/sec; 1.107 sec/batch; 95h:09m:44s remains)
INFO - root - 2017-12-07 08:59:41.055214: step 23050, loss = 1.00, batch loss = 0.93 (7.4 examples/sec; 1.075 sec/batch; 92h:24m:24s remains)
INFO - root - 2017-12-07 08:59:51.732724: step 23060, loss = 0.73, batch loss = 0.66 (7.4 examples/sec; 1.078 sec/batch; 92h:40m:13s remains)
INFO - root - 2017-12-07 09:00:02.238750: step 23070, loss = 0.90, batch loss = 0.83 (7.5 examples/sec; 1.073 sec/batch; 92h:15m:05s remains)
INFO - root - 2017-12-07 09:00:12.974498: step 23080, loss = 0.79, batch loss = 0.72 (7.6 examples/sec; 1.050 sec/batch; 90h:16m:34s remains)
INFO - root - 2017-12-07 09:00:23.660186: step 23090, loss = 0.84, batch loss = 0.77 (7.4 examples/sec; 1.084 sec/batch; 93h:12m:17s remains)
INFO - root - 2017-12-07 09:00:34.486763: step 23100, loss = 0.83, batch loss = 0.76 (7.3 examples/sec; 1.098 sec/batch; 94h:19m:53s remains)
2017-12-07 09:00:35.277093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2967064 -2.2315061 -2.0015054 -1.9897859 -2.3301427 -2.77802 -3.0905542 -3.1084118 -2.8437626 -2.6383102 -2.6360116 -2.7260792 -2.7970567 -2.897295 -2.9753511][-2.490036 -2.4570963 -2.332485 -2.5077872 -2.9340734 -3.3279071 -3.5072532 -3.3130245 -2.7448673 -2.3950298 -2.5330608 -2.7925782 -2.9022341 -3.010637 -3.0822747][-3.0923276 -3.0637851 -2.9703348 -3.1796403 -3.450546 -3.5787148 -3.5505376 -3.1369326 -2.3874755 -2.1061223 -2.5168934 -2.9811091 -3.1691732 -3.3444989 -3.4620531][-3.600208 -3.4264841 -3.2452526 -3.483228 -3.669559 -3.6691184 -3.6115966 -3.1014771 -2.3071146 -2.1916676 -2.7769322 -3.2445352 -3.4077482 -3.6536591 -3.8445346][-3.5731959 -3.0807533 -2.7280467 -3.0663214 -3.3972642 -3.621316 -3.8089802 -3.2904272 -2.441195 -2.4060016 -2.9776392 -3.229856 -3.1879938 -3.3936234 -3.6138253][-3.1497722 -2.2831924 -1.8379788 -2.3514798 -2.904563 -3.368557 -3.7052846 -3.0309629 -1.9787085 -1.9389734 -2.4360762 -2.4860048 -2.3332324 -2.5158775 -2.6951389][-2.8134933 -1.7307367 -1.3863003 -2.094445 -2.6556616 -2.9394646 -2.9451659 -1.8626068 -0.625046 -0.69846511 -1.2676957 -1.3868966 -1.4144697 -1.728991 -1.8918743][-2.9114761 -1.8377268 -1.608932 -2.3490438 -2.6841652 -2.5078189 -1.8785272 -0.33730698 0.83277607 0.292933 -0.62085509 -1.0151403 -1.3165426 -1.7336452 -1.868875][-2.8125594 -1.9355865 -1.9451625 -2.7224934 -2.831414 -2.2505066 -1.1589625 0.49026346 1.2553434 0.15568066 -0.98366427 -1.465241 -1.8090715 -2.1109338 -2.139225][-2.2056046 -1.5857799 -1.9020095 -2.7419176 -2.7640724 -2.1355393 -1.1175456 0.052001953 0.16466093 -0.98944259 -1.8402042 -2.1124711 -2.2509608 -2.2300565 -2.0378377][-1.806443 -1.3503184 -1.7894146 -2.5930541 -2.6115537 -2.1765354 -1.53157 -1.0097244 -1.3600562 -2.2521048 -2.6155124 -2.6717303 -2.6996255 -2.4868376 -2.2018619][-1.7959743 -1.4150681 -1.8325036 -2.5299673 -2.588155 -2.3163691 -1.9139712 -1.7929852 -2.381639 -3.019248 -3.1176672 -3.2237754 -3.3289456 -3.1403246 -2.9743257][-2.1024134 -1.7782013 -2.1025462 -2.672087 -2.7219777 -2.4099016 -2.0006137 -2.0454471 -2.6862061 -3.05485 -3.0014868 -3.1775413 -3.29917 -3.1888835 -3.2620988][-2.3265829 -2.1898558 -2.5779276 -3.1267836 -3.0860586 -2.5356054 -1.9222245 -1.9345698 -2.4937639 -2.6528263 -2.5755973 -2.7440879 -2.6890805 -2.4815259 -2.7144837][-2.1287117 -2.2431443 -2.8114262 -3.4481483 -3.4080417 -2.7232156 -1.9059465 -1.8209724 -2.2217538 -2.2548504 -2.2464728 -2.4109333 -2.2133083 -2.0118802 -2.4190302]]...]
INFO - root - 2017-12-07 09:00:46.165499: step 23110, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.082 sec/batch; 93h:00m:46s remains)
INFO - root - 2017-12-07 09:00:56.938999: step 23120, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.087 sec/batch; 93h:22m:57s remains)
INFO - root - 2017-12-07 09:01:07.623046: step 23130, loss = 0.97, batch loss = 0.90 (7.6 examples/sec; 1.057 sec/batch; 90h:49m:33s remains)
INFO - root - 2017-12-07 09:01:18.383559: step 23140, loss = 0.88, batch loss = 0.81 (7.5 examples/sec; 1.066 sec/batch; 91h:36m:39s remains)
INFO - root - 2017-12-07 09:01:29.086459: step 23150, loss = 0.81, batch loss = 0.74 (7.5 examples/sec; 1.065 sec/batch; 91h:32m:28s remains)
INFO - root - 2017-12-07 09:01:39.881840: step 23160, loss = 0.83, batch loss = 0.76 (7.5 examples/sec; 1.064 sec/batch; 91h:23m:40s remains)
INFO - root - 2017-12-07 09:01:50.365857: step 23170, loss = 0.74, batch loss = 0.67 (7.4 examples/sec; 1.082 sec/batch; 93h:00m:11s remains)
INFO - root - 2017-12-07 09:02:01.012537: step 23180, loss = 0.77, batch loss = 0.70 (7.3 examples/sec; 1.091 sec/batch; 93h:47m:00s remains)
INFO - root - 2017-12-07 09:02:11.775856: step 23190, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.074 sec/batch; 92h:14m:24s remains)
INFO - root - 2017-12-07 09:02:22.581562: step 23200, loss = 0.81, batch loss = 0.73 (7.4 examples/sec; 1.075 sec/batch; 92h:20m:40s remains)
2017-12-07 09:02:23.432641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8956492 -2.8201063 -2.7527647 -2.7496958 -2.8078418 -2.8676736 -2.9141459 -2.9695404 -2.9976232 -3.0292335 -3.0014811 -2.915307 -2.8602309 -2.8405118 -2.8476045][-2.9052138 -2.850625 -2.7988482 -2.8230362 -2.9057922 -2.9629531 -3.0011468 -3.0544729 -3.0856755 -3.1180949 -3.06695 -2.9401689 -2.8588734 -2.8390672 -2.8688021][-2.881372 -2.8327007 -2.7959635 -2.8666682 -2.9895353 -3.041157 -3.0597284 -3.1045156 -3.1698694 -3.2684417 -3.2524858 -3.1078124 -2.9661365 -2.8899775 -2.8953097][-2.8696342 -2.8145413 -2.7876086 -2.8915439 -3.0146198 -2.9912739 -2.9135995 -2.8955777 -3.0229568 -3.2880337 -3.4166873 -3.3408258 -3.1666908 -3.0192888 -2.9574487][-2.8710694 -2.8271093 -2.8301282 -2.9476452 -2.9908848 -2.769675 -2.4339688 -2.2305603 -2.4153111 -2.95597 -3.3706346 -3.4741151 -3.3153329 -3.1183877 -2.9941063][-2.8333638 -2.8461738 -2.9306092 -3.0553749 -2.9245377 -2.3304965 -1.5113106 -0.95834064 -1.1953878 -2.1391361 -3.0124855 -3.44545 -3.3895905 -3.1852083 -3.0153475][-2.7151384 -2.8107328 -3.0084276 -3.1210105 -2.7356267 -1.630851 -0.17693233 0.76456833 0.36571693 -1.1490388 -2.6162899 -3.4430687 -3.5155935 -3.30744 -3.0837083][-2.640377 -2.8209937 -3.1319063 -3.2417154 -2.6358485 -1.0929725 0.92241955 2.2002602 1.6070485 -0.41908169 -2.3695085 -3.4851203 -3.6524124 -3.4325821 -3.1657078][-2.6991625 -2.9422393 -3.3589258 -3.5527864 -2.9839721 -1.4302878 0.67226887 2.021337 1.4204621 -0.602129 -2.5296507 -3.5926933 -3.7271509 -3.48026 -3.2035797][-2.8388839 -3.0707426 -3.5182555 -3.8240921 -3.5174837 -2.3769724 -0.67930889 0.43600321 0.0032906532 -1.508383 -2.9532795 -3.7027669 -3.7351444 -3.4899375 -3.2470541][-3.0463359 -3.19956 -3.5570307 -3.8891215 -3.8508463 -3.2157497 -2.075058 -1.2603219 -1.4360111 -2.2556798 -3.0903492 -3.5168982 -3.5040843 -3.3558216 -3.2197313][-3.2277365 -3.267508 -3.4481287 -3.6853108 -3.7734044 -3.5110705 -2.8548121 -2.3338265 -2.3163359 -2.5924871 -2.9567409 -3.1770179 -3.1901541 -3.183218 -3.1669908][-3.3464766 -3.2693014 -3.2726369 -3.3669221 -3.445641 -3.3594508 -3.0298376 -2.7453272 -2.6406164 -2.6167936 -2.7306252 -2.8814816 -2.9748392 -3.1092696 -3.1741648][-3.4568934 -3.2908077 -3.1779962 -3.18079 -3.2248144 -3.1981926 -3.0421944 -2.8841653 -2.7330136 -2.5620337 -2.5554271 -2.6903448 -2.8584206 -3.092186 -3.197031][-3.5661006 -3.3491952 -3.1646605 -3.1030507 -3.0994465 -3.0680566 -2.973712 -2.8520856 -2.6841331 -2.4933019 -2.4670448 -2.617842 -2.8402386 -3.1074457 -3.2060895]]...]
INFO - root - 2017-12-07 09:02:34.224781: step 23210, loss = 0.81, batch loss = 0.74 (7.4 examples/sec; 1.078 sec/batch; 92h:35m:00s remains)
INFO - root - 2017-12-07 09:02:44.965499: step 23220, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.070 sec/batch; 91h:55m:37s remains)
INFO - root - 2017-12-07 09:02:55.806629: step 23230, loss = 0.98, batch loss = 0.90 (7.7 examples/sec; 1.043 sec/batch; 89h:37m:07s remains)
INFO - root - 2017-12-07 09:03:06.509641: step 23240, loss = 0.77, batch loss = 0.70 (7.3 examples/sec; 1.099 sec/batch; 94h:25m:17s remains)
INFO - root - 2017-12-07 09:03:17.331765: step 23250, loss = 0.67, batch loss = 0.60 (7.2 examples/sec; 1.110 sec/batch; 95h:20m:51s remains)
INFO - root - 2017-12-07 09:03:28.169771: step 23260, loss = 0.79, batch loss = 0.72 (7.6 examples/sec; 1.056 sec/batch; 90h:42m:21s remains)
INFO - root - 2017-12-07 09:03:38.712106: step 23270, loss = 0.89, batch loss = 0.82 (7.5 examples/sec; 1.064 sec/batch; 91h:25m:59s remains)
INFO - root - 2017-12-07 09:03:49.452245: step 23280, loss = 0.76, batch loss = 0.68 (7.4 examples/sec; 1.080 sec/batch; 92h:47m:48s remains)
INFO - root - 2017-12-07 09:04:00.111265: step 23290, loss = 0.80, batch loss = 0.73 (7.6 examples/sec; 1.049 sec/batch; 90h:04m:12s remains)
INFO - root - 2017-12-07 09:04:10.901321: step 23300, loss = 0.84, batch loss = 0.77 (7.5 examples/sec; 1.070 sec/batch; 91h:56m:28s remains)
2017-12-07 09:04:11.664292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6191635 -3.6043925 -3.8394141 -3.9669564 -3.7760212 -3.6544213 -3.9173641 -4.3527856 -4.4402227 -4.1180034 -3.492059 -3.0339684 -3.4324441 -4.6088853 -5.4393778][-3.234653 -3.4940622 -3.9422679 -4.1700726 -4.0805092 -3.9745843 -4.1207504 -4.4109497 -4.4146147 -4.0690384 -3.4378533 -2.9070864 -3.2242014 -4.3082848 -5.0806336][-3.4604373 -3.9564438 -4.4333482 -4.5956383 -4.4533181 -4.2262931 -4.2900424 -4.5650864 -4.6330919 -4.3980927 -3.8310881 -3.2590351 -3.4294624 -4.3423457 -4.9944539][-4.1174207 -4.569653 -4.7687864 -4.6730223 -4.3257871 -3.9075372 -3.9311006 -4.2843266 -4.5039535 -4.5114651 -4.1337757 -3.6678751 -3.7836709 -4.624414 -5.2876987][-4.62896 -4.6979351 -4.3380957 -3.8234696 -3.1904871 -2.6473391 -2.7729752 -3.3082693 -3.7555618 -4.0175338 -3.8425891 -3.5351088 -3.6703863 -4.5591187 -5.381608][-4.1930957 -3.7118773 -2.7237632 -1.7373455 -0.72464871 -0.027867317 -0.2354269 -0.94824791 -1.6587515 -2.1721013 -2.2437258 -2.2154148 -2.5563264 -3.6706507 -4.7964072][-3.4256828 -2.589385 -1.2224839 0.072054386 1.3457136 2.1601224 1.9945312 1.2843966 0.50190783 -0.098379135 -0.38426781 -0.68757486 -1.4088674 -2.8516426 -4.2428985][-4.0847983 -3.3235822 -2.0955532 -0.9402287 0.12371588 0.68783379 0.53531647 0.030000687 -0.5474894 -0.93935657 -1.1365705 -1.397341 -2.0834956 -3.4120646 -4.6781068][-4.8100724 -4.3718672 -3.6600506 -3.000648 -2.4221878 -2.2008734 -2.3470769 -2.6917892 -3.0931056 -3.2542238 -3.205862 -3.1915803 -3.5245285 -4.3657403 -5.1828213][-4.1130376 -3.8787174 -3.57235 -3.3166778 -3.1254942 -3.1686687 -3.3770554 -3.6930618 -4.0448422 -4.0916829 -3.835891 -3.557749 -3.5738337 -4.0774965 -4.6041813][-3.3899951 -3.2781017 -3.2027435 -3.1986728 -3.2335813 -3.4568472 -3.7601719 -4.0734434 -4.345366 -4.2499161 -3.7702162 -3.2337916 -3.0358193 -3.4107618 -3.8656032][-3.3932953 -3.5207636 -3.7131712 -3.9366653 -4.1154332 -4.3894792 -4.6425362 -4.8297763 -4.915947 -4.6728191 -4.0911555 -3.4956355 -3.3046575 -3.7092643 -4.1273952][-3.8893793 -4.2165852 -4.5859818 -4.901515 -5.0721164 -5.2142291 -5.2745028 -5.2261271 -5.0435467 -4.639318 -4.0630016 -3.63591 -3.6886954 -4.2623768 -4.7300587][-4.4719877 -4.7951202 -5.1033921 -5.2877207 -5.2740221 -5.1900682 -5.026648 -4.7878675 -4.4660439 -4.0589452 -3.63618 -3.4598434 -3.7510238 -4.4023414 -4.8927646][-4.6668277 -4.84923 -4.955184 -4.9163122 -4.7059321 -4.4710813 -4.2332468 -4.030828 -3.8429682 -3.6680975 -3.5070465 -3.5236566 -3.8775859 -4.4155264 -4.7909336]]...]
INFO - root - 2017-12-07 09:04:22.530616: step 23310, loss = 0.82, batch loss = 0.75 (7.4 examples/sec; 1.086 sec/batch; 93h:18m:11s remains)
INFO - root - 2017-12-07 09:04:33.391730: step 23320, loss = 0.57, batch loss = 0.50 (7.4 examples/sec; 1.083 sec/batch; 93h:01m:36s remains)
INFO - root - 2017-12-07 09:04:44.098521: step 23330, loss = 0.81, batch loss = 0.74 (7.4 examples/sec; 1.080 sec/batch; 92h:42m:58s remains)
INFO - root - 2017-12-07 09:04:54.715130: step 23340, loss = 0.68, batch loss = 0.61 (7.5 examples/sec; 1.064 sec/batch; 91h:23m:01s remains)
INFO - root - 2017-12-07 09:05:05.420898: step 23350, loss = 0.66, batch loss = 0.59 (7.6 examples/sec; 1.056 sec/batch; 90h:43m:23s remains)
INFO - root - 2017-12-07 09:05:16.136069: step 23360, loss = 0.80, batch loss = 0.72 (7.3 examples/sec; 1.089 sec/batch; 93h:31m:42s remains)
INFO - root - 2017-12-07 09:05:26.585917: step 23370, loss = 0.77, batch loss = 0.70 (7.4 examples/sec; 1.076 sec/batch; 92h:24m:55s remains)
INFO - root - 2017-12-07 09:05:37.304939: step 23380, loss = 0.85, batch loss = 0.77 (7.6 examples/sec; 1.058 sec/batch; 90h:50m:37s remains)
INFO - root - 2017-12-07 09:05:47.946620: step 23390, loss = 0.94, batch loss = 0.87 (7.7 examples/sec; 1.040 sec/batch; 89h:20m:20s remains)
INFO - root - 2017-12-07 09:05:58.614459: step 23400, loss = 0.88, batch loss = 0.81 (7.5 examples/sec; 1.068 sec/batch; 91h:42m:32s remains)
2017-12-07 09:05:59.369954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.063592 -2.958385 -2.9024742 -2.7015815 -2.4997857 -2.4634047 -2.4592223 -2.4249177 -2.4890521 -2.6392307 -2.7192192 -2.7289598 -2.7910938 -2.9745867 -3.1272342][-3.1706471 -3.1303067 -3.0888462 -2.8710754 -2.6283407 -2.5397701 -2.5065718 -2.4893131 -2.551223 -2.696022 -2.8525882 -2.994822 -3.1371119 -3.2423706 -3.2508702][-3.2879512 -3.2980855 -3.2432017 -2.9912295 -2.6910434 -2.5298 -2.4859891 -2.5138588 -2.6150277 -2.7566876 -2.9377949 -3.1419559 -3.332969 -3.4162786 -3.3711863][-3.3078275 -3.3114998 -3.2103505 -2.9073787 -2.5354342 -2.2923503 -2.2106471 -2.2331929 -2.3599656 -2.5344853 -2.791873 -3.0941353 -3.3710108 -3.5116026 -3.487952][-3.1496515 -3.0955939 -2.9508188 -2.6348531 -2.2586393 -2.0003147 -1.8660009 -1.7960579 -1.886235 -2.1096296 -2.4752493 -2.9138691 -3.3121533 -3.5500474 -3.5667186][-2.8516634 -2.6732304 -2.477459 -2.17715 -1.8672497 -1.653564 -1.4747257 -1.2706413 -1.303947 -1.6148512 -2.1311367 -2.6776936 -3.1546617 -3.4738922 -3.5474696][-2.5117431 -2.1735673 -1.8969495 -1.5921116 -1.346297 -1.1551945 -0.88815188 -0.520488 -0.48419166 -0.94193006 -1.680263 -2.3725398 -2.9346006 -3.329536 -3.4702938][-2.2386127 -1.8271732 -1.5341265 -1.2375758 -1.034739 -0.81456757 -0.43317485 0.077260494 0.19958591 -0.3892622 -1.3116648 -2.1527898 -2.8015528 -3.2468886 -3.410197][-2.1460574 -1.7696738 -1.5080905 -1.2237301 -1.0241511 -0.7983098 -0.45754218 -0.02355957 0.11822796 -0.37654734 -1.2204921 -2.014931 -2.6526203 -3.1139505 -3.3148503][-2.2738283 -1.9550467 -1.7330849 -1.4886308 -1.3347809 -1.1988518 -1.0250001 -0.76389122 -0.60947442 -0.861768 -1.4097345 -1.9752769 -2.5100141 -2.9775767 -3.2519546][-2.4616036 -2.136802 -1.9305623 -1.7565303 -1.736068 -1.7564955 -1.7095716 -1.5039148 -1.2501869 -1.2756536 -1.5755122 -1.9755623 -2.448545 -2.9204586 -3.2652502][-2.5562053 -2.2078984 -2.0476644 -2.011945 -2.1551375 -2.2824464 -2.2126427 -1.9203894 -1.5355003 -1.4324586 -1.6560066 -2.0172739 -2.4731715 -2.9365582 -3.3266282][-2.4803891 -2.1702962 -2.1176605 -2.2144041 -2.4729123 -2.6177206 -2.4381976 -2.0211272 -1.5528395 -1.4182243 -1.6893368 -2.0828738 -2.514647 -2.9333115 -3.3425496][-2.322675 -2.1013644 -2.0900064 -2.183156 -2.4553137 -2.5747705 -2.3539641 -1.9300473 -1.4741917 -1.3425019 -1.6679735 -2.0908787 -2.4792335 -2.8622742 -3.2876866][-2.2644739 -2.1125965 -2.0287552 -1.9934652 -2.2232587 -2.3355372 -2.2014515 -1.9041471 -1.5027268 -1.3311818 -1.613179 -2.0078764 -2.3491385 -2.7184539 -3.1741853]]...]
INFO - root - 2017-12-07 09:06:10.215398: step 23410, loss = 0.70, batch loss = 0.63 (7.3 examples/sec; 1.093 sec/batch; 93h:50m:11s remains)
INFO - root - 2017-12-07 09:06:21.050660: step 23420, loss = 0.78, batch loss = 0.71 (7.5 examples/sec; 1.074 sec/batch; 92h:10m:04s remains)
INFO - root - 2017-12-07 09:06:31.742936: step 23430, loss = 0.96, batch loss = 0.88 (7.4 examples/sec; 1.081 sec/batch; 92h:50m:55s remains)
INFO - root - 2017-12-07 09:06:42.515772: step 23440, loss = 0.87, batch loss = 0.80 (7.4 examples/sec; 1.084 sec/batch; 93h:02m:46s remains)
INFO - root - 2017-12-07 09:06:53.183742: step 23450, loss = 0.70, batch loss = 0.63 (7.5 examples/sec; 1.063 sec/batch; 91h:13m:31s remains)
INFO - root - 2017-12-07 09:07:03.797132: step 23460, loss = 0.89, batch loss = 0.82 (7.5 examples/sec; 1.063 sec/batch; 91h:17m:26s remains)
INFO - root - 2017-12-07 09:07:14.334065: step 23470, loss = 0.83, batch loss = 0.75 (7.4 examples/sec; 1.084 sec/batch; 93h:03m:00s remains)
INFO - root - 2017-12-07 09:07:24.997596: step 23480, loss = 0.97, batch loss = 0.90 (7.5 examples/sec; 1.072 sec/batch; 92h:01m:23s remains)
INFO - root - 2017-12-07 09:07:35.850124: step 23490, loss = 0.79, batch loss = 0.72 (7.3 examples/sec; 1.100 sec/batch; 94h:25m:44s remains)
INFO - root - 2017-12-07 09:07:46.636545: step 23500, loss = 0.82, batch loss = 0.75 (7.7 examples/sec; 1.043 sec/batch; 89h:33m:00s remains)
2017-12-07 09:07:47.422871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4670155 -3.5859847 -3.5019124 -3.2558289 -2.9167843 -2.7055016 -2.9286332 -3.2678118 -3.2520061 -2.9885678 -2.7796969 -3.0311749 -3.450455 -3.2915764 -2.6254625][-3.7071671 -3.8762364 -3.752152 -3.430181 -2.979486 -2.7116821 -3.0141549 -3.4292872 -3.3917036 -3.1651158 -2.9953213 -3.3811836 -3.8707635 -3.5007436 -2.4281583][-3.8760695 -3.9555082 -3.7175789 -3.3501964 -2.8870487 -2.6884203 -3.08668 -3.4768128 -3.4141827 -3.3708889 -3.3419938 -3.7754433 -4.1683555 -3.5356581 -2.1147921][-3.8165374 -3.6524515 -3.2513413 -2.8923564 -2.5718198 -2.5443821 -3.0278435 -3.3322735 -3.3096271 -3.5766904 -3.7536707 -4.1398759 -4.2904825 -3.3767202 -1.7477798][-3.6246605 -3.2137856 -2.7109957 -2.3953502 -2.2335169 -2.2912047 -2.6853228 -2.8102489 -2.8589988 -3.55697 -4.0482988 -4.3928542 -4.3082027 -3.1696992 -1.4450042][-3.6544025 -3.2045383 -2.7723382 -2.5420206 -2.4058542 -2.2726288 -2.1964974 -1.8422267 -1.8853114 -3.1269689 -4.1160774 -4.5366769 -4.3134403 -3.0662985 -1.3319552][-3.7114463 -3.4654484 -3.2803683 -3.2356153 -3.0793319 -2.5727983 -1.732841 -0.59586787 -0.48887587 -2.3113148 -3.9503458 -4.5965691 -4.3523941 -3.0714114 -1.3211999][-3.527353 -3.5779431 -3.7137327 -3.9169686 -3.8040669 -3.0533023 -1.5330544 0.47792578 0.93829727 -1.2831695 -3.566925 -4.5513191 -4.3940625 -3.1582928 -1.4274411][-3.1463737 -3.4853339 -3.9445994 -4.3827286 -4.3773046 -3.6240094 -1.8485615 0.69674969 1.6108203 -0.59009123 -3.1337876 -4.3448195 -4.3049765 -3.2250125 -1.6104324][-2.6382236 -3.1404181 -3.848124 -4.4694853 -4.6202126 -4.0985117 -2.5571339 -0.13406229 0.970253 -0.71757722 -2.9181817 -4.0190926 -4.0427265 -3.2087173 -1.8519156][-2.2203414 -2.7049017 -3.4498057 -4.0992851 -4.3766356 -4.2408752 -3.2309904 -1.3438241 -0.33365917 -1.4309635 -2.9557848 -3.7035708 -3.7291207 -3.2255325 -2.2339396][-2.1576438 -2.5181837 -3.0774984 -3.5213184 -3.7983165 -3.9903095 -3.5111053 -2.2458432 -1.5106134 -2.1721849 -3.1036086 -3.5158074 -3.53827 -3.317296 -2.7022][-2.605598 -2.8531005 -3.1666636 -3.3430047 -3.512681 -3.8195198 -3.6408119 -2.8422275 -2.3672998 -2.7585425 -3.2730992 -3.4190917 -3.3918071 -3.2982264 -2.9571493][-3.0987024 -3.2612867 -3.3920805 -3.4219537 -3.5079219 -3.7679443 -3.6727524 -3.1872277 -2.9365025 -3.1523442 -3.373553 -3.323204 -3.2229395 -3.1163878 -2.92139][-3.2595339 -3.3413591 -3.396739 -3.3924668 -3.4237361 -3.5528259 -3.4308417 -3.1418777 -3.0714126 -3.2366738 -3.336112 -3.2200732 -3.1012168 -2.9673996 -2.8310051]]...]
INFO - root - 2017-12-07 09:07:58.246042: step 23510, loss = 0.81, batch loss = 0.73 (7.5 examples/sec; 1.064 sec/batch; 91h:17m:00s remains)
INFO - root - 2017-12-07 09:08:08.941640: step 23520, loss = 0.85, batch loss = 0.78 (7.6 examples/sec; 1.057 sec/batch; 90h:45m:39s remains)
INFO - root - 2017-12-07 09:08:19.526928: step 23530, loss = 0.70, batch loss = 0.63 (7.7 examples/sec; 1.042 sec/batch; 89h:23m:51s remains)
INFO - root - 2017-12-07 09:08:30.231642: step 23540, loss = 0.83, batch loss = 0.76 (7.6 examples/sec; 1.056 sec/batch; 90h:35m:14s remains)
INFO - root - 2017-12-07 09:08:40.980600: step 23550, loss = 0.76, batch loss = 0.68 (7.6 examples/sec; 1.059 sec/batch; 90h:55m:19s remains)
INFO - root - 2017-12-07 09:08:51.631809: step 23560, loss = 0.79, batch loss = 0.71 (7.5 examples/sec; 1.071 sec/batch; 91h:55m:32s remains)
INFO - root - 2017-12-07 09:09:02.023045: step 23570, loss = 0.92, batch loss = 0.84 (7.6 examples/sec; 1.056 sec/batch; 90h:36m:57s remains)
INFO - root - 2017-12-07 09:09:12.665788: step 23580, loss = 0.73, batch loss = 0.65 (7.5 examples/sec; 1.070 sec/batch; 91h:50m:49s remains)
INFO - root - 2017-12-07 09:09:23.491947: step 23590, loss = 0.84, batch loss = 0.76 (7.3 examples/sec; 1.089 sec/batch; 93h:24m:26s remains)
INFO - root - 2017-12-07 09:09:34.229536: step 23600, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.080 sec/batch; 92h:40m:14s remains)
2017-12-07 09:09:35.014492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5185418 -3.4389603 -3.4425383 -3.5585308 -3.7141643 -3.6893253 -3.4424279 -3.2492108 -3.11899 -3.1441464 -3.1863346 -3.1045356 -3.040132 -3.0200062 -3.1393476][-3.4696317 -3.3431871 -3.3540964 -3.5498707 -3.834363 -3.8625813 -3.5815022 -3.3732674 -3.2510223 -3.31645 -3.3825889 -3.2708335 -3.1635182 -3.0909748 -3.2505746][-3.3661451 -3.1213882 -3.0614192 -3.2828074 -3.6578579 -3.7039728 -3.3946366 -3.2158685 -3.2003393 -3.4134932 -3.5435703 -3.4060423 -3.2254992 -3.0464506 -3.1654844][-3.2399349 -2.86235 -2.6983557 -2.9004421 -3.3012071 -3.3054953 -2.9309793 -2.7642281 -2.9027138 -3.3151739 -3.5553684 -3.4583235 -3.2861195 -3.0765147 -3.1278992][-3.1464963 -2.6957297 -2.4730296 -2.6615944 -3.03617 -2.9485817 -2.4323905 -2.1496165 -2.3721781 -2.9606791 -3.3533697 -3.3970447 -3.3781381 -3.2669747 -3.2253339][-3.1139164 -2.6826611 -2.4893436 -2.6898489 -2.9857163 -2.7519121 -2.0347314 -1.5238669 -1.7403123 -2.4722714 -3.023068 -3.2577853 -3.4442511 -3.4643705 -3.2797675][-3.1129298 -2.7789474 -2.7041626 -2.9583571 -3.1209981 -2.6648493 -1.696341 -0.90490818 -1.0509524 -1.9341838 -2.6552608 -3.0631492 -3.4006062 -3.5174155 -3.2638788][-3.0994563 -2.877393 -2.969121 -3.3504848 -3.4387934 -2.796607 -1.6163733 -0.54045963 -0.50283337 -1.4479806 -2.3262687 -2.8448138 -3.20159 -3.2977262 -3.042284][-3.063015 -2.9195118 -3.1542263 -3.7096224 -3.8427527 -3.1699662 -1.9794185 -0.77705741 -0.47872996 -1.2438455 -2.1068397 -2.6336105 -2.9252534 -2.9323859 -2.7197845][-3.0280261 -2.9440546 -3.2947447 -4.033412 -4.3006015 -3.702975 -2.6517181 -1.5298376 -1.0321403 -1.4780786 -2.12824 -2.5260839 -2.6898718 -2.5929174 -2.4271934][-2.9930668 -2.9454393 -3.3895221 -4.3068275 -4.7757483 -4.33647 -3.4812315 -2.5630252 -1.9733052 -2.0703392 -2.4272768 -2.6706779 -2.7289414 -2.5625057 -2.4249523][-2.9724259 -2.9263139 -3.4036393 -4.409894 -5.0553341 -4.795228 -4.1485057 -3.5063334 -2.9985158 -2.848721 -2.9340639 -3.0274138 -3.0170126 -2.8780179 -2.82509][-2.9849513 -2.9340196 -3.3918018 -4.3605061 -5.0655837 -4.9509754 -4.4758139 -4.1032405 -3.8207726 -3.6464314 -3.6141264 -3.5781393 -3.478282 -3.3701458 -3.3818109][-3.0317116 -2.9749074 -3.3736985 -4.1847796 -4.8073707 -4.7759476 -4.4930162 -4.3852677 -4.3563895 -4.2505803 -4.1900954 -4.0907774 -3.9782407 -3.928992 -3.9521842][-3.0760965 -2.9901454 -3.2950006 -3.9276447 -4.4711809 -4.5391932 -4.4030395 -4.4177094 -4.5147243 -4.4679136 -4.3896856 -4.2604671 -4.16663 -4.2013469 -4.2597194]]...]
INFO - root - 2017-12-07 09:09:45.797554: step 23610, loss = 0.75, batch loss = 0.68 (7.5 examples/sec; 1.070 sec/batch; 91h:47m:47s remains)
INFO - root - 2017-12-07 09:09:56.513562: step 23620, loss = 0.83, batch loss = 0.75 (7.5 examples/sec; 1.068 sec/batch; 91h:39m:06s remains)
INFO - root - 2017-12-07 09:10:07.203387: step 23630, loss = 0.72, batch loss = 0.65 (7.4 examples/sec; 1.085 sec/batch; 93h:03m:00s remains)
INFO - root - 2017-12-07 09:10:18.022638: step 23640, loss = 0.83, batch loss = 0.75 (7.4 examples/sec; 1.088 sec/batch; 93h:20m:02s remains)
INFO - root - 2017-12-07 09:10:28.822159: step 23650, loss = 0.96, batch loss = 0.88 (7.4 examples/sec; 1.084 sec/batch; 93h:00m:57s remains)
INFO - root - 2017-12-07 09:10:39.596286: step 23660, loss = 0.81, batch loss = 0.74 (7.4 examples/sec; 1.084 sec/batch; 92h:59m:43s remains)
INFO - root - 2017-12-07 09:10:50.064732: step 23670, loss = 0.90, batch loss = 0.82 (7.7 examples/sec; 1.043 sec/batch; 89h:29m:28s remains)
INFO - root - 2017-12-07 09:11:00.855099: step 23680, loss = 0.72, batch loss = 0.65 (7.3 examples/sec; 1.090 sec/batch; 93h:28m:34s remains)
INFO - root - 2017-12-07 09:11:11.553572: step 23690, loss = 0.81, batch loss = 0.74 (7.5 examples/sec; 1.064 sec/batch; 91h:16m:55s remains)
INFO - root - 2017-12-07 09:11:22.378133: step 23700, loss = 0.87, batch loss = 0.79 (7.4 examples/sec; 1.080 sec/batch; 92h:40m:28s remains)
2017-12-07 09:11:23.175749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4248455 -1.0699735 -0.88777709 -1.0461662 -1.3380921 -1.7828929 -2.522207 -3.5555115 -4.4247985 -4.5807543 -3.9880381 -2.9592769 -2.0274124 -1.8920865 -2.7257044][-0.98684359 -0.49408937 -0.24266195 -0.46092057 -0.8921659 -1.4333973 -2.1155012 -2.9767261 -3.7350845 -3.9324896 -3.5398335 -2.7978969 -2.1415279 -2.0538249 -2.6657076][-0.71890545 -0.14376211 0.21119213 0.041931152 -0.48545718 -1.1742902 -1.8970768 -2.5798512 -3.0729856 -3.1580811 -2.9085312 -2.5327187 -2.2516499 -2.2455134 -2.614018][-0.853251 -0.21483803 0.30578089 0.34746742 -0.11498737 -0.89747763 -1.7556636 -2.3957663 -2.7023411 -2.7310424 -2.6761761 -2.7182655 -2.8576202 -2.9814162 -3.1795688][-1.0733416 -0.37498951 0.32482719 0.63063478 0.31984234 -0.48644924 -1.4543283 -2.0906754 -2.2685618 -2.2705336 -2.4422569 -2.9011197 -3.4618902 -3.7727172 -3.8793659][-1.2090454 -0.4270668 0.43999386 1.0019822 0.88872719 0.14075947 -0.86480689 -1.5047176 -1.590487 -1.5986779 -1.964781 -2.7146769 -3.5808151 -4.0886874 -4.1708474][-1.2716858 -0.34539318 0.76248837 1.6468682 1.8190575 1.2541113 0.31087303 -0.3416028 -0.43107986 -0.55739784 -1.176178 -2.1358979 -3.1724832 -3.8644395 -4.0482721][-1.2160137 -0.16071939 1.1379323 2.2300019 2.6523108 2.4015679 1.7364569 1.2292972 1.166791 0.80260944 -0.24598265 -1.5036986 -2.6955621 -3.5786347 -3.9324713][-1.2047055 -0.06718874 1.315917 2.4314127 2.9659629 3.0227089 2.749959 2.5766163 2.6170959 1.9516182 0.42334747 -1.1323762 -2.4442122 -3.459224 -3.9443123][-1.5190482 -0.35035324 1.030612 2.0363269 2.5072842 2.6621509 2.6549883 2.7722397 2.8333917 1.92342 0.16081429 -1.4946957 -2.7528837 -3.6823566 -4.1584139][-2.0989196 -0.96610856 0.33505583 1.2202773 1.6035933 1.7644835 1.8981376 2.1628194 2.1316619 1.0493765 -0.72105956 -2.2466702 -3.1891804 -3.7405925 -3.9646807][-2.3662992 -1.3364656 -0.17032051 0.62757492 0.96758604 1.1297498 1.3950987 1.7600956 1.5949278 0.36928415 -1.3971145 -2.8098428 -3.4672863 -3.569689 -3.4362767][-2.1676009 -1.2490296 -0.1981287 0.59432793 0.99343538 1.2649746 1.7126222 2.2149768 1.9804425 0.626276 -1.2375994 -2.7475049 -3.3751874 -3.2270074 -2.7536526][-1.9380603 -1.1326663 -0.22505379 0.50371075 0.92726755 1.3014431 1.9268947 2.601666 2.4985976 1.2771845 -0.49791241 -2.0003753 -2.6408584 -2.4043398 -1.7680755][-1.9266951 -1.2395818 -0.48664474 0.10451746 0.45881605 0.83564234 1.5165691 2.3128586 2.5010476 1.7426023 0.38624573 -0.87509418 -1.4818332 -1.2745504 -0.67338729]]...]
INFO - root - 2017-12-07 09:11:33.951950: step 23710, loss = 0.67, batch loss = 0.60 (7.4 examples/sec; 1.087 sec/batch; 93h:12m:58s remains)
INFO - root - 2017-12-07 09:11:44.746245: step 23720, loss = 0.69, batch loss = 0.62 (7.4 examples/sec; 1.080 sec/batch; 92h:37m:01s remains)
INFO - root - 2017-12-07 09:11:55.384378: step 23730, loss = 0.78, batch loss = 0.71 (7.6 examples/sec; 1.059 sec/batch; 90h:50m:26s remains)
INFO - root - 2017-12-07 09:12:06.075364: step 23740, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.060 sec/batch; 90h:56m:21s remains)
INFO - root - 2017-12-07 09:12:16.813938: step 23750, loss = 0.84, batch loss = 0.76 (7.4 examples/sec; 1.074 sec/batch; 92h:06m:24s remains)
INFO - root - 2017-12-07 09:12:27.621466: step 23760, loss = 0.81, batch loss = 0.74 (7.3 examples/sec; 1.099 sec/batch; 94h:15m:14s remains)
INFO - root - 2017-12-07 09:12:38.051997: step 23770, loss = 0.63, batch loss = 0.56 (7.5 examples/sec; 1.061 sec/batch; 90h:57m:08s remains)
INFO - root - 2017-12-07 09:12:48.947411: step 23780, loss = 1.13, batch loss = 1.06 (7.6 examples/sec; 1.056 sec/batch; 90h:35m:45s remains)
INFO - root - 2017-12-07 09:12:59.654812: step 23790, loss = 0.74, batch loss = 0.66 (7.7 examples/sec; 1.043 sec/batch; 89h:28m:01s remains)
INFO - root - 2017-12-07 09:13:10.407058: step 23800, loss = 0.83, batch loss = 0.76 (7.3 examples/sec; 1.094 sec/batch; 93h:49m:52s remains)
2017-12-07 09:13:11.189720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3379464 -3.3624463 -3.3546326 -3.278995 -3.1964641 -3.1742663 -3.1826191 -3.1938002 -3.206413 -3.2293329 -3.2401087 -3.1981406 -3.1315928 -3.1420236 -3.23476][-3.3156815 -3.3251896 -3.2869978 -3.1657293 -3.0511403 -3.0383577 -3.0905547 -3.1411147 -3.1510513 -3.1442184 -3.1320038 -3.0952475 -3.0557313 -3.0661509 -3.1217117][-3.2689648 -3.22584 -3.1379435 -2.98494 -2.8608322 -2.8428621 -2.9142745 -2.9828279 -2.9584126 -2.8936682 -2.8517675 -2.8267224 -2.8057079 -2.7956352 -2.8376918][-3.2293262 -3.0992489 -2.9591084 -2.8058951 -2.6923561 -2.6371474 -2.677022 -2.7501984 -2.7041557 -2.5890186 -2.5204802 -2.5289965 -2.543649 -2.5124979 -2.5610538][-3.2088385 -2.9378176 -2.6789289 -2.4674416 -2.314939 -2.1899827 -2.194742 -2.3213391 -2.3458483 -2.2433836 -2.1353338 -2.1054304 -2.0614803 -1.9169838 -1.931042][-3.0283525 -2.6244378 -2.2387326 -1.9373167 -1.6930547 -1.4273164 -1.3018196 -1.4180133 -1.5850627 -1.6458929 -1.5897918 -1.5201976 -1.3780389 -1.060699 -0.93438005][-2.5032382 -2.0907712 -1.7171586 -1.4269845 -1.0961378 -0.58958888 -0.14822483 -0.065672874 -0.33678722 -0.68392158 -0.87272668 -0.95636344 -0.88328624 -0.52790165 -0.26584005][-1.9316139 -1.6655858 -1.4724882 -1.3374636 -1.0263219 -0.37174368 0.35160208 0.68511868 0.44191837 -0.032192707 -0.344954 -0.5656321 -0.64727569 -0.4374392 -0.22350168][-1.8378246 -1.7133632 -1.6996481 -1.7658904 -1.6680584 -1.2152956 -0.57589579 -0.14302444 -0.14254665 -0.34931421 -0.43860269 -0.5841918 -0.7155695 -0.63765764 -0.53508306][-1.9593458 -1.9635901 -2.0795469 -2.2810047 -2.4212093 -2.248636 -1.7748859 -1.2824504 -1.0042512 -0.9194231 -0.83993578 -0.965508 -1.1684923 -1.1831872 -1.1376209][-1.9966052 -2.0813332 -2.2400916 -2.4168367 -2.6587138 -2.6449199 -2.271987 -1.748559 -1.3088856 -1.124784 -1.0546141 -1.2658937 -1.5748117 -1.6695707 -1.6782048][-1.9889629 -2.1211779 -2.2488728 -2.3063478 -2.5547493 -2.6649079 -2.4557066 -2.0470204 -1.5870941 -1.3967583 -1.2949207 -1.4420216 -1.682646 -1.7332189 -1.7729788][-2.3846436 -2.5299 -2.5517452 -2.4495382 -2.6612163 -2.840436 -2.7548776 -2.4661911 -2.0407524 -1.885083 -1.7465246 -1.759721 -1.8165357 -1.69926 -1.7111714][-2.8507056 -3.0243835 -2.9676871 -2.7252336 -2.8299661 -2.94577 -2.8707328 -2.7000413 -2.4523149 -2.4570723 -2.3971825 -2.3593338 -2.2932172 -2.0744426 -2.0702097][-2.9912992 -3.175621 -3.0979378 -2.7909112 -2.7930861 -2.8140659 -2.6952972 -2.6088705 -2.5527327 -2.673811 -2.6963234 -2.7040968 -2.6882243 -2.5520306 -2.6133947]]...]
INFO - root - 2017-12-07 09:13:22.041889: step 23810, loss = 0.71, batch loss = 0.64 (7.6 examples/sec; 1.058 sec/batch; 90h:43m:25s remains)
INFO - root - 2017-12-07 09:13:32.720504: step 23820, loss = 0.98, batch loss = 0.91 (7.6 examples/sec; 1.048 sec/batch; 89h:53m:51s remains)
INFO - root - 2017-12-07 09:13:43.326305: step 23830, loss = 0.78, batch loss = 0.71 (7.7 examples/sec; 1.035 sec/batch; 88h:46m:43s remains)
INFO - root - 2017-12-07 09:13:54.018885: step 23840, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.071 sec/batch; 91h:50m:19s remains)
INFO - root - 2017-12-07 09:14:04.683784: step 23850, loss = 0.66, batch loss = 0.59 (7.5 examples/sec; 1.060 sec/batch; 90h:55m:15s remains)
INFO - root - 2017-12-07 09:14:15.340033: step 23860, loss = 0.82, batch loss = 0.75 (7.3 examples/sec; 1.094 sec/batch; 93h:46m:05s remains)
INFO - root - 2017-12-07 09:14:25.888795: step 23870, loss = 0.96, batch loss = 0.88 (7.5 examples/sec; 1.066 sec/batch; 91h:25m:15s remains)
INFO - root - 2017-12-07 09:14:36.589652: step 23880, loss = 0.69, batch loss = 0.61 (7.4 examples/sec; 1.088 sec/batch; 93h:15m:16s remains)
INFO - root - 2017-12-07 09:14:47.235977: step 23890, loss = 0.88, batch loss = 0.81 (7.4 examples/sec; 1.085 sec/batch; 92h:58m:21s remains)
INFO - root - 2017-12-07 09:14:57.941290: step 23900, loss = 1.02, batch loss = 0.95 (7.6 examples/sec; 1.055 sec/batch; 90h:25m:13s remains)
2017-12-07 09:14:58.762768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6177278 -3.2316508 -3.0674007 -3.4291224 -3.6464956 -3.6771472 -3.8703079 -4.1098132 -4.4880118 -4.8836293 -4.84071 -4.6590643 -4.7723322 -4.8240604 -4.579843][-3.4566593 -3.1236832 -2.9762337 -3.17143 -3.1807585 -3.2414093 -3.5121317 -3.7568989 -4.2813158 -4.850234 -4.805654 -4.54174 -4.6311207 -4.6659946 -4.5356402][-3.3303452 -2.955833 -2.6102128 -2.4777761 -2.2833664 -2.37805 -2.6687632 -2.9556155 -3.6119893 -4.2088981 -4.1122074 -3.8357329 -3.9334505 -3.9246783 -3.8565915][-3.2660861 -2.759017 -2.1370969 -1.6429489 -1.272253 -1.3542249 -1.6542983 -2.057611 -2.8605494 -3.3928061 -3.2436085 -3.0400774 -3.2034996 -3.1353838 -3.0022888][-3.164494 -2.4848366 -1.6811054 -0.99728775 -0.53199744 -0.48495817 -0.66168332 -1.1211469 -1.9666297 -2.4161015 -2.3268909 -2.3955467 -2.7395821 -2.6971312 -2.5207186][-3.0081189 -2.2209179 -1.4601176 -0.82356572 -0.29896402 0.048574448 0.20053482 -0.084157467 -0.73733783 -1.1416888 -1.3138592 -1.781327 -2.3417981 -2.4641283 -2.4005959][-2.9302158 -2.1656973 -1.5902243 -1.0993342 -0.59868431 -0.0044398308 0.54266596 0.62809706 0.18939447 -0.43277502 -1.0588982 -1.8182328 -2.344238 -2.445312 -2.3610096][-2.9633021 -2.2904842 -1.8682628 -1.4679027 -1.054678 -0.44137 0.24015617 0.52654791 0.013073921 -0.98254085 -1.8757439 -2.5719187 -2.7880154 -2.6122475 -2.2958028][-3.0372667 -2.4324188 -2.0448349 -1.6046004 -1.2366457 -0.71465254 -0.1689043 -0.059450626 -0.87539029 -2.0438747 -2.7133412 -3.0934722 -3.0961585 -2.7845454 -2.3830302][-3.1223402 -2.5228879 -2.0751271 -1.5525837 -1.1745729 -0.75565743 -0.39225674 -0.50821805 -1.5334353 -2.6185715 -2.8715072 -3.0084555 -3.0336623 -2.8467584 -2.6117918][-3.2421284 -2.6452086 -2.1394575 -1.5984106 -1.2395589 -0.94684958 -0.71379948 -0.89136338 -1.8827665 -2.7665854 -2.7367325 -2.7730982 -2.8477407 -2.7768462 -2.6971979][-3.3242271 -2.8093817 -2.3883982 -1.9592311 -1.6122167 -1.3283651 -1.1021457 -1.1754324 -2.0156569 -2.7947216 -2.7678819 -2.7755244 -2.758954 -2.6016655 -2.503093][-3.3146272 -2.9084742 -2.6528986 -2.3915193 -2.0020485 -1.5542605 -1.1710699 -1.0915079 -1.7802231 -2.5428929 -2.6887741 -2.710784 -2.5531986 -2.2924533 -2.1960676][-3.274539 -2.9084058 -2.6744628 -2.4390955 -1.9567978 -1.3468084 -0.794683 -0.56349277 -1.0859618 -1.8140783 -2.0632346 -2.0688953 -1.8922377 -1.7414219 -1.78403][-3.2606518 -2.8727891 -2.5162373 -2.1621404 -1.6223559 -1.0354877 -0.51290393 -0.26020813 -0.59961247 -1.1488798 -1.3148603 -1.2308352 -1.0578952 -1.0368514 -1.235091]]...]
INFO - root - 2017-12-07 09:15:09.456563: step 23910, loss = 0.90, batch loss = 0.83 (7.3 examples/sec; 1.094 sec/batch; 93h:46m:34s remains)
INFO - root - 2017-12-07 09:15:20.112454: step 23920, loss = 0.71, batch loss = 0.64 (7.6 examples/sec; 1.047 sec/batch; 89h:45m:37s remains)
INFO - root - 2017-12-07 09:15:30.796125: step 23930, loss = 0.63, batch loss = 0.56 (7.3 examples/sec; 1.089 sec/batch; 93h:22m:43s remains)
INFO - root - 2017-12-07 09:15:41.479631: step 23940, loss = 0.65, batch loss = 0.58 (7.3 examples/sec; 1.094 sec/batch; 93h:44m:55s remains)
INFO - root - 2017-12-07 09:15:52.322466: step 23950, loss = 0.92, batch loss = 0.85 (7.3 examples/sec; 1.093 sec/batch; 93h:38m:30s remains)
INFO - root - 2017-12-07 09:16:03.056209: step 23960, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.062 sec/batch; 91h:02m:00s remains)
INFO - root - 2017-12-07 09:16:13.618101: step 23970, loss = 0.87, batch loss = 0.80 (7.6 examples/sec; 1.059 sec/batch; 90h:45m:31s remains)
INFO - root - 2017-12-07 09:16:24.448414: step 23980, loss = 0.70, batch loss = 0.63 (7.4 examples/sec; 1.077 sec/batch; 92h:20m:24s remains)
INFO - root - 2017-12-07 09:16:35.105750: step 23990, loss = 0.68, batch loss = 0.61 (7.6 examples/sec; 1.054 sec/batch; 90h:17m:30s remains)
INFO - root - 2017-12-07 09:16:45.872902: step 24000, loss = 0.94, batch loss = 0.87 (7.3 examples/sec; 1.102 sec/batch; 94h:28m:39s remains)
2017-12-07 09:16:46.669494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0825512 -2.3839481 -2.3528519 -2.3468313 -2.3330564 -2.4123418 -2.6013288 -2.6269774 -2.6189008 -2.3607743 -2.0006011 -2.5590172 -3.6414425 -3.8103166 -3.1689446][-2.3925729 -2.7756319 -2.7804956 -2.754854 -2.7957034 -2.955519 -3.1701205 -3.1232667 -2.8653555 -2.4000654 -2.0185971 -2.4902697 -3.3949733 -3.4522681 -2.8413615][-2.500453 -2.7735581 -2.7595448 -2.8355761 -3.0769887 -3.3789387 -3.5861559 -3.5431647 -3.2986488 -2.8895133 -2.644377 -3.0056653 -3.5499451 -3.2896357 -2.5606308][-2.222156 -2.2794821 -2.2623672 -2.5220697 -2.9600258 -3.3094139 -3.4194648 -3.3804431 -3.3089643 -3.204881 -3.3250191 -3.7697852 -4.05015 -3.4819307 -2.5873418][-1.8294108 -1.6923952 -1.716722 -2.1118386 -2.5767865 -2.7855983 -2.7092924 -2.571816 -2.605402 -2.8550963 -3.4981399 -4.2812252 -4.5757437 -3.921829 -2.9505787][-1.5756965 -1.3430958 -1.4172943 -1.8358994 -2.1375163 -2.1070302 -1.8784437 -1.6220889 -1.6012332 -2.0218801 -2.9592009 -4.0151191 -4.5297408 -4.0905657 -3.2780271][-1.6329784 -1.3799021 -1.4398303 -1.7314959 -1.8210726 -1.6628997 -1.5335255 -1.360992 -1.2288222 -1.4751453 -2.1902082 -3.1490498 -3.8513219 -3.7501049 -3.2212112][-2.0496006 -1.7190251 -1.6399059 -1.7102635 -1.6403661 -1.5147452 -1.6948605 -1.8160758 -1.701699 -1.7264826 -2.0109413 -2.65113 -3.3086097 -3.26859 -2.8173301][-2.8176143 -2.4753184 -2.2376828 -2.0709417 -1.8421819 -1.7257493 -2.1062014 -2.4866104 -2.5127437 -2.4657354 -2.4987254 -2.8871794 -3.3097367 -2.9929593 -2.3488429][-3.6292944 -3.414732 -3.143024 -2.8569884 -2.501564 -2.3296824 -2.6898246 -3.1126714 -3.235533 -3.2271657 -3.22271 -3.4895382 -3.6763663 -3.0705218 -2.2092392][-3.7781374 -3.7328625 -3.5851755 -3.3963115 -3.0813117 -2.8747542 -3.0637631 -3.3293018 -3.4043489 -3.378726 -3.4158297 -3.7395391 -3.9461241 -3.35182 -2.4908023][-3.3492579 -3.4225478 -3.4191585 -3.3854299 -3.2055418 -3.0270348 -3.0650868 -3.1451328 -3.0684524 -2.8774121 -2.8827698 -3.33993 -3.7994781 -3.5123968 -2.856936][-2.4627862 -2.5481691 -2.6131208 -2.6669476 -2.6218886 -2.5432911 -2.5586772 -2.5749712 -2.3958173 -2.0370686 -1.9555299 -2.5400395 -3.3149691 -3.4241314 -3.0509946][-1.9059041 -1.9167547 -1.9441259 -1.9643357 -1.9410641 -1.9218504 -1.9426732 -1.9448044 -1.7359049 -1.3083088 -1.1864836 -1.8614795 -2.8457487 -3.1946497 -3.0024428][-2.3513567 -2.2857814 -2.2483952 -2.2119691 -2.1645305 -2.14457 -2.124541 -2.0728414 -1.8548534 -1.4565723 -1.357599 -2.062474 -3.0094881 -3.2352104 -2.9169064]]...]
INFO - root - 2017-12-07 09:16:57.544455: step 24010, loss = 0.74, batch loss = 0.67 (7.3 examples/sec; 1.093 sec/batch; 93h:37m:35s remains)
INFO - root - 2017-12-07 09:17:08.172200: step 24020, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.060 sec/batch; 90h:49m:34s remains)
INFO - root - 2017-12-07 09:17:18.862055: step 24030, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.070 sec/batch; 91h:38m:58s remains)
INFO - root - 2017-12-07 09:17:29.621213: step 24040, loss = 0.89, batch loss = 0.81 (7.2 examples/sec; 1.107 sec/batch; 94h:49m:37s remains)
INFO - root - 2017-12-07 09:17:40.333844: step 24050, loss = 0.64, batch loss = 0.56 (7.6 examples/sec; 1.049 sec/batch; 89h:51m:05s remains)
INFO - root - 2017-12-07 09:17:51.080194: step 24060, loss = 0.94, batch loss = 0.87 (7.5 examples/sec; 1.072 sec/batch; 91h:53m:00s remains)
INFO - root - 2017-12-07 09:18:01.620939: step 24070, loss = 0.59, batch loss = 0.51 (7.5 examples/sec; 1.064 sec/batch; 91h:08m:36s remains)
INFO - root - 2017-12-07 09:18:12.383058: step 24080, loss = 0.73, batch loss = 0.66 (7.3 examples/sec; 1.100 sec/batch; 94h:12m:04s remains)
INFO - root - 2017-12-07 09:18:23.105242: step 24090, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.060 sec/batch; 90h:48m:27s remains)
INFO - root - 2017-12-07 09:18:33.825550: step 24100, loss = 0.89, batch loss = 0.82 (7.5 examples/sec; 1.065 sec/batch; 91h:12m:28s remains)
2017-12-07 09:18:34.588569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.84923077 -0.72792649 -1.0169744 -1.4106896 -1.4938948 -1.114253 -1.1881216 -1.5957038 -1.9482493 -2.1155691 -1.6251135 -0.91308856 -1.1461701 -1.9842572 -2.2669659][-0.8433342 -0.92702413 -1.2552371 -1.6193328 -1.5297883 -0.9848876 -1.0587211 -1.4757893 -1.8268259 -2.039922 -1.4002879 -0.44465995 -0.70224929 -1.7771931 -2.1368642][-1.0879054 -1.3776188 -1.5760539 -1.7805905 -1.5438669 -0.95023274 -0.96504664 -1.166193 -1.2817931 -1.5102789 -0.94615245 0.031817436 -0.22978973 -1.3412519 -1.5646229][-1.543992 -2.0369458 -1.9565902 -1.8235676 -1.3391805 -0.71001816 -0.63741279 -0.53900075 -0.48207378 -0.98096943 -0.76573181 0.094790936 -0.11123276 -0.97141123 -0.82550788][-2.0549548 -2.6896665 -2.2532096 -1.7147942 -0.94648075 -0.26403904 -0.082086563 0.33854866 0.513989 -0.39292192 -0.62145519 -0.022422791 -0.30537033 -0.9713316 -0.51850009][-2.3350568 -3.0458741 -2.3555346 -1.5811734 -0.71149778 -0.066358089 0.20550776 0.97559166 1.3113875 0.089254856 -0.59733462 -0.38741016 -0.76655817 -1.2024593 -0.45547175][-2.3725765 -3.2147913 -2.4728801 -1.6458035 -0.74867344 -0.047141552 0.41152334 1.5079756 1.9164324 0.3751812 -0.8197093 -1.0396857 -1.4463665 -1.6115716 -0.57983565][-2.4046454 -3.3013034 -2.6421616 -1.9282489 -1.0344596 -0.20391226 0.47184944 1.7747493 2.1814938 0.42630768 -1.1383564 -1.6823802 -2.1641288 -2.218086 -1.0583217][-2.5861981 -3.39891 -2.9149103 -2.3714545 -1.4726927 -0.57624769 0.16797638 1.4720058 1.8202929 0.11618328 -1.4475203 -2.0447783 -2.5880094 -2.68636 -1.5864255][-2.9546099 -3.672873 -3.3554192 -2.9082081 -1.9568365 -1.0591342 -0.3301115 0.83806229 1.0616679 -0.41269064 -1.7218609 -2.2084358 -2.7708666 -3.0109775 -2.1634634][-3.4523089 -4.0306349 -3.8165636 -3.3899329 -2.4569981 -1.6547558 -0.98563004 -0.0077285767 0.10760117 -1.0507107 -2.0526741 -2.4903042 -3.0578067 -3.3711114 -2.7705941][-3.7734275 -4.161212 -4.0553846 -3.7117841 -2.9338899 -2.3392949 -1.792105 -1.0277922 -0.96557021 -1.7229424 -2.3713951 -2.7692394 -3.2872646 -3.543885 -3.1114595][-3.7779303 -3.9762516 -3.9676423 -3.7525911 -3.155611 -2.7664492 -2.4215543 -1.9775066 -2.0403576 -2.4708722 -2.7604263 -3.0206094 -3.3774886 -3.5214891 -3.2476845][-3.6309903 -3.6763718 -3.7112226 -3.6002512 -3.1817603 -2.9307055 -2.7497592 -2.5747712 -2.7549281 -3.0093617 -3.0937562 -3.2157228 -3.3926086 -3.4541695 -3.3007669][-3.4077568 -3.3563914 -3.3712411 -3.325408 -3.125391 -3.0148361 -2.9354086 -2.8821726 -3.0330205 -3.141695 -3.1495447 -3.1862342 -3.2400887 -3.2571511 -3.1865709]]...]
INFO - root - 2017-12-07 09:18:45.352065: step 24110, loss = 0.89, batch loss = 0.82 (7.6 examples/sec; 1.058 sec/batch; 90h:36m:59s remains)
INFO - root - 2017-12-07 09:18:56.130612: step 24120, loss = 0.65, batch loss = 0.58 (7.4 examples/sec; 1.078 sec/batch; 92h:20m:18s remains)
INFO - root - 2017-12-07 09:19:06.827909: step 24130, loss = 0.84, batch loss = 0.76 (7.4 examples/sec; 1.082 sec/batch; 92h:43m:13s remains)
INFO - root - 2017-12-07 09:19:17.639371: step 24140, loss = 0.79, batch loss = 0.71 (7.4 examples/sec; 1.080 sec/batch; 92h:28m:04s remains)
INFO - root - 2017-12-07 09:19:28.402086: step 24150, loss = 0.83, batch loss = 0.76 (7.3 examples/sec; 1.090 sec/batch; 93h:21m:30s remains)
INFO - root - 2017-12-07 09:19:39.152591: step 24160, loss = 0.89, batch loss = 0.82 (7.6 examples/sec; 1.049 sec/batch; 89h:50m:39s remains)
INFO - root - 2017-12-07 09:19:49.836247: step 24170, loss = 0.68, batch loss = 0.61 (7.3 examples/sec; 1.089 sec/batch; 93h:14m:58s remains)
INFO - root - 2017-12-07 09:20:00.525608: step 24180, loss = 0.66, batch loss = 0.59 (7.6 examples/sec; 1.056 sec/batch; 90h:28m:04s remains)
INFO - root - 2017-12-07 09:20:11.294892: step 24190, loss = 0.72, batch loss = 0.65 (7.3 examples/sec; 1.097 sec/batch; 93h:58m:43s remains)
INFO - root - 2017-12-07 09:20:21.866045: step 24200, loss = 0.86, batch loss = 0.79 (7.5 examples/sec; 1.073 sec/batch; 91h:51m:52s remains)
2017-12-07 09:20:22.674701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7909238 -2.6056824 -2.3319583 -2.6327772 -3.1407094 -3.2705414 -3.0255103 -2.9181383 -3.0456862 -3.1214757 -3.0023341 -2.8481078 -2.7943296 -2.7436407 -2.7325377][-1.8512354 -1.429966 -1.0778663 -1.6119416 -2.2526622 -2.2915206 -2.0090916 -2.039259 -2.3877869 -2.6082597 -2.4760435 -2.1766052 -2.009917 -1.9183021 -1.8431413][-1.3798542 -0.803277 -0.49263 -1.1632888 -1.6693535 -1.4025509 -1.084492 -1.2757556 -1.7835591 -2.0830517 -1.8896623 -1.5293286 -1.5149627 -1.6905231 -1.7183814][-1.3843107 -0.72918916 -0.5702877 -1.2094719 -1.4140315 -0.95774484 -0.73154593 -1.0240529 -1.5962107 -2.0669138 -1.9881458 -1.7464662 -1.9360116 -2.1328771 -1.9049678][-1.1327908 -0.48624825 -0.59864092 -1.269489 -1.3375275 -0.89949512 -0.72923517 -0.84526777 -1.2923675 -1.8304324 -1.7846193 -1.6034265 -1.8082771 -1.7801056 -1.2631338][-0.83738661 -0.2497139 -0.62293959 -1.3306336 -1.3580124 -0.88289523 -0.4586544 -0.13983107 -0.49338794 -1.1652639 -1.1389 -0.83885121 -0.83983994 -0.57047462 -0.11480713][-0.6355691 -0.011750698 -0.32239485 -0.76509905 -0.57381654 0.037308693 0.78562021 1.2020903 0.51381588 -0.43928266 -0.37257051 0.15010929 0.39187098 0.71572876 0.80857611][-0.43682742 0.22176409 0.17902517 0.14600372 0.56956291 1.3051653 2.2518005 2.5211864 1.3905506 0.18803883 0.0753541 0.386662 0.43606281 0.52228022 0.26129627][-0.61803341 -0.0070633888 0.18141985 0.41305351 0.81504774 1.3616514 2.1034198 2.1351805 1.0666318 0.086153984 -0.12788296 -0.084657669 -0.2823863 -0.39521933 -0.73890352][-1.0141571 -0.45817995 -0.14741802 0.11333561 0.29460287 0.42399836 0.6223731 0.45298243 -0.11655283 -0.39216137 -0.28687716 -0.16740942 -0.3814497 -0.60099244 -0.95163655][-1.3692927 -0.94823647 -0.64713788 -0.42096853 -0.34934092 -0.42133617 -0.47270274 -0.62801957 -0.82459545 -0.80485654 -0.697366 -0.55583858 -0.60754061 -0.74664569 -1.0191422][-1.7388647 -1.6604173 -1.5456512 -1.3351412 -1.2207401 -1.267411 -1.2836285 -1.2925706 -1.3644185 -1.4265413 -1.5465782 -1.4226902 -1.2023015 -1.0977209 -1.2698412][-2.2495265 -2.4862323 -2.5422876 -2.305208 -2.1122079 -2.0742254 -1.9919398 -1.899821 -1.971175 -2.1368651 -2.3153775 -2.1119516 -1.7526805 -1.5947888 -1.8184838][-2.7744815 -2.9463773 -2.853574 -2.4961195 -2.2498343 -2.2299242 -2.1920402 -2.1605716 -2.2875004 -2.476701 -2.5854516 -2.3202376 -1.9636149 -1.904748 -2.2054925][-2.8700535 -2.7970588 -2.5233142 -2.1774273 -2.0446103 -2.1540251 -2.2416332 -2.3110261 -2.4170384 -2.4793565 -2.4408383 -2.2018449 -1.9936564 -2.0796862 -2.4190269]]...]
INFO - root - 2017-12-07 09:20:33.485095: step 24210, loss = 0.76, batch loss = 0.69 (7.4 examples/sec; 1.080 sec/batch; 92h:28m:28s remains)
INFO - root - 2017-12-07 09:20:44.378039: step 24220, loss = 0.86, batch loss = 0.79 (7.1 examples/sec; 1.134 sec/batch; 97h:05m:51s remains)
INFO - root - 2017-12-07 09:20:55.137243: step 24230, loss = 0.80, batch loss = 0.73 (7.5 examples/sec; 1.065 sec/batch; 91h:09m:56s remains)
INFO - root - 2017-12-07 09:21:05.830594: step 24240, loss = 0.95, batch loss = 0.88 (7.8 examples/sec; 1.030 sec/batch; 88h:12m:55s remains)
INFO - root - 2017-12-07 09:21:16.593398: step 24250, loss = 0.71, batch loss = 0.64 (7.3 examples/sec; 1.092 sec/batch; 93h:28m:06s remains)
INFO - root - 2017-12-07 09:21:27.355443: step 24260, loss = 0.89, batch loss = 0.82 (7.6 examples/sec; 1.054 sec/batch; 90h:13m:38s remains)
INFO - root - 2017-12-07 09:21:37.902128: step 24270, loss = 0.85, batch loss = 0.78 (7.5 examples/sec; 1.061 sec/batch; 90h:52m:49s remains)
INFO - root - 2017-12-07 09:21:48.660712: step 24280, loss = 0.80, batch loss = 0.73 (7.2 examples/sec; 1.115 sec/batch; 95h:26m:13s remains)
INFO - root - 2017-12-07 09:21:59.504957: step 24290, loss = 0.79, batch loss = 0.71 (7.4 examples/sec; 1.088 sec/batch; 93h:07m:18s remains)
INFO - root - 2017-12-07 09:22:10.121041: step 24300, loss = 0.80, batch loss = 0.73 (7.3 examples/sec; 1.095 sec/batch; 93h:44m:15s remains)
2017-12-07 09:22:10.906555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2762697 -2.2104213 -2.213367 -1.9343371 -1.2272539 -0.48613548 -0.20662451 -0.505049 -1.355283 -2.2904744 -2.8140662 -2.78963 -2.293339 -1.8166254 -1.7892981][-2.1999905 -2.0398805 -2.12326 -2.0820417 -1.7051272 -1.2494988 -1.0126379 -1.118762 -1.6458635 -2.3144226 -2.7631183 -2.8968959 -2.5992422 -2.1422298 -1.9277353][-2.1574218 -1.9746919 -2.1538236 -2.312654 -2.2899797 -2.2634974 -2.2659822 -2.2744861 -2.3467987 -2.494988 -2.6302044 -2.8349552 -2.8752046 -2.6842084 -2.4265575][-1.9560447 -1.8132157 -2.0759392 -2.3240464 -2.4552894 -2.6512623 -2.8103085 -2.7773743 -2.5349643 -2.2109613 -1.9359474 -2.0246217 -2.3826742 -2.7478111 -2.9092667][-1.5749953 -1.3834474 -1.6105778 -1.8284082 -1.9517958 -2.19854 -2.5084662 -2.6026444 -2.350991 -1.8241389 -1.2569251 -1.107419 -1.4356122 -1.9521039 -2.3743579][-1.2839079 -0.95531845 -0.92880487 -0.89800525 -0.88857508 -1.166039 -1.74668 -2.2027762 -2.229512 -1.774169 -1.0913224 -0.69718409 -0.75790095 -0.98938584 -1.2257822][-1.4213169 -1.0318654 -0.82369828 -0.59695673 -0.37426233 -0.45571852 -1.0203011 -1.6661582 -1.9840798 -1.8241997 -1.3101263 -0.799562 -0.49440002 -0.27434206 -0.17538548][-1.8463898 -1.5233538 -1.3090334 -0.99425173 -0.55082607 -0.3144846 -0.55945516 -0.97579122 -1.2245529 -1.1893044 -0.92300916 -0.59766078 -0.3267355 0.0014734268 0.30853891][-2.0557523 -1.7586219 -1.6386821 -1.3643446 -0.782845 -0.23927355 -0.088913918 -0.22924852 -0.40864563 -0.49579382 -0.48261452 -0.41854 -0.41770458 -0.36737919 -0.24840164][-1.9344161 -1.682781 -1.740423 -1.6794455 -1.2607806 -0.73665404 -0.3882618 -0.31814671 -0.41340733 -0.59775519 -0.8271873 -0.98907614 -1.1538732 -1.2488561 -1.268887][-1.8881214 -1.7368627 -2.0071218 -2.1992598 -2.0974212 -1.7849855 -1.4569969 -1.3392379 -1.3626838 -1.4482877 -1.5520403 -1.6341231 -1.7448182 -1.7984505 -1.7943795][-2.0076077 -1.8483474 -2.1712747 -2.5207672 -2.6118362 -2.369324 -1.9680812 -1.8003545 -1.8494408 -1.9423316 -1.9461579 -1.9194765 -1.9338758 -1.9475884 -1.939857][-2.2687516 -2.0189703 -2.1887763 -2.5036764 -2.6119595 -2.3697631 -1.9048045 -1.6823452 -1.7580054 -1.9260015 -2.014683 -2.0780609 -2.1753633 -2.2648296 -2.3252361][-2.6391912 -2.3996463 -2.5026767 -2.7903953 -2.9166632 -2.6955056 -2.2565248 -2.0218234 -2.093348 -2.2845066 -2.4243875 -2.5521772 -2.7024069 -2.834604 -2.921988][-2.8660948 -2.7644472 -2.9553638 -3.2819288 -3.4557676 -3.3308222 -3.0247505 -2.8334703 -2.8540764 -2.9503078 -3.0056748 -3.0609064 -3.1242819 -3.1626105 -3.1772823]]...]
INFO - root - 2017-12-07 09:22:21.503877: step 24310, loss = 0.90, batch loss = 0.82 (7.4 examples/sec; 1.083 sec/batch; 92h:41m:57s remains)
INFO - root - 2017-12-07 09:22:32.156217: step 24320, loss = 0.89, batch loss = 0.82 (7.8 examples/sec; 1.030 sec/batch; 88h:09m:03s remains)
INFO - root - 2017-12-07 09:22:42.847503: step 24330, loss = 0.82, batch loss = 0.75 (7.5 examples/sec; 1.061 sec/batch; 90h:46m:58s remains)
INFO - root - 2017-12-07 09:22:53.421091: step 24340, loss = 0.88, batch loss = 0.81 (7.4 examples/sec; 1.085 sec/batch; 92h:54m:06s remains)
INFO - root - 2017-12-07 09:23:03.965730: step 24350, loss = 0.71, batch loss = 0.64 (7.7 examples/sec; 1.043 sec/batch; 89h:17m:55s remains)
INFO - root - 2017-12-07 09:23:14.555385: step 24360, loss = 0.80, batch loss = 0.73 (7.7 examples/sec; 1.034 sec/batch; 88h:28m:53s remains)
INFO - root - 2017-12-07 09:23:25.028267: step 24370, loss = 0.71, batch loss = 0.64 (7.4 examples/sec; 1.078 sec/batch; 92h:18m:11s remains)
INFO - root - 2017-12-07 09:23:35.753380: step 24380, loss = 0.87, batch loss = 0.79 (7.5 examples/sec; 1.072 sec/batch; 91h:46m:28s remains)
INFO - root - 2017-12-07 09:23:46.539770: step 24390, loss = 0.92, batch loss = 0.85 (7.5 examples/sec; 1.068 sec/batch; 91h:23m:50s remains)
INFO - root - 2017-12-07 09:23:57.318745: step 24400, loss = 0.85, batch loss = 0.78 (7.3 examples/sec; 1.091 sec/batch; 93h:23m:20s remains)
2017-12-07 09:23:58.126758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.722877 -2.580925 -2.6667905 -3.0426056 -3.1733875 -3.1239429 -2.8671064 -2.4991064 -2.6089077 -2.8404803 -2.9142494 -2.9175496 -2.8107152 -2.629662 -2.551199][-2.1197925 -1.9522204 -2.0935652 -2.6270099 -2.8782213 -2.8923149 -2.7175546 -2.3790178 -2.5485601 -2.8340988 -2.8925753 -2.8504944 -2.6394799 -2.2696424 -1.9726756][-2.086139 -1.9335794 -2.1831019 -2.874655 -3.2487793 -3.3775816 -3.3495166 -3.0899239 -3.3020091 -3.627569 -3.6566486 -3.5453084 -3.199213 -2.6395922 -2.1445889][-2.3538435 -2.1099052 -2.325098 -2.9373693 -3.2551048 -3.3869133 -3.3941922 -3.1157532 -3.33253 -3.7664881 -3.9157624 -3.8564997 -3.5214038 -2.9919758 -2.5532475][-2.6280398 -2.2922421 -2.3637459 -2.6916647 -2.7734413 -2.7695932 -2.6847408 -2.3166957 -2.5443425 -3.1426816 -3.4652386 -3.5387347 -3.34878 -2.9634454 -2.6454463][-2.4373777 -2.1104369 -2.1535902 -2.34479 -2.3029184 -2.1382861 -1.799051 -1.1971631 -1.4275544 -2.1816258 -2.5956185 -2.7621379 -2.7260561 -2.4595752 -2.2221413][-2.3969777 -2.0396931 -1.9497006 -1.9917092 -1.8698769 -1.6082711 -1.0195775 -0.21403456 -0.46598291 -1.2674031 -1.6625755 -1.919544 -2.064642 -1.9249828 -1.7821765][-2.7851915 -2.5819283 -2.5305219 -2.5543346 -2.4474916 -2.273726 -1.7969642 -1.2351387 -1.5730524 -2.184804 -2.3325071 -2.4180458 -2.412173 -2.1100013 -1.8407898][-2.9447565 -2.8720808 -2.9968905 -3.1900368 -3.2071757 -3.1292572 -2.7704206 -2.4410148 -2.8409252 -3.3208408 -3.3791261 -3.3301768 -3.09559 -2.5585492 -2.060513][-2.7638855 -2.7890005 -3.0618865 -3.4064279 -3.5600965 -3.5779071 -3.278136 -3.013834 -3.3451171 -3.6957698 -3.7642906 -3.6740072 -3.3183928 -2.6963565 -2.0977733][-2.3612494 -2.5110059 -2.8816013 -3.2703047 -3.4961243 -3.607018 -3.4727182 -3.3449821 -3.6632166 -3.9304526 -3.9621739 -3.7553732 -3.2463236 -2.5753617 -1.9400787][-2.2602355 -2.3310204 -2.582593 -2.8808956 -3.1110783 -3.258122 -3.2278781 -3.1928387 -3.5030425 -3.7416103 -3.7803733 -3.5743687 -3.0943036 -2.55739 -2.0904069][-2.8109846 -2.7885084 -2.8911436 -3.0442786 -3.1990061 -3.2873549 -3.2395296 -3.1871762 -3.4148407 -3.5952835 -3.612433 -3.4697542 -3.1666164 -2.8771663 -2.6480591][-3.4613671 -3.444021 -3.5042841 -3.5587335 -3.5945845 -3.5759149 -3.4787464 -3.4180677 -3.5729666 -3.715344 -3.7503307 -3.6995947 -3.5585129 -3.4316273 -3.3112347][-3.8984811 -3.8698249 -3.88677 -3.8922665 -3.8656058 -3.784924 -3.6502759 -3.5623207 -3.6399388 -3.7445102 -3.8128061 -3.8562455 -3.8522124 -3.8506272 -3.7946653]]...]
INFO - root - 2017-12-07 09:24:08.974936: step 24410, loss = 0.60, batch loss = 0.53 (7.0 examples/sec; 1.145 sec/batch; 98h:00m:07s remains)
INFO - root - 2017-12-07 09:24:19.610319: step 24420, loss = 0.92, batch loss = 0.84 (7.6 examples/sec; 1.049 sec/batch; 89h:46m:21s remains)
INFO - root - 2017-12-07 09:24:30.357706: step 24430, loss = 0.96, batch loss = 0.88 (7.6 examples/sec; 1.048 sec/batch; 89h:43m:21s remains)
INFO - root - 2017-12-07 09:24:41.082835: step 24440, loss = 0.89, batch loss = 0.82 (7.4 examples/sec; 1.086 sec/batch; 92h:56m:25s remains)
INFO - root - 2017-12-07 09:24:51.818649: step 24450, loss = 0.87, batch loss = 0.80 (7.5 examples/sec; 1.067 sec/batch; 91h:16m:47s remains)
INFO - root - 2017-12-07 09:25:02.591024: step 24460, loss = 0.80, batch loss = 0.73 (7.7 examples/sec; 1.043 sec/batch; 89h:17m:15s remains)
INFO - root - 2017-12-07 09:25:13.099899: step 24470, loss = 0.83, batch loss = 0.75 (7.3 examples/sec; 1.093 sec/batch; 93h:31m:13s remains)
INFO - root - 2017-12-07 09:25:23.752652: step 24480, loss = 0.78, batch loss = 0.70 (7.5 examples/sec; 1.061 sec/batch; 90h:45m:30s remains)
INFO - root - 2017-12-07 09:25:34.462523: step 24490, loss = 0.87, batch loss = 0.80 (7.6 examples/sec; 1.057 sec/batch; 90h:28m:15s remains)
INFO - root - 2017-12-07 09:25:45.200866: step 24500, loss = 1.03, batch loss = 0.96 (7.5 examples/sec; 1.061 sec/batch; 90h:46m:26s remains)
2017-12-07 09:25:46.013537: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.199472 -1.9345133 -1.6630213 -1.4971788 -1.381743 -1.3389077 -1.3235238 -1.3115344 -1.3571496 -1.428509 -1.5887487 -1.7079332 -1.6144435 -1.5180316 -1.7382705][-2.0954051 -1.7244773 -1.3824954 -1.1550808 -0.92735338 -0.74628186 -0.61230063 -0.50476384 -0.48366284 -0.52221131 -0.69407392 -0.84024763 -0.78541517 -0.72718191 -0.92134][-2.1012444 -1.7483001 -1.5390692 -1.5270631 -1.5361876 -1.6039984 -1.7508094 -1.8683586 -1.9766419 -2.0532262 -2.2201447 -2.329464 -2.2304363 -2.1132276 -2.1251755][-2.3698111 -2.0972629 -2.0618782 -2.3023934 -2.6115856 -2.9489236 -3.3110261 -3.5445 -3.6707525 -3.7054152 -3.8411303 -3.95387 -3.9026213 -3.7837358 -3.6205444][-2.8291657 -2.6005774 -2.7144995 -3.1636081 -3.6811681 -4.1041541 -4.4069963 -4.5363045 -4.507288 -4.3208346 -4.3135414 -4.5328379 -4.7859893 -4.9406528 -4.83872][-2.7986078 -2.4170759 -2.3956649 -2.7859058 -3.3280859 -3.7240002 -3.9275978 -4.0438409 -4.0505428 -3.7993577 -3.6891584 -4.0773959 -4.7741933 -5.4164286 -5.5971842][-2.6146917 -1.9197361 -1.5059197 -1.5969934 -1.9474661 -2.1231332 -2.0972543 -2.1587768 -2.2696176 -2.096941 -2.0135119 -2.616684 -3.7221589 -4.8165231 -5.3367691][-2.5233085 -1.4847453 -0.65151381 -0.41830921 -0.60472918 -0.72133231 -0.74447966 -0.94453049 -1.1325655 -0.86792445 -0.6201458 -1.2974417 -2.6593595 -4.0299368 -4.8106718][-2.7647274 -1.5411108 -0.40730143 0.10788059 0.074901104 -0.11123514 -0.42126131 -1.0152161 -1.3923516 -1.0515041 -0.49604583 -0.95957303 -2.2493606 -3.621206 -4.4850168][-3.2365384 -2.1742342 -1.1376173 -0.67486572 -0.72359872 -0.9532268 -1.3938689 -2.1315842 -2.5297539 -2.0345998 -1.1169672 -1.2273781 -2.2959242 -3.6011465 -4.5099864][-3.8900261 -3.2261131 -2.548748 -2.341625 -2.5436106 -2.85191 -3.2505836 -3.731745 -3.7717154 -2.9111576 -1.5976336 -1.3448856 -2.2631898 -3.62858 -4.6886926][-4.2812762 -3.9062986 -3.4552684 -3.3286028 -3.5454361 -3.89548 -4.2813959 -4.5933466 -4.4301796 -3.4504426 -2.0269582 -1.5481074 -2.3018219 -3.5838966 -4.6108384][-4.4280667 -4.3047533 -4.0600462 -3.9785755 -4.1337652 -4.3888359 -4.6416373 -4.7696939 -4.4691067 -3.5287781 -2.192322 -1.5693507 -2.0880728 -3.2208273 -4.2030363][-4.298274 -4.3317671 -4.2518992 -4.248219 -4.4008222 -4.6120639 -4.7750993 -4.7839737 -4.4437413 -3.6790533 -2.5981646 -1.9269249 -2.1346519 -2.9903398 -3.8766985][-3.9846079 -4.0114651 -3.9885142 -4.0323806 -4.1807771 -4.3647728 -4.5170074 -4.5513892 -4.3522077 -3.935607 -3.3260279 -2.846467 -2.8404486 -3.3591099 -3.9850237]]...]
INFO - root - 2017-12-07 09:25:56.909418: step 24510, loss = 0.82, batch loss = 0.75 (7.3 examples/sec; 1.091 sec/batch; 93h:19m:24s remains)
INFO - root - 2017-12-07 09:26:07.572978: step 24520, loss = 0.81, batch loss = 0.74 (7.4 examples/sec; 1.081 sec/batch; 92h:29m:28s remains)
INFO - root - 2017-12-07 09:26:18.293577: step 24530, loss = 0.92, batch loss = 0.84 (7.3 examples/sec; 1.090 sec/batch; 93h:15m:14s remains)
INFO - root - 2017-12-07 09:26:28.944522: step 24540, loss = 0.63, batch loss = 0.56 (7.5 examples/sec; 1.072 sec/batch; 91h:43m:32s remains)
INFO - root - 2017-12-07 09:26:39.715962: step 24550, loss = 0.94, batch loss = 0.87 (7.4 examples/sec; 1.086 sec/batch; 92h:56m:13s remains)
INFO - root - 2017-12-07 09:26:50.284766: step 24560, loss = 0.67, batch loss = 0.59 (7.5 examples/sec; 1.065 sec/batch; 91h:06m:10s remains)
INFO - root - 2017-12-07 09:27:00.850509: step 24570, loss = 0.84, batch loss = 0.77 (7.5 examples/sec; 1.065 sec/batch; 91h:05m:39s remains)
INFO - root - 2017-12-07 09:27:11.583208: step 24580, loss = 0.64, batch loss = 0.57 (7.1 examples/sec; 1.131 sec/batch; 96h:44m:17s remains)
INFO - root - 2017-12-07 09:27:22.273084: step 24590, loss = 0.99, batch loss = 0.92 (7.6 examples/sec; 1.059 sec/batch; 90h:32m:55s remains)
INFO - root - 2017-12-07 09:27:33.059270: step 24600, loss = 0.92, batch loss = 0.85 (7.5 examples/sec; 1.062 sec/batch; 90h:50m:52s remains)
2017-12-07 09:27:33.862802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2589872 -3.316401 -3.4699976 -3.566365 -3.5628502 -3.6504028 -3.8556345 -4.0656404 -4.2311616 -4.3333397 -4.3642178 -4.2444968 -3.9383221 -3.6131907 -3.4402761][-3.2827461 -3.3560452 -3.4885483 -3.416554 -3.1794932 -3.2171631 -3.5126553 -3.8008785 -4.0762486 -4.4048672 -4.6713347 -4.6578269 -4.3165112 -3.8621857 -3.5120807][-3.3030124 -3.3424501 -3.3284571 -2.9352322 -2.4310889 -2.4938219 -2.8998408 -3.1637635 -3.4043055 -3.8735788 -4.3389807 -4.5237665 -4.3889737 -4.0405569 -3.6362739][-3.3029559 -3.243891 -2.9598098 -2.1858609 -1.5322692 -1.8053799 -2.3974507 -2.5949035 -2.7132897 -3.1410663 -3.5864518 -3.8237634 -3.8849368 -3.7526066 -3.4490747][-3.3090262 -3.132144 -2.564642 -1.5184574 -0.93274689 -1.505403 -2.1544704 -2.1411996 -2.0541716 -2.3760281 -2.7158246 -2.9069102 -3.0098226 -3.0270967 -2.930429][-3.3113959 -3.0308166 -2.2857449 -1.2039049 -0.83752155 -1.4983025 -1.8644273 -1.4161212 -1.1084218 -1.4471271 -1.8428745 -2.0214703 -2.0986726 -2.2235017 -2.3537116][-3.2842569 -2.9069922 -2.112577 -1.2111278 -1.0921233 -1.6034646 -1.4922469 -0.573658 -0.1155262 -0.57360506 -1.1308947 -1.2985284 -1.344882 -1.6080675 -1.958427][-3.2251611 -2.7740345 -2.0017221 -1.3422377 -1.3823817 -1.6826978 -1.2565944 -0.22402287 0.17010164 -0.36983109 -0.91236687 -0.88356304 -0.80563307 -1.1760941 -1.6468811][-3.1573348 -2.6966977 -2.013057 -1.5907695 -1.6978073 -1.8526952 -1.4445419 -0.6906867 -0.50512528 -0.9673717 -1.2324238 -0.88682818 -0.67618585 -1.0453706 -1.4408278][-3.1182737 -2.6983786 -2.1599226 -1.9270587 -2.022217 -2.0882294 -1.8660076 -1.4922051 -1.4640136 -1.7754445 -1.7430291 -1.2131214 -0.96490788 -1.2729378 -1.4821429][-3.1176805 -2.7386599 -2.3005991 -2.1020477 -2.0754027 -2.0801039 -2.0306983 -1.9040103 -1.9297161 -2.147222 -2.023967 -1.5627348 -1.4328108 -1.6555707 -1.6859553][-3.1758959 -2.8359277 -2.4204912 -2.112782 -1.9198565 -1.9345686 -2.0500467 -2.0566766 -2.1014168 -2.26522 -2.1473196 -1.8072731 -1.7656081 -1.8752022 -1.7549849][-3.2657664 -2.9786839 -2.5376141 -2.0674634 -1.7425265 -1.8186903 -2.0477138 -2.1140921 -2.1972826 -2.3589895 -2.28244 -2.0274241 -2.0143571 -2.0328336 -1.8318939][-3.2993808 -3.0571885 -2.5949707 -2.0353599 -1.6953948 -1.8339767 -2.0855205 -2.131063 -2.2427869 -2.4446392 -2.4281685 -2.2508078 -2.2501824 -2.2530465 -2.0711691][-3.2653263 -3.0367675 -2.5869362 -2.0592737 -1.8105965 -1.977953 -2.1580195 -2.1172516 -2.1941664 -2.4115052 -2.4329853 -2.3345191 -2.3730834 -2.429481 -2.3526118]]...]
INFO - root - 2017-12-07 09:27:44.468836: step 24610, loss = 0.72, batch loss = 0.64 (7.5 examples/sec; 1.067 sec/batch; 91h:12m:49s remains)
INFO - root - 2017-12-07 09:27:55.316821: step 24620, loss = 0.84, batch loss = 0.76 (7.4 examples/sec; 1.075 sec/batch; 91h:58m:01s remains)
INFO - root - 2017-12-07 09:28:06.039736: step 24630, loss = 0.64, batch loss = 0.57 (7.4 examples/sec; 1.075 sec/batch; 91h:56m:14s remains)
INFO - root - 2017-12-07 09:28:16.717224: step 24640, loss = 0.82, batch loss = 0.75 (7.6 examples/sec; 1.049 sec/batch; 89h:41m:48s remains)
INFO - root - 2017-12-07 09:28:27.358986: step 24650, loss = 0.95, batch loss = 0.87 (7.7 examples/sec; 1.037 sec/batch; 88h:38m:21s remains)
INFO - root - 2017-12-07 09:28:37.970481: step 24660, loss = 0.75, batch loss = 0.68 (7.6 examples/sec; 1.053 sec/batch; 90h:04m:22s remains)
INFO - root - 2017-12-07 09:28:48.526651: step 24670, loss = 0.73, batch loss = 0.66 (7.3 examples/sec; 1.102 sec/batch; 94h:12m:51s remains)
INFO - root - 2017-12-07 09:28:59.227607: step 24680, loss = 0.94, batch loss = 0.87 (7.5 examples/sec; 1.071 sec/batch; 91h:32m:49s remains)
INFO - root - 2017-12-07 09:29:09.996501: step 24690, loss = 0.55, batch loss = 0.47 (7.6 examples/sec; 1.052 sec/batch; 89h:57m:45s remains)
INFO - root - 2017-12-07 09:29:20.744882: step 24700, loss = 0.83, batch loss = 0.76 (7.2 examples/sec; 1.116 sec/batch; 95h:22m:34s remains)
2017-12-07 09:29:21.686846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0409031 -4.0947194 -4.1501522 -4.198895 -4.2284789 -4.2491326 -4.2740231 -4.300889 -4.3314857 -4.3609304 -4.3620043 -4.3393345 -4.3094978 -4.27496 -4.2490382][-4.2283525 -4.3041945 -4.3891139 -4.4730554 -4.5366445 -4.5896144 -4.6552835 -4.7249956 -4.7903509 -4.8325157 -4.8288927 -4.7955503 -4.7556405 -4.7108245 -4.670476][-4.2873859 -4.383966 -4.4984837 -4.6173296 -4.7156816 -4.7981429 -4.885869 -4.9682145 -5.0293994 -5.056262 -5.0356402 -4.9834518 -4.9231052 -4.8647084 -4.8180509][-4.0870423 -4.188199 -4.3171268 -4.4485378 -4.5561752 -4.6331191 -4.6812758 -4.7086277 -4.7208753 -4.7309904 -4.7022367 -4.6225419 -4.5344367 -4.4647508 -4.4287505][-3.6506662 -3.7358809 -3.8440983 -3.9517744 -4.0212026 -4.0317016 -3.9737089 -3.8763759 -3.8010535 -3.7868235 -3.7599585 -3.6575074 -3.5468264 -3.4673166 -3.4277649][-3.1839163 -3.186903 -3.1566484 -3.12275 -3.0612509 -2.9451337 -2.7550921 -2.555763 -2.4569149 -2.4687116 -2.4835248 -2.41231 -2.3507693 -2.2995934 -2.235245][-2.6917496 -2.4966693 -2.1946237 -1.9056919 -1.6404963 -1.3799102 -1.1265721 -0.98138833 -1.0425708 -1.2291963 -1.3801012 -1.420409 -1.4158895 -1.3604107 -1.2163517][-2.3654931 -1.995455 -1.4918122 -1.0472538 -0.69816041 -0.43609953 -0.29988241 -0.3931694 -0.70840788 -1.0717654 -1.3196292 -1.4050608 -1.3579433 -1.2235589 -1.0238469][-2.462018 -2.1094627 -1.6598938 -1.3073316 -1.0930772 -0.99373984 -1.0346096 -1.2738845 -1.6501555 -2.0208919 -2.2682083 -2.3540988 -2.3028142 -2.2104578 -2.1435232][-2.9517455 -2.7750826 -2.5337586 -2.3600261 -2.2763212 -2.2559803 -2.3066671 -2.4659021 -2.6864109 -2.927175 -3.1388376 -3.2569532 -3.2836366 -3.302587 -3.3728547][-3.5412419 -3.5845544 -3.5706897 -3.5558782 -3.5415413 -3.529175 -3.5315814 -3.5897667 -3.6783955 -3.8090644 -3.9701636 -4.0874882 -4.1538 -4.2166986 -4.3121552][-3.912308 -4.1257486 -4.2888513 -4.3911748 -4.4222341 -4.4112172 -4.3849449 -4.3906388 -4.421464 -4.4664564 -4.546515 -4.6209121 -4.6919856 -4.7571125 -4.8154664][-4.0574541 -4.2970543 -4.4872026 -4.6157284 -4.673327 -4.6859894 -4.6687922 -4.6671767 -4.68299 -4.681303 -4.6693931 -4.6458845 -4.6326146 -4.6227231 -4.6025686][-3.9760633 -4.1533318 -4.3097048 -4.4589925 -4.5886078 -4.6835203 -4.7341094 -4.7622256 -4.767046 -4.723258 -4.6393189 -4.5354543 -4.4300156 -4.3348188 -4.2403851][-3.789737 -3.9154291 -4.0426159 -4.1778779 -4.309948 -4.4122882 -4.4634023 -4.4697318 -4.4370275 -4.3649626 -4.2711391 -4.1711812 -4.0747867 -3.9925861 -3.9115]]...]
INFO - root - 2017-12-07 09:29:32.473892: step 24710, loss = 0.71, batch loss = 0.64 (7.5 examples/sec; 1.072 sec/batch; 91h:37m:53s remains)
INFO - root - 2017-12-07 09:29:43.230105: step 24720, loss = 0.71, batch loss = 0.63 (7.5 examples/sec; 1.060 sec/batch; 90h:36m:08s remains)
INFO - root - 2017-12-07 09:29:53.901209: step 24730, loss = 0.57, batch loss = 0.50 (7.4 examples/sec; 1.074 sec/batch; 91h:49m:56s remains)
INFO - root - 2017-12-07 09:30:04.705309: step 24740, loss = 0.82, batch loss = 0.75 (7.5 examples/sec; 1.068 sec/batch; 91h:16m:33s remains)
INFO - root - 2017-12-07 09:30:15.506362: step 24750, loss = 0.76, batch loss = 0.69 (7.6 examples/sec; 1.058 sec/batch; 90h:26m:45s remains)
INFO - root - 2017-12-07 09:30:26.258179: step 24760, loss = 1.17, batch loss = 1.10 (7.4 examples/sec; 1.088 sec/batch; 93h:02m:03s remains)
INFO - root - 2017-12-07 09:30:36.623043: step 24770, loss = 0.86, batch loss = 0.79 (7.5 examples/sec; 1.065 sec/batch; 91h:04m:13s remains)
INFO - root - 2017-12-07 09:30:47.192973: step 24780, loss = 0.81, batch loss = 0.74 (7.4 examples/sec; 1.074 sec/batch; 91h:49m:01s remains)
INFO - root - 2017-12-07 09:30:57.891440: step 24790, loss = 0.85, batch loss = 0.78 (7.5 examples/sec; 1.073 sec/batch; 91h:44m:43s remains)
INFO - root - 2017-12-07 09:31:08.558165: step 24800, loss = 0.82, batch loss = 0.75 (7.5 examples/sec; 1.071 sec/batch; 91h:34m:38s remains)
2017-12-07 09:31:09.344167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2211452 -4.1390796 -3.9752817 -3.910706 -3.8098466 -3.7671866 -3.7554188 -3.6864185 -3.7396717 -3.8246274 -3.7525964 -3.6401043 -3.555367 -3.5288639 -3.6202161][-4.2974391 -4.1074433 -3.811949 -3.7240286 -3.5734637 -3.5545783 -3.6076334 -3.5582359 -3.6810489 -3.8332958 -3.6857762 -3.4636936 -3.3181686 -3.300159 -3.4889023][-4.2114859 -3.8582866 -3.4499435 -3.3769946 -3.1824558 -3.1720376 -3.2554495 -3.1586502 -3.3195405 -3.5327904 -3.2755265 -2.9461021 -2.8039222 -2.8876944 -3.2394412][-4.0271206 -3.5028222 -3.055697 -3.0511332 -2.8192382 -2.7638023 -2.7628803 -2.5107062 -2.6669703 -2.93921 -2.5963001 -2.2187359 -2.1516271 -2.4283209 -3.003818][-3.8956165 -3.2472682 -2.8263535 -2.8826323 -2.5688984 -2.4176567 -2.2548962 -1.7891347 -1.9207497 -2.2901824 -1.983381 -1.6711404 -1.712002 -2.151839 -2.9035554][-3.9581089 -3.249939 -2.8572998 -2.8911095 -2.4280732 -2.1432297 -1.7963607 -1.1402078 -1.2689495 -1.7721112 -1.6351109 -1.5042353 -1.6557965 -2.1743875 -2.9502921][-4.0263033 -3.3102045 -2.9194622 -2.8427703 -2.2566223 -1.9143391 -1.4784396 -0.76820636 -0.93943 -1.5669553 -1.6585021 -1.7308264 -1.962285 -2.4783881 -3.1251426][-4.0105848 -3.3774996 -3.0095549 -2.7717781 -2.1253178 -1.8424904 -1.4819837 -0.92397904 -1.1502028 -1.8037815 -2.0909386 -2.3167424 -2.5581868 -2.9649096 -3.3766711][-3.8828683 -3.3908408 -3.075881 -2.709249 -2.1054118 -1.9164104 -1.6930411 -1.3841467 -1.6249037 -2.2019546 -2.5755775 -2.8305435 -3.0214903 -3.2654018 -3.4200072][-3.6472135 -3.2509422 -2.9901109 -2.6151352 -2.1524806 -2.0239627 -1.9241424 -1.8747146 -2.1162643 -2.5901976 -2.9214129 -3.0859563 -3.1910038 -3.27936 -3.2109466][-3.3796339 -3.0370488 -2.8338547 -2.5467911 -2.2774339 -2.167345 -2.1486259 -2.2469637 -2.4179108 -2.7533474 -2.9851995 -3.0624774 -3.1686261 -3.209269 -3.0430243][-3.1848555 -2.8553348 -2.6637704 -2.4876654 -2.3864965 -2.2614896 -2.2699864 -2.3820512 -2.4295959 -2.6422353 -2.7579308 -2.7950974 -2.968847 -3.049875 -2.8961325][-3.0604558 -2.7930036 -2.6260164 -2.5380182 -2.4915164 -2.2939568 -2.2822225 -2.35723 -2.3400061 -2.4976616 -2.519346 -2.537087 -2.7427793 -2.8484056 -2.7231748][-2.9302568 -2.7859273 -2.7103763 -2.7234838 -2.6660604 -2.3805046 -2.2688947 -2.2477918 -2.2135711 -2.3414133 -2.2962477 -2.2898879 -2.4755766 -2.6007061 -2.5229692][-2.8114913 -2.7217984 -2.6910586 -2.7319968 -2.6660151 -2.3576868 -2.156148 -2.0459793 -2.0137269 -2.1191645 -2.0542576 -2.0453589 -2.1919284 -2.3345416 -2.3561506]]...]
INFO - root - 2017-12-07 09:31:20.069050: step 24810, loss = 0.70, batch loss = 0.63 (7.5 examples/sec; 1.068 sec/batch; 91h:17m:17s remains)
INFO - root - 2017-12-07 09:31:30.939385: step 24820, loss = 0.86, batch loss = 0.79 (7.4 examples/sec; 1.081 sec/batch; 92h:25m:07s remains)
INFO - root - 2017-12-07 09:31:41.716530: step 24830, loss = 0.70, batch loss = 0.63 (7.5 examples/sec; 1.074 sec/batch; 91h:45m:12s remains)
INFO - root - 2017-12-07 09:31:52.511768: step 24840, loss = 0.60, batch loss = 0.53 (7.3 examples/sec; 1.099 sec/batch; 93h:56m:25s remains)
INFO - root - 2017-12-07 09:32:03.226927: step 24850, loss = 0.83, batch loss = 0.76 (7.5 examples/sec; 1.063 sec/batch; 90h:48m:47s remains)
INFO - root - 2017-12-07 09:32:13.991747: step 24860, loss = 0.71, batch loss = 0.64 (7.4 examples/sec; 1.088 sec/batch; 92h:58m:20s remains)
INFO - root - 2017-12-07 09:32:24.625761: step 24870, loss = 0.86, batch loss = 0.79 (7.3 examples/sec; 1.092 sec/batch; 93h:16m:24s remains)
INFO - root - 2017-12-07 09:32:35.576958: step 24880, loss = 0.78, batch loss = 0.71 (7.4 examples/sec; 1.086 sec/batch; 92h:46m:17s remains)
INFO - root - 2017-12-07 09:32:46.461349: step 24890, loss = 0.75, batch loss = 0.67 (7.3 examples/sec; 1.090 sec/batch; 93h:09m:17s remains)
INFO - root - 2017-12-07 09:32:57.337682: step 24900, loss = 0.75, batch loss = 0.68 (7.5 examples/sec; 1.070 sec/batch; 91h:25m:10s remains)
2017-12-07 09:32:58.111903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5703504 -1.3853543 -1.049192 -0.92027092 -0.98240352 -1.063482 -1.2998278 -1.7688723 -2.1705515 -2.262881 -1.9269974 -1.1945677 -0.44066262 -0.069531918 -0.0045313835][-1.3562155 -1.279011 -1.1184068 -1.064333 -1.0555615 -0.99440932 -1.053735 -1.3171642 -1.5595174 -1.5514519 -1.2198303 -0.63002849 -0.095798492 0.088586807 0.0077104568][-1.1257865 -1.3544476 -1.5226848 -1.5807304 -1.4675422 -1.20192 -1.0033405 -1.0195503 -1.0825422 -0.99459934 -0.75439167 -0.43356919 -0.21333456 -0.20581293 -0.32604837][-1.0424583 -1.648211 -2.1540146 -2.2299583 -1.8694882 -1.2479007 -0.71585226 -0.57266474 -0.69299912 -0.77093816 -0.72887373 -0.55842113 -0.41988897 -0.38957691 -0.39353132][-1.1510735 -2.0403011 -2.8065834 -2.8867741 -2.2037525 -1.0498886 -0.072235584 0.12453222 -0.31902933 -0.7935431 -0.99397373 -0.83997393 -0.56392145 -0.36701584 -0.17834854][-1.0922706 -2.0440092 -2.8457451 -2.8041821 -1.6959405 0.033091545 1.3484488 1.3223124 0.27911806 -0.77674603 -1.2613015 -1.083102 -0.57811141 -0.13012123 0.23794556][-0.8938508 -1.7332122 -2.3351035 -2.0420275 -0.55107 1.5057578 2.8713446 2.5231071 1.0008516 -0.43915153 -1.061022 -0.79434967 -0.051762581 0.63142776 0.99550104][-0.87729621 -1.5842566 -1.9719412 -1.5367978 -0.058226109 1.7177362 2.6870675 2.1718888 0.70808697 -0.59326673 -1.0377574 -0.58904409 0.32980871 1.0986991 1.2704654][-1.0853474 -1.6299181 -1.7912788 -1.2764266 -0.054768085 1.1078534 1.4343786 0.78132868 -0.30904341 -1.1110847 -1.1875408 -0.63872242 0.16782713 0.76821947 0.75137186][-1.2090476 -1.5673769 -1.531544 -1.0074356 -0.13768005 0.46114349 0.30178165 -0.40218782 -1.1039486 -1.3923867 -1.1195309 -0.57036924 -0.057326317 0.21405077 0.04847765][-1.0980506 -1.2717931 -1.109293 -0.67265797 -0.2011404 -0.026317596 -0.35067415 -0.8815372 -1.2228472 -1.2135541 -0.86926651 -0.50166917 -0.33826113 -0.38356543 -0.61027431][-0.90768385 -0.91322327 -0.66364479 -0.29955673 -0.095313549 -0.16697359 -0.4731586 -0.73807049 -0.774276 -0.63044786 -0.40444279 -0.28637123 -0.38088036 -0.60863709 -0.84215045][-0.77951956 -0.7033112 -0.46813774 -0.19038582 -0.12125206 -0.27763176 -0.51040339 -0.58932304 -0.45764947 -0.26971912 -0.16418552 -0.21711254 -0.41643143 -0.67329979 -0.86404634][-0.82841229 -0.7142005 -0.50958252 -0.30648422 -0.25888681 -0.36092567 -0.49342394 -0.49815655 -0.35999775 -0.2220397 -0.20060253 -0.31710625 -0.49998903 -0.69427848 -0.81952524][-0.92308331 -0.83981276 -0.66453528 -0.49317789 -0.43273306 -0.47917557 -0.54259109 -0.51220369 -0.39552641 -0.29396296 -0.27814674 -0.35061026 -0.4563992 -0.57303 -0.66867113]]...]
INFO - root - 2017-12-07 09:33:08.968974: step 24910, loss = 0.88, batch loss = 0.81 (7.4 examples/sec; 1.081 sec/batch; 92h:23m:59s remains)
INFO - root - 2017-12-07 09:33:19.592108: step 24920, loss = 1.01, batch loss = 0.94 (7.7 examples/sec; 1.044 sec/batch; 89h:13m:36s remains)
INFO - root - 2017-12-07 09:33:30.113321: step 24930, loss = 0.92, batch loss = 0.85 (7.7 examples/sec; 1.039 sec/batch; 88h:47m:23s remains)
INFO - root - 2017-12-07 09:33:40.723379: step 24940, loss = 0.90, batch loss = 0.83 (7.3 examples/sec; 1.097 sec/batch; 93h:44m:22s remains)
INFO - root - 2017-12-07 09:33:51.441604: step 24950, loss = 0.91, batch loss = 0.84 (7.5 examples/sec; 1.066 sec/batch; 91h:05m:31s remains)
INFO - root - 2017-12-07 09:34:02.118043: step 24960, loss = 0.70, batch loss = 0.62 (7.3 examples/sec; 1.099 sec/batch; 93h:51m:54s remains)
INFO - root - 2017-12-07 09:34:12.616292: step 24970, loss = 0.80, batch loss = 0.73 (7.3 examples/sec; 1.096 sec/batch; 93h:39m:11s remains)
INFO - root - 2017-12-07 09:34:23.254989: step 24980, loss = 0.70, batch loss = 0.62 (7.5 examples/sec; 1.069 sec/batch; 91h:20m:11s remains)
INFO - root - 2017-12-07 09:34:33.838533: step 24990, loss = 0.84, batch loss = 0.77 (7.7 examples/sec; 1.043 sec/batch; 89h:04m:03s remains)
INFO - root - 2017-12-07 09:34:44.475294: step 25000, loss = 0.94, batch loss = 0.87 (7.5 examples/sec; 1.067 sec/batch; 91h:07m:59s remains)
2017-12-07 09:34:45.263842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4542911 -3.6969438 -3.7183509 -4.1347294 -4.2053318 -3.8151333 -3.2617166 -1.4562211 0.3078208 -1.0312221 -2.572361 -2.9800901 -3.4805562 -3.4417944 -3.2007675][-3.5663433 -3.7348485 -3.7332237 -4.1598945 -4.2560716 -3.9647391 -3.4257736 -1.4903708 0.2250886 -1.2350409 -2.6781111 -2.9989378 -3.4051788 -3.2650304 -2.9977098][-3.4982605 -3.6602852 -3.640882 -3.9531162 -3.9632468 -3.7203093 -3.197145 -1.2584705 0.24882603 -1.3540869 -2.717154 -2.9892638 -3.3292363 -3.1395824 -2.8808086][-3.2087834 -3.3587902 -3.3373923 -3.5492566 -3.4972887 -3.3477566 -2.8366604 -0.91067266 0.34757614 -1.3068211 -2.5543034 -2.8860056 -3.2655396 -3.0966163 -2.8857715][-2.9355478 -2.96457 -2.8978996 -2.9931891 -2.870847 -2.859242 -2.4215829 -0.58219647 0.39581966 -1.2206521 -2.3365679 -2.7690308 -3.2261114 -3.11061 -2.9750652][-2.7940807 -2.6808224 -2.4516139 -2.2714384 -1.9907086 -2.13412 -1.833154 -0.10222483 0.59398556 -1.0269592 -2.1443191 -2.7558196 -3.2798209 -3.173913 -3.0895891][-2.3577223 -2.3445027 -2.125031 -1.6947815 -1.2412238 -1.4776154 -1.1546872 0.63555574 1.159462 -0.54617262 -1.8480325 -2.7069635 -3.2606316 -3.1428146 -3.1275082][-1.6089344 -1.8711033 -1.8548253 -1.3085768 -0.74382448 -0.99064994 -0.48777127 1.4367142 1.7728925 -0.044495106 -1.4786985 -2.41178 -2.8574905 -2.7305355 -2.8420684][-1.0372093 -1.4577553 -1.5612624 -0.94049191 -0.34414244 -0.6324513 -0.067758083 1.7483673 1.8211551 0.097287178 -1.2272522 -1.9803841 -2.2441337 -2.2192361 -2.5816438][-1.1354332 -1.6013937 -1.6913929 -0.98158312 -0.38069868 -0.64053726 -0.084275246 1.4734998 1.4257874 -0.0001578331 -1.1172464 -1.6728826 -1.8133106 -1.9524584 -2.5157056][-1.6888225 -2.1017292 -2.1630564 -1.5149879 -0.94095278 -1.0528288 -0.4962709 0.81671667 0.834208 -0.32578659 -1.3351765 -1.7287297 -1.7747202 -1.9379656 -2.4060886][-2.1391642 -2.3769608 -2.4633312 -2.0829566 -1.647599 -1.6159482 -1.153568 -0.14002371 -0.026628017 -1.0001476 -1.9004972 -2.1058357 -2.0415134 -2.0948248 -2.3291311][-2.481015 -2.564661 -2.7467489 -2.6820798 -2.370981 -2.2092216 -1.8877728 -1.2044215 -1.0314031 -1.7546637 -2.3998289 -2.4133756 -2.3286676 -2.3938317 -2.5457053][-2.7006264 -2.6264753 -2.8309336 -2.9315448 -2.7162952 -2.5451298 -2.4311767 -2.103899 -1.9492574 -2.3418386 -2.6000032 -2.4590459 -2.4139402 -2.5357752 -2.6920729][-2.8736825 -2.5895433 -2.6757686 -2.7813869 -2.6355433 -2.5366924 -2.5917792 -2.5454931 -2.4543824 -2.5701003 -2.5645227 -2.3878393 -2.391748 -2.5479207 -2.7284224]]...]
INFO - root - 2017-12-07 09:34:56.102046: step 25010, loss = 0.83, batch loss = 0.75 (7.2 examples/sec; 1.108 sec/batch; 94h:37m:59s remains)
INFO - root - 2017-12-07 09:35:06.816906: step 25020, loss = 0.90, batch loss = 0.83 (7.7 examples/sec; 1.045 sec/batch; 89h:16m:12s remains)
INFO - root - 2017-12-07 09:35:17.570408: step 25030, loss = 0.94, batch loss = 0.87 (7.4 examples/sec; 1.075 sec/batch; 91h:47m:55s remains)
INFO - root - 2017-12-07 09:35:28.217235: step 25040, loss = 0.87, batch loss = 0.80 (7.5 examples/sec; 1.068 sec/batch; 91h:15m:08s remains)
INFO - root - 2017-12-07 09:35:38.900767: step 25050, loss = 0.83, batch loss = 0.76 (7.4 examples/sec; 1.087 sec/batch; 92h:51m:13s remains)
INFO - root - 2017-12-07 09:35:49.657559: step 25060, loss = 0.77, batch loss = 0.70 (7.5 examples/sec; 1.071 sec/batch; 91h:27m:53s remains)
INFO - root - 2017-12-07 09:36:00.237460: step 25070, loss = 0.62, batch loss = 0.55 (7.6 examples/sec; 1.054 sec/batch; 89h:58m:05s remains)
INFO - root - 2017-12-07 09:36:10.853089: step 25080, loss = 0.69, batch loss = 0.62 (7.5 examples/sec; 1.062 sec/batch; 90h:41m:28s remains)
INFO - root - 2017-12-07 09:36:21.610746: step 25090, loss = 0.67, batch loss = 0.59 (7.4 examples/sec; 1.081 sec/batch; 92h:17m:40s remains)
INFO - root - 2017-12-07 09:36:32.385162: step 25100, loss = 1.02, batch loss = 0.95 (7.4 examples/sec; 1.085 sec/batch; 92h:38m:23s remains)
2017-12-07 09:36:33.149019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2376842 -3.1897197 -3.1129074 -3.0186787 -2.9631882 -3.0244088 -3.0902059 -3.087503 -3.0840507 -3.0950611 -3.1221075 -3.1136198 -3.0984704 -3.1428776 -3.2088706][-3.4184935 -3.3576005 -3.2372856 -3.0909271 -2.9943919 -3.0430353 -3.0658238 -3.0297446 -3.0230694 -3.0430551 -3.1612763 -3.2422729 -3.22503 -3.215766 -3.2079241][-3.8839126 -3.8539345 -3.7288623 -3.571209 -3.488771 -3.5785918 -3.6239367 -3.5813622 -3.5160131 -3.3584323 -3.3552351 -3.3878856 -3.3104968 -3.236515 -3.20966][-4.1241121 -4.0791125 -3.9164019 -3.6894367 -3.556495 -3.6218133 -3.7184837 -3.8209846 -3.9574769 -3.8923321 -3.8801887 -3.8084176 -3.5793736 -3.3412235 -3.2167265][-3.6942823 -3.6434102 -3.6016808 -3.4920545 -3.3485582 -3.2337894 -3.0916333 -3.0473886 -3.30892 -3.5705581 -3.8923259 -4.0408983 -3.8485718 -3.4623947 -3.1403952][-2.6933284 -2.5827589 -2.6669507 -2.7157927 -2.5889733 -2.3794456 -2.057157 -1.8308396 -2.1439748 -2.7928944 -3.6068771 -4.1448383 -4.086853 -3.5504935 -2.951396][-1.96315 -1.8757646 -2.0080483 -2.0297735 -1.7168715 -1.2318749 -0.47548246 0.18682384 -0.087301254 -1.2504187 -2.7693691 -3.8875887 -4.1724691 -3.7098305 -2.9828944][-2.1125388 -2.1887987 -2.3658471 -2.2713041 -1.6908643 -0.83703375 0.39932585 1.5379734 1.4055662 -0.03653717 -1.8745379 -3.2514377 -3.8622859 -3.6848791 -3.0911422][-2.8108125 -2.9175982 -2.9507356 -2.6610844 -1.9955153 -1.2193797 -0.29871416 0.49047279 0.359169 -0.75645566 -2.0850935 -3.0497615 -3.6137092 -3.5709615 -3.1469588][-3.3097944 -3.4694023 -3.4619703 -3.1911094 -2.7233319 -2.2927153 -1.8896794 -1.5141289 -1.598799 -2.2120657 -2.8686991 -3.3396008 -3.7084529 -3.6560748 -3.3140182][-3.2157841 -3.3800907 -3.4879186 -3.4868033 -3.387526 -3.3096902 -3.2419057 -3.0641212 -3.0088062 -3.1557341 -3.3286209 -3.5505416 -3.8740559 -3.8989058 -3.6551049][-2.8806834 -2.9192348 -2.9951842 -3.1239858 -3.2661679 -3.4527464 -3.5993135 -3.5283265 -3.383893 -3.2707438 -3.2489026 -3.4625411 -3.8418114 -3.9888716 -3.8575344][-2.8393359 -2.7881918 -2.7767458 -2.89256 -3.0567055 -3.2224984 -3.2881331 -3.1352913 -2.9304547 -2.8057485 -2.8590815 -3.1595407 -3.533354 -3.7203538 -3.6821516][-3.1152468 -3.0118537 -2.8912 -2.8899479 -2.9619648 -3.0646315 -3.1151364 -3.0087142 -2.8645709 -2.7856288 -2.8344483 -3.0315912 -3.250268 -3.3793955 -3.3988271][-3.4125023 -3.3358212 -3.2063997 -3.1756244 -3.2457285 -3.377203 -3.4832971 -3.4651175 -3.3701768 -3.2652245 -3.2183862 -3.2604418 -3.3287225 -3.3805232 -3.3841395]]...]
INFO - root - 2017-12-07 09:36:43.893414: step 25110, loss = 0.91, batch loss = 0.84 (7.5 examples/sec; 1.072 sec/batch; 91h:29m:32s remains)
INFO - root - 2017-12-07 09:36:54.610241: step 25120, loss = 0.77, batch loss = 0.70 (7.4 examples/sec; 1.077 sec/batch; 91h:56m:00s remains)
INFO - root - 2017-12-07 09:37:05.338569: step 25130, loss = 0.89, batch loss = 0.82 (7.4 examples/sec; 1.087 sec/batch; 92h:50m:04s remains)
INFO - root - 2017-12-07 09:37:16.169432: step 25140, loss = 0.57, batch loss = 0.50 (7.4 examples/sec; 1.076 sec/batch; 91h:52m:35s remains)
INFO - root - 2017-12-07 09:37:26.899585: step 25150, loss = 0.84, batch loss = 0.77 (7.6 examples/sec; 1.046 sec/batch; 89h:18m:01s remains)
INFO - root - 2017-12-07 09:37:37.722553: step 25160, loss = 0.95, batch loss = 0.88 (7.4 examples/sec; 1.080 sec/batch; 92h:13m:03s remains)
INFO - root - 2017-12-07 09:37:48.294876: step 25170, loss = 0.84, batch loss = 0.77 (7.6 examples/sec; 1.057 sec/batch; 90h:13m:07s remains)
INFO - root - 2017-12-07 09:37:58.945316: step 25180, loss = 0.74, batch loss = 0.67 (7.6 examples/sec; 1.050 sec/batch; 89h:39m:15s remains)
INFO - root - 2017-12-07 09:38:09.596848: step 25190, loss = 0.79, batch loss = 0.72 (7.6 examples/sec; 1.058 sec/batch; 90h:16m:51s remains)
INFO - root - 2017-12-07 09:38:20.378931: step 25200, loss = 0.79, batch loss = 0.72 (7.4 examples/sec; 1.087 sec/batch; 92h:45m:50s remains)
2017-12-07 09:38:21.155258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6302667 -3.5355008 -3.3234687 -3.0569539 -2.8425422 -2.7515187 -2.7283854 -2.7193222 -2.7191112 -2.7267475 -2.73633 -2.7412012 -2.7464108 -2.7428637 -2.7359827][-3.6091638 -3.63198 -3.4522266 -3.1628351 -2.9070768 -2.7847173 -2.7611244 -2.7896967 -2.8457036 -2.9039378 -2.9340794 -2.9263806 -2.8989844 -2.8525746 -2.8055649][-3.385355 -3.5138655 -3.3883691 -3.1410742 -2.9088376 -2.7707953 -2.719975 -2.7619553 -2.8802462 -3.0146284 -3.0954347 -3.1109319 -3.0782166 -2.9999168 -2.9065752][-3.3290691 -3.4649053 -3.3238182 -3.0786848 -2.8593779 -2.71168 -2.637634 -2.6997161 -2.9033222 -3.1319244 -3.265568 -3.2863603 -3.2448947 -3.1492774 -3.0208879][-3.7323413 -3.7006752 -3.3875608 -2.9870672 -2.6469984 -2.3630695 -2.1645091 -2.1905339 -2.5257146 -2.9740932 -3.3003149 -3.4455571 -3.4653084 -3.3882341 -3.2284684][-4.1428542 -3.879066 -3.3109775 -2.6517458 -2.0856941 -1.5800169 -1.1924515 -1.1653924 -1.6836247 -2.445231 -3.0657108 -3.4523163 -3.6186647 -3.6065054 -3.4431317][-4.2586288 -3.7937593 -2.9452043 -1.9829574 -1.1488786 -0.38905144 0.2070365 0.26796293 -0.49124503 -1.6076469 -2.5194306 -3.1389885 -3.4760768 -3.5757215 -3.470896][-4.1943946 -3.5816441 -2.4909663 -1.2763383 -0.22291183 0.77413464 1.5951467 1.6862588 0.65672922 -0.78594446 -1.9125919 -2.6651366 -3.1008875 -3.2900043 -3.2663097][-4.0668826 -3.4443829 -2.2809296 -1.0072854 0.073647976 1.0963631 1.9148135 1.9223504 0.69875956 -0.87104416 -1.9960585 -2.6499803 -2.9889255 -3.1224451 -3.08751][-4.1116843 -3.6177573 -2.6216817 -1.5510669 -0.64084005 0.28414917 0.9820385 0.86023712 -0.40370131 -1.8713825 -2.7969313 -3.1903996 -3.2685986 -3.2125959 -3.0715671][-4.3422403 -3.982132 -3.1875107 -2.3393354 -1.5542879 -0.64337468 0.011427402 -0.24224472 -1.5501697 -2.8829334 -3.5668287 -3.6693916 -3.4850478 -3.2583208 -3.0455105][-4.5413589 -4.2605748 -3.5845156 -2.8668211 -2.1597135 -1.2652509 -0.60945725 -0.933547 -2.2271955 -3.4004683 -3.8824148 -3.8031671 -3.5112283 -3.2388048 -3.0295854][-4.543931 -4.3017182 -3.7019391 -3.0458047 -2.387187 -1.5472522 -0.92671776 -1.2994101 -2.5505447 -3.5906124 -3.947145 -3.8029356 -3.5062876 -3.2318969 -3.0341473][-4.4790549 -4.2495809 -3.6807806 -3.0199513 -2.373239 -1.6001627 -1.0478303 -1.4744065 -2.6714239 -3.5854232 -3.8340397 -3.6605146 -3.3815441 -3.1278949 -2.9619646][-4.4510641 -4.2272677 -3.6645043 -2.971823 -2.3241475 -1.6236291 -1.1515083 -1.6114089 -2.743392 -3.5415258 -3.6943564 -3.4886239 -3.2164841 -2.9925652 -2.8695352]]...]
INFO - root - 2017-12-07 09:38:31.979804: step 25210, loss = 0.75, batch loss = 0.68 (7.6 examples/sec; 1.059 sec/batch; 90h:22m:17s remains)
INFO - root - 2017-12-07 09:38:42.644278: step 25220, loss = 0.75, batch loss = 0.68 (7.5 examples/sec; 1.066 sec/batch; 90h:57m:08s remains)
INFO - root - 2017-12-07 09:38:53.415608: step 25230, loss = 0.73, batch loss = 0.66 (7.5 examples/sec; 1.072 sec/batch; 91h:28m:35s remains)
INFO - root - 2017-12-07 09:39:04.154663: step 25240, loss = 0.62, batch loss = 0.54 (7.5 examples/sec; 1.073 sec/batch; 91h:35m:03s remains)
INFO - root - 2017-12-07 09:39:14.897980: step 25250, loss = 0.75, batch loss = 0.68 (7.4 examples/sec; 1.084 sec/batch; 92h:32m:24s remains)
INFO - root - 2017-12-07 09:39:25.719726: step 25260, loss = 0.70, batch loss = 0.63 (7.5 examples/sec; 1.064 sec/batch; 90h:48m:04s remains)
INFO - root - 2017-12-07 09:39:36.080763: step 25270, loss = 0.82, batch loss = 0.75 (7.7 examples/sec; 1.036 sec/batch; 88h:25m:05s remains)
INFO - root - 2017-12-07 09:39:46.744791: step 25280, loss = 0.68, batch loss = 0.61 (7.5 examples/sec; 1.070 sec/batch; 91h:21m:07s remains)
INFO - root - 2017-12-07 09:39:57.583683: step 25290, loss = 0.71, batch loss = 0.63 (7.4 examples/sec; 1.087 sec/batch; 92h:46m:05s remains)
INFO - root - 2017-12-07 09:40:08.208981: step 25300, loss = 0.75, batch loss = 0.67 (7.5 examples/sec; 1.070 sec/batch; 91h:18m:31s remains)
2017-12-07 09:40:09.072698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8293614 -3.8726647 -3.6827319 -3.4722462 -3.331861 -3.3379893 -3.414257 -3.5552044 -3.7103362 -3.7876725 -3.697154 -3.4293332 -3.1995099 -3.0196261 -2.91781][-3.5198538 -3.4474249 -3.1593394 -2.9340198 -2.8598218 -3.0168741 -3.2917404 -3.5452533 -3.6608987 -3.5694203 -3.2999582 -2.9063199 -2.6464534 -2.5269024 -2.5310106][-3.3983965 -3.2318583 -2.8455527 -2.5739229 -2.4802735 -2.6173539 -2.8896484 -3.1264229 -3.1821718 -3.0027463 -2.6821036 -2.3016858 -2.1109405 -2.0906248 -2.2043438][-3.1156204 -2.9381397 -2.5524006 -2.2834985 -2.1841617 -2.2648065 -2.4526081 -2.5893574 -2.5582433 -2.3469203 -2.0514526 -1.7273986 -1.5921698 -1.6396136 -1.8432705][-2.8272233 -2.7203012 -2.4351735 -2.2171361 -2.1239419 -2.1390939 -2.2065475 -2.1698089 -1.9984763 -1.7237666 -1.4307933 -1.1207912 -0.99937129 -1.0968993 -1.3838968][-2.8487601 -2.7528939 -2.464602 -2.1393416 -1.8817475 -1.6367052 -1.4164608 -1.122894 -0.85789371 -0.68241191 -0.57386208 -0.44738388 -0.45691562 -0.67788339 -1.0671287][-2.7606854 -2.6064157 -2.254348 -1.7828405 -1.3297639 -0.81474185 -0.30448866 0.19192934 0.46044827 0.4286437 0.25670052 0.1166091 -0.092134 -0.47429514 -0.97415948][-2.5882421 -2.3695445 -1.982265 -1.4897575 -1.0420623 -0.54638124 -0.032041073 0.44344664 0.65125036 0.5148201 0.22444916 -0.012896538 -0.26992369 -0.67376161 -1.1630554][-2.5966778 -2.380146 -2.0480313 -1.6527414 -1.3240032 -0.95513725 -0.55493522 -0.19724131 -0.08830595 -0.27011251 -0.55464292 -0.76224494 -0.94402862 -1.2415106 -1.6019666][-2.5745537 -2.3998616 -2.1807749 -1.9462953 -1.7699983 -1.5287635 -1.2568324 -1.0480049 -1.0582812 -1.2677476 -1.5030155 -1.6271541 -1.6906118 -1.8387396 -2.0525663][-2.444252 -2.3108795 -2.1900158 -2.0911722 -2.0390422 -1.9450848 -1.85251 -1.8220999 -1.9239442 -2.1015174 -2.2308178 -2.2486861 -2.2173057 -2.2515104 -2.3555927][-2.6137629 -2.5574222 -2.521667 -2.5152225 -2.5242603 -2.497752 -2.47757 -2.5037918 -2.6085572 -2.725451 -2.7756066 -2.7238276 -2.631134 -2.577188 -2.5850608][-2.9379206 -2.966331 -2.9981863 -3.0448601 -3.0816214 -3.0722218 -3.0475132 -3.0404153 -3.0855541 -3.1312561 -3.1240349 -3.0387661 -2.923727 -2.8340349 -2.7926502][-3.0752492 -3.1178579 -3.14854 -3.1798038 -3.2050495 -3.2032168 -3.1887507 -3.1715691 -3.175168 -3.1731377 -3.1407831 -3.0690119 -2.9926975 -2.9427805 -2.9223232][-3.0753489 -3.1010675 -3.110218 -3.1129856 -3.1182594 -3.1169662 -3.1129856 -3.1031678 -3.0987177 -3.0933022 -3.0755024 -3.0432119 -3.0154529 -3.0049942 -3.0042274]]...]
INFO - root - 2017-12-07 09:40:19.716755: step 25310, loss = 0.93, batch loss = 0.86 (7.5 examples/sec; 1.066 sec/batch; 90h:55m:25s remains)
INFO - root - 2017-12-07 09:40:30.337309: step 25320, loss = 0.80, batch loss = 0.73 (7.6 examples/sec; 1.050 sec/batch; 89h:37m:36s remains)
INFO - root - 2017-12-07 09:40:41.082817: step 25330, loss = 0.85, batch loss = 0.78 (7.6 examples/sec; 1.053 sec/batch; 89h:49m:20s remains)
INFO - root - 2017-12-07 09:40:51.758526: step 25340, loss = 0.97, batch loss = 0.90 (7.4 examples/sec; 1.080 sec/batch; 92h:10m:29s remains)
INFO - root - 2017-12-07 09:41:02.499630: step 25350, loss = 0.69, batch loss = 0.62 (7.5 examples/sec; 1.062 sec/batch; 90h:37m:23s remains)
INFO - root - 2017-12-07 09:41:13.248663: step 25360, loss = 0.66, batch loss = 0.59 (7.6 examples/sec; 1.054 sec/batch; 89h:55m:59s remains)
INFO - root - 2017-12-07 09:41:23.703760: step 25370, loss = 0.94, batch loss = 0.87 (7.4 examples/sec; 1.082 sec/batch; 92h:20m:51s remains)
INFO - root - 2017-12-07 09:41:34.383630: step 25380, loss = 0.81, batch loss = 0.74 (7.4 examples/sec; 1.077 sec/batch; 91h:51m:12s remains)
INFO - root - 2017-12-07 09:41:45.032623: step 25390, loss = 0.99, batch loss = 0.92 (7.7 examples/sec; 1.042 sec/batch; 88h:52m:32s remains)
INFO - root - 2017-12-07 09:41:55.820158: step 25400, loss = 0.88, batch loss = 0.81 (7.4 examples/sec; 1.087 sec/batch; 92h:45m:02s remains)
2017-12-07 09:41:56.675542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4985855 -1.1565208 -0.80741906 -0.63241863 -0.6269331 -0.59966516 -0.77442884 -1.2880173 -1.7495878 -1.6420257 -1.0347679 -0.46766305 -0.45581388 -1.2666318 -2.1779251][-0.89936209 -0.30523396 0.26365662 0.43242693 0.133008 -0.13825321 -0.47826004 -0.98528743 -1.4995446 -1.4479918 -0.75541472 -0.06253767 0.13344431 -0.49291873 -1.3603063][-0.68413067 -0.054256916 0.61788225 0.88615704 0.55169821 0.15836668 -0.26725435 -0.71354175 -1.1283822 -1.0548942 -0.41704273 0.16357899 0.36813736 -0.10088682 -0.87643266][-0.54299307 -0.085504532 0.341187 0.35041809 -0.10767031 -0.403821 -0.58998418 -0.7277739 -0.77041245 -0.42493582 0.24469614 0.76076651 1.0063429 0.62237883 -0.23410654][0.33275747 0.43742323 0.27882528 -0.32219505 -0.93543673 -0.86709356 -0.49216819 -0.083907127 0.28602409 0.80112314 1.3729663 1.8204694 2.0085115 1.6145186 0.70749378][1.6584883 1.6671882 1.2235303 0.31331491 -0.17865658 0.31772661 1.1826248 2.0385346 2.630115 3.0131922 3.2661076 3.4247355 3.2445607 2.5011468 1.4788556][2.5325093 2.8582311 2.6075644 1.8291278 1.5175419 2.0500078 2.9515076 3.9752541 4.6029673 4.7114649 4.6818323 4.5693893 4.0879221 3.0850348 2.0327539][2.3358374 2.8799596 2.7919788 2.1497841 1.8121061 1.9561405 2.4922576 3.38055 3.992734 4.1362209 4.186223 4.0859232 3.688035 2.9051886 2.1634178][0.99272203 1.348228 1.1212096 0.52035522 0.21012878 0.19105768 0.50962496 1.1452518 1.6238885 1.931828 2.2160048 2.2593198 2.03402 1.5967207 1.2329373][-0.56106377 -0.49915814 -0.94479775 -1.5447547 -1.7620864 -1.6894095 -1.3565676 -0.8537941 -0.49999309 -0.16435766 0.17011929 0.24239922 0.023198128 -0.32332277 -0.59294415][-1.6686807 -1.6702673 -2.0686929 -2.5423427 -2.6910388 -2.6334553 -2.4360549 -2.1366858 -1.9021294 -1.6921594 -1.422524 -1.36747 -1.5993812 -1.9034388 -2.1148679][-1.9884863 -1.9350646 -2.1544986 -2.4663963 -2.5825381 -2.6060104 -2.6358061 -2.5642283 -2.4550204 -2.4168093 -2.2775269 -2.1882899 -2.274111 -2.4433517 -2.5681925][-2.0396845 -1.9810116 -2.071105 -2.2702911 -2.36639 -2.4241965 -2.5730014 -2.6249964 -2.5624914 -2.5198836 -2.3256102 -2.0634236 -1.9274955 -2.022193 -2.1713934][-2.1681628 -2.1614263 -2.2681398 -2.4722614 -2.6108212 -2.7135615 -2.8903897 -2.9889774 -2.9062171 -2.7019176 -2.3173146 -1.861697 -1.5872138 -1.7405291 -2.0139894][-2.8314495 -2.8538334 -3.0170221 -3.2672424 -3.4485 -3.5339613 -3.616842 -3.6397648 -3.4670663 -3.075247 -2.5353265 -1.9928772 -1.7303433 -2.0117462 -2.3557992]]...]
INFO - root - 2017-12-07 09:42:07.478014: step 25410, loss = 0.72, batch loss = 0.65 (7.4 examples/sec; 1.084 sec/batch; 92h:29m:21s remains)
INFO - root - 2017-12-07 09:42:18.269722: step 25420, loss = 0.79, batch loss = 0.71 (7.4 examples/sec; 1.088 sec/batch; 92h:48m:30s remains)
INFO - root - 2017-12-07 09:42:28.995374: step 25430, loss = 0.62, batch loss = 0.55 (7.5 examples/sec; 1.068 sec/batch; 91h:03m:38s remains)
INFO - root - 2017-12-07 09:42:39.554504: step 25440, loss = 0.91, batch loss = 0.84 (7.5 examples/sec; 1.060 sec/batch; 90h:25m:45s remains)
INFO - root - 2017-12-07 09:42:50.178477: step 25450, loss = 0.67, batch loss = 0.59 (7.5 examples/sec; 1.073 sec/batch; 91h:32m:06s remains)
INFO - root - 2017-12-07 09:43:00.810661: step 25460, loss = 0.86, batch loss = 0.78 (7.4 examples/sec; 1.076 sec/batch; 91h:44m:05s remains)
INFO - root - 2017-12-07 09:43:11.361674: step 25470, loss = 0.91, batch loss = 0.84 (7.2 examples/sec; 1.107 sec/batch; 94h:22m:53s remains)
INFO - root - 2017-12-07 09:43:22.123158: step 25480, loss = 0.83, batch loss = 0.76 (7.5 examples/sec; 1.066 sec/batch; 90h:52m:28s remains)
INFO - root - 2017-12-07 09:43:32.834843: step 25490, loss = 0.83, batch loss = 0.75 (7.5 examples/sec; 1.072 sec/batch; 91h:24m:39s remains)
INFO - root - 2017-12-07 09:43:43.543704: step 25500, loss = 0.76, batch loss = 0.69 (7.6 examples/sec; 1.055 sec/batch; 89h:55m:38s remains)
2017-12-07 09:43:44.373907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3755736 -4.1770205 -3.8735657 -3.5614772 -3.3970466 -3.1862648 -2.9768987 -2.8667774 -2.8683746 -3.0013394 -3.1382232 -3.2101479 -3.1824584 -2.998692 -2.7383056][-4.659431 -4.3985419 -4.1679587 -3.93671 -3.7599545 -3.4907031 -3.29011 -3.3563249 -3.5883646 -3.9052305 -4.1369267 -4.2009931 -4.0420938 -3.6286566 -3.107223][-4.621767 -4.3846269 -4.260057 -4.1078076 -3.853157 -3.4017172 -3.054122 -3.1381869 -3.5004511 -3.9201212 -4.2073164 -4.3135738 -4.1568193 -3.7091308 -3.1271877][-4.1071525 -3.8684559 -3.7282097 -3.5880966 -3.2982254 -2.7674713 -2.3157063 -2.2982976 -2.652271 -3.0755577 -3.3387 -3.4469714 -3.2967312 -2.8898964 -2.3783748][-3.7777131 -3.5922351 -3.388001 -3.189817 -2.8862164 -2.4228165 -1.9681892 -1.8474386 -2.2290502 -2.790879 -3.1738815 -3.3287017 -3.141542 -2.6942377 -2.1215365][-3.2097809 -2.9576726 -2.6516778 -2.3528309 -1.9100921 -1.395299 -0.94424605 -0.80377936 -1.1850514 -1.8000779 -2.2406795 -2.4157591 -2.2611585 -1.9206154 -1.4920568][-2.6609516 -2.2176628 -1.7086751 -1.1763644 -0.42215824 0.33251047 0.85323381 0.93445158 0.512764 -0.098599911 -0.520401 -0.64597917 -0.54547739 -0.39454317 -0.28472328][-2.9628744 -2.5239396 -1.9902878 -1.3887475 -0.5291853 0.33587456 0.90552711 1.0137358 0.66281939 0.13407755 -0.20257711 -0.2186265 -0.046127319 0.13873911 0.20192289][-3.3243635 -2.9603386 -2.5581524 -2.1598084 -1.6503854 -1.1296599 -0.78083038 -0.69483042 -0.88175249 -1.1728237 -1.302387 -1.2259007 -1.03599 -0.77049732 -0.58633065][-3.5856321 -3.22153 -2.8519611 -2.5377636 -2.2823739 -2.05868 -1.917762 -1.8627882 -1.9368587 -2.0302551 -2.0127468 -1.9236531 -1.8046188 -1.5763838 -1.3440821][-4.1924429 -3.9272919 -3.6961067 -3.4980464 -3.3566794 -3.2338252 -3.1714787 -3.1708453 -3.2314425 -3.2513578 -3.1858203 -3.0883217 -2.9816842 -2.7038596 -2.3160367][-4.1469712 -3.8463502 -3.6994755 -3.65924 -3.6385186 -3.6144629 -3.6191804 -3.6621325 -3.7223332 -3.7318974 -3.7059164 -3.6579731 -3.5866814 -3.3417115 -2.9808488][-3.7211523 -3.2978091 -3.0856743 -3.0353584 -2.9997849 -2.9825611 -2.9975042 -3.0282686 -3.0633755 -3.0704532 -3.1057611 -3.1446762 -3.1752248 -3.0761051 -2.9179761][-3.7156916 -3.4132714 -3.2965546 -3.2816725 -3.2607088 -3.2522907 -3.2562132 -3.2579756 -3.2640896 -3.2761588 -3.3416805 -3.4201589 -3.450738 -3.3563466 -3.2317955][-3.7220063 -3.5644674 -3.5445004 -3.5606477 -3.5606494 -3.5615249 -3.5630448 -3.5539579 -3.5449612 -3.5431509 -3.5812213 -3.6466475 -3.6679552 -3.6010151 -3.5143709]]...]
INFO - root - 2017-12-07 09:43:55.027215: step 25510, loss = 0.82, batch loss = 0.75 (7.7 examples/sec; 1.039 sec/batch; 88h:35m:42s remains)
INFO - root - 2017-12-07 09:44:05.775497: step 25520, loss = 1.02, batch loss = 0.95 (7.6 examples/sec; 1.059 sec/batch; 90h:19m:49s remains)
INFO - root - 2017-12-07 09:44:16.538334: step 25530, loss = 0.78, batch loss = 0.71 (7.6 examples/sec; 1.049 sec/batch; 89h:27m:53s remains)
INFO - root - 2017-12-07 09:44:27.324539: step 25540, loss = 0.90, batch loss = 0.82 (7.5 examples/sec; 1.066 sec/batch; 90h:52m:48s remains)
INFO - root - 2017-12-07 09:44:38.124674: step 25550, loss = 0.86, batch loss = 0.78 (7.5 examples/sec; 1.060 sec/batch; 90h:23m:38s remains)
INFO - root - 2017-12-07 09:44:48.972472: step 25560, loss = 0.96, batch loss = 0.88 (7.4 examples/sec; 1.080 sec/batch; 92h:05m:33s remains)
INFO - root - 2017-12-07 09:44:59.540267: step 25570, loss = 0.69, batch loss = 0.62 (7.4 examples/sec; 1.077 sec/batch; 91h:51m:08s remains)
INFO - root - 2017-12-07 09:45:09.487349: step 25580, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.769 sec/batch; 65h:34m:24s remains)
INFO - root - 2017-12-07 09:45:17.139263: step 25590, loss = 0.97, batch loss = 0.90 (10.5 examples/sec; 0.765 sec/batch; 65h:11m:03s remains)
INFO - root - 2017-12-07 09:45:24.787326: step 25600, loss = 0.75, batch loss = 0.67 (10.6 examples/sec; 0.756 sec/batch; 64h:27m:48s remains)
2017-12-07 09:45:25.383523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1314139 -3.1605525 -3.1461809 -3.1713419 -3.1344497 -3.1300955 -2.7906203 -2.0656984 -1.9044113 -2.5506096 -3.2049055 -3.5853086 -3.5945327 -3.0413795 -2.4255042][-3.1316788 -3.1118951 -3.10346 -3.1380594 -3.1105528 -3.1271677 -2.7780356 -2.0340204 -1.8767209 -2.5649352 -3.2462358 -3.6163838 -3.6205835 -3.0906239 -2.4948895][-3.0961013 -3.0427783 -3.0361435 -3.0589886 -3.0367258 -3.0548046 -2.697566 -1.9965589 -1.8903649 -2.6027031 -3.2950082 -3.6481633 -3.6190352 -3.0933723 -2.5194979][-3.0791807 -3.0159006 -2.9814324 -2.9245062 -2.8543553 -2.8194447 -2.4551494 -1.8872907 -1.9130173 -2.661829 -3.354593 -3.6808317 -3.5929167 -3.0529294 -2.4993854][-3.0933278 -3.0580022 -3.0091381 -2.8532639 -2.7003736 -2.5650873 -2.1744773 -1.7603981 -1.9447792 -2.7396479 -3.4260983 -3.710597 -3.5460463 -2.9837785 -2.4541008][-3.0304961 -3.0360391 -2.986932 -2.7610095 -2.5377846 -2.3004353 -1.8852568 -1.6189094 -1.9528298 -2.8068037 -3.5014372 -3.7402992 -3.5080516 -2.9272933 -2.4116952][-2.902072 -2.94461 -2.9110668 -2.6636472 -2.3987744 -2.0762322 -1.6200545 -1.4420147 -1.8660505 -2.76495 -3.4943995 -3.7181847 -3.4651461 -2.8860426 -2.3800836][-2.8180628 -2.884572 -2.87515 -2.6674013 -2.4219878 -2.06303 -1.5441449 -1.3481543 -1.7701244 -2.670331 -3.4446852 -3.6746709 -3.4268656 -2.85385 -2.3553798][-2.7862735 -2.8621621 -2.8832278 -2.7335858 -2.537715 -2.1820695 -1.6067688 -1.3558557 -1.71967 -2.5886154 -3.3984509 -3.6400087 -3.4036775 -2.8231292 -2.3240774][-2.7867289 -2.846312 -2.8877275 -2.7928731 -2.6592004 -2.3321335 -1.7520416 -1.4753454 -1.764925 -2.5848327 -3.3946605 -3.6278365 -3.3835616 -2.7783027 -2.27744][-2.7533011 -2.7783756 -2.8311424 -2.8087015 -2.7548027 -2.4726748 -1.9475691 -1.6920528 -1.9160442 -2.6933045 -3.4786949 -3.6804967 -3.4043837 -2.7539167 -2.2383811][-2.6928535 -2.6703761 -2.711246 -2.7536755 -2.7690382 -2.5341487 -2.1004696 -1.8881283 -2.059612 -2.7919581 -3.5433249 -3.7306595 -3.4428639 -2.7606263 -2.2222819][-2.6688685 -2.5939798 -2.5988655 -2.6898909 -2.7434211 -2.5321724 -2.1821692 -1.9726508 -2.0817468 -2.7691836 -3.4879675 -3.7025695 -3.4381442 -2.7613039 -2.2130466][-2.6523705 -2.5376954 -2.505372 -2.6262708 -2.693496 -2.5034733 -2.2229877 -1.9726508 -2.0157332 -2.6628485 -3.3378372 -3.5894065 -3.3723722 -2.7323072 -2.2012517][-2.6405537 -2.4993706 -2.4272225 -2.5514531 -2.6265807 -2.4912097 -2.2986133 -2.0039115 -1.9873552 -2.5906782 -3.2004232 -3.4617035 -3.2795467 -2.6805491 -2.1849327]]...]
INFO - root - 2017-12-07 09:45:33.039981: step 25610, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.753 sec/batch; 64h:11m:34s remains)
INFO - root - 2017-12-07 09:45:40.688648: step 25620, loss = 0.75, batch loss = 0.68 (10.0 examples/sec; 0.797 sec/batch; 67h:53m:56s remains)
INFO - root - 2017-12-07 09:45:48.352581: step 25630, loss = 1.01, batch loss = 0.94 (10.4 examples/sec; 0.770 sec/batch; 65h:39m:53s remains)
INFO - root - 2017-12-07 09:45:56.002626: step 25640, loss = 0.84, batch loss = 0.77 (10.3 examples/sec; 0.774 sec/batch; 65h:57m:49s remains)
INFO - root - 2017-12-07 09:46:03.649037: step 25650, loss = 0.76, batch loss = 0.68 (10.3 examples/sec; 0.777 sec/batch; 66h:15m:28s remains)
INFO - root - 2017-12-07 09:46:11.263707: step 25660, loss = 1.03, batch loss = 0.96 (10.9 examples/sec; 0.735 sec/batch; 62h:40m:53s remains)
INFO - root - 2017-12-07 09:46:18.724530: step 25670, loss = 1.11, batch loss = 1.04 (10.1 examples/sec; 0.791 sec/batch; 67h:23m:51s remains)
INFO - root - 2017-12-07 09:46:26.594396: step 25680, loss = 0.73, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 65h:46m:19s remains)
INFO - root - 2017-12-07 09:46:34.295957: step 25690, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 65h:27m:25s remains)
INFO - root - 2017-12-07 09:46:41.985852: step 25700, loss = 0.78, batch loss = 0.71 (10.4 examples/sec; 0.771 sec/batch; 65h:43m:45s remains)
2017-12-07 09:46:42.528144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4944816 -3.6124213 -3.5325532 -3.4694254 -3.5648284 -3.7546062 -3.7805738 -3.6077814 -3.3743472 -3.1936903 -3.0810618 -3.0021477 -2.9806833 -3.1550012 -3.3760362][-3.2392378 -3.3948236 -3.35875 -3.3345351 -3.4853446 -3.6918321 -3.6678371 -3.4415231 -3.2558475 -3.1473489 -3.0274639 -2.9125776 -2.8782871 -3.0228631 -3.2211549][-2.9717627 -3.0378938 -2.9903111 -3.0375147 -3.2665458 -3.4943247 -3.4633834 -3.2635489 -3.2075381 -3.2048593 -3.081054 -2.9204981 -2.830399 -2.9325948 -3.1294661][-2.7988732 -2.8473902 -2.8197904 -2.8908834 -3.067843 -3.2177038 -3.1713543 -3.039423 -3.146076 -3.29193 -3.2231922 -3.0534239 -2.9249442 -3.0011325 -3.2155566][-2.7142243 -2.8414271 -2.9077284 -2.9862804 -3.0447612 -3.0456381 -2.9088194 -2.7889292 -3.0174451 -3.31991 -3.3373058 -3.1424966 -2.9624426 -3.0068352 -3.1934094][-2.77411 -2.926558 -3.06564 -3.1611588 -3.1627893 -3.046968 -2.7816083 -2.623908 -2.9025431 -3.3066416 -3.4061036 -3.1596203 -2.9129953 -2.8965023 -2.9844155][-2.8050821 -2.9081984 -3.0577111 -3.1454921 -3.1779299 -3.0424109 -2.6917567 -2.5083218 -2.7804797 -3.2177768 -3.4071662 -3.1697044 -2.9088368 -2.8399181 -2.8035703][-2.6960847 -2.7657547 -2.9234467 -3.0068424 -3.0910754 -2.9914994 -2.6666973 -2.5414698 -2.7830894 -3.1865692 -3.447171 -3.2627115 -3.0051055 -2.8736997 -2.7111378][-2.6994114 -2.7430186 -2.8833854 -2.9633942 -3.0732026 -3.0061703 -2.7256966 -2.6301036 -2.8196681 -3.1857233 -3.5029225 -3.3762345 -3.1115723 -2.9362659 -2.7091222][-2.9707074 -2.9548838 -3.002106 -3.0368061 -3.1301169 -3.0964224 -2.8339708 -2.6809123 -2.782341 -3.1059053 -3.4537346 -3.4039841 -3.1691883 -2.9978185 -2.7993832][-3.2862411 -3.2536597 -3.2129948 -3.195333 -3.2757454 -3.2875562 -3.0691867 -2.8733668 -2.88733 -3.0981481 -3.3814354 -3.4028759 -3.2462728 -3.1103876 -2.9541514][-3.394913 -3.3868637 -3.3435276 -3.3351245 -3.4190969 -3.445245 -3.2894547 -3.1502311 -3.1518149 -3.2232449 -3.3773272 -3.4256377 -3.337069 -3.2351255 -3.1068544][-3.1556845 -3.1917424 -3.1962519 -3.22848 -3.3184652 -3.3407407 -3.2603045 -3.2445626 -3.287823 -3.2778685 -3.3400702 -3.3687084 -3.3035853 -3.2434645 -3.1644852][-2.7986679 -2.8726697 -2.9013557 -2.924619 -2.9760129 -2.9606688 -2.9008904 -2.933429 -3.0084991 -2.9989791 -3.035814 -3.052001 -3.0235574 -3.0350254 -3.0120323][-2.7116227 -2.8037205 -2.8035417 -2.7618043 -2.7343755 -2.6471167 -2.5183659 -2.4809771 -2.5407503 -2.5636694 -2.5933833 -2.5855513 -2.594965 -2.6696038 -2.6845241]]...]
INFO - root - 2017-12-07 09:46:50.269389: step 25710, loss = 0.66, batch loss = 0.59 (10.0 examples/sec; 0.801 sec/batch; 68h:16m:41s remains)
INFO - root - 2017-12-07 09:46:57.937772: step 25720, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.770 sec/batch; 65h:38m:42s remains)
INFO - root - 2017-12-07 09:47:05.502141: step 25730, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.758 sec/batch; 64h:33m:41s remains)
INFO - root - 2017-12-07 09:47:13.162572: step 25740, loss = 0.90, batch loss = 0.83 (10.4 examples/sec; 0.769 sec/batch; 65h:31m:54s remains)
INFO - root - 2017-12-07 09:47:20.787391: step 25750, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.783 sec/batch; 66h:44m:59s remains)
INFO - root - 2017-12-07 09:47:28.588607: step 25760, loss = 1.00, batch loss = 0.93 (10.1 examples/sec; 0.792 sec/batch; 67h:28m:27s remains)
INFO - root - 2017-12-07 09:47:35.995689: step 25770, loss = 0.81, batch loss = 0.74 (10.3 examples/sec; 0.773 sec/batch; 65h:51m:45s remains)
INFO - root - 2017-12-07 09:47:43.619601: step 25780, loss = 0.91, batch loss = 0.84 (10.4 examples/sec; 0.768 sec/batch; 65h:24m:23s remains)
INFO - root - 2017-12-07 09:47:51.208820: step 25790, loss = 0.82, batch loss = 0.75 (10.6 examples/sec; 0.757 sec/batch; 64h:28m:29s remains)
INFO - root - 2017-12-07 09:47:58.883145: step 25800, loss = 0.73, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 65h:20m:38s remains)
2017-12-07 09:47:59.510943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4187622 -2.7292409 -2.2076046 -2.1267614 -2.4960575 -3.1213777 -3.5728459 -3.7509158 -3.7329943 -3.6551468 -3.5279112 -3.5473838 -3.5293963 -3.2291946 -2.678931][-3.1609035 -2.4108827 -1.9040608 -1.9287541 -2.4111481 -3.1172237 -3.4958942 -3.5102892 -3.3737106 -3.2059615 -3.0251136 -3.0071802 -2.9471502 -2.5382652 -1.8799233][-2.8981085 -2.1755152 -1.748342 -1.8940794 -2.4575183 -3.1362572 -3.2772436 -2.9724743 -2.6743529 -2.4784832 -2.3475864 -2.3572462 -2.2804389 -1.8084052 -1.1325464][-2.4899893 -1.8487375 -1.561271 -1.8488655 -2.4750359 -3.0390084 -2.8578854 -2.225323 -1.8402724 -1.7101541 -1.6832407 -1.7121131 -1.5712409 -1.0593996 -0.44974279][-1.7440603 -1.2858615 -1.2535715 -1.73697 -2.3400211 -2.6182454 -2.0470698 -1.1678634 -0.84024 -0.96920919 -1.2054231 -1.36326 -1.2699001 -0.85375237 -0.39886904][-0.98574924 -0.8427341 -1.0978377 -1.6438634 -2.0207813 -1.8426423 -0.83248615 0.25594568 0.47857094 0.023389339 -0.53467751 -0.90819383 -1.037617 -0.95509124 -0.812619][-0.63310575 -0.80706406 -1.2105794 -1.6238186 -1.644556 -0.88820004 0.6036191 1.8534236 1.8663154 1.0346632 0.16965437 -0.44624496 -0.8982029 -1.2133532 -1.342648][-0.60388684 -0.98708081 -1.4085109 -1.6713991 -1.4545212 -0.25937223 1.5351033 2.7204924 2.3872495 1.276258 0.29031134 -0.47810316 -1.1639867 -1.6972759 -1.908031][-0.79940963 -1.3086474 -1.6691673 -1.8022208 -1.4905403 -0.22027683 1.30758 1.9659314 1.3868051 0.39647532 -0.33505774 -0.99535012 -1.7212312 -2.2888293 -2.4471507][-1.3501029 -1.841779 -2.0825391 -2.1125467 -1.7808943 -0.6774435 0.33192253 0.491652 -0.052525997 -0.66728544 -1.0465798 -1.5087063 -2.1321108 -2.5988588 -2.6633461][-2.0861127 -2.4382648 -2.5119703 -2.4579828 -2.1606512 -1.3260245 -0.74588108 -0.81277657 -1.1623578 -1.4518108 -1.5989826 -1.8530381 -2.2729912 -2.6158514 -2.6766925][-2.6825857 -2.869792 -2.7621946 -2.5891292 -2.3180516 -1.7556636 -1.4730628 -1.6122518 -1.8522294 -2.0722287 -2.1950295 -2.2878094 -2.5097079 -2.7446549 -2.7950253][-3.2318959 -3.2866378 -3.0526898 -2.7729168 -2.5127511 -2.1375623 -1.9787486 -2.0967693 -2.3167965 -2.5896683 -2.7489586 -2.7640533 -2.9058712 -3.1209188 -3.1814394][-3.7416973 -3.7887704 -3.5556033 -3.2637308 -3.0297496 -2.7744679 -2.6553364 -2.7100039 -2.850065 -3.053196 -3.1790037 -3.1767778 -3.3216562 -3.5684323 -3.6751447][-3.9700012 -4.0620875 -3.9273252 -3.7323327 -3.5775113 -3.402427 -3.2844398 -3.28503 -3.346503 -3.4739189 -3.5953157 -3.6157143 -3.7345386 -3.9354041 -4.0072703]]...]
INFO - root - 2017-12-07 09:48:07.203551: step 25810, loss = 1.11, batch loss = 1.04 (10.4 examples/sec; 0.768 sec/batch; 65h:26m:46s remains)
INFO - root - 2017-12-07 09:48:14.997508: step 25820, loss = 0.88, batch loss = 0.81 (10.2 examples/sec; 0.781 sec/batch; 66h:33m:02s remains)
INFO - root - 2017-12-07 09:48:22.514765: step 25830, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.767 sec/batch; 65h:21m:37s remains)
INFO - root - 2017-12-07 09:48:30.213920: step 25840, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 66h:15m:49s remains)
INFO - root - 2017-12-07 09:48:37.839057: step 25850, loss = 0.64, batch loss = 0.56 (10.3 examples/sec; 0.774 sec/batch; 65h:57m:38s remains)
INFO - root - 2017-12-07 09:48:45.435567: step 25860, loss = 0.98, batch loss = 0.90 (10.7 examples/sec; 0.746 sec/batch; 63h:32m:57s remains)
INFO - root - 2017-12-07 09:48:52.810719: step 25870, loss = 0.98, batch loss = 0.91 (10.4 examples/sec; 0.769 sec/batch; 65h:28m:27s remains)
INFO - root - 2017-12-07 09:49:00.589699: step 25880, loss = 0.67, batch loss = 0.60 (10.7 examples/sec; 0.745 sec/batch; 63h:29m:18s remains)
INFO - root - 2017-12-07 09:49:08.215833: step 25890, loss = 0.88, batch loss = 0.81 (10.2 examples/sec; 0.785 sec/batch; 66h:53m:41s remains)
INFO - root - 2017-12-07 09:49:15.964225: step 25900, loss = 0.76, batch loss = 0.69 (10.3 examples/sec; 0.773 sec/batch; 65h:51m:38s remains)
2017-12-07 09:49:16.562658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.553308 -3.7050242 -3.9083 -3.9972978 -3.9141939 -3.8041019 -3.7850547 -3.8018136 -3.7825317 -3.7583492 -3.7332079 -3.7978823 -3.8553071 -3.8201418 -3.7122259][-3.3836725 -3.4361892 -3.547554 -3.4939888 -3.2431395 -3.0624022 -3.1152735 -3.2711916 -3.4009814 -3.540715 -3.6472998 -3.8104854 -3.9172182 -3.8600717 -3.6966383][-3.0831199 -2.9631193 -2.9022572 -2.6737611 -2.2933528 -2.116626 -2.3076305 -2.6253781 -2.8918762 -3.1875808 -3.431561 -3.6804051 -3.815346 -3.7449567 -3.5579062][-2.9250467 -2.7212424 -2.5650411 -2.2478292 -1.8325338 -1.7027278 -1.9867015 -2.3606825 -2.6341777 -2.9239883 -3.1785698 -3.4294329 -3.5485463 -3.4742606 -3.3123231][-3.0941141 -2.9763813 -2.8686004 -2.5726318 -2.2025013 -2.0988345 -2.3373263 -2.5871458 -2.6724343 -2.7448282 -2.836297 -2.9737344 -3.016937 -2.9418678 -2.8730984][-3.3875237 -3.3802104 -3.3468723 -3.1408172 -2.8853431 -2.7867818 -2.8977234 -2.9565949 -2.8253157 -2.6628103 -2.5688477 -2.5396683 -2.4300866 -2.3433704 -2.4094505][-3.4340339 -3.3975632 -3.3133049 -3.1482811 -2.9876332 -2.8607478 -2.8644114 -2.8486571 -2.726006 -2.5953422 -2.5404701 -2.46972 -2.2219548 -2.0703771 -2.1973641][-3.0265491 -2.8235881 -2.6085043 -2.4660435 -2.3927133 -2.2691481 -2.2468123 -2.2667871 -2.2436447 -2.2170208 -2.2334576 -2.1761801 -1.9044001 -1.7872944 -1.9986813][-2.3711939 -1.9136062 -1.5331233 -1.3826072 -1.383136 -1.3269122 -1.3593175 -1.4675705 -1.5328858 -1.5796041 -1.618557 -1.5633025 -1.3741736 -1.4331431 -1.8004315][-1.9635224 -1.3815541 -0.99376035 -0.92936039 -1.0299075 -1.0586052 -1.1459553 -1.2699327 -1.3252912 -1.3580325 -1.3675437 -1.2822609 -1.1499321 -1.310925 -1.7566602][-2.1900976 -1.6699564 -1.36397 -1.3345265 -1.4071789 -1.4319186 -1.4975305 -1.5367959 -1.4966838 -1.4699142 -1.4716351 -1.4002256 -1.3155515 -1.4955411 -1.9246001][-2.9212976 -2.5503302 -2.2756298 -2.1353812 -2.0483987 -1.977133 -1.9812987 -1.9772644 -1.9412875 -1.9624386 -2.0430262 -2.0658245 -2.0336976 -2.1406922 -2.4408867][-3.5882528 -3.4498737 -3.3120933 -3.1942239 -3.0833082 -3.0031366 -2.9643598 -2.9348397 -2.9246774 -2.984447 -3.083972 -3.1216793 -3.0845847 -3.0946774 -3.2156129][-3.7926145 -3.8246398 -3.8352551 -3.8412294 -3.8385141 -3.8436882 -3.8248773 -3.8101654 -3.8099184 -3.8344126 -3.8673036 -3.8482974 -3.7901227 -3.7326031 -3.7160459][-3.7508514 -3.8398325 -3.9155712 -3.9883513 -4.055747 -4.1203113 -4.155705 -4.189343 -4.2088189 -4.2174115 -4.2065706 -4.1412935 -4.0493283 -3.9377828 -3.8281987]]...]
INFO - root - 2017-12-07 09:49:24.325085: step 25910, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.770 sec/batch; 65h:36m:28s remains)
INFO - root - 2017-12-07 09:49:31.879611: step 25920, loss = 0.62, batch loss = 0.55 (10.5 examples/sec; 0.765 sec/batch; 65h:11m:24s remains)
INFO - root - 2017-12-07 09:49:39.649957: step 25930, loss = 0.67, batch loss = 0.60 (10.1 examples/sec; 0.788 sec/batch; 67h:08m:23s remains)
INFO - root - 2017-12-07 09:49:47.336698: step 25940, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.778 sec/batch; 66h:16m:12s remains)
INFO - root - 2017-12-07 09:49:54.963894: step 25950, loss = 0.91, batch loss = 0.83 (10.5 examples/sec; 0.764 sec/batch; 65h:02m:45s remains)
INFO - root - 2017-12-07 09:50:02.541444: step 25960, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.760 sec/batch; 64h:43m:10s remains)
INFO - root - 2017-12-07 09:50:10.021735: step 25970, loss = 0.85, batch loss = 0.77 (10.2 examples/sec; 0.785 sec/batch; 66h:50m:27s remains)
INFO - root - 2017-12-07 09:50:17.881442: step 25980, loss = 0.65, batch loss = 0.58 (10.1 examples/sec; 0.795 sec/batch; 67h:43m:07s remains)
INFO - root - 2017-12-07 09:50:25.654410: step 25990, loss = 0.67, batch loss = 0.60 (10.7 examples/sec; 0.748 sec/batch; 63h:39m:29s remains)
INFO - root - 2017-12-07 09:50:33.374908: step 26000, loss = 0.86, batch loss = 0.79 (10.8 examples/sec; 0.742 sec/batch; 63h:10m:58s remains)
2017-12-07 09:50:33.968640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8881562 -3.8998125 -3.745657 -3.5063992 -3.21478 -2.9719744 -2.810477 -2.6692052 -2.541729 -2.4201548 -2.2742054 -2.2264011 -2.211906 -2.2248604 -2.2386215][-4.0757732 -4.1442513 -4.0613918 -3.9160843 -3.6997514 -3.4920864 -3.3108814 -3.0852349 -2.8846426 -2.7518051 -2.6313472 -2.6403854 -2.6719909 -2.7102327 -2.718972][-3.9795687 -4.1000357 -4.0817623 -4.0503426 -3.9569428 -3.8434632 -3.6707795 -3.3954463 -3.1488948 -3.0156932 -2.9575303 -3.0639596 -3.2258739 -3.3885188 -3.4565372][-3.8580861 -3.9689887 -3.9446752 -3.950573 -3.9346881 -3.9203675 -3.831471 -3.6338396 -3.412993 -3.2452688 -3.1525095 -3.23702 -3.4397612 -3.6963387 -3.86678][-3.8081667 -3.8678174 -3.7617273 -3.6730156 -3.6094995 -3.636749 -3.6563535 -3.6520767 -3.5916126 -3.453887 -3.3213034 -3.3050532 -3.4156146 -3.6194553 -3.7827194][-3.9378011 -3.9187417 -3.704421 -3.4497705 -3.1880388 -3.0614119 -3.0241361 -3.1536694 -3.334415 -3.3917141 -3.3678663 -3.321692 -3.3191829 -3.3637419 -3.4428444][-4.1842418 -4.082428 -3.8208025 -3.4993043 -3.0709052 -2.6462421 -2.26226 -2.1968257 -2.4405055 -2.7525887 -3.0271764 -3.1689639 -3.230058 -3.2332826 -3.2754841][-4.11695 -3.9259543 -3.7173874 -3.5134416 -3.11308 -2.5563712 -1.9123027 -1.5660915 -1.6598604 -2.0283172 -2.4850123 -2.8209209 -3.0612893 -3.2268605 -3.3758576][-3.783221 -3.4636133 -3.2882872 -3.2364361 -3.0212102 -2.6657791 -2.177706 -1.821806 -1.76122 -1.9789259 -2.312933 -2.5937552 -2.8915365 -3.2375755 -3.5541029][-3.5444722 -3.1789589 -3.0422497 -3.0688899 -2.9944012 -2.9253454 -2.8076339 -2.6724126 -2.5896292 -2.6238077 -2.6696653 -2.7160053 -2.9160967 -3.27277 -3.6230247][-3.4411986 -3.1291256 -3.0768566 -3.1555891 -3.1111145 -3.1597333 -3.3280244 -3.4511375 -3.4601994 -3.3750963 -3.147445 -2.9505072 -2.9962323 -3.2680192 -3.5739229][-3.4113197 -3.1830196 -3.245275 -3.3692188 -3.3072588 -3.3704057 -3.6661747 -3.9516995 -4.0212646 -3.8541517 -3.5112951 -3.2307065 -3.1550944 -3.2722559 -3.4520683][-3.6184709 -3.4920175 -3.6702826 -3.809283 -3.7125363 -3.745146 -4.03799 -4.3675218 -4.4415994 -4.2276311 -3.8533587 -3.5517175 -3.3419662 -3.3013666 -3.4479556][-4.078836 -3.9943526 -4.2253618 -4.3796973 -4.2625151 -4.2165132 -4.3975639 -4.6712418 -4.7542658 -4.6024475 -4.2938786 -4.0030012 -3.680511 -3.5249183 -3.6977088][-4.5501914 -4.4488788 -4.6291327 -4.7861509 -4.7186093 -4.652173 -4.7242446 -4.8843036 -4.9653468 -4.8904853 -4.6899529 -4.4697156 -4.1177673 -3.8776002 -3.9975564]]...]
INFO - root - 2017-12-07 09:50:41.511976: step 26010, loss = 0.93, batch loss = 0.86 (10.6 examples/sec; 0.752 sec/batch; 63h:58m:52s remains)
INFO - root - 2017-12-07 09:50:49.254680: step 26020, loss = 0.75, batch loss = 0.68 (10.3 examples/sec; 0.779 sec/batch; 66h:21m:19s remains)
INFO - root - 2017-12-07 09:50:56.855320: step 26030, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.759 sec/batch; 64h:36m:00s remains)
INFO - root - 2017-12-07 09:51:04.518337: step 26040, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.753 sec/batch; 64h:06m:04s remains)
INFO - root - 2017-12-07 09:51:12.295385: step 26050, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.765 sec/batch; 65h:06m:19s remains)
INFO - root - 2017-12-07 09:51:19.929298: step 26060, loss = 0.90, batch loss = 0.83 (10.4 examples/sec; 0.772 sec/batch; 65h:42m:56s remains)
INFO - root - 2017-12-07 09:51:27.371955: step 26070, loss = 0.78, batch loss = 0.71 (10.7 examples/sec; 0.750 sec/batch; 63h:49m:43s remains)
INFO - root - 2017-12-07 09:51:35.097136: step 26080, loss = 0.65, batch loss = 0.58 (10.4 examples/sec; 0.769 sec/batch; 65h:27m:30s remains)
INFO - root - 2017-12-07 09:51:42.786326: step 26090, loss = 0.74, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 66h:25m:08s remains)
INFO - root - 2017-12-07 09:51:50.550831: step 26100, loss = 0.99, batch loss = 0.92 (10.2 examples/sec; 0.783 sec/batch; 66h:37m:49s remains)
2017-12-07 09:51:51.138070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5796297 -2.45001 -2.3104444 -2.2257009 -2.2578406 -2.2778192 -2.1677523 -2.0347185 -1.9459705 -1.724987 -1.4118936 -1.2034252 -1.0104918 -0.72415018 -0.62153244][-1.9176993 -1.9606545 -1.8959522 -1.8669755 -2.0129724 -2.221365 -2.3238144 -2.3548422 -2.3219681 -2.0943644 -1.7450831 -1.4548805 -1.2322228 -0.9522562 -0.7407937][-1.2138958 -1.4769511 -1.6142373 -1.7646182 -2.0467629 -2.3378513 -2.5296295 -2.6738491 -2.743156 -2.6211696 -2.3560548 -2.1116402 -2.051857 -2.0138633 -1.8500361][-0.91685414 -1.1063566 -1.3080611 -1.6458886 -2.0244734 -2.2417212 -2.3345773 -2.4127722 -2.4367213 -2.385747 -2.2525408 -2.1528645 -2.3000662 -2.545125 -2.5163207][-1.3170002 -1.1106846 -0.98225355 -1.1496701 -1.4164751 -1.5155561 -1.6199601 -1.7369814 -1.7417634 -1.7585771 -1.8107033 -1.9072609 -2.1817038 -2.5166197 -2.5512788][-2.4367733 -1.903549 -1.3088102 -0.98612809 -0.82948256 -0.67864513 -0.79856968 -0.97954011 -0.92007065 -0.90572143 -1.1049719 -1.41872 -1.8195426 -2.1992981 -2.3265228][-3.1544363 -2.7096915 -2.0052779 -1.2752707 -0.577518 -0.094845772 -0.13554859 -0.32520056 -0.21092415 -0.18580103 -0.46089506 -0.85643911 -1.2285299 -1.5718656 -1.7697728][-2.8743443 -2.7911224 -2.3714104 -1.5787756 -0.60695028 0.0750761 0.1194272 -0.041257858 0.0022964478 -0.13579798 -0.56492734 -0.94180155 -1.1086214 -1.2478006 -1.3610737][-2.463686 -2.5864928 -2.4049523 -1.8066041 -1.0213311 -0.47768331 -0.4725256 -0.64718843 -0.72218132 -0.94163251 -1.4208229 -1.7652214 -1.7712471 -1.6949253 -1.6923299][-2.4838605 -2.7778807 -2.7111993 -2.2509205 -1.7160614 -1.4356208 -1.4610734 -1.4828796 -1.4233689 -1.5263979 -1.9474308 -2.3533709 -2.5178797 -2.5796995 -2.6932683][-2.6601727 -3.0981331 -3.1624603 -2.9024911 -2.6160157 -2.5520797 -2.5920448 -2.4690967 -2.1883841 -2.01692 -2.1407955 -2.4054363 -2.6526389 -2.8997974 -3.2250032][-2.5416782 -2.8561368 -2.9721074 -2.9800186 -3.062449 -3.2557161 -3.3637137 -3.2127628 -2.8295412 -2.4626579 -2.2329183 -2.1096377 -2.064615 -2.1704948 -2.586925][-2.5741193 -2.5566695 -2.4075739 -2.3666947 -2.5881486 -2.888922 -3.0358076 -2.9265838 -2.6532178 -2.4124184 -2.2354865 -2.0289876 -1.7712297 -1.6557584 -2.0022843][-3.31561 -3.0078154 -2.5117111 -2.1976321 -2.2925155 -2.579143 -2.7775164 -2.7658029 -2.6343818 -2.5479212 -2.5299058 -2.474267 -2.3209553 -2.2434011 -2.5119882][-4.3413262 -4.0444131 -3.5105295 -3.0855675 -3.0157905 -3.1566062 -3.2623949 -3.249893 -3.1934323 -3.1905897 -3.2371302 -3.2658048 -3.2376347 -3.2758112 -3.5052576]]...]
INFO - root - 2017-12-07 09:51:58.774582: step 26110, loss = 0.71, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 66h:33m:57s remains)
INFO - root - 2017-12-07 09:52:06.486692: step 26120, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.764 sec/batch; 64h:59m:01s remains)
INFO - root - 2017-12-07 09:52:14.174392: step 26130, loss = 0.64, batch loss = 0.57 (10.6 examples/sec; 0.757 sec/batch; 64h:23m:52s remains)
INFO - root - 2017-12-07 09:52:21.777707: step 26140, loss = 0.73, batch loss = 0.65 (10.7 examples/sec; 0.751 sec/batch; 63h:55m:00s remains)
INFO - root - 2017-12-07 09:52:29.367252: step 26150, loss = 0.80, batch loss = 0.73 (10.4 examples/sec; 0.766 sec/batch; 65h:12m:58s remains)
INFO - root - 2017-12-07 09:52:37.060284: step 26160, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 65h:15m:51s remains)
INFO - root - 2017-12-07 09:52:44.636502: step 26170, loss = 0.94, batch loss = 0.87 (10.3 examples/sec; 0.777 sec/batch; 66h:07m:58s remains)
INFO - root - 2017-12-07 09:52:52.370868: step 26180, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.774 sec/batch; 65h:51m:07s remains)
INFO - root - 2017-12-07 09:53:00.140509: step 26190, loss = 0.59, batch loss = 0.52 (9.8 examples/sec; 0.812 sec/batch; 69h:06m:38s remains)
INFO - root - 2017-12-07 09:53:07.847984: step 26200, loss = 0.77, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 64h:05m:59s remains)
2017-12-07 09:53:08.484834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6826935 -2.7883453 -2.8908255 -3.096971 -3.3295584 -3.4001932 -3.4578319 -3.5802085 -3.5975661 -3.5083809 -3.4783576 -3.4583383 -3.4703243 -3.4809175 -3.3914545][-2.5406408 -2.6000252 -2.631654 -2.82764 -3.1369457 -3.2493927 -3.3710938 -3.6441488 -3.7914813 -3.7582321 -3.7904539 -3.779773 -3.7792583 -3.7804079 -3.6541357][-2.1574244 -2.0952668 -2.0186706 -2.1610575 -2.5447927 -2.775744 -3.021852 -3.4851685 -3.8291137 -3.9099507 -3.9882259 -3.972158 -3.9635928 -3.9879239 -3.8888216][-1.6761243 -1.4053254 -1.1837516 -1.2023129 -1.5680561 -1.8756008 -2.1985111 -2.7754817 -3.3084826 -3.5705996 -3.7512927 -3.8322458 -3.9285178 -4.0583482 -4.0608883][-1.3698161 -0.90820217 -0.51370645 -0.30698156 -0.48661685 -0.70560026 -0.94177032 -1.4674129 -2.0948591 -2.5051098 -2.8128846 -3.1626697 -3.6147859 -3.9854717 -4.1606965][-1.2423844 -0.74107742 -0.31874704 -0.011962891 0.022980213 0.012689114 -0.021419048 -0.39038134 -1.0362618 -1.488533 -1.7901175 -2.3025379 -3.0617671 -3.6092148 -3.9323621][-1.4447331 -0.89978981 -0.41371441 -0.04490757 0.20427227 0.47440815 0.75080729 0.65126944 -0.0034074783 -0.596581 -0.98316193 -1.6026797 -2.5118937 -3.1036775 -3.491919][-1.7827461 -1.2417235 -0.72044253 -0.3420167 0.004983902 0.41910458 0.886714 1.0284095 0.51859 -0.064534664 -0.42862415 -0.990422 -1.8653417 -2.4354334 -2.8810842][-2.1143136 -1.5402191 -0.96021461 -0.61994052 -0.34157896 -0.032461166 0.27999687 0.36623144 0.036112309 -0.33663321 -0.51278734 -0.818553 -1.4265101 -1.8619046 -2.3170443][-2.603344 -2.0618923 -1.4344223 -1.0240295 -0.71437693 -0.48393083 -0.39137363 -0.5213275 -0.83069372 -1.0798109 -1.1608162 -1.2475398 -1.5368888 -1.7708533 -2.1284626][-3.014663 -2.6928957 -2.2536929 -1.9269361 -1.6935046 -1.5714269 -1.6319046 -1.8558104 -2.0771067 -2.1776242 -2.1426697 -2.0346866 -2.0270505 -2.0654614 -2.2821119][-3.2983057 -3.2691708 -3.1103086 -2.9040701 -2.6912522 -2.5176926 -2.4962978 -2.6278372 -2.7652283 -2.8191371 -2.7817583 -2.6281333 -2.473707 -2.4316583 -2.5720859][-3.2826552 -3.4502616 -3.557153 -3.5433972 -3.4229405 -3.2202444 -3.0522857 -3.0002403 -3.0073972 -3.0324645 -3.0555978 -2.9666035 -2.814487 -2.7661762 -2.8586988][-3.1223929 -3.2720838 -3.4302247 -3.5449862 -3.5904765 -3.5326941 -3.4250305 -3.3217113 -3.2255335 -3.1747479 -3.1739521 -3.1112728 -2.9789214 -2.9132185 -2.9415216][-3.0112565 -3.1035271 -3.17911 -3.2228465 -3.2303467 -3.1993868 -3.1482809 -3.0766468 -2.9939597 -2.9547839 -2.9640975 -2.931874 -2.8439326 -2.7914519 -2.799365]]...]
INFO - root - 2017-12-07 09:53:16.055901: step 26210, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.759 sec/batch; 64h:36m:07s remains)
INFO - root - 2017-12-07 09:53:23.781119: step 26220, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.769 sec/batch; 65h:27m:55s remains)
INFO - root - 2017-12-07 09:53:31.477427: step 26230, loss = 0.79, batch loss = 0.72 (10.0 examples/sec; 0.799 sec/batch; 67h:57m:31s remains)
INFO - root - 2017-12-07 09:53:39.038350: step 26240, loss = 0.86, batch loss = 0.79 (10.4 examples/sec; 0.772 sec/batch; 65h:38m:01s remains)
INFO - root - 2017-12-07 09:53:46.586356: step 26250, loss = 0.65, batch loss = 0.58 (10.7 examples/sec; 0.751 sec/batch; 63h:52m:35s remains)
INFO - root - 2017-12-07 09:53:54.267235: step 26260, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.777 sec/batch; 66h:04m:45s remains)
INFO - root - 2017-12-07 09:54:01.747474: step 26270, loss = 1.02, batch loss = 0.95 (10.2 examples/sec; 0.788 sec/batch; 67h:01m:14s remains)
INFO - root - 2017-12-07 09:54:09.479955: step 26280, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.760 sec/batch; 64h:38m:45s remains)
INFO - root - 2017-12-07 09:54:17.146642: step 26290, loss = 0.98, batch loss = 0.91 (10.6 examples/sec; 0.753 sec/batch; 64h:02m:26s remains)
INFO - root - 2017-12-07 09:54:24.820130: step 26300, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.779 sec/batch; 66h:14m:57s remains)
2017-12-07 09:54:25.443082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8304806 -3.1108892 -2.5535545 -1.7725568 -1.7858407 -2.2606332 -2.2097442 -1.6972117 -1.543694 -1.9173303 -2.1162906 -1.5966711 -0.48938346 0.12837744 -0.44181657][-2.0585902 -2.6911626 -2.5355268 -2.1140471 -2.4182289 -2.9333305 -2.7078874 -2.0224409 -1.7556911 -2.0129318 -1.9116175 -0.86376047 0.57482243 1.1885476 0.45384932][-1.7310674 -2.4911442 -2.5839367 -2.4330382 -3.0293288 -3.7073998 -3.5017574 -2.8404286 -2.57453 -2.7416787 -2.475297 -1.228204 0.1094532 0.4339118 -0.34009266][-1.8388505 -2.3321218 -2.2456532 -2.0359187 -2.7929444 -3.7011933 -3.6446779 -3.1394053 -3.1233683 -3.482116 -3.2405813 -2.0365832 -1.0125735 -1.161756 -2.0043519][-2.1814811 -2.291194 -1.9051461 -1.4877546 -2.2401991 -3.2057309 -3.0611696 -2.521781 -2.777056 -3.4881613 -3.4070823 -2.2921789 -1.4502633 -1.9233849 -2.897953][-2.6958237 -2.3660052 -1.6082332 -0.81362152 -1.2291565 -1.86274 -1.2535181 -0.44434929 -0.99045944 -2.2437923 -2.4881761 -1.5805836 -1.0094025 -1.7621825 -2.8789625][-3.6579344 -2.9771895 -1.9090569 -0.73998666 -0.58804941 -0.53923416 0.82817459 2.2042947 1.5282516 -0.2858448 -0.99391627 -0.55645418 -0.43094826 -1.3419495 -2.3906393][-4.4607992 -3.7655272 -2.7506347 -1.6977701 -1.3716853 -0.91406417 0.89972639 2.7739873 2.3990593 0.70801878 -0.10234594 -0.069140911 -0.35274982 -1.2861969 -2.0666103][-4.658833 -4.1017694 -3.3517666 -2.6859064 -2.4944141 -2.0718453 -0.56099582 1.0155602 0.897902 -0.19932127 -0.65610218 -0.70052123 -1.1561007 -1.9982357 -2.560082][-4.7857504 -4.3429184 -3.8384798 -3.5609896 -3.4629302 -3.0749593 -1.9797034 -0.8882277 -1.0396729 -1.8099871 -2.0284026 -2.1528616 -2.6814432 -3.3509445 -3.6489408][-4.7130904 -4.2784424 -3.9628053 -3.9915535 -3.9564061 -3.6205106 -2.9111085 -2.2610986 -2.4851139 -3.0312493 -3.1418281 -3.3560991 -3.9146705 -4.4057112 -4.4790263][-4.3030758 -3.9058456 -3.7654836 -3.9872193 -3.9720809 -3.7005668 -3.3195746 -3.0084081 -3.1855235 -3.4185247 -3.3064103 -3.383337 -3.7740171 -4.1383719 -4.2716227][-3.8162096 -3.563601 -3.5638018 -3.8600938 -3.8915148 -3.7148447 -3.4967897 -3.3015656 -3.2920556 -3.1948736 -2.8614016 -2.7272279 -2.8611479 -3.0827801 -3.3358517][-3.2545135 -3.1661956 -3.2529392 -3.4966049 -3.5579245 -3.4932008 -3.395323 -3.2657337 -3.1884527 -2.9898396 -2.6316869 -2.387094 -2.3111079 -2.359452 -2.5838656][-3.0060358 -3.0030808 -3.1096716 -3.2578061 -3.3206191 -3.3329072 -3.301548 -3.2036266 -3.1347289 -3.0225859 -2.8318746 -2.6157212 -2.4299245 -2.340198 -2.4249575]]...]
INFO - root - 2017-12-07 09:54:33.025981: step 26310, loss = 0.97, batch loss = 0.90 (10.3 examples/sec; 0.776 sec/batch; 66h:01m:31s remains)
INFO - root - 2017-12-07 09:54:40.538053: step 26320, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.781 sec/batch; 66h:24m:00s remains)
INFO - root - 2017-12-07 09:54:48.212614: step 26330, loss = 0.88, batch loss = 0.80 (10.2 examples/sec; 0.788 sec/batch; 67h:01m:55s remains)
INFO - root - 2017-12-07 09:54:55.825775: step 26340, loss = 0.71, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 64h:26m:37s remains)
INFO - root - 2017-12-07 09:55:03.507757: step 26350, loss = 0.73, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 65h:15m:44s remains)
INFO - root - 2017-12-07 09:55:11.314456: step 26360, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.781 sec/batch; 66h:23m:38s remains)
INFO - root - 2017-12-07 09:55:18.860066: step 26370, loss = 0.85, batch loss = 0.78 (10.2 examples/sec; 0.788 sec/batch; 66h:58m:44s remains)
INFO - root - 2017-12-07 09:55:26.427220: step 26380, loss = 0.92, batch loss = 0.84 (10.4 examples/sec; 0.767 sec/batch; 65h:14m:51s remains)
INFO - root - 2017-12-07 09:55:34.109619: step 26390, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.782 sec/batch; 66h:28m:29s remains)
INFO - root - 2017-12-07 09:55:41.789581: step 26400, loss = 0.85, batch loss = 0.77 (10.4 examples/sec; 0.771 sec/batch; 65h:34m:39s remains)
2017-12-07 09:55:42.458573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.422251 -3.5169439 -3.6219437 -3.6505437 -3.5913877 -3.4684203 -3.4118912 -3.4977455 -3.5726857 -3.5304499 -3.4691286 -3.4509871 -3.446661 -3.5144312 -3.586807][-3.3032725 -3.4551382 -3.6466179 -3.7561555 -3.7584374 -3.67519 -3.6450679 -3.741996 -3.7976861 -3.7473819 -3.6838176 -3.6246457 -3.5371976 -3.5014749 -3.5064061][-3.2333779 -3.386682 -3.5735013 -3.6869802 -3.7261293 -3.7001467 -3.7112007 -3.7893312 -3.8034222 -3.7561741 -3.7203126 -3.6477265 -3.4920843 -3.3617916 -3.3323317][-3.3407307 -3.4384794 -3.5535283 -3.6251719 -3.6531482 -3.6300516 -3.6345239 -3.6895065 -3.6919179 -3.6733136 -3.6828196 -3.6282825 -3.4543414 -3.2748559 -3.1948538][-3.5061665 -3.5166531 -3.5140865 -3.4980347 -3.448113 -3.3680875 -3.3489971 -3.4151864 -3.4675744 -3.5147645 -3.580883 -3.5675192 -3.417057 -3.223634 -3.0772038][-3.6540995 -3.5810986 -3.4556291 -3.320338 -3.1658387 -3.0343561 -3.0441432 -3.1722112 -3.2951179 -3.4007931 -3.4944141 -3.4793565 -3.3052068 -3.066941 -2.850234][-3.7290902 -3.5712311 -3.3430986 -3.1064205 -2.8599515 -2.6876888 -2.7604437 -2.9848609 -3.1903565 -3.3398142 -3.4420137 -3.4120669 -3.2120931 -2.94978 -2.7277589][-3.6538467 -3.4447412 -3.1667733 -2.8908489 -2.6100712 -2.4318857 -2.5717952 -2.871923 -3.1224279 -3.2786598 -3.3831358 -3.3717196 -3.1972837 -2.9677269 -2.7965496][-3.5587659 -3.3559289 -3.0878725 -2.8297458 -2.5976527 -2.4923496 -2.6811283 -2.9890802 -3.212153 -3.3277133 -3.3974621 -3.3995619 -3.2656353 -3.077354 -2.9518528][-3.5668123 -3.4098494 -3.1908112 -2.9775009 -2.8221424 -2.8086014 -3.013206 -3.2526574 -3.3895059 -3.4496474 -3.4872668 -3.5115991 -3.4490559 -3.3274906 -3.2338965][-3.6157038 -3.5004592 -3.3453476 -3.1885974 -3.0998759 -3.1581903 -3.3372464 -3.4616585 -3.4938407 -3.5095098 -3.5431492 -3.5947909 -3.5977778 -3.5442545 -3.4911885][-3.6560931 -3.5738606 -3.4718351 -3.3620946 -3.3251204 -3.4264274 -3.5587809 -3.5786431 -3.53475 -3.5215278 -3.5546296 -3.6051812 -3.626874 -3.62081 -3.6176984][-3.6989927 -3.6664305 -3.6190364 -3.5427866 -3.5264783 -3.628737 -3.7014265 -3.6442471 -3.5596426 -3.5318298 -3.5554218 -3.5899577 -3.6118917 -3.6316533 -3.6522238][-3.635077 -3.6661954 -3.6801438 -3.6343567 -3.6175539 -3.6841061 -3.7026818 -3.6134892 -3.5264037 -3.4930182 -3.4929898 -3.5036204 -3.5240421 -3.5635395 -3.5945518][-3.548723 -3.6105533 -3.6628244 -3.6325369 -3.5959461 -3.6058347 -3.579308 -3.4931526 -3.4266164 -3.3870049 -3.3575697 -3.3354504 -3.3411989 -3.3910072 -3.4230671]]...]
INFO - root - 2017-12-07 09:55:50.121960: step 26410, loss = 0.76, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 65h:40m:23s remains)
INFO - root - 2017-12-07 09:55:57.758853: step 26420, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.762 sec/batch; 64h:47m:25s remains)
INFO - root - 2017-12-07 09:56:05.385512: step 26430, loss = 1.09, batch loss = 1.02 (10.4 examples/sec; 0.772 sec/batch; 65h:38m:12s remains)
INFO - root - 2017-12-07 09:56:13.045913: step 26440, loss = 0.76, batch loss = 0.69 (10.1 examples/sec; 0.788 sec/batch; 67h:00m:44s remains)
INFO - root - 2017-12-07 09:56:20.641329: step 26450, loss = 0.64, batch loss = 0.57 (10.5 examples/sec; 0.759 sec/batch; 64h:32m:21s remains)
INFO - root - 2017-12-07 09:56:28.206076: step 26460, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.775 sec/batch; 65h:50m:47s remains)
INFO - root - 2017-12-07 09:56:35.634533: step 26470, loss = 0.66, batch loss = 0.59 (10.4 examples/sec; 0.766 sec/batch; 65h:05m:51s remains)
INFO - root - 2017-12-07 09:56:43.272160: step 26480, loss = 0.93, batch loss = 0.86 (10.3 examples/sec; 0.777 sec/batch; 66h:01m:07s remains)
INFO - root - 2017-12-07 09:56:50.894225: step 26490, loss = 0.63, batch loss = 0.56 (10.7 examples/sec; 0.745 sec/batch; 63h:21m:42s remains)
INFO - root - 2017-12-07 09:56:58.555002: step 26500, loss = 0.69, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 64h:54m:36s remains)
2017-12-07 09:56:59.170810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5795131 -3.2698321 -2.8447938 -2.7405865 -2.8370194 -3.1160626 -3.2348061 -2.8533616 -2.4400184 -2.3074789 -2.396246 -2.7330766 -3.2047067 -3.5224032 -3.6066914][-3.9930558 -3.4644055 -2.9372091 -2.8844671 -2.9896855 -3.2329903 -3.3196683 -2.8624628 -2.4435284 -2.4396253 -2.7223485 -3.1444535 -3.5607882 -3.8002706 -3.8230846][-4.31232 -3.7815285 -3.3059256 -3.2364676 -3.1844261 -3.2460644 -3.2299891 -2.7909122 -2.4731212 -2.6704302 -3.1333184 -3.5842147 -3.9154956 -4.0950627 -4.0594544][-4.2566175 -3.7639158 -3.2955678 -3.1290798 -3.0250177 -3.1006639 -3.1158977 -2.7564945 -2.5230677 -2.8107808 -3.369951 -3.8327727 -4.0862184 -4.1449003 -3.9824729][-3.8295026 -3.4684632 -3.0850706 -2.9677517 -3.0219681 -3.1895146 -3.1667733 -2.7783551 -2.5048134 -2.7605045 -3.3670855 -3.8477476 -3.9688697 -3.7797527 -3.4122279][-3.4310467 -3.2066264 -2.9283686 -2.8568692 -2.9626689 -2.9255915 -2.499383 -1.8571401 -1.490819 -1.6728916 -2.1856372 -2.4939508 -2.3647766 -1.9368775 -1.4879222][-3.1910114 -2.9297695 -2.521024 -2.2169974 -2.0454788 -1.5546174 -0.68808937 0.036041737 0.18252516 -0.16866207 -0.61919832 -0.77681708 -0.57368731 -0.23724461 -0.12699032][-2.5036561 -2.3227839 -1.9482656 -1.5491884 -1.1985071 -0.53316259 0.27822065 0.601419 0.19686222 -0.50239229 -1.0612507 -1.3410759 -1.4296746 -1.5045748 -1.7828422][-2.0200725 -2.1902053 -2.2752309 -2.2540927 -2.1563842 -1.812331 -1.468977 -1.5645003 -2.042057 -2.5849075 -2.9469995 -3.1488824 -3.3058445 -3.490283 -3.7685332][-2.8673825 -3.1730406 -3.5074027 -3.718714 -3.77696 -3.6669269 -3.5911365 -3.7334068 -3.9591441 -4.1245747 -4.1618643 -4.1599822 -4.1650534 -4.1852179 -4.2314477][-3.5716314 -3.8490007 -4.2037191 -4.4320545 -4.4750791 -4.3888359 -4.3037758 -4.3355989 -4.3929548 -4.4007783 -4.3258867 -4.248857 -4.1636066 -4.0731363 -3.9783616][-3.278152 -3.4941926 -3.7934663 -4.0275412 -4.1143589 -4.1038218 -4.0804892 -4.1234107 -4.2053089 -4.2030849 -4.0697274 -3.9250441 -3.805516 -3.713495 -3.6251619][-2.7433672 -2.9133143 -3.1258731 -3.3172774 -3.4699526 -3.621305 -3.7394419 -3.8672042 -4.0925255 -4.2304206 -4.1614265 -4.0406175 -3.9495134 -3.9253287 -3.9306879][-2.7212772 -2.8071933 -2.9306107 -3.0730548 -3.2869287 -3.5838015 -3.7943079 -3.8847218 -4.0909219 -4.2973075 -4.3137941 -4.2704077 -4.2077065 -4.2175994 -4.2972412][-3.254344 -3.1786251 -3.166755 -3.1916733 -3.3267627 -3.5810075 -3.7305655 -3.7029984 -3.8006606 -4.0023718 -4.0613594 -4.049201 -3.9438653 -3.9108462 -4.0268531]]...]
INFO - root - 2017-12-07 09:57:06.737375: step 26510, loss = 0.82, batch loss = 0.74 (10.5 examples/sec; 0.759 sec/batch; 64h:28m:57s remains)
INFO - root - 2017-12-07 09:57:14.339261: step 26520, loss = 0.70, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 65h:50m:26s remains)
INFO - root - 2017-12-07 09:57:21.969390: step 26530, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.759 sec/batch; 64h:29m:37s remains)
INFO - root - 2017-12-07 09:57:29.542500: step 26540, loss = 0.73, batch loss = 0.66 (9.9 examples/sec; 0.805 sec/batch; 68h:27m:23s remains)
INFO - root - 2017-12-07 09:57:37.149989: step 26550, loss = 0.75, batch loss = 0.68 (10.8 examples/sec; 0.744 sec/batch; 63h:12m:17s remains)
INFO - root - 2017-12-07 09:57:44.771361: step 26560, loss = 0.68, batch loss = 0.61 (10.4 examples/sec; 0.769 sec/batch; 65h:20m:06s remains)
INFO - root - 2017-12-07 09:57:52.204179: step 26570, loss = 0.91, batch loss = 0.84 (10.5 examples/sec; 0.759 sec/batch; 64h:32m:18s remains)
INFO - root - 2017-12-07 09:57:59.938345: step 26580, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.781 sec/batch; 66h:21m:40s remains)
INFO - root - 2017-12-07 09:58:07.574860: step 26590, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.769 sec/batch; 65h:18m:52s remains)
INFO - root - 2017-12-07 09:58:15.212865: step 26600, loss = 0.87, batch loss = 0.79 (10.7 examples/sec; 0.749 sec/batch; 63h:39m:44s remains)
2017-12-07 09:58:15.825305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9570405 -4.0401545 -4.1455126 -4.2348685 -4.2874331 -4.2396045 -4.0931578 -3.9703455 -3.9214549 -3.904901 -3.8700628 -3.8232231 -3.7608478 -3.6786213 -3.6571784][-3.9297514 -4.0013633 -4.084156 -4.1483431 -4.1979556 -4.143796 -3.9759204 -3.844624 -3.7979951 -3.7792802 -3.751471 -3.7267215 -3.6811264 -3.6216023 -3.6610994][-3.6427932 -3.65492 -3.6832709 -3.6964326 -3.7389178 -3.6932709 -3.5017881 -3.3666883 -3.3412526 -3.3486867 -3.3774173 -3.4271526 -3.4259577 -3.4173133 -3.5535824][-3.1995242 -3.1500993 -3.1304312 -3.0989809 -3.1512272 -3.1564059 -2.9930592 -2.86207 -2.8646865 -2.8975332 -3.0010519 -3.1396427 -3.1418581 -3.1394722 -3.3321037][-2.883666 -2.7947006 -2.7442703 -2.6622293 -2.6907501 -2.7400455 -2.6387634 -2.5569911 -2.5992341 -2.6328118 -2.7908461 -3.0002651 -2.9973421 -3.0149591 -3.2559378][-2.6166158 -2.5241928 -2.492697 -2.3732581 -2.2895193 -2.2268834 -2.0891972 -2.094784 -2.2611618 -2.3127668 -2.4079232 -2.5304618 -2.4930391 -2.6020422 -2.9656091][-2.4893718 -2.3630843 -2.3273733 -2.2016883 -2.0350363 -1.7671938 -1.4218168 -1.4345663 -1.7566061 -1.9514825 -2.029263 -1.9555886 -1.731787 -1.7821825 -2.1544988][-2.5108552 -2.3481734 -2.2947083 -2.2462215 -2.16418 -1.8667409 -1.3980618 -1.2988458 -1.6201203 -1.9358103 -2.1096213 -1.9803128 -1.6228366 -1.4875534 -1.6567011][-2.4789028 -2.3872881 -2.3709404 -2.41432 -2.4797144 -2.3360772 -2.0123386 -1.9346621 -2.1370618 -2.4052124 -2.6084213 -2.5477192 -2.2816443 -2.0888669 -2.0486321][-2.3213158 -2.4477236 -2.5991769 -2.7819238 -2.9856639 -2.9889684 -2.8513842 -2.8530703 -2.9639378 -3.1252279 -3.2723053 -3.2503643 -3.0891964 -2.9313774 -2.8173659][-2.1021111 -2.5069532 -2.877002 -3.2049384 -3.4891157 -3.5621502 -3.5334368 -3.55859 -3.5828433 -3.613781 -3.6336107 -3.5905626 -3.5220957 -3.4791555 -3.4527247][-2.2079561 -2.6765685 -3.0522494 -3.3856568 -3.6645441 -3.7722697 -3.8091843 -3.8498971 -3.8432412 -3.813571 -3.7564583 -3.7137256 -3.7440317 -3.8366497 -3.9442155][-2.7159762 -3.0927672 -3.318707 -3.5365481 -3.7365656 -3.8209219 -3.8603051 -3.8885517 -3.8934236 -3.8898671 -3.8500867 -3.8220158 -3.8690619 -3.9741879 -4.1118464][-3.1706028 -3.3654785 -3.41579 -3.4978473 -3.5998693 -3.6360826 -3.65517 -3.6701159 -3.6867571 -3.7186298 -3.7388566 -3.7783587 -3.8706141 -3.9910455 -4.1134019][-3.4304953 -3.4133008 -3.3098798 -3.2843237 -3.3167942 -3.3293812 -3.3594103 -3.3856544 -3.4091015 -3.4514701 -3.5134602 -3.6169896 -3.7613385 -3.8966384 -3.9894047]]...]
INFO - root - 2017-12-07 09:58:23.394143: step 26610, loss = 0.61, batch loss = 0.54 (10.4 examples/sec; 0.766 sec/batch; 65h:07m:11s remains)
INFO - root - 2017-12-07 09:58:31.048743: step 26620, loss = 0.74, batch loss = 0.67 (10.6 examples/sec; 0.753 sec/batch; 63h:58m:46s remains)
INFO - root - 2017-12-07 09:58:38.670988: step 26630, loss = 0.79, batch loss = 0.72 (10.6 examples/sec; 0.756 sec/batch; 64h:14m:21s remains)
INFO - root - 2017-12-07 09:58:46.260339: step 26640, loss = 0.68, batch loss = 0.60 (10.7 examples/sec; 0.747 sec/batch; 63h:25m:31s remains)
INFO - root - 2017-12-07 09:58:54.001379: step 26650, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.782 sec/batch; 66h:26m:25s remains)
INFO - root - 2017-12-07 09:59:01.726679: step 26660, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.782 sec/batch; 66h:28m:19s remains)
INFO - root - 2017-12-07 09:59:08.975969: step 26670, loss = 0.85, batch loss = 0.78 (10.7 examples/sec; 0.745 sec/batch; 63h:17m:12s remains)
INFO - root - 2017-12-07 09:59:16.594221: step 26680, loss = 0.91, batch loss = 0.83 (10.4 examples/sec; 0.766 sec/batch; 65h:03m:00s remains)
INFO - root - 2017-12-07 09:59:24.203803: step 26690, loss = 1.01, batch loss = 0.94 (10.7 examples/sec; 0.749 sec/batch; 63h:39m:14s remains)
INFO - root - 2017-12-07 09:59:31.817042: step 26700, loss = 0.68, batch loss = 0.60 (10.8 examples/sec; 0.741 sec/batch; 62h:54m:32s remains)
2017-12-07 09:59:32.429479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9729748 -2.058342 -2.1198342 -2.0451319 -2.0204315 -2.141506 -2.276721 -2.2933304 -2.1841834 -2.0478733 -1.9964566 -2.0305798 -2.006084 -1.9320595 -1.8959088][-2.1009586 -2.2697215 -2.42483 -2.3739293 -2.3085508 -2.403996 -2.5356007 -2.5566235 -2.4905396 -2.3888776 -2.4000337 -2.5003076 -2.4441757 -2.2987435 -2.151314][-2.149328 -2.3974819 -2.6795623 -2.6968689 -2.5769916 -2.540427 -2.5435619 -2.4953742 -2.4959025 -2.4767449 -2.5870404 -2.7759647 -2.7239697 -2.542191 -2.2939754][-2.0706515 -2.3250647 -2.7197452 -2.8508167 -2.7376938 -2.583343 -2.4484155 -2.3332555 -2.387125 -2.4305081 -2.5794611 -2.7741132 -2.6688561 -2.4491858 -2.1158519][-1.958622 -2.2011056 -2.6788497 -2.8732543 -2.7008569 -2.3703761 -2.0242634 -1.8369052 -2.0250573 -2.2342219 -2.4605184 -2.6875167 -2.5821843 -2.3512826 -1.965158][-1.7414129 -1.9507303 -2.4515724 -2.6341233 -2.3615725 -1.8752527 -1.2893977 -0.96598577 -1.3357766 -1.8207321 -2.1934412 -2.5048847 -2.4820495 -2.289993 -1.9080794][-1.5310116 -1.6592982 -2.0671687 -2.1403847 -1.7738655 -1.231622 -0.49898624 -0.060187817 -0.672117 -1.4695876 -2.0166037 -2.4332554 -2.4999819 -2.2836418 -1.8722246][-1.281951 -1.3234129 -1.628756 -1.6253636 -1.2815242 -0.8369441 -0.095582962 0.40799141 -0.31472778 -1.2488296 -1.8913329 -2.3587468 -2.4827976 -2.2316823 -1.7923708][-1.0813215 -1.0695636 -1.3636086 -1.441473 -1.310823 -1.1153402 -0.56423068 -0.20887327 -0.78978658 -1.5553112 -2.0678802 -2.4047761 -2.4410269 -2.1046383 -1.6553526][-1.2086592 -1.2196143 -1.5643752 -1.7561996 -1.8033783 -1.7570577 -1.3567121 -1.1293585 -1.5847397 -2.1674132 -2.5698555 -2.791255 -2.7088902 -2.2921834 -1.8207598][-1.6962249 -1.8065257 -2.2219729 -2.4267421 -2.4892654 -2.434319 -2.053196 -1.8276112 -2.1635129 -2.605114 -2.9744606 -3.1897502 -3.1317368 -2.7743034 -2.3330705][-2.2717078 -2.4280517 -2.8535836 -2.9994292 -2.9963512 -2.9267511 -2.5861139 -2.3540261 -2.5451839 -2.794354 -3.0689559 -3.2839558 -3.2878406 -3.0932093 -2.7603457][-2.6206155 -2.7440038 -3.0813229 -3.1236296 -3.0598509 -3.0235782 -2.8289425 -2.6545303 -2.6956642 -2.7219663 -2.8858533 -3.1187084 -3.1803241 -3.1206236 -2.8993258][-2.7075329 -2.7673283 -2.9898796 -2.9838428 -2.9528828 -2.9822412 -2.9330974 -2.8199959 -2.7088392 -2.5580645 -2.647481 -2.8913255 -3.01505 -3.0350809 -2.8857937][-2.5467136 -2.5233097 -2.6366787 -2.6444583 -2.6856666 -2.7690167 -2.8234062 -2.774673 -2.6123686 -2.4155109 -2.4686732 -2.6732612 -2.8037848 -2.8461862 -2.7387176]]...]
INFO - root - 2017-12-07 09:59:40.094227: step 26710, loss = 0.88, batch loss = 0.80 (10.6 examples/sec; 0.757 sec/batch; 64h:18m:18s remains)
INFO - root - 2017-12-07 09:59:47.858947: step 26720, loss = 0.74, batch loss = 0.67 (9.9 examples/sec; 0.805 sec/batch; 68h:23m:00s remains)
INFO - root - 2017-12-07 09:59:55.598013: step 26730, loss = 0.68, batch loss = 0.61 (10.2 examples/sec; 0.783 sec/batch; 66h:29m:57s remains)
INFO - root - 2017-12-07 10:00:03.211492: step 26740, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.771 sec/batch; 65h:26m:36s remains)
INFO - root - 2017-12-07 10:00:10.925487: step 26750, loss = 0.92, batch loss = 0.84 (10.3 examples/sec; 0.779 sec/batch; 66h:07m:18s remains)
INFO - root - 2017-12-07 10:00:18.721936: step 26760, loss = 1.04, batch loss = 0.97 (10.2 examples/sec; 0.788 sec/batch; 66h:53m:53s remains)
INFO - root - 2017-12-07 10:00:26.308363: step 26770, loss = 0.78, batch loss = 0.71 (10.0 examples/sec; 0.799 sec/batch; 67h:52m:36s remains)
INFO - root - 2017-12-07 10:00:33.932134: step 26780, loss = 0.95, batch loss = 0.88 (10.4 examples/sec; 0.766 sec/batch; 65h:04m:50s remains)
INFO - root - 2017-12-07 10:00:41.656599: step 26790, loss = 0.78, batch loss = 0.70 (10.7 examples/sec; 0.748 sec/batch; 63h:31m:31s remains)
INFO - root - 2017-12-07 10:00:49.231634: step 26800, loss = 1.01, batch loss = 0.94 (10.5 examples/sec; 0.758 sec/batch; 64h:23m:35s remains)
2017-12-07 10:00:49.913849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8065412 -3.4903035 -3.0985122 -2.8324742 -2.6552815 -2.4557257 -2.4477472 -2.9583373 -3.4250257 -3.5519338 -3.5498729 -3.4222646 -3.2955093 -3.1946135 -2.9624743][-3.7873478 -3.4844851 -3.1144724 -2.866981 -2.6660366 -2.3351488 -2.1938818 -2.7141666 -3.2034657 -3.2995706 -3.331228 -3.2382784 -3.1041532 -3.0196826 -2.8557262][-3.7578254 -3.4739473 -3.0991049 -2.7912035 -2.5545604 -2.2007914 -2.041635 -2.5723677 -2.9454269 -2.8244114 -2.8466735 -2.8632927 -2.8042285 -2.8215857 -2.7446077][-3.7441707 -3.4503498 -2.978694 -2.4990315 -2.2081366 -1.9954085 -1.9144504 -2.3648362 -2.4545259 -1.9854 -1.9526174 -2.1251218 -2.1710169 -2.3253627 -2.3539629][-3.7381566 -3.3961234 -2.7972708 -2.1258166 -1.8063774 -1.7367864 -1.5942318 -1.7781637 -1.5996549 -0.945369 -0.94916224 -1.2687483 -1.3328135 -1.5316658 -1.6583905][-3.7287455 -3.3476396 -2.6844049 -1.9061556 -1.5008488 -1.3625617 -0.92418242 -0.83707142 -0.80729556 -0.45830822 -0.68830156 -1.0761921 -1.0254788 -1.1153059 -1.2541068][-3.7347691 -3.3835425 -2.8031087 -2.005398 -1.32988 -0.78914809 0.14503908 0.35867262 -0.29300451 -0.69856119 -1.157567 -1.5356214 -1.4319212 -1.4249461 -1.5384388][-3.7125027 -3.3860853 -2.9030838 -2.1343558 -1.2106512 -0.3564558 0.80027533 0.94445467 -0.32509375 -1.1128964 -1.418714 -1.6527479 -1.605804 -1.6434352 -1.8277762][-3.6747782 -3.3323045 -2.8479021 -2.0889983 -1.1071594 -0.37593603 0.27616405 0.087911129 -1.0958018 -1.562238 -1.387219 -1.476028 -1.6462548 -1.8417158 -2.1012352][-3.620213 -3.2387342 -2.7418656 -2.0603192 -1.2114022 -0.81943226 -0.778033 -1.1057169 -1.8163936 -1.8198225 -1.3927693 -1.6274736 -2.1580632 -2.5363984 -2.8028889][-3.5638363 -3.125284 -2.5999954 -2.0571766 -1.5224445 -1.4265161 -1.6253881 -1.799979 -2.045604 -1.9461465 -1.7473438 -2.2105949 -2.8755982 -3.2473588 -3.4057271][-3.5280647 -3.0807796 -2.6032605 -2.2168436 -1.8655779 -1.766572 -1.8607407 -1.8400192 -1.9021678 -2.0653987 -2.3002121 -2.8305168 -3.3530521 -3.5344379 -3.4822767][-3.545963 -3.1524496 -2.7835431 -2.5098395 -2.1719093 -1.8405368 -1.6527081 -1.5386546 -1.6001618 -1.9564481 -2.3572083 -2.7042255 -2.991785 -3.0563374 -2.873713][-3.6097598 -3.2741423 -2.9692502 -2.722146 -2.3192396 -1.7905984 -1.4538417 -1.4092746 -1.5402279 -1.8867919 -2.1413782 -2.1801023 -2.2709956 -2.3355496 -2.1266084][-3.6792967 -3.3813119 -3.0674319 -2.7408371 -2.2178197 -1.6193893 -1.3649123 -1.5231917 -1.7581367 -2.0359173 -2.1076126 -1.9398572 -1.944386 -2.0432532 -1.8544414]]...]
INFO - root - 2017-12-07 10:00:57.598840: step 26810, loss = 0.87, batch loss = 0.79 (10.4 examples/sec; 0.766 sec/batch; 65h:01m:08s remains)
INFO - root - 2017-12-07 10:01:05.327137: step 26820, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.770 sec/batch; 65h:24m:10s remains)
INFO - root - 2017-12-07 10:01:12.963771: step 26830, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 64h:42m:18s remains)
INFO - root - 2017-12-07 10:01:20.564300: step 26840, loss = 0.78, batch loss = 0.71 (10.7 examples/sec; 0.749 sec/batch; 63h:37m:02s remains)
INFO - root - 2017-12-07 10:01:28.206564: step 26850, loss = 0.78, batch loss = 0.71 (11.1 examples/sec; 0.719 sec/batch; 61h:00m:48s remains)
INFO - root - 2017-12-07 10:01:35.848407: step 26860, loss = 0.80, batch loss = 0.73 (10.1 examples/sec; 0.789 sec/batch; 67h:01m:19s remains)
INFO - root - 2017-12-07 10:01:43.246784: step 26870, loss = 0.89, batch loss = 0.82 (10.3 examples/sec; 0.773 sec/batch; 65h:38m:21s remains)
INFO - root - 2017-12-07 10:01:50.906931: step 26880, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.783 sec/batch; 66h:28m:31s remains)
INFO - root - 2017-12-07 10:01:58.597198: step 26890, loss = 0.69, batch loss = 0.62 (10.7 examples/sec; 0.747 sec/batch; 63h:24m:10s remains)
INFO - root - 2017-12-07 10:02:06.337833: step 26900, loss = 0.96, batch loss = 0.89 (10.4 examples/sec; 0.771 sec/batch; 65h:26m:38s remains)
2017-12-07 10:02:06.952754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4638391 -2.4830766 -2.4823616 -2.4922872 -2.5096974 -2.5217118 -2.5238006 -2.5255911 -2.529923 -2.5314496 -2.5263443 -2.5192723 -2.5165808 -2.5237355 -2.5401053][-2.4798946 -2.5286212 -2.5572269 -2.5914435 -2.6240849 -2.6442752 -2.6486115 -2.6413307 -2.6284251 -2.6164494 -2.5998383 -2.585681 -2.5817857 -2.5892758 -2.6065726][-2.517036 -2.5813053 -2.6341555 -2.7028103 -2.7740135 -2.8325982 -2.8688297 -2.8674417 -2.8310168 -2.7830799 -2.7275829 -2.6777859 -2.6389763 -2.6118445 -2.6000619][-2.9266458 -2.997566 -3.0423272 -3.1004589 -3.162591 -3.2093093 -3.222712 -3.1955419 -3.1343148 -3.0739615 -3.0219102 -2.9852257 -2.9516463 -2.9124389 -2.8817573][-3.3031716 -3.3535838 -3.3520319 -3.3353729 -3.3132391 -3.282402 -3.2183266 -3.1345341 -3.0557213 -3.0138941 -3.0071802 -3.0283952 -3.05015 -3.0601053 -3.0703788][-2.9579601 -2.9491596 -2.8855495 -2.7866492 -2.686121 -2.6053629 -2.5142975 -2.4327207 -2.3972723 -2.4157712 -2.4613652 -2.5195971 -2.5657072 -2.6065643 -2.6431465][-2.5313232 -2.4968414 -2.4213223 -2.305619 -2.1921394 -2.1186521 -2.0560071 -2.023829 -2.0611618 -2.1407831 -2.2204764 -2.2892668 -2.3243349 -2.3620212 -2.3857024][-1.8270476 -1.7906117 -1.7302806 -1.6268232 -1.5157478 -1.4400072 -1.3983512 -1.4221232 -1.5538304 -1.704855 -1.8057287 -1.8539879 -1.851053 -1.8636887 -1.8749807][-1.9724786 -1.9555809 -1.956151 -1.9160905 -1.8340886 -1.7474241 -1.6847634 -1.6912959 -1.8076994 -1.94348 -2.0301836 -2.0595171 -2.0386748 -2.0338659 -2.0494924][-3.1585665 -3.1545234 -3.1784208 -3.1681986 -3.1004281 -3.0176415 -2.9628711 -2.9636252 -3.0108175 -3.0755019 -3.127862 -3.1591749 -3.1675074 -3.1728711 -3.2027245][-4.0839562 -4.1429448 -4.2183709 -4.2533112 -4.2039776 -4.1193051 -4.0705614 -4.0904794 -4.12995 -4.1648808 -4.1869211 -4.2000055 -4.21268 -4.2007356 -4.203691][-4.6789804 -4.7888236 -4.912744 -5.0241146 -5.0631132 -5.0363512 -5.0141907 -5.0490041 -5.0845551 -5.0954227 -5.0805583 -5.0520124 -5.0375919 -4.9856334 -4.934566][-4.8924742 -4.9836721 -5.0706182 -5.1722264 -5.2500887 -5.2617359 -5.2382889 -5.2586646 -5.2809997 -5.2726512 -5.2411685 -5.1950359 -5.1657929 -5.1014295 -5.0175886][-4.8239536 -4.865665 -4.8936758 -4.9513016 -5.0342464 -5.0710759 -5.0440712 -5.053071 -5.07312 -5.0537958 -5.0149045 -4.971313 -4.9525056 -4.914825 -4.8352852][-4.7231679 -4.6969447 -4.6585135 -4.6465206 -4.69452 -4.7356739 -4.7157745 -4.7243395 -4.7507811 -4.735673 -4.69909 -4.6671133 -4.6703472 -4.6814718 -4.6375546]]...]
INFO - root - 2017-12-07 10:02:14.655411: step 26910, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.775 sec/batch; 65h:48m:13s remains)
INFO - root - 2017-12-07 10:02:22.371151: step 26920, loss = 0.92, batch loss = 0.84 (10.4 examples/sec; 0.772 sec/batch; 65h:30m:26s remains)
INFO - root - 2017-12-07 10:02:30.013779: step 26930, loss = 1.02, batch loss = 0.95 (10.4 examples/sec; 0.771 sec/batch; 65h:28m:11s remains)
INFO - root - 2017-12-07 10:02:37.780146: step 26940, loss = 0.92, batch loss = 0.85 (10.2 examples/sec; 0.785 sec/batch; 66h:37m:17s remains)
INFO - root - 2017-12-07 10:02:45.655980: step 26950, loss = 0.81, batch loss = 0.74 (10.3 examples/sec; 0.778 sec/batch; 66h:03m:16s remains)
INFO - root - 2017-12-07 10:02:53.253550: step 26960, loss = 0.92, batch loss = 0.85 (10.4 examples/sec; 0.767 sec/batch; 65h:03m:58s remains)
INFO - root - 2017-12-07 10:03:00.657116: step 26970, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.754 sec/batch; 63h:58m:03s remains)
INFO - root - 2017-12-07 10:03:08.332266: step 26980, loss = 0.78, batch loss = 0.70 (10.3 examples/sec; 0.774 sec/batch; 65h:39m:15s remains)
INFO - root - 2017-12-07 10:03:16.084092: step 26990, loss = 0.84, batch loss = 0.76 (10.1 examples/sec; 0.793 sec/batch; 67h:17m:14s remains)
INFO - root - 2017-12-07 10:03:23.692508: step 27000, loss = 0.86, batch loss = 0.79 (10.5 examples/sec; 0.764 sec/batch; 64h:50m:53s remains)
2017-12-07 10:03:24.285934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2376885 -3.1337204 -3.139246 -3.2967095 -3.4529185 -3.35015 -3.0463734 -2.7186079 -2.3878324 -2.2327054 -2.1375506 -1.9885902 -1.9166415 -1.837698 -1.849205][-3.7194085 -3.944659 -4.21295 -4.4318295 -4.50568 -4.2052507 -3.6563535 -3.1876221 -2.8165989 -2.7350988 -2.6934018 -2.491056 -2.326102 -2.0964768 -1.9504352][-4.4065781 -4.9294658 -5.34478 -5.5098624 -5.4507513 -5.0035281 -4.2958241 -3.7931609 -3.4821486 -3.4956684 -3.4646742 -3.144676 -2.7971797 -2.3410568 -2.0257864][-4.5153737 -4.9640203 -5.2853236 -5.3775568 -5.2810297 -4.821363 -4.0789237 -3.6016409 -3.4094491 -3.5289547 -3.5261993 -3.1750364 -2.7671916 -2.2308238 -1.8970692][-3.8692822 -4.0411334 -4.2742014 -4.3458481 -4.1625204 -3.5693882 -2.6678905 -2.1203089 -2.0488896 -2.341902 -2.5183449 -2.31416 -2.0389102 -1.7066107 -1.5775592][-3.0492172 -2.9849672 -3.1844053 -3.2019525 -2.8088608 -1.9292026 -0.6629436 0.12291813 0.082392216 -0.48017049 -0.9521687 -1.0891736 -1.1369207 -1.2056963 -1.3366811][-2.9363961 -2.8129048 -2.8522444 -2.5678015 -1.7311478 -0.43859553 1.2320547 2.256464 2.0374336 1.1264892 0.2597146 -0.33107471 -0.79246593 -1.2405272 -1.5130334][-3.5004091 -3.4652529 -3.3509204 -2.766983 -1.5631123 -0.02826786 1.7626128 2.7833681 2.4773545 1.5232825 0.51103973 -0.32373667 -0.9979353 -1.6101065 -1.879848][-3.8179731 -3.9242969 -3.932297 -3.4843261 -2.3190567 -0.913162 0.5229702 1.2467799 1.0737753 0.56217146 -0.12925816 -0.80675793 -1.387212 -1.9105129 -2.0726194][-3.5162003 -3.8442924 -4.1929255 -4.1178265 -3.2145324 -2.1083591 -1.1170607 -0.62759304 -0.53821206 -0.46315503 -0.60839581 -0.945456 -1.3624921 -1.7689979 -1.8872316][-3.0948873 -3.6044989 -4.1672921 -4.3079634 -3.6228666 -2.7572627 -2.07187 -1.6365862 -1.223968 -0.6648047 -0.37476969 -0.47561455 -0.83241582 -1.2457809 -1.4773476][-3.0059588 -3.5772493 -4.1092896 -4.2058449 -3.6168242 -2.8916154 -2.31068 -1.7736652 -1.1611052 -0.48428631 -0.066586018 -0.084792137 -0.44058824 -0.91328096 -1.2980111][-2.7987361 -3.201355 -3.5699773 -3.638639 -3.2296295 -2.6569533 -2.0986717 -1.4855711 -0.86526227 -0.34543371 -0.084344864 -0.20204687 -0.59551048 -1.1187117 -1.5824487][-2.3946269 -2.4825182 -2.6534586 -2.7570269 -2.575479 -2.238353 -1.8238328 -1.3446224 -0.942065 -0.69430494 -0.68548322 -0.93711543 -1.3572612 -1.8406014 -2.2250643][-2.2553089 -2.1106482 -2.1009486 -2.1569774 -2.0919008 -1.9581394 -1.765094 -1.5764837 -1.5056727 -1.5280545 -1.6687429 -1.9201596 -2.2448313 -2.587914 -2.7862022]]...]
INFO - root - 2017-12-07 10:03:31.901390: step 27010, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.752 sec/batch; 63h:49m:51s remains)
INFO - root - 2017-12-07 10:03:39.535948: step 27020, loss = 0.93, batch loss = 0.86 (10.7 examples/sec; 0.747 sec/batch; 63h:21m:33s remains)
INFO - root - 2017-12-07 10:03:47.169144: step 27030, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.769 sec/batch; 65h:14m:28s remains)
INFO - root - 2017-12-07 10:03:54.924647: step 27040, loss = 0.78, batch loss = 0.71 (10.1 examples/sec; 0.790 sec/batch; 67h:02m:48s remains)
INFO - root - 2017-12-07 10:04:02.611808: step 27050, loss = 0.62, batch loss = 0.55 (10.5 examples/sec; 0.765 sec/batch; 64h:55m:51s remains)
INFO - root - 2017-12-07 10:04:10.359738: step 27060, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.775 sec/batch; 65h:43m:50s remains)
INFO - root - 2017-12-07 10:04:17.760072: step 27070, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.759 sec/batch; 64h:23m:03s remains)
INFO - root - 2017-12-07 10:04:25.357125: step 27080, loss = 0.93, batch loss = 0.86 (10.8 examples/sec; 0.744 sec/batch; 63h:06m:21s remains)
INFO - root - 2017-12-07 10:04:33.069049: step 27090, loss = 0.82, batch loss = 0.75 (10.4 examples/sec; 0.768 sec/batch; 65h:07m:27s remains)
INFO - root - 2017-12-07 10:04:40.688737: step 27100, loss = 0.79, batch loss = 0.72 (10.7 examples/sec; 0.750 sec/batch; 63h:36m:17s remains)
2017-12-07 10:04:41.271712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3396778 -3.3100772 -3.2601948 -3.2420313 -3.2272618 -3.2331326 -3.2777693 -3.2868843 -3.2527289 -3.2551045 -3.2601285 -3.1939547 -3.1276689 -3.0616384 -2.9970884][-3.3638368 -3.2818666 -3.1208084 -3.0065284 -2.9563627 -2.972378 -3.0769165 -3.1332221 -3.0455141 -2.979929 -2.971828 -2.9352725 -2.9486094 -2.9804654 -2.9360397][-3.4133468 -3.31329 -3.0510352 -2.8365116 -2.7398124 -2.7486315 -2.9365127 -3.1165423 -3.0204287 -2.85635 -2.8091969 -2.8121047 -2.944025 -3.1241956 -3.1367397][-3.5594056 -3.4459505 -3.0924611 -2.7911236 -2.6489463 -2.6512933 -2.9118795 -3.1823716 -3.0386977 -2.75323 -2.6909003 -2.7884836 -3.0838957 -3.4510875 -3.6010182][-3.6846666 -3.5368462 -3.1222658 -2.7312946 -2.4782343 -2.4739702 -2.8330693 -3.1595144 -2.9343138 -2.5067728 -2.390543 -2.5563545 -2.9928927 -3.5318544 -3.926553][-3.6184039 -3.5484719 -3.2261209 -2.8519466 -2.455302 -2.3854029 -2.8402681 -3.2486174 -3.0352767 -2.5440831 -2.280813 -2.3159714 -2.6436782 -3.0997581 -3.6264029][-3.7638729 -3.7561979 -3.5408998 -3.1823649 -2.5562162 -2.2949748 -2.7552755 -3.2491145 -3.2272737 -2.9065137 -2.6386967 -2.5251598 -2.6010346 -2.7230453 -3.0887952][-4.1286407 -4.1055894 -3.9081478 -3.4905248 -2.5684156 -1.9963386 -2.2793107 -2.7995265 -3.0674348 -3.1063135 -3.0488665 -2.9677658 -2.9258404 -2.7644436 -2.8866456][-4.1308169 -4.0855784 -3.9354482 -3.537468 -2.4936566 -1.669678 -1.6589673 -2.1119215 -2.5847263 -2.9059842 -3.0978625 -3.2062325 -3.220499 -2.9480517 -2.8945646][-3.5501218 -3.4517884 -3.3790488 -3.1560202 -2.2973957 -1.5045307 -1.3607786 -1.7869325 -2.3902872 -2.8203979 -3.1842935 -3.4850574 -3.5549228 -3.2098627 -2.975563][-3.2010412 -3.0049653 -2.8468351 -2.6802273 -2.067317 -1.4780822 -1.3295062 -1.7289608 -2.3411458 -2.7301979 -3.1840842 -3.6593106 -3.7632465 -3.4310939 -3.1743598][-3.5964262 -3.3119178 -2.8722763 -2.4380956 -1.861619 -1.4666708 -1.4403176 -1.8159544 -2.2430062 -2.3937409 -2.773138 -3.288455 -3.4342461 -3.2787828 -3.2468436][-3.9385157 -3.7370448 -3.1889114 -2.503685 -1.8465495 -1.5634923 -1.6949308 -1.9869103 -2.0902078 -1.9742284 -2.1854303 -2.6371503 -2.8258934 -2.8602681 -3.0597837][-3.935751 -3.9764121 -3.63245 -2.9582758 -2.2179172 -1.8716233 -1.9820833 -2.0530531 -1.810915 -1.5431671 -1.7105706 -2.1818733 -2.507329 -2.7310448 -3.0089605][-3.6267314 -3.9633956 -4.0209632 -3.5823972 -2.8140874 -2.3623927 -2.3900087 -2.3062751 -1.8951511 -1.5624266 -1.6449525 -2.0244176 -2.4111543 -2.7238731 -2.989969]]...]
INFO - root - 2017-12-07 10:04:48.945897: step 27110, loss = 0.73, batch loss = 0.65 (10.6 examples/sec; 0.753 sec/batch; 63h:54m:55s remains)
INFO - root - 2017-12-07 10:04:56.723834: step 27120, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.766 sec/batch; 64h:58m:09s remains)
INFO - root - 2017-12-07 10:05:04.397080: step 27130, loss = 1.05, batch loss = 0.98 (10.5 examples/sec; 0.759 sec/batch; 64h:23m:03s remains)
INFO - root - 2017-12-07 10:05:11.945768: step 27140, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 65h:15m:16s remains)
INFO - root - 2017-12-07 10:05:19.553527: step 27150, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.757 sec/batch; 64h:11m:48s remains)
INFO - root - 2017-12-07 10:05:27.159109: step 27160, loss = 0.59, batch loss = 0.52 (10.4 examples/sec; 0.770 sec/batch; 65h:18m:02s remains)
INFO - root - 2017-12-07 10:05:34.629305: step 27170, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.765 sec/batch; 64h:51m:49s remains)
INFO - root - 2017-12-07 10:05:42.302020: step 27180, loss = 0.60, batch loss = 0.53 (10.4 examples/sec; 0.770 sec/batch; 65h:16m:04s remains)
INFO - root - 2017-12-07 10:05:49.908580: step 27190, loss = 0.81, batch loss = 0.74 (10.0 examples/sec; 0.801 sec/batch; 67h:54m:52s remains)
INFO - root - 2017-12-07 10:05:57.551367: step 27200, loss = 0.71, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:27m:52s remains)
2017-12-07 10:05:58.163574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9409485 -2.9447918 -2.9664998 -3.0321579 -3.1004803 -3.1274409 -3.1850491 -3.2631063 -3.3478863 -3.3902726 -3.3265932 -3.2761245 -3.2610497 -3.2501678 -3.2375927][-2.8042696 -2.8486211 -2.9221697 -3.0178761 -3.1099324 -3.1405964 -3.225244 -3.3473535 -3.5065556 -3.6341157 -3.592494 -3.5082965 -3.4549513 -3.4344254 -3.4416325][-2.7144125 -2.8108547 -2.882905 -2.9094639 -2.9349868 -2.9120302 -2.9720094 -3.0948145 -3.3288448 -3.581104 -3.6187029 -3.5219436 -3.4158216 -3.3806868 -3.4252548][-2.8311257 -2.9514697 -2.9357855 -2.8130722 -2.6936536 -2.5325451 -2.4549003 -2.5012097 -2.8117635 -3.245738 -3.448621 -3.3997703 -3.2437589 -3.1703868 -3.239984][-3.0518656 -3.125545 -2.9748306 -2.7004843 -2.4232967 -2.0524507 -1.6914639 -1.5443671 -1.9181643 -2.61045 -3.1276743 -3.2866032 -3.1664248 -3.0726607 -3.1568456][-3.2498193 -3.27774 -3.0334437 -2.6559749 -2.2185974 -1.5870228 -0.8053894 -0.30177259 -0.66894794 -1.6354816 -2.5259142 -2.9908602 -2.9742291 -2.8875089 -2.9959464][-3.2770457 -3.2837014 -3.0186915 -2.5846367 -1.9968312 -1.1243358 0.066050529 0.9507947 0.61085033 -0.58130169 -1.7894595 -2.5465131 -2.6673613 -2.6228046 -2.7695303][-3.1828055 -3.1476014 -2.8836617 -2.4424973 -1.8103263 -0.87447572 0.48799515 1.5528936 1.2052946 -0.043791294 -1.3210611 -2.2065821 -2.4108825 -2.3790629 -2.514255][-3.092761 -3.0036483 -2.7393165 -2.3692303 -1.9069202 -1.254571 -0.16752148 0.73773146 0.48063278 -0.47769833 -1.4570055 -2.1945364 -2.3427222 -2.198328 -2.18374][-3.0264645 -2.9184477 -2.6476617 -2.342077 -2.1354177 -1.9425886 -1.4032428 -0.86034894 -0.97086406 -1.4395685 -1.897615 -2.3089032 -2.2971857 -1.9656062 -1.728931][-2.8767438 -2.8459332 -2.623338 -2.3721967 -2.3467929 -2.4991903 -2.4083419 -2.1608996 -2.1041243 -2.1233313 -2.148025 -2.2419584 -2.054919 -1.5466506 -1.1145523][-2.6899185 -2.8623362 -2.7626276 -2.5718598 -2.6442149 -2.9800782 -3.1126533 -3.0006642 -2.770637 -2.4688578 -2.2611618 -2.1418569 -1.7837045 -1.1127784 -0.54985881][-2.7993255 -3.1833258 -3.1426272 -2.8926835 -2.9116411 -3.2705145 -3.4813488 -3.4254766 -3.0771217 -2.6279311 -2.3591411 -2.1340382 -1.6690595 -0.97370052 -0.44114327][-3.0518727 -3.4761291 -3.4156299 -3.0779872 -3.003638 -3.2788925 -3.4265161 -3.3333149 -2.9581327 -2.5627837 -2.4003522 -2.2052183 -1.7711761 -1.1870401 -0.7820909][-3.0337293 -3.3678155 -3.333281 -3.0633893 -3.0314403 -3.2414229 -3.2579896 -3.0562701 -2.726938 -2.5504594 -2.61498 -2.5137582 -2.1311209 -1.6433504 -1.2959619]]...]
INFO - root - 2017-12-07 10:06:05.830895: step 27210, loss = 0.78, batch loss = 0.70 (10.4 examples/sec; 0.767 sec/batch; 65h:01m:15s remains)
INFO - root - 2017-12-07 10:06:13.583038: step 27220, loss = 0.84, batch loss = 0.77 (10.3 examples/sec; 0.779 sec/batch; 66h:01m:26s remains)
INFO - root - 2017-12-07 10:06:21.274473: step 27230, loss = 0.71, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 67h:45m:53s remains)
INFO - root - 2017-12-07 10:06:28.847679: step 27240, loss = 0.88, batch loss = 0.81 (10.8 examples/sec; 0.741 sec/batch; 62h:51m:15s remains)
INFO - root - 2017-12-07 10:06:36.422399: step 27250, loss = 0.88, batch loss = 0.80 (10.6 examples/sec; 0.754 sec/batch; 63h:55m:20s remains)
INFO - root - 2017-12-07 10:06:44.020136: step 27260, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.763 sec/batch; 64h:43m:09s remains)
INFO - root - 2017-12-07 10:06:51.359706: step 27270, loss = 0.76, batch loss = 0.68 (10.7 examples/sec; 0.745 sec/batch; 63h:10m:43s remains)
INFO - root - 2017-12-07 10:06:58.969457: step 27280, loss = 0.88, batch loss = 0.81 (10.6 examples/sec; 0.754 sec/batch; 63h:56m:34s remains)
INFO - root - 2017-12-07 10:07:06.708266: step 27290, loss = 0.81, batch loss = 0.73 (10.1 examples/sec; 0.790 sec/batch; 67h:01m:02s remains)
INFO - root - 2017-12-07 10:07:14.474401: step 27300, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.771 sec/batch; 65h:23m:00s remains)
2017-12-07 10:07:15.225814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0540543 -1.1308997 -1.3634369 -1.6490059 -2.0211003 -2.406918 -2.6042297 -2.5904951 -2.4078705 -2.0920632 -1.8472638 -1.8165166 -1.8309538 -1.7571499 -1.8929799][-1.3083434 -1.1144104 -0.97 -0.86300659 -0.98619151 -1.1951087 -1.2379856 -1.1300509 -0.87723994 -0.58799744 -0.53732109 -0.868099 -1.2710333 -1.5676768 -1.9137964][-1.8270185 -1.6273353 -1.3687892 -1.0089841 -0.87570477 -0.89125752 -0.8978498 -0.91786647 -0.83889079 -0.65128207 -0.64688754 -0.95835471 -1.2959583 -1.5875316 -1.9382701][-2.031482 -2.1389282 -2.124485 -1.8736091 -1.7922654 -1.9379666 -2.2101569 -2.5276923 -2.6387916 -2.4237323 -2.1655402 -2.0300319 -1.8982103 -1.8431809 -1.9952962][-2.4606287 -2.912641 -3.0258141 -2.7837391 -2.756443 -3.0308492 -3.4220314 -3.7636712 -3.8184314 -3.4623232 -2.9649448 -2.476959 -2.0716548 -1.8852172 -1.9895809][-2.4916761 -3.212 -3.4606283 -3.3219795 -3.3455715 -3.5004294 -3.5145388 -3.3659029 -3.0950575 -2.674509 -2.25589 -1.8686047 -1.6268024 -1.6539245 -1.9159482][-2.5811708 -3.3426268 -3.6331003 -3.605907 -3.5587997 -3.207603 -2.377526 -1.5156326 -1.0179141 -0.86411548 -0.922673 -1.0003166 -1.1778193 -1.5282788 -1.9447706][-2.66814 -3.2091765 -3.2652817 -3.0727317 -2.7559206 -1.8354316 -0.36799097 0.81536531 1.084372 0.70621872 0.11034155 -0.46929049 -1.0597703 -1.6238382 -2.035821][-1.8551357 -2.1723771 -2.0140574 -1.547678 -0.90457988 0.27269411 1.7166767 2.5673738 2.2122846 1.2766252 0.33500051 -0.54623318 -1.3160729 -1.8650441 -2.1213353][-1.1895881 -1.3581395 -1.1104567 -0.58880258 -0.041701317 0.64646864 1.2581935 1.2996354 0.58810616 -0.26875877 -0.92750907 -1.5253317 -1.9765024 -2.1507752 -2.1051741][-1.4043386 -1.7615147 -1.8045461 -1.6903293 -1.6653037 -1.7058074 -1.8105931 -1.9975467 -2.313112 -2.3799763 -2.23306 -2.158977 -2.1078835 -1.9619038 -1.8214574][-2.0435843 -2.5083957 -2.8760083 -3.1430771 -3.3544073 -3.489697 -3.5315795 -3.4223809 -3.2813787 -2.8022726 -2.214313 -1.8395066 -1.6881332 -1.6348448 -1.6772888][-2.3777902 -2.618876 -2.9544611 -3.2248175 -3.2412968 -3.0228009 -2.7336054 -2.439378 -2.2818 -1.8883553 -1.4514656 -1.2609797 -1.3473866 -1.5694709 -1.7997918][-1.7916849 -1.8687968 -2.1071835 -2.2620974 -1.9709065 -1.3845623 -0.8981843 -0.780844 -1.0408878 -1.0900984 -1.0626936 -1.2286789 -1.5709162 -1.9184415 -2.1134923][-1.143137 -1.240519 -1.502316 -1.6646161 -1.3179567 -0.67345476 -0.21201754 -0.33092117 -0.95613933 -1.3148787 -1.5063367 -1.7789295 -2.1394689 -2.3883319 -2.3890965]]...]
INFO - root - 2017-12-07 10:07:22.720967: step 27310, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.760 sec/batch; 64h:23m:32s remains)
INFO - root - 2017-12-07 10:07:30.188229: step 27320, loss = 0.99, batch loss = 0.91 (10.4 examples/sec; 0.766 sec/batch; 64h:54m:38s remains)
INFO - root - 2017-12-07 10:07:37.994522: step 27330, loss = 0.89, batch loss = 0.82 (9.8 examples/sec; 0.814 sec/batch; 69h:01m:47s remains)
INFO - root - 2017-12-07 10:07:45.713270: step 27340, loss = 0.82, batch loss = 0.75 (10.1 examples/sec; 0.793 sec/batch; 67h:11m:26s remains)
INFO - root - 2017-12-07 10:07:53.337384: step 27350, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.782 sec/batch; 66h:15m:44s remains)
INFO - root - 2017-12-07 10:08:00.913298: step 27360, loss = 0.59, batch loss = 0.52 (10.7 examples/sec; 0.751 sec/batch; 63h:37m:02s remains)
INFO - root - 2017-12-07 10:08:08.263193: step 27370, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.753 sec/batch; 63h:49m:02s remains)
INFO - root - 2017-12-07 10:08:16.033814: step 27380, loss = 0.92, batch loss = 0.84 (10.3 examples/sec; 0.780 sec/batch; 66h:08m:22s remains)
INFO - root - 2017-12-07 10:08:23.670880: step 27390, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.778 sec/batch; 65h:53m:46s remains)
INFO - root - 2017-12-07 10:08:31.470599: step 27400, loss = 0.76, batch loss = 0.68 (10.9 examples/sec; 0.734 sec/batch; 62h:11m:15s remains)
2017-12-07 10:08:32.101173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2655783 -5.2287135 -3.0674286 0.057517529 1.8332438 1.4741817 -0.094402313 -2.0608718 -3.5300946 -4.0457125 -3.2945433 -1.3637819 0.3299861 0.45830202 -0.18245125][-5.0449362 -5.1547995 -3.3888688 -0.8029573 0.60883093 0.3147254 -0.81923366 -2.1759114 -3.2165761 -3.5723062 -2.8675647 -1.2747457 -0.0086069107 0.05273962 -0.38987875][-4.7100425 -4.9725676 -3.5805776 -1.5379455 -0.47098684 -0.63925028 -1.1972449 -1.8574247 -2.5268638 -2.8324668 -2.3472898 -1.2686951 -0.58486056 -0.71044159 -0.96913767][-4.4864326 -4.6963797 -3.4535794 -1.7277453 -0.85969687 -0.89659381 -0.96699739 -1.1577804 -1.7609828 -2.2009723 -2.0539796 -1.5540252 -1.4506969 -1.7391584 -1.8065593][-4.5317583 -4.4521933 -3.1297617 -1.552927 -0.86069655 -0.8277154 -0.56514621 -0.60088944 -1.437984 -2.1206925 -2.2961702 -2.1706553 -2.2969983 -2.5838053 -2.5563564][-4.7118874 -4.2078624 -2.7241855 -1.311306 -0.84585047 -0.66387534 -0.026353836 -0.054217339 -1.3809237 -2.5404978 -3.0387616 -2.9960775 -2.8769174 -2.8845968 -2.8081968][-4.6403341 -3.6540916 -2.0708008 -0.92156219 -0.64331055 -0.208076 0.78278208 0.61017561 -1.3296876 -3.0629897 -3.8982129 -3.8438931 -3.3479292 -3.0425475 -3.0727146][-4.1437473 -2.7283111 -1.2918806 -0.63037682 -0.64183283 -0.079110622 1.0610409 0.75128508 -1.4842088 -3.5093732 -4.5118661 -4.4149017 -3.6236534 -3.2122548 -3.5525568][-3.3034244 -1.7167006 -0.74667525 -0.81494975 -1.2626042 -0.73032212 0.35562277 0.032919884 -1.9483402 -3.7145305 -4.4813538 -4.1816244 -3.2765799 -3.0479422 -3.7537673][-2.4135568 -0.93198848 -0.5876472 -1.4056954 -2.2224047 -1.7950828 -0.87303543 -1.1156564 -2.55125 -3.7544916 -4.0038896 -3.3976665 -2.6059992 -2.7607434 -3.6822903][-1.7418575 -0.61252403 -0.90745664 -2.1793187 -3.110827 -2.7739582 -2.0453668 -2.1686449 -2.9744916 -3.5574746 -3.3030252 -2.4505527 -1.8974676 -2.4281518 -3.4036903][-1.3645678 -0.8276825 -1.6579835 -2.945781 -3.6496272 -3.3737173 -2.8627143 -2.8588562 -3.1384969 -3.2636509 -2.7539907 -1.8721704 -1.6018815 -2.3220065 -3.1575074][-1.2042291 -1.3789895 -2.5694892 -3.5922225 -3.9158022 -3.7058969 -3.3948607 -3.2990594 -3.2834611 -3.2153823 -2.7219682 -2.0419104 -1.9924212 -2.5956588 -3.10878][-1.2579436 -2.0333765 -3.3904285 -4.0903239 -4.1314464 -3.9538505 -3.7294173 -3.5637364 -3.4348664 -3.3340633 -3.00096 -2.5648952 -2.524684 -2.8063059 -3.0252895][-1.532553 -2.6337409 -3.9773896 -4.420095 -4.3070245 -4.0789495 -3.8467395 -3.6625073 -3.5385013 -3.4870872 -3.3178921 -3.0677338 -2.9394054 -2.8992081 -2.8962767]]...]
INFO - root - 2017-12-07 10:08:39.766808: step 27410, loss = 0.63, batch loss = 0.56 (10.4 examples/sec; 0.767 sec/batch; 64h:57m:33s remains)
INFO - root - 2017-12-07 10:08:47.332219: step 27420, loss = 0.77, batch loss = 0.70 (10.5 examples/sec; 0.763 sec/batch; 64h:40m:31s remains)
INFO - root - 2017-12-07 10:08:52.535503: step 27430, loss = 0.80, batch loss = 0.73 (19.4 examples/sec; 0.413 sec/batch; 34h:59m:28s remains)
INFO - root - 2017-12-07 10:08:56.357037: step 27440, loss = 0.87, batch loss = 0.80 (21.3 examples/sec; 0.375 sec/batch; 31h:45m:29s remains)
INFO - root - 2017-12-07 10:09:00.307814: step 27450, loss = 0.72, batch loss = 0.65 (20.3 examples/sec; 0.394 sec/batch; 33h:23m:48s remains)
INFO - root - 2017-12-07 10:09:04.185183: step 27460, loss = 0.60, batch loss = 0.53 (20.5 examples/sec; 0.391 sec/batch; 33h:06m:43s remains)
INFO - root - 2017-12-07 10:09:08.114184: step 27470, loss = 0.76, batch loss = 0.69 (20.5 examples/sec; 0.391 sec/batch; 33h:06m:14s remains)
INFO - root - 2017-12-07 10:09:11.977749: step 27480, loss = 0.74, batch loss = 0.67 (20.7 examples/sec; 0.386 sec/batch; 32h:40m:00s remains)
INFO - root - 2017-12-07 10:09:15.839078: step 27490, loss = 0.70, batch loss = 0.62 (21.0 examples/sec; 0.380 sec/batch; 32h:14m:03s remains)
INFO - root - 2017-12-07 10:09:19.765151: step 27500, loss = 0.81, batch loss = 0.74 (20.1 examples/sec; 0.397 sec/batch; 33h:39m:03s remains)
2017-12-07 10:09:20.131074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6187398 -2.5962625 -2.6645672 -2.6876888 -2.7090147 -2.8300254 -3.0575213 -3.3018112 -3.4633515 -3.5234971 -3.5172579 -3.3204615 -2.995101 -2.8395007 -2.9955983][-2.7971687 -2.7725644 -2.7436819 -2.5937383 -2.40228 -2.3979714 -2.6411176 -3.0569103 -3.5054317 -3.7601118 -3.7410624 -3.4174881 -2.9590306 -2.6685128 -2.7289243][-3.0382152 -3.0297446 -2.9712057 -2.7932024 -2.4947019 -2.3178372 -2.4703233 -2.9465046 -3.58467 -3.9538627 -3.8659308 -3.4714518 -3.0552034 -2.7380857 -2.6237664][-3.3118403 -3.2713184 -3.1649983 -2.9937413 -2.6420712 -2.2460084 -2.2166 -2.6524148 -3.3364596 -3.7052627 -3.5220182 -3.189533 -3.0236707 -2.8565469 -2.6410518][-3.2992022 -3.2452216 -3.1429663 -2.9711251 -2.5643613 -1.9876842 -1.748888 -2.1326115 -2.8859239 -3.3118086 -3.1653461 -2.9325695 -2.9457152 -2.8522184 -2.5840681][-3.0109382 -2.9504278 -2.9277244 -2.7824135 -2.413224 -1.8066156 -1.3066647 -1.373652 -2.04836 -2.6716781 -2.8602109 -2.8786242 -2.9582448 -2.8299103 -2.5278111][-2.7322311 -2.6596723 -2.6738029 -2.5002742 -2.1783519 -1.6869738 -1.0436652 -0.68447137 -1.120975 -1.9483798 -2.5642018 -2.8698936 -2.9891191 -2.8792565 -2.6478264][-2.8720999 -2.7746398 -2.7682137 -2.6013465 -2.3328812 -1.9327645 -1.2073298 -0.55188823 -0.74644184 -1.671232 -2.4955158 -2.9067745 -3.0000589 -2.9229815 -2.812223][-3.4738088 -3.279031 -3.186738 -3.0891078 -2.9641271 -2.697578 -2.0460455 -1.3239076 -1.2909939 -2.0886617 -2.9002733 -3.2882557 -3.2894907 -3.1203091 -2.9717379][-4.0781331 -3.8158507 -3.6879435 -3.6571298 -3.6068532 -3.4456692 -2.997642 -2.4046581 -2.2289319 -2.7470078 -3.384325 -3.6585968 -3.5057 -3.15381 -2.8533707][-4.2413521 -3.9903655 -3.9129889 -3.9329607 -3.8785944 -3.7353623 -3.3988657 -2.8777037 -2.5606546 -2.8028758 -3.2663145 -3.4349756 -3.168386 -2.7034745 -2.3117719][-3.8143981 -3.58952 -3.5409608 -3.5527282 -3.471067 -3.3401368 -3.0678849 -2.5813026 -2.1403019 -2.1021535 -2.3399856 -2.4142413 -2.1711376 -1.8129818 -1.5399606][-2.8927956 -2.7656693 -2.732234 -2.7196236 -2.6344919 -2.5179315 -2.2812932 -1.8927133 -1.4680891 -1.2750874 -1.3118601 -1.2816963 -1.085417 -0.87694836 -0.77259779][-1.7409914 -1.6991169 -1.6873999 -1.6919856 -1.6706047 -1.600507 -1.4148591 -1.1414237 -0.835006 -0.6523385 -0.62889791 -0.60651851 -0.51480055 -0.41750145 -0.38601971][-0.77986836 -0.77354884 -0.75719261 -0.76690054 -0.79995728 -0.79073238 -0.67797065 -0.52169633 -0.35427141 -0.2484746 -0.23361921 -0.25518417 -0.26077795 -0.24949837 -0.25021124]]...]
INFO - root - 2017-12-07 10:09:24.030001: step 27510, loss = 0.74, batch loss = 0.67 (20.2 examples/sec; 0.397 sec/batch; 33h:36m:31s remains)
INFO - root - 2017-12-07 10:09:27.908529: step 27520, loss = 0.77, batch loss = 0.70 (21.2 examples/sec; 0.378 sec/batch; 31h:59m:28s remains)
INFO - root - 2017-12-07 10:09:31.774269: step 27530, loss = 1.32, batch loss = 1.24 (20.9 examples/sec; 0.382 sec/batch; 32h:22m:16s remains)
INFO - root - 2017-12-07 10:09:35.585528: step 27540, loss = 0.92, batch loss = 0.85 (20.8 examples/sec; 0.385 sec/batch; 32h:37m:11s remains)
INFO - root - 2017-12-07 10:09:39.503229: step 27550, loss = 0.69, batch loss = 0.61 (21.1 examples/sec; 0.379 sec/batch; 32h:04m:02s remains)
INFO - root - 2017-12-07 10:09:43.388388: step 27560, loss = 0.77, batch loss = 0.70 (21.1 examples/sec; 0.379 sec/batch; 32h:06m:59s remains)
INFO - root - 2017-12-07 10:09:47.245896: step 27570, loss = 0.88, batch loss = 0.81 (21.8 examples/sec; 0.368 sec/batch; 31h:08m:26s remains)
INFO - root - 2017-12-07 10:09:51.077222: step 27580, loss = 0.95, batch loss = 0.87 (20.5 examples/sec; 0.390 sec/batch; 33h:01m:36s remains)
INFO - root - 2017-12-07 10:09:54.956664: step 27590, loss = 0.98, batch loss = 0.91 (21.6 examples/sec; 0.371 sec/batch; 31h:24m:41s remains)
INFO - root - 2017-12-07 10:09:58.838912: step 27600, loss = 1.00, batch loss = 0.93 (20.7 examples/sec; 0.387 sec/batch; 32h:44m:18s remains)
2017-12-07 10:09:59.266414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.375011 -1.6119807 -1.4972441 -1.1432891 -1.0365851 -1.123219 -1.1776686 -1.3216562 -1.611156 -2.02799 -2.0170579 -1.5255017 -1.4646232 -1.7762413 -2.3962765][-1.4875553 -1.6578362 -1.5560138 -1.1617932 -0.8797121 -0.822592 -1.0297034 -1.6532302 -2.4382408 -2.9538999 -2.7528958 -2.1754014 -2.0667083 -2.4013791 -2.9769225][-1.2945731 -1.4146419 -1.3910007 -1.0054767 -0.5741272 -0.41293287 -0.86633754 -1.9699595 -3.0817311 -3.4985881 -3.069241 -2.4765978 -2.3909841 -2.7630415 -3.2390521][-1.5134182 -1.445926 -1.2395422 -0.77951217 -0.38155031 -0.30152273 -0.90559292 -2.1039588 -3.160809 -3.357285 -2.8248658 -2.3208277 -2.2951536 -2.6688802 -3.0259185][-2.0221212 -1.4904888 -0.76039171 -0.12874126 0.097316742 -0.068294048 -0.8375392 -2.0113318 -2.829597 -2.7304974 -2.0517023 -1.6153386 -1.6543279 -2.0161154 -2.3137498][-1.9975109 -1.2120965 -0.2786808 0.38419104 0.51723337 0.19959068 -0.73984194 -1.8328073 -2.3367341 -1.8812106 -0.90031838 -0.37672758 -0.39855146 -0.7582655 -1.1592765][-1.7846365 -1.2994947 -0.52908373 0.20941496 0.50060654 0.13289261 -0.91702151 -1.7535565 -1.8287013 -1.0632007 0.14594698 0.74346972 0.75181913 0.42661762 -0.11197376][-1.5818367 -1.4679337 -0.94383478 -0.19834375 0.19174719 -0.10979319 -0.9852643 -1.3881919 -1.0725617 -0.21745014 0.8984189 1.3262653 1.273272 1.0196977 0.45677519][-0.68018651 -0.73287559 -0.58435464 -0.32296467 -0.25066376 -0.51821327 -1.006923 -1.0259421 -0.72639513 -0.22222662 0.45705175 0.61744118 0.57284546 0.54857159 0.201334][0.18390322 0.17877769 -0.022928238 -0.36885452 -0.75700521 -1.0554957 -1.2864501 -1.1770525 -1.0828576 -0.94846988 -0.71674538 -0.74607396 -0.75218081 -0.54976416 -0.59123468][0.10307407 0.024484634 -0.4092083 -1.0384102 -1.667335 -2.0360672 -2.2271976 -2.1779768 -2.1517205 -2.083679 -1.9635305 -1.9235799 -1.8142798 -1.5075269 -1.4233394][-0.8419013 -0.94066286 -1.4077296 -2.045639 -2.7268486 -3.1479197 -3.37637 -3.4137893 -3.3408515 -3.2017262 -3.0487132 -2.8905063 -2.6690722 -2.3250909 -2.2320013][-1.7859583 -1.8436122 -2.3576765 -2.9935927 -3.5643358 -3.821908 -3.9511623 -4.0639462 -4.0332241 -3.9100585 -3.7440269 -3.5299485 -3.3016124 -3.0350728 -3.0015752][-2.3409357 -2.50477 -3.1230669 -3.6677868 -3.9385059 -3.8632169 -3.8221328 -4.027421 -4.1647444 -4.1650963 -4.0470982 -3.8603768 -3.7324204 -3.5925655 -3.5808821][-2.9647307 -3.1122985 -3.6129289 -3.892978 -3.8567891 -3.5591681 -3.4354019 -3.6791511 -3.864151 -3.876235 -3.8010285 -3.7094016 -3.7169993 -3.6566293 -3.5969887]]...]
INFO - root - 2017-12-07 10:10:03.162423: step 27610, loss = 0.76, batch loss = 0.69 (20.5 examples/sec; 0.391 sec/batch; 33h:04m:54s remains)
INFO - root - 2017-12-07 10:10:07.084685: step 27620, loss = 0.91, batch loss = 0.83 (21.3 examples/sec; 0.376 sec/batch; 31h:51m:01s remains)
INFO - root - 2017-12-07 10:10:10.991057: step 27630, loss = 0.94, batch loss = 0.87 (20.6 examples/sec; 0.389 sec/batch; 32h:56m:57s remains)
INFO - root - 2017-12-07 10:10:14.822393: step 27640, loss = 0.80, batch loss = 0.73 (20.6 examples/sec; 0.389 sec/batch; 32h:57m:45s remains)
INFO - root - 2017-12-07 10:10:18.640035: step 27650, loss = 0.64, batch loss = 0.57 (21.2 examples/sec; 0.378 sec/batch; 31h:58m:53s remains)
INFO - root - 2017-12-07 10:10:22.540039: step 27660, loss = 0.77, batch loss = 0.70 (21.3 examples/sec; 0.375 sec/batch; 31h:45m:19s remains)
INFO - root - 2017-12-07 10:10:26.421231: step 27670, loss = 0.85, batch loss = 0.78 (20.8 examples/sec; 0.385 sec/batch; 32h:34m:41s remains)
INFO - root - 2017-12-07 10:10:30.274455: step 27680, loss = 0.89, batch loss = 0.82 (20.6 examples/sec; 0.387 sec/batch; 32h:48m:32s remains)
INFO - root - 2017-12-07 10:10:34.148677: step 27690, loss = 0.74, batch loss = 0.67 (20.2 examples/sec; 0.395 sec/batch; 33h:27m:38s remains)
INFO - root - 2017-12-07 10:10:37.956742: step 27700, loss = 0.70, batch loss = 0.63 (21.1 examples/sec; 0.380 sec/batch; 32h:09m:41s remains)
2017-12-07 10:10:38.306051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1512048 -1.0670154 -1.1221313 -1.1314576 -0.99940991 -0.93721056 -1.1086028 -1.3418379 -1.5010369 -1.6184831 -1.9064889 -2.0456507 -2.1184211 -2.3228557 -2.674324][-1.1653068 -1.1541588 -1.2942283 -1.3542995 -1.2825031 -1.2799661 -1.4557288 -1.6654422 -1.8066297 -1.8840666 -2.0789709 -2.1262469 -2.0938013 -2.1549459 -2.423558][-1.5834014 -1.6787097 -1.8781428 -1.9873648 -2.0105515 -2.1295671 -2.3550138 -2.5686998 -2.7148874 -2.7975852 -2.9472167 -2.9252603 -2.7733026 -2.5800591 -2.5497994][-2.2360537 -2.3308938 -2.4664083 -2.5592737 -2.6522307 -2.8623581 -3.1126113 -3.3064628 -3.4255614 -3.47959 -3.5972793 -3.5871966 -3.4551802 -3.1714058 -2.9319196][-2.625412 -2.6417453 -2.7002702 -2.7880008 -2.9369979 -3.1773252 -3.3859854 -3.4882088 -3.4743862 -3.447515 -3.5378485 -3.58991 -3.5288963 -3.2739098 -3.0467973][-2.2142634 -2.1758664 -2.2088273 -2.3168912 -2.517643 -2.7776346 -2.9415078 -2.9444542 -2.7572203 -2.5813928 -2.6361032 -2.8112121 -2.8670564 -2.7550974 -2.7256541][-1.3332448 -1.2263029 -1.2412806 -1.3984358 -1.6898224 -1.9992218 -2.1298602 -2.0343874 -1.7068436 -1.4295363 -1.547631 -1.9507897 -2.2028155 -2.2932117 -2.4531181][-0.81812358 -0.64531469 -0.616385 -0.78406239 -1.1456511 -1.4904549 -1.587405 -1.4124031 -1.051295 -0.79763746 -1.0486624 -1.6360652 -2.0617177 -2.3069232 -2.5551286][-0.755779 -0.5342052 -0.46220875 -0.60880303 -0.98549771 -1.3283782 -1.3925185 -1.1748326 -0.81117153 -0.6227541 -0.96824193 -1.5935588 -2.044518 -2.3018334 -2.5396638][-1.3070896 -1.1600163 -1.1535649 -1.3287473 -1.6646309 -1.9296527 -1.9300489 -1.696671 -1.3694334 -1.2624445 -1.6238532 -2.1632516 -2.4331756 -2.4387374 -2.4508567][-2.0388293 -1.9954605 -2.0952146 -2.3251829 -2.6131008 -2.7945123 -2.7714844 -2.6098948 -2.3925166 -2.3824496 -2.7448654 -3.212934 -3.300756 -3.0161121 -2.70724][-2.9995203 -3.0023921 -3.1333861 -3.3663013 -3.6006296 -3.7063065 -3.6415384 -3.4703076 -3.2728682 -3.2509975 -3.5285225 -3.9026279 -3.8976707 -3.4907897 -3.0442858][-3.0344386 -3.0729618 -3.215313 -3.4268074 -3.6203966 -3.7131906 -3.6592364 -3.4896741 -3.2672188 -3.1777048 -3.371438 -3.68611 -3.6921017 -3.3818827 -3.0743704][-2.59367 -2.6262462 -2.7403874 -2.900209 -3.0398064 -3.1029334 -3.0233679 -2.7957187 -2.4619184 -2.2570653 -2.3979981 -2.7538939 -2.888808 -2.8245859 -2.8121624][-3.2708817 -3.2651196 -3.3020227 -3.3629735 -3.4010043 -3.3710387 -3.1977725 -2.8733873 -2.4038198 -2.0588102 -2.1335475 -2.4830549 -2.6417174 -2.6647382 -2.7696648]]...]
INFO - root - 2017-12-07 10:10:42.229021: step 27710, loss = 0.66, batch loss = 0.59 (20.6 examples/sec; 0.389 sec/batch; 32h:57m:10s remains)
INFO - root - 2017-12-07 10:10:46.092769: step 27720, loss = 0.82, batch loss = 0.74 (21.1 examples/sec; 0.380 sec/batch; 32h:10m:25s remains)
INFO - root - 2017-12-07 10:10:50.000560: step 27730, loss = 0.73, batch loss = 0.66 (21.1 examples/sec; 0.379 sec/batch; 32h:03m:07s remains)
INFO - root - 2017-12-07 10:10:53.847091: step 27740, loss = 0.78, batch loss = 0.70 (20.5 examples/sec; 0.390 sec/batch; 33h:02m:58s remains)
INFO - root - 2017-12-07 10:10:57.735631: step 27750, loss = 0.86, batch loss = 0.79 (19.7 examples/sec; 0.405 sec/batch; 34h:18m:54s remains)
INFO - root - 2017-12-07 10:11:01.566960: step 27760, loss = 0.72, batch loss = 0.65 (21.4 examples/sec; 0.375 sec/batch; 31h:43m:06s remains)
INFO - root - 2017-12-07 10:11:05.443920: step 27770, loss = 0.75, batch loss = 0.67 (21.3 examples/sec; 0.376 sec/batch; 31h:50m:05s remains)
INFO - root - 2017-12-07 10:11:09.297139: step 27780, loss = 0.75, batch loss = 0.68 (20.6 examples/sec; 0.389 sec/batch; 32h:55m:01s remains)
INFO - root - 2017-12-07 10:11:13.178843: step 27790, loss = 0.95, batch loss = 0.88 (20.2 examples/sec; 0.396 sec/batch; 33h:30m:24s remains)
INFO - root - 2017-12-07 10:11:17.101708: step 27800, loss = 0.83, batch loss = 0.76 (21.0 examples/sec; 0.380 sec/batch; 32h:10m:39s remains)
2017-12-07 10:11:17.467081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6496267 -2.6996379 -2.7416811 -2.7706718 -2.7960472 -2.8257751 -2.8566394 -2.9153161 -2.9813507 -3.0140793 -3.0230041 -2.9971173 -2.9151883 -2.8164706 -2.7402048][-2.6905074 -2.7472477 -2.8108478 -2.8776569 -2.9227877 -2.9329572 -2.9191785 -2.938462 -2.9637756 -2.9675064 -2.9798284 -2.9626908 -2.8843741 -2.7982302 -2.7434676][-2.7407739 -2.7891207 -2.8645213 -2.9641495 -3.0010657 -2.9383783 -2.8263607 -2.7703922 -2.7476695 -2.7552593 -2.8143086 -2.8392437 -2.7955432 -2.7369919 -2.7017035][-2.757278 -2.7422552 -2.774735 -2.8384767 -2.7963398 -2.6363358 -2.4575984 -2.3765314 -2.3658464 -2.4724641 -2.6552532 -2.7730277 -2.7918487 -2.7547982 -2.6984916][-2.6698437 -2.5321279 -2.4459839 -2.3618529 -2.1333265 -1.8457859 -1.6646829 -1.6653132 -1.8011019 -2.1220098 -2.5184002 -2.7686009 -2.8455629 -2.8095984 -2.7160962][-2.4621563 -2.1613777 -1.9229417 -1.6266313 -1.1616094 -0.78752303 -0.71588445 -0.906132 -1.2392502 -1.758075 -2.2935755 -2.6145463 -2.71711 -2.6895561 -2.6147249][-2.2003498 -1.8027434 -1.5121443 -1.1295118 -0.60129666 -0.33873367 -0.49386477 -0.8233211 -1.2036579 -1.7223952 -2.2070024 -2.4853725 -2.5798349 -2.5808096 -2.5565457][-2.0180268 -1.7235212 -1.6462908 -1.5060949 -1.2458799 -1.2432079 -1.4842877 -1.7224326 -1.9491961 -2.268014 -2.5467148 -2.6925526 -2.737149 -2.700192 -2.668144][-2.0624878 -1.9995947 -2.2266228 -2.410213 -2.4583702 -2.6095972 -2.7846234 -2.8722572 -2.9633079 -3.114419 -3.2064095 -3.2203774 -3.1667242 -3.0158179 -2.9019115][-2.2955935 -2.4056814 -2.7760634 -3.0965216 -3.2676985 -3.4214933 -3.4973822 -3.5122883 -3.56836 -3.6360536 -3.6337845 -3.5865233 -3.46026 -3.2466445 -3.0856998][-2.5652394 -2.71011 -3.0169921 -3.2650814 -3.3960874 -3.4999781 -3.5221658 -3.5305676 -3.5938542 -3.6513703 -3.6513298 -3.6070571 -3.4650059 -3.2523284 -3.0997202][-2.7326558 -2.8461695 -3.0335498 -3.1714997 -3.2523823 -3.3407335 -3.38685 -3.4442337 -3.5364826 -3.6023414 -3.6014316 -3.5365214 -3.3769002 -3.1796331 -3.0425835][-2.7905068 -2.8680687 -2.9767118 -3.0624681 -3.143518 -3.2530465 -3.339231 -3.4092898 -3.4704888 -3.4841418 -3.4364233 -3.3368733 -3.1809807 -3.0265245 -2.9273794][-2.8060732 -2.8693712 -2.9442298 -3.0063837 -3.0705748 -3.1475739 -3.202975 -3.2308736 -3.2330756 -3.1978683 -3.1339614 -3.044867 -2.9363008 -2.8461833 -2.8012862][-2.7894912 -2.83103 -2.8787713 -2.9147727 -2.9439912 -2.9741912 -2.9905105 -2.9871526 -2.9627049 -2.917933 -2.8694067 -2.8147528 -2.7614956 -2.73239 -2.7323937]]...]
INFO - root - 2017-12-07 10:11:21.317430: step 27810, loss = 0.97, batch loss = 0.89 (21.1 examples/sec; 0.378 sec/batch; 32h:01m:39s remains)
INFO - root - 2017-12-07 10:11:25.182688: step 27820, loss = 0.88, batch loss = 0.81 (20.8 examples/sec; 0.385 sec/batch; 32h:34m:49s remains)
INFO - root - 2017-12-07 10:11:29.021392: step 27830, loss = 0.79, batch loss = 0.72 (20.6 examples/sec; 0.388 sec/batch; 32h:48m:40s remains)
