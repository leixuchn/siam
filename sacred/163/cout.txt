INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "163"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr1-clip-notzeroinit-from-scratch
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-09 05:14:35.686687: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:14:35.686721: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:14:35.686727: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:14:35.686731: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:14:35.686735: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:14:36.119041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-09 05:14:36.119078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-09 05:14:36.119085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-09 05:14:36.119092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-09 05:14:42.919992: step 0, loss = 0.90, batch loss = 0.69 (1.5 examples/sec; 5.251 sec/batch; 484h:58m:59s remains)
2017-12-09 05:14:43.755352: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00024473327 0.00024557815 0.00024090329 0.0002347385 0.00022727881 0.00021925183 0.00021695721 0.00021761714 0.00021578683 0.00022138034 0.00022614541 0.000228391 0.00023078124 0.00023552513 0.00024006589][0.00025036157 0.00024870332 0.00023937284 0.00022954066 0.0002203292 0.00021618622 0.00022006014 0.00022232717 0.00022387924 0.00023310787 0.00023536847 0.00023589078 0.00023681231 0.00023983693 0.00024371326][0.00024892986 0.00024530815 0.00023584293 0.00022323261 0.00021314611 0.00021572545 0.00022499017 0.00022810038 0.00023320234 0.000242151 0.00024053705 0.00023801238 0.00023679579 0.00023897793 0.00024201634][0.00024527841 0.00024438181 0.00023580785 0.00022726496 0.00022391768 0.00023221495 0.00024511924 0.00025040735 0.00024871092 0.00024694292 0.00024141261 0.00023359916 0.00023353797 0.00023689361 0.00024002747][0.00024646922 0.00024488813 0.00023785557 0.00023915424 0.00024951468 0.0002637587 0.00028390906 0.00028687858 0.00026965971 0.000253127 0.00023618343 0.00022564687 0.00022493918 0.00023133041 0.0002363086][0.00025354218 0.00025176126 0.00024794 0.00026256198 0.0002891146 0.00032154107 0.00036203294 0.00035373311 0.00030805814 0.00026933383 0.00023999717 0.000223774 0.00022079394 0.00022773186 0.00023417924][0.00026034139 0.0002601782 0.00026314298 0.00028975142 0.00032359129 0.00037335884 0.00044015466 0.00041461753 0.00033799588 0.00028616117 0.00024524773 0.00022272795 0.00021949942 0.00022710316 0.00023427181][0.00026854448 0.00027221537 0.00027947428 0.00030498105 0.00033530383 0.00037262414 0.0004179372 0.0003750856 0.00031166384 0.00026734162 0.00023753708 0.00022262662 0.00022128607 0.00022887779 0.00023507545][0.00027468224 0.0002813219 0.00028890357 0.00030827409 0.00032621747 0.00034121209 0.00034749709 0.0003079926 0.00026650995 0.00023872976 0.00022612713 0.00022124423 0.00022511443 0.00023178922 0.00023693808][0.00026918569 0.00027473914 0.00028269115 0.00029706769 0.000306837 0.00030797455 0.00030356389 0.00027382089 0.00024274757 0.00022238751 0.00022209226 0.00022243646 0.00022862948 0.0002349089 0.00023844946][0.00026282392 0.00026727279 0.00027289562 0.00028217683 0.00028170124 0.00027766923 0.00027355485 0.00025258635 0.00022725723 0.00021485056 0.00021795109 0.00022295088 0.00022823553 0.00023375063 0.00023910409][0.00025281074 0.00025553053 0.0002594906 0.00026299185 0.00025971781 0.00025782379 0.00025719093 0.00023833619 0.00021936494 0.00021126933 0.00021641732 0.00022326705 0.00022670091 0.00023243196 0.00024113321][0.00023977889 0.00023972857 0.00024294079 0.00024407529 0.00024134893 0.00024327004 0.00024496196 0.00023148375 0.00021641575 0.00021041717 0.00021627017 0.00022060733 0.00022502575 0.00023181205 0.00024082398][0.0002326035 0.00023134677 0.00023311954 0.00023477151 0.00023308375 0.00023598697 0.00024042593 0.00022962 0.00021538233 0.00020906351 0.00021363351 0.00021856319 0.00022339444 0.00023064602 0.00023982782][0.00022740035 0.00022575776 0.00022632259 0.00022923967 0.0002295062 0.00023420541 0.00023878468 0.0002301192 0.00021609171 0.00020805378 0.00021067902 0.0002168596 0.00022220712 0.00023031727 0.00023944239]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr1-clip-notzeroinit-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr1-clip-notzeroinit-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 05:14:50.580096: step 10, loss = 586.14, batch loss = 583.44 (12.9 examples/sec; 0.621 sec/batch; 57h:22m:43s remains)
INFO - root - 2017-12-09 05:14:56.719561: step 20, loss = 284.53, batch loss = 274.85 (12.9 examples/sec; 0.621 sec/batch; 57h:19m:26s remains)
INFO - root - 2017-12-09 05:15:02.857320: step 30, loss = 36.43, batch loss = 19.58 (13.0 examples/sec; 0.614 sec/batch; 56h:39m:37s remains)
INFO - root - 2017-12-09 05:15:08.942499: step 40, loss = 4685.67, batch loss = 4662.61 (14.6 examples/sec; 0.547 sec/batch; 50h:28m:32s remains)
INFO - root - 2017-12-09 05:15:15.023871: step 50, loss = 1831.57, batch loss = 1802.93 (13.1 examples/sec; 0.609 sec/batch; 56h:12m:14s remains)
INFO - root - 2017-12-09 05:15:21.036615: step 60, loss = 9165.96, batch loss = 9127.70 (13.2 examples/sec; 0.605 sec/batch; 55h:50m:43s remains)
INFO - root - 2017-12-09 05:15:27.124315: step 70, loss = 10001.63, batch loss = 9952.24 (13.4 examples/sec; 0.595 sec/batch; 54h:59m:21s remains)
INFO - root - 2017-12-09 05:15:33.153673: step 80, loss = 61.39, batch loss = 0.78 (13.2 examples/sec; 0.604 sec/batch; 55h:48m:57s remains)
INFO - root - 2017-12-09 05:15:39.281511: step 90, loss = 576.31, batch loss = 502.83 (13.5 examples/sec; 0.593 sec/batch; 54h:45m:31s remains)
INFO - root - 2017-12-09 05:15:45.489514: step 100, loss = 86.57, batch loss = 0.78 (12.8 examples/sec; 0.626 sec/batch; 57h:48m:46s remains)
2017-12-09 05:15:46.088707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203][-0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203 -0.82295203]]...]
INFO - root - 2017-12-09 05:15:52.300033: step 110, loss = 4583.20, batch loss = 4486.02 (13.0 examples/sec; 0.616 sec/batch; 56h:52m:38s remains)
INFO - root - 2017-12-09 05:15:58.446742: step 120, loss = 107.87, batch loss = 0.77 (13.0 examples/sec; 0.617 sec/batch; 56h:58m:47s remains)
INFO - root - 2017-12-09 05:16:04.638963: step 130, loss = 116.69, batch loss = 0.73 (13.2 examples/sec; 0.608 sec/batch; 56h:08m:21s remains)
INFO - root - 2017-12-09 05:16:10.772411: step 140, loss = 171.68, batch loss = 51.47 (13.4 examples/sec; 0.597 sec/batch; 55h:05m:07s remains)
INFO - root - 2017-12-09 05:16:16.725361: step 150, loss = 219.61, batch loss = 96.81 (13.1 examples/sec; 0.611 sec/batch; 56h:22m:42s remains)
INFO - root - 2017-12-09 05:16:22.743196: step 160, loss = 131.27, batch loss = 0.93 (14.6 examples/sec; 0.548 sec/batch; 50h:37m:05s remains)
INFO - root - 2017-12-09 05:16:28.838924: step 170, loss = 140.33, batch loss = 1.00 (13.5 examples/sec; 0.593 sec/batch; 54h:41m:54s remains)
INFO - root - 2017-12-09 05:16:34.933062: step 180, loss = 355.33, batch loss = 206.94 (13.1 examples/sec; 0.610 sec/batch; 56h:21m:06s remains)
INFO - root - 2017-12-09 05:16:41.141906: step 190, loss = 162.01, batch loss = 0.75 (12.9 examples/sec; 0.619 sec/batch; 57h:06m:52s remains)
INFO - root - 2017-12-09 05:16:47.265433: step 200, loss = 174.26, batch loss = 1.27 (13.0 examples/sec; 0.617 sec/batch; 56h:56m:14s remains)
2017-12-09 05:16:47.871250: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403][2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403 2.4349403]]...]
INFO - root - 2017-12-09 05:16:54.053261: step 210, loss = 215.42, batch loss = 33.11 (13.7 examples/sec; 0.585 sec/batch; 53h:57m:34s remains)
INFO - root - 2017-12-09 05:17:00.076041: step 220, loss = 189.56, batch loss = 0.74 (13.6 examples/sec; 0.590 sec/batch; 54h:29m:13s remains)
INFO - root - 2017-12-09 05:17:06.104070: step 230, loss = 195.99, batch loss = 0.73 (13.0 examples/sec; 0.617 sec/batch; 56h:57m:28s remains)
INFO - root - 2017-12-09 05:17:12.147787: step 240, loss = 277.26, batch loss = 77.73 (13.2 examples/sec; 0.607 sec/batch; 55h:59m:17s remains)
INFO - root - 2017-12-09 05:17:18.060718: step 250, loss = 205.03, batch loss = 0.70 (13.1 examples/sec; 0.609 sec/batch; 56h:14m:20s remains)
INFO - root - 2017-12-09 05:17:24.183748: step 260, loss = 213.12, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:25m:40s remains)
INFO - root - 2017-12-09 05:17:30.087035: step 270, loss = 221.58, batch loss = 0.70 (13.0 examples/sec; 0.614 sec/batch; 56h:41m:21s remains)
INFO - root - 2017-12-09 05:17:36.131259: step 280, loss = 235.37, batch loss = 6.21 (13.2 examples/sec; 0.608 sec/batch; 56h:08m:03s remains)
INFO - root - 2017-12-09 05:17:42.282305: step 290, loss = 437.20, batch loss = 198.11 (13.2 examples/sec; 0.606 sec/batch; 55h:54m:30s remains)
INFO - root - 2017-12-09 05:17:48.377473: step 300, loss = 246.50, batch loss = 0.69 (13.6 examples/sec; 0.589 sec/batch; 54h:22m:49s remains)
2017-12-09 05:17:49.039100: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789][0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789 0.089346789]]...]
INFO - root - 2017-12-09 05:17:55.159816: step 310, loss = 250.12, batch loss = 0.69 (13.0 examples/sec; 0.614 sec/batch; 56h:37m:52s remains)
INFO - root - 2017-12-09 05:18:01.311743: step 320, loss = 250.93, batch loss = 0.69 (12.6 examples/sec; 0.634 sec/batch; 58h:32m:31s remains)
INFO - root - 2017-12-09 05:18:07.419812: step 330, loss = 257.62, batch loss = 0.69 (13.2 examples/sec; 0.607 sec/batch; 55h:58m:41s remains)
INFO - root - 2017-12-09 05:18:13.496010: step 340, loss = 798.14, batch loss = 533.11 (13.9 examples/sec; 0.576 sec/batch; 53h:10m:54s remains)
INFO - root - 2017-12-09 05:18:19.370924: step 350, loss = 278.70, batch loss = 0.69 (13.0 examples/sec; 0.613 sec/batch; 56h:33m:40s remains)
INFO - root - 2017-12-09 05:18:25.535379: step 360, loss = 289.03, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:26m:19s remains)
INFO - root - 2017-12-09 05:18:31.426484: step 370, loss = 292.55, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:49m:56s remains)
INFO - root - 2017-12-09 05:18:37.572514: step 380, loss = 305.26, batch loss = 0.69 (13.5 examples/sec; 0.594 sec/batch; 54h:48m:04s remains)
INFO - root - 2017-12-09 05:18:43.711103: step 390, loss = 317.35, batch loss = 0.69 (12.9 examples/sec; 0.619 sec/batch; 57h:07m:10s remains)
INFO - root - 2017-12-09 05:18:49.882533: step 400, loss = 323.73, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:42m:36s remains)
2017-12-09 05:18:50.496487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043][-0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043 -0.001094043]]...]
INFO - root - 2017-12-09 05:18:56.699358: step 410, loss = 325.31, batch loss = 0.69 (12.9 examples/sec; 0.619 sec/batch; 57h:08m:41s remains)
INFO - root - 2017-12-09 05:19:02.811229: step 420, loss = 329.43, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:43m:35s remains)
INFO - root - 2017-12-09 05:19:08.911806: step 430, loss = 335.91, batch loss = 0.69 (13.3 examples/sec; 0.603 sec/batch; 55h:35m:31s remains)
INFO - root - 2017-12-09 05:19:14.898307: step 440, loss = 391.12, batch loss = 49.61 (13.5 examples/sec; 0.593 sec/batch; 54h:40m:16s remains)
INFO - root - 2017-12-09 05:19:20.851868: step 450, loss = 347.63, batch loss = 0.69 (17.6 examples/sec; 0.455 sec/batch; 42h:00m:46s remains)
INFO - root - 2017-12-09 05:19:26.908222: step 460, loss = 353.71, batch loss = 0.69 (12.9 examples/sec; 0.620 sec/batch; 57h:09m:37s remains)
INFO - root - 2017-12-09 05:19:32.832701: step 470, loss = 351.87, batch loss = 0.69 (12.9 examples/sec; 0.619 sec/batch; 57h:06m:47s remains)
INFO - root - 2017-12-09 05:19:38.831907: step 480, loss = 352.73, batch loss = 0.69 (13.5 examples/sec; 0.593 sec/batch; 54h:41m:09s remains)
INFO - root - 2017-12-09 05:19:44.902820: step 490, loss = 360.11, batch loss = 0.69 (13.6 examples/sec; 0.589 sec/batch; 54h:17m:50s remains)
INFO - root - 2017-12-09 05:19:50.890788: step 500, loss = 366.64, batch loss = 0.69 (13.5 examples/sec; 0.593 sec/batch; 54h:39m:24s remains)
2017-12-09 05:19:51.525347: I tensorflow/core/kernels/logging_ops.cc:79] [[[73397.367 118711.16 157145.77 213025.59 202458.44 253989.91 263765.66 259039.78 208159.31 200184.95 153524.47 186577.69 110776.48 118821.83 73820.484][92058.25 145423.06 152491.38 139543.97 153160.67 186717.45 133200.64 158625.27 110899.39 125931.22 87763.8 46813.828 38103.68 83271.328 77008.547][74722.242 142684.17 181562.89 185505.12 198058.16 186286.19 133720.23 163999.69 105116.87 72869.75 64384.363 51810.871 19947.217 43200.883 39130.012][22242.438 60720.402 66832.508 71113.938 92547.031 111309.79 110641.16 107415.3 74782.641 82341.805 78909.438 38413.043 33139.84 24432.529 24088.066][50663.227 69255.648 45763.09 41678.48 28926.582 12407.494 9658.3916 41932.074 11939.71 51998.922 24894.068 31640.344 13928.713 12578.845 11114.497][40496.426 49857.098 68345.039 130098.47 56609.617 70796.062 60550.227 56824.418 49936.289 51136.77 44572.32 119502.73 56446.617 43318.641 30477.885][28952.402 37214.375 46111.957 25915.986 1798.4004 41108.488 41581.055 60713.555 52491.148 61625.953 61104.477 75835.383 55621.523 61451.836 69252.984][24855.922 21642.941 6883.5854 13496.771 58727.578 63474.09 8551.6865 93721.805 34417.781 46346.719 37063.98 48050.754 41017.891 51358.922 48418.023][44751.289 67200.531 61913.746 39313.359 42879.297 54045.871 46279.3 58306.258 40204.34 58187.418 70269.8 30776.402 37690.754 46612.438 31610.006][92602.1 92481.953 33753.41 2420.6904 4817.6577 7304.918 7125.0273 17522.455 17465.355 34967.617 34258.434 4781.4385 2963.7327 2321.1982 1285.73][2142.7393 8447.8662 2478.2102 465.73721 1283.4135 1550.8484 5267.8472 11671.045 18583.602 37056.09 30107.172 20976.832 20047.361 17127.004 10611.681][7530.2793 3482.8521 3713.1074 -0.0025775782 0.0084224222 0.002922422 0.0074224221 0.0059224223 0.49092245 11.429173 31.434048 7.888423 0.36442244 0.13842243 0.014422422][12242.11 8296.6113 5709.6489 0.0035474219 0.0077974224 0.0054224222 0.0059224223 0.0046724221 0.0034224219 0.0047974219 0.0067974222 0.0062974221 0.010797422 0.004422422 0.0089224223][16038.982 10404.021 10771.821 -0.00070257823 0.0070474222 0.00098492182 0.0084224222 0.071172431 0.34467244 0.13017243 0.0079224221 0.0034224219 0.010922423 0.00042242184 0.010422423][5736.1753 10554.825 6882.3389 3200.6858 365.83371 0.0014224219 0.0094224224 0.0054224222 0.0079224221 0.0095474226 0.009672422 0.011422423 0.018422423 0.0099224225 0.013922422]]...]
INFO - root - 2017-12-09 05:19:57.588409: step 510, loss = 460.48, batch loss = 88.22 (13.2 examples/sec; 0.607 sec/batch; 56h:01m:12s remains)
INFO - root - 2017-12-09 05:20:03.581079: step 520, loss = 382.28, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:26m:13s remains)
INFO - root - 2017-12-09 05:20:09.718238: step 530, loss = 384.87, batch loss = 0.69 (12.9 examples/sec; 0.623 sec/batch; 57h:24m:26s remains)
INFO - root - 2017-12-09 05:20:15.920370: step 540, loss = 386.38, batch loss = 0.69 (13.1 examples/sec; 0.611 sec/batch; 56h:18m:52s remains)
INFO - root - 2017-12-09 05:20:22.029547: step 550, loss = 387.93, batch loss = 0.69 (12.9 examples/sec; 0.622 sec/batch; 57h:19m:28s remains)
INFO - root - 2017-12-09 05:20:27.991793: step 560, loss = 386.01, batch loss = 0.69 (13.1 examples/sec; 0.608 sec/batch; 56h:05m:57s remains)
INFO - root - 2017-12-09 05:20:33.913338: step 570, loss = 386.24, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:22m:40s remains)
INFO - root - 2017-12-09 05:20:39.986498: step 580, loss = 387.68, batch loss = 0.69 (13.5 examples/sec; 0.595 sec/batch; 54h:50m:18s remains)
INFO - root - 2017-12-09 05:20:46.003623: step 590, loss = 391.26, batch loss = 0.69 (13.5 examples/sec; 0.594 sec/batch; 54h:47m:37s remains)
INFO - root - 2017-12-09 05:20:52.141463: step 600, loss = 1011.21, batch loss = 618.48 (12.8 examples/sec; 0.626 sec/batch; 57h:41m:15s remains)
2017-12-09 05:20:52.734393: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212][0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212 0.00010270212]]...]
INFO - root - 2017-12-09 05:20:58.902482: step 610, loss = 403.17, batch loss = 0.69 (13.2 examples/sec; 0.608 sec/batch; 56h:00m:44s remains)
INFO - root - 2017-12-09 05:21:05.082027: step 620, loss = 407.58, batch loss = 0.69 (13.0 examples/sec; 0.616 sec/batch; 56h:44m:53s remains)
INFO - root - 2017-12-09 05:21:11.023485: step 630, loss = 410.08, batch loss = 0.69 (13.5 examples/sec; 0.590 sec/batch; 54h:25m:48s remains)
INFO - root - 2017-12-09 05:21:17.034820: step 640, loss = 415.67, batch loss = 0.69 (13.3 examples/sec; 0.602 sec/batch; 55h:29m:14s remains)
INFO - root - 2017-12-09 05:21:23.183661: step 650, loss = 423.46, batch loss = 0.69 (13.5 examples/sec; 0.593 sec/batch; 54h:41m:14s remains)
INFO - root - 2017-12-09 05:21:29.119638: step 660, loss = 432.51, batch loss = 0.69 (13.1 examples/sec; 0.612 sec/batch; 56h:26m:28s remains)
INFO - root - 2017-12-09 05:21:35.047640: step 670, loss = 435.30, batch loss = 0.69 (12.9 examples/sec; 0.619 sec/batch; 57h:03m:37s remains)
INFO - root - 2017-12-09 05:21:41.089889: step 680, loss = 437.05, batch loss = 0.69 (13.5 examples/sec; 0.594 sec/batch; 54h:47m:07s remains)
INFO - root - 2017-12-09 05:21:47.118756: step 690, loss = 435.25, batch loss = 0.69 (13.0 examples/sec; 0.617 sec/batch; 56h:52m:50s remains)
INFO - root - 2017-12-09 05:21:53.178841: step 700, loss = 439.81, batch loss = 0.69 (13.3 examples/sec; 0.600 sec/batch; 55h:17m:50s remains)
2017-12-09 05:21:53.773346: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05][1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05 1.0079065e-05]]...]
INFO - root - 2017-12-09 05:21:59.906310: step 710, loss = 453.55, batch loss = 0.69 (13.1 examples/sec; 0.611 sec/batch; 56h:21m:00s remains)
INFO - root - 2017-12-09 05:22:05.967123: step 720, loss = 468.00, batch loss = 0.69 (13.2 examples/sec; 0.608 sec/batch; 56h:00m:33s remains)
INFO - root - 2017-12-09 05:22:11.997429: step 730, loss = 471.80, batch loss = 0.69 (13.0 examples/sec; 0.616 sec/batch; 56h:47m:13s remains)
INFO - root - 2017-12-09 05:22:18.067320: step 740, loss = 475.19, batch loss = 0.69 (13.1 examples/sec; 0.611 sec/batch; 56h:17m:54s remains)
INFO - root - 2017-12-09 05:22:24.188543: step 750, loss = 473.65, batch loss = 0.69 (13.1 examples/sec; 0.610 sec/batch; 56h:13m:50s remains)
INFO - root - 2017-12-09 05:22:30.159977: step 760, loss = 470.38, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:41m:40s remains)
INFO - root - 2017-12-09 05:22:36.138379: step 770, loss = 465.91, batch loss = 0.69 (14.8 examples/sec; 0.540 sec/batch; 49h:45m:47s remains)
INFO - root - 2017-12-09 05:22:42.198886: step 780, loss = 464.56, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:42m:07s remains)
INFO - root - 2017-12-09 05:22:48.257354: step 790, loss = 464.96, batch loss = 0.69 (12.8 examples/sec; 0.623 sec/batch; 57h:23m:17s remains)
INFO - root - 2017-12-09 05:22:54.375054: step 800, loss = 467.45, batch loss = 0.69 (13.2 examples/sec; 0.606 sec/batch; 55h:52m:21s remains)
2017-12-09 05:22:54.977098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07][-2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07 -2.140399e-07]]...]
INFO - root - 2017-12-09 05:23:01.185034: step 810, loss = 465.74, batch loss = 0.69 (13.0 examples/sec; 0.614 sec/batch; 56h:36m:48s remains)
INFO - root - 2017-12-09 05:23:07.321284: step 820, loss = 460.83, batch loss = 0.69 (13.0 examples/sec; 0.617 sec/batch; 56h:51m:29s remains)
INFO - root - 2017-12-09 05:23:13.400741: step 830, loss = 464.04, batch loss = 0.69 (13.4 examples/sec; 0.596 sec/batch; 54h:52m:22s remains)
INFO - root - 2017-12-09 05:23:19.387109: step 840, loss = 470.64, batch loss = 0.69 (13.4 examples/sec; 0.598 sec/batch; 55h:06m:15s remains)
INFO - root - 2017-12-09 05:23:25.543281: step 850, loss = 478.77, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:19m:45s remains)
INFO - root - 2017-12-09 05:23:31.618476: step 860, loss = 482.32, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:22m:46s remains)
INFO - root - 2017-12-09 05:23:37.626168: step 870, loss = 486.77, batch loss = 0.69 (12.8 examples/sec; 0.623 sec/batch; 57h:24m:18s remains)
INFO - root - 2017-12-09 05:23:43.609524: step 880, loss = 486.41, batch loss = 0.69 (12.7 examples/sec; 0.632 sec/batch; 58h:12m:37s remains)
INFO - root - 2017-12-09 05:23:49.794670: step 890, loss = 487.73, batch loss = 0.69 (13.0 examples/sec; 0.616 sec/batch; 56h:42m:44s remains)
INFO - root - 2017-12-09 05:23:55.808187: step 900, loss = 486.41, batch loss = 0.69 (13.2 examples/sec; 0.604 sec/batch; 55h:37m:53s remains)
2017-12-09 05:23:56.392968: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07][2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07 2.2216459e-07]]...]
INFO - root - 2017-12-09 05:24:02.437207: step 910, loss = 480.02, batch loss = 0.69 (13.0 examples/sec; 0.617 sec/batch; 56h:49m:51s remains)
INFO - root - 2017-12-09 05:24:08.608456: step 920, loss = 474.34, batch loss = 0.69 (12.8 examples/sec; 0.624 sec/batch; 57h:29m:00s remains)
INFO - root - 2017-12-09 05:24:14.731857: step 930, loss = 474.50, batch loss = 0.69 (13.6 examples/sec; 0.588 sec/batch; 54h:07m:09s remains)
INFO - root - 2017-12-09 05:24:20.798010: step 940, loss = 471.47, batch loss = 0.69 (13.2 examples/sec; 0.606 sec/batch; 55h:47m:34s remains)
INFO - root - 2017-12-09 05:24:26.804088: step 950, loss = 471.95, batch loss = 0.69 (13.4 examples/sec; 0.598 sec/batch; 55h:03m:14s remains)
INFO - root - 2017-12-09 05:24:32.822699: step 960, loss = 473.45, batch loss = 0.69 (13.1 examples/sec; 0.613 sec/batch; 56h:24m:31s remains)
INFO - root - 2017-12-09 05:24:38.820587: step 970, loss = 472.58, batch loss = 0.69 (13.1 examples/sec; 0.610 sec/batch; 56h:08m:51s remains)
INFO - root - 2017-12-09 05:24:44.803940: step 980, loss = 478.19, batch loss = 0.69 (13.4 examples/sec; 0.598 sec/batch; 55h:02m:31s remains)
INFO - root - 2017-12-09 05:24:50.823710: step 990, loss = 479.97, batch loss = 0.69 (13.3 examples/sec; 0.600 sec/batch; 55h:17m:43s remains)
INFO - root - 2017-12-09 05:24:56.944064: step 1000, loss = 474.46, batch loss = 0.69 (12.8 examples/sec; 0.627 sec/batch; 57h:43m:30s remains)
2017-12-09 05:24:57.549544: I tensorflow/core/kernels/logging_ops.cc:79] [[[4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09][4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09 4.90342e-09]]...]
INFO - root - 2017-12-09 05:25:03.664633: step 1010, loss = 470.32, batch loss = 0.69 (12.7 examples/sec; 0.629 sec/batch; 57h:57m:14s remains)
INFO - root - 2017-12-09 05:25:09.837584: step 1020, loss = 469.83, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:39m:09s remains)
INFO - root - 2017-12-09 05:25:15.927633: step 1030, loss = 478.47, batch loss = 0.69 (13.3 examples/sec; 0.602 sec/batch; 55h:26m:51s remains)
INFO - root - 2017-12-09 05:25:21.974560: step 1040, loss = 482.92, batch loss = 0.69 (13.2 examples/sec; 0.608 sec/batch; 55h:57m:36s remains)
INFO - root - 2017-12-09 05:25:28.035328: step 1050, loss = 491.06, batch loss = 0.69 (13.2 examples/sec; 0.608 sec/batch; 55h:56m:23s remains)
INFO - root - 2017-12-09 05:25:34.123938: step 1060, loss = 492.26, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:19m:58s remains)
INFO - root - 2017-12-09 05:25:40.043082: step 1070, loss = 492.89, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:37m:06s remains)
INFO - root - 2017-12-09 05:25:46.014857: step 1080, loss = 495.15, batch loss = 0.69 (13.1 examples/sec; 0.609 sec/batch; 56h:04m:08s remains)
INFO - root - 2017-12-09 05:25:52.015163: step 1090, loss = 499.35, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:43m:35s remains)
INFO - root - 2017-12-09 05:25:58.145469: step 1100, loss = 504.81, batch loss = 0.69 (13.2 examples/sec; 0.606 sec/batch; 55h:44m:24s remains)
2017-12-09 05:25:58.762180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07][-1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07 -1.117191e-07]]...]
INFO - root - 2017-12-09 05:26:04.928108: step 1110, loss = 509.52, batch loss = 0.69 (13.1 examples/sec; 0.608 sec/batch; 56h:00m:29s remains)
INFO - root - 2017-12-09 05:26:11.039318: step 1120, loss = 516.18, batch loss = 0.69 (12.9 examples/sec; 0.622 sec/batch; 57h:14m:31s remains)
INFO - root - 2017-12-09 05:26:17.045923: step 1130, loss = 518.52, batch loss = 0.69 (13.3 examples/sec; 0.599 sec/batch; 55h:10m:31s remains)
INFO - root - 2017-12-09 05:26:23.056278: step 1140, loss = 523.55, batch loss = 0.69 (13.3 examples/sec; 0.600 sec/batch; 55h:15m:28s remains)
INFO - root - 2017-12-09 05:26:29.067308: step 1150, loss = 521.95, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 55h:20m:17s remains)
INFO - root - 2017-12-09 05:26:35.142454: step 1160, loss = 525.08, batch loss = 0.69 (13.2 examples/sec; 0.604 sec/batch; 55h:37m:44s remains)
INFO - root - 2017-12-09 05:26:41.106727: step 1170, loss = 530.10, batch loss = 0.69 (13.2 examples/sec; 0.608 sec/batch; 55h:56m:51s remains)
INFO - root - 2017-12-09 05:26:47.078995: step 1180, loss = 542.41, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:35m:30s remains)
INFO - root - 2017-12-09 05:26:53.228060: step 1190, loss = 558.19, batch loss = 0.69 (13.0 examples/sec; 0.617 sec/batch; 56h:45m:36s remains)
INFO - root - 2017-12-09 05:26:59.253626: step 1200, loss = 567.26, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:39m:32s remains)
2017-12-09 05:26:59.891763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07][-3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07 -3.8369379e-07]]...]
INFO - root - 2017-12-09 05:27:05.935826: step 1210, loss = 578.52, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:42m:31s remains)
INFO - root - 2017-12-09 05:27:12.013333: step 1220, loss = 583.45, batch loss = 0.69 (13.2 examples/sec; 0.607 sec/batch; 55h:50m:10s remains)
INFO - root - 2017-12-09 05:27:18.096171: step 1230, loss = 589.44, batch loss = 0.69 (12.8 examples/sec; 0.626 sec/batch; 57h:33m:44s remains)
INFO - root - 2017-12-09 05:27:24.289318: step 1240, loss = 593.40, batch loss = 0.69 (13.3 examples/sec; 0.600 sec/batch; 55h:14m:50s remains)
INFO - root - 2017-12-09 05:27:30.415243: step 1250, loss = 593.35, batch loss = 0.69 (13.6 examples/sec; 0.589 sec/batch; 54h:14m:11s remains)
INFO - root - 2017-12-09 05:27:36.440037: step 1260, loss = 596.59, batch loss = 0.69 (13.1 examples/sec; 0.609 sec/batch; 56h:02m:53s remains)
INFO - root - 2017-12-09 05:27:42.305217: step 1270, loss = 602.80, batch loss = 0.69 (17.5 examples/sec; 0.456 sec/batch; 41h:58m:55s remains)
INFO - root - 2017-12-09 05:27:48.189738: step 1280, loss = 607.04, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:41m:46s remains)
INFO - root - 2017-12-09 05:27:54.292320: step 1290, loss = 607.29, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:40m:28s remains)
INFO - root - 2017-12-09 05:28:00.268145: step 1300, loss = 608.44, batch loss = 0.69 (13.4 examples/sec; 0.597 sec/batch; 54h:54m:10s remains)
2017-12-09 05:28:00.926011: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07][2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07 2.2033616e-07]]...]
